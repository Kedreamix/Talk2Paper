<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-26  Through the Looking Glass A Dual Perspective on Weakly-Supervised   Few-Shot Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-70f733a1351b7f926cf769ac3fa854a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-26-更新"><a href="#2025-08-26-更新" class="headerlink" title="2025-08-26 更新"></a>2025-08-26 更新</h1><h2 id="Through-the-Looking-Glass-A-Dual-Perspective-on-Weakly-Supervised-Few-Shot-Segmentation"><a href="#Through-the-Looking-Glass-A-Dual-Perspective-on-Weakly-Supervised-Few-Shot-Segmentation" class="headerlink" title="Through the Looking Glass: A Dual Perspective on Weakly-Supervised   Few-Shot Segmentation"></a>Through the Looking Glass: A Dual Perspective on Weakly-Supervised   Few-Shot Segmentation</h2><p><strong>Authors:Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li</strong></p>
<p>Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1&#x2F;24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2% improvement on Pascal-5\textsuperscript{i} and a 9.7% improvement on COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jarch-ma/TLG">https://github.com/jarch-ma/TLG</a>. </p>
<blockquote>
<p>元学习旨在均匀采样具有相同类别和相似属性的同质支持-查询对，并通过相同的网络架构提取有用的归纳偏见。然而，这种相同的网络设计导致了过度语义同质化。为了解决这一问题，我们提出了一种新型的同源但异构图网络。我们将支持-查询对视为双重视角，引入异构图视觉聚合（HA）模块，以增强互补性同时保留语义共性。为了进一步减少语义噪声并放大异质语义的独特性，我们设计了异质转移（HT）模块。最后，我们提出异质CLIP（HC）文本信息，以增强多模式模型的泛化能力。在弱监督少样本语义分割（WFSS）任务中，TLG仅使用现有最先进的模型参数的1&#x2F;24，就在Pascal-5i上实现了13.2%的提升，在COCO-20i上实现了9.7%的提升。据我们所知，TLG还是第一个在相同骨干架构下，弱监督（图像级）模型优于全监督（像素级）模型的实例。代码可访问<a target="_blank" rel="noopener" href="https://github.com/jarch-ma/TLG%E3%80%82">https://github.com/jarch-ma/TLG。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16159v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种解决少样本语义分割任务的方法，通过引入同源异构网络和模块，增强模型对同质支持查询对的采样效果，并提升模型的泛化能力。同时，该方法在Pascal-5i和COCO-20i任务上取得了显著的改进效果，并提供了可用代码。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种基于元学习的方法，旨在通过统一采样同质支持查询对，提取有用的归纳偏见。</li>
<li>提出了一种新型的同源异构网络设计，通过处理支持查询对作为双重视角，增强互补性并保留语义共性。</li>
<li>引入了异构视觉聚合模块（HA），用于增强语义共同性并减少语义噪声。</li>
<li>设计了异构传输模块（HT），以进一步突出异构语义的独特性。</li>
<li>通过引入异构CLIP（HC）文本信息，增强了多模态模型的泛化能力。</li>
<li>在弱监督少样本语义分割任务上取得了显著成果，包括在Pascal-5i和COCO-20i上的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16159">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c71cdfe6f191cfd11e3da8aeeef995a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1348c487c3205d6b0f55293dd4dc2f63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99d9e7724dd5e54dc10ea151a3cb6018.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7231c6631b8c439333e9c2cff6cb0c62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a763d8ed668438a9ac0d61b9bd7d5c3e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Beyond-Human-prompting-Adaptive-Prompt-Tuning-with-Semantic-Alignment-for-Anomaly-Detection"><a href="#Beyond-Human-prompting-Adaptive-Prompt-Tuning-with-Semantic-Alignment-for-Anomaly-Detection" class="headerlink" title="Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment   for Anomaly Detection"></a>Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment   for Anomaly Detection</h2><p><strong>Authors:Pi-Wei Chen, Jerry Chun-Wei Lin, Wei-Han Chen, Jia Ji, Zih-Ching Chen, Feng-Hao Yeh, Chao-Chun Chen</strong></p>
<p>Pre-trained Vision-Language Models (VLMs) have recently shown promise in detecting anomalies. However, previous approaches are fundamentally limited by their reliance on human-designed prompts and the lack of accessible anomaly samples, leading to significant gaps in context-specific anomaly understanding. In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning with semantic alignment for anomaly detection (APT), a groundbreaking prior knowledge-free, few-shot framework and overcomes the limitations of traditional prompt-based approaches. APT uses self-generated anomaly samples with noise perturbations to train learnable prompts that capture context-dependent anomalies in different scenarios. To prevent overfitting to synthetic noise, we propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively aligns the prompts with general anomaly semantics while incorporating diverse synthetic anomaly. Our system not only advances pixel-wise anomaly detection, but also achieves state-of-the-art performance on multiple benchmark datasets without requiring prior knowledge for prompt crafting, establishing a robust and versatile solution for real-world anomaly detection. </p>
<blockquote>
<p>预训练视觉语言模型（VLMs）最近在异常检测方面显示出巨大的潜力。然而，之前的方法从根本上受限于其依赖于人为设计的提示和缺乏可访问的异常样本，导致在特定上下文中的异常理解上存在较大差距。在本文中，我们提出了用于异常检测的基于语义对齐的自适应提示调整（APT）方法。APT是一种突破性的无先验知识、小样例框架，克服了传统基于提示的方法的限制。APT使用带有噪声扰动的自生成异常样本，训练学习提示，以捕获不同场景中与上下文相关的异常。为了防止对合成噪声的过拟合，我们提出了一种自优化元提示引导方案（SMGS），该方案通过迭代方式将提示与通用异常语义对齐，同时融入多样化的合成异常。我们的系统不仅推动了像素级的异常检测，而且在多个基准数据集上实现了最先进的性能，无需为提示设计提供先验知识，为实际异常检测提供了稳健且通用的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练视觉语言模型在异常检测中展现出潜力，但受限于人工设计的提示和缺乏可访问的异常样本，导致对特定上下文中的异常理解存在差距。本文提出无需先验知识的少样本框架APT（自适应提示调整与语义对齐异常检测法），通过生成带有噪声扰动的异常样本训练可学习提示，以捕获不同场景中的上下文相关异常。为防止过度拟合合成噪声，提出自优化元提示引导方案SMGS，该方案可迭代地对齐提示与常规异常语义，同时融入多样的合成异常。APT不仅推动了像素级的异常检测，还在多个基准数据集上实现了最先进的性能，无需事先了解提示制作知识，为实际异常检测提供了稳健且通用的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练视觉语言模型在异常检测领域具有潜力。</li>
<li>传统方法受限于人工设计的提示和缺乏异常样本。</li>
<li>APT框架通过生成带有噪声扰动的异常样本训练可学习提示。</li>
<li>APT利用自优化元提示引导方案SMGS，防止过度拟合合成噪声。</li>
<li>APT可捕获不同场景中的上下文相关异常。</li>
<li>APT在像素级异常检测方面取得进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-418f58b4f434c682df091e2a579c4918.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9428d3555145cbbed7b5337fdeaad49b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e45c165c36458fc0824f0260e34c61ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Integrating-Time-Series-into-LLMs-via-Multi-layer-Steerable-Embedding-Fusion-for-Enhanced-Forecasting"><a href="#Integrating-Time-Series-into-LLMs-via-Multi-layer-Steerable-Embedding-Fusion-for-Enhanced-Forecasting" class="headerlink" title="Integrating Time Series into LLMs via Multi-layer Steerable Embedding   Fusion for Enhanced Forecasting"></a>Integrating Time Series into LLMs via Multi-layer Steerable Embedding   Fusion for Enhanced Forecasting</h2><p><strong>Authors:Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng</strong></p>
<p>Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at <a target="_blank" rel="noopener" href="https://github.com/One1sAll/MSEF">https://github.com/One1sAll/MSEF</a>. </p>
<blockquote>
<p>时间序列（TS）数据在各个应用领域都普遍存在，使得时间序列预测（TSF）成为一项基本任务。随着大型语言模型（LLM）的惊人发展，已经开发了许多方法来适应LLM进行时间序列预测。尽管现有方法已经挖掘出了LLM在理解TS数据方面的潜力，但它们固有的对TS信息浅层集成的限制也显现出来。在此，LLM通常仅在输入层访问TS表示，这导致TS表示的影响在深层中逐渐消失，并最终导致文本嵌入和TS表示之间的不适应。在本文中，我们提出了多层可转向嵌入融合（MSEF）这一新框架，使LLM能够直接在所有深度访问时间序列模式，从而减轻深层中TS信息的逐步损失。具体而言，MSEF利用现成的时间序列基础模型来提取语义丰富的嵌入，这些嵌入通过与LLM各层的中间文本表示相结合，通过特定层的转向向量进行融合。这些转向向量旨在持续优化时间序列和文本模式之间的对齐，并促进特定层的适应机制，确保有效的少样本学习能力。在七个基准测试上的实验结果表明，与基线相比，MSEF表现出显著的性能改进，在均方误差（MSE）方面平均降低了31.8%。代码可访问<a target="_blank" rel="noopener" href="https://github.com/One1sAll/MSEF%E3%80%82">https://github.com/One1sAll/MSEF。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16059v1">PDF</a> To be published in CIKM 2025</p>
<p><strong>Summary</strong>：针对时间序列数据（TS），基于大型语言模型（LLMs）的时间序列预测（TSF）方法在近年来不断出现，但由于传统方法局限于在浅层层面融合时间序列信息，导致其使用效率和深度有待提升。本论文提出了多层级可调整嵌入融合（MSEF）框架，它能够引导LLMs在不同深度层直接获取时间序列模式，进而缓解深层信息丢失的问题。该框架结合了预先训练的时间序列基础模型与文本表示模型，并借助特定层级的转向向量，在少样本场景下表现出了优越的性能。在七个基准测试上的实验结果表明，与基准模型相比，MSEF的性能显著提升了平均MSE减少达到惊人的高达近百分之三十。该项目开源的代码位于指定网址。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>时间序列数据在各领域应用广泛，时间序列预测成为关键任务。大型语言模型在时间序列预测中的应用逐渐显现。</li>
<li>传统方法对于时间序列信息的融合仅限于浅层层次，导致深层信息丢失的问题。为此，需要改进大型语言模型的融合策略以提升性能。</li>
<li>MSEF框架的提出旨在引导大型语言模型在所有深度层次获取时间序列模式，以提高预测的精准度和广度。其主要结合时间信息和文本嵌入模型的联合优化方案来达到效果。这标志着针对大规模语言的时序数据分析走向更深层次的处理策略。</li>
<li>MSEF框架通过利用预先训练的时间序列基础模型提取语义丰富的嵌入信息，并与文本表示模型相融合，实现了跨层级的融合效果。这种跨层融合有助于实现信息的有效利用和性能的进一步提高。其中特定层级的转向向量是其创新设计之一，它在融合时间信息和文本模态方面起到关键作用。同时促进了层级的特定适应性机制的形成与完善，极大程度上促进了少量数据的实际应用价值及其对未来业务模式的参考价值进一步提升及成长 。证明上述自适应调整策略和方案可以在更复杂的现实场景中获得更为出色的应用效果及市场表现。这也进一步增强了行业领域对该技术的期待和关注。具体应用场景涉及实际数据模拟场景分析及市场需求理解等 。其公开的代码可供进一步研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16059">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-95d4995175d164211d388b6eddd4a2ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b437524433691bbb4d8d8e5891cd7ca2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76820f9b411a03ea639cbb9ea8dc12be.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Boosting-Pathology-Foundation-Models-via-Few-shot-Prompt-tuning-for-Rare-Cancer-Subtyping"><a href="#Boosting-Pathology-Foundation-Models-via-Few-shot-Prompt-tuning-for-Rare-Cancer-Subtyping" class="headerlink" title="Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare   Cancer Subtyping"></a>Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare   Cancer Subtyping</h2><p><strong>Authors:Dexuan He, Xiao Zhou, Wenbin Guan, Liyuan Zhang, Xiaoman Zhang, Sinuo Xu, Ge Wang, Lifeng Wang, Xiaojun Yuan, Xin Sun, Yanfeng Wang, Kun Sun, Ya Zhang, Weidi Xie</strong></p>
<p>Rare cancers comprise 20-25% of all malignancies but face major diagnostic challenges due to limited expert availability-especially in pediatric oncology, where they represent over 70% of cases. While pathology vision-language (VL) foundation models show promising zero-shot capabilities for common cancer subtyping, their clinical performance for rare cancers remains limited. Existing multi-instance learning (MIL) methods rely only on visual features, overlooking cross-modal knowledge and compromising interpretability critical for rare cancer diagnosis. To address this limitation, we propose PathPT, a novel framework that fully exploits the potential of vision-language pathology foundation models through spatially-aware visual aggregation and task-specific prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision into fine-grained tile-level guidance by leveraging the zero-shot capabilities of VL models, thereby preserving localization on cancerous regions and enabling cross-modal reasoning through prompts aligned with histopathological semantics. We benchmark PathPT on eight rare cancer datasets(four adult and four pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer datasets, evaluating four state-of-the-art VL models and four MIL frameworks under three few-shot settings. Results show that PathPT consistently delivers superior performance, achieving substantial gains in subtyping accuracy and cancerous region grounding ability. This work advances AI-assisted diagnosis for rare cancers, offering a scalable solution for improving subtyping accuracy in settings with limited access to specialized expertise. </p>
<blockquote>
<p>罕见癌症占所有恶性肿瘤的20-25%，但由于专家资源有限，特别是在儿童肿瘤学中（罕见癌症占病例的70%以上），其诊断面临重大挑战。虽然病理学视觉语言（VL）基础模型在常见的癌症分型中显示出有前景的零样本能力，但它们在罕见癌症的临床表现仍然有限。现有的多实例学习（MIL）方法仅依赖于视觉特征，忽视了跨模态知识，这对罕见癌症诊断至关重要。为了解决这一局限性，我们提出了PathPT，这是一个全新的框架，它充分利用了视觉语言病理学基础模型的潜力，通过空间感知的视觉聚合和特定任务的提示调整。与传统的MIL不同，PathPT通过将WSI级别的监督转化为精细的瓦片级别的指导，利用VL模型的零样本能力，从而保留对癌变区域的定位，并通过与病理语义对齐的提示实现跨模态推理。我们在涵盖56种亚型和2910个WSI的八个罕见癌症数据集（四个成人和四个儿科）以及三个常见癌症数据集上测试了PathPT，在三种小样例设置下评估了四种最先进的VL模型和四种MIL框架。结果表明，PathPT始终表现出卓越的性能，在分型准确性和癌变区域定位能力方面实现了显著的改进。这项工作推动了人工智能在罕见癌症诊断方面的应用，为在缺乏专业专家的情况下提高分型准确性提供了可扩展的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15904v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了罕见癌症诊断面临的挑战，尤其是儿科肿瘤学中罕见癌症占比超过70%的情况。虽然病理视觉语言基础模型对常见癌症分型表现出零样本能力，但对罕见癌症的临床表现仍然有限。为此，提出了一种名为PathPT的新框架，该框架充分利用了视觉语言病理学基础模型的潜力，通过空间感知的视觉聚合和特定任务的提示调整来解决这个问题。PathPT将WSI级别的监督转化为精细的瓷砖级别的指导，利用VL模型的零样本能力，从而保留对癌变区域的定位，并通过与病理语义对齐的提示实现跨模态推理。在多个罕见癌症数据集上的实验表明，PathPT在子类型准确性和癌变区域定位能力方面表现出卓越的性能。这项工作为罕见癌症的AI辅助诊断提供了可扩展的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>罕见癌症占所有恶性肿瘤的20-25%，在儿科肿瘤学中占比更高，达到70%以上，诊断面临重大挑战。</li>
<li>病理视觉语言基础模型对常见癌症分型具有零样本能力，但对罕见癌症的临床表现有限。</li>
<li>PathPT框架充分利用视觉语言病理学基础模型的潜力，通过空间感知的视觉聚合和特定任务的提示调整来解决上述问题。</li>
<li>PathPT将WSI级别的监督转化为精细瓷砖级别的指导，利用VL模型的零样本能力保留癌变区域定位并实现跨模态推理。</li>
<li>PathPT在多个罕见癌症数据集上的实验表现优越，显著提高子类型准确性和癌变区域定位能力。</li>
<li>PathPT为罕见癌症的AI辅助诊断提供了可扩展的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15904">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d8c076e921f5fc8dd356b01cc00dd154.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8664dfba1e1b2a68ce02db9c8a326e00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70f733a1351b7f926cf769ac3fa854a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a5d52dbe68463983b7c7452b7c3532.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Cyberbullying-Detection-via-Aggression-Enhanced-Prompting"><a href="#Cyberbullying-Detection-via-Aggression-Enhanced-Prompting" class="headerlink" title="Cyberbullying Detection via Aggression-Enhanced Prompting"></a>Cyberbullying Detection via Aggression-Enhanced Prompting</h2><p><strong>Authors:Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</strong></p>
<p>Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks. </p>
<blockquote>
<p>检测社交媒体上的网络欺凌仍然是一个关键挑战，因为其表达微妙且多样。本研究旨在探讨在统一训练框架内将攻击检测作为辅助任务是否可以提高大型语言模型（LLM）在网络欺凌检测中的通用性和性能。实验采用五个攻击数据集和一个网络欺凌数据集，使用指令调整的大型语言模型。我们评估了多种策略：零样本、少样本、独立LoRA微调和多任务学习（MTL）。鉴于多任务学习的不一致结果，我们提出了一种丰富的提示管道方法，其中攻击预测被嵌入到网络欺凌检测提示中以提供上下文增强。初步结果表明，丰富的提示管道始终优于标准的LoRA微调，这表明攻击检测上下文可显著提高网络欺凌检测能力。该研究突出了辅助任务（如攻击检测）的潜力，可以改进大型语言模型在社交网络安全关键应用的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06360v2">PDF</a> Accepted to RANLP 2025</p>
<p><strong>Summary</strong>：本研究探讨了将攻击检测作为辅助任务，在一个统一的训练框架内，是否能够提升大型语言模型在网络欺凌检测中的通用性和性能。实验在五个攻击数据集和一个网络欺凌数据集上进行，评估了零样本、少样本、独立LoRA微调以及多任务学习的策略。在此基础上提出了增强提示管道方法，其中将攻击预测嵌入到网络欺凌检测提示中提供上下文增强。初步结果显示增强提示管道方法的性能优于标准LoRA微调方法，证明了攻击信息上下文对网络欺凌检测的重要性。这突出展示了将攻击检测作为辅助任务，对于提高大型语言模型在社交网络安全关键应用中的通用性潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>网络欺凌检测面临挑战，因其表达形式微妙多变。</li>
<li>集成攻击检测作为辅助任务能提高大型语言模型在网络欺凌检测中的性能。</li>
<li>实验在多个数据集上进行，评估了不同策略，包括零样本、少样本、独立LoRA微调及多任务学习。</li>
<li>提出增强提示管道方法，将攻击预测嵌入网络欺凌检测提示中。</li>
<li>初步结果显示增强提示管道方法性能优越，证明攻击信息上下文对网络欺凌检测的重要性。</li>
<li>辅助任务如攻击检测对于提高大型语言模型在社交网络安全关键应用中的通用性有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d9fdc1491b40969373dc8bf568ab4f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28d984c26a74d681aa4b4117dfa35a50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d65f73996b250f0e66b84d6ac7b3bf1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Perceptual-Implications-of-Automatic-Anonymization-in-Pathological-Speech"><a href="#Perceptual-Implications-of-Automatic-Anonymization-in-Pathological-Speech" class="headerlink" title="Perceptual Implications of Automatic Anonymization in Pathological   Speech"></a>Perceptual Implications of Automatic Anonymization in Pathological   Speech</h2><p><strong>Authors:Soroosh Tayebi Arasteh, Saba Afza, Tri-Thien Nguyen, Lukas Buess, Maryam Parvin, Tomas Arias-Vergara, Paula Andrea Perez-Toro, Hiu Ching Hung, Mahshad Lotfinia, Thomas Gorges, Elmar Noeth, Maria Schuster, Seung Hee Yang, Andreas Maier</strong></p>
<p>Automatic anonymization techniques are essential for ethical sharing of pathological speech data, yet their perceptual consequences remain understudied. We present a comprehensive human-centered analysis of anonymized pathological speech, using a structured protocol involving ten native and non-native German listeners with diverse linguistic, clinical, and technical backgrounds. Listeners evaluated anonymized-original utterance pairs from 180 speakers spanning Cleft Lip and Palate, Dysarthria, Dysglossia, Dysphonia, and healthy controls. Speech was anonymized using state-of-the-art automatic methods (equal error rates in the range of 30-40%). Listeners completed Turing-style discrimination and quality rating tasks under zero-shot (single-exposure) and few-shot (repeated-exposure) conditions. Discrimination accuracy was high overall (91% zero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA: p&#x3D;0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization consistently reduced perceived quality across groups (from 83% to 59%, p&lt;0.001), with pathology-specific degradation patterns (one-way ANOVA: p&#x3D;0.005). Native listeners showed a non-significant trend toward higher original speech ratings (Delta&#x3D;4%, p&#x3D;0.199), but this difference was minimal after anonymization (Delta&#x3D;1%, p&#x3D;0.724). No significant gender-based bias was observed. Perceptual outcomes did not correlate with automatic metrics; intelligibility was linked to perceived quality in original speech but not after anonymization. These findings underscore the need for listener-informed, disorder-specific anonymization strategies that preserve both privacy and perceptual integrity. </p>
<blockquote>
<p>自动匿名化技术在病理语音数据的伦理共享中至关重要，但其感知后果仍研究不足。我们采用以人为中心的综合分析方法，对匿名病理语音进行研究，使用涉及十位德语本地人和非本地听众的结构化协议，这些听众具有不同的语言、临床和技术背景。听众评估了来自180名演讲者的匿名原始语句对，包括唇裂和腭裂、口齿不清、嗓音异常和正常对照。语音采用最先进的自动方法进行匿名处理（误差率范围为30%-40%）。听众在零样本（单次曝光）和少样本（多次曝光）条件下完成了图灵风格的辨别和质量评估任务。总体鉴别准确率较高（零样本91%；少样本93%），但不同疾病的鉴别准确率有所不同（重复测量方差分析：p&#x3D;0.007），范围从96%（口齿不清）到86%（嗓音异常）。匿名处理一致地降低了各组感知质量（从83%降至59%，p&lt;0.001），并呈现出针对特定病理的退化模式（单向方差分析：p&#x3D;0.005）。本地听众对原始语音的评分有非显著性上升趋势（Delta&#x3D;4%，p&#x3D;0.199），但这一差异在匿名处理后变得微乎其微（Delta&#x3D;1%，p&#x3D;0.724）。未观察到基于性别的显著偏见。感知结果与自动度量指标不相关；可理解性与原始语音的感知质量有关，但与匿名处理后的感知质量无关。这些发现强调需要基于听众和特定疾病的匿名化策略，既保护隐私又保持感知完整性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00409v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了自动匿名化技术在病理语音数据共享中的重要作用，并进行了以人为中心的分析。实验邀请了不同语言、临床和技术背景的听众，对来自不同语音障碍人群的匿名化语音进行辨识和评分。研究发现，尽管匿名化技术的错误率在可接受范围内，但它仍然会降低语音的感知质量，并可能影响听众对语音障碍的辨识准确度。因此，需要针对听众和特定障碍的匿名化策略来同时保护隐私和感知完整性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动匿名化技术在病理语音数据共享中至关重要，但其感知影响尚待研究。</li>
<li>通过结构化协议邀请多种背景的听众参与评估匿名化病理语音。</li>
<li>匿名化技术虽减少了隐私泄露风险，但会降低语音感知质量。</li>
<li>不同语音障碍在匿名化后的辨识准确度存在差异。</li>
<li>听众对原始语音的评分在匿名化后趋于一致，性别差异在感知中不显著。</li>
<li>自动度量与感知结果不相关，而可懂度与原始语音的感知质量有关。</li>
<li>需要开发兼顾隐私保护和感知完整性的听众特定匿名化策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00409">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8f8486dcba511503d46729d0911378ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c4f3a1317e373e2a31b40ad46c87251.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Continuous-Knowledge-Preserving-Decomposition-with-Adaptive-Layer-Selection-for-Few-Shot-Class-Incremental-Learning"><a href="#Continuous-Knowledge-Preserving-Decomposition-with-Adaptive-Layer-Selection-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Continuous Knowledge-Preserving Decomposition with Adaptive Layer   Selection for Few-Shot Class-Incremental Learning"></a>Continuous Knowledge-Preserving Decomposition with Adaptive Layer   Selection for Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Xiaojie Li, Jianlong Wu, Yue Yu, Liqiang Nie, Min Zhang</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) faces a critical challenge: balancing the retention of prior knowledge with the acquisition of new classes. Existing methods either freeze the backbone to prevent catastrophic forgetting, sacrificing plasticity, or add new modules, incurring high costs. These approaches treat pretrained models as black boxes, overlooking two key opportunities to exploit their internal capacity: reusing redundant representational space within layers and selectively adapting layers based on their sensitivity to forgetting. We propose CKPD-FSCIL, a unified framework that unlocks the underutilized capacity of pretrained weights, achieving a superior stability-plasticity balance with zero inference overhead. Our design integrates two continuously adapting mechanisms: At the weight level, a Continuous Knowledge-Preserving Decomposition mechanism uses feature covariance to split each weight matrix into a frozen subspace that safeguards prior knowledge and a learnable, redundant subspace for new tasks. At the layer level, a Continuous Adaptive Layer Selection mechanism leverages an Adapter Sensitivity Ratio to automatically select layers with the highest redundant capacity and lowest forgetting risk for adaptation. By targeting only safe, high-potential subspaces and layers, CKPD-FSCIL enables efficient adaptation. After each session, the learned adapters are merged back into the original weights, ensuring zero additional parameters or FLOPs during inference. Extensive experiments on multiple FSCIL benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches in both adaptability and knowledge retention. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/CKPD-FSCIL">https://github.com/xiaojieli0903/CKPD-FSCIL</a>. </p>
<blockquote>
<p>Few-Shot Class-Incremental Learning（FSCIL）面临一个关键挑战：如何在保留先前知识的同时学习新类别。现有方法要么冻结主干以防止灾难性遗忘，牺牲可塑性，要么添加新模块，产生高昂成本。这些方法将预训练模型视为黑箱，忽略了利用其内部容量的两个关键机会：在层内重新使用冗余的表示空间以及根据它们对遗忘的敏感性选择性地适应层。我们提出了CKPD-FSCIL，这是一个解锁预训练权重未充分利用容量的统一框架，以零推理开销实现了卓越的稳定性和可塑性平衡。我们的设计集成了两种连续适应机制：在权重层面，连续知识保留分解机制利用特征协方差将每个权重矩阵分割成一个保护先前知识的冻结子空间和一个用于新任务的可学习冗余子空间。在层层面，连续自适应层选择机制利用适配器敏感度比率自动选择具有最高冗余容量和最低遗忘风险的层进行适应。通过仅针对安全、高潜力的子空间和层进行定位，CKPD-FSCIL能够实现高效适应。在每个会话结束后，学习的适配器都会合并回原始权重，确保推理过程中没有额外的参数或浮点运算。在多个FSCIL基准测试上的广泛实验表明，我们的方法在不断适应和保留知识方面均优于最新技术。代码可在<a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/CKPD-FSCIL%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaojieli0903/CKPD-FSCIL上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05017v3">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/CKPD-FSCIL">https://github.com/xiaojieli0903/CKPD-FSCIL</a></p>
<p><strong>Summary</strong></p>
<p>该文探讨了Few-Shot类增量学习（FSCIL）中的关键问题，即在保留先前知识与学习新类别之间取得平衡。现有方法要么冻结主干网络以防止灾难性遗忘，牺牲可塑性，要么增加新模块，导致成本上升。本文提出CKPD-FSCIL框架，解锁了预训练权重的未充分利用容量，实现了稳定性与可塑性的卓越平衡，且无需推理开销。该框架设计了两种连续适应机制：在权重层面，通过特征协方差将每个权重矩阵分解为冻结子空间（保护先验知识）和可学习冗余子空间（用于新任务）；在层面层面，通过适配器敏感度比率自动选择冗余容量最高、遗忘风险最低的图层进行适应。CKPD-FSCIL仅针对安全、潜力高的子空间和图层进行高效适应。实验表明，该方法在适应性和知识保留方面均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot类增量学习（FSCIL）面临保留先前知识与学习新类别的平衡挑战。</li>
<li>现有方法存在缺点：冻结主干网络会牺牲可塑性，增加新模块则会导致成本上升。</li>
<li>CKPD-FSCIL框架利用预训练模型的未充分利用容量，实现稳定性与可塑性的平衡。</li>
<li>CKPD-FSCIL通过两种连续适应机制进行工作：权重层面的知识保留分解和层面层面的自适应层选择。</li>
<li>CKPD-FSCIL仅针对安全、潜力高的子空间和图层进行高效适应，提高了效率。</li>
<li>CKPD-FSCIL实现了零推理开销，且实验表明其在适应性和知识保留方面优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05017">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1a8713f96d03b73c391efa05923a2cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61743a70899aea303a430af4e2f65b5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95dc096072fd2bac14565806138290fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e86a8eeb042d549d549d7bf40fd8e9b2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-abfe74a9b70a751ad8e43d00d328a043.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-26  Transcranial Photoacoustic Imaging for Human Intracranial Pressure   Evaluation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c2a037de8223539a3c9720f4306a63c4.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-26  LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
