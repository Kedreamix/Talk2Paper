<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-26  Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-28852eba37722a9cc7d23e4673ba84a0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-26-æ›´æ–°"><a href="#2025-08-26-æ›´æ–°" class="headerlink" title="2025-08-26 æ›´æ–°"></a>2025-08-26 æ›´æ–°</h1><h2 id="Constraints-Guided-Diffusion-Reasoner-for-Neuro-Symbolic-Learning"><a href="#Constraints-Guided-Diffusion-Reasoner-for-Neuro-Symbolic-Learning" class="headerlink" title="Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning"></a>Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning</h2><p><strong>Authors:Xuan Zhang, Zhijian Zhou, Weidi Xu, Yanting Miao, Chao Qu, Yuan Qi</strong></p>
<p>Enabling neural networks to learn complex logical constraints and fulfill symbolic reasoning is a critical challenge. Bridging this gap often requires guiding the neural networkâ€™s output distribution to move closer to the symbolic constraints. While diffusion models have shown remarkable generative capability across various domains, we employ the powerful architecture to perform neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline adopts a two-stage training strategy: the first stage focuses on cultivating basic reasoning abilities, while the second emphasizes systematic learning of logical constraints. To impose hard constraints on neural outputs in the second stage, we formulate the diffusion reasoner as a Markov decision process and innovatively fine-tune it with an improved proximal policy optimization algorithm. We utilize a rule-based reward signal derived from the logical consistency of neural outputs and adopt a flexible strategy to optimize the diffusion reasonerâ€™s policy. We evaluate our methodology on some classical symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and preference learning. Experimental results demonstrate that our approach achieves outstanding accuracy and logical consistency among neural networks. </p>
<blockquote>
<p>ä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„é€»è¾‘çº¦æŸå¹¶å®ç°ç¬¦å·æ¨ç†æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚å¼¥åˆè¿™ä¸€é¸¿æ²Ÿé€šå¸¸éœ€è¦å¼•å¯¼ç¥ç»ç½‘ç»œçš„è¾“å‡ºåˆ†å¸ƒä»¥æ¥è¿‘ç¬¦å·çº¦æŸã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å·²åœ¨å„ä¸ªé¢†åŸŸæ˜¾ç¤ºå‡ºå“è¶Šçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†æˆ‘ä»¬é‡‡ç”¨å¼ºå¤§çš„æ¶æ„è¿›è¡Œç¥ç»ç¬¦å·å­¦ä¹ å¹¶è§£å†³é€»è¾‘è°œé¢˜ã€‚æˆ‘ä»¬çš„åŸºäºæ‰©æ•£çš„ç®¡é“é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µä¾§é‡äºåŸ¹å…»åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™å¼ºè°ƒé€»è¾‘çº¦æŸçš„ç³»ç»Ÿå­¦ä¹ ã€‚ä¸ºäº†åœ¨ç¬¬äºŒé˜¶æ®µå¯¹ç¥ç»è¾“å‡ºæ–½åŠ ç¡¬æ€§çº¦æŸï¼Œæˆ‘ä»¬å°†æ‰©æ•£æ¨ç†æœºåˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶åˆ›æ–°åœ°é€šè¿‡æ”¹è¿›çš„ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ç®—æ³•å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·ï¼Œæ ¹æ®ç¥ç»è¾“å‡ºçš„é€»è¾‘ä¸€è‡´æ€§ï¼Œå¹¶é‡‡å–çµæ´»çš„ç­–ç•¥æ¥ä¼˜åŒ–æ‰©æ•£æ¨ç†æœºçš„ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä¸€äº›ç»å…¸çš„ç¬¦å·æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°ç‹¬ã€è¿·å®«ã€è·¯å¾„å¯»æ‰¾å’Œåå¥½å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¥ç»ç½‘ç»œä¸­å®ç°äº†å‡ºè‰²çš„å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16524v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚é€»è¾‘çº¦æŸå¹¶å®ç°ç¬¦å·æ¨ç†æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œç¥ç»ç¬¦å·å­¦ä¹ å¹¶è§£å†³é€»è¾‘éš¾é¢˜ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µåŸ¹å…»åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µé‡ç‚¹å­¦ä¹ é€»è¾‘çº¦æŸã€‚ç¬¬äºŒé˜¶æ®µå¯¹ç¥ç»è¾“å‡ºæ–½åŠ ç¡¬çº¦æŸï¼Œå°†æ‰©æ•£æ¨ç†å™¨åˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ç”¨æ”¹è¿›åçš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•è¿›è¡Œå¾®è°ƒã€‚åˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ‰©æ•£æ¨ç†å™¨çš„ç­–ç•¥ï¼Œå¹¶åœ¨ä¸€äº›ç»å…¸çš„ç¬¦å·æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°ç‹¬ã€è¿·å®«ã€è·¯å¾„å¯»æ‰¾å’Œåå¥½å­¦ä¹ ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¥ç»ç½‘ç»œä¸­å®ç°äº†å‡ºè‰²çš„å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚é€»è¾‘çº¦æŸæ˜¯é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºç¥ç»ç¬¦å·å­¦ä¹ ä¸é€»è¾‘éš¾é¢˜è§£å†³ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µåŸ¹å…»åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œç¬¬äºŒé˜¶æ®µé‡ç‚¹å­¦ä¹ é€»è¾‘çº¦æŸã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå°†æ‰©æ•£æ¨ç†å™¨åˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œç”¨æ”¹è¿›ç®—æ³•è¿›è¡Œå¾®è°ƒã€‚</li>
<li>åˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±ä¿¡å·ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>åœ¨å¤šç§ç¬¦å·æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬æ•°ç‹¬ã€è¿·å®«ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d86ac5fca5afcaaf21ec2f55dc2a9a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aa9ea950d3646727e2617c2a7a792b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-528d064f50e0c97c701e18e0815cc03f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1aa66efff97327bc593b0bf76df4b461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-375f8ddb907eee5c07aa1645d0805d83.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FLAMES-Improving-LLM-Math-Reasoning-via-a-Fine-Grained-Analysis-of-the-Data-Synthesis-Pipeline"><a href="#FLAMES-Improving-LLM-Math-Reasoning-via-a-Fine-Grained-Analysis-of-the-Data-Synthesis-Pipeline" class="headerlink" title="FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the   Data Synthesis Pipeline"></a>FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the   Data Synthesis Pipeline</h2><p><strong>Authors:Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng</strong></p>
<p>Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet. </p>
<blockquote>
<p>è¿‘æœŸï¼Œåˆ©ç”¨åˆæˆæ•°æ®æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å·¥ä½œé‡‡ç”¨äº†ç‹¬ç‰¹çš„è®¾ç½®ï¼Œè¿™ä½¿å¾—æ•°æ®åˆæˆç­–ç•¥çš„å¯¹æ¯”å˜å¾—ä¸åˆ‡å®é™…ã€‚è¿™å¼•å‘äº†è®¸å¤šå…³äºåˆæˆæ•°æ®æµç¨‹ä¸­ä¸åŒå› ç´ ä½œç”¨çš„é—®é¢˜æ‚¬è€Œæœªå†³ï¼Œä¾‹å¦‚è¿‡æ»¤ä½è´¨é‡é—®é¢˜çš„å½±å“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FLAMESï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†æ•°æ®åˆæˆçš„æ¡†æ¶ï¼Œå¹¶å¯¹10ç§ç°æœ‰çš„æ•°æ®åˆæˆç­–ç•¥å’Œå¤šç§å½±å“åˆæˆæ•°å­¦æ¨ç†æ•°æ®æ€§èƒ½çš„å…¶ä»–å› ç´ è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„FLAMESå®éªŒæä¾›äº†å…³äºåˆæˆæ•°æ®éš¾åº¦å’Œå¤šæ ·æ€§ä¹‹é—´æœ€ä½³å¹³è¡¡çš„å®è´µè§è§£ã€‚é¦–å…ˆï¼Œæ—¨åœ¨å¢åŠ é—®é¢˜å¤æ‚æ€§çš„æ•°æ®ä»£ç†å¯ä»¥åœ¨å¤§å¤šæ•°æ•°å­¦æŒ‡æ ‡ä¸Šå¸¦æ¥æœ€ä½³æ”¹è¿›ã€‚å…¶æ¬¡ï¼Œåœ¨å›ºå®šçš„æ•°æ®ç”Ÿæˆé¢„ç®—ä¸‹ï¼Œä¿æŒæ›´é«˜çš„é—®é¢˜è¦†ç›–ç‡æ¯”ä»…ä¿ç•™å…·æœ‰å¯é è§£å†³æ–¹æ¡ˆçš„é—®é¢˜æ›´é‡è¦ã€‚ç¬¬ä¸‰ï¼ŒåŸºäºGSM8Kå’ŒMATHçš„åˆæˆæ•°æ®å¯ä»¥åœ¨ç«èµ›çº§åˆ«çš„åŸºå‡†æµ‹è¯•ä¸Šå¸¦æ¥æ”¹è¿›ï¼Œå±•ç¤ºäº†ä»æ˜“åˆ°éš¾çš„æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨FLAMESå®éªŒçš„è§è§£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§æ–°å‹æ•°æ®åˆæˆç­–ç•¥ï¼Œä»¥æé«˜è·¨åŸŸæ³›åŒ–å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†FLAMESæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬æ–°é¢–å’Œç°æœ‰æ•°æ®åˆæˆç­–ç•¥çš„æœ‰æ•ˆèåˆï¼Œå®ƒåœ¨OlympiadBenchï¼ˆ+15.7ï¼‰ã€CollegeMathï¼ˆ+4.5ï¼‰ã€GSMPlusï¼ˆ+6.5ï¼‰å’ŒMATHï¼ˆ+3.1ï¼‰ç­‰å…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚ä½¿ç”¨FLAMESæ•°æ®é›†å¾®è°ƒQwen2.5-Math-7Bæ¨¡å‹åœ¨MATHä¸Šè¾¾åˆ°81.4%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„Llama3 405Bã€GPT-4oå’ŒClaude 3.5 Sonnetæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16514v1">PDF</a> To appear at EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºFLAMESçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ•°å­¦æ¨ç†æ•°æ®çš„åˆæˆç­–ç•¥ã€‚è¯¥ç ”ç©¶å¯¹ç°æœ‰çš„æ•°æ®åˆæˆç­–ç•¥è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶ï¼Œå¹¶æ¢è®¨äº†å½±å“åˆæˆæ•°å­¦æ¨ç†æ•°æ®æ€§èƒ½çš„å…¶ä»–å› ç´ ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè®¾è®¡ç”¨äºå¢åŠ é—®é¢˜å¤æ‚æ€§çš„æ•°æ®ä»£ç†å¯ä»¥å¸¦æ¥æœ€ä½³çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨FLAMESçš„å®éªŒç»“æœï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°å‹çš„æ•°æ®åˆæˆç­–ç•¥ï¼Œå¹¶å¼€å‘äº†FLAMESæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æœ€åï¼Œé€šè¿‡å¯¹ä¸€ä¸ªå¤§å‹æ¨¡å‹çš„å¾®è°ƒï¼Œåœ¨MATHæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥FLAMESæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ•°å­¦æ¨ç†æ•°æ®çš„åˆæˆç­–ç•¥ã€‚</li>
<li>ç³»ç»Ÿç ”ç©¶äº†ç°æœ‰çš„æ•°æ®åˆæˆç­–ç•¥ä»¥åŠå…¶ä»–å½±å“æ€§èƒ½çš„å› ç´ ã€‚</li>
<li>å‘ç°è®¾è®¡ç”¨äºå¢åŠ é—®é¢˜å¤æ‚æ€§çš„æ•°æ®ä»£ç†èƒ½å¸¦æ¥æœ€ä½³æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨å›ºå®šæ•°æ®ç”Ÿæˆé¢„ç®—ä¸‹ï¼Œä¿æŒæ›´é«˜çš„é—®é¢˜è¦†ç›–é¢æ¯”ä¿æŒåªæœ‰å¯é è§£å†³æ–¹æ¡ˆçš„é—®é¢˜æ›´é‡è¦ã€‚</li>
<li>GSM8Kå’ŒMATHä¸ºåŸºç¡€çš„æ•°æ®åˆæˆç­–ç•¥å¯ä»¥æ”¹å–„ç«èµ›çº§åˆ«çš„åŸºå‡†æµ‹è¯•è¡¨ç°ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹çš„FLAMESæ•°æ®é›†ï¼Œç»“åˆäº†æ–°å‹å’Œç°æœ‰çš„æ•°æ®åˆæˆç­–ç•¥ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae20a95963831f6f4dd880d0c4a4ab10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d96fea6ca146988bab27dacf56dd949a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05019a05cd2a664d6094f32b8d94545f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9c7226de7a2dad1e6574b7f0ffe6026.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OPERA-A-Reinforcement-Learningâ€“Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval"><a href="#OPERA-A-Reinforcement-Learningâ€“Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval" class="headerlink" title="OPERA: A Reinforcement Learningâ€“Enhanced Orchestrated Planner-Executor   Architecture for Reasoning-Oriented Multi-Hop Retrieval"></a>OPERA: A Reinforcement Learningâ€“Enhanced Orchestrated Planner-Executor   Architecture for Reasoning-Oriented Multi-Hop Retrieval</h2><p><strong>Authors:Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma</strong></p>
<p>Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERAâ€™s Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERAâ€™s superior performance, validating both the MAPGRPO method and OPERAâ€™s design. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Ameame1/OPERA">https://github.com/Ameame1/OPERA</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¯†é›†æ£€ç´¢å™¨çš„è¿›å±•æ¨åŠ¨äº†å¢å¼ºå‹æ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å‘å¤æ‚æ¨ç†çš„è·¨æ­¥æ£€ç´¢ä»»åŠ¡ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼š1ï¼‰é¢å‘æ¨ç†çš„è§„åˆ’ä¸å¤Ÿæœ‰æ•ˆï¼šä¹‹å‰çš„æ–¹æ³•åœ¨ç”Ÿæˆå¤æ‚æŸ¥è¯¢çš„ç¨³å¥å¤šæ­¥è§„åˆ’æ—¶é‡åˆ°å›°éš¾ï¼Œå› ä¸ºåŸºäºè§„åˆ™çš„åˆ†è§£å™¨åœ¨éå¸¸è§„é—®é¢˜ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚2ï¼‰æ¨ç†é©±åŠ¨çš„æ£€ç´¢ç»“æœä¸ä½³ï¼šç›¸å…³æ–¹æ³•é‡‡ç”¨æœ‰é™çš„æŸ¥è¯¢é‡æ„ï¼Œå¯¼è‡´ç»å¸¸é™·å…¥è¿­ä»£æ£€ç´¢å¾ªç¯ï¼Œæ— æ³•æ‰¾åˆ°å…³é”®æ–‡æ¡£ã€‚3ï¼‰ç¼ºä¹æ¨ç†æŒ‡å¯¼çš„è¿‡æ»¤ï¼šç°è¡Œæ–¹æ³•ç¼ºä¹ç²¾ç»†çš„æ¨ç†æ¥æœ‰æ•ˆè¿‡æ»¤å™ªå£°ç»“æœä¸­çš„å…³é”®ä¿¡æ¯ï¼Œé˜»ç¢äº†æ£€ç´¢çŸ¥è¯†çš„åˆ©ç”¨ã€‚è¿™äº›å±€é™æ€§æ ¹æœ¬æºäºå½“å‰RAGæ¶æ„ä¸­æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„å¼±è€¦åˆã€‚æˆ‘ä»¬å¼•å…¥äº†â€œååŒè§„åˆ’æ‰§è¡Œæ¨ç†æ¶æ„â€ï¼ˆOPERAï¼‰è¿™ä¸€æ–°å‹æ¨ç†é©±åŠ¨æ£€ç´¢æ¡†æ¶ã€‚OPERAçš„ç›®æ ‡è§„åˆ’æ¨¡å—ï¼ˆGPMï¼‰å°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œè¿™äº›å­ç›®æ ‡ç”±å…·æœ‰ç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆæ£€ç´¢ç»„ä»¶çš„æ¨ç†æ‰§è¡Œæ¨¡å—ï¼ˆREMï¼‰æ‰§è¡Œã€‚ä¸ºäº†è®­ç»ƒOPERAï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ™ºèƒ½ä½“æ¸è¿›ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPGRPOï¼‰ï¼Œè¿™æ˜¯GRPOçš„ä¸€ç§æ–°å‹å˜ä½“ã€‚åœ¨å¤æ‚çš„å¤šæ­¥åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ˜¾ç¤ºäº†OPERAçš„å“è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†MAPGRPOæ–¹æ³•å’ŒOPERAè®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ameame1/OPERA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Ameame1/OPERAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16438v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¯†é›†æ£€ç´¢å™¨çš„è¿›å±•æ¨åŠ¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å‘å¤æ‚æ¨ç†çš„å¤šè·³æ£€ç´¢ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Orchestrated Planner-Executor Reasoning Architectureï¼ˆOPERAï¼‰è¿™ä¸€æ–°çš„æ¨ç†é©±åŠ¨æ£€ç´¢æ¡†æ¶ã€‚OPERAçš„Goal Planning Moduleï¼ˆGPMï¼‰å°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œå¹¶ç”±Reason-Execute Moduleï¼ˆREMï¼‰æ‰§è¡Œï¼Œå…·æœ‰ç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆæ£€ç´¢çš„ä¸“é—¨ç»„ä»¶ã€‚é€šè¿‡Multi-Agents Progressive Group Relative Policy Optimizationï¼ˆMAPGRPOï¼‰æ–¹æ³•è®­ç»ƒOPERAï¼Œåœ¨å¤æ‚å¤šè·³åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†MAPGRPOæ–¹æ³•å’ŒOPERAè®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¯†é›†æ£€ç´¢å™¨çš„è¿›æ­¥æ¨åŠ¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆçš„è¿›æ­¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ¨ç†çš„å¤šè·³æ£€ç´¢ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚è§„åˆ’ã€æ£€ç´¢å’Œè¿‡æ»¤ç­‰æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>OPERAæ¡†æ¶é€šè¿‡åˆ†è§£é—®é¢˜ä¸ºå­ç›®æ ‡å¹¶æ‰§è¡Œï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚</li>
<li>OPERAåŒ…æ‹¬Goal Planning Moduleï¼ˆGPMï¼‰å’ŒReason-Execute Moduleï¼ˆREMï¼‰ï¼Œå…·æœ‰ç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆæ£€ç´¢çš„ä¸“é—¨ç»„ä»¶ã€‚</li>
<li>OPERAé€šè¿‡Multi-Agents Progressive Group Relative Policy Optimizationï¼ˆMAPGRPOï¼‰æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¤æ‚å¤šè·³åŸºå‡†æµ‹è¯•ä¸Šï¼ŒOPERAè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5288854966102d6ddb28705cd862cd8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757fdc0e351d7094bf2dee674197a1b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53bb1d94e6765c707d29c6334be8652.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b75c56225fc31b965dda4eae8b267bc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0ff9fa229a2391e799255296e28bdb4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OwkinZero-Accelerating-Biological-Discovery-with-AI"><a href="#OwkinZero-Accelerating-Biological-Discovery-with-AI" class="headerlink" title="OwkinZero: Accelerating Biological Discovery with AI"></a>OwkinZero: Accelerating Biological Discovery with AI</h2><p><strong>Authors:Nathan Bigaud, Vincent Cabeli, Meltem Gurel, Arthur Pignet, John Klein, Gilles Wainrib, Eric Durand</strong></p>
<p>While large language models (LLMs) are rapidly advancing scientific research, they continue to struggle with core biological reasoning tasks essential for translational and biomedical discovery. To address this limitation, we created and curated eight comprehensive benchmark datasets comprising over 300,000 verifiable question-and-answer pairs, each targeting critical challenges in drug discovery including target druggability, modality suitability, and drug perturbation effects. Using this resource, we developed the OwkinZero models by post-training open-source LLMs through a Reinforcement Learning from Verifiable Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero models substantially outperform larger, state-of-the-art commercial LLMs on these biological benchmarks. Remarkably, we uncover evidence of a key aspect of generalization: specialist models trained on a single task consistently outperform their base models on previously unseen tasks. This generalization effect is further amplified in our comprehensive OwkinZero models, which were trained on a mixture of datasets and achieve even broader cross-task improvements. This study represents a significant step toward addressing the biological reasoning blind spot in current LLMs, demonstrating that targeted reinforcement learning on carefully curated data can unlock generalizable performance in specialized models, thereby accelerating AI-driven biological discovery. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸå–å¾—äº†å¿«é€Ÿå‘å±•ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥åº”å¯¹å¯¹äºè¯ç‰©ç ”å‘å’Œç”Ÿç‰©åŒ»å­¦å‘ç°è‡³å…³é‡è¦çš„æ ¸å¿ƒç”Ÿç‰©æ¨ç†ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºå¹¶æ•´ç†äº†å…«ä¸ªç»¼åˆåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡30ä¸‡ä¸ªå¯éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½é’ˆå¯¹è¯ç‰©å‘ç°ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç›®æ ‡è¯ç‰©çš„å¯è¡Œæ€§ã€æ¨¡æ€çš„é€‚ç”¨æ€§ä»¥åŠè¯ç‰©çš„æ‰°åŠ¨å½±å“ã€‚åˆ©ç”¨è¿™ä¸€èµ„æºï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§åä¸ºâ€œæ¥è‡ªå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ â€çš„ç­–ç•¥ï¼Œå¯¹å¼€æºLLMè¿›è¡Œåè®­ç»ƒï¼Œå‘å±•äº†OwkinZeroæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸“ä¸šçš„8-32BOwkinZeroæ¨¡å‹åœ¨è¿™äº›ç”Ÿç‰©åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¤§å¤§è¶…è¿‡äº†æ›´å¤§ã€æœ€å…ˆè¿›çš„å•†ä¸šLLMã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®æ–¹é¢çš„è¯æ®ï¼šç»è¿‡å•ä¸€ä»»åŠ¡è®­ç»ƒçš„ä¸“é—¨æ¨¡å‹åœ¨æœªæ¥è§¦çš„ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºå…¶åŸºç¡€æ¨¡å‹ã€‚è¿™ç§æ³›åŒ–æ•ˆåº”åœ¨æˆ‘ä»¬çš„ç»¼åˆOwkinZeroæ¨¡å‹ä¸­å¾—åˆ°äº†è¿›ä¸€æ­¥çš„æ”¾å¤§ï¼Œè¯¥æ¨¡å‹æ˜¯åœ¨å¤šä¸ªæ•°æ®é›†æ··åˆçš„åŸºç¡€ä¸Šè®­ç»ƒçš„ï¼Œå¹¶å®ç°äº†æ›´å¹¿æ³›çš„è·¨ä»»åŠ¡æ”¹è¿›ã€‚æœ¬ç ”ç©¶æ˜¯æœç€è§£å†³å½“å‰LLMä¸­çš„ç”Ÿç‰©æ¨ç†ç›²ç‚¹çš„é‡è¦ä¸€æ­¥ï¼Œè¡¨æ˜åœ¨ç²¾å¿ƒé€‰æ‹©çš„æ•°æ®ä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¼ºåŒ–å­¦ä¹ å¯ä»¥åœ¨ä¸“ä¸šæ¨¡å‹ä¸­è§£é”å¯æ³›åŒ–çš„æ€§èƒ½ï¼Œä»è€ŒåŠ é€Ÿäººå·¥æ™ºèƒ½é©±åŠ¨çš„ç”Ÿç‰©å­¦å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16315v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸè¿…é€Ÿå‘å±•çš„åŒæ—¶ï¼Œä»é¢ä¸´å¯¹æ ¸å¿ƒç”Ÿç‰©æ¨ç†ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…åˆ›å»ºäº†æ¶µç›–è¶…è¿‡30ä¸‡å¯¹å¯éªŒè¯é—®ç­”çš„å…«ä¸ªç»¼åˆåŸºå‡†æ•°æ®é›†ï¼Œé’ˆå¯¹è¯ç‰©å‘ç°ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ç›®æ ‡å¯è¯æ€§ã€æ¨¡æ€é€‚ç”¨æ€§å’Œè¯ç‰©æ‰°åŠ¨æ•ˆåº”ç­‰ã€‚é€šè¿‡é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ç­–ç•¥å¯¹å¼€æºLLMè¿›è¡Œåè®­ç»ƒï¼Œç ”ç©¶è€…å¼€å‘äº†OwkinZeroæ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸“ä¸šåŒ–çš„OwkinZeroæ¨¡å‹åœ¨ç”Ÿç‰©åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ›´å¤§è§„æ¨¡ã€æœ€å…ˆè¿›çš„å•†ä¸šLLMã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å•ä¸€ä»»åŠ¡è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹åœ¨æœªè§ä»»åŠ¡ä¸Šçš„ä¸€è‡´è¡¨ç°ä¼˜äºå…¶åŸºç¡€æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…³é”®æ–¹é¢çš„æ³›åŒ–è¯æ®ã€‚é€šè¿‡æ··åˆæ•°æ®é›†è®­ç»ƒçš„OwkinZeroç»¼åˆæ¨¡å‹è¿›ä¸€æ­¥æ‰©å¤§äº†è·¨ä»»åŠ¡æ”¹è¿›ï¼Œè¡¨æ˜æœ‰é’ˆå¯¹æ€§çš„å¼ºåŒ–å­¦ä¹ å’Œç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®å¯ä»¥è§£é”ä¸“ä¸šæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œä»è€ŒåŠ é€ŸAIé©±åŠ¨çš„ç”Ÿç‰©å‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©æ¨ç†ä»»åŠ¡ä¸Šä»å­˜åœ¨å±€é™ã€‚</li>
<li>åˆ›å»ºäº†å…«ä¸ªæ¶µç›–è¯ç‰©å‘ç°ç­‰é¢†åŸŸæŒ‘æˆ˜çš„ç»¼åˆåŸºå‡†æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»å¯éªŒè¯å¥–åŠ±ç­–ç•¥è®­ç»ƒäº†OwkinZeroæ¨¡å‹ã€‚</li>
<li>OwkinZeroæ¨¡å‹åœ¨ç”Ÿç‰©åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå•†ä¸šLLMã€‚</li>
<li>å•ä¸€ä»»åŠ¡è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹åœ¨æœªè§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç»¼åˆOwkinZeroæ¨¡å‹é€šè¿‡æ··åˆæ•°æ®é›†è®­ç»ƒï¼Œå®ç°è·¨ä»»åŠ¡æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b32d6fd2e862d7fba9e49618c6d22d3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38c0cf17ce6215c115909b6d062a7667.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Vision-Language-Reasoning-for-Multimodal-Multiple-Choice-Question-Answering"><a href="#Hierarchical-Vision-Language-Reasoning-for-Multimodal-Multiple-Choice-Question-Answering" class="headerlink" title="Hierarchical Vision-Language Reasoning for Multimodal Multiple-Choice   Question Answering"></a>Hierarchical Vision-Language Reasoning for Multimodal Multiple-Choice   Question Answering</h2><p><strong>Authors:Ao Zhou, Zebo Gu, Tenghao Sun, Jiawen Chen, Mingsheng Tu, Zifeng Cheng, Yafeng Yin, Zhiwei Jiang, Qing Gu</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal understanding capabilities in Visual Question Answering (VQA) tasks by integrating visual and textual features. However, under the challenging ten-choice question evaluation paradigm, existing methods still exhibit significant limitations when processing PDF documents with complex layouts and lengthy content. Notably, current mainstream models suffer from a strong bias toward English training data, resulting in suboptimal performance for Japanese and other language scenarios. To address these challenges, this paper proposes a novel Japanese PDF document understanding framework that combines multimodal hierarchical reasoning mechanisms with Colqwen-optimized retrieval methods, while innovatively introducing a semantic verification strategy through sub-question decomposition. Experimental results demonstrate that our framework not only significantly enhances the modelâ€™s deep semantic parsing capability for complex documents, but also exhibits superior robustness in practical application scenarios. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡èåˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åé€‰ä¸€é—®ç­”æ¡ˆè¯„ä¼°æ¨¡å¼ä¸‹ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¤æ‚å¸ƒå±€å’Œå†—é•¿å†…å®¹çš„PDFæ–‡æ¡£æ—¶ä»è¡¨ç°å‡ºæ˜¾è‘—å±€é™æ€§ã€‚å°¤å…¶æ˜¯å½“å‰ä¸»æµæ¨¡å‹å¯¹è‹±æ–‡è®­ç»ƒæ•°æ®å­˜åœ¨å¼ºçƒˆåå‘ï¼Œå¯¼è‡´å…¶åœ¨æ—¥è¯­åŠå…¶ä»–è¯­è¨€åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ—¥è¯­PDFæ–‡æ¡£ç†è§£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¤šæ¨¡æ€åˆ†å±‚æ¨ç†æœºåˆ¶å’Œç»è¿‡Colqwenä¼˜åŒ–çš„æ£€ç´¢æ–¹æ³•ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†é€šè¿‡å­é—®é¢˜åˆ†è§£çš„è¯­ä¹‰éªŒè¯ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å¤æ‚æ–‡æ¡£çš„æ·±åº¦è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œè€Œä¸”åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16148v1">PDF</a> This paper has been accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­å±•ç¤ºäº†å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£åŠ›ï¼Œé€šè¿‡æ•´åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾æ¥å¤„ç†å¤æ‚çš„PDFæ–‡æ¡£å†…å®¹ã€‚ç„¶è€Œï¼Œåœ¨åé€‰ä¸€é—®ç­”è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†å…·æœ‰å¤æ‚å¸ƒå±€å’Œå†—é•¿å†…å®¹çš„PDFæ–‡æ¡£æ—¶ä»å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚é’ˆå¯¹è‹±è¯­è®­ç»ƒæ•°æ®çš„å¼ºåè§å¯¼è‡´å¯¹æ—¥è¯­å’Œå…¶ä»–è¯­è¨€åœºæ™¯çš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¤šæ¨¡æ€åˆ†å±‚æ¨ç†æœºåˆ¶å’ŒColqwenä¼˜åŒ–æ£€ç´¢æ–¹æ³•çš„æ—¥è¯­PDFæ–‡æ¡£ç†è§£æ¡†æ¶ï¼Œå¹¶åˆ›æ–°æ€§åœ°å¼•å…¥äº†é€šè¿‡å­é—®é¢˜åˆ†è§£çš„è¯­ä¹‰éªŒè¯ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¸ä»…æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹å¤æ‚æ–‡æ¡£çš„æ·±å±‚è¯­ä¹‰è§£æèƒ½åŠ›ï¼Œè€Œä¸”åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”ä»»åŠ¡ä¸­å…·å¤‡å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚PDFæ–‡æ¡£æ—¶å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åé€‰ä¸€é—®ç­”è¯„ä¼°æ¨¡å¼ä¸‹ã€‚</li>
<li>ä¸»æµæ¨¡å‹å¯¹è‹±è¯­è®­ç»ƒæ•°æ®å­˜åœ¨å¼ºåè§ï¼Œå¯¼è‡´å¯¹éè‹±è¯­åœºæ™¯çš„æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¥è¯­PDFæ–‡æ¡£ç†è§£æ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€åˆ†å±‚æ¨ç†å’ŒColqwenä¼˜åŒ–æ£€ç´¢æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥äº†é€šè¿‡å­é—®é¢˜åˆ†è§£çš„è¯­ä¹‰éªŒè¯ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶æé«˜äº†æ¨¡å‹å¯¹å¤æ‚æ–‡æ¡£çš„è¯­ä¹‰è§£æèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bdd4839b902ca15cc34ce6764136c4c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6efbbda7e94372086e11c1aadc8820a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc51f9fc6824450bbe02e5a814b642ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d172f258c62aa953846512d623cbc9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0c85fb16df3fc1b6db7cb6a4cbcc8dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c7734c8d8a582a7713ed586688ad5e5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning"><a href="#Bridging-the-Gap-in-Ophthalmic-AI-MM-Retinal-Reason-Dataset-and-OphthaReason-Model-toward-Dynamic-Multimodal-Reasoning" class="headerlink" title="Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and   OphthaReason Model toward Dynamic Multimodal Reasoning"></a>Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and   OphthaReason Model toward Dynamic Multimodal Reasoning</h2><p><strong>Authors:Ruiqi Wu, Yuang Yao, Tengfei Ma, Chenran Zhang, Na Su, Tao Zhou, Geng Chen, Wen Fan, Yi Zhou</strong></p>
<p>Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the modelâ€™s exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92%, 15.00%, 21.20%, and 17.66%. Project Page: \href{<a target="_blank" rel="noopener" href="https://github.com/lxirich/OphthaReason%7D%7Blink%7D">https://github.com/lxirich/OphthaReason}{link}</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘é‡‡ç”¨å¼ºåŒ–å­¦ä¹ èŒƒå¼å±•ç¤ºäº†å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡åŒ»ç–—é¢†åŸŸå·²ç»æ¢ç´¢äº†å¤šç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä¸»è¦å…³æ³¨åŸºæœ¬æ¨ç†ï¼Œè¿™æŒ‡çš„æ˜¯åŸºäºè§†è§‰ç‰¹å¾åŒ¹é…çš„æµ…å±‚æ¨æ–­ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠè¯Šæ–­è¶…è¶Šäº†åŸºæœ¬æ¨ç†ï¼Œéœ€è¦æ•´åˆå¼‚è´¨ä¸´åºŠä¿¡æ¯ï¼ˆå¦‚ä¸»è¯‰å’Œç—…å²ï¼‰ä¸å¤šæ¨¡æ€åŒ»å­¦æˆåƒæ•°æ®çš„è¿‡ç¨‹æ¨ç†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MM-Retinal-Reasonï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ¶µç›–å…¨è°±æ„ŸçŸ¥å’Œæ¨ç†çš„çœ¼ç§‘å¤šæ¨¡æ€æ•°æ®é›†ã€‚å®ƒæ¶µç›–åŸºæœ¬æ¨ç†ä»»åŠ¡å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæ—¨åœ¨å¢å¼ºä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºæœ¬æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ¨¡æ‹Ÿç°å®çš„ä¸´åºŠæ€ç»´æ¨¡å¼ã€‚åŸºäºMM-Retinal-Reasonï¼Œæˆ‘ä»¬æå‡ºäº†çœ¼ç§‘ä¸“ç”¨å¤šæ¨¡æ€æ¨ç†æ¨¡å‹OphthaReasonï¼Œå…·æœ‰åˆ†æ­¥æ¨ç†è½¨è¿¹ã€‚ä¸ºäº†çµæ´»é€‚åº”åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¸“é—¨è®¾è®¡äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ¨æ€æ€ç»´ï¼ˆUADTï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç†µä¼°è®¡æ ·æœ¬çº§ä¸ç¡®å®šæ€§ï¼Œå¹¶ä½¿ç”¨æˆå‹ä¼˜åŠ¿æœºåˆ¶åŠ¨æ€è°ƒèŠ‚æ¨¡å‹çš„æ¢ç´¢æ·±åº¦ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè‡³å°‘æ¯”é€šç”¨MLLMsã€åŒ»å­¦MLLMsã€åŸºäºRLçš„åŒ»å­¦MLLMså’Œçœ¼ç§‘MLLMsé«˜å‡º24.92%ã€15.00%ã€21.20%å’Œ17.66%ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/lxirich/OphthaReason">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16129v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨çœ¼ç§‘è¯Šæ–­ä¸­çš„ä½¿ç”¨ã€‚æ–‡ç« å¼ºè°ƒäº†ç°æœ‰æ¨¡å‹å¤§å¤šåªå…³æ³¨åŸºæœ¬æ¨ç†ï¼Œè€ŒçœŸå®ä¸–ç•Œä¸´åºŠè¯Šæ–­éœ€è¦æ›´å¤æ‚çš„æ¨ç†è¿‡ç¨‹ï¼Œèƒ½å¤Ÿæ•´åˆå¼‚è´¨ä¸´åºŠä¿¡æ¯ä¸å¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†MM-Retinal-Reasonæ•°æ®é›†å’ŒOphthaReasonæ¨¡å‹ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸´åºŠæ€ç»´æ¨¡å¼ï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§åä¸ºUncertainty-Aware Dynamic Thinkingï¼ˆUADTï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¹‹é—´è¿›è¡Œçµæ´»é€‚åº”ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸå…·å¤‡å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¤§å¤šä»…å…³æ³¨åŸºäºè§†è§‰ç‰¹å¾åŒ¹é…çš„åŸºæœ¬æ¨ç†ã€‚</li>
<li>çœŸå®ä¸–ç•Œçš„ä¸´åºŠè¯Šæ–­éœ€è¦æ•´åˆå¼‚è´¨ä¸´åºŠä¿¡æ¯å’Œå¤šæ¨¡æ€åŒ»å­¦å½±åƒæ•°æ®çš„å¤æ‚æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>MM-Retinal-Reasonæ•°æ®é›†å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼ŒåŒ…å«äº†åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ã€‚</li>
<li>OphthaReasonæ¨¡å‹æ˜¯é¦–ä¸ªçœ¼ç§‘ä¸“ç”¨çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œå…·å¤‡é€æ­¥æ¨ç†è½¨è¿¹ã€‚</li>
<li>Uncertainty-Aware Dynamic Thinkingï¼ˆUADTï¼‰æ–¹æ³•èƒ½å¤Ÿåœ¨åŸºæœ¬å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¹‹é—´è¿›è¡Œçµæ´»é€‚åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90d077a5edfbdfc8c884634bea8d8f9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f1478f88c8146f570252065c4577afe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a3f9e47c4a082b67c6d88e67f1f28dc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles"><a href="#InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles" class="headerlink" title="InMind: Evaluating LLMs in Capturing and Applying Individual Human   Reasoning Styles"></a>InMind: Evaluating LLMs in Capturing and Applying Individual Human   Reasoning Styles</h2><p><strong>Authors:Zizhen Li, Chuanhao Li, Yibin Wang, Qi Chen, Diping Song, Yukang Feng, Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Kaipeng Zhang</strong></p>
<p>LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMsâ€™ capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººæœ¬æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚è™½ç„¶ä¹‹å‰çš„è¯„ä¼°å·²ç»æ¢è®¨äº†LLMæ˜¯å¦èƒ½å¤Ÿæ¨æ–­æ„å›¾æˆ–æ£€æµ‹æ¬ºéª—ï¼Œä½†å®ƒä»¬ç»å¸¸å¿½ç•¥å½±å“äººä»¬åœ¨ç¤¾ä¼šç¯å¢ƒä¸­è§£é‡Šå’Œè¡Œä¸ºçš„ä¸ªæ€§åŒ–æ¨ç†é£æ ¼ã€‚ç¤¾ä¼šæ¨ç†æ¸¸æˆï¼ˆSDGï¼‰ä¸ºè¯„ä¼°ä¸ªæ€§åŒ–æ¨ç†é£æ ¼æä¾›äº†å¤©ç„¶çš„æµ‹è¯•å¹³å°ï¼Œåœ¨ä¸åŒçš„æ¡ä»¶ä¸‹ï¼Œä¸åŒçš„ç©å®¶å¯èƒ½ä¼šé‡‡ç”¨å¤šæ ·åŒ–ä½†è¯­å¢ƒæœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†InMindï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è®¤çŸ¥ä¸ºåŸºç¡€çš„è¯„ä»·æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMæ˜¯å¦èƒ½æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–æ¨ç†é£æ ¼äºSDGä¸­ã€‚InMindé€šè¿‡è§‚å¯Ÿè€…æ¨¡å¼å’Œå‚ä¸è€…æ¨¡å¼ä¸‹çš„å›åˆç­–ç•¥è½¨è¿¹å’Œæ¸¸æˆååæ€ï¼Œå¢å¼ºäº†ç»“æ„åŒ–æ¸¸æˆç©æ³•æ•°æ®ã€‚å®ƒæ”¯æŒå››é¡¹ä»¥è®¤çŸ¥ä¸ºåŠ¨æœºçš„ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å…±åŒè¯„ä¼°é™æ€å¯¹é½å’ŒåŠ¨æ€é€‚åº”ã€‚ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å°†InMindåº”ç”¨äºAvalonæ¸¸æˆï¼Œè¯„ä¼°äº†11æ¬¾æœ€å…ˆè¿›çš„LLMã€‚é€šç”¨LLMï¼Œç”šè‡³GPT-4ä¹Ÿç»å¸¸ä¾èµ–äºè¯æ±‡çº¿ç´¢ï¼Œéš¾ä»¥å°†åæ€é”šå®šåœ¨å³æ—¶æ¸¸æˆæˆ–é€‚åº”ä¸æ–­å˜åŒ–çš„ç­–ç•¥ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¢å¼ºæ¨ç†çš„LLMï¼ˆå¦‚DeepSeek-R1ï¼‰å±•ç°å‡ºæ—©æœŸé£æ ¼æ•æ„Ÿæ¨ç†çš„è¿¹è±¡ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰LLMåœ¨ä¸ªæ€§åŒ–è‡ªé€‚åº”æ¨ç†èƒ½åŠ›ä¸Šçš„å…³é”®å±€é™æ€§ï¼Œå¹¶å°†InMindå®šä½ä¸ºè®¤çŸ¥å¯¹é½çš„äººæœºäº¤äº’çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16072v1">PDF</a> EMNLP 2025 MainConference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººç±»ä¸ºä¸­å¿ƒçš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨è¯„ä¼°è¿™äº›æ¨¡å‹æ—¶ï¼Œäººä»¬å¸¸å¸¸å¿½è§†äº†å½±å“äººä»¬è§£è¯»å’Œè¡ŒåŠ¨çš„ç¤¾ä¼šèƒŒæ™¯ä¸‹çš„ä¸ªæ€§åŒ–æ¨ç†é£æ ¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†InMindè¿™ä¸€è®¤çŸ¥åŸºç¡€è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMsæ˜¯å¦èƒ½åœ¨ç¤¾ä¼šæ¨ç†æ¸¸æˆä¸­æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–çš„æ¨ç†é£æ ¼ã€‚æœ¬ç ”ç©¶ä»¥ä¸€ä¸ªåä¸ºAvalonçš„æ¸¸æˆä¸ºä¾‹ï¼Œå‘ç°é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€ä¾èµ–äºè¯æ±‡çº¿ç´¢ï¼Œéš¾ä»¥åœ¨åŠ¨æ€çš„æ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œé€‚åº”æ€§çš„åæ€ï¼Œè€Œå…·å¤‡æ¨ç†å¢å¼ºçš„æ¨¡å‹åˆ™å±•ç°å‡ºä¸ªæ€§åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ºè®¤çŸ¥å¯¹é½çš„äººæœºäº¤äº’ç ”ç©¶æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨äººç±»ä¸ºä¸­å¿ƒçš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨è¯„ä¼°ä¸­å¸¸å¸¸å¿½è§†ä¸ªæ€§åŒ–æ¨ç†é£æ ¼çš„å½±å“ã€‚</li>
<li>InMindæ¡†æ¶æ—¨åœ¨è¯„ä¼°LLMsåœ¨ç¤¾ä¼šæ¨ç†æ¸¸æˆä¸­æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–æ¨ç†é£æ ¼çš„èƒ½åŠ›ã€‚</li>
<li>InMindç»“åˆäº†ç»“æ„åŒ–æ¸¸æˆæ•°æ®å’Œå›åˆçº§åˆ«çš„ç­–ç•¥è½¨è¿¹ä»¥åŠæ¸¸æˆåçš„åæ€ï¼Œä»¥è¯„ä¼°LLMsçš„é™æ€å¯¹é½å’ŒåŠ¨æ€é€‚åº”èƒ½åŠ›ã€‚</li>
<li>åœ¨æ¸¸æˆAvalonçš„æ¡ˆä¾‹ä¸­ï¼Œé€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ä¾èµ–äºè¯æ±‡çº¿ç´¢ï¼Œéš¾ä»¥åœ¨åŠ¨æ€çš„æ¸¸æˆç¯å¢ƒä¸­è¿›è¡Œé€‚åº”æ€§çš„åæ€ã€‚</li>
<li>å…·å¤‡æ¨ç†å¢å¼ºçš„LLMsï¼ˆå¦‚DeepSeek-R1ï¼‰å±•ç°å‡ºä¸ªæ€§åŒ–çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰LLMsåœ¨ä¸ªæ€§åŒ–ã€é€‚åº”æ€§æ¨ç†æ–¹é¢å­˜åœ¨å…³é”®å±€é™ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0b6d42e4188465e024e6f9b444e5213.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed9ca84d7fc208477464b03d9cddf8ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aac2b58a71376b0ed38c1d5e0bc8e04f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Less-Redundancy-Boosting-Practicality-of-Vision-Language-Model-in-Walking-Assistants"><a href="#Less-Redundancy-Boosting-Practicality-of-Vision-Language-Model-in-Walking-Assistants" class="headerlink" title="Less Redundancy: Boosting Practicality of Vision Language Model in   Walking Assistants"></a>Less Redundancy: Boosting Practicality of Vision Language Model in   Walking Assistants</h2><p><strong>Authors:Chongyang Li, Yuan Zhiqiang, Jiapei Zhang, Ying Deng, Hanbo Bi, Zexi Jia, Xiaoyue Duan, Peixiang Luo, Jinchao Zhang</strong></p>
<p>Approximately 283 million people worldwide live with visual impairments, motivating increasing research into leveraging Visual Language Models (VLMs) to develop effective walking assistance systems for blind and low vision individuals. However, existing VLMs in walking assistant task often have outputs that contain considerable redundancy and extraneous details, adversely affecting usersâ€™ ability to accurately assess their surroundings. Moreover, these models typically lack the capability to proactively assess environmental risks and adaptively trigger reminders based on the appropriate scene, leading to excessive temporal redundancy. To mitigate output and temporal redundancy, we propose WalkVLM-LR, a walking assistance model with less redundancy. To reduce output redundancy, we introduce four human-preference-based custom reward functions within the GRPO-based reasoning framework to optimize the output in terms of conciseness, fluency, keyword density, and accuracy, thereby producing more informative and streamlined outputs. To minimize temporal redundancy, we incorporate an environment awareness discriminator, which shares the visual encoder with the VLMs to reduce redundant computations and enhance discriminative efficiency, to make WalkVLM-LR assess scene risk levels and minimize unnecessary reminders. Experimental results demonstrate that our method achieves state-of-the-art performance across all evaluation metrics compared with other models, particularly in output conciseness and less temporal redundancy. </p>
<blockquote>
<p>å…¨çƒçº¦æœ‰2.83äº¿äººå­˜åœ¨è§†è§‰éšœç¢ï¼Œè¿™ä¿ƒä½¿è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸ºç›²äººå’Œè§†åŠ›ä¸ä½³çš„äººå¼€å‘æœ‰æ•ˆçš„æ­¥è¡Œè¾…åŠ©ç³»ç»Ÿã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ­¥è¡Œè¾…åŠ©ä»»åŠ¡ä¸­çš„VLMsè¾“å‡ºå¾€å¾€åŒ…å«å¤§é‡å†—ä½™å’Œé¢å¤–çš„ç»†èŠ‚ï¼Œè¿™ä¼šå½±å“ç”¨æˆ·å‡†ç¡®è¯„ä¼°å‘¨å›´ç¯å¢ƒçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ç¼ºä¹ä¸»åŠ¨è¯„ä¼°ç¯å¢ƒé£é™©çš„èƒ½åŠ›ï¼Œä»¥åŠæ ¹æ®é€‚å½“çš„åœºæ™¯è‡ªé€‚åº”è§¦å‘æé†’çš„åŠŸèƒ½ï¼Œå¯¼è‡´æ—¶é—´å†—ä½™è¿‡å¤šã€‚ä¸ºäº†å‡å°‘è¾“å‡ºå’Œæ—¶é—´å†—ä½™ï¼Œæˆ‘ä»¬æå‡ºäº†WalkVLM-LRè¿™ä¸€å…·æœ‰æ›´å°‘å†—ä½™çš„æ­¥è¡Œè¾…åŠ©æ¨¡å‹ã€‚ä¸ºå‡å°‘è¾“å‡ºå†—ä½™ï¼Œæˆ‘ä»¬åœ¨åŸºäºGRPOçš„æ¨ç†æ¡†æ¶ä¸­å¼•å…¥äº†å››ä¸ªåŸºäºäººç±»åå¥½çš„è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–è¾“å‡ºçš„ç®€æ´æ€§ã€æµç•…æ€§ã€å…³é”®è¯å¯†åº¦å’Œå‡†ç¡®æ€§ï¼Œä»è€Œäº§ç”Ÿæ›´å…·ä¿¡æ¯é‡å’Œç®€æ´çš„è¾“å‡ºæ¥ã€‚ä¸ºå°½é‡å‡å°‘æ—¶é—´å†—ä½™ï¼Œæˆ‘ä»¬åŠ å…¥äº†ä¸€ä¸ªç¯å¢ƒæ„è¯†é‰´åˆ«å™¨ï¼Œå®ƒä¸VLMså…±äº«è§†è§‰ç¼–ç å™¨ï¼Œä»¥å‡å°‘å†—ä½™è®¡ç®—å¹¶å¢å¼ºé‰´åˆ«æ•ˆç‡ï¼Œä½¿WalkVLM-LRèƒ½å¤Ÿè¯„ä¼°åœºæ™¯é£é™©æ°´å¹³å¹¶å°½é‡å‡å°‘ä¸å¿…è¦çš„æé†’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å‡ºç®€æ´æ€§å’Œè¾ƒå°‘çš„æ—¶é—´å†—ä½™æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16070v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰è¾…åŠ©ç³»ç»Ÿä¸­ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹ç›²äººå’Œè§†åŠ›å—æŸè€…è¿›è¡Œè¾…åŠ©è¡Œèµ°ç ”ç©¶çš„æ¨è¿›ååˆ†å¿…è¦ã€‚ç°æœ‰çš„ç³»ç»Ÿå¸¸å¸¸è¾“å‡ºå¤§é‡å†—ä½™ä¿¡æ¯ï¼Œå­˜åœ¨åœºæ™¯é£é™©è¯†åˆ«ä¸è¶³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•WalkVLM-LRæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ä¼˜åŒ–è¾“å‡ºå¹¶å¢å¼ºç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›æ¥å‡å°‘å†—ä½™ä¿¡æ¯å¹¶å¢å¼ºç³»ç»Ÿæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸–ç•Œä¸Šæœ‰å¤§çº¦2.83äº¿äººå­˜åœ¨è§†è§‰éšœç¢ï¼Œè¿™ä¿ƒä½¿äº†å¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¡Œèµ°è¾…åŠ©ç³»ç»Ÿä¸­çš„ç ”ç©¶å¢åŠ ã€‚</li>
<li>ç°æœ‰çš„è¡Œèµ°è¾…åŠ©ç³»ç»Ÿä¸­çš„VLMså¸¸å¸¸è¾“å‡ºå†—ä½™å’Œå¤šä½™ç»†èŠ‚ï¼Œå½±å“ç”¨æˆ·å¯¹ç¯å¢ƒè¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>WalkVLM-LRæ¨¡å‹è¢«æå‡ºä»¥å‡å°‘å†—ä½™è¾“å‡ºå’Œä¸´æ—¶å†—ä½™ã€‚</li>
<li>ä¸ºäº†å‡å°‘è¾“å‡ºå†—ä½™ï¼Œå¼•å…¥äº†å››ä¸ªåŸºäºäººç±»åå¥½çš„è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œä¼˜åŒ–è¾“å‡ºçš„ç®€æ´æ€§ã€æµç•…æ€§ã€å…³é”®è¯å¯†åº¦å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ä¸ºäº†æœ€å°åŒ–ä¸´æ—¶å†—ä½™ï¼Œèå…¥äº†ä¸€ä¸ªç¯å¢ƒæ„ŸçŸ¥è¾¨åˆ«å™¨ï¼Œå…±äº«è§†è§‰ç¼–ç å™¨ä¸VLMsï¼Œä»¥æé«˜åˆ¤åˆ«æ•ˆç‡å’Œåœºæ™¯é£é™©çº§åˆ«çš„è¯„ä¼°èƒ½åŠ›ï¼Œå¹¶å‡å°‘ä¸å¿…è¦çš„æé†’ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒWalkVLM-LRåœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†ä¸šç•Œæœ€ä½³è¡¨ç°ï¼Œå°¤å…¶åœ¨è¾“å‡ºç®€æ´æ€§å’Œå‡å°‘ä¸´æ—¶å†—ä½™æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62c356f58d889fb057aa6d6b4bc25385.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e12924ca40b9cb06826594b6330fe77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f572af1e548cfed521fbfa717fc2b0f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbf0d5d340af25906fd0622343c30ba1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-648545872b4148fadd981f219f39a569.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning"><a href="#CARFT-Boosting-LLM-Reasoning-via-Contrastive-Learning-with-Annotated-Chain-of-Thought-based-Reinforced-Fine-Tuning" class="headerlink" title="CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated   Chain-of-Thought-based Reinforced Fine-Tuning"></a>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated   Chain-of-Thought-based Reinforced Fine-Tuning</h2><p><strong>Authors:Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang</strong></p>
<p>Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15%), and efficiency (up to 30.62%). Code is available at <a target="_blank" rel="noopener" href="https://github.com/WNQzhu/CARFT">https://github.com/WNQzhu/CARFT</a>. </p>
<blockquote>
<p>æ¨ç†èƒ½åŠ›åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºäº†æé«˜LLMçš„æ¨ç†æ€§èƒ½ï¼Œå·²ç»æå‡ºäº†å¤šç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒæ–¹æ³•æ¥è§£å†³ä»…é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„LLMçš„æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æœ‰æ•ˆï¼Œä½†ä¸¤ä¸ªä¸»è¦å±€é™æ€§é˜»ç¢äº†LLMçš„è¿›æ­¥ã€‚é¦–å…ˆï¼Œæ™®é€šçš„åŸºäºRLçš„æ–¹æ³•å¿½ç•¥äº†æ³¨é‡Šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰å¹¶èå…¥äº†ä¸ç¨³å®šçš„æ¨ç†è·¯å¾„é‡‡æ ·ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒã€è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šå’Œæ€§èƒ½ä¸ä½³ã€‚å…¶æ¬¡ï¼Œç°æœ‰çš„SFTæ–¹æ³•é€šå¸¸è¿‡äºå¼ºè°ƒæ³¨é‡Šçš„CoTï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºæ²¡èƒ½å……åˆ†å‘æ˜æ½œåœ¨çš„CoTã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ³¨é‡ŠCoTçš„å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œå³CARFTï¼ˆåŸºäºæ³¨é‡Šæ€ç»´é“¾å¯¹æ¯”å­¦ä¹ çš„å¼ºåŒ–å¾®è°ƒï¼‰ï¼Œä»¥æé«˜LLMçš„æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶è§£å†³ä¸Šè¿°å±€é™æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯æ¡CoTå­¦ä¹ ä¸€ç§è¡¨ç¤ºæ–¹æ³•ã€‚åŸºäºè¿™ç§è¡¨ç¤ºï¼Œæˆ‘ä»¬è®¾è®¡äº†æ–°çš„å¯¹æ¯”ä¿¡å·æ¥æŒ‡å¯¼å¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†å¯ç”¨çš„æ³¨é‡ŠCoTï¼Œè€Œä¸”é€šè¿‡å¼•å…¥é¢å¤–çš„æ— ç›‘ç£å­¦ä¹ ä¿¡å·æ¥ç¨³å®šå¾®è°ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ç§åŸºå‡†æ–¹æ³•ã€ä¸¤ç§åŸºç¡€æ¨¡å‹å’Œä¸¤ç§æ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„å®éªŒå’Œæ·±å…¥åˆ†æï¼Œè¯æ˜äº†CARFTåœ¨ç¨³å¥æ€§ã€æ€§èƒ½ï¼ˆæœ€é«˜æå‡10.15%ï¼‰å’Œæ•ˆç‡ï¼ˆæœ€é«˜æå‡30.6 2%ï¼‰æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WNQzhu/CARFT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WNQzhu/CARFTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15868v1">PDF</a> 14 pages, to appear in EMNLP25</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ä¸­ï¼Œæ¨ç†èƒ½åŠ›å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é’ˆå¯¹LLMçš„æ¨ç†æ€§èƒ½æå‡ï¼Œæå‡ºäº†å¤šç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒæ–¹æ³•æ¥è§£å†³ä»…é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„LLMçš„æœ‰é™æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šä¸€æ˜¯ä¼ ç»Ÿçš„RLæ–¹æ³•å¿½ç•¥äº†æ³¨é‡Šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰å¹¶èå…¥äº†ä¸ç¨³å®šçš„æ¨ç†è·¯å¾„é‡‡æ ·ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒã€è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šå’Œæ€§èƒ½ä¸ä½³ï¼›äºŒæ˜¯ç°æœ‰çš„SFTæ–¹æ³•é€šå¸¸è¿‡åˆ†å¼ºè°ƒæ³¨é‡Šçš„CoTï¼Œç”±äºæœªèƒ½å……åˆ†åˆ©ç”¨æ½œåœ¨çš„CoTï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ³¨é‡Šçš„CoTå¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼ˆTheNameï¼‰ï¼Œæ—¨åœ¨æé«˜LLMçš„æ¨ç†æ€§èƒ½å¹¶è§£å†³ä¸Šè¿°é—®é¢˜ã€‚è¯¥æ–¹æ³•å­¦ä¹ æ¯ä¸ªCoTçš„è¡¨ç¤ºï¼Œå¹¶åŸºäºæ­¤è¡¨ç¤ºè®¾è®¡æ–°çš„å¯¹æ¯”ä¿¡å·æ¥å¼•å¯¼å¾®è°ƒè¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†ç°æœ‰çš„æ³¨é‡ŠCoTï¼Œè€Œä¸”é€šè¿‡å¼•å…¥é¢å¤–çš„æ— ç›‘ç£å­¦ä¹ ä¿¡å·æ¥ç¨³å®šå¾®è°ƒè¿‡ç¨‹ã€‚ç»è¿‡ä¸ä¸‰ç§åŸºç¡€æ–¹æ³•ã€ä¸¤ç§åŸºç¡€æ¨¡å‹å’Œä¸¤ç§æ•°æ®é›†çš„ç»¼åˆå®éªŒå’Œæ·±å…¥åˆ†æï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç¨³å¥æ€§ã€æ€§èƒ½ï¼ˆæœ€é«˜æå‡10.15%ï¼‰å’Œæ•ˆç‡ï¼ˆæœ€é«˜æå‡30.62%ï¼‰æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†èƒ½åŠ›åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚</li>
<li>åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¾®è°ƒæ–¹æ³•è¢«æå‡ºæ¥æå‡LLMçš„æ¨ç†æ€§èƒ½ï¼Œè§£å†³å…¶æœ‰é™çš„æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚</li>
<li>ä¼ ç»ŸRLæ–¹æ³•å­˜åœ¨å¿½ç•¥æ³¨é‡Šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œä¸ç¨³å®šæ¨ç†è·¯å¾„é‡‡æ ·çš„ç¼ºé™·ï¼Œå¯¼è‡´æ¨¡å‹å´©æºƒã€è®­ç»ƒä¸ç¨³å®šå’Œæ€§èƒ½ä¸ä½³ã€‚</li>
<li>ç°æœ‰SFTæ–¹æ³•è¿‡åˆ†å¼ºè°ƒæ³¨é‡Šçš„CoTï¼Œå¯èƒ½å› æœªèƒ½å……åˆ†åˆ©ç”¨æ½œåœ¨CoTè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ç»“åˆæ³¨é‡Šçš„CoTå¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼ˆTheNameï¼‰ï¼Œæ—¨åœ¨æé«˜LLMæ¨ç†æ€§èƒ½ï¼Œå¹¶è§£å†³ä»¥ä¸Šé—®é¢˜ã€‚</li>
<li>TheNameæ–¹æ³•å­¦ä¹ æ¯ä¸ªCoTçš„è¡¨ç¤ºï¼Œå¹¶è®¾è®¡æ–°çš„å¯¹æ¯”ä¿¡å·æ¥å¼•å¯¼å¾®è°ƒè¿‡ç¨‹ã€‚</li>
<li>TheNameæ–¹æ³•ä¸ä»…å……åˆ†åˆ©ç”¨äº†æ³¨é‡Šçš„CoTï¼Œè€Œä¸”é€šè¿‡å¼•å…¥æ— ç›‘ç£å­¦ä¹ ä¿¡å·ç¨³å®šäº†å¾®è°ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨å®éªŒä¸Šè¯æ˜äº†å…¶åœ¨æ€§èƒ½ã€ç¨³å¥æ€§å’Œæ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23de1656a4bbf865df224e2ae8421b61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca5904eb7c839c859fefd4c3980ed535.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3dbd31dcf5ddd44f08f726cc1e777d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-910136b1cff66bc4af5bc6a1322c0310.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c40e63d670383eb693ebed4de926492b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Whoâ€™s-Asking-Investigating-Bias-Through-the-Lens-of-Disability-Framed-Queries-in-LLMs"><a href="#Whoâ€™s-Asking-Investigating-Bias-Through-the-Lens-of-Disability-Framed-Queries-in-LLMs" class="headerlink" title="Whoâ€™s Asking? Investigating Bias Through the Lens of Disability Framed   Queries in LLMs"></a>Whoâ€™s Asking? Investigating Bias Through the Lens of Disability Framed   Queries in LLMs</h2><p><strong>Authors:Srikant Panda, Vishnu Hari, Kalpana Panda, Amit Agarwal, Hitesh Laxmichand Patel</strong></p>
<p>Large Language Models (LLMs) routinely infer users demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remains largely uncharted. Thus, we present the first systematic audit of disability-conditioned demographic bias across eight state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters. Using a balanced template corpus that pairs nine disability categories with six real-world business domains, we prompt each model to predict five demographic attributes - gender, socioeconomic status, education, cultural background, and locality - under both neutral and disability-aware conditions.   Across a varied set of prompts, models deliver a definitive demographic guess in up to 97% of cases, exposing a strong tendency to make arbitrary inferences with no clear justification. Disability context heavily shifts predicted attribute distributions, and domain context can further amplify these deviations. We observe that larger models are simultaneously more sensitive to disability cues and more prone to biased reasoning, indicating that scale alone does not mitigate stereotype amplification.   Our findings reveal persistent intersections between ableism and other demographic stereotypes, pinpointing critical blind spots in current alignment strategies. We release our evaluation framework and results to encourage disability-inclusive benchmarking and recommend integrating abstention calibration and counterfactual fine-tuning to curb unwarranted demographic inference. Code and data will be released on acceptance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸ä»…å‡­æªè¾å°±èƒ½æ¨æ–­å‡ºç”¨æˆ·çš„äººå£ç»Ÿè®¡ç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¼è‡´å‡ºç°åè§ååº”ï¼Œå³ä½¿ä¸æä¾›æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ®‹ç–¾çº¿ç´¢åœ¨å½¢æˆè¿™äº›æ¨æ–­ä¸­çš„ä½œç”¨ä»å¤§éƒ¨åˆ†æœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€æ–°æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„å®¡è®¡ï¼Œè¿™äº›æ¨¡å‹çš„å‚æ•°èŒƒå›´ä»3Båˆ°72Bã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¹³è¡¡çš„æ¨¡æ¿è¯­æ–™åº“ï¼Œå°†ä¹ä¸ªæ®‹ç–¾ç±»åˆ«ä¸å…­ä¸ªç°å®ä¸–ç•Œä¸šåŠ¡åŸŸç›¸åŒ¹é…ï¼Œæç¤ºæ¯ä¸ªæ¨¡å‹åœ¨ä¸­æ€§å’Œæ®‹ç–¾æ„è¯†ä¸¤ç§æƒ…å†µä¸‹é¢„æµ‹äº”ä¸ªäººå£ç»Ÿè®¡å±æ€§â€”â€”æ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ã€æ•™è‚²ã€æ–‡åŒ–èƒŒæ™¯å’Œåœ°ç†ä½ç½®ã€‚åœ¨å„ç§æç¤ºä¸‹ï¼Œæ¨¡å‹åœ¨é«˜è¾¾97%çš„æƒ…å†µä¸‹åšå‡ºäº†æ˜ç¡®çš„äººå£ç»Ÿè®¡çŒœæµ‹ï¼Œæš´éœ²å‡ºä¸€ç§å¼ºçƒˆçš„å€¾å‘ï¼Œå³è¿›è¡Œä»»æ„æ¨æ–­è€Œæ²¡æœ‰æ˜ç¡®çš„ä¾æ®ã€‚æ®‹ç–¾èƒŒæ™¯ä¼šæå¤§åœ°æ”¹å˜é¢„æµ‹å±æ€§åˆ†å¸ƒï¼Œè€Œé¢†åŸŸèƒŒæ™¯å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™äº›åå·®ã€‚æˆ‘ä»¬å‘ç°æ›´å¤§çš„æ¨¡å‹å¯¹æ®‹ç–¾çº¿ç´¢æ›´ä¸ºæ•æ„Ÿï¼ŒåŒæ—¶æ›´å®¹æ˜“å‡ºç°åè§æ¨ç†ï¼Œè¿™è¡¨æ˜è§„æ¨¡æœ¬èº«å¹¶ä¸ä¼šå‡è½»åˆ»æ¿å°è±¡çš„æ”¾å¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†èƒ½åŠ›ä¸»ä¹‰å’Œå…¶ä»–äººå£ç»Ÿè®¡åˆ»æ¿å°è±¡ä¹‹é—´çš„æŒç»­äº¤é›†ï¼ŒæŒ‡å‡ºäº†å½“å‰å¯¹é½ç­–ç•¥ä¸­çš„å…³é”®ç›²ç‚¹ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’Œç»“æœï¼Œä»¥é¼“åŠ±è¿›è¡ŒåŒ…å®¹æ®‹ç–¾äººçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å»ºè®®æ•´åˆå¼ƒæƒæ ¡å‡†å’Œåå‘äº‹å®å¾®è°ƒæ¥éåˆ¶ä¸å¿…è¦çš„äººå£ç»Ÿè®¡æ¨æ–­ã€‚è®ºæ–‡æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15831v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼šä»æªè¾ä¸­æ¨æ–­ç”¨æˆ·çš„äººå£ç»Ÿè®¡ç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¼è‡´åè§ååº”ï¼Œå³ä½¿æœªæä¾›æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯ã€‚å¯¹äºæ®‹ç–¾çº¿ç´¢åœ¨å¡‘é€ è¿™äº›æ¨æ–­ä¸­çš„ä½œç”¨ä»ç„¶å¤§éƒ¨åˆ†æœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´å‹LLMsè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§å®¡è®¡ï¼Œå‚æ•°èŒƒå›´ä»3Båˆ°72Bã€‚æˆ‘ä»¬ä½¿ç”¨å¹³è¡¡çš„æ¨¡æ¿è¯­æ–™åº“ï¼Œå°†ä¹ä¸ªæ®‹ç–¾ç±»åˆ«ä¸å…­ä¸ªçœŸå®å•†ä¸šé¢†åŸŸç›¸åŒ¹é…ï¼Œæç¤ºæ¨¡å‹é¢„æµ‹äº”ç§äººå£ç»Ÿè®¡å±æ€§â€”â€”æ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ã€æ•™è‚²ã€æ–‡åŒ–èƒŒæ™¯å’Œåœ°ç†ä½ç½®â€”â€”åœ¨ä¸­æ€§å’Œæ®‹ç–¾æ„è¯†ä¸¤ç§æ¡ä»¶ä¸‹ã€‚åœ¨å„ç§æç¤ºä¸‹ï¼Œæ¨¡å‹åœ¨é«˜è¾¾97%çš„æƒ…å†µä¸‹ç»™å‡ºäº†æ˜ç¡®çš„äººå£ç»Ÿè®¡çŒœæµ‹ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„å€¾å‘æ€§ï¼Œåšå‡ºä»»æ„æ¨æ–­è€Œæ²¡æœ‰æ˜ç¡®çš„ä¾æ®ã€‚æ®‹ç–¾èƒŒæ™¯æå¤§åœ°æ”¹å˜äº†é¢„æµ‹çš„å± æ€§åˆ†å¸ƒï¼Œè€Œä¸”é¢†åŸŸèƒŒæ™¯å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™äº›åå·®ã€‚æˆ‘ä»¬å‘ç°æ›´å¤§çš„æ¨¡å‹å¯¹æ®‹ç–¾çº¿ç´¢æ›´åŠ æ•æ„Ÿï¼ŒåŒæ—¶æ›´å®¹æ˜“å‡ºç°åè§æ¨ç†ï¼Œè¿™è¡¨æ˜è§„æ¨¡æœ¬èº«å¹¶ä¸ä¼šå‡è½»åˆ»æ¿å°è±¡çš„æ”¾å¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†èƒ½åŠ›ä¸»ä¹‰å’Œå…¶ä»–äººå£ç»Ÿè®¡åˆ»æ¿å°è±¡ä¹‹é—´çš„æŒç»­äº¤é›†ï¼ŒæŒ‡å‡ºäº†å½“å‰å¯¹é½ç­–ç•¥ä¸­çš„å…³é”®ç›²ç‚¹ã€‚æˆ‘ä»¬å‘å¸ƒè¯„ä¼°æ¡†æ¶å’Œç»“æœï¼Œä»¥é¼“åŠ±è¿›è¡ŒåŒ…å®¹æ®‹ç–¾çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å»ºè®®ä½¿ç”¨æ”¾å¼ƒæ ¡å‡†å’Œåå‘äº‹å®å¾®è°ƒæ¥éåˆ¶ä¸å¿…è¦çš„äººå£ç»Ÿè®¡æ¨æ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½ä»æªè¾ä¸­æ¨æ–­ç”¨æˆ·çš„äººå£ç»Ÿè®¡ç‰¹å¾ï¼Œå¼•å‘åè§ååº”ã€‚</li>
<li>æ®‹ç–¾çº¿ç´¢åœ¨LLMæ¨æ–­äººå£ç»Ÿè®¡ç‰¹å¾ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å…¶ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>LLMsåœ¨é¢„æµ‹äººå£ç»Ÿè®¡å±æ€§æ—¶è¡¨ç°å‡ºå¼ºçƒˆçš„ä»»æ„æ¨æ–­å€¾å‘ï¼Œä¸”è¿™ç§å€¾å‘åœ¨æ®‹ç–¾èƒŒæ™¯ä¸‹æ›´ä¸ºæ˜æ˜¾ã€‚</li>
<li>ä¸åŒå•†ä¸šé¢†åŸŸå¯¹LLMsçš„æ¨æ–­ç»“æœæœ‰å½±å“ã€‚</li>
<li>æ›´å¤§è§„æ¨¡çš„LLMså¯¹æ®‹ç–¾çº¿ç´¢æ›´æ•æ„Ÿï¼Œä¸”æ›´å®¹æ˜“å‡ºç°åè§æ¨ç†ã€‚</li>
<li>å½“å‰LLMå¯¹é½ç­–ç•¥å­˜åœ¨å…³é”®ç›²ç‚¹ï¼Œéœ€è¦æ›´åŠ æ³¨æ„æ®‹ç–¾ç¾¤ä½“çš„ç‰¹æ®Šéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4138002bb4073b12bf7142426a5833c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-531e4df2c32cbbd6f1974d6b98bb1c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83bcaa2fbf142bd75f82b2decd5b28e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a9fe8ae6a94bac6aafd68f03a3a0bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f60b5c0443f9f73cac16ccd13fc230b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ff7e8187485c18a4360eff5274022d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedResearcher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework"><a href="#MedResearcher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework" class="headerlink" title="MedResearcher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework"></a>MedResearcher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework</h2><p><strong>Authors:Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</strong></p>
<p>Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts. We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions. Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains. </p>
<blockquote>
<p>è¿‘æœŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººçš„å‘å±•å±•ç¤ºå‡ºäº†è·¨è¶Šå¤šä¸ªé¢†åŸŸçš„ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä»¥æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸ºä¾‹ï¼Œå®ƒä»¬åœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢å’Œç»¼åˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡é€šç”¨æ·±åº¦ç ”ç©¶ä»£ç†äººåœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸï¼Œå®ƒä»¬é‡åˆ°äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œé¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿåœ¨æœ€å…ˆè¿›çš„åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æœ‰é™ã€‚ä¸»è¦å±€é™æ€§åœ¨äºï¼šï¼ˆ1ï¼‰æ¨¡å‹ç¼ºä¹ç”¨äºä¸´åºŠæ¨ç†çš„å……è¶³å¯†é›†åŒ»å­¦çŸ¥è¯†ï¼›ï¼ˆ2ï¼‰æ¡†æ¶å—åˆ°ç¼ºä¹é’ˆå¯¹åŒ»å­¦ä¸Šä¸‹æ–‡å®šåˆ¶çš„ä¸“ç”¨æ£€ç´¢å·¥å…·çš„åˆ¶çº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ»å­¦æ·±åº¦ç ”ç©¶ä»£ç†äººï¼Œé€šè¿‡ä¸¤é¡¹æ ¸å¿ƒåˆ›æ–°æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨åŒ»å­¦çŸ¥è¯†å›¾è°±å¼€å‘äº†ä¸€ç§æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œä»å›´ç»•ç½•è§åŒ»å­¦å®ä½“çš„å­å›¾ä¸­æå–æœ€é•¿çš„é“¾æ¥ç”Ÿæˆå¤æ‚çš„å¤šè·³é—®ç­”å¯¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é›†æˆäº†å®šåˆ¶çš„ç§äººåŒ»å­¦æ£€ç´¢å¼•æ“å’Œé€šç”¨å·¥å…·ï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®å…¨é¢çš„åŒ»å­¦ä¿¡æ¯ç»¼åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†è·¨è¶Š12ä¸ªåŒ»å­¦ä¸“ä¸šçš„2100+ç§ä¸åŒçš„è½¨è¿¹ï¼Œæ¯æ¡è½¨è¿¹å¹³å‡äº¤äº’å·¥å…·4.2æ¬¡ã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼å’Œå¸¦æœ‰ç»„åˆå¥–åŠ±çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬çš„MedResearcher-R1-32Bæ¨¡å‹åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬çš„æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œåœ¨æ¶æ„ã€å·¥å…·è®¾è®¡å’Œè®­ç»ƒæ•°æ®æ„å»ºæ–¹é¢æœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸç‰¹å®šåˆ›æ–°ï¼Œå¯ä»¥ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨ä¸“ç”¨é¢†åŸŸè¶…è¶Šæ›´å¤§è§„æ¨¡çš„ä¸“æœ‰ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14880v2">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ»ç–—æ·±åº¦ç ”ç©¶ä»£ç†äººï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°æ¥è§£å†³æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä½¿ç”¨åŒ»ç–—çŸ¥è¯†å›¾è°±çš„æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”Ÿæˆå¤æ‚çš„å¤šè·³é—®ç­”å¯¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é›†æˆäº†å®šåˆ¶çš„åŒ»ç–—æ£€ç´¢å¼•æ“å’Œé€šç”¨å·¥å…·ï¼Œä»¥å®ç°å‡†ç¡®çš„ä¿¡æ¯åˆæˆã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼å’Œå¥–åŠ±ç»„åˆï¼Œæˆ‘ä»¬çš„MedResearcher-R1-32Bæ¨¡å‹åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚è¿™è¯æ˜äº†åœ¨æ¶æ„ã€å·¥å…·è®¾è®¡å’Œè®­ç»ƒæ•°æ®æ„å»ºæ–¹é¢çš„ä¸“ä¸šé¢†åŸŸåˆ›æ–°å¯ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨ä¸“é—¨é¢†åŸŸä¸­ä¼˜äºè¾ƒå¤§çš„ä¸“æœ‰ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-based agents demonstrate impressive capabilities across multiple domains but face challenges in the medical domain.</li>
<li>ç°æœ‰æ¨¡å‹ç¼ºä¹è¶³å¤Ÿçš„åŒ»ç–—çŸ¥è¯†ç”¨äºä¸´åºŠæ¨ç†ï¼Œå¹¶ä¸”æ¡†æ¶å—é™äºç¼ºä¹é’ˆå¯¹åŒ»ç–—ç¯å¢ƒçš„ä¸“ç”¨æ£€ç´¢å·¥å…·ã€‚</li>
<li>åˆ›æ–°çš„åŒ»ç–—æ·±åº¦ç ”ç©¶ä»£ç†äººé€šè¿‡å¼€å‘æ–°å‹æ•°æ®åˆæˆæ¡†æ¶å’Œä½¿ç”¨å®šåˆ¶çš„åŒ»ç–—æ£€ç´¢å¼•æ“æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå¯¹ä¸€èˆ¬æ·±åº¦ç ”ç©¶ä»»åŠ¡çš„ç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-48c33b5018d4fa552d6c7cf6067fa815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee2b8e74fc607a2071d0e64725d6cf9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afeb5cd15a4bcc13c2a02a3a34fd19a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62f1e8faf609faa3d971b5c864757447.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning"><a href="#Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning" class="headerlink" title="Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning"></a>Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning</h2><p><strong>Authors:Beinuo Yang, Qishen Zhou, Junyi Li, Chenxing Su, Simon Hu</strong></p>
<p>Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling. </p>
<blockquote>
<p>ä¼˜åŒ–å»ºæ¨¡ï¼ˆOMï¼‰å¯¹äºè§£å†³å¤æ‚çš„å†³ç­–é—®é¢˜è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™ä¸€æµç¨‹ä¾ç„¶è€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œå¹¶é«˜åº¦ä¾èµ–äºé¢†åŸŸä¸“å®¶ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å…¶è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›æ˜¾ç¤ºå‡ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„å¸Œæœ›ï¼Œä½†å½“å‰çš„æ–¹æ³•é¢ä¸´ä¸‰å¤§å…³é”®å±€é™æ€§ï¼šé«˜åŸºå‡†æµ‹è¯•æ ‡ç­¾é”™è¯¯ç‡ï¼ˆé«˜è¾¾42%ï¼‰ã€è¯„ä¼°èŒƒå›´ç‹­çª„ï¼ˆä»…è€ƒè™‘æœ€ä¼˜å€¼ï¼‰ï¼Œä»¥åŠå› é«˜åº¦ä¾èµ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆ–æ¨¡å‹å¾®è°ƒè€Œå¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ç³»ç»Ÿçš„é”™è¯¯ä¿®æ­£å’Œæ›´å…¨é¢çš„æ³¨é‡Šæ¥å¢å¼ºç°æœ‰æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†LogiORè¿™ä¸€æ–°çš„ç‰©æµé¢†åŸŸçš„ä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šå…·æœ‰æ ‡å‡†åŒ–æ³¨é‡Šçš„å¤æ‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ORThoughtè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ€ç»´é“¾æ¨ç†ï¼ˆchain-of-thought reasoningï¼‰åˆ©ç”¨ä¸“å®¶çº§çš„ä¼˜åŒ–å»ºæ¨¡åŸåˆ™æ¥è‡ªåŠ¨åŒ–OMæµç¨‹ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†ORThoughtåœ¨åŒ…æ‹¬å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨å†…çš„ç°æœ‰æ–¹æ³•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œç¡®å®šäº†å…³é”®æˆåŠŸå› ç´ å’Œå¤±è´¥æ¨¡å¼ï¼Œä¸ºæœªæ¥åŸºäºLLMçš„ä¼˜åŒ–å»ºæ¨¡ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14410v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¿ç­¹ä¼˜åŒ–å»ºæ¨¡ï¼ˆOMï¼‰å¯¹äºè§£å†³å¤æ‚çš„å†³ç­–é—®é¢˜è‡³å…³é‡è¦ï¼Œä½†å…¶è¿‡ç¨‹è€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œé«˜åº¦ä¾èµ–é¢†åŸŸä¸“å®¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ­¤é¢†åŸŸå±•ç°æ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸‰å¤§å…³é”®å±€é™ï¼šé«˜åŸºå‡†æ ‡ç­¾é”™è¯¯ç‡ã€è¯„ä¼°èŒƒå›´ç‹­çª„åŠè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯¯å·®ä¿®æ­£å’Œæ›´å…¨é¢çš„æ³¨é‡Šå¢å¼ºç°æœ‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥LogiORç‰©æµé¢†åŸŸä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•é›†ã€‚æ­¤å¤–ï¼Œæå‡ºORThoughtæ¡†æ¶ï¼Œåˆ©ç”¨ä¼˜åŒ–å»ºæ¨¡åŸåˆ™é€šè¿‡é“¾å¼æ€ç»´æ¨ç†è‡ªåŠ¨åŒ–OMè¿‡ç¨‹ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒORThoughtä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸Šã€‚æœ€åï¼Œè¿›è¡Œç³»ç»Ÿåˆ†æï¼Œä¸ºåŸºäºLLMçš„ä¼˜åŒ–å»ºæ¨¡æœªæ¥ç ”ç©¶æä¾›æœ‰ä»·å€¼è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿ç­¹ä¼˜åŒ–å»ºæ¨¡ï¼ˆOMï¼‰åœ¨è§£å†³å¤æ‚å†³ç­–é—®é¢˜ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†è¿‡ç¨‹è€—æ—¶ã€æ˜“é”™ä¸”ä¾èµ–ä¸“å®¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨OMé¢†åŸŸå…·æœ‰åº”ç”¨æ½œåŠ›ï¼Œä½†å­˜åœ¨é«˜é”™è¯¯ç‡ã€è¯„ä¼°èŒƒå›´ç‹­çª„å’Œè®¡ç®—æ•ˆç‡ä½çš„å±€é™ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯¯å·®ä¿®æ­£å’Œæ›´å…¨é¢çš„æ³¨é‡Šå¢å¼ºäº†ç°æœ‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†æ–°çš„ç‰©æµé¢†åŸŸä¼˜åŒ–å»ºæ¨¡åŸºå‡†æµ‹è¯•é›†LogiORã€‚</li>
<li>æå‡ºçš„ORThoughtæ¡†æ¶åˆ©ç”¨ä¼˜åŒ–å»ºæ¨¡åŸåˆ™é€šè¿‡é“¾å¼æ€ç»´æ¨ç†è‡ªåŠ¨åŒ–OMè¿‡ç¨‹ã€‚</li>
<li>ORThoughtåœ¨å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ç³»ç»Ÿåˆ†æä¸ºåŸºäºLLMçš„ä¼˜åŒ–å»ºæ¨¡æä¾›äº†æœªæ¥ç ”ç©¶çš„å…³é”®æˆåŠŸå› ç´ å’Œå¤±è´¥æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b49f194f508bd88798b59af3a5249e36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1217039f00592b49c11bcaf7a2836893.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1909c69cdea75d6dc96d29bc8f00a82b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96093fc8129788b6df44f9cc5a24d7b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ade61ce866f06c45c73bb60746837150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-127fbbe12fd8e6fb16f6105490a870ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55206638a9556e0a725174980723d238.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS"><a href="#Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS" class="headerlink" title="Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS"></a>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS</h2><p><strong>Authors:Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</strong></p>
<p>Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs. </p>
<blockquote>
<p>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿„ä»Šä¸ºæ­¢ä¸»è¦è½å…¥ä¸¤ç§æˆªç„¶ä¸åŒçš„èŒƒå¼ï¼š<br>ï¼ˆä¸€ï¼‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•èƒ½å¤Ÿä¼˜åŒ–åŸºäºç¨€ç–ç»“æœçš„å¥–åŠ±ï¼Œä½†å­˜åœ¨ä¸ç¨³å®šæ€§å’Œä½æ ·æœ¬æ•ˆç‡çš„é—®é¢˜ï¼›<br>ï¼ˆäºŒï¼‰åŸºäºç‹¬ç«‹è®­ç»ƒã€é™æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æŒ‡å¯¼çš„æœç´¢æŠ€æœ¯ï¼Œè¿™éœ€è¦æ˜‚è´µçš„äººåŠ›æˆ–LLMç”Ÿæˆçš„æ ‡ç­¾ï¼Œå¹¶ä¸”åœ¨åˆ†å¸ƒå˜åŒ–æ—¶æ€§èƒ½é€šå¸¸ä¼šä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14313v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†AIRL-Sï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰æ–¹æ³•ï¼Œç”¨äºç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŸºäºæœç´¢çš„TTSã€‚å®ƒé‡‡ç”¨å¯¹æŠ—æ€§é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¯†é›†ã€åŠ¨æ€çš„PRMï¼Œæ— éœ€æ ‡è®°çš„ä¸­é—´è¿‡ç¨‹æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½ä¼˜è¶Šï¼Œå¹³å‡æé«˜9%çš„æ€§èƒ½ï¼ŒåŒ¹é…GPT-4oçš„æ€§èƒ½ã€‚å…¶ä¼˜åŠ¿åœ¨äºå¥–åŠ±å‡½æ•°èƒ½å¤ŸæŒ‡å¯¼ä¸‹æ¸¸æœç´¢å¹¶ä½œä¸ºç­–ç•¥ä¼˜åŒ–çš„æŒ‡å¯¼ï¼Œå¢å¼ºè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIRL-Sç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œæœç´¢çš„TTSæ–¹æ³•ï¼Œç»Ÿä¸€äº†ä¸¤ç§ä¸åŒçš„LLMæµ‹è¯•æ—¶é—´ç¼©æ”¾èŒƒå¼ã€‚</li>
<li>é€šè¿‡å¯¹æŠ—æ€§é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¯†é›†ã€åŠ¨æ€çš„PRMã€‚</li>
<li>å¥–åŠ±å‡½æ•°åœ¨RLè®­ç»ƒä¸­å†…åœ¨åœ°ä»£è¡¨äº†ç†æƒ³çš„PRMï¼Œç”¨äºæŒ‡å¯¼ä¸‹æ¸¸æœç´¢ã€‚</li>
<li>å®éªŒç»“æœåœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­è¯æ˜äº†AIRL-Sçš„æœ‰æ•ˆæ€§ï¼Œå¹³å‡æé«˜æ€§èƒ½9%ï¼Œè¶…è¶Šäº†åŸºå‡†æ¨¡å‹ï¼ŒåŒ¹é…GPT-4oçš„æ€§èƒ½ã€‚</li>
<li>PRMåŒæ—¶ä½œä¸ºRL rolloutsçš„æ‰¹è¯„è€…å’Œæœç´¢è¿‡ç¨‹çš„å¯å‘å¼æŒ‡å¯¼ï¼Œå¢å¼ºäº†è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6305645bd1561078e02d5d498f5e26dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f57c37203deab202935d2ac2bd89c171.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability"><a href="#ReasonRank-Empowering-Passage-Ranking-with-Strong-Reasoning-Ability" class="headerlink" title="ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability"></a>ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</h2><p><strong>Authors:Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou</strong></p>
<p>Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{<a target="_blank" rel="noopener" href="https://brightbenchmark.github.io/%7D.%7D">https://brightbenchmark.github.io/}.}</a> Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/8421BCD/ReasonRank">https://github.com/8421BCD/ReasonRank</a>. </p>
<blockquote>
<p>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ—è¡¨æ’åºåœ¨è®¸å¤šæ®µè½æ’åºä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚éšç€å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„å‘å±•ï¼Œè®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶çš„é€æ­¥æ¨ç†æœ‰åŠ©äºæé«˜åˆ—è¡¨æ’åºæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œç°æœ‰çš„é‡æ–°æ’åå™¨åœ¨è®¸å¤šå¤æ‚çš„æ’ååœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”æ¨ç†å¯†é›†å‹é‡æ–°æ’åå™¨çš„æ’åèƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªå¼€å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»å¤šä¸ªé¢†åŸŸè·å–è®­ç»ƒæŸ¥è¯¢å’Œæ®µè½ï¼Œå¹¶åº”ç”¨DeepSeek-R1ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ‡ç­¾ã€‚è®¾è®¡äº†ä¸€ç§è‡ªæˆ‘ä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ï¼Œä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚ä¸ºäº†èµ‹äºˆåˆ—è¡¨é‡æ–°æ’åå™¨å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ç”¨äºå­¦ä¹ æ¨ç†æ¨¡å¼çš„å†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå’Œç”¨äºè¿›ä¸€æ­¥å¢å¼ºæ’åèƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µã€‚åœ¨RLé˜¶æ®µï¼ŒåŸºäºåˆ—è¡¨æ’åºçš„ç‰¹æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šè§†è§’æ’åå¥–åŠ±ï¼Œå®ƒæ¯”åŸºäºæ’åæŒ‡æ ‡çš„å¥–åŠ±æ›´æœ‰æ•ˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ¨ç†å¯†é›†å‹é‡æ–°æ’åå™¨ReasonRankæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶ä¸”ä¸é€ç‚¹é‡æ–°æ’åå™¨Rank1ç›¸æ¯”ï¼Œå»¶è¿Ÿæ—¶é—´æ›´ä½ã€‚<strong>é€šè¿‡è¿›ä¸€æ­¥çš„å®éªŒï¼Œæˆ‘ä»¬çš„ReasonRankåœ¨BRIGHTæ’è¡Œæ¦œä¸Šå–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¾—åˆ†ä¸º40.6ï¼ˆ<a target="_blank" rel="noopener" href="https://brightbenchmark.github.io/%EF%BC%89%E3%80%82">https://brightbenchmark.github.io/ï¼‰ã€‚</a></strong>æˆ‘ä»¬çš„ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/8421BCD/ReasonRank%E3%80%82">https://github.com/8421BCD/ReasonRankã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07050v2">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>     åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ—è¡¨æ’åºæ–¹å¼åœ¨è®¸å¤šæ®µè½æ’åºä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚éšç€å¤§å‹æ¨ç†æ¨¡å‹çš„å‘å±•ï¼Œè®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œæµ‹è¯•æ—¶çš„é€æ­¥æ¨ç†æœ‰åŠ©äºæé«˜åˆ—è¡¨æ’åºæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œç°æœ‰é‡æ’å™¨åœ¨å¤æ‚çš„æ’åºåœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸”æ¨ç†å¯†é›†å‹é‡æ’å™¨çš„æ’åºèƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†å¼€å‘ã€‚æœ¬æ–‡é¦–å…ˆæå‡ºä¸€ä¸ªè‡ªåŠ¨åŒ–æ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼Œä»å¤šä¸ªé¢†åŸŸè·å–è®­ç»ƒæŸ¥è¯¢å’Œæ®µè½ï¼Œå¹¶ä½¿ç”¨DeepSeek-R1ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ‡ç­¾ã€‚è®¾è®¡äº†ä¸€ç§è‡ªæˆ‘ä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚ä¸ºäº†èµ‹äºˆåˆ—è¡¨é‡æ’å™¨å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µåè®­ç»ƒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç”¨äºå­¦ä¹ æ¨ç†æ¨¡å¼çš„å†·å¯åŠ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é˜¶æ®µå’Œç”¨äºè¿›ä¸€æ­¥å¢å¼ºæ’åºèƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µã€‚åœ¨RLé˜¶æ®µï¼ŒåŸºäºåˆ—è¡¨æ’åºçš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šè§†è§’æ’åå¥–åŠ±ï¼Œå®ƒæ¯”åŸºäºæ’åæŒ‡æ ‡çš„å¥–åŠ±æ›´æœ‰æ•ˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ¨ç†å¯†é›†å‹é‡æ’å™¨ReasonRankæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶ä¸”ä¸å•ç‚¹é‡æ’å™¨Rank1ç›¸æ¯”å®ç°äº†æ›´ä½çš„å»¶è¿Ÿã€‚<strong>åœ¨è¿›ä¸€æ­¥å®éªŒä¸­ï¼ŒReasonRankåœ¨BRIGHTæ’è¡Œæ¦œä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½40.6ã€‚</strong></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ®µè½æ’åºä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæµ‹è¯•æ—¶çš„é€æ­¥æ¨ç†æœ‰åŠ©äºæé«˜åˆ—è¡¨æ’åºæ€§èƒ½ã€‚</li>
<li>ç°æœ‰é‡æ’å™¨åœ¨å¤æ‚æ’åºåœºæ™¯ä¸­è¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºè‡ªåŠ¨åŒ–æ¨ç†å¯†é›†å‹è®­ç»ƒæ•°æ®åˆæˆæ¡†æ¶ï¼ŒåŒ…æ‹¬ä»å¤šä¸ªé¢†åŸŸè·å–æ•°æ®å’Œä½¿ç”¨DeepSeek-R1ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ ‡ç­¾ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘ä¸€è‡´æ€§æ•°æ®è¿‡æ»¤æœºåˆ¶ä»¥ç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µåè®­ç»ƒæ–¹æ³•æ¥å¢å¼ºåˆ—è¡¨é‡æ’å™¨çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µã€‚</li>
<li>å¤šè§†è§’æ’åå¥–åŠ±è®¾è®¡æ›´æœ‰æ•ˆæé«˜æ’åæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58074a9e2aaf4c3ef27669ebe5389b55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bf74ad19c087b0645870523525ca004.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd2e577899524bfb6181c5094398b050.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e00b06cc0960d3a7fdb209345eed01a1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Dynamically-Adaptive-Reasoning-via-LLM-Guided-MCTS-for-Efficient-and-Context-Aware-KGQA"><a href="#Dynamically-Adaptive-Reasoning-via-LLM-Guided-MCTS-for-Efficient-and-Context-Aware-KGQA" class="headerlink" title="Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA"></a>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA</h2><p><strong>Authors:Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siyang Gao, Siwei Liu, Nan Yin</strong></p>
<p>Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ—¨åœ¨è§£é‡Šè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„å…³ç³»å’Œè¯­ä¹‰ç»“æ„è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œä»¥è·å–å‡†ç¡®ç­”æ¡ˆã€‚æœ€è¿‘çš„KGQAæ–¹æ³•ä¸»è¦éµå¾ªâ€œæ£€ç´¢åæ¨ç†â€çš„æ¨¡å¼ï¼Œä¾èµ–äºå›¾ç¥ç»ç½‘ç»œ(GNNs)æˆ–å¯å‘å¼è§„åˆ™è¿›è¡Œé™æ€è·¯å¾„æå–ï¼Œæˆ–è€…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æç¤ºæ¥è”åˆæ‰§è¡Œæ£€ç´¢å’Œæ¨ç†çš„åŠ¨æ€è·¯å¾„ç”Ÿæˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå‰è€…ç”±äºé™æ€è·¯å¾„æå–å’Œç¼ºä¹ä¸Šä¸‹æ–‡ç»†åŒ–è€Œé€‚åº”æ€§æœ‰é™ï¼Œåè€…åˆ™ç”±äºä¾èµ–äºå›ºå®šçš„è¯„åˆ†å‡½æ•°å’Œå¤§é‡çš„LLMè°ƒç”¨è€Œè®¡ç®—æˆæœ¬é«˜ï¼Œå¹¶ä¸”åœ¨è·¯å¾„è¯„ä¼°æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºåŠ¨æ€è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ¨ç†ï¼ˆDAMRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ç¬¦å·æœç´¢ä¸è‡ªé€‚åº”è·¯å¾„è¯„ä¼°ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚DAMRé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºéª¨å¹²ï¼Œä»¥åŸºäºLLMçš„è§„åˆ’å™¨ä¸ºæŒ‡å¯¼ï¼Œé€‰æ‹©å‰kä¸ªç›¸å…³å…³ç³»ä½œä¸ºæ¯ä¸€æ­¥éª¤æ¥å‡å°‘æœç´¢ç©ºé—´ã€‚ä¸ºäº†æé«˜è·¯å¾„è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºTransformerçš„è¯„åˆ†å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ï¼Œè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯è¡Œæ€§ä¼°è®¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šè·³æ¨ç†è¿‡ç¨‹ä¸­æ•æ‰ç»†å¾®çš„è¯­ä¹‰å˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£é«˜è´¨é‡ç›‘ç£çš„ç¨€ç¼ºæ€§ï¼ŒDAMRé‡‡ç”¨äº†ä¸€ç§åŠ¨æ€ä¼ªè·¯å¾„ç»†åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å®šæœŸä»æœç´¢è¿‡ç¨‹ä¸­æ¢ç´¢çš„éƒ¨åˆ†è·¯å¾„ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼Œä½¿è¯„åˆ†å™¨èƒ½å¤Ÿä¸æ–­é€‚åº”ä¸æ–­å˜åŒ–çš„æ¨ç†è½¨è¿¹åˆ†å¸ƒã€‚åœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDAMRæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00719v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>KGQAæ—¨åœ¨é€šè¿‡çŸ¥è¯†å›¾è°±çš„å…³ç³»å’Œè¯­ä¹‰ç»“æ„æ¥è§£é‡Šè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¹¶è¿›è¡Œç»“æ„åŒ–æ¨ç†ä»¥è·å–å‡†ç¡®ç­”æ¡ˆã€‚å½“å‰KGQAæ–¹æ³•ä¸»è¦éµå¾ªæ£€ç´¢åæ¨ç†æ¨¡å¼ï¼Œä¾èµ–äºå›¾ç¥ç»ç½‘ç»œæˆ–å¯å‘å¼è§„åˆ™è¿›è¡Œé™æ€è·¯å¾„æå–ï¼Œæˆ–ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŠ¨æ€è·¯å¾„ç”Ÿæˆã€‚ç„¶è€Œï¼Œå‰è€…å—é™äºé™æ€è·¯å¾„æå–å’Œç¼ºä¹ä¸Šä¸‹æ–‡ç»†åŒ–ï¼Œåè€…è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”ç”±äºä¾èµ–å›ºå®šè¯„åˆ†å‡½æ•°å’Œå¤§é‡è¯­è¨€æ¨¡å‹è°ƒç”¨ï¼Œè·¯å¾„è¯„ä¼°å‡†ç¡®æ€§ä¸é«˜ã€‚æœ¬æ–‡æå‡ºåŠ¨æ€è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¨ç†ï¼ˆDAMRï¼‰ï¼Œä¸€ä¸ªå°†ç¬¦å·æœç´¢ä¸è‡ªé€‚åº”è·¯å¾„è¯„ä¼°ç›¸ç»“åˆçš„é«˜æ•ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥KGQAæ¡†æ¶ã€‚DAMRé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ä½œä¸ºéª¨æ¶ï¼Œä»¥è¯­è¨€æ¨¡å‹ä¸ºåŸºç¡€çš„è§„åˆ’å™¨ä¸ºæŒ‡å¯¼ï¼Œæ¯ä¸€æ­¥é€‰æ‹©æœ€ç›¸å…³çš„å‰kä¸ªå…³ç³»æ¥å‡å°‘æœç´¢ç©ºé—´ã€‚ä¸ºæé«˜è·¯å¾„è¯„ä¼°å‡†ç¡®æ€§ï¼Œå¼•å…¥è½»é‡çº§åŸºäºTransformerçš„è¯„åˆ†å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ï¼Œè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆç†æ€§ä¼°è®¡ï¼Œä½¿æ¨¡å‹åœ¨è·¨è·³æ¨ç†è¿‡ç¨‹ä¸­æ•æ‰ç»†å¾®è¯­ä¹‰å˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºç¼“è§£é«˜è´¨é‡ç›‘ç£æ•°æ®çš„ç¨€ç¼ºæ€§ï¼ŒDAMRå¼•å…¥åŠ¨æ€ä¼ªè·¯å¾„ç»†åŒ–æœºåˆ¶ï¼Œå®šæœŸä»æœç´¢è¿‡ç¨‹ä¸­æ¢ç´¢çš„éƒ¨åˆ†è·¯å¾„ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼Œä½¿è¯„åˆ†å™¨èƒ½å¤Ÿä¸æ–­é€‚åº”ä¸æ–­å˜åŒ–çš„æ¨ç†è½¨è¿¹åˆ†å¸ƒã€‚åœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDAMRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>KGQAæ—¨åœ¨åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„ç»“æ„è¿›è¡Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢è§£è¯»å’Œç»“æ„åŒ–æ¨ç†ä»¥è·å–å‡†ç¡®ç­”æ¡ˆã€‚</li>
<li>å½“å‰KGQAæ–¹æ³•å­˜åœ¨é™æ€è·¯å¾„æå–çš„å±€é™æ€§ä»¥åŠé«˜è®¡ç®—æˆæœ¬å’Œè·¯å¾„è¯„ä¼°å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>DAMRæ¡†æ¶é€šè¿‡æ•´åˆç¬¦å·æœç´¢ä¸è‡ªé€‚åº”è·¯å¾„è¯„ä¼°æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>DAMRé‡‡ç”¨åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æ–¹æ³•ï¼Œä»¥è¯­è¨€æ¨¡å‹ä¸ºåŸºç¡€çš„è§„åˆ’å™¨æ¥å‡å°‘æœç´¢ç©ºé—´ã€‚</li>
<li>å¼•å…¥åŸºäºTransformerçš„è¯„åˆ†å™¨è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆç†æ€§ä¼°è®¡ï¼Œæé«˜è·¯å¾„è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>åŠ¨æ€ä¼ªè·¯å¾„ç»†åŒ–æœºåˆ¶ç”¨äºé€‚åº”ä¸æ–­å˜åŒ–çš„æ¨ç†è½¨è¿¹åˆ†å¸ƒï¼Œå¹¶ç¼“è§£é«˜è´¨é‡ç›‘ç£æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a91e4f4174fd25aea319827b527c740.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c62f7894d735a00a6dea0d5634122089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf505b9dc6c487552431b50ab86f1112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e52da940666d043ab1ac88bb37555e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-082a9cf3864a93bde0621052557ae1d0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Toolbox-Not-a-Hammer-â€“-Multi-TAG-Scaling-Math-Reasoning-with-Multi-Tool-Aggregation"><a href="#A-Toolbox-Not-a-Hammer-â€“-Multi-TAG-Scaling-Math-Reasoning-with-Multi-Tool-Aggregation" class="headerlink" title="A Toolbox, Not a Hammer â€“ Multi-TAG: Scaling Math Reasoning with   Multi-Tool Aggregation"></a>A Toolbox, Not a Hammer â€“ Multi-TAG: Scaling Math Reasoning with   Multi-Tool Aggregation</h2><p><strong>Authors:Bohan Yao, Vikas Yadav</strong></p>
<p>Augmenting large language models (LLMs) with external tools is a promising avenue for developing high-performance mathematical reasoning systems. Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps. To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool AGgregation-based framework. Instead of relying on a single tool, Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step. It then aggregates their diverse outputs to verify and refine the reasoning process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a finetuning-free, inference-only framework, making it readily applicable to any LLM backbone, including large open-weight models which are computationally expensive to finetune and proprietary frontier models which cannot be finetuned with custom recipes. We evaluate Multi-TAG on four challenging benchmarks: MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and closed-source LLM backbones, Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines. </p>
<blockquote>
<p>é€šè¿‡å¤–éƒ¨å·¥å…·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¼€å‘é«˜æ€§èƒ½æ•°å­¦æ¨ç†ç³»ç»Ÿçš„ä¸€æ¡æœ‰å‰é€”çš„é€”å¾„ã€‚å…ˆå‰çš„å·¥å…·å¢å¼ºæ–¹æ³•é€šå¸¸ä¼šå¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨æ¯ä¸€æ­¥æ¨ç†ä¸­é€‰æ‹©å¹¶è°ƒç”¨å•ä¸ªå·¥å…·ï¼Œå¹¶åœ¨GSM8Kç­‰ç®€å•çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†éœ€è¦å¤šæ­¥éª¤ç²¾ç¡®æ¨ç†çš„æ›´å¤æ‚çš„æ•°å­¦é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºå¤šå·¥å…·èšåˆçš„æ¡†æ¶Multi-TAGã€‚ä¸ä¾èµ–å•ä¸€å·¥å…·ä¸åŒï¼ŒMulti-TAGå¼•å¯¼LLMåœ¨æ¯ä¸€æ­¥æ¨ç†ä¸­åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·ã€‚ç„¶åï¼Œå®ƒèšåˆäº†è¿™äº›ä¸åŒçš„è¾“å‡ºæ¥éªŒè¯å’Œç»†åŒ–æ¨ç†è¿‡ç¨‹ï¼Œæé«˜äº†è§£å†³æ–¹æ¡ˆçš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMulti-TAGæ˜¯ä¸€ä¸ªæ— éœ€å¾®è°ƒã€ä»…ç”¨äºæ¨æ–­çš„æ¡†æ¶ï¼Œä½¿å…¶é€‚ç”¨äºä»»ä½•LLMä¸»å¹²ï¼ŒåŒ…æ‹¬è®¡ç®—ä¸Šæ˜‚è´µä¸”éš¾ä»¥å¾®è°ƒçš„å¤§å‹å¼€æ”¾æƒé‡æ¨¡å‹ä»¥åŠæ— æ³•ç”¨è‡ªå®šä¹‰é…æ–¹è¿›è¡Œå¾®è°ƒçš„ä¸“æœ‰å‰æ²¿æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†Multi-TAGï¼šMATH500ã€AIMEã€AMCå’ŒOlympiadBenchã€‚åœ¨å…¬å¼€æƒé‡å’Œå°é—­æºä»£ç çš„LLMä¸»å¹²ç½‘ä¸Šï¼ŒMulti-TAGæŒç»­ä¸”æ˜¾è‘—åœ°ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œå¹³å‡æ¯”æœ€æ–°åŸºçº¿é«˜å‡º6.0%è‡³7.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18973v2">PDF</a> Published at EMNLP Findings 2025; 21 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¤–éƒ¨å·¥å…·å¢å¼ºæ˜¯å®ç°é«˜æ€§èƒ½æ•°å­¦æ¨ç†ç³»ç»Ÿçš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚è¿‡å»çš„æ–¹æ³•é€šå¸¸åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­é€‰æ‹©å¹¶è°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼Œè¿™åœ¨ç®€å•çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚GSM8Kï¼‰ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†éœ€è¦å¤šæ­¥éª¤ç²¾ç¡®æ¨ç†çš„å¤æ‚æ•°å­¦é—®é¢˜æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Multi-TAGæ¡†æ¶ï¼Œå®ƒåŸºäºå¤šå·¥å…·èšåˆçš„æ–¹æ³•ã€‚ä¸åŒäºä¾èµ–å•ä¸€å·¥å…·çš„æ–¹æ³•ï¼ŒMulti-TAGæŒ‡å¯¼LLMåœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·ï¼Œç„¶åèšåˆå®ƒä»¬çš„ä¸åŒè¾“å‡ºæ¥éªŒè¯å’Œç»†åŒ–æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæé«˜è§£å†³æ–¹æ¡ˆçš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚ç‰¹åˆ«åœ°ï¼ŒMulti-TAGæ˜¯ä¸€ä¸ªæ— éœ€å¾®è°ƒã€ä»…ç”¨äºæ¨æ–­çš„æ¡†æ¶ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§LLMä¸»å¹²ç½‘ç»œï¼ŒåŒ…æ‹¬è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¤§å‹å¼€æ”¾æƒé‡æ¨¡å‹å’Œæ— æ³•ç”¨è‡ªå®šä¹‰é…æ–¹è¿›è¡Œå¾®è°ƒçš„ä¸“ä¸šå‰æ²¿æ¨¡å‹ã€‚åœ¨MATH500ã€AIMEã€AMCå’ŒOlympiadBenchå››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šï¼ŒMulti-TAGåœ¨å¼€æºå’Œé—­æºLLMä¸»å¹²ç½‘ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´ä¸”æ˜¾è‘—çš„ä¼˜äºæœ€æ–°æŠ€æœ¯åŸºå‡†çº¿çš„æ€§èƒ½ï¼Œå¹³å‡æå‡6.0%è‡³7.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¤–éƒ¨å·¥å…·å¢å¼ºåœ¨æ•°å­¦æ¨ç†é¢†åŸŸå…·æœ‰å‰æ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ•°å­¦é—®é¢˜æ—¶è¡¨ç°æœ‰é™ï¼Œéœ€è¦å¤šæ­¥éª¤ç²¾ç¡®æ¨ç†ã€‚</li>
<li>Multi-TAGæ¡†æ¶æ˜¯ä¸€ç§åŸºäºå¤šå·¥å…·èšåˆçš„æ–¹æ³•ï¼Œå¯æŒ‡å¯¼LLMåŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·è¿›è¡Œæ¨ç†ã€‚</li>
<li>Multi-TAGæé«˜äº†è§£å†³æ–¹æ¡ˆçš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œé€šè¿‡èšåˆå¤šä¸ªå·¥å…·çš„è¾“å‡ºè¿›è¡ŒéªŒè¯å’Œç»†åŒ–ã€‚</li>
<li>Multi-TAGæ˜¯ä¸€ä¸ªæ— éœ€å¾®è°ƒçš„æ¡†æ¶ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§LLMæ¨¡å‹ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒMulti-TAGæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹³å‡æå‡6.0%è‡³7.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ecc09386edba3a23e276b9e72a5d3cd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b56f04d153dcf67969231cef195900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e3625b46dfdf458b2a2f0512f7f34ad.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Evaluating-Speech-to-Text-x-LLM-x-Text-to-Speech-Combinations-for-AI-Interview-Systems"><a href="#Evaluating-Speech-to-Text-x-LLM-x-Text-to-Speech-Combinations-for-AI-Interview-Systems" class="headerlink" title="Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI   Interview Systems"></a>Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI   Interview Systems</h2><p><strong>Authors:Rumi Allbert, Nima Yazdani, Ali Ansari, Aruj Mahajan, Amirhossein Afsharrad, Seyed Shahabeddin Mousavi</strong></p>
<p>Voice-based conversational AI systems increasingly rely on cascaded architectures that combine speech-to-text (STT), large language models (LLMs), and text-to-speech (TTS) components. We present a large-scale empirical comparison of STT x LLM x TTS stacks using data sampled from over 300,000 AI-conducted job interviews. We used an LLM-as-a-Judge automated evaluation framework to assess conversational quality, technical accuracy, and skill assessment capabilities. Our analysis of five production configurations reveals that a stack combining Googleâ€™s STT, GPT-4.1, and Cartesiaâ€™s TTS outperforms alternatives in both objective quality metrics and user satisfaction scores. Surprisingly, we find that objective quality metrics correlate weakly with user satisfaction scores, suggesting that user experience in voice-based AI systems depends on factors beyond technical performance. Our findings provide practical guidance for selecting components in multimodal conversations and contribute a validated evaluation methodology for human-AI interactions. </p>
<blockquote>
<p>åŸºäºè¯­éŸ³çš„èŠå¤©AIç³»ç»Ÿè¶Šæ¥è¶Šä¾èµ–äºçº§è”æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†è¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç»„ä»¶ã€‚æˆ‘ä»¬é€šè¿‡å¯¹æ¥è‡ªè¶…è¿‡30ä¸‡ä»½AIé¢è¯•çš„æ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå¯¹STT x LLM x TTSå †æ ˆè¿›è¡Œäº†å¤§è§„æ¨¡å®è¯æ¯”è¾ƒã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥LLMä¸ºè¯„åˆ¤æ ‡å‡†çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå¯¹å¯¹è¯è´¨é‡ã€æŠ€æœ¯å‡†ç¡®æ€§å’ŒæŠ€èƒ½è¯„ä¼°èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å¯¹äº”ç§ç”Ÿäº§é…ç½®çš„åˆ†æè¡¨æ˜ï¼Œç»“åˆè°·æ­Œçš„STTã€GPT-4.1å’Œå¡ç‰¹è¥¿äºšçš„TTSçš„å †æ ˆåœ¨å®¢è§‚è´¨é‡æŒ‡æ ‡å’Œç”¨æˆ·æ»¡æ„åº¦å¾—åˆ†æ–¹é¢éƒ½ä¼˜äºå…¶ä»–é€‰æ‹©ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å®¢è§‚è´¨é‡æŒ‡æ ‡ä¸ç”¨æˆ·æ»¡æ„åº¦å¾—åˆ†ä¹‹é—´çš„ç›¸å…³æ€§å¾ˆå¼±ï¼Œè¿™è¡¨æ˜ç”¨æˆ·åœ¨åŸºäºè¯­éŸ³çš„AIç³»ç»Ÿä¸­çš„ä½“éªŒé™¤äº†æŠ€æœ¯æ€§èƒ½å¤–è¿˜å—å…¶ä»–å› ç´ å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºé€‰æ‹©å¤šæ¨¡å¼å¯¹è¯ä¸­çš„ç»„ä»¶æä¾›äº†å®é™…æŒ‡å¯¼ï¼Œå¹¶ä¸ºäººç±»ä¸AIä¹‹é—´çš„äº¤äº’æä¾›äº†ä¸€ç§ç»è¿‡éªŒè¯çš„è¯„ä¼°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16835v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­éŸ³çš„èŠå¤©AIç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°ä¾èµ–äºç»“åˆè¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç»„ä»¶çš„çº§è”æ¶æ„ã€‚æœ¬ç ”ç©¶é€šè¿‡å®è¯å¯¹æ¯”è¶…è¿‡30ä¸‡ä»½AIé¢è¯•æ•°æ®ï¼Œè¯„ä¼°äº†ä¸åŒSTT x LLM x TTSç»„åˆçš„è¡¨ç°ã€‚ç ”ç©¶ä½¿ç”¨LLMä½œä¸ºè¯„ä¼°æ¡†æ¶å¯¹ä¼šè¯è´¨é‡ã€æŠ€æœ¯å‡†ç¡®æ€§å’ŒæŠ€èƒ½è¯„ä¼°èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚åˆ†æè¡¨æ˜ï¼Œç»“åˆè°·æ­ŒSTTã€GPT-4.1å’Œå¡ç‰¹è¥¿äºšTTSçš„ç»„åˆåœ¨å®¢è§‚è´¨é‡æŒ‡æ ‡å’Œç”¨æˆ·æ»¡æ„åº¦å¾—åˆ†ä¸Šè¡¨ç°æœ€ä½³ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå®¢è§‚è´¨é‡æŒ‡æ ‡ä¸ç”¨æˆ·æ»¡æ„åº¦å¾—åˆ†ç›¸å…³æ€§è¾ƒä½ï¼Œè¡¨æ˜ç”¨æˆ·ä½“éªŒä¸ä»…å–å†³äºæŠ€æœ¯æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸ºé€‰æ‹©å¤šæ¨¡æ€å¯¹è¯ä¸­çš„ç»„ä»¶æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œå¹¶ä¸ºäººç±»ä¸AIçš„äº’åŠ¨éªŒè¯è¯„ä¼°æ–¹æ³•åšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¯¹è¯AIç³»ç»Ÿä¾èµ–çº§è”æ¶æ„ç»“åˆSTTã€LLMå’ŒTTSç»„ä»¶ã€‚</li>
<li>é€šè¿‡è¶…è¿‡30ä¸‡ä»½AIé¢è¯•æ•°æ®çš„å®è¯å¯¹æ¯”ï¼Œè¯„ä¼°äº†ä¸åŒSTT x LLM x TTSç»„åˆçš„è¡¨ç°ã€‚</li>
<li>LLMä½œä¸ºè¯„ä¼°æ¡†æ¶ç”¨äºè¯„ä¼°ä¼šè¯è´¨é‡ã€æŠ€æœ¯å‡†ç¡®æ€§å’ŒæŠ€èƒ½è¯„ä¼°èƒ½åŠ›ã€‚</li>
<li>æœ€ä½³ç»„åˆä¸ºè°·æ­ŒSTTã€GPT-4.1å’Œå¡ç‰¹è¥¿äºšTTSçš„ç»„åˆï¼Œåœ¨å®¢è§‚è´¨é‡æŒ‡æ ‡å’Œç”¨æˆ·æ»¡æ„åº¦å¾—åˆ†ä¸Šè¡¨ç°çªå‡ºã€‚</li>
<li>å®¢è§‚è´¨é‡æŒ‡æ ‡ä¸ç”¨æˆ·æ»¡æ„åº¦å¾—åˆ†ç›¸å…³æ€§è¾ƒä½ï¼Œè¡¨æ˜ç”¨æˆ·ä½“éªŒä¸ä»…å—æŠ€æœ¯æ€§èƒ½å½±å“ã€‚</li>
<li>ç ”ç©¶ä¸ºå¤šæ¨¡æ€å¯¹è¯ä¸­çš„ç»„ä»¶é€‰æ‹©æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13aff6ed5fa1e4fa315c6bd457471a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c7f82571786a87224c4a6abf3ef1254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c8ce325334936973a11eb903ae2e576.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b61ac44bfb743a03c1edc40d888790e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c98d36d471d9fee88e00704e55eb67ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-319ba15e726b0835e41f30f90d341752.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Simple-â€œTry-Againâ€-Can-Elicit-Multi-Turn-LLM-Reasoning"><a href="#A-Simple-â€œTry-Againâ€-Can-Elicit-Multi-Turn-LLM-Reasoning" class="headerlink" title="A Simple â€œTry Againâ€ Can Elicit Multi-Turn LLM Reasoning"></a>A Simple â€œTry Againâ€ Can Elicit Multi-Turn LLM Reasoning</h2><p><strong>Authors:Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li</strong></p>
<p>Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., â€œLetâ€™s try againâ€) after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: <a target="_blank" rel="noopener" href="https://github.com/lichengliu03/unary-feedback">https://github.com/lichengliu03/unary-feedback</a> </p>
<blockquote>
<p>å¤šè½®é—®é¢˜æ±‚è§£å¯¹äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰æ¥è¯´æ˜¯è‡³å…³é‡è¦çš„ï¼Œä½†ä¹Ÿé¢‡å…·æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ¨¡å‹åæ€è‡ªå·±çš„æ¨ç†å¹¶æ ¹æ®åé¦ˆè¿›è¡Œä¿®æ”¹ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•æ˜¯åœ¨å•è½®æ¨¡å¼ä¸‹è®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹ï¼Œå¹¶è¾…ä»¥å¯éªŒè¯çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé‡‡ç”¨ç°æœ‰RLæ¨¡å¼è®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¸§å¤±äº†å¤šè½®é—®é¢˜æ±‚è§£çš„èƒ½åŠ›ï¼Œéš¾ä»¥æ ¹æ®ä¸Šä¸‹æ–‡åé¦ˆä¿®æ”¹ç­”æ¡ˆï¼Œå¯¼è‡´é‡å¤å“åº”ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šLRMèƒ½å¦åœ¨å¤šè½®å¯¹è¯çš„æƒ…å¢ƒä¸­å­¦ä¼šåæ€è‡ªå·±çš„ç­”æ¡ˆï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»…åœ¨é”™è¯¯ç­”æ¡ˆåæä¾›ä¸€å…ƒåé¦ˆï¼ˆä¾‹å¦‚â€œè¯·å†è¯•ä¸€æ¬¡â€ï¼‰ï¼Œå°±å¯ä»¥æé«˜å•è½®æ€§èƒ½å’Œå¤šè½®æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºå¼ºåŒ–å­¦ä¹ å¼•å…¥äº†åä¸ºâ€œUnary Feedback as Observationâ€ï¼ˆUFOï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è¿­ä»£é—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ä½¿ç”¨æœ€å°‘ä½†å¸¸è§çš„ä¸€å…ƒç”¨æˆ·åé¦ˆã€‚å®ƒå¯ä»¥è½»æ¾åº”ç”¨äºç°æœ‰çš„å•è½®RLè®­ç»ƒè®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨UFOçš„RLè®­ç»ƒä¿æŒäº†å•è½®æ€§èƒ½ï¼Œå¹¶é€šè¿‡æœ€å¤šæé«˜14%çš„å¤šè½®æ¨ç†å‡†ç¡®æ€§ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šè½®é—®é¢˜æ±‚è§£ä¸­æ›´å¥½åœ°åº”å¯¹åé¦ˆã€‚ä¸ºäº†å°½é‡å‡å°‘å¾—åˆ°æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„è½®æ•°ï¼ŒåŒæ—¶åœ¨å‡ºç°é”™è¯¯æ—¶é¼“åŠ±å¤šæ ·åŒ–çš„æ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¥–åŠ±ç»“æ„ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹åœ¨æ¯è½®ä¸­ç»™å‡ºè°¨æ…å’Œå‘¨å…¨çš„ç­”æ¡ˆã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/lichengliu03/unary-feedback">https://github.com/lichengliu03/unary-feedback</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14295v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šå›åˆé—®é¢˜è§£å†³ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åæ€å¹¶åŸºäºåé¦ˆè¿›è¡Œä¿®è®¢ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•é€šå¸¸åœ¨å•å›åˆæ¨¡å¼ä¸‹è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å¯éªŒè¯çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œè¿™äº›è®­ç»ƒåçš„æ¨¡å‹å¸¸å¸¸ç¼ºä¹å¤šå›åˆè§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œå¹¶ä¸”éš¾ä»¥åŸºäºä¸Šä¸‹æ–‡åé¦ˆä¿®è®¢ç­”æ¡ˆï¼Œå¯¼è‡´é‡å¤å›åº”ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤šå›åˆç¯å¢ƒä¸‹çš„åé¦ˆæœºåˆ¶ã€‚é€šè¿‡å¼•å…¥ä»…ä½¿ç”¨ä¸€å…ƒåé¦ˆï¼ˆä¾‹å¦‚â€œå†è¯•ä¸€æ¬¡â€ï¼‰çš„å¤šå›åˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†å•å›åˆæ€§èƒ½å’Œå¤šå›åˆæ¨ç†èƒ½åŠ›ã€‚è¿˜ä»‹ç»äº†ç”¨äºå¼ºåŒ–å­¦ä¹ çš„ä¸€å…ƒåé¦ˆä½œä¸ºè§‚å¯Ÿï¼ˆUFOï¼‰ï¼Œå®ƒå¯ä»¥åœ¨è¿­ä»£é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ä½¿ç”¨æœ€å°‘ä½†å¸¸è§çš„ä¸€å…ƒç”¨æˆ·åé¦ˆã€‚å®éªŒç»“æœè¯æ˜ï¼Œä½¿ç”¨UFOçš„RLè®­ç»ƒåœ¨ä¿æŒå•å›åˆæ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†å¤šå›åˆæ¨ç†çš„å‡†ç¡®æ€§ï¼Œæœ€å¤šå¯æé«˜14%ã€‚ä¸ºé¼“åŠ±åœ¨å‡ºé”™æ—¶å‡å°‘å›åˆæ•°å¹¶æä¾›å¤šæ ·åŒ–çš„æ¨ç†ï¼Œè®¾è®¡äº†å¥–åŠ±ç»“æ„æ¥æŒ‡å¯¼æ¨¡å‹åœ¨æ¯ä¸ªå›åˆä¸­ä½œå‡ºè°¨æ…å’Œæ·±æ€ç†Ÿè™‘çš„å›ç­”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šå›åˆé—®é¢˜è§£å†³ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¢å¼ºåæ€å’Œä¿®è®¢èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¸»è¦åŸºäºå•å›åˆæ¨¡å¼ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤šå›åˆç¯å¢ƒä¸­æ€§èƒ½ä¸‹é™ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸€å…ƒåé¦ˆä½œä¸ºè§‚å¯Ÿï¼ˆUFOï¼‰ï¼Œæé«˜äº†æ¨¡å‹åœ¨å¤šå›åˆç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨UFOçš„RLè®­ç»ƒå¯åŒæ—¶æé«˜å•å›åˆæ€§èƒ½å’Œå¤šå›åˆæ¨ç†çš„å‡†ç¡®æ€§ï¼Œæé«˜å¹…åº¦æœ€å¤šå¯è¾¾14%ã€‚</li>
<li>ä¸ºé¼“åŠ±åœ¨å‡ºé”™æ—¶æä¾›å¤šæ ·åŒ–çš„æ¨ç†å¹¶å‡å°‘å›åˆæ•°ï¼Œè®¾è®¡äº†æ–°çš„å¥–åŠ±ç»“æ„ã€‚</li>
<li>è¯¥æ–¹æ³•å¯é€šè¿‡æœ€å°‘ä½†å¸¸è§çš„ä¸€å…ƒç”¨æˆ·åé¦ˆè¿›è¡Œè¿­ä»£é—®é¢˜è§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50bb1802b4046fb585bf6e04edc88061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28852eba37722a9cc7d23e4673ba84a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1b03959cfff1e6915f8bba60b0c85c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f896cf85fdb9c8dd85ab208eebd9c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ab6f68cd514cf3f670dca8336b3356.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Generalized-Tree-Edit-Distance-GTED-A-Faithful-Evaluation-Metric-for-Statement-Autoformalization"><a href="#Generalized-Tree-Edit-Distance-GTED-A-Faithful-Evaluation-Metric-for-Statement-Autoformalization" class="headerlink" title="Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for   Statement Autoformalization"></a>Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for   Statement Autoformalization</h2><p><strong>Authors:Yuntian Liu, Tao Zhu, Xiaoyang Liu, Yu Chen, Zhaoxuan Liu, Qingfeng Guo, Jiashuo Zhang, Kangjie Bao, Tao Luo</strong></p>
<p>Statement autoformalization, the automated translation of statements from natural language into formal languages, has become a subject of extensive research, yet the development of robust automated evaluation metrics remains limited. Existing evaluation methods often lack semantic understanding, face challenges with high computational costs, and are constrained by the current progress of automated theorem proving. To address these issues, we propose GTED (Generalized Tree Edit Distance), a novel evaluation framework that first standardizes formal statements and converts them into operator trees, then determines the semantic similarity using the eponymous GTED metric. Across the miniF2F and ProofNet benchmarks, GTED consistently ranks as a top-performing metric, achieving the highest accuracy and Kappa on miniF2F and the joint-highest accuracy on ProofNet. This strong overall performance provides the community with a computationally lightweight and more faithful metric for automated evaluation. The code and experimental results are available at <a target="_blank" rel="noopener" href="https://github.com/XiaoyangLiu-sjtu/GTED">https://github.com/XiaoyangLiu-sjtu/GTED</a>. </p>
<blockquote>
<p>è¯­å¥è‡ªåŠ¨å½¢å¼åŒ–æ˜¯å°†è‡ªç„¶è¯­è¨€è¯­å¥è‡ªåŠ¨è½¬æ¢ä¸ºå½¢å¼åŒ–è¯­è¨€ï¼Œè¿™å·²æˆä¸ºå¹¿æ³›ç ”ç©¶çš„è¯¾é¢˜ï¼Œç„¶è€Œï¼Œé²æ£’çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å¼€å‘ä»ç„¶æœ‰é™ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œé¢ä¸´è®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ï¼Œå¹¶å—åˆ°è‡ªåŠ¨å®šç†è¯æ˜å½“å‰è¿›å±•çš„åˆ¶çº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GTEDï¼ˆå¹¿ä¹‰æ ‘ç¼–è¾‘è·ç¦»ï¼‰è¿™ä¸€æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œå®ƒé¦–å…ˆæ ‡å‡†åŒ–å½¢å¼åŒ–è¯­å¥å¹¶å°†å…¶è½¬æ¢ä¸ºæ“ä½œæ ‘ï¼Œç„¶åä½¿ç”¨åŒåçš„GTEDæŒ‡æ ‡ç¡®å®šè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚åœ¨miniF2Få’ŒProofNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTEDä¸€ç›´è¡¨ç°æœ€ä½³ï¼Œåœ¨miniF2Fä¸Šå®ç°äº†æœ€é«˜å‡†ç¡®ç‡å’ŒKappaå€¼ï¼Œåœ¨ProofNetä¸Šå®ç°äº†è”åˆæœ€é«˜å‡†ç¡®ç‡ã€‚è¿™ç§å¼ºå¤§çš„æ•´ä½“æ€§èƒ½è¡¨ç°ä½¿ç¤¾åŒºèƒ½å¤Ÿä½¿ç”¨è®¡ç®—è½»é‡ä¸”æ›´å¯é çš„æŒ‡æ ‡æ¥è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚ä»£ç å’Œå®éªŒç»“æœå¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/XiaoyangLiu-sjtu/GTED%E4%BA%86%E8%A7%A3%E3%80%82">https://github.com/XiaoyangLiu-sjtu/GTEDäº†è§£ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07399v2">PDF</a> Accepted to AI4Math@ICML25</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è¯­å¥å½¢å¼åŒ–æ˜¯å°†è‡ªç„¶è¯­è¨€è¯­å¥è‡ªåŠ¨è½¬åŒ–ä¸ºå½¢å¼è¯­è¨€è¯­å¥çš„ç ”ç©¶è¯¾é¢˜ï¼Œä½†å¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å¼€å‘ä»å­˜åœ¨é™åˆ¶ã€‚ä¸ºè§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•ç¼ºä¹è¯­ä¹‰ç†è§£ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠä¸è‡ªåŠ¨å®šç†è¯æ˜å½“å‰è¿›å±•ç›¸åˆ¶çº¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GTEDï¼ˆå¹¿ä¹‰æ ‘ç¼–è¾‘è·ç¦»ï¼‰è¿™ä¸€æ–°å‹è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ ‡å‡†åŒ–å½¢å¼è¯­å¥å¹¶å°†å…¶è½¬åŒ–ä¸ºæ“ä½œæ ‘ï¼Œç„¶åä½¿ç”¨åŒåçš„GTEDæŒ‡æ ‡ç¡®å®šè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚åœ¨miniF2Få’ŒProofNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTEDè¡¨ç°ä¼˜å¼‚ï¼Œå–å¾—äº†æœ€é«˜å‡†ç¡®ç‡å’ŒKappaç³»æ•°ã€‚å®ƒä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªè®¡ç®—é‡å°ã€æ›´å‡†ç¡®çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚ç›¸å…³ä»£ç å’Œå®éªŒç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaoyangLiu-sjtu/GTED%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XiaoyangLiu-sjtu/GTEDæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­å¥å½¢å¼åŒ–æ˜¯ä¸€ä¸ªçƒ­é—¨ç ”ç©¶é¢†åŸŸï¼Œä½†è¯„ä¼°æŒ‡æ ‡çš„å¯é æ€§ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•å­˜åœ¨è¯­ä¹‰ç†è§£ä¸è¶³ã€è®¡ç®—æˆæœ¬é«˜ä»¥åŠä¸è‡ªåŠ¨å®šç†è¯æ˜è¿›å±•åˆ¶çº¦çš„é—®é¢˜ã€‚</li>
<li>GTEDæ˜¯ä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ ‡å‡†åŒ–å½¢å¼è¯­å¥å¹¶è½¬åŒ–ä¸ºæ“ä½œæ ‘æ¥è§£å†³ç°æœ‰é—®é¢˜ã€‚</li>
<li>GTEDåˆ©ç”¨å¹¿ä¹‰æ ‘ç¼–è¾‘è·ç¦»æ¥ç¡®å®šè¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGTEDè¡¨ç°ä¼˜ç§€ï¼Œå–å¾—äº†é«˜å‡†ç¡®ç‡å’ŒKappaç³»æ•°ã€‚</li>
<li>GTEDä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªè®¡ç®—é‡å°ã€æ›´å‡†ç¡®çš„è‡ªåŠ¨åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a6b6491a8f237d5344d6368ab22f2a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e01cc1045b13f1a5454ec141034b1094.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201aa4121f2820ebd6de8a15098b0056.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b8cce74600a01cc6f49fc3043f0b170.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a2ef0cea8552dc7c890689d93ffdc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-150566a857d5e3feca543e79f4a8567a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs"><a href="#Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs" class="headerlink" title="Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of   LLMs"></a>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of   LLMs</h2><p><strong>Authors:Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang</strong></p>
<p>Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the modelâ€™s evolving performance on those pairs. By adaptively weighting samples according to both data quality and the modelâ€™s learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO">https://github.com/pspdada/Omni-DPO</a>. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å› å…¶ç®€å•é«˜æ•ˆè€Œæˆä¸ºå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„æ ¸å¿ƒæ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºDPOçš„æ–¹æ³•é€šå¸¸å°†åå¥½å¯¹ç»Ÿä¸€å¤„ç†ï¼Œå¿½ç•¥äº†å…¶å†…åœ¨è´¨é‡å’Œå­¦ä¹ å®ç”¨æ€§çš„é‡è¦å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨ä¸ä½³å’Œæ€§èƒ½ä¸è¶³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Omni-DPOï¼Œä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ—¶è€ƒè™‘ï¼ˆ1ï¼‰æ¯ä¸ªåå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œï¼ˆ2ï¼‰æ¨¡å‹åœ¨è¿™äº›åå¥½å¯¹ä¸Šçš„æ€§èƒ½å˜åŒ–ã€‚Omni-DPOé€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€è‡ªé€‚åº”åœ°åŠ æƒæ ·æœ¬ï¼Œä»è€Œå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨å’Œæ›´å¥½çš„æ€§èƒ½ã€‚åœ¨å„ç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†Omni-DPOçš„ä¼˜è¶Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Omni-DPOå¾®è°ƒåçš„Gemma-2-9b-itåœ¨Arena-HardåŸºå‡†æµ‹è¯•ä¸Šå¤§å¹…é¢†å…ˆé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹Claude 3 Opusï¼Œå¾—åˆ†é«˜å‡º6.7åˆ†ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒOmni-DPOåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºæˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§æä¾›äº†æœ‰åŠ›çš„å®è¯è¯æ®ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/pspdada/Omni-DPOä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10054v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ä¸­ï¼ŒDirect Preference Optimizationï¼ˆDPOï¼‰æ–¹æ³•å› å…¶ç®€å•é«˜æ•ˆè€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰DPOæ–¹æ³•å¾€å¾€å¿½ç•¥åå¥½å¯¹æœ¬èº«çš„è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨ä¸Šçš„å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨ä¸å¤Ÿä¼˜åŒ–å’Œæ€§èƒ½ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºOmni-DPOï¼Œä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ—¶è€ƒè™‘åå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½å˜åŒ–ã€‚é€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹å­¦ä¹ åŠ¨æ€è‡ªé€‚åº”åœ°è°ƒæ•´æ ·æœ¬æƒé‡ï¼ŒOmni-DPOå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨å’Œæ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-DPOåœ¨å¤šä¸ªæ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DPOæ˜¯å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­çš„ä¸€ä¸ªé‡è¦æ–¹æ³•ï¼Œä½†å…¶å­˜åœ¨å¯¹åå¥½å¯¹è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨çš„å¿½è§†ã€‚</li>
<li>Omni-DPOæ¡†æ¶è€ƒè™‘äº†åå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚</li>
<li>Omni-DPOé€šè¿‡è‡ªé€‚åº”è°ƒæ•´æ ·æœ¬æƒé‡ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOmni-DPOåœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Omni-DPOçš„Gemma-2-9b-itæ¨¡å‹åœ¨Arena-HardåŸºå‡†æµ‹è¯•ä¸Šå¤§å¹…è¶…è¶Šé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹Claude 3 Opusã€‚</li>
<li>Omni-DPOåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸ŠåŒæ ·è¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5f6f80e1e7333c65abbcc512cd4c4be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92768d16160a9b46a78802d60fce2959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79ad950d6f31bded15a91cbddd0da112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6604a16813e11a35aa5efba4ebb3208f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-169f8e5805d16b63539c2e6567304608.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-01cfd708b7f9a059552248c794d5f179.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-26  LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-25/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d8320d48f65c7350736f76c6f7f830e5.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-25  Mobile-R1 Towards Interactive Reinforcement Learning for VLM-Based   Mobile Agent via Task-Level Rewards
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
