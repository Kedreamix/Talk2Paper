<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-08-26  Vevo2 Bridging Controllable Speech and Singing Voice Generation via   Unified Prosody Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0fab05b621b1f919e40cf07f0d4234d5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-26-更新"><a href="#2025-08-26-更新" class="headerlink" title="2025-08-26 更新"></a>2025-08-26 更新</h1><h2 id="Vevo2-Bridging-Controllable-Speech-and-Singing-Voice-Generation-via-Unified-Prosody-Learning"><a href="#Vevo2-Bridging-Controllable-Speech-and-Singing-Voice-Generation-via-Unified-Prosody-Learning" class="headerlink" title="Vevo2: Bridging Controllable Speech and Singing Voice Generation via   Unified Prosody Learning"></a>Vevo2: Bridging Controllable Speech and Singing Voice Generation via   Unified Prosody Learning</h2><p><strong>Authors:Xueyao Zhang, Junan Zhang, Yuancheng Wang, Chaoren Wang, Yuanzhe Chen, Dongya Jia, Zhuo Chen, Zhizheng Wu</strong></p>
<p>Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5 Hz) content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during pre-training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the AR model’s ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2’s effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at <a target="_blank" rel="noopener" href="https://versasinger.github.io/">https://versasinger.github.io/</a>. </p>
<blockquote>
<p>可控的人声生成，特别是在歌唱等表达领域，仍然是一个巨大的挑战。本文介绍了Vevo2，一个用于可控语音和歌唱声音生成的统一框架。为了解决标注歌唱数据稀缺的问题并实现灵活的可控性，Vevo2引入了两种音频标记器：(1)一种无需音乐符号的语调标记器，可以从语音、歌唱甚至乐器声音中捕捉语调和旋律；(2)一种低帧率（12.5 Hz）的内容风格标记器，编码语音和歌唱的语言内容、语调和风格，同时实现音色分离。Vevo2包括一个自回归（AR）的内容风格建模阶段，旨在实现对文本、语调和风格的控制，以及一个流匹配声学建模阶段，允许音色控制。特别是在AR模型的预训练过程中，我们提出了显性和隐性的语调学习策略，以弥合语音和歌唱声音之间的差距。此外，为了进一步提高AR模型对文本和语调的跟随能力，我们设计了一个多目标后训练任务，融合了可理解性和语调相似性对齐。实验结果表明，Vevo2的统一建模对语音和歌唱声音生成都带来了互惠互利的效果。此外，Vevo2在语音和歌唱的广泛合成、转换和编辑任务中的有效性进一步证明了其强大的通用性和多功能性。音频样本可在<a target="_blank" rel="noopener" href="https://versasinger.github.io/">https://versasinger.github.io/</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16332v1">PDF</a> We will release code and model checkpoints at   <a target="_blank" rel="noopener" href="https://github.com/open-mmlab/Amphion">https://github.com/open-mmlab/Amphion</a></p>
<p><strong>摘要</strong></p>
<p>该论文提出了一种统一的框架Vevo2，用于可控的人声生成，特别是适用于唱歌等表达领域。Vevo2通过引入两种音频标记器来解决标注歌唱数据稀缺和灵活性控制问题。其一是无需音乐符号的语调标记器，能够捕捉语音、歌唱甚至器乐声音中的语调和旋律；其二是低帧率（12.5Hz）的内容风格标记器，能够对语音和歌唱进行文本、语调和风格的控制，同时实现音色分离。Vevo2包括一个自回归内容风格建模阶段，旨在实现对文本、语调和风格的控制，以及一个流程匹配声学建模阶段，可实现音色控制。论文还介绍了自回归模型预训练和基于文本与语调跟踪的多目标后训练任务的设计。实验结果表明，Vevo2的统一建模为语音和歌唱声音生成带来了互利效益，其在合成、转换和编辑任务上的广泛适用性进一步证明了其强大的通用性和灵活性。相关音频样本可在<a target="_blank" rel="noopener" href="https://versasinger.github.io/%E8%8E%B7%E5%8F%96%E3%80%82">https://versasinger.github.io/获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>Vevo2是一个统一的框架，用于可控的人声生成，包括语音和歌唱。</li>
<li>引入两种音频标记器：无需音乐符号的语调标记器和低帧率内容风格标记器。</li>
<li>自回归内容风格建模阶段实现对文本、语调和风格的控制。</li>
<li>流程匹配声学建模阶段允许音色控制。</li>
<li>提出预训练中的明确和隐含语调学习策略，以缩小语音和歌唱之间的差距。</li>
<li>设计多目标后训练任务，以提高自回归模型对文本和语调的跟踪能力。</li>
<li>实验结果表明Vevo2在语音和歌唱生成上的优越性，及其在多种任务上的广泛适用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16332">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-518466d300adafab1546116098a83a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e52e7a15033ca231cc66c24059f8000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3d44f302a8a0618c15f6f66e961b48d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hybrid-Pruning-In-Situ-Compression-of-Self-Supervised-Speech-Models-for-Speaker-Verification-and-Anti-Spoofing"><a href="#Hybrid-Pruning-In-Situ-Compression-of-Self-Supervised-Speech-Models-for-Speaker-Verification-and-Anti-Spoofing" class="headerlink" title="Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for   Speaker Verification and Anti-Spoofing"></a>Hybrid Pruning: In-Situ Compression of Self-Supervised Speech Models for   Speaker Verification and Anti-Spoofing</h2><p><strong>Authors:Junyi Peng, Lin Zhang, Jiangyu Han, Oldřich Plchot, Johan Rohdin, Themos Stafylakis, Shuai Wang, Jan Černocký</strong></p>
<p>Although large-scale self-supervised learning (SSL) models like WavLM have achieved state-of-the-art performance in speech processing, their significant size impedes deployment on resource-constrained devices. While structured pruning is a key technique for model compression, existing methods typically separate it from task-specific fine-tuning. This multi-stage approach struggles to create optimal architectures tailored for diverse downstream tasks. In this work, we introduce a unified framework that integrates structured pruning into the downstream fine-tuning process. Our framework unifies these steps, jointly optimizing for task performance and model sparsity in a single stage. This allows the model to learn a compressed architecture specifically for the end task, eliminating the need for complex multi-stage pipelines and knowledge distillation. Our pruned models achieve up to a 70% parameter reduction with negligible performance degradation on large-scale datasets, achieving equal error rates of 0.7%, 0.8%, and 1.6% on Vox1-O, -E, and -H, respectively. Furthermore, our approach demonstrates improved generalization in low-resource scenarios, reducing overfitting and achieving a state-of-the-art 3.7% EER on ASVspoof5. </p>
<blockquote>
<p>尽管像WavLM这样的大规模自监督学习（SSL）模型在语音处理方面已经达到了最前沿的性能，但它们的大规模阻碍了其在资源受限设备上的部署。虽然结构化剪枝是模型压缩的关键技术，但现有方法通常将其与特定任务的微调分开。这种多阶段的方法很难为各种下游任务创建最佳架构。在这项工作中，我们介绍了一个统一的框架，该框架将结构化剪枝集成到下游微调过程中。我们的框架统一了这些步骤，在一个阶段中联合优化任务性能和模型稀疏性。这允许模型为终端任务学习一个压缩架构，从而无需复杂的多阶段管道和蒸馏。我们的剪枝模型实现了高达70%的参数减少，在大规模数据集上的性能下降可以忽略不计，在Vox1-O、-E和-H上的错误率分别为0.7%、0.8%和1.6%。此外，我们的方法在资源稀缺场景中表现出更好的泛化能力，减少了过拟合现象，并在ASVspoof5上达到了最先进的3.7%的拒真率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模自监督学习模型如WavLM在语音处理方面达到了先进的技术水平，但其庞大的规模阻碍了其在资源受限设备上的部署。本研究引入了一个统一的框架，将结构化剪枝集成到下游微调过程中，以优化任务性能和模型稀疏性。该框架消除了复杂的多阶段管道和知识蒸馏的需求，使模型能够学习专门针对终端任务的压缩架构。经过修剪的模型在大型数据集上实现了高达70%的参数减少，同时性能下降微乎其微，在Vox1-O、-E和-H上分别达到了0.7%、0.8%和1.6%的错误率。此外，该方法在低资源场景中的泛化性能有所提升，减少了过拟合现象，并在ASVspoof5上达到了最先进的3.7%的EER。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型自监督学习模型如WavLM在语音处理上表现卓越，但难以在资源受限设备上部署。</li>
<li>结构化剪枝是模型压缩的关键技术，但现有方法通常将其与任务特定微调分开。</li>
<li>本研究提出一个统一框架，将结构化剪枝与下游微调过程相结合，优化任务性能和模型稀疏性。</li>
<li>该框架实现了高达70%的参数减少，同时保持或略微提高了性能。</li>
<li>在大型数据集上，经过修剪的模型性能下降微乎其微。</li>
<li>该方法在低资源场景中的泛化性能有所提升，减少了过拟合现象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16232">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-87dfb1dc5100a02fb35f12963672aa8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ff19f3fb84fe66449fe1697bf1f3bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8766636084923fa5222106dfee5b49c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f36f0f25113ed79ee1834d09b72c951.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Counterspeech-for-Mitigating-the-Influence-of-Media-Bias-Comparing-Human-and-LLM-Generated-Responses"><a href="#Counterspeech-for-Mitigating-the-Influence-of-Media-Bias-Comparing-Human-and-LLM-Generated-Responses" class="headerlink" title="Counterspeech for Mitigating the Influence of Media Bias: Comparing   Human and LLM-Generated Responses"></a>Counterspeech for Mitigating the Influence of Media Bias: Comparing   Human and LLM-Generated Responses</h2><p><strong>Authors:Luyang Lin, Zijin Feng, Lingzhi Wang, Kam-Fai Wong</strong></p>
<p>Biased news contributes to societal polarization and is often reinforced by hostile reader comments, constituting a vital yet often overlooked aspect of news dissemination. Our study reveals that offensive comments support biased content, amplifying bias and causing harm to targeted groups or individuals. Counterspeech is an effective approach to counter such harmful speech without violating freedom of speech, helping to limit the spread of bias. To the best of our knowledge, this is the first study to explore counterspeech generation in the context of news articles. We introduce a manually annotated dataset linking media bias, offensive comments, and counterspeech. We conduct a detailed analysis showing that over 70% offensive comments support biased articles, amplifying bias and thus highlighting the importance of counterspeech generation. Comparing counterspeech generated by humans and large language models, we find model-generated responses are more polite but lack the novelty and diversity. Finally, we improve generated counterspeech through few-shot learning and integration of news background information, enhancing both diversity and relevance. </p>
<blockquote>
<p>有偏见的新闻会导致社会两极化，往往还会得到敌对读者评论的强化，这是新闻传播的一个重要但常被忽视的方面。我们的研究表明，攻击性评论支持有偏见的内容，放大偏见，对目标群体或个人造成伤害。反语是一种在不侵犯言论自由的情况下对抗这种有害言论的有效方法，有助于限制偏见的传播。据我们所知，这是第一项探索新闻文章背景下反语生成的研究。我们引入了一个手动注释的数据集，将媒体偏见、攻击性评论和反语联系起来。我们进行了详细的分析，发现超过70%的攻击性评论支持有偏见的文章，放大偏见，从而凸显反语生成的重要性。通过对比人类和反语生成的大型语言模型生成的回应，我们发现模型生成的回应更加礼貌，但缺乏新颖性和多样性。最后，我们通过小样本学习和新闻背景信息的整合，改进了生成的反语，增强了其多样性和相关性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15855v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了偏见新闻如何导致社会两极化，并指出攻击性读者评论强化了这种偏见。研究发现，攻击性评论支持有偏见的内容，放大了偏见，对目标群体或个人造成伤害。文章提出了用反语作为一种有效的应对方式，这种方式可以在不侵犯言论自由的前提下对抗有害言论。这是首个探索新闻文章中反语生成的研究。研究介绍了一个手动标注的数据集，涉及媒体偏见、攻击性评论和反语。分析显示，超过70%的攻击性评论支持有偏见的文章，凸显了生成反语的重要性。比较了人类和大型语言模型生成的反语，发现模型生成的反语更为礼貌，但缺乏新颖性和多样性。通过少量样本学习和整合新闻背景信息，提高了生成反语的质量和相关性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>偏见新闻加剧社会两极化，受攻击性读者评论强化。</li>
<li>攻击性评论支持偏见内容，放大偏见，对目标造成损害。</li>
<li>反语是一种有效应对有害言论的方式，不侵犯言论自由。</li>
<li>首次探索新闻文章中的反语生成。</li>
<li>研究介绍了一个涉及媒体偏见、攻击性评论和反语的手动标注数据集。</li>
<li>超过70%的攻击性评论支持有偏见的文章，凸显反语生成的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-01947c2a2ec22b59af70075bbc274d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2276f55c7a57ea72b9b80c0daaa3dd90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e3ea9996c299354f2e06783d3e463d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b7138f1e49bf5038a6bf5209dec8eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f33da90a33afc0fedad054b4bfbb8076.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Improving-Speech-Enhancement-with-Multi-Metric-Supervision-from-Learned-Quality-Assessment"><a href="#Improving-Speech-Enhancement-with-Multi-Metric-Supervision-from-Learned-Quality-Assessment" class="headerlink" title="Improving Speech Enhancement with Multi-Metric Supervision from Learned   Quality Assessment"></a>Improving Speech Enhancement with Multi-Metric Supervision from Learned   Quality Assessment</h2><p><strong>Authors:Wei Wang, Wangyou Zhang, Chenda Li, Jiatong Shi, Shinji Watanabe, Yanmin Qian</strong></p>
<p>Speech quality assessment (SQA) aims to predict the perceived quality of speech signals under a wide range of distortions. It is inherently connected to speech enhancement (SE), which seeks to improve speech quality by removing unwanted signal components. While SQA models are widely used to evaluate SE performance, their potential to guide SE training remains underexplored. In this work, we investigate a training framework that leverages a SQA model, trained to predict multiple evaluation metrics from a public SE leaderboard, as a supervisory signal for SE. This approach addresses a key limitation of conventional SE objectives, such as SI-SNR, which often fail to align with perceptual quality and generalize poorly across evaluation metrics. Moreover, it enables training on real-world data where clean references are unavailable. Experiments on both simulated and real-world test sets show that SQA-guided training consistently improves performance across a range of quality metrics. Code and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/urgent-challenge/urgent2026_challenge_track2">https://github.com/urgent-challenge/urgent2026_challenge_track2</a> </p>
<blockquote>
<p>语音质量评估（SQA）旨在预测大范围失真情况下的语音信号感知质量。它与语音增强（SE）紧密相连，旨在通过消除不需要的信号成分来提高语音质量。虽然SQA模型广泛用于评估SE性能，但其指导SE训练方面的潜力尚未得到充分探索。在这项工作中，我们研究了一个训练框架，该框架利用SQA模型，通过训练以预测来自公共SE排行榜的多个评价指标，作为SE的监督信号。这种方法解决了传统SE目标的一个关键局限性，如SI-SNR常常无法与感知质量保持一致，并且在评价指标上的泛化能力较差。此外，它可以在没有干净参考的情况下进行真实世界数据的训练。在模拟和真实世界测试集上的实验表明，SQA引导的训练在各种质量指标上的表现持续得到改善。代码和检查点可通过<a target="_blank" rel="noopener" href="https://github.com/urgent-challenge/urgent2026_challenge_track2%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/urgent-challenge/urgent2026_challenge_track2获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12260v2">PDF</a> Accepted by ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音质量评估（SQA）在预测各种失真情况下的语音感知质量方面的应用。文章还探讨了将SQA模型作为监督信号应用于语音增强（SE）训练的可能性。该模型经过训练，能够预测公开SE排行榜中的多个评估指标，解决了传统SE目标（如SI-SNR）与感知质量不一致以及在跨评估指标上表现不佳的问题。此外，该模型能够在无清洁参考数据的情况下进行现实数据的训练。实验表明，在模拟和真实测试集上，SQA引导的训练在多种质量指标上的表现都有所提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音质量评估（SQA）旨在预测各种失真情况下的语音感知质量。</li>
<li>SQA模型作为监督信号应用于语音增强（SE）训练是可能的。</li>
<li>SQA模型能够预测公开SE排行榜中的多个评估指标，解决了传统SE目标的问题。</li>
<li>传统语音增强目标（如SI-SNR）存在与感知质量不一致的问题。</li>
<li>SQA引导的训练在多种质量指标上的表现均有所提高。</li>
<li>该模型能够在无清洁参考数据的情况下进行现实数据的训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12260">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2b5ab1d2ed593e5730d5a8b7c19f3e07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a59a94d3fafb020072f23d94b7a876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57ba793e7eed46aecde7b67568a55391.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e32842c9b3f333f84f8ba7bdc9e69f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e3aab1e1554c659829944c7b9a26e9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb88690a71df9ec05a4a8beb920aa72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56465d18b6a32c49aa8283475c54d2bd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Code-switched-Text-to-Speech-Synthesis-Capability-in-Large-Language-Models-with-only-Monolingual-Corpora"><a href="#Enhancing-Code-switched-Text-to-Speech-Synthesis-Capability-in-Large-Language-Models-with-only-Monolingual-Corpora" class="headerlink" title="Enhancing Code-switched Text-to-Speech Synthesis Capability in Large   Language Models with only Monolingual Corpora"></a>Enhancing Code-switched Text-to-Speech Synthesis Capability in Large   Language Models with only Monolingual Corpora</h2><p><strong>Authors:Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen</strong></p>
<p>While Large Language Models (LLMs) have shown potential in speech generation and recognition, their applications are mainly confined to monolingual scenarios, with limited explorations in code-switched (CS) contexts. In this paper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the code-switched text-to-speech synthesis (CS TTS) capability in LLMs with only monolingual corpora. Specifically, we begin by enhancing the multilingual speech processing ability of LLMs through multilingual speech recognition and synthesis tasks. Then, we develop an effective code-switched (CS) data construction strategy that splits and concatenates words from different monolingual speech corpora to equip LLMs with improved CS TTS ability. Experiments show that our approach outperforms baselines in CS TTS in terms of naturalness, speaker consistency and similarity even with limited data. Additionally, the constructed CS data further improves multilingual speech synthesis and recognition. </p>
<blockquote>
<p>虽然大型语言模型（LLM）在语音生成和识别方面显示出潜力，但它们的应用主要局限于单语场景，对混合语言（CS）环境的探索有限。在本文中，我们提出了一种混合语言大型语言模型（CS-LLM），旨在利用单语语料库增强大型语言模型在混合文本到语音合成（CS TTS）方面的能力。具体来说，我们首先通过多语言语音识别和合成任务增强LLM的多语言语音处理能力。然后，我们开发了一种有效的混合（CS）数据构建策略，该策略可以拆分和合并来自不同单语语音语料库的单词，以赋予LLM改进的CSTTS能力。实验表明，即使在有限数据的情况下，我们的方法在CSTTS方面的表现也优于基准线，体现在自然度、说话人一致性和相似性等方面。此外，构建的CS数据进一步提高了多语言语音合成和识别。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10969v2">PDF</a> Accepted to ASRU2025</p>
<p><strong>Summary</strong><br>多语言模型在语音生成和识别方面显示出潜力，但其应用主要集中在单语场景上，对混合语言环境下的应用探索有限。本文提出了一种混合大型语言模型（CS-LLM），旨在通过仅使用单语语料库提高大型语言模型在混合文本到语音合成（CS TTS）中的能力。具体来说，我们首先通过多语言语音识别与合成任务来增强大型语言模型的多语言语音处理能力。然后，我们开发了一种有效的混合（CS）数据构建策略，该策略可以拆分和合并来自不同单语语音语料库的单词，以赋予大型语言模型改进后的CSTTS能力。实验表明，即使在有限的数据下，我们的方法在自然度、说话人一致性和相似性方面也比基线在CSTTS中的表现要好。此外，构建的CS数据进一步提高了多语言语音合成和识别的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在语音生成和识别方面具有潜力，但其在混合语言环境下的应用仍然有限。</li>
<li>论文提出了一种混合大型语言模型（CS-LLM），旨在提高大型语言模型在混合文本到语音合成（CS TTS）中的性能。</li>
<li>通过多语言语音识别与合成任务增强大型语言模型的多语言语音处理能力。</li>
<li>开发了一种有效的混合数据构建策略，通过拆分和合并不同单语语音语料库的单词，提高模型的CSTTS能力。</li>
<li>实验结果显示，CS-LLM在自然度、说话人一致性和相似性方面优于基线方法。</li>
<li>构建的CS数据不仅提高了CSTTS性能，还进一步提高了多语言语音合成和识别的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a908af713a34505116462c93f786110f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a5c412e4336f3fa4d45d437ada4a79e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b106dc05ae9dceb6cfb947888e59934.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e33bdada5d5730c3a2fe8cedf0450ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b36921ecf2c14d02cce26561494ccdff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-883581d812ed7a1f0a5ed13a0c6ecb2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5497c144b6196c388a3aad3c3213c99c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-601af261525aca80cad651a13b503b9c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sentiment-Reasoning-for-Healthcare"><a href="#Sentiment-Reasoning-for-Healthcare" class="headerlink" title="Sentiment Reasoning for Healthcare"></a>Sentiment Reasoning for Healthcare</h2><p><strong>Authors:Khai-Nguyen Nguyen, Khai Le-Duc, Bach Phan Tat, Duy Le, Long Vo-Dang, Truong-Son Hy</strong></p>
<p>Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)’s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the world’s largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving model’s classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/Sentiment-Reasoning">https://github.com/leduckhai/Sentiment-Reasoning</a> </p>
<blockquote>
<p>人工智能医疗决策中的透明度至关重要。通过将每个预测标签的理由融入其中，用户可以理解大型语言模型（LLM）的推理，以做出更好的决策。在这项工作中，我们为语音和文本模态引入了一项新任务——情感推理，以及我们提出的多模态多任务框架和世界上最大的多模态情感分析数据集。情感推理是情感分析中的辅助任务，模型根据输入内容预测情感标签并生成其背后的理由。我们的研究对人类转录和自动语音识别（ASR）转录的文本进行了实验，结果表明，情感推理通过为模型预测提供理由，提高了模型的透明度，其语义质量与人类相当，同时还通过增加理由的微调提高了模型的分类性能（+2%准确度和宏观F1分数提高）。此外，人类和ASR转录文本在生成理由的语义质量上没有显著差异。所有代码、数据（五种语言：越南语、英语、中文、德语和法语）和模型均在线发布：<a target="_blank" rel="noopener" href="https://github.com/leduckhai/Sentiment-Reasoning">https://github.com/leduckhai/Sentiment-Reasoning</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21054v5">PDF</a> ACL 2025 Industry Track (Oral)</p>
<p><strong>Summary</strong></p>
<p>本文强调人工智能健康决策中的透明度至关重要。通过融入解释预测标签的理由，用户能更好地理解大型语言模型（LLMs）的推理过程以做出更好的决策。本文介绍了一项新的任务——情感推理，涉及语音和文本两种模式，并提出了一个多模态多任务框架和全球最大的多模态情感分析数据集。情感推理是情感分析中的辅助任务，模型根据输入内容预测情感标签并生成背后的理由。研究表明，情感推理有助于提高模型的透明度，并为模型预测提供理由，其语义质量与人类相似。此外，通过基于理由的微调，情感推理还可提高模型的分类性能（+2%的提升）。人类和自动语音识别（ASR）转录生成的文本在语义质量上没有显著差异。所有代码、数据和模型均已在线发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能健康决策需要更高的透明度。</li>
<li>通过融入预测标签的理由，用户能更好地理解大型语言模型的推理过程。</li>
<li>介绍了一项新的任务——情感推理，适用于语音和文本两种模式。</li>
<li>提出了多模态多任务框架和全球最大多模态情感分析数据集。</li>
<li>情感推理有助于提升模型的透明度并提高其分类性能。</li>
<li>对比研究表明，情感推理模型在语义质量上与人类表现相似。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21054">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f83bf1431feae2a3829552a80dbac05e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0fab05b621b1f919e40cf07f0d4234d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddd2bffaeb4740293e5aa80114720b9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ebd727f82ac483c20f0551d024ceeb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01b0386a9416cd6cd89c64068b387e01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-007689b88c689a0dd9268ea12d90e5b8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f2d5033a863484e9e869976d32fa7d40.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-26  Two-flow Feedback Multi-scale Progressive Generative Adversarial Network
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0eedf4321ffa9fd69ea7a963635fd932.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-08-26  SAMFusion Sensor-Adaptive Multimodal Fusion for 3D Object Detection in   Adverse Weather
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
