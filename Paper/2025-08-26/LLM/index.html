<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-26  LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-01cfd708b7f9a059552248c794d5f179.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-28
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-26-æ›´æ–°"><a href="#2025-08-26-æ›´æ–°" class="headerlink" title="2025-08-26 æ›´æ–°"></a>2025-08-26 æ›´æ–°</h1><h2 id="LLM-Based-Agents-for-Competitive-Landscape-Mapping-in-Drug-Asset-Due-Diligence"><a href="#LLM-Based-Agents-for-Competitive-Landscape-Mapping-in-Drug-Asset-Due-Diligence" class="headerlink" title="LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence"></a>LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence</h2><p><strong>Authors:Alisa Vinogradova, Vlad Vinogradov, Dmitrii Radkevich, Ilya Yasny, Dmitry Kobyzev, Ivan Izmailov, Katsiaryna Yanchanka, Andrey Doronichev</strong></p>
<p>In this paper, we describe and benchmark a competitor-discovery component used within an agentic AI system for fast drug asset due diligence. A competitor-discovery AI agent, given an indication, retrieves all drugs comprising the competitive landscape of that indication and extracts canonical attributes for these drugs. The competitor definition is investor-specific, and data is paywalled&#x2F;licensed, fragmented across registries, ontology-mismatched by indication, alias-heavy for drug names, multimodal, and rapidly changing. Although considered the best tool for this problem, the current LLM-based AI systems arenâ€™t capable of reliably retrieving all competing drug names, and there is no accepted public benchmark for this task. To address the lack of evaluation, we use LLM-based agents to transform five years of multi-modal, unstructured diligence memos from a private biotech VC fund into a structured evaluation corpus mapping indications to competitor drugs with normalized attributes. We also introduce a competitor validating LLM-as-a-judge agent that filters out false positives from the list of predicted competitors to maximize precision and suppress hallucinations. On this benchmark, our competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research (65%) and Perplexity Labs (60%). The system is deployed in production with enterprise users; in a case study with a biotech VC investment fund, analyst turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the competitive analysis. </p>
<blockquote>
<p>æœ¬æ–‡æè¿°å¹¶è¯„ä¼°äº†ä¸€ä¸ªç”¨äºå¿«é€Ÿå°½èŒè°ƒæŸ¥è¯ç‰©èµ„äº§çš„æ™ºèƒ½AIç³»ç»Ÿä¸­çš„ç«äº‰å¯¹æ‰‹å‘ç°ç»„ä»¶ã€‚ç»™å®šä¸€ä¸ªæŒ‡æ ‡ï¼Œç«äº‰å¯¹æ‰‹å‘ç°AIä»£ç†ä¼šæ£€ç´¢æ„æˆè¯¥æŒ‡æ ‡ç«äº‰æ€åŠ¿çš„æ‰€æœ‰è¯ç‰©ï¼Œå¹¶æå–è¿™äº›è¯ç‰©çš„è§„èŒƒå±æ€§ã€‚ç«äº‰å¯¹æ‰‹çš„å®šä¹‰æ˜¯æŠ•èµ„è€…ç‰¹å®šçš„ï¼Œå¹¶ä¸”æ•°æ®æ˜¯ä»˜è´¹çš„&#x2F;è®¸å¯çš„ï¼Œè·¨æ³¨å†Œæœºæ„åˆ†æ•£ï¼ŒæŒ‰æŒ‡ç¤ºå­˜åœ¨æœ¬ä½“ä¸åŒ¹é…ï¼Œè¯ç‰©åç§°åˆ«åç¹å¤šï¼Œå¤šæ¨¡å¼ä¸”å¿«é€Ÿå˜åŒ–ã€‚å°½ç®¡è¢«è®¤ä¸ºæ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„æœ€ä½³å·¥å…·ï¼Œä½†å½“å‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„AIç³»ç»Ÿæ— æ³•å¯é åœ°æ£€ç´¢æ‰€æœ‰ç«äº‰è¯ç‰©åç§°ï¼Œå¹¶ä¸”æ²¡æœ‰é’ˆå¯¹æ­¤ä»»åŠ¡çš„å…¬è®¤å…¬å…±åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³ç¼ºä¹è¯„ä¼°çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºLLMçš„ä»£ç†å°†ä¸€å®¶ç§äººç”Ÿç‰©æŠ€æœ¯VCåŸºé‡‘çš„äº”å¹´å¤šæ¨¡å¼éç»“æ„åŒ–å°½èŒè°ƒæŸ¥å¤‡å¿˜å½•è½¬åŒ–ä¸ºç»“æ„åŒ–è¯„ä¼°è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“å°†æŒ‡æ ‡æ˜ å°„åˆ°å…·æœ‰æ ‡å‡†åŒ–å±æ€§çš„ç«äº‰å¯¹æ‰‹è¯ç‰©ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç«äº‰å¯¹æ‰‹éªŒè¯LLMä½œä¸ºæ³•å®˜ä»£ç†ï¼Œä»é¢„æµ‹çš„ç«äº‰å¯¹æ‰‹åå•ä¸­ç­›é€‰å‡ºå‡é˜³æ€§ç»“æœï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜ç²¾åº¦å¹¶æŠ‘åˆ¶å¹»è§‰ã€‚åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ç«äº‰å¯¹æ‰‹å‘ç°ä»£ç†å®ç°äº†83%çš„å¬å›ç‡ï¼Œè¶…è¿‡äº†OpenAIæ·±åº¦ç ”ç©¶ï¼ˆ65%ï¼‰å’Œå›°æƒ‘å®éªŒå®¤ï¼ˆ60%ï¼‰ã€‚è¯¥ç³»ç»Ÿå·²åœ¨ä¼ä¸šç”¨æˆ·ä¸­éƒ¨ç½²ç”Ÿäº§ç¯å¢ƒï¼›åœ¨ä¸ç”Ÿç‰©æŠ€æœ¯VCæŠ•èµ„åŸºé‡‘çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œåˆ†æå¸ˆå‘¨è½¬æ—¶é—´ä»2.5å¤©ç¼©çŸ­åˆ°çº¦3å°æ—¶ï¼ˆçº¦20å€ï¼‰ç”¨äºç«äº‰åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16571v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æè¿°å¹¶è¯„ä¼°äº†ä¸€ä¸ªç”¨äºå¿«é€Ÿè¯ç‰©èµ„äº§å°½èŒè°ƒæŸ¥çš„ç«äº‰å¯¹æ‰‹å‘ç°ç»„ä»¶ã€‚è¯¥ç»„ä»¶åˆ©ç”¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸ºåŸºç¡€çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œæ ¹æ®ç‰¹å®šæŒ‡æ ‡æ£€ç´¢æ‰€æœ‰ç«äº‰å¯¹æ‰‹è¯ç‰©å¹¶æå–å…¶è§„èŒƒå±æ€§ã€‚å°½ç®¡æ˜¯ç›®å‰è§£å†³æ­¤é—®é¢˜çš„æœ€ä½³å·¥å…·ï¼Œä½†ç°æœ‰LLMç³»ç»Ÿæ— æ³•å¯é åœ°æ£€ç´¢æ‰€æœ‰ç«äº‰å¯¹æ‰‹è¯ç‰©åç§°ï¼Œä¸”ç¼ºä¹å…¬è®¤çš„å…¬å…±åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨LLMä»£ç†å°†äº”å¹´çš„å¤šæ¨¡å¼ã€éç»“æ„åŒ–å°½èŒè°ƒæŸ¥å¤‡å¿˜å½•è½¬åŒ–ä¸ºç»“æ„åŒ–è¯„ä¼°è¯­æ–™åº“ï¼Œå¹¶å¼•å…¥ç«äº‰å¯¹æ‰‹éªŒè¯ä»£ç†ä»¥è¿‡æ»¤å‡ºé¢„æµ‹ä¸­çš„å‡é˜³æ€§ç»“æœï¼Œæé«˜ç²¾åº¦å¹¶æŠ‘åˆ¶è™šæ„ç»“æœã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œç«äº‰å¯¹æ‰‹å‘ç°ä»£ç†çš„å¬å›ç‡è¾¾åˆ°83%ï¼Œè¶…è¿‡äº†OpenAI Deep Researchï¼ˆ65%ï¼‰å’ŒPerplexity Labsï¼ˆ60%ï¼‰ã€‚è¯¥ç³»ç»Ÿå·²åœ¨ä¼ä¸šç”¨æˆ·ä¸­éƒ¨ç½²ç”Ÿäº§ç¯å¢ƒï¼Œå¹¶åœ¨ä¸ç”Ÿç‰©æŠ€æœ¯é£é™©æŠ•èµ„åŸºé‡‘è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶æ—¶ï¼Œåˆ†æå¸ˆçš„å‘¨è½¬æ—¶é—´ä»2.5å¤©ç¼©çŸ­åˆ°çº¦3å°æ—¶ï¼ˆçº¦å‡å°‘20å€ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç«äº‰å¯¹æ‰‹å‘ç°ç»„ä»¶è¢«ç”¨äºå¿«é€Ÿè¯ç‰©èµ„äº§å°½èŒè°ƒæŸ¥ã€‚</li>
<li>LLMä¸ºåŸºç¡€çš„äººå·¥æ™ºèƒ½ä»£ç†ç”¨äºæ£€ç´¢ç«äº‰å¯¹æ‰‹è¯ç‰©å¹¶æå–å…¶å±æ€§ã€‚</li>
<li>å½“å‰LLMç³»ç»Ÿåœ¨æ£€ç´¢æ‰€æœ‰ç«äº‰å¯¹æ‰‹è¯ç‰©åç§°æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨LLMä»£ç†åˆ›å»ºäº†ä¸€ä¸ªç»“æ„åŒ–è¯„ä¼°è¯­æ–™åº“ä»¥è¯„ä¼°ç«äº‰å¯¹æ‰‹å‘ç°æ€§èƒ½ã€‚</li>
<li>å¼•å…¥ç«äº‰å¯¹æ‰‹éªŒè¯ä»£ç†ä»¥æé«˜ç²¾åº¦å¹¶æŠ‘åˆ¶è™šæ„ç»“æœã€‚</li>
<li>ç«äº‰å¯¹æ‰‹å‘ç°ä»£ç†çš„å¬å›ç‡è¾¾åˆ°83%ï¼Œä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c760b9430f62db57178f4d0578b14d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-769a696ff9464dc2d7b02bfeac288182.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16590369d3ee194825e7b2341c441d28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e014317ae5457c888c609b81f73ad5f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f048b19692b265b58a25ec9835386f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Open-World-Detection-A-Survey"><a href="#Towards-Open-World-Detection-A-Survey" class="headerlink" title="Towards Open World Detection: A Survey"></a>Towards Open World Detection: A Survey</h2><p><strong>Authors:Andrei-Stefan Bulzan, Cosmin Cernazanu-Glavan</strong></p>
<p>For decades, Computer Vision has aimed at enabling machines to perceive the external world. Initial limitations led to the development of highly specialized niches. As success in each task accrued and research progressed, increasingly complex perception tasks emerged. This survey charts the convergence of these tasks and, in doing so, introduces Open World Detection (OWD), an umbrella term we propose to unify class-agnostic and generally applicable detection models in the vision domain. We start from the history of foundational vision subdomains and cover key concepts, methodologies and datasets making up todayâ€™s state-of-the-art landscape. This traverses topics starting from early saliency detection, foreground&#x2F;background separation, out of distribution detection and leading up to open world object detection, zero-shot detection and Vision Large Language Models (VLLMs). We explore the overlap between these subdomains, their increasing convergence, and their potential to unify into a singular domain in the future, perception. </p>
<blockquote>
<p>æ•°åå¹´æ¥ï¼Œè®¡ç®—æœºè§†è§‰çš„ç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿæ„ŸçŸ¥å¤–éƒ¨ä¸–ç•Œã€‚æœ€åˆçš„å±€é™æ€§å¯¼è‡´äº†é«˜åº¦ä¸“ä¸šåŒ–çš„ç»†åˆ†é¢†åŸŸçš„å¼€å‘ã€‚éšç€æ¯ä¸ªä»»åŠ¡çš„æˆåŠŸç´¯ç§¯å’Œç ”ç©¶è¿›å±•ï¼Œå‡ºç°äº†è¶Šæ¥è¶Šå¤æ‚çš„æ„ŸçŸ¥ä»»åŠ¡ã€‚è¿™ç¯‡ç»¼è¿°æè¿°äº†è¿™äº›ä»»åŠ¡çš„èåˆï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­å¼•å…¥äº†å¼€æ”¾ä¸–ç•Œæ£€æµ‹ï¼ˆOWDï¼‰ï¼Œè¿™æ˜¯æˆ‘ä»¬æå‡ºçš„ä¸€ä¸ªæ€»ç§°ï¼Œæ—¨åœ¨ç»Ÿä¸€è§†è§‰é¢†åŸŸä¸­çš„ç±»æ— å…³å’Œæ™®éé€‚ç”¨çš„æ£€æµ‹æ¨¡å‹ã€‚æˆ‘ä»¬ä»åŸºç¡€è§†è§‰å­åŸŸçš„å†å²å¼€å§‹ï¼Œæ¶µç›–äº†æ„æˆå½“ä»Šæœ€æ–°æŠ€æœ¯æ™¯è§‚çš„å…³é”®æ¦‚å¿µã€æ–¹æ³•å’Œæ•°æ®é›†ã€‚è¿™æ¶µç›–äº†ä»æ—©æœŸçš„æ˜¾è‘—æ€§æ£€æµ‹ã€å‰æ™¯&#x2F;èƒŒæ™¯åˆ†ç¦»ã€ç¦»ç¾¤æ£€æµ‹ï¼Œä¸€ç›´åˆ°å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ã€é›¶æ ·æœ¬æ£€æµ‹å’Œè§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰ç­‰ä¸»é¢˜ã€‚æˆ‘ä»¬æ¢ç´¢äº†è¿™äº›å­åŸŸä¹‹é—´çš„é‡å ï¼Œå®ƒä»¬æ—¥ç›Šèåˆä»¥åŠåœ¨å°†æ¥å¯èƒ½ç»Ÿä¸€ä¸ºä¸€ä¸ªå•ç‹¬çš„æ„ŸçŸ¥é¢†åŸŸçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16527v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong>ï¼š<br>æ•°åå¹´ä»¥æ¥ï¼Œè®¡ç®—æœºè§†è§‰æ—¨åœ¨è®©æœºå™¨æ„ŸçŸ¥å¤–éƒ¨ä¸–ç•Œã€‚éšç€ç ”ç©¶ä»»åŠ¡çš„é€æ¸æˆåŠŸå’Œç ”ç©¶è¿›å±•çš„ä¸æ–­æ·±åŒ–ï¼Œå‡ºç°äº†è¶Šæ¥è¶Šå¤æ‚çš„æ„ŸçŸ¥ä»»åŠ¡ã€‚æœ¬æ–‡æ¦‚è¿°äº†è¿™äº›ä»»åŠ¡çš„èåˆï¼Œå¹¶ä»‹ç»äº†æˆ‘ä»¬æå‡ºçš„å¼€æ”¾ä¸–ç•Œæ£€æµ‹ï¼ˆOWDï¼‰è¿™ä¸€é€šç”¨æœ¯è¯­ï¼Œä»¥ç»Ÿä¸€è§†è§‰é¢†åŸŸä¸­çš„ç±»æ— å…³å’Œæ™®éé€‚ç”¨çš„æ£€æµ‹æ¨¡å‹ã€‚æœ¬æ–‡å›é¡¾äº†åŸºç¡€è§†è§‰å­åŸŸçš„å†å²ï¼Œæ¶µç›–äº†æ„æˆå½“ä»Šæœ€æ–°æŠ€æœ¯æ™¯è§‚çš„å…³é”®æ¦‚å¿µã€æ–¹æ³•å’Œæ•°æ®é›†ã€‚æ¶‰åŠä»æ—©æœŸçš„æ˜¾è‘—æ€§æ£€æµ‹ã€å‰æ™¯&#x2F;èƒŒæ™¯åˆ†ç¦»ã€ç¦»ç¾¤åˆ†å¸ƒæ£€æµ‹ï¼Œåˆ°å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ã€é›¶å°„å‡»æ£€æµ‹å’Œè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰ç­‰è¯é¢˜ã€‚æœ¬æ–‡æ¢è®¨äº†è¿™äº›å­åŸŸä¹‹é—´çš„é‡å æ€§ã€å…¶æ—¥ç›Šèåˆçš„è¶‹åŠ¿ä»¥åŠå°†æ¥ç»Ÿä¸€ä¸ºå•ä¸€æ„ŸçŸ¥åŸŸçš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®¡ç®—æœºè§†è§‰æ—¨åœ¨è®©æœºå™¨æ„ŸçŸ¥å¤–éƒ¨ä¸–ç•Œï¼Œç»å†äº†ä»é«˜åº¦ä¸“ä¸šåŒ–çš„é¢†åŸŸåˆ°è¶Šæ¥è¶Šå¤æ‚çš„æ„ŸçŸ¥ä»»åŠ¡çš„è½¬å˜ã€‚</li>
<li>å¼€æ”¾ä¸–ç•Œæ£€æµ‹ï¼ˆOWDï¼‰æ˜¯ä¸€ä¸ªæ–°æå‡ºçš„é€šç”¨æœ¯è¯­ï¼Œç”¨äºç»Ÿä¸€è§†è§‰é¢†åŸŸä¸­çš„ç±»æ— å…³å’Œæ™®éé€‚ç”¨çš„æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>æ–‡ç« å›é¡¾äº†è®¡ç®—æœºè§†è§‰çš„åŸºç¡€å­åŸŸå†å²ï¼ŒåŒ…æ‹¬æ˜¾è‘—æ€§æ£€æµ‹ã€å‰æ™¯&#x2F;èƒŒæ™¯åˆ†ç¦»ç­‰æ—©æœŸè¯é¢˜ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†å½“ä»Šæœ€æ–°çš„æŠ€æœ¯å’Œæ•°æ®é›†ï¼Œè¿™äº›æŠ€æœ¯å’Œæ•°æ®é›†æ„æˆäº†å½“å‰çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯æ™¯è§‚ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰çš„ä¸åŒå­åŸŸä¹‹é—´å­˜åœ¨é‡å ï¼Œä¸”è¿™äº›å­åŸŸæ­£å˜å¾—è¶Šæ¥è¶Šèåˆã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†è¿™äº›å­åŸŸæœªæ¥çš„å‘å±•è¶‹åŠ¿ï¼ŒåŒ…æ‹¬å®ƒä»¬å¯èƒ½ç»Ÿä¸€ä¸ºä¸€ä¸ªå•ä¸€çš„æ„ŸçŸ¥åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02097bb5805b93a09f0ac08c31809ead.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9589261dbf7a8d64a6bbcf64829f1486.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669c78951919e460fdec057e45db9d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-605d04b595b8712dc6f85bbfb47ad21c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0463c5840d0087bf475593f5a0723f8a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ARSP-Automated-Repair-of-Verilog-Designs-via-Semantic-Partitioning"><a href="#ARSP-Automated-Repair-of-Verilog-Designs-via-Semantic-Partitioning" class="headerlink" title="ARSP: Automated Repair of Verilog Designs via Semantic Partitioning"></a>ARSP: Automated Repair of Verilog Designs via Semantic Partitioning</h2><p><strong>Authors:Bingkun Yao, Ning Wang, Xiangfeng Liu, Yuxin Du, Yuchen Hu, Hong Gao, Zhe Jiang, Nan Guan</strong></p>
<p>Debugging functional Verilog bugs consumes a significant portion of front-end design time. While Large Language Models (LLMs) have demonstrated great potential in mitigating this effort, existing LLM-based automated debugging methods underperform on industrial-scale modules. A major reason for this is bug signal dilution in long contexts, where a few bug-relevant tokens are overwhelmed by hundreds of unrelated lines, diffusing the modelâ€™s attention. To address this issue, we introduce ARSP, a two-stage system that mitigates dilution via semantics-guided fragmentation. A Partition LLM splits a module into semantically tight fragments; a Repair LLM patches each fragment; edits are merged without altering unrelated logic. A synthetic data framework generates fragment-level training pairs spanning bug types, design styles, and scales to supervise both models. Experiments show that ARSP achieves 77.92% pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also, semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over whole-module debugging, validating the effectiveness of fragment-level scope reduction in LLM-based Verilog debugging. </p>
<blockquote>
<p>è°ƒè¯•åŠŸèƒ½Verilogé”™è¯¯ä¼šæ¶ˆè€—å‰ç«¯è®¾è®¡çš„å¤§é‡æ—¶é—´ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¼“è§£è¿™ä¸€åŠªåŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åŸºäºLLMçš„ç°æœ‰è‡ªåŠ¨åŒ–è°ƒè¯•æ–¹æ³•åœ¨å·¥ä¸šè§„æ¨¡æ¨¡å—ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸»è¦åŸå› æ˜¯é•¿ä¸Šä¸‹æ–‡ä¸­çš„é”™è¯¯ä¿¡å·ç¨€é‡Šï¼Œå…¶ä¸­å‡ ä¸ªä¸é”™è¯¯ç›¸å…³çš„ä»¤ç‰Œè¢«æ•°ç™¾è¡Œæ— å…³ä»£ç æ·¹æ²¡ï¼Œåˆ†æ•£äº†æ¨¡å‹çš„æ³¨æ„åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARSPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç³»ç»Ÿï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼çš„ç¢ç‰‡åŒ–æ¥ç¼“è§£ç¨€é‡Šé—®é¢˜ã€‚ä¸€ä¸ªåˆ†åŒºLLMå°†æ¨¡å—åˆ†å‰²æˆè¯­ä¹‰ç´§å¯†çš„ç‰‡æ®µï¼›ä¸€ä¸ªä¿®å¤LLMå¯¹æ¯ä¸ªç‰‡æ®µè¿›è¡Œä¿®å¤ï¼›ç¼–è¾‘åœ¨ä¸æ”¹å˜æ— å…³é€»è¾‘çš„æƒ…å†µä¸‹åˆå¹¶ã€‚åˆæˆæ•°æ®æ¡†æ¶ç”Ÿæˆæ¶µç›–é”™è¯¯ç±»å‹ã€è®¾è®¡é£æ ¼å’Œè§„æ¨¡çš„ç‰‡æ®µçº§è®­ç»ƒå¯¹ï¼Œä»¥ç›‘ç£ä¸¤ä¸ªæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒARSPè¾¾åˆ°äº†77.92%çš„pass@1å’Œ83.88%çš„pass@5ï¼Œä¼˜äºä¸»æµå•†ä¸šLLMï¼ˆåŒ…æ‹¬Claude-3.7ï¼‰ä»¥åŠæœ€å…ˆè¿›çš„Verilogè‡ªåŠ¨åŒ–è°ƒè¯•å·¥å…·Striderå’ŒMEICã€‚æ­¤å¤–ï¼Œè¯­ä¹‰åˆ†åŒºåœ¨pass@1ä¸Šæé«˜äº†11.6%ï¼Œåœ¨pass@5ä¸Šæé«˜äº†10.2%ï¼Œä¼˜äºå…¨æ¨¡å—è°ƒè¯•ï¼ŒéªŒè¯äº†ç‰‡æ®µçº§èŒƒå›´ç¼©å‡åœ¨LLMåŸºäºVerilogè°ƒè¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16517v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ARSPæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µç³»ç»Ÿï¼Œé€šè¿‡è¯­ä¹‰å¼•å¯¼åˆ†æ®µç¼“è§£è°ƒè¯•ä¿¡æ¯ç¨€é‡Šé—®é¢˜ï¼Œæé«˜LLMåœ¨Verilogè‡ªåŠ¨åŒ–è°ƒè¯•ä¸­çš„æ€§èƒ½ã€‚ç³»ç»ŸåŒ…æ‹¬åˆ†å‰²LLMå’Œä¿®å¤LLMï¼Œå¯é’ˆå¯¹æ¨¡å—è¿›è¡Œè¯­ä¹‰ç´§å¯†åˆ†æ®µï¼Œå¹¶å¯¹æ¯æ®µè¿›è¡Œä¿®å¤ã€‚åˆæˆæ•°æ®æ¡†æ¶ç”Ÿæˆç‰‡æ®µçº§è®­ç»ƒå¯¹ï¼Œæ¶µç›–é”™è¯¯ç±»å‹ã€è®¾è®¡é£æ ¼å’Œè§„æ¨¡ï¼Œä»¥ç›‘ç£ä¸¤ä¸ªæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒARSPåœ¨Verilogè‡ªåŠ¨åŒ–è°ƒè¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºä¸»æµå•†ä¸šLLMå’Œæœ€æ–°è‡ªåŠ¨åŒ–Verilogè°ƒè¯•å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARSPæ˜¯ä¸€ä¸ªé’ˆå¯¹Verilogè‡ªåŠ¨åŒ–è°ƒè¯•çš„ä¸¤é˜¶æ®µç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è°ƒè¯•å·¥ä¸šè§„æ¨¡æ¨¡å—æ—¶çš„æ€§èƒ½ä¸è¶³ã€‚</li>
<li>ARSPé€šè¿‡è¯­ä¹‰å¼•å¯¼åˆ†æ®µï¼ˆsemantic-guided fragmentationï¼‰æ¥ç¼“è§£è°ƒè¯•ä¿¡æ¯ç¨€é‡Šé—®é¢˜ã€‚</li>
<li>ARSPåŒ…æ‹¬åˆ†å‰²LLMå’Œä¿®å¤LLMï¼Œåˆ†åˆ«å¯¹æ¨¡å—è¿›è¡Œè¯­ä¹‰ç´§å¯†åˆ†æ®µå¹¶ä¿®å¤æ¯æ®µã€‚</li>
<li>åˆæˆæ•°æ®æ¡†æ¶ç”¨äºç”Ÿæˆæ¶µç›–å„ç§é”™è¯¯ç±»å‹ã€è®¾è®¡é£æ ¼å’Œè§„æ¨¡çš„ç‰‡æ®µçº§è®­ç»ƒå¯¹ï¼Œä»¥ç›‘ç£LLMã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒARSPåœ¨Verilogè‡ªåŠ¨åŒ–è°ƒè¯•æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¼˜äºä¸»æµå•†ä¸šLLMä»¥åŠStriderå’ŒMEICç­‰æœ€æ–°è‡ªåŠ¨åŒ–Verilogè°ƒè¯•å·¥å…·ã€‚</li>
<li>è¯­ä¹‰åˆ†æ®µæ˜¾è‘—æé«˜äº†ARSPçš„è°ƒè¯•æ€§èƒ½ï¼ŒéªŒè¯äº†ç‰‡æ®µçº§åˆ«èŒƒå›´ç¼©å‡åœ¨LLMåŸºäºVerilogè°ƒè¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16517">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8fe6e897b7a57982d730d055464898d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68901df9dcd500288f13fa0d7f7c8214.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-632caaa6124bc0faf6f8b66a641ad544.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e6a5d4ca5fa9598013d3069d9a323af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d809f729ea0ba61f98cdd058e2522962.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e809067e357633d56d3e6b82dd18a3d5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FLAMES-Improving-LLM-Math-Reasoning-via-a-Fine-Grained-Analysis-of-the-Data-Synthesis-Pipeline"><a href="#FLAMES-Improving-LLM-Math-Reasoning-via-a-Fine-Grained-Analysis-of-the-Data-Synthesis-Pipeline" class="headerlink" title="FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the   Data Synthesis Pipeline"></a>FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the   Data Synthesis Pipeline</h2><p><strong>Authors:Parker Seegmiller, Kartik Mehta, Soumya Saha, Chenyang Tao, Shereen Oraby, Arpit Gupta, Tagyoung Chung, Mohit Bansal, Nanyun Peng</strong></p>
<p>Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet. </p>
<blockquote>
<p>è¿‘æœŸä½¿ç”¨åˆæˆæ•°æ®æ”¹è¿›LLMæ•°å­¦æ¨ç†çš„ç ”ç©¶é‡‡ç”¨äº†ç‹¬ç‰¹çš„è®¾ç½®ï¼Œè¿™ä½¿å¾—æ•°æ®åˆæˆç­–ç•¥çš„å¯¹æ¯”å˜å¾—ä¸åˆ‡å®é™…ã€‚è¿™ç•™ä¸‹äº†è®¸å¤šå…³äºåˆæˆæ•°æ®ç®¡é“ä¸­ä¸åŒå› ç´ ä½œç”¨çš„é—®é¢˜æ‚¬è€Œæœªå†³ï¼Œä¾‹å¦‚è¿‡æ»¤ä½è´¨é‡é—®é¢˜çš„å½±å“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FLAMESï¼Œä¸€ä¸ªç”¨äºLLMæ•°å­¦æ¨ç†æ•°æ®åˆæˆçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¯¹ç°æœ‰çš„10ç§æ•°æ®åˆæˆç­–ç•¥å’Œå¤šç§å½±å“åˆæˆæ•°å­¦æ¨ç†æ•°æ®æ€§èƒ½çš„å…¶ä»–å› ç´ è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬çš„FLAMESå®éªŒæä¾›äº†å…³äºåˆæˆæ•°æ®çš„éš¾åº¦å’Œå¤šæ ·æ€§ä¹‹é—´æœ€ä½³å¹³è¡¡çš„å®è´µè§è§£ã€‚é¦–å…ˆï¼Œæ—¨åœ¨å¢åŠ é—®é¢˜å¤æ‚æ€§çš„æ•°æ®ä»£ç†åœ¨å¤§å¤šæ•°æ•°å­¦æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€ä½³æ”¹è¿›æ•ˆæœã€‚å…¶æ¬¡ï¼Œåœ¨å›ºå®šçš„æ•°æ®ç”Ÿæˆé¢„ç®—ä¸‹ï¼Œä¿æŒæ›´é«˜çš„é—®é¢˜è¦†ç›–é¢æ¯”åªä¿ç•™å…·æœ‰å¯é è§£å†³æ–¹æ¡ˆçš„é—®é¢˜æ›´é‡è¦ã€‚ç¬¬ä¸‰ï¼ŒåŸºäºGSM8Kå’ŒMATHçš„åˆæ•°æ®å¯ä»¥åœ¨ç«èµ›æ°´å¹³çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æ”¹è¿›ï¼Œå±•ç¤ºäº†ä»æ˜“åˆ°éš¾çš„æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨æˆ‘ä»¬ä»FLAMESå®éªŒä¸­è·å¾—çš„è§è§£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ç§æ–°å‹æ•°æ®åˆæˆç­–ç•¥ï¼Œä»¥æé«˜è·¨åŸŸæ³›åŒ–å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†FLAMESæ•°æ®é›†ï¼Œå®ƒæ˜¯æˆ‘ä»¬æ–°é¢–å’Œç°æœ‰æ•°æ®åˆæˆç­–ç•¥çš„æœ‰æ•ˆèåˆï¼Œåœ¨OlympiadBenchï¼ˆé«˜å‡º+15.7ï¼‰ã€CollegeMathï¼ˆ+4.5ï¼‰ã€GSMPlusï¼ˆ+6.5ï¼‰å’ŒMATHï¼ˆ+3.1ï¼‰ç­‰å…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚å¯¹FLAMESæ•°æ®é›†è¿›è¡Œå¾®è°ƒåï¼ŒQwen2.5-Math-7Båœ¨MATHä¸Šè¾¾åˆ°äº†81.4%ï¼Œè¶…è¿‡äº†æ›´å¤§çš„Llama3 405Bã€GPT-4oå’ŒClaude 3.5 Sonnetã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16514v1">PDF</a> To appear at EMNLP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†FLAMESæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ•°å­¦æ¨ç†æ•°æ®åˆæˆç­–ç•¥ã€‚ä½œè€…å¯¹10ç§ç°æœ‰çš„æ•°æ®åˆæˆç­–ç•¥å’Œå¤šç§å½±å“åˆæˆæ•°å­¦æ¨ç†æ•°æ®æ€§èƒ½çš„å› ç´ è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œæä¾›äº†å…³äºåˆæˆæ•°æ®éš¾åº¦å’Œå¤šæ ·æ€§å¹³è¡¡æ–¹é¢çš„å®è´µè§è§£ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¾è®¡å¢åŠ é—®é¢˜å¤æ‚æ€§çš„æ•°æ®ä»£ç†å¯åœ¨å¤§å¤šæ•°æ•°å­¦æŒ‡æ ‡ä¸Šè·å¾—æœ€ä½³æ”¹è¿›ï¼›åœ¨å›ºå®šæ•°æ®ç”Ÿæˆé¢„ç®—ä¸‹ï¼Œä¿æŒæ›´é«˜çš„é—®é¢˜è¦†ç›–é¢æ¯”ä¿æŒåªæœ‰å¯é è§£å†³æ–¹æ¡ˆçš„é—®é¢˜æ›´é‡è¦ï¼›åŸºäºGSM8Kå’ŒMATHçš„åˆæˆæ•°æ®å¯åœ¨ç«èµ›çº§åˆ«çš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°æ”¹è¿›ï¼Œå±•ç¤ºä»æ˜“åˆ°éš¾çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¾è®¡å¢åŠ é—®é¢˜å¤æ‚æ€§çš„æ•°æ®ä»£ç†åœ¨æå‡æ•°å­¦æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>åœ¨æœ‰é™çš„æ•°æ®ç”Ÿæˆé¢„ç®—ä¸‹ï¼Œå¹¿æ³›è¦†ç›–ä¸åŒé—®é¢˜æ¯”åªå…³æ³¨æœ‰å¯é è§£å†³æ–¹æ¡ˆçš„é—®é¢˜æ›´é‡è¦ã€‚</li>
<li>GSM8Kå’ŒMATHåŸºç¡€ä¸Šçš„åˆæˆæ•°æ®åœ¨ç«èµ›çº§åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ”¹è¿›æ•ˆæœï¼Œè¯æ˜äº†ä»ç®€å•åˆ°å¤æ‚é—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨FLAMESå®éªŒä¸­çš„è§è§£ï¼Œæå‡ºäº†ä¸¤ç§æ–°çš„æ•°æ®åˆæˆç­–ç•¥ï¼Œæ—¨åœ¨æé«˜è·¨åŸŸæ³›åŒ–å’Œé²æ£’æ€§ã€‚</li>
<li>å¼€å‘çš„FLAMESæ•°æ®é›†èåˆäº†æ–°é¢–å’Œç°æœ‰çš„æ•°æ®åˆæˆç­–ç•¥ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¯¹QWEN2.5-MATH-7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨FLAMESæ•°æ®é›†åœ¨MATHä»»åŠ¡ä¸Šè¾¾åˆ°äº†81.4%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†å…¶ä»–å¤§å‹æ¨¡å‹å¦‚Llama3 405Bã€GPT-4oå’ŒClaude 3.5 Sonnetã€‚</li>
<li>FLAMESæ¡†æ¶ä¸ºè¯„ä¼°å’Œæ”¹è¿›LLMæ•°å­¦æ¨ç†æ•°æ®åˆæˆç­–ç•¥æä¾›äº†ä¸€ä¸ªé‡è¦å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae20a95963831f6f4dd880d0c4a4ab10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d96fea6ca146988bab27dacf56dd949a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05019a05cd2a664d6094f32b8d94545f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9c7226de7a2dad1e6574b7f0ffe6026.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beyond-Interpretability-Exploring-the-Comprehensibility-of-Adaptive-Video-Streaming-through-Large-Language-Models"><a href="#Beyond-Interpretability-Exploring-the-Comprehensibility-of-Adaptive-Video-Streaming-through-Large-Language-Models" class="headerlink" title="Beyond Interpretability: Exploring the Comprehensibility of Adaptive   Video Streaming through Large Language Models"></a>Beyond Interpretability: Exploring the Comprehensibility of Adaptive   Video Streaming through Large Language Models</h2><p><strong>Authors:Lianchen Jia, Chaoyang Li, Ziqi Yuan, Jiahui Chen, Tianchi Huang, Jiangchuan Liu, Lifeng Sun</strong></p>
<p>Over the past decade, adaptive video streaming technology has witnessed significant advancements, particularly driven by the rapid evolution of deep learning techniques. However, the black-box nature of deep learning algorithms presents challenges for developers in understanding decision-making processes and optimizing for specific application scenarios. Although existing research has enhanced algorithm interpretability through decision tree conversion, interpretability does not directly equate to developersâ€™ subjective comprehensibility. To address this challenge, we introduce \texttt{ComTree}, the first bitrate adaptation algorithm generation framework that considers comprehensibility. The framework initially generates the complete set of decision trees that meet performance requirements, then leverages large language models to evaluate these trees for developer comprehensibility, ultimately selecting solutions that best facilitate human understanding and enhancement. Experimental results demonstrate that \texttt{ComTree} significantly improves comprehensibility while maintaining competitive performance, showing potential for further advancement. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/thu-media/ComTree">https://github.com/thu-media/ComTree</a>. </p>
<blockquote>
<p>è¿‡å»åå¹´æ¥ï¼Œè‡ªé€‚åº”è§†é¢‘æµåª’ä½“æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå°¤å…¶æ˜¯å—åˆ°æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿…é€Ÿå‘å±•çš„æ¨åŠ¨ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•çš„é»‘ç›’æ€§è´¨ç»™å¼€å‘è€…å¸¦æ¥äº†ç†è§£å†³ç­–è¿‡ç¨‹å’Œé’ˆå¯¹ç‰¹å®šåº”ç”¨åœºæ™¯è¿›è¡Œä¼˜åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶å·²é€šè¿‡å†³ç­–æ ‘è½¬æ¢æé«˜äº†ç®—æ³•çš„å¯è§£é‡Šæ€§ï¼Œä½†å¯è§£é‡Šæ€§å¹¶ä¸ç›´æ¥ç­‰åŒäºå¼€å‘è€…çš„ä¸»è§‚å¯ç†è§£æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<code>ComTree&#39;ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè€ƒè™‘å¯ç†è§£æ€§çš„æ¯”ç‰¹ç‡è‡ªé€‚åº”ç®—æ³•ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆç”Ÿæˆæ»¡è¶³æ€§èƒ½è¦æ±‚çš„å®Œæ•´å†³ç­–æ ‘é›†åˆï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¼€å‘è€…çš„å¯ç†è§£æ€§å¯¹è¿™äº›æ ‘è¿›è¡Œè¯„ä¼°ï¼Œæœ€ç»ˆé€‰æ‹©æœ€æœ‰åˆ©äºäººç±»ç†è§£å’Œä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ</code>ComTreeâ€™åœ¨ä¿æŒç«äº‰åŠ›çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†å¯ç†è§£æ€§ï¼Œæ˜¾ç¤ºå‡ºè¿›ä¸€æ­¥å‘å±•çš„æ½œåŠ›ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/thu-media/ComTree%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/thu-media/ComTreeè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16448v1">PDF</a> ACM Multimedia2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œè‡ªé€‚åº”è§†é¢‘æµåª’ä½“æŠ€æœ¯å—åˆ°æ·±åº¦å­¦ä¹ çš„æ¨åŠ¨å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•çš„é»‘ç›’æ€§è´¨ä½¿å¾—å¼€å‘è€…éš¾ä»¥äº†è§£å†³ç­–è¿‡ç¨‹å¹¶ä¼˜åŒ–ç‰¹å®šåº”ç”¨åœºæ™¯ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾è€ƒè™‘å¯ç†è§£æ€§çš„æ¯”ç‰¹ç‡è‡ªé€‚åº”ç®—æ³•ç”Ÿæˆæ¡†æ¶â€”â€”ComTreeã€‚å®ƒé¦–å…ˆç”Ÿæˆæ»¡è¶³æ€§èƒ½è¦æ±‚çš„å®Œæ•´å†³ç­–æ ‘é›†ï¼Œå†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°è¿™äº›æ ‘çš„å¯ç†è§£æ€§ç¨‹åº¦ï¼Œæœ€ç»ˆé€‰æ‹©æœ€èƒ½ä¿ƒè¿›äººç±»ç†è§£å’Œä¼˜åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒComTreeåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—æé«˜äº†å¯ç†è§£æ€§ï¼Œå±•ç°å‡ºè¿›ä¸€æ­¥å‘å±•çš„æ½œåŠ›ã€‚æºç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/thu-media/ComTree">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªé€‚åº”è§†é¢‘æµåª’ä½“æŠ€æœ¯å—ç›Šäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ç®—æ³•çš„é»‘ç›’æ€§è´¨å¯¼è‡´å¼€å‘è€…åœ¨ç†è§£å’Œä¼˜åŒ–å†³ç­–è¿‡ç¨‹æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ComTreeæ˜¯é¦–ä¸ªè€ƒè™‘å¯ç†è§£æ€§çš„æ¯”ç‰¹ç‡è‡ªé€‚åº”ç®—æ³•ç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>ComTreeé€šè¿‡ç”Ÿæˆå†³ç­–æ ‘é›†å¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å…¶å¯ç†è§£æ€§æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ComTreeåœ¨ä¿æŒæ€§èƒ½ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç®—æ³•çš„å¯ç†è§£æ€§ã€‚</li>
<li>ComTreeå…·æœ‰è¿›ä¸€æ­¥å‘å±•çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3f219d08475ae80f3d000eb83b1ba84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1ec41e8262dc447431c4634b8064caf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1bec806a121d7cd42ed6741993656a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b10da1ff6fea0be7cced245b93c1a5b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-981e8b4eb3f458da8ab2dba8953ccbb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1493e3734c986ab4edcbce921ebd98a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Boardwalk-Towards-a-Framework-for-Creating-Board-Games-with-LLMs"><a href="#Boardwalk-Towards-a-Framework-for-Creating-Board-Games-with-LLMs" class="headerlink" title="Boardwalk: Towards a Framework for Creating Board Games with LLMs"></a>Boardwalk: Towards a Framework for Creating Board Games with LLMs</h2><p><strong>Authors:Ãlvaro Guglielmin Becker, Gabriel Bauer de Oliveira, Lana Bertoldo Rossato, Anderson Rocha Tavares</strong></p>
<p>Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible. </p>
<blockquote>
<p>å°†æ¿æ¸¸æˆä»¥ä»£ç å½¢å¼å®ç°å¯èƒ½ä¼šæ˜¯ä¸€é¡¹è€—æ—¶çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ©ç”¨ç®€å•çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸ºç‰¹å®šé¢†åŸŸä»»åŠ¡ç”Ÿæˆä»£ç æ–¹é¢å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç ”ç©¶LLMæ˜¯å¦èƒ½ä»è‡ªç„¶è¯­è¨€æè¿°çš„æ¸¸æˆè§„åˆ™å®ç°æ•°å­—åŒ–æ¿æ¸¸æˆç‰ˆæœ¬ã€‚è¿™å°†æœç€å»ºç«‹ä¸€ä¸ªç”±LLMè¾…åŠ©çš„å¿«é€Ÿæ¿æ¸¸æˆä»£ç ç”Ÿæˆæ¡†æ¶è¿ˆå‡ºä¸€æ­¥ã€‚æˆ‘ä»¬æœŸæœ›ç¡®å®šLLMå®ç°æ¿æ¸¸æˆçš„ä¸»è¦æŒ‘æˆ˜ï¼Œä»¥åŠä¸åŒçš„æ–¹æ³•å’Œæ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰æ¬¾æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆClaudeã€DeepSeekå’ŒChatGPTï¼‰ä¸ºä¸€äº›æµè¡Œå’Œä¸å¤ªä¸ºäººçŸ¥çš„æ¸¸æˆç¼–å†™ä»£ç ï¼ŒåŒ…æ‹¬åœ¨æˆ‘ä»¬æå‡ºçš„é€šç”¨æ¸¸æˆæ¥å£Boardwalkä¸­ä»¥è‡ªç”±å½¢å¼çš„æ¸¸æˆã€‚æˆ‘ä»¬å°†æ¸¸æˆå’Œç»„ä»¶åŒ¿ååŒ–ï¼Œä»¥é¿å…æ¿€å‘é¢„è®­ç»ƒLLMçš„çŸ¥è¯†ã€‚å®ç°çš„ä»£ç ä¼šè¿›è¡Œå¯ç©æ€§å’Œè§„åˆ™ç¬¦åˆåº¦æµ‹è¯•ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒLLMã€æ¸¸æˆæµè¡Œåº¦å’ŒæˆåŠŸç‡çš„æ™®éé”™è¯¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¯è¡Œçš„ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹Claude 3.7 Sonnetåœ¨æ— é”™è¯¯çš„æƒ…å†µä¸‹å®ç°äº†55.6%çš„æ¸¸æˆã€‚è™½ç„¶ç¬¦åˆAPIä¼šå¢åŠ é”™è¯¯é¢‘ç‡ï¼Œä½†é”™è¯¯çš„ä¸¥é‡æ€§æ›´å–å†³äºLLMã€‚æˆ‘ä»¬æ¦‚è¿°äº†åˆ›å»ºæ•´åˆæ­¤è¿‡ç¨‹çš„æ¡†æ¶çš„æœªæ¥æ­¥éª¤ï¼Œä½¿æ¿æ¸¸æˆçš„åˆ¶ä½œæ›´åŠ å®¹æ˜“ä¸Šæ‰‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16447v1">PDF</a> Accepted at SBGames 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºç”Ÿæˆå®ç°æ£‹ç›˜æ¸¸æˆçš„ä»£ç ï¼Œè¿™å¤§å¤§å‡å°‘äº†ç¼–ç¨‹æ—¶é—´ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨LLMæ˜¯å¦èƒ½å¤Ÿæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°çš„è§„åˆ™å®ç°æ£‹ç›˜æ¸¸æˆçš„æ•°å­—åŒ–ç‰ˆæœ¬ã€‚é€šè¿‡å¯¹ä¸‰æ¬¾å°–ç«¯LLMï¼ˆClaudeã€DeepSeekå’ŒChatGPTï¼‰çš„æµ‹è¯•ï¼Œå‘ç°å…¶åœ¨é€šç”¨æ£‹ç›˜æ¸¸æˆAPI Boardwalkä¸­çš„è¡¨ç°å…·æœ‰å¯è¡Œæ€§ï¼Œå…¶ä¸­æœ€å¥½çš„æ¨¡å‹Claude 3.7 Sonnetçš„é”™è¯¯ç‡ä¸ºé›¶ã€‚è™½ç„¶APIåˆè§„æ€§ä¼šå¢åŠ é”™è¯¯é¢‘ç‡ï¼Œä½†é”™è¯¯çš„ä¸¥é‡æ€§æ›´å¤šåœ°å–å†³äºLLMæœ¬èº«ã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥åˆ›å»ºé›†æˆæ­¤è¿‡ç¨‹çš„æ¡†æ¶å¥ å®šäº†åŸºç¡€ï¼Œä½¿æ£‹ç›˜æ¸¸æˆçš„åˆ¶ä½œæ›´åŠ ä¾¿æ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½æœ‰æ•ˆç”Ÿæˆå®ç°æ£‹ç›˜æ¸¸æˆçš„ä»£ç ï¼Œæé«˜ç¼–ç¨‹æ•ˆç‡ã€‚</li>
<li>LLMå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°çš„è§„åˆ™å®ç°æ£‹ç›˜æ¸¸æˆçš„æ•°å­—åŒ–ã€‚</li>
<li>ä¸‰æ¬¾å°–ç«¯LLMï¼ˆClaudeã€DeepSeekå’ŒChatGPTï¼‰åœ¨æ£‹ç›˜æ¸¸æˆAPI Boardwalkä¸­çš„è¡¨ç°å…·æœ‰å¯è¡Œæ€§ã€‚</li>
<li>Claude 3.7 Sonnetåœ¨æµ‹è¯•ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œé”™è¯¯ç‡ä¸ºé›¶ã€‚</li>
<li>APIåˆè§„æ€§ä¼šå¢åŠ é”™è¯¯é¢‘ç‡ï¼Œä½†é”™è¯¯çš„ä¸¥é‡æ€§æ›´å¤šå–å†³äºLLMæœ¬èº«ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºåˆ›å»ºé›†æˆæ­¤è¿‡ç¨‹çš„æ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87edfb0f26a41e18d7fa8b62bcc73f1f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Using-LLMs-and-Essence-to-Support-Software-Practice-Adoption"><a href="#Using-LLMs-and-Essence-to-Support-Software-Practice-Adoption" class="headerlink" title="Using LLMs and Essence to Support Software Practice Adoption"></a>Using LLMs and Essence to Support Software Practice Adoption</h2><p><strong>Authors:Sonia Nicoletti, Paolo Ciancarini</strong></p>
<p>Recent advancements in natural language processing (NLP) have enabled the development of automated tools that support various domains, including software engineering. However, while NLP and artificial intelligence (AI) research has extensively focused on tasks such as code generation, less attention has been given to automating support for the adoption of best practices, the evolution of ways of working, and the monitoring of process health. This study addresses this gap by exploring the integration of Essence, a standard and thinking framework for managing software engineering practices, with large language models (LLMs). To this end, a specialised chatbot was developed to assist students and professionals in understanding and applying Essence. The chatbot employs a retrieval-augmented generation (RAG) system to retrieve relevant contextual information from a curated knowledge base. Four different LLMs were used to create multiple chatbot configurations, each evaluated both as a base model and augmented with the RAG system. The system performance was evaluated through both the relevance of retrieved context and the quality of generated responses. Comparative analysis against the general-purpose LLMs demonstrated that the proposed system consistently outperforms its baseline counterpart in domain-specific tasks. By facilitating access to structured software engineering knowledge, this work contributes to bridging the gap between theoretical frameworks and practical application, potentially improving process management and the adoption of software development practices. While further validation through user studies is required, these findings highlight the potential of LLM-based automation to enhance learning and decision-making in software engineering. </p>
<blockquote>
<p>æœ€è¿‘è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è¿›å±•ä¸ºæ”¯æŒåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹åœ¨å†…çš„å„ä¸ªé¢†åŸŸè‡ªåŠ¨åŒ–å·¥å…·çš„å¼€å‘æä¾›äº†å¯èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡NLPå’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ç ”ç©¶åœ¨ä»£ç ç”Ÿæˆç­‰æ–¹é¢è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¯¹äºè‡ªåŠ¨åŒ–æ”¯æŒé‡‡ç”¨æœ€ä½³å®è·µã€å·¥ä½œæ–¹å¼æ¼”å˜ä»¥åŠè¿‡ç¨‹å¥åº·ç›‘æ§çš„å…³æ³¨è¾ƒå°‘ã€‚æœ¬ç ”ç©¶é€šè¿‡æ¢ç´¢ç®¡ç†è½¯ä»¶å·¥ç¨‹å®è·µçš„é€šç”¨æ¡†æ¶æ ‡å‡†â€œEssenceâ€ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆæ¥è§£å†³è¿™ä¸€å·®è·ã€‚ä¸ºæ­¤ï¼Œå¼€å‘äº†ä¸€ç§ä¸“é—¨èŠå¤©æœºå™¨äººï¼Œæ—¨åœ¨å¸®åŠ©å­¦ç”Ÿå’Œä¸“ä¸šäººå£«ç†è§£å’Œåº”ç”¨Essenceã€‚èŠå¤©æœºå™¨äººé‡‡ç”¨å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä½¿ç”¨äº†å››ç§ä¸åŒçš„LLMåˆ›å»ºäº†å¤šä¸ªèŠå¤©æœºå™¨äººé…ç½®ï¼Œæ¯ç§é…ç½®éƒ½è¢«è¯„ä¼°ä¸ºåŸºç¡€æ¨¡å‹å’Œä¸RAGç³»ç»Ÿç»“åˆçš„å¢å¼ºæ¨¡å‹ã€‚ç³»ç»Ÿæ€§èƒ½é€šè¿‡æ£€ç´¢ä¸Šä¸‹æ–‡çš„å…³è”æ€§å’Œç”Ÿæˆå“åº”çš„è´¨é‡æ¥è¯„ä¼°ã€‚ä¸ä¸€èˆ¬ç”¨é€”çš„LLMçš„æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼Œæ‰€æå‡ºç³»ç»Ÿåœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„è¡¨ç°å§‹ç»ˆä¼˜äºåŸºçº¿ç³»ç»Ÿã€‚é€šè¿‡ç®€åŒ–è®¿é—®ç»“æ„åŒ–è½¯ä»¶å·¥ç¨‹çŸ¥è¯†ï¼Œè¿™é¡¹å·¥ä½œæœ‰åŠ©äºå¼¥åˆç†è®ºæ¡†æ¶ä¸å®è·µåº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä»è€Œå¯èƒ½æé«˜è¿‡ç¨‹ç®¡ç†å’Œè½¯ä»¶å¼€å‘å±•è§ˆå®è·µçš„é‡‡ç”¨ã€‚è™½ç„¶éœ€è¦é€šè¿‡ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯ï¼Œä½†è¿™äº›å‘ç°å‡¸æ˜¾äº†åŸºäºLLMçš„è‡ªåŠ¨åŒ–åœ¨æé«˜è½¯ä»¶å·¥ç¨‹å­¦ä¹ å’Œå†³ç­–åˆ¶å®šæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16445v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯çš„è¿›å±•ä¸ºæ”¯æŒè½¯ä»¶å·¥ç¨‹ç­‰é¢†åŸŸçš„è‡ªåŠ¨åŒ–å·¥å…·å¼€å‘æä¾›äº†å¯èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡NLPå’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶åœ¨ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨æ”¯æŒæœ€ä½³å®è·µçš„é‡‡ç”¨ã€å·¥ä½œæ–¹å¼æ¼”å˜ä»¥åŠè¿‡ç¨‹å¥åº·ç›‘æµ‹ç­‰æ–¹é¢çš„è‡ªåŠ¨åŒ–å…³æ³¨è¾ƒå°‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæ¢ç´¢ç®¡ç†è½¯ä»¶å·¥ç¨‹å®è·µçš„æ ‡å‡†å’Œæ€ç»´æ¡†æ¶â€œEssenceâ€ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›†æˆã€‚ä¸ºæ­¤ï¼Œå¼€å‘äº†ä¸€æ¬¾ä¸“é—¨ç”¨äºååŠ©å­¦ç”Ÿå’Œä¸“å®¶ç†è§£å’Œåº”ç”¨Essenceçš„èŠå¤©æœºå™¨äººã€‚è¯¥èŠå¤©æœºå™¨äººé‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œä»ç²¾é€‰çš„çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä½¿ç”¨å››ç§ä¸åŒçš„LLMåˆ›å»ºå¤šä¸ªèŠå¤©æœºå™¨äººé…ç½®ï¼Œå¹¶å¯¹æ¯ç§é…ç½®åœ¨ä½œä¸ºåŸºç¡€æ¨¡å‹å’Œä½¿ç”¨RAGç³»ç»Ÿå¢å¼ºçš„æƒ…å†µè¿›è¡Œè¯„ä»·ã€‚é€šè¿‡è¯„ä¼°æ£€ç´¢ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§å’Œç”Ÿæˆå“åº”çš„è´¨é‡æ¥è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚ä¸é€šç”¨LLMçš„å¯¹æ¯”åˆ†ææ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºå‡†ç³»ç»Ÿã€‚é€šè¿‡æä¾›å¯¹ç»“æ„åŒ–è½¯ä»¶å·¥ç¨‹çŸ¥è¯†çš„è®¿é—®ï¼Œæœ¬ç ”ç©¶æœ‰åŠ©äºå¼¥ç†è®ºæ¡†æ¶ä¸å®è·µåº”ç”¨ä¹‹é—´çš„å·®è·ï¼Œæœ‰å¯èƒ½æ”¹è¿›è¿‡ç¨‹ç®¡ç†å’Œè½¯ä»¶å¼€å‘å®è·µçš„é‡‡ç”¨ã€‚è™½ç„¶éœ€è¦é€šè¿‡ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥éªŒè¯ï¼Œä½†è¿™äº›å‘ç°å‡¸æ˜¾äº†åŸºäºLLMçš„è‡ªåŠ¨åŒ–åœ¨æé«˜è½¯ä»¶å·¥ç¨‹å­¦ä¹ å’Œå†³ç­–åˆ¶å®šæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NLPæŠ€æœ¯çš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†è‡ªåŠ¨åŒ–å·¥å…·çš„å¼€å‘ï¼Œæ”¯æŒåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹åœ¨å†…çš„å„ä¸ªé¢†åŸŸã€‚</li>
<li>å°½ç®¡AIå’ŒNLPåœ¨ä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†åœ¨è½¯ä»¶å·¥ç¨‹å®è·µè‡ªåŠ¨åŒ–æ”¯æŒæ–¹é¢ä»å­˜åœ¨ä¸€å®šå·®è·ã€‚</li>
<li>Essenceä½œä¸ºç®¡ç†å’Œåº”ç”¨è½¯ä»¶å·¥ç¨‹å®è·µçš„æ¡†æ¶è¢«å¼•å…¥ï¼Œä»¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚</li>
<li>å¼€å‘äº†ä¸€æ¬¾èŠå¤©æœºå™¨äººï¼Œèƒ½å¤ŸååŠ©ç†è§£å’Œåº”ç”¨Essenceï¼Œç‰¹åˆ«æ˜¯ä¸ºå­¦ç”Ÿå’Œä¸“ä¸šäººå£«æä¾›æ”¯æŒã€‚</li>
<li>èŠå¤©æœºå™¨äººä½¿ç”¨RAGç³»ç»Ÿä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºå¯¹é¢†åŸŸç›¸å…³çŸ¥è¯†çš„è·å–å’Œåº”ç”¨ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸åŒçš„LLMåˆ›å»ºèŠå¤©æœºå™¨äººé…ç½®å¹¶è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d4ba31af7d33f950db37e9e24a8f8c90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-278c9539a9952d616ca91aebfe90e77a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-315c7f390f47e68183202808d768cd8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef748c22a2edf4076b9c60e2c099cb87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6c7caa1c9f8b28c9d8be5868fbb11a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PediatricsMQA-a-Multi-modal-Pediatrics-Question-Answering-Benchmark"><a href="#PediatricsMQA-a-Multi-modal-Pediatrics-Question-Answering-Benchmark" class="headerlink" title="PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark"></a>PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</h2><p><strong>Authors:Adil Bahaj, Mounir Ghogho</strong></p>
<p>Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¢å¼ºè§†è§‰çš„LLMï¼ˆVLMï¼‰åœ¨åŒ»å­¦ä¿¡æ¯åŒ–ã€è¯Šæ–­å’Œå†³ç­–æ”¯æŒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å­˜åœ¨ç³»ç»Ÿæ€§åè§ï¼Œç‰¹åˆ«æ˜¯å¹´é¾„åè§ï¼ŒæŸå®³äº†å®ƒä»¬çš„å¯é æ€§å’Œå…¬å¹³æ€§ã€‚è¿™åœ¨é’ˆå¯¹å„¿ç§‘æ–‡æœ¬å’Œè§†è§‰é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ä¸Šè¡¨ç°è¾ƒå·®æ—¶å°¤ä¸ºæ˜æ˜¾ã€‚è¿™ç§åè§åæ˜ äº†åŒ»ç–—ç ”ç©¶ä¸­çš„ä¸å¹³è¡¡çŠ¶å†µï¼Œå„¿ç§‘ç ”ç©¶è·å¾—çš„èµ„é‡‘å’Œæ”¯æŒè¾ƒå°‘ï¼Œå°½ç®¡å„¿ç«¥ç–¾ç—…è´Ÿæ‹…ååˆ†æ²‰é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„å…¨é¢çš„å¤šæ¨¡å¼å„¿ç§‘é—®ç­”åŸºå‡†æµ‹è¯•PediatricsMQAã€‚å®ƒåŒ…å«3417ä¸ªåŸºäºæ–‡æœ¬çš„é€‰æ‹©é¢˜ï¼ˆMCQsï¼‰ï¼Œæ¶µç›–ä»äº§å‰åˆ°é’å°‘å¹´çš„ä¸ƒä¸ªå‘è‚²é˜¶æ®µçš„131ä¸ªå„¿ç§‘ä¸»é¢˜ï¼Œä»¥åŠä½¿ç”¨æ¥è‡ª67ç§æˆåƒæ¨¡æ€çš„634å¼ å„¿ç§‘å›¾åƒçš„2067ä¸ªåŸºäºè§†è§‰çš„MCQsã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡æ··åˆæ‰‹åŠ¨è‡ªåŠ¨ç®¡é“å¼€å‘çš„ï¼Œç»“åˆäº†ç»è¿‡åŒè¡Œè¯„å®¡çš„å„¿ç§‘æ–‡çŒ®ã€éªŒè¯è¿‡çš„é—®é¢˜åº“ã€ç°æœ‰åŸºå‡†æµ‹è¯•å’Œé—®ç­”èµ„æºã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¼€æ”¾æ¨¡å‹ï¼Œå‘ç°åœ¨å¹´è½»äººç¾¤ä¸­å‡ºç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦å¹´é¾„æ„ŸçŸ¥æ–¹æ³•æ¥ç¡®ä¿åœ¨å„¿ç§‘æŠ¤ç†ä¸­å®ç°å…¬å¹³çš„AIæ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰å¢å¼ºå‹LLMï¼ˆVLMï¼‰åœ¨åŒ»ç–—ä¿¡æ¯åŒ–ã€è¯Šæ–­å’Œå†³ç­–æ”¯æŒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å­˜åœ¨ç³»ç»Ÿæ€§åè§ï¼Œå°¤å…¶æ˜¯å¹´é¾„åè§ï¼Œå½±å“å®ƒä»¬çš„å¯é æ€§å’Œå…¬å¹³æ€§ã€‚ä¸ºæ­¤ï¼Œæ¨å‡ºå…¨æ–°å¤šæ¨¡å¼å„¿ç§‘é—®ç­”åŸºå‡†æµ‹è¯•å¹³å°PediatricsMQAï¼ŒåŒ…å«æ–‡æœ¬å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³å„¿ç§‘é¢†åŸŸäººå·¥æ™ºèƒ½æ”¯æŒä¸è¶³çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå’ŒVLMåœ¨åŒ»ç–—é¢†åŸŸæœ‰é‡å¤§è¿›å±•ï¼Œä½†å­˜åœ¨ç³»ç»Ÿæ€§åè§ï¼Œå½±å“å¯é æ€§å’Œå…¬å¹³æ€§ã€‚</li>
<li>å¹´é¾„åè§åœ¨å„¿ç§‘ç›¸å…³çš„æ–‡æœ¬å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>å„¿ç§‘ç ”ç©¶èµ„é‡‘å’Œæ”¯æŒç›¸å¯¹è¾ƒå°‘ï¼Œå¯¼è‡´æ¨¡å‹åè§ã€‚</li>
<li>æ¨å‡ºPediatricsMQAåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³å„¿ç§‘é¢†åŸŸäººå·¥æ™ºèƒ½æ”¯æŒä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¹³å°åŒ…å«æ–‡æœ¬å’Œè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ï¼Œè¦†ç›–131ä¸ªå„¿ç§‘ä¸»é¢˜ï¼Œæ¶‰åŠä»èƒå„¿åˆ°é’å°‘å¹´çš„ä¸ƒä¸ªå‘å±•é˜¶æ®µã€‚</li>
<li>æ•°æ®é›†é€šè¿‡æ··åˆæ‰‹åŠ¨è‡ªåŠ¨ç®¡é“å¼€å‘ï¼Œæ•´åˆäº†ç»è¿‡å®¡æŸ¥çš„å„¿ç§‘æ–‡çŒ®ã€éªŒè¯é¢˜åº“ã€ç°æœ‰åŸºå‡†æµ‹è¯•å’Œé—®ç­”èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0bb4de54740c7b8ad66f52bf2f3e480f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-300b22fad5457794447a24295dfe661a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc3ec3b03fdeb0de98704dfb83b077be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fecf4b519cb2f2f3a5dd18eba18b7d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af84e496eebdb5b2030a0c2821e17175.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OPERA-A-Reinforcement-Learningâ€“Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval"><a href="#OPERA-A-Reinforcement-Learningâ€“Enhanced-Orchestrated-Planner-Executor-Architecture-for-Reasoning-Oriented-Multi-Hop-Retrieval" class="headerlink" title="OPERA: A Reinforcement Learningâ€“Enhanced Orchestrated Planner-Executor   Architecture for Reasoning-Oriented Multi-Hop Retrieval"></a>OPERA: A Reinforcement Learningâ€“Enhanced Orchestrated Planner-Executor   Architecture for Reasoning-Oriented Multi-Hop Retrieval</h2><p><strong>Authors:Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma</strong></p>
<p>Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERAâ€™s Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERAâ€™s superior performance, validating both the MAPGRPO method and OPERAâ€™s design. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Ameame1/OPERA">https://github.com/Ameame1/OPERA</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¯†é›†æ£€ç´¢å™¨çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å‘å¤æ‚æ¨ç†çš„è·¨æ­¥æ£€ç´¢ä»»åŠ¡ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼š1ï¼‰é¢å‘æ¨ç†çš„è§„åˆ’æ— æ•ˆï¼šå…ˆå‰çš„æ–¹æ³•åœ¨ç”Ÿæˆå¤æ‚æŸ¥è¯¢çš„ç¨³å¥å¤šæ­¥è§„åˆ’æ–¹é¢è¡¨ç°æŒ£æ‰ï¼ŒåŸºäºè§„åˆ™çš„åˆ†è§£å™¨åœ¨éå¸¸è§„é—®é¢˜ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚2ï¼‰æ¨ç†é©±åŠ¨çš„æ£€ç´¢ç»“æœä¸ä½³ï¼šç›¸å…³æ–¹æ³•é‡‡ç”¨æœ‰é™çš„æŸ¥è¯¢é‡æ„ï¼Œå¯¼è‡´ç»å¸¸é™·å…¥è¿­ä»£æ£€ç´¢å¾ªç¯ï¼Œæ— æ³•æ‰¾åˆ°å…³é”®æ–‡æ¡£ã€‚3ï¼‰ç¼ºä¹æ¨ç†æŒ‡å¯¼çš„è¿‡æ»¤ï¼šæµè¡Œçš„æ–¹æ³•ç¼ºä¹ç²¾ç»†æ¨ç†ï¼Œæ— æ³•ä»å˜ˆæ‚çš„ç»“æœä¸­æœ‰æ•ˆè¿‡æ»¤é‡è¦ä¿¡æ¯ï¼Œé˜»ç¢äº†æ£€ç´¢çŸ¥è¯†çš„åˆ©ç”¨ã€‚è¿™äº›å±€é™æ€§æ ¹æœ¬æºäºå½“å‰RAGæ¶æ„ä¸­æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„å¼±è€¦åˆã€‚æˆ‘ä»¬å¼•å…¥äº†â€œååŒè§„åˆ’æ‰§è¡Œæ¨ç†æ¶æ„â€ï¼ˆOPERAï¼‰è¿™ä¸€æ–°é¢–çš„æ¨ç†é©±åŠ¨æ£€ç´¢æ¡†æ¶ã€‚OPERAçš„ç›®æ ‡è§„åˆ’æ¨¡å—ï¼ˆGPMï¼‰å°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œå¹¶ç”±å…·æœ‰ç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆæ£€ç´¢çš„ç»„ä»¶çš„æ¨ç†æ‰§è¡Œæ¨¡å—ï¼ˆREMï¼‰æ‰§è¡Œã€‚ä¸ºäº†è®­ç»ƒOPERAï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ™ºèƒ½ä½“æ¸è¿›ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPGRPOï¼‰ï¼Œè¿™æ˜¯GRPOçš„ä¸€ç§æ–°å‹å˜ä½“ã€‚åœ¨å¤æ‚çš„å¤šæ­¥åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ˜¾ç¤ºäº†OPERAçš„å“è¶Šæ€§èƒ½ï¼ŒéªŒè¯äº†MAPGRPOæ–¹æ³•å’ŒOPERAè®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ameame1/OPERA%E3%80%82">https://github.com/Ameame1/OPERAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16438v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¯†é›†æ£€ç´¢å™¨çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨é¢å‘å¤æ‚æ¨ç†çš„å¤šè·³æ£€ç´¢ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ¨ç†å¯¼å‘è§„åˆ’ä¸åˆç†ã€æ¨ç†é©±åŠ¨æ£€ç´¢æ•ˆæœå·®ä»¥åŠç¼ºä¹æ¨ç†æŒ‡å¯¼çš„è¿‡æ»¤èƒ½åŠ›ã€‚è¿™äº›é—®é¢˜çš„æ ¹æºåœ¨äºå½“å‰RAGæ¶æ„ä¸­æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„å¼±è€¦åˆã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„æ¨ç†é©±åŠ¨æ£€ç´¢æ¡†æ¶â€”â€”Orchestrated Planner-Executor Reasoning Architectureï¼ˆOPERAï¼‰ã€‚OPERAçš„ç›®æ ‡è§„åˆ’æ¨¡å—ï¼ˆGPMï¼‰å°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œå¹¶ç”±å…·æœ‰ç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆæ£€ç´¢åŠŸèƒ½çš„Reason-Executeæ¨¡å—ï¼ˆREMï¼‰æ‰§è¡Œã€‚ä¸ºäº†è®­ç»ƒOPERAï¼Œæå‡ºäº†ä¸€ç§æ–°çš„MAPGRPOæ–¹æ³•ã€‚åœ¨å¤æ‚çš„å¤šè·³åŸºå‡†æµ‹è¯•ä¸Šï¼ŒOPERAè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†MAPGRPOæ–¹æ³•å’ŒOPERAè®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå’Œå¯†é›†æ£€ç´¢å™¨çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†RAGçš„æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨å¤æ‚æ¨ç†çš„å¤šè·³æ£€ç´¢ä»»åŠ¡ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ¨ç†å¯¼å‘è§„åˆ’ä¸åˆç†ã€æ¨ç†é©±åŠ¨æ£€ç´¢æ•ˆæœå·®å’Œç¼ºä¹ç²¾ç»†æ¨ç†è¿‡æ»¤èƒ½åŠ›çš„é—®é¢˜ã€‚</li>
<li>OPERAæ¡†æ¶é€šè¿‡å¼•å…¥ç›®æ ‡è§„åˆ’æ¨¡å—ï¼ˆGPMï¼‰å’ŒReason-Executeæ¨¡å—ï¼ˆREMï¼‰è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå®ç°äº†ç²¾ç¡®çš„æ¨ç†å’Œæœ‰æ•ˆçš„æ£€ç´¢ã€‚</li>
<li>OPERAçš„GPMå°†é—®é¢˜åˆ†è§£ä¸ºå­ç›®æ ‡ï¼ŒREMåˆ™è´Ÿè´£æ‰§è¡Œè¿™äº›å­ç›®æ ‡ï¼Œå…·æœ‰ä¸“é—¨çš„ç»„ä»¶è¿›è¡Œç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆæ£€ç´¢ã€‚</li>
<li>ä¸ºäº†è®­ç»ƒOPERAï¼Œæå‡ºäº†ä¸€ç§æ–°çš„MAPGRPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚çš„å¤šè·³åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOPERAçš„è®¾è®¡å’Œæ–¹æ³•éªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5288854966102d6ddb28705cd862cd8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757fdc0e351d7094bf2dee674197a1b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b53bb1d94e6765c707d29c6334be8652.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b75c56225fc31b965dda4eae8b267bc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0ff9fa229a2391e799255296e28bdb4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ChatGPT-generated-texts-show-authorship-traits-that-identify-them-as-non-human"><a href="#ChatGPT-generated-texts-show-authorship-traits-that-identify-them-as-non-human" class="headerlink" title="ChatGPT-generated texts show authorship traits that identify them as   non-human"></a>ChatGPT-generated texts show authorship traits that identify them as   non-human</h2><p><strong>Authors:Vittoria Dentella, Weihang Huang, Silvia Angela Mansi, Jack Grieve, Evelina Leivada</strong></p>
<p>Large Language Models can emulate different writing styles, ranging from composing poetry that appears indistinguishable from that of famous poets to using slang that can convince people that they are chatting with a human online. While differences in style may not always be visible to the untrained eye, we can generally distinguish the writing of different people, like a linguistic fingerprint. This work examines whether a language model can also be linked to a specific fingerprint. Through stylometric and multidimensional register analyses, we compare human-authored and model-authored texts from different registers. We find that the model can successfully adapt its style depending on whether it is prompted to produce a Wikipedia entry vs. a college essay, but not in a way that makes it indistinguishable from humans. Concretely, the model shows more limited variation when producing outputs in different registers. Our results suggest that the model prefers nouns to verbs, thus showing a distinct linguistic backbone from humans, who tend to anchor language in the highly grammaticalized dimensions of tense, aspect, and mood. It is possible that the more complex domains of grammar reflect a mode of thought unique to humans, thus acting as a litmus test for Artificial Intelligence. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿä¸åŒçš„å†™ä½œé£æ ¼ï¼Œä»åˆ›ä½œçœ‹ä¼¼ä¸è‘—åè¯—äººä½œå“æ— å¼‚çš„è¯—æ­Œï¼Œåˆ°ä½¿ç”¨ç½‘ç»œèŠå¤©ä¸­å¸¸è§çš„ä¿šè¯­ä¸ä¸€è€Œè¶³ã€‚è™½ç„¶æœªç»è®­ç»ƒçš„äººçœ¼å¯èƒ½æ— æ³•æ€»æ˜¯çœ‹åˆ°é£æ ¼ä¸Šçš„å·®å¼‚ï¼Œä½†æˆ‘ä»¬é€šå¸¸å¯ä»¥åƒè¯†åˆ«è¯­è¨€æŒ‡çº¹ä¸€æ ·åŒºåˆ†ä¸åŒäººçš„å†™ä½œã€‚è¿™é¡¹å·¥ä½œè€ƒå¯Ÿäº†è¯­è¨€æ¨¡å‹æ˜¯å¦ä¹Ÿå¯ä»¥ä¸ç‰¹å®šçš„æŒ‡çº¹ç›¸å…³è”ã€‚é€šè¿‡æ–‡ä½“å’Œå¤šç»´è¯­åŸŸåˆ†æï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†äººç±»ä½œè€…å’Œä¸åŒè¯­åŸŸæ¨¡å‹ä½œè€…çš„æ–‡æœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ ¹æ®æç¤ºæˆåŠŸé€‚åº”ä¸åŒçš„å†™ä½œé£æ ¼ï¼Œæ— è®ºæ˜¯æ’°å†™ç»´åŸºç™¾ç§‘è¯æ¡è¿˜æ˜¯å¤§å­¦è®ºæ–‡ï¼Œä½†å¹¶æœªè¾¾åˆ°ä¸äººç±»ä½œå“æ— æ³•åŒºåˆ†çš„åœ°æ­¥ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹åœ¨ä¸åŒè¯­åŸŸçš„è¾“å‡ºè¡¨ç°ä¸­å˜åŒ–æ›´ä¸ºæœ‰é™ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ›´å€¾å‘äºä½¿ç”¨åè¯è€ŒéåŠ¨è¯ï¼Œä»è€Œæ˜¾ç¤ºå‡ºä¸äººç±»ä¸åŒçš„ç‹¬ç‰¹è¯­è¨€ä¸»å¹²ã€‚äººç±»å€¾å‘äºå°†è¯­è¨€é”šå®šåœ¨æ—¶æ€ã€è¯­æ€å’Œè¯­æ°”ç­‰é«˜åº¦è¯­æ³•åŒ–çš„ç»´åº¦ä¸Šï¼Œè€Œå¤æ‚çš„è¯­æ³•é¢†åŸŸå¯èƒ½åæ˜ äº†äººç±»ç‰¹æœ‰çš„æ€ç»´æ¨¡å¼ï¼Œå› æ­¤æˆä¸ºäººå·¥æ™ºèƒ½çš„è¯•é‡‘çŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16385v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿä¸åŒçš„å†™ä½œé£æ ¼ï¼Œä»åˆ›ä½œçœ‹ä¼¼å‡ºè‡ªè‘—åè¯—äººä¹‹æ‰‹çš„è¯—æ­Œï¼Œåˆ°ä½¿ç”¨ç½‘ç»œèŠå¤©ä¸­çš„ä¿šè¯­ã€‚æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹æ˜¯å¦å¯ä¸å…¶ç‰¹æœ‰çš„â€œæŒ‡çº¹â€ç›¸å…³è”ã€‚é€šè¿‡æ–‡ä½“å­¦å’Œå¤šå…ƒè®°å½•åˆ†æï¼Œæ¯”è¾ƒäººç±»å’Œæ¨¡å‹æ’°å†™çš„ä¸åŒæ–‡æœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹è™½èƒ½æ ¹æ®æŒ‡ç¤ºäº§å‡ºå¦‚ç»´åŸºç™¾ç§‘è¯æ¡å’Œå­¦é™¢æ´¾è®ºæ–‡ç­‰ä¸åŒé£æ ¼çš„æ–‡æœ¬ï¼Œä½†ç¦»è¾¾åˆ°ä¸äººç±»å®Œå…¨éš¾ä»¥åŒºåˆ†çš„æ°´å‡†ä»æœ‰å·®è·ã€‚å°¤å…¶æ˜¯åœ¨ä½¿ç”¨ä¸åŒè®°å½•æ—¶ï¼Œæ¨¡å‹å±•ç°çš„å˜ä½“æ›´ä¸ºæœ‰é™ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åå¥½åè¯ï¼Œç›¸è¾ƒäºäººç±»çš„è¯­è¨€å±•ç°å‡ºäº†ä¸åŒçš„è¯­è¨€ä¸»å¹²ç‰¹å¾ã€‚å¤æ‚çš„è¯­æ³•é¢†åŸŸåæ˜ äº†äººç±»ç‹¬æœ‰çš„æ€ç»´å½¢å¼ï¼Œæ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å¤§è€ƒéªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿä¸åŒçš„å†™ä½œé£æ ¼ã€‚</li>
<li>è¯­è¨€æ¨¡å‹å¯ä»¥æ ¹æ®æŒ‡ç¤ºäº§å‡ºä¸åŒé£æ ¼çš„æ–‡æœ¬ï¼Œå¦‚ç»´åŸºç™¾ç§‘è¯æ¡å’Œå­¦é™¢æ´¾è®ºæ–‡ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨é€‚åº”ä¸åŒé£æ ¼æ—¶ï¼Œç¦»è¾¾åˆ°ä¸äººç±»éš¾ä»¥åŒºåˆ†çš„æ°´å‡†ä»æœ‰å·®è·ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸åŒè®°å½•æ—¶å±•ç°çš„å˜ä½“æ›´ä¸ºæœ‰é™ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åå¥½ä½¿ç”¨åè¯ï¼Œç›¸è¾ƒäºäººç±»çš„è¯­è¨€å±•ç°å‡ºäº†ä¸åŒçš„è¯­è¨€ä¸»å¹²ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹åœ¨è¯­æ³•å¤æ‚é¢†åŸŸçš„è¡¨ç°åæ˜ äº†äººå·¥æ™ºèƒ½ä¸äººç±»æ€ç»´çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d676a5f8dfdd199a99a3e06956a1c421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-593fa257f04f15f8e1190a642e66297c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LLMs-that-Understand-Processes-Instruction-tuning-for-Semantics-Aware-Process-Mining"><a href="#LLMs-that-Understand-Processes-Instruction-tuning-for-Semantics-Aware-Process-Mining" class="headerlink" title="LLMs that Understand Processes: Instruction-tuning for Semantics-Aware   Process Mining"></a>LLMs that Understand Processes: Instruction-tuning for Semantics-Aware   Process Mining</h2><p><strong>Authors:Vira Pyrih, Adrian Rebmann, Han van der Aa</strong></p>
<p>Process mining is increasingly using textual information associated with events to tackle tasks such as anomaly detection and process discovery. Such semantics-aware process mining focuses on what behavior should be possible in a process (i.e., expectations), thus providing an important complement to traditional, frequency-based techniques that focus on recorded behavior (i.e., reality). Large Language Models (LLMs) provide a powerful means for tackling semantics-aware tasks. However, the best performance is so far achieved through task-specific fine-tuning, which is computationally intensive and results in models that can only handle one specific task. To overcome this lack of generalization, we use this paper to investigate the potential of instruction-tuning for semantics-aware process mining. The idea of instruction-tuning here is to expose an LLM to prompt-answer pairs for different tasks, e.g., anomaly detection and next-activity prediction, making it more familiar with process mining, thus allowing it to also perform better at unseen tasks, such as process discovery. Our findings demonstrate a varied impact of instruction-tuning: while performance considerably improved on process discovery and prediction tasks, it varies across models on anomaly detection tasks, highlighting that the selection of tasks for instruction-tuning is critical to achieving desired outcomes. </p>
<blockquote>
<p>æµç¨‹æŒ–æ˜è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨ä¸äº‹ä»¶ç›¸å…³çš„æ–‡æœ¬ä¿¡æ¯æ¥å¤„ç†å¼‚å¸¸æ£€æµ‹ã€æµç¨‹å‘ç°ç­‰ä»»åŠ¡ã€‚è¿™ç§è¯­ä¹‰æ„ŸçŸ¥çš„æµç¨‹æŒ–æ˜å…³æ³¨çš„æ˜¯åœ¨æµç¨‹ä¸­å“ªäº›è¡Œä¸ºåº”è¯¥æ˜¯å¯èƒ½çš„ï¼ˆå³æœŸæœ›ï¼‰ï¼Œä»è€Œä¸ºä¸“æ³¨äºè®°å½•è¡Œä¸ºï¼ˆå³ç°å®ï¼‰çš„ä¼ ç»ŸåŸºäºé¢‘ç‡çš„æŠ€æœ¯æä¾›äº†é‡è¦è¡¥å……ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§£å†³è¯­ä¹‰æ„ŸçŸ¥ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„æ‰‹æ®µã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢çš„æœ€ä½³æ€§èƒ½æ˜¯é€šè¿‡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒå®ç°çš„ï¼Œè¿™è®¡ç®—é‡å¤§ï¼Œä¸”ç»“æœæ¨¡å‹åªèƒ½å¤„ç†ä¸€ä¸ªç‰¹å®šä»»åŠ¡ã€‚ä¸ºäº†å…‹æœè¿™ç§ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œæœ¬æ–‡è°ƒæŸ¥äº†æŒ‡ä»¤å¾®è°ƒåœ¨è¯­ä¹‰æ„ŸçŸ¥æµç¨‹æŒ–æ˜ä¸­çš„æ½œåŠ›ã€‚è¿™é‡Œçš„æŒ‡ä»¤å¾®è°ƒçš„æƒ³æ³•æ˜¯ä½¿LLMæš´éœ²äºä¸åŒä»»åŠ¡çš„æç¤º-ç­”æ¡ˆå¯¹ï¼Œä¾‹å¦‚å¼‚å¸¸æ£€æµ‹å’Œä¸‹ä¸€ä¸ªæ´»åŠ¨é¢„æµ‹ï¼Œä½¿å…¶å¯¹æµç¨‹æŒ–æ˜æ›´åŠ ç†Ÿæ‚‰ï¼Œä»è€Œèƒ½å¤Ÿåœ¨æœªçŸ¥ä»»åŠ¡ï¼ˆå¦‚æµç¨‹å‘ç°ï¼‰ä¸Šè¡¨ç°å¾—æ›´å¥½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°æŒ‡ä»¤å¾®è°ƒçš„å½±å“å„ä¸ç›¸åŒï¼šè™½ç„¶æµç¨‹å‘ç°å’Œé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ï¼Œä½†åœ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šä¸åŒæ¨¡å‹ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼Œè¿™å¼ºè°ƒäº†é€‰æ‹©ç”¨äºæŒ‡ä»¤è°ƒæ ¡çš„ä»»åŠ¡å¯¹äºå®ç°é¢„æœŸç»“æœè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16270v1">PDF</a> Accepted at IEEE ICPM 2025, 8 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰æ„ŸçŸ¥æµç¨‹æŒ–æ˜æ­£åœ¨å…´èµ·ã€‚æ­¤æ–¹æ³•ä¾§é‡äºæµç¨‹ä¸­çš„é¢„æœŸè¡Œä¸ºï¼Œä¸ä¼ ç»Ÿçš„åŸºäºé¢‘ç‡çš„æŠ€æœ¯ç›¸ç»“åˆï¼Œæ—¨åœ¨å¤„ç†å¦‚å¼‚å¸¸æ£€æµ‹å’Œæµç¨‹å‘ç°ç­‰ä»»åŠ¡ã€‚ä¸ºæé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†é’ˆå¯¹è¯­ä¹‰æ„ŸçŸ¥æµç¨‹æŒ–æ˜çš„æŒ‡ä»¤å¾®è°ƒæ½œåŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŒ‡ä»¤å¾®è°ƒåœ¨æµç¨‹å‘ç°å’Œé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†åœ¨å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ•ˆæœå› æ¨¡å‹è€Œå¼‚ï¼Œæç¤ºé€‰æ‹©é€‚å½“çš„ä»»åŠ¡è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¯¹å®ç°é¢„æœŸæˆæœè‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµç¨‹æŒ–æ˜æ­£è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨ä¸äº‹ä»¶ç›¸å…³çš„æ–‡æœ¬ä¿¡æ¯æ¥å¤„ç†ä»»åŠ¡ï¼Œå¦‚å¼‚å¸¸æ£€æµ‹å’Œæµç¨‹å‘ç°ã€‚</li>
<li>è¯­ä¹‰æ„ŸçŸ¥æµç¨‹æŒ–æ˜å…³æ³¨æµç¨‹ä¸­çš„é¢„æœŸè¡Œä¸ºï¼Œä¸ä¼ ç»ŸåŸºäºé¢‘ç‡çš„æŠ€æœ¯ç›¸è¾…ç›¸æˆã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå¤„ç†è¯­ä¹‰æ„ŸçŸ¥ä»»åŠ¡æä¾›äº†æœ‰åŠ›æ‰‹æ®µã€‚</li>
<li>ç›®å‰æœ€ä½³æ€§èƒ½æ˜¯é€šè¿‡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒå®ç°çš„ï¼Œä½†è¿™ç§æ–¹æ³•è®¡ç®—é‡å¤§ä¸”æ¨¡å‹åªèƒ½å¤„ç†å•ä¸€ä»»åŠ¡ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒæœ‰åŠ©äºLLMé€‚åº”ä¸åŒçš„ä»»åŠ¡ï¼Œå¦‚å¼‚å¸¸æ£€æµ‹å’Œä¸‹ä¸€æ­¥æ´»åŠ¨é¢„æµ‹ï¼Œä½¿å…¶æ›´åŠ ç†Ÿæ‚‰æµç¨‹æŒ–æ˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤å¾®è°ƒåœ¨æµç¨‹å‘ç°å’Œé¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a99209cc7c924384446a6400c29e1419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d52dcbf05b04eb57eba315050ee25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f3b4bf095429c9cc6ce19bf44108db9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab343dc56c4232ce38f474c731d2c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d89e52495ed8ca6f81d55752399ce2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b5bef9984923123ae0aa3bb57275472.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CYCLE-INSTRUCT-Fully-Seed-Free-Instruction-Tuning-via-Dual-Self-Training-and-Cycle-Consistency"><a href="#CYCLE-INSTRUCT-Fully-Seed-Free-Instruction-Tuning-via-Dual-Self-Training-and-Cycle-Consistency" class="headerlink" title="CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual   Self-Training and Cycle Consistency"></a>CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual   Self-Training and Cycle Consistency</h2><p><strong>Authors:Zhanming Shen, Hao Chen, Yulei Tang, Shaolin Zhu, Wentao Ye, Xiaomeng Hu, Haobo Wang, Gang Chen, Junbo Zhao</strong></p>
<p>Instruction tuning is vital for aligning large language models (LLMs) with human intent, but current methods typically rely on costly human-annotated seed data or powerful external teacher models. While instruction back-translation techniques reduce this dependency, they remain fundamentally tethered to an initial seed set, which limits full automation, introduces biases, and can lead to inefficient use of unlabeled corpora. In this paper, we propose Cycle-Instruct, a novel framework that achieves fully seed-free instruction tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual self-training loop where two models-an answer generator and a question generator-are bootstrapped solely from raw, unlabeled text. These models mutually supervise each other by reconstructing original text segments from their counterpartâ€™s generated pseudo-labels, effectively learning from the intrinsic structure of the data without any human-provided seeds. We demonstrate Cycle-Instructâ€™s efficacy across four diverse data tracks, including general instruction-following, domain-specific tasks, dialogue logs, and plain text. Our extensive experiments show that Cycle-Instruct not only outperforms seed-driven back-translation baselines but also achieves performance comparable to strongly supervised methods. </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»æ„å›¾å¯¹é½è‡³å…³é‡è¦ï¼Œä½†å½“å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ³¨é‡Šç§å­æ•°æ®æˆ–å¼ºå¤§çš„å¤–éƒ¨æ•™å¸ˆæ¨¡å‹ã€‚è™½ç„¶æŒ‡ä»¤åå‘ç¿»è¯‘æŠ€æœ¯å‡å°‘äº†å¯¹æ­¤ç±»æ•°æ®çš„ä¾èµ–ï¼Œä½†å®ƒä»¬ä»ç„¶åŸºæœ¬ä¸Šä¾èµ–äºåˆå§‹ç§å­é›†ï¼Œè¿™é™åˆ¶äº†å…¨è‡ªåŠ¨å¤„ç†è¿‡ç¨‹ã€å¼•å…¥äº†åè§ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´å¯¹æœªæ ‡è®°è¯­æ–™åº“çš„æ— æ•ˆä½¿ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€ç§å­çš„æŒ‡ä»¤è°ƒæ•´æ¡†æ¶Cycle-Instructã€‚å—åˆ°å¾ªç¯ä¸€è‡´æ€§çš„å¯å‘ï¼ŒCycle-Instructé‡‡ç”¨åŒé‡è‡ªè®­ç»ƒå¾ªç¯ï¼Œå…¶ä¸­ä¸¤ä¸ªæ¨¡å‹ï¼ˆç­”æ¡ˆç”Ÿæˆå™¨å’Œé—®é¢˜ç”Ÿæˆå™¨ï¼‰ä»…ä»åŸå§‹æœªæ ‡è®°æ–‡æœ¬ä¸­è¿›è¡Œå¼•å¯¼ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹é€šè¿‡é‡å»ºæ¥è‡ªå¯¹æ–¹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µæ¥ç›¸äº’ç›‘ç£ï¼Œä»è€Œæœ‰æ•ˆåœ°ä»æ•°æ®çš„å†…åœ¨ç»“æ„ä¸­å­¦ä¹ ï¼Œæ— éœ€ä»»ä½•äººç±»æä¾›çš„ç§å­ã€‚æˆ‘ä»¬åœ¨å››ç§ä¸åŒç±»åˆ«çš„æ•°æ®é›†ä¸ŠéªŒè¯äº†Cycle-Instructçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å¸¸è§„æŒ‡ä»¤éµå¾ªä»»åŠ¡ã€ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ã€å¯¹è¯æ—¥å¿—å’Œçº¯æ–‡æœ¬ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCycle-Instructä¸ä»…ä¼˜äºåŸºäºç§å­çš„åå‘ç¿»è¯‘åŸºçº¿æ¨¡å‹ï¼Œè€Œä¸”å…¶æ€§èƒ½ä¸ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16100v1">PDF</a> EMNLP 2025 Main</p>
<p><strong>Summary</strong><br>åœ¨ç¼ºä¹ç§å­æŒ‡å¯¼çš„æƒ…å†µä¸‹å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»æ„å›¾çš„å¯¹é½æ˜¯é‡è¦ä¸”å›°éš¾çš„ã€‚å½“å‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„äººåŠ›æ ‡æ³¨ç§å­æ•°æ®æˆ–å¼ºå¤§çš„å¤–éƒ¨æ•™å¸ˆæ¨¡å‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶Cycle-Instructï¼Œå®ç°æ— ç§å­æŒ‡å¯¼çš„æŒ‡ä»¤è°ƒæ•´ã€‚å—å¾ªç¯ä¸€è‡´æ€§çš„å¯å‘ï¼ŒCycle-Instructé‡‡ç”¨åŒè‡ªè®­ç»ƒå¾ªç¯ï¼Œä»…é€šè¿‡åŸå§‹æœªæ ‡è®°æ–‡æœ¬å¯åŠ¨ä¸¤ä¸ªæ¨¡å‹â€”â€”ç­”æ¡ˆç”Ÿæˆå™¨å’Œé—®é¢˜ç”Ÿæˆå™¨ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹é€šè¿‡é‡å»ºå¯¹æ–¹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹åº”çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µæ¥ç›¸äº’ç›‘ç£ï¼Œä»è€Œä»æ•°æ®å†…åœ¨ç»“æ„å­¦ä¹ ï¼Œæ— éœ€ä»»ä½•äººå·¥æä¾›çš„ç§å­ã€‚å®éªŒè¯æ˜ï¼ŒCycle-Instructåœ¨å››ç§ä¸åŒæ•°æ®è½¨è¿¹ä¸Šçš„è¡¨ç°ä¼˜äºç§å­é©±åŠ¨çš„å›è¯‘åŸºçº¿ï¼Œå¹¶ä¸”ä¸å¼ºç›‘ç£æ–¹æ³•çš„è¡¨ç°ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤è°ƒæ•´å¯¹äºå®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»æ„å›¾çš„åŒ¹é…è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºç§å­æ•°æ®æˆ–å¤–éƒ¨æ•™å¸ˆæ¨¡å‹ï¼Œè¿™å¢åŠ äº†æˆæœ¬å¹¶é™åˆ¶äº†è‡ªåŠ¨åŒ–ã€‚</li>
<li>Cycle-Instructæ¡†æ¶é€šè¿‡åŒè‡ªè®­ç»ƒå¾ªç¯å®ç°æ— ç§å­æŒ‡å¯¼çš„æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>è¯¥æ¡†æ¶ä»…ä½¿ç”¨åŸå§‹æœªæ ‡è®°æ–‡æœ¬å¯åŠ¨ä¸¤ä¸ªæ¨¡å‹ï¼šç­”æ¡ˆç”Ÿæˆå™¨å’Œé—®é¢˜ç”Ÿæˆå™¨ã€‚</li>
<li>ä¸¤ä¸ªæ¨¡å‹é€šè¿‡é‡å»ºå¯¹æ–¹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹åº”çš„åŸå§‹æ–‡æœ¬ç‰‡æ®µæ¥ç›¸äº’ç›‘ç£å­¦ä¹ ã€‚</li>
<li>Cycle-Instructæ— éœ€ä»»ä½•äººå·¥æä¾›çš„ç§å­ï¼Œèƒ½ä»æ•°æ®çš„å†…åœ¨ç»“æ„å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b3423430692257b44e1bb3c7e9e2fb21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9df2ae4040b709f2f21f644663c14e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529b41811621177fa6055eaa1ddc3687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9bdeabbb0e9b22f8e49b699739cb9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c47b1b3be7191382a59e6bb9fb547f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Whoâ€™s-Asking-Investigating-Bias-Through-the-Lens-of-Disability-Framed-Queries-in-LLMs"><a href="#Whoâ€™s-Asking-Investigating-Bias-Through-the-Lens-of-Disability-Framed-Queries-in-LLMs" class="headerlink" title="Whoâ€™s Asking? Investigating Bias Through the Lens of Disability Framed   Queries in LLMs"></a>Whoâ€™s Asking? Investigating Bias Through the Lens of Disability Framed   Queries in LLMs</h2><p><strong>Authors:Srikant Panda, Vishnu Hari, Kalpana Panda, Amit Agarwal, Hitesh Laxmichand Patel</strong></p>
<p>Large Language Models (LLMs) routinely infer users demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remains largely uncharted. Thus, we present the first systematic audit of disability-conditioned demographic bias across eight state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters. Using a balanced template corpus that pairs nine disability categories with six real-world business domains, we prompt each model to predict five demographic attributes - gender, socioeconomic status, education, cultural background, and locality - under both neutral and disability-aware conditions.   Across a varied set of prompts, models deliver a definitive demographic guess in up to 97% of cases, exposing a strong tendency to make arbitrary inferences with no clear justification. Disability context heavily shifts predicted attribute distributions, and domain context can further amplify these deviations. We observe that larger models are simultaneously more sensitive to disability cues and more prone to biased reasoning, indicating that scale alone does not mitigate stereotype amplification.   Our findings reveal persistent intersections between ableism and other demographic stereotypes, pinpointing critical blind spots in current alignment strategies. We release our evaluation framework and results to encourage disability-inclusive benchmarking and recommend integrating abstention calibration and counterfactual fine-tuning to curb unwarranted demographic inference. Code and data will be released on acceptance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ä»…é€šè¿‡æªè¾å°±èƒ½æ¨æ–­å‡ºç”¨æˆ·çš„äººå£ç»Ÿè®¡ç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¼è‡´å‡ºç°åè§å“åº”ï¼Œå³ä½¿æœªæä¾›æ˜ç¡®çš„äººå£ç»Ÿè®¡ä¿¡æ¯ã€‚æ®‹ç–¾çº¿ç´¢åœ¨å¡‘é€ è¿™äº›æ¨æ–­ä¸­çš„ä½œç”¨ä»ç„¶å¤§éƒ¨åˆ†æœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´å‹LLMè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„å®¡è®¡ï¼Œè¿™äº›æ¨¡å‹çš„å‚æ•°èŒƒå›´ä»3Båˆ°72Bã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¹³è¡¡çš„æ¨¡æ¿è¯­æ–™åº“ï¼Œå°†ä¹ä¸ªæ®‹ç–¾ç±»åˆ«ä¸å…­ä¸ªç°å®ä¸–ç•Œå•†ä¸šé¢†åŸŸç›¸åŒ¹é…ï¼Œæç¤ºæ¯ä¸ªæ¨¡å‹åœ¨ä¸­æ€§å’Œæ®‹ç–¾æ„è¯†ä¸¤ç§æƒ…å†µä¸‹é¢„æµ‹äº”ä¸ªäººå£ç»Ÿè®¡å±æ€§â€”â€”æ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ã€æ•™è‚²ã€æ–‡åŒ–èƒŒæ™¯å’Œæ‰€åœ¨åœ°ã€‚åœ¨å„ç§å„æ ·çš„æç¤ºä¸‹ï¼Œæ¨¡å‹åœ¨é«˜è¾¾97%çš„æƒ…å†µä¸‹åšå‡ºäº†æ˜ç¡®çš„äººå£ç»Ÿè®¡çŒœæµ‹ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„å€¾å‘ï¼Œåšå‡ºä»»æ„æ¨æ–­è€Œæ²¡æœ‰æ˜ç¡®çš„ä¾æ®ã€‚æ®‹ç–¾èƒŒæ™¯æå¤§åœ°æ”¹å˜äº†é¢„æµ‹å±æ€§åˆ†å¸ƒï¼Œè€Œé¢†åŸŸèƒŒæ™¯å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™äº›åå·®ã€‚æˆ‘ä»¬å‘ç°æ›´å¤§çš„æ¨¡å‹å¯¹æ®‹ç–¾çº¿ç´¢æ›´ä¸ºæ•æ„Ÿï¼ŒåŒæ—¶æ›´å®¹æ˜“å‡ºç°åè§æ¨ç†ï¼Œè¿™è¡¨æ˜è§„æ¨¡æœ¬èº«å¹¶ä¸èƒ½å‡è½»åˆ»æ¿å°è±¡çš„æ”¾å¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†æŒç»­å­˜åœ¨çš„å¯¹æ®‹ç–¾å’Œå…¶ä»–äººå£ç»Ÿè®¡ç‰¹å¾çš„åˆ»æ¿å°è±¡ä¹‹é—´çš„äº¤é›†ï¼ŒæŒ‡å‡ºäº†å½“å‰å¯¹é½ç­–ç•¥ä¸­çš„å…³é”®ç›²ç‚¹ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶å’Œç»“æœï¼Œä»¥é¼“åŠ±åŒ…å®¹æ®‹ç–¾çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å»ºè®®æ•´åˆå¼ƒæƒæ ¡å‡†å’Œå‡è®¾å¾®è°ƒæ¥éåˆ¶ä¸å¿…è¦çš„äººå£ç»Ÿè®¡æ¨æ–­ã€‚è®ºæ–‡æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¸ƒä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15831v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»…é€šè¿‡æªè¾å°±èƒ½æ¨æ–­ç”¨æˆ·çš„äººå£ç‰¹å¾ï¼Œå¯èƒ½å¯¼è‡´åè§å“åº”ï¼Œå³ä½¿æœªæä¾›æ˜ç¡®çš„äººå£ä¿¡æ¯ã€‚å¯¹äºæ®‹ç–¾çº¿ç´¢åœ¨å½¢æˆè¿™äº›æ¨æ–­ä¸­çš„ä½œç”¨ä»çŸ¥ä¹‹ç”šå°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´å‹LLMè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿçš„å®¡è®¡ï¼Œå‚æ•°èŒƒå›´ä»3Båˆ°72Bã€‚é€šè¿‡ä½¿ç”¨åŒ…å«ä¹ä¸ªæ®‹ç–¾ç±»åˆ«ä¸å…­ä¸ªçœŸå®ä¸šåŠ¡é¢†åŸŸçš„å¹³è¡¡æ¨¡æ¿è¯­æ–™åº“ï¼Œæˆ‘ä»¬æç¤ºæ¨¡å‹é¢„æµ‹äº”ä¸ªäººå£ç‰¹å¾â€”â€”æ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ã€æ•™è‚²ã€æ–‡åŒ–èƒŒæ™¯å’Œåœ°ç‚¹â€”â€”åœ¨ä¸­æ€§ç¯å¢ƒå’Œæ®‹ç–¾æ„è¯†ç¯å¢ƒä¸‹éƒ½æ˜¯å¦‚æ­¤ã€‚åœ¨å„ç§æç¤ºä¸‹ï¼Œæ¨¡å‹åœ¨é«˜è¾¾97%çš„æƒ…å†µä¸‹ç»™å‡ºäº†æ˜ç¡®çš„äººå£ç‰¹å¾çŒœæµ‹ï¼Œè¡¨ç°å‡ºå¼ºçƒˆçš„å€¾å‘æ€§ï¼Œåšå‡ºä»»æ„æ¨æ–­è€Œæ²¡æœ‰æ˜ç¡®çš„ä¾æ®ã€‚æ®‹ç–¾èƒŒæ™¯ä¼šæå¤§åœ°æ”¹å˜é¢„æµ‹å±æ€§åˆ†å¸ƒï¼Œä¸šåŠ¡èƒŒæ™¯å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¾å¤§è¿™äº›åå·®ã€‚æˆ‘ä»¬å‘ç°æ›´å¤§çš„æ¨¡å‹å¯¹æ®‹ç–¾çº¿ç´¢æ›´æ•æ„Ÿï¼ŒåŒæ—¶æ›´å®¹æ˜“å‡ºç°åè§æ¨ç†ï¼Œè¿™è¡¨æ˜è§„æ¨¡æœ¬èº«å¹¶ä¸èƒ½ç¼“è§£åˆ»æ¿å°è±¡çš„æ”¾å¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†éšæ€§çš„èƒ½åŠ›ä¸»ä¹‰ä¸å…¶ä»–äººå£ç‰¹å¾åˆ»æ¿å°è±¡çš„æŒç»­äº¤é›†ï¼ŒæŒ‡å‡ºäº†å½“å‰å¯¹é½ç­–ç•¥çš„å…³é”®ç›²ç‚¹ã€‚æˆ‘ä»¬å‘å¸ƒè¯„ä¼°æ¡†æ¶å’Œç»“æœï¼Œä»¥é¼“åŠ±åŒ…å®¹æ®‹ç–¾çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å»ºè®®æ•´åˆæ”¾å¼ƒæ ¡å‡†å’Œåäº‹å®å¾®è°ƒæ¥éåˆ¶ä¸å¿…è¦çš„äººå£æ¨æ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsèƒ½å¤Ÿä»…é€šè¿‡æªè¾æ¨æ–­ç”¨æˆ·çš„äººå£ç‰¹å¾ï¼ŒåŒ…æ‹¬æ€§åˆ«ã€ç¤¾ä¼šç»æµåœ°ä½ç­‰ã€‚</li>
<li>æ®‹ç–¾çº¿ç´¢åœ¨LLMçš„äººå£ç‰¹å¾æ¨æ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä¸”å¯¹æ¨¡å‹çš„é¢„æµ‹äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>LLMsåœ¨é¢„æµ‹äººå£ç‰¹å¾æ—¶å­˜åœ¨åè§ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ˜ç¡®äººå£ä¿¡æ¯çš„æƒ…å†µä¸‹ä¹Ÿä¼šåšå‡ºä»»æ„æ¨æ–­ã€‚</li>
<li>ä¸åŒçš„ä¸šåŠ¡èƒŒæ™¯å’Œé¢†åŸŸä¼šå½±å“LLMså¯¹äººå£ç‰¹å¾çš„é¢„æµ‹ã€‚</li>
<li>æ›´å¤§çš„LLMæ¨¡å‹å¯¹æ®‹ç–¾çº¿ç´¢æ›´æ•æ„Ÿï¼Œä¸”æ›´å®¹æ˜“å—åˆ°åè§çš„å½±å“ã€‚</li>
<li>å½“å‰LLMçš„å¯¹é½ç­–ç•¥å­˜åœ¨å…³é”®ç›²ç‚¹ï¼Œéœ€è¦æ›´åŠ æ³¨æ„éšæ€§çš„èƒ½åŠ›ä¸»ä¹‰å’Œå…¶ä»–äººå£ç‰¹å¾åˆ»æ¿å°è±¡çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4138002bb4073b12bf7142426a5833c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-531e4df2c32cbbd6f1974d6b98bb1c98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d83bcaa2fbf142bd75f82b2decd5b28e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a9fe8ae6a94bac6aafd68f03a3a0bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f60b5c0443f9f73cac16ccd13fc230b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73ff7e8187485c18a4360eff5274022d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds"><a href="#MeshCoder-LLM-Powered-Structured-Mesh-Code-Generation-from-Point-Clouds" class="headerlink" title="MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds"></a>MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds</h2><p><strong>Authors:Bingquan Dai, Li Ray Luo, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, Minghan Qin, Xudong Xu, Bo Dai, Haoqian Wang, Zhaoyang Lyu, Jiangmiao Pang</strong></p>
<p>Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding. The project homepage is available at \href{<a target="_blank" rel="noopener" href="https://daibingquan.github.io/MeshCoder%7D%7Bthis">https://daibingquan.github.io/MeshCoder}{this</a> link}. </p>
<blockquote>
<p>å°†3Då¯¹è±¡é‡å»ºä¸ºå¯ç¼–è¾‘çš„ç¨‹åºåœ¨é€†å‘å·¥ç¨‹å’Œå½¢çŠ¶ç¼–è¾‘ç­‰åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæœ‰é™çš„ç‰¹å®šé¢†åŸŸè¯­è¨€ï¼ˆDSLï¼‰å’Œå°è§„æ¨¡æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹å¤æ‚å‡ ä½•å’Œç»“æ„çš„å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MeshCoderï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†å¤æ‚çš„3Då¯¹è±¡ä»ç‚¹äº‘é‡å»ºä¸ºå¯ç¼–è¾‘çš„Blender Pythonè„šæœ¬ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å…¨é¢çš„ã€è¡¨è¾¾æ€§å¼ºçš„Blender Python APIï¼Œèƒ½å¤Ÿåˆæˆå¤æ‚çš„å‡ ä½•å½¢çŠ¶ã€‚åˆ©ç”¨è¿™äº›APIï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¯¹è±¡-ä»£ç é…å¯¹æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªå¯¹è±¡çš„ä»£ç è¢«åˆ†è§£æˆä¸åŒçš„è¯­ä¹‰éƒ¨åˆ†ã€‚éšåï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå°†3Dç‚¹äº‘ç¿»è¯‘æˆå¯æ‰§è¡Œçš„Blender Pythonè„šæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å½¢çŠ¶åˆ°ä»£ç çš„é‡å»ºä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè€Œä¸”é€šè¿‡æ–¹ä¾¿çš„ä»£ç ä¿®æ”¹ï¼Œä¿ƒè¿›äº†ç›´è§‚å‡ ä½•å’Œæ‹“æ‰‘ç¼–è¾‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŸºäºä»£ç çš„è¡¨ç¤ºå½¢å¼å¢å¼ºäº†LLMåœ¨3Då½¢çŠ¶ç†è§£ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¿™äº›è´¡çŒ®å…±åŒç¡®ç«‹äº†MeshCoderåœ¨ç¨‹åºåŒ–3Då½¢çŠ¶é‡å»ºå’Œç†è§£æ–¹é¢çš„å¼ºå¤§å’Œçµæ´»è§£å†³æ–¹æ¡ˆçš„åœ°ä½ã€‚é¡¹ç›®ä¸»é¡µå¯åœ¨<a target="_blank" rel="noopener" href="https://daibingquan.github.io/MeshCoder">æ­¤é“¾æ¥</a>æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14879v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹é€†å‘å·¥ç¨‹å’Œå½¢çŠ¶ç¼–è¾‘ç­‰åº”ç”¨ï¼ŒMeshCoderæ¡†æ¶å®ç°äº†ä»ç‚¹äº‘é‡å»ºå¤æ‚ä¸‰ç»´ç‰©ä½“åˆ°å¯ç¼–è¾‘çš„Blender Pythonè„šæœ¬çš„åˆ›æ–°æŠ€æœ¯ã€‚é€šè¿‡å¼€å‘ä¸€å¥—è¡¨è¾¾æ€§å¼ºçš„Blender Python APIï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆæˆç²¾ç»†å‡ ä½•ç»“æ„ï¼Œå¹¶å»ºç«‹å¤§è§„æ¨¡ç‰©ä½“å¯¹è±¡ä»£ç æ•°æ®é›†ã€‚åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°ä»ç‚¹äº‘åˆ°å¯æ‰§è¡ŒBlender Pythonè„šæœ¬çš„ç¿»è¯‘ã€‚æ­¤æ–¹æ³•ä¸ä»…å®ç°äº†å½¢çŠ¶åˆ°ä»£ç çš„ä¼˜è¶Šé‡å»ºæ€§èƒ½ï¼Œè€Œä¸”é€šè¿‡æ–¹ä¾¿çš„ä»£ç ä¿®æ”¹ä¿ƒè¿›äº†ç›´è§‚çš„å‡ ä½•å’Œæ‹“æ‰‘ç¼–è¾‘ã€‚æ­¤å¤–ï¼ŒåŸºäºä»£ç çš„è¡¨è¾¾æ–¹å¼å¢å¼ºäº†LLMåœ¨ä¸‰ç»´å½¢çŠ¶ç†è§£ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚è¿™äº›è´¡çŒ®ä½¿MeshCoderæˆä¸ºå¼ºå¤§çš„å¯ç¼–ç¨‹ä¸‰ç»´å½¢çŠ¶é‡å»ºå’Œç†è§£è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MeshCoderæ˜¯ä¸€ä¸ªä»ç‚¹äº‘é‡å»ºå¤æ‚ä¸‰ç»´ç‰©ä½“çš„åˆ›æ–°æ¡†æ¶ï¼Œè¾“å‡ºä¸ºå¯ç¼–è¾‘çš„Blender Pythonè„šæœ¬ã€‚</li>
<li>é€šè¿‡å¼€å‘ä¸€å¥—è¡¨è¾¾æ€§å¼ºçš„Blender Python APIï¼Œå®ç°äº†ç²¾ç»†å‡ ä½•ç»“æ„çš„åˆæˆã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡ç‰©ä½“å¯¹è±¡ä»£ç æ•°æ®é›†ï¼Œæ•°æ®é›†ä¸­ç‰©ä½“çš„ä»£ç è¢«åˆ†è§£ä¸ºä¸åŒçš„è¯­ä¹‰éƒ¨åˆ†ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ä»ç‚¹äº‘åˆ°å¯æ‰§è¡Œè„šæœ¬çš„ç¿»è¯‘ã€‚</li>
<li>MeshCoderä¸ä»…å®ç°äº†é«˜æ•ˆçš„å½¢çŠ¶åˆ°ä»£ç é‡å»ºï¼Œè¿˜ä¾¿äºé€šè¿‡ä»£ç ä¿®æ”¹è¿›è¡Œç›´è§‚çš„å‡ ä½•å’Œæ‹“æ‰‘ç¼–è¾‘ã€‚</li>
<li>åŸºäºä»£ç çš„è¡¨è¾¾æ–¹å¼å¢å¼ºäº†LLMåœ¨ä¸‰ç»´å½¢çŠ¶ç†è§£ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MeshCoderä¸ºä¸‰ç»´å½¢çŠ¶é‡å»ºå’Œç†è§£æä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”çµæ´»çš„ç¨‹åºåŒ–è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-18df71759adb8a500882bcf1b879be06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5875f68f8bb45757c7c06672425ce08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f323297888cac2b5987b45aba457042.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3397a6a3f7ce1dc5fdaa3e1120d88d4a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS"><a href="#Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS" class="headerlink" title="Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS"></a>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS</h2><p><strong>Authors:Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</strong></p>
<p>Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿„ä»Šä¸ºæ­¢ä¸»è¦åˆ†ä¸ºä¸¤ç§æˆªç„¶ä¸åŒçš„èŒƒå¼ï¼šï¼ˆ1ï¼‰ä¼˜åŒ–åŸºäºç¨€ç–ç»“æœçš„å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œå°½ç®¡å­˜åœ¨ä¸ç¨³å®šæ€§å’Œä½æ ·æœ¬æ•ˆç‡çš„é—®é¢˜ï¼›ï¼ˆ2ï¼‰ç”±ç‹¬ç«‹è®­ç»ƒçš„é™æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¼•å¯¼çš„çš„æœç´¢æŠ€æœ¯ï¼Œè¿™éœ€è¦æ˜‚è´µçš„äººæˆ–LLMç”Ÿæˆçš„æ ‡ç­¾ï¼Œå¹¶ä¸”åœ¨åˆ†å¸ƒå˜åŒ–çš„æƒ…å†µä¸‹ç»å¸¸ä¼šé€€åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†AIRL-Sï¼Œè¿™æ˜¯åŸºäºRLå’Œæœç´¢çš„TTSçš„é¦–ä¸ªè‡ªç„¶ç»Ÿä¸€ã€‚AIRL-Sçš„æ ¸å¿ƒè§è§£æ˜¯ï¼Œåœ¨RLè®­ç»ƒæœŸé—´å­¦ä¹ çš„å¥–åŠ±å‡½æ•°æœ¬è´¨ä¸Šä»£è¡¨äº†ç”¨äºæŒ‡å¯¼ä¸‹æ¸¸æœç´¢çš„ç†æƒ³PRMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å¯¹æŠ—æ€§é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¯†é›†ã€åŠ¨æ€çš„PRMï¼Œä»è€Œå®Œå…¨æ¶ˆé™¤äº†å¯¹æ ‡è®°çš„ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚åœ¨æ¨ç†æ—¶ï¼Œæ‰€å¾—çš„PRMåŒæ—¶ä½œä¸ºRLæ¼”ç»ƒçš„è¯„åˆ¤æ ‡å‡†å’Œæœ‰æ•ˆæŒ‡å¯¼æœç´¢ç¨‹åºçš„å¯å‘å¼æ–¹æ³•ï¼Œä¿ƒè¿›ç¨³å¥çš„æ¨ç†é“¾æ‰©å±•ï¼Œç¼“è§£å¥–åŠ±ç ´è§£ï¼Œå¹¶å¢å¼ºè·¨ä»»åŠ¡æ³›åŒ–ã€‚åœ¨åŒ…æ‹¬æ•°å­¦ã€ç§‘å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰åœ¨å†…çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ–¹æ³•å¹³å‡æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†9%çš„æ€§èƒ½ï¼Œä¸GPT-4oç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œå½“é›†æˆåˆ°å¤šç§æœç´¢ç®—æ³•ä¸­æ—¶ï¼Œæˆ‘ä»¬çš„PRMå§‹ç»ˆä¼˜äºä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒçš„æ‰€æœ‰åŸºçº¿PRMã€‚è¿™äº›ç»“æœå¼ºè°ƒï¼Œå®é™…ä¸Šï¼Œæ‚¨çš„RLå¥–åŠ±å‡½æ•°æ˜¯æ‚¨æœ€ä½³çš„æœç´¢PRMï¼Œä¸ºLLMä¸­çš„å¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”ç»æµçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14313v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæœç´¢æ–¹æ³•çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼ˆAIRL-Sï¼‰ï¼Œè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´çš„éš¾é¢˜ã€‚ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ç¨€ç–å¥–åŠ±å‡½æ•°ä¼˜åŒ–å­˜åœ¨ä¸ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œè€ŒåŸºäºæœç´¢çš„æ–¹æ³•éœ€è¦ç‹¬ç«‹è®­ç»ƒçš„é™æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ï¼Œå¾€å¾€å­˜åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹é™çš„æƒ…å†µã€‚æœ¬æ–‡åˆ›æ–°ç‚¹åœ¨äºå¼•å…¥äº†å¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå­¦ä¹ ä¸€ä¸ªå¯†é›†çš„åŠ¨æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹æ ‡è®°ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†9%çš„æ€§èƒ½ï¼Œå¹¶æˆåŠŸé›†æˆåˆ°å¤šç§æœç´¢ç®—æ³•ä¸­ï¼Œä¼˜äºæ‰€æœ‰ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒçš„åŸºå‡†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚è¿™è¡¨æ˜å¥–åŠ±å‡½æ•°æ˜¯æœç´¢çš„æœ€ä½³è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä¸ºå¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”ç»æµçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•AIRL-Sï¼Œç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œæœç´¢æ–¹æ³•ã€‚</li>
<li>è§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡é—®é¢˜ä»¥åŠåŸºäºæœç´¢çš„æ–¹æ³•ä¾èµ–é™æ€å¥–åŠ±æ¨¡å‹çš„ç¼ºé™·ã€‚</li>
<li>é€šè¿‡å¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å­¦ä¹ å¯†é›†çš„åŠ¨æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>æˆåŠŸé›†æˆåˆ°å¤šç§æœç´¢ç®—æ³•ä¸­ï¼Œå¹¶ä¼˜äºä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒçš„åŸºå‡†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6305645bd1561078e02d5d498f5e26dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f57c37203deab202935d2ac2bd89c171.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Cyberbullying-Detection-via-Aggression-Enhanced-Prompting"><a href="#Cyberbullying-Detection-via-Aggression-Enhanced-Prompting" class="headerlink" title="Cyberbullying Detection via Aggression-Enhanced Prompting"></a>Cyberbullying Detection via Aggression-Enhanced Prompting</h2><p><strong>Authors:Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</strong></p>
<p>Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks. </p>
<blockquote>
<p>æ£€æµ‹ç¤¾äº¤åª’ä½“ä¸Šçš„ç½‘ç»œæ¬ºå‡Œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç½‘ç»œæ¬ºå‡Œçš„è¡¨è¾¾æ–¹å¼å¾ˆå¾®å¦™ä¸”å¤šç§å¤šæ ·ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶åœ¨ç»Ÿä¸€è®­ç»ƒæ¡†æ¶å†…å°†æ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡æ•´åˆï¼Œæ˜¯å¦èƒ½æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚ç ”ç©¶ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹äº”ä¸ªæ”»å‡»æ•°æ®é›†å’Œä¸€ä¸ªç½‘ç»œæ¬ºå‡Œæ•°æ®é›†è¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§ç­–ç•¥ï¼šé›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€ç‹¬ç«‹LoRAå¾®è°ƒä»¥åŠå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ã€‚é‰´äºå¤šä»»åŠ¡å­¦ä¹ çš„ç»“æœä¸ä¸€è‡´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸°å¯Œçš„æç¤ºç®¡é“æ–¹æ³•ï¼Œå…¶ä¸­æ”»å‡»é¢„æµ‹è¢«åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­ä»¥æä¾›ä¸Šä¸‹æ–‡å¢å¼ºã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸°å¯Œçš„æç¤ºç®¡é“å§‹ç»ˆä¼˜äºæ ‡å‡†çš„LoRAå¾®è°ƒï¼Œè¿™è¡¨æ˜æ”»å‡»æ£€æµ‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯å¤§å¤§å¢å¼ºç½‘ç»œæ¬ºå‡Œæ£€æµ‹ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è¾…åŠ©ä»»åŠ¡ï¼ˆå¦‚æ”»å‡»æ£€æµ‹ï¼‰çš„æ½œåŠ›ï¼Œå¯ä»¥æ”¹è¿›LLMåœ¨ç¤¾äº¤ç½‘ç»œå®‰å…¨å…³é”®åº”ç”¨çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06360v2">PDF</a> Accepted to RANLP 2025</p>
<p><strong>Summary</strong><br>ç¤¾äº¤åª’ä½“ä¸Šçš„ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºæ£€æµ‹æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› å…¶è¡¨è¾¾æ–¹å¼å¤šæ ·ä¸”éšè”½ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¸€ä¸ªç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶å†…æ•´åˆæ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œæ˜¯å¦èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„é€šç”¨æ€§å’Œæ€§èƒ½ã€‚å®éªŒé‡‡ç”¨äº”ä¸ªæ”»å‡»æ•°æ®é›†å’Œä¸€ä¸ªç½‘ç»œæ¬ºå‡Œæ•°æ®é›†ï¼Œå¯¹LLMè¿›è¡Œäº†æŒ‡ä»¤è°ƒä¼˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€ç‹¬ç«‹LoRAå¾®è°ƒä»¥åŠå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ç­‰å¤šç§ç­–ç•¥ã€‚é‰´äºMTLçš„ä¸ä¸€è‡´ç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸°å¯Œçš„æç¤ºç®¡é“æ–¹æ³•ï¼Œå…¶ä¸­æ”»å‡»é¢„æµ‹è¢«åµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­ä»¥æä¾›ä¸Šä¸‹æ–‡å¢å¼ºã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸°å¯Œçš„æç¤ºç®¡é“å§‹ç»ˆä¼˜äºæ ‡å‡†çš„LoRAå¾®è°ƒï¼Œè¡¨æ˜æ”»å‡»ä¿¡æ¯ä¸Šä¸‹æ–‡å¯¹ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è¾…åŠ©ä»»åŠ¡ï¼ˆå¦‚æ”»å‡»æ£€æµ‹ï¼‰åœ¨ç¤¾äº¤åª’ä½“å®‰å…¨å…³é”®åº”ç”¨ä¸­æé«˜LLMé€šç”¨æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç½‘ç»œæ¬ºå‡Œè¡Œä¸ºåœ¨ç¤¾äº¤åª’ä½“ä¸Šçš„æ£€æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› å…¶è¡¨è¾¾æ–¹å¼å¤šæ ·ä¸”éšè”½ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†åœ¨ç»Ÿä¸€çš„è®­ç»ƒæ¡†æ¶å†…æ•´åˆæ”»å‡»æ£€æµ‹ä½œä¸ºè¾…åŠ©ä»»åŠ¡ï¼Œä»¥æé«˜LLMåœ¨ç½‘ç»œæ¬ºå‡Œæ£€æµ‹ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒé‡‡ç”¨äº†å¤šä¸ªæ•°æ®é›†å¹¶å¯¹LLMè¿›è¡Œäº†æŒ‡ä»¤è°ƒä¼˜ï¼Œè¯„ä¼°äº†ä¸åŒç­–ç•¥ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ çš„ç»“æœä¸ä¸€è‡´ï¼Œæå‡ºäº†ä¸€ç§ä¸°å¯Œçš„æç¤ºç®¡é“æ–¹æ³•ï¼Œå…¶ä¸­æ”»å‡»é¢„æµ‹ä½œä¸ºä¸Šä¸‹æ–‡å¢å¼ºåµŒå…¥åˆ°ç½‘ç»œæ¬ºå‡Œæ£€æµ‹æç¤ºä¸­ã€‚</li>
<li>åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸°å¯Œçš„æç¤ºç®¡é“æ–¹æ³•ä¼˜äºæ ‡å‡†å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>æ”»å‡»ä¿¡æ¯ä¸Šä¸‹æ–‡å¯¹ç½‘ç»œæ¬ºå‡Œæ£€æµ‹çš„æ”¹å–„æœ‰é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d9fdc1491b40969373dc8bf568ab4f19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28d984c26a74d681aa4b4117dfa35a50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d65f73996b250f0e66b84d6ac7b3bf1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Simple-â€œTry-Againâ€-Can-Elicit-Multi-Turn-LLM-Reasoning"><a href="#A-Simple-â€œTry-Againâ€-Can-Elicit-Multi-Turn-LLM-Reasoning" class="headerlink" title="A Simple â€œTry Againâ€ Can Elicit Multi-Turn LLM Reasoning"></a>A Simple â€œTry Againâ€ Can Elicit Multi-Turn LLM Reasoning</h2><p><strong>Authors:Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li</strong></p>
<p>Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., â€œLetâ€™s try againâ€) after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: <a target="_blank" rel="noopener" href="https://github.com/lichengliu03/unary-feedback">https://github.com/lichengliu03/unary-feedback</a> </p>
<blockquote>
<p>å¤šè½®é—®é¢˜æ±‚è§£å¯¹äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰æ¥è¯´æ˜¯è‡³å…³é‡è¦çš„ï¼Œä½†ä¹Ÿé¢‡å…·æŒ‘æˆ˜æ€§ï¼Œè¦æ±‚å®ƒä»¬èƒ½å¤Ÿåæ€è‡ªå·±çš„æ¨ç†å¹¶æ ¹æ®åé¦ˆè¿›è¡Œä¿®æ”¹ã€‚ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•æ˜¯åœ¨å•è½®èŒƒå¼ä¸Šè®­ç»ƒå¤§å‹æ¨ç†æ¨¡å‹ï¼Œå¹¶è¾…ä»¥å¯éªŒè¯çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé‡‡ç”¨ç°æœ‰RLèŒƒå¼è®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¸§å¤±äº†å¤šè½®é—®é¢˜æ±‚è§£çš„èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨åŸºäºä¸Šä¸‹æ–‡åé¦ˆä¿®æ­£ç­”æ¡ˆæ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´é‡å¤å“åº”ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šLRMèƒ½å¦åœ¨å¤šè½®æƒ…å¢ƒä¸‹å­¦ä¼šåæ€è‡ªå·±çš„ç­”æ¡ˆï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä»…åœ¨é”™è¯¯ç­”æ¡ˆåæä¾›ä¸€å…ƒåé¦ˆï¼ˆä¾‹å¦‚ï¼Œâ€œè®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡â€ï¼‰ï¼Œå¯ä»¥æé«˜å•è½®æ€§èƒ½å’Œå¤šè½®æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºå¼ºåŒ–å­¦ä¹ å¼•å…¥äº†ä½œä¸ºè§‚å¯Ÿçš„ä¸€å…ƒåé¦ˆï¼ˆUFOï¼‰ï¼Œåœ¨è¿­ä»£é—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ä½¿ç”¨æœ€å°‘ä½†å¸¸è§çš„ä¸€å…ƒç”¨æˆ·åé¦ˆã€‚å®ƒå¯ä»¥è½»æ¾åº”ç”¨äºç°æœ‰çš„å•è½®RLè®­ç»ƒè®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨UFOçš„RLè®­ç»ƒä¿æŒäº†å•è½®æ€§èƒ½ï¼Œå¹¶é€šè¿‡æé«˜å¤šè½®æ¨ç†å‡†ç¡®æ€§æœ€å¤šè¾¾14%ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šè½®é—®é¢˜æ±‚è§£ä¸­æ›´å¥½åœ°åº”å¯¹åé¦ˆã€‚ä¸ºäº†å°½é‡å‡å°‘å¾—åˆ°æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„è½®æ•°ï¼Œå¹¶åœ¨å‡ºç°é”™è¯¯æ—¶é¼“åŠ±å¤šæ ·åŒ–çš„æ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¥–åŠ±ç»“æ„ï¼Œä»¥å¼•å¯¼æ¨¡å‹åœ¨æ¯è½®ä¸­ç»™å‡ºè°¨æ…å’Œæ…é‡çš„ç­”æ¡ˆã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/lichengliu03/unary-feedback">https://github.com/lichengliu03/unary-feedback</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14295v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šè½®é—®é¢˜è§£å†³ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åæ€å¹¶åŸºäºåé¦ˆè¿›è¡Œä¿®è®¢ã€‚ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¸»è¦åŸºäºå•è½®æ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œè™½ç„¶èƒ½éªŒè¯å¥–åŠ±ï¼Œä½†æ¨¡å‹å¾€å¾€ç¼ºä¹å¤šè½®é—®é¢˜è§£å†³çš„èƒ½åŠ›ï¼Œéš¾ä»¥æ ¹æ®ä¸Šä¸‹æ–‡åé¦ˆä¿®è®¢ç­”æ¡ˆï¼Œå¯¼è‡´é‡å¤å“åº”ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œé€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä»…åˆ©ç”¨é”™è¯¯ç­”æ¡ˆåçš„ä¸€å…ƒåé¦ˆï¼ˆå¦‚â€œå†è¯•ä¸€æ¬¡â€ï¼‰ï¼Œå³å¯æé«˜å•è½®å’Œå¤šè½®æ¨ç†æ€§èƒ½ã€‚æœ¬ç ”ç©¶å¼•å…¥ç”¨äºå¼ºåŒ–å­¦ä¹ çš„ä¸€å…ƒåé¦ˆè§‚æµ‹ï¼ˆUFOï¼‰ï¼Œåœ¨è¿­ä»£é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ä½¿ç”¨æœ€å°ä¸”å¸¸è§çš„ä¸€å…ƒç”¨æˆ·åé¦ˆã€‚å®ƒå¯è½»æ¾åº”ç”¨äºç°æœ‰çš„å•è½®RLè®­ç»ƒè®¾ç½®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»“åˆUFOçš„RLè®­ç»ƒåœ¨ä¿æŒå•è½®æ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†å¤šè½®æ¨ç†çš„å‡†ç¡®æ€§ï¼Œæœ€å¤šå¯æé«˜14%ã€‚ä¸ºå‡å°‘è·å¾—æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„è½®æ¬¡å¹¶åœ¨å‡ºé”™æ—¶é¼“åŠ±å¤šæ ·åŒ–æ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¥–åŠ±ç»“æ„ï¼Œå¼•å¯¼æ¨¡å‹æ¯è½®ç»™å‡ºè°¨æ…å’Œæ…é‡çš„ç­”æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤šè½®é—®é¢˜è§£å†³ä¸­éœ€è¦åæ€å’Œä¿®è®¢èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¸»è¦åŸºäºå•è½®æ¨¡å¼è®­ç»ƒï¼Œå¯¼è‡´æ¨¡å‹ç¼ºä¹å¤šè½®é—®é¢˜è§£å†³çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œåˆ©ç”¨ä¸€å…ƒåé¦ˆï¼ˆå¦‚â€œå†è¯•ä¸€æ¬¡â€ï¼‰å¯ä»¥æé«˜å•è½®å’Œå¤šè½®æ¨ç†æ€§èƒ½ã€‚</li>
<li>ä¸€å…ƒåé¦ˆè§‚æµ‹ï¼ˆUFOï¼‰å¯è½»æ¾åº”ç”¨äºç°æœ‰å•è½®RLè®­ç»ƒè®¾ç½®ã€‚</li>
<li>UFOç»“åˆRLè®­ç»ƒåœ¨ä¿æŒå•è½®æ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜å¤šè½®æ¨ç†å‡†ç¡®æ€§è¾¾14%ã€‚</li>
<li>ä¸ºæé«˜æ•ˆç‡å’Œå¤šæ ·åŒ–æ¨ç†ï¼Œè®¾è®¡äº†æ–°çš„å¥–åŠ±ç»“æ„ã€‚</li>
<li>è¯¥ç ”ç©¶é¼“åŠ±æ¨¡å‹åœ¨æ¯è½®ç»™å‡ºè°¨æ…å’Œæ…é‡çš„ç­”æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50bb1802b4046fb585bf6e04edc88061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28852eba37722a9cc7d23e4673ba84a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1b03959cfff1e6915f8bba60b0c85c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f896cf85fdb9c8dd85ab208eebd9c4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ab6f68cd514cf3f670dca8336b3356.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Is-Small-Language-Model-the-Silver-Bullet-to-Low-Resource-Languages-Machine-Translation"><a href="#Is-Small-Language-Model-the-Silver-Bullet-to-Low-Resource-Languages-Machine-Translation" class="headerlink" title="Is Small Language Model the Silver Bullet to Low-Resource Languages   Machine Translation?"></a>Is Small Language Model the Silver Bullet to Low-Resource Languages   Machine Translation?</h2><p><strong>Authors:Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, Radu State, TegawendÃ© F. BissyandÃ©, Jacques Klein</strong></p>
<p>Low-resource languages (LRLs) lack sufficient linguistic resources and are underrepresented in benchmark datasets, resulting in persistently lower translation quality than high-resource languages, especially in privacy-sensitive and resource-limited contexts. Firstly, this study systematically evaluates state-of-the-art smaller Large Language Models in 200 languages using the FLORES-200 benchmark, highlighting persistent deficiencies and disparities in the translation of LRLs. To mitigate these limitations, we investigate knowledge distillation from large pre-trained teacher models to Small Language Models (SLMs) through supervised fine-tuning. The results show substantial improvements; for example, the translation performance of English to Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases from 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further investigate various fine-tuning configurations and tasks to clarify the trade-offs between data scale and training efficiency, verify that the model retains its general capabilities without significant catastrophic forgetting after training, and explore the distillation benefits to other LRLs on SLMs (Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations and fairness issues of current SLMs in LRL translation and systematically explores the potential of using the distillation of knowledge from large to small models, offering practical, empirically grounded recommendations to improve LRL translation systems </p>
<blockquote>
<p>ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰ç¼ºä¹è¶³å¤Ÿçš„è¯­è¨€èµ„æºï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸­çš„ä»£è¡¨æ€§ä¸è¶³ï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡æŒç»­ä½äºé«˜èµ„æºè¯­è¨€ï¼Œå°¤å…¶æ˜¯åœ¨éšç§æ•æ„Ÿå’Œèµ„æºæœ‰é™çš„æƒ…å¢ƒä¸­å°¤ä¸ºæ˜æ˜¾ã€‚é¦–å…ˆï¼Œæœ¬ç ”ç©¶ä½¿ç”¨FLORES-200åŸºå‡†æ•°æ®é›†ç³»ç»Ÿè¯„ä¼°äº†200ç§è¯­è¨€çš„æœ€æ–°å°å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œçªå‡ºäº†ä½èµ„æºè¯­è¨€ç¿»è¯‘ä¸­çš„æŒç»­ç¼ºé™·å’Œå·®å¼‚ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†é€šè¿‡æœ‰ç›‘ç£çš„å¾®è°ƒä»å¤§å‹é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹å‘å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è¿›è¡ŒçŸ¥è¯†è’¸é¦çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨éªŒè¯é›†ä¸Šï¼Œè‹±è¯­åˆ°å¢æ£®å ¡è¯­ï¼ˆENåˆ°LBï¼‰çš„ç¿»è¯‘æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…çš„åˆ†æ•°ä»0.36æé«˜åˆ°0.89ï¼ˆä»¥Llama-3.2-3Bä¸ºä¾‹ï¼‰ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†å„ç§å¾®è°ƒé…ç½®å’Œä»»åŠ¡ï¼Œä»¥æ˜ç¡®æ•°æ®è§„æ¨¡å’Œè®­ç»ƒæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼ŒéªŒè¯æ¨¡å‹åœ¨è®­ç»ƒåæ˜¯å¦ä¿æŒå…¶ä¸€èˆ¬èƒ½åŠ›è€Œæ²¡æœ‰æ˜¾è‘—çš„ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶æ¢ç´¢äº†å¯¹å…¶ä»–ä½èµ„æºè¯­è¨€ï¼ˆå¦‚å¡å¡”å…‹è¯­ã€é˜¿è¨å§†è¯­å’Œä¹Œå…‹å…°è¯­ï¼‰çš„å°å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è’¸é¦å¥½å¤„ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæ­ç¤ºäº†å½“å‰ä½èµ„æºè¯­è¨€ç¿»è¯‘ä¸­å°å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§å’Œå…¬å¹³æ€§é—®é¢˜ï¼Œå¹¶ç³»ç»Ÿåœ°æ¢ç´¢äº†ä»å¤§å‹æ¨¡å‹åˆ°å°å‹æ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦çš„æ½œåŠ›ï¼Œä¸ºæ”¹è¿›ä½èµ„æºè¯­è¨€ç¿»è¯‘ç³»ç»Ÿæä¾›äº†å®ç”¨ä¸”åŸºäºå®è¯çš„å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24102v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶è¯„ä¼°äº†å°å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰ä¸Šçš„ç¿»è¯‘æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨éšç§æ•æ„Ÿå’Œèµ„æºå—é™ç¯å¢ƒä¸­çš„æŒç»­é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œè¯¥ç ”ç©¶å°è¯•ä»å¤§å‹é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯å¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è¿›è¡Œå¾®è°ƒæ¥æå‡ç¿»è¯‘æ€§èƒ½ã€‚ç ”ç©¶å‘ç°è¿™ç§æ–¹æ³•å¯ä»¥æ˜¾è‘—æ”¹å–„ç¿»è¯‘æ€§èƒ½ï¼Œæ¯”å¦‚åœ¨è‹±è¯­åˆ°å¢æ£®å ¡è¯­çš„ç¿»è¯‘ä¸­ï¼ŒLLMè¯„åˆ†ä»0.36æé«˜åˆ°0.89ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒå¾®è°ƒé…ç½®å’Œä»»åŠ¡ä¹‹é—´çš„æƒè¡¡ï¼ŒéªŒè¯äº†æ¨¡å‹åœ¨è®­ç»ƒåä¸ä¼šä¸§å¤±å…¶é€šç”¨èƒ½åŠ›ï¼Œå¹¶æ¢ç´¢äº†å…¶ä»–ä½èµ„æºè¯­è¨€ï¼ˆå¦‚å¡å¥‡è¯­ã€é˜¿è¨å§†è¯­å’Œä¹Œå…‹å…°è¯­ï¼‰çš„å°å‹è¯­è¨€æ¨¡å‹çš„è’¸é¦æ•ˆç›Šã€‚è¿™é¡¹ç ”ç©¶ä¸ºæ”¹è¿›ä½èµ„æºè¯­è¨€çš„ç¿»è¯‘ç³»ç»Ÿæä¾›äº†å®ç”¨çš„ç»éªŒä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>ä½èµ„æºè¯­è¨€åœ¨åŸºå‡†æ•°æ®é›†ä¸Šç¼ºä¹è¶³å¤Ÿçš„è¯­è¨€èµ„æºï¼Œå¯¼è‡´ç¿»è¯‘è´¨é‡æŒç»­ä½äºé«˜èµ„æºè¯­è¨€ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨FLORES-200åŸºå‡†å¯¹æœ€æ–°å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œ200ç§è¯­è¨€çš„ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æŠ€æœ¯ç”¨äºé€šè¿‡å¤§å‹é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹æé«˜å°å‹è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ˜¾è‘—æé«˜ç¿»è¯‘æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨è‹±è¯­åˆ°å¢æ£®å ¡è¯­çš„ç¿»è¯‘ä¸­LLMè¯„åˆ†æ˜¾è‘—æé«˜ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ä¸åŒå¾®è°ƒé…ç½®å’Œä»»åŠ¡ä¹‹é—´çš„æƒè¡¡ï¼ŒéªŒè¯äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›å’Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶è¿˜æ¢ç´¢äº†å…¶ä»–ä½èµ„æºè¯­è¨€çš„å°å‹è¯­è¨€æ¨¡å‹çš„è’¸é¦æ•ˆç›Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01ad5cb9461e2bc0b94f59f7ee496423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c1c655d58f271b8b110774e9e0fbb6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b83c25c6e787ce4a0e216866ff96bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40616053b938b981f4ce028f6ece0728.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9050db94ca3ca8392b1d98593fdd4eed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0efdad52079f3cafd40cbf55c1a832bf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Simulate-Human-Responses-A-Case-Study-of-Stated-Preference-Experiments-in-the-Context-of-Heating-related-Choices"><a href="#Can-Large-Language-Models-Simulate-Human-Responses-A-Case-Study-of-Stated-Preference-Experiments-in-the-Context-of-Heating-related-Choices" class="headerlink" title="Can Large Language Models Simulate Human Responses? A Case Study of   Stated Preference Experiments in the Context of Heating-related Choices"></a>Can Large Language Models Simulate Human Responses? A Case Study of   Stated Preference Experiments in the Context of Heating-related Choices</h2><p><strong>Authors:Han Wang, Jacek Pawlak, Aruna Sivakumar</strong></p>
<p>Stated preference (SP) surveys are a key method to research how individuals make trade-offs in hypothetical, also futuristic, scenarios. In energy context this includes key decarbonisation enablement contexts, such as low-carbon technologies, distributed renewable energy generation, and demand-side response [1,2]. However, they tend to be costly, time-consuming, and can be affected by respondent fatigue and ethical constraints. Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like textual responses, prompting growing interest in their application to survey research. This study investigates the use of LLMs to simulate consumer choices in energy-related SP surveys and explores their integration into data analysis workflows. A series of test scenarios were designed to systematically assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and DeepSeek-R1) at both individual and aggregated levels, considering contexts factors such as prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, LLM types, integration with traditional choice models, and potential biases. Cloud-based LLMs do not consistently outperform smaller local models. In this study, the reasoning model DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Across models, systematic biases are observed against the gas boiler and no-retrofit options, with a preference for more energy-efficient alternatives. The findings suggest that previous SP choices are the most effective input factor, while longer prompts with additional factors and varied formats can cause LLMs to lose focus, reducing accuracy. </p>
<blockquote>
<p>é™ˆè¿°åå¥½ï¼ˆSPï¼‰è°ƒæŸ¥æ˜¯ç ”ç©¶ä¸ªä½“å¦‚ä½•åœ¨å‡è®¾çš„ã€æœªæ¥çš„åœºæ™¯ä¸­åšå‡ºæƒè¡¡çš„å…³é”®æ–¹æ³•ã€‚åœ¨èƒ½æºé¢†åŸŸï¼Œè¿™åŒ…æ‹¬å…³é”®çš„è„±ç¢³èµ‹èƒ½ç¯å¢ƒï¼Œå¦‚ä½ç¢³æŠ€æœ¯ã€åˆ†å¸ƒå¼å¯å†ç”Ÿèƒ½æºå‘ç”µå’Œéœ€æ±‚ä¾§å“åº”[1,2]ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€æˆæœ¬é«˜æ˜‚ã€è€—è´¹æ—¶é—´ï¼Œå¹¶å¯èƒ½å—åˆ°å—è®¿è€…ç–²åŠ³å’Œä¼¦ç†çº¦æŸçš„å½±å“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬å“åº”æ–¹é¢è¡¨ç°å‡ºä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œä¿ƒä½¿äººä»¬å¯¹å…¶åº”ç”¨äºè°ƒæŸ¥ç ”ç©¶çš„å…´è¶£æ—¥ç›Šæµ“åšã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨LLMæ¨¡æ‹Ÿèƒ½æºç›¸å…³SPè°ƒæŸ¥ä¸­çš„æ¶ˆè´¹è€…é€‰æ‹©ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬èå…¥æ•°æ®åˆ†æå·¥ä½œæµç¨‹çš„æ–¹æ³•ã€‚è®¾è®¡äº†ä¸€ç³»åˆ—æµ‹è¯•åœºæ™¯ï¼Œä»¥ç³»ç»Ÿåœ°è¯„ä¼°å‡ ç§LLMï¼ˆLLaMA 3.1ã€Mistralã€GPT-3.5å’ŒDeepSeek-R1ï¼‰åœ¨ä¸ªä½“å’Œèšåˆå±‚é¢çš„æ¨¡æ‹Ÿæ€§èƒ½ï¼Œè€ƒè™‘ä¸Šä¸‹æ–‡å› ç´ ï¼Œå¦‚æç¤ºè®¾è®¡ã€ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€LLMç±»å‹ã€ä¸ä¼ ç»Ÿé€‰æ‹©æ¨¡å‹çš„é›†æˆä»¥åŠæ½œåœ¨åè§ã€‚åŸºäºäº‘çš„LLMå¹¶ä¸æ€»æ˜¯ä¼˜äºè¾ƒå°çš„æœ¬åœ°æ¨¡å‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæ¨ç†æ¨¡å‹DeepSeek-R1è¾¾åˆ°äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®ç‡ï¼ˆ77%ï¼‰ï¼Œåœ¨å‡†ç¡®ç‡ã€å› ç´ è¯†åˆ«å’Œé€‰æ‹©åˆ†å¸ƒå¯¹é½æ–¹é¢ä¼˜äºéæ¨ç†LLMã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œéƒ½å­˜åœ¨å¯¹ç‡ƒæ°”é”…ç‚‰å’Œæ— æ”¹é€ é€‰æ‹©çš„ç³»ç»Ÿæ€§åè§ï¼Œæ›´å€¾å‘äºé€‰æ‹©æ›´èŠ‚èƒ½çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ˆå‰çš„SPé€‰æ‹©æ˜¯æœ€æœ‰æ•ˆçš„è¾“å…¥å› ç´ ï¼Œè€Œè¾ƒé•¿çš„æç¤ºå’Œå¸¦æœ‰é¢å¤–å› ç´ å’Œä¸åŒæ ¼å¼çš„æç¤ºå¯èƒ½ä¼šä½¿LLMå¤±å»ç„¦ç‚¹ï¼Œé™ä½å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10652v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨¡æ‹Ÿæ¶ˆè´¹è€…èƒ½æºç›¸å…³åå¥½é€‰æ‹©ä¸­çš„åº”ç”¨ï¼Œæœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨LLMè¿›è¡Œå‡è®¾åå¥½è°ƒæŸ¥çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å¯¹æ¯”äº†ä¸åŒLLMåœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„æ¨¡æ‹Ÿæ€§èƒ½ï¼ŒåŒ…æ‹¬DeepSeek-R1åœ¨å†…çš„å¤šç§æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ¶ˆè´¹è€…é€‰æ‹©æ–¹é¢å±•ç°å‡ºè¾ƒé«˜å‡†ç¡®æ€§ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¹‹å‰çš„é€‰æ‹©åå¥½æ˜¯æœ€æœ‰æ•ˆçš„è¾“å…¥å› ç´ ï¼Œè€Œæ›´é•¿çš„æç¤ºä¸å¤šæ ·åŒ–çš„æ ¼å¼å¯èƒ½ä¼šå½±å“LLMçš„ç²¾ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºæ¨¡æ‹Ÿèƒ½æºç›¸å…³çš„æ¶ˆè´¹è€…åå¥½é€‰æ‹©ã€‚</li>
<li>LLMåœ¨æ¨¡æ‹Ÿæ¶ˆè´¹è€…é€‰æ‹©æ–¹é¢å±•ç°å‡ºé«˜å‡†ç¡®æ€§ï¼Œå…¶ä¸­DeepSeek-R1æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå¹³å‡å‡†ç¡®åº¦è¾¾åˆ°77%ã€‚</li>
<li>ç›¸è¾ƒäºéæ¨ç†LLMï¼Œæ¨ç†LLMåœ¨å‡†ç¡®æ€§ã€å› ç´ è¯†åˆ«å’Œé€‰æ‹©åˆ†å¸ƒå¯¹é½æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>åœ¨æ¨¡å‹ä¸­å­˜åœ¨å¯¹èƒ½æºæ•ˆç‡æ›´é«˜é€‰é¡¹çš„åå¥½å’Œå¯¹ç‡ƒæ°”é”…ç‚‰ä»¥åŠæ— æ”¹é€ é€‰é¡¹çš„ç³»ç»Ÿæ€§åè§ã€‚</li>
<li>ä¹‹å‰çš„é€‰æ‹©åå¥½æ˜¯æ¨¡æ‹Ÿæ¶ˆè´¹è€…å†³ç­–ä¸­æœ€æœ‰æ•ˆçš„è¾“å…¥å› ç´ ã€‚</li>
<li>æ›´é•¿çš„æç¤ºå’Œå¤šæ ·åŒ–çš„æ ¼å¼å¯èƒ½ä¼šå½±å“LLMçš„ç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-01cfd708b7f9a059552248c794d5f179.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Can-Hallucinations-Help-Boosting-LLMs-for-Drug-Discovery"><a href="#Can-Hallucinations-Help-Boosting-LLMs-for-Drug-Discovery" class="headerlink" title="Can Hallucinations Help? Boosting LLMs for Drug Discovery"></a>Can Hallucinations Help? Boosting LLMs for Drug Discovery</h2><p><strong>Authors:Shuzhou Yuan, Zhan Qu, Ashish Yashwanth Kangen, Michael FÃ¤rber</strong></p>
<p>Hallucinations in large language models (LLMs), plausible but factually inaccurate text, are often viewed as undesirable. However, recent work suggests that such outputs may hold creative potential. In this paper, we investigate whether hallucinations can improve LLMs on molecule property prediction, a key task in early-stage drug discovery. We prompt LLMs to generate natural language descriptions from molecular SMILES strings and incorporate these often hallucinated descriptions into downstream classification tasks. Evaluating seven instruction-tuned LLMs across five datasets, we find that hallucinations significantly improve predictive accuracy for some models. Notably, Falcon3-Mamba-7B outperforms all baselines when hallucinated text is included, while hallucinations generated by GPT-4o consistently yield the greatest gains between models. We further identify and categorize over 18,000 beneficial hallucinations, with structural misdescriptions emerging as the most impactful type, suggesting that hallucinated statements about molecular structure may increase model confidence. Ablation studies show that larger models benefit more from hallucinations, while temperature has a limited effect. Our findings challenge conventional views of hallucination as purely problematic and suggest new directions for leveraging hallucinations as a useful signal in scientific modeling tasks like drug discovery. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ï¼Œå³çœ‹ä¼¼åˆç†ä½†äº‹å®ä¸Šä¸å‡†ç¡®çš„æ–‡æœ¬ï¼Œé€šå¸¸è¢«è§†ä¸ºä¸å—æ¬¢è¿ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™æ ·çš„è¾“å‡ºå¯èƒ½å…·æœ‰åˆ›é€ æ€§æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¹»è§‰æ˜¯å¦å¯ä»¥æé«˜LLMåœ¨åˆ†å­å±æ€§é¢„æµ‹æ–¹é¢çš„è¡¨ç°ï¼Œè¿™æ˜¯æ—©æœŸè¯ç‰©å‘ç°ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å¯¼LLMæ ¹æ®åˆ†å­çš„SMILESå­—ç¬¦ä¸²ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶å°†è¿™äº›é€šå¸¸å¸¦æœ‰å¹»è§‰çš„æè¿°çº³å…¥ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†ä¸ƒä¸ªæŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMï¼Œæˆ‘ä»¬å‘ç°å¹»è§‰æ˜¾è‘—æé«˜äº†æŸäº›æ¨¡å‹çš„é¢„æµ‹ç²¾åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“åŒ…å«å¹»è§‰æ–‡æœ¬æ—¶ï¼ŒFalcon3-Mamba-7Bè¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œè€ŒGPT-4oäº§ç”Ÿçš„å¹»è§‰å§‹ç»ˆåœ¨æ¨¡å‹ä¹‹é—´äº§ç”Ÿäº†æœ€å¤§çš„æ”¶ç›Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯†åˆ«å’Œåˆ†ç±»äº†18000å¤šä¸ªæœ‰ç›Šå¹»è§‰ï¼Œç»“æ„ä¸Šçš„é”™è¯¯æè¿°æ˜¯æœ€æœ‰å½±å“çš„ä¸€ç±»ï¼Œè¿™è¡¨æ˜å…³äºåˆ†å­ç»“æ„çš„å¹»è§‰é™ˆè¿°å¯èƒ½ä¼šå¢åŠ æ¨¡å‹çš„ä¿¡å¿ƒã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè¾ƒå¤§çš„æ¨¡å‹ä»å¹»è§‰ä¸­è·ç›Šæ›´å¤šï¼Œè€Œæ¸©åº¦çš„å½±å“æœ‰é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å¹»è§‰ä»…è¢«è§†ä¸ºé—®é¢˜çš„ä¼ ç»Ÿè§‚ç‚¹ï¼Œå¹¶ä¸ºåˆ©ç”¨å¹»è§‰ä½œä¸ºç§‘å­¦å»ºæ¨¡ä»»åŠ¡ï¼ˆå¦‚è¯ç‰©å‘ç°ï¼‰ä¸­çš„æœ‰ç”¨ä¿¡å·æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13824v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰æ–‡æœ¬ï¼ˆhallucinationsï¼‰è™½ç„¶å¯èƒ½ä¸å‡†ç¡®ï¼Œä½†è¿‘æœŸç ”ç©¶è¡¨æ˜å…¶å…·å¤‡åˆ›é€ æ€§æ½œåŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¹»è§‰æ–‡æœ¬åœ¨é¢„æµ‹åˆ†å­å±æ€§æ–¹é¢çš„ä½œç”¨ï¼Œè¿™æ˜¯æ—©æœŸè¯ç‰©å‘ç°ä¸­çš„å…³é”®ä»»åŠ¡ã€‚é€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆ†å­SMILESå­—ç¬¦ä¸²çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶å°†å…¶çº³å…¥ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç ”ç©¶å‘ç°å¹»è§‰æ–‡æœ¬å¯¹éƒ¨åˆ†æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§æœ‰æ˜¾è‘—æ”¹å–„ã€‚å°¤å…¶æ˜¯Falcon3-Mamba-7Bæ¨¡å‹åœ¨åŒ…å«å¹»è§‰æ–‡æœ¬æ—¶è¡¨ç°æœ€ä½³ï¼Œè€ŒGPT-4oäº§ç”Ÿçš„å¹»è§‰æ–‡æœ¬åœ¨ä¸åŒæ¨¡å‹ä¸­å§‹ç»ˆå¸¦æ¥æœ€å¤§æ”¶ç›Šã€‚ç ”ç©¶è¿˜è¯†åˆ«å’Œåˆ†ç±»äº†è¶…è¿‡18,000ç§æœ‰ç›Šå¹»è§‰ï¼Œå…¶ä¸­ç»“æ„è¯¯æè¿°å½±å“æœ€ä¸ºæ˜¾è‘—ï¼Œè¡¨æ˜å…³äºåˆ†å­ç»“æ„çš„å¹»è§‰é™ˆè¿°å¯èƒ½å¢åŠ æ¨¡å‹ä¿¡å¿ƒã€‚ç ”ç©¶è¿˜å‘ç°æ›´å¤§çš„æ¨¡å‹æ›´èƒ½ä»å¹»è§‰ä¸­å—ç›Šï¼Œè€Œæ¸©åº¦å¯¹å¹»è§‰çš„å½±å“æœ‰é™ã€‚è¯¥ç ”ç©¶æŒ‘æˆ˜äº†å¹»è§‰ä»…è¢«è§†ä¸ºé—®é¢˜çš„ä¼ ç»Ÿè§‚ç‚¹ï¼Œå¹¶ä¸ºåœ¨è¯ç‰©å‘ç°ç­‰ç§‘å­¦å»ºæ¨¡ä»»åŠ¡ä¸­åˆ©ç”¨å¹»è§‰ä½œä¸ºæœ‰ç”¨ä¿¡å·æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰æ–‡æœ¬å…·å¤‡åˆ›é€ æ€§æ½œåŠ›ã€‚</li>
<li>å¹»è§‰æ–‡æœ¬åœ¨é¢„æµ‹åˆ†å­å±æ€§æ–¹é¢å¯¹éƒ¨åˆ†æ¨¡å‹æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>Falcon3-Mamba-7Bæ¨¡å‹åœ¨åŒ…å«å¹»è§‰æ–‡æœ¬æ—¶è¡¨ç°æœ€ä½³ã€‚</li>
<li>GPT-4oäº§ç”Ÿçš„å¹»è§‰æ–‡æœ¬åœ¨ä¸åŒæ¨¡å‹ä¸­å¸¦æ¥æœ€å¤§æ”¶ç›Šã€‚</li>
<li>ç»“æ„è¯¯æè¿°çš„å¹»è§‰å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“æœ€ä¸ºæ˜¾è‘—ã€‚</li>
<li>æ›´å¤§çš„æ¨¡å‹æ›´èƒ½ä»å¹»è§‰ä¸­å—ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa948b19e68ebac3ede011b8c61dcb3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc56025499de4f7758620f07bd603bf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de3c0713ced44bf94721b21adbed6ef8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c0237dc66f809eb213b3aba59a86ecd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1b0480fedfcd26cd9f8a8908dec7ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f09a0d26b190e4c9b920f04322e086a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-26/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c2a037de8223539a3c9720f4306a63c4.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-26  LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due   Diligence
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-26/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-28852eba37722a9cc7d23e4673ba84a0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-26  Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
