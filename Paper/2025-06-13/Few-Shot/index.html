<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Analytic Task Scheduler Recursive Least Squares Based Method for   Continual Learning in Embodied Foundation Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-254e8b1bd893ced0abcbd6ee53784532.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="Analytic-Task-Scheduler-Recursive-Least-Squares-Based-Method-for-Continual-Learning-in-Embodied-Foundation-Models"><a href="#Analytic-Task-Scheduler-Recursive-Least-Squares-Based-Method-for-Continual-Learning-in-Embodied-Foundation-Models" class="headerlink" title="Analytic Task Scheduler: Recursive Least Squares Based Method for   Continual Learning in Embodied Foundation Models"></a>Analytic Task Scheduler: Recursive Least Squares Based Method for   Continual Learning in Embodied Foundation Models</h2><p><strong>Authors:Lipei Xie, Yingxin Li, Huiping Zhuang</strong></p>
<p>Embodied foundation models are crucial for Artificial Intelligence (AI) interacting with the physical world by integrating multi-modal inputs, such as proprioception, vision and language, to understand human intentions and generate actions to control robots. While these models demonstrate strong generalization and few-shot learning capabilities, they face significant challenges in continually acquiring new skills without forgetting previously learned skills, a problem known as catastrophic forgetting. To address this issue, we propose the Analytic Task Scheduler (ATS), a novel framework for continual learning in embodied foundation models. ATS consists of a task-specific model library, where each model is fine-tuned independently on a single task, and an analytic scheduler trained using recursive least squares (RLS) to learn the mapping between language instructions and task-specific models. This architecture enables accurate task recognition and dynamic model selection while fundamentally avoiding parameter interference across tasks. The scheduler updates its parameters incrementally using only statistics (autocorrelation and cross-correlation matrices), enabling forgetting-resistant learning without the need to revisit historical data. We validate ATS on a real-world robot platform (RM65B), demonstrating superior resistance to forgetting and strong adaptability to task variations. The results highlight ATS as an effective, scalable, and deployable solution for continual learning in embodied foundation models operating in complex, dynamic environments. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler">https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler</a> </p>
<blockquote>
<p>åµŒå…¥å¼çš„åŸºç¡€æ¨¡å‹é€šè¿‡æ•´åˆå¤šç§æ¨¡å¼è¾“å…¥ï¼ˆå¦‚æœ¬ä½“æ„Ÿå—ã€è§†è§‰å’Œè¯­è¨€ï¼‰ï¼Œç†è§£äººç±»æ„å›¾å¹¶ç”Ÿæˆæ§åˆ¶æœºå™¨äººçš„åŠ¨ä½œï¼Œå¯¹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸ç‰©ç†ä¸–ç•Œçš„äº¤äº’è‡³å…³é‡è¦ã€‚è¿™äº›æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä½†åœ¨æŒç»­è·å–æ–°æŠ€èƒ½æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå³é—å¿˜å…ˆå‰å­¦åˆ°çš„æŠ€èƒ½ï¼Œè¿™ä¸ªé—®é¢˜è¢«ç§°ä¸ºç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†æä»»åŠ¡è°ƒåº¦å™¨ï¼ˆATSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåµŒå…¥å¼åŸºç¡€æ¨¡å‹ä¸­æŒç»­å­¦ä¹ çš„å…¨æ–°æ¡†æ¶ã€‚ATSåŒ…æ‹¬ä¸€ä¸ªç‰¹å®šä»»åŠ¡æ¨¡å‹åº“ï¼Œå…¶ä¸­çš„æ¯ä¸ªæ¨¡å‹éƒ½æ˜¯é’ˆå¯¹å•ä¸€ä»»åŠ¡ç‹¬ç«‹ç²¾ç»†è°ƒæ•´çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨é€’å½’æœ€å°äºŒä¹˜æ³•ï¼ˆRLSï¼‰è®­ç»ƒäº†ä¸€ä¸ªåˆ†æè°ƒåº¦å™¨ï¼Œå­¦ä¹ è¯­è¨€æŒ‡ä»¤å’Œä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¹‹é—´çš„æ˜ å°„ã€‚è¿™ç§æ¶æ„èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„ä»»åŠ¡è¯†åˆ«å’ŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼Œä»æ ¹æœ¬ä¸Šé¿å…ä»»åŠ¡é—´çš„å‚æ•°å¹²æ‰°ã€‚è°ƒåº¦å™¨ä»…ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆè‡ªç›¸å…³å’Œäº’ç›¸å…³çŸ©é˜µï¼‰æ¥å¢é‡æ›´æ–°å…¶å‚æ•°ï¼Œä»è€Œå®ç°æŠ—é—å¿˜å­¦ä¹ ï¼Œæ— éœ€å›é¡¾å†å²æ•°æ®ã€‚æˆ‘ä»¬åœ¨å®é™…æœºå™¨äººå¹³å°RM65Bä¸ŠéªŒè¯äº†ATSï¼Œæ˜¾ç¤ºå‡ºå…¶å‡ºè‰²çš„æŠ—é—å¿˜æ€§å’Œå¯¹ä»»åŠ¡å˜åŒ–çš„å¼ºå¤§é€‚åº”æ€§ã€‚ç»“æœå¼ºè°ƒATSæ˜¯åµŒå…¥å¼åŸºç¡€æ¨¡å‹åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­æŒç»­å­¦ä¹ çš„æœ‰æ•ˆã€å¯æ‰©å±•å’Œå¯éƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/MIAA-Embodied-AI/AnalyticTaskSchedulerä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09623v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä½“è²ŒåŸºç¡€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½ä¸ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œé€šè¿‡æ•´åˆå¤šæ¨¡å¼è¾“å…¥å¦‚èº«ä½“æ„Ÿè§‰ã€è§†è§‰å’Œè¯­è¨€ï¼Œæ¥ç†è§£äººç±»æ„å›¾å¹¶ç”Ÿæˆæ§åˆ¶æœºå™¨äººçš„åŠ¨ä½œã€‚é¢ä¸´æŒç»­å­¦ä¹ æ–°æŠ€èƒ½æ—¶é—å¿˜æ—§æŠ€èƒ½çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºè§£æä»»åŠ¡è°ƒåº¦å™¨ï¼ˆATSï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œç”¨äºä½“è²ŒåŸºç¡€æ¨¡å‹çš„æŒç»­å­¦ä¹ ã€‚ATSåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæ¨¡å‹åº“ï¼Œæ¯ä¸ªæ¨¡å‹ç‹¬ç«‹å¾®è°ƒå•ä¸€ä»»åŠ¡ï¼Œè§£æè°ƒåº¦å™¨é€šè¿‡é€’å½’æœ€å°äºŒä¹˜æ³•ï¼ˆRLSï¼‰å­¦ä¹ è¯­è¨€æŒ‡ä»¤ä¸ä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¹‹é—´çš„æ˜ å°„ã€‚æ­¤æ¶æ„å®ç°äº†ç²¾ç¡®çš„ä»»åŠ¡è¯†åˆ«å’ŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼Œä»æ ¹æœ¬ä¸Šé¿å…äº†ä»»åŠ¡é—´çš„å‚æ•°å¹²æ‰°ã€‚è°ƒåº¦å™¨ä»…é€šè¿‡ç»Ÿè®¡ä¿¡æ¯ï¼ˆè‡ªç›¸å…³å’Œäº’ç›¸å…³çŸ©é˜µï¼‰é€æ­¥æ›´æ–°å‚æ•°ï¼Œå®ç°äº†é—å¿˜æŠµæŠ—æ€§å­¦ä¹ ï¼Œæ— éœ€å›é¡¾å†å²æ•°æ®ã€‚æˆ‘ä»¬åœ¨çœŸå®æœºå™¨äººå¹³å°RM65Bä¸ŠéªŒè¯äº†ATSï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æŠ—é—å¿˜æ€§å’Œä»»åŠ¡é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½“è²ŒåŸºç¡€æ¨¡å‹åœ¨AIä¸ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­èµ·å…³é”®ä½œç”¨ï¼Œé›†æˆå¤šæ¨¡å¼è¾“å…¥å¦‚èº«ä½“æ„Ÿè§‰ã€è§†è§‰å’Œè¯­è¨€ã€‚</li>
<li>ä½“è²ŒåŸºç¡€æ¨¡å‹é¢ä¸´æŒç»­å­¦ä¹ æ–°æŠ€èƒ½æ—¶çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>è§£æä»»åŠ¡è°ƒåº¦å™¨ï¼ˆATSï¼‰æ¡†æ¶ç”¨äºä½“è²ŒåŸºç¡€æ¨¡å‹çš„æŒç»­å­¦ä¹ ã€‚</li>
<li>ATSåŒ…æ‹¬ä»»åŠ¡ç‰¹å®šæ¨¡å‹åº“å’Œè§£æè°ƒåº¦å™¨ï¼Œé€šè¿‡é€’å½’æœ€å°äºŒä¹˜æ³•ï¼ˆRLSï¼‰å­¦ä¹ è¯­è¨€æŒ‡ä»¤ä¸ä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¹‹é—´çš„æ˜ å°„ã€‚</li>
<li>ATSæ¶æ„é¿å…ä»»åŠ¡é—´çš„å‚æ•°å¹²æ‰°ï¼Œå®ç°ç²¾ç¡®ä»»åŠ¡è¯†åˆ«å’ŒåŠ¨æ€æ¨¡å‹é€‰æ‹©ã€‚</li>
<li>è°ƒåº¦å™¨é€šè¿‡ç»Ÿè®¡ä¿¡æ¯é€æ­¥æ›´æ–°å‚æ•°ï¼Œå®ç°é—å¿˜æŠµæŠ—æ€§å­¦ä¹ ã€‚</li>
<li>åœ¨çœŸå®æœºå™¨äººå¹³å°ä¸ŠéªŒè¯äº†ATSçš„æœ‰æ•ˆæ€§ã€å¯ä¼¸ç¼©æ€§å’Œéƒ¨ç½²æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce87097aecbb779704ce213cc4a50235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf9f782187e6271e95a2f7a918679353.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eabd51b80ad8ed9a445ead2a35cd26ad.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Provoking-Multi-modal-Few-Shot-LVLM-via-Exploration-Exploitation-In-Context-Learning"><a href="#Provoking-Multi-modal-Few-Shot-LVLM-via-Exploration-Exploitation-In-Context-Learning" class="headerlink" title="Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation   In-Context Learning"></a>Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation   In-Context Learning</h2><p><strong>Authors:Cheng Chen, Yunpeng Zhai, Yifan Zhao, Jinyang Gao, Bolin Ding, Jia Li</strong></p>
<p>In-context learning (ICL), a predominant trend in instruction learning, aims at enhancing the performance of large language models by providing clear task guidance and examples, improving their capability in task understanding and execution. This paper investigates ICL on Large Vision-Language Models (LVLMs) and explores the policies of multi-modal demonstration selection. Existing research efforts in ICL face significant challenges: First, they rely on pre-defined demonstrations or heuristic selecting strategies based on human intuition, which are usually inadequate for covering diverse task requirements, leading to sub-optimal solutions; Second, individually selecting each demonstration fails in modeling the interactions between them, resulting in information redundancy. Unlike these prevailing efforts, we propose a new exploration-exploitation reinforcement learning framework, which explores policies to fuse multi-modal information and adaptively select adequate demonstrations as an integrated whole. The framework allows LVLMs to optimize themselves by continually refining their demonstrations through self-exploration, enabling the ability to autonomously identify and generate the most effective selection policies for in-context learning. Experimental results verify the superior performance of our approach on four Visual Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing the generalization capability of few-shot LVLMs. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ˜¯æŒ‡ç¤ºå­¦ä¹ ä¸­çš„ä¸€ç§ä¸»è¦è¶‹åŠ¿ï¼Œæ—¨åœ¨é€šè¿‡æä¾›æ˜ç¡®çš„ä»»åŠ¡æŒ‡å¯¼å’Œç¤ºä¾‹æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¢å¼ºå…¶åœ¨ä»»åŠ¡ç†è§£å’Œæ‰§è¡Œæ–¹é¢çš„èƒ½åŠ›ã€‚æœ¬æ–‡é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šçš„ICLè¿›è¡Œäº†è°ƒæŸ¥ï¼Œå¹¶æ¢è®¨äº†å¤šæ¨¡æ€æ¼”ç¤ºé€‰æ‹©çš„ç­–ç•¥ã€‚ç°æœ‰çš„ICLç ”ç©¶é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼šé¦–å…ˆï¼Œå®ƒä»¬ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„æ¼”ç¤ºæˆ–åŸºäºäººç±»ç›´è§‰çš„å¯å‘å¼é€‰æ‹©ç­–ç•¥ï¼Œé€šå¸¸æ— æ³•è¦†ç›–å„ç§ä»»åŠ¡éœ€æ±‚ï¼Œå¯¼è‡´æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆï¼›å…¶æ¬¡ï¼Œä¸ªåˆ«é€‰æ‹©æ¯ä¸ªæ¼”ç¤ºå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„äº¤äº’ï¼Œå¯¼è‡´ä¿¡æ¯å†—ä½™ã€‚ä¸åŒäºè¿™äº›æ™®éçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¢ç´¢-åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¢ç´¢äº†èåˆå¤šæ¨¡æ€ä¿¡æ¯å’Œè‡ªé€‚åº”é€‰æ‹©å……è¶³æ¼”ç¤ºå“çš„ç­–ç•¥ï¼Œå°†å…¶ä½œä¸ºä¸€ä¸ªæ•´ä½“ã€‚è¯¥æ¡†æ¶å…è®¸LVLMsé€šè¿‡ä¸æ–­è‡ªæˆ‘æ¢ç´¢ä¼˜åŒ–å…¶æ¼”ç¤ºï¼Œå…·å¤‡è‡ªä¸»è¯†åˆ«å’Œç”Ÿæˆæœ€æœ‰æ•ˆçš„é€‰æ‹©ç­–ç•¥æ¥è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ã€‚å®éªŒç»“æœåœ¨å››ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œè¯æ˜äº†å…¶åœ¨æé«˜å°‘æ•°LVLMsçš„æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09473v1">PDF</a> 10 pages, 6 figures, CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨è¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰ä¸­ï¼Œé’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„ç ”ç©¶æ­£åœ¨æ¢ç´¢å¤šæ¨¡æ€æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ã€‚ç°æœ‰ç ”ç©¶é¢ä¸´ä¾èµ–é¢„å®šä¹‰æ¼”ç¤ºæˆ–åŸºäºäººç±»ç›´è§‰çš„å¯å‘å¼é€‰æ‹©ç­–ç•¥çš„æŒ‘æˆ˜ï¼Œæ— æ³•è¦†ç›–å¤šæ ·åŒ–çš„ä»»åŠ¡éœ€æ±‚ï¼Œå¯¼è‡´æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„æ¢ç´¢-åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶è‡ªé€‚åº”åœ°é€‰æ‹©é€‚å½“çš„æ¼”ç¤ºä½œä¸ºä¸€ä¸ªæ•´ä½“ã€‚è¯¥æ¡†æ¶ä½¿LVLMsèƒ½å¤Ÿé€šè¿‡è‡ªæˆ‘æ¢ç´¢ä¸æ–­ä¼˜åŒ–æ¼”ç¤ºï¼Œè‡ªä¸»è¯†åˆ«å’Œç”Ÿæˆæœ€æœ‰æ•ˆçš„é€‰æ‹©ç­–ç•¥ï¼Œä»è€Œæé«˜å°‘æ ·æœ¬LVLMsçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ICLæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œé€šè¿‡æä¾›æ˜ç¡®çš„ä»»åŠ¡æŒ‡å¯¼å’Œç¤ºä¾‹æ¥å¢å¼ºå…¶åœ¨ä»»åŠ¡ç†è§£å’Œæ‰§è¡Œæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰ICLåœ¨LVLMsä¸Šçš„ç ”ç©¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä¾èµ–é¢„å®šä¹‰æ¼”ç¤ºå’Œå¯å‘å¼é€‰æ‹©ç­–ç•¥ï¼Œä»¥åŠæ— æ³•å»ºæ¨¡æ¼”ç¤ºä¹‹é—´çš„äº¤äº’ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¢«ç”¨æ¥æ¢ç´¢å¤šæ¨¡æ€ä¿¡æ¯èåˆå’Œè‡ªé€‚åº”æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ã€‚</li>
<li>è¯¥æ¡†æ¶å…è®¸LVLMsé€šè¿‡è‡ªæˆ‘æ¢ç´¢ä¼˜åŒ–æ¼”ç¤ºï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å››ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæé«˜å°‘æ ·æœ¬LVLMsçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09473">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff2b64565c7e13059c655f0c286728ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7287d8c1d8bf7371b93830961e43e433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bccf02786db3bf6d3441189c90cebf86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e8e6e46903f3c297ddab2aa9814bfa7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hidden-in-Plain-Sight-Evaluation-of-the-Deception-Detection-Capabilities-of-LLMs-in-Multimodal-Settings"><a href="#Hidden-in-Plain-Sight-Evaluation-of-the-Deception-Detection-Capabilities-of-LLMs-in-Multimodal-Settings" class="headerlink" title="Hidden in Plain Sight: Evaluation of the Deception Detection   Capabilities of LLMs in Multimodal Settings"></a>Hidden in Plain Sight: Evaluation of the Deception Detection   Capabilities of LLMs in Multimodal Settings</h2><p><strong>Authors:Md Messal Monem Miah, Adrita Anika, Xi Shi, Ruihong Huang</strong></p>
<p>Detecting deception in an increasingly digital world is both a critical and challenging task. In this study, we present a comprehensive evaluation of the automated deception detection capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) across diverse domains. We assess the performance of both open-source and commercial LLMs on three distinct datasets: real life trial interviews (RLTD), instructed deception in interpersonal scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the effectiveness of different experimental setups for deception detection, including zero-shot and few-shot approaches with random or similarity-based in-context example selection. Our results show that fine-tuned LLMs achieve state-of-the-art performance on textual deception detection tasks, while LMMs struggle to fully leverage cross-modal cues. Additionally, we analyze the impact of auxiliary features, such as non-verbal gestures and video summaries, and examine the effectiveness of different prompting strategies, including direct label generation and chain-of-thought reasoning. Our findings provide key insights into how LLMs process and interpret deceptive cues across modalities, highlighting their potential and limitations in real-world deception detection applications. </p>
<blockquote>
<p>åœ¨æ—¥ç›Šæ•°å­—åŒ–çš„ä¸–ç•Œä¸­æ£€æµ‹æ¬ºéª—æ˜¯ä¸€é¡¹æ—¢å…³é”®åˆå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨å„ä¸ªé¢†åŸŸä¸­çš„è‡ªåŠ¨åŒ–æ¬ºéª—æ£€æµ‹èƒ½åŠ›è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¼€æºå’Œå•†ä¸šLLMåœ¨ä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼šç°å®ç”Ÿæ´»å®¡åˆ¤è®¿è°ˆï¼ˆRLTDï¼‰ã€äººé™…åœºæ™¯ä¸­çš„æŒ‡ä»¤æ€§æ¬ºéª—ï¼ˆMU3Dï¼‰å’Œæ¬ºéª—æ€§è¯„è®ºï¼ˆOpSpamï¼‰ã€‚æˆ‘ä»¬ç³»ç»Ÿåˆ†æäº†ä¸åŒå®éªŒè®¾ç½®åœ¨æ¬ºéª—æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•é‡‡ç”¨éšæœºæˆ–åŸºäºç›¸ä¼¼æ€§çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬æ¬ºéª—æ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè€Œå¤šæ¨¡æ€æ¨¡å‹åœ¨å……åˆ†åˆ©ç”¨è·¨æ¨¡æ€çº¿ç´¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†è¾…åŠ©ç‰¹å¾ï¼ˆå¦‚éè¯­è¨€æ‰‹åŠ¿å’Œè§†é¢‘æ‘˜è¦ï¼‰çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†ä¸åŒçš„æç¤ºç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç›´æ¥æ ‡ç­¾ç”Ÿæˆå’Œé“¾å¼æ€ç»´æ¨ç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å’Œè§£é‡Šè·¨æ¨¡æ€çš„æ¬ºéª—çº¿ç´¢æä¾›äº†å…³é”®è§è§£ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œæ¬ºéª—æ£€æµ‹åº”ç”¨ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09424v1">PDF</a> Accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è‡ªåŠ¨åŒ–æ¬ºéª—æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹çœŸå®ç”Ÿæ´»å®¡åˆ¤è®¿è°ˆã€äººé™…åœºæ™¯ä¸­çš„æŒ‡ä»¤æ€§æ¬ºéª—å’Œæ¬ºéª—æ€§è¯„è®ºç­‰ä¸‰ä¸ªæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç³»ç»Ÿåˆ†æäº†ä¸åŒå®éªŒè®¾ç½®åœ¨æ¬ºéª—æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•ï¼Œä»¥åŠåŸºäºéšæœºæˆ–ç›¸ä¼¼æ€§çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç²¾ç»†è°ƒæ•´è¿‡çš„LLMsåœ¨æ–‡æœ¬æ¬ºéª—æ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè€ŒLMMsåœ¨å……åˆ†åˆ©ç”¨è·¨æ¨¡æ€çº¿ç´¢æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æ­¤å¤–ï¼Œè¿˜åˆ†æäº†è¾…åŠ©ç‰¹å¾ï¼ˆå¦‚éè¯­è¨€æ‰‹åŠ¿å’Œè§†é¢‘æ‘˜è¦ï¼‰å’Œä¸åŒæç¤ºç­–ç•¥ï¼ˆå¦‚ç›´æ¥æ ‡ç­¾ç”Ÿæˆå’Œé“¾å¼æ€ç»´æ¨ç†ï¼‰çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ä¸ºLLMsåœ¨è·¨æ¨¡æ€ä¸­å¤„ç†å’Œè§£è¯»æ¬ºéª—çº¿ç´¢æä¾›äº†å…³é”®è§è§£ï¼Œçªå‡ºäº†å…¶åœ¨çœŸå®ä¸–ç•Œæ¬ºéª—æ£€æµ‹åº”ç”¨ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–æ¬ºéª—æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬æ¬ºéª—æ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨åˆ©ç”¨è·¨æ¨¡æ€çº¿ç´¢æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéš¾ä»¥å®Œå…¨èåˆéè¯­è¨€ä¿¡æ¯å’Œè§†é¢‘æ‘˜è¦ç­‰è¾…åŠ©ç‰¹å¾ã€‚</li>
<li>é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•åœ¨æ¬ºéª—æ£€æµ‹å®éªŒä¸­è¡¨ç°å‡ºä¸åŒçš„æœ‰æ•ˆæ€§ï¼ŒåŸºäºéšæœºæˆ–ç›¸ä¼¼æ€§çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©ç­–ç•¥æ˜¯å…³é”®å› ç´ ä¹‹ä¸€ã€‚</li>
<li>ç›´æ¥æ ‡ç­¾ç”Ÿæˆå’Œé“¾å¼æ€ç»´æ¨ç†ç­‰ä¸åŒçš„æç¤ºç­–ç•¥å¯¹æ¬ºéª—æ£€æµ‹æœ‰å½±å“ã€‚</li>
<li>LLMsåœ¨å¤„ç†å’Œåˆ†ææ¬ºéª—çº¿ç´¢æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„æ½œåŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä¿¡æ¯æ—¶ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å…³äºLLMså¦‚ä½•è§£è¯»æ¬ºéª—çº¿ç´¢çš„æ·±å…¥è§è§£ï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥æ”¹è¿›å’Œä¼˜åŒ–è‡ªåŠ¨åŒ–æ¬ºéª—æ£€æµ‹æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61078d82f3027f4640bc63f2502d09d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27355ff566d18c7294f9265aa2e7f621.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SensorLM-Learning-the-Language-of-Wearable-Sensors"><a href="#SensorLM-Learning-the-Language-of-Wearable-Sensors" class="headerlink" title="SensorLM: Learning the Language of Wearable Sensors"></a>SensorLM: Learning the Language of Wearable Sensors</h2><p><strong>Authors:Yuwei Zhang, Kumar Ayush, Siyuan Qiao, A. Ali Heydari, Girish Narayanswamy, Maxwell A. Xu, Ahmed A. Metwally, Shawn Xu, Jake Garrison, Xuhai Xu, Tim Althoff, Yun Liu, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak Patel, Cecilia Mascolo, Xin Liu, Daniel McDuff, Yuzhe Yang</strong></p>
<p>We present SensorLM, a family of sensor-language foundation models that enable wearable sensor data understanding with natural language. Despite its pervasive nature, aligning and interpreting sensor data with language remains challenging due to the lack of paired, richly annotated sensor-text descriptions in uncurated, real-world wearable data. We introduce a hierarchical caption generation pipeline designed to capture statistical, structural, and semantic information from sensor data. This approach enabled the curation of the largest sensor-language dataset to date, comprising over 59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and recovers them as specific variants within a generic architecture. Extensive experiments on real-world tasks in human activity analysis and healthcare verify the superior performance of SensorLM over state-of-the-art in zero-shot recognition, few-shot learning, and cross-modal retrieval. SensorLM also demonstrates intriguing capabilities including scaling behaviors, label efficiency, sensor captioning, and zero-shot generalization to unseen tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SensorLMï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¼ æ„Ÿå™¨è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ç†è§£å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ã€‚å°½ç®¡å…¶æ™®åŠæ€§å¾ˆé«˜ï¼Œä½†ç”±äºæœªæ•´ç†çš„çœŸå®ä¸–ç•Œå¯ç©¿æˆ´æ•°æ®ä¸­ç¼ºä¹é…å¯¹çš„ä¸°å¯Œæ³¨é‡Šçš„ä¼ æ„Ÿå™¨æ–‡æœ¬æè¿°ï¼Œå°†ä¼ æ„Ÿå™¨æ•°æ®ä¸è¯­è¨€è¿›è¡Œå¯¹é½å’Œè§£é‡Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ ‡é¢˜ç”Ÿæˆç®¡é“ï¼Œæ—¨åœ¨ä»ä¼ æ„Ÿå™¨æ•°æ®ä¸­æ•è·ç»Ÿè®¡ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ•´ç†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¼ æ„Ÿå™¨è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡5970ä¸‡å°æ—¶çš„æ•°æ®ï¼Œæ¥è‡ªè¶…è¿‡10ä¸‡3åƒäººçš„æ•°æ®ã€‚æ­¤å¤–ï¼ŒSensorLMæ‰©å±•äº†è‘—åçš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¶æ„ï¼ˆä¾‹å¦‚CLIPï¼ŒCoCaï¼‰ï¼Œå°†å®ƒä»¬ä½œä¸ºé€šç”¨æ¶æ„å†…çš„ç‰¹å®šå˜ä½“è¿›è¡Œæ¢å¤ã€‚åœ¨äººç±»æ´»åŠ¨åˆ†æå’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„çœŸå®ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†SensorLMåœ¨é›¶æ ·æœ¬è¯†åˆ«ã€å°æ ·å­¦ä¹ æœ¬å’Œè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢çš„å“è¶Šæ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚SensorLMè¿˜å±•ç¤ºäº†ä»¤äººæ„Ÿå…´è¶£çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç¼©æ”¾è¡Œä¸ºã€æ ‡ç­¾æ•ˆç‡ã€ä¼ æ„Ÿå™¨æ ‡é¢˜å’Œé›¶æ ·æœ¬æ¨å¹¿åˆ°æœªè§ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09108v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æå‡ºä¸€ç§åä¸ºSensorLMçš„ä¼ æ„Ÿå™¨è¯­è¨€åŸºç¡€æ¨¡å‹å®¶æ—ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è‡ªç„¶è¯­è¨€ç†è§£å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ã€‚ç ”ç©¶è§£å†³äº†ç”±äºç¼ºä¹æœªåŠ å·¥çœŸå®ä¸–ç•Œå¯ç©¿æˆ´æ•°æ®çš„ä¸°å¯Œæ³¨é‡Šä¼ æ„Ÿå™¨æ–‡æœ¬æè¿°ï¼Œå¯¼è‡´ä¼ æ„Ÿå™¨æ•°æ®ä¸è¯­è¨€çš„å¯¹é½å’Œè§£é‡Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§åˆ†å±‚æ ‡é¢˜ç”Ÿæˆç®¡é“ï¼Œèƒ½å¤Ÿæ•è·ä¼ æ„Ÿå™¨æ•°æ®çš„ç»Ÿè®¡ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚æ­¤æ–¹æ³•æˆåŠŸæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¼ æ„Ÿå™¨è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡5970ä¸‡å°æ—¶çš„æ•°æ®ï¼Œæ¥è‡ªè¶…è¿‡10ä¸‡3åƒäººã€‚SensorLMæ‰©å±•äº†è‘—åçš„å¤šæ¨¡å¼é¢„è®­ç»ƒæ¶æ„ï¼ˆå¦‚CLIPã€CoCaï¼‰ï¼Œå¹¶åœ¨é€šç”¨æ¶æ„å†…å°†å…¶å›æ”¶ä¸ºç‰¹å®šå˜ä½“ã€‚åœ¨äººç±»æ´»åŠ¨åˆ†æå’ŒåŒ»ç–—ä¿å¥æ–¹é¢çš„å®é™…ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSensorLMåœ¨é›¶æ ·æœ¬è¯†åˆ«ã€å°æ ·å­¦ä¹ ä»¥åŠè·¨æ¨¡æ€æ£€ç´¢æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚SensorLMè¿˜å±•ç¤ºäº†æœ‰è¶£çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬è§„æ¨¡åŒ–è¡Œä¸ºã€æ ‡ç­¾æ•ˆç‡ã€ä¼ æ„Ÿå™¨æè¿°å’Œé›¶æ ·æœ¬æ¦‚æ‹¬æœªè§ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SensorLMæ˜¯ä¸€ç§ä¼ æ„Ÿå™¨è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨è‡ªç„¶è¯­è¨€ç†è§£å¯ç©¿æˆ´ä¼ æ„Ÿå™¨æ•°æ®ã€‚</li>
<li>ç¼ºä¹ä¸°å¯Œçš„æ³¨é‡Šä¼ æ„Ÿå™¨æ–‡æœ¬æè¿°æ˜¯è§£é‡Šä¼ æ„Ÿå™¨æ•°æ®ä¸è¯­è¨€çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåˆ†å±‚æ ‡é¢˜ç”Ÿæˆç®¡é“ï¼Œç”¨äºæ•è·ä¼ æ„Ÿå™¨æ•°æ®çš„ç»Ÿè®¡ã€ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>æˆåŠŸæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¼ æ„Ÿå™¨è¯­è¨€æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡çœŸå®ä¸–ç•Œçš„å¯ç©¿æˆ´æ•°æ®ã€‚</li>
<li>SensorLMæ‰©å±•äº†å¤šæ¨¡å¼é¢„è®­ç»ƒæ¶æ„ï¼Œå¹¶åœ¨é€šç”¨æ¶æ„å†…å…·æœ‰ç‰¹å®šå˜ä½“ã€‚</li>
<li>åœ¨å®é™…ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSensorLMåœ¨å¤šä¸ªæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬è¯†åˆ«ã€å°æ ·å­¦ä¹ ä»¥åŠè·¨æ¨¡æ€æ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac46ad82c288747669e0bb6040480226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1caf7dacc781bbc1d5b6590f8fbec84c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cc0d4d84ef47c411c27e232c99b3679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cdef689aa4bec8a91189571b9350877.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-350ee635d1b45feb65eeca022baf4293.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP"><a href="#AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP" class="headerlink" title="AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP"></a>AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP</h2><p><strong>Authors:Ahmed Hasanaath, Aisha Alansari, Ahmed Ashraf, Chafik Salmane, Hamzah Luqman, Saad Ezzini</strong></p>
<p>Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299">https://anonymous.4open.science/r/AraReasoner41299</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ä¸€èˆ¬ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†ä»¥ä¸°å¯Œçš„å½¢æ€ã€å¤šæ ·çš„æ–¹è¨€å’Œå¤æ‚çš„è„šæœ¬ä¸ºç‰¹ç‚¹çš„é˜¿æ‹‰ä¼¯æ•°æ®æ–¹é¢çš„æ€§èƒ½ä»ç„¶è¢«ä½ä¼°ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å¤šä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«å…³æ³¨æ–°æ¨å‡ºçš„DeepSeekæ¨¡å‹ï¼Œåœ¨åäº”ä¸ªé˜¿æ‹‰ä¼¯NLPä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å°è¯•äº†å„ç§ç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°åœ¨è¦†ç›–å„ç§åº”ç”¨çš„æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œä»¥æ£€éªŒå®ƒä»¬åœ¨ä¸åŒå¤æ‚ç¨‹åº¦ä¸‹çš„è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†å‡ ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œç²¾å¿ƒé€‰æ‹©ä»…ä¸‰ä¸ªä¸Šä¸‹æ–‡å®ä¾‹ï¼Œå¯ä»¥åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šå¹³å‡æé«˜è¶…è¿‡13ä¸ªF1ç‚¹â€”â€”æƒ…æ„Ÿåˆ†æä»35.3%æé«˜åˆ°87.5%ï¼ŒåŒä¹‰æ›¿æ¢æ£€æµ‹ä»56.1%æé«˜åˆ°87.0%ã€‚å…¶æ¬¡ï¼Œä»¥æ¨ç†ä¸ºé‡ç‚¹çš„DeepSeekæ¶æ„åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå¹³å‡æ¯”å¼ºå¤§çš„GPT o4-miniåŸºçº¿é«˜å‡º12ä¸ªF1ç‚¹ã€‚ç¬¬ä¸‰ï¼Œä¸æ¨¡å‹è§„æ¨¡çš„ç­‰æ•ˆå¢åŠ ç›¸æ¯”ï¼ŒåŸºäºLoRAçš„å¾®è°ƒåœ¨F1å’ŒBLEUä¸Šäº§ç”Ÿäº†é¢å¤–çš„é«˜è¾¾8ä¸ªç‚¹ã€‚ä»£ç å¯é€šè¿‡åŒ¿åè®¿é—®ï¼š[é“¾æ¥åœ¨æ­¤å¤„]ï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299%EF%BC%89%E3%80%82">https://anonymous.4open.science/r/AraReasoner41299ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08768v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”å¯¹é˜¿æ‹‰ä¼¯è¯­æ•°æ®ä¸Šå±•ç°å‡ºå“è¶Šè¿›æ­¥ï¼Œæ¶‰åŠä¸°å¯Œçš„å½¢æ€ã€å¤šæ ·çš„æ–¹è¨€å’Œå¤æ‚çš„è„šæœ¬ã€‚æœ¬æ–‡å…¨é¢è¯„ä¼°äº†å¤šä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„LLMæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°æ¨å‡ºçš„DeepSeekæ¨¡å‹ä¸Šï¼Œè·¨è¶Šåäº”é¡¹é˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä»…é€‰æ‹©ä¸‰ä¸ªä¸Šä¸‹æ–‡å®ä¾‹è¿›è¡Œå¾®è°ƒå¹³å‡æå‡äº†è¶…è¿‡13ä¸ªF1ç‚¹ï¼›DeepSeekæ¶æ„åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹å¹³å‡ä¼˜äºGPT o4-miniåŸºçº¿12ä¸ªF1ç‚¹ï¼›åŸºäºLoRAçš„å¾®è°ƒæŠ€æœ¯ç›¸æ¯”å¢åŠ æ¨¡å‹è§„æ¨¡æé«˜äº†é¢å¤–çš„8ä¸ªF1ç‚¹å’ŒBLEUåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—è¿›æ­¥ï¼Œä½†ä»éœ€æ›´å¤šæ¢ç´¢ã€‚</li>
<li>ç²¾å¿ƒé€‰æ‹©çš„ä¸‰ä¸ªä¸Šä¸‹æ–‡å®ä¾‹èƒ½æ˜¾è‘—æé«˜åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>DeepSeekæ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ã€‚</li>
<li>LoRA-basedå¾®è°ƒæŠ€æœ¯èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡æä¾›äº†è¯¦ç»†çš„å®éªŒæ–¹æ³•å’Œç»“æœï¼ŒåŒ…æ‹¬åœ¨å¤šä¸ªé˜¿æ‹‰ä¼¯è¯­NLPä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†é˜¿æ‹‰ä¼¯è¯­æ•°æ®çš„ç‹¬ç‰¹æ€§ï¼ŒåŒ…æ‹¬ä¸°å¯Œçš„å½¢æ€ã€å¤šæ ·çš„æ–¹è¨€å’Œå¤æ‚çš„è„šæœ¬ï¼Œå¯¹LLMçš„æŒ‘æˆ˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3fab19299e973f743c9de33d3072803f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1b4e28a76543540f761275a06b6abda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7b71d304e3eb728dd55864cbf2e976d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ClimateViz-A-Benchmark-for-Statistical-Reasoning-and-Fact-Verification-on-Scientific-Charts"><a href="#ClimateViz-A-Benchmark-for-Statistical-Reasoning-and-Fact-Verification-on-Scientific-Charts" class="headerlink" title="ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification   on Scientific Charts"></a>ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification   on Scientific Charts</h2><p><strong>Authors:Ruiran Su, Jiasheng Si, Zhijiang Guo, Janet B. Pierrehumbert</strong></p>
<p>Scientific fact-checking has mostly focused on text and tables, overlooking scientific charts, which are key for presenting quantitative evidence and statistical reasoning. We introduce ClimateViz, the first large-scale benchmark for scientific fact-checking using expert-curated scientific charts. ClimateViz contains 49,862 claims linked to 2,896 visualizations, each labeled as support, refute, or not enough information. To improve interpretability, each example includes structured knowledge graph explanations covering trends, comparisons, and causal relations. We evaluate state-of-the-art multimodal language models, including both proprietary and open-source systems, in zero-shot and few-shot settings. Results show that current models struggle with chart-based reasoning: even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to 77.8 percent accuracy in label-only settings, far below human performance (89.3 and 92.7 percent). Explanation-augmented outputs improve performance in some models. We released our dataset and code alongside the paper. </p>
<blockquote>
<p>ç§‘å­¦äº‹å®æ ¸æŸ¥ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬å’Œè¡¨æ ¼ä¸Šï¼Œå´å¿½è§†äº†ç§‘å­¦å›¾è¡¨è¿™ä¸€å‘ˆç°å®šé‡è¯æ®å’Œç»Ÿè®¡æ¨ç†çš„å…³é”®è¦ç´ ã€‚æˆ‘ä»¬æ¨å‡ºäº†ClimateVizï¼Œè¿™æ˜¯ä½¿ç”¨ä¸“å®¶åˆ¶ä½œçš„ç§‘å­¦å›¾è¡¨è¿›è¡Œäº‹å®æ ¸æŸ¥çš„é¦–ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚ClimateVizåŒ…å«ä¸å¯è§†åŒ–å†…å®¹ç›¸å…³çš„å£°æ˜æ•°é‡è¾¾49,862æ¡ï¼Œæ¶‰åŠå¯è§†åŒ–å›¾è¡¨æ•°é‡ä¸º2,896ä¸ªï¼Œæ¯ä¸ªå£°æ˜éƒ½è¢«æ ‡è®°ä¸ºæ”¯æŒã€åé©³æˆ–ä¿¡æ¯ä¸è¶³ã€‚ä¸ºæé«˜å¯è§£é‡Šæ€§ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½åŒ…å«ç»“æ„åŒ–çŸ¥è¯†å›¾è°±è§£é‡Šï¼Œæ¶µç›–è¶‹åŠ¿ã€æ¯”è¾ƒå’Œå› æœå…³ç³»ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„è·¨æ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸“æœ‰å’Œå¼€æºç³»ç»Ÿï¼Œåœ¨æ— é¢„è®¾æƒ…å†µå’Œå°‘é‡é¢„è®¾æƒ…å†µçš„ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜å½“å‰æ¨¡å‹åœ¨åŸºäºå›¾è¡¨çš„æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼šå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ç³»ç»Ÿå¦‚Gemini 2.5å’ŒInternVL 2.5åœ¨ä»…æ ‡ç­¾çš„ç¯å¢ƒä¸­å‡†ç¡®ç‡ä¹Ÿåªæœ‰76.2è‡³77.8%ï¼Œè¿œä½äºäººç±»çš„è¡¨ç°ï¼ˆåˆ†åˆ«ä¸º89.3%å’Œ92.7%ï¼‰ã€‚é€šè¿‡å¢åŠ è§£é‡Šæ€§çš„è¾“å‡ºèƒ½æé«˜éƒ¨åˆ†æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åŒæ—¶å‘å¸ƒäº†æ•°æ®é›†å’Œä»£ç ä»¥ä¾›å…¬ä¼—å‚è€ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08700v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ç§‘å­¦å›¾è¡¨çš„äº‹å®æ ¸æŸ¥ç ”ç©¶ã€‚ç”±äºç§‘å­¦äº‹å®æ ¸æŸ¥ä¸»è¦å…³æ³¨æ–‡æœ¬å’Œè¡¨æ ¼ï¼Œå¿½ç•¥äº†ç§‘å­¦å›¾è¡¨åœ¨å‘ˆç°å®šé‡è¯æ®å’Œç»Ÿè®¡æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œå› æ­¤å¼•å…¥äº†ClimateVizè¿™ä¸€å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ã€‚ClimateVizåŒ…å«ä¸“å®¶ç¼–åˆ¶çš„ç§‘å­¦å›¾è¡¨ï¼Œæ¶‰åŠæ°”å€™å˜åŒ–é¢†åŸŸçš„äº‹å®æ ¸æŸ¥ã€‚è¯„ä¼°äº†å¤šç§æ¨¡æ€çš„è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨å›¾è¡¨æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå³ä½¿æœ€å¥½çš„ç³»ç»Ÿä¹Ÿä»…è¾¾åˆ°çº¦77%çš„å‡†ç¡®ç‡ï¼Œè¿œä½äºäººç±»çš„è¡¨ç°ã€‚é€šè¿‡è§£é‡Šå¢å¼ºçš„è¾“å‡ºå¯æé«˜æŸäº›æ¨¡å‹çš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å·²éšè®ºæ–‡å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦äº‹å®æ ¸æŸ¥é€šå¸¸å¿½è§†ç§‘å­¦å›¾è¡¨çš„é‡è¦æ€§ï¼Œè¿™äº›å›¾è¡¨å¯¹äºå‘ˆç°å®šé‡è¯æ®å’Œç»Ÿè®¡æ¨ç†è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥ClimateVizä½œä¸ºå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«ä¸“å®¶ç¼–åˆ¶çš„ç§‘å­¦å›¾è¡¨ä¸æ°”å€™å˜åŒ–é¢†åŸŸçš„äº‹å®æ ¸æŸ¥ä¿¡æ¯ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°æ ·æœ¬æƒ…å¢ƒä¸‹çš„æ€§èƒ½ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨åŸºäºå›¾è¡¨çš„äº‹å®æ ¸æŸ¥æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå³ä½¿æœ€å¥½çš„ç³»ç»Ÿå‡†ç¡®ç‡ä¹Ÿåªæœ‰çº¦77%ã€‚</li>
<li>äººç±»è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå‡†ç¡®ç‡è¾¾åˆ°çº¦90%ã€‚</li>
<li>é€šè¿‡å¢åŠ è§£é‡Šå¢å¼ºè¾“å‡ºå¯æé«˜æŸäº›æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0e3f8ac1cb85e2e78527249026e7b447.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6368c28cf322b83e3b278bd330873e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14b370b22dae9dedaae604369dd5f8e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ff5ddf4df5725cb1dbb52e8812f57b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da32eeba9056854a8b1b600142fc4a1d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Distillation-from-Speech-and-Music-Representation-Models"><a href="#Multi-Distillation-from-Speech-and-Music-Representation-Models" class="headerlink" title="Multi-Distillation from Speech and Music Representation Models"></a>Multi-Distillation from Speech and Music Representation Models</h2><p><strong>Authors:Jui-Chiang Wei, Yi-Cheng Lin, Fabian Ritter-Gutierrez, Hung-yi Lee</strong></p>
<p>Real-world audio often mixes speech and music, yet models typically handle only one domain. This paper introduces a multi-teacher distillation framework that unifies speech and music models into a single one while significantly reducing model size. Our approach leverages the strengths of domain-specific teacher models, such as HuBERT for speech and MERT for music, and explores various strategies to balance both domains. Experiments across diverse tasks demonstrate that our model matches the performance of domain-specific models, showing the effectiveness of cross-domain distillation. Additionally, we conduct few-shot learning experiments, highlighting the need for general models in real-world scenarios where labeled data is limited. Our results show that our model not only performs on par with specialized models but also outperforms them in few-shot scenarios, proving that a cross-domain approach is essential and effective for diverse tasks with limited data. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„éŸ³é¢‘ç»å¸¸æ··åˆäº†è¯­éŸ³å’ŒéŸ³ä¹ï¼Œä½†æ¨¡å‹é€šå¸¸åªå¤„ç†å…¶ä¸­ä¸€ä¸ªé¢†åŸŸã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªå¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†è¯­éŸ³å’ŒéŸ³ä¹æ¨¡å‹ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ï¼Œå¹¶æ˜¾è‘—å‡å°äº†æ¨¡å‹å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•™å¸ˆæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¦‚ç”¨äºè¯­éŸ³çš„HuBERTå’Œç”¨äºéŸ³ä¹çš„MERTï¼Œå¹¶æ¢ç´¢äº†å„ç§ç­–ç•¥æ¥å¹³è¡¡è¿™ä¸¤ä¸ªé¢†åŸŸã€‚åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„æ€§èƒ½ä¸ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ç›¸åŒ¹é…ï¼Œè¯æ˜äº†è·¨åŸŸè’¸é¦çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å°æ ·æœ¬å­¦ä¹ å®éªŒï¼Œå¼ºè°ƒåœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­éœ€è¦é€šç”¨æ¨¡å‹ï¼Œå› ä¸ºè¿™äº›åœºæ™¯ä¸­æ ‡æ³¨æ•°æ®æœ‰é™ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…åœ¨æ€§èƒ½ä¸Šä¸ä¸“ç”¨æ¨¡å‹ç›¸å½“ï¼Œè€Œä¸”åœ¨å°æ ·æœ¬æ¥è‡ªä¸åŒé¢†åŸŸçš„æ•°æ®åœºæ™¯ä¸­è¡¨ç°æ›´å¥½ï¼Œè¯æ˜äº†è·¨åŸŸæ–¹æ³•å¯¹äºæœ‰é™æ•°æ®çš„å¤šæ ·ä»»åŠ¡çš„é‡è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07237v2">PDF</a> 8 pages, 1 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»Ÿä¸€äº†è¯­éŸ³å’ŒéŸ³ä¹æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘æ¨¡å‹å¤§å°çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡æ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚ç”¨äºè¯­éŸ³çš„HuBERTå’Œç”¨äºéŸ³ä¹çš„MERTï¼‰çš„ä¼˜åŠ¿ï¼Œå¹¶é€šè¿‡å„ç§ç­–ç•¥å¹³è¡¡ä¸¤ä¸ªé¢†åŸŸã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ ·ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ç›¸åŒ¹é…ï¼Œå¹¶ä¸”åœ¨æœ‰é™æ ‡ç­¾æ•°æ®çš„çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½ç”šè‡³è¶…è¶Šäº†ä¸“é—¨é¢†åŸŸçš„æ¨¡å‹ï¼Œè¯æ˜äº†è·¨åŸŸç­–ç•¥å¯¹æœ‰é™æ•°æ®å¤šæ ·åŒ–ä»»åŠ¡çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œæ•´åˆè¯­éŸ³å’ŒéŸ³ä¹æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç­–ç•¥å¹³è¡¡ä¸åŒé¢†åŸŸï¼Œåˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ•™å¸ˆæ¨¡å‹ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹å¤§å°æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶ä¿æŒæˆ–æå‡æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šæ ·ä»»åŠ¡ä¸Šï¼Œæ¨¡å‹æ€§èƒ½ä¸ç‰¹å®šé¢†åŸŸæ¨¡å‹ç›¸åŒ¹é…ã€‚</li>
<li>åœ¨æœ‰é™æ ‡ç­¾æ•°æ®çš„çœŸå®åœºæ™¯ä¸­ï¼Œæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œæ¨¡å‹æ€§èƒ½è¶…è¶Šä¸“é—¨é¢†åŸŸæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e572db084fabfe9cf13f04dfa7160aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ddb37f7c54f40fd5698c1156db8c15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf99e13d17b5b58b486e00209dde03a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-543992c7a6137dab54c8d60aba78096c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc50ffc3bccd04bd39d56d6a25360b39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9a879b6d376dc913c5a167d7f62742.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ab31b4609537f22bd290559c0a0c8c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-624f4672327fe8a4e8dd919ea52c1e18.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Efficient-Heuristics-Generation-for-Solving-Combinatorial-Optimization-Problems-Using-Large-Language-Models"><a href="#Efficient-Heuristics-Generation-for-Solving-Combinatorial-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Efficient Heuristics Generation for Solving Combinatorial Optimization   Problems Using Large Language Models"></a>Efficient Heuristics Generation for Solving Combinatorial Optimization   Problems Using Large Language Models</h2><p><strong>Authors:Xuan Wu, Di Wang, Chunguo Wu, Lijie Wen, Chunyan Miao, Yubin Xiao, You Zhou</strong></p>
<p>Recent studies exploited Large Language Models (LLMs) to autonomously generate heuristics for solving Combinatorial Optimization Problems (COPs), by prompting LLMs to first provide search directions and then derive heuristics accordingly. However, the absence of task-specific knowledge in prompts often leads LLMs to provide unspecific search directions, obstructing the derivation of well-performing heuristics. Moreover, evaluating the derived heuristics remains resource-intensive, especially for those semantically equivalent ones, often requiring omissible resource expenditure. To enable LLMs to provide specific search directions, we propose the Hercules algorithm, which leverages our designed Core Abstraction Prompting (CAP) method to abstract the core components from elite heuristics and incorporate them as prior knowledge in prompts. We theoretically prove the effectiveness of CAP in reducing unspecificity and provide empirical results in this work. To reduce computing resources required for evaluating the derived heuristics, we propose few-shot Performance Prediction Prompting (PPP), a first-of-its-kind method for the Heuristic Generation (HG) task. PPP leverages LLMs to predict the fitness values of newly derived heuristics by analyzing their semantic similarity to previously evaluated ones. We further develop two tailored mechanisms for PPP to enhance predictive accuracy and determine unreliable predictions, respectively. The use of PPP makes Hercules more resource-efficient and we name this variant Hercules-P. Extensive experiments across four HG tasks, five COPs, and eight LLMs demonstrate that Hercules outperforms the state-of-the-art LLM-based HG algorithms, while Hercules-P excels at minimizing required computing resources. In addition, we illustrate the effectiveness of CAP, PPP, and the other proposed mechanisms by conducting relevant ablation studies. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªä¸»ç”Ÿæˆè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPï¼‰çš„å¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡æç¤ºLLMé¦–å…ˆæä¾›æœç´¢æ–¹å‘ï¼Œç„¶åæ®æ­¤æ¨å¯¼å¯å‘å¼ã€‚ç„¶è€Œï¼Œæç¤ºä¸­ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†å¾€å¾€å¯¼è‡´LLMæä¾›éç‰¹å®šçš„æœç´¢æ–¹å‘ï¼Œé˜»ç¢é«˜æ€§èƒ½å¯å‘å¼çš„æ¨å¯¼ã€‚æ­¤å¤–ï¼Œè¯„ä¼°æ´¾ç”Ÿå¯å‘å¼ä»ç„¶éœ€è¦å¤§é‡èµ„æºï¼Œç‰¹åˆ«æ˜¯é‚£äº›è¯­ä¹‰ç­‰æ•ˆçš„å¯å‘å¼ï¼Œé€šå¸¸éœ€è¦å¤§é‡èµ„æºæ¶ˆè€—ã€‚ä¸ºäº†èƒ½å¤Ÿè®©LLMæä¾›å…·ä½“çš„æœç´¢æ–¹å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Herculesç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨æˆ‘ä»¬è®¾è®¡çš„æ ¸å¿ƒæŠ½è±¡æç¤ºï¼ˆCAPï¼‰æ–¹æ³•ä»ç²¾è‹±å¯å‘å¼æ–¹æ³•ä¸­æŠ½è±¡å‡ºæ ¸å¿ƒç»„ä»¶ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºå…ˆéªŒçŸ¥è¯†èå…¥æç¤ºä¸­ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†CAPåœ¨å‡å°‘éç‰¹å¼‚æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æœ¬æ–‡ä¸­æä¾›äº†å®è¯ç»“æœã€‚ä¸ºäº†é™ä½è¯„ä¼°æ´¾ç”Ÿå¯å‘å¼æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œæˆ‘ä»¬æå‡ºäº†é¦–åˆ›çš„å°‘é•œå¤´æ€§èƒ½é¢„æµ‹æç¤ºï¼ˆPPPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯å‘å¼ç”Ÿæˆï¼ˆHGï¼‰ä»»åŠ¡çš„æ–¹æ³•ã€‚PPPåˆ©ç”¨LLMé€šè¿‡åˆ†ææ–°æ´¾ç”Ÿå¯å‘å¼å’Œå·²è¯„ä¼°å¯å‘å¼ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥é¢„æµ‹æ–°å¯å‘å¼çš„é€‚åº”åº¦å€¼ã€‚æˆ‘ä»¬è¿˜é’ˆå¯¹PPPå¼€å‘äº†ä¸¤ä¸ªå®šåˆ¶æœºåˆ¶ï¼Œåˆ†åˆ«ç”¨äºæé«˜é¢„æµ‹ç²¾åº¦å’Œç¡®å®šä¸å¯é çš„é¢„æµ‹ã€‚PPPçš„ä½¿ç”¨ä½¿Herculesæ›´åŠ èµ„æºé«˜æ•ˆï¼Œæˆ‘ä»¬å°†è¿™ç§å˜ä½“å‘½åä¸ºHercules-Pã€‚åœ¨å››ä¸ªHGä»»åŠ¡ã€äº”ä¸ªCOPå’Œå…«ä¸ªLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHerculesä¼˜äºæœ€æ–°çš„LLMåŸºäºHGçš„ç®—æ³•ï¼Œè€ŒHercules-Påœ¨æœ€å°åŒ–æ‰€éœ€è®¡ç®—èµ„æºæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è¿›è¡Œç›¸å…³çš„æ¶ˆèç ”ç©¶ï¼Œè¯´æ˜äº†CAPã€PPPå’Œå…¶ä»–æè®®æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12627v2">PDF</a> Accepted by SIGKDD 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«ç”¨æ¥è‡ªä¸»ç”Ÿæˆè§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ï¼ˆCOPsï¼‰çš„å¯å‘å¼æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç‰¹å®šä»»åŠ¡çš„æç¤ºçŸ¥è¯†ï¼ŒLLMsç»å¸¸æä¾›ä¸å…·ä½“çš„æœç´¢æ–¹å‘ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†Herculesç®—æ³•ï¼Œé€šè¿‡æ ¸å¿ƒæŠ½è±¡æç¤ºæ³•ï¼ˆCAPï¼‰èå…¥ç²¾è‹±å¯å‘å¼æ–¹æ³•çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºå‡å°‘è¯„ä¼°æ´¾ç”Ÿå¯å‘å¼æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œè¿›ä¸€æ­¥æå‡ºäº†æ€§èƒ½é¢„æµ‹æç¤ºæ³•ï¼ˆPPPï¼‰ã€‚å®éªŒè¯æ˜ï¼ŒHerculesåŠå…¶å˜ä½“Hercules-Påœ¨æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¢«ç”¨äºè‡ªä¸»ç”Ÿæˆè§£å†³COPsçš„å¯å‘å¼æ–¹æ³•ï¼Œä½†ç¼ºä¹ç‰¹å®šä»»åŠ¡æç¤ºçŸ¥è¯†å¯¼è‡´æœç´¢æ–¹å‘ä¸æ˜ç¡®ã€‚</li>
<li>Herculesç®—æ³•é€šè¿‡å¼•å…¥CAPæ–¹æ³•ï¼Œèå…¥ç²¾è‹±å¯å‘å¼æ–¹æ³•çš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜LLMsæä¾›å…·ä½“æœç´¢æ–¹å‘çš„èƒ½åŠ›ã€‚</li>
<li>PPPæ–¹æ³•ç”¨äºå‡å°‘è¯„ä¼°æ´¾ç”Ÿå¯å‘å¼æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œé€šè¿‡åˆ†æè¯­ä¹‰ç›¸ä¼¼æ€§è¿›è¡Œæ€§èƒ½é¢„æµ‹ã€‚</li>
<li>Herculesç®—æ³•åŠå…¶å˜ä½“Hercules-Påœ¨æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ä¸Šè¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡å¹¿æ³›å®éªŒå¾—åˆ°éªŒè¯ã€‚</li>
<li>CAPã€PPPç­‰æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šè¿‡ç›¸å…³æ¶ˆèç ”ç©¶å¾—åˆ°è¯å®ã€‚</li>
<li>Herculesç®—æ³•å¯ä»¥åº”ç”¨äºå¤šä¸ªå¯å‘å¼ç”Ÿæˆä»»åŠ¡å’Œå¤šç§ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75237570ad972fc35470e5a0dd21b28d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c470456764cfa651a0029e4a69c269e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e814e76b1414867c4813eed5b69799ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-819ae5e5850262090f3a813caadca3ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91ce38bcde0474b5d1f1d05d91c5b27b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-brain-inspired-generative-model-for-EEG-based-cognitive-state-identification"><a href="#A-brain-inspired-generative-model-for-EEG-based-cognitive-state-identification" class="headerlink" title="A brain-inspired generative model for EEG-based cognitive state   identification"></a>A brain-inspired generative model for EEG-based cognitive state   identification</h2><p><strong>Authors:Bin Hu, Zhi-Hong Guan</strong></p>
<p>This article proposes a brain-inspired generative (BIG) model that merges an impulsive-attention neural network and a variational autoencoder (VAE) for identifying cognitive states based on electroencephalography (EEG) data. A hybrid learning method is presented for training the model by integrating gradient-based learning and heteroassociative memory. The BIG model is capable of achieving multi-task objectives: EEG classification, generating new EEG, and brain network interpretation, alleviating the limitations of excessive data training and high computational cost in conventional approaches. Experimental results on two public EEG datasets with different sampling rates demonstrate that the BIG model achieves a classification accuracy above 89%, comparable with state-of-the-art methods, while reducing computational cost by nearly 11% over the baseline EEGNet. Incorporating the generated EEG data for training, the BIG model exhibits comparative performance in a few-shot pattern. Ablation studies justify the poised brain-inspired characteristic regarding the impulsive-attention module and the hybrid learning method. Thanks to the performance advantages with interpretable outputs, this BIG model has application potential for building digital twins of the brain. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å—å¤§è„‘å¯å‘çš„ç”Ÿæˆï¼ˆBIGï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å†²åŠ¨æ³¨æ„åŠ›ç¥ç»ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œç”¨äºæ ¹æ®è„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®è¯†åˆ«è®¤çŸ¥çŠ¶æ€ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§æ··åˆå­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºæ¢¯åº¦çš„å­¦ä¹ å’Œå¼‚è”æƒ³è®°å¿†ã€‚BIGæ¨¡å‹èƒ½å¤Ÿå®Œæˆå¤šä»»åŠ¡ç›®æ ‡ï¼šEEGåˆ†ç±»ã€ç”Ÿæˆæ–°EEGä»¥åŠè§£é‡Šè„‘ç½‘ç»œï¼Œä»è€Œç¼“è§£äº†ä¼ ç»Ÿæ–¹æ³•ä¸­è¿‡åº¦æ•°æ®è®­ç»ƒå’Œè®¡ç®—æˆæœ¬è¿‡é«˜çš„å±€é™æ€§ã€‚åœ¨ä¸¤ä¸ªå…·æœ‰ä¸åŒé‡‡æ ·ç‡çš„å…¬å¼€EEGæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBIGæ¨¡å‹çš„åˆ†ç±»å‡†ç¡®ç‡é«˜äº89%ï¼Œä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶è¾ƒåŸºçº¿EEGNeté™ä½äº†è¿‘11%çš„è®¡ç®—æˆæœ¬ã€‚é€šè¿‡åˆ©ç”¨ç”Ÿæˆçš„EEGæ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒBIGæ¨¡å‹åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹å±•ç°å‡ºç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚æ¶ˆèç ”ç©¶è¯å®äº†å†²åŠ¨æ³¨æ„åŠ›æ¨¡å—å’Œæ··åˆå­¦ä¹ æ–¹æ³•åœ¨å—å¤§è„‘å¯å‘æ–¹é¢çš„ç‰¹ç‚¹ã€‚ç”±äºå…·æœ‰æ€§èƒ½ä¼˜åŠ¿å’Œå¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œè¿™ä¸€BIGæ¨¡å‹åœ¨æ„å»ºè„‘çš„æ•°å­—åŒèƒèƒæ–¹é¢å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01685v2">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§å—å¤§è„‘å¯å‘çš„ç”Ÿæˆï¼ˆBIGï¼‰æ¨¡å‹ï¼Œå®ƒå°†å†²åŠ¨æ³¨æ„åŠ›ç¥ç»ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç›¸ç»“åˆï¼ŒåŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®è¯†åˆ«è®¤çŸ¥çŠ¶æ€ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ç§æ··åˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ¢¯åº¦å­¦ä¹ å’Œå¼‚è”æƒ³è®°å¿†æ¥è®­ç»ƒæ¨¡å‹ã€‚BIGæ¨¡å‹èƒ½å¤Ÿå®Œæˆå¤šä»»åŠ¡ç›®æ ‡ï¼šEEGåˆ†ç±»ã€ç”Ÿæˆæ–°çš„EEGä»¥åŠè§£é‡Šè„‘ç½‘ç»œï¼Œç¼“è§£äº†ä¼ ç»Ÿæ–¹æ³•ä¸­è¿‡åº¦æ•°æ®è®­ç»ƒå’Œè®¡ç®—æˆæœ¬é«˜çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰ä¸åŒé‡‡æ ·ç‡çš„ä¸¤ä¸ªå…¬å…±EEGæ•°æ®é›†ä¸Šï¼ŒBIGæ¨¡å‹çš„åˆ†ç±»å‡†ç¡®ç‡é«˜äº89%ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ¯”åŸºçº¿EEGNeté™ä½äº†è¿‘11%ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ç”Ÿæˆçš„EEGæ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒBIGæ¨¡å‹åœ¨å°‘æ•°æ ·æœ¬æƒ…å†µä¸‹å±•ç°å‡ºæ¯”è¾ƒä¼˜ç§€çš„æ€§èƒ½ã€‚å¯¹å†²åŠ¨æ³¨æ„åŠ›æ¨¡å—å’Œæ··åˆå­¦ä¹ æ–¹æ³•çš„å‰–æç ”ç©¶è¯å®äº†å…¶ä½œä¸ºå—å¤§è„‘å¯å‘çš„æ¨¡å‹çš„æ½œåŠ›ã€‚ç”±äºå…¶åœ¨æ€§èƒ½æ–¹é¢çš„ä¼˜åŠ¿ä»¥åŠå¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œè¯¥BIGæ¨¡å‹åœ¨æ„å»ºå¤§è„‘çš„æ•°å­—åŒ–åŒèƒèƒæ–¹é¢å…·æœ‰åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è„‘å¯å‘ç”Ÿæˆï¼ˆBIGï¼‰æ¨¡å‹ï¼Œèåˆäº†å†²åŠ¨æ³¨æ„åŠ›ç¥ç»ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ··åˆå­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†æ¢¯åº¦å­¦ä¹ å’Œå¼‚è”æƒ³è®°å¿†ã€‚</li>
<li>BIGæ¨¡å‹å¯å®ç°å¤šä»»åŠ¡ç›®æ ‡ï¼šEEGåˆ†ç±»ã€ç”Ÿæˆæ–°EEGä»¥åŠè§£é‡Šè„‘ç½‘ç»œã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…¬å…±EEGæ•°æ®é›†ä¸Šï¼ŒBIGæ¨¡å‹åˆ†ç±»å‡†ç¡®ç‡é«˜ä¸”è®¡ç®—æˆæœ¬ä½ã€‚</li>
<li>åœ¨å°‘æ•°æ ·æœ¬æƒ…å†µä¸‹ï¼ŒBIGæ¨¡å‹è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>å†²åŠ¨æ³¨æ„åŠ›æ¨¡å—å’Œæ··åˆå­¦ä¹ æ–¹æ³•çš„å‰–æç ”ç©¶è¯æ˜äº†è¯¥æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c13e77845d60d1d39f8411ecf30f141.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-254e8b1bd893ced0abcbd6ee53784532.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d47297884bf5b4fbd6d697b004e7470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87951d93fff3a2199d579fa0817b3e70.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation"><a href="#TerraMind-Large-Scale-Generative-Multimodality-for-Earth-Observation" class="headerlink" title="TerraMind: Large-Scale Generative Multimodality for Earth Observation"></a>TerraMind: Large-Scale Generative Multimodality for Earth Observation</h2><p><strong>Authors:Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, Rahul Ramachandran, Paolo Fraccaro, Thomas Brunschwiler, Gabriele Cavallaro, Juan Bernabe-Moreno, Nicolas LongÃ©pÃ©</strong></p>
<p>We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMindâ€™s dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces â€œThinking-in-Modalitiesâ€ (TiM) â€“ the capability of generating additional artificial data during finetuning and inference to improve the model output â€“ and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºTerraMindï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰çš„ä»»æ„åˆ°ä»»æ„ç”Ÿæˆã€å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ã€‚ä¸å…¶ä»–å¤šæ¨¡å¼æ¨¡å‹ä¸åŒï¼ŒTerraMindæ˜¯åœ¨åŒå°ºåº¦è¡¨ç¤ºä¸Šé¢„è®­ç»ƒçš„ï¼Œç»“åˆäº†è·¨æ¨¡å¼çš„ä»¤ç‰Œçº§åˆ«å’Œåƒç´ çº§åˆ«æ•°æ®ã€‚åœ¨ä»¤ç‰Œçº§åˆ«ä¸Šï¼ŒTerraMindç¼–ç é«˜çº§ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å­¦ä¹ è·¨æ¨¡å¼å…³ç³»ï¼Œè€Œåœ¨åƒç´ çº§åˆ«ä¸Šï¼ŒTerraMindåˆ©ç”¨ç²¾ç»†è¡¨ç¤ºæ¥æ•æ‰å…³é”®çš„ç©ºé—´ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬åœ¨å…¨çƒå¤§è§„æ¨¡æ•°æ®é›†çš„ä¹ä¸ªåœ°ç†ç©ºé—´æ¨¡å¼ä¸Šé¢„è®­ç»ƒäº†TerraMindã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ï¼ˆiï¼‰TerraMindçš„åŒå°ºåº¦æ—©æœŸèåˆæ–¹æ³•è§£é”äº†ä¸€ç³»åˆ—é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœ°çƒè§‚æµ‹åº”ç”¨ï¼Œï¼ˆiiï¼‰TerraMindå¼•å…¥äº†â€œæ€è€ƒæ¨¡å¼â€ï¼ˆThinking-in-Modalitiesï¼Œç®€ç§°TiMï¼‰â€”â€”åœ¨å¾®è°ƒï¼ˆfinetuningï¼‰å’Œæ¨æ–­è¿‡ç¨‹ä¸­ç”Ÿæˆé¢å¤–çš„æ¨¡æ‹Ÿæ•°æ®çš„èƒ½åŠ›ï¼Œä»¥æé«˜æ¨¡å‹è¾“å‡ºçš„è´¨é‡â€”â€”ä»¥åŠï¼ˆiiiï¼‰TerraMindåœ¨åƒæ³›åœ°çƒï¼ˆPANGAEAï¼‰è¿™æ ·çš„åœ°çƒè§‚æµ‹ç¤¾åŒºæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¶…è¶Šæœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚é¢„è®­ç»ƒæ•°æ®é›†ã€æ¨¡å‹æƒé‡å’Œæˆ‘ä»¬çš„ä»£ç éƒ½æ˜¯åœ¨è®¸å¯ä¸‹å¼€æºçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11171v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TerraMindæ˜¯é¦–ä¸ªé¢å‘åœ°çƒè§‚æµ‹çš„ä»»æ„è¾“å…¥ä»»æ„è¾“å‡ºçš„ç”Ÿæˆå¼ã€å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚å®ƒé‡‡ç”¨åŒå°ºåº¦è¡¨ç¤ºæ³•ï¼Œç»“åˆæ ‡è®°çº§åˆ«å’Œåƒç´ çº§åˆ«çš„æ•°æ®è·¨æ¨¡æ€è¿›è¡Œé¢„è®­ç»ƒã€‚TerraMindçš„é¢„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹æƒé‡éƒ½æ˜¯å¼€æºçš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TerraMindæ˜¯é¦–ä¸ªåœ°çƒè§‚æµ‹é¢†åŸŸçš„ä»»æ„è‡³ä»»æ„ç”Ÿæˆå¼å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>TerraMindé‡‡ç”¨åŒå°ºåº¦é¢„è®­ç»ƒæ–¹æ³•ï¼Œç»“åˆæ ‡è®°çº§åˆ«å’Œåƒç´ çº§åˆ«çš„æ•°æ®è·¨æ¨¡æ€è¿›è¡Œè®­ç»ƒã€‚</li>
<li>TerraMindèƒ½åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹åº”ç”¨åœ¨å„ç§åœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­ã€‚</li>
<li>TerraMindå¼•å…¥â€œæ¨¡æ€æ€è€ƒâ€ï¼ˆThinking-in-Modalitiesï¼ŒTiMï¼‰èƒ½åŠ›ï¼Œåœ¨å¾®è°ƒåŠæ¨æ–­è¿‡ç¨‹ä¸­ç”Ÿæˆé¢å¤–çš„æ¨¡æ‹Ÿæ•°æ®ä»¥æé«˜æ¨¡å‹è¾“å‡ºè´¨é‡ã€‚</li>
<li>TerraMindåœ¨åƒPANGAEAè¿™æ ·çš„ç¤¾åŒºæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>TerraMindçš„é¢„è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹æƒé‡éƒ½æ˜¯å¼€æºçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-490e04fcb5aae12073469e393d6f1879.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3347939d427941279cfe77bf0ababf97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b652173cc8f70650183827763fd12d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36245c755aff9a356a0b6cede4891cc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7536641f71f28dded866d249d4218319.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="7B-Fully-Open-Source-Moxin-LLM-VLM-â€“-From-Pretraining-to-GRPO-based-Reinforcement-Learning-Enhancement"><a href="#7B-Fully-Open-Source-Moxin-LLM-VLM-â€“-From-Pretraining-to-GRPO-based-Reinforcement-Learning-Enhancement" class="headerlink" title="7B Fully Open Source Moxin-LLM&#x2F;VLM â€“ From Pretraining to GRPO-based   Reinforcement Learning Enhancement"></a>7B Fully Open Source Moxin-LLM&#x2F;VLM â€“ From Pretraining to GRPO-based   Reinforcement Learning Enhancement</h2><p><strong>Authors:Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang</strong></p>
<p>Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin Reasoning model. Moreover, we develop our vision language model based on our Moxin model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å†äº†ä¸€æ¬¡é‡å¤§å˜é©ï¼Œå…¶å—æ¬¢è¿ç¨‹åº¦å’Œèƒ½åŠ›éƒ½è¿…é€Ÿä¸Šå‡ã€‚å¼•é¢†è¿™ä¸€å˜é©çš„æ˜¯åƒGPT-4å’ŒGPT-o1è¿™æ ·çš„ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬å‡­å€Ÿå‡ºè‰²çš„æ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚åŒæ—¶ï¼Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚LLaMAï¼Œç”±äºå…¶æ˜“äºå®šåˆ¶å’Œéƒ¨ç½²åœ¨å„ç§åº”ç”¨ç¨‹åºä¸­ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ—¥ç›Šæ™®åŠåšå‡ºäº†å·¨å¤§è´¡çŒ®ã€‚å°½ç®¡å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºåˆ›æ–°å’Œç ”å‘æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹çš„å•†ä¸šåŒ–å¼•å‘äº†å…³äºé€æ˜åº¦ã€å¯é‡å¤æ€§å’Œå®‰å…¨çš„æ‹…å¿§ã€‚è®¸å¤šå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹æœªèƒ½æ»¡è¶³åŸºæœ¬çš„é€æ˜åº¦è¦æ±‚ï¼Œéšç’äº†å…³é”®ç»„ä»¶ï¼Œå¦‚è®­ç»ƒä»£ç å’Œæ•°æ®ï¼Œè¿™å¯èƒ½é˜»ç¢å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›ä¸€æ­¥åˆ›æ–°ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Moxin 7Bï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œéµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æ”¾æºä»£ç ã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®çš„åŸåˆ™ã€‚æˆ‘ä»¬å‘å¸ƒäº†é¢„è®­ç»ƒä»£ç å’Œé…ç½®ã€è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†ä»¥åŠä¸­é—´å’Œæœ€ç»ˆæ£€æŸ¥ç‚¹ï¼Œè‡´åŠ›äºå®Œå…¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒç»­æ‰¿è¯ºã€‚åœ¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹åï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€æ–°çš„åè®­ç»ƒæ¡†æ¶å’ŒæŒ‡ä»¤æ•°æ®å¯¹MoxinåŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è·å¾—MoxinæŒ‡ä»¤æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨æ¥è‡ªDeepSeek R1çš„è’¸é¦æ€ç»´é“¾æ•°æ®å¯¹æŒ‡ä»¤æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç„¶åéµå¾ªDeepSeek R1ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å†æ¬¡å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¾—åˆ°Moxinæ¨ç†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŸºäºMoxinæ¨¡å‹å¼€å‘äº†æˆ‘ä»¬çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ã€å°æ ·ä¾‹è¯„ä¼°å’Œæ€ç»´é“¾è¯„ä¼°ç­‰å„ç§è¯„ä¼°ä¸­å‡å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06845v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸæ­£ç»å†å¿«é€Ÿçš„å‘å±•ä¸å˜é©ï¼Œå…¶ä¸­ä»¥GPT-4å’ŒGPT-o1ç­‰ä¸“æœ‰LLMä»¥åŠLLaMAç­‰å¼€æºLLMä¸ºä»£è¡¨ã€‚ç„¶è€Œï¼Œå•†ä¸šåŒ–çš„LLMåœ¨é€æ˜åº¦ã€å¯å¤åˆ¶æ€§å’Œå®‰å…¨æ€§æ–¹é¢å¼•å‘å…³æ³¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºMoxin 7Bè¿™ä¸€éµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®åŸåˆ™çš„å…¨å¼€æºLLMã€‚å…¶æ€§èƒ½é€šè¿‡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ã€ä½¿ç”¨æœ€ä½³å®è·µçš„åè®­ç»ƒæ¡†æ¶å’ŒæŒ‡ä»¤æ•°æ®å¾®è°ƒã€ç»“åˆé€šè¿‡DeepSeek R1æç‚¼çš„æ€ç»´é“¾æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒï¼Œå¹¶é‡‡ç”¨Group Relative Policy Optimization (GRPO)è¿›è¡Œä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾è¯„ä¼°ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸæ­£ç»å†å¿«é€Ÿå‘å±•ï¼Œä¸“æœ‰å’Œå¼€æºLLMå‡å—åˆ°å…³æ³¨ã€‚</li>
<li>å•†ä¸šåŒ–LLMçš„é€æ˜åº¦ã€å¯å¤åˆ¶æ€§å’Œå®‰å…¨æ€§å¼•å‘å…³æ³¨ã€‚</li>
<li>Moxin 7Bæ˜¯ä¸€ä¸ªå…¨å¼€æºçš„LLMï¼Œéµå¾ªå…¬å¼€ç§‘å­¦ã€å¼€æºã€å¼€æ”¾æ•°æ®å’Œå¼€æ”¾è®¿é—®åŸåˆ™ã€‚</li>
<li>Moxin 7Bæ€§èƒ½é€šè¿‡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡ä»¤æ•°æ®å’Œæ€ç»´é“¾æ•°æ®å¾®è°ƒæ¥æå‡ã€‚</li>
<li>Group Relative Policy Optimization (GRPO)ç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Moxin 7Bæ¨¡å‹åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€ç»´é“¾è¯„ä¼°ä¸­è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9cf377d7903e43a0bd604177bc3840e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10394cc63cdb90605fff5feef7f244c7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PEFTGuard-Detecting-Backdoor-Attacks-Against-Parameter-Efficient-Fine-Tuning"><a href="#PEFTGuard-Detecting-Backdoor-Attacks-Against-Parameter-Efficient-Fine-Tuning" class="headerlink" title="PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient   Fine-Tuning"></a>PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient   Fine-Tuning</h2><p><strong>Authors:Zhen Sun, Tianshuo Cong, Yule Liu, Chenhao Lin, Xinlei He, Rongmao Chen, Xingshuo Han, Xinyi Huang</strong></p>
<p>Fine-tuning is an essential process to improve the performance of Large Language Models (LLMs) in specific domains, with Parameter-Efficient Fine-Tuning (PEFT) gaining popularity due to its capacity to reduce computational demands through the integration of low-rank adapters. These lightweight adapters, such as LoRA, can be shared and utilized on open-source platforms. However, adversaries could exploit this mechanism to inject backdoors into these adapters, resulting in malicious behaviors like incorrect or harmful outputs, which pose serious security risks to the community. Unfortunately, few current efforts concentrate on analyzing the backdoor patterns or detecting the backdoors in the adapters. To fill this gap, we first construct and release PADBench, a comprehensive benchmark that contains 13,300 benign and backdoored adapters fine-tuned with various datasets, attack strategies, PEFT methods, and LLMs. Moreover, we propose PEFTGuard, the first backdoor detection framework against PEFT-based adapters. Extensive evaluation upon PADBench shows that PEFTGuard outperforms existing detection methods, achieving nearly perfect detection accuracy (100%) in most cases. Notably, PEFTGuard exhibits zero-shot transferability on three aspects, including different attacks, PEFT methods, and adapter ranks. In addition, we consider various adaptive attacks to demonstrate the high robustness of PEFTGuard. We further explore several possible backdoor mitigation defenses, finding fine-mixing to be the most effective method. We envision that our benchmark and method can shed light on future LLM backdoor detection research. </p>
<blockquote>
<p>å¾®è°ƒæ˜¯æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸæ€§èƒ½çš„å…³é”®è¿‡ç¨‹ã€‚ç”±äºå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰èƒ½å¤Ÿé€šè¿‡é›†æˆä½é˜¶é€‚é…å™¨é™ä½è®¡ç®—éœ€æ±‚ï¼Œå› æ­¤è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚è¿™äº›è½»é‡çº§é€‚é…å™¨å¦‚LoRAå¯å…±äº«å¹¶åœ¨å¼€æºå¹³å°ä¸Šä½¿ç”¨ã€‚ç„¶è€Œï¼Œå¯¹æ‰‹å¯èƒ½ä¼šåˆ©ç”¨è¿™ä¸€æœºåˆ¶åœ¨è¿™äº›é€‚é…å™¨ä¸­æ³¨å…¥åé—¨ï¼Œå¯¼è‡´é”™è¯¯æˆ–æœ‰å®³è¾“å‡ºç­‰æ¶æ„è¡Œä¸ºï¼Œç»™ç¤¾åŒºå¸¦æ¥ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚é—æ†¾çš„æ˜¯ï¼Œç›®å‰å¾ˆå°‘æœ‰å·¥ä½œå…³æ³¨åˆ†æåé—¨æ¨¡å¼æˆ–æ£€æµ‹é€‚é…å™¨ä¸­çš„åé—¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºå¹¶å‘å¸ƒäº†PADBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«ç”¨å„ç§æ•°æ®é›†ã€æ”»å‡»ç­–ç•¥ã€PEFTæ–¹æ³•å’ŒLLMå¾®è°ƒå¾—åˆ°çš„13,300ä¸ªè‰¯æ€§åŠè¢«åé—¨æ”»å‡»çš„é€‚é…å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŸºäºPEFTçš„é€‚é…å™¨çš„é¦–ä¸ªåé—¨æ£€æµ‹æ¡†æ¶PEFTGuardã€‚åœ¨PADBenchä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒPEFTGuardåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¶…è¿‡äº†ç°æœ‰çš„æ£€æµ‹æ–¹æ³•ï¼Œå‡ ä¹è¾¾åˆ°äº†å®Œç¾çš„æ£€æµ‹ç²¾åº¦ï¼ˆ100%ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPEFTGuardåœ¨æ”»å‡»ã€PEFTæ–¹æ³•å’Œé€‚é…å™¨ç­‰çº§æ–¹é¢å‡å±•ç°å‡ºé›¶å°„å‡»è½¬ç§»èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å„ç§è‡ªé€‚åº”æ”»å‡»æ¥å±•ç¤ºPEFTGuardçš„é«˜ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å‡ ç§å¯èƒ½çš„åé—¨ç¼“è§£é˜²å¾¡æªæ–½ï¼Œå‘ç°fine-mixingæ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬æœŸæœ›æˆ‘ä»¬çš„åŸºå‡†å’Œæ–¹æ³•èƒ½ä¸ºæœªæ¥çš„LLMåé—¨æ£€æµ‹ç ”ç©¶æä¾›å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17453v2">PDF</a> 21 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒæ˜¯æå‡å…¶åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°çš„å…³é”®æµç¨‹ï¼Œå‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰å› å…¶èƒ½å¤Ÿå€ŸåŠ©ä½é˜¶é€‚é…å™¨å‡å°‘è®¡ç®—éœ€æ±‚è€Œå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼Œè¿™äº›è½»é‡çº§é€‚é…å™¨å­˜åœ¨å®‰å…¨é£é™©ï¼Œå¯èƒ½ä¼šè¢«å¯¹æ‰‹åˆ©ç”¨æ³¨å…¥åé—¨ï¼Œå¯¼è‡´æ¨¡å‹è¾“å‡ºé”™è¯¯æˆ–æœ‰å®³ä¿¡æ¯ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ„å»ºäº†PADBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œå¹¶æ¨å‡ºäº†PEFTGuardåé—¨æ£€æµ‹æ¡†æ¶ã€‚è¯„ä»·æ˜¾ç¤ºï¼ŒPEFTGuardåœ¨å¤šæ•°æƒ…å†µä¸‹è¡¨ç°å‡ºè¿‘å®Œç¾çš„æ£€æµ‹ç²¾åº¦ï¼Œå¹¶ä¸”å…·æœ‰é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ï¼Œèƒ½åº”å¯¹ä¸åŒæ”»å‡»ã€PEFTæ–¹æ³•å’Œé€‚é…å™¨ç­‰çº§ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æ¢ç´¢äº†å‡ ç§å¯èƒ½çš„åé—¨ç¼“è§£é˜²å¾¡æªæ–½ï¼Œå‘ç°fine-mixingæ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PEFTæˆä¸ºæå‡LLMåœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°çš„å…³é”®æµç¨‹ï¼Œå¹¶å› å‡å°‘è®¡ç®—éœ€æ±‚è€Œå—åˆ°æ¬¢è¿ã€‚</li>
<li>LoRAç­‰è½»é‡çº§é€‚é…å™¨å­˜åœ¨å®‰å…¨é£é™©ï¼Œå¯èƒ½è¢«å¯¹æ‰‹åˆ©ç”¨æ³¨å…¥åé—¨ã€‚</li>
<li>æ„å»ºäº†PADBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«è‰¯æ€§åŠè¢«åé—¨çš„é€‚é…å™¨ã€‚</li>
<li>æ¨å‡ºäº†PEFTGuardåé—¨æ£€æµ‹æ¡†æ¶ï¼Œè¡¨ç°å‡ºé«˜æ£€æµ‹ç²¾åº¦å’Œé›¶æ ·æœ¬è¿ç§»èƒ½åŠ›ã€‚</li>
<li>PEFTGuardèƒ½æœ‰æ•ˆåº”å¯¹ä¸åŒæ”»å‡»ã€PEFTæ–¹æ³•å’Œé€‚é…å™¨ç­‰çº§ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†å¤šç§åé—¨ç¼“è§£æªæ–½ï¼Œå‘ç°fine-mixingæ˜¯æœ€æœ‰æ•ˆçš„é˜²å¾¡æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fdf297f5fe22b16b5218bf971883f940.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-377e9cb69036c26bdb1879b3cdba9a85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db04a037bccddcdb295dc6941f9d4c01.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a92a027269a964f051e7652c965bede3.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  MetricHMR Metric Human Mesh Recovery from Monocular Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ab1bc245e0ca943e7cf083d750d46a2b.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  MTVQA Benchmarking Multilingual Text-Centric Visual Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
