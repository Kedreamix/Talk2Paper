<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Text-Aware Image Restoration with Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d71fa48e29ad67bf5c0448791478be58.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="Text-Aware-Image-Restoration-with-Diffusion-Models"><a href="#Text-Aware-Image-Restoration-with-Diffusion-Models" class="headerlink" title="Text-Aware Image Restoration with Diffusion Models"></a>Text-Aware Image Restoration with Diffusion Models</h2><p><strong>Authors:Jaewon Min, Jin Hyeon Kim, Paul Hyunbin Cho, Jaeeun Lee, Jihye Park, Minkyu Park, Sangpil Kim, Hyunhee Park, Seungryong Kim</strong></p>
<p>Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/TAIR/">https://cvlab-kaist.github.io/TAIR/</a> </p>
<blockquote>
<p>å›¾åƒä¿®å¤æ—¨åœ¨æ¢å¤é€€åŒ–å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„ä¿®å¤æ–¹æ³•è™½ç„¶åœ¨è‡ªç„¶å›¾åƒä¿®å¤ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨é€€åŒ–å›¾åƒçš„æ–‡æœ¬åŒºåŸŸé‡å»ºä¸­å´ç»å¸¸é¢ä¸´å›°éš¾ã€‚è¿™äº›æ–¹æ³•ç»å¸¸ç”Ÿæˆåˆç†ä½†é”™è¯¯çš„æ–‡æœ¬æ¨¡å¼ï¼Œæˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºæ–‡æœ¬å›¾åƒå¹»è§‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰è¿™ä¸€æ–°çš„ä¿®å¤ä»»åŠ¡ï¼Œå®ƒè¦æ±‚åŒæ—¶æ¢å¤è§†è§‰å†…å®¹å’Œæ–‡æœ¬ä¿çœŸåº¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SA-Textï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«åä¸‡å¼ é«˜è´¨é‡åœºæ™¯å›¾åƒï¼Œå¹¶å¯†é›†æ ‡æ³¨äº†å¤šæ ·ä¸”å¤æ‚çš„æ–‡æœ¬å®ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶TeReDiffï¼Œå®ƒå°†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾é›†æˆåˆ°æ–‡æœ¬è¯†åˆ«æ¨¡å—ä¸­ï¼Œä½¿ä¸¤ä¸ªç»„ä»¶éƒ½èƒ½ä»è”åˆè®­ç»ƒä¸­å—ç›Šã€‚è¿™å…è®¸æå–ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºå½¢å¼ï¼Œå¹¶å°†å…¶ç”¨ä½œåç»­å»å™ªæ­¥éª¤ä¸­çš„æç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„ä¿®å¤æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®ç‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ›´å¤šè¯¦æƒ…è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/TAIR/%EF%BC%88%E6%B3%A8%EF%BC%9A%E7%94%B1%E4%BA%8E%E6%B6%89%E5%8F%8A%E5%88%B0%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E5%92%8C%E6%A6%82%E5%BF%B5%EF%BC%8C%E7%BF%BB%E8%AF%91%E6%97%B6%E5%8F%AF%E8%83%BD%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E4%B8%80%E5%AE%9A%E7%9A%84%E8%A7%A3%E9%87%8A%E5%92%8C%E8%A1%A5%E5%85%85%E3%80%82%EF%BC%89">https://cvlab-kaist.github.io/TAIR/ï¼ˆæ³¨ï¼šç”±äºæ¶‰åŠåˆ°ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µï¼Œç¿»è¯‘æ—¶å¯èƒ½éœ€è¦è¿›è¡Œä¸€å®šçš„è§£é‡Šå’Œè¡¥å……ã€‚ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09993v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/TAIR/">https://cvlab-kaist.github.io/TAIR/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¿®å¤æ—¨åœ¨æ¢å¤é€€åŒ–å›¾åƒï¼Œä½†ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒä¿®å¤æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸçš„åŒæ—¶ï¼Œåœ¨æ¢å¤å›¾åƒä¸­çš„æ–‡æœ¬åŒºåŸŸæ—¶å´å¸¸å¸¸éš¾ä»¥ä¿æŒå¿ å®æ€§ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºæ–‡æœ¬å›¾åƒå¹»è§‰ã€‚æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œè¦æ±‚åŒæ—¶æ¢å¤è§†è§‰å†…å®¹å’Œæ–‡æœ¬å¿ å®æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SA-Textï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«10ä¸‡å¼ é«˜è´¨é‡åœºæ™¯å›¾åƒçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ï¼Œå…¶ä¸­å¯†é›†æ ‡æ³¨äº†å¤šæ ·ä¸”å¤æ‚çš„æ–‡æœ¬å®ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶TeReDiffï¼Œå®ƒå°†æ‰©æ•£æ¨¡å‹çš„å†…éƒ¨ç‰¹å¾é›†æˆåˆ°æ–‡æœ¬è¯†åˆ«æ¨¡å—ä¸­ï¼Œä½¿ä¸¤è€…éƒ½èƒ½ä»è”åˆè®­ç»ƒä¸­å—ç›Šã€‚è¿™æœ‰åŠ©äºæå–ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åœ¨éšåçš„å»å™ªæ­¥éª¤ä¸­ä½œä¸ºæç¤ºä½¿ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€è‡´åœ°ä¼˜äºæœ€æ–°çš„ä¿®å¤æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®æ€§ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ„ŸçŸ¥å›¾åƒä¿®å¤ï¼ˆTAIRï¼‰æ˜¯ä¸€é¡¹è¦æ±‚åŒæ—¶æ¢å¤å›¾åƒè§†è§‰å†…å®¹å’Œæ–‡æœ¬å¿ å®æ€§çš„æ–°ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬åŒºåŸŸçš„å›¾åƒä¿®å¤æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°æ–‡æœ¬å›¾åƒå¹»è§‰ç°è±¡ã€‚</li>
<li>SA-Textæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«å¯†é›†æ ‡æ³¨çš„å¤šæ ·ä¸”å¤æ‚çš„æ–‡æœ¬å®ä¾‹çš„å›¾åƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡æ‰©æ•£æ¡†æ¶TeReDiffï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œæ–‡æœ¬è¯†åˆ«æ¨¡å—ï¼Œæé«˜äº†æ–‡æœ¬è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>TeReDiffæ¡†æ¶é€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºä½œä¸ºå»å™ªæ­¥éª¤çš„æç¤ºï¼Œä¼˜åŒ–äº†å›¾åƒä¿®å¤è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTeReDiffåœ¨æ–‡æœ¬è¯†åˆ«å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ä¿®å¤æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d552871085a1ee5683888226fd488e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb60e3380b277cb38aa7f9ee229ff23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-511bb8bf09e45804c3a98364bdb10570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c5eec05910710dbada7c22db87db7aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f57900cb98032e547d883d08dea1d2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4250366fe6a6b383eff719e28a13adb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations"><a href="#HadaNorm-Diffusion-Transformer-Quantization-through-Mean-Centered-Transformations" class="headerlink" title="HadaNorm: Diffusion Transformer Quantization through Mean-Centered   Transformations"></a>HadaNorm: Diffusion Transformer Quantization through Mean-Centered   Transformations</h2><p><strong>Authors:Marco Federici, Riccardo Del Chiaro, Boris van Breugel, Paul Whatmough, Markus Nagel</strong></p>
<p>Diffusion models represent the cutting edge in image generation, but their high memory and computational demands hinder deployment on resource-constrained devices. Post-Training Quantization (PTQ) offers a promising solution by reducing the bitwidth of matrix operations. However, standard PTQ methods struggle with outliers, and achieving higher compression often requires transforming model weights and activations before quantization. In this work, we propose HadaNorm, a novel linear transformation that extends existing approaches and effectively mitigates outliers by normalizing activations feature channels before applying Hadamard transformations, enabling more aggressive activation quantization. We demonstrate that HadaNorm consistently reduces quantization error across the various components of transformer blocks, achieving superior efficiency-performance trade-offs when compared to state-of-the-art methods. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆé¢†åŸŸçš„å‰æ²¿æŠ€æœ¯ï¼Œä½†å…¶é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰é€šè¿‡é™ä½çŸ©é˜µæ“ä½œçš„ä½å®½æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ ‡å‡†PTQæ–¹æ³•éš¾ä»¥å¤„ç†å¼‚å¸¸å€¼ï¼Œå®ç°æ›´é«˜çš„å‹ç¼©é€šå¸¸éœ€è¦é‡åŒ–å‰å¯¹æ¨¡å‹æƒé‡å’Œæ¿€æ´»è¿›è¡Œè½¬æ¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†HadaNormï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çº¿æ€§å˜æ¢ï¼Œå®ƒæ‰©å±•äº†ç°æœ‰æ–¹æ³•å¹¶é€šè¿‡å¯¹æ¿€æ´»ç‰¹å¾é€šé“è¿›è¡Œå½’ä¸€åŒ–æœ‰æ•ˆåœ°å‡è½»äº†å¼‚å¸¸å€¼é—®é¢˜ï¼Œç„¶åå†åº”ç”¨å“ˆè¾¾ç›å˜æ¢ä»¥å®ç°æ›´æç«¯çš„æ¿€æ´»é‡åŒ–ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒHadaNormå¯ä»¥ä¸€è‡´åœ°åœ¨transformerå—çš„å„ç§ç»„ä»¶ä¸­å‡å°‘é‡åŒ–è¯¯å·®ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†å“è¶Šçš„æ•ˆç‡æ€§èƒ½æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09932v1">PDF</a> 4 Pages, 5 Figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†åä¸ºHadaNormçš„æ–°å‹çº¿æ€§å˜æ¢æ–¹æ³•ï¼Œç”¨äºå›¾åƒç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹ã€‚å®ƒé€šè¿‡å½’ä¸€åŒ–æ¿€æ´»ç‰¹å¾é€šé“ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†æ ‡å‡†åŒ–PTQæ–¹æ³•åœ¨å¤„ç†ç¦»ç¾¤å€¼æ—¶çš„å›°éš¾ï¼Œå¹¶å…è®¸æ›´æ¿€çƒˆçš„æ¿€æ´»é‡åŒ–ã€‚HadaNormåœ¨å˜å‹å™¨å—çš„å„ä¸ªç»„ä»¶ä¸­ä¸€è‡´åœ°é™ä½äº†é‡åŒ–è¯¯å·®ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´ä¼˜è¶Šçš„æ•ˆç‡å’Œæ€§èƒ½æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æ˜¯å›¾åƒç”Ÿæˆçš„å‰æ²¿æŠ€æœ¯ï¼Œä½†é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„åº”ç”¨ã€‚</li>
<li>Post-Training Quantization (PTQ) æ–¹æ³•é€šè¿‡é™ä½çŸ©é˜µæ“ä½œçš„ä½å®½æ¥è§£å†³é—®é¢˜ã€‚</li>
<li>ç°æœ‰PTQæ–¹æ³•åœ¨å¤„ç†ç¦»ç¾¤å€¼æ—¶é‡åˆ°å›°éš¾ï¼Œéœ€è¦è½¬æ¢æ¨¡å‹æƒé‡å’Œæ¿€æ´»æ¥å®ç°æ›´é«˜çš„å‹ç¼©ã€‚</li>
<li>æå‡ºçš„HadaNormæ–¹æ³•é€šè¿‡å½’ä¸€åŒ–æ¿€æ´»ç‰¹å¾é€šé“ï¼Œåœ¨åº”ç”¨Hadamardå˜æ¢ä¹‹å‰æœ‰æ•ˆåœ°ç¼“è§£äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>HadaNormåœ¨å˜å‹å™¨å—çš„å„ä¸ªç»„ä»¶ä¸­é™ä½äº†é‡åŒ–è¯¯å·®ã€‚</li>
<li>HadaNormä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´ä¼˜è¶Šçš„æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3206faebf990e537852fa0341525d875.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6359d83a4db74a6389e8cf69390560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a392e0470ad91f06e917ad2c05cf918.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d9431e75b680190dc2f7a826cdd0b53.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DGAE-Diffusion-Guided-Autoencoder-for-Efficient-Latent-Representation-Learning"><a href="#DGAE-Diffusion-Guided-Autoencoder-for-Efficient-Latent-Representation-Learning" class="headerlink" title="DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation   Learning"></a>DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation   Learning</h2><p><strong>Authors:Dongxu Liu, Yuang Peng, Haomiao Tang, Yuwei Chen, Chunrui Han, Zheng Ge, Daxin Jiang, Mingxue Liao</strong></p>
<p>Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoderâ€™s expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model. </p>
<blockquote>
<p>è‡ªåŠ¨ç¼–ç å™¨é€šè¿‡è§†è§‰ä»¤ç‰ŒåŒ–å°†åƒç´ å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œä»è€Œèµ‹èƒ½æœ€å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚å°½ç®¡æœ€è¿‘çš„è¿›å±•å·²ç»å‡è½»äº†é«˜å‹ç¼©æ¯”ä¸‹è‡ªåŠ¨ç¼–ç å™¨çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä½†è§£å†³ç”±GANå¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šä»ç„¶æ˜¯ä¸€ä¸ªå…¬å¼€çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨æé«˜ç©ºé—´å‹ç¼©çš„åŒæ—¶ï¼Œä¹Ÿè‡´åŠ›äºæœ€å°åŒ–æ½œåœ¨ç©ºé—´çš„ç»´åº¦ï¼Œä»¥å®ç°æ›´é«˜æ•ˆã€æ›´ç´§å‡‘çš„è¡¨ç¤ºã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæé«˜è§£ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†DGAEï¼ˆæ‰©æ•£å¼•å¯¼è‡ªç¼–ç å™¨ï¼‰ï¼Œå®ƒé‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥æŒ‡å¯¼è§£ç å™¨æ¢å¤ä»æ½œåœ¨è¡¨ç¤ºä¸­æœªå®Œå…¨è§£ç çš„ä¿¡æ¯ä¿¡å·ã€‚é€šè¿‡è¿™ç§è®¾è®¡ï¼ŒDGAEåœ¨é«˜ç©ºé—´å‹ç¼©ç‡ä¸‹æœ‰æ•ˆåœ°ç¼“è§£äº†æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚åŒæ—¶ï¼ŒDGAEåœ¨å…·æœ‰æ›´å°ä¸¤å€æ½œåœ¨ç©ºé—´çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å½“ä¸æ‰©æ•£æ¨¡å‹é›†æˆæ—¶ï¼ŒDGAEåœ¨ImageNet-1Kçš„å›¾åƒç”Ÿæˆä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶è¯æ˜è¿™ç§ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºæœ‰åŠ©äºæ‰©æ•£æ¨¡å‹çš„æ›´å¿«æ”¶æ•›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09644v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é€šè¿‡è§†è§‰ä»¤ç‰ŒåŒ–å°†åƒç´ å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´ï¼Œè‡ªåŠ¨ç¼–ç å™¨ä¸ºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›äº†åŠ¨åŠ›ã€‚å°½ç®¡å·²æœ‰è¿›å±•å‡è½»äº†é«˜å‹ç¼©æ¯”ä¸‹è‡ªåŠ¨ç¼–ç å™¨çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä½†ç”±ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ä»å¾…è§£å†³ã€‚åœ¨æ”¹è¿›ç©ºé—´å‹ç¼©çš„åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜è‡´åŠ›äºæœ€å°åŒ–æ½œåœ¨ç©ºé—´çš„ç»´æ•°ï¼Œä»¥å®ç°æ›´ç´§å‡‘å’Œé«˜æ•ˆçš„è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæé«˜è§£ç å™¨çš„è¡¨è¾¾èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¼•å¯¼è§£ç å™¨æ¢å¤ä»æ½œåœ¨è¡¨ç¤ºä¸­æœªå®Œå…¨è§£ç çš„ä¿¡å·çš„DGAEã€‚è¯¥è®¾è®¡æœ‰æ•ˆç¼“è§£äº†é«˜ç©ºé—´å‹ç¼©ç‡ä¸‹çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼ŒåŒæ—¶å®ç°äº†è¾ƒå°çš„æ½œåœ¨ç©ºé—´ï¼ˆç¼©å°äº†2å€ï¼‰ã€‚ä¸æ‰©æ•£æ¨¡å‹é›†æˆåï¼ŒDGAEåœ¨ImageNet-1Kçš„å›¾åƒç”Ÿæˆä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶è¯æ˜è¿™ç§ç´§å‡‘çš„æ½œåœ¨è¡¨ç¤ºä¿ƒè¿›äº†æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿæ”¶æ•›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨ç¼–ç å™¨é€šè¿‡è§†è§‰ä»¤ç‰ŒåŒ–å‹ç¼©åƒç´ è‡³æ½œåœ¨ç©ºé—´ï¼Œä¸ºå›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹æä¾›åŠ¨åŠ›ã€‚</li>
<li>é«˜å‹ç¼©æ¯”ä¸‹çš„è‡ªåŠ¨ç¼–ç å™¨æ€§èƒ½ä¸‹é™é—®é¢˜è™½æœ‰æ‰€ç¼“è§£ï¼Œä½†ç”±GANå¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>åœ¨æ”¹è¿›ç©ºé—´å‹ç¼©çš„åŒæ—¶ï¼Œæœ€å°åŒ–æ½œåœ¨ç©ºé—´ç»´åº¦ä»¥å®ç°æ›´ç´§å‡‘å’Œé«˜æ•ˆçš„è¡¨ç¤ºæ˜¯ç›®æ ‡ã€‚</li>
<li>DGAEé€šè¿‡é‡‡ç”¨æ‰©æ•£æ¨¡å‹å¼•å¯¼è§£ç å™¨æ¢å¤æœªå®Œå…¨è§£ç çš„ä¿¡å·ï¼Œè§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>DGAEåœ¨é«˜ç©ºé—´å‹ç¼©ç‡ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸”å®ç°äº†è¾ƒå°çš„æ½œåœ¨ç©ºé—´ã€‚</li>
<li>DGAEåœ¨ImageNet-1Kçš„å›¾åƒç”Ÿæˆä¸Šè¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a557cdc406d9187b7cd0b228cb63bc81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01302ceab2bb0c0b4927c3f274f64b26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f14289878d03423810d491dd528488e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d976bb503a4c49c9a7a19ad6f524bc6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AngleRoCL-Angle-Robust-Concept-Learning-for-Physically-View-Invariant-T2I-Adversarial-Patches"><a href="#AngleRoCL-Angle-Robust-Concept-Learning-for-Physically-View-Invariant-T2I-Adversarial-Patches" class="headerlink" title="AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant   T2I Adversarial Patches"></a>AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant   T2I Adversarial Patches</h2><p><strong>Authors:Wenjun Ji, Yuxiang Fu, Luyang Ying, Deng-Ping Fan, Yuyi Wang, Ming-Ming Cheng, Ivor Tsang, Qing Guo</strong></p>
<p>Cutting-edge works have demonstrated that text-to-image (T2I) diffusion models can generate adversarial patches that mislead state-of-the-art object detectors in the physical world, revealing detectorsâ€™ vulnerabilities and risks. However, these methods neglect the T2I patchesâ€™ attack effectiveness when observed from different views in the physical world (i.e., angle robustness of the T2I adversarial patches). In this paper, we study the angle robustness of T2I adversarial patches comprehensively, revealing their angle-robust issues, demonstrating that texts affect the angle robustness of generated patches significantly, and task-specific linguistic instructions fail to enhance the angle robustness. Motivated by the studies, we introduce Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that learns a generalizable concept (i.e., text embeddings in implementation) representing the capability of generating angle-robust patches. The learned concept can be incorporated into textual prompts and guides T2I models to generate patches with their attack effectiveness inherently resistant to viewpoint variations. Through extensive simulation and physical-world experiments on five SOTA detectors across multiple views, we demonstrate that AngleRoCL significantly enhances the angle robustness of T2I adversarial patches compared to baseline methods. Our patches maintain high attack success rates even under challenging viewing conditions, with over 50% average relative improvement in attack effectiveness across multiple angles. This research advances the understanding of physically angle-robust patches and provides insights into the relationship between textual concepts and physical properties in T2I-generated contents. </p>
<blockquote>
<p>å‰æ²¿ç ”ç©¶è¡¨æ˜ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¯¹æŠ—æ€§è¡¥ä¸ï¼Œè¿™äº›è¡¥ä¸å¯ä»¥è¯¯å¯¼ç°å®ä¸–ç•Œä¸­çš„æœ€å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹å™¨ï¼Œä»è€Œæ­ç¤ºæ£€æµ‹å™¨çš„è„†å¼±æ€§å’Œé£é™©ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†ä»ç°å®ä¸–ç•Œä¸­çš„ä¸åŒè§†è§’è§‚å¯Ÿåˆ°çš„T2Iè¡¥ä¸çš„æ”»å‡»æ•ˆæœï¼ˆå³T2Iå¯¹æŠ—æ€§è¡¥ä¸çš„è§’åº¦ç¨³å¥æ€§ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…¨é¢ç ”ç©¶äº†T2Iå¯¹æŠ—æ€§è¡¥ä¸çš„è§’åº¦ç¨³å¥æ€§ï¼Œæ­ç¤ºäº†å…¶è§’åº¦ç¨³å¥é—®é¢˜ï¼Œå¹¶è¯æ˜äº†æ–‡æœ¬å¯¹ç”Ÿæˆè¡¥ä¸çš„è§’åº¦ç¨³å¥æ€§å½±å“æ˜¾è‘—ï¼Œè€Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è¯­è¨€æŒ‡ä»¤æ— æ³•å¢å¼ºè§’åº¦ç¨³å¥æ€§ã€‚å—è¿™äº›ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§’åº¦ç¨³å¥æ¦‚å¿µå­¦ä¹ ï¼ˆAngleRoCLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çµæ´»çš„æ–¹æ³•ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸ªå¯æ¨å¹¿çš„æ¦‚å¿µï¼ˆå³åœ¨å®ç°ä¸­çš„æ–‡æœ¬åµŒå…¥ï¼‰ï¼Œç”¨äºä»£è¡¨ç”Ÿæˆè§’åº¦ç¨³å¥è¡¥ä¸çš„èƒ½åŠ›ã€‚å­¦åˆ°çš„æ¦‚å¿µå¯ä»¥èå…¥æ–‡æœ¬æç¤ºä¸­ï¼Œå¹¶æŒ‡å¯¼T2Iæ¨¡å‹ç”Ÿæˆå›ºæœ‰åœ°æŠµæŠ—è§†è§’å˜åŒ–çš„è¡¥ä¸ã€‚é€šè¿‡å¯¹äº”ä¸ªæœ€å…ˆè¿›çš„æ£€æµ‹å™¨è¿›è¡Œè·¨å¤šä¸ªè§†è§’çš„å¹¿æ³›æ¨¡æ‹Ÿå’Œç°å®ä¸–ç•Œå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒAngleRoCLæ˜¾è‘—æé«˜äº†T2Iå¯¹æŠ—æ€§è¡¥ä¸çš„è§’åº¦ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„è¡¥ä¸å³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§‚çœ‹æ¡ä»¶ä¸‹ä¹Ÿä¿æŒäº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼Œåœ¨å¤šä¸ªè§’åº¦çš„æ”»å‡»æ•ˆæœä¸Šå¹³å‡ç›¸å¯¹æé«˜äº†50%ä»¥ä¸Šã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†ç‰©ç†è§’åº¦ç¨³å¥è¡¥ä¸çš„ç†è§£ï¼Œå¹¶æ·±å…¥äº†è§£äº†æ–‡æœ¬æ¦‚å¿µå’ŒT2Iç”Ÿæˆå†…å®¹ä¸­çš„ç‰©ç†å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09538v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å‰æ²¿ç ”ç©¶è¡¨æ˜ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¯ç”Ÿæˆå¯¹æŠ—æ€§è¡¥ä¸ï¼Œè¯¯å¯¼ç°å®ä¸–ç•Œä¸­çš„å…ˆè¿›ç›®æ ‡æ£€æµ‹å™¨ï¼Œæ­ç¤ºæ£€æµ‹å™¨çš„è„†å¼±æ€§å’Œé£é™©ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†ä»ç‰©ç†ä¸–ç•Œçš„ä¸åŒè§†è§’è§‚å¯Ÿåˆ°çš„æ–‡æœ¬åˆ°å›¾åƒçš„è¡¥ä¸æ”»å‡»æ•ˆæœï¼ˆå³æ–‡æœ¬åˆ°å›¾åƒå¯¹æŠ—æ€§è¡¥ä¸çš„è§†è§’é²æ£’æ€§ï¼‰ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†T2Iå¯¹æŠ—æ€§è¡¥ä¸çš„è§†è§’é²æ£’æ€§ï¼Œæ­ç¤ºäº†å…¶è§†è§’é²æ£’æ€§é—®é¢˜ï¼Œè¡¨æ˜æ–‡æœ¬å¯¹ç”Ÿæˆçš„è¡¥ä¸çš„è§†è§’é²æ£’æ€§å½±å“æ˜¾è‘—ï¼Œè€Œç‰¹å®šçš„ä»»åŠ¡è¯­è¨€æŒ‡ä»¤æ— æ³•å¢å¼ºè§†è§’é²æ£’æ€§ã€‚å—ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§’åº¦ç¨³å¥æ¦‚å¿µå­¦ä¹ ï¼ˆAngleRoCLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çµæ´»çš„æ–¹æ³•ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸ªå¯æ¦‚æ‹¬çš„æ¦‚å¿µï¼ˆåœ¨å®æ–½ä¸­ä¸ºæ–‡æœ¬åµŒå…¥ï¼‰ï¼Œä»£è¡¨ç”Ÿæˆè§†è§’ç¨³å¥è¡¥ä¸çš„èƒ½åŠ›ã€‚å­¦åˆ°çš„æ¦‚å¿µå¯ä»¥èå…¥æ–‡æœ¬æç¤ºï¼Œå¹¶æŒ‡å¯¼T2Iæ¨¡å‹ç”Ÿæˆå¯¹è§†è§’å˜åŒ–å…·æœ‰å†…åœ¨æŠµæŠ—åŠ›çš„è¡¥ä¸ã€‚é€šè¿‡å¹¿æ³›çš„æ¨¡æ‹Ÿå’Œç‰©ç†ä¸–ç•Œå®éªŒï¼Œåœ¨äº”ä¸ªå…ˆè¿›æ£€æµ‹å™¨ä¸Šè·¨å¤šä¸ªè§†è§’è¿›è¡ŒéªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜AngleRoCLæ˜¾è‘—æé«˜äº†T2Iå¯¹æŠ—æ€§è¡¥ä¸çš„è§†è§’é²æ£’æ€§ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è¡¥ä¸å³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§’æ¡ä»¶ä¸‹ä¹Ÿä¿æŒäº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼Œå¤šä¸ªè§’åº¦çš„æ”»å‡»æ•ˆç‡å¹³å‡æé«˜äº†50%ä»¥ä¸Šã€‚è¯¥ç ”ç©¶æ¨è¿›äº†å¯¹ç‰©ç†è§’åº¦ç¨³å¥è¡¥ä¸çš„ç†è§£ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†æ–‡æœ¬æ¦‚å¿µä¸T2Iç”Ÿæˆå†…å®¹ç‰©ç†å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚</p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¯¹æŠ—æ€§è¡¥ä¸ï¼Œå¯ä»¥è¯¯å¯¼ç°å®ä¸–ç•Œçš„ç›®æ ‡æ£€æµ‹å™¨ã€‚</li>
<li>ç°æœ‰çš„ç ”ç©¶å¿½ç•¥äº†ä»ç‰©ç†ä¸–ç•Œçš„ä¸åŒè§†è§’è§‚å¯Ÿè¿™äº›è¡¥ä¸çš„æ•ˆæœï¼Œæœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚</li>
<li>æœ¬æ–‡æ­ç¤ºäº†æ–‡æœ¬å¯¹ç”Ÿæˆçš„å¯¹æŠ—æ€§è¡¥ä¸çš„è§†è§’é²æ£’æ€§æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>è¯­è¨€æŒ‡ä»¤åœ¨å¢å¼ºè¡¥ä¸çš„è§†è§’é²æ£’æ€§æ–¹é¢æ•ˆæœæœ‰é™ã€‚</li>
<li>å¼•å…¥äº†AngleRoCLæ–¹æ³•ï¼Œèƒ½å­¦ä¹ ç”Ÿæˆè§†è§’ç¨³å¥è¡¥ä¸çš„èƒ½åŠ›ï¼Œå¹¶å°†å…¶èå…¥æ–‡æœ¬æç¤ºä¸­ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒAngleRoCLæ˜¾è‘—æé«˜äº†T2Iå¯¹æŠ—æ€§è¡¥ä¸çš„è§†è§’é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d7fbb22f48066ab711b84c69b25b350.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28ba46a2d1cfc8dd317f2e51f2c15473.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1607691c85b090b683898f28bbab59a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dc50e7d1b70e635d5803b6419d0c3b8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Marrying-Autoregressive-Transformer-and-Diffusion-with-Multi-Reference-Autoregression"><a href="#Marrying-Autoregressive-Transformer-and-Diffusion-with-Multi-Reference-Autoregression" class="headerlink" title="Marrying Autoregressive Transformer and Diffusion with Multi-Reference   Autoregression"></a>Marrying Autoregressive Transformer and Diffusion with Multi-Reference   Autoregression</h2><p><strong>Authors:Dingcheng Zhen, Qian Qiao, Tan Yu, Kangxi Wu, Ziwei Zhang, Siyuan Liu, Shunshun Yin, Ming Tao</strong></p>
<p>We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Fr&#39;echet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†TransDiffï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†è‡ªå›å½’ï¼ˆARï¼‰Transformerä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ã€‚åœ¨è¿™ä¸ªè”åˆå»ºæ¨¡æ¡†æ¶ä¸­ï¼ŒTransDiffå°†æ ‡ç­¾å’Œå›¾åƒç¼–ç ä¸ºé«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥ä¼°è®¡å›¾åƒæ ·æœ¬çš„åˆ†å¸ƒã€‚åœ¨ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTransDiffæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºäºç‹¬ç«‹AR Transformeræˆ–æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒTransDiffå®ç°äº†FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ä¸º1.61ï¼ŒInception Scoreï¼ˆISï¼‰ä¸º293.4ï¼Œä¸åŸºäºAR Transformerçš„ç°æœ‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œæä¾›äº†x2æ›´å¿«çš„æ¨ç†å»¶è¿Ÿï¼Œä¸ä»…ä½¿ç”¨æ‰©æ•£çš„æ¨¡å‹ç›¸æ¯”ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†x112å€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»¥TransDiffæ¨¡å‹ä¸ºåŸºç¡€ï¼Œä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼â€”â€”å¤šå‚è€ƒè‡ªå›å½’ï¼ˆMRARï¼‰ã€‚MRARé€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªå›¾åƒè¿›è¡Œè‡ªå›å½’ç”Ÿæˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå‚è€ƒå¤šä¸ªå…ˆå‰ç”Ÿæˆçš„å›¾åƒï¼Œä»è€Œæ›´å®¹æ˜“å­¦ä¹ æ›´å¤šæ ·åŒ–çš„è¡¨ç¤ºï¼Œå¹¶åœ¨åç»­è¿­ä»£ä¸­æé«˜ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚é€šè¿‡åº”ç”¨MRARï¼ŒTransDiffçš„æ€§èƒ½å¾—åˆ°äº†æå‡ï¼ŒFIDä»1.61é™ä½åˆ°äº†1.42ã€‚æˆ‘ä»¬æœŸæœ›TransDiffèƒ½åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå¼€è¾Ÿæ–°çš„å‰æ²¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09482v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TransDiffæ¨¡å‹ç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰Transformerä¸æ‰©æ•£æ¨¡å‹ï¼Œæˆä¸ºé¦–ä¸ªå›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å•ä¸€æ¨¡å‹ã€‚TransDiffåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¼°è®¡å›¾åƒæ ·æœ¬åˆ†å¸ƒï¼Œå…·æœ‰å¿«é€Ÿæ¨ç†é€Ÿåº¦ä¸è¾ƒé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼ŒåŸºäºTransDiffæ¨¡å‹çš„MRARï¼ˆå¤šå‚è€ƒè‡ªå›å½’ï¼‰å›¾åƒç”ŸæˆèŒƒå¼å¯å­¦ä¹ æ›´å¹¿æ³›çš„è¡¨ç¤ºå¹¶æ”¹å–„ç”Ÿæˆå›¾åƒè´¨é‡ã€‚è¿™äº›ä¼˜åŠ¿ä½¿TransDiffæœ‰æœ›æˆä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸçš„æ–°å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransDiffæ˜¯é¦–ä¸ªç»“åˆè‡ªå›å½’ï¼ˆARï¼‰Transformerä¸æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>TransDiffåœ¨ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒFIDè¾¾åˆ°1.61å’ŒISè¾¾åˆ°293.4ã€‚</li>
<li>TransDiffç›¸å¯¹äºåŸºäºAR Transformerçš„å½“å‰æ–¹æ³•å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>TransDiffç›¸è¾ƒäºä»…ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ¨¡å‹ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ã€‚</li>
<li>TransDiffå¼•å…¥çš„MRARèŒƒå¼å¯ä»¥å­¦ä¹ å¤šæ ·åŒ–çš„è¡¨ç¤ºå¹¶æå‡åç»­è¿­ä»£ä¸­ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
<li>MRARé€šè¿‡åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªå›¾åƒæ—¶å‚è€ƒå¤šä¸ªå…ˆå‰ç”Ÿæˆçš„å›¾åƒï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa8f812b2680388c3a8cc6fad435acd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5157efcc81eb617f0354c76c0fee05ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbf3cd845bf01ed1d5d43437489096f0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Noise-Conditional-Variational-Score-Distillation"><a href="#Noise-Conditional-Variational-Score-Distillation" class="headerlink" title="Noise Conditional Variational Score Distillation"></a>Noise Conditional Variational Score Distillation</h2><p><strong>Authors:Xinyu Peng, Ziyang Zheng, Yaoming Wang, Han Li, Nuowen Kan, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong</strong></p>
<p>We propose Noise Conditional Variational Score Distillation (NCVSD), a novel method for distilling pretrained diffusion models into generative denoisers. We achieve this by revealing that the unconditional score function implicitly characterizes the score function of denoising posterior distributions. By integrating this insight into the Variational Score Distillation (VSD) framework, we enable scalable learning of generative denoisers capable of approximating samples from the denoising posterior distribution across a wide range of noise levels. The proposed generative denoisers exhibit desirable properties that allow fast generation while preserve the benefit of iterative refinement: (1) fast one-step generation through sampling from pure Gaussian noise at high noise levels; (2) improved sample quality by scaling the test-time compute with multi-step sampling; and (3) zero-shot probabilistic inference for flexible and controllable sampling. We evaluate NCVSD through extensive experiments, including class-conditional image generation and inverse problem solving. By scaling the test-time compute, our method outperforms teacher diffusion models and is on par with consistency models of larger sizes. Additionally, with significantly fewer NFEs than diffusion-based methods, we achieve record-breaking LPIPS on inverse problems. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å™ªå£°æ¡ä»¶å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆNCVSDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è’¸é¦æˆç”Ÿæˆå»å™ªå™¨çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡æ­ç¤ºæ— æ¡ä»¶åˆ†æ•°å‡½æ•°éšå«åœ°åˆ»ç”»äº†å»å™ªåéªŒåˆ†å¸ƒçš„åˆ†æ•°å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹ã€‚é€šè¿‡å°†è¿™ä¸€è§è§£èå…¥å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰æ¡†æ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå­¦ä¹ å¯åœ¨å¹¿æ³›å™ªå£°æ°´å¹³ä¸‹å¯¹å»å™ªåéªŒåˆ†å¸ƒè¿›è¡Œé‡‡æ ·çš„ç”Ÿæˆå»å™ªå™¨ã€‚æ‰€æå‡ºç”Ÿæˆå»å™ªå™¨å±•ç°å‡ºç†æƒ³çš„ç‰¹æ€§ï¼Œå…è®¸å¿«é€Ÿç”Ÿæˆçš„åŒæ—¶ä¿ç•™è¿­ä»£ç»†åŒ–çš„ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰é€šè¿‡ä»é«˜å™ªå£°æ°´å¹³çš„çº¯é«˜æ–¯å™ªå£°ä¸­è¿›è¡Œé‡‡æ ·ï¼Œå®ç°å¿«é€Ÿå•æ­¥ç”Ÿæˆï¼›ï¼ˆ2ï¼‰é€šè¿‡å¤šæ­¥é‡‡æ ·æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œæé«˜æ ·æœ¬è´¨é‡ï¼›ï¼ˆ3ï¼‰é›¶æ ·æœ¬æ¦‚ç‡æ¨ç†ï¼Œç”¨äºçµæ´»å¯æ§çš„é‡‡æ ·ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒè¯„ä¼°äº†NCVSDï¼ŒåŒ…æ‹¬ç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé€†é—®é¢˜è§£å†³ã€‚é€šè¿‡æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æ•™å¸ˆæ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä¸æ›´å¤§è§„æ¨¡çš„ä¸€è‡´æ€§æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œä¸åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é€†å‘é—®é¢˜ä¸Šçš„NFEæ˜¾è‘—å‡å°‘ï¼Œå®ç°äº†ç ´çºªå½•çš„LPIPSã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09416v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å™ªå£°æ¡ä»¶å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆNCVSDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è’¸é¦ä¸ºç”Ÿæˆå»å™ªå™¨çš„æ–°æ–¹æ³•ã€‚é€šè¿‡æ­ç¤ºæ— æ¡ä»¶åˆ†æ•°å‡½æ•°éšå¼è¡¨å¾å»å™ªåéªŒåˆ†å¸ƒçš„åˆ†æ•°å‡½æ•°ï¼Œæˆ‘ä»¬å°†è¿™ä¸€è§è§£èå…¥å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰æ¡†æ¶ï¼Œä½¿å­¦ä¹ èƒ½å¤Ÿåœ¨å„ç§å™ªå£°æ°´å¹³ä¸Šè¿‘ä¼¼å»å™ªåéªŒåˆ†å¸ƒçš„æ ·æœ¬ã€‚æ‰€æå‡ºçš„ç”Ÿæˆå»å™ªå™¨å…·æœ‰ä»¤äººæœŸæœ›çš„ç‰¹æ€§ï¼Œå…è®¸å¿«é€Ÿç”ŸæˆåŒæ—¶ä¿ç•™è¿­ä»£ä¼˜åŒ–çš„å¥½å¤„ï¼šä¸€æ˜¯ä»é«˜å™ªå£°æ°´å¹³çš„çº¯é«˜æ–¯å™ªå£°ä¸­å¿«é€Ÿè¿›è¡Œä¸€æ­¥ç”Ÿæˆï¼›äºŒæ˜¯é€šè¿‡å¤šæ­¥é‡‡æ ·æé«˜æ ·æœ¬è´¨é‡æ¥æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼›ä¸‰æ˜¯å®ç°çµæ´»å¯æ§é‡‡æ ·çš„é›¶æ ·æœ¬æ¦‚ç‡æ¨ç†ã€‚é€šè¿‡å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ç±»æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé€†é—®é¢˜è§£å†³ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•™å¸ˆæ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨å¤§å‹ä¸€è‡´æ€§æ¨¡å‹æ–¹é¢è¡¨ç°ç›¸å½“ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºæ‰©æ•£æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é€†é—®é¢˜ä¸Šå¤§å¹…å‡å°‘äº†è®¡ç®—æ¬¡æ•°ï¼Œå¹¶åˆ›ä¸‹äº†LPIPSçš„æ–°çºªå½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Noise Conditional Variational Score Distillation (NCVSD) æ–¹æ³•ï¼Œç”¨äºå°†æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºç”Ÿæˆå»å™ªå™¨ã€‚</li>
<li>é€šè¿‡æ•´åˆæ— æ¡ä»¶åˆ†æ•°å‡½æ•°çš„è§è§£åˆ°VSDæ¡†æ¶ï¼Œå®ç°äº†å¯¹å„ç§å™ªå£°æ°´å¹³ä¸‹æ ·æœ¬çš„è¿‘ä¼¼å­¦ä¹ ã€‚</li>
<li>ç”Ÿæˆçš„å»å™ªå™¨å…·å¤‡å¿«é€Ÿä¸€æ­¥ç”Ÿæˆã€æé«˜æ ·æœ¬è´¨é‡å’Œé›¶æ ·æœ¬æ¦‚ç‡æ¨ç†ç­‰ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬ç±»æ¡ä»¶å›¾åƒç”Ÿæˆå’Œé€†é—®é¢˜è§£å†³ï¼Œè¯¥æ–¹æ³•åœ¨æ•™å¸ˆæ‰©æ•£æ¨¡å‹ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æ–¹æ³•åœ¨å¤§å‹ä¸€è‡´æ€§æ¨¡å‹æ–¹é¢è¡¨ç°ç›¸å½“ï¼Œå¹¶ä¸”åœ¨é€†é—®é¢˜ä¸Šæ˜¾è‘—å‡å°‘äº†è®¡ç®—æ¬¡æ•°ã€‚</li>
<li>åœ¨é€†é—®é¢˜ä¸Šå®ç°äº†LPIPSçš„æ–°çºªå½•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ‰©æ•£æ¨¡å‹çš„è¿›ä¸€æ­¥ä¼˜åŒ–å’Œå®é™…åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08c365a87cf6aa12278fcd6fb23943a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35fb51093ee7f1091561fb8fe4656e91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53629030a4bdcf1531e384af37a34f30.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Revisiting-Diffusion-Models-From-Generative-Pre-training-to-One-Step-Generation"><a href="#Revisiting-Diffusion-Models-From-Generative-Pre-training-to-One-Step-Generation" class="headerlink" title="Revisiting Diffusion Models: From Generative Pre-training to One-Step   Generation"></a>Revisiting Diffusion Models: From Generative Pre-training to One-Step   Generation</h2><p><strong>Authors:Bowen Zheng, Tianming Yang</strong></p>
<p>Diffusion distillation is a widely used technique to reduce the sampling cost of diffusion models, yet it often requires extensive training, and the student performance tends to be degraded. Recent studies show that incorporating a GAN objective may alleviate these issues, yet the underlying mechanism remains unclear. In this work, we first identify a key limitation of distillation: mismatched step sizes and parameter numbers between the teacher and the student model lead them to converge to different local minima, rendering direct imitation suboptimal. We further demonstrate that a standalone GAN objective, without relying a distillation loss, overcomes this limitation and is sufficient to convert diffusion models into efficient one-step generators. Based on this finding, we propose that diffusion training may be viewed as a form of generative pre-training, equipping models with capabilities that can be unlocked through lightweight GAN fine-tuning. Supporting this view, we create a one-step generation model by fine-tuning a pre-trained model with 85% of parameters frozen, achieving strong performance with only 0.2M images and near-SOTA results with 5M images. We further present a frequency-domain analysis that may explain the one-step generative capability gained in diffusion training. Overall, our work provides a new perspective for diffusion training, highlighting its role as a powerful generative pre-training process, which can be the basis for building efficient one-step generation models. </p>
<blockquote>
<p>æ‰©æ•£è’¸é¦æ˜¯å‡å°‘æ‰©æ•£æ¨¡å‹é‡‡æ ·æˆæœ¬çš„ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œç„¶è€Œå®ƒé€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒï¼Œå¹¶ä¸”å­¦ç”Ÿçš„è¡¨ç°å¾€å¾€è¾ƒå·®ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼•å…¥GANç›®æ ‡å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†èƒŒåçš„æœºåˆ¶ä»ç„¶ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šäº†è’¸é¦çš„ä¸€ä¸ªå…³é”®é™åˆ¶ï¼šæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„æ­¥é•¿å‚æ•°ä¸åŒ¹é…å¯¼è‡´å®ƒä»¬æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€å°å€¼ï¼Œä½¿å¾—ç›´æ¥æ¨¡ä»¿å˜å¾—ä¸å¤Ÿç†æƒ³ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œä¸€ä¸ªç‹¬ç«‹çš„GANç›®æ ‡ï¼Œåœ¨ä¸ä¾èµ–è’¸é¦æŸå¤±çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå…‹æœè¿™ä¸€é™åˆ¶ï¼Œå¹¶è¶³ä»¥å°†æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºé«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆå™¨ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºå¯ä»¥å°†æ‰©æ•£è®­ç»ƒè§†ä¸ºä¸€ç§ç”Ÿæˆé¢„è®­ç»ƒçš„å½¢å¼ï¼Œä¸ºæ¨¡å‹æä¾›è§£é”çš„èƒ½åŠ›ï¼Œé€šè¿‡è½»é‡çº§çš„GANå¾®è°ƒæ¥æé«˜æ€§èƒ½ã€‚æ”¯æŒè¿™ä¸€è§‚ç‚¹çš„æ˜¯ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªä¸€æ­¥ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå†»ç»“äº†85%çš„å‚æ•°ï¼‰ï¼Œä»…ä½¿ç”¨0.2Må›¾åƒå°±å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½¿ç”¨5Må›¾åƒæ—¶æ¥è¿‘SOTAç»“æœã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€æ¬¡é¢‘åŸŸåˆ†æï¼Œè¿™å¯èƒ½è§£é‡Šäº†æ‰©æ•£è®­ç»ƒä¸­è·å¾—çš„ä¸€æ­¥ç”Ÿæˆèƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºæ‰©æ•£è®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†å…¶ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆé¢„è®­ç»ƒè¿‡ç¨‹çš„ä½œç”¨ï¼Œå¯ä»¥ä½œä¸ºæ„å»ºé«˜æ•ˆä¸€æ­¥ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09376v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­çš„è’¸é¦æŠ€æœ¯å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚è®­ç»ƒæˆæœ¬è¾ƒé«˜ä¸”å­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶å‘ç°ï¼Œå¼•å…¥GANç›®æ ‡å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å…¶å†…åœ¨æœºåˆ¶å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡æŒ‡å‡ºè’¸é¦çš„å…³é”®å±€é™æ€§åœ¨äºæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„æ­¥é•¿å’Œå‚æ•°ä¸åŒ¹é…ï¼Œå¯¼è‡´å®ƒä»¬æ”¶æ•›åˆ°ä¸åŒçš„å±€éƒ¨æœ€å°å€¼ï¼Œç›´æ¥æ¨¡ä»¿å¹¶ä¸ç†æƒ³ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œä»…ä½¿ç”¨GANç›®æ ‡ï¼Œæ— éœ€ä¾èµ–è’¸é¦æŸå¤±ï¼Œèƒ½å¤Ÿå…‹æœè¿™ä¸€å±€é™ï¼Œå¹¶å°†æ‰©æ•£æ¨¡å‹è½¬åŒ–ä¸ºé«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆå™¨ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºå°†æ‰©æ•£è®­ç»ƒè§†ä¸ºä¸€ç§ç”Ÿæˆå¼é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¯é€šè¿‡è½»é‡çº§çš„GANå¾®è°ƒè§£é”æ¨¡å‹èƒ½åŠ›ã€‚é€šè¿‡å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„85%å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†ä»…ä½¿ç”¨0.2Må›¾åƒå³å¯è·å¾—å¼ºå¤§æ€§èƒ½ï¼Œä½¿ç”¨5Må›¾åƒæ—¶æ¥è¿‘æœ€ä½³ç»“æœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†é¢‘ç‡åŸŸåˆ†æï¼Œä»¥è§£é‡Šåœ¨æ‰©æ•£è®­ç»ƒä¸­è·å¾—çš„ä¸€æ­¥ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„è’¸é¦æŠ€æœ¯è™½ç„¶å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨è®­ç»ƒæˆæœ¬é«˜å’Œå­¦ç”Ÿæ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„æ­¥é•¿å’Œå‚æ•°ä¸åŒ¹é…æ˜¯è’¸é¦çš„å…³é”®å±€é™ã€‚</li>
<li>å¼•å…¥GANç›®æ ‡å¯ä»¥ç¼“è§£è’¸é¦çš„å±€é™ï¼Œä½¿æ‰©æ•£æ¨¡å‹æˆä¸ºé«˜æ•ˆçš„ä¸€æ­¥ç”Ÿæˆå™¨ã€‚</li>
<li>æ‰©æ•£è®­ç»ƒå¯è§†ä¸ºä¸€ç§ç”Ÿæˆå¼é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¯é€šè¿‡è½»é‡çº§GANå¾®è°ƒè§£é”æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å†»ç»“å¤§éƒ¨åˆ†é¢„è®­ç»ƒå‚æ•°è¿›è¡Œå¾®è°ƒï¼Œå¯åœ¨è¾ƒå°‘å›¾åƒæ•°æ®ä¸‹å®ç°å¼ºå¤§æ€§èƒ½ã€‚</li>
<li>é¢‘ç‡åŸŸåˆ†ææœ‰åŠ©äºè§£é‡Šæ‰©æ•£è®­ç»ƒä¸­ä¸€æ­¥ç”Ÿæˆèƒ½åŠ›çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08fd42e268e2e652d35e40d607787f00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45e982bbb5e881e18505a77319741526.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd30c6715bfefd839d1e9b1ed3fcee94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8ded50444f2a3ba559a62a52ea6adc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6508465941db66b6f125b593f7bf244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-911f7858ee30ce936cc78f9e8eb59731.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SAGE-Exploring-the-Boundaries-of-Unsafe-Concept-Domain-with-Semantic-Augment-Erasing"><a href="#SAGE-Exploring-the-Boundaries-of-Unsafe-Concept-Domain-with-Semantic-Augment-Erasing" class="headerlink" title="SAGE: Exploring the Boundaries of Unsafe Concept Domain with   Semantic-Augment Erasing"></a>SAGE: Exploring the Boundaries of Unsafe Concept Domain with   Semantic-Augment Erasing</h2><p><strong>Authors:Hongguang Zhu, Yunchao Wei, Mengyu Wang, Siyu Jiao, Yan Fang, Jiannan Huang, Yao Zhao</strong></p>
<p>Diffusion models (DMs) have achieved significant progress in text-to-image generation. However, the inevitable inclusion of sensitive information during pre-training poses safety risks, such as unsafe content generation and copyright infringement. Concept erasing finetunes weights to unlearn undesirable concepts, and has emerged as a promising solution. However, existing methods treat unsafe concept as a fixed word and repeatedly erase it, trapping DMs in &#96;&#96;word concept abyssâ€™â€™, which prevents generalized concept-related erasing. To escape this abyss, we introduce semantic-augment erasing which transforms concept word erasure into concept domain erasure by the cyclic self-check and self-erasure. It efficiently explores and unlearns the boundary representation of concept domain through semantic spatial relationships between original and training DMs, without requiring additional preprocessed data. Meanwhile, to mitigate the retention degradation of irrelevant concepts while erasing unsafe concepts, we further propose the global-local collaborative retention mechanism that combines global semantic relationship alignment with local predicted noise preservation, effectively expanding the retentive receptive field for irrelevant concepts. We name our method SAGE, and extensive experiments demonstrate the comprehensive superiority of SAGE compared with other methods in the safe generation of DMs. The code and weights will be open-sourced at <a target="_blank" rel="noopener" href="https://github.com/KevinLight831/SAGE">https://github.com/KevinLight831/SAGE</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å¯é¿å…åœ°ä¼šåŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œè¿™å¸¦æ¥äº†å®‰å…¨é£é™©ï¼Œå¦‚ä¸å®‰å…¨å†…å®¹ç”Ÿæˆå’Œç‰ˆæƒä¾µçŠ¯ã€‚æ¦‚å¿µæ¶ˆé™¤é€šè¿‡å¯¹æƒé‡è¿›è¡Œå¾®è°ƒæ¥é—å¿˜ä¸éœ€è¦çš„æ¦‚å¿µï¼Œå¹¶å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å°†ä¸å®‰å…¨æ¦‚å¿µè§†ä¸ºå›ºå®šå•è¯å¹¶åå¤æ¶ˆé™¤å®ƒï¼Œä½¿æ‰©æ•£æ¨¡å‹é™·å…¥â€œå•è¯æ¦‚å¿µæ·±æ¸Šâ€ï¼Œè¿™é˜»ç¢äº†é€šç”¨çš„æ¦‚å¿µç›¸å…³æ¶ˆé™¤ã€‚ä¸ºäº†é€ƒç¦»è¿™ä¸ªæ·±æ¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰å¢å¼ºæ¶ˆé™¤æ³•ï¼Œé€šè¿‡å¾ªç¯è‡ªæ£€å’Œè‡ªæˆ‘æ¶ˆé™¤ï¼Œå°†æ¦‚å¿µè¯æ±‡æ¶ˆé™¤è½¬å˜ä¸ºæ¦‚å¿µé¢†åŸŸæ¶ˆé™¤ã€‚å®ƒæœ‰æ•ˆåœ°æ¢ç´¢å’Œé—å¿˜äº†æ¦‚å¿µé¢†åŸŸçš„è¾¹ç•Œè¡¨ç¤ºï¼Œé€šè¿‡åŸå§‹å’ŒåŸ¹è®­æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„è¯­ä¹‰ç©ºé—´å…³ç³»ï¼Œè€Œæ— éœ€é¢å¤–çš„é¢„å¤„ç†æ•°æ®ã€‚åŒæ—¶ï¼Œä¸ºäº†å‡è½»åœ¨æ¶ˆé™¤ä¸å®‰å…¨æ¦‚å¿µæ—¶æ— å…³æ¦‚å¿µçš„ä¿ç•™åº¦ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†å…¨å±€-å±€éƒ¨åä½œä¿ç•™æœºåˆ¶ï¼Œå°†å…¨å±€è¯­ä¹‰å…³ç³»å¯¹é½ä¸å±€éƒ¨é¢„æµ‹å™ªå£°ä¿ç•™ç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°æ‰©å¤§äº†æ— å…³æ¦‚å¿µçš„ä¿ç•™æ„Ÿå—é‡ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•å‘½åä¸ºSAGEï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒSAGEåœ¨å®‰å…¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºå…¨é¢çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å’Œæƒé‡å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/KevinLight831/SAGE%E4%B8%8A%E5%BC%BA%E6%BA%90%E3%80%82">https://github.com/KevinLight831/SAGEä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09363v1">PDF</a> Under review</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†é¢„è®­ç»ƒæ—¶ä¸å¯é¿å…åœ°ä¼šåŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œå¸¦æ¥å®‰å…¨éšæ‚£ï¼Œå¦‚ä¸å®‰å…¨å†…å®¹ç”Ÿæˆå’Œç‰ˆæƒä¾µçŠ¯ã€‚æ¦‚å¿µæ¶ˆé™¤æ˜¯ä¸€ç§æ–°å…´è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å°†ä¸å®‰å…¨æ¦‚å¿µè§†ä¸ºå›ºå®šè¯æ±‡è¿›è¡Œåå¤æ¶ˆé™¤ï¼Œä½¿DMsé™·å…¥â€œå•è¯æ¦‚å¿µæ·±æ¸Šâ€ï¼Œæ— æ³•å®ç°é€šç”¨æ¦‚å¿µç›¸å…³æ¶ˆé™¤ã€‚ä¸ºæ‘†è„±æ­¤å›°å¢ƒï¼Œæˆ‘ä»¬æå‡ºè¯­ä¹‰å¢å¼ºæ¶ˆé™¤æ³•ï¼Œé€šè¿‡å¾ªç¯è‡ªæ£€è‡ªæ¶ˆï¼Œå°†æ¦‚å¿µè¯æ±‡æ¶ˆé™¤è½¬å˜ä¸ºæ¦‚å¿µåŸŸæ¶ˆé™¤ã€‚è¯¥æ–¹æ³•é€šè¿‡åŸå§‹å’Œè®­ç»ƒDMsä¹‹é—´çš„è¯­ä¹‰ç©ºé—´å…³ç³»æœ‰æ•ˆæ¢ç´¢å¹¶æ¶ˆé™¤æ¦‚å¿µåŸŸçš„è¾¹ç•Œè¡¨ç¤ºï¼Œæ— éœ€é¢å¤–çš„é¢„å¤„ç†æ•°æ®ã€‚åŒæ—¶ï¼Œä¸ºç¼“è§£åœ¨æ¶ˆé™¤ä¸å®‰å…¨æ¦‚å¿µæ—¶æ— å…³æ¦‚å¿µçš„ä¿ç•™åº¦ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå…¨å±€-å±€éƒ¨åä½œä¿ç•™æœºåˆ¶ï¼Œç»“åˆå…¨å±€è¯­ä¹‰å…³ç³»å¯¹é½å’Œå±€éƒ¨é¢„æµ‹å™ªå£°ä¿ç•™ï¼Œæœ‰æ•ˆæ‰©å¤§æ— å…³æ¦‚å¿µçš„ä¿ç•™æ„Ÿå—é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åä¸ºSAGEï¼Œå®éªŒè¡¨æ˜å…¶åœ¨å®‰å…¨ç”ŸæˆDMsæ–¹é¢è¡¨ç°å…¨é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ä»£ç å’Œæƒé‡å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/KevinLight831/SAGE%E5%BC%B9%E6%BA%90%E3%80%82">https://github.com/KevinLight831/SAGEå¼€æºã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†é¢„è®­ç»ƒåŒ…å«æ•æ„Ÿä¿¡æ¯å¼•å‘å®‰å…¨éšæ‚£ã€‚</li>
<li>æ¦‚å¿µæ¶ˆé™¤æ˜¯ä¸€ç§è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™ï¼Œé™·å…¥â€œå•è¯æ¦‚å¿µæ·±æ¸Šâ€ã€‚</li>
<li>è¯­ä¹‰å¢å¼ºæ¶ˆé™¤æ³•ï¼ˆSAGEï¼‰æå‡ºå¾ªç¯è‡ªæ£€è‡ªæ¶ˆï¼Œå®ç°æ¦‚å¿µåŸŸæ¶ˆé™¤ã€‚</li>
<li>SAGEé€šè¿‡è¯­ä¹‰ç©ºé—´å…³ç³»æ¢ç´¢å¹¶æ¶ˆé™¤æ¦‚å¿µåŸŸè¾¹ç•Œï¼Œæ— éœ€é¢å¤–é¢„å¤„ç†æ•°æ®ã€‚</li>
<li>SAGEæå‡ºå…¨å±€-å±€éƒ¨åä½œä¿ç•™æœºåˆ¶ï¼Œä»¥ä¿ç•™æ— å…³æ¦‚å¿µã€‚</li>
<li>å®éªŒè¯æ˜SAGEåœ¨æ‰©æ•£æ¨¡å‹çš„å®‰å…¨ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfb2e72585eb1cfce0ec86570aa09313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1a3cec0f2ed089f8cb8c77d8e4a88a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f43a3e29c5bced954d3df6fdbf414f95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f804a53488b938ec4f2691cb9ed7ce1b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Seedance-1-0-Exploring-the-Boundaries-of-Video-Generation-Models"><a href="#Seedance-1-0-Exploring-the-Boundaries-of-Video-Generation-Models" class="headerlink" title="Seedance 1.0: Exploring the Boundaries of Video Generation Models"></a>Seedance 1.0: Exploring the Boundaries of Video Generation Models</h2><p><strong>Authors:Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, Feilong Zuo</strong></p>
<p>Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. </p>
<blockquote>
<p>åœ¨æ‰©æ•£å»ºæ¨¡æ–¹é¢ï¼Œæ˜¾è‘—çš„çªç ´æ¨åŠ¨äº†è§†é¢‘ç”Ÿæˆçš„å¿«é€Ÿå‘å±•ï¼Œç„¶è€Œï¼Œå½“å‰çš„åŸºç¡€æ¨¡å‹ä»ç„¶é¢ä¸´ç€åœ¨æç¤ºéµå¾ªã€è¿åŠ¨åˆç†æ€§å’Œè§†è§‰è´¨é‡ä¹‹é—´å¹³è¡¡çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Seedance 1.0ï¼Œè¿™æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½ä¸”æ¨ç†æ•ˆç‡é«˜çš„è§†é¢‘åŸºç¡€ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒèåˆäº†å¤šé¡¹æ ¸å¿ƒæŠ€æœ¯æ”¹è¿›ï¼šä¸€æ˜¯å¯¹å¤šå…ƒæ•°æ®æºè¿›è¡Œç²¾å‡†å’Œæœ‰æ„ä¹‰çš„è§†é¢‘å­—å¹•å¢å¼ºçš„æ•°æ®æ•´ç†ï¼Œå®ç°äº†ä¸åŒåœºæ™¯çš„å…¨é¢å­¦ä¹ ï¼›äºŒæ˜¯é‡‡ç”¨é«˜æ•ˆæ¶æ„è®¾è®¡å¹¶æå‡ºçš„è®­ç»ƒèŒƒå¼ï¼ŒåŸç”Ÿæ”¯æŒå¤šé•œå¤´ç”Ÿæˆå¹¶è”åˆå­¦ä¹ æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘çš„ä»»åŠ¡ï¼›ä¸‰æ˜¯åˆ©ç”¨ç²¾ç»†ç›‘ç£å¾®è°ƒåŠé’ˆå¯¹è§†é¢‘çš„ç‰¹å®šRLHFå’Œå¤šç»´åº¦å¥–åŠ±æœºåˆ¶è¿›è¡Œä»”ç»†ä¼˜åŒ–çš„è®­ç»ƒåæ–¹æ³•ï¼Œä»¥å®ç°å…¨é¢çš„æ€§èƒ½æå‡ï¼›å››æ˜¯é‡‡ç”¨å¤šé˜¶æ®µè’¸é¦ç­–ç•¥å’Œç³»ç»ŸåŒ–ä¼˜åŒ–å®ç°çº¦10å€çš„æ¨ç†åŠ é€Ÿã€‚Seedance 1.0èƒ½å¤Ÿä»¥41.4ç§’ï¼ˆNVIDIA-L20ï¼‰çš„é€Ÿåº¦ç”Ÿæˆåˆ†è¾¨ç‡ä¸º1080pçš„5ç§’è§†é¢‘ã€‚ä¸æœ€å…ˆè¿›çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒSeedance 1.0ä»¥å…¶é«˜è´¨é‡å’Œå¿«é€Ÿè§†é¢‘ç”Ÿæˆèƒ½åŠ›è„±é¢–è€Œå‡ºï¼Œå…·æœ‰å“è¶Šçš„æ—¶ç©ºæµç•…æ€§å’Œç»“æ„ç¨³å®šæ€§ã€åœ¨å¤æ‚çš„å¤šä¸»é¢˜ç¯å¢ƒä¸‹çš„ç²¾ç¡®æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œä»¥åŠä¸€è‡´çš„ä¸»é¢˜è¡¨ç¤ºçš„åŸç”Ÿå¤šé•œå¤´å™äº‹è¿è´¯æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09113v1">PDF</a> Seedance 1.0 Technical Report</p>
<p><strong>Summary</strong></p>
<p>Seedance 1.0æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œè§£å†³äº†å½“å‰æ¨¡å‹åœ¨æç¤ºéµå¾ªã€è¿åŠ¨åˆç†æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡å¤šæºæ•°æ®æ•´åˆã€é«˜æ•ˆæ¶æ„è®¾è®¡ã€ç²¾ç»†è®­ç»ƒåä¼˜åŒ–å’Œæ¨¡å‹åŠ é€Ÿç­‰æŠ€æœ¯çªç ´ï¼Œå®ç°äº†å¿«é€Ÿä¸”é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seedance 1.0å¼•å…¥äº†å¤šæºæ•°æ®æ•´åˆæŠ€æœ¯ï¼Œé€šè¿‡ç²¾ç¡®å’Œæœ‰æ„ä¹‰çš„è§†é¢‘æè¿°è¿›è¡Œå¢å¼ºå­¦ä¹ ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒåœºæ™¯ä¸‹è¿›è¡Œå…¨é¢å­¦ä¹ ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨é«˜æ•ˆæ¶æ„è®¾è®¡ï¼Œæ”¯æŒå¤šé•œå¤´ç”Ÿæˆï¼Œå¹¶è”åˆå­¦ä¹ æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘çš„ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è®­ç»ƒçš„ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨ç²¾ç»†ç›‘ç£å¾®è°ƒä»¥åŠé’ˆå¯¹è§†é¢‘çš„ç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†å…¨é¢çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨å¤šé˜¶æ®µè’¸é¦ç­–ç•¥å’Œç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œå®ç°äº†çº¦10å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚</li>
<li>Seedance 1.0èƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œä¾‹å¦‚ï¼Œåœ¨NVIDIA-L20ä¸Šä»…éœ€è¦41.4ç§’å°±èƒ½ç”Ÿæˆä¸€ä¸ªåˆ†è¾¨ç‡ä¸º1080pçš„5ç§’è§†é¢‘ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ—¶ç©ºæµç•…æ€§ã€ç»“æ„ç¨³å®šæ€§æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„å¤šä¸»é¢˜ç¯å¢ƒä¸­ç²¾ç¡®éµå¾ªæŒ‡ä»¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cb32d8906590a03bedf26895dba4eed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be09149a4c53c37902d37e8120a83062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71fa48e29ad67bf5c0448791478be58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-142323af4d403cf32936fc33e8834b6f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Flow-Diverse-and-Efficient-Learning-Momentum-Flow-Matching-via-Stochastic-Velocity-Field-Sampling"><a href="#Flow-Diverse-and-Efficient-Learning-Momentum-Flow-Matching-via-Stochastic-Velocity-Field-Sampling" class="headerlink" title="Flow Diverse and Efficient: Learning Momentum Flow Matching via   Stochastic Velocity Field Sampling"></a>Flow Diverse and Efficient: Learning Momentum Flow Matching via   Stochastic Velocity Field Sampling</h2><p><strong>Authors:Zhiyuan Ma, Ruixun Liu, Sixian Liu, Jianjun Li, Bowen Zhou</strong></p>
<p>Recently, the rectified flow (RF) has emerged as the new state-of-the-art among flow-based diffusion models due to its high efficiency advantage in straight path sampling, especially with the amazing images generated by a series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line connection between the noisy and natural data distributions is intuitive, fast, and easy to optimize, it still inevitably leads to: 1) Diversity concerns, which arise since straight-line paths only cover a fairly restricted sampling space. 2) Multi-scale noise modeling concerns, since the straight line flow only needs to optimize the constant velocity field $\bm v$ between the two distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present Discretized-RF, a new family of rectified flow (also called momentum flow models since they refer to the previous velocity component and the random velocity component in each diffusion step), which discretizes the straight path into a series of variable velocity field sub-paths (namely &#96;&#96;momentum fieldsâ€™â€™) to expand the search space, especially when close to the distribution $p_\text{noise}$. Different from the previous case where noise is directly superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the sub-path to change its direction in order to improve the diversity and multi-scale noise modeling abilities. Experimental results on several representative datasets demonstrate that learning momentum flow matching by sampling random velocity fields will produce trajectories that are both diverse and efficient, and can consistently generate high-quality and diverse results. Code is available at <a target="_blank" rel="noopener" href="https://github.com/liuruixun/momentum-fm">https://github.com/liuruixun/momentum-fm</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”±äºå…¶åœ¨ç›´çº¿è·¯å¾„é‡‡æ ·ä¸­çš„é«˜æ•ˆç‡ä¼˜åŠ¿ï¼Œæ•´æµæµï¼ˆRectified Flowï¼Œç®€ç§°RFï¼‰å·²æˆä¸ºåŸºäºæµçš„æ‰©æ•£æ¨¡å‹ä¸­çš„æœ€æ–°å…ˆè¿›æŠ€æœ¯ã€‚å°¤å…¶æ˜¯ä¸€ç³»åˆ—RFæ¨¡å‹ï¼ˆå¦‚Flux 1.0å’ŒSD 3.0ï¼‰æ‰€ç”Ÿæˆçš„æƒŠäººå›¾åƒã€‚å°½ç®¡å™ªå£°æ•°æ®å’Œè‡ªç„¶æ•°æ®åˆ†å¸ƒä¹‹é—´çš„ç›´çº¿è¿æ¥ç›´è§‚ã€å¿«é€Ÿä¸”æ˜“äºä¼˜åŒ–ï¼Œä½†å®ƒä»ç„¶ä¸å¯é¿å…åœ°å¯¼è‡´ä»¥ä¸‹é—®é¢˜ï¼š1ï¼‰å¤šæ ·æ€§é—®é¢˜ï¼Œå› ä¸ºç›´çº¿è·¯å¾„åªè¦†ç›–äº†ä¸€ä¸ªç›¸å¯¹æœ‰é™çš„é‡‡æ ·ç©ºé—´ã€‚2ï¼‰å¤šå°ºåº¦å™ªå£°å»ºæ¨¡é—®é¢˜ï¼Œå› ä¸ºç›´çº¿æµåªéœ€è¦ä¼˜åŒ–ä¸¤ä¸ªåˆ†å¸ƒ$\bm\pi_0$å’Œ$\bm\pi_1$ä¹‹é—´çš„æ’å®šé€Ÿåº¦åœº$\bm v$ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¦»æ•£åŒ–RFï¼Œè¿™æ˜¯ä¸€ç±»æ–°çš„æ•´æµæµï¼ˆä¹Ÿç§°ä¸ºåŠ¨é‡æµæ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æŒ‡æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­çš„å…ˆå‰é€Ÿåº¦åˆ†é‡å’Œéšæœºé€Ÿåº¦åˆ†é‡ï¼‰ï¼Œå®ƒå°†ç›´çº¿è·¯å¾„ç¦»æ•£åŒ–æˆä¸€ç³»åˆ—å¯å˜é€Ÿåº¦åœºå­è·¯å¾„ï¼ˆå³â€œåŠ¨é‡åœºâ€ï¼‰ï¼Œä»¥æ‰©å¤§æœç´¢ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¥è¿‘åˆ†å¸ƒ$p_\text{noise}$æ—¶ã€‚ä¸åŒäºä»¥å‰ç›´æ¥å°†å™ªå£°å åŠ åˆ°$\bm x$ä¸Šçš„æƒ…å†µï¼Œæˆ‘ä»¬åœ¨å­è·¯å¾„çš„é€Ÿåº¦$\bm v$ä¸Šå¼•å…¥å™ªå£°ï¼Œä»¥æ”¹å˜å…¶æ–¹å‘ï¼Œä»è€Œæé«˜å¤šæ ·æ€§å’Œå¤šå°ºåº¦å™ªå£°å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨å‡ ä¸ªä»£è¡¨æ€§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œé€šè¿‡å­¦ä¹ åŠ¨é‡æµåŒ¹é…å¹¶é‡‡æ ·éšæœºé€Ÿåº¦åœºï¼Œå¯ä»¥äº§ç”Ÿæ—¢å¤šæ ·åŒ–åˆé«˜æ•ˆçš„è½¨è¿¹ï¼Œå¹¶ä¸”èƒ½æŒç»­ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/liuruixun/momentum-fm%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liuruixun/momentum-fmä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08796v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°å‡ºç°çš„æ•´æµæµï¼ˆRectified Flowï¼Œç®€ç§°RFï¼‰åŸºäºå…¶åœ¨ç›´çº¿è·¯å¾„é‡‡æ ·ä¸Šçš„é«˜æ•ˆç‡ä¼˜åŠ¿ï¼Œå·²æˆä¸ºå½“å‰æµè¡Œçš„æ‰©æ•£æ¨¡å‹ã€‚å°½ç®¡ä»å™ªå£°æ•°æ®åˆ†å¸ƒåˆ°è‡ªç„¶æ•°æ®åˆ†å¸ƒçš„ç›´çº¿è¿æ¥ç›´è§‚ã€å¿«é€Ÿä¸”æ˜“äºä¼˜åŒ–ï¼Œä½†å®ƒä»ç„¶é¢ä¸´å¤šæ ·æ€§å’Œå¤šå°ºåº¦å™ªå£°å»ºæ¨¡çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºç¦»æ•£åŒ–æ•´æµæµï¼ˆDiscretized-RFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ•´æµæµå®¶æ—ï¼ˆä¹Ÿç§°ä¸ºåŠ¨é‡æµæ¨¡å‹ï¼‰ï¼Œå®ƒå°†ç›´çº¿è·¯å¾„ç¦»æ•£åŒ–ä¸ºä¸€ç³»åˆ—å¯å˜é€Ÿåº¦åœºå­è·¯å¾„ï¼ˆå³â€œåŠ¨é‡åœºâ€ï¼‰ï¼Œä»¥æ‰©å¤§æœç´¢ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¥è¿‘å™ªå£°åˆ†å¸ƒæ—¶ã€‚ä¸åŒäºä»¥å¾€ç›´æ¥åœ¨$\bm x$ä¸Šå åŠ å™ªå£°çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å­è·¯å¾„çš„é€Ÿåº¦$\bm v$ä¸Šå¼•å…¥å™ªå£°ï¼Œä»¥æ”¹å˜å…¶æ–¹å‘ï¼Œä»è€Œæé«˜å¤šæ ·æ€§å’Œå¤šå°ºåº¦å™ªå£°å»ºæ¨¡èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡éšæœºé€Ÿåº¦åœºé‡‡æ ·å­¦ä¹ åŠ¨é‡æµåŒ¹é…ï¼Œå¯ä»¥äº§ç”Ÿæ—¢å¤šæ ·åˆé«˜æ•ˆçš„è½¨è¿¹ï¼Œå¹¶èƒ½æŒç»­ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•´æµæµï¼ˆRFï¼‰å·²æˆä¸ºåŸºäºæµæ‰©æ•£æ¨¡å‹çš„æ–°æŠ€æœ¯å‰æ²¿ï¼Œä»¥å…¶ç›´çº¿è·¯å¾„é‡‡æ ·çš„é«˜æ•ˆç‡è‘—ç§°ã€‚</li>
<li>RFè™½ç„¶ç›´è§‚ã€å¿«é€Ÿä¸”æ˜“äºä¼˜åŒ–ï¼Œä½†å­˜åœ¨å¤šæ ·æ€§å’Œå¤šå°ºåº¦å™ªå£°å»ºæ¨¡çš„é—®é¢˜ã€‚</li>
<li>ç¦»æ•£åŒ–æ•´æµæµï¼ˆDiscretized-RFï¼‰æ˜¯ä¸€ç§æ–°çš„RFæ¨¡å‹ï¼Œé€šè¿‡ç¦»æ•£åŒ–ç›´çº¿è·¯å¾„å¹¶ä½¿ç”¨â€œåŠ¨é‡åœºâ€æ¥æ‰©å¤§æœç´¢ç©ºé—´ï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒDiscretized-RFåœ¨å­è·¯å¾„çš„é€Ÿåº¦ä¸Šå¼•å…¥å™ªå£°ï¼Œä»¥æ”¹å˜æ–¹å‘ï¼Œä»è€Œæé«˜å¤šæ ·æ€§å’Œå¤šå°ºåº¦å™ªå£°å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡éšæœºé€Ÿåº¦åœºé‡‡æ ·å­¦ä¹ åŠ¨é‡æµåŒ¹é…èƒ½å¤Ÿäº§ç”Ÿé«˜æ•ˆä¸”å¤šæ ·åŒ–çš„è½¨è¿¹ã€‚</li>
<li>Discretized-RFæ¨¡å‹å¯ä»¥æŒç»­ç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-011e7b2a700c897d72b5f6581d8e9ab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902eeb2f12f5f8fbf55e1034b8617ec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5033bdb8d371c21d589fe924c4ed4267.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MAMBO-High-Resolution-Generative-Approach-for-Mammography-Images"><a href="#MAMBO-High-Resolution-Generative-Approach-for-Mammography-Images" class="headerlink" title="MAMBO: High-Resolution Generative Approach for Mammography Images"></a>MAMBO: High-Resolution Generative Approach for Mammography Images</h2><p><strong>Authors:Milica Å kipina, Nikola JoviÅ¡iÄ‡, Nicola Dallâ€™Asen, Vanja Å venda, Anil Osman Tur, Slobodan IliÄ‡, Elisa Ricci, Dubravko Ä†ulibrk</strong></p>
<p>Mammography is the gold standard for the detection and diagnosis of breast cancer. This procedure can be significantly enhanced with Artificial Intelligence (AI)-based software, which assists radiologists in identifying abnormalities. However, training AI systems requires large and diverse datasets, which are often difficult to obtain due to privacy and ethical constraints. To address this issue, the paper introduces MAMmography ensemBle mOdel (MAMBO), a novel patch-based diffusion approach designed to generate full-resolution mammograms. Diffusion models have shown breakthrough results in realistic image generation, yet few studies have focused on mammograms, and none have successfully generated high-resolution outputs required to capture fine-grained features of small lesions. To achieve this, MAMBO integrates separate diffusion models to capture both local and global (image-level) contexts. The contextual information is then fed into the final patch-based model, significantly aiding the noise removal process. This thoughtful design enables MAMBO to generate highly realistic mammograms of up to 3840x3840 pixels. Importantly, this approach can be used to enhance the training of classification models and extended to anomaly detection. Experiments, both numerical and radiologist validation, assess MAMBOâ€™s capabilities in image generation, super-resolution, and anomaly detection, highlighting its potential to enhance mammography analysis for more accurate diagnoses and earlier lesion detection. </p>
<blockquote>
<p>ä¹³è…ºXå…‰æ‘„å½±æ˜¯æ£€æµ‹å’Œè¯Šæ–­ä¹³è…ºç™Œçš„é‡‘æ ‡å‡†ã€‚é€šè¿‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è½¯ä»¶å¯ä»¥æå¤§åœ°å¢å¼ºè¿™ä¸€ç¨‹åºçš„æ•ˆèƒ½ï¼Œå¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿè¯†åˆ«å¼‚å¸¸ã€‚ç„¶è€Œï¼Œè®­ç»ƒAIç³»ç»Ÿéœ€è¦å¤§é‡çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œç”±äºéšç§å’Œä¼¦ç†çº¦æŸï¼Œè¿™äº›æ•°æ®é›†çš„è·å–å¾€å¾€å¾ˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†MAMmography ensemBle mOdelï¼ˆMAMBOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¡¥ä¸çš„æ–°å‹æ‰©æ•£æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå…¨åˆ†è¾¨ç‡ä¹³è…ºXå…‰ç‰‡ã€‚æ‰©æ•£æ¨¡å‹åœ¨çœŸå®å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†çªç ´æ€§æˆæœï¼Œä½†å¯¹ä¹³è…ºXå…‰ç‰‡çš„ç ”ç©¶å¾ˆå°‘ï¼Œè€Œä¸”æ²¡æœ‰ä»»ä½•ç ”ç©¶èƒ½å¤ŸæˆåŠŸç”Ÿæˆæ•æ‰å°ç—…ç¶ç»†å¾®ç‰¹å¾æ‰€éœ€çš„é«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒMAMBOé›†æˆäº†å•ç‹¬çš„æ‰©æ•£æ¨¡å‹æ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ï¼ˆå›¾åƒçº§åˆ«ï¼‰çš„ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œå°†ä¸Šä¸‹æ–‡ä¿¡æ¯è¾“å…¥æœ€ç»ˆçš„åŸºäºè¡¥ä¸çš„æ¨¡å‹ï¼Œæå¤§åœ°æœ‰åŠ©äºå»å™ªè¿‡ç¨‹ã€‚è¿™ä¸€ç²¾å¿ƒè®¾è®¡ä½¿MAMBOèƒ½å¤Ÿç”Ÿæˆé«˜è¾¾3840x3840åƒç´ çš„éå¸¸é€¼çœŸçš„ä¹³è…ºXå…‰ç‰‡ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•å¯ç”¨äºæé«˜åˆ†ç±»æ¨¡å‹çš„è®­ç»ƒæ•ˆæœï¼Œå¹¶å¯æ‰©å±•åˆ°å¼‚å¸¸æ£€æµ‹ã€‚å®éªŒåŒ…æ‹¬æ•°å€¼å’Œæ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯ï¼Œè¯„ä¼°äº†MAMBOåœ¨å›¾åƒç”Ÿæˆã€è¶…åˆ†è¾¨ç‡å’Œå¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ï¼Œçªå‡ºäº†å…¶åœ¨æé«˜ä¹³è…ºXå…‰æ‘„å½±åˆ†ææ–¹é¢å®ç°æ›´ç²¾ç¡®è¯Šæ–­å’Œæ›´æ—©ç—…ç¶æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08677v1">PDF</a> 21 pages, 14 figures, 7 tables</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†MAMmography ensemBle mOdelï¼ˆMAMBOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ä¹³è…ºXå…‰ç…§ç‰‡ã€‚è¯¥æŠ€æœ¯ç»“åˆäº†å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€šè¿‡å™ªå£°å»é™¤è¿‡ç¨‹ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œæœ‰åŠ©äºæé«˜ä¹³è…ºç™Œçš„è¯Šæ–­å‡†ç¡®æ€§å’Œæ—©æœŸç—…å˜æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºXå…‰æ‘„å½±æ˜¯ä¹³è…ºç™Œæ£€æµ‹å’Œè¯Šæ–­çš„é‡‘æ ‡å‡†ï¼Œè€Œäººå·¥æ™ºèƒ½è½¯ä»¶çš„è¾…åŠ©å¯ä»¥æ˜¾è‘—æé«˜è¯¥è¿‡ç¨‹çš„æ•ˆç‡ã€‚</li>
<li>è®­ç»ƒAIç³»ç»Ÿéœ€è¦å¤§é‡çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œä½†ç”±äºéšç§å’Œä¼¦ç†çº¦æŸï¼Œè¿™äº›æ•°æ®é›†å¾€å¾€éš¾ä»¥è·å¾—ã€‚</li>
<li>MAMBOæ˜¯ä¸€ç§åˆ›æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¹³è…ºXå…‰ç…§ç‰‡ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°æ•°æ®è·å–é—®é¢˜ã€‚</li>
<li>MAMBOç»“åˆäº†å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆé«˜åº¦é€¼çœŸçš„é«˜åˆ†è¾¨ç‡ä¹³è…ºXå…‰ç…§ç‰‡ï¼Œæœ‰åŠ©äºæ›´å‡†ç¡®çš„è¯Šæ–­å’Œæ—©æœŸç—…å˜æ£€æµ‹ã€‚</li>
<li>MAMBOé‡‡ç”¨ç‹¬ç‰¹çš„å™ªå£°å»é™¤è¿‡ç¨‹ï¼Œæœ‰åŠ©äºå¢å¼ºå›¾åƒè´¨é‡å’Œè¯†åˆ«å¾®å°ç—…å˜çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ•°å€¼å’Œæ”¾å°„ç§‘åŒ»ç”ŸéªŒè¯çš„å®éªŒè¯„ä¼°äº†MAMBOåœ¨å›¾åƒç”Ÿæˆã€è¶…åˆ†è¾¨ç‡å’Œå¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b6874cb03fa650655811aa497b381a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-814b01bbf0533942a94cc7d93b95043a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21bb3a3fd14b5c33cc78d69bacb21aa9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4c9af08352d6bae5bcbad85c4028a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3efeb2312ad1c51823bc522bf0d49f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a2eacfabf619bb4e0d8c28e7ab58fc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RoboSwap-A-GAN-driven-Video-Diffusion-Framework-For-Unsupervised-Robot-Arm-Swapping"><a href="#RoboSwap-A-GAN-driven-Video-Diffusion-Framework-For-Unsupervised-Robot-Arm-Swapping" class="headerlink" title="RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot   Arm Swapping"></a>RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot   Arm Swapping</h2><p><strong>Authors:Yang Bai, Liudi Yang, George Eskandar, Fengyi Shen, Dong Chen, Mohammad Altillawi, Ziyuan Liu, Gitta Kutyniok</strong></p>
<p>Recent advancements in generative models have revolutionized video synthesis and editing. However, the scarcity of diverse, high-quality datasets continues to hinder video-conditioned robotic learning, limiting cross-platform generalization. In this work, we address the challenge of swapping a robotic arm in one video with another: a key step for crossembodiment learning. Unlike previous methods that depend on paired video demonstrations in the same environmental settings, our proposed framework, RoboSwap, operates on unpaired data from diverse environments, alleviating the data collection needs. RoboSwap introduces a novel video editing pipeline integrating both GANs and diffusion models, combining their isolated advantages. Specifically, we segment robotic arms from their backgrounds and train an unpaired GAN model to translate one robotic arm to another. The translated arm is blended with the original video background and refined with a diffusion model to enhance coherence, motion realism and object interaction. The GAN and diffusion stages are trained independently. Our experiments demonstrate that RoboSwap outperforms state-of-the-art video and image editing models on three benchmarks in terms of both structural coherence and motion consistency, thereby offering a robust solution for generating reliable, cross-embodiment data in robotic learning. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆæ¨¡å‹çš„å‘å±•åœ¨è§†é¢‘åˆæˆå’Œç¼–è¾‘æ–¹é¢å¼•å‘äº†é©å‘½æ€§çš„å˜é©ã€‚ç„¶è€Œï¼Œå¤šæ ·ä¸”é«˜è´¨é‡æ•°æ®é›†çš„ç¨€ç¼ºä»ç„¶é˜»ç¢äº†è§†é¢‘æ§åˆ¶ä¸‹çš„æœºå™¨äººå­¦ä¹ ï¼Œé™åˆ¶äº†è·¨å¹³å°çš„æ³›åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åœ¨ä¸€ä¸ªè§†é¢‘ä¸­ç”¨ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚æ›¿æ¢å¦ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯è·¨embodimentå­¦ä¹ çš„å…³é”®æ­¥éª¤ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ä¾èµ–äºç›¸åŒç¯å¢ƒè®¾ç½®ä¸­çš„é…å¯¹è§†é¢‘æ¼”ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„RoboSwapæ¡†æ¶èƒ½å¤Ÿåœ¨æ¥è‡ªä¸åŒç¯å¢ƒçš„ä¸é…å¯¹æ•°æ®ä¸Šè¿›è¡Œæ“ä½œï¼Œä»è€Œå‡è½»äº†æ•°æ®æ”¶é›†çš„éœ€æ±‚ã€‚RoboSwapå¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘ç¼–è¾‘ç®¡é“ï¼Œèåˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»èƒŒæ™¯ä¸­åˆ†å‰²å‡ºæœºå™¨äººæ‰‹è‡‚ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªä¸åŒ¹é…çš„GANæ¨¡å‹å°†ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚ç¿»è¯‘åˆ°å¦ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚ä¸Šã€‚å°†ç¿»è¯‘åçš„æ‰‹è‡‚ä¸åŸå§‹è§†é¢‘èƒŒæ™¯æ··åˆï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜è¿è´¯æ€§ã€è¿åŠ¨çœŸå®æ€§å’Œç‰©ä½“äº¤äº’æ€§ã€‚GANå’Œæ‰©æ•£é˜¶æ®µæ˜¯ç‹¬ç«‹è®­ç»ƒçš„ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸‰é¡¹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRoboSwapåœ¨ç»“æ„è¿è´¯æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„è§†é¢‘å’Œå›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œä»è€Œä¸ºæœºå™¨äººå­¦ä¹ ä¸­ç”Ÿæˆå¯é ã€è·¨embodimentæ•°æ®æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08632v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„å‘å±•å·²å¯¹è§†é¢‘åˆæˆå’Œç¼–è¾‘äº§ç”Ÿå·¨å¤§å½±å“ï¼Œä½†ä»é¢ä¸´é«˜è´¨é‡æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†è§†é¢‘æ¡ä»¶ä¸‹çš„æœºå™¨äººå­¦ä¹ å¹¶é˜»ç¢äº†è·¨å¹³å°æ³›åŒ–ã€‚æœ¬ç ”ç©¶è§£å†³äº†ä¸€ä¸ªéš¾é¢˜ï¼šåœ¨ä¸€ä¸ªè§†é¢‘ä¸­æ›¿æ¢æ‰æœºå™¨äººæ‰‹è‡‚ä¸ºå¦ä¸€ä¸ªæ‰‹è‡‚çš„å…³é”®æ­¥éª¤ï¼Œè¿™æ˜¯è·¨æœºå™¨äººå­¦ä¹ çš„é‡è¦ä¸€ç¯ã€‚ä¸åŒäºä¾èµ–ç›¸åŒç¯å¢ƒè®¾ç½®ä¸‹çš„é…å¯¹è§†é¢‘ç¤ºèŒƒçš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºçš„RoboSwapæ¡†æ¶ä½¿ç”¨æ¥è‡ªä¸åŒç¯å¢ƒçš„éé…å¯¹æ•°æ®ï¼Œå‡è½»äº†æ•°æ®æ”¶é›†çš„éœ€æ±‚ã€‚RoboSwapå¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘ç¼–è¾‘ç®¡é“ï¼Œç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»èƒŒæ™¯ä¸­åˆ†å‰²å‡ºæœºå™¨äººæ‰‹è‡‚ï¼Œå¹¶ç”¨éé…å¯¹GANæ¨¡å‹ç¿»è¯‘ä¸€ä¸ªæœºå™¨äººæ‰‹è‡‚ä¸ºå¦ä¸€ä¸ªæ‰‹è‡‚ã€‚ç¿»è¯‘åçš„æ‰‹è‡‚ä¸åŸè§†é¢‘èƒŒæ™¯ç»“åˆï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜è¿è´¯æ€§ã€è¿åŠ¨çœŸå®æ€§å’Œç‰©ä½“äº¤äº’æ€§ã€‚GANå’Œæ‰©æ•£é˜¶æ®µæ˜¯ç‹¬ç«‹è®­ç»ƒçš„ã€‚å®éªŒè¯æ˜ï¼ŒRoboSwapåœ¨ç»“æ„è¿è´¯æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„è§†é¢‘å’Œå›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œä¸ºæœºå™¨äººå­¦ä¹ ä¸­ç”Ÿæˆå¯é ã€è·¨æœºå™¨äººæ•°æ®æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸç”Ÿæˆæ¨¡å‹çš„å‘å±•å¯¹è§†é¢‘åˆæˆå’Œç¼–è¾‘äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚</li>
<li>æ•°æ®é›†ç¨€ç¼ºæ˜¯é˜»ç¢è§†é¢‘æ¡ä»¶ä¸‹çš„æœºå™¨äººå­¦ä¹ çš„ä¸»è¦éš¾é¢˜ä¹‹ä¸€ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶RoboSwapæ¥è§£å†³æœºå™¨äººæ‰‹è‡‚æ›¿æ¢çš„é—®é¢˜ã€‚</li>
<li>RoboSwapä½¿ç”¨éé…å¯¹æ•°æ®ï¼Œæ¥è‡ªä¸åŒçš„ç¯å¢ƒï¼Œå‡è½»äº†æ•°æ®æ”¶é›†çš„å‹åŠ›ã€‚</li>
<li>RoboSwapç»“åˆäº†GANså’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„è§†é¢‘ç¼–è¾‘ç®¡é“ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRoboSwapåœ¨ç»“æ„è¿è´¯æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4ac53fc63a7d3a41a4f67a651727f0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c253953e323a1c092a6b9f83d1f0bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ac1ccda10d0047c545f737bae5a297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06e40d79b25479dd4de6d39f7a7b4472.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e665a1b30dd34211e4ffe0954362bb05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92a027269a964f051e7652c965bede3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="How-Much-To-Guide-Revisiting-Adaptive-Guidance-in-Classifier-Free-Guidance-Text-to-Vision-Diffusion-Models"><a href="#How-Much-To-Guide-Revisiting-Adaptive-Guidance-in-Classifier-Free-Guidance-Text-to-Vision-Diffusion-Models" class="headerlink" title="How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free   Guidance Text-to-Vision Diffusion Models"></a>How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free   Guidance Text-to-Vision Diffusion Models</h2><p><strong>Authors:Huixuan Zhang, Junzhe Zhang, Xiaojun Wan</strong></p>
<p>With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method. </p>
<blockquote>
<p>éšç€æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ— æ¡ä»¶æŒ‡å¯¼å·²ç»æˆä¸ºæœ€æµè¡Œçš„æ¡ä»¶åŒ–æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨æ¨¡å‹å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦ä¸¤å€çš„æ­¥éª¤ï¼Œç›¸æ¯”äºæ— æ¡ä»¶ç”Ÿæˆï¼Œå¯¼è‡´æˆæœ¬æ˜¾è‘—ä¸Šå‡ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²ç»å¼•å…¥äº†è‡ªé€‚åº”æŒ‡å¯¼çš„æ¦‚å¿µï¼Œä½†å®ƒç¼ºä¹åšå®åˆ†æå’Œå®è¯ç»“æœï¼Œä½¿å¾—ä¹‹å‰çš„æ–¹æ³•æ— æ³•åº”ç”¨äºé€šç”¨æ‰©æ•£æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å¦ä¸€ä¸ªè§’åº¦æ¢è®¨äº†è‡ªé€‚åº”æŒ‡å¯¼çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†é€šç”¨è‡ªé€‚åº”æŒ‡å¯¼ç­–ç•¥Step AGã€‚æˆ‘ä»¬çš„è¯„ä¼°æ—¢å…³æ³¨å›¾åƒè´¨é‡åˆå…³æ³¨å›¾åƒæ–‡æœ¬å¯¹é½æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå°†æ— åˆ†ç±»æŒ‡å¯¼é™åˆ¶åœ¨æœ€åˆçš„å‡ ä¸ªå»å™ªæ­¥éª¤ä¸­è¶³ä»¥ç”Ÿæˆé«˜è´¨é‡ã€æ¡ä»¶è‰¯å¥½çš„å›¾åƒï¼Œå®ç°äº†å¹³å‡20%åˆ°30%çš„é€Ÿåº¦æå‡ã€‚è¿™ç§æ”¹è¿›åœ¨ä¸åŒçš„æ¨ç†æ­¥éª¤å’Œä¸åŒçš„æ¨¡å‹ï¼ˆåŒ…æ‹¬è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼‰ä¸­éƒ½æ˜¯ä¸€è‡´çš„ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†éšç€æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•æˆä¸ºäº†æœ€æµè¡Œçš„æ¡ä»¶åŒ–æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç›¸æ¯”äºæ— æ¡ä»¶ç”Ÿæˆéœ€è¦ä¸¤å€çš„æ¨¡å‹å‰å‘ä¼ æ’­æ­¥éª¤ï¼Œå¯¼è‡´æˆæœ¬æ˜¾è‘—ä¸Šå‡ã€‚æœ¬æ–‡æå‡ºäº†å¦ä¸€ç§åº”ç”¨è‡ªé€‚åº”å¼•å¯¼çš„è§†è§’ï¼Œå¹¶æ¨å‡ºäº†Step AGï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ã€é€šç”¨é€‚ç”¨çš„è‡ªé€‚åº”å¼•å¯¼ç­–ç•¥ã€‚é€šè¿‡å¯¹å›¾åƒè´¨é‡å’Œå›¾åƒæ–‡æœ¬å¯¹é½æ€§çš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå°†æ— åˆ†ç±»å™¨å¼•å¯¼é™åˆ¶åœ¨æœ€åˆçš„å‡ ä¸ªå»å™ªæ­¥éª¤ä¸­è¶³ä»¥ç”Ÿæˆé«˜è´¨é‡ã€æ¡ä»¶è‰¯å¥½çš„å›¾åƒï¼Œå®ç°äº†å¹³å‡20%è‡³30%çš„åŠ é€Ÿã€‚è¿™ç§æ”¹è¿›åœ¨ä¸åŒè®¾ç½®å’Œæ¨¡å‹ä¸­éƒ½è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°è§†è§‰ç”Ÿæˆæ‰©æ•£æ¨¡å‹å¿«é€Ÿå‘å±•ï¼Œæ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•æˆä¸ºæœ€æµè¡Œçš„æ¡ä»¶åŒ–æ–¹æ³•ã€‚</li>
<li>æ— åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•éœ€è¦æ›´å¤šçš„æ¨¡å‹å‰å‘ä¼ æ’­æ­¥éª¤ï¼Œå¯¼è‡´æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æå‡ºäº†Step AGï¼Œä¸€ç§ç®€å•ã€é€šç”¨é€‚ç”¨çš„è‡ªé€‚åº”å¼•å¯¼ç­–ç•¥ã€‚</li>
<li>é™åˆ¶æ— åˆ†ç±»å™¨å¼•å¯¼åœ¨æœ€åˆçš„å‡ ä¸ªå»å™ªæ­¥éª¤ä¸­å¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€æ¡ä»¶è‰¯å¥½çš„å›¾åƒã€‚</li>
<li>Step AGæ–¹æ³•å®ç°äº†å¹³å‡20%è‡³30%çš„åŠ é€Ÿï¼Œæ”¹è¿›åœ¨ä¸åŒè®¾ç½®å’Œæ¨¡å‹ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è‡ªé€‚åº”å¼•å¯¼ç­–ç•¥åœ¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹ä¸­ä¹Ÿæœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-735056ca6503eba366e68a1603f10266.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09b871bd316a768bf3c90cc57431f277.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465c7b32502881b6ed6d110bc4454d26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90149503eaa8bd28fda0c163d0269d09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f792cdeaf4a0d90bb003626e5dd7eb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a012fd8674e9927a685e20e8a0dc2098.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Is-Perturbation-Based-Image-Protection-Disruptive-to-Image-Editing"><a href="#Is-Perturbation-Based-Image-Protection-Disruptive-to-Image-Editing" class="headerlink" title="Is Perturbation-Based Image Protection Disruptive to Image Editing?"></a>Is Perturbation-Based Image Protection Disruptive to Image Editing?</h2><p><strong>Authors:Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati</strong></p>
<p>The remarkable image generation capabilities of state-of-the-art diffusion models, such as Stable Diffusion, can also be misused to spread misinformation and plagiarize copyrighted materials. To mitigate the potential risks associated with image editing, current image protection methods rely on adding imperceptible perturbations to images to obstruct diffusion-based editing. A fully successful protection for an image implies that the output of editing attempts is an undesirable, noisy image which is completely unrelated to the reference image. In our experiments with various perturbation-based image protection methods across multiple domains (natural scene images and artworks) and editing tasks (image-to-image generation and style editing), we discover that such protection does not achieve this goal completely. In most scenarios, diffusion-based editing of protected images generates a desirable output image which adheres precisely to the guidance prompt. Our findings suggest that adding noise to images may paradoxically increase their association with given text prompts during the generation process, leading to unintended consequences such as better resultant edits. Hence, we argue that perturbation-based methods may not provide a sufficient solution for robust image protection against diffusion-based editing. </p>
<blockquote>
<p>å…ˆè¿›æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„å‡ºè‰²å›¾åƒç”Ÿæˆèƒ½åŠ›ä¹Ÿå¯èƒ½è¢«è¯¯ç”¨ï¼Œä»¥ä¼ æ’­è™šå‡ä¿¡æ¯å’ŒæŠ„è¢­ç‰ˆæƒææ–™ã€‚ä¸ºäº†å‡å°‘ä¸å›¾åƒç¼–è¾‘ç›¸å…³çš„æ½œåœ¨é£é™©ï¼Œå½“å‰çš„å›¾åƒä¿æŠ¤æ–¹æ³•ä¾èµ–äºå‘å›¾åƒæ·»åŠ å‡ ä¹æ— æ³•å¯Ÿè§‰çš„æ‰°åŠ¨æ¥é˜»ç¢åŸºäºæ‰©æ•£çš„ç¼–è¾‘ã€‚å¯¹å›¾åƒè¿›è¡Œå®Œå…¨æˆåŠŸçš„ä¿æŠ¤æ„å‘³ç€ç¼–è¾‘å°è¯•çš„è¾“å‡ºæ˜¯ä¸€ä¸ªä¸å—æ¬¢è¿çš„ã€å¸¦æœ‰å™ªéŸ³çš„å›¾åƒï¼Œä¸å‚è€ƒå›¾åƒå®Œå…¨æ— å…³ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¢†åŸŸï¼ˆè‡ªç„¶åœºæ™¯å›¾åƒå’Œè‰ºæœ¯å“ï¼‰çš„å„ç§åŸºäºæ‰°åŠ¨çš„å›¾åƒä¿æŠ¤æ–¹æ³•ä¸ç¼–è¾‘ä»»åŠ¡ï¼ˆå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆå’Œæ ·å¼ç¼–è¾‘ï¼‰çš„å®éªŒä¸­å‘ç°ï¼Œè¿™ç§ä¿æŠ¤å¹¶æ²¡æœ‰å®Œå…¨å®ç°è¿™ä¸€ç›®æ ‡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯¹å—ä¿æŠ¤å›¾åƒçš„æ‰©æ•£å¼ç¼–è¾‘ä¼šäº§ç”Ÿä¸€ä¸ªç¬¦åˆæŒ‡å¯¼æç¤ºçš„ç†æƒ³è¾“å‡ºå›¾åƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå‘å›¾åƒä¸­æ·»åŠ å™ªéŸ³å¯èƒ½ä¼šåå¸¸åœ°å¢åŠ å®ƒä»¬åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¸ç»™å®šæ–‡æœ¬æç¤ºçš„å…³è”åº¦ï¼Œä»è€Œå¯¼è‡´æ„æƒ³ä¸åˆ°çš„åæœï¼Œä¾‹å¦‚æ›´å¥½çš„ç¼–è¾‘ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºåŸºäºæ‰°åŠ¨çš„æ–¹æ³•å¯èƒ½æ— æ³•ä¸ºæŠµæŠ—æ‰©æ•£å¼ç¼–è¾‘æä¾›è¶³å¤Ÿçš„å›¾åƒä¿æŠ¤è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04394v2">PDF</a> 6 pages, 8 figures, accepted by ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>å…ˆè¿›æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„å›¾åƒç”Ÿæˆèƒ½åŠ›å¼ºå¤§ï¼Œä½†ä¹Ÿå¯èƒ½è¢«è¯¯ç”¨ä¼ æ’­è™šå‡ä¿¡æ¯å’ŒæŠ„è¢­ç‰ˆæƒææ–™ã€‚å½“å‰å›¾åƒä¿æŠ¤æ–¹æ³•è¯•å›¾é€šè¿‡ç»™å›¾åƒæ·»åŠ ç»†å¾®æ‰°åŠ¨æ¥é˜»æ­¢åŸºäºæ‰©æ•£çš„ç¼–è¾‘ï¼Œä½†å®éªŒè¡¨æ˜è¿™ç§æ–¹æ³•å¹¶ä¸å®Œå…¨æœ‰æ•ˆã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå—ä¿æŠ¤çš„å›¾åƒè¿›è¡Œæ‰©æ•£ç¼–è¾‘åä»èƒ½ç”Ÿæˆç¬¦åˆæŒ‡å¯¼æç¤ºçš„ç†æƒ³å›¾åƒã€‚å› æ­¤ï¼ŒåŸºäºæ‰°åŠ¨çš„æ–¹æ³•å¯èƒ½æ— æ³•ä¸ºå¯¹æŠ—åŸºäºæ‰©æ•£çš„ç¼–è¾‘æä¾›è¶³å¤Ÿçš„å›¾åƒä¿æŠ¤è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›æ‰©æ•£æ¨¡å‹å…·å¤‡å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä½†å­˜åœ¨è¯¯ç”¨é£é™©ï¼Œå¦‚ä¼ æ’­è™šå‡ä¿¡æ¯å’Œä¾µçŠ¯ç‰ˆæƒã€‚</li>
<li>å½“å‰å›¾åƒä¿æŠ¤æ–¹æ³•ä¸»è¦é€šè¿‡æ·»åŠ ç»†å¾®æ‰°åŠ¨æ¥é˜»æ­¢åŸºäºæ‰©æ•£çš„ç¼–è¾‘ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å¹¶ä¸æ€»èƒ½æœ‰æ•ˆä¿æŠ¤å›¾åƒå…å—æ‰©æ•£ç¼–è¾‘çš„å½±å“ã€‚</li>
<li>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå—ä¿æŠ¤çš„å›¾åƒä»ç„¶èƒ½å¤Ÿæ ¹æ®æŒ‡å¯¼æç¤ºç”Ÿæˆç†æƒ³çš„æ‰©æ•£ç¼–è¾‘å›¾åƒã€‚</li>
<li>é€šè¿‡æ·»åŠ å™ªå£°ï¼Œå›¾åƒåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½ä¸ç»™å®šçš„æ–‡æœ¬æç¤ºäº§ç”Ÿæ›´ç´§å¯†çš„è”ç³»ï¼Œå¯¼è‡´æ›´å¥½çš„ç¼–è¾‘ç»“æœã€‚</li>
<li>è¿™ç§ç°è±¡å¯èƒ½å¸¦æ¥æ„æƒ³ä¸åˆ°çš„åæœï¼Œå¦‚æé«˜ç¼–è¾‘è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce0cb17fd3eb6c7ee6f1d9d1f2ece068.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7b422ab92ad88db4eba00bf75b270c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f4e05efa69775f77cddc438402eeb16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ffe1f5b22fb8e9f35925aa199d61c59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64a1de61d8fd632141d77cf64e9fafa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4334572ee6ba7b7ade5781da0c1947c6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="EAM-Enhancing-Anything-with-Diffusion-Transformers-for-Blind-Super-Resolution"><a href="#EAM-Enhancing-Anything-with-Diffusion-Transformers-for-Blind-Super-Resolution" class="headerlink" title="EAM: Enhancing Anything with Diffusion Transformers for Blind   Super-Resolution"></a>EAM: Enhancing Anything with Diffusion Transformers for Blind   Super-Resolution</h2><p><strong>Authors:Haizhen Xie, Kunpeng Du, Qiangyu Yan, Sen Lu, Jianhong Han, Hanting Chen, Hailin Hu, Jie Hu</strong></p>
<p>Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality. </p>
<blockquote>
<p>åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹æ¥æŒ‡å¯¼ç›²è¶…åˆ†è¾¨ç‡ï¼ˆBSRï¼‰å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»è¦æ–¹æ³•ã€‚è™½ç„¶T2Iæ¨¡å‹ä¼ ç»Ÿä¸Šä¾èµ–äºU-Netæ¶æ„ï¼Œä½†æœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰åœ¨è¿™ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æ›´é«˜çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¢å¼ºä»»ä½•æ¨¡å‹ï¼ˆEAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„BSRæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨DiTå¹¶è¶…è¶Šäº†ä¹‹å‰çš„U-Netæ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¨¡å—Î¨-DiTï¼Œå®ƒæœ‰æ•ˆåœ°å¼•å¯¼DiTè¿›è¡Œå›¾åƒæ¢å¤ã€‚è¯¥æ¨¡å—é‡‡ç”¨ä½åˆ†è¾¨ç‡æ½œåœ¨å€¼ä½œä¸ºå¯åˆ†ç¦»æµæ³¨å…¥æ§åˆ¶ï¼Œå½¢æˆæœ‰æ•ˆçš„ä¸‰æµæ¶æ„ï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒDiTä¸­çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨T2Iæ¨¡å‹çš„å…ˆéªŒæŒ‡å¯¼èƒ½åŠ›å¹¶å¢å¼ºå…¶åœ¨BSRä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¸è¿›å¼é®ç½©å›¾åƒå»ºæ¨¡ç­–ç•¥ï¼Œè¿™ä¹Ÿé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸»é¢˜æ„ŸçŸ¥çš„æç¤ºç”Ÿæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ä¸­é‡‡ç”¨å¥å£®çš„å¤šæ¨¡å¼æ¨¡å‹ã€‚è¯¥ç­–ç•¥è‡ªåŠ¨è¯†åˆ«å…³é”®å›¾åƒåŒºåŸŸï¼Œæä¾›è¯¦ç»†æè¿°ï¼Œå¹¶ä¼˜åŒ–T2Iæ‰©æ•£å…ˆéªŒçš„åˆ©ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒEAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05209v2">PDF</a> The company audit did not pass, there are some mistake in paper</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å¼•å¯¼ç›²è¶…åˆ†è¾¨ç‡ï¼ˆBSRï¼‰å·²æˆä¸ºè¯¥é¢†åŸŸçš„ä¸»æµæ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„BSRæ–¹æ³•â€”â€”å¢å¼ºä»»ä½•æ¨¡å‹ï¼ˆEAMï¼‰ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰å¹¶è¶…è¶Šäº†ä¼ ç»Ÿçš„U-Netæ–¹æ³•ã€‚EAMå¼•å…¥äº†ä¸€ä¸ªåä¸º$\Psi$-DiTçš„æ–°æ¨¡å—ï¼Œæœ‰æ•ˆåœ°å¼•å¯¼å›¾åƒæ¢å¤ã€‚ç»“åˆä½åˆ†è¾¨ç‡æ½œåœ¨å€¼ä½œä¸ºå¯åˆ†ç¦»æµæ³¨å…¥æ§åˆ¶ï¼Œå½¢æˆäº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‰é‡æµæ¶æ„ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ¸è¿›å¼é®æŒ¡å›¾åƒå»ºæ¨¡ç­–ç•¥ï¼Œå……åˆ†åˆ©ç”¨T2Iæ¨¡å‹çš„å…ˆéªŒæŒ‡å¯¼èƒ½åŠ›ï¼Œæé«˜äº†åœ¨BSRä¸­çš„é€šç”¨æ€§å¹¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ä¸»é¢˜æ„ŸçŸ¥æç¤ºç”Ÿæˆç­–ç•¥ï¼Œåœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ä¸­ä½¿ç”¨é²æ£’çš„å¤šæ¨¡å¼æ¨¡å‹ã€‚å®éªŒè¯æ˜ï¼ŒEAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³ç»“æœï¼Œåœ¨å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EAMåˆ©ç”¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰è¿›è¡Œç›²è¶…åˆ†è¾¨ç‡ï¼ˆBSRï¼‰ï¼Œæ€§èƒ½è¶…è¶Šä¼ ç»ŸU-Netæ–¹æ³•ã€‚</li>
<li>å¼•å…¥$\Psi$-DiTæ¨¡å—ï¼Œæœ‰æ•ˆå¼•å¯¼å›¾åƒæ¢å¤è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡ç»“åˆä½åˆ†è¾¨ç‡æ½œåœ¨å€¼ï¼Œå½¢æˆä¸‰é‡æµæ¶æ„ï¼Œæé«˜å›¾åƒæ¢å¤è´¨é‡ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›å¼é®æŒ¡å›¾åƒå»ºæ¨¡ç­–ç•¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒT2Iæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œæé«˜æ¨¡å‹åœ¨BSRä¸­çš„é€šç”¨æ€§å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>æå‡ºä¸»é¢˜æ„ŸçŸ¥æç¤ºç”Ÿæˆç­–ç•¥ï¼Œè‡ªåŠ¨è¯†åˆ«å…³é”®å›¾åƒåŒºåŸŸå¹¶æä¾›è¯¦ç»†æè¿°ï¼Œä¼˜åŒ–T2Iæ‰©æ•£å…ˆéªŒçš„åˆ©ç”¨ã€‚</li>
<li>EAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³ç»“æœï¼Œä¸ä»…åœ¨å®šé‡æŒ‡æ ‡ä¸Šï¼Œè€Œä¸”åœ¨è§†è§‰è´¨é‡ä¸Šä¹Ÿè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæœªæ¥å›¾åƒæ¢å¤å’Œå¢å¼ºæä¾›äº†æ–°æ€è·¯ï¼Œå¯èƒ½æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-de8b74f5037c54def645b6c23767c627.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-caee6a667b678014880e0e98e8c867ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff44f97272cfc8f63e21f2f00bca19bc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Dynamic-Negative-Guidance-of-Diffusion-Models"><a href="#Dynamic-Negative-Guidance-of-Diffusion-Models" class="headerlink" title="Dynamic Negative Guidance of Diffusion Models"></a>Dynamic Negative Guidance of Diffusion Models</h2><p><strong>Authors:Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni</strong></p>
<p>Negative Prompting (NP) is widely utilized in diffusion models, particularly in text-to-image applications, to prevent the generation of undesired features. In this paper, we show that conventional NP is limited by the assumption of a constant guidance scale, which may lead to highly suboptimal results, or even complete failure, due to the non-stationarity and state-dependence of the reverse process. Based on this analysis, we derive a principled technique called Dynamic Negative Guidance, which relies on a near-optimal time and state dependent modulation of the guidance without requiring additional training. Unlike NP, negative guidance requires estimating the posterior class probability during the denoising process, which is achieved with limited additional computational overhead by tracking the discrete Markov Chain during the generative process. We evaluate the performance of DNG class-removal on MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation of class balance and image quality when compared with baseline methods. Furthermore, we show that it is possible to use DNG with Stable Diffusion to obtain more accurate and less invasive guidance than NP. </p>
<blockquote>
<p>è´Ÿå‘æç¤ºï¼ˆNPï¼‰åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„åº”ç”¨ä¸­ï¼Œç”¨äºé˜²æ­¢ç”Ÿæˆä¸æƒ³è¦çš„ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜ä¼ ç»Ÿçš„NPå—åˆ°æ’å®šæŒ‡å¯¼å°ºåº¦çš„å‡è®¾çš„é™åˆ¶ï¼Œè¿™å¯èƒ½å¯¼è‡´ç»“æœé«˜åº¦ä¸ç†æƒ³ï¼Œç”šè‡³å®Œå…¨å¤±è´¥ï¼Œå› ä¸ºåå‘è¿‡ç¨‹å…·æœ‰éå¹³ç¨³æ€§å’ŒçŠ¶æ€ä¾èµ–æ€§ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ç§åŸºäºåŸåˆ™çš„æŠ€æœ¯ï¼Œç§°ä¸ºåŠ¨æ€è´Ÿå‘æŒ‡å¯¼ï¼ˆDNGï¼‰ï¼Œå®ƒä¾èµ–äºè¿‘ä¼˜çš„æ—¶é—´å’ŒçŠ¶æ€ä¾èµ–çš„æŒ‡å¯¼è°ƒåˆ¶ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ä¸NPä¸åŒï¼Œè´Ÿå‘æŒ‡å¯¼éœ€è¦åœ¨å»å™ªè¿‡ç¨‹ä¸­ä¼°è®¡åéªŒç±»æ¦‚ç‡ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è·Ÿè¸ªç¦»æ•£é©¬å°”å¯å¤«é“¾æ¥å®ç°ï¼Œåªéœ€å¢åŠ æœ‰é™çš„è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬åœ¨MNISTå’ŒCIFAR10ä¸Šè¯„ä¼°äº†DNGç±»å»é™¤çš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒDNGæé«˜äº†å®‰å…¨æ€§ï¼Œä¿æŒäº†ç±»å¹³è¡¡å’Œå›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¯ä»¥ä¸ç¨³å®šæ‰©æ•£ä¸€èµ·ä½¿ç”¨DNGï¼Œä»¥è·å¾—æ¯”NPæ›´å‡†ç¡®ã€ä¾µå…¥æ€§è¾ƒå°çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14398v3">PDF</a> Paper accepted at ICLR 2025 (poster). Our implementation is available   at <a target="_blank" rel="noopener" href="https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git">https://github.com/FelixKoulischer/Dynamic-Negative-Guidance.git</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿæç¤ºï¼ˆNPï¼‰çš„å±€é™æ€§ï¼Œå¹¶åŸºäºæ­¤æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”åŠ¨æ€è´ŸæŒ‡å¯¼ï¼ˆDNGï¼‰ã€‚DNGå…‹æœäº†NPåœ¨æ–‡æœ¬åˆ°å›¾åƒåº”ç”¨ä¸­çš„å‡è®¾é™åˆ¶ï¼Œå®ç°äº†å¯¹æŒ‡å¯¼çš„è¿‘æœ€ä¼˜æ—¶é—´å’ŒçŠ¶æ€ä¾èµ–è°ƒåˆ¶ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚åœ¨MNISTå’ŒCIFAR10ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDNGåœ¨å®‰å…¨æ€§ã€ç±»å¹³è¡¡å’Œå›¾åƒè´¨é‡æ–¹é¢ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä¸NPç›¸æ¯”ï¼ŒDNGè¿˜å¯ä¸Stable Diffusionç»“åˆä½¿ç”¨ï¼Œè·å¾—æ›´å‡†ç¡®ã€ä¾µå…¥æ€§è¾ƒä½çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è´Ÿæç¤ºï¼ˆNPï¼‰åœ¨æ‰©æ•£æ¨¡å‹ä¸­å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„åº”ç”¨ä¸­ç”¨äºé˜²æ­¢ç”Ÿæˆä¸éœ€è¦çš„ç‰¹å¾ã€‚</li>
<li>å¸¸è§„NPå­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå‡è®¾å­˜åœ¨ä¸€ä¸ªæ’å®šçš„æŒ‡å¯¼è§„æ¨¡ï¼Œå¯èƒ½å¯¼è‡´ç»“æœé«˜åº¦ä¸ç†æƒ³æˆ–å®Œå…¨å¤±è´¥ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”åŠ¨æ€è´ŸæŒ‡å¯¼ï¼ˆDNGï¼‰ï¼Œå…‹æœäº†NPçš„å±€é™æ€§ï¼Œå®ç°äº†å¯¹æŒ‡å¯¼çš„è¿‘æœ€ä¼˜æ—¶é—´å’ŒçŠ¶æ€ä¾èµ–è°ƒåˆ¶ã€‚</li>
<li>DNGåœ¨ä¼°è®¡åéªŒç±»æ¦‚ç‡æ–¹é¢æœ‰æ‰€åˆ›æ–°ï¼Œè¿™åœ¨å»å™ªè¿‡ç¨‹ä¸­æ˜¯å¿…è¦çš„ã€‚</li>
<li>DNGåœ¨MNISTå’ŒCIFAR10ä¸Šçš„è¯„ä¼°è¡¨æ˜å…¶åœ¨å®‰å…¨æ€§ã€ç±»å¹³è¡¡å’Œå›¾åƒè´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>DNGå¯ç”¨äºä¸Stable Diffusionç»“åˆï¼Œè·å¾—æ›´å‡†ç¡®ã€ä¾µå…¥æ€§è¾ƒä½çš„æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14398">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9b5cc08d96e458e073b5958e8d8589d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1324eacfecf0ba9dbb15a65e90b6de93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e397234689a161bb9a76bfd2b6c60eee.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multimodal-Pragmatic-Jailbreak-on-Text-to-image-Models"><a href="#Multimodal-Pragmatic-Jailbreak-on-Text-to-image-Models" class="headerlink" title="Multimodal Pragmatic Jailbreak on Text-to-image Models"></a>Multimodal Pragmatic Jailbreak on Text-to-image Models</h2><p><strong>Authors:Tong Liu, Zhixin Lai, Jiawen Wang, Gengyuan Zhang, Shuo Chen, Philip Torr, Vera Demberg, Volker Tresp, Jindong Gu</strong></p>
<p>Diffusion models have recently achieved remarkable advancements in terms of image quality and fidelity to textual prompts. Concurrently, the safety of such generative models has become an area of growing concern. This work introduces a novel type of jailbreak, which triggers T2I models to generate the image with visual text, where the image and the text, although considered to be safe in isolation, combine to form unsafe content. To systematically explore this phenomenon, we propose a dataset to evaluate the current diffusion-based text-to-image (T2I) models under such jailbreak. We benchmark nine representative T2I models, including two closed-source commercial models. Experimental results reveal a concerning tendency to produce unsafe content: all tested models suffer from such type of jailbreak, with rates of unsafe generation ranging from around 10% to 70% where DALLE 3 demonstrates almost the highest unsafety. In real-world scenarios, various filters such as keyword blocklists, customized prompt filters, and NSFW image filters, are commonly employed to mitigate these risks. We evaluate the effectiveness of such filters against our jailbreak and found that, while these filters may be effective for single modality detection, they fail to work against our jailbreak. We also investigate the underlying reason for such jailbreaks, from the perspective of text rendering capability and training data. Our work provides a foundation for further development towards more secure and reliable T2I models. Project page at <a target="_blank" rel="noopener" href="https://multimodalpragmatic.github.io/">https://multimodalpragmatic.github.io/</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬æç¤ºçš„ä¿çœŸåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚åŒæ—¶ï¼Œæ­¤ç±»ç”Ÿæˆæ¨¡å‹çš„å®‰å…¨æ€§ä¹Ÿè¶Šæ¥è¶Šå¼•èµ·äººä»¬çš„å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹è¶Šç‹±æ–¹å¼ï¼Œå®ƒè§¦å‘T2Iæ¨¡å‹ç”Ÿæˆå…·æœ‰è§†è§‰æ–‡æœ¬çš„å›¾åƒï¼Œå°½ç®¡å›¾åƒå’Œæ–‡æœ¬æœ¬èº«è¢«è®¤ä¸ºæ˜¯å®‰å…¨çš„ï¼Œä½†å®ƒä»¬ç»„åˆèµ·æ¥ä¼šå½¢æˆä¸å®‰å…¨çš„å†…å®¹ã€‚ä¸ºäº†ç³»ç»Ÿåœ°æ¢ç´¢è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†æ¥è¯„ä¼°åœ¨è¿™ç§è¶Šç‹±æƒ…å†µä¸‹å½“å‰çš„åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯¹ä¹ä¸ªä»£è¡¨æ€§çš„T2Iæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé—­æºçš„å•†ä¸šæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜å­˜åœ¨ä»¤äººæ‹…å¿§çš„äº§ç”Ÿä¸å®‰å…¨å†…å®¹çš„å€¾å‘ï¼šæ‰€æœ‰æµ‹è¯•æ¨¡å‹éƒ½å—åˆ°è¿™ç§æ–°å‹è¶Šç‹±çš„å½±å“ï¼Œä¸å®‰å…¨ç”Ÿæˆç‡ä»çº¦10%åˆ°70%ä¸ç­‰ï¼Œå…¶ä¸­DALLE 3è¡¨ç°å‡ºçš„ä¸å®‰å…¨æ€§å‡ ä¹æœ€é«˜ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œé€šå¸¸ä½¿ç”¨å„ç§è¿‡æ»¤å™¨ï¼ˆå¦‚å…³é”®è¯é»‘åå•ã€è‡ªå®šä¹‰æç¤ºè¿‡æ»¤å™¨å’ŒNSFWå›¾åƒè¿‡æ»¤å™¨ï¼‰æ¥å‡è½»è¿™äº›é£é™©ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¿™äº›è¿‡æ»¤å™¨å¯¹æŠ—æˆ‘ä»¬è¶Šç‹±çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°è¿™äº›è¿‡æ»¤å™¨å¯¹äºå•ä¸€æ¨¡æ€æ£€æµ‹å¯èƒ½æœ‰æ•ˆï¼Œä½†æ— æ³•å¯¹æŠ—æˆ‘ä»¬çš„è¶Šç‹±ã€‚æˆ‘ä»¬è¿˜ä»æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›å’Œè®­ç»ƒæ•°æ®è§’åº¦è°ƒæŸ¥äº†è¿™ç§è¶Šç‹±çš„æ ¹æœ¬åŸå› ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¿›ä¸€æ­¥å¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„T2Iæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ä¸º<a target="_blank" rel="noopener" href="https://multimodalpragmatic.github.io/%E3%80%82">https://multimodalpragmatic.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19149v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬æç¤ºçš„ä¿çœŸåº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ç”Ÿæˆæ¨¡å‹çš„å®‰å…¨æ€§ä¹Ÿæˆä¸ºäº†ä¸€ä¸ªæ—¥ç›Šå…³æ³¨çš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹è¶Šç‹±ç°è±¡ï¼Œå³æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶ä¼šå°†è§†è§‰æ–‡æœ¬ç»“åˆï¼Œè™½ç„¶å•ç‹¬çš„å›¾åƒå’Œæ–‡æœ¬è¢«è®¤ä¸ºæ˜¯å®‰å…¨çš„ï¼Œä½†ç»“åˆèµ·æ¥ä¼šå½¢æˆä¸å®‰å…¨çš„å†…å®¹ã€‚ä¸ºäº†ç³»ç»Ÿåœ°æ¢ç´¢è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†æ¥è¯„ä¼°å½“å‰æ‰©æ•£åŸºç¡€ä¸Šçš„T2Iæ¨¡å‹åœ¨è¿™ç§è¶Šç‹±ç°è±¡ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰æµ‹è¯•æ¨¡å‹éƒ½å­˜åœ¨è¿™ç§è¶Šç‹±ç°è±¡ï¼Œä¸å®‰å…¨å†…å®¹çš„ç”Ÿæˆç‡ä»çº¦10%åˆ°70%ä¸ç­‰ï¼Œå…¶ä¸­DALLE 3çš„å®‰å…¨é£é™©æœ€é«˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¸¸è§çš„è¿‡æ»¤å™¨ï¼ˆå¦‚å…³é”®è¯é»‘åå•ã€è‡ªå®šä¹‰æç¤ºè¿‡æ»¤å™¨å’ŒNSFWå›¾åƒè¿‡æ»¤å™¨ï¼‰å¯¹è¿™ç§è¶Šç‹±çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°è¿™äº›è¿‡æ»¤å™¨å¯¹äºå•ä¸€æ¨¡æ€æ£€æµ‹å¯èƒ½æœ‰æ•ˆï¼Œä½†æ— æ³•å¯¹æŠ—æˆ‘ä»¬çš„è¶Šç‹±ç°è±¡ã€‚æˆ‘ä»¬è¿˜ä»æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›å’Œè®­ç»ƒæ•°æ®çš„è§’åº¦æ¢è®¨äº†è¿™ç§è¶Šç‹±ç°è±¡çš„æ½œåœ¨åŸå› ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºè¿›ä¸€æ­¥å¼€å‘æ›´å®‰å…¨ã€æ›´å¯é çš„T2Iæ¨¡å‹æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å®‰å…¨æ€§æˆä¸ºå…³æ³¨ç„¦ç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„â€œè¶Šç‹±â€ç°è±¡ï¼Œå³T2Iæ¨¡å‹åœ¨ç»“åˆè§†è§‰æ–‡æœ¬æ—¶ç”Ÿæˆä¸å®‰å…¨å†…å®¹ã€‚</li>
<li>ç³»ç»Ÿåœ°æ¢ç´¢äº†è¿™ä¸€ç°è±¡ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ•°æ®é›†æ¥è¯„ä¼°æ‰©æ•£åŸºç¡€ä¸Šçš„T2Iæ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæ‰€æœ‰æµ‹è¯•æ¨¡å‹éƒ½å­˜åœ¨å®‰å…¨é£é™©ï¼Œå…¶ä¸­DALLE 3å®‰å…¨é£é™©æœ€é«˜ã€‚</li>
<li>å¸¸è§è¿‡æ»¤å™¨ï¼ˆå¦‚å…³é”®è¯é»‘åå•ç­‰ï¼‰æ— æ³•æœ‰æ•ˆå¯¹æŠ—æ–°å‹â€œè¶Šç‹±â€ç°è±¡ã€‚</li>
<li>ä»æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›å’Œè®­ç»ƒæ•°æ®è§’åº¦æ¢è®¨äº†è¿™ç§è¶Šç‹±ç°è±¡çš„æ½œåœ¨åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cba09d71a729b60609591f62c0cdb35c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ba9706baeacf9adb5bb7f4d2fbc1c9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd97c5be418fba58d02f4763fa8c4acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e3d946f20ac8c1da58066b4bd5fd9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75001f3e48656dcbd197bf8481c3a86a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9e7c9e275e906033e04e4c5f96cc32b4.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Single Cu Atom Sites on Co3O4 Activate Interfacial Oxygen for Enhanced   Reactivity and Selective Gas Sensing at Low Temperature
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f1895cc5423e9285b465a0937fc760ac.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  The Less You Depend, The More You Learn Synthesizing Novel Views from   Sparse, Unposed Images without Any 3D Knowledge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
