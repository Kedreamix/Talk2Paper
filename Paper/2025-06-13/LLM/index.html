<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-13  Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-89f8b96b14d87014d76a7f04f96333ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    64 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-13-更新"><a href="#2025-06-13-更新" class="headerlink" title="2025-06-13 更新"></a>2025-06-13 更新</h1><h2 id="Large-Language-Models-for-Toxic-Language-Detection-in-Low-Resource-Balkan-Languages"><a href="#Large-Language-Models-for-Toxic-Language-Detection-in-Low-Resource-Balkan-Languages" class="headerlink" title="Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages"></a>Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages</h2><p><strong>Authors:Amel Muminovic, Amela Kadric Muminovic</strong></p>
<p>Online toxic language causes real harm, especially in regions with limited moderation tools. In this study, we evaluate how large language models handle toxic comments in Serbian, Croatian, and Bosnian, languages with limited labeled data. We built and manually labeled a dataset of 4,500 YouTube and TikTok comments drawn from videos across diverse categories, including music, politics, sports, modeling, influencer content, discussions of sexism, and general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) were tested in two modes: zero-shot and context-augmented. We measured precision, recall, F1 score, accuracy and false positive rates. Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini in context-augmented mode, reaching an F1 score of 0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the lowest false alarms. We show how adding minimal context can improve toxic language detection in low-resource settings and suggest practical strategies such as improved prompt design and threshold calibration. These results show that prompt design alone can yield meaningful gains in toxicity detection for underserved Balkan language communities. </p>
<blockquote>
<p>网络有毒语言会导致真实伤害，特别是在缺乏管理工具的地区。在这项研究中，我们评估大型语言模型如何处理塞尔维亚语、克罗地亚语和波斯尼亚语的有毒评论，这些语言的标记数据有限。我们构建并手动标记了一个包含4500条YouTube和TikTok评论的数据集，这些评论来自音乐、政治、体育、模特、网红内容、性别主义讨论和一般话题等各类视频。我们测试了四种模型（GPT-3.5 Turbo、GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus）在零射模式和增强上下文模式下的表现。我们测量了精确度、召回率、F1分数、准确率和误报率。包含简短上下文片段平均提高了约0.12的召回率，并使F1分数提高了高达0.10，但有时也会增加误报。最佳的平衡来自于增强上下文模式下的Gemini，其F1分数和准确率均达到0.82，而零射模式下的GPT-4.1在精确度上领先，并且误报率最低。我们展示了如何在资源有限的环境中通过添加最少的上下文来改善有毒语言检测，并提出了一些实用策略，如改进提示设计和阈值校准。这些结果表明，仅凭提示设计就可以在检测对巴尔干地区服务不足的语言社区的有毒性方面取得有意义的效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09992v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong>：<br>本研究评估了大型语言模型在处理塞尔维亚语、克罗地亚语和波斯尼亚语的毒性评论时的表现。研究者手动标注了一个包含4500条YouTube和TikTok评论的数据集，测试了四种模型在零样本和上下文增强模式下的性能。结果显示，添加上下文片段可以提高召回率和F1分数，但也会增加误报。Gemini在上下文增强模式下表现最佳，F1分数和准确率均达到0.82。研究结果表明，添加最小量的上下文可以改善低资源环境下的有毒语言检测，提示设计良好的提示和阈值校准等实用策略可以带来有意义的有毒性检测增益，对巴尔干语族的社区尤为关键。</p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>大型语言模型在处理低资源环境的毒性语言时的挑战。</li>
<li>手动标注的数据集用于评估模型性能。</li>
<li>上下文片段的添加可以提高模型的召回率和F1分数。</li>
<li>Gemini在上下文增强模式下表现最佳。</li>
<li>添加最小量的上下文能显著提升有毒语言检测的准确性。</li>
<li>良好的提示设计和阈值校准能提高毒性检测的增益。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09992">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ea1ff86bf8685ace52a37bdedbc7608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb526417475bb3dba62bb98d2113745.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf86acfdcc0adba6440e11c0acbbe36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9add948c41b2dac381b5325d8654ca8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcd885e82484fe16af26261cf6c18878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d629d6fba6fe3a27dabe4b449f84202b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning"><a href="#V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning" class="headerlink" title="V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning"></a>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning</h2><p><strong>Authors:Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes,  Mojtaba,  Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas</strong></p>
<p>A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world. </p>
<blockquote>
<p>现代人工智能面临的一个主要挑战是学会通过大量的观察理解世界并采取行动。本文探索了一种自监督方法，它结合了互联网规模的视频数据和小量的交互数据（机器人轨迹），以开发能够在物理世界中理解、预测和规划模型。我们首先在一个包含超过一百万小时互联网视频的视频和图像数据集上，对无动作联合嵌入预测架构V-JEPA 2进行预训练。V-JEPA 2在运动理解方面表现出强大的性能（在Something-Something v2上达到77.3%的top-1准确率），并在人类行为预测方面达到了最先进的性能（在Epic-Kitchens-100上的召回率为39.7%）。此外，在与大型语言模型对齐后，我们在参数规模为8亿的多个视频问答任务上表现出了最先进的性能（例如，PerceptionTest上为84.0%，TempCompass上为76.9%）。最后，我们展示了如何通过事后训练潜在的动作条件世界模型V-JEPA 2-AC，将自监督学习应用于机器人规划任务。我们使用来自Droid数据集的不到62小时的无标签机器人视频对V-JEPA 2-AC进行训练，然后将其零射击部署在Franka机械臂上，在两个不同的实验室环境中实现了基于图像目标的物体抓取和放置。值得注意的是，这无需从环境中的机器人收集任何数据，也无需任何特定任务的训练或奖励。这项工作展示了如何从互联网规模的数据和少量的机器人交互数据中通过自监督学习获得能够在物理世界中规划的世界模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09985v1">PDF</a> 48 pages, 19 figures</p>
<p><strong>摘要</strong><br>利用互联网规模视频数据和小量交互数据（机器人轨迹），通过自监督学习方法，发展出能够理解和预测物理世界的模型。预训练的V-JEPA 2模型在动作理解和人类行动预期方面表现出强大性能，与大型语言模型对齐后，在视频问答任务上表现出最新技术水平。此外，通过在后训练中使用机器人视频数据，我们展示了自监督学习在机器人规划任务中的应用。这项工作展示了如何从互联网规模数据和少量机器人交互数据中学习世界模型，并能够在物理世界中规划行动。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>现代AI面临的主要挑战是通过观察学习理解世界和采取行动。</li>
<li>研究采用自监督方法，结合互联网规模视频数据与少量交互数据（机器人轨迹）。</li>
<li>V-JEPA 2模型在动作理解和人类行动预期方面表现出强大的性能。</li>
<li>与大型语言模型对齐后，V-JEPA 2在视频问答任务上达到最新技术水平。</li>
<li>通过使用机器人视频数据的后训练，展示了自监督学习在机器人规划任务中的应用。</li>
<li>V-JEPA 2模型能够在不同的物理环境中实现零样本物体抓取和放置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c20e7fd378bb75f6277bc2b568fab79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52726ea443acb63e798931278b8709a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c12be16346c246b398c8c3094751fe8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb56100036aa6a8241569ba35d98df63.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TOGA-Temporally-Grounded-Open-Ended-Video-QA-with-Weak-Supervision"><a href="#TOGA-Temporally-Grounded-Open-Ended-Video-QA-with-Weak-Supervision" class="headerlink" title="TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision"></a>TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</h2><p><strong>Authors:Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha</strong></p>
<p>We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks. </p>
<blockquote>
<p>我们解决了在弱监督设置下带有时间定位的视频问答（Video QA）问题，而无需任何时间注释。给定一个视频和一个问题，我们生成一个基于开始和结束时间的开放式答案。为此任务，我们提出了TOGA：一个用于带有时间定位的开放式视频问答的视听语言模型，该模型在弱监督下进行训练。我们指导并调整TOGA以联合生成答案和时间定位。我们在没有时间定位注释的弱监督设置中进行操作。我们为时间定位生成伪标签，并通过在定位响应的问题和针对同一时间段的响应之间施加一致性约束来确保这些标签的有效性。我们发现联合生成答案和定位可以提高问答和定位的性能。我们在定位问答和开放式问答任务上评估了TOGA。对于定位问答，我们考虑了NExT-GQA基准测试，该测试旨在评估弱监督下的定位问答。对于开放式问答，我们考虑了MSVD-QA和活动网QA基准测试。我们在这些基准测试上的这两项任务中都达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09445v1">PDF</a> </p>
<p><strong>Summary</strong><br>视频问答中的时序定位问题可在无时序标注的弱监督设置下解决。给定视频和问题，我们生成基于起始和结束时间的开放答案。为此任务，我们提出TOGA模型，该模型在弱监督下联合生成答案和时序定位。通过生成时序定位伪标签并确保标签与问题的一致性约束来提高标签的有效性。联合生成答案和定位提高了问答和定位性能。我们在基准测试上评估了TOGA的性能，包括有依据的QA和开放式的QA任务，取得了显著的成绩。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究解决了视频问答中的时序定位问题，在无时序标注的弱监督环境下进行。</li>
<li>提出TOGA模型，用于在弱监督环境下联合生成答案和时序定位。</li>
<li>生成时序定位的伪标签并通过一致性约束来保证标签的有效性。</li>
<li>联合生成答案和定位提高了问答和定位的性能。</li>
<li>在基准测试上评估了TOGA模型在有依据的QA和开放式的QA任务上的性能。</li>
<li>在NExT-GQA、MSVD-QA和ActivityNet-QA等基准测试上取得了显著的成绩。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3ef4e6c100bf63e90f45b8fdfb57ce26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88131fa4ee2ea1703a178a16362567fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45e5036f0c2df612c525c2bb553dfdc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GigaChat-Family-Efficient-Russian-Language-Modeling-Through-Mixture-of-Experts-Architecture"><a href="#GigaChat-Family-Efficient-Russian-Language-Modeling-Through-Mixture-of-Experts-Architecture" class="headerlink" title="GigaChat Family: Efficient Russian Language Modeling Through Mixture of   Experts Architecture"></a>GigaChat Family: Efficient Russian Language Modeling Through Mixture of   Experts Architecture</h2><p><strong>Authors: GigaChat team, Mamedov Valentin, Evgenii Kosarev, Gregory Leleytner, Ilya Shchuckin, Valeriy Berezovskiy, Daniil Smirnov, Dmitry Kozlov, Sergei Averkiev, Lukyanenko Ivan, Aleksandr Proshunin, Ainur Israfilova, Ivan Baskov, Artem Chervyakov, Emil Shakirov, Mikhail Kolesov, Daria Khomich, Darya Latortseva, Sergei Porkhun, Yury Fedorov, Oleg Kutuzov, Polina Kudriavtseva, Sofiia Soldatova, Kolodin Egor, Stanislav Pyatkin, Dzmitry Menshykh, Grafov Sergei, Eldar Damirov, Karlov Vladimir, Ruslan Gaitukiev, Arkadiy Shatenov, Alena Fenogenova, Nikita Savushkin, Fedor Minkin</strong></p>
<p>Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (<a target="_blank" rel="noopener" href="https://huggingface.co/ai-sage">https://huggingface.co/ai-sage</a>), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language. </p>
<blockquote>
<p>生成式大型语言模型（LLM）已成为现代多语言NLP研究与应用的核心。然而，针对俄语特别定制的基础模型开发受到限制，主要是由于所需的大量计算资源。本文介绍了GigaChat系列俄语LLM，该系列模型有基本模型和指令调整版本，提供多种大小可选。我们详细报告了模型架构、预训练过程和实验，以指导设计选择。此外，我们评估了它们在俄语和英语基准测试上的表现，并将GigaChat与多语种类似模型进行了比较。本文展示了通过API、Telegram机器人和Web界面访问的最佳模型的系统演示。此外，我们在开源平台（<a target="_blank" rel="noopener" href="https://huggingface.co/ai-sage%EF%BC%89%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%B8%89%E4%B8%AA%E5%BC%80%E6%BA%90GigaChat%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%97%A8%E5%9C%A8%E6%89%A9%E5%A4%A7NLP%E7%A0%94%E7%A9%B6%E6%9C%BA%E4%BC%9A%E5%B9%B6%E6%94%AF%E6%8C%81%E4%BF%84%E8%AF%AD%E5%B7%A5%E4%B8%9A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%BC%80%E5%8F%91%E3%80%82">https://huggingface.co/ai-sage）上发布了三个开源GigaChat模型，旨在扩大NLP研究机会并支持俄语工业解决方案的开发。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09440v1">PDF</a> ACL-2025 System Demo</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对俄语开发的GigaChat系列大型生成式语言模型（LLM）。文章详细描述了模型架构、预训练过程和实验设计选择，并评估了其在俄语和英语基准测试上的性能表现。此外，还展示了GigaChat模型的高级表现并通过API、Telegram bot和Web界面进行演示。本文还公布了三个开源的GigaChat模型，旨在扩展NLP研究机会并支持俄语工业解决方案的开发。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GigaChat系列是专门为俄语设计的大型生成式语言模型（LLM）。</li>
<li>文章详细阐述了GigaChat的模型架构、预训练过程以及实验设计选择。</li>
<li>GigaChat模型在俄语和英语基准测试上表现出良好性能。</li>
<li>GigaChat模型可以通过API、Telegram bot和Web界面进行访问和使用。</li>
<li>公开了三个开源的GigaChat模型，旨在支持NLP研究及俄语工业解决方案开发。</li>
<li>GigaChat系列模型包括基础模型和指令微调版本，可满足不同的应用需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09440">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dad26832f6837f5775e6aabcc039148f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25cb61d7b2a2c4c1301a14eca2706c37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77baf64d73293ffa4df98181822bb78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f356203e5be9378287d050034162c1c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2c56501586edf65a3ba6266e2b9652e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f9c912b05f45496013abaac6cad1af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7555c8e17b7506b46f4d2f86bda9f9e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-and-Effective-Alignment-of-Large-Language-Models"><a href="#Towards-Efficient-and-Effective-Alignment-of-Large-Language-Models" class="headerlink" title="Towards Efficient and Effective Alignment of Large Language Models"></a>Towards Efficient and Effective Alignment of Large Language Models</h2><p><strong>Authors:Yuxin Jiang</strong></p>
<p>Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMs’ ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current models’ constraint adherence, offering insights for future improvements. </p>
<blockquote>
<p>大型语言模型（LLM）在多种任务中展现出显著的能力，然而，如何有效且高效地将它们与人类期望对齐仍然是一个关键挑战。本论文通过引入数据收集、训练和评估的新方法，推动了LLM的对齐性。我们首先解决对齐数据的收集问题。现有方法严重依赖于人工编制的数据集或专有模型。为了克服这些局限性，我们提出了Lion，一个对抗蒸馏框架，通过识别和生成具有挑战性的指令来迭代地优化训练数据，从而实现最先进的零样本推理。此外，我们还介绍了WebR（网络重建），一个完全自动化的框架，直接从原始网络文档合成指令调整数据，显著提高了数据的多样性和可扩展性，超越了现有的合成数据方法。接下来，我们通过新型优化技术增强对齐训练。我们开发了Learning to Edit（LTE），一个框架，使LLM能够高效地整合新知识，同时保留现有信息。LTE利用元学习改进实时和批量知识更新。此外，我们对直接偏好优化（DPO）进行了改进，引入了Bridging and Modeling Correlations（BMC），它能够显式捕获偏好数据中的令牌级相关性，从而在问答和数学推理任务中实现对齐的优越性。最后，我们解决了对齐评估的挑战。现有基准测试侧重于响应质量，但忽视了特定约束的遵守情况。为了填补这一空白，我们推出了FollowBench，这是一个多层次、精细的基准测试，评估LLM遵循多种指令类型的复杂约束的能力。我们的结果揭示了当前模型在约束遵守方面的关键弱点，为未来的改进提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09329v1">PDF</a> PhD thesis</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在各项任务中展现出卓越的能力，但如何有效地与人类期望对齐仍是关键挑战。本论文通过引入数据收集、训练和评估的新方法，推进了LLM的对齐研究。通过解决数据收集问题，提出对抗蒸馏框架Lion，能够迭代优化训练数据，实现最先进的零样本推理能力。同时，介绍全自动合成指令调整数据的WebR框架，提高了数据的多样性和可扩展性。在训练方面，通过新型优化技术提高对齐效果，开发LTE框架使LLM能够高效集成新知识同时保留现有信息。此外，改进DPO方法，提出BMC，更好地捕捉偏好数据的token级关联，在问答和数学推理任务中实现对齐的优越性。最后，针对评估问题，现有基准测试主要关注响应质量，忽视了特定约束的遵守。为此，引入多级别精细基准测试FollowBench，评估LLM遵循复杂指令的能力，揭示当前模型的弱点，为改进提供方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入对抗蒸馏框架Lion解决LLM对齐中的数据收集问题，提高零样本推理能力。</li>
<li>提出WebR框架，从原始网页文档中直接合成指令调整数据，增强数据的多样性和可扩展性。</li>
<li>通过新型优化技术增强LLM的对齐训练效果，如LTE框架集成新知识和保留现有信息。</li>
<li>改进DPO方法，提出BMC以更好地捕捉偏好数据的token级关联，提升QA和数学推理任务的对齐效果。</li>
<li>强调现有LLM评估基准测试的不足，需要同时关注响应质量和特定约束的遵守。</li>
<li>引入多级别精细基准测试FollowBench来评估LLM遵循复杂指令的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b65663b1692ba3809ee620897d5329c7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention"><a href="#On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention" class="headerlink" title="On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention"></a>On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention</h2><p><strong>Authors:Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella</strong></p>
<p>Large language models (LLMs) excel at capturing global token dependencies via self-attention but face prohibitive compute and memory costs on lengthy inputs. While sub-quadratic methods (e.g., linear attention) can reduce these costs, they often degrade accuracy due to overemphasizing recent tokens. In this work, we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel design that maintains two specialized hidden states-one for preserving historical context and one for tracking recency-thereby mitigating the short-range bias typical of linear-attention architectures. To further balance efficiency and accuracy under dynamic workload conditions, we introduce \textbf{\serve}, an online \textit{adaptive distillation} framework that progressively replaces Transformer layers with DSLA layers at inference time, guided by a sensitivity-based layer ordering. \serve\ uses a chained fine-tuning strategy to ensure that each newly converted DSLA layer remains consistent with previously replaced layers, preserving the overall quality. Extensive evaluations on commonsense reasoning, long-context QA, and text summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while retaining comparable performance across downstream tasks. Our ablation studies show that DSLA’s dual states capture both global and local dependencies, addressing the historical-token underrepresentation seen in prior linear attentions. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve">https://github.com/utnslab/DSLA-Serve</a>. </p>
<blockquote>
<p>大型语言模型（LLM）擅长通过自注意力捕捉全局令牌依赖关系，但在处理长输入时面临着计算量大和内存成本高的挑战。虽然次二次方法（如线性注意力）可以降低这些成本，但它们往往会因为过度强调最近的令牌而降低准确性。在这项工作中，我们首先提出了\textit{双态线性注意力}(\textbf{DSLA})，这是一种新型设计，它维护了两个专门的隐藏状态——一个用于保留历史上下文，另一个用于跟踪近期信息——从而减轻了线性注意力架构通常存在的短程偏差。为了进一步提高动态工作负载条件下的效率和准确性之间的平衡，我们引入了\textbf{SERVE}——一种在线自适应蒸馏框架。在推理阶段，它会根据基于敏感性的层排序逐步用DSLA层替换Transformer层。SERVE使用级联微调策略确保新转换的DSLA层与先前替换的层保持一致，保持整体质量。在常识推理、长上下文问答和文本摘要方面的广泛评估表明，与Llama2-7B相比，SERVE提高了\textbf{2.3倍}的推理速度；与混合模型Zamba-7B相比，提高了\textbf{3.0倍}，同时在下游任务上保持了相当的性能。我们的消融研究表明，DSLA的双状态能够捕捉全局和局部依赖关系，解决了先前线性注意力中出现的历史令牌欠表征问题。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/utnslab/DSLA-Serve找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09316v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了双状态线性注意力（DSLA）机制，旨在解决大型语言模型在处理长输入时面临的计算与内存成本问题。通过维护两个专门化的隐藏状态来平衡历史上下文与近期信息的追踪，从而减轻线性注意力架构的短程偏见。此外，还介绍了在线自适应蒸馏框架\serve，根据敏感层排序在推理过程中逐步替换Transformer层，旨在平衡效率和准确性。实验表明，\serve在保持性能的同时，推理速度比现有模型更快。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>双状态线性注意力（DSLA）机制通过维护两个隐藏状态，分别用于保留历史上下文和追踪近期信息，解决了大型语言模型处理长输入时的计算与内存成本问题。</li>
<li>\serve是一个在线自适应蒸馏框架，可以根据动态工作量条件逐步替换Transformer层，以提高效率并维持准确性。</li>
<li>\serve通过敏感度导向的层序进行推理，逐步替换Transformer层，并采用链式微调策略确保新替换的DSLA层与之前替换的层保持一致。</li>
<li>实验表明，\serve在常识推理、长文本问答和文本摘要等多个任务上保持了与现有模型相当的性能，同时推理速度更快。</li>
<li>DSLA机制能够捕捉全局和局部依赖关系，解决了先前线性注意力模型中历史令牌表示不足的问题。</li>
<li>提供的代码可在<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/utnslab/DSLA-Serve上获取。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09316">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4f1de63693593cdef26d41f2fa43aaed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60c67094e0bcd3f621e31431b29f3ded.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90de5cd9ba73d7e8bc828d401a22efc8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unifying-Block-wise-PTQ-and-Distillation-based-QAT-for-Progressive-Quantization-toward-2-bit-Instruction-Tuned-LLMs"><a href="#Unifying-Block-wise-PTQ-and-Distillation-based-QAT-for-Progressive-Quantization-toward-2-bit-Instruction-Tuned-LLMs" class="headerlink" title="Unifying Block-wise PTQ and Distillation-based QAT for Progressive   Quantization toward 2-bit Instruction-Tuned LLMs"></a>Unifying Block-wise PTQ and Distillation-based QAT for Progressive   Quantization toward 2-bit Instruction-Tuned LLMs</h2><p><strong>Authors:Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, An Chen</strong></p>
<p>As the rapid scaling of large language models (LLMs) poses significant challenges for deployment on resource-constrained devices, there is growing interest in extremely low-bit quantization, such as 2-bit. Although prior works have shown that 2-bit large models are pareto-optimal over their 4-bit smaller counterparts in both accuracy and latency, these advancements have been limited to pre-trained LLMs and have not yet been extended to instruction-tuned models. To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2) that unifies block-wise post-training quantization (PTQ) with distillation-based quantization-aware training (Distill-QAT) for INT2 instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned models to INT4 using block-wise PTQ to significantly reduce the quantization error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT to enable INT2 instruction-tuned LLMs to generate responses consistent with their original FP16 counterparts by minimizing the generalized Jensen-Shannon divergence (JSD) between the two. To the best of our knowledge, we are the first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs to INT2 without relying on proprietary post-training data, while achieving state-of-the-art performances on MMLU and IFEval$-$two of the most representative benchmarks for evaluating instruction-tuned LLMs. </p>
<blockquote>
<p>随着大型语言模型（LLM）的快速扩展对资源受限设备上的部署带来重大挑战，人们对极低比特量化（如2比特）的兴趣日益增长。尽管之前的研究已经表明，在准确度和延迟方面，2比特大型模型比其4比特小型模型具有帕累托最优性能，但这些进展仅限于预训练LLM，尚未扩展到指令调优模型。为了弥补这一差距，我们提出了统一渐进量化（UPQ）——一种新颖的渐进量化框架（FP16→INT4→INT2），它将块级后训练量化（PTQ）与基于蒸馏的量化感知训练（Distill-QAT）相结合，用于INT2指令调优LLM量化。UPQ首先使用块级PTQ将FP16指令调优模型量化为INT4，以显著减少后续INT2量化引入的量化误差。接下来，UPQ应用Distill-QAT，使INT2指令调优LLM生成的响应与其原始FP16版本一致，通过最小化两者之间的广义Jensen-Shannon散度（JSD）来实现。据我们所知，我们首次证明UPQ可以在不依赖专有后训练数据的情况下将开源指令调优LLM量化为INT2，同时在MMLU和IFEval——两个评估指令调优LLM的最具代表性的基准测试上取得了最新性能表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09104v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>     随着大型语言模型（LLM）的快速扩展，在资源受限的设备上进行部署面临重大挑战。对于极低位量化（如2位）的需求日益增长。本文提出一种新型渐进量化框架——统一渐进量化（UPQ），它将块级后训练量化（PTQ）与基于蒸馏的量化感知训练（Distill-QAT）结合起来，用于INT2指令调优的LLM量化。UPQ使用FP16指令调优模型进行块级PTQ量化，以减少后续INT2量化的量化误差，然后通过应用Distill-QAT实现INT2指令调优的LLM生成响应与其原始的FP16模型响应的一致性。研究表明，UPQ可在不依赖专有后训练数据的情况下，将开源指令调优的LLM量化至INT2位，同时在MMLU和IFeval这两个最具代表性的指令调优LLM评估基准测试中取得最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在资源受限设备上的部署具有挑战，极低位量化（如2位）成为解决方案。</li>
<li>提出一种新型渐进量化框架——统一渐进量化（UPQ）。</li>
<li>UPQ结合了块级后训练量化（PTQ）与基于蒸馏的量化感知训练（Distill-QAT）。</li>
<li>UPQ首先使用FP16指令调优模型进行块级PTQ量化，以减少后续INT2量化的误差。</li>
<li>Distill-QAT使INT2指令调优的LLM生成的响应与原始的FP16模型响应一致。</li>
<li>UPQ在不依赖专有后训练数据的情况下，成功将开源指令调优的LLM量化至INT2位。</li>
<li>在最具代表性的指令调优LLM评估基准测试中，UPQ取得最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09104">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-81c09811d5d41c2f7f4d7406a243b33a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1760e21abe95f9e26870d09531fb7f8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94f164007bbfba084ebbd3f0ab8eee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac59547714dab4d29705b06f660d737d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f1ce8d38c208ec928f5619c48abd93d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CAD-Llama-Leveraging-Large-Language-Models-for-Computer-Aided-Design-Parametric-3D-Model-Generation"><a href="#CAD-Llama-Leveraging-Large-Language-Models-for-Computer-Aided-Design-Parametric-3D-Model-Generation" class="headerlink" title="CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation"></a>CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation</h2><p><strong>Authors:Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou</strong></p>
<p>Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines. </p>
<blockquote>
<p>最近，大型语言模型（LLM）取得了巨大成功，激发了人们将其生成能力扩展到通用文本之外的特定领域的兴趣。本研究探讨了使用LLM为计算机辅助设计（CAD）模型生成参数序列。这一努力是朝着使用LLM创建参数化3D形状迈出的初步一步，因为CAD模型参数直接与三维空间中的形状相关联。尽管LLM具有强大的生成能力，但这一任务仍然具有挑战性，因为这些模型在预训练阶段并未遇到参数序列，也不直接了解三维结构。为了解决这一问题，我们推出了CAD-Llama框架，旨在增强预训练的LLM生成参数化三维CAD模型的能力。具体而言，我们开发了一种分层注释管道和一种类似于代码的格式，将参数化的三维CAD命令序列翻译成结构化参数化CAD代码（SPCC），并融入分层语义描述。此外，我们提出了一种使用SPCC的自适应预训练方法，随后是符合CAD特定指南的指令调整过程。该方法旨在让LLM具备参数序列中的空间知识。实验结果表明，我们的框架显著优于先前的自回归方法和现有的LLM基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04481v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在生成通用文本方面取得了显著成功，并正逐渐向特定领域扩展其生成能力。本研究探讨了利用LLM生成计算机辅助设计（CAD）模型的参数序列。此研究为利用LLM创建参数化三维形状迈出了初步的一步，CAD模型参数与三维空间中的形状直接相关。尽管LLM具有强大的生成能力，但生成参数序列的任务仍然具有挑战性，因为这些模型在预训练阶段并未遇到参数序列，并且缺乏对三维结构的直接了解。为此，我们提出了CAD-Llama框架，旨在增强预训练的LLM生成参数化三维CAD模型的能力。通过开发层次化注释管道和类似代码的格式，将参数化三维CAD命令序列翻译成结构化参数化CAD代码（SPCC），并结合层次语义描述。我们还提出了一种利用SPCC的适应性预训练方法和与CAD特定指南对齐的指令调整过程。此方法旨在赋予LLM参数序列中的空间知识。实验结果表明，我们的框架显著优于先前的自动回归方法和现有的LLM基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在生成领域特定内容（如计算机辅助设计（CAD）模型的参数序列）方面展现出潜力。</li>
<li>CAD-Llama框架旨在增强LLM在生成参数化三维CAD模型方面的能力。</li>
<li>通过层次化注释管道和类似代码的格式，将三维CAD模型的参数序列转化为结构化参数化CAD代码（SPCC）。</li>
<li>提出了一种适应性预训练方法和指令调整过程，以赋予LLM空间知识和对CAD特定指南的理解。</li>
<li>实验结果显示，CAD-Llama框架在生成参数化三维CAD模型方面显著优于其他方法。</li>
<li>LLM在生成参数序列时面临挑战，如对三维结构的了解不足和在预训练阶段未接触参数序列。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f11abfccedb357e72deb330c4e9da804.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-684434d743fe23e809756fcf3763a8ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-579e6bb792154690ebea265c8ed18a62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab3d1d82c87db6797182ab2329750f98.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs"><a href="#Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs" class="headerlink" title="Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs"></a>Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs</h2><p><strong>Authors:Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco</strong></p>
<p>This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81% in the best-case scenario. </p>
<blockquote>
<p>本文评估了大语言模型（LLM）利用结构化的语言表示形式中的上下文信息的能力。具体来说，我们研究了使用抽象意义表示（AMR）结构对短长和两种上下文进行编码对一系列语言任务的影响。我们使用8位量化和指令调整的Llama 3.1（8B）、Phi-3和Mistral 7B版本进行分析。我们的结果表明，对于涉及短上下文的任务，通过提示增强原始语言上下文的AMR往往会降低基础LLM的性能。然而，对于涉及长上下文的任务，如在SAMSum数据集中的对话摘要，这种增强会提高LLM的性能，例如，将Llama 3.1的零样本余弦相似度得分从66%提高到76%。这种改进在更新和更大的LLM中更为明显，但并不适用于旧或较小的LLM。此外，我们观察到LLM可以有效地从线性化的AMR中重建原始文本，在最佳情况下达到81%的余弦相似度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04745v3">PDF</a> 13 pages, 23 figures. Accepted at XLLM @ ACL 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了大型语言模型（LLM）在利用结构化语言表示形式进行上下文信息方面的能力。研究通过抽象意义表示（AMR）结构编码短长和不同语境，对一系列语言任务进行了评估。分析表明，对于短语境任务，添加原始语境的AMR往往会降低LLM性能；而对于长语境任务，如对话摘要，这种增强则能提高LLM性能，特别是在新且大型的LLM中表现更为明显。此外，LLM还能有效地从线性化的AMR中重建原始文本。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）能够利用抽象意义表示（AMR）结构进行上下文信息编码。</li>
<li>在短语境任务中，添加AMR会降低LLM性能。</li>
<li>对于长语境任务，如对话摘要，使用AMR结构增强能提高LLM性能。</li>
<li>新且大型的LLM在利用AMR结构方面的性能提升更为显著。</li>
<li>LLM能从线性化的AMR有效地重建原始文本。</li>
<li>LLM在处理语境信息时，不同语境（短或长）需要不同的处理策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-916744d3207705176c15600842072da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2876cad9086bbf6e797d1ec62315f518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e910886f66f98fb042a4a850007dfe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b038b015a028b0bad607c72cf974316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6030c7386ed715da1172957f4e455f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0625426b1f81ffa2a742d52923fa0f9d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models"><a href="#ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models" class="headerlink" title="ASIDE: Architectural Separation of Instructions and Data in Language   Models"></a>ASIDE: Architectural Separation of Instructions and Data in Language   Models</h2><p><strong>Authors:Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Alexandra Volkova, Soroush Tabesh, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert</strong></p>
<p>Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) leads to highly increased instruction-data separation without a loss in model utility and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at <a target="_blank" rel="noopener" href="https://github.com/egozverev/aside">https://github.com/egozverev/aside</a>. </p>
<blockquote>
<p>尽管大型语言模型表现出色，但它们缺乏基本的安全功能，使其容易受到众多恶意攻击。特别是，先前的工作已经确定指令和数据之间缺乏内在分离是提示注入攻击成功的根本原因。在这项工作中，我们提出了一种新的架构元素ASIDE，它允许语言模型在嵌入层面上清晰地分离指令和数据。ASIDE通过对数据嵌入进行正交旋转，从而创建指令和数据的清晰表示，且无需引入任何额外的参数。我们在一系列模型上进行的实验表明，使用ASIDE进行指令微调的大型语言模型（1）能够在不损失模型效用的前提下，实现高度增强的指令数据分离；（2）即使不经过专门的安全训练，也能使模型对提示注入基准测试更具鲁棒性。此外，我们还通过对模型表示的分析，深入了解了我们的方法背后的机制。源代码和培训脚本可在<a target="_blank" rel="noopener" href="https://github.com/egozverev/aside">https://github.com/egozverev/aside</a>公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10566v3">PDF</a> Preliminary version accepted to ICLR 2025 Workshop on Building Trust   in Language Models and Applications</p>
<p><strong>Summary</strong><br>大型语言模型虽然表现优异，但缺乏基本的安全特性，容易受到多种恶意攻击。本文提出一种新型架构元素ASIDE，能在嵌入层面将指令和数据明确分离。通过应用数据标记嵌入的正交旋转，创造出指令和数据的清晰不同表示，无需引入额外参数。实验证明，在大型语言模型中采用ASIDE指令调优，不仅提高了指令与数据的分离度，且提升了模型对提示注入基准测试的鲁棒性，即使没有专门的安全训练也是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型存在安全漏洞，易受到恶意攻击。</li>
<li>缺乏指令和数据的内在分离是提示注入攻击成功的原因。</li>
<li>ASIDE架构能够在嵌入层面分离指令和数据。</li>
<li>ASIDE通过正交旋转数据标记嵌入来创建指令和数据的不同表示。</li>
<li>ASIDE提高了指令与数据的分离度，同时不损失模型的实用性。</li>
<li>采用ASIDE的大型语言模型对提示注入基准测试更具鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5ca0f72a9e2a91688ecce278077eeb8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbacb3fe22c862b791d355cc584a5e12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c693c649d0a82d40cb0553d51cb884e2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Peri-LN-Revisiting-Normalization-Layer-in-the-Transformer-Architecture"><a href="#Peri-LN-Revisiting-Normalization-Layer-in-the-Transformer-Architecture" class="headerlink" title="Peri-LN: Revisiting Normalization Layer in the Transformer Architecture"></a>Peri-LN: Revisiting Normalization Layer in the Transformer Architecture</h2><p><strong>Authors:Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo</strong></p>
<p>Selecting a layer normalization (LN) strategy that stabilizes training and speeds convergence in Transformers remains difficult, even for today’s large language models (LLM). We present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformers. Until recently, Pre-LN and Post-LN have long dominated practices despite their limitations in large-scale training. However, several open-source models have recently begun silently adopting a third strategy without much explanation. This strategy places normalization layer peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis delineates the distinct behaviors of LN strategies, showing how each placement shapes activation variance and gradient propagation. To validate our theoretical insight, we conduct extensive experiments on Transformers up to $3.2$B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement of LN. </p>
<blockquote>
<p>选择一种层归一化（LN）策略，以稳定训练并加速转换器中的收敛，即使在今天的大型语言模型（LLM）中仍然是一项艰巨的任务。我们提供了全面的分析基础，以了解不同的LN策略如何影响大规模转换器的训练动态。直到最近，尽管在大型训练中存在局限性，但Pre-LN和Post-LN的实践一直占据主导地位。然而，最近有几个开源模型开始默默地采用第三种策略，但没有太多解释。该策略将归一化层放置在子层周围，我们将其称为Peri-LN。虽然Peri-LN已显示出有前途的性能，但其精确机制和好处几乎尚未被探索。我们的深入分析阐明了LN策略的不同行为，展示了每种放置方式如何影响激活方差和梯度传播。为了验证我们的理论洞察力，我们在高达3.2B参数的转换器上进行了大量实验，结果显示Peri-LN始终实现了更平衡的方差增长、更稳定的梯度流动和收敛稳定性。我们的结果表明，对于大规模转换器架构，Peri-LN值得更广泛的考虑，为LN的最佳放置位置提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02732v3">PDF</a> ICML2025 Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>不同层归一化（LN）策略对大型Transformer训练的影响仍然难以确定，即使对于现今的大型语言模型（LLM）。研究者们提供了深入理解不同LN策略如何影响大规模Transformer训练动力的综合分析基础。直到最近，Pre-LN和Post-LN一直主导着实践，尽管它们在大型训练中存在局限性。然而，一些开源模型最近开始悄悄地采用第三种策略，即周边层归一化（Peri-LN），但很少解释。Peri-LN将归一化层置于子层周围，虽然已显示出有前景的性能，但其精确机制和好处几乎尚未被探索。本文深入分析勾勒出LN策略的不同行为，展示每种放置方式如何影响激活方差和梯度传播。为了验证理论见解，我们对高达3.2B参数的Transformer进行了大量实验，结果显示Peri-LN在方差增长、梯度流动的稳定性和收敛稳定性方面表现更稳定。我们的结果建议，对于大规模Transformer架构，应更广泛地考虑Peri-LN，并为LN的最佳放置位置提供新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>不同层归一化（LN）策略在大型Transformer模型训练中具有重要影响。</li>
<li>Pre-LN和Post-LN是实践中常见的两种策略，但在大型训练中存在局限性。</li>
<li>最近开源模型开始采用一种新兴的周边层归一化（Peri-LN）策略，但缺乏解释。</li>
<li>Peri-LN通过将归一化层置于子层周围进行设计，显示出有前景的性能。</li>
<li>LN策略的不同放置方式会影响激活方差和梯度传播。</li>
<li>实验结果表明，Peri-LN在方差增长、梯度稳定性及模型收敛等方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02732">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-29ecad38e926b3737199a0fd0d3139a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fc23775807db4c1242fe2af5e7e7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83578fa48ebfe502b69a5c8b36f28cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2da047c3f48a807d7cbd05b6a26b36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0476bbacf97b48e423cf44bfa81ec69.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Monet-Mixture-of-Monosemantic-Experts-for-Transformers"><a href="#Monet-Mixture-of-Monosemantic-Experts-for-Transformers" class="headerlink" title="Monet: Mixture of Monosemantic Experts for Transformers"></a>Monet: Mixture of Monosemantic Experts for Transformers</h2><p><strong>Authors:Jungwoo Park, Young Jin Ahn, Kee-Eung Kim, Jaewoo Kang</strong></p>
<p>Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity – where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust model behavior. The source code and pretrained checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Monet">https://github.com/dmis-lab/Monet</a>. </p>
<blockquote>
<p>理解大型语言模型（LLM）的内部计算对于将其与人类价值观对齐并防止生成有毒内容等不当行为至关重要。然而，多义性（即单个神经元对多个不相关概念的响应）阻碍了机械解释性。虽然稀疏自动编码器（SAE）试图通过稀疏字典学习来解开这些特征，但它们依赖于事后重建损失，从而损害了LLM的性能。为了解决这一问题，我们引入了“混合单义专家转换器”（Monet）架构，它将稀疏字典学习直接融入端到端的混合专家预训练。我们新颖的专家分解方法使专家数量按层增加到每层262,144个，同时总参数按专家数量的平方根成比例增长。我们的分析证明了专家之间知识的相互独立性，并展示了单个专家所包含的参数知识。此外，Monet允许在领域、语言和毒性减轻方面进行操作知识，而不会降低整体性能。我们对透明LLM的追求突显了增加专家数量以提高机械解释性的潜力，并可以直接切除内部知识来根本性地调整模型行为。源代码和预先训练的检查点可在<a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Monet%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/dmis-lab/Monet获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04139v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的内部计算理解对于与人类价值观对齐及防止生成有毒内容等不可取行为至关重要。然而，由于单词的多义性，即单个神经元会对多个不相关概念作出反应，导致机制性解释性受阻。尽管稀疏自动编码器（SAE）试图通过稀疏字典学习来解决这些特征的分解问题，但它们依赖于事后重建损失，从而损害了LLM的性能。为解决这一问题，我们引入了Mixture of Monosemantic Experts for Transformers（Monet）架构，该架构将稀疏字典学习直接纳入端到端的专家混合预训练。我们的新型专家分解方法使每层的专家数量能够扩展到262,144个，同时总参数与专家数量的平方根成比例扩展。分析表明，各专家之间的知识储备相互独立，展示了单个专家所封装的知识参数。此外，Monet能够在不降低整体性能的情况下，实现跨领域、跨语言的知识操纵和毒性缓解。我们对透明LLM的追求突显了增加专家数量以提高机制解释性的潜力，并可以直接调整内部知识以根本改变模型行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的内部计算理解对于与人类价值观对齐及避免不良行为至关重要。</li>
<li>单词的多义性阻碍了对LLM机制性解释的理解。</li>
<li>稀疏自动编码器（SAE）虽然试图解决特征分解问题，但会因为依赖事后重建损失而损害LLM性能。</li>
<li>Mixture of Monosemantic Experts for Transformers（Monet）架构直接纳入稀疏字典学习至端到端的专家混合预训练，以提高机制解释性。</li>
<li>Monet通过新型专家分解方法使每层的专家数量能够大幅扩展。</li>
<li>专家之间的知识储备相互独立，且可以实现跨领域、跨语言的知识操纵和毒性缓解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04139">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-034ae5bbdf9a74cf98d874c39b2764c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5950e0b2ac2df741f831b6736f45726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89f8b96b14d87014d76a7f04f96333ae.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="pLDDT-Predictor-High-speed-Protein-Screening-Using-Transformer-and-ESM2"><a href="#pLDDT-Predictor-High-speed-Protein-Screening-Using-Transformer-and-ESM2" class="headerlink" title="pLDDT-Predictor: High-speed Protein Screening Using Transformer and ESM2"></a>pLDDT-Predictor: High-speed Protein Screening Using Transformer and ESM2</h2><p><strong>Authors:Joongwon Chae, Zhenyu Wang, Ijaz Gul, Jiansong Ji, Zhenglin Chen, Peiwu Qin</strong></p>
<p>Recent advancements in protein structure prediction, particularly AlphaFold2, have revolutionized structural biology by achieving near-experimental accuracy ($\text{average RMSD} &lt; 1.5\text{\AA}$). However, the computational demands of these models (approximately 30 minutes per protein on an RTX 4090) significantly limit their application in high-throughput protein screening. While large language models like ESM (Evolutionary Scale Modeling) have shown promise in extracting structural information directly from protein sequences, rapid assessment of protein structure quality for large-scale analyses remains a major challenge.   We introduce pLDDT-Predictor, a high-speed protein screening tool that achieves a $250,000\times$ speedup compared to AlphaFold2 by leveraging pre-trained ESM2 protein embeddings and a Transformer architecture. Our model predicts AlphaFold2’s pLDDT (predicted Local Distance Difference Test) scores with a Pearson correlation of 0.7891 and processes proteins in just 0.007 seconds on average. Using a comprehensive dataset of 1.5 million diverse protein sequences (ranging from 50 to 2048 amino acids), we demonstrate that pLDDT-Predictor accurately classifies high-confidence structures (pLDDT $&gt;$ 70) with 91.2% accuracy and achieves an MSE of 84.8142 compared to AlphaFold2’s predictions.   The source code and pre-trained models are freely available at <a target="_blank" rel="noopener" href="https://github.com/jw-chae/pLDDT_Predictor">https://github.com/jw-chae/pLDDT_Predictor</a>, enabling the research community to perform rapid, large-scale protein structure quality assessments. </p>
<blockquote>
<p>近期蛋白质结构预测方面的进展，特别是AlphaFold2，已经通过实现接近实验精度（平均RMSD &lt; 1.5Å）的方式，彻底改变了结构生物学。然而，这些模型的计算需求（在RTX 4090上每个蛋白大约需要30分钟）显著限制了它们在高通量蛋白质筛选中的应用。虽然像ESM（进化规模建模）这样的大规模语言模型在直接从蛋白质序列中提取结构信息方面显示出希望，但对于大规模分析的蛋白质结构质量快速评估仍然是一个主要挑战。</p>
</blockquote>
<p>我们引入了pLDDT-Predictor，这是一个高速蛋白质筛选工具，它通过利用预训练的ESM2蛋白质嵌入和Transformer架构，实现了相对于AlphaFold2的速度提升250,000倍。我们的模型预测AlphaFold2的pLDDT（预测局部距离差异测试）分数，Pearson相关系数为0.7891，平均每个蛋白质的处理时间仅为0.007秒。我们使用包含150万个不同蛋白质序列的综合数据集（从50到2048个氨基酸），证明了pLDDT-Predictor能够准确分类高可信度的结构（pLDDT &gt; 70），准确率为91.2％，与AlphaFold2的预测相比，MSE为84.8142。</p>
<p>源代码和预训练模型可在<a target="_blank" rel="noopener" href="https://github.com/jw-chae/pLDDT_Predictor%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%BE%97%EF%BC%8C%E4%BD%BF%E7%A0%94%E7%A9%B6%E7%A4%BE%E5%8C%BA%E8%83%BD%E5%A4%9F%E8%BF%9B%E8%A1%8C%E5%BF%AB%E9%80%9F%E7%9A%84%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%E3%80%82">https://github.com/jw-chae/pLDDT_Predictor免费获得，使研究社区能够进行快速的大规模蛋白质结构质量评估。</a></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21283v3">PDF</a> Further experiments confirmed overfitting, and we are retracting the   paper</p>
<p><strong>摘要</strong><br>    AlphaFold2等蛋白质结构预测技术的最新进展对结构生物学产生革命性影响，但计算需求大限制了其高通量蛋白质筛选的应用。ESM等大型语言模型在直接从蛋白质序列中提取结构信息方面显示出潜力，但对大规模分析中蛋白质结构质量的快速评估仍是挑战。本研究引入pLDDT-Predictor，利用预训练的ESM2蛋白质嵌入和Transformer架构，实现了与AlphaFold2相比高达$250,000\times$的加速。该模型预测AlphaFold2的pLDDT分数与真实值高度相关，并对蛋白质进行快速处理。使用包含150万多种蛋白质序列的综合性数据集，展示pLDDT-Predictor准确分类高置信度结构，并接近AlphaFold2的预测精度。源代码和预训练模型可免费获取。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>AlphaFold2等蛋白质结构预测技术取得了显著进展，但在高通量蛋白质筛选方面存在计算需求大的限制。</li>
<li>ESM等大型语言模型在提取蛋白质序列的结构信息方面展现出潜力。</li>
<li>pLDDT-Predictor是一个快速的蛋白质筛选工具，利用预训练的ESM2蛋白质嵌入和Transformer架构实现了高速处理。</li>
<li>pLDDT-Predictor能预测AlphaFold2的pLDDT分数，与真实值高度相关，并具备高准确性。</li>
<li>pLDDT-Predictor可在综合性数据集上准确分类高置信度结构。</li>
<li>pLDDT-Predictor的源代码和预训练模型已公开发布，供研究社区使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21283">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5839fd8d2e9b40b507cdda7ebc0c25b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f72cd817914f1a55489189e730c3c09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61b08a47a4c008bc894a8e0e4367f790.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca69fcd9524dd885d7f943e5cf4cc2f7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CHOSEN-Compilation-to-Hardware-Optimization-Stack-for-Efficient-Vision-Transformer-Inference"><a href="#CHOSEN-Compilation-to-Hardware-Optimization-Stack-for-Efficient-Vision-Transformer-Inference" class="headerlink" title="CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision   Transformer Inference"></a>CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision   Transformer Inference</h2><p><strong>Authors:Mohammad Erfan Sadeghi, Arash Fayyazi, Suhas Somashekar, Armin Abdollahi, Massoud Pedram</strong></p>
<p>Vision Transformers (ViTs) represent a groundbreaking shift in machine learning approaches to computer vision. Unlike traditional approaches, ViTs employ the self-attention mechanism, which has been widely used in natural language processing, to analyze image patches. Despite their advantages in modeling visual tasks, deploying ViTs on hardware platforms, notably Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges. These challenges stem primarily from the non-linear calculations and high computational and memory demands of ViTs. This paper introduces CHOSEN, a software-hardware co-design framework to address these challenges and offer an automated framework for ViT deployment on the FPGAs in order to maximize performance. Our framework is built upon three fundamental contributions: multi-kernel design to maximize the bandwidth, mainly targeting benefits of multi DDR memory banks, approximate non-linear functions that exhibit minimal accuracy degradation, and efficient use of available logic blocks on the FPGA, and efficient compiler to maximize the performance and memory-efficiency of the computing kernels by presenting a novel algorithm for design space exploration to find optimal hardware configuration that achieves optimal throughput and latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a 1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models. </p>
<blockquote>
<p>视觉转换器（ViTs）代表了计算机视觉机器学习方法的突破性转变。与传统的计算机视觉方法不同，ViTs采用自注意力机制（已在自然语言处理中广泛使用）来分析图像块。尽管ViTs在建模视觉任务方面具有优势，但在硬件平台（特别是现场可编程门阵列（FPGA））上部署ViTs仍存在相当大的挑战。这些挑战主要源于ViTs的非线性计算以及其对计算和内存的高需求。本文介绍了CHOSEN，这是一个软硬件协同设计框架，旨在解决这些挑战，并为FPGA上的ViT部署提供自动化框架，以最大化性能。我们的框架建立在三个基本贡献之上：多核设计以最大化带宽（主要针对多DDR内存银行的优势）、近似非线性函数以展现最小的精度损失以及高效使用FPGA上的可用逻辑块，以及高效的编译器。编译器通过提供一种新型算法来进行设计空间探索，以找到实现最佳吞吐量和延迟的最佳硬件配置，从而最大限度地提高计算内核的性能和内存效率。与最先进的ViT加速器相比，CHOSEN在DeiT-S和DeiT-B模型上的吞吐量提高了1.5倍和1.42倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12736v4">PDF</a> </p>
<p><strong>Summary</strong>:<br>ViTs在机器视觉领域掀起革命性变革，但其在FPGA等硬件平台上的部署面临巨大挑战。本文提出CHOSEN软硬件协同设计框架，通过多核设计、近似非线性函数和高效编译器等技术，优化ViT在FPGA上的性能。相较于现有ViT加速器，CHOSEN在DeiT-S和DeiT-B模型上分别实现了1.5倍和1.42倍的吞吐性能提升。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Vision Transformers (ViTs) 引入自注意力机制，为计算机视觉领域带来革新。</li>
<li>ViTs在FPGA硬件平台部署面临挑战，主要源于其非线性计算和高计算内存需求。</li>
<li>CHOSEN框架通过多核设计最大化带宽，主要面向多DDR内存银行的优点。</li>
<li>CHOSEN采用近似非线性函数，在极小准确性损失下优化性能。</li>
<li>该框架有效利用FPGA的逻辑块，实现高效运行。</li>
<li>CHOSEN的编译器通过新型算法进行设计时域探索，找到实现最佳吞吐量和延迟的硬件配置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12736">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-65a75bb9474e0f49db6b1bec65e09ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd2eb2d72bd32b0c076d775844ac3e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0858ad978a13be34ddd102a46452f86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ca39d4a76696f85393ea06c0fd8c78e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12fe27472e0dd270c46b783e5f85a091.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TextSquare-Scaling-up-Text-Centric-Visual-Instruction-Tuning"><a href="#TextSquare-Scaling-up-Text-Centric-Visual-Instruction-Tuning" class="headerlink" title="TextSquare: Scaling up Text-Centric Visual Instruction Tuning"></a>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</h2><p><strong>Authors:Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Yangfan He, Kuan Lu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang</strong></p>
<p>Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M. </p>
<blockquote>
<p>文本聚焦的视觉问答（VQA）随着多模态大型语言模型（MLLMs）的发展而取得了巨大进步。然而，开源模型仍然无法赶超如GPT4V和Gemini等领先模型，部分原因在于缺乏广泛的高质量指令调整数据。为此，我们介绍了一种创建大规模高质量指令调整数据集Square-10M的新方法，该方法使用封闭源代码的MLLMs生成。数据构建过程名为Square，分为四个步骤：自问、回答、推理和评价。我们使用Square-10M进行的实验得到了三个关键发现：</p>
</blockquote>
<ol>
<li>我们的模型TextSquare显著超越了之前开源的先进文本聚焦型MLLMs，并在OCRBench上设定了新标准（62.2%）。它甚至在10个文本聚焦的基准测试中中有6个超越了顶尖模型如GPT4V和Gemini。</li>
<li>此外，我们证明了VQA推理数据在提供特定问题的全面背景信息方面的关键作用。这不仅提高了准确性，而且有效减轻了幻觉现象。具体来说，TextSquare在四个通用VQA和幻觉评估数据集上的平均得分为75.1%，超过了之前的先进模型。</li>
<li>值得注意的是，在扩大文本聚焦的VQA数据集时观察到的现象呈现出一个清晰的模式：指令调整数据量的指数增长与模型性能的提高直接相关，从而验证了数据集规模和Square-10M高质量数据的必要性。</li>
</ol>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12803v3">PDF</a> </p>
<p><strong>摘要</strong><br>    基于多模态大型语言模型（MLLMs）的文本中心视觉问答（VQA）已取得显著进展，但仍落后于GPT4V和Gemini等领先模型，部分原因在于缺乏大量高质量指令调整数据。为此，我们引入了一种创建大规模高质量指令调整数据集Square-10M的新方法，该方法利用闭源MLLMs生成。数据构建过程包括自我提问、回答、推理和评估四个步骤。使用Square-10M的实验表明：1）我们的TextSquare模型超越了之前开源的文本中心型MLLMs的先进水平，在OCRBench上设定了新标准（62.2%），并在10个文本中心型基准测试中6项超过GPT4V和Gemini。2）我们还证明了VQA推理数据在提供特定问题的全面上下文洞察中的关键作用。这不仅提高了准确性，而且显著减轻了虚构现象。TextSquare在四个一般VQA和虚构现象评估数据集上的平均得分为75.1%，优于之前的最佳模型。3）文本中心型VQA数据集扩展现象显示，指令调整数据量的指数增长与模型性能的提高直接相关，验证了数据集规模和Square-10M高质量数据的必要性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用闭源MLLMs，推出了新的大规模高质量指令调整数据集Square-10M。</li>
<li>TextSquare模型在多个文本中心型基准测试中表现出卓越性能，超越了现有开源模型及GPT4V和Gemini等顶尖模型。</li>
<li>VQA推理数据对于提供问题的全面上下文洞察至关重要，不仅能提高准确性，还能有效抑制虚构现象。</li>
<li>TextSquare在VQA评估中的表现优异，证明了其在处理复杂问题时的有效性。</li>
<li>数据集扩展现象显示，指令调整数据的数量与模型性能的提高呈正相关，凸显了大规模高质量数据集的重要性。</li>
<li>Square-10M的推出对于推动文本中心型VQA领域的进步具有重大意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b71496f5799230982f5f975d4a7a97a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69cc7db9e4dbfe676879e7331e2da6aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-096f153eb7d4e01b4053d1e02784d5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7acb0085b072d6bf157006077fbafa7a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-df04937e3f74637c3683c7c30cd2b471.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-13  "What are my options?" Explaining RL Agents with Diverse Near-Optimal   Alternatives (Extended)
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f2339f60031315509f905003087eeda9.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-13  A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
