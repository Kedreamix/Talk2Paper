<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-89f8b96b14d87014d76a7f04f96333ae.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="Large-Language-Models-for-Toxic-Language-Detection-in-Low-Resource-Balkan-Languages"><a href="#Large-Language-Models-for-Toxic-Language-Detection-in-Low-Resource-Balkan-Languages" class="headerlink" title="Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages"></a>Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages</h2><p><strong>Authors:Amel Muminovic, Amela Kadric Muminovic</strong></p>
<p>Online toxic language causes real harm, especially in regions with limited moderation tools. In this study, we evaluate how large language models handle toxic comments in Serbian, Croatian, and Bosnian, languages with limited labeled data. We built and manually labeled a dataset of 4,500 YouTube and TikTok comments drawn from videos across diverse categories, including music, politics, sports, modeling, influencer content, discussions of sexism, and general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) were tested in two modes: zero-shot and context-augmented. We measured precision, recall, F1 score, accuracy and false positive rates. Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini in context-augmented mode, reaching an F1 score of 0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the lowest false alarms. We show how adding minimal context can improve toxic language detection in low-resource settings and suggest practical strategies such as improved prompt design and threshold calibration. These results show that prompt design alone can yield meaningful gains in toxicity detection for underserved Balkan language communities. </p>
<blockquote>
<p>ç½‘ç»œæœ‰æ¯’è¯­è¨€ä¼šå¯¼è‡´çœŸå®ä¼¤å®³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹ç®¡ç†å·¥å…·çš„åœ°åŒºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­çš„æœ‰æ¯’è¯„è®ºï¼Œè¿™äº›è¯­è¨€çš„æ ‡è®°æ•°æ®æœ‰é™ã€‚æˆ‘ä»¬æ„å»ºå¹¶æ‰‹åŠ¨æ ‡è®°äº†ä¸€ä¸ªåŒ…å«4500æ¡YouTubeå’ŒTikTokè¯„è®ºçš„æ•°æ®é›†ï¼Œè¿™äº›è¯„è®ºæ¥è‡ªéŸ³ä¹ã€æ”¿æ²»ã€ä½“è‚²ã€æ¨¡ç‰¹ã€ç½‘çº¢å†…å®¹ã€æ€§åˆ«ä¸»ä¹‰è®¨è®ºå’Œä¸€èˆ¬è¯é¢˜ç­‰å„ç±»è§†é¢‘ã€‚æˆ‘ä»¬æµ‹è¯•äº†å››ç§æ¨¡å‹ï¼ˆGPT-3.5 Turboã€GPT-4.1ã€Gemini 1.5 Proå’ŒClaude 3 Opusï¼‰åœ¨é›¶å°„æ¨¡å¼å’Œå¢å¼ºä¸Šä¸‹æ–‡æ¨¡å¼ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬æµ‹é‡äº†ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å‡†ç¡®ç‡å’Œè¯¯æŠ¥ç‡ã€‚åŒ…å«ç®€çŸ­ä¸Šä¸‹æ–‡ç‰‡æ®µå¹³å‡æé«˜äº†çº¦0.12çš„å¬å›ç‡ï¼Œå¹¶ä½¿F1åˆ†æ•°æé«˜äº†é«˜è¾¾0.10ï¼Œä½†æœ‰æ—¶ä¹Ÿä¼šå¢åŠ è¯¯æŠ¥ã€‚æœ€ä½³çš„å¹³è¡¡æ¥è‡ªäºå¢å¼ºä¸Šä¸‹æ–‡æ¨¡å¼ä¸‹çš„Geminiï¼Œå…¶F1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¾¾åˆ°0.82ï¼Œè€Œé›¶å°„æ¨¡å¼ä¸‹çš„GPT-4.1åœ¨ç²¾ç¡®åº¦ä¸Šé¢†å…ˆï¼Œå¹¶ä¸”è¯¯æŠ¥ç‡æœ€ä½ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­é€šè¿‡æ·»åŠ æœ€å°‘çš„ä¸Šä¸‹æ–‡æ¥æ”¹å–„æœ‰æ¯’è¯­è¨€æ£€æµ‹ï¼Œå¹¶æå‡ºäº†ä¸€äº›å®ç”¨ç­–ç•¥ï¼Œå¦‚æ”¹è¿›æç¤ºè®¾è®¡å’Œé˜ˆå€¼æ ¡å‡†ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä»…å‡­æç¤ºè®¾è®¡å°±å¯ä»¥åœ¨æ£€æµ‹å¯¹å·´å°”å¹²åœ°åŒºæœåŠ¡ä¸è¶³çš„è¯­è¨€ç¤¾åŒºçš„æœ‰æ¯’æ€§æ–¹é¢å–å¾—æœ‰æ„ä¹‰çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09992v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­çš„æ¯’æ€§è¯„è®ºæ—¶çš„è¡¨ç°ã€‚ç ”ç©¶è€…æ‰‹åŠ¨æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«4500æ¡YouTubeå’ŒTikTokè¯„è®ºçš„æ•°æ®é›†ï¼Œæµ‹è¯•äº†å››ç§æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ·»åŠ ä¸Šä¸‹æ–‡ç‰‡æ®µå¯ä»¥æé«˜å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œä½†ä¹Ÿä¼šå¢åŠ è¯¯æŠ¥ã€‚Geminiåœ¨ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹è¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¾¾åˆ°0.82ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ·»åŠ æœ€å°é‡çš„ä¸Šä¸‹æ–‡å¯ä»¥æ”¹å–„ä½èµ„æºç¯å¢ƒä¸‹çš„æœ‰æ¯’è¯­è¨€æ£€æµ‹ï¼Œæç¤ºè®¾è®¡è‰¯å¥½çš„æç¤ºå’Œé˜ˆå€¼æ ¡å‡†ç­‰å®ç”¨ç­–ç•¥å¯ä»¥å¸¦æ¥æœ‰æ„ä¹‰çš„æœ‰æ¯’æ€§æ£€æµ‹å¢ç›Šï¼Œå¯¹å·´å°”å¹²è¯­æ—çš„ç¤¾åŒºå°¤ä¸ºå…³é”®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºç¯å¢ƒçš„æ¯’æ€§è¯­è¨€æ—¶çš„æŒ‘æˆ˜ã€‚</li>
<li>æ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®é›†ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç‰‡æ®µçš„æ·»åŠ å¯ä»¥æé«˜æ¨¡å‹çš„å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚</li>
<li>Geminiåœ¨ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ·»åŠ æœ€å°é‡çš„ä¸Šä¸‹æ–‡èƒ½æ˜¾è‘—æå‡æœ‰æ¯’è¯­è¨€æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>è‰¯å¥½çš„æç¤ºè®¾è®¡å’Œé˜ˆå€¼æ ¡å‡†èƒ½æé«˜æ¯’æ€§æ£€æµ‹çš„å¢ç›Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ea1ff86bf8685ace52a37bdedbc7608.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb526417475bb3dba62bb98d2113745.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf86acfdcc0adba6440e11c0acbbe36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9add948c41b2dac381b5325d8654ca8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcd885e82484fe16af26261cf6c18878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d629d6fba6fe3a27dabe4b449f84202b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning"><a href="#V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning" class="headerlink" title="V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning"></a>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning</h2><p><strong>Authors:Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes,  Mojtaba,  Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas</strong></p>
<p>A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world. </p>
<blockquote>
<p>ç°ä»£äººå·¥æ™ºèƒ½é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å­¦ä¼šé€šè¿‡å¤§é‡çš„è§‚å¯Ÿç†è§£ä¸–ç•Œå¹¶é‡‡å–è¡ŒåŠ¨ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œå®ƒç»“åˆäº†äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®å’Œå°é‡çš„äº¤äº’æ•°æ®ï¼ˆæœºå™¨äººè½¨è¿¹ï¼‰ï¼Œä»¥å¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç†è§£ã€é¢„æµ‹å’Œè§„åˆ’æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨ä¸€ä¸ªåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡å°æ—¶äº’è”ç½‘è§†é¢‘çš„è§†é¢‘å’Œå›¾åƒæ•°æ®é›†ä¸Šï¼Œå¯¹æ— åŠ¨ä½œè”åˆåµŒå…¥é¢„æµ‹æ¶æ„V-JEPA 2è¿›è¡Œé¢„è®­ç»ƒã€‚V-JEPA 2åœ¨è¿åŠ¨ç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ˆåœ¨Something-Something v2ä¸Šè¾¾åˆ°77.3%çš„top-1å‡†ç¡®ç‡ï¼‰ï¼Œå¹¶åœ¨äººç±»è¡Œä¸ºé¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆåœ¨Epic-Kitchens-100ä¸Šçš„å¬å›ç‡ä¸º39.7%ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½åï¼Œæˆ‘ä»¬åœ¨å‚æ•°è§„æ¨¡ä¸º8äº¿çš„å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒPerceptionTestä¸Šä¸º84.0%ï¼ŒTempCompassä¸Šä¸º76.9%ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡äº‹åè®­ç»ƒæ½œåœ¨çš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹V-JEPA 2-ACï¼Œå°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºæœºå™¨äººè§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªDroidæ•°æ®é›†çš„ä¸åˆ°62å°æ—¶çš„æ— æ ‡ç­¾æœºå™¨äººè§†é¢‘å¯¹V-JEPA 2-ACè¿›è¡Œè®­ç»ƒï¼Œç„¶åå°†å…¶é›¶å°„å‡»éƒ¨ç½²åœ¨Frankaæœºæ¢°è‡‚ä¸Šï¼Œåœ¨ä¸¤ä¸ªä¸åŒçš„å®éªŒå®¤ç¯å¢ƒä¸­å®ç°äº†åŸºäºå›¾åƒç›®æ ‡çš„ç‰©ä½“æŠ“å–å’Œæ”¾ç½®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ— éœ€ä»ç¯å¢ƒä¸­çš„æœºå™¨äººæ”¶é›†ä»»ä½•æ•°æ®ï¼Œä¹Ÿæ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–å¥–åŠ±ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ä»äº’è”ç½‘è§„æ¨¡çš„æ•°æ®å’Œå°‘é‡çš„æœºå™¨äººäº¤äº’æ•°æ®ä¸­é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è·å¾—èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­è§„åˆ’çš„ä¸–ç•Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09985v1">PDF</a> 48 pages, 19 figures</p>
<p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨äº’è”ç½‘è§„æ¨¡è§†é¢‘æ•°æ®å’Œå°é‡äº¤äº’æ•°æ®ï¼ˆæœºå™¨äººè½¨è¿¹ï¼‰ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå‘å±•å‡ºèƒ½å¤Ÿç†è§£å’Œé¢„æµ‹ç‰©ç†ä¸–ç•Œçš„æ¨¡å‹ã€‚é¢„è®­ç»ƒçš„V-JEPA 2æ¨¡å‹åœ¨åŠ¨ä½œç†è§£å’Œäººç±»è¡ŒåŠ¨é¢„æœŸæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½åï¼Œåœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨åè®­ç»ƒä¸­ä½¿ç”¨æœºå™¨äººè§†é¢‘æ•°æ®ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è‡ªç›‘ç£å­¦ä¹ åœ¨æœºå™¨äººè§„åˆ’ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ä»äº’è”ç½‘è§„æ¨¡æ•°æ®å’Œå°‘é‡æœºå™¨äººäº¤äº’æ•°æ®ä¸­å­¦ä¹ ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­è§„åˆ’è¡ŒåŠ¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°ä»£AIé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯é€šè¿‡è§‚å¯Ÿå­¦ä¹ ç†è§£ä¸–ç•Œå’Œé‡‡å–è¡ŒåŠ¨ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è‡ªç›‘ç£æ–¹æ³•ï¼Œç»“åˆäº’è”ç½‘è§„æ¨¡è§†é¢‘æ•°æ®ä¸å°‘é‡äº¤äº’æ•°æ®ï¼ˆæœºå™¨äººè½¨è¿¹ï¼‰ã€‚</li>
<li>V-JEPA 2æ¨¡å‹åœ¨åŠ¨ä½œç†è§£å’Œäººç±»è¡ŒåŠ¨é¢„æœŸæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½åï¼ŒV-JEPA 2åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æœºå™¨äººè§†é¢‘æ•°æ®çš„åè®­ç»ƒï¼Œå±•ç¤ºäº†è‡ªç›‘ç£å­¦ä¹ åœ¨æœºå™¨äººè§„åˆ’ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>V-JEPA 2æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„ç‰©ç†ç¯å¢ƒä¸­å®ç°é›¶æ ·æœ¬ç‰©ä½“æŠ“å–å’Œæ”¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c20e7fd378bb75f6277bc2b568fab79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52726ea443acb63e798931278b8709a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c12be16346c246b398c8c3094751fe8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb56100036aa6a8241569ba35d98df63.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TOGA-Temporally-Grounded-Open-Ended-Video-QA-with-Weak-Supervision"><a href="#TOGA-Temporally-Grounded-Open-Ended-Video-QA-with-Weak-Supervision" class="headerlink" title="TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision"></a>TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</h2><p><strong>Authors:Ayush Gupta, Anirban Roy, Rama Chellappa, Nathaniel D. Bastian, Alvaro Velasquez, Susmit Jha</strong></p>
<p>We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate weakly supervised grounded question answering. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬è§£å†³äº†åœ¨å¼±ç›‘ç£è®¾ç½®ä¸‹å¸¦æœ‰æ—¶é—´å®šä½çš„è§†é¢‘é—®ç­”ï¼ˆVideo QAï¼‰é—®é¢˜ï¼Œè€Œæ— éœ€ä»»ä½•æ—¶é—´æ³¨é‡Šã€‚ç»™å®šä¸€ä¸ªè§†é¢‘å’Œä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªåŸºäºå¼€å§‹å’Œç»“æŸæ—¶é—´çš„å¼€æ”¾å¼ç­”æ¡ˆã€‚ä¸ºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†TOGAï¼šä¸€ä¸ªç”¨äºå¸¦æœ‰æ—¶é—´å®šä½çš„å¼€æ”¾å¼è§†é¢‘é—®ç­”çš„è§†å¬è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¼±ç›‘ç£ä¸‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æŒ‡å¯¼å¹¶è°ƒæ•´TOGAä»¥è”åˆç”Ÿæˆç­”æ¡ˆå’Œæ—¶é—´å®šä½ã€‚æˆ‘ä»¬åœ¨æ²¡æœ‰æ—¶é—´å®šä½æ³¨é‡Šçš„å¼±ç›‘ç£è®¾ç½®ä¸­è¿›è¡Œæ“ä½œã€‚æˆ‘ä»¬ä¸ºæ—¶é—´å®šä½ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶é€šè¿‡åœ¨å®šä½å“åº”çš„é—®é¢˜å’Œé’ˆå¯¹åŒä¸€æ—¶é—´æ®µçš„å“åº”ä¹‹é—´æ–½åŠ ä¸€è‡´æ€§çº¦æŸæ¥ç¡®ä¿è¿™äº›æ ‡ç­¾çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘ç°è”åˆç”Ÿæˆç­”æ¡ˆå’Œå®šä½å¯ä»¥æé«˜é—®ç­”å’Œå®šä½çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å®šä½é—®ç­”å’Œå¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸Šè¯„ä¼°äº†TOGAã€‚å¯¹äºå®šä½é—®ç­”ï¼Œæˆ‘ä»¬è€ƒè™‘äº†NExT-GQAåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¼±ç›‘ç£ä¸‹çš„å®šä½é—®ç­”ã€‚å¯¹äºå¼€æ”¾å¼é—®ç­”ï¼Œæˆ‘ä»¬è€ƒè™‘äº†MSVD-QAå’Œæ´»åŠ¨ç½‘QAåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸Šçš„è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09445v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘é—®ç­”ä¸­çš„æ—¶åºå®šä½é—®é¢˜å¯åœ¨æ— æ—¶åºæ ‡æ³¨çš„å¼±ç›‘ç£è®¾ç½®ä¸‹è§£å†³ã€‚ç»™å®šè§†é¢‘å’Œé—®é¢˜ï¼Œæˆ‘ä»¬ç”ŸæˆåŸºäºèµ·å§‹å’Œç»“æŸæ—¶é—´çš„å¼€æ”¾ç­”æ¡ˆã€‚ä¸ºæ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºTOGAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¼±ç›‘ç£ä¸‹è”åˆç”Ÿæˆç­”æ¡ˆå’Œæ—¶åºå®šä½ã€‚é€šè¿‡ç”Ÿæˆæ—¶åºå®šä½ä¼ªæ ‡ç­¾å¹¶ç¡®ä¿æ ‡ç­¾ä¸é—®é¢˜çš„ä¸€è‡´æ€§çº¦æŸæ¥æé«˜æ ‡ç­¾çš„æœ‰æ•ˆæ€§ã€‚è”åˆç”Ÿæˆç­”æ¡ˆå’Œå®šä½æé«˜äº†é—®ç­”å’Œå®šä½æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†TOGAçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æœ‰ä¾æ®çš„QAå’Œå¼€æ”¾å¼çš„QAä»»åŠ¡ï¼Œå–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è§£å†³äº†è§†é¢‘é—®ç­”ä¸­çš„æ—¶åºå®šä½é—®é¢˜ï¼Œåœ¨æ— æ—¶åºæ ‡æ³¨çš„å¼±ç›‘ç£ç¯å¢ƒä¸‹è¿›è¡Œã€‚</li>
<li>æå‡ºTOGAæ¨¡å‹ï¼Œç”¨äºåœ¨å¼±ç›‘ç£ç¯å¢ƒä¸‹è”åˆç”Ÿæˆç­”æ¡ˆå’Œæ—¶åºå®šä½ã€‚</li>
<li>ç”Ÿæˆæ—¶åºå®šä½çš„ä¼ªæ ‡ç­¾å¹¶é€šè¿‡ä¸€è‡´æ€§çº¦æŸæ¥ä¿è¯æ ‡ç­¾çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è”åˆç”Ÿæˆç­”æ¡ˆå’Œå®šä½æé«˜äº†é—®ç­”å’Œå®šä½çš„æ€§èƒ½ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†TOGAæ¨¡å‹åœ¨æœ‰ä¾æ®çš„QAå’Œå¼€æ”¾å¼çš„QAä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨NExT-GQAã€MSVD-QAå’ŒActivityNet-QAç­‰åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3ef4e6c100bf63e90f45b8fdfb57ce26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88131fa4ee2ea1703a178a16362567fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45e5036f0c2df612c525c2bb553dfdc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GigaChat-Family-Efficient-Russian-Language-Modeling-Through-Mixture-of-Experts-Architecture"><a href="#GigaChat-Family-Efficient-Russian-Language-Modeling-Through-Mixture-of-Experts-Architecture" class="headerlink" title="GigaChat Family: Efficient Russian Language Modeling Through Mixture of   Experts Architecture"></a>GigaChat Family: Efficient Russian Language Modeling Through Mixture of   Experts Architecture</h2><p><strong>Authors: GigaChat team, Mamedov Valentin, Evgenii Kosarev, Gregory Leleytner, Ilya Shchuckin, Valeriy Berezovskiy, Daniil Smirnov, Dmitry Kozlov, Sergei Averkiev, Lukyanenko Ivan, Aleksandr Proshunin, Ainur Israfilova, Ivan Baskov, Artem Chervyakov, Emil Shakirov, Mikhail Kolesov, Daria Khomich, Darya Latortseva, Sergei Porkhun, Yury Fedorov, Oleg Kutuzov, Polina Kudriavtseva, Sofiia Soldatova, Kolodin Egor, Stanislav Pyatkin, Dzmitry Menshykh, Grafov Sergei, Eldar Damirov, Karlov Vladimir, Ruslan Gaitukiev, Arkadiy Shatenov, Alena Fenogenova, Nikita Savushkin, Fedor Minkin</strong></p>
<p>Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (<a target="_blank" rel="noopener" href="https://huggingface.co/ai-sage">https://huggingface.co/ai-sage</a>), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language. </p>
<blockquote>
<p>ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºç°ä»£å¤šè¯­è¨€NLPç ”ç©¶ä¸åº”ç”¨çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œé’ˆå¯¹ä¿„è¯­ç‰¹åˆ«å®šåˆ¶çš„åŸºç¡€æ¨¡å‹å¼€å‘å—åˆ°é™åˆ¶ï¼Œä¸»è¦æ˜¯ç”±äºæ‰€éœ€çš„å¤§é‡è®¡ç®—èµ„æºã€‚æœ¬æ–‡ä»‹ç»äº†GigaChatç³»åˆ—ä¿„è¯­LLMï¼Œè¯¥ç³»åˆ—æ¨¡å‹æœ‰åŸºæœ¬æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´ç‰ˆæœ¬ï¼Œæä¾›å¤šç§å¤§å°å¯é€‰ã€‚æˆ‘ä»¬è¯¦ç»†æŠ¥å‘Šäº†æ¨¡å‹æ¶æ„ã€é¢„è®­ç»ƒè¿‡ç¨‹å’Œå®éªŒï¼Œä»¥æŒ‡å¯¼è®¾è®¡é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬åœ¨ä¿„è¯­å’Œè‹±è¯­åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå¹¶å°†GigaChatä¸å¤šè¯­ç§ç±»ä¼¼æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æœ¬æ–‡å±•ç¤ºäº†é€šè¿‡APIã€Telegramæœºå™¨äººå’ŒWebç•Œé¢è®¿é—®çš„æœ€ä½³æ¨¡å‹çš„ç³»ç»Ÿæ¼”ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å¼€æºå¹³å°ï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/ai-sage%EF%BC%89%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E4%B8%89%E4%B8%AA%E5%BC%80%E6%BA%90GigaChat%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%97%A8%E5%9C%A8%E6%89%A9%E5%A4%A7NLP%E7%A0%94%E7%A9%B6%E6%9C%BA%E4%BC%9A%E5%B9%B6%E6%94%AF%E6%8C%81%E4%BF%84%E8%AF%AD%E5%B7%A5%E4%B8%9A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%BC%80%E5%8F%91%E3%80%82">https://huggingface.co/ai-sageï¼‰ä¸Šå‘å¸ƒäº†ä¸‰ä¸ªå¼€æºGigaChatæ¨¡å‹ï¼Œæ—¨åœ¨æ‰©å¤§NLPç ”ç©¶æœºä¼šå¹¶æ”¯æŒä¿„è¯­å·¥ä¸šè§£å†³æ–¹æ¡ˆçš„å¼€å‘ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09440v1">PDF</a> ACL-2025 System Demo</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¿„è¯­å¼€å‘çš„GigaChatç³»åˆ—å¤§å‹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æ–‡ç« è¯¦ç»†æè¿°äº†æ¨¡å‹æ¶æ„ã€é¢„è®­ç»ƒè¿‡ç¨‹å’Œå®éªŒè®¾è®¡é€‰æ‹©ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨ä¿„è¯­å’Œè‹±è¯­åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†GigaChatæ¨¡å‹çš„é«˜çº§è¡¨ç°å¹¶é€šè¿‡APIã€Telegram botå’ŒWebç•Œé¢è¿›è¡Œæ¼”ç¤ºã€‚æœ¬æ–‡è¿˜å…¬å¸ƒäº†ä¸‰ä¸ªå¼€æºçš„GigaChatæ¨¡å‹ï¼Œæ—¨åœ¨æ‰©å±•NLPç ”ç©¶æœºä¼šå¹¶æ”¯æŒä¿„è¯­å·¥ä¸šè§£å†³æ–¹æ¡ˆçš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GigaChatç³»åˆ—æ˜¯ä¸“é—¨ä¸ºä¿„è¯­è®¾è®¡çš„å¤§å‹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>æ–‡ç« è¯¦ç»†é˜è¿°äº†GigaChatçš„æ¨¡å‹æ¶æ„ã€é¢„è®­ç»ƒè¿‡ç¨‹ä»¥åŠå®éªŒè®¾è®¡é€‰æ‹©ã€‚</li>
<li>GigaChatæ¨¡å‹åœ¨ä¿„è¯­å’Œè‹±è¯­åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>GigaChatæ¨¡å‹å¯ä»¥é€šè¿‡APIã€Telegram botå’ŒWebç•Œé¢è¿›è¡Œè®¿é—®å’Œä½¿ç”¨ã€‚</li>
<li>å…¬å¼€äº†ä¸‰ä¸ªå¼€æºçš„GigaChatæ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒNLPç ”ç©¶åŠä¿„è¯­å·¥ä¸šè§£å†³æ–¹æ¡ˆå¼€å‘ã€‚</li>
<li>GigaChatç³»åˆ—æ¨¡å‹åŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ï¼Œå¯æ»¡è¶³ä¸åŒçš„åº”ç”¨éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dad26832f6837f5775e6aabcc039148f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25cb61d7b2a2c4c1301a14eca2706c37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77baf64d73293ffa4df98181822bb78a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f356203e5be9378287d050034162c1c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2c56501586edf65a3ba6266e2b9652e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56f9c912b05f45496013abaac6cad1af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7555c8e17b7506b46f4d2f86bda9f9e8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-and-Effective-Alignment-of-Large-Language-Models"><a href="#Towards-Efficient-and-Effective-Alignment-of-Large-Language-Models" class="headerlink" title="Towards Efficient and Effective Alignment of Large Language Models"></a>Towards Efficient and Effective Alignment of Large Language Models</h2><p><strong>Authors:Yuxin Jiang</strong></p>
<p>Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMsâ€™ ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current modelsâ€™ constraint adherence, offering insights for future improvements. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆä¸”é«˜æ•ˆåœ°å°†å®ƒä»¬ä¸äººç±»æœŸæœ›å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬è®ºæ–‡é€šè¿‡å¼•å…¥æ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œè¯„ä¼°çš„æ–°æ–¹æ³•ï¼Œæ¨åŠ¨äº†LLMçš„å¯¹é½æ€§ã€‚æˆ‘ä»¬é¦–å…ˆè§£å†³å¯¹é½æ•°æ®çš„æ”¶é›†é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºäººå·¥ç¼–åˆ¶çš„æ•°æ®é›†æˆ–ä¸“æœ‰æ¨¡å‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Lionï¼Œä¸€ä¸ªå¯¹æŠ—è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«å’Œç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤æ¥è¿­ä»£åœ°ä¼˜åŒ–è®­ç»ƒæ•°æ®ï¼Œä»è€Œå®ç°æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†WebRï¼ˆç½‘ç»œé‡å»ºï¼‰ï¼Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç›´æ¥ä»åŸå§‹ç½‘ç»œæ–‡æ¡£åˆæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†æ•°æ®çš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åˆæˆæ•°æ®æ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡æ–°å‹ä¼˜åŒ–æŠ€æœ¯å¢å¼ºå¯¹é½è®­ç»ƒã€‚æˆ‘ä»¬å¼€å‘äº†Learning to Editï¼ˆLTEï¼‰ï¼Œä¸€ä¸ªæ¡†æ¶ï¼Œä½¿LLMèƒ½å¤Ÿé«˜æ•ˆåœ°æ•´åˆæ–°çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™ç°æœ‰ä¿¡æ¯ã€‚LTEåˆ©ç”¨å…ƒå­¦ä¹ æ”¹è¿›å®æ—¶å’Œæ‰¹é‡çŸ¥è¯†æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œäº†æ”¹è¿›ï¼Œå¼•å…¥äº†Bridging and Modeling Correlationsï¼ˆBMCï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ˜¾å¼æ•è·åå¥½æ•°æ®ä¸­çš„ä»¤ç‰Œçº§ç›¸å…³æ€§ï¼Œä»è€Œåœ¨é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°å¯¹é½çš„ä¼˜è¶Šæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è§£å†³äº†å¯¹é½è¯„ä¼°çš„æŒ‘æˆ˜ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¾§é‡äºå“åº”è´¨é‡ï¼Œä½†å¿½è§†äº†ç‰¹å®šçº¦æŸçš„éµå®ˆæƒ…å†µã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FollowBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡ã€ç²¾ç»†çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMéµå¾ªå¤šç§æŒ‡ä»¤ç±»å‹çš„å¤æ‚çº¦æŸçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨çº¦æŸéµå®ˆæ–¹é¢çš„å…³é”®å¼±ç‚¹ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09329v1">PDF</a> PhD thesis</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°ä¸äººç±»æœŸæœ›å¯¹é½ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚æœ¬è®ºæ–‡é€šè¿‡å¼•å…¥æ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œè¯„ä¼°çš„æ–°æ–¹æ³•ï¼Œæ¨è¿›äº†LLMçš„å¯¹é½ç ”ç©¶ã€‚é€šè¿‡è§£å†³æ•°æ®æ”¶é›†é—®é¢˜ï¼Œæå‡ºå¯¹æŠ—è’¸é¦æ¡†æ¶Lionï¼Œèƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–è®­ç»ƒæ•°æ®ï¼Œå®ç°æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä»‹ç»å…¨è‡ªåŠ¨åˆæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®çš„WebRæ¡†æ¶ï¼Œæé«˜äº†æ•°æ®çš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œé€šè¿‡æ–°å‹ä¼˜åŒ–æŠ€æœ¯æé«˜å¯¹é½æ•ˆæœï¼Œå¼€å‘LTEæ¡†æ¶ä½¿LLMèƒ½å¤Ÿé«˜æ•ˆé›†æˆæ–°çŸ¥è¯†åŒæ—¶ä¿ç•™ç°æœ‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ”¹è¿›DPOæ–¹æ³•ï¼Œæå‡ºBMCï¼Œæ›´å¥½åœ°æ•æ‰åå¥½æ•°æ®çš„tokençº§å…³è”ï¼Œåœ¨é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°å¯¹é½çš„ä¼˜è¶Šæ€§ã€‚æœ€åï¼Œé’ˆå¯¹è¯„ä¼°é—®é¢˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å“åº”è´¨é‡ï¼Œå¿½è§†äº†ç‰¹å®šçº¦æŸçš„éµå®ˆã€‚ä¸ºæ­¤ï¼Œå¼•å…¥å¤šçº§åˆ«ç²¾ç»†åŸºå‡†æµ‹è¯•FollowBenchï¼Œè¯„ä¼°LLMéµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œæ­ç¤ºå½“å‰æ¨¡å‹çš„å¼±ç‚¹ï¼Œä¸ºæ”¹è¿›æä¾›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¯¹æŠ—è’¸é¦æ¡†æ¶Lionè§£å†³LLMå¯¹é½ä¸­çš„æ•°æ®æ”¶é›†é—®é¢˜ï¼Œæé«˜é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºWebRæ¡†æ¶ï¼Œä»åŸå§‹ç½‘é¡µæ–‡æ¡£ä¸­ç›´æ¥åˆæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œå¢å¼ºæ•°æ®çš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡æ–°å‹ä¼˜åŒ–æŠ€æœ¯å¢å¼ºLLMçš„å¯¹é½è®­ç»ƒæ•ˆæœï¼Œå¦‚LTEæ¡†æ¶é›†æˆæ–°çŸ¥è¯†å’Œä¿ç•™ç°æœ‰ä¿¡æ¯ã€‚</li>
<li>æ”¹è¿›DPOæ–¹æ³•ï¼Œæå‡ºBMCä»¥æ›´å¥½åœ°æ•æ‰åå¥½æ•°æ®çš„tokençº§å…³è”ï¼Œæå‡QAå’Œæ•°å­¦æ¨ç†ä»»åŠ¡çš„å¯¹é½æ•ˆæœã€‚</li>
<li>å¼ºè°ƒç°æœ‰LLMè¯„ä¼°åŸºå‡†æµ‹è¯•çš„ä¸è¶³ï¼Œéœ€è¦åŒæ—¶å…³æ³¨å“åº”è´¨é‡å’Œç‰¹å®šçº¦æŸçš„éµå®ˆã€‚</li>
<li>å¼•å…¥å¤šçº§åˆ«ç²¾ç»†åŸºå‡†æµ‹è¯•FollowBenchæ¥è¯„ä¼°LLMéµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b65663b1692ba3809ee620897d5329c7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention"><a href="#On-the-Fly-Adaptive-Distillation-of-Transformer-to-Dual-State-Linear-Attention" class="headerlink" title="On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention"></a>On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear   Attention</h2><p><strong>Authors:Yeonju Ro, Zhenyu Zhang, Souvik Kundu, Zhangyang Wang, Aditya Akella</strong></p>
<p>Large language models (LLMs) excel at capturing global token dependencies via self-attention but face prohibitive compute and memory costs on lengthy inputs. While sub-quadratic methods (e.g., linear attention) can reduce these costs, they often degrade accuracy due to overemphasizing recent tokens. In this work, we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel design that maintains two specialized hidden states-one for preserving historical context and one for tracking recency-thereby mitigating the short-range bias typical of linear-attention architectures. To further balance efficiency and accuracy under dynamic workload conditions, we introduce \textbf{\serve}, an online \textit{adaptive distillation} framework that progressively replaces Transformer layers with DSLA layers at inference time, guided by a sensitivity-based layer ordering. \serve\ uses a chained fine-tuning strategy to ensure that each newly converted DSLA layer remains consistent with previously replaced layers, preserving the overall quality. Extensive evaluations on commonsense reasoning, long-context QA, and text summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while retaining comparable performance across downstream tasks. Our ablation studies show that DSLAâ€™s dual states capture both global and local dependencies, addressing the historical-token underrepresentation seen in prior linear attentions. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve">https://github.com/utnslab/DSLA-Serve</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿é€šè¿‡è‡ªæ³¨æ„åŠ›æ•æ‰å…¨å±€ä»¤ç‰Œä¾èµ–å…³ç³»ï¼Œä½†åœ¨å¤„ç†é•¿è¾“å…¥æ—¶é¢ä¸´ç€è®¡ç®—é‡å¤§å’Œå†…å­˜æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚è™½ç„¶æ¬¡äºŒæ¬¡æ–¹æ³•ï¼ˆå¦‚çº¿æ€§æ³¨æ„åŠ›ï¼‰å¯ä»¥é™ä½è¿™äº›æˆæœ¬ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå› ä¸ºè¿‡åº¦å¼ºè°ƒæœ€è¿‘çš„ä»¤ç‰Œè€Œé™ä½å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†\textit{åŒæ€çº¿æ€§æ³¨æ„åŠ›}(\textbf{DSLA})ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®¾è®¡ï¼Œå®ƒç»´æŠ¤äº†ä¸¤ä¸ªä¸“é—¨çš„éšè—çŠ¶æ€â€”â€”ä¸€ä¸ªç”¨äºä¿ç•™å†å²ä¸Šä¸‹æ–‡ï¼Œå¦ä¸€ä¸ªç”¨äºè·Ÿè¸ªè¿‘æœŸä¿¡æ¯â€”â€”ä»è€Œå‡è½»äº†çº¿æ€§æ³¨æ„åŠ›æ¶æ„é€šå¸¸å­˜åœ¨çš„çŸ­ç¨‹åå·®ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜åŠ¨æ€å·¥ä½œè´Ÿè½½æ¡ä»¶ä¸‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textbf{SERVE}â€”â€”ä¸€ç§åœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå®ƒä¼šæ ¹æ®åŸºäºæ•æ„Ÿæ€§çš„å±‚æ’åºé€æ­¥ç”¨DSLAå±‚æ›¿æ¢Transformerå±‚ã€‚SERVEä½¿ç”¨çº§è”å¾®è°ƒç­–ç•¥ç¡®ä¿æ–°è½¬æ¢çš„DSLAå±‚ä¸å…ˆå‰æ›¿æ¢çš„å±‚ä¿æŒä¸€è‡´ï¼Œä¿æŒæ•´ä½“è´¨é‡ã€‚åœ¨å¸¸è¯†æ¨ç†ã€é•¿ä¸Šä¸‹æ–‡é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦æ–¹é¢çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸Llama2-7Bç›¸æ¯”ï¼ŒSERVEæé«˜äº†\textbf{2.3å€}çš„æ¨ç†é€Ÿåº¦ï¼›ä¸æ··åˆæ¨¡å‹Zamba-7Bç›¸æ¯”ï¼Œæé«˜äº†\textbf{3.0å€}ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¿æŒäº†ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼ŒDSLAçš„åŒçŠ¶æ€èƒ½å¤Ÿæ•æ‰å…¨å±€å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œè§£å†³äº†å…ˆå‰çº¿æ€§æ³¨æ„åŠ›ä¸­å‡ºç°çš„å†å²ä»¤ç‰Œæ¬ è¡¨å¾é—®é¢˜ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/utnslab/DSLA-Serveæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09316v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŒçŠ¶æ€çº¿æ€§æ³¨æ„åŠ›ï¼ˆDSLAï¼‰æœºåˆ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾“å…¥æ—¶é¢ä¸´çš„è®¡ç®—ä¸å†…å­˜æˆæœ¬é—®é¢˜ã€‚é€šè¿‡ç»´æŠ¤ä¸¤ä¸ªä¸“é—¨åŒ–çš„éšè—çŠ¶æ€æ¥å¹³è¡¡å†å²ä¸Šä¸‹æ–‡ä¸è¿‘æœŸä¿¡æ¯çš„è¿½è¸ªï¼Œä»è€Œå‡è½»çº¿æ€§æ³¨æ„åŠ›æ¶æ„çš„çŸ­ç¨‹åè§ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶\serveï¼Œæ ¹æ®æ•æ„Ÿå±‚æ’åºåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€æ­¥æ›¿æ¢Transformerå±‚ï¼Œæ—¨åœ¨å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œ\serveåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æ¯”ç°æœ‰æ¨¡å‹æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒçŠ¶æ€çº¿æ€§æ³¨æ„åŠ›ï¼ˆDSLAï¼‰æœºåˆ¶é€šè¿‡ç»´æŠ¤ä¸¤ä¸ªéšè—çŠ¶æ€ï¼Œåˆ†åˆ«ç”¨äºä¿ç•™å†å²ä¸Šä¸‹æ–‡å’Œè¿½è¸ªè¿‘æœŸä¿¡æ¯ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿è¾“å…¥æ—¶çš„è®¡ç®—ä¸å†…å­˜æˆæœ¬é—®é¢˜ã€‚</li>
<li>\serveæ˜¯ä¸€ä¸ªåœ¨çº¿è‡ªé€‚åº”è’¸é¦æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®åŠ¨æ€å·¥ä½œé‡æ¡ä»¶é€æ­¥æ›¿æ¢Transformerå±‚ï¼Œä»¥æé«˜æ•ˆç‡å¹¶ç»´æŒå‡†ç¡®æ€§ã€‚</li>
<li>\serveé€šè¿‡æ•æ„Ÿåº¦å¯¼å‘çš„å±‚åºè¿›è¡Œæ¨ç†ï¼Œé€æ­¥æ›¿æ¢Transformerå±‚ï¼Œå¹¶é‡‡ç”¨é“¾å¼å¾®è°ƒç­–ç•¥ç¡®ä¿æ–°æ›¿æ¢çš„DSLAå±‚ä¸ä¹‹å‰æ›¿æ¢çš„å±‚ä¿æŒä¸€è‡´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œ\serveåœ¨å¸¸è¯†æ¨ç†ã€é•¿æ–‡æœ¬é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦ç­‰å¤šä¸ªä»»åŠ¡ä¸Šä¿æŒäº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æ›´å¿«ã€‚</li>
<li>DSLAæœºåˆ¶èƒ½å¤Ÿæ•æ‰å…¨å±€å’Œå±€éƒ¨ä¾èµ–å…³ç³»ï¼Œè§£å†³äº†å…ˆå‰çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ä¸­å†å²ä»¤ç‰Œè¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æä¾›çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/utnslab/DSLA-Serve%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/utnslab/DSLA-Serveä¸Šè·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4f1de63693593cdef26d41f2fa43aaed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60c67094e0bcd3f621e31431b29f3ded.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90de5cd9ba73d7e8bc828d401a22efc8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unifying-Block-wise-PTQ-and-Distillation-based-QAT-for-Progressive-Quantization-toward-2-bit-Instruction-Tuned-LLMs"><a href="#Unifying-Block-wise-PTQ-and-Distillation-based-QAT-for-Progressive-Quantization-toward-2-bit-Instruction-Tuned-LLMs" class="headerlink" title="Unifying Block-wise PTQ and Distillation-based QAT for Progressive   Quantization toward 2-bit Instruction-Tuned LLMs"></a>Unifying Block-wise PTQ and Distillation-based QAT for Progressive   Quantization toward 2-bit Instruction-Tuned LLMs</h2><p><strong>Authors:Jung Hyun Lee, Seungjae Shin, Vinnam Kim, Jaeseong You, An Chen</strong></p>
<p>As the rapid scaling of large language models (LLMs) poses significant challenges for deployment on resource-constrained devices, there is growing interest in extremely low-bit quantization, such as 2-bit. Although prior works have shown that 2-bit large models are pareto-optimal over their 4-bit smaller counterparts in both accuracy and latency, these advancements have been limited to pre-trained LLMs and have not yet been extended to instruction-tuned models. To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2) that unifies block-wise post-training quantization (PTQ) with distillation-based quantization-aware training (Distill-QAT) for INT2 instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned models to INT4 using block-wise PTQ to significantly reduce the quantization error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT to enable INT2 instruction-tuned LLMs to generate responses consistent with their original FP16 counterparts by minimizing the generalized Jensen-Shannon divergence (JSD) between the two. To the best of our knowledge, we are the first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs to INT2 without relying on proprietary post-training data, while achieving state-of-the-art performances on MMLU and IFEval$-$two of the most representative benchmarks for evaluating instruction-tuned LLMs. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ‰©å±•å¯¹èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²å¸¦æ¥é‡å¤§æŒ‘æˆ˜ï¼Œäººä»¬å¯¹æä½æ¯”ç‰¹é‡åŒ–ï¼ˆå¦‚2æ¯”ç‰¹ï¼‰çš„å…´è¶£æ—¥ç›Šå¢é•¿ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²ç»è¡¨æ˜ï¼Œåœ¨å‡†ç¡®åº¦å’Œå»¶è¿Ÿæ–¹é¢ï¼Œ2æ¯”ç‰¹å¤§å‹æ¨¡å‹æ¯”å…¶4æ¯”ç‰¹å°å‹æ¨¡å‹å…·æœ‰å¸•ç´¯æ‰˜æœ€ä¼˜æ€§èƒ½ï¼Œä½†è¿™äº›è¿›å±•ä»…é™äºé¢„è®­ç»ƒLLMï¼Œå°šæœªæ‰©å±•åˆ°æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€æ¸è¿›é‡åŒ–ï¼ˆUPQï¼‰â€”â€”ä¸€ç§æ–°é¢–çš„æ¸è¿›é‡åŒ–æ¡†æ¶ï¼ˆFP16â†’INT4â†’INT2ï¼‰ï¼Œå®ƒå°†å—çº§åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä¸åŸºäºè’¸é¦çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆDistill-QATï¼‰ç›¸ç»“åˆï¼Œç”¨äºINT2æŒ‡ä»¤è°ƒä¼˜LLMé‡åŒ–ã€‚UPQé¦–å…ˆä½¿ç”¨å—çº§PTQå°†FP16æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹é‡åŒ–ä¸ºINT4ï¼Œä»¥æ˜¾è‘—å‡å°‘åç»­INT2é‡åŒ–å¼•å…¥çš„é‡åŒ–è¯¯å·®ã€‚æ¥ä¸‹æ¥ï¼ŒUPQåº”ç”¨Distill-QATï¼Œä½¿INT2æŒ‡ä»¤è°ƒä¼˜LLMç”Ÿæˆçš„å“åº”ä¸å…¶åŸå§‹FP16ç‰ˆæœ¬ä¸€è‡´ï¼Œé€šè¿‡æœ€å°åŒ–ä¸¤è€…ä¹‹é—´çš„å¹¿ä¹‰Jensen-Shannonæ•£åº¦ï¼ˆJSDï¼‰æ¥å®ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬é¦–æ¬¡è¯æ˜UPQå¯ä»¥åœ¨ä¸ä¾èµ–ä¸“æœ‰åè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å°†å¼€æºæŒ‡ä»¤è°ƒä¼˜LLMé‡åŒ–ä¸ºINT2ï¼ŒåŒæ—¶åœ¨MMLUå’ŒIFEvalâ€”â€”ä¸¤ä¸ªè¯„ä¼°æŒ‡ä»¤è°ƒä¼˜LLMçš„æœ€å…·ä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09104v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>     éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ‰©å±•ï¼Œåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œéƒ¨ç½²é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å¯¹äºæä½ä½é‡åŒ–ï¼ˆå¦‚2ä½ï¼‰çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¸è¿›é‡åŒ–æ¡†æ¶â€”â€”ç»Ÿä¸€æ¸è¿›é‡åŒ–ï¼ˆUPQï¼‰ï¼Œå®ƒå°†å—çº§åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä¸åŸºäºè’¸é¦çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆDistill-QATï¼‰ç»“åˆèµ·æ¥ï¼Œç”¨äºINT2æŒ‡ä»¤è°ƒä¼˜çš„LLMé‡åŒ–ã€‚UPQä½¿ç”¨FP16æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è¿›è¡Œå—çº§PTQé‡åŒ–ï¼Œä»¥å‡å°‘åç»­INT2é‡åŒ–çš„é‡åŒ–è¯¯å·®ï¼Œç„¶åé€šè¿‡åº”ç”¨Distill-QATå®ç°INT2æŒ‡ä»¤è°ƒä¼˜çš„LLMç”Ÿæˆå“åº”ä¸å…¶åŸå§‹çš„FP16æ¨¡å‹å“åº”çš„ä¸€è‡´æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒUPQå¯åœ¨ä¸ä¾èµ–ä¸“æœ‰åè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°†å¼€æºæŒ‡ä»¤è°ƒä¼˜çš„LLMé‡åŒ–è‡³INT2ä½ï¼ŒåŒæ—¶åœ¨MMLUå’ŒIFevalè¿™ä¸¤ä¸ªæœ€å…·ä»£è¡¨æ€§çš„æŒ‡ä»¤è°ƒä¼˜LLMè¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šçš„éƒ¨ç½²å…·æœ‰æŒ‘æˆ˜ï¼Œæä½ä½é‡åŒ–ï¼ˆå¦‚2ä½ï¼‰æˆä¸ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ¸è¿›é‡åŒ–æ¡†æ¶â€”â€”ç»Ÿä¸€æ¸è¿›é‡åŒ–ï¼ˆUPQï¼‰ã€‚</li>
<li>UPQç»“åˆäº†å—çº§åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰ä¸åŸºäºè’¸é¦çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆDistill-QATï¼‰ã€‚</li>
<li>UPQé¦–å…ˆä½¿ç”¨FP16æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è¿›è¡Œå—çº§PTQé‡åŒ–ï¼Œä»¥å‡å°‘åç»­INT2é‡åŒ–çš„è¯¯å·®ã€‚</li>
<li>Distill-QATä½¿INT2æŒ‡ä»¤è°ƒä¼˜çš„LLMç”Ÿæˆçš„å“åº”ä¸åŸå§‹çš„FP16æ¨¡å‹å“åº”ä¸€è‡´ã€‚</li>
<li>UPQåœ¨ä¸ä¾èµ–ä¸“æœ‰åè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸå°†å¼€æºæŒ‡ä»¤è°ƒä¼˜çš„LLMé‡åŒ–è‡³INT2ä½ã€‚</li>
<li>åœ¨æœ€å…·ä»£è¡¨æ€§çš„æŒ‡ä»¤è°ƒä¼˜LLMè¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUPQå–å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81c09811d5d41c2f7f4d7406a243b33a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1760e21abe95f9e26870d09531fb7f8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94f164007bbfba084ebbd3f0ab8eee2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac59547714dab4d29705b06f660d737d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f1ce8d38c208ec928f5619c48abd93d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CAD-Llama-Leveraging-Large-Language-Models-for-Computer-Aided-Design-Parametric-3D-Model-Generation"><a href="#CAD-Llama-Leveraging-Large-Language-Models-for-Computer-Aided-Design-Parametric-3D-Model-Generation" class="headerlink" title="CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation"></a>CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation</h2><p><strong>Authors:Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou</strong></p>
<p>Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæ¿€å‘äº†äººä»¬å°†å…¶ç”Ÿæˆèƒ½åŠ›æ‰©å±•åˆ°é€šç”¨æ–‡æœ¬ä¹‹å¤–çš„ç‰¹å®šé¢†åŸŸçš„å…´è¶£ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨LLMä¸ºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹ç”Ÿæˆå‚æ•°åºåˆ—ã€‚è¿™ä¸€åŠªåŠ›æ˜¯æœç€ä½¿ç”¨LLMåˆ›å»ºå‚æ•°åŒ–3Då½¢çŠ¶è¿ˆå‡ºçš„åˆæ­¥ä¸€æ­¥ï¼Œå› ä¸ºCADæ¨¡å‹å‚æ•°ç›´æ¥ä¸ä¸‰ç»´ç©ºé—´ä¸­çš„å½¢çŠ¶ç›¸å…³è”ã€‚å°½ç®¡LLMå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¿™ä¸€ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå¹¶æœªé‡åˆ°å‚æ•°åºåˆ—ï¼Œä¹Ÿä¸ç›´æ¥äº†è§£ä¸‰ç»´ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CAD-Llamaæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒçš„LLMç”Ÿæˆå‚æ•°åŒ–ä¸‰ç»´CADæ¨¡å‹çš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ†å±‚æ³¨é‡Šç®¡é“å’Œä¸€ç§ç±»ä¼¼äºä»£ç çš„æ ¼å¼ï¼Œå°†å‚æ•°åŒ–çš„ä¸‰ç»´CADå‘½ä»¤åºåˆ—ç¿»è¯‘æˆç»“æ„åŒ–å‚æ•°åŒ–CADä»£ç ï¼ˆSPCCï¼‰ï¼Œå¹¶èå…¥åˆ†å±‚è¯­ä¹‰æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨SPCCçš„è‡ªé€‚åº”é¢„è®­ç»ƒæ–¹æ³•ï¼Œéšåæ˜¯ç¬¦åˆCADç‰¹å®šæŒ‡å—çš„æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•æ—¨åœ¨è®©LLMå…·å¤‡å‚æ•°åºåˆ—ä¸­çš„ç©ºé—´çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå…ˆå‰çš„è‡ªå›å½’æ–¹æ³•å’Œç°æœ‰çš„LLMåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04481v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé€šç”¨æ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå¹¶æ­£é€æ¸å‘ç‰¹å®šé¢†åŸŸæ‰©å±•å…¶ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨LLMç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„å‚æ•°åºåˆ—ã€‚æ­¤ç ”ç©¶ä¸ºåˆ©ç”¨LLMåˆ›å»ºå‚æ•°åŒ–ä¸‰ç»´å½¢çŠ¶è¿ˆå‡ºäº†åˆæ­¥çš„ä¸€æ­¥ï¼ŒCADæ¨¡å‹å‚æ•°ä¸ä¸‰ç»´ç©ºé—´ä¸­çš„å½¢çŠ¶ç›´æ¥ç›¸å…³ã€‚å°½ç®¡LLMå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†ç”Ÿæˆå‚æ•°åºåˆ—çš„ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µå¹¶æœªé‡åˆ°å‚æ•°åºåˆ—ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹ä¸‰ç»´ç»“æ„çš„ç›´æ¥äº†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-Llamaæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºé¢„è®­ç»ƒçš„LLMç”Ÿæˆå‚æ•°åŒ–ä¸‰ç»´CADæ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¼€å‘å±‚æ¬¡åŒ–æ³¨é‡Šç®¡é“å’Œç±»ä¼¼ä»£ç çš„æ ¼å¼ï¼Œå°†å‚æ•°åŒ–ä¸‰ç»´CADå‘½ä»¤åºåˆ—ç¿»è¯‘æˆç»“æ„åŒ–å‚æ•°åŒ–CADä»£ç ï¼ˆSPCCï¼‰ï¼Œå¹¶ç»“åˆå±‚æ¬¡è¯­ä¹‰æè¿°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨SPCCçš„é€‚åº”æ€§é¢„è®­ç»ƒæ–¹æ³•å’Œä¸CADç‰¹å®šæŒ‡å—å¯¹é½çš„æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ã€‚æ­¤æ–¹æ³•æ—¨åœ¨èµ‹äºˆLLMå‚æ•°åºåˆ—ä¸­çš„ç©ºé—´çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—ä¼˜äºå…ˆå‰çš„è‡ªåŠ¨å›å½’æ–¹æ³•å’Œç°æœ‰çš„LLMåŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆé¢†åŸŸç‰¹å®šå†…å®¹ï¼ˆå¦‚è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„å‚æ•°åºåˆ—ï¼‰æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>CAD-Llamaæ¡†æ¶æ—¨åœ¨å¢å¼ºLLMåœ¨ç”Ÿæˆå‚æ•°åŒ–ä¸‰ç»´CADæ¨¡å‹æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å±‚æ¬¡åŒ–æ³¨é‡Šç®¡é“å’Œç±»ä¼¼ä»£ç çš„æ ¼å¼ï¼Œå°†ä¸‰ç»´CADæ¨¡å‹çš„å‚æ•°åºåˆ—è½¬åŒ–ä¸ºç»“æ„åŒ–å‚æ•°åŒ–CADä»£ç ï¼ˆSPCCï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€‚åº”æ€§é¢„è®­ç»ƒæ–¹æ³•å’ŒæŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ï¼Œä»¥èµ‹äºˆLLMç©ºé—´çŸ¥è¯†å’Œå¯¹CADç‰¹å®šæŒ‡å—çš„ç†è§£ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAD-Llamaæ¡†æ¶åœ¨ç”Ÿæˆå‚æ•°åŒ–ä¸‰ç»´CADæ¨¡å‹æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>LLMåœ¨ç”Ÿæˆå‚æ•°åºåˆ—æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å¯¹ä¸‰ç»´ç»“æ„çš„äº†è§£ä¸è¶³å’Œåœ¨é¢„è®­ç»ƒé˜¶æ®µæœªæ¥è§¦å‚æ•°åºåˆ—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f11abfccedb357e72deb330c4e9da804.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-684434d743fe23e809756fcf3763a8ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-579e6bb792154690ebea265c8ed18a62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab3d1d82c87db6797182ab2329750f98.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs"><a href="#Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs" class="headerlink" title="Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs"></a>Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs</h2><p><strong>Authors:Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco</strong></p>
<p>This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81% in the best-case scenario. </p>
<blockquote>
<p>æœ¬æ–‡è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨ç»“æ„åŒ–çš„è¯­è¨€è¡¨ç¤ºå½¢å¼ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„å¯¹çŸ­é•¿å’Œä¸¤ç§ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç å¯¹ä¸€ç³»åˆ—è¯­è¨€ä»»åŠ¡çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨8ä½é‡åŒ–å’ŒæŒ‡ä»¤è°ƒæ•´çš„Llama 3.1ï¼ˆ8Bï¼‰ã€Phi-3å’ŒMistral 7Bç‰ˆæœ¬è¿›è¡Œåˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ¶‰åŠçŸ­ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼Œé€šè¿‡æç¤ºå¢å¼ºåŸå§‹è¯­è¨€ä¸Šä¸‹æ–‡çš„AMRå¾€å¾€ä¼šé™ä½åŸºç¡€LLMçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºæ¶‰åŠé•¿ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼Œå¦‚åœ¨SAMSumæ•°æ®é›†ä¸­çš„å¯¹è¯æ‘˜è¦ï¼Œè¿™ç§å¢å¼ºä¼šæé«˜LLMçš„æ€§èƒ½ï¼Œä¾‹å¦‚ï¼Œå°†Llama 3.1çš„é›¶æ ·æœ¬ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ä»66%æé«˜åˆ°76%ã€‚è¿™ç§æ”¹è¿›åœ¨æ›´æ–°å’Œæ›´å¤§çš„LLMä¸­æ›´ä¸ºæ˜æ˜¾ï¼Œä½†å¹¶ä¸é€‚ç”¨äºæ—§æˆ–è¾ƒå°çš„LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LLMå¯ä»¥æœ‰æ•ˆåœ°ä»çº¿æ€§åŒ–çš„AMRä¸­é‡å»ºåŸå§‹æ–‡æœ¬ï¼Œåœ¨æœ€ä½³æƒ…å†µä¸‹è¾¾åˆ°81%çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04745v3">PDF</a> 13 pages, 23 figures. Accepted at XLLM @ ACL 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ©ç”¨ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºå½¢å¼è¿›è¡Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„ç¼–ç çŸ­é•¿å’Œä¸åŒè¯­å¢ƒï¼Œå¯¹ä¸€ç³»åˆ—è¯­è¨€ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ã€‚åˆ†æè¡¨æ˜ï¼Œå¯¹äºçŸ­è¯­å¢ƒä»»åŠ¡ï¼Œæ·»åŠ åŸå§‹è¯­å¢ƒçš„AMRå¾€å¾€ä¼šé™ä½LLMæ€§èƒ½ï¼›è€Œå¯¹äºé•¿è¯­å¢ƒä»»åŠ¡ï¼Œå¦‚å¯¹è¯æ‘˜è¦ï¼Œè¿™ç§å¢å¼ºåˆ™èƒ½æé«˜LLMæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°ä¸”å¤§å‹çš„LLMä¸­è¡¨ç°æ›´ä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒLLMè¿˜èƒ½æœ‰æ•ˆåœ°ä»çº¿æ€§åŒ–çš„AMRä¸­é‡å»ºåŸå§‹æ–‡æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåˆ©ç”¨æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„è¿›è¡Œä¸Šä¸‹æ–‡ä¿¡æ¯ç¼–ç ã€‚</li>
<li>åœ¨çŸ­è¯­å¢ƒä»»åŠ¡ä¸­ï¼Œæ·»åŠ AMRä¼šé™ä½LLMæ€§èƒ½ã€‚</li>
<li>å¯¹äºé•¿è¯­å¢ƒä»»åŠ¡ï¼Œå¦‚å¯¹è¯æ‘˜è¦ï¼Œä½¿ç”¨AMRç»“æ„å¢å¼ºèƒ½æé«˜LLMæ€§èƒ½ã€‚</li>
<li>æ–°ä¸”å¤§å‹çš„LLMåœ¨åˆ©ç”¨AMRç»“æ„æ–¹é¢çš„æ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ã€‚</li>
<li>LLMèƒ½ä»çº¿æ€§åŒ–çš„AMRæœ‰æ•ˆåœ°é‡å»ºåŸå§‹æ–‡æœ¬ã€‚</li>
<li>LLMåœ¨å¤„ç†è¯­å¢ƒä¿¡æ¯æ—¶ï¼Œä¸åŒè¯­å¢ƒï¼ˆçŸ­æˆ–é•¿ï¼‰éœ€è¦ä¸åŒçš„å¤„ç†ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-916744d3207705176c15600842072da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2876cad9086bbf6e797d1ec62315f518.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e910886f66f98fb042a4a850007dfe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b038b015a028b0bad607c72cf974316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6030c7386ed715da1172957f4e455f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0625426b1f81ffa2a742d52923fa0f9d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models"><a href="#ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models" class="headerlink" title="ASIDE: Architectural Separation of Instructions and Data in Language   Models"></a>ASIDE: Architectural Separation of Instructions and Data in Language   Models</h2><p><strong>Authors:Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Alexandra Volkova, Soroush Tabesh, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert</strong></p>
<p>Despite their remarkable performance, large language models lack elementary safety features, making them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause of the success of prompt injection attacks. In this work, we propose a new architectural element, ASIDE, that allows language models to clearly separate instructions and data at the level of embeddings. ASIDE applies an orthogonal rotation to the embeddings of data tokens, thus creating clearly distinct representations of instructions and data tokens without introducing any additional parameters. As we demonstrate experimentally across a range of models, instruction-tuning LLMs with ASIDE (1) leads to highly increased instruction-data separation without a loss in model utility and (2) makes the models more robust to prompt injection benchmarks, even without dedicated safety training. Additionally, we provide insights into the mechanism underlying our method through an analysis of the model representations. The source code and training scripts are openly accessible at <a target="_blank" rel="noopener" href="https://github.com/egozverev/aside">https://github.com/egozverev/aside</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹åŸºæœ¬çš„å®‰å…¨åŠŸèƒ½ï¼Œä½¿å…¶å®¹æ˜“å—åˆ°ä¼—å¤šæ¶æ„æ”»å‡»ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…ˆå‰çš„å·¥ä½œå·²ç»ç¡®å®šæŒ‡ä»¤å’Œæ•°æ®ä¹‹é—´ç¼ºä¹å†…åœ¨åˆ†ç¦»æ˜¯æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸçš„æ ¹æœ¬åŸå› ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„å…ƒç´ ASIDEï¼Œå®ƒå…è®¸è¯­è¨€æ¨¡å‹åœ¨åµŒå…¥å±‚é¢ä¸Šæ¸…æ™°åœ°åˆ†ç¦»æŒ‡ä»¤å’Œæ•°æ®ã€‚ASIDEé€šè¿‡å¯¹æ•°æ®åµŒå…¥è¿›è¡Œæ­£äº¤æ—‹è½¬ï¼Œä»è€Œåˆ›å»ºæŒ‡ä»¤å’Œæ•°æ®çš„æ¸…æ™°è¡¨ç¤ºï¼Œä¸”æ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ASIDEè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆ1ï¼‰èƒ½å¤Ÿåœ¨ä¸æŸå¤±æ¨¡å‹æ•ˆç”¨çš„å‰æä¸‹ï¼Œå®ç°é«˜åº¦å¢å¼ºçš„æŒ‡ä»¤æ•°æ®åˆ†ç¦»ï¼›ï¼ˆ2ï¼‰å³ä½¿ä¸ç»è¿‡ä¸“é—¨çš„å®‰å…¨è®­ç»ƒï¼Œä¹Ÿèƒ½ä½¿æ¨¡å‹å¯¹æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•æ›´å…·é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡å¯¹æ¨¡å‹è¡¨ç¤ºçš„åˆ†æï¼Œæ·±å…¥äº†è§£äº†æˆ‘ä»¬çš„æ–¹æ³•èƒŒåçš„æœºåˆ¶ã€‚æºä»£ç å’ŒåŸ¹è®­è„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/egozverev/aside">https://github.com/egozverev/aside</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10566v3">PDF</a> Preliminary version accepted to ICLR 2025 Workshop on Building Trust   in Language Models and Applications</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç¼ºä¹åŸºæœ¬çš„å®‰å…¨ç‰¹æ€§ï¼Œå®¹æ˜“å—åˆ°å¤šç§æ¶æ„æ”»å‡»ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¶æ„å…ƒç´ ASIDEï¼Œèƒ½åœ¨åµŒå…¥å±‚é¢å°†æŒ‡ä»¤å’Œæ•°æ®æ˜ç¡®åˆ†ç¦»ã€‚é€šè¿‡åº”ç”¨æ•°æ®æ ‡è®°åµŒå…¥çš„æ­£äº¤æ—‹è½¬ï¼Œåˆ›é€ å‡ºæŒ‡ä»¤å’Œæ•°æ®çš„æ¸…æ™°ä¸åŒè¡¨ç¤ºï¼Œæ— éœ€å¼•å…¥é¢å¤–å‚æ•°ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é‡‡ç”¨ASIDEæŒ‡ä»¤è°ƒä¼˜ï¼Œä¸ä»…æé«˜äº†æŒ‡ä»¤ä¸æ•°æ®çš„åˆ†ç¦»åº¦ï¼Œä¸”æå‡äº†æ¨¡å‹å¯¹æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•çš„é²æ£’æ€§ï¼Œå³ä½¿æ²¡æœ‰ä¸“é—¨çš„å®‰å…¨è®­ç»ƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨å®‰å…¨æ¼æ´ï¼Œæ˜“å—åˆ°æ¶æ„æ”»å‡»ã€‚</li>
<li>ç¼ºä¹æŒ‡ä»¤å’Œæ•°æ®çš„å†…åœ¨åˆ†ç¦»æ˜¯æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸçš„åŸå› ã€‚</li>
<li>ASIDEæ¶æ„èƒ½å¤Ÿåœ¨åµŒå…¥å±‚é¢åˆ†ç¦»æŒ‡ä»¤å’Œæ•°æ®ã€‚</li>
<li>ASIDEé€šè¿‡æ­£äº¤æ—‹è½¬æ•°æ®æ ‡è®°åµŒå…¥æ¥åˆ›å»ºæŒ‡ä»¤å’Œæ•°æ®çš„ä¸åŒè¡¨ç¤ºã€‚</li>
<li>ASIDEæé«˜äº†æŒ‡ä»¤ä¸æ•°æ®çš„åˆ†ç¦»åº¦ï¼ŒåŒæ—¶ä¸æŸå¤±æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>é‡‡ç”¨ASIDEçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•æ›´å…·é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ca0f72a9e2a91688ecce278077eeb8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbacb3fe22c862b791d355cc584a5e12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c693c649d0a82d40cb0553d51cb884e2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Peri-LN-Revisiting-Normalization-Layer-in-the-Transformer-Architecture"><a href="#Peri-LN-Revisiting-Normalization-Layer-in-the-Transformer-Architecture" class="headerlink" title="Peri-LN: Revisiting Normalization Layer in the Transformer Architecture"></a>Peri-LN: Revisiting Normalization Layer in the Transformer Architecture</h2><p><strong>Authors:Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo</strong></p>
<p>Selecting a layer normalization (LN) strategy that stabilizes training and speeds convergence in Transformers remains difficult, even for todayâ€™s large language models (LLM). We present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformers. Until recently, Pre-LN and Post-LN have long dominated practices despite their limitations in large-scale training. However, several open-source models have recently begun silently adopting a third strategy without much explanation. This strategy places normalization layer peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis delineates the distinct behaviors of LN strategies, showing how each placement shapes activation variance and gradient propagation. To validate our theoretical insight, we conduct extensive experiments on Transformers up to $3.2$B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement of LN. </p>
<blockquote>
<p>é€‰æ‹©ä¸€ç§å±‚å½’ä¸€åŒ–ï¼ˆLNï¼‰ç­–ç•¥ï¼Œä»¥ç¨³å®šè®­ç»ƒå¹¶åŠ é€Ÿè½¬æ¢å™¨ä¸­çš„æ”¶æ•›ï¼Œå³ä½¿åœ¨ä»Šå¤©çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æä¾›äº†å…¨é¢çš„åˆ†æåŸºç¡€ï¼Œä»¥äº†è§£ä¸åŒçš„LNç­–ç•¥å¦‚ä½•å½±å“å¤§è§„æ¨¡è½¬æ¢å™¨çš„è®­ç»ƒåŠ¨æ€ã€‚ç›´åˆ°æœ€è¿‘ï¼Œå°½ç®¡åœ¨å¤§å‹è®­ç»ƒä¸­å­˜åœ¨å±€é™æ€§ï¼Œä½†Pre-LNå’ŒPost-LNçš„å®è·µä¸€ç›´å æ®ä¸»å¯¼åœ°ä½ã€‚ç„¶è€Œï¼Œæœ€è¿‘æœ‰å‡ ä¸ªå¼€æºæ¨¡å‹å¼€å§‹é»˜é»˜åœ°é‡‡ç”¨ç¬¬ä¸‰ç§ç­–ç•¥ï¼Œä½†æ²¡æœ‰å¤ªå¤šè§£é‡Šã€‚è¯¥ç­–ç•¥å°†å½’ä¸€åŒ–å±‚æ”¾ç½®åœ¨å­å±‚å‘¨å›´ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºPeri-LNã€‚è™½ç„¶Peri-LNå·²æ˜¾ç¤ºå‡ºæœ‰å‰é€”çš„æ€§èƒ½ï¼Œä½†å…¶ç²¾ç¡®æœºåˆ¶å’Œå¥½å¤„å‡ ä¹å°šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†æé˜æ˜äº†LNç­–ç•¥çš„ä¸åŒè¡Œä¸ºï¼Œå±•ç¤ºäº†æ¯ç§æ”¾ç½®æ–¹å¼å¦‚ä½•å½±å“æ¿€æ´»æ–¹å·®å’Œæ¢¯åº¦ä¼ æ’­ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„ç†è®ºæ´å¯ŸåŠ›ï¼Œæˆ‘ä»¬åœ¨é«˜è¾¾3.2Bå‚æ•°çš„è½¬æ¢å™¨ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœæ˜¾ç¤ºPeri-LNå§‹ç»ˆå®ç°äº†æ›´å¹³è¡¡çš„æ–¹å·®å¢é•¿ã€æ›´ç¨³å®šçš„æ¢¯åº¦æµåŠ¨å’Œæ”¶æ•›ç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯¹äºå¤§è§„æ¨¡è½¬æ¢å™¨æ¶æ„ï¼ŒPeri-LNå€¼å¾—æ›´å¹¿æ³›çš„è€ƒè™‘ï¼Œä¸ºLNçš„æœ€ä½³æ”¾ç½®ä½ç½®æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02732v3">PDF</a> ICML2025 Camera-ready version</p>
<p><strong>Summary</strong></p>
<p>ä¸åŒå±‚å½’ä¸€åŒ–ï¼ˆLNï¼‰ç­–ç•¥å¯¹å¤§å‹Transformerè®­ç»ƒçš„å½±å“ä»ç„¶éš¾ä»¥ç¡®å®šï¼Œå³ä½¿å¯¹äºç°ä»Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç ”ç©¶è€…ä»¬æä¾›äº†æ·±å…¥ç†è§£ä¸åŒLNç­–ç•¥å¦‚ä½•å½±å“å¤§è§„æ¨¡Transformerè®­ç»ƒåŠ¨åŠ›çš„ç»¼åˆåˆ†æåŸºç¡€ã€‚ç›´åˆ°æœ€è¿‘ï¼ŒPre-LNå’ŒPost-LNä¸€ç›´ä¸»å¯¼ç€å®è·µï¼Œå°½ç®¡å®ƒä»¬åœ¨å¤§å‹è®­ç»ƒä¸­å­˜åœ¨å±€é™æ€§ã€‚ç„¶è€Œï¼Œä¸€äº›å¼€æºæ¨¡å‹æœ€è¿‘å¼€å§‹æ‚„æ‚„åœ°é‡‡ç”¨ç¬¬ä¸‰ç§ç­–ç•¥ï¼Œå³å‘¨è¾¹å±‚å½’ä¸€åŒ–ï¼ˆPeri-LNï¼‰ï¼Œä½†å¾ˆå°‘è§£é‡Šã€‚Peri-LNå°†å½’ä¸€åŒ–å±‚ç½®äºå­å±‚å‘¨å›´ï¼Œè™½ç„¶å·²æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å…¶ç²¾ç¡®æœºåˆ¶å’Œå¥½å¤„å‡ ä¹å°šæœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡æ·±å…¥åˆ†æå‹¾å‹’å‡ºLNç­–ç•¥çš„ä¸åŒè¡Œä¸ºï¼Œå±•ç¤ºæ¯ç§æ”¾ç½®æ–¹å¼å¦‚ä½•å½±å“æ¿€æ´»æ–¹å·®å’Œæ¢¯åº¦ä¼ æ’­ã€‚ä¸ºäº†éªŒè¯ç†è®ºè§è§£ï¼Œæˆ‘ä»¬å¯¹é«˜è¾¾3.2Bå‚æ•°çš„Transformerè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœæ˜¾ç¤ºPeri-LNåœ¨æ–¹å·®å¢é•¿ã€æ¢¯åº¦æµåŠ¨çš„ç¨³å®šæ€§å’Œæ”¶æ•›ç¨³å®šæ€§æ–¹é¢è¡¨ç°æ›´ç¨³å®šã€‚æˆ‘ä»¬çš„ç»“æœå»ºè®®ï¼Œå¯¹äºå¤§è§„æ¨¡Transformeræ¶æ„ï¼Œåº”æ›´å¹¿æ³›åœ°è€ƒè™‘Peri-LNï¼Œå¹¶ä¸ºLNçš„æœ€ä½³æ”¾ç½®ä½ç½®æä¾›æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸åŒå±‚å½’ä¸€åŒ–ï¼ˆLNï¼‰ç­–ç•¥åœ¨å¤§å‹Transformeræ¨¡å‹è®­ç»ƒä¸­å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>Pre-LNå’ŒPost-LNæ˜¯å®è·µä¸­å¸¸è§çš„ä¸¤ç§ç­–ç•¥ï¼Œä½†åœ¨å¤§å‹è®­ç»ƒä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ€è¿‘å¼€æºæ¨¡å‹å¼€å§‹é‡‡ç”¨ä¸€ç§æ–°å…´çš„å‘¨è¾¹å±‚å½’ä¸€åŒ–ï¼ˆPeri-LNï¼‰ç­–ç•¥ï¼Œä½†ç¼ºä¹è§£é‡Šã€‚</li>
<li>Peri-LNé€šè¿‡å°†å½’ä¸€åŒ–å±‚ç½®äºå­å±‚å‘¨å›´è¿›è¡Œè®¾è®¡ï¼Œæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ã€‚</li>
<li>LNç­–ç•¥çš„ä¸åŒæ”¾ç½®æ–¹å¼ä¼šå½±å“æ¿€æ´»æ–¹å·®å’Œæ¢¯åº¦ä¼ æ’­ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPeri-LNåœ¨æ–¹å·®å¢é•¿ã€æ¢¯åº¦ç¨³å®šæ€§åŠæ¨¡å‹æ”¶æ•›ç­‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29ecad38e926b3737199a0fd0d3139a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fc23775807db4c1242fe2af5e7e7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83578fa48ebfe502b69a5c8b36f28cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2da047c3f48a807d7cbd05b6a26b36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0476bbacf97b48e423cf44bfa81ec69.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Monet-Mixture-of-Monosemantic-Experts-for-Transformers"><a href="#Monet-Mixture-of-Monosemantic-Experts-for-Transformers" class="headerlink" title="Monet: Mixture of Monosemantic Experts for Transformers"></a>Monet: Mixture of Monosemantic Experts for Transformers</h2><p><strong>Authors:Jungwoo Park, Young Jin Ahn, Kee-Eung Kim, Jaewoo Kang</strong></p>
<p>Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity â€“ where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust model behavior. The source code and pretrained checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Monet">https://github.com/dmis-lab/Monet</a>. </p>
<blockquote>
<p>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è®¡ç®—å¯¹äºå°†å…¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½å¹¶é˜²æ­¢ç”Ÿæˆæœ‰æ¯’å†…å®¹ç­‰ä¸å½“è¡Œä¸ºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤šä¹‰æ€§ï¼ˆå³å•ä¸ªç¥ç»å…ƒå¯¹å¤šä¸ªä¸ç›¸å…³æ¦‚å¿µçš„å“åº”ï¼‰é˜»ç¢äº†æœºæ¢°è§£é‡Šæ€§ã€‚è™½ç„¶ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è¯•å›¾é€šè¿‡ç¨€ç–å­—å…¸å­¦ä¹ æ¥è§£å¼€è¿™äº›ç‰¹å¾ï¼Œä½†å®ƒä»¬ä¾èµ–äºäº‹åé‡å»ºæŸå¤±ï¼Œä»è€ŒæŸå®³äº†LLMçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œæ··åˆå•ä¹‰ä¸“å®¶è½¬æ¢å™¨â€ï¼ˆMonetï¼‰æ¶æ„ï¼Œå®ƒå°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥èå…¥ç«¯åˆ°ç«¯çš„æ··åˆä¸“å®¶é¢„è®­ç»ƒã€‚æˆ‘ä»¬æ–°é¢–çš„ä¸“å®¶åˆ†è§£æ–¹æ³•ä½¿ä¸“å®¶æ•°é‡æŒ‰å±‚å¢åŠ åˆ°æ¯å±‚262,144ä¸ªï¼ŒåŒæ—¶æ€»å‚æ•°æŒ‰ä¸“å®¶æ•°é‡çš„å¹³æ–¹æ ¹æˆæ¯”ä¾‹å¢é•¿ã€‚æˆ‘ä»¬çš„åˆ†æè¯æ˜äº†ä¸“å®¶ä¹‹é—´çŸ¥è¯†çš„ç›¸äº’ç‹¬ç«‹æ€§ï¼Œå¹¶å±•ç¤ºäº†å•ä¸ªä¸“å®¶æ‰€åŒ…å«çš„å‚æ•°çŸ¥è¯†ã€‚æ­¤å¤–ï¼ŒMonetå…è®¸åœ¨é¢†åŸŸã€è¯­è¨€å’Œæ¯’æ€§å‡è½»æ–¹é¢è¿›è¡Œæ“ä½œçŸ¥è¯†ï¼Œè€Œä¸ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹é€æ˜LLMçš„è¿½æ±‚çªæ˜¾äº†å¢åŠ ä¸“å®¶æ•°é‡ä»¥æé«˜æœºæ¢°è§£é‡Šæ€§çš„æ½œåŠ›ï¼Œå¹¶å¯ä»¥ç›´æ¥åˆ‡é™¤å†…éƒ¨çŸ¥è¯†æ¥æ ¹æœ¬æ€§åœ°è°ƒæ•´æ¨¡å‹è¡Œä¸ºã€‚æºä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dmis-lab/Monet%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/dmis-lab/Monetè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04139v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è®¡ç®—ç†è§£å¯¹äºä¸äººç±»ä»·å€¼è§‚å¯¹é½åŠé˜²æ­¢ç”Ÿæˆæœ‰æ¯’å†…å®¹ç­‰ä¸å¯å–è¡Œä¸ºè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå•è¯çš„å¤šä¹‰æ€§ï¼Œå³å•ä¸ªç¥ç»å…ƒä¼šå¯¹å¤šä¸ªä¸ç›¸å…³æ¦‚å¿µä½œå‡ºååº”ï¼Œå¯¼è‡´æœºåˆ¶æ€§è§£é‡Šæ€§å—é˜»ã€‚å°½ç®¡ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è¯•å›¾é€šè¿‡ç¨€ç–å­—å…¸å­¦ä¹ æ¥è§£å†³è¿™äº›ç‰¹å¾çš„åˆ†è§£é—®é¢˜ï¼Œä½†å®ƒä»¬ä¾èµ–äºäº‹åé‡å»ºæŸå¤±ï¼Œä»è€ŒæŸå®³äº†LLMçš„æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Mixture of Monosemantic Experts for Transformersï¼ˆMonetï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„å°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥çº³å…¥ç«¯åˆ°ç«¯çš„ä¸“å®¶æ··åˆé¢„è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–°å‹ä¸“å®¶åˆ†è§£æ–¹æ³•ä½¿æ¯å±‚çš„ä¸“å®¶æ•°é‡èƒ½å¤Ÿæ‰©å±•åˆ°262,144ä¸ªï¼ŒåŒæ—¶æ€»å‚æ•°ä¸ä¸“å®¶æ•°é‡çš„å¹³æ–¹æ ¹æˆæ¯”ä¾‹æ‰©å±•ã€‚åˆ†æè¡¨æ˜ï¼Œå„ä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†å‚¨å¤‡ç›¸äº’ç‹¬ç«‹ï¼Œå±•ç¤ºäº†å•ä¸ªä¸“å®¶æ‰€å°è£…çš„çŸ¥è¯†å‚æ•°ã€‚æ­¤å¤–ï¼ŒMonetèƒ½å¤Ÿåœ¨ä¸é™ä½æ•´ä½“æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ç°è·¨é¢†åŸŸã€è·¨è¯­è¨€çš„çŸ¥è¯†æ“çºµå’Œæ¯’æ€§ç¼“è§£ã€‚æˆ‘ä»¬å¯¹é€æ˜LLMçš„è¿½æ±‚çªæ˜¾äº†å¢åŠ ä¸“å®¶æ•°é‡ä»¥æé«˜æœºåˆ¶è§£é‡Šæ€§çš„æ½œåŠ›ï¼Œå¹¶å¯ä»¥ç›´æ¥è°ƒæ•´å†…éƒ¨çŸ¥è¯†ä»¥æ ¹æœ¬æ”¹å˜æ¨¡å‹è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…éƒ¨è®¡ç®—ç†è§£å¯¹äºä¸äººç±»ä»·å€¼è§‚å¯¹é½åŠé¿å…ä¸è‰¯è¡Œä¸ºè‡³å…³é‡è¦ã€‚</li>
<li>å•è¯çš„å¤šä¹‰æ€§é˜»ç¢äº†å¯¹LLMæœºåˆ¶æ€§è§£é‡Šçš„ç†è§£ã€‚</li>
<li>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è™½ç„¶è¯•å›¾è§£å†³ç‰¹å¾åˆ†è§£é—®é¢˜ï¼Œä½†ä¼šå› ä¸ºä¾èµ–äº‹åé‡å»ºæŸå¤±è€ŒæŸå®³LLMæ€§èƒ½ã€‚</li>
<li>Mixture of Monosemantic Experts for Transformersï¼ˆMonetï¼‰æ¶æ„ç›´æ¥çº³å…¥ç¨€ç–å­—å…¸å­¦ä¹ è‡³ç«¯åˆ°ç«¯çš„ä¸“å®¶æ··åˆé¢„è®­ç»ƒï¼Œä»¥æé«˜æœºåˆ¶è§£é‡Šæ€§ã€‚</li>
<li>Moneté€šè¿‡æ–°å‹ä¸“å®¶åˆ†è§£æ–¹æ³•ä½¿æ¯å±‚çš„ä¸“å®¶æ•°é‡èƒ½å¤Ÿå¤§å¹…æ‰©å±•ã€‚</li>
<li>ä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†å‚¨å¤‡ç›¸äº’ç‹¬ç«‹ï¼Œä¸”å¯ä»¥å®ç°è·¨é¢†åŸŸã€è·¨è¯­è¨€çš„çŸ¥è¯†æ“çºµå’Œæ¯’æ€§ç¼“è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-034ae5bbdf9a74cf98d874c39b2764c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5950e0b2ac2df741f831b6736f45726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89f8b96b14d87014d76a7f04f96333ae.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="pLDDT-Predictor-High-speed-Protein-Screening-Using-Transformer-and-ESM2"><a href="#pLDDT-Predictor-High-speed-Protein-Screening-Using-Transformer-and-ESM2" class="headerlink" title="pLDDT-Predictor: High-speed Protein Screening Using Transformer and ESM2"></a>pLDDT-Predictor: High-speed Protein Screening Using Transformer and ESM2</h2><p><strong>Authors:Joongwon Chae, Zhenyu Wang, Ijaz Gul, Jiansong Ji, Zhenglin Chen, Peiwu Qin</strong></p>
<p>Recent advancements in protein structure prediction, particularly AlphaFold2, have revolutionized structural biology by achieving near-experimental accuracy ($\text{average RMSD} &lt; 1.5\text{\AA}$). However, the computational demands of these models (approximately 30 minutes per protein on an RTX 4090) significantly limit their application in high-throughput protein screening. While large language models like ESM (Evolutionary Scale Modeling) have shown promise in extracting structural information directly from protein sequences, rapid assessment of protein structure quality for large-scale analyses remains a major challenge.   We introduce pLDDT-Predictor, a high-speed protein screening tool that achieves a $250,000\times$ speedup compared to AlphaFold2 by leveraging pre-trained ESM2 protein embeddings and a Transformer architecture. Our model predicts AlphaFold2â€™s pLDDT (predicted Local Distance Difference Test) scores with a Pearson correlation of 0.7891 and processes proteins in just 0.007 seconds on average. Using a comprehensive dataset of 1.5 million diverse protein sequences (ranging from 50 to 2048 amino acids), we demonstrate that pLDDT-Predictor accurately classifies high-confidence structures (pLDDT $&gt;$ 70) with 91.2% accuracy and achieves an MSE of 84.8142 compared to AlphaFold2â€™s predictions.   The source code and pre-trained models are freely available at <a target="_blank" rel="noopener" href="https://github.com/jw-chae/pLDDT_Predictor">https://github.com/jw-chae/pLDDT_Predictor</a>, enabling the research community to perform rapid, large-scale protein structure quality assessments. </p>
<blockquote>
<p>è¿‘æœŸè›‹ç™½è´¨ç»“æ„é¢„æµ‹æ–¹é¢çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯AlphaFold2ï¼Œå·²ç»é€šè¿‡å®ç°æ¥è¿‘å®éªŒç²¾åº¦ï¼ˆå¹³å‡RMSD &lt; 1.5Ã…ï¼‰çš„æ–¹å¼ï¼Œå½»åº•æ”¹å˜äº†ç»“æ„ç”Ÿç‰©å­¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„è®¡ç®—éœ€æ±‚ï¼ˆåœ¨RTX 4090ä¸Šæ¯ä¸ªè›‹ç™½å¤§çº¦éœ€è¦30åˆ†é’Ÿï¼‰æ˜¾è‘—é™åˆ¶äº†å®ƒä»¬åœ¨é«˜é€šé‡è›‹ç™½è´¨ç­›é€‰ä¸­çš„åº”ç”¨ã€‚è™½ç„¶åƒESMï¼ˆè¿›åŒ–è§„æ¨¡å»ºæ¨¡ï¼‰è¿™æ ·çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨ç›´æ¥ä»è›‹ç™½è´¨åºåˆ—ä¸­æå–ç»“æ„ä¿¡æ¯æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ï¼Œä½†å¯¹äºå¤§è§„æ¨¡åˆ†æçš„è›‹ç™½è´¨ç»“æ„è´¨é‡å¿«é€Ÿè¯„ä¼°ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†pLDDT-Predictorï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜é€Ÿè›‹ç™½è´¨ç­›é€‰å·¥å…·ï¼Œå®ƒé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ESM2è›‹ç™½è´¨åµŒå…¥å’ŒTransformeræ¶æ„ï¼Œå®ç°äº†ç›¸å¯¹äºAlphaFold2çš„é€Ÿåº¦æå‡250,000å€ã€‚æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹AlphaFold2çš„pLDDTï¼ˆé¢„æµ‹å±€éƒ¨è·ç¦»å·®å¼‚æµ‹è¯•ï¼‰åˆ†æ•°ï¼ŒPearsonç›¸å…³ç³»æ•°ä¸º0.7891ï¼Œå¹³å‡æ¯ä¸ªè›‹ç™½è´¨çš„å¤„ç†æ—¶é—´ä»…ä¸º0.007ç§’ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«150ä¸‡ä¸ªä¸åŒè›‹ç™½è´¨åºåˆ—çš„ç»¼åˆæ•°æ®é›†ï¼ˆä»50åˆ°2048ä¸ªæ°¨åŸºé…¸ï¼‰ï¼Œè¯æ˜äº†pLDDT-Predictorèƒ½å¤Ÿå‡†ç¡®åˆ†ç±»é«˜å¯ä¿¡åº¦çš„ç»“æ„ï¼ˆpLDDT &gt; 70ï¼‰ï¼Œå‡†ç¡®ç‡ä¸º91.2ï¼…ï¼Œä¸AlphaFold2çš„é¢„æµ‹ç›¸æ¯”ï¼ŒMSEä¸º84.8142ã€‚</p>
<p>æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jw-chae/pLDDT_Predictor%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%BE%97%EF%BC%8C%E4%BD%BF%E7%A0%94%E7%A9%B6%E7%A4%BE%E5%8C%BA%E8%83%BD%E5%A4%9F%E8%BF%9B%E8%A1%8C%E5%BF%AB%E9%80%9F%E7%9A%84%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%E3%80%82">https://github.com/jw-chae/pLDDT_Predictorå…è´¹è·å¾—ï¼Œä½¿ç ”ç©¶ç¤¾åŒºèƒ½å¤Ÿè¿›è¡Œå¿«é€Ÿçš„å¤§è§„æ¨¡è›‹ç™½è´¨ç»“æ„è´¨é‡è¯„ä¼°ã€‚</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21283v3">PDF</a> Further experiments confirmed overfitting, and we are retracting the   paper</p>
<p><strong>æ‘˜è¦</strong><br>    AlphaFold2ç­‰è›‹ç™½è´¨ç»“æ„é¢„æµ‹æŠ€æœ¯çš„æœ€æ–°è¿›å±•å¯¹ç»“æ„ç”Ÿç‰©å­¦äº§ç”Ÿé©å‘½æ€§å½±å“ï¼Œä½†è®¡ç®—éœ€æ±‚å¤§é™åˆ¶äº†å…¶é«˜é€šé‡è›‹ç™½è´¨ç­›é€‰çš„åº”ç”¨ã€‚ESMç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç›´æ¥ä»è›‹ç™½è´¨åºåˆ—ä¸­æå–ç»“æ„ä¿¡æ¯æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¯¹å¤§è§„æ¨¡åˆ†æä¸­è›‹ç™½è´¨ç»“æ„è´¨é‡çš„å¿«é€Ÿè¯„ä¼°ä»æ˜¯æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥pLDDT-Predictorï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ESM2è›‹ç™½è´¨åµŒå…¥å’ŒTransformeræ¶æ„ï¼Œå®ç°äº†ä¸AlphaFold2ç›¸æ¯”é«˜è¾¾$250,000\times$çš„åŠ é€Ÿã€‚è¯¥æ¨¡å‹é¢„æµ‹AlphaFold2çš„pLDDTåˆ†æ•°ä¸çœŸå®å€¼é«˜åº¦ç›¸å…³ï¼Œå¹¶å¯¹è›‹ç™½è´¨è¿›è¡Œå¿«é€Ÿå¤„ç†ã€‚ä½¿ç”¨åŒ…å«150ä¸‡å¤šç§è›‹ç™½è´¨åºåˆ—çš„ç»¼åˆæ€§æ•°æ®é›†ï¼Œå±•ç¤ºpLDDT-Predictorå‡†ç¡®åˆ†ç±»é«˜ç½®ä¿¡åº¦ç»“æ„ï¼Œå¹¶æ¥è¿‘AlphaFold2çš„é¢„æµ‹ç²¾åº¦ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯å…è´¹è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AlphaFold2ç­‰è›‹ç™½è´¨ç»“æ„é¢„æµ‹æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é«˜é€šé‡è›‹ç™½è´¨ç­›é€‰æ–¹é¢å­˜åœ¨è®¡ç®—éœ€æ±‚å¤§çš„é™åˆ¶ã€‚</li>
<li>ESMç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æå–è›‹ç™½è´¨åºåˆ—çš„ç»“æ„ä¿¡æ¯æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>pLDDT-Predictoræ˜¯ä¸€ä¸ªå¿«é€Ÿçš„è›‹ç™½è´¨ç­›é€‰å·¥å…·ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ESM2è›‹ç™½è´¨åµŒå…¥å’ŒTransformeræ¶æ„å®ç°äº†é«˜é€Ÿå¤„ç†ã€‚</li>
<li>pLDDT-Predictorèƒ½é¢„æµ‹AlphaFold2çš„pLDDTåˆ†æ•°ï¼Œä¸çœŸå®å€¼é«˜åº¦ç›¸å…³ï¼Œå¹¶å…·å¤‡é«˜å‡†ç¡®æ€§ã€‚</li>
<li>pLDDT-Predictorå¯åœ¨ç»¼åˆæ€§æ•°æ®é›†ä¸Šå‡†ç¡®åˆ†ç±»é«˜ç½®ä¿¡åº¦ç»“æ„ã€‚</li>
<li>pLDDT-Predictorçš„æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å·²å…¬å¼€å‘å¸ƒï¼Œä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5839fd8d2e9b40b507cdda7ebc0c25b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f72cd817914f1a55489189e730c3c09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61b08a47a4c008bc894a8e0e4367f790.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca69fcd9524dd885d7f943e5cf4cc2f7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CHOSEN-Compilation-to-Hardware-Optimization-Stack-for-Efficient-Vision-Transformer-Inference"><a href="#CHOSEN-Compilation-to-Hardware-Optimization-Stack-for-Efficient-Vision-Transformer-Inference" class="headerlink" title="CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision   Transformer Inference"></a>CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision   Transformer Inference</h2><p><strong>Authors:Mohammad Erfan Sadeghi, Arash Fayyazi, Suhas Somashekar, Armin Abdollahi, Massoud Pedram</strong></p>
<p>Vision Transformers (ViTs) represent a groundbreaking shift in machine learning approaches to computer vision. Unlike traditional approaches, ViTs employ the self-attention mechanism, which has been widely used in natural language processing, to analyze image patches. Despite their advantages in modeling visual tasks, deploying ViTs on hardware platforms, notably Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges. These challenges stem primarily from the non-linear calculations and high computational and memory demands of ViTs. This paper introduces CHOSEN, a software-hardware co-design framework to address these challenges and offer an automated framework for ViT deployment on the FPGAs in order to maximize performance. Our framework is built upon three fundamental contributions: multi-kernel design to maximize the bandwidth, mainly targeting benefits of multi DDR memory banks, approximate non-linear functions that exhibit minimal accuracy degradation, and efficient use of available logic blocks on the FPGA, and efficient compiler to maximize the performance and memory-efficiency of the computing kernels by presenting a novel algorithm for design space exploration to find optimal hardware configuration that achieves optimal throughput and latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a 1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä»£è¡¨äº†è®¡ç®—æœºè§†è§‰æœºå™¨å­¦ä¹ æ–¹æ³•çš„çªç ´æ€§è½¬å˜ã€‚ä¸ä¼ ç»Ÿçš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ä¸åŒï¼ŒViTsé‡‡ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå·²åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å¹¿æ³›ä½¿ç”¨ï¼‰æ¥åˆ†æå›¾åƒå—ã€‚å°½ç®¡ViTsåœ¨å»ºæ¨¡è§†è§‰ä»»åŠ¡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨ç¡¬ä»¶å¹³å°ï¼ˆç‰¹åˆ«æ˜¯ç°åœºå¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAï¼‰ï¼‰ä¸Šéƒ¨ç½²ViTsä»å­˜åœ¨ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜ä¸»è¦æºäºViTsçš„éçº¿æ€§è®¡ç®—ä»¥åŠå…¶å¯¹è®¡ç®—å’Œå†…å­˜çš„é«˜éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†CHOSENï¼Œè¿™æ˜¯ä¸€ä¸ªè½¯ç¡¬ä»¶ååŒè®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶ä¸ºFPGAä¸Šçš„ViTéƒ¨ç½²æä¾›è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œä»¥æœ€å¤§åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨ä¸‰ä¸ªåŸºæœ¬è´¡çŒ®ä¹‹ä¸Šï¼šå¤šæ ¸è®¾è®¡ä»¥æœ€å¤§åŒ–å¸¦å®½ï¼ˆä¸»è¦é’ˆå¯¹å¤šDDRå†…å­˜é“¶è¡Œçš„ä¼˜åŠ¿ï¼‰ã€è¿‘ä¼¼éçº¿æ€§å‡½æ•°ä»¥å±•ç°æœ€å°çš„ç²¾åº¦æŸå¤±ä»¥åŠé«˜æ•ˆä½¿ç”¨FPGAä¸Šçš„å¯ç”¨é€»è¾‘å—ï¼Œä»¥åŠé«˜æ•ˆçš„ç¼–è¯‘å™¨ã€‚ç¼–è¯‘å™¨é€šè¿‡æä¾›ä¸€ç§æ–°å‹ç®—æ³•æ¥è¿›è¡Œè®¾è®¡ç©ºé—´æ¢ç´¢ï¼Œä»¥æ‰¾åˆ°å®ç°æœ€ä½³ååé‡å’Œå»¶è¿Ÿçš„æœ€ä½³ç¡¬ä»¶é…ç½®ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°æé«˜è®¡ç®—å†…æ ¸çš„æ€§èƒ½å’Œå†…å­˜æ•ˆç‡ã€‚ä¸æœ€å…ˆè¿›çš„ViTåŠ é€Ÿå™¨ç›¸æ¯”ï¼ŒCHOSENåœ¨DeiT-Så’ŒDeiT-Bæ¨¡å‹ä¸Šçš„ååé‡æé«˜äº†1.5å€å’Œ1.42å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12736v4">PDF</a> </p>
<p><strong>Summary</strong>:<br>ViTsåœ¨æœºå™¨è§†è§‰é¢†åŸŸæ€èµ·é©å‘½æ€§å˜é©ï¼Œä½†å…¶åœ¨FPGAç­‰ç¡¬ä»¶å¹³å°ä¸Šçš„éƒ¨ç½²é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºCHOSENè½¯ç¡¬ä»¶ååŒè®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡å¤šæ ¸è®¾è®¡ã€è¿‘ä¼¼éçº¿æ€§å‡½æ•°å’Œé«˜æ•ˆç¼–è¯‘å™¨ç­‰æŠ€æœ¯ï¼Œä¼˜åŒ–ViTåœ¨FPGAä¸Šçš„æ€§èƒ½ã€‚ç›¸è¾ƒäºç°æœ‰ViTåŠ é€Ÿå™¨ï¼ŒCHOSENåœ¨DeiT-Så’ŒDeiT-Bæ¨¡å‹ä¸Šåˆ†åˆ«å®ç°äº†1.5å€å’Œ1.42å€çš„ååæ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Vision Transformers (ViTs) å¼•å…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸå¸¦æ¥é©æ–°ã€‚</li>
<li>ViTsåœ¨FPGAç¡¬ä»¶å¹³å°éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æºäºå…¶éçº¿æ€§è®¡ç®—å’Œé«˜è®¡ç®—å†…å­˜éœ€æ±‚ã€‚</li>
<li>CHOSENæ¡†æ¶é€šè¿‡å¤šæ ¸è®¾è®¡æœ€å¤§åŒ–å¸¦å®½ï¼Œä¸»è¦é¢å‘å¤šDDRå†…å­˜é“¶è¡Œçš„ä¼˜ç‚¹ã€‚</li>
<li>CHOSENé‡‡ç”¨è¿‘ä¼¼éçº¿æ€§å‡½æ•°ï¼Œåœ¨æå°å‡†ç¡®æ€§æŸå¤±ä¸‹ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨FPGAçš„é€»è¾‘å—ï¼Œå®ç°é«˜æ•ˆè¿è¡Œã€‚</li>
<li>CHOSENçš„ç¼–è¯‘å™¨é€šè¿‡æ–°å‹ç®—æ³•è¿›è¡Œè®¾è®¡æ—¶åŸŸæ¢ç´¢ï¼Œæ‰¾åˆ°å®ç°æœ€ä½³ååé‡å’Œå»¶è¿Ÿçš„ç¡¬ä»¶é…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65a75bb9474e0f49db6b1bec65e09ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bd2eb2d72bd32b0c076d775844ac3e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0858ad978a13be34ddd102a46452f86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ca39d4a76696f85393ea06c0fd8c78e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12fe27472e0dd270c46b783e5f85a091.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TextSquare-Scaling-up-Text-Centric-Visual-Instruction-Tuning"><a href="#TextSquare-Scaling-up-Text-Centric-Visual-Instruction-Tuning" class="headerlink" title="TextSquare: Scaling up Text-Centric Visual Instruction Tuning"></a>TextSquare: Scaling up Text-Centric Visual Instruction Tuning</h2><p><strong>Authors:Jingqun Tang, Chunhui Lin, Zhen Zhao, Shu Wei, Binghong Wu, Qi Liu, Yangfan He, Kuan Lu, Hao Feng, Yang Li, Siqi Wang, Lei Liao, Wei Shi, Yuliang Liu, Hao Liu, Yuan Xie, Xiang Bai, Can Huang</strong></p>
<p>Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini, partly due to a lack of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, which is generated using closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art Text-centric MLLMs and sets a new standard on OCRBench(62.2%). It even outperforms top-tier models like GPT4V and Gemini in 6 of 10 text-centric benchmarks. 2) Additionally, we demonstrate the critical role of VQA reasoning data in offering comprehensive contextual insights for specific questions. This not only improves accuracy but also significantly mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: the exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M. </p>
<blockquote>
<p>æ–‡æœ¬èšç„¦çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‘å±•è€Œå–å¾—äº†å·¨å¤§è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¼€æºæ¨¡å‹ä»ç„¶æ— æ³•èµ¶è¶…å¦‚GPT4Vå’ŒGeminiç­‰é¢†å…ˆæ¨¡å‹ï¼Œéƒ¨åˆ†åŸå› åœ¨äºç¼ºä¹å¹¿æ³›çš„é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åˆ›å»ºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Square-10Mçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å°é—­æºä»£ç çš„MLLMsç”Ÿæˆã€‚æ•°æ®æ„å»ºè¿‡ç¨‹åä¸ºSquareï¼Œåˆ†ä¸ºå››ä¸ªæ­¥éª¤ï¼šè‡ªé—®ã€å›ç­”ã€æ¨ç†å’Œè¯„ä»·ã€‚æˆ‘ä»¬ä½¿ç”¨Square-10Mè¿›è¡Œçš„å®éªŒå¾—åˆ°äº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼š</p>
</blockquote>
<ol>
<li>æˆ‘ä»¬çš„æ¨¡å‹TextSquareæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰å¼€æºçš„å…ˆè¿›æ–‡æœ¬èšç„¦å‹MLLMsï¼Œå¹¶åœ¨OCRBenchä¸Šè®¾å®šäº†æ–°æ ‡å‡†ï¼ˆ62.2%ï¼‰ã€‚å®ƒç”šè‡³åœ¨10ä¸ªæ–‡æœ¬èšç„¦çš„åŸºå‡†æµ‹è¯•ä¸­ä¸­æœ‰6ä¸ªè¶…è¶Šäº†é¡¶å°–æ¨¡å‹å¦‚GPT4Vå’ŒGeminiã€‚</li>
<li>æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†VQAæ¨ç†æ•°æ®åœ¨æä¾›ç‰¹å®šé—®é¢˜çš„å…¨é¢èƒŒæ™¯ä¿¡æ¯æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚è¿™ä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”æœ‰æ•ˆå‡è½»äº†å¹»è§‰ç°è±¡ã€‚å…·ä½“æ¥è¯´ï¼ŒTextSquareåœ¨å››ä¸ªé€šç”¨VQAå’Œå¹»è§‰è¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º75.1%ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ‰©å¤§æ–‡æœ¬èšç„¦çš„VQAæ•°æ®é›†æ—¶è§‚å¯Ÿåˆ°çš„ç°è±¡å‘ˆç°å‡ºä¸€ä¸ªæ¸…æ™°çš„æ¨¡å¼ï¼šæŒ‡ä»¤è°ƒæ•´æ•°æ®é‡çš„æŒ‡æ•°å¢é•¿ä¸æ¨¡å‹æ€§èƒ½çš„æé«˜ç›´æ¥ç›¸å…³ï¼Œä»è€ŒéªŒè¯äº†æ•°æ®é›†è§„æ¨¡å’ŒSquare-10Mé«˜è´¨é‡æ•°æ®çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12803v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–‡æœ¬ä¸­å¿ƒè§†è§‰é—®ç­”ï¼ˆVQAï¼‰å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»è½åäºGPT4Vå’ŒGeminiç­‰é¢†å…ˆæ¨¡å‹ï¼Œéƒ¨åˆ†åŸå› åœ¨äºç¼ºä¹å¤§é‡é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›å»ºå¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Square-10Mçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é—­æºMLLMsç”Ÿæˆã€‚æ•°æ®æ„å»ºè¿‡ç¨‹åŒ…æ‹¬è‡ªæˆ‘æé—®ã€å›ç­”ã€æ¨ç†å’Œè¯„ä¼°å››ä¸ªæ­¥éª¤ã€‚ä½¿ç”¨Square-10Mçš„å®éªŒè¡¨æ˜ï¼š1ï¼‰æˆ‘ä»¬çš„TextSquareæ¨¡å‹è¶…è¶Šäº†ä¹‹å‰å¼€æºçš„æ–‡æœ¬ä¸­å¿ƒå‹MLLMsçš„å…ˆè¿›æ°´å¹³ï¼Œåœ¨OCRBenchä¸Šè®¾å®šäº†æ–°æ ‡å‡†ï¼ˆ62.2%ï¼‰ï¼Œå¹¶åœ¨10ä¸ªæ–‡æœ¬ä¸­å¿ƒå‹åŸºå‡†æµ‹è¯•ä¸­6é¡¹è¶…è¿‡GPT4Vå’ŒGeminiã€‚2ï¼‰æˆ‘ä»¬è¿˜è¯æ˜äº†VQAæ¨ç†æ•°æ®åœ¨æä¾›ç‰¹å®šé—®é¢˜çš„å…¨é¢ä¸Šä¸‹æ–‡æ´å¯Ÿä¸­çš„å…³é”®ä½œç”¨ã€‚è¿™ä¸ä»…æé«˜äº†å‡†ç¡®æ€§ï¼Œè€Œä¸”æ˜¾è‘—å‡è½»äº†è™šæ„ç°è±¡ã€‚TextSquareåœ¨å››ä¸ªä¸€èˆ¬VQAå’Œè™šæ„ç°è±¡è¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º75.1%ï¼Œä¼˜äºä¹‹å‰çš„æœ€ä½³æ¨¡å‹ã€‚3ï¼‰æ–‡æœ¬ä¸­å¿ƒå‹VQAæ•°æ®é›†æ‰©å±•ç°è±¡æ˜¾ç¤ºï¼ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®é‡çš„æŒ‡æ•°å¢é•¿ä¸æ¨¡å‹æ€§èƒ½çš„æé«˜ç›´æ¥ç›¸å…³ï¼ŒéªŒè¯äº†æ•°æ®é›†è§„æ¨¡å’ŒSquare-10Mé«˜è´¨é‡æ•°æ®çš„å¿…è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨é—­æºMLLMsï¼Œæ¨å‡ºäº†æ–°çš„å¤§è§„æ¨¡é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Square-10Mã€‚</li>
<li>TextSquareæ¨¡å‹åœ¨å¤šä¸ªæ–‡æœ¬ä¸­å¿ƒå‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰å¼€æºæ¨¡å‹åŠGPT4Vå’ŒGeminiç­‰é¡¶å°–æ¨¡å‹ã€‚</li>
<li>VQAæ¨ç†æ•°æ®å¯¹äºæä¾›é—®é¢˜çš„å…¨é¢ä¸Šä¸‹æ–‡æ´å¯Ÿè‡³å…³é‡è¦ï¼Œä¸ä»…èƒ½æé«˜å‡†ç¡®æ€§ï¼Œè¿˜èƒ½æœ‰æ•ˆæŠ‘åˆ¶è™šæ„ç°è±¡ã€‚</li>
<li>TextSquareåœ¨VQAè¯„ä¼°ä¸­çš„è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ•°æ®é›†æ‰©å±•ç°è±¡æ˜¾ç¤ºï¼ŒæŒ‡ä»¤è°ƒæ•´æ•°æ®çš„æ•°é‡ä¸æ¨¡å‹æ€§èƒ½çš„æé«˜å‘ˆæ­£ç›¸å…³ï¼Œå‡¸æ˜¾äº†å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†çš„é‡è¦æ€§ã€‚</li>
<li>Square-10Mçš„æ¨å‡ºå¯¹äºæ¨åŠ¨æ–‡æœ¬ä¸­å¿ƒå‹VQAé¢†åŸŸçš„è¿›æ­¥å…·æœ‰é‡å¤§æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b71496f5799230982f5f975d4a7a97a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69cc7db9e4dbfe676879e7331e2da6aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-096f153eb7d4e01b4053d1e02784d5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7acb0085b072d6bf157006077fbafa7a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-df04937e3f74637c3683c7c30cd2b471.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  "What are my options?" Explaining RL Agents with Diverse Near-Optimal   Alternatives (Extended)
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f2339f60031315509f905003087eeda9.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
