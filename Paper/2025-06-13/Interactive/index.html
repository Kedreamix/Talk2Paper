<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-06-13  Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over   Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-2e218bb04615e59f7b2b6c0e5d24016e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-13-更新"><a href="#2025-06-13-更新" class="headerlink" title="2025-06-13 更新"></a>2025-06-13 更新</h1><h2 id="Outside-Knowledge-Conversational-Video-OKCV-Dataset-–-Dialoguing-over-Videos"><a href="#Outside-Knowledge-Conversational-Video-OKCV-Dataset-–-Dialoguing-over-Videos" class="headerlink" title="Outside Knowledge Conversational Video (OKCV) Dataset – Dialoguing over   Videos"></a>Outside Knowledge Conversational Video (OKCV) Dataset – Dialoguing over   Videos</h2><p><strong>Authors:Benjamin Reichman, Constantin Patsch, Jack Truxal, Atishay Jain, Larry Heck</strong></p>
<p>In outside knowledge visual question answering (OK-VQA), the model must identify relevant visual information within an image and incorporate external knowledge to accurately respond to a question. Extending this task to a visually grounded dialogue setting based on videos, a conversational model must both recognize pertinent visual details over time and answer questions where the required information is not necessarily present in the visual information. Moreover, the context of the overall conversation must be considered for the subsequent dialogue. To explore this task, we introduce a dataset comprised of $2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$ interleaved dialogue turns. While the dialogue context is visually grounded in specific video segments, the questions further require external knowledge that is not visually present. Thus, the model not only has to identify relevant video parts but also leverage external knowledge to converse within the dialogue. We further provide several baselines evaluated on our dataset and show future challenges associated with this task. The dataset is made publicly available here: <a target="_blank" rel="noopener" href="https://github.com/c-patsch/OKCV">https://github.com/c-patsch/OKCV</a>. </p>
<blockquote>
<p>在外部知识视觉问答（OK-VQA）中，模型必须识别图像中的相关视觉信息，并结合外部知识来准确回答问题。将这项任务扩展到基于视频的视觉对话场景时，对话模型必须能够在一段时间内识别出重要的视觉细节，并回答那些所需信息并不完全存在于视觉信息中的问题。此外，还必须考虑整个对话的上下文来进行后续对话。为了探索这项任务，我们引入了一个包含$ 2,017 $个视频和$ 5,986 $个人工注释对话的数据集，这些对话包含$ 40,954 $个交叉对话轮次。虽然对话上下文在特定的视频片段中是视觉主导的，但问题还需要一些视觉上不存在的外部知识。因此，模型不仅要能够识别出相关的视频部分，还必须利用外部知识进行对话。我们进一步提供了在我们数据集上评估的几个基线，并展示了与此任务相关的未来挑战。数据集已公开提供：<a target="_blank" rel="noopener" href="https://github.com/c-patsch/OKCV%E3%80%82">https://github.com/c-patsch/OKCV。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09953v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在外部知识视觉问答（OK-VQA）中，模型必须识别图像中的相关视觉信息并结合外部知识准确回答问题。当任务扩展到基于视频的视觉对话场景时，对话模型必须能够随时间识别重要的视觉细节，并回答所需信息不一定在视觉信息中的问题。此外，还必须考虑对话的整体上下文。为了研究此任务，我们引入了包含2,017个视频和5,986个人工注释对话的数据集，包含40,954个对话轮次。对话上下文虽然以特定视频片段为基础，但问题还需要额外的外部知识，这些知识在视频中并不直接呈现。因此，模型不仅要识别相关的视频部分，还要利用外部知识进行对话。我们在数据集上提供了几个基线评估并展示了与此任务相关的未来挑战。数据集已公开访问：<a target="_blank" rel="noopener" href="https://github.com/c-patsch/OKCV%E3%80%82">https://github.com/c-patsch/OKCV。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OK-VQA模型需结合视觉信息和外部知识回答问题。</li>
<li>模型须识别视频中的关键视觉信息并在对话中利用这些信息。</li>
<li>对话模型需处理的问题涉及外部知识，这些知识可能不在视觉信息中直接呈现。</li>
<li>模型应能识别相关视频部分并整合外部知识以进行对话。</li>
<li>数据集包含大量视频和人工注释的对话轮次，适合用于评估和研究视觉对话任务。</li>
<li>此任务面临挑战，包括识别重要视觉细节、整合外部知识和对话上下文的能力等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09953">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7780a53848ed12dbcef28511a23d64a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0890b6f7df72028484c2ffa401e2ddb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e9cac1062166fc02fbd578306e64b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72bd850f9febac54eab209e9ff7d648e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-386a11ad114429ee6ce7ac13e024d425.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Approaching-Dialogue-State-Tracking-via-Aligning-Speech-Encoders-and-LLMs"><a href="#Approaching-Dialogue-State-Tracking-via-Aligning-Speech-Encoders-and-LLMs" class="headerlink" title="Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs"></a>Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs</h2><p><strong>Authors:Šimon Sedláček, Bolaji Yusuf, Ján Švec, Pradyoth Hegde, Santosh Kesiraju, Oldřich Plchot, Jan Černocký</strong></p>
<p>In this work, we approach spoken Dialogue State Tracking (DST) by bridging the representation spaces of speech encoders and LLMs via a small connector module, with a focus on fully open-sourced and open-data components (WavLM-large, OLMo). We focus on ablating different aspects of such systems including full&#x2F;LoRA adapter fine-tuning, the effect of agent turns in the dialogue history, as well as fuzzy matching-based output post-processing, which greatly improves performance of our systems on named entities in the dialogue slot values. We conduct our experiments on the SpokenWOZ dataset, and additionally utilize the Speech-Aware MultiWOZ dataset to augment our training data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17% JGA on SpokenWOZ test. </p>
<blockquote>
<p>在这项工作中，我们通过一个小型连接器模块来连接语音编码器和大型语言模型（LLM）的表示空间，从而解决口头对话状态跟踪（DST）问题。我们的重点是在完全开源和公开数据组件（WavLM-大型和OLMo）的基础上开展工作。我们重点关注这种系统的不同方面，包括全&#x2F;LoRA适配器微调、对话历史中代理回合的影响，以及基于模糊匹配的输出后处理，这大大提高了我们在对话槽值中的命名实体的性能。我们在SpokenWOZ数据集上进行实验，并额外使用Speech-Aware MultiWOZ数据集来扩充我们的训练数据。最终，我们表现最佳的WavLM+连接器+OLMo-1B对齐模型在SpokenWOZ测试集上达到了业界领先水平（JGA达到34.66%），我们的Gemma-2-9B-instruct系统进一步超越了这一结果，在SpokenWOZ测试集上达到42.17%的JGA。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08633v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了基于语音的对话状态跟踪（DST），通过小型连接器模块桥接语音编码器和大型语言模型（LLM）的表示空间。研究重点包括系统的不同方面，如全&#x2F;LoRA适配器微调、对话历史中代理回合的影响，以及基于模糊匹配的输出后处理，这大大提高了对话槽值中命名实体的系统性能。实验在SpokenWOZ数据集上进行，并额外使用语音感知MultiWOZ数据集来增强训练数据。最终，最优秀的模型在SpokenWOZ测试集上达到了最新的状态（JGA 34.66%），而采用Gemma-2-9B-instruct的系统进一步超越了这一结果，达到了42.17%的JGA。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究通过连接器模块结合了语音编码器和大型语言模型，专注于全开源和开放数据组件。</li>
<li>聚焦于系统的不同方面，包括全&#x2F;LoRA适配器微调、对话历史中代理回合的影响。</li>
<li>引入模糊匹配输出后处理，提高了对话槽值中命名实体的系统性能。</li>
<li>实验在SpokenWOZ数据集上进行，并使用语音感知MultiWOZ数据集增强训练数据。</li>
<li>最优秀的模型在SpokenWOZ测试集上达到了最新的状态（JGA 34.66%）。</li>
<li>采用Gemma-2-9B-instruct的系统进一步提高了性能，达到了更高的JGA值（42.17%）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0614131100f296c82b9e8c8e3ae97891.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c473815707d481415276e04e26d11b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7d81082b43b293f6a7eaa047bec043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa3ebf5b304febaa784593ceeff090c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d3f5e7893f1ddf50f41acf088f7ba6e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NTPP-Generative-Speech-Language-Modeling-for-Dual-Channel-Spoken-Dialogue-via-Next-Token-Pair-Prediction"><a href="#NTPP-Generative-Speech-Language-Modeling-for-Dual-Channel-Spoken-Dialogue-via-Next-Token-Pair-Prediction" class="headerlink" title="NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction"></a>NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction</h2><p><strong>Authors:Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao</strong></p>
<p>Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications. </p>
<blockquote>
<p>受GPT-4o强大功能的启发，人们越来越感兴趣让语言模型（SLM）与人类进行自然流畅的口语交互。最近的进步导致了许多SLM的开发，在这一领域取得了令人鼓舞的结果。然而，当前的方法还没有充分利用双通道语音数据，这些数据天然地捕捉了人类对话的结构和动态。在这项工作中，我们系统地探索了在现代大型语言模型背景下使用双通道语音数据的方法，并引入了一种新型的生成建模范式——下一个令牌对预测（NTPP），首次使用仅解码器架构实现说话者独立的双通道口语对话学习。我们在标准基准测试上评估了我们的方法，实证结果表明，我们提出的方法NTPP在话轮预测、响应连贯性和自然性方面显著提高了SLM的会话能力。此外，与现有方法相比，NTPP实现了更低的推理延迟，突出了其实时应用的实用效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00975v4">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用双通道语音数据在现代大型语言模型中的使用，并介绍了一种新型生成建模范式——Next-Token-Pair Prediction（NTPP）。该研究旨在使语言模型能够进行更自然的对话，通过解码器仅架构实现说话者独立的双通道口语对话学习。实验结果表明，NTPP显著提高了语言模型的对话能力，包括回合制预测、响应连贯性和自然性。此外，与现有方法相比，NTPP实现了更低的推理延迟，表明其在实时应用中的实际效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4o的出色能力激发了人们对使语言模型能够进行自然流畅的人机对话的兴趣。</li>
<li>近期进展已开发出能在这一领域展现出色结果的多个语言模型。</li>
<li>当前方法尚未充分利用双通道语音数据，这些数据捕捉人类对话的结构和动态。</li>
<li>引入了一种新型生成建模范式NTPP，首次实现了使用解码器仅架构的说话者独立的双通道口语对话学习。</li>
<li>NTPP显著提高了语言模型的对话能力，包括回合制预测、响应连贯性和自然性。</li>
<li>与现有方法相比，NTPP的推理延迟更低，表明其实时应用的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58d11a4c58528feb7b0f1d0c54c64e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9193c9a422c4d0b9f844beb15fbc0de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-460b5f01675ab590d78ac6354cccef13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e06fcdb427360a4912f19054b7061996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d1f046507aa2e5bae7cb6f99fd64da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e19b060e851d1050754172c8dce2e86b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks"><a href="#Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks" class="headerlink" title="Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks"></a>Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks</h2><p><strong>Authors:Yongyi Zang, Sean O’Brien, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack</strong></p>
<p>Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, on par or above most LALMs. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening: Robust Understanding through Listening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question’s reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information-text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework’s effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities. </p>
<blockquote>
<p>大规模音频语言模型（LALMs）通过在音频输入上微调预训练的文本大型语言模型（LLMs），在音乐理解方面取得了显著的进步。然而，当前的评估方法存在严重的局限性：在领先的Music Question Answering基准测试MuchoMusic上，没有音频感知能力的纯文本LLMs出人意料地达到了高达56.4%的高准确率，与大多数LALMs持平或更高。此外，当面对随机高斯噪声而非实际音频时，LALMs的表现仍然显著超过随机水平。这些发现表明，现有的基准测试主要评估的是推理能力，而不是音频感知能力。为了克服这一挑战，我们提出了RUListening：通过倾听实现稳健理解，这是一个增强音乐问答基准测试中的感知评估的框架。我们引入了感知指数（PI）这一量化指标，通过分析纯文本语言模型的日志概率分布来衡量问题对音频感知的依赖程度。使用该指标，我们生成了合成且具有挑战性的干扰项，以创建必须依赖真实音频感知的QA对。在MuchoMusic上的应用显示，我们的筛选数据集成功地迫使模型依赖感知信息——纯文本LLMs的表现达到了随机水平，而当音频输入被噪声替换时，LALMs的表现也类似地下降。这些结果验证了我们框架在创建更准确地评估音频感知能力的基准测试中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00369v2">PDF</a> ISMIR 2025</p>
<p><strong>Summary</strong></p>
<p>预训练文本LLM通过音频输入进行微调的大型音频语言模型（LALMs）在音乐理解方面取得了显著进展。然而，现有的评估方法存在重大局限性。在领先的Music Question Answering基准测试MuchoMusic上，没有音频感知能力的纯文本LLMs达到了惊人的高准确率（高达56.4%），与大多数LALM的表现相当或更好。此外，当面对随机高斯噪声而非实际音频时，LALM的表现仍然远高于平均水平。这些发现表明当前基准测试主要评估的是推理能力而非音频感知能力。为解决这一挑战，提出了RUListening框架，通过音乐问答基准测试增强感知评估。引入感知指数（PI）这一量化指标，通过分析纯文本语言模型的日志概率分布来衡量问题对音频感知的依赖程度。利用这一指标生成合成、具有挑战性的干扰项来创建需要真实音频感知的QA对。在MuchoMusic上的应用显示，过滤后的数据集成功迫使模型依赖感知信息——纯文本LLM表现接近平均水平，而LALM在音频输入替换为噪声时表现也显著下降。这些结果验证了框架在创建更准确评估音频感知能力的基准测试方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型音频语言模型（LALMs）在音乐理解上表现出显著进展，但现有评估方法存在局限性。</li>
<li>纯文本LLMs在MuchoMusic基准测试上的表现惊人，显示现有评估方法更侧重于推理能力而非音频感知能力。</li>
<li>引入RUListening框架和感知指数（PI）量化指标来增强音乐问答基准测试中的感知评估。</li>
<li>PI指标通过分析日志概率分布衡量问题对音频感知的依赖程度。</li>
<li>利用PI指标生成合成QA对，以更准确评估模型的音频感知能力。</li>
<li>过滤后的数据集迫使模型依赖感知信息，纯文本LLM表现接近平均水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9e75927e67ae641210b97fa491826595.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53ecdd4b40fc1d9db6ac2bdbf25798fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08dc867e251bc8d0589ee6b0e1a7a85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5efc1f6526760eecb9d90a7a1c1214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c05123211c1128a8a197e68a8ca7265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0b024c6dce476b40455641c68c98290.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Party-Supervised-Fine-tuning-of-Language-Models-for-Multi-Party-Dialogue-Generation"><a href="#Multi-Party-Supervised-Fine-tuning-of-Language-Models-for-Multi-Party-Dialogue-Generation" class="headerlink" title="Multi-Party Supervised Fine-tuning of Language Models for Multi-Party   Dialogue Generation"></a>Multi-Party Supervised Fine-tuning of Language Models for Multi-Party   Dialogue Generation</h2><p><strong>Authors:Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji</strong></p>
<p>Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe. </p>
<blockquote>
<p>大型语言模型（LLM）通常经过微调以参与二元或两方对话，但它们无法很好地适应多方对话（MPD），这阻碍了它们在包括多人会议、讨论和日常交流等场景中的应用。之前基于LLM的研究主要集中在多代理框架上，而其基础LLM仍然是对对对话进行微调。在这项工作中，我们为LLM设计了一个多方微调框架（MuPaS），用于多方对话数据集，并证明这种简单的框架可以让LLM高效且有效地适应多方对话风格。我们还设计了两种可以将MuPaS转换为MPD模拟器的训练策略。大量实验表明，MuPaS可以实现最新的多方响应、更高的下一个发言者预测准确率、更高的人类和自动评估的话语质量，并且可以在分布外的场景、主题和角色描述中生成合理的响应。MuPaS框架将LLM训练与更复杂的多方应用（如对话生成、虚拟排练或元宇宙）联系起来。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05342v5">PDF</a> Accepted by IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在应对多方对话（MPD）时的适应性问题，制约了其在多人会议、讨论和日常沟通等场景的应用。本研究设计了一种针对多方对话数据集的多方微调框架（MuPaS），该框架能使LLM有效适应多方对话风格。同时，研究还提出了两种训练策略，可将MuPaS转化为MPD模拟器。实验表明，MuPaS在多方响应、预测下一位发言者等方面达到领先水平，提高了人类和自动评估的话语质量，并能够适应离分布场景、话题和角色描述生成合理的回应。MuPaS框架为将LLM训练应用于更复杂的场景如对话生成、虚拟排练或元宇宙等搭建了桥梁。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多方对话（MPD）中的适应能力有待提高。</li>
<li>提出了一种新的多方微调框架（MuPaS）以改善LLM在多方对话中的表现。</li>
<li>MuPaS框架可以有效地让LLM适应多方对话风格。</li>
<li>提出了两种训练策略，将MuPaS转化为MPD模拟器。</li>
<li>MuPaS在多方响应和预测下一位发言者方面具有领先水平。</li>
<li>MuPaS提高了人类和自动评估的话语质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e8aaaff995b4b46e0b44b82ea83c43d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c759b74361ce62e885f11fe0ff426a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e240d0846af948e78ef05ec69953e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a2aceddf8b653a6a912f94e89992b2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e218bb04615e59f7b2b6c0e5d24016e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-175891677aa08f162d3d4d7a2fac72a8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SHARE-Shared-Memory-Aware-Open-Domain-Long-Term-Dialogue-Dataset-Constructed-from-Movie-Script"><a href="#SHARE-Shared-Memory-Aware-Open-Domain-Long-Term-Dialogue-Dataset-Constructed-from-Movie-Script" class="headerlink" title="SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset   Constructed from Movie Script"></a>SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset   Constructed from Movie Script</h2><p><strong>Authors:Eunwon Kim, Chanho Park, Buru Chang</strong></p>
<p>Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/e1kim/SHARE">https://github.com/e1kim/SHARE</a>. </p>
<blockquote>
<p>两个人之间的共享记忆会加强他们之间的联系，对于促进他们持续的对话至关重要。本研究旨在利用这些共享记忆使长期对话更具吸引力。为此，我们引入了一个新的长期对话数据集SHARE，该数据集由电影剧本构建而成，电影剧本是各种关系中共享记忆的丰富来源。我们的对话数据集包含两个人在对话中明确展现的个性信息和事件概述，以及可隐式提取的共享记忆。我们还介绍了基于SHARE的EPISODE长期对话框架，该框架利用个人之间的共享经历。通过使用SHARE进行的实验表明，两个人之间的共享记忆使长期对话更具吸引力和可持续性，而EPISODE在对话期间有效地管理共享记忆。我们的数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/e1kim/SHARE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/e1kim/SHARE获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20682v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究利用共享记忆来增强长期对话的吸引力与可持续性。为此，研究团队构建了一个名为SHARE的新长期对话数据集，该数据集来自电影剧本，其中包含了丰富的共享记忆资源。此外，他们还推出了基于SHARE的EPISODE长期对话框架，该框架利用个体间的共享经历。研究证明了共享记忆在对话中的重要性，而EPISODE能有效地管理对话中的共享记忆。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>共享记忆可以加强两个人之间的纽带，并促进他们的持续对话。</li>
<li>此研究旨在通过利用共享记忆使长期对话更加引人入胜。</li>
<li>引入了名为SHARE的新长期对话数据集，该数据集来自电影剧本，包含丰富的共享记忆资源。</li>
<li>数据集包含了个人信息的摘要以及两个人之间的活动概要，这些都在他们的对话中明确体现出来，以及可以隐含提取的共享记忆。</li>
<li>推出了基于SHARE的EPISODE长期对话框架，该框架利用个体间的共享经历。</li>
<li>实验表明，共享记忆可以使长期对话更加引人入胜和可持续。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20682">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-28c30f0aecf0b08459ff0f65ba2de389.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcb786f889ede5896877a22187ae44f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73adc500f7c923d4312b16a2e88400e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3819067dc181b1675860582e1f20186c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FairMT-Bench-Benchmarking-Fairness-for-Multi-turn-Dialogue-in-Conversational-LLMs"><a href="#FairMT-Bench-Benchmarking-Fairness-for-Multi-turn-Dialogue-in-Conversational-LLMs" class="headerlink" title="FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in   Conversational LLMs"></a>FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in   Conversational LLMs</h2><p><strong>Authors:Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu</strong></p>
<p>The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, \textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, \texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on \texttt{FairMT-10K} reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, \texttt{FairMT-1K}, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of \texttt{FairMT-1K} in such efforts. </p>
<blockquote>
<p>随着大型语言模型（LLM）聊天机器人的使用日益增多，关于公平性的担忧也随之增加。LLM中的公平问题可能导致严重后果，如偏见放大、歧视和对边缘化群体的伤害。现有的公平基准测试主要关注单轮对话，而实际上更能反映真实世界对话的多轮对话场景带来了更大的挑战，因为对话的复杂性和潜在的偏见累积。在本文中，我们为LLM在多轮对话场景提出了全面的公平基准测试，名为<strong>FairMT-Bench</strong>。具体来说，我们制定了针对LLM公平能力的任务分类，包括三个阶段：上下文理解、用户交互和指令权衡，每个阶段包含两个任务。为了确保涵盖多种偏见类型和属性，我们借鉴了现有的公平数据集，并使用我们的模板构建了多轮对话数据集<strong>FairMT-10K</strong>。为了进行评估，我们应用了GPT-4以及偏见分类器Llama-Guard-3和人类验证以确保稳健性。在<strong>FairMT-10K</strong>上的实验和分析表明，在多轮对话场景中，当前的LLM更容易产生有偏见的回应，不同任务和模型之间的性能存在显著差异。基于此，我们整理了一个具有挑战性的数据集<strong>FairMT-1K</strong>，并在该数据集上测试了15个当前最先进的LLM。结果表明LLM当前的公平状态，并展示了这种新方法在更现实的多轮对话上下文中评估公平性的实用性，呼吁未来的工作要关注LLM公平性的改进，并在此类努力中采用<strong>FairMT-1K</strong>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19317v2">PDF</a> ICLR 2025 spotlight</p>
<p><strong>Summary</strong></p>
<p>本文关注大型语言模型（LLM）在聊天机器人应用中的公平性挑战。在多轮对话场景中，LLM面临更大的公平性考验。为此，本文提出一个全面的公平性基准测试FairMT-Bench，用于评估LLM在多轮对话中的公平性能力。通过构建FairMT-10K数据集并利用GPT-4等模型进行实验分析，发现当前LLM在多轮对话中更容易产生偏见。基于这一观察，本文进一步推出了一个更富挑战性的数据集FairMT-1K，测试了当前先进的LLM，呼吁未来工作关注LLM公平性的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在聊天机器人应用中的公平性受到关注，多轮对话场景带来更大挑战。</li>
<li>提出了FairMT-Bench基准测试，用于评估LLM在多轮对话中的公平性能力。</li>
<li>通过FairMT-10K数据集的实验分析，发现当前LLM更容易产生偏见响应。</li>
<li>推出了挑战性的FairMT-1K数据集，测试了当前先进的LLM在公平性方面的表现。</li>
<li>实验结果表明当前LLM在公平性方面存在不足，展示了使用FairMT-1K数据集评估公平性的实用性。</li>
<li>强调未来工作应关注LLM公平性的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19317">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c8db37b456adbeec2030a2d87e58d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb1c03e16b480a1297db6819fde214f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5d5ed48c761d58c208bde06d5d2790.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51197bfd6189e1dc8a355bbb327d6c43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0cc149bb3f4e33c8041f54f96ea7f27.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Emphasising-Structured-Information-Integrating-Abstract-Meaning-Representation-into-LLMs-for-Enhanced-Open-Domain-Dialogue-Evaluation"><a href="#Emphasising-Structured-Information-Integrating-Abstract-Meaning-Representation-into-LLMs-for-Enhanced-Open-Domain-Dialogue-Evaluation" class="headerlink" title="Emphasising Structured Information: Integrating Abstract Meaning   Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"></a>Emphasising Structured Information: Integrating Abstract Meaning   Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation</h2><p><strong>Authors:Bohao Yang, Kun Zhao, Dong Liu, Liang Zhan, Chenghua Lin</strong></p>
<p>Automatic open-domain dialogue evaluation has attracted increasing attention, yet remains challenging due to the complexity of assessing response appropriateness. Traditional evaluation metrics, typically trained with true positive and randomly selected negative responses, tend to assign higher scores to responses that share greater content similarity with contexts. However, adversarial negative responses, despite possessing high lexical overlap with contexts, can be semantically incongruous. Consequently, existing metrics struggle to effectively evaluate such responses, resulting in low correlations with human judgments. While recent studies have demonstrated the effectiveness of Large Language Models (LLMs) for open-domain dialogue evaluation, they still face challenges in handling adversarial negative examples. We propose a novel evaluation framework that integrates Abstract Meaning Representation (AMR) enhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly incorporate AMR graph information through a gating mechanism for enhanced semantic representation learning, while both SLM predictions and AMR knowledge are integrated into LLM prompts for robust evaluation. Extensive experiments on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to state-of-the-art baselines. Our comprehensive ablation studies reveal that AMR graph information contributes substantially more to performance improvements. Our framework achieves strong correlations with human judgments across multiple datasets, establishing a new benchmark for dialogue evaluation. Our code and data are publicly available. </p>
<blockquote>
<p>自动开放域对话评价已经引起了越来越多的关注，但由于评价响应适当性的复杂性，仍然面临挑战。传统评价指标通常通过与真实积极和随机选择的消极响应进行训练，倾向于给那些与上下文内容相似性更高的响应赋予更高的分数。然而，对抗性消极响应尽管与上下文有高度的词汇重叠，但语义上可能不相符。因此，现有指标在评估此类响应时遇到困难，与人类判断的相关性较低。虽然最近的研究已经证明了大型语言模型（LLM）在开放域对话评价中的有效性，但它们仍然面临处理对抗性负面示例的挑战。我们提出了一种新的评价框架，该框架结合了抽象意义表示（AMR）增强领域特定语言模型（SLM）和LLM。我们的SLM通过门控机制显式地融入AMR图信息，以促进语义表示学习，而SLM预测和AMR知识都被融入到LLM提示中，以实现稳健的评价。在开放域对话评价任务上的大量实验表明，我们的方法相较于最新基线技术具有优越性。我们的综合消融研究结果表明，AMR图信息对性能提升贡献更大。我们的框架在多个数据集上与人类判断的相关性很强，为对话评价建立了新的基准。我们的代码和数据已公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01129v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的评价框架，结合抽象意义表示（AMR）增强的领域特定语言模型（SLMs）与大型语言模型（LLMs）进行自动开放域对话评价。该框架通过门控机制显式地融入AMR图信息，增强语义表示学习，并将SLM预测和AMR知识融入LLM提示中进行稳健评价。实验证明，该方法在开放域对话评价任务上优于现有技术，且与人类判断高度一致。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动开放域对话评价面临挑战，因为评估响应的适当性很复杂。</li>
<li>传统评估指标倾向于给内容上与上下文更相似的回应更高的分数，但对抗性负面回应尽管词汇上与上下文重叠度高，语义上却可能不相符。</li>
<li>大型语言模型（LLMs）在开放域对话评价中虽然有效，但仍面临处理对抗性负面示例的挑战。</li>
<li>提出的评估框架结合了抽象意义表示（AMR）增强的领域特定语言模型（SLMs）与LLMs，通过门控机制显式地融入AMR图信息，增强语义表示学习。</li>
<li>框架将SLM预测和AMR知识融入LLM提示中，实现稳健评价。</li>
<li>实验证明，该方法在开放域对话评价任务上优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.01129">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-996815cfbd9892db257e79718db24fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51781f95e501ce1247f76b3c3a577914.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f7c4a7b6d391e388379ad7be03a00c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fb5d04238aa831540ba72ea5fff8f8b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience"><a href="#Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience" class="headerlink" title="Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience"></a>Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience</h2><p><strong>Authors:Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani</strong></p>
<p>In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces “Listen, Chat, and Remix” (LCR), a novel multimodal sound remixer that controls each sound source in a mixture based on user-provided text instructions. LCR distinguishes itself with a user-friendly text interface and its unique ability to remix multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for remixing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles filtered components back to the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse remixing tasks including extraction, removal, and volume control of single or multiple sources. Our experiments demonstrate significant improvements in signal quality across all remixing tasks and robust performance in zero-shot scenarios with varying numbers and types of sound sources. An audio demo is available at: <a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a>. </p>
<blockquote>
<p>在日常生活中，我们会遇到各种声音，包括希望听到的和不想听到的，但对它们的存在和音量只有有限的控制能力。我们的工作引入了“Listen, Chat, and Remix”（LCR）这一新型多模式声音混音器，它可以根据用户提供的文本指令控制混合中的各种声音源。LCR通过友好的文本接口及其在同一混合物中同时混音多个声音源的独特能力来区分自己，而无需将它们分开。用户输入开放词汇文本提示，由大型语言模型解释以创建用于混音声音混合物的语义过滤器。然后，系统将混合物分解成其组成部分，应用语义过滤器，并将过滤后的组件重新组装为所需的输出。我们开发了一个包含超过10万混合物的160小时数据集，其中包括语音和各种音频源以及用于各种混音任务的文本提示，包括提取、删除和单个或多个源的声音控制。我们的实验表明在所有混音任务中信号质量都有显著提高，并且在具有不同数量和类型的音源的场景中具有稳健的性能表现。音频演示可在：<a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03710v2">PDF</a> Accepted by IEEE Journal of Selected Topics in Signal Processing   (JSTSP)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为“Listen，Chat，and Remix”（LCR）的新型多模式声音混音器，它可根据用户提供的文本指令控制混合中的每个声音源。LCR通过用户友好的文本界面，无需分离即可同时混音多个声音源。系统通过大型语言模型解释用户输入的开放词汇文本提示，创建语义过滤器以混音声音混合物。实验证明，LCR在所有混音任务中的信号质量都有显著提高，并且在不同数量和类型的音源场景下表现稳健。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCR是一种多模式声音混音器，能够控制混合中的每个声音源，基于用户提供的文本指令进行操作。</li>
<li>LCR具备用户友好的文本界面，可以方便地创建语义过滤器以混音声音混合物。</li>
<li>LCR能够同时混音多个声音源，而无需事先将其分离。</li>
<li>系统通过大型语言模型解释用户输入的文本提示，为混音过程提供语义过滤。</li>
<li>LCR拥有160小时的数据集，包含语音和各种音频源，以及用于各种混音任务的文本提示。</li>
<li>实验证明，LCR在信号质量上有显著改善，并且对不同类型和数量的声音源表现出稳健的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7dd10c16d26f590cbf14dcd0ddcf596e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e4a9c637a1f386f8f6db21f83434ec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc06ba878ad5fe8d3e3c90572ca5a480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8531b4f1e51e0995113da1f15c1c31c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89f167e5f57e22f658633e04e0ae2322.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce442293e5f9981fa7bb76deb83ae172.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-06-13  HunyuanVideo-HOMA Generic Human-Object Interaction in Multimodal Driven   Human Animation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-13  EmoNet-Voice A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
