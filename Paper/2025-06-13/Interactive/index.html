<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over   Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-2e218bb04615e59f7b2b6c0e5d24016e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="Outside-Knowledge-Conversational-Video-OKCV-Dataset-â€“-Dialoguing-over-Videos"><a href="#Outside-Knowledge-Conversational-Video-OKCV-Dataset-â€“-Dialoguing-over-Videos" class="headerlink" title="Outside Knowledge Conversational Video (OKCV) Dataset â€“ Dialoguing over   Videos"></a>Outside Knowledge Conversational Video (OKCV) Dataset â€“ Dialoguing over   Videos</h2><p><strong>Authors:Benjamin Reichman, Constantin Patsch, Jack Truxal, Atishay Jain, Larry Heck</strong></p>
<p>In outside knowledge visual question answering (OK-VQA), the model must identify relevant visual information within an image and incorporate external knowledge to accurately respond to a question. Extending this task to a visually grounded dialogue setting based on videos, a conversational model must both recognize pertinent visual details over time and answer questions where the required information is not necessarily present in the visual information. Moreover, the context of the overall conversation must be considered for the subsequent dialogue. To explore this task, we introduce a dataset comprised of $2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$ interleaved dialogue turns. While the dialogue context is visually grounded in specific video segments, the questions further require external knowledge that is not visually present. Thus, the model not only has to identify relevant video parts but also leverage external knowledge to converse within the dialogue. We further provide several baselines evaluated on our dataset and show future challenges associated with this task. The dataset is made publicly available here: <a target="_blank" rel="noopener" href="https://github.com/c-patsch/OKCV">https://github.com/c-patsch/OKCV</a>. </p>
<blockquote>
<p>åœ¨å¤–éƒ¨çŸ¥è¯†è§†è§‰é—®ç­”ï¼ˆOK-VQAï¼‰ä¸­ï¼Œæ¨¡å‹å¿…é¡»è¯†åˆ«å›¾åƒä¸­çš„ç›¸å…³è§†è§‰ä¿¡æ¯ï¼Œå¹¶ç»“åˆå¤–éƒ¨çŸ¥è¯†æ¥å‡†ç¡®å›ç­”é—®é¢˜ã€‚å°†è¿™é¡¹ä»»åŠ¡æ‰©å±•åˆ°åŸºäºè§†é¢‘çš„è§†è§‰å¯¹è¯åœºæ™¯æ—¶ï¼Œå¯¹è¯æ¨¡å‹å¿…é¡»èƒ½å¤Ÿåœ¨ä¸€æ®µæ—¶é—´å†…è¯†åˆ«å‡ºé‡è¦çš„è§†è§‰ç»†èŠ‚ï¼Œå¹¶å›ç­”é‚£äº›æ‰€éœ€ä¿¡æ¯å¹¶ä¸å®Œå…¨å­˜åœ¨äºè§†è§‰ä¿¡æ¯ä¸­çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¿…é¡»è€ƒè™‘æ•´ä¸ªå¯¹è¯çš„ä¸Šä¸‹æ–‡æ¥è¿›è¡Œåç»­å¯¹è¯ã€‚ä¸ºäº†æ¢ç´¢è¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«$ 2,017 $ä¸ªè§†é¢‘å’Œ$ 5,986 $ä¸ªäººå·¥æ³¨é‡Šå¯¹è¯çš„æ•°æ®é›†ï¼Œè¿™äº›å¯¹è¯åŒ…å«$ 40,954 $ä¸ªäº¤å‰å¯¹è¯è½®æ¬¡ã€‚è™½ç„¶å¯¹è¯ä¸Šä¸‹æ–‡åœ¨ç‰¹å®šçš„è§†é¢‘ç‰‡æ®µä¸­æ˜¯è§†è§‰ä¸»å¯¼çš„ï¼Œä½†é—®é¢˜è¿˜éœ€è¦ä¸€äº›è§†è§‰ä¸Šä¸å­˜åœ¨çš„å¤–éƒ¨çŸ¥è¯†ã€‚å› æ­¤ï¼Œæ¨¡å‹ä¸ä»…è¦èƒ½å¤Ÿè¯†åˆ«å‡ºç›¸å…³çš„è§†é¢‘éƒ¨åˆ†ï¼Œè¿˜å¿…é¡»åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†è¿›è¡Œå¯¹è¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†åœ¨æˆ‘ä»¬æ•°æ®é›†ä¸Šè¯„ä¼°çš„å‡ ä¸ªåŸºçº¿ï¼Œå¹¶å±•ç¤ºäº†ä¸æ­¤ä»»åŠ¡ç›¸å…³çš„æœªæ¥æŒ‘æˆ˜ã€‚æ•°æ®é›†å·²å…¬å¼€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/c-patsch/OKCV%E3%80%82">https://github.com/c-patsch/OKCVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09953v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤–éƒ¨çŸ¥è¯†è§†è§‰é—®ç­”ï¼ˆOK-VQAï¼‰ä¸­ï¼Œæ¨¡å‹å¿…é¡»è¯†åˆ«å›¾åƒä¸­çš„ç›¸å…³è§†è§‰ä¿¡æ¯å¹¶ç»“åˆå¤–éƒ¨çŸ¥è¯†å‡†ç¡®å›ç­”é—®é¢˜ã€‚å½“ä»»åŠ¡æ‰©å±•åˆ°åŸºäºè§†é¢‘çš„è§†è§‰å¯¹è¯åœºæ™¯æ—¶ï¼Œå¯¹è¯æ¨¡å‹å¿…é¡»èƒ½å¤Ÿéšæ—¶é—´è¯†åˆ«é‡è¦çš„è§†è§‰ç»†èŠ‚ï¼Œå¹¶å›ç­”æ‰€éœ€ä¿¡æ¯ä¸ä¸€å®šåœ¨è§†è§‰ä¿¡æ¯ä¸­çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¿…é¡»è€ƒè™‘å¯¹è¯çš„æ•´ä½“ä¸Šä¸‹æ–‡ã€‚ä¸ºäº†ç ”ç©¶æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«2,017ä¸ªè§†é¢‘å’Œ5,986ä¸ªäººå·¥æ³¨é‡Šå¯¹è¯çš„æ•°æ®é›†ï¼ŒåŒ…å«40,954ä¸ªå¯¹è¯è½®æ¬¡ã€‚å¯¹è¯ä¸Šä¸‹æ–‡è™½ç„¶ä»¥ç‰¹å®šè§†é¢‘ç‰‡æ®µä¸ºåŸºç¡€ï¼Œä½†é—®é¢˜è¿˜éœ€è¦é¢å¤–çš„å¤–éƒ¨çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†åœ¨è§†é¢‘ä¸­å¹¶ä¸ç›´æ¥å‘ˆç°ã€‚å› æ­¤ï¼Œæ¨¡å‹ä¸ä»…è¦è¯†åˆ«ç›¸å…³çš„è§†é¢‘éƒ¨åˆ†ï¼Œè¿˜è¦åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†è¿›è¡Œå¯¹è¯ã€‚æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸Šæä¾›äº†å‡ ä¸ªåŸºçº¿è¯„ä¼°å¹¶å±•ç¤ºäº†ä¸æ­¤ä»»åŠ¡ç›¸å…³çš„æœªæ¥æŒ‘æˆ˜ã€‚æ•°æ®é›†å·²å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/c-patsch/OKCV%E3%80%82">https://github.com/c-patsch/OKCVã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OK-VQAæ¨¡å‹éœ€ç»“åˆè§†è§‰ä¿¡æ¯å’Œå¤–éƒ¨çŸ¥è¯†å›ç­”é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é¡»è¯†åˆ«è§†é¢‘ä¸­çš„å…³é”®è§†è§‰ä¿¡æ¯å¹¶åœ¨å¯¹è¯ä¸­åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚</li>
<li>å¯¹è¯æ¨¡å‹éœ€å¤„ç†çš„é—®é¢˜æ¶‰åŠå¤–éƒ¨çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯èƒ½ä¸åœ¨è§†è§‰ä¿¡æ¯ä¸­ç›´æ¥å‘ˆç°ã€‚</li>
<li>æ¨¡å‹åº”èƒ½è¯†åˆ«ç›¸å…³è§†é¢‘éƒ¨åˆ†å¹¶æ•´åˆå¤–éƒ¨çŸ¥è¯†ä»¥è¿›è¡Œå¯¹è¯ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤§é‡è§†é¢‘å’Œäººå·¥æ³¨é‡Šçš„å¯¹è¯è½®æ¬¡ï¼Œé€‚åˆç”¨äºè¯„ä¼°å’Œç ”ç©¶è§†è§‰å¯¹è¯ä»»åŠ¡ã€‚</li>
<li>æ­¤ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯†åˆ«é‡è¦è§†è§‰ç»†èŠ‚ã€æ•´åˆå¤–éƒ¨çŸ¥è¯†å’Œå¯¹è¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7780a53848ed12dbcef28511a23d64a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0890b6f7df72028484c2ffa401e2ddb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e9cac1062166fc02fbd578306e64b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-72bd850f9febac54eab209e9ff7d648e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-386a11ad114429ee6ce7ac13e024d425.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Approaching-Dialogue-State-Tracking-via-Aligning-Speech-Encoders-and-LLMs"><a href="#Approaching-Dialogue-State-Tracking-via-Aligning-Speech-Encoders-and-LLMs" class="headerlink" title="Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs"></a>Approaching Dialogue State Tracking via Aligning Speech Encoders and   LLMs</h2><p><strong>Authors:Å imon SedlÃ¡Äek, Bolaji Yusuf, JÃ¡n Å vec, Pradyoth Hegde, Santosh Kesiraju, OldÅ™ich Plchot, Jan ÄŒernockÃ½</strong></p>
<p>In this work, we approach spoken Dialogue State Tracking (DST) by bridging the representation spaces of speech encoders and LLMs via a small connector module, with a focus on fully open-sourced and open-data components (WavLM-large, OLMo). We focus on ablating different aspects of such systems including full&#x2F;LoRA adapter fine-tuning, the effect of agent turns in the dialogue history, as well as fuzzy matching-based output post-processing, which greatly improves performance of our systems on named entities in the dialogue slot values. We conduct our experiments on the SpokenWOZ dataset, and additionally utilize the Speech-Aware MultiWOZ dataset to augment our training data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17% JGA on SpokenWOZ test. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå°å‹è¿æ¥å™¨æ¨¡å—æ¥è¿æ¥è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡¨ç¤ºç©ºé—´ï¼Œä»è€Œè§£å†³å£å¤´å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰é—®é¢˜ã€‚æˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨å®Œå…¨å¼€æºå’Œå…¬å¼€æ•°æ®ç»„ä»¶ï¼ˆWavLM-å¤§å‹å’ŒOLMoï¼‰çš„åŸºç¡€ä¸Šå¼€å±•å·¥ä½œã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨è¿™ç§ç³»ç»Ÿçš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬å…¨&#x2F;LoRAé€‚é…å™¨å¾®è°ƒã€å¯¹è¯å†å²ä¸­ä»£ç†å›åˆçš„å½±å“ï¼Œä»¥åŠåŸºäºæ¨¡ç³ŠåŒ¹é…çš„è¾“å‡ºåå¤„ç†ï¼Œè¿™å¤§å¤§æé«˜äº†æˆ‘ä»¬åœ¨å¯¹è¯æ§½å€¼ä¸­çš„å‘½åå®ä½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨SpokenWOZæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œå¹¶é¢å¤–ä½¿ç”¨Speech-Aware MultiWOZæ•°æ®é›†æ¥æ‰©å……æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¡¨ç°æœ€ä½³çš„WavLM+è¿æ¥å™¨+OLMo-1Bå¯¹é½æ¨¡å‹åœ¨SpokenWOZæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼ˆJGAè¾¾åˆ°34.66%ï¼‰ï¼Œæˆ‘ä»¬çš„Gemma-2-9B-instructç³»ç»Ÿè¿›ä¸€æ­¥è¶…è¶Šäº†è¿™ä¸€ç»“æœï¼Œåœ¨SpokenWOZæµ‹è¯•é›†ä¸Šè¾¾åˆ°42.17%çš„JGAã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08633v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºè¯­éŸ³çš„å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰ï¼Œé€šè¿‡å°å‹è¿æ¥å™¨æ¨¡å—æ¡¥æ¥è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¡¨ç¤ºç©ºé—´ã€‚ç ”ç©¶é‡ç‚¹åŒ…æ‹¬ç³»ç»Ÿçš„ä¸åŒæ–¹é¢ï¼Œå¦‚å…¨&#x2F;LoRAé€‚é…å™¨å¾®è°ƒã€å¯¹è¯å†å²ä¸­ä»£ç†å›åˆçš„å½±å“ï¼Œä»¥åŠåŸºäºæ¨¡ç³ŠåŒ¹é…çš„è¾“å‡ºåå¤„ç†ï¼Œè¿™å¤§å¤§æé«˜äº†å¯¹è¯æ§½å€¼ä¸­å‘½åå®ä½“çš„ç³»ç»Ÿæ€§èƒ½ã€‚å®éªŒåœ¨SpokenWOZæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¹¶é¢å¤–ä½¿ç”¨è¯­éŸ³æ„ŸçŸ¥MultiWOZæ•°æ®é›†æ¥å¢å¼ºè®­ç»ƒæ•°æ®ã€‚æœ€ç»ˆï¼Œæœ€ä¼˜ç§€çš„æ¨¡å‹åœ¨SpokenWOZæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„çŠ¶æ€ï¼ˆJGA 34.66%ï¼‰ï¼Œè€Œé‡‡ç”¨Gemma-2-9B-instructçš„ç³»ç»Ÿè¿›ä¸€æ­¥è¶…è¶Šäº†è¿™ä¸€ç»“æœï¼Œè¾¾åˆ°äº†42.17%çš„JGAã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é€šè¿‡è¿æ¥å™¨æ¨¡å—ç»“åˆäº†è¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºå…¨å¼€æºå’Œå¼€æ”¾æ•°æ®ç»„ä»¶ã€‚</li>
<li>èšç„¦äºç³»ç»Ÿçš„ä¸åŒæ–¹é¢ï¼ŒåŒ…æ‹¬å…¨&#x2F;LoRAé€‚é…å™¨å¾®è°ƒã€å¯¹è¯å†å²ä¸­ä»£ç†å›åˆçš„å½±å“ã€‚</li>
<li>å¼•å…¥æ¨¡ç³ŠåŒ¹é…è¾“å‡ºåå¤„ç†ï¼Œæé«˜äº†å¯¹è¯æ§½å€¼ä¸­å‘½åå®ä½“çš„ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>å®éªŒåœ¨SpokenWOZæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¹¶ä½¿ç”¨è¯­éŸ³æ„ŸçŸ¥MultiWOZæ•°æ®é›†å¢å¼ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>æœ€ä¼˜ç§€çš„æ¨¡å‹åœ¨SpokenWOZæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„çŠ¶æ€ï¼ˆJGA 34.66%ï¼‰ã€‚</li>
<li>é‡‡ç”¨Gemma-2-9B-instructçš„ç³»ç»Ÿè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„JGAå€¼ï¼ˆ42.17%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0614131100f296c82b9e8c8e3ae97891.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c473815707d481415276e04e26d11b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7d81082b43b293f6a7eaa047bec043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa3ebf5b304febaa784593ceeff090c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d3f5e7893f1ddf50f41acf088f7ba6e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NTPP-Generative-Speech-Language-Modeling-for-Dual-Channel-Spoken-Dialogue-via-Next-Token-Pair-Prediction"><a href="#NTPP-Generative-Speech-Language-Modeling-for-Dual-Channel-Spoken-Dialogue-via-Next-Token-Pair-Prediction" class="headerlink" title="NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction"></a>NTPP: Generative Speech Language Modeling for Dual-Channel Spoken   Dialogue via Next-Token-Pair Prediction</h2><p><strong>Authors:Qichao Wang, Ziqiao Meng, Wenqian Cui, Yifei Zhang, Pengcheng Wu, Bingzhe Wu, Irwin King, Liang Chen, Peilin Zhao</strong></p>
<p>Inspired by the impressive capabilities of GPT-4o, there is growing interest in enabling speech language models (SLMs) to engage in natural, fluid spoken interactions with humans. Recent advancements have led to the development of several SLMs that demonstrate promising results in this area. However, current approaches have yet to fully exploit dual-channel speech data, which inherently captures the structure and dynamics of human conversation. In this work, we systematically explore the use of dual-channel speech data in the context of modern large language models, and introduce a novel generative modeling paradigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent dual-channel spoken dialogue learning using decoder-only architectures for the first time. We evaluate our approach on standard benchmarks, and empirical results show that our proposed method, NTPP, significantly improves the conversational abilities of SLMs in terms of turn-taking prediction, response coherence, and naturalness. Moreover, compared to existing methods, NTPP achieves substantially lower inference latency, highlighting its practical efficiency for real-time applications. </p>
<blockquote>
<p>å—GPT-4oå¼ºå¤§åŠŸèƒ½çš„å¯å‘ï¼Œäººä»¬è¶Šæ¥è¶Šæ„Ÿå…´è¶£è®©è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä¸äººç±»è¿›è¡Œè‡ªç„¶æµç•…çš„å£è¯­äº¤äº’ã€‚æœ€è¿‘çš„è¿›æ­¥å¯¼è‡´äº†è®¸å¤šSLMçš„å¼€å‘ï¼Œåœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•è¿˜æ²¡æœ‰å……åˆ†åˆ©ç”¨åŒé€šé“è¯­éŸ³æ•°æ®ï¼Œè¿™äº›æ•°æ®å¤©ç„¶åœ°æ•æ‰äº†äººç±»å¯¹è¯çš„ç»“æ„å’ŒåŠ¨æ€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹èƒŒæ™¯ä¸‹ä½¿ç”¨åŒé€šé“è¯­éŸ³æ•°æ®çš„æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼â€”â€”ä¸‹ä¸€ä¸ªä»¤ç‰Œå¯¹é¢„æµ‹ï¼ˆNTPPï¼‰ï¼Œé¦–æ¬¡ä½¿ç”¨ä»…è§£ç å™¨æ¶æ„å®ç°è¯´è¯è€…ç‹¬ç«‹çš„åŒé€šé“å£è¯­å¯¹è¯å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®è¯ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•NTPPåœ¨è¯è½®é¢„æµ‹ã€å“åº”è¿è´¯æ€§å’Œè‡ªç„¶æ€§æ–¹é¢æ˜¾è‘—æé«˜äº†SLMçš„ä¼šè¯èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNTPPå®ç°äº†æ›´ä½çš„æ¨ç†å»¶è¿Ÿï¼Œçªå‡ºäº†å…¶å®æ—¶åº”ç”¨çš„å®ç”¨æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00975v4">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨åŒé€šé“è¯­éŸ³æ•°æ®åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä½¿ç”¨ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ–°å‹ç”Ÿæˆå»ºæ¨¡èŒƒå¼â€”â€”Next-Token-Pair Predictionï¼ˆNTPPï¼‰ã€‚è¯¥ç ”ç©¶æ—¨åœ¨ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæ›´è‡ªç„¶çš„å¯¹è¯ï¼Œé€šè¿‡è§£ç å™¨ä»…æ¶æ„å®ç°è¯´è¯è€…ç‹¬ç«‹çš„åŒé€šé“å£è¯­å¯¹è¯å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNTPPæ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…æ‹¬å›åˆåˆ¶é¢„æµ‹ã€å“åº”è¿è´¯æ€§å’Œè‡ªç„¶æ€§ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNTPPå®ç°äº†æ›´ä½çš„æ¨ç†å»¶è¿Ÿï¼Œè¡¨æ˜å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„å®é™…æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oçš„å‡ºè‰²èƒ½åŠ›æ¿€å‘äº†äººä»¬å¯¹ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„äººæœºå¯¹è¯çš„å…´è¶£ã€‚</li>
<li>è¿‘æœŸè¿›å±•å·²å¼€å‘å‡ºèƒ½åœ¨è¿™ä¸€é¢†åŸŸå±•ç°å‡ºè‰²ç»“æœçš„å¤šä¸ªè¯­è¨€æ¨¡å‹ã€‚</li>
<li>å½“å‰æ–¹æ³•å°šæœªå……åˆ†åˆ©ç”¨åŒé€šé“è¯­éŸ³æ•°æ®ï¼Œè¿™äº›æ•°æ®æ•æ‰äººç±»å¯¹è¯çš„ç»“æ„å’ŒåŠ¨æ€ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ç”Ÿæˆå»ºæ¨¡èŒƒå¼NTPPï¼Œé¦–æ¬¡å®ç°äº†ä½¿ç”¨è§£ç å™¨ä»…æ¶æ„çš„è¯´è¯è€…ç‹¬ç«‹çš„åŒé€šé“å£è¯­å¯¹è¯å­¦ä¹ ã€‚</li>
<li>NTPPæ˜¾è‘—æé«˜äº†è¯­è¨€æ¨¡å‹çš„å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…æ‹¬å›åˆåˆ¶é¢„æµ‹ã€å“åº”è¿è´¯æ€§å’Œè‡ªç„¶æ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒNTPPçš„æ¨ç†å»¶è¿Ÿæ›´ä½ï¼Œè¡¨æ˜å…¶å®æ—¶åº”ç”¨çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58d11a4c58528feb7b0f1d0c54c64e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9193c9a422c4d0b9f844beb15fbc0de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-460b5f01675ab590d78ac6354cccef13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e06fcdb427360a4912f19054b7061996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87d1f046507aa2e5bae7cb6f99fd64da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e19b060e851d1050754172c8dce2e86b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks"><a href="#Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks" class="headerlink" title="Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks"></a>Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks</h2><p><strong>Authors:Yongyi Zang, Sean Oâ€™Brien, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack</strong></p>
<p>Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, on par or above most LALMs. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening: Robust Understanding through Listening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a questionâ€™s reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information-text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our frameworkâ€™s effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities. </p>
<blockquote>
<p>å¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰é€šè¿‡åœ¨éŸ³é¢‘è¾“å…¥ä¸Šå¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œåœ¨éŸ³ä¹ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸¥é‡çš„å±€é™æ€§ï¼šåœ¨é¢†å…ˆçš„Music Question AnsweringåŸºå‡†æµ‹è¯•MuchoMusicä¸Šï¼Œæ²¡æœ‰éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›çš„çº¯æ–‡æœ¬LLMså‡ºäººæ„æ–™åœ°è¾¾åˆ°äº†é«˜è¾¾56.4%çš„é«˜å‡†ç¡®ç‡ï¼Œä¸å¤§å¤šæ•°LALMsæŒå¹³æˆ–æ›´é«˜ã€‚æ­¤å¤–ï¼Œå½“é¢å¯¹éšæœºé«˜æ–¯å™ªå£°è€Œéå®é™…éŸ³é¢‘æ—¶ï¼ŒLALMsçš„è¡¨ç°ä»ç„¶æ˜¾è‘—è¶…è¿‡éšæœºæ°´å¹³ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°çš„æ˜¯æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RUListeningï¼šé€šè¿‡å€¾å¬å®ç°ç¨³å¥ç†è§£ï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºéŸ³ä¹é—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„æ„ŸçŸ¥è¯„ä¼°çš„æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†æ„ŸçŸ¥æŒ‡æ•°ï¼ˆPIï¼‰è¿™ä¸€é‡åŒ–æŒ‡æ ‡ï¼Œé€šè¿‡åˆ†æçº¯æ–‡æœ¬è¯­è¨€æ¨¡å‹çš„æ—¥å¿—æ¦‚ç‡åˆ†å¸ƒæ¥è¡¡é‡é—®é¢˜å¯¹éŸ³é¢‘æ„ŸçŸ¥çš„ä¾èµ–ç¨‹åº¦ã€‚ä½¿ç”¨è¯¥æŒ‡æ ‡ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†åˆæˆä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å¹²æ‰°é¡¹ï¼Œä»¥åˆ›å»ºå¿…é¡»ä¾èµ–çœŸå®éŸ³é¢‘æ„ŸçŸ¥çš„QAå¯¹ã€‚åœ¨MuchoMusicä¸Šçš„åº”ç”¨æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ç­›é€‰æ•°æ®é›†æˆåŠŸåœ°è¿«ä½¿æ¨¡å‹ä¾èµ–æ„ŸçŸ¥ä¿¡æ¯â€”â€”çº¯æ–‡æœ¬LLMsçš„è¡¨ç°è¾¾åˆ°äº†éšæœºæ°´å¹³ï¼Œè€Œå½“éŸ³é¢‘è¾“å…¥è¢«å™ªå£°æ›¿æ¢æ—¶ï¼ŒLALMsçš„è¡¨ç°ä¹Ÿç±»ä¼¼åœ°ä¸‹é™ã€‚è¿™äº›ç»“æœéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶åœ¨åˆ›å»ºæ›´å‡†ç¡®åœ°è¯„ä¼°éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00369v2">PDF</a> ISMIR 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ–‡æœ¬LLMé€šè¿‡éŸ³é¢‘è¾“å…¥è¿›è¡Œå¾®è°ƒçš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨éŸ³ä¹ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨é‡å¤§å±€é™æ€§ã€‚åœ¨é¢†å…ˆçš„Music Question AnsweringåŸºå‡†æµ‹è¯•MuchoMusicä¸Šï¼Œæ²¡æœ‰éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›çš„çº¯æ–‡æœ¬LLMsè¾¾åˆ°äº†æƒŠäººçš„é«˜å‡†ç¡®ç‡ï¼ˆé«˜è¾¾56.4%ï¼‰ï¼Œä¸å¤§å¤šæ•°LALMçš„è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚æ­¤å¤–ï¼Œå½“é¢å¯¹éšæœºé«˜æ–¯å™ªå£°è€Œéå®é™…éŸ³é¢‘æ—¶ï¼ŒLALMçš„è¡¨ç°ä»ç„¶è¿œé«˜äºå¹³å‡æ°´å¹³ã€‚è¿™äº›å‘ç°è¡¨æ˜å½“å‰åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°çš„æ˜¯æ¨ç†èƒ½åŠ›è€ŒééŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†RUListeningæ¡†æ¶ï¼Œé€šè¿‡éŸ³ä¹é—®ç­”åŸºå‡†æµ‹è¯•å¢å¼ºæ„ŸçŸ¥è¯„ä¼°ã€‚å¼•å…¥æ„ŸçŸ¥æŒ‡æ•°ï¼ˆPIï¼‰è¿™ä¸€é‡åŒ–æŒ‡æ ‡ï¼Œé€šè¿‡åˆ†æçº¯æ–‡æœ¬è¯­è¨€æ¨¡å‹çš„æ—¥å¿—æ¦‚ç‡åˆ†å¸ƒæ¥è¡¡é‡é—®é¢˜å¯¹éŸ³é¢‘æ„ŸçŸ¥çš„ä¾èµ–ç¨‹åº¦ã€‚åˆ©ç”¨è¿™ä¸€æŒ‡æ ‡ç”Ÿæˆåˆæˆã€å…·æœ‰æŒ‘æˆ˜æ€§çš„å¹²æ‰°é¡¹æ¥åˆ›å»ºéœ€è¦çœŸå®éŸ³é¢‘æ„ŸçŸ¥çš„QAå¯¹ã€‚åœ¨MuchoMusicä¸Šçš„åº”ç”¨æ˜¾ç¤ºï¼Œè¿‡æ»¤åçš„æ•°æ®é›†æˆåŠŸè¿«ä½¿æ¨¡å‹ä¾èµ–æ„ŸçŸ¥ä¿¡æ¯â€”â€”çº¯æ–‡æœ¬LLMè¡¨ç°æ¥è¿‘å¹³å‡æ°´å¹³ï¼Œè€ŒLALMåœ¨éŸ³é¢‘è¾“å…¥æ›¿æ¢ä¸ºå™ªå£°æ—¶è¡¨ç°ä¹Ÿæ˜¾è‘—ä¸‹é™ã€‚è¿™äº›ç»“æœéªŒè¯äº†æ¡†æ¶åœ¨åˆ›å»ºæ›´å‡†ç¡®è¯„ä¼°éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨éŸ³ä¹ç†è§£ä¸Šè¡¨ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>çº¯æ–‡æœ¬LLMsåœ¨MuchoMusicåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æƒŠäººï¼Œæ˜¾ç¤ºç°æœ‰è¯„ä¼°æ–¹æ³•æ›´ä¾§é‡äºæ¨ç†èƒ½åŠ›è€ŒééŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥RUListeningæ¡†æ¶å’Œæ„ŸçŸ¥æŒ‡æ•°ï¼ˆPIï¼‰é‡åŒ–æŒ‡æ ‡æ¥å¢å¼ºéŸ³ä¹é—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„æ„ŸçŸ¥è¯„ä¼°ã€‚</li>
<li>PIæŒ‡æ ‡é€šè¿‡åˆ†ææ—¥å¿—æ¦‚ç‡åˆ†å¸ƒè¡¡é‡é—®é¢˜å¯¹éŸ³é¢‘æ„ŸçŸ¥çš„ä¾èµ–ç¨‹åº¦ã€‚</li>
<li>åˆ©ç”¨PIæŒ‡æ ‡ç”ŸæˆåˆæˆQAå¯¹ï¼Œä»¥æ›´å‡†ç¡®è¯„ä¼°æ¨¡å‹çš„éŸ³é¢‘æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>è¿‡æ»¤åçš„æ•°æ®é›†è¿«ä½¿æ¨¡å‹ä¾èµ–æ„ŸçŸ¥ä¿¡æ¯ï¼Œçº¯æ–‡æœ¬LLMè¡¨ç°æ¥è¿‘å¹³å‡æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e75927e67ae641210b97fa491826595.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53ecdd4b40fc1d9db6ac2bdbf25798fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08dc867e251bc8d0589ee6b0e1a7a85e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5efc1f6526760eecb9d90a7a1c1214.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c05123211c1128a8a197e68a8ca7265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0b024c6dce476b40455641c68c98290.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Party-Supervised-Fine-tuning-of-Language-Models-for-Multi-Party-Dialogue-Generation"><a href="#Multi-Party-Supervised-Fine-tuning-of-Language-Models-for-Multi-Party-Dialogue-Generation" class="headerlink" title="Multi-Party Supervised Fine-tuning of Language Models for Multi-Party   Dialogue Generation"></a>Multi-Party Supervised Fine-tuning of Language Models for Multi-Party   Dialogue Generation</h2><p><strong>Authors:Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji</strong></p>
<p>Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ç»è¿‡å¾®è°ƒä»¥å‚ä¸äºŒå…ƒæˆ–ä¸¤æ–¹å¯¹è¯ï¼Œä½†å®ƒä»¬æ— æ³•å¾ˆå¥½åœ°é€‚åº”å¤šæ–¹å¯¹è¯ï¼ˆMPDï¼‰ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨åŒ…æ‹¬å¤šäººä¼šè®®ã€è®¨è®ºå’Œæ—¥å¸¸äº¤æµç­‰åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¹‹å‰åŸºäºLLMçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¤šä»£ç†æ¡†æ¶ä¸Šï¼Œè€Œå…¶åŸºç¡€LLMä»ç„¶æ˜¯å¯¹å¯¹å¯¹è¯è¿›è¡Œå¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºLLMè®¾è®¡äº†ä¸€ä¸ªå¤šæ–¹å¾®è°ƒæ¡†æ¶ï¼ˆMuPaSï¼‰ï¼Œç”¨äºå¤šæ–¹å¯¹è¯æ•°æ®é›†ï¼Œå¹¶è¯æ˜è¿™ç§ç®€å•çš„æ¡†æ¶å¯ä»¥è®©LLMé«˜æ•ˆä¸”æœ‰æ•ˆåœ°é€‚åº”å¤šæ–¹å¯¹è¯é£æ ¼ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸¤ç§å¯ä»¥å°†MuPaSè½¬æ¢ä¸ºMPDæ¨¡æ‹Ÿå™¨çš„è®­ç»ƒç­–ç•¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMuPaSå¯ä»¥å®ç°æœ€æ–°çš„å¤šæ–¹å“åº”ã€æ›´é«˜çš„ä¸‹ä¸€ä¸ªå‘è¨€è€…é¢„æµ‹å‡†ç¡®ç‡ã€æ›´é«˜çš„äººç±»å’Œè‡ªåŠ¨è¯„ä¼°çš„è¯è¯­è´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥åœ¨åˆ†å¸ƒå¤–çš„åœºæ™¯ã€ä¸»é¢˜å’Œè§’è‰²æè¿°ä¸­ç”Ÿæˆåˆç†çš„å“åº”ã€‚MuPaSæ¡†æ¶å°†LLMè®­ç»ƒä¸æ›´å¤æ‚çš„å¤šæ–¹åº”ç”¨ï¼ˆå¦‚å¯¹è¯ç”Ÿæˆã€è™šæ‹Ÿæ’ç»ƒæˆ–å…ƒå®‡å®™ï¼‰è”ç³»èµ·æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05342v5">PDF</a> Accepted by IJCNN 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”å¯¹å¤šæ–¹å¯¹è¯ï¼ˆMPDï¼‰æ—¶çš„é€‚åº”æ€§é—®é¢˜ï¼Œåˆ¶çº¦äº†å…¶åœ¨å¤šäººä¼šè®®ã€è®¨è®ºå’Œæ—¥å¸¸æ²Ÿé€šç­‰åœºæ™¯çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§é’ˆå¯¹å¤šæ–¹å¯¹è¯æ•°æ®é›†çš„å¤šæ–¹å¾®è°ƒæ¡†æ¶ï¼ˆMuPaSï¼‰ï¼Œè¯¥æ¡†æ¶èƒ½ä½¿LLMæœ‰æ•ˆé€‚åº”å¤šæ–¹å¯¹è¯é£æ ¼ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸¤ç§è®­ç»ƒç­–ç•¥ï¼Œå¯å°†MuPaSè½¬åŒ–ä¸ºMPDæ¨¡æ‹Ÿå™¨ã€‚å®éªŒè¡¨æ˜ï¼ŒMuPaSåœ¨å¤šæ–¹å“åº”ã€é¢„æµ‹ä¸‹ä¸€ä½å‘è¨€è€…ç­‰æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œæé«˜äº†äººç±»å’Œè‡ªåŠ¨è¯„ä¼°çš„è¯è¯­è´¨é‡ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”ç¦»åˆ†å¸ƒåœºæ™¯ã€è¯é¢˜å’Œè§’è‰²æè¿°ç”Ÿæˆåˆç†çš„å›åº”ã€‚MuPaSæ¡†æ¶ä¸ºå°†LLMè®­ç»ƒåº”ç”¨äºæ›´å¤æ‚çš„åœºæ™¯å¦‚å¯¹è¯ç”Ÿæˆã€è™šæ‹Ÿæ’ç»ƒæˆ–å…ƒå®‡å®™ç­‰æ­å»ºäº†æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ–¹å¯¹è¯ï¼ˆMPDï¼‰ä¸­çš„é€‚åº”èƒ½åŠ›æœ‰å¾…æé«˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ–¹å¾®è°ƒæ¡†æ¶ï¼ˆMuPaSï¼‰ä»¥æ”¹å–„LLMåœ¨å¤šæ–¹å¯¹è¯ä¸­çš„è¡¨ç°ã€‚</li>
<li>MuPaSæ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°è®©LLMé€‚åº”å¤šæ–¹å¯¹è¯é£æ ¼ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§è®­ç»ƒç­–ç•¥ï¼Œå°†MuPaSè½¬åŒ–ä¸ºMPDæ¨¡æ‹Ÿå™¨ã€‚</li>
<li>MuPaSåœ¨å¤šæ–¹å“åº”å’Œé¢„æµ‹ä¸‹ä¸€ä½å‘è¨€è€…æ–¹é¢å…·æœ‰é¢†å…ˆæ°´å¹³ã€‚</li>
<li>MuPaSæé«˜äº†äººç±»å’Œè‡ªåŠ¨è¯„ä¼°çš„è¯è¯­è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8aaaff995b4b46e0b44b82ea83c43d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c759b74361ce62e885f11fe0ff426a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e240d0846af948e78ef05ec69953e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a2aceddf8b653a6a912f94e89992b2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e218bb04615e59f7b2b6c0e5d24016e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-175891677aa08f162d3d4d7a2fac72a8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SHARE-Shared-Memory-Aware-Open-Domain-Long-Term-Dialogue-Dataset-Constructed-from-Movie-Script"><a href="#SHARE-Shared-Memory-Aware-Open-Domain-Long-Term-Dialogue-Dataset-Constructed-from-Movie-Script" class="headerlink" title="SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset   Constructed from Movie Script"></a>SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset   Constructed from Movie Script</h2><p><strong>Authors:Eunwon Kim, Chanho Park, Buru Chang</strong></p>
<p>Shared memories between two individuals strengthen their bond and are crucial for facilitating their ongoing conversations. This study aims to make long-term dialogue more engaging by leveraging these shared memories. To this end, we introduce a new long-term dialogue dataset named SHARE, constructed from movie scripts, which are a rich source of shared memories among various relationships. Our dialogue dataset contains the summaries of persona information and events of two individuals, as explicitly revealed in their conversation, along with implicitly extractable shared memories. We also introduce EPISODE, a long-term dialogue framework based on SHARE that utilizes shared experiences between individuals. Through experiments using SHARE, we demonstrate that shared memories between two individuals make long-term dialogues more engaging and sustainable, and that EPISODE effectively manages shared memories during dialogue. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/e1kim/SHARE">https://github.com/e1kim/SHARE</a>. </p>
<blockquote>
<p>ä¸¤ä¸ªäººä¹‹é—´çš„å…±äº«è®°å¿†ä¼šåŠ å¼ºä»–ä»¬ä¹‹é—´çš„è”ç³»ï¼Œå¯¹äºä¿ƒè¿›ä»–ä»¬æŒç»­çš„å¯¹è¯è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨è¿™äº›å…±äº«è®°å¿†ä½¿é•¿æœŸå¯¹è¯æ›´å…·å¸å¼•åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„é•¿æœŸå¯¹è¯æ•°æ®é›†SHAREï¼Œè¯¥æ•°æ®é›†ç”±ç”µå½±å‰§æœ¬æ„å»ºè€Œæˆï¼Œç”µå½±å‰§æœ¬æ˜¯å„ç§å…³ç³»ä¸­å…±äº«è®°å¿†çš„ä¸°å¯Œæ¥æºã€‚æˆ‘ä»¬çš„å¯¹è¯æ•°æ®é›†åŒ…å«ä¸¤ä¸ªäººåœ¨å¯¹è¯ä¸­æ˜ç¡®å±•ç°çš„ä¸ªæ€§ä¿¡æ¯å’Œäº‹ä»¶æ¦‚è¿°ï¼Œä»¥åŠå¯éšå¼æå–çš„å…±äº«è®°å¿†ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºSHAREçš„EPISODEé•¿æœŸå¯¹è¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸ªäººä¹‹é—´çš„å…±äº«ç»å†ã€‚é€šè¿‡ä½¿ç”¨SHAREè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸¤ä¸ªäººä¹‹é—´çš„å…±äº«è®°å¿†ä½¿é•¿æœŸå¯¹è¯æ›´å…·å¸å¼•åŠ›å’Œå¯æŒç»­æ€§ï¼Œè€ŒEPISODEåœ¨å¯¹è¯æœŸé—´æœ‰æ•ˆåœ°ç®¡ç†å…±äº«è®°å¿†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/e1kim/SHARE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/e1kim/SHAREè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20682v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶åˆ©ç”¨å…±äº«è®°å¿†æ¥å¢å¼ºé•¿æœŸå¯¹è¯çš„å¸å¼•åŠ›ä¸å¯æŒç»­æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåä¸ºSHAREçš„æ–°é•¿æœŸå¯¹è¯æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªç”µå½±å‰§æœ¬ï¼Œå…¶ä¸­åŒ…å«äº†ä¸°å¯Œçš„å…±äº«è®°å¿†èµ„æºã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜æ¨å‡ºäº†åŸºäºSHAREçš„EPISODEé•¿æœŸå¯¹è¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸ªä½“é—´çš„å…±äº«ç»å†ã€‚ç ”ç©¶è¯æ˜äº†å…±äº«è®°å¿†åœ¨å¯¹è¯ä¸­çš„é‡è¦æ€§ï¼Œè€ŒEPISODEèƒ½æœ‰æ•ˆåœ°ç®¡ç†å¯¹è¯ä¸­çš„å…±äº«è®°å¿†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…±äº«è®°å¿†å¯ä»¥åŠ å¼ºä¸¤ä¸ªäººä¹‹é—´çš„çº½å¸¦ï¼Œå¹¶ä¿ƒè¿›ä»–ä»¬çš„æŒç»­å¯¹è¯ã€‚</li>
<li>æ­¤ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ©ç”¨å…±äº«è®°å¿†ä½¿é•¿æœŸå¯¹è¯æ›´åŠ å¼•äººå…¥èƒœã€‚</li>
<li>å¼•å…¥äº†åä¸ºSHAREçš„æ–°é•¿æœŸå¯¹è¯æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¥è‡ªç”µå½±å‰§æœ¬ï¼ŒåŒ…å«ä¸°å¯Œçš„å…±äº«è®°å¿†èµ„æºã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº†ä¸ªäººä¿¡æ¯çš„æ‘˜è¦ä»¥åŠä¸¤ä¸ªäººä¹‹é—´çš„æ´»åŠ¨æ¦‚è¦ï¼Œè¿™äº›éƒ½åœ¨ä»–ä»¬çš„å¯¹è¯ä¸­æ˜ç¡®ä½“ç°å‡ºæ¥ï¼Œä»¥åŠå¯ä»¥éšå«æå–çš„å…±äº«è®°å¿†ã€‚</li>
<li>æ¨å‡ºäº†åŸºäºSHAREçš„EPISODEé•¿æœŸå¯¹è¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸ªä½“é—´çš„å…±äº«ç»å†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå…±äº«è®°å¿†å¯ä»¥ä½¿é•¿æœŸå¯¹è¯æ›´åŠ å¼•äººå…¥èƒœå’Œå¯æŒç»­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-28c30f0aecf0b08459ff0f65ba2de389.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcb786f889ede5896877a22187ae44f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73adc500f7c923d4312b16a2e88400e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3819067dc181b1675860582e1f20186c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FairMT-Bench-Benchmarking-Fairness-for-Multi-turn-Dialogue-in-Conversational-LLMs"><a href="#FairMT-Bench-Benchmarking-Fairness-for-Multi-turn-Dialogue-in-Conversational-LLMs" class="headerlink" title="FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in   Conversational LLMs"></a>FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in   Conversational LLMs</h2><p><strong>Authors:Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu</strong></p>
<p>The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, \textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, \texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on \texttt{FairMT-10K} reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, \texttt{FairMT-1K}, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of \texttt{FairMT-1K} in such efforts. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èŠå¤©æœºå™¨äººçš„ä½¿ç”¨æ—¥ç›Šå¢å¤šï¼Œå…³äºå…¬å¹³æ€§çš„æ‹…å¿§ä¹Ÿéšä¹‹å¢åŠ ã€‚LLMä¸­çš„å…¬å¹³é—®é¢˜å¯èƒ½å¯¼è‡´ä¸¥é‡åæœï¼Œå¦‚åè§æ”¾å¤§ã€æ­§è§†å’Œå¯¹è¾¹ç¼˜åŒ–ç¾¤ä½“çš„ä¼¤å®³ã€‚ç°æœ‰çš„å…¬å¹³åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å•è½®å¯¹è¯ï¼Œè€Œå®é™…ä¸Šæ›´èƒ½åæ˜ çœŸå®ä¸–ç•Œå¯¹è¯çš„å¤šè½®å¯¹è¯åœºæ™¯å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå¯¹è¯çš„å¤æ‚æ€§å’Œæ½œåœ¨çš„åè§ç´¯ç§¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºLLMåœ¨å¤šè½®å¯¹è¯åœºæ™¯æå‡ºäº†å…¨é¢çš„å…¬å¹³åŸºå‡†æµ‹è¯•ï¼Œåä¸º<strong>FairMT-Bench</strong>ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ¶å®šäº†é’ˆå¯¹LLMå…¬å¹³èƒ½åŠ›çš„ä»»åŠ¡åˆ†ç±»ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šä¸Šä¸‹æ–‡ç†è§£ã€ç”¨æˆ·äº¤äº’å’ŒæŒ‡ä»¤æƒè¡¡ï¼Œæ¯ä¸ªé˜¶æ®µåŒ…å«ä¸¤ä¸ªä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿æ¶µç›–å¤šç§åè§ç±»å‹å’Œå±æ€§ï¼Œæˆ‘ä»¬å€Ÿé‰´äº†ç°æœ‰çš„å…¬å¹³æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿æ„å»ºäº†å¤šè½®å¯¹è¯æ•°æ®é›†<strong>FairMT-10K</strong>ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬åº”ç”¨äº†GPT-4ä»¥åŠåè§åˆ†ç±»å™¨Llama-Guard-3å’Œäººç±»éªŒè¯ä»¥ç¡®ä¿ç¨³å¥æ€§ã€‚åœ¨<strong>FairMT-10K</strong>ä¸Šçš„å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œåœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ï¼Œå½“å‰çš„LLMæ›´å®¹æ˜“äº§ç”Ÿæœ‰åè§çš„å›åº”ï¼Œä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†<strong>FairMT-1K</strong>ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†ä¸Šæµ‹è¯•äº†15ä¸ªå½“å‰æœ€å…ˆè¿›çš„LLMã€‚ç»“æœè¡¨æ˜LLMå½“å‰çš„å…¬å¹³çŠ¶æ€ï¼Œå¹¶å±•ç¤ºäº†è¿™ç§æ–°æ–¹æ³•åœ¨æ›´ç°å®çš„å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ä¸­è¯„ä¼°å…¬å¹³æ€§çš„å®ç”¨æ€§ï¼Œå‘¼åæœªæ¥çš„å·¥ä½œè¦å…³æ³¨LLMå…¬å¹³æ€§çš„æ”¹è¿›ï¼Œå¹¶åœ¨æ­¤ç±»åŠªåŠ›ä¸­é‡‡ç”¨<strong>FairMT-1K</strong>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19317v2">PDF</a> ICLR 2025 spotlight</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨èŠå¤©æœºå™¨äººåº”ç”¨ä¸­çš„å…¬å¹³æ€§æŒ‘æˆ˜ã€‚åœ¨å¤šè½®å¯¹è¯åœºæ™¯ä¸­ï¼ŒLLMé¢ä¸´æ›´å¤§çš„å…¬å¹³æ€§è€ƒéªŒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªå…¨é¢çš„å…¬å¹³æ€§åŸºå‡†æµ‹è¯•FairMT-Benchï¼Œç”¨äºè¯„ä¼°LLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„å…¬å¹³æ€§èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºFairMT-10Kæ•°æ®é›†å¹¶åˆ©ç”¨GPT-4ç­‰æ¨¡å‹è¿›è¡Œå®éªŒåˆ†æï¼Œå‘ç°å½“å‰LLMåœ¨å¤šè½®å¯¹è¯ä¸­æ›´å®¹æ˜“äº§ç”Ÿåè§ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæœ¬æ–‡è¿›ä¸€æ­¥æ¨å‡ºäº†ä¸€ä¸ªæ›´å¯ŒæŒ‘æˆ˜æ€§çš„æ•°æ®é›†FairMT-1Kï¼Œæµ‹è¯•äº†å½“å‰å…ˆè¿›çš„LLMï¼Œå‘¼åæœªæ¥å·¥ä½œå…³æ³¨LLMå…¬å¹³æ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨èŠå¤©æœºå™¨äººåº”ç”¨ä¸­çš„å…¬å¹³æ€§å—åˆ°å…³æ³¨ï¼Œå¤šè½®å¯¹è¯åœºæ™¯å¸¦æ¥æ›´å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†FairMT-BenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMåœ¨å¤šè½®å¯¹è¯ä¸­çš„å…¬å¹³æ€§èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡FairMT-10Kæ•°æ®é›†çš„å®éªŒåˆ†æï¼Œå‘ç°å½“å‰LLMæ›´å®¹æ˜“äº§ç”Ÿåè§å“åº”ã€‚</li>
<li>æ¨å‡ºäº†æŒ‘æˆ˜æ€§çš„FairMT-1Kæ•°æ®é›†ï¼Œæµ‹è¯•äº†å½“å‰å…ˆè¿›çš„LLMåœ¨å…¬å¹³æ€§æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜å½“å‰LLMåœ¨å…¬å¹³æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå±•ç¤ºäº†ä½¿ç”¨FairMT-1Kæ•°æ®é›†è¯„ä¼°å…¬å¹³æ€§çš„å®ç”¨æ€§ã€‚</li>
<li>å¼ºè°ƒæœªæ¥å·¥ä½œåº”å…³æ³¨LLMå…¬å¹³æ€§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c8db37b456adbeec2030a2d87e58d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb1c03e16b480a1297db6819fde214f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5d5ed48c761d58c208bde06d5d2790.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51197bfd6189e1dc8a355bbb327d6c43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0cc149bb3f4e33c8041f54f96ea7f27.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Emphasising-Structured-Information-Integrating-Abstract-Meaning-Representation-into-LLMs-for-Enhanced-Open-Domain-Dialogue-Evaluation"><a href="#Emphasising-Structured-Information-Integrating-Abstract-Meaning-Representation-into-LLMs-for-Enhanced-Open-Domain-Dialogue-Evaluation" class="headerlink" title="Emphasising Structured Information: Integrating Abstract Meaning   Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation"></a>Emphasising Structured Information: Integrating Abstract Meaning   Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation</h2><p><strong>Authors:Bohao Yang, Kun Zhao, Dong Liu, Liang Zhan, Chenghua Lin</strong></p>
<p>Automatic open-domain dialogue evaluation has attracted increasing attention, yet remains challenging due to the complexity of assessing response appropriateness. Traditional evaluation metrics, typically trained with true positive and randomly selected negative responses, tend to assign higher scores to responses that share greater content similarity with contexts. However, adversarial negative responses, despite possessing high lexical overlap with contexts, can be semantically incongruous. Consequently, existing metrics struggle to effectively evaluate such responses, resulting in low correlations with human judgments. While recent studies have demonstrated the effectiveness of Large Language Models (LLMs) for open-domain dialogue evaluation, they still face challenges in handling adversarial negative examples. We propose a novel evaluation framework that integrates Abstract Meaning Representation (AMR) enhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly incorporate AMR graph information through a gating mechanism for enhanced semantic representation learning, while both SLM predictions and AMR knowledge are integrated into LLM prompts for robust evaluation. Extensive experiments on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to state-of-the-art baselines. Our comprehensive ablation studies reveal that AMR graph information contributes substantially more to performance improvements. Our framework achieves strong correlations with human judgments across multiple datasets, establishing a new benchmark for dialogue evaluation. Our code and data are publicly available. </p>
<blockquote>
<p>è‡ªåŠ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·å·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†ç”±äºè¯„ä»·å“åº”é€‚å½“æ€§çš„å¤æ‚æ€§ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿè¯„ä»·æŒ‡æ ‡é€šå¸¸é€šè¿‡ä¸çœŸå®ç§¯æå’Œéšæœºé€‰æ‹©çš„æ¶ˆæå“åº”è¿›è¡Œè®­ç»ƒï¼Œå€¾å‘äºç»™é‚£äº›ä¸ä¸Šä¸‹æ–‡å†…å®¹ç›¸ä¼¼æ€§æ›´é«˜çš„å“åº”èµ‹äºˆæ›´é«˜çš„åˆ†æ•°ã€‚ç„¶è€Œï¼Œå¯¹æŠ—æ€§æ¶ˆæå“åº”å°½ç®¡ä¸ä¸Šä¸‹æ–‡æœ‰é«˜åº¦çš„è¯æ±‡é‡å ï¼Œä½†è¯­ä¹‰ä¸Šå¯èƒ½ä¸ç›¸ç¬¦ã€‚å› æ­¤ï¼Œç°æœ‰æŒ‡æ ‡åœ¨è¯„ä¼°æ­¤ç±»å“åº”æ—¶é‡åˆ°å›°éš¾ï¼Œä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§è¾ƒä½ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´å¤„ç†å¯¹æŠ—æ€§è´Ÿé¢ç¤ºä¾‹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰å¢å¼ºé¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å’ŒLLMã€‚æˆ‘ä»¬çš„SLMé€šè¿‡é—¨æ§æœºåˆ¶æ˜¾å¼åœ°èå…¥AMRå›¾ä¿¡æ¯ï¼Œä»¥ä¿ƒè¿›è¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ï¼Œè€ŒSLMé¢„æµ‹å’ŒAMRçŸ¥è¯†éƒ½è¢«èå…¥åˆ°LLMæç¤ºä¸­ï¼Œä»¥å®ç°ç¨³å¥çš„è¯„ä»·ã€‚åœ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°åŸºçº¿æŠ€æœ¯å…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç»¼åˆæ¶ˆèç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAMRå›¾ä¿¡æ¯å¯¹æ€§èƒ½æå‡è´¡çŒ®æ›´å¤§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§å¾ˆå¼ºï¼Œä¸ºå¯¹è¯è¯„ä»·å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.01129v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶ï¼Œç»“åˆæŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰å¢å¼ºçš„é¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè‡ªåŠ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·ã€‚è¯¥æ¡†æ¶é€šè¿‡é—¨æ§æœºåˆ¶æ˜¾å¼åœ°èå…¥AMRå›¾ä¿¡æ¯ï¼Œå¢å¼ºè¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å°†SLMé¢„æµ‹å’ŒAMRçŸ¥è¯†èå…¥LLMæç¤ºä¸­è¿›è¡Œç¨³å¥è¯„ä»·ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè¯„ä¼°å“åº”çš„é€‚å½“æ€§å¾ˆå¤æ‚ã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡å€¾å‘äºç»™å†…å®¹ä¸Šä¸ä¸Šä¸‹æ–‡æ›´ç›¸ä¼¼çš„å›åº”æ›´é«˜çš„åˆ†æ•°ï¼Œä½†å¯¹æŠ—æ€§è´Ÿé¢å›åº”å°½ç®¡è¯æ±‡ä¸Šä¸ä¸Šä¸‹æ–‡é‡å åº¦é«˜ï¼Œè¯­ä¹‰ä¸Šå´å¯èƒ½ä¸ç›¸ç¬¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·ä¸­è™½ç„¶æœ‰æ•ˆï¼Œä½†ä»é¢ä¸´å¤„ç†å¯¹æŠ—æ€§è´Ÿé¢ç¤ºä¾‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„è¯„ä¼°æ¡†æ¶ç»“åˆäº†æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰å¢å¼ºçš„é¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¸LLMsï¼Œé€šè¿‡é—¨æ§æœºåˆ¶æ˜¾å¼åœ°èå…¥AMRå›¾ä¿¡æ¯ï¼Œå¢å¼ºè¯­ä¹‰è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>æ¡†æ¶å°†SLMé¢„æµ‹å’ŒAMRçŸ¥è¯†èå…¥LLMæç¤ºä¸­ï¼Œå®ç°ç¨³å¥è¯„ä»·ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æ”¾åŸŸå¯¹è¯è¯„ä»·ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.01129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-996815cfbd9892db257e79718db24fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51781f95e501ce1247f76b3c3a577914.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f7c4a7b6d391e388379ad7be03a00c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fb5d04238aa831540ba72ea5fff8f8b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience"><a href="#Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience" class="headerlink" title="Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience"></a>Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience</h2><p><strong>Authors:Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani</strong></p>
<p>In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces â€œListen, Chat, and Remixâ€ (LCR), a novel multimodal sound remixer that controls each sound source in a mixture based on user-provided text instructions. LCR distinguishes itself with a user-friendly text interface and its unique ability to remix multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for remixing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles filtered components back to the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse remixing tasks including extraction, removal, and volume control of single or multiple sources. Our experiments demonstrate significant improvements in signal quality across all remixing tasks and robust performance in zero-shot scenarios with varying numbers and types of sound sources. An audio demo is available at: <a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a>. </p>
<blockquote>
<p>åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°å„ç§å£°éŸ³ï¼ŒåŒ…æ‹¬å¸Œæœ›å¬åˆ°çš„å’Œä¸æƒ³å¬åˆ°çš„ï¼Œä½†å¯¹å®ƒä»¬çš„å­˜åœ¨å’ŒéŸ³é‡åªæœ‰æœ‰é™çš„æ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†â€œListen, Chat, and Remixâ€ï¼ˆLCRï¼‰è¿™ä¸€æ–°å‹å¤šæ¨¡å¼å£°éŸ³æ··éŸ³å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤æ§åˆ¶æ··åˆä¸­çš„å„ç§å£°éŸ³æºã€‚LCRé€šè¿‡å‹å¥½çš„æ–‡æœ¬æ¥å£åŠå…¶åœ¨åŒä¸€æ··åˆç‰©ä¸­åŒæ—¶æ··éŸ³å¤šä¸ªå£°éŸ³æºçš„ç‹¬ç‰¹èƒ½åŠ›æ¥åŒºåˆ†è‡ªå·±ï¼Œè€Œæ— éœ€å°†å®ƒä»¬åˆ†å¼€ã€‚ç”¨æˆ·è¾“å…¥å¼€æ”¾è¯æ±‡æ–‡æœ¬æç¤ºï¼Œç”±å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šä»¥åˆ›å»ºç”¨äºæ··éŸ³å£°éŸ³æ··åˆç‰©çš„è¯­ä¹‰è¿‡æ»¤å™¨ã€‚ç„¶åï¼Œç³»ç»Ÿå°†æ··åˆç‰©åˆ†è§£æˆå…¶ç»„æˆéƒ¨åˆ†ï¼Œåº”ç”¨è¯­ä¹‰è¿‡æ»¤å™¨ï¼Œå¹¶å°†è¿‡æ»¤åçš„ç»„ä»¶é‡æ–°ç»„è£…ä¸ºæ‰€éœ€çš„è¾“å‡ºã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡10ä¸‡æ··åˆç‰©çš„160å°æ—¶æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬è¯­éŸ³å’Œå„ç§éŸ³é¢‘æºä»¥åŠç”¨äºå„ç§æ··éŸ³ä»»åŠ¡çš„æ–‡æœ¬æç¤ºï¼ŒåŒ…æ‹¬æå–ã€åˆ é™¤å’Œå•ä¸ªæˆ–å¤šä¸ªæºçš„å£°éŸ³æ§åˆ¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜åœ¨æ‰€æœ‰æ··éŸ³ä»»åŠ¡ä¸­ä¿¡å·è´¨é‡éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨å…·æœ‰ä¸åŒæ•°é‡å’Œç±»å‹çš„éŸ³æºçš„åœºæ™¯ä¸­å…·æœ‰ç¨³å¥çš„æ€§èƒ½è¡¨ç°ã€‚éŸ³é¢‘æ¼”ç¤ºå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03710v2">PDF</a> Accepted by IEEE Journal of Selected Topics in Signal Processing   (JSTSP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œListenï¼ŒChatï¼Œand Remixâ€ï¼ˆLCRï¼‰çš„æ–°å‹å¤šæ¨¡å¼å£°éŸ³æ··éŸ³å™¨ï¼Œå®ƒå¯æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤æ§åˆ¶æ··åˆä¸­çš„æ¯ä¸ªå£°éŸ³æºã€‚LCRé€šè¿‡ç”¨æˆ·å‹å¥½çš„æ–‡æœ¬ç•Œé¢ï¼Œæ— éœ€åˆ†ç¦»å³å¯åŒæ—¶æ··éŸ³å¤šä¸ªå£°éŸ³æºã€‚ç³»ç»Ÿé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šç”¨æˆ·è¾“å…¥çš„å¼€æ”¾è¯æ±‡æ–‡æœ¬æç¤ºï¼Œåˆ›å»ºè¯­ä¹‰è¿‡æ»¤å™¨ä»¥æ··éŸ³å£°éŸ³æ··åˆç‰©ã€‚å®éªŒè¯æ˜ï¼ŒLCRåœ¨æ‰€æœ‰æ··éŸ³ä»»åŠ¡ä¸­çš„ä¿¡å·è´¨é‡éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°é‡å’Œç±»å‹çš„éŸ³æºåœºæ™¯ä¸‹è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCRæ˜¯ä¸€ç§å¤šæ¨¡å¼å£°éŸ³æ··éŸ³å™¨ï¼Œèƒ½å¤Ÿæ§åˆ¶æ··åˆä¸­çš„æ¯ä¸ªå£°éŸ³æºï¼ŒåŸºäºç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤è¿›è¡Œæ“ä½œã€‚</li>
<li>LCRå…·å¤‡ç”¨æˆ·å‹å¥½çš„æ–‡æœ¬ç•Œé¢ï¼Œå¯ä»¥æ–¹ä¾¿åœ°åˆ›å»ºè¯­ä¹‰è¿‡æ»¤å™¨ä»¥æ··éŸ³å£°éŸ³æ··åˆç‰©ã€‚</li>
<li>LCRèƒ½å¤ŸåŒæ—¶æ··éŸ³å¤šä¸ªå£°éŸ³æºï¼Œè€Œæ— éœ€äº‹å…ˆå°†å…¶åˆ†ç¦»ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æç¤ºï¼Œä¸ºæ··éŸ³è¿‡ç¨‹æä¾›è¯­ä¹‰è¿‡æ»¤ã€‚</li>
<li>LCRæ‹¥æœ‰160å°æ—¶çš„æ•°æ®é›†ï¼ŒåŒ…å«è¯­éŸ³å’Œå„ç§éŸ³é¢‘æºï¼Œä»¥åŠç”¨äºå„ç§æ··éŸ³ä»»åŠ¡çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒLCRåœ¨ä¿¡å·è´¨é‡ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶ä¸”å¯¹ä¸åŒç±»å‹å’Œæ•°é‡çš„å£°éŸ³æºè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dd10c16d26f590cbf14dcd0ddcf596e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e4a9c637a1f386f8f6db21f83434ec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc06ba878ad5fe8d3e3c90572ca5a480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8531b4f1e51e0995113da1f15c1c31c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89f167e5f57e22f658633e04e0ae2322.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ce442293e5f9981fa7bb76deb83ae172.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  HunyuanVideo-HOMA Generic Human-Object Interaction in Multimodal Driven   Human Animation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  EmoNet-Voice A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
