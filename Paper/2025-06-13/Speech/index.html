<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-13  EmoNet-Voice A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7dd10c16d26f590cbf14dcd0ddcf596e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-13-更新"><a href="#2025-06-13-更新" class="headerlink" title="2025-06-13 更新"></a>2025-06-13 更新</h1><h2 id="EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection"><a href="#EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection" class="headerlink" title="EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection"></a>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection</h2><p><strong>Authors:Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer</strong></p>
<p>The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. </p>
<blockquote>
<p>文本转语音和音频生成模型的进步为评估人工智能系统的情感理解能力提供了强大的基准。当前语音情感识别（SER）数据集在情感粒度、隐私担忧或依赖表演表现方面存在局限性。本文介绍了用于语音情感检测的新资源EmoNet-Voice，包括EmoNet-Voice Big，这是一个大规模预训练数据集（跨越11种声音、40种情感和4种语言，包含超过4500小时的语音），以及带有专家注释的新型基准数据集EmoNet-Voice Bench。EmoNet-Voice旨在评估SER模型在40个情感类别的精细粒度光谱上的表现，具有不同的强度级别。我们利用最先进的语音生成技术，精心制作了模拟演员表现特定情绪场景的合成音频片段。关键的是，我们由心理学专家进行了严格验证，他们分配了感知强度标签。这种合成、保护隐私的方法可以包含现有数据集中通常不存在的敏感情感状态。最后，我们推出了Empathic Insight Voice模型，该模型在语音情感识别方面树立了新的标准，与人类专家的契合度极高。我们在当前模型景观中的评估展现了有价值的发现，例如高唤醒情感（如愤怒）比低唤醒状态（如专注）更容易检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09827v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个新型的语音情感识别数据集EmoNet-Voice，包括大规模预训练数据集EmoNet-Voice Big和新基准数据集EmoNet-Voice Bench。该数据集涵盖了40种情感类别的精细粒度，并利用先进的语音生成技术模拟演员演绎特定情感的场景。通过心理学专家进行的严格验证，确保情感强度的准确性。此外，采用合成和隐私保护的方法，可以包含现有数据集中缺失的敏感情感状态。最后，引入了Empathic Insight Voice模型，为语音情感识别设立了新的标准，并揭示了如愤怒等高唤起的情绪更容易被检测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入新型语音情感识别数据集EmoNet-Voice，包含大规模预训练数据集和新基准数据集。</li>
<li>涵盖40种情感类别的精细粒度，满足不同强度的情感检测需求。</li>
<li>利用先进的语音生成技术模拟真实情感场景。</li>
<li>通过心理学专家验证情感强度的准确性。</li>
<li>采用合成和隐私保护方法，包含敏感情感状态。</li>
<li>引入Empathic Insight Voice模型，为语音情感识别设立新标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-434d9a1abef531c21b5483583b56d3c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead12f776165c52d3bd1fc04d82aeaba.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Speech-Assessment-with-Visual-Cues"><a href="#A-Study-on-Speech-Assessment-with-Visual-Cues" class="headerlink" title="A Study on Speech Assessment with Visual Cues"></a>A Study on Speech Assessment with Visual Cues</h2><p><strong>Authors:Shafique Ahmed, Ryandhimas E. Zezario, Nasir Saleem, Amir Hussain, Hsin-Min Wang, Yu Tsao</strong></p>
<p>Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397-&gt;0.9205) for PESQ and 11.47% (0.7403-&gt;0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment. </p>
<blockquote>
<p>在无干净参考信号的情况下，进行非侵入性的语音质量和清晰度评估至关重要。在这项工作中，我们提出了一个多模态框架，该框架融合了音频特征和视觉线索来预测PESQ和STOI分数。它采用双分支架构，其中通过STFT提取光谱特征，并通过视觉编码器获得视觉嵌入。然后，这些特征通过带有注意力的CNN-BLSTM进行融合和处理，随后进行多任务学习，以同时预测PESQ和STOI。在LRS3-TED数据集上的评估，辅以来自DEMAND语料库的噪声增强，表明我们的模型优于仅使用音频的基线模型。在未见过的噪声条件下，PESQ的LCC提高了9.61%（从0.8397提高到0.9205），STOI的LCC提高了11.47%（从0.7403提高到0.8253）。这些结果凸显了结合视觉线索在提高非侵入性语音评估的准确性方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09549v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本研究提出一种非侵入性的多模态评估框架，该框架在无法获取干净参考信号的情况下，通过整合音频特征和视觉线索来预测PESQ和STOI分数。该框架采用双分支架构，从音频信号提取光谱特征，并通过视觉编码器获取视觉嵌入。这些特征经过融合后，通过结合卷积神经网络和双向长短时记忆网络的模型进行处理，并引入注意力机制。通过多任务学习，该模型能同时预测PESQ和STOI分数。在LRS3-TED数据集上的实验表明，与仅使用音频的基线模型相比，该模型表现出更好的性能。在面对未见过的噪声条件下，PESQ的LCC提高了9.61%，STOI的LCC提高了11.47%。结果证明了结合视觉线索能有效提高非侵入性语音评估的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非侵入性地评估语音质量和可懂度在缺乏干净参考信号时尤为重要。</li>
<li>研究提出了一种多模态评估框架，融合音频和视觉信息来预测语音质量指数（PESQ）和短时光信号可懂度指数（STOI）。</li>
<li>框架采用双分支架构，分别处理音频和视觉信息，通过卷积神经网络和双向长短时记忆网络（CNN-BLSTM）结合注意力机制进行特征融合和处理。</li>
<li>该框架采用了多任务学习以同时预测PESQ和STOI分数。</li>
<li>在LRS3-TED数据集上的实验表明，该模型在未见噪声条件下性能优异，相较于基线模型有显著提升。</li>
<li>模型的性能提升体现在对语音质量和可懂度的准确评估上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09549">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9524bef04cddd1458d037663e92d7dd8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83774bb864594c8dbb762b92f257830d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15434a35504bff902bb3a74587cbb60c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6797e7d69244ce6a9d086d62403cf1e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniDRCA-Parallel-Speech-Text-Foundation-Model-via-Dual-Resolution-Speech-Representations-and-Contrastive-Alignment"><a href="#OmniDRCA-Parallel-Speech-Text-Foundation-Model-via-Dual-Resolution-Speech-Representations-and-Contrastive-Alignment" class="headerlink" title="OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment"></a>OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment</h2><p><strong>Authors:Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang, Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, Chong Zhang, Yukun Ma, Yafeng Chen, Hui Wang, Jiaqing Liu, Jieping Ye</strong></p>
<p>Recent studies on end-to-end speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM’s autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling, featuring dual-resolution speech representations and contrastive cross-modal alignment. Our approach processes speech and text representations in parallel while enhancing audio comprehension through contrastive alignment. Experimental results on Spoken Question Answering benchmarks demonstrate that OmniDRCA establishes new state-of-the-art (SOTA) performance among parallel joint speech-text modeling based foundation models, and achieves competitive performance compared to interleaved models. Additionally, we explore the potential of extending the framework to full-duplex conversational scenarios. </p>
<blockquote>
<p>关于使用大型语言模型（LLM）进行端到端语音生成的最新研究已经引起了社区的关注，多项工作将基于文本的LLM扩展到生成离散语音标记。现有方法主要可分为两类：（1）独立生成离散语音标记的方法，这些方法不会将其纳入LLM的自回归过程中，导致文本生成不知道当前的语音合成。（2）通过联合自回归建模生成交错或并行语音文本标记的模型，从而在生成过程中实现跨模态感知。本文介绍了OmniDRCA，一种基于联合自回归建模的并行语音文本基础模型，具有双分辨率语音表示和对比跨模态对齐功能。我们的方法并行处理语音和文本表示，同时通过对比对齐提高音频理解。在口语问答基准测试上的实验结果表明，OmniDRCA在基于并行联合语音文本建模的基础模型中建立了新的最先进的性能，并在与交错模型的比较中取得了具有竞争力的性能。此外，我们还探讨了将框架扩展到全双工对话场景的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09349v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>近期关于利用大型语言模型（LLMs）进行端到端语音生成的研究引起了社区的关注，多项工作将文本基础的LLMs扩展至生成离散语音标记。现有方法主要分为两类：（1）独立生成离散语音标记的方法，不将其纳入LLM的自回归过程，导致文本生成无法意识到并发的语音合成。（2）通过联合自回归建模生成交织或并行语音文本标记的模型，在生成过程中实现模态间的相互感知。本文提出了基于联合自回归建模的OmniDRCA并行语音文本基础模型，具有双分辨率语音表示和对比跨模态对齐的特点。该方法在并行处理语音和文本表示的同时，通过对齐对比增强音频理解。在口语问答基准测试上的实验结果表明，OmniDRCA在基于并行联合语音文本建模的基础模型中建立了新的最先进的性能，并在交织模型中实现了具有竞争力的性能。此外，我们还探讨了将框架扩展到全双工对话场景的可能性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>端到端语音生成研究正吸引社区关注，大型语言模型（LLMs）被应用于生成离散语音标记。</li>
<li>现有方法分为独立生成和联合自回归建模两类。</li>
<li>OmniDRCA模型采用基于联合自回归建模的并行语音文本基础架构。</li>
<li>OmniDRCA具有双分辨率语音表示和对比跨模态对齐特点。</li>
<li>模型能并行处理语音和文本表示，增强音频理解。</li>
<li>在口语问答基准测试上，OmniDRCA表现出先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b74cf6f5443b481996530d1700600b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d5ca7802866f9586dbadb2d15fb9de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-035877bff8289b603e5b93fe4a79be5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Advancing-STT-for-Low-Resource-Real-World-Speech"><a href="#Advancing-STT-for-Low-Resource-Real-World-Speech" class="headerlink" title="Advancing STT for Low-Resource Real-World Speech"></a>Advancing STT for Low-Resource Real-World Speech</h2><p><strong>Authors:Flavio D’Intino, Hans-Peter Hutter</strong></p>
<p>Swiss German is a low-resource language represented by diverse dialects that differ significantly from Standard German and from each other, lacking a standardized written form. As a result, transcribing Swiss German involves translating into Standard German. Existing datasets have been collected in controlled environments, yielding effective speech-to-text (STT) models, but these models struggle with spontaneous conversational speech.   This paper, therefore, introduces the new SRB-300 dataset, a 300-hour annotated speech corpus featuring real-world long-audio recordings from 39 Swiss German radio and TV stations. It captures spontaneous speech across all major Swiss dialects recorded in various realistic environments and overcomes the limitation of prior sentence-level corpora.   We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, achieving notable enhancements over previous zero-shot performance metrics. Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for developing effective and robust STT systems for Swiss German and other low-resource languages in real-world contexts. </p>
<blockquote>
<p>瑞士德语是一种资源匮乏的语言，由多种方言组成，这些方言与标准德语以及彼此之间存在显著差异，并且没有标准化的书面形式。因此，转录瑞士德语需要将其翻译为标准德语。现有的数据集已在受控环境中收集，产生了有效的语音识别（STT）模型，但这些模型在处理日常会话语音时遇到困难。因此，本文介绍了新的SRB-300数据集，这是一个包含来自瑞士德语广播电台和电视台的300小时注释语音语料库的真实世界长音频录音。它捕捉了来自各种现实环境的所有主要瑞士方言的即兴演讲，克服了以前句子层面语料库的局限性。我们在SRB-300数据集上微调了多个OpenAIwhisper模型，相较于之前的零样本性能评价指标取得了显著的提升。单词错误率（WER）的改进范围在19%至33%之间，而BLEU分数提高了8%至40%。最佳的微调模型“大型v3”实现了单词错误率为17.1%，BLEU分数为74.8%。这一进展对于开发适用于瑞士德语和其他资源匮乏的语言的有效和稳健的STT系统在现实世界中具有重要意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08836v1">PDF</a> Conference: HCI International 2025, 20 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了针对瑞士德语——一种缺乏标准化书面形式且表现为多样方言的语言——所创建的新数据集SRB-300。该数据集包含来自瑞士各大方言区的真实长音频记录，能反映出自发的口语表达。此外，文章还介绍了使用OpenAI Whisper模型在该数据集上进行微调所取得的显著成果，显著提高了语音转文本的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>瑞士德语缺乏标准化书面形式，表现为多样的方言，且各方言间差异显著。</li>
<li>现有数据集主要在受控环境下收集，对于自然口语表达的识别效果有限。</li>
<li>本文引入了新的SRB-300数据集，包含来自瑞士各大方言区的真实长音频记录，旨在解决这一难题。</li>
<li>使用OpenAI Whisper模型在SRB-300数据集上进行微调，显著提高了语音转文本的准确率。</li>
<li>在微调过程中，字错误率（WER）降低了19%至33%，BLEU分数提高了8%至40%。</li>
<li>最佳微调模型（large-v3）在WER和BLEU分数方面达到了显著成果，分别为17.1%和74.8%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af7238b5e55b366bded11e21879030a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d0707e56feb32549c4168f275f269b5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Teacher-Language-Aware-Knowledge-Distillation-for-Multilingual-Speech-Emotion-Recognition"><a href="#Multi-Teacher-Language-Aware-Knowledge-Distillation-for-Multilingual-Speech-Emotion-Recognition" class="headerlink" title="Multi-Teacher Language-Aware Knowledge Distillation for Multilingual   Speech Emotion Recognition"></a>Multi-Teacher Language-Aware Knowledge Distillation for Multilingual   Speech Emotion Recognition</h2><p><strong>Authors:Mehedi Hasan Bijoy, Dejan Porjazovski, Tamás Grósz, Mikko Kurimo</strong></p>
<p>Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness. </p>
<blockquote>
<p>语音情绪识别（SER）对于改善人机交互至关重要。尽管单语SER取得了进展，但将其扩展到构建多语言系统仍然具有挑战性。我们的目标是通过从多个教师模型中提炼知识，训练一个能够进行多语言SER的单一模型。为解决这一问题，我们引入了一种新型的语言感知多教师知识提炼方法，以推进英语、芬兰语和法语中的SER。它利用Wav2Vec2.0作为单语教师模型的基础，然后将知识提炼到一个单一的多语言学生模型中。学生模型表现出卓越的性能，在英语数据集上的加权召回率为72.9，芬兰数据集上的未加权召回率为63.4，超过了微调和知识提炼的基线。我们的方法在改进悲伤和中性情绪的召回方面表现出色，尽管在识别愤怒和快乐情绪方面仍然面临挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08717v1">PDF</a> Accepted to INTERSPEECH 2025 conference</p>
<p><strong>Summary</strong></p>
<p>基于情感识别在提升人机交互中的重要性，当前对于跨多语种情感识别的挑战正在加大。研究目标是通过多教师模型知识蒸馏的方式训练出一个单语种情感识别模型，并扩展到多语种情感识别领域。该研究引入了一种新型的语言感知多教师知识蒸馏方法，以推进英语、芬兰语和法语的情感识别。该研究以Wav2Vec2.0为基础构建单语种教师模型，并将其知识蒸馏到单一的多语种学生模型中。该学生模型表现出卓越的性能，在英语数据集上的加权召回率为72.9%，芬兰数据集上的非加权召回率为63.4%，超越了微调与知识蒸馏基线。该研究尤其擅长提高悲伤和中性情感的识别能力，但在愤怒和快乐情感的识别上仍面临挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究目标是利用多教师模型知识蒸馏训练单语种情感识别模型并扩展为多语种。</li>
<li>提出一种语言感知的多教师知识蒸馏新方法以提升多语种情感识别。</li>
<li>使用Wav2Vec2.0作为基础构建教师模型，然后将其蒸馏至学生模型。</li>
<li>学生模型展现出优异性能，包括英语数据集上的加权召回率和芬兰数据集上的非加权召回率均达到较高水平。</li>
<li>该方法擅长识别悲伤和中性情感，但在愤怒和快乐情感的识别上仍有提升空间。</li>
<li>研究实现了知识蒸馏和模型训练的有效结合，提升了模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5a904f34c7e650b6ba9522c24c2ca6af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f86da02bd1edede585444312ff82bd5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b15dacda69f35e1d63f414f16eab1cab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d39a9d13091545c0114419184f343cd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Review-on-Score-based-Generative-Models-for-Audio-Applications"><a href="#A-Review-on-Score-based-Generative-Models-for-Audio-Applications" class="headerlink" title="A Review on Score-based Generative Models for Audio Applications"></a>A Review on Score-based Generative Models for Audio Applications</h2><p><strong>Authors:Ge Zhu, Yutong Wen, Zhiyao Duan</strong></p>
<p>Diffusion models have emerged as powerful deep generative techniques, producing high-quality and diverse samples in applications in various domains including audio. These models have many different design choices suitable for different applications, however, existing reviews lack in-depth discussions of these design choices. The audio diffusion model literature also lacks principled guidance for the implementation of these design choices and their comparisons for different applications. This survey provides a comprehensive review of diffusion model design with an emphasis on design principles for quality improvement and conditioning for audio applications. We adopt the score modeling perspective as a unifying framework that accommodates various interpretations, including recent approaches like flow matching. We systematically examine the training and sampling procedures of diffusion models, and audio applications through different conditioning mechanisms. To address the lack of audio diffusion model codebases and to promote reproducible research and rapid prototyping, we introduce an open-source codebase at <a target="_blank" rel="noopener" href="https://github.com/gzhu06/AudioDiffuser">https://github.com/gzhu06/AudioDiffuser</a> that implements our reviewed framework for various audio applications. We demonstrate its capabilities through three case studies: audio generation, speech enhancement, and text-to-speech synthesis, with benchmark evaluations on standard datasets. </p>
<blockquote>
<p>扩散模型已经作为强大的深度生成技术出现，在各种领域的应用中产生了高质量和多样化的样本，包括音频。这些模型有许多不同的设计选择，适合不同的应用，然而，现有的评论缺乏对这些设计选择的深入讨论。音频扩散模型文献也缺乏这些设计选择实施的原则性指导以及它们在不同应用中的比较。这篇综述对扩散模型设计进行了全面回顾，重点介绍了改进质量和音频应用条件的设计原则。我们采用评分建模视角作为统一框架，可以容纳各种解释，包括最新的方法如流匹配。我们系统地研究了扩散模型的训练和采样程序，以及通过不同条件机制进行的音频应用程序。为了解决音频扩散模型代码库缺乏的问题，并推动可重复的研究和快速原型设计，我们在<a target="_blank" rel="noopener" href="https://github.com/gzhu06/AudioDiffuser%E4%B8%8A%E6%8E%A8%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E5%BA%93%EF%BC%8C%E4%B8%BA%E5%90%84%E7%A7%8D%E9%9F%B3%E9%A2%91%E5%BA%94%E7%94%A8%E5%AE%9E%E7%8E%B0%E4%BA%86%E6%88%91%E4%BB%AC%E5%9B%9E%E9%A1%BE%E8%BF%87%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82%E6%88%91%E4%BB%AC%E9%80%9A%E8%BF%87%E4%B8%89%E4%B8%AA%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E5%B1%95%E7%A4%BA%E4%BA%86%E5%85%B6%E8%83%BD%E5%8A%9B%EF%BC%9A%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E3%80%81%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA%E5%92%8C%E6%96%87%E6%9C%AC%E5%88%B0%E8%AF%AD%E9%9F%B3%E7%9A%84%E5%90%88%E6%88%90%EF%BC%8C%E5%B9%B6%E5%9C%A8%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%9F%BA%E5%87%86%E8%AF%84%E4%BC%B0%E3%80%82">https://github.com/gzhu06/AudioDiffuser上推出了一个开源代码库，为各种音频应用实现了我们回顾过的框架。我们通过三个案例研究展示了其能力：音频生成、语音增强和文本到语音的合成，并在标准数据集上进行了基准评估。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08457v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>扩散模型作为强大的深度生成技术，已广泛应用于包括音频在内的多个领域的应用中，并能产生高质量和多样化的样本。本文为扩散模型设计提供了全面的综述，重点介绍了提高质量和音频应用的条件设计原则。本文从得分建模的角度出发，统一框架接纳了各种解读，包括流程匹配等最新方法。系统地探讨了扩散模型的训练和采样过程以及通过不同条件机制在音频应用中的应用。为解决音频扩散模型代码库缺乏的问题，促进可重复研究和快速原型设计，本文引入了开源代码库，并展示了其在音频生成、语音增强和文本到语音合成三个案例研究中的能力，同时在标准数据集上进行基准评估。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型已成为强大的深度生成技术，广泛应用于多个领域，包括音频。</li>
<li>现有文献缺乏对扩散模型设计的深入探讨，特别是在音频应用方面的原则性指导。</li>
<li>本文提供扩散模型设计的全面综述，重点介绍设计原则以提高质量和音频应用的条件。</li>
<li>采用得分建模的角度作为统一框架，接纳各种解读，包括流程匹配等最新方法。</li>
<li>系统地探讨扩散模型的训练和采样过程。</li>
<li>介绍开源代码库，解决音频扩散模型代码库缺乏的问题，促进可重复研究和快速原型设计。</li>
<li>通过音频生成、语音增强和文本到语音合成三个案例研究展示其能力，并在标准数据集上进行基准评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f9e16552380d590d116459549aa33f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616fc2c99ee5d5cf0d6299e3ad16ed8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-299d24dcf1490c9f35aa3a3a396ab484.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multilingual-Hate-Speech-Detection-in-Social-Media-Using-Translation-Based-Approaches-with-Large-Language-Models"><a href="#Multilingual-Hate-Speech-Detection-in-Social-Media-Using-Translation-Based-Approaches-with-Large-Language-Models" class="headerlink" title="Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models"></a>Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models</h2><p><strong>Authors:Muhammad Usman, Muhammad Ahmad, M. Shahiki Tash, Irina Gelbukh, Rolando Quintero Tellez, Grigori Sidorov</strong></p>
<p>Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleiss’ Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide. </p>
<blockquote>
<p>社交媒体平台是公众话语的重要空间，塑造意见和社区动态，然而它们的广泛使用也放大了有害内容，特别是仇恨言论，威胁网络安全和包容性。虽然英语和西班牙语的仇恨言论检测已经得到了广泛研究，但乌尔都语仍然被探索得不够深入，尤其是使用基于翻译的方法。为了弥补这一空白，我们引入了一个包含英语（3834个样本）、乌尔都语（3197个样本）和西班牙语（3162个样本）的三种语言数据集，通过关键词过滤收集，包含4849个仇恨标签和5344个非仇恨标签的平衡分布。我们的方法利用注意力层作为基于Transformer模型和大语言模型（LLM）的先决条件，增强特征提取能力，用于多语言仇恨言论检测。对于非Transformer模型，我们使用TF-IDF进行特征提取。该数据集采用最先进的模型进行基准测试，包括GPT-3.5 Turbo和Qwen 2.5 72B，以及传统的机器学习模型如SVM和其他Transformer（如BERT、RoBERTa）。三个注释者遵循严格的指导方针，确保数据集的高质量，达到Fleiss Kappa值为0.821。我们的方法将注意力层与GPT-3.5 Turbo和Qwen 2.5 72B相结合，取得了强大的性能表现。英语宏F1分数为0.87（GPT-3.5 Turbo），西班牙语为0.85（GPT-3.5 Turbo），乌尔都语为0.81（Qwen 2.5 72B），多语言联合模型为0.88（Qwen 2.5 72B）。这些结果反映了相对于SVM基准线的改进：英语提高8.75%（从0.80提高到0.87），西班牙语提高8.97%（从0.78提高到0.87），乌尔都语提高5.19%（从0.77提高到0.82），多语言联合模型提高7.32%（从0.82提高到0.88）。我们的框架为多语言仇恨言论检测提供了稳健的解决方案，促进了全球更安全的数字社区建设。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08147v1">PDF</a> </p>
<p><strong>Summary</strong>：社交媒体平台是公众话语的重要空间，塑造意见和社区动态，但其广泛使用也放大了有害内容，特别是仇恨言论，威胁网络安全和包容性。针对多语言仇恨言论检测的研究尚存空白，本研究引入了一个包含英语、乌尔都语和西班牙语的三语数据集，并利用注意力层作为基于Transformer的大型语言模型的预处理步骤，以提高特征提取能力。该研究使用了前沿模型如GPT-3.5 Turbo和Qwen 2.5 72B以及传统的机器学习方法进行基准测试。最终研究表明该研究实现了在多语言环境下的高效仇恨言论检测，有助于提高全球数字社区的安全性。研究基于文本创建的分类模型能够为有效检测多语言环境下的仇恨言论提供坚实基础。总体而言，此数据集旨在实现更高效的多语言仇恨言论检测以促进网络安全发展，造福人类社会。这是一项具有重要意义的工作，对整个社交媒体发展也至关重要。它的数据集和创新方法可以推广到相关领域为更多的任务提供帮助。希望这项工作能够促进网络安全的发展，进一步保护用户的在线安全和个人隐私。这将是一项重大的贡献，具有广阔的应用前景。我们相信该工作将对未来的社交媒体平台产生积极影响。同时，该研究对于理解社交媒体平台上的仇恨言论传播机制以及制定有效的应对策略具有重要意义。研究团队的工作值得赞赏和支持。尽管挑战重重，他们仍取得了令人瞩目的成果。我们将密切关注该研究的后续进展和实际应用情况。<br><strong>Key Takeaways</strong>:</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-591fa32e8fbb94df72edc74843ea10b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92eafcb69d8203b7fd01f45e11b1c695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97d484b0421e6b3fba39dc98626d5a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-511dacfbfee786817b512c6c73eb500b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94e16a64785f1ddbcd90d51424549dfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1026150a7805dbf57506ddb1009e1e1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42922acae3f4ebc70f6b2793266ec126.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LID-Models-are-Actually-Accent-Classifiers-Implications-and-Solutions-for-LID-on-Accented-Speech"><a href="#LID-Models-are-Actually-Accent-Classifiers-Implications-and-Solutions-for-LID-on-Accented-Speech" class="headerlink" title="LID Models are Actually Accent Classifiers: Implications and Solutions   for LID on Accented Speech"></a>LID Models are Actually Accent Classifiers: Implications and Solutions   for LID on Accented Speech</h2><p><strong>Authors:Niyati Bafna, Matthew Wiesner</strong></p>
<p>Prior research indicates that LID model performance significantly declines on accented speech; however, the specific causes, extent, and characterization of these errors remain under-explored. (i) We identify a common failure mode on accented speech whereby LID systems often misclassify L2 accented speech as the speaker’s native language or a related language. (ii) We present evidence suggesting that state-of-the-art models are invariant to permutations of short spans of speech, implying they classify on the basis of short phonotactic features indicative of accent rather than language. Our analysis reveals a simple method to enhance model robustness to accents through input chunking. (iii) We present an approach that integrates sequence-level information into our model without relying on monolingual ASR systems; this reduces accent-language confusion and significantly enhances performance on accented speech while maintaining comparable results on standard LID. </p>
<blockquote>
<p>先前的研究表明，LID模型在有口音的语音上的表现会显著下降；然而，这些错误的具体原因、程度和特征仍然缺乏深入探索。（i）我们确定了在有口音的语音上的一种常见失效模式，即LID系统经常将带有口音的第二语言误分类为说话人的母语或相关语言。（ii）我们提供了证据表明，最先进的模型对于短语音段的排列具有不变性，这意味着它们是基于指示口音而非语言的短音系特征进行分类的。我们的分析揭示了一种通过输入分块来提高模型对口音的稳健性的简单方法。（iii）我们提出了一种将序列级别的信息集成到模型中的方法，无需依赖单语种自动语音识别系统；这减少了口音语言混淆，并显著提高了在有口音的语音上的性能，同时保持了标准LID的相当结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00628v2">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了语言身份识别（LID）模型在带有口音的语音上的性能问题。研究发现，LID模型在处理带有口音的语音时会出现误分类的情况，即将非母语带有口音的语音错误地识别为说话者的母语或相关语言。文章分析了当前模型的局限性，并提出了一种通过输入分块增强模型对口音的鲁棒性的方法。此外，还介绍了一种不依赖单语自动语音识别系统的方法，将序列级别的信息融入模型中，从而减少口音引起的语言混淆，同时保持对标准LID的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LID模型在处理带口音语音时性能显著下降，存在将L2口音语音误分类为母语或相关语言的常见失败模式。</li>
<li>当前先进模型对短语音片段的排列顺序具有不变性，暗示它们基于短语的音韵特征而非语言进行分类。</li>
<li>通过输入分块可以提高模型对口音的鲁棒性。</li>
<li>提出了一种集成序列级别信息的方法，无需依赖单语自动语音识别系统。</li>
<li>该方法能减少因口音导致的语言混淆，显著提高带口音语音的识别性能。</li>
<li>该方法在标准LID上的性能维持不变。</li>
<li>这些发现对于改进LID模型的性能和适应性具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58ee9d581bad8eb626c901e3b575d0fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84e1502fde70503d34a2f763cc0b82ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28397b80e70b087f80dfee8818cf7002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2a24dcaa347db9e2df90db5738f8014.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement"><a href="#LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement" class="headerlink" title="LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement"></a>LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement</h2><p><strong>Authors:Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie</strong></p>
<p>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area. </p>
<blockquote>
<p>最近的语言模型（LM）进展表明，其在语义理解和上下文建模方面展现出强大的能力，并在生成性语音增强（SE）中繁荣发展。然而，许多基于LM的SE方法主要关注语义信息，往往忽视了声音信息的关键作用，这导致增强后的语音出现声音不一致，以及在各种SE任务中的泛化能力受限。在本文中，我们介绍了LLaSE-G1，这是一个基于LLaMA的语言模型，旨在激励其在语音增强方面的泛化能力。LLaSE-G1有以下关键贡献：首先，为了缓解声音不一致的问题，LLaSE-G1采用WavLM的连续表示作为输入，并通过X-Codec2预测语音令牌，最大限度地保留声音。其次，为了提升泛化能力，LLaSE-G1引入了双通道输入和输出，可以统一多种SE任务而无需特定任务标识。第三，LLaSE-G1在测试时间展现出规模化效果，优于之前的特定任务判别式和生成式SE模型，并具有处理未见过的SE任务的能力。此外，我们公开了代码和模型以支持该领域的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00493v4">PDF</a> ACL2025 main, Codes available at   <a target="_blank" rel="noopener" href="https://github.com/Kevin-naticl/LLaSE-G1">https://github.com/Kevin-naticl/LLaSE-G1</a></p>
<p><strong>摘要</strong><br>本文介绍了基于LLaMA的LLaSE-G1语言模型在语音增强中的表现。该模型旨在解决现有模型在语义理解和上下文建模方面的不足，并引入了一系列创新技术来增强模型的性能。LLaSE-G1通过采用WavLM的连续表示作为输入并预测X-Codec2的语音令牌来减轻声学不一致的问题，同时采用双通道输入和输出以统一多种语音增强任务，无需特定任务标识。此外，该模型还表现出卓越的性能，超越了现有的任务特定判别和生成式语音增强模型，具有出色的泛化能力和测试时间扩展效应。本文还公开了相关代码和模型，以支持该领域的进一步研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li><p>LLaSE-G1语言模型旨在解决现有语言模型在语音增强中忽略声学信息的问题，通过采用WavLM的连续表示来减轻声学不一致性。</p>
</li>
<li><p>LLaSE-G1使用X-Codec2预测语音令牌以最大化声学保留效果。</p>
</li>
<li><p>该模型引入双频道输入和输出以促进泛化能力，统一多种语音增强任务，无需特定任务标识。</p>
</li>
<li><p>LLaSE-G1的性能超越了现有的任务特定判别和生成式语音增强模型。</p>
</li>
<li><p>LLaSE-G1模型在测试时展现出扩展效应，并能应对未见过的语音增强任务。</p>
</li>
<li><p>公开的代码和模型将支持该领域的进一步研究。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2164145bff5233c7a4b10e30bbf9a51b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b0519543f682a16f37db49b59d4486d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience"><a href="#Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience" class="headerlink" title="Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience"></a>Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience</h2><p><strong>Authors:Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani</strong></p>
<p>In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces “Listen, Chat, and Remix” (LCR), a novel multimodal sound remixer that controls each sound source in a mixture based on user-provided text instructions. LCR distinguishes itself with a user-friendly text interface and its unique ability to remix multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for remixing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles filtered components back to the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse remixing tasks including extraction, removal, and volume control of single or multiple sources. Our experiments demonstrate significant improvements in signal quality across all remixing tasks and robust performance in zero-shot scenarios with varying numbers and types of sound sources. An audio demo is available at: <a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a>. </p>
<blockquote>
<p>在日常生活中，我们会遇到各种声音，包括想要的和不想听到的，但对它们的存在和音量只有有限的控制能力。我们的工作引入了“Listen, Chat, and Remix”（LCR）这一新颖的多模式声音混音器，它可以根据用户提供的文本指令控制混合中的每个声音源。LCR通过用户友好的文本接口和其在混合中同时混音多个声音源而无需分离的独有能力来区分自己。用户输入开放式词汇文本提示，由大型语言模型进行解释，为混音声音混合物创建语义过滤器。然后该系统将混合物分解成其组成部分，应用语义过滤器，并将过滤后的组件重新组装成所需的输出。我们开发了一个包含超过10万混合物的160小时数据集，包括语音和各种音频源以及用于各种混音任务的文本提示，包括提取、删除和控制单个或多个源的音量。我们的实验表明在所有混音任务中信号质量都有显著提高，并且在不同数量和类型的音源场景中零样本情景下的性能稳健。音频演示作品可在：<a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a> 访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03710v2">PDF</a> Accepted by IEEE Journal of Selected Topics in Signal Processing   (JSTSP)</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种名为“Listen, Chat, and Remix”（LCR）的新型多模式声音混音器。LCR通过用户提供的文本指令控制混合声音中的每个声音源。其特点为用户友好的文本界面，以及同时混音多个声音源的能力，无需单独分离。系统通过大型语言模型解释用户输入的开放词汇文本提示，创建语义过滤器来混音声音混合物。实验表明，LCR在所有的混音任务中都显著提高信号质量，并在不同数量和类型的音源场景中表现稳健。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCR是一种新型多模式声音混音器，可以通过用户提供的文本指令控制混合声音中的每个声音源。</li>
<li>LCR具有用户友好的文本界面，可创建语义过滤器来混音声音混合物，无需单独分离多个声音源。</li>
<li>系统通过大型语言模型解释用户输入的开放词汇文本提示。</li>
<li>LCR能处理包括语音和各种音频源在内的混合声音，并可以进行提取、删除、控制单一或多个源的音量等多样化的混音任务。</li>
<li>LCR拥有160小时的数据集，包含超过10万个混合物。</li>
<li>实验表明，LCR在所有的混音任务中都显著提高信号质量，表现稳健。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7dd10c16d26f590cbf14dcd0ddcf596e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4a9c637a1f386f8f6db21f83434ec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc06ba878ad5fe8d3e3c90572ca5a480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8531b4f1e51e0995113da1f15c1c31c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89f167e5f57e22f658633e04e0ae2322.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-45e982bbb5e881e18505a77319741526.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-06-13  DGAE Diffusion-Guided Autoencoder for Efficient Latent Representation   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-de6bdfd4aaf563eff942e34710409360.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-06-13  DIsoN Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
