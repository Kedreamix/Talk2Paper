<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  EmoNet-Voice A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7dd10c16d26f590cbf14dcd0ddcf596e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection"><a href="#EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection" class="headerlink" title="EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection"></a>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection</h2><p><strong>Authors:Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, SÃ¶ren Auer</strong></p>
<p>The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³å’ŒéŸ³é¢‘ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä¸ºè¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æƒ…æ„Ÿç†è§£èƒ½åŠ›æä¾›äº†å¼ºå¤§çš„åŸºå‡†ã€‚å½“å‰è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ•°æ®é›†åœ¨æƒ…æ„Ÿç²’åº¦ã€éšç§æ‹…å¿§æˆ–ä¾èµ–è¡¨æ¼”è¡¨ç°æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ç”¨äºè¯­éŸ³æƒ…æ„Ÿæ£€æµ‹çš„æ–°èµ„æºEmoNet-Voiceï¼ŒåŒ…æ‹¬EmoNet-Voice Bigï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼ˆè·¨è¶Š11ç§å£°éŸ³ã€40ç§æƒ…æ„Ÿå’Œ4ç§è¯­è¨€ï¼ŒåŒ…å«è¶…è¿‡4500å°æ—¶çš„è¯­éŸ³ï¼‰ï¼Œä»¥åŠå¸¦æœ‰ä¸“å®¶æ³¨é‡Šçš„æ–°å‹åŸºå‡†æ•°æ®é›†EmoNet-Voice Benchã€‚EmoNet-Voiceæ—¨åœ¨è¯„ä¼°SERæ¨¡å‹åœ¨40ä¸ªæƒ…æ„Ÿç±»åˆ«çš„ç²¾ç»†ç²’åº¦å…‰è°±ä¸Šçš„è¡¨ç°ï¼Œå…·æœ‰ä¸åŒçš„å¼ºåº¦çº§åˆ«ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­éŸ³ç”ŸæˆæŠ€æœ¯ï¼Œç²¾å¿ƒåˆ¶ä½œäº†æ¨¡æ‹Ÿæ¼”å‘˜è¡¨ç°ç‰¹å®šæƒ…ç»ªåœºæ™¯çš„åˆæˆéŸ³é¢‘ç‰‡æ®µã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬ç”±å¿ƒç†å­¦ä¸“å®¶è¿›è¡Œäº†ä¸¥æ ¼éªŒè¯ï¼Œä»–ä»¬åˆ†é…äº†æ„ŸçŸ¥å¼ºåº¦æ ‡ç­¾ã€‚è¿™ç§åˆæˆã€ä¿æŠ¤éšç§çš„æ–¹æ³•å¯ä»¥åŒ…å«ç°æœ‰æ•°æ®é›†ä¸­é€šå¸¸ä¸å­˜åœ¨çš„æ•æ„Ÿæƒ…æ„ŸçŠ¶æ€ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†Empathic Insight Voiceæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ–¹é¢æ ‘ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œä¸äººç±»ä¸“å®¶çš„å¥‘åˆåº¦æé«˜ã€‚æˆ‘ä»¬åœ¨å½“å‰æ¨¡å‹æ™¯è§‚ä¸­çš„è¯„ä¼°å±•ç°äº†æœ‰ä»·å€¼çš„å‘ç°ï¼Œä¾‹å¦‚é«˜å”¤é†’æƒ…æ„Ÿï¼ˆå¦‚æ„¤æ€’ï¼‰æ¯”ä½å”¤é†’çŠ¶æ€ï¼ˆå¦‚ä¸“æ³¨ï¼‰æ›´å®¹æ˜“æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09827v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›†EmoNet-Voiceï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†EmoNet-Voice Bigå’Œæ–°åŸºå‡†æ•°æ®é›†EmoNet-Voice Benchã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†40ç§æƒ…æ„Ÿç±»åˆ«çš„ç²¾ç»†ç²’åº¦ï¼Œå¹¶åˆ©ç”¨å…ˆè¿›çš„è¯­éŸ³ç”ŸæˆæŠ€æœ¯æ¨¡æ‹Ÿæ¼”å‘˜æ¼”ç»ç‰¹å®šæƒ…æ„Ÿçš„åœºæ™¯ã€‚é€šè¿‡å¿ƒç†å­¦ä¸“å®¶è¿›è¡Œçš„ä¸¥æ ¼éªŒè¯ï¼Œç¡®ä¿æƒ…æ„Ÿå¼ºåº¦çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨åˆæˆå’Œéšç§ä¿æŠ¤çš„æ–¹æ³•ï¼Œå¯ä»¥åŒ…å«ç°æœ‰æ•°æ®é›†ä¸­ç¼ºå¤±çš„æ•æ„Ÿæƒ…æ„ŸçŠ¶æ€ã€‚æœ€åï¼Œå¼•å…¥äº†Empathic Insight Voiceæ¨¡å‹ï¼Œä¸ºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«è®¾ç«‹äº†æ–°çš„æ ‡å‡†ï¼Œå¹¶æ­ç¤ºäº†å¦‚æ„¤æ€’ç­‰é«˜å”¤èµ·çš„æƒ…ç»ªæ›´å®¹æ˜“è¢«æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°å‹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ•°æ®é›†EmoNet-Voiceï¼ŒåŒ…å«å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†å’Œæ–°åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>æ¶µç›–40ç§æƒ…æ„Ÿç±»åˆ«çš„ç²¾ç»†ç²’åº¦ï¼Œæ»¡è¶³ä¸åŒå¼ºåº¦çš„æƒ…æ„Ÿæ£€æµ‹éœ€æ±‚ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„è¯­éŸ³ç”ŸæˆæŠ€æœ¯æ¨¡æ‹ŸçœŸå®æƒ…æ„Ÿåœºæ™¯ã€‚</li>
<li>é€šè¿‡å¿ƒç†å­¦ä¸“å®¶éªŒè¯æƒ…æ„Ÿå¼ºåº¦çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨åˆæˆå’Œéšç§ä¿æŠ¤æ–¹æ³•ï¼ŒåŒ…å«æ•æ„Ÿæƒ…æ„ŸçŠ¶æ€ã€‚</li>
<li>å¼•å…¥Empathic Insight Voiceæ¨¡å‹ï¼Œä¸ºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«è®¾ç«‹æ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-434d9a1abef531c21b5483583b56d3c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead12f776165c52d3bd1fc04d82aeaba.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Study-on-Speech-Assessment-with-Visual-Cues"><a href="#A-Study-on-Speech-Assessment-with-Visual-Cues" class="headerlink" title="A Study on Speech Assessment with Visual Cues"></a>A Study on Speech Assessment with Visual Cues</h2><p><strong>Authors:Shafique Ahmed, Ryandhimas E. Zezario, Nasir Saleem, Amir Hussain, Hsin-Min Wang, Yu Tsao</strong></p>
<p>Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397-&gt;0.9205) for PESQ and 11.47% (0.7403-&gt;0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment. </p>
<blockquote>
<p>åœ¨æ— å¹²å‡€å‚è€ƒä¿¡å·çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œéä¾µå…¥æ€§çš„è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦è¯„ä¼°è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†éŸ³é¢‘ç‰¹å¾å’Œè§†è§‰çº¿ç´¢æ¥é¢„æµ‹PESQå’ŒSTOIåˆ†æ•°ã€‚å®ƒé‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œå…¶ä¸­é€šè¿‡STFTæå–å…‰è°±ç‰¹å¾ï¼Œå¹¶é€šè¿‡è§†è§‰ç¼–ç å™¨è·å¾—è§†è§‰åµŒå…¥ã€‚ç„¶åï¼Œè¿™äº›ç‰¹å¾é€šè¿‡å¸¦æœ‰æ³¨æ„åŠ›çš„CNN-BLSTMè¿›è¡Œèåˆå’Œå¤„ç†ï¼Œéšåè¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œä»¥åŒæ—¶é¢„æµ‹PESQå’ŒSTOIã€‚åœ¨LRS3-TEDæ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¾…ä»¥æ¥è‡ªDEMANDè¯­æ–™åº“çš„å™ªå£°å¢å¼ºï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„åŸºçº¿æ¨¡å‹ã€‚åœ¨æœªè§è¿‡çš„å™ªå£°æ¡ä»¶ä¸‹ï¼ŒPESQçš„LCCæé«˜äº†9.61%ï¼ˆä»0.8397æé«˜åˆ°0.9205ï¼‰ï¼ŒSTOIçš„LCCæé«˜äº†11.47%ï¼ˆä»0.7403æé«˜åˆ°0.8253ï¼‰ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ç»“åˆè§†è§‰çº¿ç´¢åœ¨æé«˜éä¾µå…¥æ€§è¯­éŸ³è¯„ä¼°çš„å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09549v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§éä¾µå…¥æ€§çš„å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ— æ³•è·å–å¹²å‡€å‚è€ƒä¿¡å·çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æ•´åˆéŸ³é¢‘ç‰¹å¾å’Œè§†è§‰çº¿ç´¢æ¥é¢„æµ‹PESQå’ŒSTOIåˆ†æ•°ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œä»éŸ³é¢‘ä¿¡å·æå–å…‰è°±ç‰¹å¾ï¼Œå¹¶é€šè¿‡è§†è§‰ç¼–ç å™¨è·å–è§†è§‰åµŒå…¥ã€‚è¿™äº›ç‰¹å¾ç»è¿‡èåˆåï¼Œé€šè¿‡ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œå’ŒåŒå‘é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œçš„æ¨¡å‹è¿›è¡Œå¤„ç†ï¼Œå¹¶å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ï¼Œè¯¥æ¨¡å‹èƒ½åŒæ—¶é¢„æµ‹PESQå’ŒSTOIåˆ†æ•°ã€‚åœ¨LRS3-TEDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ä»…ä½¿ç”¨éŸ³é¢‘çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚åœ¨é¢å¯¹æœªè§è¿‡çš„å™ªå£°æ¡ä»¶ä¸‹ï¼ŒPESQçš„LCCæé«˜äº†9.61%ï¼ŒSTOIçš„LCCæé«˜äº†11.47%ã€‚ç»“æœè¯æ˜äº†ç»“åˆè§†è§‰çº¿ç´¢èƒ½æœ‰æ•ˆæé«˜éä¾µå…¥æ€§è¯­éŸ³è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éä¾µå…¥æ€§åœ°è¯„ä¼°è¯­éŸ³è´¨é‡å’Œå¯æ‡‚åº¦åœ¨ç¼ºä¹å¹²å‡€å‚è€ƒä¿¡å·æ—¶å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶ï¼ŒèåˆéŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯æ¥é¢„æµ‹è¯­éŸ³è´¨é‡æŒ‡æ•°ï¼ˆPESQï¼‰å’ŒçŸ­æ—¶å…‰ä¿¡å·å¯æ‡‚åº¦æŒ‡æ•°ï¼ˆSTOIï¼‰ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œåˆ†åˆ«å¤„ç†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œé€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŒå‘é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆCNN-BLSTMï¼‰ç»“åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œç‰¹å¾èåˆå’Œå¤„ç†ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨äº†å¤šä»»åŠ¡å­¦ä¹ ä»¥åŒæ—¶é¢„æµ‹PESQå’ŒSTOIåˆ†æ•°ã€‚</li>
<li>åœ¨LRS3-TEDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœªè§å™ªå£°æ¡ä»¶ä¸‹æ€§èƒ½ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ¨¡å‹çš„æ€§èƒ½æå‡ä½“ç°åœ¨å¯¹è¯­éŸ³è´¨é‡å’Œå¯æ‡‚åº¦çš„å‡†ç¡®è¯„ä¼°ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9524bef04cddd1458d037663e92d7dd8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83774bb864594c8dbb762b92f257830d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15434a35504bff902bb3a74587cbb60c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6797e7d69244ce6a9d086d62403cf1e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniDRCA-Parallel-Speech-Text-Foundation-Model-via-Dual-Resolution-Speech-Representations-and-Contrastive-Alignment"><a href="#OmniDRCA-Parallel-Speech-Text-Foundation-Model-via-Dual-Resolution-Speech-Representations-and-Contrastive-Alignment" class="headerlink" title="OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment"></a>OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment</h2><p><strong>Authors:Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang, Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, Chong Zhang, Yukun Ma, Yafeng Chen, Hui Wang, Jiaqing Liu, Jieping Ye</strong></p>
<p>Recent studies on end-to-end speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLMâ€™s autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling, featuring dual-resolution speech representations and contrastive cross-modal alignment. Our approach processes speech and text representations in parallel while enhancing audio comprehension through contrastive alignment. Experimental results on Spoken Question Answering benchmarks demonstrate that OmniDRCA establishes new state-of-the-art (SOTA) performance among parallel joint speech-text modeling based foundation models, and achieves competitive performance compared to interleaved models. Additionally, we explore the potential of extending the framework to full-duplex conversational scenarios. </p>
<blockquote>
<p>å…³äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç«¯åˆ°ç«¯è¯­éŸ³ç”Ÿæˆçš„æœ€æ–°ç ”ç©¶å·²ç»å¼•èµ·äº†ç¤¾åŒºçš„å…³æ³¨ï¼Œå¤šé¡¹å·¥ä½œå°†åŸºäºæ–‡æœ¬çš„LLMæ‰©å±•åˆ°ç”Ÿæˆç¦»æ•£è¯­éŸ³æ ‡è®°ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å¯åˆ†ä¸ºä¸¤ç±»ï¼šï¼ˆ1ï¼‰ç‹¬ç«‹ç”Ÿæˆç¦»æ•£è¯­éŸ³æ ‡è®°çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¸ä¼šå°†å…¶çº³å…¥LLMçš„è‡ªå›å½’è¿‡ç¨‹ä¸­ï¼Œå¯¼è‡´æ–‡æœ¬ç”Ÿæˆä¸çŸ¥é“å½“å‰çš„è¯­éŸ³åˆæˆã€‚ï¼ˆ2ï¼‰é€šè¿‡è”åˆè‡ªå›å½’å»ºæ¨¡ç”Ÿæˆäº¤é”™æˆ–å¹¶è¡Œè¯­éŸ³æ–‡æœ¬æ ‡è®°çš„æ¨¡å‹ï¼Œä»è€Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°è·¨æ¨¡æ€æ„ŸçŸ¥ã€‚æœ¬æ–‡ä»‹ç»äº†OmniDRCAï¼Œä¸€ç§åŸºäºè”åˆè‡ªå›å½’å»ºæ¨¡çš„å¹¶è¡Œè¯­éŸ³æ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰åŒåˆ†è¾¨ç‡è¯­éŸ³è¡¨ç¤ºå’Œå¯¹æ¯”è·¨æ¨¡æ€å¯¹é½åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹¶è¡Œå¤„ç†è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºï¼ŒåŒæ—¶é€šè¿‡å¯¹æ¯”å¯¹é½æé«˜éŸ³é¢‘ç†è§£ã€‚åœ¨å£è¯­é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniDRCAåœ¨åŸºäºå¹¶è¡Œè”åˆè¯­éŸ³æ–‡æœ¬å»ºæ¨¡çš„åŸºç¡€æ¨¡å‹ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸äº¤é”™æ¨¡å‹çš„æ¯”è¾ƒä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å°†æ¡†æ¶æ‰©å±•åˆ°å…¨åŒå·¥å¯¹è¯åœºæ™¯çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09349v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>è¿‘æœŸå…³äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç«¯åˆ°ç«¯è¯­éŸ³ç”Ÿæˆçš„ç ”ç©¶å¼•èµ·äº†ç¤¾åŒºçš„å…³æ³¨ï¼Œå¤šé¡¹å·¥ä½œå°†æ–‡æœ¬åŸºç¡€çš„LLMsæ‰©å±•è‡³ç”Ÿæˆç¦»æ•£è¯­éŸ³æ ‡è®°ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šï¼ˆ1ï¼‰ç‹¬ç«‹ç”Ÿæˆç¦»æ•£è¯­éŸ³æ ‡è®°çš„æ–¹æ³•ï¼Œä¸å°†å…¶çº³å…¥LLMçš„è‡ªå›å½’è¿‡ç¨‹ï¼Œå¯¼è‡´æ–‡æœ¬ç”Ÿæˆæ— æ³•æ„è¯†åˆ°å¹¶å‘çš„è¯­éŸ³åˆæˆã€‚ï¼ˆ2ï¼‰é€šè¿‡è”åˆè‡ªå›å½’å»ºæ¨¡ç”Ÿæˆäº¤ç»‡æˆ–å¹¶è¡Œè¯­éŸ³æ–‡æœ¬æ ‡è®°çš„æ¨¡å‹ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°æ¨¡æ€é—´çš„ç›¸äº’æ„ŸçŸ¥ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºè”åˆè‡ªå›å½’å»ºæ¨¡çš„OmniDRCAå¹¶è¡Œè¯­éŸ³æ–‡æœ¬åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰åŒåˆ†è¾¨ç‡è¯­éŸ³è¡¨ç¤ºå’Œå¯¹æ¯”è·¨æ¨¡æ€å¯¹é½çš„ç‰¹ç‚¹ã€‚è¯¥æ–¹æ³•åœ¨å¹¶è¡Œå¤„ç†è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºçš„åŒæ—¶ï¼Œé€šè¿‡å¯¹é½å¯¹æ¯”å¢å¼ºéŸ³é¢‘ç†è§£ã€‚åœ¨å£è¯­é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniDRCAåœ¨åŸºäºå¹¶è¡Œè”åˆè¯­éŸ³æ–‡æœ¬å»ºæ¨¡çš„åŸºç¡€æ¨¡å‹ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨äº¤ç»‡æ¨¡å‹ä¸­å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å°†æ¡†æ¶æ‰©å±•åˆ°å…¨åŒå·¥å¯¹è¯åœºæ™¯çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯è¯­éŸ³ç”Ÿæˆç ”ç©¶æ­£å¸å¼•ç¤¾åŒºå…³æ³¨ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«åº”ç”¨äºç”Ÿæˆç¦»æ•£è¯­éŸ³æ ‡è®°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åˆ†ä¸ºç‹¬ç«‹ç”Ÿæˆå’Œè”åˆè‡ªå›å½’å»ºæ¨¡ä¸¤ç±»ã€‚</li>
<li>OmniDRCAæ¨¡å‹é‡‡ç”¨åŸºäºè”åˆè‡ªå›å½’å»ºæ¨¡çš„å¹¶è¡Œè¯­éŸ³æ–‡æœ¬åŸºç¡€æ¶æ„ã€‚</li>
<li>OmniDRCAå…·æœ‰åŒåˆ†è¾¨ç‡è¯­éŸ³è¡¨ç¤ºå’Œå¯¹æ¯”è·¨æ¨¡æ€å¯¹é½ç‰¹ç‚¹ã€‚</li>
<li>æ¨¡å‹èƒ½å¹¶è¡Œå¤„ç†è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¢å¼ºéŸ³é¢‘ç†è§£ã€‚</li>
<li>åœ¨å£è¯­é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šï¼ŒOmniDRCAè¡¨ç°å‡ºå…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b74cf6f5443b481996530d1700600b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d5ca7802866f9586dbadb2d15fb9de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-035877bff8289b603e5b93fe4a79be5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Advancing-STT-for-Low-Resource-Real-World-Speech"><a href="#Advancing-STT-for-Low-Resource-Real-World-Speech" class="headerlink" title="Advancing STT for Low-Resource Real-World Speech"></a>Advancing STT for Low-Resource Real-World Speech</h2><p><strong>Authors:Flavio Dâ€™Intino, Hans-Peter Hutter</strong></p>
<p>Swiss German is a low-resource language represented by diverse dialects that differ significantly from Standard German and from each other, lacking a standardized written form. As a result, transcribing Swiss German involves translating into Standard German. Existing datasets have been collected in controlled environments, yielding effective speech-to-text (STT) models, but these models struggle with spontaneous conversational speech.   This paper, therefore, introduces the new SRB-300 dataset, a 300-hour annotated speech corpus featuring real-world long-audio recordings from 39 Swiss German radio and TV stations. It captures spontaneous speech across all major Swiss dialects recorded in various realistic environments and overcomes the limitation of prior sentence-level corpora.   We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset, achieving notable enhancements over previous zero-shot performance metrics. Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for developing effective and robust STT systems for Swiss German and other low-resource languages in real-world contexts. </p>
<blockquote>
<p>ç‘å£«å¾·è¯­æ˜¯ä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œç”±å¤šç§æ–¹è¨€ç»„æˆï¼Œè¿™äº›æ–¹è¨€ä¸æ ‡å‡†å¾·è¯­ä»¥åŠå½¼æ­¤ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶ä¸”æ²¡æœ‰æ ‡å‡†åŒ–çš„ä¹¦é¢å½¢å¼ã€‚å› æ­¤ï¼Œè½¬å½•ç‘å£«å¾·è¯­éœ€è¦å°†å…¶ç¿»è¯‘ä¸ºæ ‡å‡†å¾·è¯­ã€‚ç°æœ‰çš„æ•°æ®é›†å·²åœ¨å—æ§ç¯å¢ƒä¸­æ”¶é›†ï¼Œäº§ç”Ÿäº†æœ‰æ•ˆçš„è¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨å¤„ç†æ—¥å¸¸ä¼šè¯è¯­éŸ³æ—¶é‡åˆ°å›°éš¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†æ–°çš„SRB-300æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ¥è‡ªç‘å£«å¾·è¯­å¹¿æ’­ç”µå°å’Œç”µè§†å°çš„300å°æ—¶æ³¨é‡Šè¯­éŸ³è¯­æ–™åº“çš„çœŸå®ä¸–ç•Œé•¿éŸ³é¢‘å½•éŸ³ã€‚å®ƒæ•æ‰äº†æ¥è‡ªå„ç§ç°å®ç¯å¢ƒçš„æ‰€æœ‰ä¸»è¦ç‘å£«æ–¹è¨€çš„å³å…´æ¼”è®²ï¼Œå…‹æœäº†ä»¥å‰å¥å­å±‚é¢è¯­æ–™åº“çš„å±€é™æ€§ã€‚æˆ‘ä»¬åœ¨SRB-300æ•°æ®é›†ä¸Šå¾®è°ƒäº†å¤šä¸ªOpenAIwhisperæ¨¡å‹ï¼Œç›¸è¾ƒäºä¹‹å‰çš„é›¶æ ·æœ¬æ€§èƒ½è¯„ä»·æŒ‡æ ‡å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰çš„æ”¹è¿›èŒƒå›´åœ¨19%è‡³33%ä¹‹é—´ï¼Œè€ŒBLEUåˆ†æ•°æé«˜äº†8%è‡³40%ã€‚æœ€ä½³çš„å¾®è°ƒæ¨¡å‹â€œå¤§å‹v3â€å®ç°äº†å•è¯é”™è¯¯ç‡ä¸º17.1%ï¼ŒBLEUåˆ†æ•°ä¸º74.8%ã€‚è¿™ä¸€è¿›å±•å¯¹äºå¼€å‘é€‚ç”¨äºç‘å£«å¾·è¯­å’Œå…¶ä»–èµ„æºåŒ®ä¹çš„è¯­è¨€çš„æœ‰æ•ˆå’Œç¨³å¥çš„STTç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08836v1">PDF</a> Conference: HCI International 2025, 20 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†é’ˆå¯¹ç‘å£«å¾·è¯­â€”â€”ä¸€ç§ç¼ºä¹æ ‡å‡†åŒ–ä¹¦é¢å½¢å¼ä¸”è¡¨ç°ä¸ºå¤šæ ·æ–¹è¨€çš„è¯­è¨€â€”â€”æ‰€åˆ›å»ºçš„æ–°æ•°æ®é›†SRB-300ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç‘å£«å„å¤§æ–¹è¨€åŒºçš„çœŸå®é•¿éŸ³é¢‘è®°å½•ï¼Œèƒ½åæ˜ å‡ºè‡ªå‘çš„å£è¯­è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä½¿ç”¨OpenAI Whisperæ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæ‰€å–å¾—çš„æ˜¾è‘—æˆæœï¼Œæ˜¾è‘—æé«˜äº†è¯­éŸ³è½¬æ–‡æœ¬çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‘å£«å¾·è¯­ç¼ºä¹æ ‡å‡†åŒ–ä¹¦é¢å½¢å¼ï¼Œè¡¨ç°ä¸ºå¤šæ ·çš„æ–¹è¨€ï¼Œä¸”å„æ–¹è¨€é—´å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦åœ¨å—æ§ç¯å¢ƒä¸‹æ”¶é›†ï¼Œå¯¹äºè‡ªç„¶å£è¯­è¡¨è¾¾çš„è¯†åˆ«æ•ˆæœæœ‰é™ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†æ–°çš„SRB-300æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªç‘å£«å„å¤§æ–¹è¨€åŒºçš„çœŸå®é•¿éŸ³é¢‘è®°å½•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€éš¾é¢˜ã€‚</li>
<li>ä½¿ç”¨OpenAI Whisperæ¨¡å‹åœ¨SRB-300æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ˜¾è‘—æé«˜äº†è¯­éŸ³è½¬æ–‡æœ¬çš„å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå­—é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†19%è‡³33%ï¼ŒBLEUåˆ†æ•°æé«˜äº†8%è‡³40%ã€‚</li>
<li>æœ€ä½³å¾®è°ƒæ¨¡å‹ï¼ˆlarge-v3ï¼‰åœ¨WERå’ŒBLEUåˆ†æ•°æ–¹é¢è¾¾åˆ°äº†æ˜¾è‘—æˆæœï¼Œåˆ†åˆ«ä¸º17.1%å’Œ74.8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af7238b5e55b366bded11e21879030a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d0707e56feb32549c4168f275f269b5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Teacher-Language-Aware-Knowledge-Distillation-for-Multilingual-Speech-Emotion-Recognition"><a href="#Multi-Teacher-Language-Aware-Knowledge-Distillation-for-Multilingual-Speech-Emotion-Recognition" class="headerlink" title="Multi-Teacher Language-Aware Knowledge Distillation for Multilingual   Speech Emotion Recognition"></a>Multi-Teacher Language-Aware Knowledge Distillation for Multilingual   Speech Emotion Recognition</h2><p><strong>Authors:Mehedi Hasan Bijoy, Dejan Porjazovski, TamÃ¡s GrÃ³sz, Mikko Kurimo</strong></p>
<p>Speech Emotion Recognition (SER) is crucial for improving human-computer interaction. Despite strides in monolingual SER, extending them to build a multilingual system remains challenging. Our goal is to train a single model capable of multilingual SER by distilling knowledge from multiple teacher models. To address this, we introduce a novel language-aware multi-teacher knowledge distillation method to advance SER in English, Finnish, and French. It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and then distills their knowledge into a single multilingual student model. The student model demonstrates state-of-the-art performance, with a weighted recall of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish dataset, surpassing fine-tuning and knowledge distillation baselines. Our method excels in improving recall for sad and neutral emotions, although it still faces challenges in recognizing anger and happiness. </p>
<blockquote>
<p>è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰å¯¹äºæ”¹å–„äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚å°½ç®¡å•è¯­SERå–å¾—äº†è¿›å±•ï¼Œä½†å°†å…¶æ‰©å±•åˆ°æ„å»ºå¤šè¯­è¨€ç³»ç»Ÿä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ä»å¤šä¸ªæ•™å¸ˆæ¨¡å‹ä¸­æç‚¼çŸ¥è¯†ï¼Œè®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œå¤šè¯­è¨€SERçš„å•ä¸€æ¨¡å‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¯­è¨€æ„ŸçŸ¥å¤šæ•™å¸ˆçŸ¥è¯†æç‚¼æ–¹æ³•ï¼Œä»¥æ¨è¿›è‹±è¯­ã€èŠ¬å…°è¯­å’Œæ³•è¯­ä¸­çš„SERã€‚å®ƒåˆ©ç”¨Wav2Vec2.0ä½œä¸ºå•è¯­æ•™å¸ˆæ¨¡å‹çš„åŸºç¡€ï¼Œç„¶åå°†çŸ¥è¯†æç‚¼åˆ°ä¸€ä¸ªå•ä¸€çš„å¤šè¯­è¨€å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚å­¦ç”Ÿæ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è‹±è¯­æ•°æ®é›†ä¸Šçš„åŠ æƒå¬å›ç‡ä¸º72.9ï¼ŒèŠ¬å…°æ•°æ®é›†ä¸Šçš„æœªåŠ æƒå¬å›ç‡ä¸º63.4ï¼Œè¶…è¿‡äº†å¾®è°ƒå’ŒçŸ¥è¯†æç‚¼çš„åŸºçº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ”¹è¿›æ‚²ä¼¤å’Œä¸­æ€§æƒ…ç»ªçš„å¬å›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°½ç®¡åœ¨è¯†åˆ«æ„¤æ€’å’Œå¿«ä¹æƒ…ç»ªæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08717v1">PDF</a> Accepted to INTERSPEECH 2025 conference</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæƒ…æ„Ÿè¯†åˆ«åœ¨æå‡äººæœºäº¤äº’ä¸­çš„é‡è¦æ€§ï¼Œå½“å‰å¯¹äºè·¨å¤šè¯­ç§æƒ…æ„Ÿè¯†åˆ«çš„æŒ‘æˆ˜æ­£åœ¨åŠ å¤§ã€‚ç ”ç©¶ç›®æ ‡æ˜¯é€šè¿‡å¤šæ•™å¸ˆæ¨¡å‹çŸ¥è¯†è’¸é¦çš„æ–¹å¼è®­ç»ƒå‡ºä¸€ä¸ªå•è¯­ç§æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼Œå¹¶æ‰©å±•åˆ°å¤šè¯­ç§æƒ…æ„Ÿè¯†åˆ«é¢†åŸŸã€‚è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¯­è¨€æ„ŸçŸ¥å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œä»¥æ¨è¿›è‹±è¯­ã€èŠ¬å…°è¯­å’Œæ³•è¯­çš„æƒ…æ„Ÿè¯†åˆ«ã€‚è¯¥ç ”ç©¶ä»¥Wav2Vec2.0ä¸ºåŸºç¡€æ„å»ºå•è¯­ç§æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶å°†å…¶çŸ¥è¯†è’¸é¦åˆ°å•ä¸€çš„å¤šè¯­ç§å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚è¯¥å­¦ç”Ÿæ¨¡å‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨è‹±è¯­æ•°æ®é›†ä¸Šçš„åŠ æƒå¬å›ç‡ä¸º72.9%ï¼ŒèŠ¬å…°æ•°æ®é›†ä¸Šçš„éåŠ æƒå¬å›ç‡ä¸º63.4%ï¼Œè¶…è¶Šäº†å¾®è°ƒä¸çŸ¥è¯†è’¸é¦åŸºçº¿ã€‚è¯¥ç ”ç©¶å°¤å…¶æ“…é•¿æé«˜æ‚²ä¼¤å’Œä¸­æ€§æƒ…æ„Ÿçš„è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨æ„¤æ€’å’Œå¿«ä¹æƒ…æ„Ÿçš„è¯†åˆ«ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®æ ‡æ˜¯åˆ©ç”¨å¤šæ•™å¸ˆæ¨¡å‹çŸ¥è¯†è’¸é¦è®­ç»ƒå•è¯­ç§æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹å¹¶æ‰©å±•ä¸ºå¤šè¯­ç§ã€‚</li>
<li>æå‡ºä¸€ç§è¯­è¨€æ„ŸçŸ¥çš„å¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦æ–°æ–¹æ³•ä»¥æå‡å¤šè¯­ç§æƒ…æ„Ÿè¯†åˆ«ã€‚</li>
<li>ä½¿ç”¨Wav2Vec2.0ä½œä¸ºåŸºç¡€æ„å»ºæ•™å¸ˆæ¨¡å‹ï¼Œç„¶åå°†å…¶è’¸é¦è‡³å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹å±•ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼ŒåŒ…æ‹¬è‹±è¯­æ•°æ®é›†ä¸Šçš„åŠ æƒå¬å›ç‡å’ŒèŠ¬å…°æ•°æ®é›†ä¸Šçš„éåŠ æƒå¬å›ç‡å‡è¾¾åˆ°è¾ƒé«˜æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•æ“…é•¿è¯†åˆ«æ‚²ä¼¤å’Œä¸­æ€§æƒ…æ„Ÿï¼Œä½†åœ¨æ„¤æ€’å’Œå¿«ä¹æƒ…æ„Ÿçš„è¯†åˆ«ä¸Šä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>ç ”ç©¶å®ç°äº†çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹è®­ç»ƒçš„æœ‰æ•ˆç»“åˆï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5a904f34c7e650b6ba9522c24c2ca6af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f86da02bd1edede585444312ff82bd5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b15dacda69f35e1d63f414f16eab1cab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d39a9d13091545c0114419184f343cd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Review-on-Score-based-Generative-Models-for-Audio-Applications"><a href="#A-Review-on-Score-based-Generative-Models-for-Audio-Applications" class="headerlink" title="A Review on Score-based Generative Models for Audio Applications"></a>A Review on Score-based Generative Models for Audio Applications</h2><p><strong>Authors:Ge Zhu, Yutong Wen, Zhiyao Duan</strong></p>
<p>Diffusion models have emerged as powerful deep generative techniques, producing high-quality and diverse samples in applications in various domains including audio. These models have many different design choices suitable for different applications, however, existing reviews lack in-depth discussions of these design choices. The audio diffusion model literature also lacks principled guidance for the implementation of these design choices and their comparisons for different applications. This survey provides a comprehensive review of diffusion model design with an emphasis on design principles for quality improvement and conditioning for audio applications. We adopt the score modeling perspective as a unifying framework that accommodates various interpretations, including recent approaches like flow matching. We systematically examine the training and sampling procedures of diffusion models, and audio applications through different conditioning mechanisms. To address the lack of audio diffusion model codebases and to promote reproducible research and rapid prototyping, we introduce an open-source codebase at <a target="_blank" rel="noopener" href="https://github.com/gzhu06/AudioDiffuser">https://github.com/gzhu06/AudioDiffuser</a> that implements our reviewed framework for various audio applications. We demonstrate its capabilities through three case studies: audio generation, speech enhancement, and text-to-speech synthesis, with benchmark evaluations on standard datasets. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»ä½œä¸ºå¼ºå¤§çš„æ·±åº¦ç”ŸæˆæŠ€æœ¯å‡ºç°ï¼Œåœ¨å„ç§é¢†åŸŸçš„åº”ç”¨ä¸­äº§ç”Ÿäº†é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ ·æœ¬ï¼ŒåŒ…æ‹¬éŸ³é¢‘ã€‚è¿™äº›æ¨¡å‹æœ‰è®¸å¤šä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œé€‚åˆä¸åŒçš„åº”ç”¨ï¼Œç„¶è€Œï¼Œç°æœ‰çš„è¯„è®ºç¼ºä¹å¯¹è¿™äº›è®¾è®¡é€‰æ‹©çš„æ·±å…¥è®¨è®ºã€‚éŸ³é¢‘æ‰©æ•£æ¨¡å‹æ–‡çŒ®ä¹Ÿç¼ºä¹è¿™äº›è®¾è®¡é€‰æ‹©å®æ–½çš„åŸåˆ™æ€§æŒ‡å¯¼ä»¥åŠå®ƒä»¬åœ¨ä¸åŒåº”ç”¨ä¸­çš„æ¯”è¾ƒã€‚è¿™ç¯‡ç»¼è¿°å¯¹æ‰©æ•£æ¨¡å‹è®¾è®¡è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼Œé‡ç‚¹ä»‹ç»äº†æ”¹è¿›è´¨é‡å’ŒéŸ³é¢‘åº”ç”¨æ¡ä»¶çš„è®¾è®¡åŸåˆ™ã€‚æˆ‘ä»¬é‡‡ç”¨è¯„åˆ†å»ºæ¨¡è§†è§’ä½œä¸ºç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥å®¹çº³å„ç§è§£é‡Šï¼ŒåŒ…æ‹¬æœ€æ–°çš„æ–¹æ³•å¦‚æµåŒ¹é…ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œé‡‡æ ·ç¨‹åºï¼Œä»¥åŠé€šè¿‡ä¸åŒæ¡ä»¶æœºåˆ¶è¿›è¡Œçš„éŸ³é¢‘åº”ç”¨ç¨‹åºã€‚ä¸ºäº†è§£å†³éŸ³é¢‘æ‰©æ•£æ¨¡å‹ä»£ç åº“ç¼ºä¹çš„é—®é¢˜ï¼Œå¹¶æ¨åŠ¨å¯é‡å¤çš„ç ”ç©¶å’Œå¿«é€ŸåŸå‹è®¾è®¡ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/gzhu06/AudioDiffuser%E4%B8%8A%E6%8E%A8%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E5%BA%93%EF%BC%8C%E4%B8%BA%E5%90%84%E7%A7%8D%E9%9F%B3%E9%A2%91%E5%BA%94%E7%94%A8%E5%AE%9E%E7%8E%B0%E4%BA%86%E6%88%91%E4%BB%AC%E5%9B%9E%E9%A1%BE%E8%BF%87%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82%E6%88%91%E4%BB%AC%E9%80%9A%E8%BF%87%E4%B8%89%E4%B8%AA%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E5%B1%95%E7%A4%BA%E4%BA%86%E5%85%B6%E8%83%BD%E5%8A%9B%EF%BC%9A%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E3%80%81%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA%E5%92%8C%E6%96%87%E6%9C%AC%E5%88%B0%E8%AF%AD%E9%9F%B3%E7%9A%84%E5%90%88%E6%88%90%EF%BC%8C%E5%B9%B6%E5%9C%A8%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%9F%BA%E5%87%86%E8%AF%84%E4%BC%B0%E3%80%82">https://github.com/gzhu06/AudioDiffuserä¸Šæ¨å‡ºäº†ä¸€ä¸ªå¼€æºä»£ç åº“ï¼Œä¸ºå„ç§éŸ³é¢‘åº”ç”¨å®ç°äº†æˆ‘ä»¬å›é¡¾è¿‡çš„æ¡†æ¶ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†å…¶èƒ½åŠ›ï¼šéŸ³é¢‘ç”Ÿæˆã€è¯­éŸ³å¢å¼ºå’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„åˆæˆï¼Œå¹¶åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†è¯„ä¼°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08457v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„æ·±åº¦ç”ŸæˆæŠ€æœ¯ï¼Œå·²å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬éŸ³é¢‘åœ¨å†…çš„å¤šä¸ªé¢†åŸŸçš„åº”ç”¨ä¸­ï¼Œå¹¶èƒ½äº§ç”Ÿé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ ·æœ¬ã€‚æœ¬æ–‡ä¸ºæ‰©æ•£æ¨¡å‹è®¾è®¡æä¾›äº†å…¨é¢çš„ç»¼è¿°ï¼Œé‡ç‚¹ä»‹ç»äº†æé«˜è´¨é‡å’ŒéŸ³é¢‘åº”ç”¨çš„æ¡ä»¶è®¾è®¡åŸåˆ™ã€‚æœ¬æ–‡ä»å¾—åˆ†å»ºæ¨¡çš„è§’åº¦å‡ºå‘ï¼Œç»Ÿä¸€æ¡†æ¶æ¥çº³äº†å„ç§è§£è¯»ï¼ŒåŒ…æ‹¬æµç¨‹åŒ¹é…ç­‰æœ€æ–°æ–¹æ³•ã€‚ç³»ç»Ÿåœ°æ¢è®¨äº†æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ä»¥åŠé€šè¿‡ä¸åŒæ¡ä»¶æœºåˆ¶åœ¨éŸ³é¢‘åº”ç”¨ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³éŸ³é¢‘æ‰©æ•£æ¨¡å‹ä»£ç åº“ç¼ºä¹çš„é—®é¢˜ï¼Œä¿ƒè¿›å¯é‡å¤ç ”ç©¶å’Œå¿«é€ŸåŸå‹è®¾è®¡ï¼Œæœ¬æ–‡å¼•å…¥äº†å¼€æºä»£ç åº“ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨éŸ³é¢‘ç”Ÿæˆã€è¯­éŸ³å¢å¼ºå’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­çš„èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†è¯„ä¼°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå¼ºå¤§çš„æ·±åº¦ç”ŸæˆæŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬éŸ³é¢‘ã€‚</li>
<li>ç°æœ‰æ–‡çŒ®ç¼ºä¹å¯¹æ‰©æ•£æ¨¡å‹è®¾è®¡çš„æ·±å…¥æ¢è®¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³é¢‘åº”ç”¨æ–¹é¢çš„åŸåˆ™æ€§æŒ‡å¯¼ã€‚</li>
<li>æœ¬æ–‡æä¾›æ‰©æ•£æ¨¡å‹è®¾è®¡çš„å…¨é¢ç»¼è¿°ï¼Œé‡ç‚¹ä»‹ç»è®¾è®¡åŸåˆ™ä»¥æé«˜è´¨é‡å’ŒéŸ³é¢‘åº”ç”¨çš„æ¡ä»¶ã€‚</li>
<li>é‡‡ç”¨å¾—åˆ†å»ºæ¨¡çš„è§’åº¦ä½œä¸ºç»Ÿä¸€æ¡†æ¶ï¼Œæ¥çº³å„ç§è§£è¯»ï¼ŒåŒ…æ‹¬æµç¨‹åŒ¹é…ç­‰æœ€æ–°æ–¹æ³•ã€‚</li>
<li>ç³»ç»Ÿåœ°æ¢è®¨æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>ä»‹ç»å¼€æºä»£ç åº“ï¼Œè§£å†³éŸ³é¢‘æ‰©æ•£æ¨¡å‹ä»£ç åº“ç¼ºä¹çš„é—®é¢˜ï¼Œä¿ƒè¿›å¯é‡å¤ç ”ç©¶å’Œå¿«é€ŸåŸå‹è®¾è®¡ã€‚</li>
<li>é€šè¿‡éŸ³é¢‘ç”Ÿæˆã€è¯­éŸ³å¢å¼ºå’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºå…¶èƒ½åŠ›ï¼Œå¹¶åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f9e16552380d590d116459549aa33f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616fc2c99ee5d5cf0d6299e3ad16ed8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-299d24dcf1490c9f35aa3a3a396ab484.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multilingual-Hate-Speech-Detection-in-Social-Media-Using-Translation-Based-Approaches-with-Large-Language-Models"><a href="#Multilingual-Hate-Speech-Detection-in-Social-Media-Using-Translation-Based-Approaches-with-Large-Language-Models" class="headerlink" title="Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models"></a>Multilingual Hate Speech Detection in Social Media Using   Translation-Based Approaches with Large Language Models</h2><p><strong>Authors:Muhammad Usman, Muhammad Ahmad, M. Shahiki Tash, Irina Gelbukh, Rolando Quintero Tellez, Grigori Sidorov</strong></p>
<p>Social media platforms are critical spaces for public discourse, shaping opinions and community dynamics, yet their widespread use has amplified harmful content, particularly hate speech, threatening online safety and inclusivity. While hate speech detection has been extensively studied in languages like English and Spanish, Urdu remains underexplored, especially using translation-based approaches. To address this gap, we introduce a trilingual dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and Spanish (3,162 samples), collected via keyword filtering, with a balanced distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology leverages attention layers as a precursor to transformer-based models and large language models (LLMs), enhancing feature extraction for multilingual hate speech detection. For non-transformer models, we use TF-IDF for feature extraction. The dataset is benchmarked using state-of-the-art models, including GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators, following rigorous guidelines, ensured high dataset quality, achieving a Fleissâ€™ Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5 Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of 0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B). These results reflect improvements of 8.75% in English (over SVM baseline 0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline 0.82). Our framework offers a robust solution for multilingual hate speech detection, fostering safer digital communities worldwide. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°æ˜¯å…¬ä¼—è¯è¯­çš„é‡è¦ç©ºé—´ï¼Œå¡‘é€ æ„è§å’Œç¤¾åŒºåŠ¨æ€ï¼Œç„¶è€Œå®ƒä»¬çš„å¹¿æ³›ä½¿ç”¨ä¹Ÿæ”¾å¤§äº†æœ‰å®³å†…å®¹ï¼Œç‰¹åˆ«æ˜¯ä»‡æ¨è¨€è®ºï¼Œå¨èƒç½‘ç»œå®‰å…¨å’ŒåŒ…å®¹æ€§ã€‚è™½ç„¶è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„ä»‡æ¨è¨€è®ºæ£€æµ‹å·²ç»å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†ä¹Œå°”éƒ½è¯­ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨åŸºäºç¿»è¯‘çš„æ–¹æ³•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«è‹±è¯­ï¼ˆ3834ä¸ªæ ·æœ¬ï¼‰ã€ä¹Œå°”éƒ½è¯­ï¼ˆ3197ä¸ªæ ·æœ¬ï¼‰å’Œè¥¿ç­ç‰™è¯­ï¼ˆ3162ä¸ªæ ·æœ¬ï¼‰çš„ä¸‰ç§è¯­è¨€æ•°æ®é›†ï¼Œé€šè¿‡å…³é”®è¯è¿‡æ»¤æ”¶é›†ï¼ŒåŒ…å«4849ä¸ªä»‡æ¨æ ‡ç­¾å’Œ5344ä¸ªéä»‡æ¨æ ‡ç­¾çš„å¹³è¡¡åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ³¨æ„åŠ›å±‚ä½œä¸ºåŸºäºTransformeræ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆå†³æ¡ä»¶ï¼Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œç”¨äºå¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹ã€‚å¯¹äºéTransformeræ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨TF-IDFè¿›è¡Œç‰¹å¾æå–ã€‚è¯¥æ•°æ®é›†é‡‡ç”¨æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPT-3.5 Turboå’ŒQwen 2.5 72Bï¼Œä»¥åŠä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹å¦‚SVMå’Œå…¶ä»–Transformerï¼ˆå¦‚BERTã€RoBERTaï¼‰ã€‚ä¸‰ä¸ªæ³¨é‡Šè€…éµå¾ªä¸¥æ ¼çš„æŒ‡å¯¼æ–¹é’ˆï¼Œç¡®ä¿æ•°æ®é›†çš„é«˜è´¨é‡ï¼Œè¾¾åˆ°Fleiss Kappaå€¼ä¸º0.821ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ³¨æ„åŠ›å±‚ä¸GPT-3.5 Turboå’ŒQwen 2.5 72Bç›¸ç»“åˆï¼Œå–å¾—äº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ã€‚è‹±è¯­å®F1åˆ†æ•°ä¸º0.87ï¼ˆGPT-3.5 Turboï¼‰ï¼Œè¥¿ç­ç‰™è¯­ä¸º0.85ï¼ˆGPT-3.5 Turboï¼‰ï¼Œä¹Œå°”éƒ½è¯­ä¸º0.81ï¼ˆQwen 2.5 72Bï¼‰ï¼Œå¤šè¯­è¨€è”åˆæ¨¡å‹ä¸º0.88ï¼ˆQwen 2.5 72Bï¼‰ã€‚è¿™äº›ç»“æœåæ˜ äº†ç›¸å¯¹äºSVMåŸºå‡†çº¿çš„æ”¹è¿›ï¼šè‹±è¯­æé«˜8.75%ï¼ˆä»0.80æé«˜åˆ°0.87ï¼‰ï¼Œè¥¿ç­ç‰™è¯­æé«˜8.97%ï¼ˆä»0.78æé«˜åˆ°0.87ï¼‰ï¼Œä¹Œå°”éƒ½è¯­æé«˜5.19%ï¼ˆä»0.77æé«˜åˆ°0.82ï¼‰ï¼Œå¤šè¯­è¨€è”åˆæ¨¡å‹æé«˜7.32%ï¼ˆä»0.82æé«˜åˆ°0.88ï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œä¿ƒè¿›äº†å…¨çƒæ›´å®‰å…¨çš„æ•°å­—ç¤¾åŒºå»ºè®¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08147v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç¤¾äº¤åª’ä½“å¹³å°æ˜¯å…¬ä¼—è¯è¯­çš„é‡è¦ç©ºé—´ï¼Œå¡‘é€ æ„è§å’Œç¤¾åŒºåŠ¨æ€ï¼Œä½†å…¶å¹¿æ³›ä½¿ç”¨ä¹Ÿæ”¾å¤§äº†æœ‰å®³å†…å®¹ï¼Œç‰¹åˆ«æ˜¯ä»‡æ¨è¨€è®ºï¼Œå¨èƒç½‘ç»œå®‰å…¨å’ŒåŒ…å®¹æ€§ã€‚é’ˆå¯¹å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹çš„ç ”ç©¶å°šå­˜ç©ºç™½ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåŒ…å«è‹±è¯­ã€ä¹Œå°”éƒ½è¯­å’Œè¥¿ç­ç‰™è¯­çš„ä¸‰è¯­æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›å±‚ä½œä¸ºåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„å¤„ç†æ­¥éª¤ï¼Œä»¥æé«˜ç‰¹å¾æå–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä½¿ç”¨äº†å‰æ²¿æ¨¡å‹å¦‚GPT-3.5 Turboå’ŒQwen 2.5 72Bä»¥åŠä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æœ€ç»ˆç ”ç©¶è¡¨æ˜è¯¥ç ”ç©¶å®ç°äº†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„é«˜æ•ˆä»‡æ¨è¨€è®ºæ£€æµ‹ï¼Œæœ‰åŠ©äºæé«˜å…¨çƒæ•°å­—ç¤¾åŒºçš„å®‰å…¨æ€§ã€‚ç ”ç©¶åŸºäºæ–‡æœ¬åˆ›å»ºçš„åˆ†ç±»æ¨¡å‹èƒ½å¤Ÿä¸ºæœ‰æ•ˆæ£€æµ‹å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„ä»‡æ¨è¨€è®ºæä¾›åšå®åŸºç¡€ã€‚æ€»ä½“è€Œè¨€ï¼Œæ­¤æ•°æ®é›†æ—¨åœ¨å®ç°æ›´é«˜æ•ˆçš„å¤šè¯­è¨€ä»‡æ¨è¨€è®ºæ£€æµ‹ä»¥ä¿ƒè¿›ç½‘ç»œå®‰å…¨å‘å±•ï¼Œé€ ç¦äººç±»ç¤¾ä¼šã€‚è¿™æ˜¯ä¸€é¡¹å…·æœ‰é‡è¦æ„ä¹‰çš„å·¥ä½œï¼Œå¯¹æ•´ä¸ªç¤¾äº¤åª’ä½“å‘å±•ä¹Ÿè‡³å…³é‡è¦ã€‚å®ƒçš„æ•°æ®é›†å’Œåˆ›æ–°æ–¹æ³•å¯ä»¥æ¨å¹¿åˆ°ç›¸å…³é¢†åŸŸä¸ºæ›´å¤šçš„ä»»åŠ¡æä¾›å¸®åŠ©ã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿä¿ƒè¿›ç½‘ç»œå®‰å…¨çš„å‘å±•ï¼Œè¿›ä¸€æ­¥ä¿æŠ¤ç”¨æˆ·çš„åœ¨çº¿å®‰å…¨å’Œä¸ªäººéšç§ã€‚è¿™å°†æ˜¯ä¸€é¡¹é‡å¤§çš„è´¡çŒ®ï¼Œå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚æˆ‘ä»¬ç›¸ä¿¡è¯¥å·¥ä½œå°†å¯¹æœªæ¥çš„ç¤¾äº¤åª’ä½“å¹³å°äº§ç”Ÿç§¯æå½±å“ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶å¯¹äºç†è§£ç¤¾äº¤åª’ä½“å¹³å°ä¸Šçš„ä»‡æ¨è¨€è®ºä¼ æ’­æœºåˆ¶ä»¥åŠåˆ¶å®šæœ‰æ•ˆçš„åº”å¯¹ç­–ç•¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç ”ç©¶å›¢é˜Ÿçš„å·¥ä½œå€¼å¾—èµèµå’Œæ”¯æŒã€‚å°½ç®¡æŒ‘æˆ˜é‡é‡ï¼Œä»–ä»¬ä»å–å¾—äº†ä»¤äººç©ç›®çš„æˆæœã€‚æˆ‘ä»¬å°†å¯†åˆ‡å…³æ³¨è¯¥ç ”ç©¶çš„åç»­è¿›å±•å’Œå®é™…åº”ç”¨æƒ…å†µã€‚<br><strong>Key Takeaways</strong>:</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-591fa32e8fbb94df72edc74843ea10b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92eafcb69d8203b7fd01f45e11b1c695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a97d484b0421e6b3fba39dc98626d5a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-511dacfbfee786817b512c6c73eb500b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94e16a64785f1ddbcd90d51424549dfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1026150a7805dbf57506ddb1009e1e1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42922acae3f4ebc70f6b2793266ec126.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LID-Models-are-Actually-Accent-Classifiers-Implications-and-Solutions-for-LID-on-Accented-Speech"><a href="#LID-Models-are-Actually-Accent-Classifiers-Implications-and-Solutions-for-LID-on-Accented-Speech" class="headerlink" title="LID Models are Actually Accent Classifiers: Implications and Solutions   for LID on Accented Speech"></a>LID Models are Actually Accent Classifiers: Implications and Solutions   for LID on Accented Speech</h2><p><strong>Authors:Niyati Bafna, Matthew Wiesner</strong></p>
<p>Prior research indicates that LID model performance significantly declines on accented speech; however, the specific causes, extent, and characterization of these errors remain under-explored. (i) We identify a common failure mode on accented speech whereby LID systems often misclassify L2 accented speech as the speakerâ€™s native language or a related language. (ii) We present evidence suggesting that state-of-the-art models are invariant to permutations of short spans of speech, implying they classify on the basis of short phonotactic features indicative of accent rather than language. Our analysis reveals a simple method to enhance model robustness to accents through input chunking. (iii) We present an approach that integrates sequence-level information into our model without relying on monolingual ASR systems; this reduces accent-language confusion and significantly enhances performance on accented speech while maintaining comparable results on standard LID. </p>
<blockquote>
<p>å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLIDæ¨¡å‹åœ¨æœ‰å£éŸ³çš„è¯­éŸ³ä¸Šçš„è¡¨ç°ä¼šæ˜¾è‘—ä¸‹é™ï¼›ç„¶è€Œï¼Œè¿™äº›é”™è¯¯çš„å…·ä½“åŸå› ã€ç¨‹åº¦å’Œç‰¹å¾ä»ç„¶ç¼ºä¹æ·±å…¥æ¢ç´¢ã€‚ï¼ˆiï¼‰æˆ‘ä»¬ç¡®å®šäº†åœ¨æœ‰å£éŸ³çš„è¯­éŸ³ä¸Šçš„ä¸€ç§å¸¸è§å¤±æ•ˆæ¨¡å¼ï¼Œå³LIDç³»ç»Ÿç»å¸¸å°†å¸¦æœ‰å£éŸ³çš„ç¬¬äºŒè¯­è¨€è¯¯åˆ†ç±»ä¸ºè¯´è¯äººçš„æ¯è¯­æˆ–ç›¸å…³è¯­è¨€ã€‚ï¼ˆiiï¼‰æˆ‘ä»¬æä¾›äº†è¯æ®è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹å¯¹äºçŸ­è¯­éŸ³æ®µçš„æ’åˆ—å…·æœ‰ä¸å˜æ€§ï¼Œè¿™æ„å‘³ç€å®ƒä»¬æ˜¯åŸºäºæŒ‡ç¤ºå£éŸ³è€Œéè¯­è¨€çš„çŸ­éŸ³ç³»ç‰¹å¾è¿›è¡Œåˆ†ç±»çš„ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ç§é€šè¿‡è¾“å…¥åˆ†å—æ¥æé«˜æ¨¡å‹å¯¹å£éŸ³çš„ç¨³å¥æ€§çš„ç®€å•æ–¹æ³•ã€‚ï¼ˆiiiï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†åºåˆ—çº§åˆ«çš„ä¿¡æ¯é›†æˆåˆ°æ¨¡å‹ä¸­çš„æ–¹æ³•ï¼Œæ— éœ€ä¾èµ–å•è¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼›è¿™å‡å°‘äº†å£éŸ³è¯­è¨€æ··æ·†ï¼Œå¹¶æ˜¾è‘—æé«˜äº†åœ¨æœ‰å£éŸ³çš„è¯­éŸ³ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ ‡å‡†LIDçš„ç›¸å½“ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00628v2">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€èº«ä»½è¯†åˆ«ï¼ˆLIDï¼‰æ¨¡å‹åœ¨å¸¦æœ‰å£éŸ³çš„è¯­éŸ³ä¸Šçš„æ€§èƒ½é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒLIDæ¨¡å‹åœ¨å¤„ç†å¸¦æœ‰å£éŸ³çš„è¯­éŸ³æ—¶ä¼šå‡ºç°è¯¯åˆ†ç±»çš„æƒ…å†µï¼Œå³å°†éæ¯è¯­å¸¦æœ‰å£éŸ³çš„è¯­éŸ³é”™è¯¯åœ°è¯†åˆ«ä¸ºè¯´è¯è€…çš„æ¯è¯­æˆ–ç›¸å…³è¯­è¨€ã€‚æ–‡ç« åˆ†æäº†å½“å‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šè¿‡è¾“å…¥åˆ†å—å¢å¼ºæ¨¡å‹å¯¹å£éŸ³çš„é²æ£’æ€§çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸€ç§ä¸ä¾èµ–å•è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„æ–¹æ³•ï¼Œå°†åºåˆ—çº§åˆ«çš„ä¿¡æ¯èå…¥æ¨¡å‹ä¸­ï¼Œä»è€Œå‡å°‘å£éŸ³å¼•èµ·çš„è¯­è¨€æ··æ·†ï¼ŒåŒæ—¶ä¿æŒå¯¹æ ‡å‡†LIDçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LIDæ¨¡å‹åœ¨å¤„ç†å¸¦å£éŸ³è¯­éŸ³æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå­˜åœ¨å°†L2å£éŸ³è¯­éŸ³è¯¯åˆ†ç±»ä¸ºæ¯è¯­æˆ–ç›¸å…³è¯­è¨€çš„å¸¸è§å¤±è´¥æ¨¡å¼ã€‚</li>
<li>å½“å‰å…ˆè¿›æ¨¡å‹å¯¹çŸ­è¯­éŸ³ç‰‡æ®µçš„æ’åˆ—é¡ºåºå…·æœ‰ä¸å˜æ€§ï¼Œæš—ç¤ºå®ƒä»¬åŸºäºçŸ­è¯­çš„éŸ³éŸµç‰¹å¾è€Œéè¯­è¨€è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>é€šè¿‡è¾“å…¥åˆ†å—å¯ä»¥æé«˜æ¨¡å‹å¯¹å£éŸ³çš„é²æ£’æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é›†æˆåºåˆ—çº§åˆ«ä¿¡æ¯çš„æ–¹æ³•ï¼Œæ— éœ€ä¾èµ–å•è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å‡å°‘å› å£éŸ³å¯¼è‡´çš„è¯­è¨€æ··æ·†ï¼Œæ˜¾è‘—æé«˜å¸¦å£éŸ³è¯­éŸ³çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†LIDä¸Šçš„æ€§èƒ½ç»´æŒä¸å˜ã€‚</li>
<li>è¿™äº›å‘ç°å¯¹äºæ”¹è¿›LIDæ¨¡å‹çš„æ€§èƒ½å’Œé€‚åº”æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58ee9d581bad8eb626c901e3b575d0fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84e1502fde70503d34a2f763cc0b82ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28397b80e70b087f80dfee8818cf7002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2a24dcaa347db9e2df90db5738f8014.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement"><a href="#LLaSE-G1-Incentivizing-Generalization-Capability-for-LLaMA-based-Speech-Enhancement" class="headerlink" title="LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement"></a>LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech   Enhancement</h2><p><strong>Authors:Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie</strong></p>
<p>Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area. </p>
<blockquote>
<p>æœ€è¿‘çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¿›å±•è¡¨æ˜ï¼Œå…¶åœ¨è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶åœ¨ç”Ÿæˆæ€§è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ä¸­ç¹è£å‘å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šåŸºäºLMçš„SEæ–¹æ³•ä¸»è¦å…³æ³¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¾€å¾€å¿½è§†äº†å£°éŸ³ä¿¡æ¯çš„å…³é”®ä½œç”¨ï¼Œè¿™å¯¼è‡´å¢å¼ºåçš„è¯­éŸ³å‡ºç°å£°éŸ³ä¸ä¸€è‡´ï¼Œä»¥åŠåœ¨å„ç§SEä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LLaSE-G1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLaMAçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ¿€åŠ±å…¶åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚LLaSE-G1æœ‰ä»¥ä¸‹å…³é”®è´¡çŒ®ï¼šé¦–å…ˆï¼Œä¸ºäº†ç¼“è§£å£°éŸ³ä¸ä¸€è‡´çš„é—®é¢˜ï¼ŒLLaSE-G1é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå¹¶é€šè¿‡X-Codec2é¢„æµ‹è¯­éŸ³ä»¤ç‰Œï¼Œæœ€å¤§é™åº¦åœ°ä¿ç•™å£°éŸ³ã€‚å…¶æ¬¡ï¼Œä¸ºäº†æå‡æ³›åŒ–èƒ½åŠ›ï¼ŒLLaSE-G1å¼•å…¥äº†åŒé€šé“è¾“å…¥å’Œè¾“å‡ºï¼Œå¯ä»¥ç»Ÿä¸€å¤šç§SEä»»åŠ¡è€Œæ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚ç¬¬ä¸‰ï¼ŒLLaSE-G1åœ¨æµ‹è¯•æ—¶é—´å±•ç°å‡ºè§„æ¨¡åŒ–æ•ˆæœï¼Œä¼˜äºä¹‹å‰çš„ç‰¹å®šä»»åŠ¡åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼SEæ¨¡å‹ï¼Œå¹¶å…·æœ‰å¤„ç†æœªè§è¿‡çš„SEä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹ä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00493v4">PDF</a> ACL2025 main, Codes available at   <a target="_blank" rel="noopener" href="https://github.com/Kevin-naticl/LLaSE-G1">https://github.com/Kevin-naticl/LLaSE-G1</a></p>
<p><strong>æ‘˜è¦</strong><br>æœ¬æ–‡ä»‹ç»äº†åŸºäºLLaMAçš„LLaSE-G1è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ï¼Œå¹¶å¼•å…¥äº†ä¸€ç³»åˆ—åˆ›æ–°æŠ€æœ¯æ¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚LLaSE-G1é€šè¿‡é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºä½œä¸ºè¾“å…¥å¹¶é¢„æµ‹X-Codec2çš„è¯­éŸ³ä»¤ç‰Œæ¥å‡è½»å£°å­¦ä¸ä¸€è‡´çš„é—®é¢˜ï¼ŒåŒæ—¶é‡‡ç”¨åŒé€šé“è¾“å…¥å’Œè¾“å‡ºä»¥ç»Ÿä¸€å¤šç§è¯­éŸ³å¢å¼ºä»»åŠ¡ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä»»åŠ¡ç‰¹å®šåˆ¤åˆ«å’Œç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œæµ‹è¯•æ—¶é—´æ‰©å±•æ•ˆåº”ã€‚æœ¬æ–‡è¿˜å…¬å¼€äº†ç›¸å…³ä»£ç å’Œæ¨¡å‹ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li><p>LLaSE-G1è¯­è¨€æ¨¡å‹æ—¨åœ¨è§£å†³ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­å¿½ç•¥å£°å­¦ä¿¡æ¯çš„é—®é¢˜ï¼Œé€šè¿‡é‡‡ç”¨WavLMçš„è¿ç»­è¡¨ç¤ºæ¥å‡è½»å£°å­¦ä¸ä¸€è‡´æ€§ã€‚</p>
</li>
<li><p>LLaSE-G1ä½¿ç”¨X-Codec2é¢„æµ‹è¯­éŸ³ä»¤ç‰Œä»¥æœ€å¤§åŒ–å£°å­¦ä¿ç•™æ•ˆæœã€‚</p>
</li>
<li><p>è¯¥æ¨¡å‹å¼•å…¥åŒé¢‘é“è¾“å…¥å’Œè¾“å‡ºä»¥ä¿ƒè¿›æ³›åŒ–èƒ½åŠ›ï¼Œç»Ÿä¸€å¤šç§è¯­éŸ³å¢å¼ºä»»åŠ¡ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡æ ‡è¯†ã€‚</p>
</li>
<li><p>LLaSE-G1çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„ä»»åŠ¡ç‰¹å®šåˆ¤åˆ«å’Œç”Ÿæˆå¼è¯­éŸ³å¢å¼ºæ¨¡å‹ã€‚</p>
</li>
<li><p>LLaSE-G1æ¨¡å‹åœ¨æµ‹è¯•æ—¶å±•ç°å‡ºæ‰©å±•æ•ˆåº”ï¼Œå¹¶èƒ½åº”å¯¹æœªè§è¿‡çš„è¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚</p>
</li>
<li><p>å…¬å¼€çš„ä»£ç å’Œæ¨¡å‹å°†æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2164145bff5233c7a4b10e30bbf9a51b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b0519543f682a16f37db49b59d4486d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience"><a href="#Listen-Chat-and-Remix-Text-Guided-Soundscape-Remixing-for-Enhanced-Auditory-Experience" class="headerlink" title="Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience"></a>Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced   Auditory Experience</h2><p><strong>Authors:Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani</strong></p>
<p>In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces â€œListen, Chat, and Remixâ€ (LCR), a novel multimodal sound remixer that controls each sound source in a mixture based on user-provided text instructions. LCR distinguishes itself with a user-friendly text interface and its unique ability to remix multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for remixing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles filtered components back to the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse remixing tasks including extraction, removal, and volume control of single or multiple sources. Our experiments demonstrate significant improvements in signal quality across all remixing tasks and robust performance in zero-shot scenarios with varying numbers and types of sound sources. An audio demo is available at: <a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a>. </p>
<blockquote>
<p>åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬ä¼šé‡åˆ°å„ç§å£°éŸ³ï¼ŒåŒ…æ‹¬æƒ³è¦çš„å’Œä¸æƒ³å¬åˆ°çš„ï¼Œä½†å¯¹å®ƒä»¬çš„å­˜åœ¨å’ŒéŸ³é‡åªæœ‰æœ‰é™çš„æ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†â€œListen, Chat, and Remixâ€ï¼ˆLCRï¼‰è¿™ä¸€æ–°é¢–çš„å¤šæ¨¡å¼å£°éŸ³æ··éŸ³å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤æ§åˆ¶æ··åˆä¸­çš„æ¯ä¸ªå£°éŸ³æºã€‚LCRé€šè¿‡ç”¨æˆ·å‹å¥½çš„æ–‡æœ¬æ¥å£å’Œå…¶åœ¨æ··åˆä¸­åŒæ—¶æ··éŸ³å¤šä¸ªå£°éŸ³æºè€Œæ— éœ€åˆ†ç¦»çš„ç‹¬æœ‰èƒ½åŠ›æ¥åŒºåˆ†è‡ªå·±ã€‚ç”¨æˆ·è¾“å…¥å¼€æ”¾å¼è¯æ±‡æ–‡æœ¬æç¤ºï¼Œç”±å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè§£é‡Šï¼Œä¸ºæ··éŸ³å£°éŸ³æ··åˆç‰©åˆ›å»ºè¯­ä¹‰è¿‡æ»¤å™¨ã€‚ç„¶åè¯¥ç³»ç»Ÿå°†æ··åˆç‰©åˆ†è§£æˆå…¶ç»„æˆéƒ¨åˆ†ï¼Œåº”ç”¨è¯­ä¹‰è¿‡æ»¤å™¨ï¼Œå¹¶å°†è¿‡æ»¤åçš„ç»„ä»¶é‡æ–°ç»„è£…æˆæ‰€éœ€çš„è¾“å‡ºã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡10ä¸‡æ··åˆç‰©çš„160å°æ—¶æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¯­éŸ³å’Œå„ç§éŸ³é¢‘æºä»¥åŠç”¨äºå„ç§æ··éŸ³ä»»åŠ¡çš„æ–‡æœ¬æç¤ºï¼ŒåŒ…æ‹¬æå–ã€åˆ é™¤å’Œæ§åˆ¶å•ä¸ªæˆ–å¤šä¸ªæºçš„éŸ³é‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜åœ¨æ‰€æœ‰æ··éŸ³ä»»åŠ¡ä¸­ä¿¡å·è´¨é‡éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°é‡å’Œç±»å‹çš„éŸ³æºåœºæ™¯ä¸­é›¶æ ·æœ¬æƒ…æ™¯ä¸‹çš„æ€§èƒ½ç¨³å¥ã€‚éŸ³é¢‘æ¼”ç¤ºä½œå“å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://listenchatremix.github.io/demo">https://listenchatremix.github.io/demo</a> è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.03710v2">PDF</a> Accepted by IEEE Journal of Selected Topics in Signal Processing   (JSTSP)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºâ€œListen, Chat, and Remixâ€ï¼ˆLCRï¼‰çš„æ–°å‹å¤šæ¨¡å¼å£°éŸ³æ··éŸ³å™¨ã€‚LCRé€šè¿‡ç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤æ§åˆ¶æ··åˆå£°éŸ³ä¸­çš„æ¯ä¸ªå£°éŸ³æºã€‚å…¶ç‰¹ç‚¹ä¸ºç”¨æˆ·å‹å¥½çš„æ–‡æœ¬ç•Œé¢ï¼Œä»¥åŠåŒæ—¶æ··éŸ³å¤šä¸ªå£°éŸ³æºçš„èƒ½åŠ›ï¼Œæ— éœ€å•ç‹¬åˆ†ç¦»ã€‚ç³»ç»Ÿé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šç”¨æˆ·è¾“å…¥çš„å¼€æ”¾è¯æ±‡æ–‡æœ¬æç¤ºï¼Œåˆ›å»ºè¯­ä¹‰è¿‡æ»¤å™¨æ¥æ··éŸ³å£°éŸ³æ··åˆç‰©ã€‚å®éªŒè¡¨æ˜ï¼ŒLCRåœ¨æ‰€æœ‰çš„æ··éŸ³ä»»åŠ¡ä¸­éƒ½æ˜¾è‘—æé«˜ä¿¡å·è´¨é‡ï¼Œå¹¶åœ¨ä¸åŒæ•°é‡å’Œç±»å‹çš„éŸ³æºåœºæ™¯ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCRæ˜¯ä¸€ç§æ–°å‹å¤šæ¨¡å¼å£°éŸ³æ··éŸ³å™¨ï¼Œå¯ä»¥é€šè¿‡ç”¨æˆ·æä¾›çš„æ–‡æœ¬æŒ‡ä»¤æ§åˆ¶æ··åˆå£°éŸ³ä¸­çš„æ¯ä¸ªå£°éŸ³æºã€‚</li>
<li>LCRå…·æœ‰ç”¨æˆ·å‹å¥½çš„æ–‡æœ¬ç•Œé¢ï¼Œå¯åˆ›å»ºè¯­ä¹‰è¿‡æ»¤å™¨æ¥æ··éŸ³å£°éŸ³æ··åˆç‰©ï¼Œæ— éœ€å•ç‹¬åˆ†ç¦»å¤šä¸ªå£°éŸ³æºã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡Šç”¨æˆ·è¾“å…¥çš„å¼€æ”¾è¯æ±‡æ–‡æœ¬æç¤ºã€‚</li>
<li>LCRèƒ½å¤„ç†åŒ…æ‹¬è¯­éŸ³å’Œå„ç§éŸ³é¢‘æºåœ¨å†…çš„æ··åˆå£°éŸ³ï¼Œå¹¶å¯ä»¥è¿›è¡Œæå–ã€åˆ é™¤ã€æ§åˆ¶å•ä¸€æˆ–å¤šä¸ªæºçš„éŸ³é‡ç­‰å¤šæ ·åŒ–çš„æ··éŸ³ä»»åŠ¡ã€‚</li>
<li>LCRæ‹¥æœ‰160å°æ—¶çš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡ä¸ªæ··åˆç‰©ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLCRåœ¨æ‰€æœ‰çš„æ··éŸ³ä»»åŠ¡ä¸­éƒ½æ˜¾è‘—æé«˜ä¿¡å·è´¨é‡ï¼Œè¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.03710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7dd10c16d26f590cbf14dcd0ddcf596e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4a9c637a1f386f8f6db21f83434ec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc06ba878ad5fe8d3e3c90572ca5a480.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8531b4f1e51e0995113da1f15c1c31c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89f167e5f57e22f658633e04e0ae2322.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-45e982bbb5e881e18505a77319741526.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  DGAE Diffusion-Guided Autoencoder for Efficient Latent Representation   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-de6bdfd4aaf563eff942e34710409360.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  DIsoN Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
