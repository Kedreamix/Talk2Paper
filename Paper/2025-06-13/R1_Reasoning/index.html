<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f2339f60031315509f905003087eeda9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="A-Shortcut-aware-Video-QA-Benchmark-for-Physical-Understanding-via-Minimal-Video-Pairs"><a href="#A-Shortcut-aware-Video-QA-Benchmark-for-Physical-Understanding-via-Minimal-Video-Pairs" class="headerlink" title="A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs"></a>A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs</h2><p><strong>Authors:Benno Krojer, Mojtaba Komeili, Candace Ross, Quentin Garrido, Koustuv Sinha, Nicolas Ballas, Mahmoud Assran</strong></p>
<p>Existing benchmarks for assessing the spatio-temporal understanding and reasoning abilities of video language models are susceptible to score inflation due to the presence of shortcut solutions based on superficial visual or textual cues. This paper mitigates the challenges in accurately assessing model performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple shortcut-aware video QA benchmark for assessing the physical understanding of video language models. The benchmark is comprised of 55K high-quality multiple-choice video QA examples focusing on physical world understanding. Examples are curated from nine video data sources, spanning first-person egocentric and exocentric videos, robotic interaction data, and cognitive science intuitive physics benchmarks. To mitigate shortcut solutions that rely on superficial visual or textual cues and biases, each sample in MVP has a minimal-change pair â€“ a visually similar video accompanied by an identical question but an opposing answer. To answer a question correctly, a model must provide correct answers for both examples in the minimal-change pair; as such, models that solely rely on visual or textual biases would achieve below random performance. Human performance on MVP is 92.9%, while the best open-source state-of-the-art video-language model achieves 40.2% compared to random performance at 25%. </p>
<blockquote>
<p>ç°æœ‰è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ—¶ç©ºç†è§£å’Œæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œç”±äºå­˜åœ¨åŸºäºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢çš„æ·å¾„è§£å†³æ–¹æ¡ˆï¼Œå®¹æ˜“å¼•å‘å¾—åˆ†è†¨èƒ€ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥æœ€å°è§†é¢‘å¯¹ï¼ˆMVPï¼‰åŸºå‡†æµ‹è¯•æ¥ç¼“è§£å‡†ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‘æˆ˜ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ˜¯ä¸€ä¸ªç®€å•çš„æ·å¾„æ„ŸçŸ¥è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«5.5ä¸‡ä¸ªé«˜è´¨é‡çš„å¤šé¡¹é€‰æ‹©è§†é¢‘é—®ç­”ç¤ºä¾‹ï¼Œä¾§é‡äºå¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚è¿™äº›ç¤ºä¾‹æ˜¯ä»ä¹ä¸ªè§†é¢‘æ•°æ®æºä¸­ç²¾é€‰å‡ºæ¥çš„ï¼ŒåŒ…æ‹¬ç¬¬ä¸€äººç§°ä¸»è§‚å’Œå®¢è§‚è§†é¢‘ã€æœºå™¨äººäº¤äº’æ•°æ®å’Œè®¤çŸ¥ç§‘å­¦ç›´è§‰ç‰©ç†åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†ç¼“è§£ä¾èµ–äºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢å’Œåè§çš„æ·å¾„è§£å†³æ–¹æ¡ˆï¼ŒMVPä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–çš„å¯¹â€”â€”ä¸€ä¸ªè§†è§‰ç›¸ä¼¼çš„è§†é¢‘ä¼´éšä¸€ä¸ªç›¸åŒçš„é—®é¢˜ä½†ç­”æ¡ˆç›¸åã€‚è¦æ­£ç¡®å›ç­”é—®é¢˜ï¼Œæ¨¡å‹å¿…é¡»ä¸ºæœ€å°å˜åŒ–å¯¹ä¸­çš„ä¸¤ä¸ªç¤ºä¾‹æä¾›æ­£ç¡®ç­”æ¡ˆï¼›å› æ­¤ï¼Œä»…ä¾èµ–è§†è§‰æˆ–æ–‡æœ¬åè§çš„æ¨¡å‹å°†ä½äºéšæœºæ€§èƒ½ã€‚äººç±»åœ¨MVPä¸Šçš„è¡¨ç°æ˜¯92.9%ï¼Œè€Œæœ€ä½³å¼€æºçš„å½“å‰å…ˆè¿›è§†é¢‘è¯­è¨€æ¨¡å‹è¾¾åˆ°40.2%ï¼Œè€Œéšæœºæ€§èƒ½ä¸º25%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹æ—¶ç©ºç†è§£å’Œæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•â€”â€”æœ€å°è§†é¢‘å¯¹ï¼ˆMVPï¼‰ã€‚è¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•å› ä¾èµ–è¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢è€Œå¯¼è‡´è¯„åˆ†è†¨èƒ€çš„é—®é¢˜ã€‚MVPåŒ…å«ä»ä¹ä¸ªè§†é¢‘æ•°æ®æºä¸­ç²¾é€‰çš„5ä¸‡äº”åƒä¸ªé«˜è´¨é‡çš„è§†é¢‘é—®ç­”é€‰æ‹©é¢˜æ ·æœ¬ï¼Œä¸»è¦å…³æ³¨ç‰©ç†ä¸–ç•Œç†è§£ã€‚ä¸ºäº†é™ä½ä¾èµ–è¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢å’Œåè§çš„æ·å¾„è§£å†³æ–¹æ¡ˆï¼ŒMVPä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–çš„å¯¹â€”â€”ä¸€ä¸ªè§†è§‰ä¸Šç›¸ä¼¼ä½†ç­”æ¡ˆç›¸åçš„è§†é¢‘å’Œç›¸åŒçš„é—®é¢˜ã€‚ä¸ºäº†æ­£ç¡®å›ç­”é—®é¢˜ï¼Œæ¨¡å‹å¿…é¡»ä¸ºæœ€å°å˜åŒ–å¯¹ä¸­çš„ä¸¤ä¸ªç¤ºä¾‹éƒ½æä¾›æ­£ç¡®ç­”æ¡ˆï¼›å› æ­¤ï¼Œä»…ä¾èµ–è§†è§‰æˆ–æ–‡æœ¬åè§çš„æ¨¡å‹å°†ä½äºéšæœºæ€§èƒ½ã€‚äººç±»åœ¨MVPä¸Šçš„è¡¨ç°è¾¾åˆ°äº†92.9%ï¼Œè€Œç›®å‰æœ€å…ˆè¿›çš„å¼€æºè§†é¢‘è¯­è¨€æ¨¡å‹çš„æœ€ä½³è¡¨ç°ä»…ä¸ºéšæœºæ€§èƒ½çš„ååˆ†ä¹‹ä¸€ä¸åˆ°ï¼ˆè¾¾åˆ°äº†ä¸åˆ°ä»…æœ‰åäººä¹‹å¹´çš„é¢„ä¼°èƒ½åŠ›çš„çº§åˆ«ï¼‰ã€‚åœ¨è¿™ä¸ªå…¨æ–°çš„è¯„ä¼°ä½“ç³»ä¸‹ï¼Œè™½ç„¶ç›®å‰è¿˜æ²¡æœ‰èƒ½å¤Ÿè¾¾åˆ°æ¥è¿‘äººç±»ç†è§£æ°´å¹³çš„è§†é¢‘è¯­è¨€æ¨¡å‹å‡ºç°ã€‚æ­¤æ¡†æ¶æ—¨åœ¨å¼•å¯¼ç ”ç©¶äººå‘˜é€šè¿‡åˆ©ç”¨è§†é¢‘çš„æ—¶ç©ºç‰¹å¾è¿›è¡Œè§†é¢‘ç†è§£çš„ç ”ç©¶ã€‚è¿™ä¸ºæ¨åŠ¨æ›´æ™ºèƒ½çš„è§†é¢‘è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†ä¸€ä¸ªé‡è¦å·¥å…·ã€‚è¿™å°†é¼“åŠ±å¼€å‘èƒ½æ›´å¥½åœ°åº”å¯¹è§†å¬å†…å®¹çš„æ¨¡å‹å’Œç³»ç»Ÿçš„åº”ç”¨è¿›ä¸€æ­¥éªŒè¯æ–°æŠ€æœ¯å¹¶å°†å…¶éƒ¨ç½²åœ¨å®é™…çš„åº”ç”¨åœºæ™¯ä¸­æ‰€é‡è§†æ–¹æ³•çš„å®é™…éœ€æ±‚å±•ç¤ºé‡è¦ä½œç”¨è¯æ˜äº†åœ¨ä¸åŒæ¡ä»¶å’Œè®¾å®šä¸‹çš„ä¼˜åŠ¿æ˜ç¡®äº†çœŸæ­£ä¼˜è´¨æˆæœå’Œæ ‡å‡†æ‰€é¢ä¸´çš„æŒ‘æˆ˜æ˜¯éœ€è¦è¿›ä¸€æ­¥çš„è¯„ä¼°å’Œç²¾ç¡®çš„æ€§èƒ½è€ƒæ ¸ä»è€Œåœ¨è®­ç»ƒè¿™äº›å…ˆè¿›AIçš„åŒæ—¶å°½å¯èƒ½èå…¥æ­£ç¡®çš„æŒ‡å¯¼å’Œè€ƒå¯Ÿæˆä¸ºæŠ€æœ¯è¿›æ­¥é‡è¦æ¨åŠ›ä»¥å¸®åŠ©å¼€å‘æ›´åŠ ç¬¦åˆäººç±»æœŸæœ›çš„æ™ºèƒ½è§†é¢‘è¯­è¨€æ¨¡å‹ã€‚éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œç ”ç©¶çš„æ·±å…¥ç›¸ä¿¡æœªæ¥ä¼šæœ‰æ›´å¤šçš„åˆ›æ–°æ–¹æ³•å’Œåº”ç”¨æ¶Œç°å‡ºæ¥ä¸ºè§†é¢‘è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥æ›´åŠ å¹¿é˜”çš„è§†é‡å’Œå¯èƒ½æ€§ã€‚æœªæ¥çš„ç ”ç©¶å°†éœ€è¦å…³æ³¨å¦‚ä½•åœ¨ä¸åŒçš„è§†é¢‘å†…å®¹åœºæ™¯ä¸‹æœ‰æ•ˆåœ°åº”ç”¨è¿™ä¸€è¯„ä¼°ä½“ç³»ä»¥åŠå¦‚ä½•åˆ©ç”¨è¯¥ä½“ç³»æ¥æŒ‡å¯¼æ¨¡å‹è®¾è®¡ä»¥æé«˜è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ç­‰é—®é¢˜ä¸Šä»¥å®ç°çœŸæ­£çš„æ™ºèƒ½åŒ–è§†é¢‘å¤„ç†åº”ç”¨å‰æ™¯å¹¿é˜”æœªæ¥å¯æœŸã€‚éšç€æœªæ¥å¯¹äºæ›´ç²¾ç»†æ›´å…¨é¢çš„è¯„ä¼°æ ‡å‡†çš„è¿½æ±‚å¯¹äºè¿™ä¸€é¢†åŸŸçš„å‘å±•å°†èµ·åˆ°ç§¯æçš„æ¨åŠ¨ä½œç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡æœªæ¥çš„è§†é¢‘è¯­è¨€æ¨¡å‹å°†åœ¨æ—¶ç©ºç†è§£å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ›´å¤§çš„çªç ´ä¸ºäººç±»å¸¦æ¥æ›´åŠ ä¾¿æ·å’Œæ™ºèƒ½çš„è§†å¬ä½“éªŒã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†å­˜åœ¨è¯„åˆ†è†¨èƒ€é—®é¢˜ï¼Œå› ä¸ºéƒ¨åˆ†æ¨¡å‹ä¾èµ–è¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢æ¥å›ç­”é—®é¢˜ã€‚</li>
<li>MVPåŸºå‡†æµ‹è¯•æ—¨åœ¨è§£å†³è¯„åˆ†è†¨èƒ€é—®é¢˜ï¼ŒåŒ…å«é«˜è´¨é‡çš„è§†é¢‘é—®ç­”é€‰æ‹©é¢˜æ ·æœ¬ï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†ä¸–ç•Œç†è§£ã€‚</li>
<li>MVPä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–çš„å¯¹ï¼ŒåŒ…æ‹¬è§†è§‰ä¸Šç›¸ä¼¼ä½†ç­”æ¡ˆä¸åŒçš„è§†é¢‘å’Œç›¸åŒé—®é¢˜ï¼Œä»¥åŒºåˆ†çœŸæ­£ç†è§£å’Œä¾èµ–è¡¨é¢çº¿ç´¢çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a40150f7ccd33ca14f5fa2f9256ce26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69c438fc11f30eede4b199f5dbf55e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07f1bce7c38a7137a9759a645973da4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7635d15b552978158bf81194df6ab85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66964b08c5bd718512a85fc0a5039f4b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Spatial-Reasoning-in-Vision-Language-Models-with-Interwoven-Thinking-and-Visual-Drawing"><a href="#Reinforcing-Spatial-Reasoning-in-Vision-Language-Models-with-Interwoven-Thinking-and-Visual-Drawing" class="headerlink" title="Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven   Thinking and Visual Drawing"></a>Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven   Thinking and Visual Drawing</h2><p><strong>Authors:Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan</strong></p>
<p>As textual reasoning with large language models (LLMs) has advanced significantly, there has been growing interest in enhancing the multimodal reasoning capabilities of large vision-language models (LVLMs). However, existing methods primarily approach multimodal reasoning in a straightforward, text-centric manner, where both reasoning and answer derivation are conducted purely through text, with the only difference being the presence of multimodal input. As a result, these methods often encounter fundamental limitations in spatial reasoning tasks that demand precise geometric understanding and continuous spatial tracking-capabilities that humans achieve through mental visualization and manipulation. To address the limitations, we propose drawing to reason in space, a novel paradigm that enables LVLMs to reason through elementary drawing operations in the visual space. By equipping models with basic drawing operations, including annotating bounding boxes and drawing auxiliary lines, we empower them to express and analyze spatial relationships through direct visual manipulation, meanwhile avoiding the performance ceiling imposed by specialized perception tools in previous tool-integrated reasoning approaches. To cultivate this capability, we develop a three-stage training framework: cold-start training with synthetic data to establish basic drawing abilities, reflective rejection sampling to enhance self-reflection behaviors, and reinforcement learning to directly optimize for target rewards. Extensive experiments demonstrate that our model, named VILASR, consistently outperforms existing methods across diverse spatial reasoning benchmarks, involving maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning tasks, with an average improvement of 18.4%. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œå¢å¼ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›æ—¥ç›Šå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é‡‡å–ç›´æ¥ã€ä»¥æ–‡æœ¬ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æ¨ç†æ–¹å¼ï¼Œæ¨ç†å’Œç­”æ¡ˆæ¨å¯¼éƒ½çº¯ç²¹é€šè¿‡æ–‡æœ¬è¿›è¡Œï¼Œä¸åŒä¹‹å¤„çš„å”¯ä¸€åœ¨äºæ˜¯å¦å­˜åœ¨å¤šæ¨¡æ€è¾“å…¥ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•åœ¨éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’ŒæŒç»­ç©ºé—´è·Ÿè¸ªèƒ½åŠ›çš„ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ç»å¸¸é‡åˆ°æ ¹æœ¬æ€§çš„å±€é™ï¼Œè€Œäººç±»åˆ™é€šè¿‡å¿ƒç†å¯è§†åŒ–å’Œæ“ä½œæ¥å®ç°è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹å¼â€”â€”é€šè¿‡ç»˜å›¾è¿›è¡Œç©ºé—´æ¨ç†ã€‚è¿™ç§æ–¹å¼ä½¿LVLMèƒ½å¤Ÿé€šè¿‡è§†è§‰ç©ºé—´çš„åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œæ¨ç†ã€‚é€šè¿‡ä¸ºæ¨¡å‹é…å¤‡åŸºæœ¬çš„ç»˜å›¾æ“ä½œï¼ŒåŒ…æ‹¬æ ‡æ³¨è¾¹ç•Œæ¡†å’Œç»˜åˆ¶è¾…åŠ©çº¿ï¼Œæˆ‘ä»¬å¯ä»¥è®©å®ƒä»¬é€šè¿‡ç›´æ¥çš„è§†è§‰æ“ä½œæ¥è¡¨è¾¾å’Œåˆ†æç©ºé—´å…³ç³»ï¼ŒåŒæ—¶é¿å…ä¹‹å‰å·¥å…·é›†æˆæ¨ç†æ–¹æ³•ä¸­ä¸“ç”¨æ„ŸçŸ¥å·¥å…·æ‰€å¸¦æ¥çš„æ€§èƒ½ä¸Šé™ã€‚ä¸ºäº†åŸ¹å…»è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒä»¥å»ºç«‹åŸºæœ¬çš„ç»˜å›¾èƒ½åŠ›ï¼Œç„¶åæ˜¯åæ€æ‹’ç»é‡‡æ ·ä»¥å¢å¼ºè‡ªæˆ‘åæ€è¡Œä¸ºï¼Œæœ€åæ˜¯å¼ºåŒ–å­¦ä¹ ä»¥ç›´æ¥ä¼˜åŒ–ç›®æ ‡å¥–åŠ±ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹â€”â€”VILASRåœ¨å¤šç§ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ¶‰åŠè¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€åŸºäºè§†é¢‘å’ŒåŸºäºå¤šè§†è§’çš„æ¨ç†ä»»åŠ¡ç­‰ï¼Œå¹³å‡æé«˜äº†18.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09965v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é‡‡å–ç›´æ¥çš„æ–‡æœ¬ä¸­å¿ƒæ–¹å¼ï¼Œä»…åœ¨å­˜åœ¨å¤šæ¨¡æ€è¾“å…¥æ—¶æœ‰æ‰€ä¸åŒã€‚åœ¨éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’Œè¿ç»­ç©ºé—´è¿½è¸ªèƒ½åŠ›çš„ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§å±€é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¨¡å¼â€”â€”â€œç»˜å›¾æ¨ç†â€ï¼Œä½¿LVLMsèƒ½å¤Ÿé€šè¿‡è§†è§‰ç©ºé—´çš„åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œæ¨ç†ã€‚é€šè¿‡æä¾›æ ‡æ³¨è¾¹ç•Œæ¡†ã€ç»˜åˆ¶è¾…åŠ©çº¿ç­‰åŸºæœ¬ç»˜å›¾åŠŸèƒ½ï¼Œæ¨¡å‹èƒ½å¤Ÿç›´æ¥é€šè¿‡è§†è§‰æ“ä½œè¡¨è¾¾å’Œåˆ†æç©ºé—´å…³ç³»ã€‚ä¸ºåŸ¹å…»æ­¤èƒ½åŠ›ï¼Œå¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬ç”¨åˆæˆæ•°æ®è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒã€å¢å¼ºè‡ªæˆ‘åæ€è¡Œä¸ºçš„åå°„æ‹’ç»é‡‡æ ·ä»¥åŠç›´æ¥ä¼˜åŒ–ç›®æ ‡å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼ŒVILASRæ¨¡å‹åœ¨å¤šç§ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¿·å®«å¯¼èˆªã€é™æ€ç©ºé—´æ¨ç†ã€è§†é¢‘æ¨ç†å’Œå¤šè§†è§’æ¨ç†ä»»åŠ¡ï¼Œå¹³å‡æé«˜äº†18.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç¡®å‡ ä½•ç†è§£å’Œè¿ç»­ç©ºé—´è¿½è¸ªèƒ½åŠ›çš„ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é‡‡å–æ–‡æœ¬ä¸­å¿ƒçš„å¤šæ¨¡æ€æ¨ç†æ–¹å¼ï¼Œç¼ºä¹ç›´æ¥è§†è§‰æ“ä½œçš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†â€œç»˜å›¾æ¨ç†â€çš„æ–°æ¨¡å¼ï¼Œä½¿LVLMsèƒ½å¤Ÿé€šè¿‡åŸºæœ¬ç»˜å›¾æ“ä½œè¿›è¡Œç©ºé—´æ¨ç†ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶æ¥åŸ¹å…»æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VILASRæ¨¡å‹åœ¨å¤šç§ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æé«˜äº†18.4%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ace44b1b90309499b676f3f4ced7d716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f096d0b7fd10edd3976099a74ae8172f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51dad34af7e3694e2389b328032cf602.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Kvasir-VQA-x1-A-Multimodal-Dataset-for-Medical-Reasoning-and-Robust-MedVQA-in-Gastrointestinal-Endoscopy"><a href="#Kvasir-VQA-x1-A-Multimodal-Dataset-for-Medical-Reasoning-and-Robust-MedVQA-in-Gastrointestinal-Endoscopy" class="headerlink" title="Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust   MedVQA in Gastrointestinal Endoscopy"></a>Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust   MedVQA in Gastrointestinal Endoscopy</h2><p><strong>Authors:Sushant Gautam, Michael A. Riegler, PÃ¥l Halvorsen</strong></p>
<p>Medical Visual Question Answering (MedVQA) is a promising field for developing clinical decision support systems, yet progress is often limited by the available datasets, which can lack clinical complexity and visual diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new, large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs that are designed to test deeper clinical reasoning. We developed a systematic method using large language models to generate these questions, which are stratified by complexity to better assess a modelâ€™s inference capabilities. To ensure our dataset prepares models for real-world clinical scenarios, we have also introduced a variety of visual augmentations that mimic common imaging artifacts. The dataset is structured to support two main evaluation tracks: one for standard VQA performance and another to test model robustness against these visual perturbations. By providing a more challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate the development of more reliable and effective multimodal AI systems for use in clinical settings. The dataset is fully accessible and adheres to FAIR data principles, making it a valuable resource for the wider research community. Code and data: <a target="_blank" rel="noopener" href="https://github.com/Simula/Kvasir-VQA-x1">https://github.com/Simula/Kvasir-VQA-x1</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1">https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1</a> </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMedVQAï¼‰æ˜¯å¼€å‘ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿçš„ä¸€ä¸ªæœ‰å‰é€”çš„é¢†åŸŸï¼Œç„¶è€Œï¼Œå…¶è¿›å±•å¾€å¾€å—åˆ°å¯ç”¨æ•°æ®é›†çš„é™åˆ¶ï¼Œè¿™äº›æ•°æ®é›†å¯èƒ½ç¼ºä¹ä¸´åºŠå¤æ‚æ€§å’Œè§†è§‰å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Kvasir-VQA-x1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºèƒƒè‚ é“ï¼ˆGIï¼‰å†…çª¥é•œæ£€æŸ¥çš„å¤§å‹æ–°æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨åŸå§‹Kvasir-VQAçš„åŸºç¡€ä¸Šè¿›è¡Œäº†é‡å¤§æ‰©å±•ï¼Œçº³å…¥äº†159549ä¸ªæ–°é—®ç­”å¯¹ï¼Œæ—¨åœ¨æµ‹è¯•æ›´æ·±å±‚æ¬¡çš„ä¸´åºŠæ¨ç†ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»ŸåŒ–æ–¹æ³•æ¥ç”Ÿæˆè¿™äº›é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æŒ‰å¤æ‚æ€§åˆ†å±‚ï¼Œä»¥æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„æ•°æ®é›†ä¸ºç°å®ä¸–ç•Œçš„ä¸´åºŠåœºæ™¯åšå¥½å‡†å¤‡ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å„ç§è§†è§‰å¢å¼ºåŠŸèƒ½ï¼Œä»¥æ¨¡æ‹Ÿå¸¸è§çš„æˆåƒä¼ªå½±ã€‚è¯¥æ•°æ®é›†çš„ç»“æ„æ”¯æŒä¸¤ç§ä¸»è¦çš„è¯„ä¼°è½¨è¿¹ï¼šä¸€ç§ç”¨äºæ ‡å‡†VQAæ€§èƒ½ï¼Œå¦ä¸€ç§ç”¨äºæµ‹è¯•æ¨¡å‹å¯¹è¿™äº›è§†è§‰æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚é€šè¿‡æä¾›æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸´åºŠç›¸å…³æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒKvasir-VQA-x1æ—¨åœ¨åŠ é€Ÿå¼€å‘æ›´å¯é ã€æ›´æœ‰æ•ˆçš„å¤šæ¨¡å¼AIç³»ç»Ÿï¼Œç”¨äºä¸´åºŠç¯å¢ƒã€‚è¯¥æ•°æ®é›†å®Œå…¨å¯è®¿é—®ï¼Œå¹¶éµå¾ªFAIRæ•°æ®åŸåˆ™ï¼Œä½¿å…¶æˆä¸ºæ›´å¹¿æ³›ç ”ç©¶ç¤¾åŒºå®è´µçš„èµ„æºã€‚ä»£ç å’Œæ•°æ®é›†åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Simula/Kvasir-VQA-x1%E5%92%8Chttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1">https://github.com/Simula/Kvasir-VQA-x1å’Œhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09958v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä»‹ç»äº†ä¸€ä¸ªåä¸ºKvasir-VQA-x1çš„æ–°å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºèƒƒè‚ é“å†…çª¥é•œé¢†åŸŸçš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMedVQAï¼‰ã€‚è¯¥æ•°æ®é›†åœ¨åŸå§‹Kvasir-VQAçš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ˜¾è‘—æ‰©å±•ï¼Œæ–°å¢äº†159,549ä¸ªé—®ç­”å¯¹ï¼Œæ—¨åœ¨æµ‹è¯•æ›´æ·±çš„ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ°ç”Ÿæˆé—®é¢˜ï¼Œå¹¶æŒ‰å¤æ‚æ€§åˆ†å±‚ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ¨¡æ‹ŸçœŸå®ä¸´åºŠåœºæ™¯ï¼Œå¼•å…¥äº†å¤šç§è§†è§‰å¢å¼ºæŠ€æœ¯æ¥æ¨¡æ‹Ÿå¸¸è§çš„æˆåƒä¼ªå½±ã€‚è¯¥æ•°æ®é›†æ”¯æŒä¸¤ç§ä¸»è¦çš„è¯„ä¼°æ¨¡å¼ï¼šæ ‡å‡†VQAæ€§èƒ½è¯„ä¼°å’Œæ¨¡å‹å¯¹è¿™äº›è§†è§‰æ‰°åŠ¨çš„ç¨³å¥æ€§æµ‹è¯•ã€‚Kvasir-VQA-x1æ—¨åœ¨ä¸ºä¸´åºŠç¯å¢ƒä¸­æ›´å¯é å’Œæœ‰æ•ˆçš„å¤šæ¨¡å¼AIç³»ç»Ÿçš„å‘å±•æä¾›åŠ é€Ÿã€‚æ•°æ®é›†å®Œå…¨ç¬¦åˆå…¬å¹³æ•°æ®åŸåˆ™ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„ç ”ç©¶ç¾¤ä½“æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Kvasir-VQA-x1æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸“æ³¨äºèƒƒè‚ é“å†…çª¥é•œé¢†åŸŸçš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMedVQAï¼‰ã€‚</li>
<li>ç›¸æ¯”åŸå§‹Kvasir-VQAï¼Œæ–°æ•°æ®é›†å¢åŠ äº†159,549ä¸ªé—®ç­”å¯¹ï¼ŒåŠ å¼ºäº†å¯¹ä¸´åºŠæ¨ç†èƒ½åŠ›çš„æµ‹è¯•ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿåœ°ç”Ÿæˆé—®é¢˜ï¼Œå¹¶æŒ‰å¤æ‚æ€§è¿›è¡Œåˆ†å±‚ã€‚</li>
<li>å¼•å…¥å¤šç§è§†è§‰å¢å¼ºæŠ€æœ¯ï¼Œæ¨¡æ‹ŸçœŸå®ä¸´åºŠåœºæ™¯ä¸­çš„æˆåƒä¼ªå½±ã€‚</li>
<li>æ•°æ®é›†æ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼šæ ‡å‡†VQAæ€§èƒ½å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>Kvasir-VQA-x1æ—¨åœ¨åŠ é€Ÿä¸´åºŠç¯å¢ƒä¸­å¯é å’Œæœ‰æ•ˆçš„å¤šæ¨¡å¼AIç³»ç»Ÿçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ab607ee5c18365d63a94c2377fe92ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dd6066b6881954d617838efff5e2d6f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Query-Focused-Retrieval-Heads-Improve-Long-Context-Reasoning-and-Re-ranking"><a href="#Query-Focused-Retrieval-Heads-Improve-Long-Context-Reasoning-and-Re-ranking" class="headerlink" title="Query-Focused Retrieval Heads Improve Long-Context Reasoning and   Re-ranking"></a>Query-Focused Retrieval Heads Improve Long-Context Reasoning and   Re-ranking</h2><p><strong>Authors:Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen, Xi Ye</strong></p>
<p>Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs. </p>
<blockquote>
<p>æœ€è¿‘çš„å·¥ä½œå·²ç»ç¡®å®šäº†æ£€ç´¢å¤´ï¼ˆWuç­‰äººï¼Œ2025bï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„æ³¨æ„åŠ›å¤´ï¼Œè´Ÿè´£åœ¨é•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸­æ£€ç´¢æ˜¾è‘—ä¿¡æ¯ï¼Œè¿™æ˜¯é€šè¿‡å®ƒä»¬åœ¨â€œæµ·é‡å¯»é’ˆâ€ä»»åŠ¡ä¸­çš„å¤åˆ¶ç²˜è´´è¡Œä¸ºæ¥è¡¡é‡çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†QRHEADï¼ˆæŸ¥è¯¢èšç„¦æ£€ç´¢å¤´ï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„æ”¹è¿›çš„æ³¨æ„åŠ›å¤´ï¼Œå¯æé«˜ä»é•¿è¯­å¢ƒä¸­çš„æ£€ç´¢èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡èšåˆå…³äºè¾“å…¥æŸ¥è¯¢çš„æ³¨æ„åŠ›åˆ†æ•°æ¥è¯†åˆ«QRHEADï¼Œä½¿ç”¨æ¥è‡ªçœŸå®ä»»åŠ¡çš„ä¸€äº›ä¾‹å­ï¼ˆä¾‹å¦‚ï¼Œé•¿è¯­å¢ƒé—®ç­”ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»äº†QR-RETRIEVERï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”æœ‰æ•ˆçš„æ£€ç´¢å™¨ï¼Œå®ƒä½¿ç”¨QRHEADçš„ç´¯ç§¯æ³¨æ„åŠ›è´¨é‡ä½œä¸ºæ£€ç´¢åˆ†æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨QR-RETRIEVERè¿›è¡Œé•¿è¯­å¢ƒæ¨ç†ï¼Œé€šè¿‡é€‰æ‹©å…·æœ‰æœ€é«˜æ£€ç´¢åˆ†æ•°çš„æœ€ç›¸å…³éƒ¨åˆ†ã€‚åœ¨å¤šè·³æ¨ç†ä»»åŠ¡LongMemEvalå’ŒCLIPPERä¸Šï¼Œè¿™ç›¸å¯¹äºå…¨æ–‡ä¸Šä¸‹æ–‡äº§ç”Ÿäº†è¶…è¿‡10%çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”ä¼˜äºå¼ºå¤§çš„å¯†é›†æ£€ç´¢å™¨ã€‚æˆ‘ä»¬è¿˜å¯¹QRRETRIEVERåœ¨BEIRåŸºå‡†æµ‹è¯•ä¸Šçš„é‡æ–°æ’åºåŠŸèƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å®ƒå…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–åŸºäºLLMçš„é‡æ–°æ’åºå™¨ï¼Œå¦‚RankGPTã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒæŸ¥è¯¢ä¸Šä¸‹æ–‡æ³¨æ„åŠ›è¯„åˆ†å’Œä»»åŠ¡é€‰æ‹©å¯¹äºè¯†åˆ«å…·æœ‰å¼ºå¤§ä¸‹æ¸¸å®ç”¨æ€§çš„QRHEADéƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œè´¡çŒ®äº†ä¸€ä¸ªé€šç”¨æ£€ç´¢å™¨ï¼Œå¹¶ä¸ºLMsçš„é•¿è¯­å¢ƒèƒ½åŠ›æä¾›äº†å¯è§£é‡Šæ€§æ´å¯Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09944v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸå…³äºæ£€ç´¢å¤´çš„ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„å…³æ³¨å¤´é›†åˆQRHEADï¼Œç”¨äºæé«˜ä»é•¿æ–‡æœ¬ä¸­çš„æ£€ç´¢èƒ½åŠ›ã€‚é€šè¿‡é’ˆå¯¹è¾“å…¥æŸ¥è¯¢çš„æ³¨æ„åŠ›å¾—åˆ†è¿›è¡Œèšåˆï¼Œè¯†åˆ«å‡ºQRHEADã€‚å¹¶å¼•å…¥äº†QR-RETRIEVERä½œä¸ºé«˜æ•ˆä¸”æœ‰æ•ˆçš„æ£€ç´¢å™¨ï¼Œä½¿ç”¨QRHEADçš„ç´¯ç§¯æ³¨æ„åŠ›è´¨é‡ä½œä¸ºæ£€ç´¢å¾—åˆ†ã€‚åœ¨å¤šä¸ªé•¿æ–‡æœ¬æ¨ç†ä»»åŠ¡ä¸Šï¼ŒQR-RETRIEVERé€šè¿‡é€‰æ‹©æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œå–å¾—äº†è¶…è¿‡å…¨æ–‡æœ¬æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚åŒæ—¶ï¼Œè¿˜å¯¹å…¶è¿›è¡Œäº†å†æ’åºå™¨çš„è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æœ¬æ–‡çš„è´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç§é€šç”¨çš„æ£€ç´¢å™¨ï¼Œå¹¶ä¸ºç†è§£é•¿æ–‡æœ¬è¯­å¢ƒèƒ½åŠ›æä¾›äº†æ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†QRHEADï¼Œä¸€ç§æ”¹è¿›çš„å…³æ³¨å¤´é›†åˆï¼Œç”¨äºæé«˜ä»é•¿æ–‡æœ¬ä¸­çš„æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡èšåˆæ³¨æ„åŠ›å¾—åˆ†ä¸è¾“å…¥æŸ¥è¯¢çš„å…³ç³»æ¥è¯†åˆ«QRHEADã€‚</li>
<li>QR-RETRIEVERä½œä¸ºé«˜æ•ˆä¸”æœ‰æ•ˆçš„æ£€ç´¢å™¨ï¼Œä½¿ç”¨QRHEADçš„ç´¯ç§¯æ³¨æ„åŠ›è´¨é‡è¿›è¡Œæ£€ç´¢ã€‚</li>
<li>QR-RETRIEVERåœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¨æ–‡æœ¬æ¨¡å‹å’Œå¼ºå¯†é›†æ£€ç´¢å™¨ã€‚</li>
<li>QR-RETRIEVERä½œä¸ºå†æ’åºå™¨åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>æŸ¥è¯¢ä¸Šä¸‹æ–‡å…³æ³¨å¾—åˆ†å’Œä»»åŠ¡é€‰æ‹©å¯¹äºè¯†åˆ«QRHEADè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4d3ad63aa93dcf387a091a74a495ed6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-695d802348fcba3a638b8e3e82827fb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d893760c20c97f6b90638bd11faf8068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6136dd2f73b82307c51320d7d8bf3dec.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CausalVQA-A-Physically-Grounded-Causal-Reasoning-Benchmark-for-Video-Models"><a href="#CausalVQA-A-Physically-Grounded-Causal-Reasoning-Benchmark-for-Video-Models" class="headerlink" title="CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video   Models"></a>CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video   Models</h2><p><strong>Authors:Aaron Foss, Chloe Evans, Sasha Mitts, Koustuv Sinha, Ammar Rizvi, Justine T. Kao</strong></p>
<p>We introduce CausalVQA, a benchmark dataset for video question answering (VQA) composed of question-answer pairs that probe modelsâ€™ understanding of causality in the physical world. Existing VQA benchmarks either tend to focus on surface perceptual understanding of real-world videos, or on narrow physical reasoning questions created using simulation environments. CausalVQA fills an important gap by presenting challenging questions that are grounded in real-world scenarios, while focusing on modelsâ€™ ability to predict the likely outcomes of different actions and events through five question types: counterfactual, hypothetical, anticipation, planning and descriptive. We designed quality control mechanisms that prevent models from exploiting trivial shortcuts, requiring models to base their answers on deep visual understanding instead of linguistic cues. We find that current frontier multimodal models fall substantially below human performance on the benchmark, especially on anticipation and hypothetical questions. This highlights a challenge for current systems to leverage spatial-temporal reasoning, understanding of physical principles, and comprehension of possible alternatives to make accurate predictions in real-world settings. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CausalVQAï¼Œè¿™æ˜¯ä¸€ä¸ªè§†é¢‘é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«æ¢ç©¶æ¨¡å‹å¯¹ç‰©ç†ä¸–ç•Œå› æœå…³ç³»çš„ç†è§£çš„é—®ç­”å¯¹ã€‚ç°æœ‰çš„VQAåŸºå‡†æµ‹è¯•è¦ä¹ˆä¾§é‡äºå¯¹ç°å®è§†é¢‘çš„è¡¨é¢æ„ŸçŸ¥ç†è§£ï¼Œè¦ä¹ˆä¾§é‡äºä½¿ç”¨æ¨¡æ‹Ÿç¯å¢ƒåˆ›å»ºçš„ç‹­çª„çš„ç‰©ç†æ¨ç†é—®é¢˜ã€‚CausalVQAå¡«è¡¥äº†ä¸€ä¸ªé‡è¦ç©ºç™½ï¼Œå®ƒæå‡ºäº†åŸºäºç°å®åœºæ™¯çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼ŒåŒæ—¶ä¾§é‡äºæ¨¡å‹é¢„æµ‹ä¸åŒè¡Œä¸ºå’Œäº‹ä»¶å¯èƒ½ç»“æœçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬äº”ç§é—®é¢˜ç±»å‹ï¼šåäº‹å®ã€å‡è®¾ã€é¢„æœŸã€è§„åˆ’å’Œæè¿°ã€‚æˆ‘ä»¬è®¾è®¡äº†è´¨é‡æ§åˆ¶æœºåˆ¶ï¼Œé˜²æ­¢æ¨¡å‹åˆ©ç”¨å¾®ä¸è¶³é“çš„æ·å¾„ï¼Œè¦æ±‚æ¨¡å‹åœ¨å›ç­”æ—¶åŸºäºæ·±åº¦è§†è§‰ç†è§£è€Œéè¯­è¨€çº¿ç´¢ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤šåª’ä½“æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿œè¿œä½äºäººç±»ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æœŸå’Œå‡è®¾é—®é¢˜ä¸Šã€‚è¿™çªæ˜¾äº†å½“å‰ç³»ç»Ÿåœ¨åˆ©ç”¨æ—¶ç©ºæ¨ç†ã€ç‰©ç†åŸç†çš„ç†è§£å’Œå¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆæ¥åšå‡ºçœŸå®ç¯å¢ƒä¸­çš„å‡†ç¡®é¢„æµ‹æ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09943v1">PDF</a> 35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CausalVQAï¼Œä¸€ä¸ªé’ˆå¯¹è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰çš„åŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¢ç©¶æ¨¡å‹å¯¹ç°å®ä¸–ç•Œå› æœå…³ç³»çš„ç†è§£çš„é—®ç­”å¯¹ã€‚ç°æœ‰VQAåŸºå‡†æµ‹è¯•å€¾å‘äºå…³æ³¨å¯¹ç°å®ä¸–ç•Œçš„è¡¨é¢æ„ŸçŸ¥ç†è§£æˆ–åŸºäºæ¨¡æ‹Ÿç¯å¢ƒçš„ç‰©ç†æ¨ç†é—®é¢˜ã€‚CausalVQAå¡«è¡¥äº†è¿™ä¸€é‡è¦ç©ºç™½ï¼Œé€šè¿‡æå‡ºåŸºäºç°å®åœºæ™¯çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œé‡ç‚¹è€ƒå¯Ÿæ¨¡å‹é¢„æµ‹ä¸åŒè¡Œä¸ºå’Œäº‹ä»¶å¯èƒ½ç»“æœçš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬åäº‹å®ã€å‡è®¾ã€é¢„æœŸã€è§„åˆ’å’Œæè¿°ç­‰äº”ç§ç±»å‹çš„é—®é¢˜ã€‚è®¾è®¡äº†è´¨é‡æ§åˆ¶æœºåˆ¶ï¼Œé˜²æ­¢æ¨¡å‹åˆ©ç”¨æµ…å±‚æ·å¾„ï¼Œè¦æ±‚æ¨¡å‹åŸºäºæ·±åº¦è§†è§‰ç†è§£è€Œéè¯­è¨€çº¿ç´¢æ¥å›ç­”é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰å…ˆè¿›çš„å¤šæ¨¡å¼æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¿œè¿œä½äºäººç±»ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æœŸå’Œå‡è®¾æ€§é—®é¢˜ä¸Šã€‚è¿™çªæ˜¾äº†å½“å‰ç³»ç»Ÿåœ¨åˆ©ç”¨æ—¶ç©ºæ¨ç†ã€ç‰©ç†åŸç†ç†è§£å’Œå¯èƒ½æ›¿ä»£æ–¹æ¡ˆçš„ç†è§£æ¥åšå‡ºç°å®ä¸–ç•Œçš„å‡†ç¡®é¢„æµ‹æ–¹é¢çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CausalVQAæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†é¢‘é—®ç­”çš„åŸºå‡†æ•°æ®é›†ï¼Œå…³æ³¨æ¨¡å‹å¯¹ç°å®ä¸–ç•Œå› æœå…³ç³»çš„ç†è§£ã€‚</li>
<li>åŒ…å«äº”ç§ç±»å‹çš„é—®é¢˜ï¼šåäº‹å®ã€å‡è®¾ã€é¢„æœŸã€è§„åˆ’å’Œæè¿°ã€‚</li>
<li>è®¾è®¡äº†è´¨é‡æ§åˆ¶æœºåˆ¶ï¼Œé˜²æ­¢æ¨¡å‹åˆ©ç”¨æµ…å±‚è§†è§‰ç†è§£æˆ–è¯­è¨€çº¿ç´¢å›ç­”é—®é¢˜ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡å¼æ¨¡å‹åœ¨CausalVQAä¸Šçš„è¡¨ç°ä½äºäººç±»ï¼Œå°¤å…¶åœ¨é¢„æœŸå’Œå‡è®¾æ€§é—®é¢˜ä¸Šã€‚</li>
<li>è¿™çªæ˜¾äº†æ¨¡å‹åœ¨åˆ©ç”¨æ—¶ç©ºæ¨ç†ã€ç‰©ç†åŸç†ç†è§£ä»¥åŠè€ƒè™‘å¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆè¿›è¡Œé¢„æµ‹æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>CausalVQAæ•°æ®é›†æœ‰åŠ©äºæ¨åŠ¨æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæƒ…å¢ƒä¸­çš„è¡¨ç°ï¼Œæé«˜å…¶ç†è§£å’Œé¢„æµ‹å› æœå…³ç³»çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6814892b0006d7e129e1e329dd5b79fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94c672c8114de65df2c3ad3ff0b52421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585e3c53a161c681e25f1ba685eea322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f399df182ce7b5bf4a15d206f68160a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8712bbd48d4c0033157cfa28b450178b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VerIF-Verification-Engineering-for-Reinforcement-Learning-in-Instruction-Following"><a href="#VerIF-Verification-Engineering-for-Reinforcement-Learning-in-Instruction-Following" class="headerlink" title="VerIF: Verification Engineering for Reinforcement Learning in   Instruction Following"></a>VerIF: Verification Engineering for Reinforcement Learning in   Instruction Following</h2><p><strong>Authors:Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at <a target="_blank" rel="noopener" href="https://github.com/THU-KEG/VerIF">https://github.com/THU-KEG/VerIF</a>. </p>
<blockquote>
<p>åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®æŠ€æœ¯ï¼ŒéªŒè¯å·¥ç¨‹åœ¨å…¶ä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ã€‚ç„¶è€Œï¼Œåœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢çš„å¼ºåŒ–å­¦ä¹ æœ€ä½³å®è·µä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æŒ‡ä»¤éµå¾ªä¸­å¼ºåŒ–å­¦ä¹ çš„éªŒè¯æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†VerIFæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§å°†åŸºäºè§„åˆ™çš„ä»£ç éªŒè¯ä¸æ¥è‡ªå¤§å‹æ¨ç†æ¨¡å‹çš„LLMéªŒè¯ç›¸ç»“åˆçš„æ–¹æ³•ï¼ˆä¾‹å¦‚QwQ-32Bï¼‰ã€‚ä¸ºäº†æ”¯æŒè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤éµå¾ªæ•°æ®é›†VerInstructï¼Œå…¶ä¸­åŒ…å«å¤§çº¦22,000ä¸ªå¸¦æœ‰ç›¸å…³éªŒè¯ä¿¡å·çš„å®ä¾‹ã€‚æˆ‘ä»¬å¯¹ä¸¤ä¸ªæ¨¡å‹åº”ç”¨äº†ä½¿ç”¨VerIFçš„RLè®­ç»ƒï¼Œåœ¨å¤šä¸ªä»£è¡¨æ€§çš„æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç»è¿‡è®­ç»ƒçš„æ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­çš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„çº¦æŸä¸Šå…·æœ‰å¾ˆå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œä»–ä»¬çš„é€šç”¨èƒ½åŠ›å¹¶æœªå—åˆ°å½±å“ï¼Œè¿™è¡¨æ˜å¯ä»¥å°†å¸¦æœ‰VerIFçš„RLé›†æˆåˆ°ç°æœ‰çš„RLé…æ–¹ä¸­ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/VerIF%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/THU-KEG/VerIFå‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09942v1">PDF</a> 16 pages, 8 figures</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®æŠ€æœ¯ï¼Œå…¶ä¸­éªŒè¯å·¥ç¨‹å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå…³äºå¼ºåŒ–å­¦ä¹ åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢çš„æœ€ä½³å®è·µä»ç„¶ç¼ºä¹æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨æŒ‡ä»¤éµå¾ªä¸­çš„éªŒè¯æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†VerIFéªŒè¯æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºè§„åˆ™çš„ä»£ç éªŒè¯å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„éªŒè¯ï¼ˆä¾‹å¦‚QwQ-32Bï¼‰ã€‚ä¸ºäº†æ”¯æŒè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤éµå¾ªæ•°æ®é›†VerInstructï¼ŒåŒ…å«å¤§çº¦22,000ä¸ªå®ä¾‹å’Œç›¸å…³çš„éªŒè¯ä¿¡å·ã€‚é€šè¿‡å¯¹ä¸¤ä¸ªæ¨¡å‹åº”ç”¨å¸¦æœ‰VerIFçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªä»£è¡¨æ€§æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è®­ç»ƒåçš„æ¨¡å‹åœ¨ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½åœ¨æœªè§è¿‡çš„çº¦æŸä¸Šå®ç°è‰¯å¥½çš„æ³›åŒ–ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œå®ƒä»¬çš„æ•´ä½“èƒ½åŠ›å¹¶æœªå—åˆ°å½±å“ï¼Œè¿™è¡¨æ˜å¸¦æœ‰VerIFçš„å¼ºåŒ–å­¦ä¹ å¯ä»¥é›†æˆåˆ°ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ é…æ–¹ä¸­ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å·²åœ¨GitHubä¸Šå‘å¸ƒï¼Œä»¥æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/THU-KEG/VerIF">https://github.com/THU-KEG/VerIF</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œå…¶ä¸­éªŒè¯å·¥ç¨‹æ˜¯æ ¸å¿ƒç¯èŠ‚ã€‚</li>
<li>ç›®å‰å…³äºå¼ºåŒ–å­¦ä¹ åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢çš„æœ€ä½³å®è·µä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„éªŒè¯æ–¹æ³•VerIFï¼Œç»“åˆäº†åŸºäºè§„åˆ™çš„ä»£ç éªŒè¯å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„éªŒè¯ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤éµå¾ªæ•°æ®é›†VerInstructï¼ŒåŒ…å«å¤§çº¦22,000ä¸ªå®ä¾‹å’Œç›¸å…³çš„éªŒè¯ä¿¡å·ã€‚</li>
<li>åº”ç”¨VerIFæ–¹æ³•çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåœ¨å¤šä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è®­ç»ƒåçš„æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86711825ae5cf859f523a3c8c38cb8d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6008d6a924159d0953c6cc9a115562d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bef9335318e7f4ce68eadb18157637c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ca7dfbb6bb84e50c57051fb47223613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c90addb9279311733dadc0dfd926d96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af27f454ddf42d7642dc79417a8eada8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-Intention-to-Execution-Probing-the-Generalization-Boundaries-of-Vision-Language-Action-Models"><a href="#From-Intention-to-Execution-Probing-the-Generalization-Boundaries-of-Vision-Language-Action-Models" class="headerlink" title="From Intention to Execution: Probing the Generalization Boundaries of   Vision-Language-Action Models"></a>From Intention to Execution: Probing the Generalization Boundaries of   Vision-Language-Action Models</h2><p><strong>Authors:Irving Fang, Juexiao Zhang, Shengbang Tong, Chen Feng</strong></p>
<p>One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, â€œgeneralistâ€ robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLMâ€™s generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at <a target="_blank" rel="noopener" href="https://ai4ce.github.io/INT-ACT/">https://ai4ce.github.io/INT-ACT/</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ç›¸è¾ƒäºä¼ ç»Ÿæœºå™¨äººæ¨¡ä»¿å­¦ä¹ çš„ä¸€ä¸ªæ‰¿è¯ºæ˜¯ï¼Œå€ŸåŠ©å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ï¼Œç”Ÿæˆé€šç”¨â€œä¸‡èƒ½â€æœºå™¨äººç­–ç•¥ã€‚ç„¶è€Œï¼Œå¯¹VLAçš„å½“å‰è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚ç”±äºç¼ºå°‘è¯­è¨€æŒ‡ä»¤ï¼Œä¼ ç»Ÿçš„æ¨¡ä»¿å­¦ä¹ åŸºå‡†æµ‹è¯•å¹¶ä¸é€‚ç”¨ã€‚æ–°å…´çš„VLAåŸºå‡†æµ‹è¯•è™½ç„¶èå…¥äº†è¯­è¨€ï¼Œä½†è¯„ä¼°ä»»åŠ¡æœ‰é™ï¼Œå¹¶ä¸æ‰“ç®—æ·±å…¥è°ƒæŸ¥VLMé¢„è®­ç»ƒåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæœ‰åŠ©äºä¸‹æ¸¸æœºå™¨äººç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè®¸å¤šç ”ç©¶ä¾èµ–äºä¸åŒæœºæ„å•ç‹¬è®¾è®¡çš„çœŸå®æœºå™¨äººç¯å¢ƒï¼Œè¿™å¢åŠ äº†å¯å¤åˆ¶æ€§å’Œå¯è®¿é—®æ€§çš„éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ…å«10ä¸ªå­ç±»åˆ«å…±50é¡¹æ¨¡æ‹Ÿä»»åŠ¡çš„ç»Ÿä¸€æ¢æŸ¥å¥—ä»¶ï¼Œæ¶µç›–è¯­è¨€æŒ‡ä»¤ã€è§†è§‰å’Œç‰©ä½“ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å‡ ç§æœ€æ–°VLAæ¶æ„çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶VLMçš„éª¨å¹²èµ‹äºˆäº†VLAå¼ºå¤§çš„æ„ŸçŸ¥ç†è§£èƒ½åŠ›å’Œé«˜çº§è§„åˆ’èƒ½åŠ›ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºè‰¯å¥½æ„å›¾ï¼‰ï¼Œä½†è¿™å¹¶ä¸èƒ½å¯é åœ°è½¬åŒ–ä¸ºç²¾ç¡®çš„åŠ¨ä½œæ‰§è¡Œï¼šåœ¨é¢å¯¹è¶…å‡ºåˆ†å¸ƒçš„è§‚å¯Ÿæ—¶ï¼Œç­–ç•¥å¾€å¾€è¡¨ç°å‡ºè¿è´¯çš„æ„å›¾ï¼Œä½†åœ¨åŠ¨ä½œæ‰§è¡Œæ—¶å´å¤±è´¥äº†ã€‚æ­¤å¤–ï¼Œåœ¨åŠ¨ä½œæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šä¾µèš€åŸå§‹VLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒæˆ‘ä»¬çš„ä»»åŠ¡å¥—ä»¶å’Œè¯„ä¼°ä»£ç ï¼Œæ—¨åœ¨ä½œä¸ºæœªæ¥VLAçš„æ ‡å‡†åŸºå‡†ï¼Œå¹¶æ¨åŠ¨ç¼©å°æ„ŸçŸ¥ä¸è¡ŒåŠ¨å·®è·çš„ç ”ç©¶ã€‚æ›´å¤šä¿¡æ¯åŒ…æ‹¬æºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://ai4ce.github.io/INT-ACT/]%E6%89%BE%E5%88%B0%E3%80%82">https://ai4ce.github.io/INT-ACT/]æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09930v1">PDF</a> Under review</p>
<p><strong>Summary</strong>ï¼š<br>VLAæ¨¡å‹åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœºå™¨äººå­¦ä¹ æä¾›äº†é€šç”¨ç­–ç•¥ã€‚ç„¶è€Œï¼Œå½“å‰å¯¹VLAæ¨¡å‹çš„è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œç¼ºä¹è¯­è¨€æŒ‡ä»¤çš„ä¼ ç»Ÿæ¨¡ä»¿å­¦ä¹ åŸºå‡†å’Œæ–°å…´åŒ…å«è¯­è¨€çš„åŸºå‡†éƒ½ä¸è¶³ä»¥å…¨é¢è¯„ä¼°å…¶è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€å¥—ç»Ÿä¸€çš„åŒ…å«è¯­è¨€æŒ‡ä»¤ã€è§†è§‰å’Œç‰©ä½“çš„ä»¿çœŸä»»åŠ¡å¥—ä»¶æ¥è¯„ä¼°å¤šä¸ªæœ€å…ˆè¿›çš„VLAæ¶æ„çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶VLMä¸»å¹²èµ‹äºˆäº†VLAså¼ºå¤§çš„æ„ŸçŸ¥ç†è§£å’Œé«˜çº§è§„åˆ’èƒ½åŠ›ï¼Œä½†é¢ä¸´è¶…å‡ºåˆ†å¸ƒçš„è§‚æµ‹æ—¶ï¼Œç²¾ç¡®åŠ¨ä½œæ‰§è¡Œèƒ½åŠ›ä»ç„¶ä¸è¶³ã€‚æ­¤å¤–ï¼Œå¯¹åŠ¨ä½œæ•°æ®çš„å¾®è°ƒå¯èƒ½ä¼šå‰Šå¼±åŸå§‹VLMçš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å…¬å¼€äº†ä»»åŠ¡å¥—ä»¶å’Œè¯„ä¼°ä»£ç ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•å¹³å°å¹¶åŠªåŠ›å¼¥æ„ŸçŸ¥ä¸åŠ¨ä½œä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¦æƒ…å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://ai4ce.github.io/INT-ACT/">https://ai4ce.github.io/INT-ACT/</a>äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>VLAæ¨¡å‹å…·å¤‡é€šç”¨æ€§ï¼Œå¯è¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›åˆ¶å®šæœºå™¨äººç­–ç•¥ã€‚</li>
<li>å½“å‰å¯¹VLAæ¨¡å‹çš„è¯„ä¼°å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹åŒ…å«è¯­è¨€æŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>æ–°å…´çš„VLAåŸºå‡†æµ‹è¯•å¹³å°åŒ…å«è¯­è¨€æŒ‡ä»¤ã€è§†è§‰å’Œç‰©ä½“å…ƒç´ ï¼Œå…±åŒ…å«äº”åé¡¹ä»¿çœŸä»»åŠ¡ã€‚</li>
<li>VLAæ¨¡å‹åœ¨é¢ä¸´è¶…å‡ºåˆ†å¸ƒçš„è§‚æµ‹æ—¶ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ„å›¾ä½†åŠ¨ä½œæ‰§è¡Œèƒ½åŠ›ä¸è¶³ã€‚</li>
<li>å¯¹åŠ¨ä½œæ•°æ®çš„å¾®è°ƒå¯èƒ½ä¼šå½±å“VLAæ¨¡å‹çš„é€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æˆ‘ä»¬å…¬å¼€çš„ä»»åŠ¡å¥—ä»¶å’Œè¯„ä¼°ä»£ç ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-05a3bde84e1c6e26abb98f1d8c69b1fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba77741d50cf2fe03bbb06550c12f57d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbea99bcb4fe49cdaa60cd7052e578ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d284edc46ef5c5574f9dfc89d474e66a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Only-Style-Stylistic-Consistency-in-Image-Generation-without-Content-Leakage"><a href="#Only-Style-Stylistic-Consistency-in-Image-Generation-without-Content-Leakage" class="headerlink" title="Only-Style: Stylistic Consistency in Image Generation without Content   Leakage"></a>Only-Style: Stylistic Consistency in Image Generation without Content   Leakage</h2><p><strong>Authors:Tilemachos Aravanis, Panagiotis Filntisis, Petros Maragos, George Retsinas</strong></p>
<p>Generating images in a consistent reference visual style remains a challenging computer vision task. State-of-the-art methods aiming for style-consistent generation struggle to effectively separate semantic content from stylistic elements, leading to content leakage from the image provided as a reference to the targets. To address this challenge, we propose Only-Style: a method designed to mitigate content leakage in a semantically coherent manner while preserving stylistic consistency. Only-Style works by localizing content leakage during inference, allowing the adaptive tuning of a parameter that controls the style alignment process, specifically within the image patches containing the subject in the reference image. This adaptive process best balances stylistic consistency with leakage elimination. Moreover, the localization of content leakage can function as a standalone component, given a reference-target image pair, allowing the adaptive tuning of any method-specific parameter that provides control over the impact of the stylistic reference. In addition, we propose a novel evaluation framework to quantify the success of style-consistent generations in avoiding undesired content leakage. Our approach demonstrates a significant improvement over state-of-the-art methods through extensive evaluation across diverse instances, consistently achieving robust stylistic consistency without undesired content leakage. </p>
<blockquote>
<p>åœ¨ä¸€è‡´çš„å‚è€ƒè§†è§‰é£æ ¼ä¸­ç”Ÿæˆå›¾åƒä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚æœ€æ–°æ–¹æ³•æ—¨åœ¨å®ç°é£æ ¼ä¸€è‡´çš„ç”Ÿæˆï¼Œä½†éš¾ä»¥æœ‰æ•ˆåœ°å°†è¯­ä¹‰å†…å®¹ä¸é£æ ¼å…ƒç´ åˆ†å¼€ï¼Œå¯¼è‡´ä»æä¾›çš„å‚è€ƒå›¾åƒåˆ°ç›®æ ‡å›¾åƒçš„å†…å®¹æ³„æ¼ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Only-Styleæ–¹æ³•ï¼šæ—¨åœ¨ä»¥è¯­ä¹‰è¿è´¯çš„æ–¹å¼å‡è½»å†…å®¹æ³„æ¼ï¼ŒåŒæ—¶ä¿ç•™é£æ ¼ä¸€è‡´æ€§ã€‚Only-Styleé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®šä½å†…å®¹æ³„æ¼æ¥å·¥ä½œï¼Œå…è®¸è‡ªé€‚åº”è°ƒæ•´æ§åˆ¶é£æ ¼å¯¹é½è¿‡ç¨‹çš„å‚æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å‚è€ƒå›¾åƒä¸»é¢˜çš„å›¾åƒè¡¥ä¸å†…ã€‚è¿™ç§è‡ªé€‚åº”è¿‡ç¨‹æœ€èƒ½å¹³è¡¡é£æ ¼ä¸€è‡´æ€§ä¸æ¶ˆé™¤æ³„æ¼ã€‚æ­¤å¤–ï¼Œç»™å®šå‚è€ƒ-ç›®æ ‡å›¾åƒå¯¹ï¼Œå†…å®¹æ³„æ¼çš„å®šä½å¯ä»¥ä½œä¸ºç‹¬ç«‹ç»„ä»¶è¿è¡Œï¼Œå…è®¸è°ƒæ•´ä»»ä½•ç‰¹å®šæ–¹æ³•çš„å‚æ•°ï¼Œä»¥æ§åˆ¶é£æ ¼å‚è€ƒçš„å½±å“ã€‚å¦å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è¯„ä»·æ¡†æ¶ï¼Œä»¥é‡åŒ–é£æ ¼ä¸€è‡´ç”Ÿæˆçš„æˆåŠŸç¨‹åº¦ï¼Œé¿å…ä¸å¿…è¦çš„å†…å®¹æ³„æ¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œåœ¨å¤šç§å®ä¾‹ä¸Šå‡æ˜¾ç¤ºå‡ºå¯¹æœ€æ–°æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ï¼ŒæŒç»­å®ç°ç¨³å¥çš„é£æ ¼ä¸€è‡´æ€§ï¼Œæ²¡æœ‰ä¸å¿…è¦çš„å†…å®¹æ³„æ¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09916v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¿æŒä¸€è‡´çš„å‚è€ƒè§†è§‰é£æ ¼ä¸‹ç”Ÿæˆå›¾åƒä»æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹æŒ‘æˆ˜ã€‚å½“å‰å…ˆè¿›çš„æ–¹æ³•åœ¨è¿½æ±‚é£æ ¼ä¸€è‡´æ€§çš„ç”Ÿæˆæ—¶ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°å°†è¯­ä¹‰å†…å®¹ä¸é£æ ¼å…ƒç´ åˆ†ç¦»ï¼Œå¯¼è‡´ä»æä¾›çš„å‚è€ƒå›¾åƒåˆ°ç›®æ ‡å›¾åƒçš„å†…å®¹æ³„éœ²ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Only-Styleæ–¹æ³•ï¼Œæ—¨åœ¨ä»¥è¯­ä¹‰è¿è´¯çš„æ–¹å¼å‡è½»å†…å®¹æ³„éœ²ï¼ŒåŒæ—¶ä¿ç•™é£æ ¼ä¸€è‡´æ€§ã€‚Only-Styleé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®šä½å†…å®¹æ³„éœ²ï¼Œå…è®¸è‡ªé€‚åº”è°ƒæ•´æ§åˆ¶é£æ ¼å¯¹é½è¿‡ç¨‹çš„å‚æ•°ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«å‚è€ƒå›¾åƒä¸»ä½“çš„å›¾åƒè¡¥ä¸å†…ã€‚è¿™ç§è‡ªé€‚åº”è¿‡ç¨‹æœ€ä½³åœ°å¹³è¡¡äº†é£æ ¼ä¸€è‡´æ€§ä¸æ¶ˆé™¤æ³„éœ²ã€‚æ­¤å¤–ï¼Œå†…å®¹æ³„éœ²çš„å®šä½å¯ä»¥ä½œä¸ºç»™å®šå‚è€ƒ-ç›®æ ‡å›¾åƒå¯¹çš„ç‹¬ç«‹ç»„ä»¶è¿è¡Œï¼Œå…è®¸å¯¹å½±å“é£æ ¼å‚è€ƒçš„ä»»ä½•æ–¹æ³•ç‰¹å®šå‚æ•°è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚å¦å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥é‡åŒ–é£æ ¼ä¸€è‡´ç”Ÿæˆçš„æˆåŠŸç¨‹åº¦ï¼Œé¿å…ä¸å¿…è¦çš„å†…å®¹æ³„éœ²ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¹¿æ³›çš„è¯„ä¼°è¯æ˜äº†å¯¹å…ˆè¿›æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨å„ç§ä¸åŒå®ä¾‹ä¸­å§‹ç»ˆå®ç°ç¨³å¥çš„é£æ ¼ä¸€è‡´æ€§ï¼Œè€Œæ²¡æœ‰ä¸å¿…è¦çš„å†…å®¹æ³„éœ²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆä¸å‚è€ƒè§†è§‰é£æ ¼ä¸€è‡´çš„å›¾åƒä»æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•éš¾ä»¥åˆ†ç¦»è¯­ä¹‰å†…å®¹å’Œé£æ ¼å…ƒç´ ï¼Œå¯¼è‡´å†…å®¹æ³„éœ²ã€‚</li>
<li>Only-Styleæ–¹æ³•æ—¨åœ¨å‡è½»å†…å®¹æ³„éœ²ï¼ŒåŒæ—¶ä¿æŒé£æ ¼ä¸€è‡´æ€§ã€‚</li>
<li>Only-Styleé€šè¿‡è‡ªé€‚åº”è°ƒæ•´å‚æ•°æ¥å¹³è¡¡é£æ ¼ä¸€è‡´æ€§ä¸æ¶ˆé™¤æ³„éœ²ã€‚</li>
<li>å†…å®¹æ³„éœ²çš„å®šä½å¯ä»¥ä½œä¸ºç‹¬ç«‹ç»„ä»¶è¿è¡Œï¼Œå…è®¸å¯¹æ–¹æ³•å‚æ•°è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥é‡åŒ–é£æ ¼ä¸€è‡´ç”Ÿæˆçš„æˆåŠŸç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0308c4ed0fb9b5c48044d0b20f7fabd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dc0d0df614845868c73716fef3f90a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e06ab4d9c6ef1318fb8762698ae655f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a3c55fa5a9247107d8a1e8f04ef5fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-036260c55a30968ac154459282e882dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="â€œWhat-are-my-options-â€-Explaining-RL-Agents-with-Diverse-Near-Optimal-Alternatives-Extended"><a href="#â€œWhat-are-my-options-â€-Explaining-RL-Agents-with-Diverse-Near-Optimal-Alternatives-Extended" class="headerlink" title="â€œWhat are my options?â€: Explaining RL Agents with Diverse Near-Optimal   Alternatives (Extended)"></a>â€œWhat are my options?â€: Explaining RL Agents with Diverse Near-Optimal   Alternatives (Extended)</h2><p><strong>Authors:Noel Brindise, Vijeth Hebbar, Riya Shah, Cedric Langbort</strong></p>
<p>In this work, we provide an extended discussion of a new approach to explainable Reinforcement Learning called Diverse Near-Optimal Alternatives (DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable â€œoptionsâ€ for trajectory-planning agents, optimizing policies to produce qualitatively diverse trajectories in Euclidean space. In the spirit of explainability, these distinct policies are used to â€œexplainâ€ an agentâ€™s options in terms of available trajectory shapes from which a human user may choose. In particular, DNA applies to value function-based policies on Markov decision processes where agents are limited to continuous trajectories. Here, we describe DNA, which uses reward shaping in local, modified Q-learning problems to solve for distinct policies with guaranteed epsilon-optimality. We show that it successfully returns qualitatively different policies that constitute meaningfully different â€œoptionsâ€ in simulation, including a brief comparison to related approaches in the stochastic optimization field of Quality Diversity. Beyond the explanatory motivation, this work opens new possibilities for exploration and adaptive planning in RL. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸€ç§åä¸ºDiverse Near-Optimal Alternativesï¼ˆDNAï¼‰çš„å¯è§£é‡Šå¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œè¯¥æ–¹æ³•é¦–æ¬¡åœ¨L4DC 2025ä¸Šæå‡ºã€‚DNAæ—¨åœ¨ä¸ºè½¨è¿¹è§„åˆ’æ™ºèƒ½ä½“æä¾›ä¸€ç»„åˆç†çš„â€œé€‰é¡¹â€ï¼Œä¼˜åŒ–ç­–ç•¥ä»¥åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­äº§ç”Ÿå®šæ€§å¤šæ ·åŒ–çš„è½¨è¿¹ã€‚æœ¬ç€å¯è§£é‡Šæ€§çš„ç²¾ç¥ï¼Œè¿™äº›ä¸åŒçš„ç­–ç•¥è¢«ç”¨æ¥â€œè§£é‡Šâ€æ™ºèƒ½ä½“çš„é€‰é¡¹ï¼Œç”¨å¯ä¾›äººç±»ç”¨æˆ·é€‰æ‹©çš„è½¨è¿¹å½¢çŠ¶æ¥è¡¨ç¤ºã€‚ç‰¹åˆ«æ˜¯ï¼ŒDNAé€‚ç”¨äºåŸºäºå€¼å‡½æ•°çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ç­–ç•¥ï¼Œå…¶ä¸­æ™ºèƒ½ä½“çš„è½¨è¿¹æ˜¯è¿ç»­çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æè¿°äº†DNAå¦‚ä½•åœ¨å±€éƒ¨ä¿®æ”¹åçš„Qå­¦ä¹ é—®é¢˜ä¸­åˆ©ç”¨å¥–åŠ±å¡‘é€ æ¥è§£å†³å…·æœ‰ä¿è¯epsilonæœ€ä¼˜æ€§çš„ä¸åŒç­–ç•¥ã€‚æˆ‘ä»¬è¯æ˜äº†å®ƒèƒ½å¤ŸæˆåŠŸè¿”å›å®šæ€§ä¸åŒçš„ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥åœ¨æ¨¡æ‹Ÿä¸­æ„æˆäº†æœ‰æ„ä¹‰çš„â€œé€‰é¡¹â€ï¼ŒåŒ…æ‹¬å¯¹è´¨é‡å¤šæ ·æ€§é¢†åŸŸä¸­ç›¸å…³æ–¹æ³•çš„ç®€è¦æ¯”è¾ƒã€‚é™¤äº†è§£é‡ŠåŠ¨æœºä¹‹å¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜ä¸ºå¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢å’Œè‡ªé€‚åº”è§„åˆ’å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09901v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†ä¸€ç§æ–°çš„å¯è§£é‡Šçš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼Œç®€ç§°RLï¼‰æ–¹æ³•â€”â€”å¤šæ ·è¿‘ä¼˜é€‰æ‹©ï¼ˆDiverse Near-Optimal Alternativesï¼Œç®€ç§°DNAï¼‰ã€‚DNAæ—¨åœ¨ä¸ºä¸€ç»„è½¨è¿¹è§„åˆ’æ™ºèƒ½ä½“æä¾›åˆç†çš„é€‰æ‹©é›†ï¼Œé€šè¿‡ä¼˜åŒ–ç­–ç•¥ä»¥åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­äº§ç”Ÿå®šæ€§å¤šæ ·åŒ–çš„è½¨è¿¹ã€‚ä¸ºäº†å¢å¼ºè§£é‡Šæ€§ï¼Œè¿™äº›ä¸åŒçš„ç­–ç•¥è¢«ç”¨æ¥è§£é‡Šæ™ºèƒ½ä½“çš„é€‰æ‹©ï¼Œäººç±»ç”¨æˆ·å¯ä»¥æ ¹æ®å¯ç”¨çš„è½¨è¿¹å½¢çŠ¶è¿›è¡Œé€‰æ‹©ã€‚ç‰¹åˆ«æ˜¯DNAé€‚ç”¨äºåŸºäºå€¼å‡½æ•°çš„ç­–ç•¥å¤„ç†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œæ™ºèƒ½ä½“ä»…é™äºè¿ç»­è½¨è¿¹ã€‚æœ¬æ–‡æè¿°äº†DNAé€šè¿‡å±€éƒ¨ä¿®æ”¹Q-learningé—®é¢˜ä¸­çš„å¥–åŠ±å½¢çŠ¶æ¥è§£å†³å…·æœ‰ä¿è¯çš„epsilonæœ€ä¼˜æ€§çš„ä¸åŒç­–ç•¥ã€‚æˆ‘ä»¬åœ¨ä»¿çœŸä¸­å±•ç¤ºäº†DNAæˆåŠŸåœ°è¿”å›äº†å…·æœ‰ä¸åŒæ€§è´¨çš„ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥æ„æˆäº†æœ‰æ„ä¹‰çš„é€‰é¡¹ï¼Œå¹¶ä¸è´¨é‡å¤šæ ·æ€§é¢†åŸŸçš„éšæœºä¼˜åŒ–æ–¹æ³•è¿›è¡Œäº†ç®€è¦æ¯”è¾ƒã€‚é™¤äº†è§£é‡ŠåŠ¨æœºå¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜ä¸ºRLçš„æ¢ç´¢å’Œé€‚åº”æ€§è§„åˆ’å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>DNAæ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ è§£é‡Šæ–¹æ³•ï¼Œæ—¨åœ¨æä¾›æ™ºèƒ½ä½“çš„åˆç†é€‰é¡¹é›†ã€‚</li>
<li>DNAé€šè¿‡ä¼˜åŒ–ç­–ç•¥ä»¥äº§ç”Ÿå¤šæ ·åŒ–è½¨è¿¹ï¼Œæœ‰åŠ©äºå¢å¼ºè§£é‡Šæ€§ã€‚</li>
<li>DNAé€‚ç”¨äºåŸºäºå€¼å‡½æ•°çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œé€‚ç”¨äºè¿ç»­è½¨è¿¹çš„æ™ºèƒ½ä½“ã€‚</li>
<li>DNAé€šè¿‡å±€éƒ¨ä¿®æ”¹Q-learningä¸­çš„å¥–åŠ±å½¢çŠ¶æ¥è§£å†³ä¸åŒç­–ç•¥é—®é¢˜ï¼Œå…·æœ‰ä¿è¯çš„epsilonæœ€ä¼˜æ€§ã€‚</li>
<li>åœ¨ä»¿çœŸä¸­ï¼ŒDNAæˆåŠŸè¿”å›äº†å…·æœ‰ä¸åŒæ€§è´¨çš„ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥ä¸ºæœ‰æ„ä¹‰çš„é€‰é¡¹ï¼Œå¯ç”¨äºäººç±»ç”¨æˆ·çš„é€‰æ‹©ã€‚</li>
<li>DNAä¸ç›¸å…³é¢†åŸŸçš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-247d7314fd55748b654d320260438b92.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CoRT-Code-integrated-Reasoning-within-Thinking"><a href="#CoRT-Code-integrated-Reasoning-within-Thinking" class="headerlink" title="CoRT: Code-integrated Reasoning within Thinking"></a>CoRT: Code-integrated Reasoning within Thinking</h2><p><strong>Authors:Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</strong></p>
<p>Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the modelâ€™s internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4% and 8% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30% fewer tokens for the 32B model and 50% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at <a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT">https://github.com/ChengpengLi1003/CoRT</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚O1å’ŒDeepSeek-R1ï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯åœ¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ–¹é¢ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„æ•°å­¦è¿ç®—æ—¶ä»ç„¶æ•ˆç‡ä½ä¸‹æˆ–ä¸å‡†ç¡®ã€‚é€šè¿‡è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—åº“å’Œç¬¦å·æ±‚è§£å™¨ï¼‰æ¥è§£å†³è¿™äº›é™åˆ¶æ˜¯æœ‰å‰é€”çš„ï¼Œä½†å®ƒå¸¦æ¥äº†ä¸€ä¸ªæŠ€æœ¯æŒ‘æˆ˜ï¼šä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰å¸¦æ¥äº†æ¨¡å‹å†…éƒ¨æ–‡æœ¬è¡¨ç¤ºä¹‹å¤–çš„å¤–éƒ¨çŸ¥è¯†ï¼Œå› æ­¤ç›´æ¥ç»„åˆå¹¶ä¸é«˜æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†CoRTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•™æˆLRMæœ‰æ•ˆä¸”é«˜æ•ˆåœ°ä½¿ç”¨CIçš„åè®­ç»ƒæ¡†æ¶ã€‚ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬é€šè¿‡Hint-Engineeringåˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå®ƒä¼šåœ¨é€‚å½“çš„ä½ç½®æ’å…¥ä¸åŒçš„æç¤ºæ¥ä¼˜åŒ–LRM-CIäº¤äº’ã€‚æˆ‘ä»¬æ‰‹åŠ¨åˆ›å»ºäº†30ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæˆ‘ä»¬åœ¨è¿™äº›æ ·æœ¬ä¸Šå¯¹èŒƒå›´ä»1.5Båˆ°32Bå‚æ•°çš„æ¨¡å‹è¿›è¡Œäº†åè®­ç»ƒï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒã€æ‹’ç»å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒHint-Engineeringæ¨¡å‹åœ¨DeepSeek-R1-Distill-Qwen-32Bå’ŒDeepSeek-R1-Distill-Qwen-1.5Bä¸Šåˆ†åˆ«å®ç°äº†4%å’Œ8%çš„ç»å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œä¸è‡ªç„¶è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒHint-Engineeringæ¨¡å‹ä½¿ç”¨çš„ä»¤ç‰Œå¤§çº¦å‡å°‘äº†30%ï¼ˆé’ˆå¯¹32Bæ¨¡å‹ï¼‰å’Œ50%ï¼ˆé’ˆå¯¹1.5Bæ¨¡å‹ï¼‰ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ChengpengLi1003/CoRTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09820v1">PDF</a> work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢å±•ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶ä»é¢ä¸´æ•ˆç‡ä½ä¸‹æˆ–å‡†ç¡®æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCoRTçš„æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è®©LRMsæœ‰æ•ˆåˆ©ç”¨è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—åº“å’Œç¬¦å·æ±‚è§£å™¨ï¼‰ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨Hint-EngineeringæŠ€æœ¯åˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œä¼˜åŒ–LRMä¸è®¡ç®—å·¥å…·é—´çš„äº¤äº’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHint-Engineeringæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶ä¸”ç›¸è¾ƒäºè‡ªç„¶è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨¡å‹ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡ã€‚æ¨¡å‹å’Œä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—åº“å’Œç¬¦å·æ±‚è§£å™¨ï¼‰çš„å¼•å…¥ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†å¸Œæœ›ï¼Œä½†å®ç°æœ‰æ•ˆç»“åˆé¢ä¸´æŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>CoRTæ¡†æ¶æ—¨åœ¨æ•™æˆLRMsæœ‰æ•ˆåˆ©ç”¨è®¡ç®—å·¥å…·ã€‚</li>
<li>Hint-EngineeringæŠ€æœ¯ç”¨äºåˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œä»¥ä¼˜åŒ–LRMä¸è®¡ç®—å·¥å…·çš„äº¤äº’ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºHint-Engineeringæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ä¸è‡ªç„¶è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒHint-Engineeringæ¨¡å‹æ˜¾è‘—å‡å°‘äº†ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4876080c68457989859d843cbb0e25f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d6190eaa22e4fbe05d7c76da35c849f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f57cf3c8d4c5cfb52f63e0ebb21de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65d38625977d2df8ec8dfb87a1d80972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15301f8b0bda7e88920f302d26a08905.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ComfyUI-R1-Exploring-Reasoning-Models-for-Workflow-Generation"><a href="#ComfyUI-R1-Exploring-Reasoning-Models-for-Workflow-Generation" class="headerlink" title="ComfyUI-R1: Exploring Reasoning Models for Workflow Generation"></a>ComfyUI-R1: Exploring Reasoning Models for Workflow Generation</h2><p><strong>Authors:Zhenran Xu, Yiyu Wang, Xue Yang, Longyue Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang</strong></p>
<p>AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹å·²ç»ä»å•ä¸€æ¨¡å‹è¿›åŒ–åˆ°æ¨¡å—åŒ–å·¥ä½œæµç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ComfyUIç­‰å¹³å°ä¸Šï¼Œèƒ½å¤Ÿå®ç°åˆ›æ„ç®¡é“ä¸­çš„è‡ªå®šä¹‰ã€‚ç„¶è€Œï¼Œè¦æ„å»ºæœ‰æ•ˆçš„å·¥ä½œæµç¨‹ï¼Œéœ€è¦æå¤§çš„ä¸“ä¸šçŸ¥è¯†æ¥åè°ƒä¼—å¤šä¸“ä¸šç»„ä»¶ï¼Œä¸ºç”¨æˆ·å‘ˆç°é™¡å³­çš„å­¦ä¹ æ›²çº¿ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ComfyUI-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆçš„å¤§å‹æ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬ä»ç²¾é€‰çš„4000ä¸ªå·¥ä½œæµç¨‹æ•°æ®é›†å¼€å§‹ï¼Œæ„å»ºé•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ•°æ®ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹é€‰æ‹©ã€å·¥ä½œæµç¨‹è§„åˆ’å’Œä»£ç çº§å·¥ä½œæµç¨‹è¡¨ç¤ºã€‚ComfyUI-R1é€šè¿‡ä¸¤é˜¶æ®µæ¡†æ¶è¿›è¡Œè®­ç»ƒï¼šï¼ˆ1ï¼‰å†·å¯åŠ¨æ—¶çš„CoTå¾®è°ƒï¼Œä½¿æ¨¡å‹é€‚åº”ComfyUIé¢†åŸŸï¼›ï¼ˆ2ï¼‰é€šè¿‡ç²¾ç»†ç²’åº¦çš„è§„åˆ™åº¦é‡æ··åˆå¥–åŠ±æ¿€åŠ±æ¨ç†èƒ½åŠ›ï¼Œç¡®ä¿æ ¼å¼æœ‰æ•ˆæ€§ã€ç»“æ„å®Œæ•´æ€§å’ŒèŠ‚ç‚¹çº§ä¿çœŸåº¦ï¼Œå¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„7Bå‚æ•°æ¨¡å‹è¾¾åˆ°äº†97%çš„æ ¼å¼æœ‰æ•ˆæ€§ç‡ï¼ŒåŒæ—¶å…·æœ‰è¾ƒé«˜çš„é€šè¿‡ç‡ã€èŠ‚ç‚¹çº§å’Œå›¾å½¢çº§F1åˆ†æ•°ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…ˆå‰é‡‡ç”¨é¢†å…ˆé—­æºæ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒClaudeç³»åˆ—ï¼‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æçªå‡ºäº†æ¨ç†è¿‡ç¨‹çš„å…³é”®ä½œç”¨ä»¥åŠå°†å·¥ä½œæµç¨‹è½¬æ¢ä¸ºä»£ç çš„ä¼˜åŠ¿ã€‚å®šæ€§æ¯”è¾ƒæ˜¾ç¤ºæˆ‘ä»¬åœ¨åˆæˆå…·æœ‰å¤šæ ·èŠ‚ç‚¹çš„å¤æ‚å·¥ä½œæµç¨‹æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¼ºè°ƒäº†é•¿é“¾æ€ç»´åœ¨äººå·¥æ™ºèƒ½è‰ºæœ¯åˆ›ä½œä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09790v1">PDF</a> Work in progress. Try it out in ComfyUI-Copilot   <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/ComfyUI-Copilot">https://github.com/AIDC-AI/ComfyUI-Copilot</a></p>
<p><strong>Summary</strong>ï¼šAIç”Ÿæˆå†…å®¹å·²ä»å•ä¸€æ¨¡å‹å‘æ¨¡å—åŒ–å·¥ä½œæµç¨‹å‘å±•ï¼ŒComfyUIç­‰å¹³å°å®ç°äº†åˆ›æ„ç®¡é“å®šåˆ¶ã€‚ä¸ºè§£å†³ç”¨æˆ·é…ç½®æœ‰æ•ˆå·¥ä½œæµç¨‹çš„éš¾é¢˜ï¼Œæ¨å‡ºComfyUI-R1ï¼Œé¦–æ¬¾ç”¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆçš„å¤§å‹æ¨ç†æ¨¡å‹ã€‚é€šè¿‡æ„å»ºåŒ…å«èŠ‚ç‚¹é€‰æ‹©ã€å·¥ä½œæµç¨‹è§„åˆ’åŠä»£ç çº§å·¥ä½œæµè¡¨ç¤ºçš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ•°æ®ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶è®­ç»ƒï¼šé€‚åº”ComfyUIé¢†åŸŸçš„å†·å¯åŠ¨CoTå¾®è°ƒåŠæ¿€åŠ±æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒæ˜¾ç¤ºï¼Œ7Bå‚æ•°çš„æ¨¡å‹è¾¾åˆ°97%çš„æ ¼å¼æœ‰æ•ˆæ€§ç‡ï¼Œå¹¶åœ¨é€šè¿‡ç‡ã€èŠ‚ç‚¹çº§å’Œå›¾å½¢çº§çš„F1åˆ†æ•°ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ä½¿ç”¨GPT-4oå’ŒClaudeç³»åˆ—ç­‰é¢†å…ˆæ¨¡å‹çš„ç°æœ‰æŠ€æœ¯ã€‚åˆ†æçªæ˜¾äº†æ¨ç†æµç¨‹çš„å…³é”®ä½œç”¨å’Œå·¥ä½œæµç¨‹è½¬åŒ–ä¸ºä»£ç çš„ä¼˜åŠ¿ã€‚å®šæ€§æ¯”è¾ƒå±•ç¤ºäº†åœ¨åˆæˆå¤æ‚å·¥ä½œæµç¨‹å’Œå¤šæ ·èŠ‚ç‚¹æ–¹é¢çš„å®åŠ›ï¼Œçªæ˜¾äº†é•¿é“¾æ€ç»´åœ¨AIè‰ºæœ¯åˆ›ä½œä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AIç”Ÿæˆå†…å®¹æ­£æœæ¨¡å—åŒ–å·¥ä½œæµç¨‹å‘å±•ï¼ŒComfyUIç­‰å¹³å°ä¿ƒè¿›åˆ›æ„ç®¡é“çš„å®šåˆ¶ã€‚</li>
<li>å¼•å…¥ComfyUI-R1æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ç”Ÿæˆï¼Œè§£å†³ç”¨æˆ·é…ç½®å·¥ä½œæµçš„æŒ‘æˆ˜ã€‚</li>
<li>ComfyUI-R1æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µæ¡†æ¶è®­ç»ƒï¼ŒåŒ…æ‹¬å†·å¯åŠ¨çš„CoTå¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>æ¨¡å‹è¾¾åˆ°é«˜æ ¼å¼æœ‰æ•ˆæ€§ç‡ï¼Œæ€§èƒ½å“è¶Šï¼Œè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ¨ç†æµç¨‹å¯¹å·¥ä½œæµç¨‹åˆ›å»ºè‡³å…³é‡è¦ï¼Œè½¬åŒ–ä¸ºä»£ç å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹åœ¨åˆæˆå¤æ‚å·¥ä½œæµç¨‹å’Œå¤šæ ·èŠ‚ç‚¹æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„å®åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09790">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-417e138e120d98f221c28c3f2ecc4e61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-312eb0810854cb8199016a6c3c6446f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8ec2e1fc586d3a64953605f346ae35e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vision-Matters-Simple-Visual-Perturbations-Can-Boost-Multimodal-Math-Reasoning"><a href="#Vision-Matters-Simple-Visual-Perturbations-Can-Boost-Multimodal-Math-Reasoning" class="headerlink" title="Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math   Reasoning"></a>Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math   Reasoning</h2><p><strong>Authors:Yuting Li, Lai Wei, Kaipeng Zheng, Jingyuan Huang, Linghe Kong, Lichao Sun, Weiran Huang</strong></p>
<p>Despite the rapid progress of multimodal large language models (MLLMs), they have largely overlooked the importance of visual processing. In a simple yet revealing experiment, we interestingly find that language-only models, when provided with image captions, can achieve comparable or even better performance than MLLMs that consume raw visual inputs. This suggests that current MLLMs may generate accurate visual descriptions but fail to effectively integrate them during reasoning. Motivated by this, we propose a simple visual perturbation framework that enhances perceptual robustness without requiring algorithmic modifications or additional training data. Our approach introduces three targeted perturbations: distractor concatenation, dominance-preserving mixup, and random rotation, that can be easily integrated into existing post-training pipelines including SFT, DPO, and GRPO. Through extensive experiments across multiple datasets, we demonstrate consistent improvements in mathematical reasoning performance, with gains comparable to those achieved through algorithmic changes. Additionally, we achieve competitive performance among open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual perturbation. Through comprehensive ablation studies, we analyze the effectiveness of different perturbation strategies, revealing that each perturbation type contributes uniquely to different aspects of visual reasoning. Our findings highlight the critical role of visual perturbation in multimodal mathematical reasoning: better reasoning begins with better seeing. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YutingLi0606/Vision-Matters">https://github.com/YutingLi0606/Vision-Matters</a>. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿…é€Ÿå–å¾—è¿›å±•ï¼Œä½†å®ƒä»¬å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†è§†è§‰å¤„ç†çš„é‡è¦æ€§ã€‚åœ¨ä¸€é¡¹ç®€å•è€Œå¯Œæœ‰å¯å‘æ€§çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå½“åªæä¾›è¯­è¨€æ¨¡å‹å›¾åƒæ ‡é¢˜æ—¶ï¼Œå…¶æ€§èƒ½å¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¿‡é‚£äº›æ¶ˆè€—åŸå§‹è§†è§‰è¾“å…¥çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚è¿™è¡¨æ˜å½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½å¤Ÿç”Ÿæˆå‡†ç¡®çš„è§†è§‰æè¿°ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­å´æ— æ³•æœ‰æ•ˆåœ°æ•´åˆè¿™äº›æè¿°ã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç®€å•çš„è§†è§‰æ‰°åŠ¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æé«˜æ„ŸçŸ¥ç¨³å¥æ€§æ¥å¢å¼ºæ€§èƒ½ï¼Œè€Œæ— éœ€è¿›è¡Œç®—æ³•ä¿®æ”¹æˆ–å¢åŠ é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ç§é’ˆå¯¹æ€§æ‰°åŠ¨ï¼šå¹²æ‰°ç‰©æ‹¼æ¥ã€ä¿æŒä¸»å¯¼åœ°ä½çš„æ··åˆä»¥åŠéšæœºæ—‹è½¬ï¼Œè¿™äº›æ‰°åŠ¨å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ç°æœ‰çš„è®­ç»ƒåç®¡é“ä¸­ï¼ŒåŒ…æ‹¬SFTã€DPOå’ŒGRPOã€‚é€šè¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨æ•°å­¦æ¨ç†æ€§èƒ½ä¸Šçš„ä¸€è‡´æ”¹è¿›ï¼Œå…¶æ”¶ç›Šä¸é€šè¿‡ç®—æ³•æ›´æ”¹æ‰€å®ç°çš„æ”¶ç›Šç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒQwen2.5-VL-7Bæ¨¡å‹å®ç°äº†å¼€æº7B RLå¾®è°ƒæ¨¡å‹ä¹‹é—´çš„ç«äº‰åŠ›ã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒæ‰°åŠ¨ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜æ¯ç§æ‰°åŠ¨ç±»å‹å¯¹è§†è§‰æ¨ç†çš„ä¸åŒæ–¹é¢éƒ½æœ‰ç‹¬ç‰¹çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†è§†è§‰æ‰°åŠ¨åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ï¼šæ›´å¥½çš„æ¨ç†å§‹äºæ›´å¥½çš„è§†è§‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YutingLi0606/Vision-Matters%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YutingLi0606/Vision-Mattersä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09736v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å¤„ç†ä¸Šçš„ä¸è¶³ã€‚å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨è¯­è¨€æ¨¡å‹çš„å›¾åƒæè¿°è¾“å…¥ï¼Œå…¶æ€§èƒ½å¯ä¸æ¶ˆè€—åŸå§‹è§†è§‰è¾“å…¥çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç›¸åª²ç¾ç”šè‡³æ›´ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªç®€å•çš„è§†è§‰æ‰°åŠ¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯æé«˜æ„ŸçŸ¥ç¨³å¥æ€§ï¼Œä¸”æ— éœ€è¿›è¡Œç®—æ³•ä¿®æ”¹æˆ–å¢åŠ è®­ç»ƒæ•°æ®ã€‚é€šè¿‡å¼•å…¥ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„æ‰°åŠ¨æ–¹æ³•ï¼šå¹²æ‰°ç‰©ä¸²è”ã€ä¿æŒä¸»å¯¼åœ°ä½çš„æ··åˆå’Œéšæœºæ—‹è½¬ï¼Œè¯¥æ¡†æ¶å¯è½»æ¾é›†æˆåˆ°ç°æœ‰è®­ç»ƒåç®¡é“ä¸­ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼Œè¯¥æ¡†æ¶åœ¨æ•°å­¦æ¨ç†æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºæŒç»­çš„æ”¹è¿›ã€‚é€šè¿‡å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œåˆ†æäº†ä¸åŒæ‰°åŠ¨ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºäº†æ¯ç§æ‰°åŠ¨ç±»å‹å¯¹è§†è§‰æ¨ç†ä¸åŒæ–¹é¢çš„ç‹¬ç‰¹è´¡çŒ®ã€‚ç ”ç©¶å‘ç°è§†è§‰æ‰°åŠ¨åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„å…³é”®ä½œç”¨ï¼šæ›´å¥½çš„æ¨ç†å§‹äºæ›´å¥½çš„è§†è§‰èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å¤„ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨æä¾›å›¾åƒæè¿°è¾“å…¥æ—¶ï¼Œæ€§èƒ½å¯ä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¥½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•çš„è§†è§‰æ‰°åŠ¨æ¡†æ¶ï¼Œèƒ½æé«˜æ¨¡å‹çš„æ„ŸçŸ¥ç¨³å¥æ€§ã€‚</li>
<li>è§†è§‰æ‰°åŠ¨æ¡†æ¶é›†æˆäº†ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„æ‰°åŠ¨æ–¹æ³•ï¼Œå¯è½»æ¾é›†æˆåˆ°ç°æœ‰è®­ç»ƒåç®¡é“ä¸­ã€‚</li>
<li>è§†è§‰æ‰°åŠ¨æ¡†æ¶åœ¨æ•°å­¦æ¨ç†æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºæŒç»­çš„æ”¹è¿›ã€‚</li>
<li>æ¶ˆèç ”ç©¶åˆ†æäº†ä¸åŒæ‰°åŠ¨ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæ­ç¤ºäº†å„è‡ªå¯¹è§†è§‰æ¨ç†çš„ç‹¬ç‰¹è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0cd1009df7a6e6abbba383445838eed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e17fdcf12c75662a829ce662c26c3b68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6bb81171bb5822e970ff9e62abca1fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5d80b157ea3542e71a6b06501ef17a4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Reasoning-Models-Are-More-Easily-Gaslighted-Than-You-Think"><a href="#Reasoning-Models-Are-More-Easily-Gaslighted-Than-You-Think" class="headerlink" title="Reasoning Models Are More Easily Gaslighted Than You Think"></a>Reasoning Models Are More Easily Gaslighted Than You Think</h2><p><strong>Authors:Bin Zhu, Hailong Yin, Jingjing Chen, Yu-Gang Jiang</strong></p>
<p>Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand misleading user input remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAIâ€™s o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation prompts, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning modelsâ€™ susceptibility to defend their belief under gaslighting negation prompt. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings reveal fundamental limitations in the robustness of reasoning models, highlighting the gap between step-by-step reasoning and belief persistence. </p>
<blockquote>
<p>è¿‘æœŸä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„æ¨¡å‹çš„æ–°è¿›å±•é€šè¿‡è¯¸å¦‚æ€ç»´é“¾æç¤ºå’Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ç­‰æœºåˆ¶ï¼Œæé«˜äº†ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬æŠµå¾¡è¯¯å¯¼æ€§ç”¨æˆ·è¾“å…¥çš„èƒ½åŠ›ä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸‰ä¸ªæœ€å…ˆè¿›æ¨ç†æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå³OpenAIçš„o4-miniã€Claude-3.7-Sonnetå’ŒGemini-2.5-Flashï¼Œè·¨è¶Šäº†ä¸‰ä¸ªå¤šæ¨¡å¼åŸºå‡†æµ‹è¯•ï¼šMMMUã€MathVistaå’ŒCharXivã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨é­å—â€œç…¤æ°”ç¯æ•ˆåº”â€ï¼ˆGaslightingï¼‰å¦å®šæç¤ºåï¼Œå‡†ç¡®æ€§å‡ºç°æ˜¾è‘—ä¸‹é™ï¼ˆå¹³å‡ä¸‹é™25-29%ï¼‰ï¼Œè¿™è¡¨æ˜å³ä½¿åœ¨æ“çºµæ€§ç”¨æˆ·åé¦ˆä¸‹ï¼Œå³ä½¿æ˜¯é¡¶å°–æ¨ç†æ¨¡å‹ä¹Ÿéš¾ä»¥ä¿æŒæ­£ç¡®ç­”æ¡ˆã€‚åŸºäºè¯„ä¼°çš„è§è§£ï¼Œä¸ºäº†è¿›ä¸€æ­¥æ¢æŸ¥è¿™ä¸€æ¼æ´ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GaslightingBench-Rï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æ¨ç†æ¨¡å‹åœ¨é­å—â€œç…¤æ°”ç¯æ•ˆåº”â€å¦å®šæç¤ºæ—¶èƒ½å¦åšæŒè‡ªèº«è§‚ç‚¹çš„æ–°è¯Šæ–­åŸºå‡†æµ‹è¯•ã€‚GaslightingBench-Ré€šè¿‡ä»ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ç­›é€‰å’Œç­›é€‰1025ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬è€Œæ„å»ºï¼Œå…¶å¼•å‘çš„å¤±è´¥æ›´åŠ å‰§çƒˆï¼Œå¹³å‡å‡†ç¡®æ€§ä¸‹é™è¶…è¿‡53%ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†æ¨ç†æ¨¡å‹ç¨³å¥æ€§çš„æ ¹æœ¬å±€é™æ€§ï¼Œçªå‡ºäº†é€æ­¥æ¨ç†ä¸ä¿¡å¿µæŒä¹…æ€§ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09677v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ¨ç†æ¨¡å‹åœ¨åº”å¯¹è¯¯å¯¼æ€§ç”¨æˆ·è¾“å…¥æ—¶çš„ç¨³å¥æ€§äºŸå¾…ç ”ç©¶ã€‚æœ¬æ–‡å¯¹ä¸‰æ¬¾å…ˆè¿›çš„æ¨ç†æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°å®ƒä»¬åœ¨é¢ä¸´æ°”ç¯å¦å®šæç¤ºæ—¶å­˜åœ¨æ˜æ˜¾çš„å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†GaslightingBench-Rè¯Šæ–­åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä»¥è¿›ä¸€æ­¥æ¢ç´¢è¿™ä¸€è„†å¼±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨é¢å¯¹è¯¯å¯¼æ€§ç”¨æˆ·è¾“å…¥æ—¶çš„ç¨³å¥æ€§æœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
<li>ä¸‰æ¬¾å…ˆè¿›æ¨ç†æ¨¡å‹åœ¨é¢ä¸´æ°”ç¯å¦å®šæç¤ºæ—¶ï¼Œå‡†ç¡®ç‡å¹³å‡ä¸‹é™25-29%ã€‚</li>
<li>GaslightingBench-Ræ˜¯ä¸€ä¸ªæ–°çš„è¯Šæ–­åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°æ¨ç†æ¨¡å‹åœ¨é¢ä¸´æ°”ç¯å¦å®šæç¤ºæ—¶çš„ç¨³å¥æ€§ã€‚</li>
<li>GaslightingBench-Ré€šè¿‡ç­›é€‰å’Œæ•´ç†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„1025ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬æ„å»ºè€Œæˆã€‚</li>
<li>åœ¨GaslightingBench-Ræµ‹è¯•ä¸­ï¼Œæ¨ç†æ¨¡å‹çš„å‡†ç¡®ç‡å¹³å‡ä¸‹é™è¶…è¿‡53%ï¼Œè¡¨æ˜å…¶é¢ä¸´æ›´å¤§çš„å¤±è´¥é£é™©ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹åœ¨é€æ­¥æ¨ç†å’Œä¿¡å¿µæŒä¹…æ€§ä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e649238460c4bcaa79021642824b3a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423e100f3a4862595d2ec49c744ac713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12d65d747dcb5c2790c381eb430ce226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1448d39de0e224dc978c83625a96ee95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-168c854c90a223e352d4da9b7294ea35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a09848553ddecd4b07af69e470ade5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-932980f07286966b13862b4e8707e96e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-Efficient-and-Generalizable-Graph-Retriever-for-Knowledge-Graph-Question-Answering"><a href="#Learning-Efficient-and-Generalizable-Graph-Retriever-for-Knowledge-Graph-Question-Answering" class="headerlink" title="Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph   Question Answering"></a>Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph   Question Answering</h2><p><strong>Authors:Tianjun Yao, Haoxuan Li, Zhiqiang Shen, Pan Li, Tongliang Liu, Kun Zhang</strong></p>
<p>Large Language Models (LLMs) have shown strong inductive reasoning ability across various domains, but their reliability is hindered by the outdated knowledge and hallucinations. Retrieval-Augmented Generation mitigates these issues by grounding LLMs with external knowledge; however, most existing RAG pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledge graphs, which represent facts as relational triples, offer a more structured and compact alternative. Recent studies have explored integrating knowledge graphs with LLMs for knowledge graph question answering (KGQA), with a significant proportion adopting the retrieve-then-reasoning paradigm. In this framework, graph-based retrievers have demonstrated strong empirical performance, yet they still face challenges in generalization ability. In this work, we propose RAPL, a novel framework for efficient and effective graph retrieval in KGQA. RAPL addresses these limitations through three aspects: (1) a two-stage labeling strategy that combines heuristic signals with parametric models to provide causally grounded supervision; (2) a model-agnostic graph transformation approach to capture both intra- and inter-triple interactions, thereby enhancing representational capacity; and (3) a path-based reasoning strategy that facilitates learning from the injected rational knowledge, and supports downstream reasoner through structured inputs. Empirically, RAPL outperforms state-of-the-art methods by $2.66%-20.34%$, and significantly reduces the performance gap between smaller and more powerful LLM-based reasoners, as well as the gap under cross-dataset settings, highlighting its superior retrieval capability and generalizability. Codes are available at: <a target="_blank" rel="noopener" href="https://github.com/tianyao-aka/RAPL">https://github.com/tianyao-aka/RAPL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§é¢†åŸŸè¡¨ç°å‡ºäº†å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶å¯é æ€§å—åˆ°è¿‡æ—¶çŸ¥è¯†å’Œå¹»è§‰çš„é˜»ç¢ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰é€šè¿‡ç”¨å¤–éƒ¨çŸ¥è¯†æ¥æ”¯æ’‘LLMï¼Œç¼“è§£è¿™äº›é—®é¢˜ï¼›ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„RAGç®¡é“ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬ï¼Œè¿™é™åˆ¶äº†å¯è§£é‡Šæ€§å’Œç»“æ„åŒ–æ¨ç†ã€‚çŸ¥è¯†å›¾è°±ä»¥å…³ç³»ä¸‰å…ƒç»„çš„å½¢å¼è¡¨ç¤ºäº‹å®ï¼Œæä¾›äº†æ›´ç»“æ„å’Œæ›´ç´§å‡‘çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å°†çŸ¥è¯†å›¾è°±ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆç”¨äºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ï¼Œå…¶ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†é‡‡ç”¨äº†å…ˆæ£€ç´¢åæ¨ç†çš„æ¨¡å¼ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼ŒåŸºäºå›¾çš„æ£€ç´¢å™¨å·²ç»è¡¨ç°å‡ºäº†å¼ºå¤§çš„ç»éªŒæ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´æ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAPLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºKGQAä¸­è¿›è¡Œé«˜æ•ˆæœ‰æ•ˆå›¾æ£€ç´¢çš„æ–°å‹æ¡†æ¶ã€‚RAPLé€šè¿‡ä¸‰ä¸ªæ–¹é¢è§£å†³è¿™äº›é™åˆ¶ï¼šï¼ˆ1ï¼‰ä¸€ç§ä¸¤é˜¶æ®µæ ‡è®°ç­–ç•¥ï¼Œå®ƒå°†å¯å‘å¼ä¿¡å·ä¸å‚æ•°æ¨¡å‹ç›¸ç»“åˆï¼Œæä¾›å› æœç›‘ç£ï¼›ï¼ˆ2ï¼‰ä¸€ç§æ¨¡å‹é€šç”¨çš„å›¾è½¬æ¢æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•æ‰ä¸‰å…ƒç»„å†…éƒ¨å’Œå¤–éƒ¨çš„äº¤äº’ï¼Œä»è€Œæé«˜è¡¨ç¤ºèƒ½åŠ›ï¼›ï¼ˆ3ï¼‰åŸºäºè·¯å¾„çš„æ¨ç†ç­–ç•¥ï¼Œä¾¿äºä»æ³¨å…¥çš„ç†æ€§çŸ¥è¯†ä¸­å­¦ä¹ ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–è¾“å…¥æ”¯æŒä¸‹æ¸¸æ¨ç†å™¨ã€‚ç»éªŒä¸Šï¼ŒRAPLåœ¨æœ€æ–°æ–¹æ³•çš„åŸºç¡€ä¸Šæå‡äº†æ€§èƒ½æå‡äº†$2.66%\è‡³é«˜è¾¾$$-$åŒæ—¶æ˜¾è‘—å‡å°‘äº†å°å‹ä¸å¤§å‹LLMæ¨ç†å™¨ä¹‹é—´çš„æ€§èƒ½å·®è·ä»¥åŠè·¨æ•°æ®é›†è®¾ç½®ä¸‹çš„æ€§èƒ½å·®è·ï¼Œçªå‡ºäº†å…¶å‡ºè‰²çš„æ£€ç´¢èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/tianyao-aka/RAPL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/tianyao-aka/RAPLè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09645v1">PDF</a> 32 pages, 28 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºè·¨é¢†åŸŸçš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œä½†å—é™äºè¿‡æ—¶çŸ¥è¯†å’Œå¹»è§‰å½±å“å¯é æ€§ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰é€šè¿‡ä¸ºLLMsæä¾›å¤–éƒ¨çŸ¥è¯†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰RAGç®¡é“ä¾èµ–äºéç»“æ„åŒ–æ–‡æœ¬ï¼Œå½±å“äº†è§£é‡Šæ€§å’Œç»“æ„åŒ–æ¨ç†ã€‚çŸ¥è¯†å›¾è°±ä½œä¸ºå…³ç³»ä¸‰å…ƒçš„è¡¨ç¤ºï¼Œæä¾›äº†æ›´ç»“æ„å’Œç´§å‡‘çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ€è¿‘ç ”ç©¶æ¢ç´¢äº†å°†çŸ¥è¯†å›¾è°±ä¸LLMsç»“åˆç”¨äºçŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†é‡‡ç”¨äº†å…ˆæ£€ç´¢åæ¨ç†çš„æ¨¡å¼ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼ŒåŸºäºå›¾çš„æ£€ç´¢å™¨å·²å±•ç°å‡ºå¼ºå¤§çš„å®è¯æ€§èƒ½ï¼Œä½†ä»é¢ä¸´æ³›åŒ–èƒ½åŠ›æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†RAPLï¼Œä¸€ä¸ªç”¨äºKGQAçš„é«˜æ•ˆæœ‰æ•ˆå›¾æ£€ç´¢æ–°æ¡†æ¶ã€‚RAPLé€šè¿‡ä¸‰ä¸ªæ–¹é¢è§£å†³è¿™äº›é™åˆ¶ï¼šç»“åˆå¯å‘å¼ä¿¡å·å’Œå‚æ•°æ¨¡å‹çš„ä¸¤é˜¶æ®µæ ‡ç­¾ç­–ç•¥ï¼Œæä¾›å› æœåŸºç¡€ç›‘ç£ï¼›æ¨¡å‹æ— å…³çš„å›¾è½¬æ¢æ–¹æ³•ï¼Œæ•æ‰ä¸‰å…ƒå†…å’Œä¸‰å…ƒé—´çš„äº’åŠ¨ï¼Œå¢å¼ºè¡¨ç¤ºèƒ½åŠ›ï¼›ä»¥åŠåŸºäºè·¯å¾„çš„æ¨ç†ç­–ç•¥ï¼Œä¿ƒè¿›ä»æ³¨å…¥çš„ç†æ€§çŸ¥è¯†å­¦ä¹ ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–è¾“å…¥æ”¯æŒä¸‹æ¸¸æ¨ç†ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒRAPLä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†å°å‹å’Œå¼ºå¤§LLMåŸºç¡€æ¨ç†å™¨ä¹‹é—´çš„æ€§èƒ½å·®è·ä»¥åŠè·¨æ•°æ®é›†è®¾ç½®çš„å·®è·ï¼Œçªæ˜¾å…¶å“è¶Šçš„æ£€ç´¢èƒ½åŠ›å’Œæ³›åŒ–æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºè·¨é¢†åŸŸæ¨ç†èƒ½åŠ›ï¼Œä½†å—é™äºè¿‡æ—¶çŸ¥è¯†å’Œå¹»è§‰ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰é€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†æé«˜LLMsæ€§èƒ½ã€‚</li>
<li>çŸ¥è¯†å›¾è°±ä¸ºé—®ç­”ä»»åŠ¡æä¾›ç»“æ„åŒ–çŸ¥è¯†ï¼Œæ˜¯LLMsçš„é‡è¦è¡¥å……ã€‚</li>
<li>RAPLæ¡†æ¶é€šè¿‡ç»“åˆå¯å‘å¼ä¿¡å·å’Œå‚æ•°æ¨¡å‹æä¾›ç›‘ç£ï¼Œæé«˜å›¾æ£€ç´¢æ•ˆç‡ã€‚</li>
<li>RAPLé‡‡ç”¨æ¨¡å‹æ— å…³çš„å›¾è½¬æ¢æ–¹æ³•ï¼Œå¢å¼ºè¡¨ç¤ºèƒ½åŠ›å¹¶æ•æ‰ä¸‰å…ƒäº’åŠ¨ã€‚</li>
<li>åŸºäºè·¯å¾„çš„æ¨ç†ç­–ç•¥ä¿ƒè¿›ä»æ³¨å…¥çš„ç†æ€§çŸ¥è¯†å­¦ä¹ ï¼Œæ”¯æŒç»“æ„åŒ–è¾“å…¥å’Œä¸‹æ¸¸æ¨ç†ã€‚</li>
<li>RAPLåœ¨å®è¯ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç¼©å°äº†ä¸åŒè§„æ¨¡LLMçš„æ€§èƒ½å·®è·ï¼Œå¹¶çªæ˜¾å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23aa932a23a087459597f6484ae8c533.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-216ca558ff3100a6e03d521b113e4773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-198e79be3c6eeeffef4fd30b88b1cefa.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AD-2-Bench-A-Hierarchical-CoT-Benchmark-for-MLLM-in-Autonomous-Driving-under-Adverse-Conditions"><a href="#AD-2-Bench-A-Hierarchical-CoT-Benchmark-for-MLLM-in-Autonomous-Driving-under-Adverse-Conditions" class="headerlink" title="AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving   under Adverse Conditions"></a>AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving   under Adverse Conditions</h2><p><strong>Authors:Zhaoyang Wei, Chenhui Qiang, Bowen Jiang, Xumeng Han, Xuehui Yu, Zhenjun Han</strong></p>
<p>Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to enhance the structured, multi-step decision-making capabilities of Multi-Modal Large Models (MLLMs), is particularly crucial for autonomous driving with adverse weather conditions and complex traffic environments. However, existing benchmarks have largely overlooked the need for rigorous evaluation of CoT processes in these specific and challenging scenarios. To address this critical gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically designed for autonomous driving with adverse weather and complex scenes. AD^2-Bench is meticulously constructed to fulfill three key criteria: comprehensive data coverage across diverse adverse environments, fine-grained annotations that support multi-step reasoning, and a dedicated evaluation framework tailored for assessing CoT performance. The core contribution of AD^2-Bench is its extensive collection of over 5.4k high-quality, manually annotated CoT instances. Each intermediate reasoning step in these annotations is treated as an atomic unit with explicit ground truth, enabling unprecedented fine-grained analysis of MLLMsâ€™ inferential processes under text-level, point-level, and region-level visual prompts. Our comprehensive evaluation of state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting the benchmarkâ€™s difficulty and the need to advance robust, interpretable end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized evaluation platform, driving research forward by improving MLLMsâ€™ reasoning in autonomous driving, making it an invaluable resource. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»“æ„åŒ–ã€å¤šæ­¥éª¤å†³ç­–èƒ½åŠ›ï¼Œåœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶å’Œå¤æ‚äº¤é€šç¯å¢ƒçš„è‡ªåŠ¨é©¾é©¶ä¸­å°¤å…¶å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†åœ¨è¿™äº›ç‰¹å®šå’Œæœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ä¸¥æ ¼è¯„ä¼°CoTè¿‡ç¨‹çš„å¿…è¦æ€§ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†AD^2-Benchï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºæ¶åŠ£å¤©æ°”å’Œå¤æ‚åœºæ™¯çš„è‡ªåŠ¨é©¾é©¶è®¾è®¡çš„ç¬¬ä¸€ä¸ªé“¾å¼æ€ç»´åŸºå‡†æµ‹è¯•ã€‚AD^2-Benchç»è¿‡ç²¾å¿ƒæ„å»ºï¼Œä»¥æ»¡è¶³ä¸‰ä¸ªå…³é”®æ ‡å‡†ï¼šæ¶µç›–å„ç§æ¶åŠ£ç¯å¢ƒçš„ç»¼åˆæ•°æ®ã€æ”¯æŒå¤šæ­¥éª¤æ¨ç†çš„ç²¾ç»†æ³¨é‡Šä»¥åŠä¸“ä¸ºè¯„ä¼°CoTæ€§èƒ½é‡èº«å®šåˆ¶çš„è¯„ä¼°æ¡†æ¶ã€‚AD^2-Benchçš„æ ¸å¿ƒè´¡çŒ®æ˜¯å…¶æ”¶é›†çš„è¶…è¿‡5400ä¸ªé«˜è´¨é‡ã€æ‰‹åŠ¨æ³¨é‡Šçš„CoTå®ä¾‹ã€‚è¿™äº›æ³¨é‡Šä¸­çš„æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤éƒ½è¢«è§†ä¸ºå…·æœ‰æ˜ç¡®äº‹å®ä¾æ®çš„åŸå­å•ä½ï¼Œä»è€Œèƒ½å¤Ÿå¯¹MLLMsåœ¨æ–‡æœ¬çº§åˆ«ã€ç‚¹çº§åˆ«å’ŒåŒºåŸŸçº§åˆ«è§†è§‰æç¤ºä¸‹çš„æ¨ç†è¿‡ç¨‹è¿›è¡Œå‰æ‰€æœªæœ‰çš„ç²¾ç»†åˆ†æã€‚æˆ‘ä»¬å¯¹AD^2-Benchä¸Šçš„æœ€æ–°MLLMsçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå‡†ç¡®ç‡ä½äº60%ï¼Œè¿™çªå‡ºäº†åŸºå‡†æµ‹è¯•çš„éš¾åº¦ä»¥åŠå‘å±•ç¨³å¥ã€å¯è§£é‡Šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¿…è¦æ€§ã€‚å› æ­¤ï¼ŒAD^2-Benchæä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°å¹³å°ï¼Œé€šè¿‡æ”¹è¿›MLLMsåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†èƒ½åŠ›æ¥æ¨åŠ¨ç ”ç©¶å‘å‰å‘å±•ï¼Œä½¿å…¶æˆä¸ºå®è´µçš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09557v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æ¶åŠ£å¤©æ°”å’Œå¤æ‚äº¤é€šç¯å¢ƒä¸‹ï¼ŒåŸºäºChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†çš„å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç»“æ„åŒ–ã€å¤šæ­¥éª¤å†³ç­–åˆ¶å®šèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„ä¼˜åŠ¿ï¼Œå¯¹äºè‡ªåŠ¨é©¾é©¶å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¤§å¤šå¿½ç•¥äº†åœ¨è¿™äº›ç‰¹å®šå’ŒæŒ‘æˆ˜æ€§åœºæ™¯ä¸‹å¯¹CoTè¿‡ç¨‹è¿›è¡Œä¸¥æ ¼è¯„ä¼°çš„éœ€æ±‚ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AD^2-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹æ¶åŠ£å¤©æ°”å’Œå¤æ‚åœºæ™¯è‡ªåŠ¨é©¾é©¶çš„Chain-of-ThoughtåŸºå‡†æµ‹è¯•ã€‚AD^2-Benchçš„è®¾è®¡æ»¡è¶³ä¸‰ä¸ªå…³é”®æ ‡å‡†ï¼šè¦†ç›–å„ç§æ¶åŠ£ç¯å¢ƒçš„ç»¼åˆæ•°æ®ã€æ”¯æŒå¤šæ­¥éª¤æ¨ç†çš„ç²¾ç»†æ³¨é‡Šä»¥åŠä¸“é—¨è¯„ä¼°CoTæ€§èƒ½çš„è¯„ä¼°æ¡†æ¶ã€‚æ ¸å¿ƒè´¡çŒ®æ˜¯AD^2-Benchæ‹¥æœ‰è¶…è¿‡5.4kä¸ªé«˜è´¨é‡çš„æ‰‹åŠ¨æ³¨é‡ŠCoTå®ä¾‹ã€‚è¿™äº›æ³¨é‡Šä¸­çš„æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤éƒ½è¢«è§†ä¸ºå…·æœ‰æ˜ç¡®çœŸå®å€¼çš„åŸå­å•ä½ï¼Œå®ç°äº†å¯¹MLLMsåœ¨æ–‡æœ¬çº§ã€ç‚¹çº§å’ŒåŒºåŸŸçº§è§†è§‰æç¤ºä¸‹çš„æ¨ç†è¿‡ç¨‹çš„ç»†è‡´åˆ†æã€‚å¯¹æœ€å…ˆè¿›çš„MLLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨AD^2-Benchä¸Šçš„å‡†ç¡®ç‡ä½äº60%ï¼Œçªæ˜¾äº†åŸºå‡†æµ‹è¯•çš„éš¾åº¦ä»¥åŠå‘å±•ç¨³å¥ã€å¯è§£é‡Šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å¿…è¦æ€§ã€‚AD^2-Benchæä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°å¹³å°ï¼Œæ¨åŠ¨äº†é€šè¿‡æé«˜MLLMsåœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„æ¨ç†èƒ½åŠ›çš„ç ”ç©¶ï¼Œæˆä¸ºå®è´µçš„èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoTæ¨ç†åœ¨æ¶åŠ£å¤©æ°”å’Œå¤æ‚äº¤é€šç¯å¢ƒä¸‹çš„è‡ªåŠ¨é©¾é©¶ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¿½è§†äº†åœ¨ç‰¹å®šæŒ‘æˆ˜åœºæ™¯ä¸‹å¯¹CoTè¿‡ç¨‹çš„ä¸¥æ ¼è¯„ä¼°ã€‚</li>
<li>AD^2-Benchæ˜¯é¦–ä¸ªä¸“é—¨ä¸ºæ¶åŠ£å¤©æ°”å’Œå¤æ‚åœºæ™¯è‡ªåŠ¨é©¾é©¶è®¾è®¡çš„Chain-of-ThoughtåŸºå‡†æµ‹è¯•ã€‚</li>
<li>AD^2-BenchåŒ…å«ç»¼åˆæ•°æ®ã€ç²¾ç»†æ³¨é‡Šå’Œä¸“ç”¨è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ‹¥æœ‰è¶…è¿‡5.4kä¸ªé«˜è´¨é‡æ‰‹åŠ¨æ³¨é‡Šçš„CoTå®ä¾‹ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰MLLMsåœ¨AD^2-Benchä¸Šçš„å‡†ç¡®ç‡ä½äº60%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f3d99f6844d9cd7c37b245c4bc6c5c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e6d05891d41094e9973486e89626d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66571d6d77afeffd7a5dcfa23372432b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff7572c6bede5c021b1f0a8d328f22dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-889496f706fac257f98974988c981d8c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Athena-Enhancing-Multimodal-Reasoning-with-Data-efficient-Process-Reward-Models"><a href="#Athena-Enhancing-Multimodal-Reasoning-with-Data-efficient-Process-Reward-Models" class="headerlink" title="Athena: Enhancing Multimodal Reasoning with Data-efficient Process   Reward Models"></a>Athena: Enhancing Multimodal Reasoning with Data-efficient Process   Reward Models</h2><p><strong>Authors:Shuai Wang, Zhenhua Liu, Jiaheng Wei, Xuanwu Yin, Dong Li, Emad Barsoum</strong></p>
<p>We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Athena-PRMï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è§£å†³å¤æ‚æ¨ç†é—®é¢˜æ¯ä¸€æ­¥çš„å¥–åŠ±åˆ†æ•°ã€‚å¼€å‘é«˜æ€§èƒ½çš„PRMé€šå¸¸éœ€è¦å¤§é‡çš„æ—¶é—´å’Œé‡‘é’±æŠ•å…¥ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºéœ€è¦å¯¹æ¨ç†æ­¥éª¤è¿›è¡Œæ­¥éª¤çº§åˆ«çš„æ³¨é‡Šã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›ä¼°è®¡ï¼Œå¾€å¾€ä¼šäº§ç”Ÿå™ªå£°æ ‡ç­¾å¹¶äº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†æœ‰æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„è¿‡ç¨‹æ ‡æ³¨æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨å¼±å®Œæˆè€…å’Œå¼ºå®Œæˆè€…ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§ä½œä¸ºè¯†åˆ«å¯é è¿‡ç¨‹æ ‡ç­¾çš„å‡†åˆ™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAthena-PRMåœ¨å„ç§åœºæ™¯å’ŒåŸºå‡†æµ‹è¯•ä¸Šä»…ä½¿ç”¨5000ä¸ªæ ·æœ¬å°±è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸¤ç§æé«˜PRMæ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ï¼šORMåˆå§‹åŒ–å’Œè´Ÿæ•°æ®çš„ä¸Šé‡‡æ ·ã€‚æˆ‘ä»¬åœ¨ä¸‰ç§ç‰¹å®šåœºæ™¯ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šæµ‹è¯•æ—¶é—´ç¼©æ”¾éªŒè¯ã€ç›´æ¥è¯„ä¼°æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œä»¥åŠå¥–åŠ±æ’åå¾®è°ƒã€‚æˆ‘ä»¬çš„Athena-PRMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œåœºæ™¯ä¸­å§‹ç»ˆå®ç°å“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä½¿ç”¨Qwen2.5-VL-7Bä½œä¸ºç­–ç•¥æ¨¡å‹æ—¶ï¼ŒAthena-PRMåœ¨WeMathä¸Šæé«˜äº†10.2åˆ†ï¼Œåœ¨MathVistaä¸Šæé«˜äº†7.1åˆ†ï¼Œå®ç°äº†æµ‹è¯•æ—¶é—´ç¼©æ”¾çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒAthena-PRMåœ¨VisualProcessBenchä¸Šè¾¾åˆ°äº†æœ€æ–°çš„ç»“æœï¼Œå¹¶ä»¥å‰æ‰€æœªæœ‰çš„3.9 F1å¾—åˆ†è¶…è¶Šäº†ä¹‹å‰çš„æœ€æ–°ç»“æœï¼Œå±•ç¤ºäº†å…¶å‡†ç¡®è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„ç¨³å¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»¥Athena-PRMä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œå¼€å‘äº†ç»è¿‡å¥–åŠ±æ’åå¾®è°ƒçš„Athena-7Bï¼Œåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¤§å¤§è¶…è¿‡äº†åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09532v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é›…å…¸å¨œå¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆAthena-PRMï¼‰æ˜¯ç”¨äºè¯„ä¼°è§£å†³å¤æ‚æ¨ç†é—®é¢˜æ¯ä¸€æ­¥çš„å¥–åŠ±åˆ†æ•°çš„é«˜æ•ˆå·¥å…·ã€‚å…¶æ˜¾è‘—ç‰¹ç‚¹æ˜¯åªéœ€å°‘é‡çš„æ ·æœ¬ï¼Œå³èƒ½å¤Ÿåœ¨å¤šç§æƒ…å¢ƒå’ŒåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºè‰²çš„æ€§èƒ½ã€‚Athena-PRMåˆ©ç”¨å¼±å®Œæˆè€…å’Œå¼ºå®Œæˆè€…ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§æ¥è¯†åˆ«å¯é çš„è¿‡ç¨‹æ ‡ç­¾ï¼Œæ˜¾è‘—æé«˜äº†è¿‡ç¨‹æ ‡ç­¾æ•°æ®çš„ç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜é‡‡ç”¨äº†ORMåˆå§‹åŒ–å’Œè´Ÿæ•°æ®ä¸Šé‡‡æ ·ä¸¤ç§ç­–ç•¥æ¥æå‡æ€§èƒ½ã€‚åœ¨æµ‹è¯•æ—¶é—´ç¼©æ”¾ã€ç›´æ¥è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§å’Œå¥–åŠ±æ’åå¾®è°ƒç­‰ä¸‰ä¸ªåœºæ™¯ä¸­ï¼ŒAthena-PRMå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°¤å…¶æ˜¯åœ¨ä½¿ç”¨Qwen2.5-VL-7Bä½œä¸ºç­–ç•¥æ¨¡å‹æ—¶ï¼ŒAthena-PRMåœ¨WeMathå’ŒMathVistaä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†10.2ç‚¹å’Œ7.1ç‚¹ã€‚åŒæ—¶ï¼ŒAthena-PRMåœ¨VisualProcessBenchä¸Šè¾¾åˆ°äº†æœ€æ–°çš„ç ”ç©¶ç»“æœï¼Œä»¥è¶…è¶Šå‰çŠ¶æ€æœ€ä¼˜ç»“æœçš„F1å¾—åˆ†æé«˜åˆ°äº†æœ€è¿‘çš„State of the Artçš„æ°´å¹³ä¸Šå±•ç¤ºäº†å…¶åœ¨ç²¾å‡†è¯„ä¼°æ¨ç†æ­¥éª¤æ­£ç¡®æ€§çš„èƒ½åŠ›ä¸Šç¨³å¥ã€‚å¹¶ä¸”ä½œä¸ºå¥–åŠ±æ¨¡å‹å¼€å‘å‡ºæ¥çš„é›…å…¸å¨œé›…å…¸å¨œç¥ç»ç½‘ç»œä¹Ÿåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¤§å¤§ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒAthena-PRMæ˜¯ä¸€ç§é«˜æ•ˆä¸”å¼ºå¤§çš„å·¥å…·ï¼Œä¸ºå¤æ‚æ¨ç†é—®é¢˜çš„å¥–åŠ±è¯„ä¼°æä¾›äº†å…¨æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f30d94d56adab123a485186a2188abeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9434f8ebba98581e0a4a85f52e93d5db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e92ce14f63d221583d71ece318ce0699.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ReasonMed-A-370K-Multi-Agent-Generated-Dataset-for-Advancing-Medical-Reasoning"><a href="#ReasonMed-A-370K-Multi-Agent-Generated-Dataset-for-Advancing-Medical-Reasoning" class="headerlink" title="ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical   Reasoning"></a>ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical   Reasoning</h2><p><strong>Authors:Yu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li, Yu Rong, Wenbing Huang, Qifeng Bai, Tingyang Xu</strong></p>
<p>Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a \textit{multi-agent verification and refinement process}, where we design an \textit{Error Refiner} to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60%. </p>
<blockquote>
<p>è™½ç„¶åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨çŸ¥è¯†å¯†é›†å‹çš„åŒ»ç–—é—®é¢˜å›ç­”æ–¹é¢çš„èƒ½åŠ›ä»è¢«æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ReasonMedï¼Œè¿™æ˜¯æœ€å¤§çš„åŒ»ç–—æ¨ç†æ•°æ®é›†ï¼Œç”±ä»å„ç§LLMç”Ÿæˆçš„170ä¸‡æ¡åˆå§‹æ¨ç†è·¯å¾„ä¸­æç‚¼å‡ºçš„37ä¸‡æ¡é«˜è´¨é‡æ ·æœ¬ç»„æˆã€‚ReasonMedçš„æ„å»ºè¿‡ç¨‹ç»è¿‡å¤šæ™ºèƒ½ä½“éªŒè¯å’Œç»†åŒ–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§Error Refinerï¼Œé€šè¿‡è¯†åˆ«éªŒè¯å™¨æ ‡è®°çš„æ˜“å‡ºé”™æ­¥éª¤æ¥ä¼˜åŒ–æ¨ç†è·¯å¾„å¹¶å¯¹å…¶è¿›è¡Œä¿®æ­£ã€‚åˆ©ç”¨ReasonMedï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†è®­ç»ƒåŒ»ç–—æ¨ç†æ¨¡å‹çš„æœ€ä½³å®è·µï¼Œå¹¶å‘ç°å°†è¯¦ç»†çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸ç®€æ´çš„ç­”æ¡ˆæ‘˜è¦ç›¸ç»“åˆï¼Œæ˜¯æœ€æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚åŸºäºè¿™ä¸€ç­–ç•¥ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ReasonMed-7Bæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ºå°äº10Bçš„æ¨¡å‹è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œæ¯”ä¹‹å‰çš„æœ€ä½³æ€§èƒ½é«˜å‡º4.17%ï¼Œç”šè‡³åœ¨PubMedQAä¸Šè¶…è¿‡äº†LLaMA3.1-70Bæ¨¡å‹ï¼Œé«˜å‡º4.60%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09513v1">PDF</a> 24 pages, 6 figures, 7 tables</p>
<p><strong>Summary</strong>ï¼š<br>åŒ»ç–—é¢†åŸŸçŸ¥è¯†å¯†é›†å‹é—®ç­”å¯¹åŸºäºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºReasonMedæ•°æ®é›†ï¼Œå®ƒæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«ç”±å¤šç§LLMsç”Ÿæˆçš„åˆå§‹æ¨ç†è·¯å¾„ä¸­æç‚¼å‡ºçš„é«˜è´¨é‡æ ·æœ¬ã€‚æ•°æ®é›†ReasonMedçš„æ„å»ºè¿‡ç¨‹ç»è¿‡å¤šæ™ºèƒ½ä½“éªŒè¯å’Œä¿®æ­£ï¼Œé€šè¿‡è®¾è®¡Error Refineræ¥è¯†åˆ«å¹¶ä¿®æ­£éªŒè¯å™¨æ ‡è®°çš„é”™è¯¯æ­¥éª¤ã€‚åˆ©ç”¨ReasonMedæ•°æ®é›†ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†è®­ç»ƒåŒ»ç–—æ¨ç†æ¨¡å‹çš„æœ€ä½³å®è·µï¼Œå‘ç°å°†è¯¦ç»†çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸ç®€æ´çš„ç­”æ¡ˆæ‘˜è¦ç›¸ç»“åˆæ˜¯æœ€æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ã€‚åŸºäºæ­¤ç­–ç•¥è®­ç»ƒçš„ReasonMed-7Bæ¨¡å‹åœ¨åŒ»ç–—é—®ç­”é¢†åŸŸè®¾ç«‹äº†æ–°çš„åŸºå‡†çº¿ï¼Œè¾ƒä¹‹å‰æœ€ä½³æ¨¡å‹æå‡äº†4.17%ï¼Œå¹¶åœ¨PubMedQAä¸Šè¶…è¿‡äº†LLaMA3.1-70Bæ¨¡å‹ï¼Œæå‡äº†4.6%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹åŒ»ç–—é—®ç­”æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ReasonMedæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŒ»ç–—æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«ç”±å¤šç§LLMsç”Ÿæˆçš„æ¨ç†è·¯å¾„æç‚¼å‡ºçš„é«˜è´¨é‡æ ·æœ¬ã€‚</li>
<li>æ•°æ®é›†ReasonMedçš„æ„å»ºè¿‡ç¨‹ç»è¿‡å¤šæ™ºèƒ½ä½“éªŒè¯å’Œä¿®æ­£ï¼Œç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ€ä½³å®è·µæ˜¯å°†è¯¦ç»†çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ä¸ç®€æ´çš„ç­”æ¡ˆæ‘˜è¦ç›¸ç»“åˆæ¥è®­ç»ƒåŒ»ç–—æ¨ç†æ¨¡å‹ã€‚</li>
<li>åŸºäºæ­¤ç­–ç•¥è®­ç»ƒçš„ReasonMed-7Bæ¨¡å‹åœ¨åŒ»ç–—é—®ç­”é¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œè¾ƒä¹‹å‰æ¨¡å‹æœ‰æ‰€æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b0aec02e882cf820ce23e33852620c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b20991e3bbf0911822ae4daef4d4c1c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fc4e085a65eb353acb0d0091f9349d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39b8ba42df02e579db98f763c94dd1f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5c3a51a46d2cc8e9b8ec42df834c960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-243b8247193ffcd5ba409b46bbad94e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cb4f3536fafebd0812061000bf671bd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Give-Me-FP32-or-Give-Me-Death-Challenges-and-Solutions-for-Reproducible-Reasoning"><a href="#Give-Me-FP32-or-Give-Me-Death-Challenges-and-Solutions-for-Reproducible-Reasoning" class="headerlink" title="Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible   Reasoning"></a>Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible   Reasoning</h2><p><strong>Authors:Jiayi Yuan, Hao Li, Xinheng Ding, Wenya Xie, Yu-Jhe Li, Wentian Zhao, Kun Wan, Jing Shi, Xia Hu, Zirui Liu</strong></p>
<p>Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision â€“ while critical for reproducibility â€“ is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/nanomaoli/llm_reproducibility">https://github.com/nanomaoli/llm_reproducibility</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç°å·²å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œå¹¶å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶è¿›å±•çš„å‰ææ˜¯åŸºå‡†æµ‹è¯•åˆ†æ•°æ—¢å‡†ç¡®åˆå¯å¤åˆ¶ã€‚æˆ‘ä»¬è¯æ˜LLMæ€§èƒ½çš„å¤ç°æ€§æ˜¯è„†å¼±çš„ï¼šæ”¹å˜ç³»ç»Ÿé…ç½®ï¼Œå¦‚è¯„ä¼°æ‰¹æ¬¡å¤§å°ã€GPUæ•°é‡å’Œç‰ˆæœ¬ï¼Œä¼šç»™ç”Ÿæˆçš„å“åº”å¸¦æ¥æ˜¾è‘—å·®å¼‚ã€‚è¿™ä¸ªé—®é¢˜åœ¨æ¨ç†æ¨¡å‹ä¸­å°¤å…¶çªå‡ºï¼Œå…¶ä¸­æ—©æœŸæ ‡è®°çš„è½»å¾®èˆå…¥å·®å¼‚å¯èƒ½ä¼šçº§è”æˆä¸åŒçš„æ€ç»´é“¾ï¼Œæœ€ç»ˆå½±å“å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨bfloat16ç²¾åº¦å’Œè´ªå¿ƒè§£ç ä¸‹ï¼ŒDeepSeek-R1-Distill-Qwen-7Bç­‰æ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§å¯èƒ½å­˜åœ¨é«˜è¾¾9%çš„å˜åŒ–ï¼Œå“åº”é•¿åº¦å¯èƒ½å­˜åœ¨9000ä¸ªæ ‡è®°çš„å·®å¼‚ï¼Œè¿™æ˜¯ç”±äºGPUæ•°é‡ã€ç±»å‹å’Œè¯„ä¼°æ‰¹æ¬¡å¤§å°çš„ä¸åŒæ‰€å¯¼è‡´çš„ã€‚æˆ‘ä»¬å°†è¿™ç§å¯å˜æ€§çš„æ ¹æœ¬åŸå› å½’ç»“ä¸ºæœ‰é™æ•°å€¼ç²¾åº¦ä¸‹æµ®ç‚¹è¿ç®—çš„éç»“åˆæ€§ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡ç³»ç»Ÿç ”ç©¶äº†æ•°å€¼ç²¾åº¦å¦‚ä½•å½±å“LLMæ¨æ–­çš„å¤ç°æ€§ã€‚é€šè¿‡è·¨å„ç§ç¡¬ä»¶ã€è½¯ä»¶å’Œç²¾åº¦è®¾ç½®çš„ä¸¥æ ¼æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬é‡åŒ–äº†æ¨¡å‹è¾“å‡ºä½•æ—¶ä»¥åŠå¦‚ä½•å‘æ•£ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶æµ®ç‚¹ç²¾åº¦å¯¹äºå¤ç°æ€§è‡³å…³é‡è¦ï¼Œä½†åœ¨è¯„ä¼°å®è·µä¸­å¾€å¾€è¢«å¿½è§†ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè½»é‡çº§çš„æ¨ç†ç®¡é“ï¼Œåä¸ºLayerCastï¼Œå®ƒä»¥16ä½ç²¾åº¦å­˜å‚¨æƒé‡ï¼Œä½†æ‰€æœ‰è®¡ç®—éƒ½åœ¨FP32ä¸­è¿›è¡Œï¼Œå¹³è¡¡äº†å†…å­˜æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nanomaoli/llm_reproducibility%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nanomaoli/llm_reproducibilityæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09501v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§é¢†åŸŸä¸­çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ï¼Œå…¶è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œæ¨¡å‹æ€§èƒ½çš„å¯é‡å¤æ€§å´ååˆ†è„†å¼±ã€‚ç³»ç»Ÿé…ç½®çš„å˜åŒ–ï¼Œå¦‚è¯„ä¼°æ‰¹æ¬¡å¤§å°ã€GPUæ•°é‡å’Œç‰ˆæœ¬ï¼Œéƒ½å¯èƒ½æ˜¾è‘—å½±å“LLMç”Ÿæˆçš„å“åº”ã€‚è¿™ä¸€é—®é¢˜åœ¨æ¨ç†æ¨¡å‹ä¸­å°¤ä¸ºçªå‡ºï¼Œæ—©æœŸæ ‡è®°çš„å¾®å°èˆå…¥å·®å¼‚å¯èƒ½ä¼šå¯¼è‡´æ€ç»´é“¾çš„å‘æ•£ï¼Œæœ€ç»ˆå½±å“å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨bfloat16ç²¾åº¦å’Œè´ªå©ªè§£ç ä¸‹ï¼ŒDeepSeek-R1-Distill-Qwen-7Bç­‰æ¨ç†æ¨¡å‹åœ¨GPUæ•°é‡ã€ç±»å‹å’Œè¯„ä¼°æ‰¹æ¬¡å¤§å°æ–¹é¢çš„å·®å¼‚å¯èƒ½ä¼šå¯¼è‡´å‡†ç¡®ç‡é«˜è¾¾9%çš„å˜åŒ–ï¼Œä»¥åŠé•¿è¾¾9000ä¸ªæ ‡è®°çš„å“åº”é•¿åº¦å·®å¼‚ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°æ¢è®¨äº†æ•°å€¼ç²¾åº¦å¦‚ä½•å½±å“LLMæ¨æ–­çš„å¯é‡å¤æ€§ã€‚é€šè¿‡è·¨å„ç§ç¡¬ä»¶ã€è½¯ä»¶å’Œç²¾åº¦è®¾ç½®çš„ä¸¥æ ¼æ§åˆ¶å®éªŒï¼Œæˆ‘ä»¬é‡åŒ–äº†æ¨¡å‹è¾“å‡ºä½•æ—¶ä»¥åŠå¦‚ä½•å‘ç”Ÿåˆ†æ­§ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè™½ç„¶æµ®ç‚¹ç²¾åº¦å¯¹äºå¯é‡å¤æ€§è‡³å…³é‡è¦ï¼Œä½†åœ¨è¯„ä¼°å®è·µä¸­å¾€å¾€è¢«å¿½è§†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è½»é‡çº§çš„æ¨æ–­ç®¡é“LayerCastï¼Œå®ƒä»¥16ä½ç²¾åº¦å­˜å‚¨æƒé‡ï¼Œä½†åœ¨FP32ä¸­è¿›è¡Œæ‰€æœ‰è®¡ç®—ï¼Œå¹³è¡¡äº†å†…å­˜æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½å¯é‡å¤æ€§å—åˆ°ç³»ç»Ÿé…ç½®å˜åŒ–çš„å½±å“ï¼ŒåŒ…æ‹¬è¯„ä¼°æ‰¹æ¬¡å¤§å°ã€GPUè®¡æ•°å’Œç‰ˆæœ¬ã€‚</li>
<li>æ¨ç†æ¨¡å‹çš„æ€§èƒ½å—æ—©æœŸæ ‡è®°èˆå…¥å·®å¼‚çš„å½±å“è¾ƒå¤§ï¼Œå¯èƒ½å¯¼è‡´æ€ç»´é“¾çš„å‘æ•£ã€‚</li>
<li>åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ŒLLMçš„æ€§èƒ½å·®å¼‚å¯èƒ½è¡¨ç°åœ¨å‡†ç¡®ç‡å’Œå“åº”é•¿åº¦ä¸Šï¼Œå¦‚DeepSeek-R1-Distill-Qwen-7Bæ¨¡å‹åœ¨bfloat16ç²¾åº¦å’Œè´ªå©ªè§£ç ä¸‹è¡¨ç°å‡ºé«˜è¾¾9%çš„å‡†ç¡®ç‡å˜åŒ–å’Œé•¿è¾¾9000ä¸ªæ ‡è®°çš„å“åº”é•¿åº¦å˜åŒ–ã€‚</li>
<li>æµ®ç‚¹ç²¾åº¦å¯¹äºLLMæ¨æ–­çš„å¯é‡å¤æ€§è‡³å…³é‡è¦ï¼Œä½†åœ¨è¯„ä¼°å®è·µä¸­å¾€å¾€è¢«å¿½è§†ã€‚</li>
<li>æ–‡ä¸­é¦–æ¬¡ç³»ç»Ÿåœ°æ¢è®¨äº†æ•°å€¼ç²¾åº¦å¯¹LLMæ¨æ–­å¯é‡å¤æ€§çš„å½±å“ï¼Œå¹¶é€šè¿‡å¯¹å„ç§ç¡¬ä»¶ã€è½¯ä»¶å’Œç²¾åº¦è®¾ç½®çš„å®éªŒè¿›è¡Œäº†é‡åŒ–åˆ†æã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¹³è¡¡å†…å­˜æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§çš„è½»é‡çº§æ¨æ–­ç®¡é“LayerCastï¼Œé‡‡ç”¨16ä½ç²¾åº¦å­˜å‚¨æƒé‡å¹¶åœ¨FP32ä¸­è¿›è¡Œè®¡ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-753f4027855faed1e24af532712f0f2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4e6b9a8e04a821d54c9a790c9b0a9bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-996ce1d603c86769a1fcefe7a6080208.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e0c44f678f286db7ad7660be946ac2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b01b514e3787e408cd9b4695c05748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb201337d1a95f2575f2792573185fec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb0c3f0ce7842893e0542503c8c4e1b9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LPO-Towards-Accurate-GUI-Agent-Interaction-via-Location-Preference-Optimization"><a href="#LPO-Towards-Accurate-GUI-Agent-Interaction-via-Location-Preference-Optimization" class="headerlink" title="LPO: Towards Accurate GUI Agent Interaction via Location Preference   Optimization"></a>LPO: Towards Accurate GUI Agent Interaction via Location Preference   Optimization</h2><p><strong>Authors:Jiaqi Tang, Yu Xia, Yi-Feng Wu, Yuwei Hu, Yuhui Chen, Qing-Guo Chen, Xiaogang Xu, Xiangyu Wu, Hao Lu, Yanqing Ma, Shiyin Lu, Qifeng Chen</strong></p>
<p>The advent of autonomous agents is transforming interactions with Graphical User Interfaces (GUIs) by employing natural language as a powerful intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods in current GUI agents for achieving spatial localization, these methods face substantial challenges due to their limited capacity to accurately perceive positional data. Existing strategies, such as reinforcement learning, often fail to assess positional accuracy effectively, thereby restricting their utility. In response, we introduce Location Preference Optimization (LPO), a novel approach that leverages locational data to optimize interaction preferences. LPO uses information entropy to predict interaction positions by focusing on zones rich in information. Besides, it further introduces a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Supported by Group Relative Preference Optimization (GRPO), LPO facilitates an extensive exploration of GUI environments and significantly enhances interaction precision. Comprehensive experiments demonstrate LPOâ€™s superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations. Our code will be made publicly available soon, at <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/LPO">https://github.com/AIDC-AI/LPO</a>. </p>
<blockquote>
<p>è‡ªä¸»ä»£ç†çš„å‡ºç°é€šè¿‡è¿ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºå¼ºå¤§çš„ä¸­ä»‹ï¼Œæ­£åœ¨æ”¹å˜ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„äº¤äº’æ–¹å¼ã€‚å°½ç®¡å½“å‰GUIä»£ç†åœ¨å®ç°ç©ºé—´å®šä½æ—¶ä»¥ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ä¸ºä¸»ï¼Œä½†è¿™äº›æ–¹æ³•é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å‡†ç¡®æ„ŸçŸ¥ä½ç½®æ•°æ®çš„èƒ½åŠ›æœ‰é™ã€‚ç°æœ‰ç­–ç•¥å¦‚å¼ºåŒ–å­¦ä¹ å¾€å¾€æ— æ³•æœ‰æ•ˆåœ°è¯„ä¼°ä½ç½®å‡†ç¡®æ€§ï¼Œä»è€Œé™åˆ¶äº†å…¶æ•ˆç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½ç½®åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä½ç½®æ•°æ®æ¥ä¼˜åŒ–äº¤äº’åå¥½ã€‚LPOé€šè¿‡å…³æ³¨ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œåˆ©ç”¨ä¿¡æ¯ç†µæ¥é¢„æµ‹äº¤äº’ä½ç½®ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŸºäºç‰©ç†è·ç¦»å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€ä½ç½®å¥–åŠ±å‡½æ•°ï¼Œåæ˜ äº†äº¤äº’ä½ç½®çš„ä¸åŒé‡è¦æ€§ã€‚å¾—ç›Šäºç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ”¯æŒï¼ŒLPOä¿ƒè¿›äº†GUIç¯å¢ƒçš„å¹¿æ³›æ¢ç´¢ï¼Œå¹¶å¤§å¤§æé«˜äº†äº¤äº’ç²¾åº¦ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLPOæ€§èƒ½å“è¶Šï¼Œåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œåœ¨çº¿è¯„ä¼°ä¸­éƒ½å–å¾—äº†æœ€æ–°æˆæœã€‚æˆ‘ä»¬çš„ä»£ç å¾ˆå¿«å°±ä¼šå…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/LPO%E4%B8%8A%E6%8F%9B%E4%BB%A5%E5%BC%BA%E5%A4%AF%E4%BA%BA%E5%B7%A5%E7%9A%84%E5%AE%9E%E7%8E%B0">https://github.com/AIDC-AI/LPOä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09373v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‡ªä¸»ä»£ç†çš„å‡ºç°æ­£åœ¨é€šè¿‡åˆ©ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºå¼ºå¤§çš„ä¸­ä»‹æ¥å˜é©ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çš„äº¤äº’æ–¹å¼ã€‚å½“å‰GUIä»£ç†ä¸»è¦ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å®ç°ç©ºé—´å®šä½ï¼Œä½†é¢ä¸´å‡†ç¡®æ„ŸçŸ¥å®šä½æ•°æ®çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½ç½®åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä½ç½®æ•°æ®ä¼˜åŒ–äº¤äº’åå¥½ã€‚LPOé€šè¿‡ä¿¡æ¯ç†µé¢„æµ‹äº¤äº’ä½ç½®ï¼Œé‡ç‚¹å…³æ³¨ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œå¹¶åŸºäºç‰©ç†è·ç¦»å¼•å…¥åŠ¨æ€ä½ç½®å¥–åŠ±å‡½æ•°ï¼Œåæ˜ äº¤äº’ä½ç½®çš„ä¸åŒé‡è¦æ€§ã€‚LPOç»“åˆç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¿ƒè¿›äº†GUIç¯å¢ƒçš„å¹¿æ³›æ¢ç´¢ï¼Œå¹¶æ˜¾è‘—æé«˜äº¤äº’ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒLPOåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œåœ¨çº¿è¯„ä¼°ä¸­éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å°†å¾ˆå¿«åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/LPO%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIDC-AI/LPOå…¬å¼€æä¾›ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»ä»£ç†é€šè¿‡è‡ªç„¶è¯­è¨€ä¸­ä»‹æ”¹å˜ä¸GUIçš„äº¤äº’æ–¹å¼ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨GUIä»£ç†çš„ç©ºé—´å®šä½ä¸Šè™½æ™®éåº”ç”¨ï¼Œä½†å­˜åœ¨å‡†ç¡®æ„ŸçŸ¥å®šä½æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>ä½ç½®åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰æ–¹æ³•åˆ©ç”¨ä½ç½®æ•°æ®ä¼˜åŒ–äº¤äº’åå¥½ï¼Œé¢„æµ‹äº¤äº’ä½ç½®å¹¶å…³æ³¨ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€‚</li>
<li>LPOå¼•å…¥åŠ¨æ€ä½ç½®å¥–åŠ±å‡½æ•°ï¼Œåæ˜ äº¤äº’ä½ç½®çš„ä¸åŒé‡è¦æ€§ã€‚</li>
<li>LPOç»“åˆç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä¿ƒè¿›GUIç¯å¢ƒçš„å¹¿æ³›æ¢ç´¢ã€‚</li>
<li>LPOæ˜¾è‘—æé«˜äº¤äº’ç²¾åº¦ï¼Œåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œåœ¨çº¿è¯„ä¼°ä¸­è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16c66ecf37df6ff1c4a0c1ed7044158e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81f6d466210a8f2a3f15eba28dbd2d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2339f60031315509f905003087eeda9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RePO-Replay-Enhanced-Policy-Optimization"><a href="#RePO-Replay-Enhanced-Policy-Optimization" class="headerlink" title="RePO: Replay-Enhanced Policy Optimization"></a>RePO: Replay-Enhanced Policy Optimization</h2><p><strong>Authors:Siheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, Chaochao Lu</strong></p>
<p>Reinforcement learning (RL) is vital for optimizing large language models (LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages using multiple on-policy outputs per prompt, leading to high computational costs and low data efficiency. To address this, we introduce Replay-Enhanced Policy Optimization (RePO), which leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, allowing policy optimization based on a broader and more diverse set of samples for each prompt. Experiments on five LLMs across seven mathematical reasoning benchmarks demonstrate that RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further analysis indicates that RePO increases computational cost by $15%$ while raising the number of effective optimization steps by $48%$ for Qwen3-1.7B, with both on-policy and off-policy sample numbers set to $8$. The repository can be accessed at <a target="_blank" rel="noopener" href="https://github.com/SihengLi99/RePO">https://github.com/SihengLi99/RePO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹äºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡åœ¨æ¯ä¸ªæç¤ºä¸‹ä½¿ç”¨å¤šä¸ªç­–ç•¥è¾“å‡ºä¼°ç®—ä¼˜åŠ¿ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜å’Œæ•°æ®æ•ˆç‡ä½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›æ”¾å¢å¼ºç­–ç•¥ä¼˜åŒ–ï¼ˆRePOï¼‰ï¼Œå®ƒåˆ©ç”¨å„ç§å›æ”¾ç­–ç•¥ä»å›æ”¾ç¼“å†²åŒºä¸­æ£€ç´¢éç­–ç•¥æ ·æœ¬ï¼Œå…è®¸åŸºäºæ›´å¹¿æ³›å’Œæ›´å¤šæ ·åŒ–çš„æ ·æœ¬é›†å¯¹æ¯ä¸ªæç¤ºè¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚åœ¨äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸GRPOç›¸æ¯”ï¼ŒRePOåœ¨Qwen2.5-Math-1.5Bå’ŒQwen3-1.7Bä¸Šåˆ†åˆ«å®ç°äº†ç»å¯¹å¹³å‡æ€§èƒ½æå‡18.4ç‚¹å’Œ4.1ç‚¹ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒRePOçš„è®¡ç®—æˆæœ¬æ¯”GRPOé«˜å‡º15%ï¼Œä½†ä¸ºQwen3-1.7Bçš„æœ‰æ•ˆä¼˜åŒ–æ­¥éª¤æ•°é‡å¢åŠ äº†48%ï¼ŒåŒæ—¶å°†ç­–ç•¥å†…æ ·æœ¬å’Œç­–ç•¥å¤–æ ·æœ¬æ•°é‡éƒ½è®¾ç½®ä¸º8ã€‚è¯¥ä»“åº“å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/SihengLi99/RePO%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/SihengLi99/RePOè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09340v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/SihengLi99/RePO">https://github.com/SihengLi99/RePO</a></p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ å¯¹äºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰é«˜è®¡ç®—æˆæœ¬åŠä½æ•°æ®æ•ˆç‡çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Replay-Enhanced Policy Optimizationï¼ˆRePOï¼‰ã€‚å®ƒé€šè¿‡åˆ©ç”¨ä¸åŒçš„å›æ”¾ç­–ç•¥ï¼Œä»å›æ”¾ç¼“å†²åŒºä¸­æ£€ç´¢éç­–ç•¥æ ·æœ¬ï¼ŒåŸºäºæ›´å¹¿æ³›ã€æ›´å¤šæ ·çš„æ ·æœ¬é›†è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºGRPOï¼ŒRePOåœ¨ä¸ƒä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå–å¾—äº†å¹³å‡ç»å¯¹æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>Group Relative Policy Optimization (GRPO)å­˜åœ¨é«˜è®¡ç®—æˆæœ¬åŠä½æ•°æ®æ•ˆç‡çš„é—®é¢˜ã€‚</li>
<li>Replay-Enhanced Policy Optimization (RePO)åˆ©ç”¨ä¸åŒçš„å›æ”¾ç­–ç•¥æ¥ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>RePOä»å›æ”¾ç¼“å†²åŒºä¸­æ£€ç´¢éç­–ç•¥æ ·æœ¬ï¼Œæ‰©å¤§äº†æ ·æœ¬çš„å¤šæ ·æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºGRPOï¼ŒRePOåœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„æ•°å­¦æ¨ç†æ€§èƒ½æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>RePOåœ¨å¢åŠ è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆæé«˜ä¼˜åŒ–æ­¥éª¤çš„æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88ebe99c06f60ada890816e9acb5a380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3295a87654fd37f82e32804d53d62614.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-643b241c536bf5cd58e8573121ba6bbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235658a2da84a703a296034d6eae1604.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74d73a182ee4b1290d2b8085c03bf034.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-89f8b96b14d87014d76a7f04f96333ae.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01bd51cf355ba2bd9554a5a5c451e0f6.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-12  AraReasoner Evaluating Reasoning-Based LLMs for Arabic NLP
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
