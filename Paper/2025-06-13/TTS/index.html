<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-06-13  EmoNet-Voice A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    22 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-13-更新"><a href="#2025-06-13-更新" class="headerlink" title="2025-06-13 更新"></a>2025-06-13 更新</h1><h2 id="EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection"><a href="#EmoNet-Voice-A-Fine-Grained-Expert-Verified-Benchmark-for-Speech-Emotion-Detection" class="headerlink" title="EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection"></a>EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection</h2><p><strong>Authors:Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer</strong></p>
<p>The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration. </p>
<blockquote>
<p>文本转语音和音频生成模型的进步为评估人工智能系统的情感理解能力提供了强大的基准测试。现有的语音情感识别（SER）数据集通常在情感粒度、隐私担忧或依赖表演表现方面存在局限性。本文介绍了用于语音情感检测的新资源EmoNet-Voice，它包括EmoNet-Voice Big，这是一个大规模预训练数据集（跨越11个声音、40种情感和4种语言，包含超过4500小时的语音），以及带有专家注释的新型基准数据集EmoNet-Voice Bench。EmoNet-Voice旨在在一个精细的粒度谱上评估SER模型对情绪强度的感知能力，包括特定的40种情绪类别。借助最先进的语音生成技术，我们精心制作了模拟演员扮演特定情感场景合成音频片段。重要的是，我们经过心理学专家的严格验证并获得了感知强度标签。这种合成的方法保护隐私并允许引入现有数据集中通常缺少的敏感情绪状态。最后，我们引入了Empathic Insight Voice模型，该模型在语音情感识别方面树立了新的标准并与人类专家达成高度共识。对当前模型景观的评估显示出有价值的发现，例如愤怒等强烈唤醒的情绪更容易被检测到，而集中等低唤醒状态则相对困难。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09827v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的语音情感检测资源EmoNet-Voice，包括用于预训练的EmoNet-Voice Big大型数据集和作为基准测试集的EmoNet-Voice Bench。EmoNet-Voice旨在评估语音情感识别模型在精细粒度情感分类上的性能，同时考虑了不同强度的情感。通过心理学专家的严格验证，采用先进的语音生成技术模拟演员表演场景以激发特定情感。此外，本文还引入了Empathic Insight Voice模型，为语音情感识别设定了新的标准，并在当前模型景观中展示了高人类专家共识的价值发现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了新的语音情感检测资源EmoNet-Voice，包括大型预训练数据集EmoNet-Voice Big和基准测试集EmoNet-Voice Bench。</li>
<li>EmoNet-Voice强调对精细粒度情感分类的评估，考虑不同强度的情感。</li>
<li>利用先进的语音生成技术模拟演员表演场景，激发特定情感，并经心理学专家验证。</li>
<li>引入了Empathic Insight Voice模型，为语音情感识别设定了新的标准。</li>
<li>展现了高人类专家共识的价值发现，如高唤醒情感（如愤怒）比低唤醒状态（如专注）更容易检测。</li>
<li>EmoNet-Voice的设计解决了现有数据集在情感粒度、隐私保护以及模拟真实情感表达方面的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-434d9a1abef531c21b5483583b56d3c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead12f776165c52d3bd1fc04d82aeaba.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OmniDRCA-Parallel-Speech-Text-Foundation-Model-via-Dual-Resolution-Speech-Representations-and-Contrastive-Alignment"><a href="#OmniDRCA-Parallel-Speech-Text-Foundation-Model-via-Dual-Resolution-Speech-Representations-and-Contrastive-Alignment" class="headerlink" title="OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment"></a>OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution   Speech Representations and Contrastive Alignment</h2><p><strong>Authors:Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang, Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, Chong Zhang, Yukun Ma, Yafeng Chen, Hui Wang, Jiaqing Liu, Jieping Ye</strong></p>
<p>Recent studies on end-to-end speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM’s autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling, featuring dual-resolution speech representations and contrastive cross-modal alignment. Our approach processes speech and text representations in parallel while enhancing audio comprehension through contrastive alignment. Experimental results on Spoken Question Answering benchmarks demonstrate that OmniDRCA establishes new state-of-the-art (SOTA) performance among parallel joint speech-text modeling based foundation models, and achieves competitive performance compared to interleaved models. Additionally, we explore the potential of extending the framework to full-duplex conversational scenarios. </p>
<blockquote>
<p>最近关于使用大型语言模型（LLM）进行端到端语音生成的研究引起了社区的关注，多项工作将基于文本的LLM扩展到生成离散语音标记。现有方法主要可分为两类：（1）独立生成离散语音标记的方法，不将其纳入LLM的自回归过程，导致文本生成无法意识到并发的语音合成。（2）通过联合自回归建模生成交织或并行语音文本标记的模型，从而在生成过程中实现跨模态相互感知。本文提出了OmniDRCA，一种基于联合自回归建模的并行语音文本基础模型，具有双分辨率语音表示和对比跨模态对齐功能。我们的方法并行处理语音和文本表示，同时通过对比对齐增强音频理解。在口语问答基准测试上的实验结果表明，OmniDRCA在基于并行联合语音文本建模的基础模型中建立了新的最先进的性能，并在与交织模型的比较中取得了具有竞争力的表现。此外，我们还探讨了将框架扩展到全双工对话场景的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期关于利用大型语言模型（LLMs）进行端到端语音生成的研究引起了社区的关注。现有方法主要分为两类：独立生成离散语音令牌的方法和通过联合自回归建模生成交替或并行语音文本令牌的方法。本文提出OmniDRCA，一个基于联合自回归建模的并行语音文本基础模型，具有双分辨率语音表示和对比跨模态对齐特点。实验结果表明，OmniDRCA在基于并行联合语音文本建模的基础模型中建立了新的性能水平，并在与交替模型的比较中实现了具有竞争力的性能。此外，本文还探讨了将该框架扩展到全双工对话场景的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端到端语音生成研究利用大型语言模型（LLMs）吸引了社区关注。</li>
<li>现有方法主要分为独立生成离散语音令牌和通过联合自回归建模生成交替或并行语音文本令牌两类。</li>
<li>OmniDRCA是一个基于联合自回归建模的并行语音文本基础模型，具有双分辨率语音表示和对比跨模态对齐特点。</li>
<li>OmniDRCA在基于并行联合语音文本建模的基础模型中建立了新的性能水平。</li>
<li>OmniDRCA在Spoken Question Answering benchmarks上的表现优于其他模型，具有竞争力。</li>
<li>OmniDRCA框架有潜力扩展到全双工对话场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b74cf6f5443b481996530d1700600b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d5ca7802866f9586dbadb2d15fb9de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-035877bff8289b603e5b93fe4a79be5b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MetaTT-A-Global-Tensor-Train-Adapter-for-Parameter-Efficient-Fine-Tuning"><a href="#MetaTT-A-Global-Tensor-Train-Adapter-for-Parameter-Efficient-Fine-Tuning" class="headerlink" title="MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient   Fine-Tuning"></a>MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient   Fine-Tuning</h2><p><strong>Authors:Javier Lopez-Piqueres, Pranav Deshpande, Archan Ray, Mattia J. Villani, Marco Pistoia, Niraj Kumar</strong></p>
<p>We present MetaTT, a unified Tensor Train (TT) adapter framework for global low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes each weight matrix independently, MetaTT uses a single shared TT to factorize all transformer sub-modules – query, key, value, projection, and feed-forward layers – by indexing the structural axes like layer and matrix type, and optionally heads and tasks. For a given rank, while LoRA adds parameters proportional to the product across modes, MetaTT only adds parameters proportional to the sum across modes leading to a significantly compressed final adapter. Our benchmarks compare MetaTT with LoRA along with recent state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We observe that when tested on standard language modeling benchmarks, MetaTT leads to the most reduction in the parameters while maintaining similar accuracy to LoRA and even outperforming other tensor-based methods. Unlike CP or other rank-factorizations, the TT ansatz benefits from mature optimization routines – e.g., DMRG-style rank adaptive minimization in addition to Adam, which we find simplifies training. Because new modes can be appended cheaply, MetaTT naturally extends to shared adapters across many tasks without redesigning the core tensor. </p>
<blockquote>
<p>我们提出了MetaTT，这是一个统一的Tensor Train（TT）适配器框架，用于预训练转换器的全局低秩微调。与独立微调每个权重矩阵的LoRA不同，MetaTT使用单个共享的TT来分解所有转换器子模块——查询、键、值、投影和前馈层——通过索引结构轴，如层和矩阵类型，以及可选的头部和任务。对于给定的等级，虽然LoRA添加的参数与各模式之积成正比，但MetaTT添加的参数仅与各模式之和成正比，从而得到了显著压缩的最终适配器。我们的基准测试将MetaTT与LoRA以及最近的最新矩阵和张量分解微调方案进行了比较。我们在标准语言建模基准测试中发现，MetaTT在减少参数的同时，保持了与LoRA相似的精度，甚至超越了其他张量方法。与CP或其他等级分解不同，TT ansatz受益于成熟的优化算法——例如，除了Adam之外，我们还发现了DMRG风格的等级自适应最小化算法简化了训练。由于可以便宜地添加新模式，MetaTT自然地扩展到跨多个任务的共享适配器，而无需重新设计核心张量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09105v1">PDF</a> </p>
<p><strong>Summary</strong><br>     我们提出了MetaTT，一个统一的Tensor Train（TT）适配器框架，用于预训练转换器的全局低秩微调。不同于LoRA独立微调每个权重矩阵，MetaTT使用一个共享的TT对所有转换器子模块进行因子分解，通过索引结构轴（如层、矩阵类型）以及可选的头和任务。对于给定的秩，MetaTT添加的参数是各模式之和的比例，导致最终的适配器得到显著压缩。我们的基准测试将MetaTT与LoRA以及最新的先进的矩阵和张量分解微调方案进行了比较。在标准语言建模基准测试中，MetaTT在保持与LoRA相似准确率的同时，实现了参数的最大减少，并超越了其他张量方法。TT ansatz受益于成熟的优化算法，如结合Adam的DMRG风格秩自适应最小化算法，简化了训练。由于可以廉价地添加新模式，MetaTT自然地扩展到跨多个任务的共享适配器，无需重新设计核心张量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MetaTT是一个统一的Tensor Train适配器框架，用于预训练转换器的全球低秩微调。</li>
<li>与LoRA不同，MetaTT使用共享的TT对所有转换器子模块进行因子分解。</li>
<li>MetaTT通过索引结构轴（如层和矩阵类型）来添加参数，实现了参数的显著压缩。</li>
<li>在标准语言建模基准测试中，MetaTT在保持与LoRA相似准确率的同时，实现了参数的最优减少。</li>
<li>TT ansatz受益于成熟的优化算法，如结合Adam的DMRG风格秩自适应最小化算法。</li>
<li>MetaTT可以廉价地添加新模式，自然扩展到跨多个任务的共享适配器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09105">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-53d1039ed90c00e5e7f1fc7e9136ed30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7079b210828736430e66ab2207aa9b13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79710be379be63d5b3ad12c75a04dd3c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Review-on-Score-based-Generative-Models-for-Audio-Applications"><a href="#A-Review-on-Score-based-Generative-Models-for-Audio-Applications" class="headerlink" title="A Review on Score-based Generative Models for Audio Applications"></a>A Review on Score-based Generative Models for Audio Applications</h2><p><strong>Authors:Ge Zhu, Yutong Wen, Zhiyao Duan</strong></p>
<p>Diffusion models have emerged as powerful deep generative techniques, producing high-quality and diverse samples in applications in various domains including audio. These models have many different design choices suitable for different applications, however, existing reviews lack in-depth discussions of these design choices. The audio diffusion model literature also lacks principled guidance for the implementation of these design choices and their comparisons for different applications. This survey provides a comprehensive review of diffusion model design with an emphasis on design principles for quality improvement and conditioning for audio applications. We adopt the score modeling perspective as a unifying framework that accommodates various interpretations, including recent approaches like flow matching. We systematically examine the training and sampling procedures of diffusion models, and audio applications through different conditioning mechanisms. To address the lack of audio diffusion model codebases and to promote reproducible research and rapid prototyping, we introduce an open-source codebase at <a target="_blank" rel="noopener" href="https://github.com/gzhu06/AudioDiffuser">https://github.com/gzhu06/AudioDiffuser</a> that implements our reviewed framework for various audio applications. We demonstrate its capabilities through three case studies: audio generation, speech enhancement, and text-to-speech synthesis, with benchmark evaluations on standard datasets. </p>
<blockquote>
<p>扩散模型作为一种强大的深度生成技术，已经在包括音频在内的各种应用领域中产生了高质量和多样化的样本。这些模型有许多不同的设计选择，适合不同的应用，然而现有的评论缺乏对这些设计选择的深入讨论。音频扩散模型文献也缺乏这些设计选择实施的原则指导以及它们在不同应用中的比较。这篇综述对扩散模型设计进行了全面回顾，重点介绍了提高质量和针对音频应用进行条件设置的设计原则。我们采用评分建模的视角，作为一个统一的框架，可以容纳各种解释，包括最新的方法如流匹配。我们系统地研究了扩散模型的训练和采样程序，以及通过不同条件机制在音频领域的应用。为了解决音频扩散模型代码库缺乏的问题，并推动可重复研究和快速原型开发，我们在<a target="_blank" rel="noopener" href="https://github.com/gzhu06/AudioDiffuser%E4%B8%8A%E6%8E%A8%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81%E5%BA%93%EF%BC%8C%E4%B8%BA%E5%90%84%E7%A7%8D%E9%9F%B3%E9%A2%91%E5%BA%94%E7%94%A8%E5%AE%9E%E7%8E%B0%E4%BA%86%E6%88%91%E4%BB%AC%E5%AE%A1%E6%9F%A5%E8%BF%87%E7%9A%84%E6%A1%86%E6%9E%B6%E3%80%82%E6%88%91%E4%BB%AC%E9%80%9A%E8%BF%87%E4%B8%89%E9%A1%B9%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E8%AF%81%E6%98%8E%E4%BA%86%E5%85%B6%E8%83%BD%E5%8A%9B%EF%BC%9A%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E3%80%81%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA%E5%92%8C%E6%96%87%E6%9C%AC%E5%88%B0%E8%AF%AD%E9%9F%B3%E7%9A%84%E5%90%88%E6%88%90%EF%BC%8C%E5%B9%B6%E5%9C%A8%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%9F%BA%E5%87%86%E8%AF%84%E4%BC%B0%E3%80%82">https://github.com/gzhu06/AudioDiffuser上推出了一个开源代码库，为各种音频应用实现了我们审查过的框架。我们通过三项案例研究证明了其能力：音频生成、语音增强和文本到语音的合成，并在标准数据集上进行了基准评估。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08457v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型作为强大的深度生成技术，已广泛应用于多个领域，包括音频。本文全面回顾了扩散模型的设计原则，重点介绍了提高质量和针对音频应用的条件设计。采用评分建模视角作为统一框架，系统地研究了扩散模型的训练和采样程序以及音频应用程序的不同条件机制。此外，为了弥补音频扩散模型代码库的缺乏，促进可重复研究和快速原型开发，我们介绍了开源代码库。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已成为深度生成技术的强大工具，广泛应用于多个领域，包括音频。</li>
<li>现有文献缺乏对扩散模型设计选择的深入探讨。</li>
<li>本文重点介绍了扩散模型的设计原则，以提高质量和针对音频应用的条件设计。</li>
<li>采用评分建模视角作为统一框架，容纳各种解读，包括最新方法如流匹配。</li>
<li>系统地研究了扩散模型的训练和采样程序。</li>
<li>为了促进可重复研究和快速原型开发，介绍了开源代码库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9e16552380d590d116459549aa33f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-616fc2c99ee5d5cf0d6299e3ad16ed8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-299d24dcf1490c9f35aa3a3a396ab484.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Voice-Impression-Control-in-Zero-Shot-TTS"><a href="#Voice-Impression-Control-in-Zero-Shot-TTS" class="headerlink" title="Voice Impression Control in Zero-Shot TTS"></a>Voice Impression Control in Zero-Shot TTS</h2><p><strong>Authors:Keinichi Fujita, Shota Horiguchi, Yusuke Ijima</strong></p>
<p>Para-&#x2F;non-linguistic information in speech is pivotal in shaping the listeners’ impression. Although zero-shot text-to-speech (TTS) has achieved high speaker fidelity, modulating subtle para-&#x2F;non-linguistic information to control perceived voice characteristics, i.e., impressions, remains challenging. We have therefore developed a voice impression control method in zero-shot TTS that utilizes a low-dimensional vector to represent the intensities of various voice impression pairs (e.g., dark-bright). The results of both objective and subjective evaluations have demonstrated our method’s effectiveness in impression control. Furthermore, generating this vector via a large language model enables target-impression generation from a natural language description of the desired impression, thus eliminating the need for manual optimization. Audio examples are available on our demo page (<a target="_blank" rel="noopener" href="https://ntt-hilab-gensp.github.io/is2025voiceimpression/">https://ntt-hilab-gensp.github.io/is2025voiceimpression/</a>). </p>
<blockquote>
<p>语音中的副语言或非语言信息对于塑造听众的印象至关重要。尽管零样本文本到语音（TTS）已经实现了高保真度的说话人识别，但是通过微调副语言或非语言信息来控制感知到的语音特征，即印象，仍然是一个挑战。因此，我们在零样本TTS中开发了一种语音印象控制方法，该方法使用低维向量来表示各种语音印象对的强度（例如，暗淡-明亮）。客观和主观评估的结果均证明了我们方法在印象控制方面的有效性。此外，通过大型语言模型生成该向量能够根据自然语言的期望印象描述来生成目标印象，从而无需手动优化。音频示例可在我们的演示页面（<a target="_blank" rel="noopener" href="https://ntt-hilab-gensp.github.io/is2025voiceimpression/%EF%BC%89%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://ntt-hilab-gensp.github.io/is2025voiceimpression/）上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05688v2">PDF</a> 5 pages,5 figures, Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一项零样本文本转语音（TTS）中的语音印象控制方法。该方法利用低维向量表示各种语音印象对的强度（如阴暗-明亮），实现了对语音印象的精细控制。客观和主观评估结果均证明了该方法的有效性。此外，通过大型语言模型生成该向量，可以从对所需印象的自然语言描述中生成目标印象，从而无需手动优化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音中的副语言&#x2F;非语言信息对听众的印象形成至关重要。</li>
<li>零样本文本转语音（TTS）已实现了高保真度的语音转换。</li>
<li>挑战在于调整微妙的副语言&#x2F;非语言信息以控制感知到的语音特征，即印象。</li>
<li>提出了一种新的语音印象控制方法，使用低维向量代表各种语音印象对的强度。</li>
<li>客观和主观评估证明了该方法在印象控制方面的有效性。</li>
<li>通过大型语言模型生成向量，可从自然语言描述中生成目标印象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8fdbe61650db932305a8bf29772e88f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8341a00795f21d35fd6df9ab628bce29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0896c06d12270a59321f1c9c5032a94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83e651d68a990f7a99767314469b06e7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Speech-Synthesis-By-Unrolling-Diffusion-Process-using-Neural-Network-Layers"><a href="#Speech-Synthesis-By-Unrolling-Diffusion-Process-using-Neural-Network-Layers" class="headerlink" title="Speech Synthesis By Unrolling Diffusion Process using Neural Network   Layers"></a>Speech Synthesis By Unrolling Diffusion Process using Neural Network   Layers</h2><p><strong>Authors:Peter Ochieng</strong></p>
<p>This work introduces UDPNet, a novel architecture designed to accelerate the reverse diffusion process in speech synthesis. Unlike traditional diffusion models that rely on timestep embeddings and shared network parameters, UDPNet unrolls the reverse diffusion process directly into the network architecture, with successive layers corresponding to equally spaced steps in the diffusion schedule. Each layer progressively refines the noisy input, culminating in a high-fidelity estimation of the original data, (x_0). Additionally, we redefine the learning target by predicting latent variables instead of the conventional (x_0) or noise (\epsilon_0). This shift addresses the common issue of large prediction errors in early denoising stages, effectively reducing speech distortion. Extensive evaluations on single- and multi-speaker datasets demonstrate that UDPNet consistently outperforms state-of-the-art methods in both quality and efficiency, while generalizing effectively to unseen speech. These results position UDPNet as a robust solution for real-time speech synthesis applications. Sample audio is available at <a target="_blank" rel="noopener" href="https://onexpeters.github.io/UDPNet">https://onexpeters.github.io/UDPNet</a>. </p>
<blockquote>
<p>本文介绍了UDPNet，这是一种新型架构，旨在加速语音合成中的反向扩散过程。与传统的依赖于时间步长嵌入和共享网络参数的扩散模型不同，UDPNet直接将反向扩散过程展开到网络架构中，连续层对应于扩散计划中等间距的步骤。每层都对噪声输入进行逐步精细处理，最终得到对原始数据的高保真估计(x_0)。此外，我们通过预测潜在变量来重新定义学习目标，而不是传统的(x_0)或噪声(\epsilon_0)。这种转变解决了早期去噪阶段预测误差较大的常见问题，有效地减少了语音失真。在单说话人和多说话人数据集上的广泛评估表明，UDPNet在质量和效率方面均优于最新方法，并且能够有效地推广到未见过的语音。这些结果使UDPNet成为实时语音合成应用的稳健解决方案。示例音频可在<a target="_blank" rel="noopener" href="https://onexpeters.github.io/UDPNet%E6%89%BE%E5%88%B0%E3%80%82">https://onexpeters.github.io/UDPNet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09652v5">PDF</a> 10 pages</p>
<p><strong>摘要</strong><br>    本文介绍了UDPNet，这是一种新型架构，旨在加速语音合成中的反向扩散过程。UDPNet将反向扩散过程直接融入网络架构，与传统依赖时间步长嵌入和共享网络参数的扩散模型不同。其连续层对应于扩散计划中的等间隔步骤。每层都对噪声输入进行渐进细化，最终得到对原始数据的高保真估计。此外，通过预测潜在变量来重新定义学习目标，而不是传统的x_0或噪声ε_0，解决了早期去噪阶段预测误差较大的常见问题，有效减少了语音失真。在单说话人和多说话人数据集上的广泛评估表明，UDPNet在质量和效率方面均优于最新方法，并且能有效地推广到未见过的语音。这些结果使UDPNet成为实时语音合成应用的稳健解决方案。</p>
<p><strong>要点</strong></p>
<ol>
<li>UDPNet是一种加速语音合成中反向扩散过程的新型架构。</li>
<li>UDPNet将反向扩散过程直接融入网络架构，连续层对应扩散计划中的等间隔步骤。<br>3.每层对噪声输入进行渐进细化，最终得到高保真度的原始数据估计。</li>
<li>通过预测潜在变量来重新定义学习目标，有效减少语音失真。</li>
<li>UDPNet在质量和效率方面均优于最新方法。</li>
<li>UDPNet能有效地推广到未见过的语音，具有稳健性。</li>
<li>样本音频可在<a target="_blank" rel="noopener" href="https://onexpeters.github.io/UDPNet">链接</a>找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.09652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0944ffe908783bd39eb1c67ac2ca7424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d952a6831c2cac105b5a1b88aa69a063.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2e218bb04615e59f7b2b6c0e5d24016e.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-06-13  Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over   Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9e7c9e275e906033e04e4c5f96cc32b4.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-13  Single Cu Atom Sites on Co3O4 Activate Interfacial Oxygen for Enhanced   Reactivity and Selective Gas Sensing at Low Temperature
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
