<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    27 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="A-Shortcut-aware-Video-QA-Benchmark-for-Physical-Understanding-via-Minimal-Video-Pairs"><a href="#A-Shortcut-aware-Video-QA-Benchmark-for-Physical-Understanding-via-Minimal-Video-Pairs" class="headerlink" title="A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs"></a>A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs</h2><p><strong>Authors:Benno Krojer, Mojtaba Komeili, Candace Ross, Quentin Garrido, Koustuv Sinha, Nicolas Ballas, Mahmoud Assran</strong></p>
<p>Existing benchmarks for assessing the spatio-temporal understanding and reasoning abilities of video language models are susceptible to score inflation due to the presence of shortcut solutions based on superficial visual or textual cues. This paper mitigates the challenges in accurately assessing model performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple shortcut-aware video QA benchmark for assessing the physical understanding of video language models. The benchmark is comprised of 55K high-quality multiple-choice video QA examples focusing on physical world understanding. Examples are curated from nine video data sources, spanning first-person egocentric and exocentric videos, robotic interaction data, and cognitive science intuitive physics benchmarks. To mitigate shortcut solutions that rely on superficial visual or textual cues and biases, each sample in MVP has a minimal-change pair â€“ a visually similar video accompanied by an identical question but an opposing answer. To answer a question correctly, a model must provide correct answers for both examples in the minimal-change pair; as such, models that solely rely on visual or textual biases would achieve below random performance. Human performance on MVP is 92.9%, while the best open-source state-of-the-art video-language model achieves 40.2% compared to random performance at 25%. </p>
<blockquote>
<p>å½“å‰è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹å¯¹æ—¶ç©ºçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å®¹æ˜“å—åˆ°è¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢çš„æ·å¾„è§£å†³æ–¹æ¡ˆçš„å½±å“ï¼Œå¯¼è‡´åˆ†æ•°è†¨èƒ€ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥æœ€å°è§†é¢‘å¯¹ï¼ˆMVPï¼‰åŸºå‡†æµ‹è¯•æ¥ç¼“è§£å‡†ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ä¸”èƒ½è¯†åˆ«æ·å¾„çš„è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«5.5ä¸‡ä¸ªé«˜è´¨é‡çš„å¤šé¡¹é€‰æ‹©é¢˜è§†é¢‘é—®ç­”ç¤ºä¾‹ï¼Œé‡ç‚¹è€ƒå¯Ÿç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚è¿™äº›ç¤ºä¾‹æ˜¯ä»ä¹ä¸ªè§†é¢‘æ•°æ®æºä¸­ç²¾é€‰å‡ºæ¥çš„ï¼ŒåŒ…æ‹¬ç¬¬ä¸€äººç§°ä¸»è§‚å’Œå®¢è§‚è§†é¢‘ã€æœºå™¨äººäº¤äº’æ•°æ®å’Œè®¤çŸ¥ç§‘å­¦ç›´è§‰ç‰©ç†åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†ç¼“è§£ä¾èµ–äºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢å’Œåè§çš„æ·å¾„è§£å†³æ–¹æ¡ˆï¼ŒMVPä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–çš„å¯¹â€”â€”ä¸€ä¸ªè§†è§‰ç›¸ä¼¼çš„è§†é¢‘ä¼´éšä¸€ä¸ªç›¸åŒçš„é—®é¢˜ä½†ç­”æ¡ˆç›¸åã€‚ä¸ºäº†æ­£ç¡®å›ç­”é—®é¢˜ï¼Œæ¨¡å‹å¿…é¡»ä¸ºæœ€å°å˜åŒ–å¯¹ä¸­çš„ä¸¤ä¸ªç¤ºä¾‹éƒ½æä¾›æ­£ç¡®ç­”æ¡ˆï¼›å› æ­¤ï¼Œé‚£äº›ä»…ä¾èµ–äºè§†è§‰æˆ–æ–‡æœ¬åè§çš„æ¨¡å‹çš„è¡¨ç°å°†ä½äºéšæœºè¡¨ç°ã€‚äººç±»åœ¨MVPä¸Šçš„è¡¨ç°æ˜¯92.9%ï¼Œè€Œæœ€ä½³çš„å¼€æºå…ˆè¿›è§†é¢‘è¯­è¨€æ¨¡å‹ä¸éšæœºè¡¨ç°çš„25%ç›¸æ¯”ï¼Œè¾¾åˆ°äº†40.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰çš„è§†é¢‘è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†å­˜åœ¨åŸºäºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢çš„æ·å¾„è§£å†³æ–¹æ¡ˆå¯¼è‡´è¯„åˆ†è†¨èƒ€çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†æœ€å°è§†é¢‘å¯¹ï¼ˆMVPï¼‰åŸºå‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„ã€èƒ½è¯†åˆ«æ·å¾„çš„è§†é¢‘é—®ç­”è¯„ä¼°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚MVPåŒ…å«æ¥è‡ªä¹ä¸ªè§†é¢‘æ•°æ®æºçš„é«˜è´¨é‡é€‰æ‹©é¢˜ï¼Œæ¶µç›–ç¬¬ä¸€äººç§°è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒè§†é¢‘ã€æœºå™¨äººäº¤äº’æ•°æ®å’Œè®¤çŸ¥ç§‘å­¦ç›´è§‰ç‰©ç†åŸºå‡†æµ‹è¯•ã€‚ä¸ºç¼“è§£ä¾èµ–è¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢å’Œåè§çš„æ·å¾„è§£å†³æ–¹æ¡ˆï¼ŒMVPä¸­çš„æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸€ä¸ªæœ€å°å˜åŒ–å¯¹â€”â€”ä¸€ä¸ªè§†è§‰ä¸Šç›¸ä¼¼çš„è§†é¢‘ä¼´éšä¸€ä¸ªç›¸åŒçš„é—®é¢˜ä½†ç­”æ¡ˆç›¸åã€‚ä¸ºäº†æ­£ç¡®å›ç­”é—®é¢˜ï¼Œæ¨¡å‹å¿…é¡»ä¸ºæœ€å°å˜åŒ–å¯¹ä¸­çš„ä¸¤ä¸ªä¾‹å­éƒ½æä¾›æ­£ç¡®ç­”æ¡ˆï¼›å› æ­¤ï¼Œåªä¾èµ–è§†è§‰æˆ–æ–‡æœ¬åè§çš„æ¨¡å‹çš„è¡¨ç°å°†ä½äºéšæœºè¡¨ç°ã€‚äººç±»åœ¨MVPä¸Šçš„è¡¨ç°æ˜¯92.9%ï¼Œè€Œæœ€ä½³çš„å¼€æºå…ˆè¿›è§†é¢‘è¯­è¨€æ¨¡å‹çš„è¡¨ç°æ˜¯40.2%ï¼Œç›¸æ¯”ä¹‹ä¸‹éšæœºè¡¨ç°æ˜¯25%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹è¯„ä¼°åŸºå‡†æ˜“å—åŸºäºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢çš„æ·å¾„è§£å†³æ–¹æ¡ˆå½±å“ï¼Œå¯¼è‡´è¯„åˆ†è†¨èƒ€ã€‚</li>
<li>å¼•å…¥æœ€å°è§†é¢‘å¯¹ï¼ˆMVPï¼‰åŸºå‡†ï¼Œä»¥è¯„ä¼°è§†é¢‘è¯­è¨€æ¨¡å‹å¯¹ç‰©ç†ä¸–ç•Œçš„ç†è§£ã€‚</li>
<li>MVPåŒ…å«æ¥è‡ªå¤šä¸ªè§†é¢‘æ•°æ®æºçš„é«˜è´¨é‡é€‰æ‹©é¢˜ï¼Œæ¶µç›–å¤šç§è§†é¢‘ç±»å‹ã€‚</li>
<li>MVPé€šè¿‡æœ€å°å˜åŒ–å¯¹è®¾è®¡æ¥æŠ‘åˆ¶åŸºäºè¡¨é¢è§†è§‰æˆ–æ–‡æœ¬çº¿ç´¢çš„æ·å¾„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ­£ç¡®å›ç­”é—®é¢˜éœ€è¦æ¨¡å‹åœ¨æœ€å°å˜åŒ–å¯¹ä¸­éƒ½æä¾›æ­£ç¡®ç­”æ¡ˆï¼Œä»¥æ­¤åŒºåˆ†çœŸæ­£ç†è§£å’Œä¾èµ–åè§çš„æ¨¡å‹ã€‚</li>
<li>ç›¸æ¯”éšæœºè¡¨ç°ï¼Œäººç±»åœ¨MVPä¸Šçš„è¡¨ç°è¾ƒé«˜ï¼Œè€Œå½“å‰æœ€ä½³çš„è§†é¢‘è¯­è¨€æ¨¡å‹è¡¨ç°ä»ä½äºéšæœºè¡¨ç°ã€‚è¿™è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢è¿˜æœ‰æå‡ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a40150f7ccd33ca14f5fa2f9256ce26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69c438fc11f30eede4b199f5dbf55e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07f1bce7c38a7137a9759a645973da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7635d15b552978158bf81194df6ab85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66964b08c5bd718512a85fc0a5039f4b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning"><a href="#V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning" class="headerlink" title="V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning"></a>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning</h2><p><strong>Authors:Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes,  Mojtaba,  Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas</strong></p>
<p>A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world. </p>
<blockquote>
<p>ç°ä»£äººå·¥æ™ºèƒ½é¢ä¸´çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å­¦ä¹ ç†è§£ä¸–ç•Œå¹¶é€šè¿‡è§‚å¯Ÿè¿›è¡Œè¡ŒåŠ¨ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®å’Œå°é‡çš„äº¤äº’æ•°æ®ï¼ˆæœºå™¨äººè½¨è¿¹ï¼‰ï¼Œä»¥å¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç†è§£ã€é¢„æµ‹å’Œè§„åˆ’æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨äº’è”ç½‘è§†é¢‘ç»„æˆçš„è§†é¢‘å’Œå›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒäº†ä¸€ç§æ— åŠ¨ä½œè”åˆåµŒå…¥é¢„æµ‹æ¶æ„V-JEPA 2ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 1 äº¿å°æ—¶çš„è§†é¢‘ã€‚V-JEPA 2åœ¨è¿åŠ¨ç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ˆSomething-Something v2ä¸Šè¾¾åˆ°äº† 77.3% çš„Top-1å‡†ç¡®ç‡ï¼‰ï¼Œå¹¶åœ¨äººç±»åŠ¨ä½œé¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆEpic-Kitchens-100ä¸Šçš„å¬å›ç‡ä¸º 39.7%ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½åï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒPerceptionTestä¸Šä¸º 84.0%ï¼ŒTempCompassä¸Šä¸º 76.9%ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åˆ©ç”¨æ¥è‡ªDroidæ•°æ®é›†çš„ä¸åˆ° 62 å°æ—¶çš„æ— æ ‡ç­¾æœºå™¨äººè§†é¢‘å¯¹æ½œåœ¨çš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹V-JEPA 2-ACè¿›è¡Œåè®­ç»ƒï¼Œæ¥å°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºæœºå™¨äººè§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„å®éªŒå®¤éƒ¨ç½²äº†FrankåŒè‡‚æœºå™¨äººé›¶æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨å›¾åƒç›®æ ‡çš„è§„åˆ’å®ç°äº†ç‰©ä½“çš„æŠ“å–å’Œæ”¾ç½®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯åœ¨æ²¡æœ‰ä»»ä½•ç¯å¢ƒä¸­æœºå™¨äººæ•°æ®æ”¶é›†çš„æƒ…å†µä¸‹å®ç°çš„ï¼Œä¹Ÿä¸éœ€è¦ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæˆ–å¥–åŠ±ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å¦‚ä½•ä»äº’è”ç½‘è§„æ¨¡çš„æ•°æ®å’Œå°é‡çš„æœºå™¨äººäº¤äº’æ•°æ®ä¸­è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œä»è€Œå»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­è§„åˆ’çš„ä¸–ç•Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09985v1">PDF</a> 48 pages, 19 figures</p>
<p><strong>æ‘˜è¦</strong><br>è¯¥è®ºæ–‡æ¢è®¨äº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®å’Œå°é‡çš„äº¤äº’æ•°æ®ï¼ˆæœºå™¨äººè½¨è¿¹ï¼‰ï¼Œä»¥å¼€å‘èƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç†è§£ã€é¢„æµ‹å’Œè§„åˆ’æ¨¡å‹çš„èƒ½åŠ›ã€‚é¦–å…ˆï¼Œå¯¹æ— åŠ¨ä½œè”åˆåµŒå…¥é¢„æµ‹æ¶æ„V-JEPA 2è¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ¶æ„åœ¨äº’è”ç½‘è§†é¢‘å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡å°æ—¶çš„è§†é¢‘æ•°æ®ã€‚V-JEPA 2åœ¨è¿åŠ¨ç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼ˆåœ¨Something-Something v2ä¸Šè¾¾åˆ°77.3çš„top-1å‡†ç¡®ç‡ï¼‰ï¼Œå¹¶åœ¨äººç±»åŠ¨ä½œé¢„æµ‹æ–¹é¢è¾¾åˆ°æœ€ä½³è¡¨ç°ï¼ˆåœ¨Epic-Kitchens-100ä¸Šçš„å¬å›ç‡ä¸º39.7ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½åï¼Œå®ƒåœ¨å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œåœ¨PerceptionTestä¸Šè¾¾åˆ°84.0ï¼Œåœ¨TempCompassä¸Šè¾¾åˆ°76.9ï¼‰ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å°†è‡ªç›‘ç£å­¦ä¹ åº”ç”¨äºæœºå™¨äººè§„åˆ’ä»»åŠ¡ï¼Œé€šè¿‡åˆ©ç”¨ä¸åˆ°62å°æ—¶çš„æ— äººæ ‡æ³¨çš„æœºå™¨äººè§†é¢‘æ•°æ®ï¼ˆæ¥è‡ªDroidæ•°æ®é›†ï¼‰å¯¹æ½œåœ¨çš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹V-JEPA 2-ACè¿›è¡Œåè®­ç»ƒã€‚åœ¨ä¸¤å®¶å®éªŒå®¤çš„FrankåŒè‡‚ä¸Šéƒ¨ç½²V-JEPA 2-ACé›¶æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨å›¾åƒç›®æ ‡è¿›è¡Œè§„åˆ’ä»¥å®ç°ç‰©ä½“çš„æ‹¾å–å’Œæ”¾ç½®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯åœ¨æ²¡æœ‰ä»»ä½•æœºå™¨äººç¯å¢ƒæ•°æ®é‡‡é›†å’Œä»»åŠ¡ç‰¹å®šè®­ç»ƒæˆ–å¥–åŠ±çš„æƒ…å†µä¸‹å®ç°çš„ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è‡ªç›‘ç£å­¦ä¹ å¯ä»¥ä»ç½‘ç»œè§„æ¨¡çš„æ•°æ®å’Œå°‘é‡çš„æœºå™¨äººäº¤äº’æ•°æ®ä¸­æ„å»ºèƒ½å¤Ÿè§„åˆ’ç‰©ç†ä¸–ç•Œçš„ä¸–ç•Œæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬è®ºæ–‡æ¢è®¨äº†è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç°ä»£AIé¢ä¸´çš„å¤§è§„æ¨¡è§‚å¯Ÿå’Œç†è§£çš„æŒ‘æˆ˜ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚é€šè¿‡ç»“åˆäº†äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘æ•°æ®å’Œå°‘é‡æœºå™¨äººäº¤äº’æ•°æ®æ¥å¼€å‘ç†è§£ç‰©ç†ä¸–ç•Œçš„æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§æ— åŠ¨ä½œè”åˆåµŒå…¥é¢„æµ‹æ¶æ„V-JEPA 2ï¼Œè¯¥æ¶æ„åœ¨è§†é¢‘å’Œå›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨è¿åŠ¨ç†è§£å’Œäººç±»åŠ¨ä½œé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªè§†é¢‘é—®ç­”ä»»åŠ¡ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åˆ©ç”¨æ— äººæ ‡æ³¨çš„æœºå™¨äººè§†é¢‘æ•°æ®è¿›è¡Œåè®­ç»ƒæ¥å¼€å‘æ½œåœ¨çš„åŠ¨ä½œæ¡ä»¶ä¸–ç•Œæ¨¡å‹V-JEPA 2-ACã€‚è¿™ä¸€æ¨¡å‹å¯ä»¥åœ¨ä¸åŒå®éªŒå®¤çš„æœºå™¨äººæ‰‹è‡‚ä¸Šå®ç°é›¶æ ·æœ¬éƒ¨ç½²ï¼Œå¹¶æ‰§è¡Œç‰©ä½“æ‹¾å–å’Œæ”¾ç½®çš„ä»»åŠ¡è§„åˆ’ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†è‡ªç›‘ç£å­¦ä¹ çš„æ½œåŠ›ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿä»å¤§è§„æ¨¡ç½‘ç»œæ•°æ®å’Œæœ‰é™çš„æœºå™¨äººäº¤äº’æ•°æ®ä¸­å­¦ä¹ ç‰©ç†ä¸–ç•Œçš„æ¨¡å‹ã€‚è¿™ä¸ºæœªæ¥AIç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ–°æ€è·¯ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨äººæŠ€æœ¯å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚</li>
<li>æœ¬ç ”ç©¶å±•ç¤ºäº†åœ¨æ²¡æœ‰ä»æœºå™¨äººç¯å¢ƒä¸­æ”¶é›†ä»»ä½•æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è‡ªç›‘ç£å­¦ä¹ å®ç°æœºå™¨äººè§„åˆ’ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚è¿™å‡å°‘äº†æ•°æ®æ”¶é›†çš„å¤æ‚æ€§å¹¶æé«˜äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c20e7fd378bb75f6277bc2b568fab79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52726ea443acb63e798931278b8709a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c12be16346c246b398c8c3094751fe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb56100036aa6a8241569ba35d98df63.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VersaVid-R1-A-Versatile-Video-Understanding-and-Reasoning-Model-from-Question-Answering-to-Captioning-Tasks"><a href="#VersaVid-R1-A-Versatile-Video-Understanding-and-Reasoning-Model-from-Question-Answering-to-Captioning-Tasks" class="headerlink" title="VersaVid-R1: A Versatile Video Understanding and Reasoning Model from   Question Answering to Captioning Tasks"></a>VersaVid-R1: A Versatile Video Understanding and Reasoning Model from   Question Answering to Captioning Tasks</h2><p><strong>Authors:Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, Tieniu Tan</strong></p>
<p>Recent advancements in multimodal large language models have successfully extended the Reason-Then-Respond paradigm to image-based reasoning, yet video-based reasoning remains an underdeveloped frontier, primarily due to the scarcity of high-quality reasoning-oriented data and effective training methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA, two novel datasets specifically designed to stimulate the modelâ€™s advanced video understanding and reasoning abilities. DarkEventinfer presents videos with masked event segments, requiring models to infer the obscured content based on contextual video cues. MixVidQA, on the other hand, presents interleaved video sequences composed of two distinct clips, challenging models to isolate and reason about one while disregarding the other. Leveraging these carefully curated training samples together with reinforcement learning guided by diverse reward functions, we develop VersaVid-R1, the first versatile video understanding and reasoning model under the Reason-Then-Respond paradigm capable of handling multiple-choice and open-ended question answering, as well as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1 significantly outperforms existing models across a broad spectrum of benchmarks, covering video general understanding, cognitive reasoning, and captioning tasks. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å·²ç»æˆåŠŸåœ°å°†â€œReason-Then-Respondâ€èŒƒå¼æ‰©å±•åˆ°åŸºäºå›¾åƒçš„ç†è§£ï¼Œä½†åŸºäºè§†é¢‘çš„ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªæœªå……åˆ†å¼€å‘çš„å‰æ²¿é¢†åŸŸï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„æ•°æ®å’Œæœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•è®ºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DarkEventInferå’ŒMixVidQAä¸¤ä¸ªä¸“é—¨è®¾è®¡çš„æ–°æ•°æ®é›†ï¼Œä»¥åˆºæ¿€æ¨¡å‹å¯¹é«˜çº§è§†é¢‘çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚DarkEventInferå‘ˆç°å¸¦æœ‰é®æŒ¡äº‹ä»¶ç‰‡æ®µçš„è§†é¢‘ï¼Œè¦æ±‚æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡è§†é¢‘çº¿ç´¢æ¨æ–­é®æŒ¡å†…å®¹ã€‚å¦ä¸€æ–¹é¢ï¼ŒMixVidQAå‘ˆç°ç”±ä¸¤ä¸ªä¸åŒç‰‡æ®µç»„æˆçš„äº¤ç»‡è§†é¢‘åºåˆ—ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨å¿½ç•¥å¦ä¸€ä¸ªç‰‡æ®µçš„æƒ…å†µä¸‹ï¼Œå¯¹ä¸€ä¸ªç‰‡æ®µè¿›è¡Œéš”ç¦»å’Œæ¨ç†ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™äº›ç²¾å¿ƒæŒ‘é€‰çš„è®­ç»ƒæ ·æœ¬ä»¥åŠç”±å¤šç§å¥–åŠ±å‡½æ•°å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¼€å‘å‡ºVersaVid-R1ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨â€œReason-Then-Respondâ€èŒƒå¼ä¸‹çš„é€šç”¨è§†é¢‘ç†è§£å’Œæ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»¥åŠè§†é¢‘æè¿°ä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVersaVid-R1åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ¶µç›–è§†é¢‘ä¸€èˆ¬ç†è§£ã€è®¤çŸ¥æ¨ç†å’Œæè¿°ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09079v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘ç†è§£é¢†åŸŸçš„æ–°è¿›å±•ã€‚ç”±äºç¼ºå°‘é«˜è´¨é‡æ¨ç†å¯¼å‘çš„æ•°æ®å’Œæœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼Œè§†é¢‘æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†å¼€å‘çš„å‰æ²¿ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡å¼•å…¥äº†DarkEventInferå’ŒMixVidQAä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œåˆºæ¿€æ¨¡å‹å¯¹é«˜çº§è§†é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„éœ€æ±‚ã€‚åˆ©ç”¨è¿™äº›ç²¾å¿ƒç­–åˆ’çš„è®­ç»ƒæ ·æœ¬ä»¥åŠé€šè¿‡ä¸åŒçš„å¥–åŠ±å‡½æ•°å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¼€å‘å‡ºVersaVid-R1æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨â€œæ¨ç†åå›åº”â€æ¡†æ¶ä¸‹å¤„ç†å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»¥åŠè§†é¢‘æè¿°ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒVersaVid-R1åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œæ¶µç›–è§†é¢‘é€šç”¨ç†è§£ã€è®¤çŸ¥æ¨ç†å’Œæè¿°ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°è¿›å±•å·²å°†â€œReason-Then-Respondâ€èŒƒå¼æ‰©å±•åˆ°åŸºäºå›¾åƒçš„ç†è§£ã€‚ç„¶è€Œï¼Œè§†é¢‘ç†è§£ä»ç„¶æ˜¯æœªè¢«å……åˆ†å¼€å‘çš„å‰æ²¿ã€‚</li>
<li>DarkEventInferæ•°æ®é›†æ¨å‡ºï¼Œé€šè¿‡é®è”½è§†é¢‘äº‹ä»¶ç‰‡æ®µè¦æ±‚æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡è§†é¢‘çº¿ç´¢è¿›è¡Œæ¨æ–­ã€‚</li>
<li>MixVidQAæ•°æ®é›†æ¨å‡ºï¼Œé€šè¿‡å±•ç¤ºç”±ä¸¤ä¸ªä¸åŒç‰‡æ®µç»„æˆçš„äº¤é”™è§†é¢‘åºåˆ—ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨å¿½ç•¥ä¸€ä¸ªç‰‡æ®µçš„åŒæ—¶å¯¹å¦ä¸€ä¸ªç‰‡æ®µè¿›è¡Œéš”ç¦»å’Œæ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„è®­ç»ƒæ ·æœ¬å’Œå¤šç§å¥–åŠ±å‡½æ•°å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¼€å‘å‡ºVersaVid-R1æ¨¡å‹ã€‚</li>
<li>VersaVid-R1æ˜¯é¦–ä¸ªåœ¨â€œReason-Then-Respondâ€æ¡†æ¶ä¸‹å®ç°å¤šåŠŸèƒ½è§†é¢‘ç†è§£å’Œæ¨ç†çš„æ¨¡å‹ã€‚</li>
<li>VersaVid-R1èƒ½å¤Ÿå¤„ç†å¤šé¡¹é€‰æ‹©å’Œå¼€æ”¾å¼é—®ç­”ä»¥åŠè§†é¢‘æè¿°ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c6a3ea7bc8f4dae8dbc1a9993a4e14a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aeef6402f0608deb6cd34da0ba17cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f5aa25cb4dc3948977bf49da1240493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b297e766810193c83d405cd55a058ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bee208b930c30363750143c4c0b0e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3301ca6be6862e8eaf2f9ad8c1d6c12d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Video-CoT-A-Comprehensive-Dataset-for-Spatiotemporal-Understanding-of-Videos-Based-on-Chain-of-Thought"><a href="#Video-CoT-A-Comprehensive-Dataset-for-Spatiotemporal-Understanding-of-Videos-Based-on-Chain-of-Thought" class="headerlink" title="Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of   Videos Based on Chain-of-Thought"></a>Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of   Videos Based on Chain-of-Thought</h2><p><strong>Authors:Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang</strong></p>
<p>Video content comprehension is essential for various applications, ranging from video analysis to interactive systems. Despite advancements in large-scale vision-language models (VLMs), these models often struggle to capture the nuanced, spatiotemporal details essential for thorough video analysis. To address this gap, we introduce Video-CoT, a groundbreaking dataset designed to enhance spatiotemporal understanding using Chain-of-Thought (CoT) methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal question-answer pairs and 23,000 high-quality CoT-annotated samples, providing a solid foundation for evaluating spatiotemporal understanding in video comprehension. Additionally, we provide a comprehensive benchmark for assessing these tasks, with each task featuring 750 images and tailored evaluation metrics. Our extensive experiments reveal that current VLMs face significant challenges in achieving satisfactory performance, high-lighting the difficulties of effective spatiotemporal understanding. Overall, the Video-CoT dataset and benchmark open new avenues for research in multimedia understanding and support future innovations in intelligent systems requiring advanced video analysis capabilities. By making these resources publicly available, we aim to encourage further exploration in this critical area. Project website:<a target="_blank" rel="noopener" href="https://video-cot.github.io/">https://video-cot.github.io/</a> . </p>
<blockquote>
<p>è§†é¢‘å†…å®¹ç†è§£å¯¹äºä»è§†é¢‘åˆ†æåˆ°äº¤äº’ç³»ç»Ÿçš„å„ç§åº”ç”¨è‡³å…³é‡è¦ã€‚å°½ç®¡å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨æ•æ‰å…¨é¢è§†é¢‘åˆ†ææ‰€éœ€çš„å¾®å¦™æ—¶ç©ºç»†èŠ‚æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Video-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„æ•°æ®é›†ï¼Œæ—¨åœ¨åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•å¢å¼ºæ—¶ç©ºç†è§£ã€‚Video-CoTåŒ…å«19ä¸‡ç»„ç²¾ç»†çš„æ—¶ç©ºé—®ç­”å¯¹å’Œ2.3ä¸‡ä¸ªé«˜è´¨é‡çš„CoTæ³¨é‡Šæ ·æœ¬ï¼Œä¸ºè¯„ä¼°è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºç†è§£èƒ½åŠ›æä¾›äº†åšå®çš„åŸºç¡€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºè¯„ä¼°è¿™äº›ä»»åŠ¡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«750å¼ å›¾åƒå’Œä¸“é—¨çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›®å‰çš„VLMsåœ¨å–å¾—ä»¤äººæ»¡æ„çš„è¡¨ç°æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œçªå‡ºäº†å®ç°æœ‰æ•ˆæ—¶ç©ºç†è§£çš„å›°éš¾ã€‚æ€»çš„æ¥è¯´ï¼ŒVideo-CoTæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸ºå¤šåª’ä½“ç†è§£çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é€”å¾„ï¼Œå¹¶æ”¯æŒæœªæ¥éœ€è¦é«˜çº§è§†é¢‘åˆ†æåŠŸèƒ½çš„æ™ºèƒ½ç³»ç»Ÿçš„åˆ›æ–°ã€‚æˆ‘ä»¬å…¬å¼€æä¾›è¿™äº›èµ„æºï¼Œæ—¨åœ¨é¼“åŠ±åœ¨è¿™ä¸€å…³é”®é¢†åŸŸè¿›è¡Œè¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://video-cot.github.io/%E3%80%82">https://video-cot.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08817v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘å†…å®¹ç†è§£åœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸä¸­è‡³å…³é‡è¦ï¼Œå¦‚è§†é¢‘åˆ†æä¸äº¤äº’ç³»ç»Ÿã€‚å°½ç®¡å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬ä»éš¾ä»¥æ•æ‰ç»†è‡´çš„ç©ºé—´æ—¶é—´ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚å¯¹äºå½»åº•çš„è§†é¢‘åˆ†æè‡³å…³é‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Video-CoTæ•°æ®é›†ï¼Œè¿ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰æ–¹æ³•å¢å¼ºæ—¶ç©ºç†è§£ã€‚Video-CoTåŒ…å«19.2ä¸‡ç²¾ç»†æ—¶ç©ºé—®ç­”å¯¹å’Œ2.3ä¸‡é«˜è´¨é‡CoTæ³¨é‡Šæ ·æœ¬ï¼Œä¸ºè¯„ä¼°è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºç†è§£æä¾›äº†åšå®åŸºç¡€ã€‚æˆ‘ä»¬è¿˜ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«750å¼ å›¾åƒå’Œå®šåˆ¶è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰VLMsåœ¨å–å¾—ä»¤äººæ»¡æ„çš„æ•ˆæœæ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œçªæ˜¾äº†æœ‰æ•ˆæ—¶ç©ºç†è§£çš„å›°éš¾ã€‚æ€»çš„æ¥è¯´ï¼ŒVideo-CoTæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ä¸ºå¤šåª’ä½“ç†è§£å¼€è¾Ÿäº†æ–°é€”å¾„ï¼Œå¹¶æ”¯æŒæœªæ¥éœ€è¦å…ˆè¿›è§†é¢‘åˆ†æèƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿçš„åˆ›æ–°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å†…å®¹ç†è§£åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰é‡è¦æ€§ï¼Œå¦‚è§†é¢‘åˆ†æå’Œäº¤äº’ç³»ç»Ÿã€‚</li>
<li>å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹æœ‰æ‰€è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ•æ‰ç»†è‡´æ—¶ç©ºç»†èŠ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>Video-CoTæ•°æ®é›†é€šè¿‡è¿ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰æ–¹æ³•å¢å¼ºæ—¶ç©ºç†è§£ã€‚</li>
<li>Video-CoTåŒ…å«å¤§é‡ç²¾ç»†æ—¶ç©ºé—®ç­”å¯¹å’ŒCoTæ³¨é‡Šæ ·æœ¬ã€‚</li>
<li>æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°è§†é¢‘ç†è§£ä¸­çš„æ—¶ç©ºç†è§£ã€‚</li>
<li>å½“å‰VLMsåœ¨è§†é¢‘ç†è§£æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„ç®—æ³•æ¥æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db01653d38fb5acc6cfd62eae67b5916.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5890d348e7b15a99917844b979f04f49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81e6af03fee934f6cbef5b505675218d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ebbb3343e73c8d3c6ebffc462ac9b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d0df7409820e50a90e275f3845979f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TinyLLaVA-Video-Towards-Smaller-LMMs-for-Video-Understanding-with-Group-Resampler"><a href="#TinyLLaVA-Video-Towards-Smaller-LMMs-for-Video-Understanding-with-Group-Resampler" class="headerlink" title="TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group   Resampler"></a>TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group   Resampler</h2><p><strong>Authors:Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, Lei Huang</strong></p>
<p>Video behavior recognition and scene understanding are fundamental tasks in multimodal intelligence, serving as critical building blocks for numerous real-world applications. Through large multimodal models (LMMs) have achieved remarkable progress in video understanding, most existing open-source models rely on over 7B parameters and require large-scale datasets for training, making them resource-intensive and inaccessible to many researchers. Furthermore, lightweight models face persistent challenges in effectively processing long visual sequences and temporal understanding. In this work, we introduce TinyLLaVA-Video, a lightweight yet powerful video understanding model with approximately 3.6B parameters. The cornerstone of our design is the video-level group resampler, a novel mechanism that significantly reduces and controls the number of visual tokens at the video level. Unlike traditional image-level resampler, our approach effectively mitigates redundancy while enhancing temporal comprehension, leading to improved performance on video-based tasks. In addition, TinyLLaVA-Video demonstrates exceptional efficiency, requiring only one day of training on 8 A100-40G GPUs. It surpasses several existing 7B-parameter models on multiple benchmarks. We believe this work provides a valuable foundation for future research on lightweight video understanding models. The code and weights is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video">https://github.com/ZhangXJ199/TinyLLaVA-Video</a>. </p>
<blockquote>
<p>è§†é¢‘è¡Œä¸ºè¯†åˆ«å’Œåœºæ™¯ç†è§£æ˜¯å¤šæ¨¡æ€æ™ºèƒ½ä¸­çš„åŸºæœ¬ä»»åŠ¡ï¼Œæ˜¯ä¼—å¤šç°å®ä¸–ç•Œåº”ç”¨çš„å…³é”®æ„å»ºæ¨¡å—ã€‚å°½ç®¡é€šè¿‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰çš„å¼€æºæ¨¡å‹ä¾èµ–äºè¶…è¿‡70äº¿ä¸ªå‚æ•°ï¼Œå¹¶éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬èµ„æºå¯†é›†ï¼Œè®¸å¤šç ”ç©¶è€…æ— æ³•æ¥è§¦ã€‚æ­¤å¤–ï¼Œè½»é‡çº§æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘åºåˆ—å’Œæ—¶åºç†è§£æ–¹é¢æŒç»­é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TinyLLaVA-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä½†åŠŸèƒ½å¼ºå¤§çš„è§†é¢‘ç†è§£æ¨¡å‹ï¼Œæ‹¥æœ‰çº¦3.6äº¿ä¸ªå‚æ•°ã€‚è®¾è®¡çš„æ ¸å¿ƒæ˜¯è§†é¢‘çº§ç»„é‡é‡‡æ ·å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æœºåˆ¶ï¼Œå¯ä»¥åœ¨è§†é¢‘çº§åˆ«æ˜¾è‘—å‡å°‘å’Œæ§åˆ¶è§†è§‰æ ‡è®°çš„æ•°é‡ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒçº§é‡é‡‡æ ·å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†å†—ä½™ï¼ŒåŒæ—¶æé«˜äº†æ—¶åºç†è§£èƒ½åŠ›ï¼Œåœ¨åŸºäºè§†é¢‘çš„ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼ŒTinyLLaVA-Videoå±•ç¤ºäº†å‡ºè‰²çš„æ•ˆç‡ï¼Œä»…åœ¨8ä¸ªA100-40G GPUä¸Šè®­ç»ƒä¸€å¤©å³å¯è¾¾åˆ°æ•ˆæœã€‚å®ƒåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†å‡ ä¸ªç°æœ‰çš„70äº¿å‚æ•°æ¨¡å‹ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œä¸ºæœªæ¥è½»é‡çº§è§†é¢‘ç†è§£æ¨¡å‹çš„ç ”ç©¶æä¾›äº†å®è´µçš„åŸºçŸ³ã€‚ä»£ç å’Œæƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhangXJ199/TinyLLaVA-Videoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15513v2">PDF</a> code and training recipes are available at   <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video">https://github.com/ZhangXJ199/TinyLLaVA-Video</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§è§†é¢‘ç†è§£æ¨¡å‹TinyLLaVA-Videoï¼Œå…·æœ‰çº¦3.6Bå‚æ•°ï¼Œé€šè¿‡è§†é¢‘çº§ç»„é‡é‡‡æ ·å™¨å‡å°‘è§†è§‰ä»¤ç‰Œæ•°é‡ï¼Œæé«˜æ—¶é—´ç†è§£èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè®­ç»ƒæ•ˆç‡é«˜ï¼Œä»…ä¸€å¤©å³å¯å®Œæˆè®­ç»ƒï¼Œå¹¶ä¸”æºä»£ç å’Œæƒé‡å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TinyLLaVA-Videoæ˜¯ä¸€ç§è½»é‡çº§è§†é¢‘ç†è§£æ¨¡å‹ï¼Œå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨è§†é¢‘çº§ç»„é‡é‡‡æ ·å™¨æŠ€æœ¯ï¼Œæé«˜æ—¶é—´ç†è§£èƒ½åŠ›ã€‚</li>
<li>TinyLLaVA-Videoå¯æœ‰æ•ˆå¤„ç†é•¿è§†é¢‘åºåˆ—ã€‚</li>
<li>æ¨¡å‹ä»…éœ€è¦çº¦ä¸€å¤©çš„è®­ç»ƒæ—¶é—´ï¼Œå…·æœ‰è¾ƒé«˜çš„æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶ä»–å…·æœ‰7Bå‚æ•°çš„æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å…·æœ‰å…¬å¼€çš„æºä»£ç å’Œæƒé‡ï¼Œä¾¿äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69e54c7b70bceee670b964df8db59b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc865e4f1d11cdc81d17601e26e83fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fb181622d26fb6dd141d9ae75ca010.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b3ecd57ad6e4c0f047f79d249a4053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-652bbc77a292a61be98e8fb8e4d4247f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f0d7387c280e35c368d3aaf6466552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138b7716296b1edf95e981b87d090749.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0050cf88e0a6c0931bb1bbcaeb970c0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Understanding-Long-Videos-with-Multimodal-Language-Models"><a href="#Understanding-Long-Videos-with-Multimodal-Language-Models" class="headerlink" title="Understanding Long Videos with Multimodal Language Models"></a>Understanding Long Videos with Multimodal Language Models</h2><p><strong>Authors:Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo</strong></p>
<p>Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. Building on this, we explore injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos, and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establish its strong generality. Code: <a target="_blank" rel="noopener" href="https://github.com/kahnchana/mvu">https://github.com/kahnchana/mvu</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿å¾—æœ€è¿‘çš„åŸºäºLLMçš„æ–¹æ³•åœ¨é•¿æ—¶é—´è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬ç ”ç©¶äº†åŸºäºLLMçš„å¹¿æ³›ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›æ˜¯å¦‚ä½•å½±å“è¿™ä¸€å‡ºè‰²æ€§èƒ½çš„ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åŸºäºLLMçš„æ–¹æ³•åœ¨é•¿æ—¶é—´è§†é¢‘ä»»åŠ¡ä¸Šå¯ä»¥åœ¨æœ‰é™çš„è§†é¢‘ä¿¡æ¯ä¸‹è¾¾åˆ°å‡ºäººæ„æ–™çš„å‡†ç¡®æ€§ï¼Œæœ‰æ—¶ç”šè‡³ä¸éœ€è¦ç‰¹å®šçš„è§†é¢‘ä¿¡æ¯ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ¢ç´¢å°†è§†é¢‘ç‰¹å®šä¿¡æ¯æ³¨å…¥åŸºäºLLMçš„æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„è§†è§‰å·¥å…·ä»è§†é¢‘ä¸­æå–ä¸‰ç§ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ä¿¡æ¯æ¨¡å¼ï¼Œç„¶ååˆ©ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºèåˆè¿™äº›ä¿¡æ¯çš„åª’ä»‹ã€‚æˆ‘ä»¬æ„å»ºçš„å¤šåª’ä½“è§†é¢‘ç†è§£ï¼ˆMVUï¼‰æ¡†æ¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å…¶åœ¨æœºå™¨äººé¢†åŸŸä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°ä¹Ÿè¯æ˜äº†å…¶å¼ºå¤§çš„é€šç”¨æ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/kahnchana/mvu">https://github.com/kahnchana/mvu</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16998v5">PDF</a> 17 pages (main paper), 7 pages appendix. ICLR 2025 conference paper</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMä¸°å¯Œçš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›å¯¹å…¶åœ¨é•¿è§†é¢‘ä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°èµ·åˆ°å…³é”®ä½œç”¨ã€‚å³ä½¿åœ¨æœ‰é™çš„è§†é¢‘ä¿¡æ¯æˆ–æ— ç‰¹å®šè§†é¢‘ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒLLMæ–¹æ³•ä¹Ÿèƒ½è¾¾åˆ°ä»¤äººæƒŠè®¶çš„å‡†ç¡®åº¦ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶å›¢é˜Ÿå°è¯•å°†è§†é¢‘ç‰¹å®šä¿¡æ¯æ³¨å…¥LLMæ¡†æ¶ä¸­ã€‚é€šè¿‡ä½¿ç”¨ç°æˆçš„è§†è§‰å·¥å…·æå–è§†é¢‘ä¸­çš„ä¸‰ç§å¯¹è±¡ä¸­å¿ƒä¿¡æ¯æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºèåˆè¿™äº›ä¿¡æ¯çš„åª’ä»‹ï¼Œæ„å»ºäº†å¤šæ¨¡æ€è§†é¢‘ç†è§£ï¼ˆMVUï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨æœºå™¨äººé¢†åŸŸä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LLMçš„ä¸–ç•ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›å¯¹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>LLMæ–¹æ³•èƒ½åœ¨æœ‰é™æˆ–æ— ç‰¹å®šè§†é¢‘ä¿¡æ¯çš„æƒ…å†µä¸‹è¾¾åˆ°è‰¯å¥½çš„å‡†ç¡®åº¦ã€‚</li>
<li>æå‡ºå°†è§†é¢‘ç‰¹å®šä¿¡æ¯æ³¨å…¥LLMæ¡†æ¶çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ç°æˆçš„è§†è§‰å·¥å…·æå–è§†é¢‘ä¸­çš„å¯¹è±¡ä¸­å¿ƒä¿¡æ¯æ¨¡å¼ã€‚</li>
<li>åˆ©ç”¨è‡ªç„¶è¯­è¨€èåˆå¤šæ¨¡æ€è§†é¢‘ä¿¡æ¯ã€‚</li>
<li>å¤šæ¨¡æ€è§†é¢‘ç†è§£ï¼ˆMVUï¼‰æ¡†æ¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå¹¶åœ¨æœºå™¨äººé¢†åŸŸå…·æœ‰å¼ºå¤§çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93abd5bd0b15a9b2f4452d9bb422116c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05ca46d8fc08265ca05ea4d06422ea4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c43c1b1301de63902b6813e99b8e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-049a5b8f580a2f1377d20c3099855791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c860c416b474c556b6afc583263cdf6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-12fe27472e0dd270c46b783e5f85a091.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  PatchGuard Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a92a027269a964f051e7652c965bede3.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  MetricHMR Metric Human Mesh Recovery from Monocular Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
