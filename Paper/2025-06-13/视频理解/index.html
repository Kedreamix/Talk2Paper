<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-06-13  A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-13-更新"><a href="#2025-06-13-更新" class="headerlink" title="2025-06-13 更新"></a>2025-06-13 更新</h1><h2 id="A-Shortcut-aware-Video-QA-Benchmark-for-Physical-Understanding-via-Minimal-Video-Pairs"><a href="#A-Shortcut-aware-Video-QA-Benchmark-for-Physical-Understanding-via-Minimal-Video-Pairs" class="headerlink" title="A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs"></a>A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs</h2><p><strong>Authors:Benno Krojer, Mojtaba Komeili, Candace Ross, Quentin Garrido, Koustuv Sinha, Nicolas Ballas, Mahmoud Assran</strong></p>
<p>Existing benchmarks for assessing the spatio-temporal understanding and reasoning abilities of video language models are susceptible to score inflation due to the presence of shortcut solutions based on superficial visual or textual cues. This paper mitigates the challenges in accurately assessing model performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple shortcut-aware video QA benchmark for assessing the physical understanding of video language models. The benchmark is comprised of 55K high-quality multiple-choice video QA examples focusing on physical world understanding. Examples are curated from nine video data sources, spanning first-person egocentric and exocentric videos, robotic interaction data, and cognitive science intuitive physics benchmarks. To mitigate shortcut solutions that rely on superficial visual or textual cues and biases, each sample in MVP has a minimal-change pair – a visually similar video accompanied by an identical question but an opposing answer. To answer a question correctly, a model must provide correct answers for both examples in the minimal-change pair; as such, models that solely rely on visual or textual biases would achieve below random performance. Human performance on MVP is 92.9%, while the best open-source state-of-the-art video-language model achieves 40.2% compared to random performance at 25%. </p>
<blockquote>
<p>当前评估视频语言模型对时空的理解和推理能力的基准测试容易受到表面视觉或文本线索的捷径解决方案的影响，导致分数膨胀。本文通过引入最小视频对（MVP）基准测试来缓解准确评估模型性能的挑战，这是一个简单且能识别捷径的视频问答基准测试，用于评估视频语言模型对物理世界的理解。该基准测试包含5.5万个高质量的多项选择题视频问答示例，重点考察物理世界的理解。这些示例是从九个视频数据源中精选出来的，包括第一人称主观和客观视频、机器人交互数据和认知科学直觉物理基准测试。为了缓解依赖于表面视觉或文本线索和偏见的捷径解决方案，MVP中的每个样本都有一个最小变化的对——一个视觉相似的视频伴随一个相同的问题但答案相反。为了正确回答问题，模型必须为最小变化对中的两个示例都提供正确答案；因此，那些仅依赖于视觉或文本偏见的模型的表现将低于随机表现。人类在MVP上的表现是92.9%，而最佳的开源先进视频语言模型与随机表现的25%相比，达到了40.2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了现有的视频语言模型评估基准存在基于表面视觉或文本线索的捷径解决方案导致评分膨胀的问题。为解决此问题，本文引入了最小视频对（MVP）基准，这是一个简单的、能识别捷径的视频问答评估基准，用于评估视频语言模型对物理世界的理解。MVP包含来自九个视频数据源的高质量选择题，涵盖第一人称自我中心和外部中心视频、机器人交互数据和认知科学直觉物理基准测试。为缓解依赖表面视觉或文本线索和偏见的捷径解决方案，MVP中的每个样本都有一个最小变化对——一个视觉上相似的视频伴随一个相同的问题但答案相反。为了正确回答问题，模型必须为最小变化对中的两个例子都提供正确答案；因此，只依赖视觉或文本偏见的模型的表现将低于随机表现。人类在MVP上的表现是92.9%，而最佳的开源先进视频语言模型的表现是40.2%，相比之下随机表现是25%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有视频语言模型评估基准易受基于表面视觉或文本线索的捷径解决方案影响，导致评分膨胀。</li>
<li>引入最小视频对（MVP）基准，以评估视频语言模型对物理世界的理解。</li>
<li>MVP包含来自多个视频数据源的高质量选择题，涵盖多种视频类型。</li>
<li>MVP通过最小变化对设计来抑制基于表面视觉或文本线索的捷径解决方案。</li>
<li>正确回答问题需要模型在最小变化对中都提供正确答案，以此区分真正理解和依赖偏见的模型。</li>
<li>相比随机表现，人类在MVP上的表现较高，而当前最佳的视频语言模型表现仍低于随机表现。这表明现有模型在理解和推理方面还有提升空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09987">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a40150f7ccd33ca14f5fa2f9256ce26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f69c438fc11f30eede4b199f5dbf55e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e07f1bce7c38a7137a9759a645973da4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7635d15b552978158bf81194df6ab85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66964b08c5bd718512a85fc0a5039f4b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning"><a href="#V-JEPA-2-Self-Supervised-Video-Models-Enable-Understanding-Prediction-and-Planning" class="headerlink" title="V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning"></a>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction   and Planning</h2><p><strong>Authors:Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes,  Mojtaba,  Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, Nicolas Ballas</strong></p>
<p>A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world. </p>
<blockquote>
<p>现代人工智能面临的一个主要挑战是学习理解世界并通过观察进行行动。本文探索了一种自监督方法，该方法结合了互联网规模的视频数据和小量的交互数据（机器人轨迹），以开发能够在物理世界中理解、预测和规划模型。我们首先在互联网视频组成的视频和图像数据集上预训练了一种无动作联合嵌入预测架构V-JEPA 2，该数据集包含超过 1 亿小时的视频。V-JEPA 2在运动理解方面表现出强大的性能（Something-Something v2上达到了 77.3% 的Top-1准确率），并在人类动作预测方面达到了最新技术水平（Epic-Kitchens-100上的召回率为 39.7%）。此外，在与大型语言模型对齐后，我们在多个视频问答任务上展示了最先进的性能（例如，PerceptionTest上为 84.0%，TempCompass上为 76.9%）。最后，我们展示了如何通过利用来自Droid数据集的不到 62 小时的无标签机器人视频对潜在的动作条件世界模型V-JEPA 2-AC进行后训练，来将自监督学习应用于机器人规划任务。我们在两个不同的实验室部署了Frank双臂机器人零样本，并利用图像目标的规划实现了物体的抓取和放置。值得注意的是，这是在没有任何环境中机器人数据收集的情况下实现的，也不需要特定任务的训练或奖励。这项工作展示了如何从互联网规模的数据和小量的机器人交互数据中进行自监督学习，从而建立一个能够在物理世界中规划的世界模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09985v1">PDF</a> 48 pages, 19 figures</p>
<p><strong>摘要</strong><br>该论文探讨了一种自监督学习方法，该方法结合了互联网规模的视频数据和小量的交互数据（机器人轨迹），以开发能够在物理世界中理解、预测和规划模型的能力。首先，对无动作联合嵌入预测架构V-JEPA 2进行预训练，该架构在互联网视频图像数据集上进行训练，包含超过一百万小时的视频数据。V-JEPA 2在运动理解方面表现出强劲性能（在Something-Something v2上达到77.3的top-1准确率），并在人类动作预测方面达到最佳表现（在Epic-Kitchens-100上的召回率为39.7）。此外，在与大型语言模型对齐后，它在多个视频问答任务中表现出最佳性能（例如，在PerceptionTest上达到84.0，在TempCompass上达到76.9）。最后，该研究展示了如何将自监督学习应用于机器人规划任务，通过利用不到62小时的无人标注的机器人视频数据（来自Droid数据集）对潜在的动作条件世界模型V-JEPA 2-AC进行后训练。在两家实验室的Frank双臂上部署V-JEPA 2-AC零样本，并使用图像目标进行规划以实现物体的拾取和放置。值得注意的是，这是在没有任何机器人环境数据采集和任务特定训练或奖励的情况下实现的。这项工作证明了自监督学习可以从网络规模的数据和少量的机器人交互数据中构建能够规划物理世界的世界模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>本论文探讨了自监督学习方法在处理现代AI面临的大规模观察和理解的挑战中的实际应用价值。通过结合了互联网规模的视频数据和少量机器人交互数据来开发理解物理世界的模型。</li>
<li>研究采用了一种无动作联合嵌入预测架构V-JEPA 2，该架构在视频和图像数据集上进行了预训练，并在运动理解和人类动作预测方面表现出卓越性能。</li>
<li>通过与大型语言模型的结合，该模型在多个视频问答任务中实现了最佳性能。</li>
<li>研究展示了如何通过利用无人标注的机器人视频数据进行后训练来开发潜在的动作条件世界模型V-JEPA 2-AC。这一模型可以在不同实验室的机器人手臂上实现零样本部署，并执行物体拾取和放置的任务规划。</li>
<li>该研究强调了自监督学习的潜力，表明其能够从大规模网络数据和有限的机器人交互数据中学习物理世界的模型。这为未来AI系统的开发提供了新思路，尤其是在机器人技术和自动驾驶等领域。</li>
<li>本研究展示了在没有从机器人环境中收集任何数据的情况下，通过自监督学习实现机器人规划任务的可能性。这减少了数据收集的复杂性并提高了模型的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c20e7fd378bb75f6277bc2b568fab79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52726ea443acb63e798931278b8709a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c12be16346c246b398c8c3094751fe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb56100036aa6a8241569ba35d98df63.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VersaVid-R1-A-Versatile-Video-Understanding-and-Reasoning-Model-from-Question-Answering-to-Captioning-Tasks"><a href="#VersaVid-R1-A-Versatile-Video-Understanding-and-Reasoning-Model-from-Question-Answering-to-Captioning-Tasks" class="headerlink" title="VersaVid-R1: A Versatile Video Understanding and Reasoning Model from   Question Answering to Captioning Tasks"></a>VersaVid-R1: A Versatile Video Understanding and Reasoning Model from   Question Answering to Captioning Tasks</h2><p><strong>Authors:Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, Tieniu Tan</strong></p>
<p>Recent advancements in multimodal large language models have successfully extended the Reason-Then-Respond paradigm to image-based reasoning, yet video-based reasoning remains an underdeveloped frontier, primarily due to the scarcity of high-quality reasoning-oriented data and effective training methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA, two novel datasets specifically designed to stimulate the model’s advanced video understanding and reasoning abilities. DarkEventinfer presents videos with masked event segments, requiring models to infer the obscured content based on contextual video cues. MixVidQA, on the other hand, presents interleaved video sequences composed of two distinct clips, challenging models to isolate and reason about one while disregarding the other. Leveraging these carefully curated training samples together with reinforcement learning guided by diverse reward functions, we develop VersaVid-R1, the first versatile video understanding and reasoning model under the Reason-Then-Respond paradigm capable of handling multiple-choice and open-ended question answering, as well as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1 significantly outperforms existing models across a broad spectrum of benchmarks, covering video general understanding, cognitive reasoning, and captioning tasks. </p>
<blockquote>
<p>最近的多模态大型语言模型的进步已经成功地将“Reason-Then-Respond”范式扩展到基于图像的理解，但基于视频的理解仍然是一个未充分开发的前沿领域，这主要是由于缺乏高质量、以推理为导向的数据和有效的训练方法论。为了弥补这一差距，我们推出了DarkEventInfer和MixVidQA两个专门设计的新数据集，以刺激模型对高级视频的理解和推理能力。DarkEventInfer呈现带有遮挡事件片段的视频，要求模型根据上下文视频线索推断遮挡内容。另一方面，MixVidQA呈现由两个不同片段组成的交织视频序列，挑战模型在忽略另一个片段的情况下，对一个片段进行隔离和推理。我们利用这些精心挑选的训练样本以及由多种奖励函数引导的强化学习，开发出VersaVid-R1，这是第一个在“Reason-Then-Respond”范式下的通用视频理解和推理模型，能够处理多项选择和开放式问答以及视频描述任务。大量实验表明，VersaVid-R1在广泛的基准测试中显著优于现有模型，涵盖视频一般理解、认知推理和描述任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09079v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了视频理解领域的新进展。由于缺少高质量推理导向的数据和有效的训练方法，视频推理仍然是一个未被充分开发的前沿。为了弥补这一差距，本文引入了DarkEventInfer和MixVidQA两个新数据集，刺激模型对高级视频理解和推理能力的需求。利用这些精心策划的训练样本以及通过不同的奖励函数引导的强化学习，开发出VersaVid-R1模型，它能够在“推理后回应”框架下处理多项选择和开放式问答以及视频描述任务。实验表明，VersaVid-R1在广泛的基准测试中显著优于现有模型，涵盖视频通用理解、认知推理和描述任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型的新进展已将“Reason-Then-Respond”范式扩展到基于图像的理解。然而，视频理解仍然是未被充分开发的前沿。</li>
<li>DarkEventInfer数据集推出，通过遮蔽视频事件片段要求模型基于上下文视频线索进行推断。</li>
<li>MixVidQA数据集推出，通过展示由两个不同片段组成的交错视频序列，挑战模型在忽略一个片段的同时对另一个片段进行隔离和推理的能力。</li>
<li>通过使用精心策划的训练样本和多种奖励函数引导的强化学习，开发出VersaVid-R1模型。</li>
<li>VersaVid-R1是首个在“Reason-Then-Respond”框架下实现多功能视频理解和推理的模型。</li>
<li>VersaVid-R1能够处理多项选择和开放式问答以及视频描述任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c6a3ea7bc8f4dae8dbc1a9993a4e14a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aeef6402f0608deb6cd34da0ba17cd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f5aa25cb4dc3948977bf49da1240493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b297e766810193c83d405cd55a058ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bee208b930c30363750143c4c0b0e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3301ca6be6862e8eaf2f9ad8c1d6c12d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Video-CoT-A-Comprehensive-Dataset-for-Spatiotemporal-Understanding-of-Videos-Based-on-Chain-of-Thought"><a href="#Video-CoT-A-Comprehensive-Dataset-for-Spatiotemporal-Understanding-of-Videos-Based-on-Chain-of-Thought" class="headerlink" title="Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of   Videos Based on Chain-of-Thought"></a>Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of   Videos Based on Chain-of-Thought</h2><p><strong>Authors:Shuyi Zhang, Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Pengwei Wang, Zhongyuan Wang, Hongxuan Ma, Shanghang Zhang</strong></p>
<p>Video content comprehension is essential for various applications, ranging from video analysis to interactive systems. Despite advancements in large-scale vision-language models (VLMs), these models often struggle to capture the nuanced, spatiotemporal details essential for thorough video analysis. To address this gap, we introduce Video-CoT, a groundbreaking dataset designed to enhance spatiotemporal understanding using Chain-of-Thought (CoT) methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal question-answer pairs and 23,000 high-quality CoT-annotated samples, providing a solid foundation for evaluating spatiotemporal understanding in video comprehension. Additionally, we provide a comprehensive benchmark for assessing these tasks, with each task featuring 750 images and tailored evaluation metrics. Our extensive experiments reveal that current VLMs face significant challenges in achieving satisfactory performance, high-lighting the difficulties of effective spatiotemporal understanding. Overall, the Video-CoT dataset and benchmark open new avenues for research in multimedia understanding and support future innovations in intelligent systems requiring advanced video analysis capabilities. By making these resources publicly available, we aim to encourage further exploration in this critical area. Project website:<a target="_blank" rel="noopener" href="https://video-cot.github.io/">https://video-cot.github.io/</a> . </p>
<blockquote>
<p>视频内容理解对于从视频分析到交互系统的各种应用至关重要。尽管大规模视觉语言模型（VLMs）已经取得了进展，但这些模型在捕捉全面视频分析所需的微妙时空细节方面仍面临挑战。为了解决这一差距，我们引入了Video-CoT，这是一个开创性的数据集，旨在利用思维链（CoT）方法增强时空理解。Video-CoT包含19万组精细的时空问答对和2.3万个高质量的CoT注释样本，为评估视频理解中的时空理解能力提供了坚实的基础。此外，我们还为评估这些任务提供了一个全面的基准测试，每个任务包含750张图像和专门的评估指标。我们的大量实验表明，目前的VLMs在取得令人满意的表现方面面临巨大挑战，突出了实现有效时空理解的困难。总的来说，Video-CoT数据集和基准测试为多媒体理解的研究开辟了新的途径，并支持未来需要高级视频分析功能的智能系统的创新。我们公开提供这些资源，旨在鼓励在这一关键领域进行进一步的探索。项目网站：<a target="_blank" rel="noopener" href="https://video-cot.github.io/%E3%80%82">https://video-cot.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08817v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视频内容理解在多个应用领域中至关重要，如视频分析与交互系统。尽管大规模视觉语言模型（VLMs）有所进展，但它们仍难以捕捉细致的空间时间细节，这些细节对于彻底的视频分析至关重要。为解决这一差距，我们推出了Video-CoT数据集，运用Chain-of-Thought（CoT）方法增强时空理解。Video-CoT包含19.2万精细时空问答对和2.3万高质量CoT注释样本，为评估视频理解中的时空理解提供了坚实基础。我们还为每个任务提供了全面的基准测试，每个任务包含750张图像和定制评估指标。实验表明，当前VLMs在取得令人满意的效果方面面临重大挑战，突显了有效时空理解的困难。总的来说，Video-CoT数据集和基准测试为多媒体理解开辟了新途径，并支持未来需要先进视频分析能力的智能系统的创新。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频内容理解在多个领域具有重要性，如视频分析和交互系统。</li>
<li>尽管视觉语言模型有所进展，但仍存在捕捉细致时空细节的挑战。</li>
<li>Video-CoT数据集通过运用Chain-of-Thought（CoT）方法增强时空理解。</li>
<li>Video-CoT包含大量精细时空问答对和CoT注释样本。</li>
<li>提供了全面的基准测试来评估视频理解中的时空理解。</li>
<li>当前VLMs在视频理解方面面临挑战，需要更有效的算法来提升性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08817">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-db01653d38fb5acc6cfd62eae67b5916.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5890d348e7b15a99917844b979f04f49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81e6af03fee934f6cbef5b505675218d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82ebbb3343e73c8d3c6ebffc462ac9b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d0df7409820e50a90e275f3845979f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TinyLLaVA-Video-Towards-Smaller-LMMs-for-Video-Understanding-with-Group-Resampler"><a href="#TinyLLaVA-Video-Towards-Smaller-LMMs-for-Video-Understanding-with-Group-Resampler" class="headerlink" title="TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group   Resampler"></a>TinyLLaVA-Video: Towards Smaller LMMs for Video Understanding with Group   Resampler</h2><p><strong>Authors:Xingjian Zhang, Xi Weng, Yihao Yue, Zhaoxin Fan, Wenjun Wu, Lei Huang</strong></p>
<p>Video behavior recognition and scene understanding are fundamental tasks in multimodal intelligence, serving as critical building blocks for numerous real-world applications. Through large multimodal models (LMMs) have achieved remarkable progress in video understanding, most existing open-source models rely on over 7B parameters and require large-scale datasets for training, making them resource-intensive and inaccessible to many researchers. Furthermore, lightweight models face persistent challenges in effectively processing long visual sequences and temporal understanding. In this work, we introduce TinyLLaVA-Video, a lightweight yet powerful video understanding model with approximately 3.6B parameters. The cornerstone of our design is the video-level group resampler, a novel mechanism that significantly reduces and controls the number of visual tokens at the video level. Unlike traditional image-level resampler, our approach effectively mitigates redundancy while enhancing temporal comprehension, leading to improved performance on video-based tasks. In addition, TinyLLaVA-Video demonstrates exceptional efficiency, requiring only one day of training on 8 A100-40G GPUs. It surpasses several existing 7B-parameter models on multiple benchmarks. We believe this work provides a valuable foundation for future research on lightweight video understanding models. The code and weights is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video">https://github.com/ZhangXJ199/TinyLLaVA-Video</a>. </p>
<blockquote>
<p>视频行为识别和场景理解是多模态智能中的基本任务，是众多现实世界应用的关键构建模块。尽管通过大型多模态模型（LMMs）在视频理解方面取得了显著的进展，但大多数现有的开源模型依赖于超过70亿个参数，并需要大规模数据集进行训练，这使得它们资源密集，许多研究者无法接触。此外，轻量级模型在处理长视频序列和时序理解方面持续面临挑战。在这项工作中，我们介绍了TinyLLaVA-Video，这是一个轻量级但功能强大的视频理解模型，拥有约3.6亿个参数。设计的核心是视频级组重采样器，这是一种新颖的机制，可以在视频级别显著减少和控制视觉标记的数量。与传统的图像级重采样器不同，我们的方法有效地减轻了冗余，同时提高了时序理解能力，在基于视频的任务上实现了性能提升。此外，TinyLLaVA-Video展示了出色的效率，仅在8个A100-40G GPU上训练一天即可达到效果。它在多个基准测试上超越了几个现有的70亿参数模型。我们相信这项工作为未来轻量级视频理解模型的研究提供了宝贵的基石。代码和权重可在<a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhangXJ199/TinyLLaVA-Video找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15513v2">PDF</a> code and training recipes are available at   <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/TinyLLaVA-Video">https://github.com/ZhangXJ199/TinyLLaVA-Video</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种轻量级视频理解模型TinyLLaVA-Video，具有约3.6B参数，通过视频级组重采样器减少视觉令牌数量，提高时间理解能力，在多个基准测试中表现优异，训练效率高，仅一天即可完成训练，并且源代码和权重已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TinyLLaVA-Video是一种轻量级视频理解模型，具有优秀的性能。</li>
<li>模型采用视频级组重采样器技术，提高时间理解能力。</li>
<li>TinyLLaVA-Video可有效处理长视频序列。</li>
<li>模型仅需要约一天的训练时间，具有较高的效率。</li>
<li>模型在多个基准测试中超越了其他具有7B参数的模型。</li>
<li>模型具有公开的源代码和权重，便于研究人员使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-69e54c7b70bceee670b964df8db59b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc865e4f1d11cdc81d17601e26e83fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fb181622d26fb6dd141d9ae75ca010.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b3ecd57ad6e4c0f047f79d249a4053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-652bbc77a292a61be98e8fb8e4d4247f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f0d7387c280e35c368d3aaf6466552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138b7716296b1edf95e981b87d090749.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0050cf88e0a6c0931bb1bbcaeb970c0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Understanding-Long-Videos-with-Multimodal-Language-Models"><a href="#Understanding-Long-Videos-with-Multimodal-Language-Models" class="headerlink" title="Understanding Long Videos with Multimodal Language Models"></a>Understanding Long Videos with Multimodal Language Models</h2><p><strong>Authors:Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo</strong></p>
<p>Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. Building on this, we explore injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos, and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establish its strong generality. Code: <a target="_blank" rel="noopener" href="https://github.com/kahnchana/mvu">https://github.com/kahnchana/mvu</a> </p>
<blockquote>
<p>大型语言模型（LLM）使得最近的基于LLM的方法在长时间视频理解基准测试上取得了卓越的性能。我们研究了基于LLM的广泛世界知识和强大的推理能力是如何影响这一出色性能的。令人惊讶的是，我们发现基于LLM的方法在长时间视频任务上可以在有限的视频信息下达到出人意料的准确性，有时甚至不需要特定的视频信息。在此基础上，我们探索将视频特定信息注入基于LLM的框架中。我们使用现成的视觉工具从视频中提取三种以对象为中心的信息模式，然后利用自然语言作为融合这些信息的媒介。我们构建的多媒体视频理解（MVU）框架在多个视频理解基准测试中表现出卓越的性能。其在机器人领域任务上的出色表现也证明了其强大的通用性。代码地址：<a target="_blank" rel="noopener" href="https://github.com/kahnchana/mvu">https://github.com/kahnchana/mvu</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16998v5">PDF</a> 17 pages (main paper), 7 pages appendix. ICLR 2025 conference paper</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在视频理解领域表现出卓越性能，尤其是在长视频理解基准测试中。研究发现，LLM丰富的世界知识和强大的推理能力对其在长视频任务上的出色表现起到关键作用。即使在有限的视频信息或无特定视频信息的情况下，LLM方法也能达到令人惊讶的准确度。在此基础上，研究团队尝试将视频特定信息注入LLM框架中。通过使用现成的视觉工具提取视频中的三种对象中心信息模式，并利用自然语言作为融合这些信息的媒介，构建了多模态视频理解（MVU）框架。该框架在多个视频理解基准测试中表现出卓越性能，并在机器人领域任务中展现出强大的通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在长视频理解任务中表现优异。</li>
<li>LLM的世界知识和推理能力对性能有重要影响。</li>
<li>LLM方法能在有限或无特定视频信息的情况下达到良好的准确度。</li>
<li>提出将视频特定信息注入LLM框架的方法。</li>
<li>通过使用现成的视觉工具提取视频中的对象中心信息模式。</li>
<li>利用自然语言融合多模态视频信息。</li>
<li>多模态视频理解（MVU）框架在多个视频理解基准测试中表现卓越，并在机器人领域具有强大的通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93abd5bd0b15a9b2f4452d9bb422116c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05ca46d8fc08265ca05ea4d06422ea4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c43c1b1301de63902b6813e99b8e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-049a5b8f580a2f1377d20c3099855791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c860c416b474c556b6afc583263cdf6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-12fe27472e0dd270c46b783e5f85a091.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-06-13  PatchGuard Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a92a027269a964f051e7652c965bede3.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-13  MetricHMR Metric Human Mesh Recovery from Monocular Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
