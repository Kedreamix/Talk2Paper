<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-06-13  PatchGuard Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-12fe27472e0dd270c46b783e5f85a091.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-13-更新"><a href="#2025-06-13-更新" class="headerlink" title="2025-06-13 更新"></a>2025-06-13 更新</h1><h2 id="PatchGuard-Adversarially-Robust-Anomaly-Detection-and-Localization-through-Vision-Transformers-and-Pseudo-Anomalies"><a href="#PatchGuard-Adversarially-Robust-Anomaly-Detection-and-Localization-through-Vision-Transformers-and-Pseudo-Anomalies" class="headerlink" title="PatchGuard: Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies"></a>PatchGuard: Adversarially Robust Anomaly Detection and Localization   through Vision Transformers and Pseudo Anomalies</h2><p><strong>Authors:Mojtaba Nafez, Amirhossein Koochakian, Arad Maleki, Jafar Habibi, Mohammad Hossein Rohban</strong></p>
<p>Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields that demand high reliability, such as medical imaging and industrial monitoring. However, current AD and AL approaches are often susceptible to adversarial attacks due to limitations in training data, which typically include only normal, unlabeled samples. This study introduces PatchGuard, an adversarially robust AD and AL method that incorporates pseudo anomalies with localization masks within a Vision Transformer (ViT)-based architecture to address these vulnerabilities. We begin by examining the essential properties of pseudo anomalies, and follow it by providing theoretical insights into the attention mechanisms required to enhance the adversarial robustness of AD and AL systems. We then present our approach, which leverages Foreground-Aware Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware methods. Our method incorporates these crafted pseudo-anomaly samples into a ViT-based framework, with adversarial training guided by a novel loss function designed to improve model robustness, as supported by our theoretical analysis. Experimental results on well-established industrial and medical datasets demonstrate that PatchGuard significantly outperforms previous methods in adversarial settings, achieving performance gains of $53.2%$ in AD and $68.5%$ in AL, while also maintaining competitive accuracy in non-adversarial settings. The code repository is available at <a target="_blank" rel="noopener" href="https://github.com/rohban-lab/PatchGuard">https://github.com/rohban-lab/PatchGuard</a> . </p>
<blockquote>
<p>异常检测（AD）和异常定位（AL）在高可靠性需求的领域，如医学影像和工业监控中，具有至关重要的作用。然而，当前的AD和AL方法由于训练数据的局限性（通常只有正常的未标记样本），很容易受到对抗性攻击。本研究引入了PatchGuard，这是一种对抗性稳健的AD和AL方法，它结合了伪异常和定位掩码在视觉转换器（ViT）架构中解决这些漏洞。我们首先研究伪异常的基本属性，然后通过理论洞察注意机制，增强AD和AL系统的对抗性稳健性。然后我们提出的方法利用了前景感知伪异常来克服之前异常感知方法的不足。我们的方法将这些精心制作的伪异常样本纳入基于ViT的框架中，通过对抗训练由我们理论分析结果支持的新型损失函数来指导模型提高稳健性。在公认的工业和医疗数据集上的实验结果表明，PatchGuard在对抗环境中显著优于以前的方法，在AD上实现了高达53.2％的性能提升，在AL上实现了高达68.5％的性能提升，同时在非对抗环境中也保持了较高的准确性。代码仓库可在<a target="_blank" rel="noopener" href="https://github.com/rohban-lab/PatchGuard%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rohban-lab/PatchGuard找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09237v1">PDF</a> Accepted to the Conference on Computer Vision and Pattern Recognition   (CVPR) 2025</p>
<p><strong>Summary</strong><br>在医学成像和工业监控等需要高可靠性的领域，异常检测（AD）和异常定位（AL）尤为重要。然而，现有的AD和AL方法容易受到对抗性攻击的干扰，其训练数据仅限于正常未标记样本的局限性是主要诱因。本研究引入PatchGuard，一种基于Vision Transformer（ViT）架构的对抗性稳健AD和AL方法，通过融入伪异常和定位掩膜来应对这些漏洞。本研究首先探讨伪异常的关键属性，接着从理论层面研究增强AD和AL系统对抗性稳健性所需的注意力机制。然后提出一种利用前景感知伪异常的方法，以克服现有异常感知方法的不足。该方法将这些精心制作的伪异常样本融入ViT框架，通过新型损失函数进行对抗训练来提升模型稳健性，并由理论分析结果提供支持。在成熟的工业和医疗数据集上的实验表明，PatchGuard在对抗环境中显著优于先前的方法，在AD和AL中分别提高了53.2%和68.5%的性能，同时在非对抗环境中也保持了竞争力。相关代码库可通过<a target="_blank" rel="noopener" href="https://github.com/rohban-lab/PatchGuard%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/rohban-lab/PatchGuard访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AD和AL方法在高可靠性领域的重要性。</li>
<li>当前AD和AL方法存在因训练数据局限性易受对抗性攻击的缺陷。</li>
<li>PatchGuard是一种基于Vision Transformer的对抗性稳健AD和AL方法。</li>
<li>PatchGuard通过融入伪异常和定位掩膜来应对漏洞并提升模型性能。</li>
<li>该方法通过新型损失函数进行对抗训练来提升模型稳健性。</li>
<li>在工业和医疗数据集上的实验证明PatchGuard显著优于先前的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b26d50eded7c5f46fec8065d9aba2762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb240ce2885a425462ef83c434ef3e38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6c82a47e2ab7cd1a3f2ef76ac14f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-801e280f4f63cdfbebd255667d113bf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6843c8242968b31ca5c0e3f31be9df14.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cross-Spectral-Body-Recognition-with-Side-Information-Embedding-Benchmarks-on-LLCM-and-Analyzing-Range-Induced-Occlusions-on-IJB-MDF"><a href="#Cross-Spectral-Body-Recognition-with-Side-Information-Embedding-Benchmarks-on-LLCM-and-Analyzing-Range-Induced-Occlusions-on-IJB-MDF" class="headerlink" title="Cross-Spectral Body Recognition with Side Information Embedding:   Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF"></a>Cross-Spectral Body Recognition with Side Information Embedding:   Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF</h2><p><strong>Authors:Anirudh Nanduri, Siyuan Huang, Rama Chellappa</strong></p>
<p>Vision Transformers (ViTs) have demonstrated impressive performance across a wide range of biometric tasks, including face and body recognition. In this work, we adapt a ViT model pretrained on visible (VIS) imagery to the challenging problem of cross-spectral body recognition, which involves matching images captured in the visible and infrared (IR) domains. Recent ViT architectures have explored incorporating additional embeddings beyond traditional positional embeddings. Building on this idea, we integrate Side Information Embedding (SIE) and examine the impact of encoding domain and camera information to enhance cross-spectral matching. Surprisingly, our results show that encoding only camera information - without explicitly incorporating domain information - achieves state-of-the-art performance on the LLCM dataset. While occlusion handling has been extensively studied in visible-spectrum person re-identification (Re-ID), occlusions in visible-infrared (VI) Re-ID remain largely underexplored - primarily because existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly feature full-body, unoccluded images. To address this gap, we analyze the impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared images captured at various distances, enabling cross-range, cross-spectral evaluations. </p>
<blockquote>
<p>视觉Transformer（ViT）在各种生物识别任务中表现出令人印象深刻的效果，包括面部和身体识别。在这项工作中，我们将预训练在可见（VIS）图像上的ViT模型应用于跨光谱身体识别这一具有挑战性的问题，该问题涉及匹配可见和红外（IR）领域捕获的图像。最近的ViT架构已经探索了除了传统位置嵌入之外，还融入额外的嵌入。基于这一想法，我们整合了Side Information Embedding（SIE），并研究编码领域和相机信息对增强跨光谱匹配的影响。令人惊讶的是，我们的结果表明，仅编码相机信息——而不显式地融入领域信息——就能在LLCM数据集上实现最先进的性能。虽然可见光谱中的人再识别（Re-ID）中的遮挡处理已经得到了广泛的研究，但可见光与红外（VI）Re-ID中的遮挡仍然在很大程度上被忽视——主要是因为现有的VI-ReID数据集，如LLCM、SYSU-MM01和RegDB，主要以全身、无遮挡的图像为主。为了弥补这一差距，我们使用IARPA Janus Benchmark Multi-Domain Face（IJB-MDF）数据集分析了距离引起的遮挡范围的影响，该数据集提供了在各种距离捕获的多样化的可见和红外图像集，可实现跨范围、跨光谱评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08953v1">PDF</a> </p>
<p><strong>Summary</strong><br>     视觉转换器（ViT）在生物识别任务中表现出卓越性能，包括面部和人体识别。本研究将预训练的ViT模型应用于跨光谱人体识别问题，通过整合Side Information Embedding（SIE）并编码域和相机信息以增强跨光谱匹配效果。令人惊讶的是，仅编码相机信息即实现了LLCM数据集上的领先水平，无需显式纳入域信息。虽然可见光谱的人体再识别已广泛研究遮挡问题，但可见-红外（VI）再识别的遮挡问题仍大多未被探讨。主要因为现有VI-ReID数据集如LLCM、SYSU-MM01和RegDB主要展示全身、无遮挡图像。为解决这一差距，本研究利用IJB-MDF数据集分析距离引起的遮挡问题，该数据集提供不同距离捕捉的可见和红外图像，实现跨范围、跨光谱评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViT模型在生物识别任务中表现优异，包括面部和人体识别。</li>
<li>预训练的ViT模型被应用于跨光谱人体识别问题。</li>
<li>Side Information Embedding（SIE）的整合增强了跨光谱匹配的效果。</li>
<li>仅编码相机信息即可实现先进性能，无需显式纳入域信息。</li>
<li>可见-红外再识别的遮挡问题仍是研究中的一大空白。</li>
<li>现有VI-ReID数据集主要展示无遮挡的全身图像。</li>
<li>利用IJB-MDF数据集分析距离引起的遮挡问题，实现跨范围、跨光谱评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08953">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a849eeed08eba4a6b81d86aa00c3aeaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-081aa1451184137e980c012b8d4906a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3d6c0867f711640d4b84329bf787f11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3388e3cdafcf4ff5db88b45d03b21d36.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SSS-Semi-Supervised-SAM-2-with-Efficient-Prompting-for-Medical-Imaging-Segmentation"><a href="#SSS-Semi-Supervised-SAM-2-with-Efficient-Prompting-for-Medical-Imaging-Segmentation" class="headerlink" title="SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging   Segmentation"></a>SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging   Segmentation</h2><p><strong>Authors:Hongjie Zhu, Xiwei Liu, Rundong Xue, Zeyu Zhang, Yong Xu, Daji Ergu, Ying Cai, Yang Zhao</strong></p>
<p>In the era of information explosion, efficiently leveraging large-scale unlabeled data while minimizing the reliance on high-quality pixel-level annotations remains a critical challenge in the field of medical imaging. Semi-supervised learning (SSL) enhances the utilization of unlabeled data by facilitating knowledge transfer, significantly improving the performance of fully supervised models and emerging as a highly promising research direction in medical image analysis. Inspired by the ability of Vision Foundation Models (e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised SAM-2), a novel approach that leverages SAM-2’s robust feature extraction capabilities to uncover latent knowledge in unlabeled medical images, thus effectively enhancing feature support for fully supervised medical image segmentation. Specifically, building upon the single-stream “weak-to-strong” consistency regularization framework, this paper introduces a Discriminative Feature Enhancement (DFE) mechanism to further explore the feature discrepancies introduced by various data augmentation strategies across multiple views. By leveraging feature similarity and dissimilarity across multi-scale augmentation techniques, the method reconstructs and models the features, thereby effectively optimizing the salient regions. Furthermore, a prompt generator is developed that integrates Physical Constraints with a Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data, fulfilling SAM-2’s requirement for additional prompts. Extensive experiments demonstrate the superiority of the proposed method for semi-supervised medical image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably, SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous state-of-the-art method by +3.65 Dice. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/SSS">https://github.com/AIGeeksGroup/SSS</a>. </p>
<blockquote>
<p>在信息爆炸时代，如何有效利用大规模的无标签数据，同时尽量减少对高质量像素级注释的依赖，仍然是医学成像领域的一个关键挑战。半监督学习（SSL）通过促进知识迁移，提高了无标签数据的利用率，显著提升了全监督模型的表现，并成为医学图像分析中一个极具前景的研究方向。受Vision Foundation Models（例如SAM-2）提供丰富先验知识的能力的启发，我们提出了SSS（Semi-Supervised SAM-2），这是一种利用SAM-2的稳健特征提取能力来挖掘无标签医学图像中潜在知识的新方法，从而有效增强全监督医学图像分割的特征支持。具体而言，该论文基于单流“弱到强”的一致性正则化框架，引入判别特征增强（DFE）机制，以进一步探索由多种数据增强策略在不同视图下引入的特征差异。通过利用多尺度增强技术中的特征相似性和不相似性，该方法重构并建模特征，从而有效地优化显著区域。此外，还开发了一个提示生成器，该生成器结合了物理约束和滑动窗口（PCSW）机制，以为无标签数据生成输入提示，满足SAM-2对额外提示的要求。大量实验表明，该方法在ACDC和BHSD两个多标签数据集上进行半监督医学图像分割时表现优异。值得注意的是，SSS在BHSD上取得了平均Dice系数为53.15的成绩，超过了之前的最先进方法+3.65 Dice。代码将在<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/SSS%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIGeeksGroup/SSS上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08949v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大数据时代，如何利用大量未标注数据并减少对高质量像素级标注的依赖是医学影像领域的一大挑战。半监督学习（SSL）通过促进知识迁移，提高了未标注数据的利用率，显著提升了全监督模型的表现，成为医学影像分析中极具前景的研究方向。本研究受Vision Foundation Models（如SAM-2）提供丰富先验知识的启发，提出了SSS（Semi-Supervised SAM-2）新方法，利用SAM-2的稳健特征提取能力，挖掘未标注医学影像中的潜在知识，从而增强全监督医学影像分割的特征支持。研究在单流“弱到强”一致性正则化框架的基础上，引入判别性特征增强（DFE）机制，进一步探索各种数据增强策略在不同视图下引入的特征差异。通过利用多尺度增强技术的特征相似性和差异性来重建和建模特征，从而有效优化显著区域。此外，研究还开发了结合物理约束与滑动窗口（PCSW）机制的提示生成器，为未标注数据生成输入提示，满足SAM-2对额外提示的要求。在ACDC和BHSD两个多标签数据集上的实验表明，该方法在半监督医学影像分割上具有优越性。特别是在BHSD数据集上，SSS的平均Dice得分达到53.15，超越了之前的最先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在大数据时代，半监督学习（SSL）在医学图像分析中具有高度前景，能利用大量未标注数据并提升全监督模型性能。</li>
<li>SSS方法利用SAM-2的稳健特征提取能力，有效挖掘未标注医学图像中的潜在知识。</li>
<li>研究引入判别性特征增强（DFE）机制，探索数据增强策略下的特征差异。</li>
<li>通过结合物理约束与滑动窗口（PCSW）机制的提示生成器，满足SAM-2对额外提示的需求。</li>
<li>在ACDC和BHSD数据集上的实验表明SSS方法在半监督医学图像分割上的优越性。</li>
<li>SSS在BHSD数据集上的平均Dice得分达到53.15，显著超越先前的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08949">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-831ccab936dc1d475b89c04f76fa3388.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-638d89e25fc2ef54d2512d9854df2753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fc74b469aa219e98d8c357d41f16d7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Time-Series-Representations-for-Classification-Lie-Hidden-in-Pretrained-Vision-Transformers"><a href="#Time-Series-Representations-for-Classification-Lie-Hidden-in-Pretrained-Vision-Transformers" class="headerlink" title="Time Series Representations for Classification Lie Hidden in Pretrained   Vision Transformers"></a>Time Series Representations for Classification Lie Hidden in Pretrained   Vision Transformers</h2><p><strong>Authors:Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata</strong></p>
<p>Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal yet another direction for reusing vision representations in a non-visual domain. </p>
<blockquote>
<p>时间序列分类是医疗和工业中的基本任务，然而，时间序列基础模型（TSFMs）的发展受到公开可用时间序列数据集稀缺的限制。在这项工作中，我们提出了时间序列视觉转换器（TiViT），这是一个将时间序列转换为图像框架，以利用在大型图像数据集上预训练的冻结视觉转换器（ViTs）的表示能力。首先，我们通过分析用于时间序列的ViTs的2D补丁，从理论上论证了我们的方法，表明它可以增加标签相关令牌的数量并降低样本复杂性。其次，我们通过实证表明，TiViT利用大型OpenCLIP模型的隐藏表示，在标准时间序列分类基准测试上实现了最先进的性能。我们探索了TiViT表示的结构，并发现具有高内在维度的中间层对时间序列分类最为有效。最后，我们评估了TiViT和TSFM表示空间的对齐性，并发现了强烈的互补性，通过结合它们的特征可以实现进一步的性能提升。我们的研究揭示了在非视觉领域重新使用时间序列表示的另一个方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08641v1">PDF</a> </p>
<p><strong>Summary</strong><br>时间序列分类在医疗和工业中是一项基础任务，但由于缺乏公开可用的时间序列数据集，时间序列基础模型（TSFMs）的发展受到限制。本研究提出了时间视觉转换器（TiViT），它将时间序列转换为图像，以利用在大型图像数据集上预训练的冻结视觉转换器（ViTs）的表示能力。本研究从理论角度分析了ViTs用于时间序列的2D补丁，证明了其优势。此外，TiViT在标准时间序列分类基准测试中实现了卓越性能。研究发现，具有高内在维度的中间层对时间序列分类最为有效。最后，评估了TiViT和TSFM表示空间的契合度，发现二者具有很强的互补性，结合其特征可实现进一步的性能提升。本研究揭示了将视觉表示用于非视觉领域的另一方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列分类是医疗和工业中的关键任务，受限于公共数据集的可获取性。</li>
<li>提出了时间视觉转换器（TiViT）框架，能将时间序列转换为图像并利用视觉转换器的表示能力。</li>
<li>理论分析证明了使用ViTs进行时间序列处理的优越性，表现在增加标签相关标记的数量并降低样本复杂性。</li>
<li>在标准时间序列分类基准测试中，TiViT取得了最佳性能表现。</li>
<li>研究发现具有高内在维度的中间层对于时间序列分类最为有效。</li>
<li>TiViT和现有时间序列基础模型（TSFMs）在表示空间上具有强互补性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08641">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33644056e5179673b7a5387c3b427793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68dabd349b0edca2710bdadfa4b1e97a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d5db1d05b0f73808e26833de76f865f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f0fc2851b59daa19fb025977333b987.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d611405351f06e3c06b686cddb70717d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8e820857fde1ba60e882ba2b8843b15.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Open-World-Scene-Graph-Generation-using-Vision-Language-Models"><a href="#Open-World-Scene-Graph-Generation-using-Vision-Language-Models" class="headerlink" title="Open World Scene Graph Generation using Vision Language Models"></a>Open World Scene Graph Generation using Vision Language Models</h2><p><strong>Authors:Amartya Dutta, Kazi Sajeed Mehrab, Medha Sawhney, Abhilash Neog, Mridul Khurana, Sepideh Fatemi, Aanish Pradhan, M. Maruf, Ismini Lourentzou, Arka Daw, Anuj Karpatne</strong></p>
<p>Scene-Graph Generation (SGG) seeks to recognize objects in an image and distill their salient pairwise relationships. Most methods depend on dataset-specific supervision to learn the variety of interactions, restricting their usefulness in open-world settings, involving novel objects and&#x2F;or relations. Even methods that leverage large Vision Language Models (VLMs) typically require benchmark-specific fine-tuning. We introduce Open-World SGG, a training-free, efficient, model-agnostic framework that taps directly into the pretrained knowledge of VLMs to produce scene graphs with zero additional learning. Casting SGG as a zero-shot structured-reasoning problem, our method combines multimodal prompting, embedding alignment, and a lightweight pair-refinement strategy, enabling inference over unseen object vocabularies and relation sets. To assess this setting, we formalize an Open-World evaluation protocol that measures performance when no SGG-specific data have been observed either in terms of objects and relations. Experiments on Visual Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate the capacity of pretrained VLMs to perform relational understanding without task-level training. </p>
<blockquote>
<p>场景图生成（SGG）旨在识别图像中的对象并提炼它们之间的重要二元关系。大多数方法都依赖于特定数据集进行互动学习，这在涉及新对象或新关系的开放世界环境中限制了其效用。即使利用大型视觉语言模型（VLMs）的方法也需要针对基准测试进行特定微调。我们引入了开放世界SGG，这是一个无训练、高效、模型通用的框架，它直接利用预训练的VLM知识生成场景图，无需额外学习。将SGG视为零射式结构化推理问题，我们的方法结合了多模式提示、嵌入对齐和轻量级配对优化策略，能够对未见过的对象词汇和关系集进行推断。为了评估这一设置，我们制定了开放世界评估协议，以衡量在没有观察到SGG特定数据的情况下在对象和关系方面的性能。在视觉基因组、开放图像V6和全景场景图（PSG）数据集上的实验证明了预训练VLM进行关系理解的能力，无需任务级训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08189v1">PDF</a> Accepted in CVPR 2025 Workshop (CVinW)</p>
<p><strong>Summary</strong><br>     本研究提出Open-World SGG框架，这是一个无需训练、高效且模型通用的框架，可直接利用预训练的视觉语言模型（VLMs）知识生成场景图。通过将场景图生成（SGG）视为零样本结构化推理问题，结合多模态提示、嵌入对齐和轻量级对偶精炼策略，实现未在对象词汇和关系集上见过的新对象的推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-World SGG框架允许模型在无需特定数据集监督的情况下识别图像中的对象及其重要关系。</li>
<li>该框架利用预训练的视觉语言模型（VLMs）知识，无需额外学习即可生成场景图。</li>
<li>通过将场景图生成（SGG）视为零样本结构化推理问题，该框架能够处理未见过的对象词汇和关系集。</li>
<li>多模态提示、嵌入对齐和轻量级对偶精炼策略的结合，增强了模型的推理能力。</li>
<li>提出的Open-World评估协议用于衡量在对象和关系上未观察过的情况下模型的性能。</li>
<li>在Visual Genome、Open Images V6和Panoptic Scene Graph数据集上的实验证明了预训练VLMs进行关系理解的能力，而无需任务级别训练。</li>
<li>该框架具有广泛的应用前景，特别是在开放世界环境中处理新型对象和关系时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08189">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2882567fd0228be296243926301e3303.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73e4ce26820bb526aa1ace892adaf45a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df45cbf2e8008457402510844280c88b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CHOSEN-Compilation-to-Hardware-Optimization-Stack-for-Efficient-Vision-Transformer-Inference"><a href="#CHOSEN-Compilation-to-Hardware-Optimization-Stack-for-Efficient-Vision-Transformer-Inference" class="headerlink" title="CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision   Transformer Inference"></a>CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision   Transformer Inference</h2><p><strong>Authors:Mohammad Erfan Sadeghi, Arash Fayyazi, Suhas Somashekar, Armin Abdollahi, Massoud Pedram</strong></p>
<p>Vision Transformers (ViTs) represent a groundbreaking shift in machine learning approaches to computer vision. Unlike traditional approaches, ViTs employ the self-attention mechanism, which has been widely used in natural language processing, to analyze image patches. Despite their advantages in modeling visual tasks, deploying ViTs on hardware platforms, notably Field-Programmable Gate Arrays (FPGAs), introduces considerable challenges. These challenges stem primarily from the non-linear calculations and high computational and memory demands of ViTs. This paper introduces CHOSEN, a software-hardware co-design framework to address these challenges and offer an automated framework for ViT deployment on the FPGAs in order to maximize performance. Our framework is built upon three fundamental contributions: multi-kernel design to maximize the bandwidth, mainly targeting benefits of multi DDR memory banks, approximate non-linear functions that exhibit minimal accuracy degradation, and efficient use of available logic blocks on the FPGA, and efficient compiler to maximize the performance and memory-efficiency of the computing kernels by presenting a novel algorithm for design space exploration to find optimal hardware configuration that achieves optimal throughput and latency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a 1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models. </p>
<blockquote>
<p>视觉Transformer（ViTs）代表了计算机视觉机器学习方法的突破性转变。与传统的计算机视觉方法不同，ViTs采用自注意力机制（这在自然语言处理中得到了广泛应用）来分析图像块。尽管ViTs在建模视觉任务上具有优势，但在硬件平台（特别是现场可编程门阵列（FPGA））上部署ViTs却面临相当大的挑战。这些挑战主要源于ViTs的非线性计算以及其对计算和内存的高需求。本文介绍了CHOSEN，这是一个软硬件协同设计框架，旨在解决这些挑战，并提供一个自动化框架，以最大限度地提高FPGA上ViT的性能。我们的框架建立在三个基本贡献之上：多核设计以最大化带宽（主要针对多DDR内存银行的优势）、近似非线性函数以展现最小的精度损失以及有效地使用FPGA上的可用逻辑块，以及高效的编译器。通过提供一种新型算法来设计空间探索，编译器能够最大限度地提高计算内核的性能和内存效率，从而实现最佳的吞吐量和延迟，找到实现最佳吞吐量和延迟的最佳硬件配置。与最先进的ViT加速器相比，CHOSEN在DeiT-S和DeiT-B模型上的吞吐量提高了1.5倍和1.42倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.12736v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vision Transformers（ViTs）在计算机视觉领域的应用及其面临的挑战，特别是在硬件平台（如FPGA）上的部署。文章提出了一种名为CHOSEN的软件硬件协同设计框架，旨在解决这些问题并实现ViT在FPGA上的高效部署。该框架通过三个主要贡献来实现优化：多核设计以最大化带宽、利用DDR内存银行的优势，近似非线性函数以最小化精度损失，以及有效利用FPGA上的逻辑块。相较于现有ViT加速器，CHOSEN在DeiT-S和DeiT-B模型上的吞吐量分别提升了1.5倍和1.42倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) 引入自注意力机制处理图像补丁，是计算机视觉领域的一种创新方法。</li>
<li>ViT在FPGA等硬件平台部署面临非线性计算和高计算内存需求的挑战。</li>
<li>CHOSEN框架通过多核设计最大化带宽，特别利用DDR内存银行优势。</li>
<li>CHOSEN通过近似非线性函数实现精度损失最小化。</li>
<li>CHOSEN有效利用FPGA的逻辑块，并提供一个编译器以优化计算内核的性能和内存效率。</li>
<li>CHOSEN提供了一种自动化框架，用于在FPGA上部署ViT，以最大化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.12736">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-65a75bb9474e0f49db6b1bec65e09ea5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bd2eb2d72bd32b0c076d775844ac3e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0858ad978a13be34ddd102a46452f86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ca39d4a76696f85393ea06c0fd8c78e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12fe27472e0dd270c46b783e5f85a091.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-1f6684274aa763b8958d930e7c05db26.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-06-13  The Four Color Theorem for Cell Instance Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b6d7d7667d62afb50bbf5505e6cfdb8f.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-06-13  A Shortcut-aware Video-QA Benchmark for Physical Understanding via   Minimal Video Pairs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
