<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Single Cu Atom Sites on Co3O4 Activate Interfacial Oxygen for Enhanced   Reactivity and Selective Gas Sensing at Low Temperature">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9e7c9e275e906033e04e4c5f96cc32b4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="Single-Cu-Atom-Sites-on-Co3O4-Activate-Interfacial-Oxygen-for-Enhanced-Reactivity-and-Selective-Gas-Sensing-at-Low-Temperature"><a href="#Single-Cu-Atom-Sites-on-Co3O4-Activate-Interfacial-Oxygen-for-Enhanced-Reactivity-and-Selective-Gas-Sensing-at-Low-Temperature" class="headerlink" title="Single Cu Atom Sites on Co3O4 Activate Interfacial Oxygen for Enhanced   Reactivity and Selective Gas Sensing at Low Temperature"></a>Single Cu Atom Sites on Co3O4 Activate Interfacial Oxygen for Enhanced   Reactivity and Selective Gas Sensing at Low Temperature</h2><p><strong>Authors:Hamin Shin, Matteo Dâ€™Andria, Jaehyun Ko, Dong-Ha Kim, Frank Krumeich, Andreas T. Guentner</strong></p>
<p>Controlling the redox landscape of transition metal oxides is central to advancing their reactivity for heterogeneous catalysis or high-performance gas sensing. Here we report single Cu atom sites (1.42 wt%) anchored on Co3O4 nanoparticles (Cu1-Co3O4) that dramatically enhance reactivity and molecular sensing properties of the support at low temperature. The Cu1 are identified by X-ray adsorption near edge structure and feature strong metal-support interaction between Cu2+ and Co3O4, as revealed by X-ray photoelectron spectroscopy. The ability of Cu1 to form interfacial Cu-O-Co linkages strongly reduces the temperature of lattice oxygen activation compared to CuO nanoparticles on Co3O4 (CuONP-Co3O4), as demonstrated by temperature-programmed reduction and desorption analyses. To demonstrate immediate practical impact, we deploy such Cu1-Co3O4 nanoparticles as chemoresistive sensor for formaldehyde vapor that yields more than an order of magnitude higher response than CuONP-Co3O4 and consistently outperforms state-of-the-art sensors. That way, formaldehyde is detected down to 5 parts-per-billion at 50% relative humidity and 75 {\deg}C with excellent selectivity over various critical interferents. These results establish a mechanistic platform for activating redox-active supports using single-atom isolates of non-noble nature that yield drastically enhanced and well-defined reactivity to promote low-temperature oxidation reactions and selective analyte sensing. </p>
<blockquote>
<p>æ§åˆ¶è¿‡æ¸¡é‡‘å±æ°§åŒ–ç‰©çš„æ°§åŒ–è¿˜åŸæ™¯è§‚å¯¹äºä¿ƒè¿›å…¶åœ¨å¼‚ç›¸å‚¬åŒ–æˆ–é«˜æ€§èƒ½æ°”ä½“æ£€æµ‹ä¸­çš„ååº”æ€§è‡³å…³é‡è¦ã€‚è¿™é‡Œæˆ‘ä»¬æŠ¥é“äº†é”šå®šåœ¨Co3O4çº³ç±³é¢—ç²’ä¸Šçš„å•ä¸ªCuåŸå­ä½ç‚¹ï¼ˆ1.42 wt%ï¼‰ï¼ˆCu1-Co3O4ï¼‰ï¼Œå®ƒä»¬åœ¨ä½æ¸©ä¸‹æå¤§åœ°å¢å¼ºäº†è½½ä½“çš„ååº”æ€§å’Œåˆ†å­ä¼ æ„Ÿæ€§èƒ½ã€‚Cu1é€šè¿‡Xå°„çº¿å¸æ”¶è¿‘è¾¹ç»“æ„è¿›è¡Œè¯†åˆ«ï¼Œå¹¶ä¸”å¦‚Xå°„çº¿å…‰ç”µå­å…‰è°±æ‰€ç¤ºï¼ŒCu2+ä¸Co3O4ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„é‡‘å±è½½ä½“ç›¸äº’ä½œç”¨ã€‚Cu1å½¢æˆç•Œé¢Cu-O-Coé“¾æ¥çš„èƒ½åŠ›å¤§å¤§é™ä½äº†æ™¶æ ¼æ°§æ¿€æ´»çš„æ¸©åº¦ï¼Œä¸é”šå®šåœ¨Co3O4ä¸Šçš„CuOçº³ç±³é¢—ç²’ï¼ˆCuONP-Co3O4ï¼‰ç›¸æ¯”ï¼Œå¦‚ç¨‹åºå‡æ¸©è¿˜åŸå’Œè„±é™„åˆ†ææ‰€ç¤ºã€‚ä¸ºäº†å±•ç¤ºå…¶å³æ—¶å®é™…åº”ç”¨çš„å½±å“ï¼Œæˆ‘ä»¬å°†è¿™ç§Cu1-Co3O4çº³ç±³é¢—ç²’éƒ¨ç½²ä¸ºç”²é†›è’¸æ±½çš„åŒ–å­¦ç”µé˜»å¼ä¼ æ„Ÿå™¨ï¼Œå…¶å“åº”æ¯”CuONP-Co3O4é«˜å‡ºè¶…è¿‡ä¸€ä¸ªæ•°é‡çº§ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„ä¼ æ„Ÿå™¨ã€‚å› æ­¤ï¼Œåœ¨ç›¸å¯¹æ¹¿åº¦ä¸º50%ã€æ¸©åº¦ä¸º75æ‘„æ°åº¦çš„æ¡ä»¶ä¸‹ï¼Œå¯ä»¥æ£€æµ‹åˆ°ä½è‡³æ¯åäº¿ä»½ä¹‹äº”çš„ç”²é†›ï¼Œå¯¹å„ç§å…³é”®å¹²æ‰°ç‰©å…·æœ‰å‡ºè‰²çš„é€‰æ‹©æ€§ã€‚è¿™äº›ç»“æœå»ºç«‹äº†ä¸€ä¸ªæœºåˆ¶å¹³å°ï¼Œåˆ©ç”¨éè´µé‡‘å±æ€§è´¨çš„å•åŸå­éš”ç¦»ç‰©æ¥æ¿€æ´»æ°§åŒ–è¿˜åŸæ´»æ€§è½½ä½“ï¼Œäº§ç”Ÿå¤§å¹…åº¦å¢å¼ºå’Œæ˜ç¡®å®šä¹‰çš„ååº”æ€§ï¼Œä»¥ä¿ƒè¿›ä½æ¸©æ°§åŒ–ååº”å’Œé€‰æ‹©æ€§åˆ†æç‰©æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09761v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æŠ¥é“äº†ä»¥Cuå•åŸå­ä½ç‚¹ï¼ˆ1.42 wt%ï¼‰é”šå®šåœ¨Co3O4çº³ç±³ç²’å­ä¸Šçš„å¤åˆå‚¬åŒ–å‰‚ï¼ˆCu1-Co3O4ï¼‰ï¼Œæ˜¾è‘—æé«˜äº†å…¶åœ¨ä½æ¸©ä¸‹çš„ååº”æ€§å’Œåˆ†å­ä¼ æ„Ÿæ€§èƒ½ã€‚é€šè¿‡Xå°„çº¿å¸æ”¶è¿‘è¾¹ç»“æ„å’ŒXå°„çº¿å…‰ç”µå­èƒ½è°±è¡¨å¾ï¼Œè¯å®äº†Cu1çš„å­˜åœ¨åŠå…¶ä¸Co3O4ä¹‹é—´çš„å¼ºé‡‘å±-è½½ä½“ç›¸äº’ä½œç”¨ã€‚Cu1å½¢æˆç•Œé¢Cu-O-Coé“¾æ¥çš„èƒ½åŠ›é™ä½äº†æ™¶æ ¼æ°§æ¿€æ´»çš„æ¸©åº¦ï¼Œç›¸è¾ƒäºåœ¨Co3O4ä¸Šçš„CuOçº³ç±³ç²’å­ï¼ˆCuONP-Co3O4ï¼‰ï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„æ´»æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†Cu1-Co3O4çº³ç±³ç²’å­ä½œä¸ºç”²é†›è’¸æ±½çš„åŒ–å­¦ç”µé˜»ä¼ æ„Ÿå™¨ï¼Œå…¶å“åº”æ¯”CuONP-Co3O4é«˜å‡ºæ•°å€ï¼Œä¸”åœ¨æ¹¿åº¦å’Œæ¸©åº¦å˜åŒ–çš„æ¡ä»¶ä¸‹ä»è¡¨ç°å‡ºå“è¶Šçš„é€‰æ‹©æ€§ã€‚è¿™ä¸€å‘ç°ä¸ºåˆ©ç”¨éè´µé‡‘å±å•åŸå­æ¿€æ´»æ°§åŒ–è¿˜åŸæ´»æ€§è½½ä½“æä¾›äº†æœºåˆ¶å¹³å°ï¼Œå¯æœ‰æ•ˆä¿ƒè¿›ä½æ¸©æ°§åŒ–ååº”å’Œé€‰æ‹©æ€§åˆ†æç‰©ä¼ æ„Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cuå•åŸå­ä½ç‚¹æˆåŠŸé”šå®šåœ¨Co3O4çº³ç±³ç²’å­ä¸Šï¼Œæ˜¾è‘—æé«˜äº†å‚¬åŒ–æ€§èƒ½ã€‚</li>
<li>Cu1ä¸Co3O4ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„é‡‘å±-è½½ä½“ç›¸äº’ä½œç”¨ã€‚</li>
<li>Cu1å½¢æˆç•Œé¢Cu-O-Coé“¾æ¥ï¼Œé™ä½äº†æ™¶æ ¼æ°§æ¿€æ´»çš„æ¸©åº¦ã€‚</li>
<li>Cu1-Co3O4çº³ç±³ç²’å­åœ¨ä½æ¸©ä¸‹çš„ååº”æ€§å’Œåˆ†å­ä¼ æ„Ÿæ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥ææ–™åœ¨æ£€æµ‹ç”²é†›è’¸æ±½æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå“åº”åº¦é«˜ä¸”é€‰æ‹©æ€§å¥½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨å•åŸå­æ¿€æ´»æ°§åŒ–è¿˜åŸæ´»æ€§è½½ä½“æä¾›äº†æœºåˆ¶å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22610d7354f7ca14ec52e6340fdfcc3f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Practical-Alzheimerâ€™s-Disease-Diagnosis-A-Lightweight-and-Interpretable-Spiking-Neural-Model"><a href="#Towards-Practical-Alzheimerâ€™s-Disease-Diagnosis-A-Lightweight-and-Interpretable-Spiking-Neural-Model" class="headerlink" title="Towards Practical Alzheimerâ€™s Disease Diagnosis: A Lightweight and   Interpretable Spiking Neural Model"></a>Towards Practical Alzheimerâ€™s Disease Diagnosis: A Lightweight and   Interpretable Spiking Neural Model</h2><p><strong>Authors:Changwei Wu, Yifei Chen, Yuxin Du, Jinying Zong, Jie Dong, Mingxuan Liu, Yong Peng, Jin Fan, Feiwei Qin, Changmiao Wang</strong></p>
<p>Early diagnosis of Alzheimerâ€™s Disease (AD), especially at the mild cognitive impairment (MCI) stage, is vital yet hindered by subjective assessments and the high cost of multimodal imaging modalities. Although deep learning methods offer automated alternatives, their energy inefficiency and computational demands limit real-world deployment, particularly in resource-constrained settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are inherently well-suited for modeling the sparse, event-driven patterns of neural degeneration in AD, offering a promising foundation for interpretable and low-power medical diagnostics. However, existing SNNs often suffer from weak expressiveness and unstable training, which restrict their effectiveness in complex medical tasks. To address these limitations, we propose FasterSNN, a hybrid neural architecture that integrates biologically inspired LIF neurons with region-adaptive convolution and multi-scale spiking attention. This design enables sparse, efficient processing of 3D MRI while preserving diagnostic accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves competitive performance with substantially improved efficiency and stability, supporting its potential for practical AD screening. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/wuchangw/FasterSNN">https://github.com/wuchangw/FasterSNN</a>. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æ—©æœŸè¯Šæ–­ï¼Œç‰¹åˆ«æ˜¯åœ¨è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰é˜¶æ®µï¼Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¸»è§‚è¯„ä¼°å’Œå¤šç§æ¨¡å¼çš„æˆåƒæ¨¡å¼çš„é«˜æˆæœ¬éƒ½é˜»ç¢äº†è¯Šæ–­çš„è¿›ç¨‹ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ–¹æ³•æä¾›äº†è‡ªåŠ¨åŒ–çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶èƒ½æºæ•ˆç‡ä¸è¶³å’Œè®¡ç®—éœ€æ±‚é«˜é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚ä½œä¸ºå—å¤§è„‘å¯å‘çš„æ¨¡å‹ï¼Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å¤©ç”Ÿå°±é€‚åˆæ¨¡æ‹ŸADä¸­ç¥ç»å˜æ€§çš„ç¨€ç–ã€äº‹ä»¶é©±åŠ¨çš„æ¨¡å¼ï¼Œä¸ºå¯è§£é‡Šå’Œä½åŠŸè€—çš„åŒ»å­¦è¯Šæ–­æä¾›äº†æœ‰å‰æ™¯çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„SNNså¸¸å¸¸å­˜åœ¨è¡¨è¾¾æ€§å¼±å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å¤æ‚åŒ»å­¦ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†FasterSNNï¼Œè¿™æ˜¯ä¸€ç§æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ƒç»“åˆäº†ç”Ÿç‰©å¯å‘çš„LIFç¥ç»å…ƒä¸åŒºåŸŸè‡ªé€‚åº”å·ç§¯å’Œå¤šå°ºåº¦è„‰å†²æ³¨æ„åŠ›ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿç¨€ç–ã€é«˜æ•ˆåœ°å¤„ç†3D MRIï¼ŒåŒæ—¶ä¿ç•™è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFasterSNNåœ¨å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æé«˜æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œæ”¯æŒå…¶å®è·µä¸­ç”¨äºADç­›æŸ¥çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wuchangw/FasterSNN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wuchangw/FasterSNNæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09695v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å±€é™æ€§ï¼Œä»¥åŠå¤§è„‘å—å¯å‘çš„äººå·¥ç¥ç»ç½‘ç»œæ¨¡å‹åœ¨æ—©æœŸè¯Šæ–­é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­çš„é‡è¦æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆç¥ç»ç½‘ç»œæ¶æ„â€”â€”FasterSNNã€‚è¯¥æ¶æ„ç»“åˆäº†ç”Ÿç‰©å¯å‘ç¥ç»å…ƒä¸è‡ªé€‚åº”å·ç§¯å’Œå¤šå°ºåº¦è„‰å†²æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å®ç°é«˜æ•ˆçš„MRIå¤„ç†ï¼Œå¹¶ç»´æŒè¯Šæ–­å‡†ç¡®æ€§ã€‚å…¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­ç­›æŸ¥é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨è½»åº¦è®¤çŸ¥éšœç¢é˜¶æ®µã€‚å½“å‰è¯„ä¼°æ‰‹æ®µå—åˆ°ä¸»è§‚è¯„ä¼°å’ŒæˆåƒæŠ€æœ¯æˆæœ¬é«˜çš„é™åˆ¶ã€‚</li>
<li>è™½ç„¶æ·±åº¦å­¦ä¹ æä¾›äº†è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œä½†å…¶èƒ½æºæ•ˆç‡å’Œè®¡ç®—éœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ã€‚</li>
<li>åŸºäºå¤§è„‘å—å¯å‘çš„äººå·¥ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆå¦‚è„‰å†²ç¥ç»ç½‘ç»œï¼‰å¯¹äºæ¨¡æ‹Ÿç¥ç»é€€å˜çš„ç¨€ç–ã€äº‹ä»¶é©±åŠ¨æ¨¡å¼å…·æœ‰ä¼˜åŠ¿ï¼Œæœ‰åŠ©äºè§£é‡Šå’ŒèŠ‚çº¦èƒ½æºå‹åŒ»å­¦è¯Šæ–­ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æœ‰æ—¶ä¼šå‡ºç°è¡¨è¾¾èƒ½åŠ›å¼±å’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>FasterSNNæ˜¯ä¸€ä¸ªæ–°å‹æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†ç”Ÿç‰©å¯å‘ç¥ç»å…ƒå’Œè‡ªé€‚åº”å·ç§¯æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜å¹¶å®ç°é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>FasterSNNæ¶æ„å®ç°äº†å¯¹MRIæ•°æ®çš„ç¨€ç–å¤„ç†ï¼ŒåŒæ—¶ä¿æŒäº†è¯Šæ–­å‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜å…¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„è¿˜æ”¯æŒæ½œåœ¨çš„å®é™…åº”ç”¨ï¼Œå¦‚é˜¿å°”èŒ¨æµ·é»˜ç—…çš„ç­›æŸ¥ã€‚</li>
<li>FasterSNNçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶æ”¹è¿›çš„æ•ˆç‡ã€ç¨³å®šæ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚è¿™å¯¹äºèµ„æºå—é™çš„ç¯å¢ƒä¸­çš„åŒ»å­¦è¯Šæ–­å°¤ä¸ºé‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa9668a027f3f0ae49d23d087228c1cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-880d73c3d3fe22b6285d30551331d534.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HSENet-Hybrid-Spatial-Encoding-Network-for-3D-Medical-Vision-Language-Understanding"><a href="#HSENet-Hybrid-Spatial-Encoding-Network-for-3D-Medical-Vision-Language-Understanding" class="headerlink" title="HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language   Understanding"></a>HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language   Understanding</h2><p><strong>Authors:Yanzhao Shi, Xiaodan Zhang, Junzhong Ji, Haoning Jiang, Chengxin Zheng, Yinong Wang, Liangqiong Qu</strong></p>
<p>Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based decisions by enhancing diagnostic accuracy and workflow efficiency. While multimodal large language models (MLLMs) exhibit promising performance in visual-language understanding, existing methods mainly focus on 2D medical images, which fundamentally limits their ability to capture complex 3D anatomical structures. This limitation often leads to misinterpretation of subtle pathologies and causes diagnostic hallucinations. In this paper, we present Hybrid Spatial Encoding Network (HSENet), a framework that exploits enriched 3D medical visual cues by effective visual perception and projection for accurate and robust vision-language understanding. Specifically, HSENet employs dual-3D vision encoders to perceive both global volumetric contexts and fine-grained anatomical details, which are pre-trained by dual-stage alignment with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient multimodal projector that condenses high-resolution 3D spatial regions into a compact set of informative visual tokens via centroid-based compression. By assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly perceive and transfer hybrid visual representations to LLMâ€™s semantic space, facilitating accurate diagnostic text generation. Experimental results demonstrate that our method achieves state-of-the-art performance in 3D language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering (73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YanzhaoShi/HSENet">https://github.com/YanzhaoShi/HSENet</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ä¸‰ç»´CTè¯Šæ–­é€šè¿‡æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿåšå‡ºåŠæ—¶ã€åŸºäºè¯æ®çš„å†³ç­–ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨äºŒç»´åŒ»å­¦å›¾åƒä¸Šï¼Œè¿™ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†å®ƒä»¬æ•æ‰å¤æ‚ä¸‰ç»´è§£å‰–ç»“æ„çš„èƒ½åŠ›ã€‚è¿™ç§é™åˆ¶å¸¸å¸¸å¯¼è‡´å¯¹ç»†å¾®ç—…ç†çš„è¯¯è§£å’Œè¯Šæ–­å¹»è§‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Hybrid Spatial Encoding Networkï¼ˆHSENetï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡æœ‰æ•ˆçš„è§†è§‰æ„ŸçŸ¥å’ŒæŠ•å½±ï¼Œåˆ©ç”¨ä¸°å¯Œçš„ä¸‰ç»´åŒ»å­¦è§†è§‰çº¿ç´¢ï¼Œå®ç°å‡†ç¡®ç¨³å¥çš„è§†å¬è¯­è¨€ç†è§£ã€‚å…·ä½“è€Œè¨€ï¼ŒHSENeté‡‡ç”¨åŒä¸‰ç»´è§†è§‰ç¼–ç å™¨æ¥æ„ŸçŸ¥å…¨å±€ä½“ç§¯ä¸Šä¸‹æ–‡å’Œç²¾ç»†çš„è§£å‰–ç»†èŠ‚ï¼Œå¹¶é€šè¿‡ä¸è¯Šæ–­æŠ¥å‘Šçš„åŒé‡é˜¶æ®µå¯¹é½è¿›è¡Œé¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†Spatial Packerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€æŠ•å½±ä»ªï¼Œå®ƒå¯ä»¥é€šè¿‡åŸºäºè´¨å¿ƒçš„å‹ç¼©å°†é«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´ç©ºé—´åŒºåŸŸæµ“ç¼©æˆä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰æ ‡è®°ã€‚é€šè¿‡ä¸ºåŒä¸‰ç»´è§†è§‰ç¼–ç å™¨åˆ†é…ç©ºé—´æ‰“åŒ…å™¨ï¼ŒHSENetå¯ä»¥æ— ç¼åœ°æ„ŸçŸ¥å¹¶å°†æ··åˆè§†è§‰è¡¨ç¤ºè½¬ç§»åˆ°LLMè¯­ä¹‰ç©ºé—´ï¼Œä¿ƒè¿›å‡†ç¡®çš„è¯Šæ–­æ–‡æœ¬ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´è¯­è¨€è§†è§‰æ£€ç´¢ï¼ˆR@100è¾¾åˆ°39.85%ï¼Œå¢ç›Š+5.96%ï¼‰ã€ä¸‰ç»´åŒ»å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆBLEU-4è¾¾åˆ°24.01%ï¼Œå¢ç›Š+8.01%ï¼‰å’Œä¸‰ç»´è§†è§‰é—®ç­”ï¼ˆä¸»è¦ç±»åˆ«å‡†ç¡®ç‡è¾¾åˆ°73.60%ï¼Œå¢ç›Š+1.99%ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YanzhaoShi/HSENet">https://github.com/YanzhaoShi/HSENet</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09634v1">PDF</a> 27 pages, 9 figures. arXiv admin note: text overlap with   arXiv:2410.14200 by other authors</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨åŒ–ä¸‰ç»´CTè¯Šæ–­é€šè¿‡æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œèµ‹èƒ½ä¸´åºŠåŒ»ç”Ÿåšå‡ºåŠæ—¶ã€åŸºäºè¯æ®çš„å†³å®šã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨äºŒç»´åŒ»å­¦å›¾åƒï¼Œåœ¨æ•æ‰å¤æ‚ä¸‰ç»´è§£å‰–ç»“æ„æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯èƒ½å¯¼è‡´å¯¹ç»†å¾®ç—…ç†çš„è¯¯è§£å’Œè¯Šæ–­å¹»è§‰ã€‚æœ¬æ–‡æå‡ºHybrid Spatial Encoding Network (HSENet)ï¼Œé€šè¿‡æœ‰æ•ˆçš„è§†è§‰æ„ŸçŸ¥å’ŒæŠ•å½±ï¼Œåˆ©ç”¨ä¸°å¯Œçš„ä¸‰ç»´åŒ»å­¦è§†è§‰çº¿ç´¢ï¼Œå®ç°å‡†ç¡®è€Œç¨³å¥çš„è§†è¯­è¨€ç†è§£ã€‚HSENeté‡‡ç”¨åŒä¸‰ç»´è§†è§‰ç¼–ç å™¨ï¼Œæ„ŸçŸ¥å…¨å±€ä½“ç§¯ä¸Šä¸‹æ–‡å’Œç²¾ç»†è§£å‰–ç»†èŠ‚ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„æŠ¥å‘Šé˜¶æ®µè¿›è¡Œé¢„è®­ç»ƒå¯¹é½è·å¾—é«˜è´¨é‡çš„è§†æ„ŸçŸ¥ï¼›å¦å¤–é€šè¿‡é«˜æ•ˆçš„å¤šæ¨¡æ€æŠ•å½±å™¨ï¼ˆSpatial Packerï¼‰å°†é«˜åˆ†è¾¨ç‡çš„ä¸‰ç»´ç©ºé—´åŒºåŸŸå‹ç¼©æˆä¸€ç»„ç´§å‡‘çš„ä¿¡æ¯è§†è§‰ç¬¦å·ã€‚HSENetå°†HSENetæ„ŸçŸ¥ä¸è¯­è¨€æ¨¡å‹è¯­ä¹‰ç©ºé—´ç›¸ç»“åˆï¼Œå®ç°äº†è¯Šæ–­æ–‡æœ¬çš„å‡†ç¡®ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ä¸‰ç»´è¯­è¨€è§†è§‰æ£€ç´¢ã€ä¸‰ç»´åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå’Œä¸‰ç»´è§†è§‰é—®ç­”æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ä¸‰ç»´CTè¯Šæ–­èƒ½æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œå·¥ä½œæµç¨‹æ•ˆç‡ï¼Œå¸®åŠ©åŒ»ç”Ÿåšå‡ºåŠæ—¶ã€åŸºäºè¯æ®çš„å†³å®šã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£è§†è§‰è¯­è¨€æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„ä¸‰ç»´åŒ»å­¦å›¾åƒæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ•æ‰ä¸‰ç»´è§£å‰–ç»“æ„æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¯èƒ½å¯¼è‡´å¯¹ç»†å¾®ç—…ç†çš„è¯¯è§£å’Œè¯Šæ–­å¹»è§‰ã€‚</li>
<li>Hybrid Spatial Encoding Network (HSENet)èƒ½å……åˆ†åˆ©ç”¨ä¸°å¯Œçš„ä¸‰ç»´åŒ»å­¦è§†è§‰çº¿ç´¢ï¼Œå®ç°å‡†ç¡®è€Œç¨³å¥çš„è§†è¯­è¨€ç†è§£ã€‚</li>
<li>HSENetç»“åˆäº†è§†è§‰ç¼–ç å™¨çš„ä¼˜åŠ¿å’Œç©ºé—´æ‰“åŒ…å™¨æŠ€æœ¯ï¼Œä½¿å¾—å¤æ‚çš„ä¸‰ç»´ä¿¡æ¯èƒ½å¤Ÿè¢«é«˜æ•ˆå¤„ç†å¹¶è½¬åŒ–ä¸ºè¯­è¨€æ¨¡å‹å¯ä»¥ç†è§£çš„ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºHSENetåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒ…æ‹¬ä¸‰ç»´è¯­è¨€è§†è§‰æ£€ç´¢ã€ä¸‰ç»´åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå’Œä¸‰ç»´è§†è§‰é—®ç­”ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-765557eeccd6fa39ef31283939b182d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb543e1620876370af34f969eb67d33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6544c2845a39330c43f148968a3b9761.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-intermediate-mass-black-hole-2XMM-J123103-2-110648-a-varying-disc-accretion-rate-during-possible-X-ray-quasi-periodic-eruptions"><a href="#The-intermediate-mass-black-hole-2XMM-J123103-2-110648-a-varying-disc-accretion-rate-during-possible-X-ray-quasi-periodic-eruptions" class="headerlink" title="The intermediate-mass black hole 2XMM J123103.2+110648: a varying disc   accretion rate during possible X-ray quasi-periodic eruptions?"></a>The intermediate-mass black hole 2XMM J123103.2+110648: a varying disc   accretion rate during possible X-ray quasi-periodic eruptions?</h2><p><strong>Authors:Z. Cao, P. G. Jonker, S. Wen, N. C. Stone, A. I. Zabludoff</strong></p>
<p>We fit the evolving X-ray spectra of the variable and fading source 2XMM J123103.2+110648 (J1231), which is an intermediate-mass black hole (IMBH) candidate. Recent X-ray timing studies have proposed that J1231â€™s quasi-periodic oscillation (QPO) observed at the peak of its X-ray lightcurve is a variant of the quasi-periodic eruptions (QPEs) observed in other sources. Here, we fit X-ray spectra from XMM-Newton, Swift, and Chandra using a slim disc model for the black holeâ€™s accretion disc, obtaining a best-fit black hole mass of ($6\pm3)\times10^{4}$ $M_\odot$ and spin of $&gt;0.6$ at 2$\sigma$ confidence. This mass is consistent with past estimates, supporting the IMBH interpretation, and the spin measurement is new. Yet the nature of J1231 remains uncertain: its long-term variability (decade-long continuum evolution) could signal a tidal disruption event or active galactic nuclear variability. We find that the spectral evolution within the first three years after the sourceâ€™s detection can be well explained by either a varying disc accretion rate $\dot m$ or a varying disc inclination $\theta$. Meanwhile, we find that during the short-term variability (the QPO with a ~3.8hr period), each oscillation does not show the â€œhard-rise-soft-decayâ€ typical of QPEs. We fit the average spectrum at the QPO lightcurve maxima and the average spectrum at its minima, finding that the spectral difference is well explained by $\dot m$ decreasing from peaks to valleys if $\theta&lt;30^{\circ}$ and constant between all data epochs. This result suggests that the short-term QPO behaviour might also be driven by a varying disc $\dot m$. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹å¯å˜ä¸”è¡°å‡æº2XMM J123103.2+110648ï¼ˆJ1231ï¼‰çš„æ¼”åŒ–Xå°„çº¿å…‰è°±è¿›è¡Œäº†æ‹Ÿåˆï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­ç­‰è´¨é‡é»‘æ´ï¼ˆIMBHï¼‰çš„å€™é€‰æºã€‚æœ€è¿‘çš„Xå°„çº¿æ—¶é—´ç ”ç©¶æå‡ºï¼ŒJ1231åœ¨Xå°„çº¿å…‰å³°è§‚å¯Ÿåˆ°çš„å‡†å‘¨æœŸæŒ¯è¡ï¼ˆQPOï¼‰æ˜¯ä¸å…¶ä»–æºè§‚å¯Ÿåˆ°çš„å‡†å‘¨æœŸçˆ†å‘ï¼ˆQPEsï¼‰çš„ä¸€ç§å˜ä½“ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨é»‘æ´å¸ç§¯ç›˜çš„è–„ç›˜æ¨¡å‹å¯¹XMM-ç‰›é¡¿ã€Swiftå’Œé’±å¾·æ‹‰æœ›è¿œé•œçš„Xå°„çº¿å…‰è°±è¿›è¡Œäº†æ‹Ÿåˆï¼Œå¾—åˆ°æœ€ä½³æ‹Ÿåˆé»‘æ´è´¨é‡ä¸ºï¼ˆ$6\pm3)\times10^{4}$ $M_\odot$ï¼Œåœ¨$ 2\sigma $ç½®ä¿¡æ°´å¹³ä¸‹è‡ªè½¬é€Ÿåº¦å¤§äº$ 0.6 $ã€‚è¿™ä¸€è´¨é‡ä¸ä¹‹å‰çš„ä¼°è®¡ä¸€è‡´ï¼Œæ”¯æŒIMBHè§£é‡Šï¼Œå¹¶ä¸”è‡ªè½¬é€Ÿåº¦æµ‹é‡æ˜¯æ–°çš„ã€‚ç„¶è€Œï¼ŒJ1231çš„æœ¬è´¨ä»ç„¶ä¸ç¡®å®šï¼šå…¶é•¿æœŸå˜åŒ–ï¼ˆåå¹´è¿ç»­è°±æ¼”åŒ–ï¼‰å¯èƒ½è¡¨æ˜æ½®æ±æ’•è£‚äº‹ä»¶æˆ–æ´»åŠ¨æ˜Ÿç³»æ ¸çš„å˜å¼‚æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æºæ£€æµ‹åçš„å‰ä¸‰å¹´å†…ï¼Œå…‰è°±æ¼”åŒ–å¯ä»¥ç”¨å˜åŒ–çš„ç›˜å¸ç§¯ç‡$\dot m$æˆ–å˜åŒ–çš„ç›˜å€¾è§’$\theta$å¾ˆå¥½åœ°è§£é‡Šã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°çŸ­æœŸå˜åŒ–ï¼ˆå‘¨æœŸä¸º~3.8å°æ—¶çš„QPOï¼‰æœŸé—´ï¼Œæ¯æ¬¡æŒ¯è¡å¹¶ä¸æ˜¾ç¤ºå‡ºå…¸å‹çš„â€œç¡¬å‡è½¯è¡°â€QPEsç‰¹å¾ã€‚æˆ‘ä»¬å¯¹QPOå…‰æ›²çº¿æœ€å¤§å€¼å’Œæœ€å°å€¼æ—¶çš„å¹³å‡å…‰è°±è¿›è¡Œäº†æ‹Ÿåˆï¼Œå‘ç°å¦‚æœ$\theta&lt;30^{\circ}$ï¼Œå…‰è°±å·®å¼‚å¯ä»¥é€šè¿‡$\dot m$ä»å³°å€¼åˆ°è°·å€¼çš„å‡å°‘æ¥å¾ˆå¥½åœ°è§£é‡Šï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æ•°æ®æ—¶æœŸä¹‹é—´ä¿æŒä¸å˜ã€‚è¿™ä¸€ç»“æœæš—ç¤ºçŸ­æœŸQPOè¡Œä¸ºä¹Ÿå¯èƒ½ç”±å˜åŒ–çš„ç›˜$\dot m$é©±åŠ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09603v1">PDF</a> 13 pages, 6 figures, 8 tables, with Appendix. Accepted for   publication in A&amp;A</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡å¯¹ä¸­é—´è´¨é‡é»‘æ´å€™é€‰ä½“2XMM J123103.2+110648ï¼ˆJ1231ï¼‰çš„Xå°„çº¿å…‰è°±è¿›è¡Œäº†æ‹Ÿåˆç ”ç©¶ã€‚é€šè¿‡å¯¹XMM-Newtonã€Swiftå’ŒChandraçš„Xå°„çº¿å…‰è°±è¿›è¡Œæ‹Ÿåˆï¼Œä½¿ç”¨é»‘æ´å¸ç§¯ç›˜çš„ç˜¦ç›˜æ¨¡å‹ï¼Œå¾—åˆ°äº†æœ€ä½³æ‹Ÿåˆé»‘æ´è´¨é‡çº¦ä¸ºï¼ˆ6Â±3ï¼‰Ã—10^4 M_odotï¼Œè‡ªæ—‹é€Ÿåº¦å¤§äº0.6ï¼Œåœ¨2Ïƒç½®ä¿¡æ°´å¹³ä¸‹ã€‚é•¿æœŸå˜åŒ–å¯èƒ½æš—ç¤ºæ½®æ±æ’•è£‚äº‹ä»¶æˆ–æ´»åŠ¨æ˜Ÿç³»æ ¸å˜åŒ–ã€‚çŸ­æœŸå˜åŒ–åˆ™æ˜¾ç¤ºå‡ºè°±å˜åŒ–ä¸ç›˜å¸ç§¯ç‡å˜åŒ–ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>J1231çš„Xå°„çº¿å…‰è°±æ‹Ÿåˆæ­ç¤ºäº†å…¶ä¸ºå‡†å‘¨æœŸæ€§æŒ¯è¡ï¼ˆQPOï¼‰ç°è±¡ï¼Œä¸å…¶ä»–æºçš„å‡†å‘¨æœŸæ€§çˆ†å‘ï¼ˆQPEsï¼‰ç±»ä¼¼ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ç˜¦ç›˜æ¨¡å‹æ‹Ÿåˆï¼Œå¾—åˆ°é»‘æ´è´¨é‡ä¼°è®¡ä¸ºï¼ˆ$6\pm3ï¼‰\times10^{4}$ $M_\odot$ï¼Œè‡ªæ—‹é€Ÿåº¦å¤§äº0.6ï¼Œä¸è¿‡å»ä¼°è®¡ç›¸ç¬¦ï¼Œæ”¯æŒä¸­é—´è´¨é‡é»‘æ´ï¼ˆIMBHï¼‰çš„è§£é‡Šã€‚</li>
<li>J1231çš„é•¿æœŸå˜åŒ–å¯èƒ½æ˜¯æ½®æ±æ’•è£‚äº‹ä»¶æˆ–æ´»åŠ¨æ˜Ÿç³»æ ¸å˜åŒ–çš„è¿¹è±¡ã€‚</li>
<li>çŸ­æœŸå˜åŒ–æ˜¾ç¤ºè°±å˜åŒ–ä¸ç›˜çš„å¸ç§¯ç‡å˜åŒ–æœ‰å…³ã€‚</li>
<li>åœ¨QPOçš„çŸ­å‘¨æœŸï¼ˆçº¦3.8å°æ—¶ï¼‰å†…ï¼Œæ¯ä¸ªæŒ¯è¡å¹¶ä¸æ˜¾ç¤ºå‡ºå…¸å‹çš„â€œç¡¬å‡è½¯è¡°â€ç‰¹å¾ã€‚</li>
<li>å¹³å‡å…‰è°±åœ¨QPOå…‰æ›²çº¿å³°å€¼å’Œè°·å€¼çš„å·®å¼‚å¯ä»¥é€šè¿‡å¸ç§¯ç‡ä»å³°å€¼åˆ°è°·å€¼çš„å˜åŒ–æ¥è§£é‡Šï¼Œå¦‚æœç›˜çš„å€¾è§’Î¸å°äº30Â°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd3f58c3881b0b6146a86dad2dbc5b6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32462a76450b59a6aa474ad94bccd2f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d423374da5a5b1401fd5164ad489d9a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ae1b210ec3e0c328645cff57e5ef18b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20602a818d360d0c131ea5114a61eb01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8bf99e31b4f11b21aad0282dff8e75e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Commissioning-characterization-and-first-high-dose-rate-irradiations-at-a-compact-X-ray-tube-for-microbeam-and-minibeam-radiation-therapy"><a href="#Commissioning-characterization-and-first-high-dose-rate-irradiations-at-a-compact-X-ray-tube-for-microbeam-and-minibeam-radiation-therapy" class="headerlink" title="Commissioning, characterization and first high dose rate irradiations at   a compact X-ray tube for microbeam and minibeam radiation therapy"></a>Commissioning, characterization and first high dose rate irradiations at   a compact X-ray tube for microbeam and minibeam radiation therapy</h2><p><strong>Authors:Christian Petrich, Johanna Winter, Anton Dimroth, Thomas Beiser, Monika Dehn, Jessica Stolz, Jacopo Frignani, Stephanie E. Combs, Franz Schilling, Ghaleb Natour, Kurt Aulenbacher, Thomas E. Schmid, Jan J. Wilkens, Stefan Bartzsch</strong></p>
<p>Minibeam and microbeam radiation therapy promise improved treatment outcomes through reduced normal tissue toxicity at better tumor control rates. The lack of suitable compact radiation sources limits the clinical application of minibeams to superficial tumors and renders it impossible for microbeams. We developed and constructed the first prototype of a compact line-focus X-ray tube (LFXT) with technology potentially suitable for clinical translation of minibeams and microbeams. We give an overview of the commissioning process preceding the first operation, present optical and radiological focal spot characterization methods, and dosimetric measurements. Additionally, we report on first preclinical in vitro cell and in vivo mouse brain irradiations conducted with the LFXT prototype. The focal spot characterization resulted in a strongly eccentric electron distribution with a width of 72.3 $\mu$m. Dosimetry showed sharp microbeam dose profiles with steep lateral penumbras and a peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy&#x2F;s was measured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In vitro and in vivo experiments demonstrated the feasibility of the LFXT for minibeam and microbeam applications with field sizes of 1.5-2 cm. The mice displayed no observable side effects throughout the follow-up period after whole-brain 260 $\mu$m-minibeam irradiation. We successfully constructed and commissioned the first proof-of-concept LFXT prototype. Dosimetric characterizations of the achieved microbeam field showed the superiority of the LFXT compared to conventional X-ray tubes in terms of beam quality. In future developments, the remaining limitations of the prototype will be addressed for improved minibeam and first ever microbeam radiation therapy in a clinical setting. </p>
<blockquote>
<p>å¾®å‹æŸåŠå¾®æŸæ”¾å°„æ²»ç–—é€šè¿‡å‡å°‘æ­£å¸¸ç»„ç»‡çš„æ¯’æ€§å¹¶æé«˜è‚¿ç˜¤æ§åˆ¶ç‡ï¼Œæœ‰æœ›æ”¹å–„æ²»ç–—æ•ˆæœã€‚ç”±äºç¼ºä¹åˆé€‚çš„ç´§å‡‘å‹è¾å°„æºï¼Œå¾®å‹æŸçš„ä¸´åºŠåº”ç”¨ä»…é™äºæµ…è¡¨è‚¿ç˜¤ï¼Œè€Œå¾®å‹æŸåˆ™æ— æ³•å®ç°ã€‚æˆ‘ä»¬å¼€å‘å¹¶æ„å»ºäº†ç¬¬ä¸€å°ç´§å‡‘å‹çº¿èšç„¦Xå°„çº¿ç®¡ï¼ˆLFXTï¼‰çš„åŸå‹ï¼Œè¿™é¡¹æŠ€æœ¯å…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ï¼Œå¯ç”¨äºå¾®å‹æŸå’Œå¾®æŸçš„ä¸´åºŠè½¬åŒ–ã€‚æœ¬æ–‡æ¦‚è¿°äº†è®¾å¤‡è°ƒè¯•è¿‡ç¨‹ï¼Œä»‹ç»äº†å…‰å­¦å’Œæ”¾å°„å­¦ç„¦ç‚¹ç‰¹å¾åŒ–æ–¹æ³•ä»¥åŠå‰‚é‡å­¦æµ‹é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æŠ¥å‘Šäº†ä½¿ç”¨LFXTåŸå‹è¿›è¡Œçš„ç¬¬ä¸€æ‰¹ä½“å¤–ç»†èƒå’Œå°é¼ è„‘éƒ¨çš„ä½“å†…å¤–é¢„ä¸´åºŠç ”ç©¶ç»“æœã€‚ç„¦ç‚¹ç‰¹å¾åŒ–ç»“æœæ˜¾ç¤ºç”µå­åˆ†å¸ƒå¼ºçƒˆåå¿ƒï¼Œå®½åº¦ä¸º72.3å¾®ç±³ã€‚å‰‚é‡å­¦æ˜¾ç¤ºå¾®æŸå‰‚é‡åˆ†å¸ƒæ¸…æ™°ï¼Œä¾§å‘ç¬”è¿¹é™¡å³­ï¼Œåœ¨70æ¯«ç±³åšçš„PMMAå¹»å½±ä¸­å³°è°·å‰‚é‡æ¯”è¶…è¿‡10ã€‚åœ¨åŠ é€Ÿç”µå‹ä¸º150åƒä¼ã€ç¦»ç„¦ç‚¹è·ç¦»150æ¯«ç±³å¤„ï¼ŒæŸæµä¸º17.4æ¯«å®‰çš„æƒ…å†µä¸‹ï¼Œå¼€æ”¾åœºå‰‚é‡ç‡æµ‹é‡å€¼ä¸ºæ¯ç§’4.3æˆˆç‘ã€‚ä½“å†…å¤–å®éªŒè¯æ˜äº†LFXTåœ¨å°å‹æŸå’Œå¾®æŸåº”ç”¨ä¸­çš„å¯è¡Œæ€§ï¼Œåœºé¢ç§¯ä¸º1.5-2å˜ç±³ã€‚åœ¨ä¸ºæœŸè§‚å¯ŸæœŸé—´å¯¹å…¨è„‘è¿›è¡Œäº†ç»é¼»å…‰ç„¦è€³æ¯å¾®æŸæ”¾å°„åçš„26åªå°é¼ å¹¶æœªæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å‰¯ä½œç”¨ã€‚æˆ‘ä»¬æˆåŠŸæ„å»ºäº†é¦–ä¸ªæ¦‚å¿µçš„LFXTåŸå‹å¹¶è¿›è¡Œè°ƒè¯•ã€‚å¾®æŸåœºçš„å‰‚é‡å­¦ç‰¹å¾è¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„Xå°„çº¿ç®¡ç›¸æ¯”ï¼ŒLFXTåœ¨å…‰æŸè´¨é‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚åœ¨æœªæ¥çš„å‘å±•ä¸­ï¼Œæˆ‘ä»¬å°†è§£å†³åŸå‹çš„å‰©ä½™å±€é™æ€§ï¼Œä»¥æ”¹è¿›ä¸´åºŠç¯å¢ƒä¸­çš„å¾®å‹æŸæ”¾å°„æ²»ç–—å’Œé¦–æ¬¡å¾®æŸæ”¾å°„æ²»ç–—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09536v1">PDF</a> CP, JW, and AD share first authorship</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¾®å‹æŸå’Œå¾®æŸæ”¾å°„æ²»ç–—çš„ç†è®ºä¼˜åŠ¿ï¼Œä¾‹å¦‚å‡å°‘æ­£å¸¸ç»„ç»‡æ¯’æ€§å’Œæé«˜è‚¿ç˜¤æ§åˆ¶ç‡ï¼Œæˆ‘ä»¬ç ”å‘äº†é¦–ä¸ªç´§å‡‘å‹çº¿èšç„¦Xå°„çº¿ç®¡ï¼ˆLFXTï¼‰åŸå‹ï¼Œç”¨äºä¸´åºŠè½¬åŒ–çš„å¾®å‹æŸå’Œå¾®æŸã€‚æœ¬æ–‡ä»‹ç»äº†è®¾å¤‡å¯ç”¨å‰çš„è°ƒè¯•æµç¨‹ã€å…‰å­¦å’Œæ”¾å°„å­¦ç„¦ç‚¹ç‰¹å¾æ–¹æ³•ä»¥åŠå‰‚é‡æµ‹å®šã€‚æ­¤å¤–ï¼Œè¿˜æŠ¥å‘Šäº†ä½¿ç”¨LFXTåŸå‹è¿›è¡Œçš„ç¬¬ä¸€æ‰¹ä½“å¤–ç»†èƒå’Œå°é¼ è„‘éƒ¨çš„ç…§å°„å®éªŒã€‚ç„¦ç‚¹ç‰¹å¾ä¸ºåå¿ƒç”µå­åˆ†å¸ƒï¼Œå®½åº¦ä¸º72.3å¾®ç±³ã€‚å‰‚é‡å­¦æ˜¾ç¤ºå‡ºå°–é”çš„å¾®æŸå‰‚é‡åˆ†å¸ƒæ›²çº¿ï¼Œä¾§å³°ä¹‹é—´å‰‚é‡æ¯”è¶…è¿‡10ï¼Œç©¿é€7å˜ç±³åšçš„PMMAå¹»å½±ã€‚åœ¨è·ç¦»ç„¦ç‚¹15å˜ç±³å¤„ä»¥åŠ é€Ÿç”µå‹ä¸ºä¼ç‰¹å’Œç”µæµä¸ºæ¯«å®‰æ—¶ï¼Œå¼€æ”¾åœºå‰‚é‡ç‡ä¸ºæ¯ç§’å¢åŠ ç°åº¦å•ä½ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥è®¾å¤‡ç”¨äºå¾®å‹æŸå’Œå¾®æŸåº”ç”¨å…·æœ‰å¯è¡Œæ€§ï¼Œåœºå°ºå¯¸åœ¨å˜ç±³å·¦å³ã€‚åœ¨è·Ÿè¸ªæœŸé—´å†…å°é¼ æœªè§‚å¯Ÿåˆ°å…¨èº«ä¸è‰¯ååº”ã€‚å·²æˆåŠŸæ„å»ºå¹¶æµ‹è¯•äº†é¦–ä¾‹LFXTæ¦‚å¿µåŸå‹æœºã€‚ä¸å¸¸è§„Xå°„çº¿ç®¡ç›¸æ¯”ï¼Œè¯¥è®¾å¤‡çš„å‰‚é‡å­¦ç‰¹æ€§è¡¨ç°å‡ºå…‰æŸè´¨é‡çš„ä¼˜è¶Šæ€§ã€‚æœªæ¥å°†å¯¹åŸå‹æœºçš„å‰©ä½™å±€é™æ€§è¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜å¾®å‹æŸå’Œé¦–æ¬¡å¾®æŸæ”¾å°„æ²»ç–—çš„ä¸´åºŠæ²»ç–—æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾®å‹æŸå’Œå¾®æŸæ”¾å°„æ²»ç–—å…·æœ‰å‡å°‘æ­£å¸¸ç»„ç»‡æ¯’æ€§å’Œæé«˜è‚¿ç˜¤æ§åˆ¶ç‡çš„æ½œåŠ›ã€‚</li>
<li>LFXTåŸå‹è¢«å¼€å‘ç”¨äºä¸´åºŠè½¬åŒ–çš„å¾®å‹æŸå’Œå¾®æŸæ²»ç–—ã€‚</li>
<li>LFXTçš„ç„¦ç‚¹ç‰¹å¾æ˜¾ç¤ºå‡ºåå¿ƒç”µå­åˆ†å¸ƒï¼Œå®½åº¦ä¸º72.3Î¼mã€‚</li>
<li>LFXTçš„å‰‚é‡å­¦è¡¨ç°å‡ºå°–é”çš„å¾®æŸå‰‚é‡åˆ†å¸ƒæ›²çº¿å’Œé«˜çš„å‰‚é‡ç‡ã€‚</li>
<li>åœ¨ä½“å¤–å’Œä½“å†…å®éªŒä¸­ï¼ŒLFXTåœ¨å¾®å‹æŸå’Œå¾®æŸåº”ç”¨ä¸­çš„å¯è¡Œæ€§å¾—åˆ°éªŒè¯ã€‚</li>
<li>å°é¼ åœ¨æ¥å—å¾®å‹æŸç…§å°„åæœªå‡ºç°æ˜æ˜¾çš„å‰¯ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fb8f772cc3d7280cbaaa4c001cb9fa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff6295c351cf61d05e4ae86433626dcf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SRPL-SFDA-SAM-Guided-Reliable-Pseudo-Labels-for-Source-Free-Domain-Adaptation-in-Medical-Image-Segmentation"><a href="#SRPL-SFDA-SAM-Guided-Reliable-Pseudo-Labels-for-Source-Free-Domain-Adaptation-in-Medical-Image-Segmentation" class="headerlink" title="SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain   Adaptation in Medical Image Segmentation"></a>SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain   Adaptation in Medical Image Segmentation</h2><p><strong>Authors:Xinya Liu, Jianghao Wu, Tao Lu, Shaoting Zhang, Guotai Wang</strong></p>
<p>Domain Adaptation (DA) is crucial for robust deployment of medical image segmentation models when applied to new clinical centers with significant domain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal with privacy concerns and access constraints on source-domain data during adaptation to target-domain data. However, SFDA faces challenges such as insufficient supervision in the target domain with unlabeled images. In this work, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels method for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch Intensity Enhancement (T3IE) that not only improves quality of raw pseudo-labels in the target domain, but also leads to SAM-compatible inputs with three channels to better leverage SAMâ€™s zero-shot inference ability for refining the pseudo-labels; 2) A reliable pseudo-label selection module that rejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs (CMSO) under input perturbations with T3IE; and 3) A reliability-aware training procedure in the unlabeled target domain where reliable pseudo-labels are used for supervision and unreliable parts are regularized by entropy minimization. Experiments conducted on two multi-domain medical image segmentation datasets for fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA effectively enhances pseudo-label quality in the unlabeled target domain, and improves SFDA performance by leveraging the reliability-aware training; 2) SRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is close to that of supervised training in the target domain. The code of this work is available online: <a target="_blank" rel="noopener" href="https://github.com/HiLab-git/SRPL-SFDA">https://github.com/HiLab-git/SRPL-SFDA</a>. </p>
<blockquote>
<p>é¢†åŸŸé€‚åº”ï¼ˆDAï¼‰å¯¹äºå°†åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åº”ç”¨äºå…·æœ‰æ˜¾è‘—é¢†åŸŸåç§»çš„æ–°ä¸´åºŠä¸­å¿ƒæ—¶ï¼Œç¡®ä¿æ¨¡å‹çš„ç¨³å¥éƒ¨ç½²è‡³å…³é‡è¦ã€‚æ— æºé¢†åŸŸé€‚åº”ï¼ˆSFDAï¼‰å¾ˆæœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒåœ¨é€‚åº”ç›®æ ‡åŸŸæ•°æ®æ—¶ï¼Œèƒ½å¤Ÿåº”å¯¹æºåŸŸæ•°æ®çš„éšç§å…³æ³¨å’Œè®¿é—®çº¦æŸã€‚ç„¶è€Œï¼ŒSFDAé¢ä¸´ç›®æ ‡åŸŸä¸­æ— æ ‡ç­¾å›¾åƒç›‘ç£ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºSFDAçš„åŸºäºSegment Anything Modelï¼ˆSAMï¼‰å¼•å¯¼çš„å¯é ä¼ªæ ‡ç­¾æ–¹æ³•ï¼ˆSRPL-SFDAï¼‰ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š1ï¼‰æµ‹è¯•æ—¶ä¸‰åˆ†æ”¯å¼ºåº¦å¢å¼ºï¼ˆT3IEï¼‰æŠ€æœ¯ï¼Œå®ƒä¸ä»…èƒ½æé«˜ç›®æ ‡åŸŸä¸­åŸå§‹ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œè¿˜èƒ½ç”Ÿæˆä¸SAMå…¼å®¹çš„ä¸‰é€šé“è¾“å…¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨SAMçš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›æ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼›2ï¼‰ä¸€ä¸ªå¯é çš„ä¼ªæ ‡ç­¾é€‰æ‹©æ¨¡å—ï¼Œè¯¥æ¨¡å—åŸºäºè¾“å…¥æ‰°åŠ¨ä¸‹çš„å¤šä¸ªSAMè¾“å‡ºä¸€è‡´æ€§ï¼ˆCMSOï¼‰æ¥æ‹’ç»ä½è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼›3ï¼‰åœ¨æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸä¸­é‡‡ç”¨å¯é æ€§æ„ŸçŸ¥è®­ç»ƒæµç¨‹ï¼Œå…¶ä¸­å¯é çš„ä¼ªæ ‡ç­¾ç”¨äºç›‘ç£ï¼Œä¸å¯é çš„éƒ¨åˆ†åˆ™é€šè¿‡æœ€å°åŒ–ç†µè¿›è¡Œæ­£åˆ™åŒ–ã€‚åœ¨èƒå„¿å¤§è„‘å’Œå‰åˆ—è…ºä¸¤ä¸ªå¤šåŸŸåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼š1ï¼‰SRPL-SFDAé€šè¿‡åˆ©ç”¨å¯é æ€§æ„ŸçŸ¥è®­ç»ƒï¼Œæœ‰æ•ˆæé«˜äº†ç›®æ ‡åŸŸä¸­æ— æ ‡ç­¾å›¾åƒçš„ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå¹¶æå‡äº†SFDAçš„æ€§èƒ½ï¼›2ï¼‰SRPL-SFDAä¼˜äºæœ€æ–°çš„SFDAæ–¹æ³•ï¼Œå…¶æ€§èƒ½æ¥è¿‘ç›®æ ‡åŸŸä¸­çš„æœ‰ç›‘ç£è®­ç»ƒã€‚è¯¥å·¥ä½œçš„ä»£ç å¯åœ¨ç½‘ä¸Šè·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/SRPL-SFDA">https://github.com/HiLab-git/SRPL-SFDA</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09403v1">PDF</a> 18 pages, 4 figures. Accepted for publication in Neurocomputing</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨é¢ä¸´æ–°çš„ä¸´åºŠä¸­å¿ƒæ—¶å‡ºç°é¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”ï¼ˆDAï¼‰ã€‚æ— æºé¢†åŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰å¯ä»¥è§£å†³éšç§é—®é¢˜å’ŒæºåŸŸæ•°æ®è®¿é—®é™åˆ¶çš„é—®é¢˜ã€‚ç„¶è€Œï¼ŒSFDAé¢ä¸´ç›®æ ‡åŸŸä¸­æ— æ ‡ç­¾å›¾åƒç›‘ç£ä¸è¶³çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºSegment Anything Modelï¼ˆSAMï¼‰å¼•å¯¼çš„å¯é ä¼ªæ ‡ç­¾æ–¹æ³•ï¼ˆSRPL-SFDAï¼‰ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæµ‹è¯•æ—¶ä¸‰åˆ†æ”¯å¼ºåº¦å¢å¼ºï¼ˆT3IEï¼‰ï¼Œå¯æé«˜ç›®æ ‡åŸŸä¸­åŸå§‹ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶äº§ç”ŸSAMå…¼å®¹çš„ä¸‰é€šé“è¾“å…¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨SAMçš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›æ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ï¼›å¯é çš„ä¼ªæ ‡ç­¾é€‰æ‹©æ¨¡å—ï¼ŒåŸºäºè¾“å…¥æ‰°åŠ¨ä¸‹çš„å¤šä¸ªSAMè¾“å‡ºçš„ä¸€è‡´æ€§ï¼ˆCMSOï¼‰æ‹’ç»ä½è´¨é‡çš„ä¼ªæ ‡ç­¾ï¼›ä»¥åŠåœ¨æ— æ ‡ç­¾ç›®æ ‡åŸŸä¸­çš„å¯é æ€§æ„ŸçŸ¥è®­ç»ƒç¨‹åºï¼Œå…¶ä¸­å¯é çš„ä¼ªæ ‡ç­¾ç”¨äºç›‘ç£ï¼Œä¸å¯é çš„éƒ¨åˆ†é€šè¿‡æœ€å°åŒ–ç†µè¿›è¡Œæ­£åˆ™åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒSRPL-SFDAæœ‰æ•ˆæé«˜ç›®æ ‡åŸŸä¸­ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶æé«˜äº†SFDAçš„æ€§èƒ½ï¼›ç›¸è¾ƒäºç°æœ‰SFDAæ–¹æ³•ï¼ŒSRPL-SFDAè¡¨ç°æ›´ä¼˜ï¼Œä¸”æ€§èƒ½æ¥è¿‘ç›®æ ‡åŸŸä¸­çš„ç›‘ç£è®­ç»ƒã€‚ç›¸å…³ä»£ç å·²åœ¨çº¿å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢†åŸŸè‡ªé€‚åº”ï¼ˆDAï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹éƒ¨ç½²åˆ°æ–°ä¸´åºŠä¸­å¿ƒæ—¶è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯é¢ä¸´é¢†åŸŸæ¼‚ç§»é—®é¢˜æ—¶ã€‚</li>
<li>æ— æºé¢†åŸŸè‡ªé€‚åº”ï¼ˆSFDAï¼‰èƒ½å¤Ÿåº”å¯¹éšç§å’ŒæºåŸŸæ•°æ®è®¿é—®é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>SFDAé¢ä¸´ç›®æ ‡åŸŸä¸­ç›‘ç£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†SRPL-SFDAæ–¹æ³•ï¼ŒåŒ…å«T3IEã€å¯é çš„ä¼ªæ ‡ç­¾é€‰æ‹©æ¨¡å—å’Œå¯é æ€§æ„ŸçŸ¥è®­ç»ƒç¨‹åºä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>T3IEèƒ½æé«˜ç›®æ ‡åŸŸä¸­ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå¹¶äº§ç”ŸSAMå…¼å®¹çš„ä¸‰é€šé“è¾“å…¥ã€‚</li>
<li>SRPL-SFDAåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰SFDAæ–¹æ³•ï¼Œæ€§èƒ½æ¥è¿‘ç›®æ ‡åŸŸä¸­çš„ç›‘ç£è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1645d6df0caeaaabda9240f4f57dd665.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-325ff04621c1d9b1d1b0dc3f368bc449.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DIsoN-Decentralized-Isolation-Networks-for-Out-of-Distribution-Detection-in-Medical-Imaging"><a href="#DIsoN-Decentralized-Isolation-Networks-for-Out-of-Distribution-Detection-in-Medical-Imaging" class="headerlink" title="DIsoN: Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging"></a>DIsoN: Decentralized Isolation Networks for Out-of-Distribution   Detection in Medical Imaging</h2><p><strong>Authors:Felix Wagner, Pramit Saha, Harry Anthony, J. Alison Noble, Konstantinos Kamnitsas</strong></p>
<p>Safe deployment of machine learning (ML) models in safety-critical domains such as medical imaging requires detecting inputs with characteristics not seen during training, known as out-of-distribution (OOD) detection, to prevent unreliable predictions. Effective OOD detection after deployment could benefit from access to the training data, enabling direct comparison between test samples and the training data distribution to identify differences. State-of-the-art OOD detection methods, however, either discard training data after deployment or assume that test samples and training data are centrally stored together, an assumption that rarely holds in real-world settings. This is because shipping training data with the deployed model is usually impossible due to the size of training databases, as well as proprietary or privacy constraints. We introduce the Isolation Network, an OOD detection framework that quantifies the difficulty of separating a target test sample from the training data by solving a binary classification task. We then propose Decentralized Isolation Networks (DIsoN), which enables the comparison of training and test data when data-sharing is impossible, by exchanging only model parameters between the remote computational nodes of training and deployment. We further extend DIsoN with class-conditioning, comparing a target sample solely with training data of its predicted class. We evaluate DIsoN on four medical imaging datasets (dermatology, chest X-ray, breast ultrasound, histopathology) across 12 OOD detection tasks. DIsoN performs favorably against existing methods while respecting data-privacy. This decentralized OOD detection framework opens the way for a new type of service that ML developers could provide along with their models: providing remote, secure utilization of their training data for OOD detection services. Code will be available upon acceptance at: ***** </p>
<blockquote>
<p>åœ¨å®‰å…¨å…³é”®çš„é¢†åŸŸå¦‚åŒ»å­¦æˆåƒä¸­ï¼Œéƒ¨ç½²æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹çš„å®‰å…¨æ€§éœ€è¦æ£€æµ‹è®­ç»ƒæœŸé—´æœªå‡ºç°çš„è¾“å…¥ç‰¹å¾ï¼Œè¿™è¢«ç§°ä¸ºç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹ï¼Œä»¥é˜²æ­¢ä¸å¯é çš„é¢„æµ‹ã€‚éƒ¨ç½²åçš„æœ‰æ•ˆOODæ£€æµ‹å¯ä»¥å—ç›Šäºè®¿é—®è®­ç»ƒæ•°æ®ï¼Œä»è€Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ ·æœ¬å’Œè®­ç»ƒæ•°æ®åˆ†å¸ƒä¹‹é—´è¿›è¡Œç›´æ¥æ¯”è¾ƒä»¥è¯†åˆ«å·®å¼‚ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•è¦ä¹ˆåœ¨éƒ¨ç½²åä¸¢å¼ƒè®­ç»ƒæ•°æ®ï¼Œè¦ä¹ˆå‡è®¾æµ‹è¯•æ ·æœ¬å’Œè®­ç»ƒæ•°æ®æ˜¯é›†ä¸­å­˜å‚¨åœ¨ä¸€èµ·çš„ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„ç¯å¢ƒä¸­å¾ˆå°‘æˆç«‹ã€‚è¿™æ˜¯å› ä¸ºç”±äºè®­ç»ƒæ•°æ®åº“çš„å¤§å°ä»¥åŠä¸“æœ‰æˆ–éšç§çº¦æŸï¼Œé€šå¸¸ä¸å¯èƒ½å°†è®­ç»ƒæ•°æ®ä¸å·²éƒ¨ç½²çš„æ¨¡å‹ä¸€èµ·å‘é€ã€‚æˆ‘ä»¬å¼•å…¥äº†éš”ç¦»ç½‘ç»œï¼ˆIsolation Networkï¼‰ï¼Œè¿™æ˜¯ä¸€ç§OODæ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è§£å†³äºŒåˆ†ç±»ä»»åŠ¡æ¥é‡åŒ–å°†ç›®æ ‡æµ‹è¯•æ ·æœ¬ä»è®­ç»ƒæ•°æ®ä¸­åˆ†ç¦»çš„éš¾åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†æ•£å¼éš”ç¦»ç½‘ç»œï¼ˆDIsoNï¼‰ï¼Œå½“æ•°æ®å…±äº«ä¸å¯èƒ½æ—¶ï¼Œå®ƒå¯ä»¥é€šè¿‡äº¤æ¢æ¨¡å‹å‚æ•°æ¥æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œåœ¨è®­ç»ƒå’Œéƒ¨ç½²çš„è¿œç¨‹è®¡ç®—èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†DIsoNæ‰©å±•åˆ°ç±»æ¡ä»¶ï¼Œä»…å°†ç›®æ ‡æ ·æœ¬ä¸å…¶é¢„æµ‹ç±»çš„è®­ç»ƒæ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨å››ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ï¼ˆçš®è‚¤ç—…å­¦ã€èƒ¸éƒ¨Xå°„çº¿ã€ä¹³è…ºè¶…å£°ã€ç»„ç»‡ç—…ç†å­¦ï¼‰ä¸Šè¯„ä¼°äº†DIsoNåœ¨12é¡¹OODæ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚DIsoNåœ¨è¡¨ç°ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å°Šé‡æ•°æ®éšç§ã€‚è¿™ç§åˆ†æ•£å¼çš„OODæ£€æµ‹æ¡†æ¶ä¸ºMLå¼€å‘è€…æä¾›äº†ä¸€ç§æ–°çš„æœåŠ¡æ–¹å¼ï¼šåœ¨æ¨¡å‹éƒ¨ç½²æ—¶æä¾›è¿œç¨‹å®‰å…¨åœ°åˆ©ç”¨å…¶è®­ç»ƒæ•°æ®è¿›è¡ŒOODæ£€æµ‹æœåŠ¡ã€‚ä»£ç å°†åœ¨æ¥å—åäºä»¥ä¸‹ç½‘å€æä¾›ï¼š*****ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09024v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§åä¸ºIsolation Networkçš„OODæ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡è§£å†³äºŒåˆ†ç±»ä»»åŠ¡æ¥é‡åŒ–ç›®æ ‡æµ‹è¯•æ ·æœ¬ä¸è®­ç»ƒæ•°æ®åˆ†ç¦»çš„éš¾åº¦ã€‚åœ¨æ•°æ®å…±äº«æ— æ³•å®ç°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»…åœ¨è®­ç»ƒå’Œéƒ¨ç½²çš„è¿œç¨‹è®¡ç®—èŠ‚ç‚¹ä¹‹é—´äº¤æ¢æ¨¡å‹å‚æ•°ï¼Œå®ç°äº†ä¸è®­ç»ƒæ•°æ®çš„æ¯”è¾ƒã€‚è¿›ä¸€æ­¥æ‰©å±•äº†ç±»æ¡ä»¶DIsoNï¼Œä»…å°†ç›®æ ‡æ ·æœ¬ä¸é¢„æµ‹ç±»çš„è®­ç»ƒæ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚åœ¨å››ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šè¯„ä¼°DIsoNçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜å…¶åœ¨OODæ£€æµ‹ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼ŒåŒæ—¶å°Šé‡æ•°æ®éšç§ã€‚è¿™ç§å»ä¸­å¿ƒåŒ–çš„OODæ£€æµ‹æ¡†æ¶ä¸ºMLå¼€å‘è€…æä¾›äº†ä¸€ç§æ–°å‹æœåŠ¡æ–¹å¼ï¼šåœ¨æ¨¡å‹éƒ¨ç½²æ—¶æä¾›è¿œç¨‹ã€å®‰å…¨çš„åˆ©ç”¨è®­ç»ƒæ•°æ®è¿›è¡ŒOODæ£€æµ‹æœåŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åœ¨åŒ»å­¦æˆåƒç­‰å®‰å…¨å…³é”®é¢†åŸŸéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œéœ€è¦è¿›è¡ŒOODæ£€æµ‹çš„é‡è¦æ€§ï¼Œä»¥é˜²æ­¢ä¸å¯é çš„é¢„æµ‹ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•éœ€è¦åœ¨éƒ¨ç½²åè®¿é—®è®­ç»ƒæ•°æ®æˆ–å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å¯ä»¥é›†ä¸­å­˜å‚¨ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­å¹¶ä¸å¸¸è§ã€‚</li>
<li>æå‡ºäº†Isolation Networkæ¡†æ¶æ¥é‡åŒ–æµ‹è¯•æ ·æœ¬ä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„å·®å¼‚ã€‚è¯¥æ¡†æ¶é€šè¿‡è§£å†³äºŒåˆ†ç±»ä»»åŠ¡æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>å¼•å…¥Decentralized Isolation Networks (DIsoN)ï¼Œåœ¨æ•°æ®å…±äº«æ— æ³•å®ç°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡äº¤æ¢æ¨¡å‹å‚æ•°æ¥æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚</li>
<li>DIsoNæ¡†æ¶æ‰©å±•äº†ç±»æ¡ä»¶åŠŸèƒ½ï¼Œä»…å°†ç›®æ ‡æ ·æœ¬ä¸è®­ç»ƒä¸­çš„é¢„æµ‹ç±»çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDIsoNåœ¨OODæ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å°Šé‡æ•°æ®éšç§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de6bdfd4aaf563eff942e34710409360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56ba19fe1f0f5eab06657bb503fa4187.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62efa083c794aade80914b95d6aca820.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SSS-Semi-Supervised-SAM-2-with-Efficient-Prompting-for-Medical-Imaging-Segmentation"><a href="#SSS-Semi-Supervised-SAM-2-with-Efficient-Prompting-for-Medical-Imaging-Segmentation" class="headerlink" title="SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging   Segmentation"></a>SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging   Segmentation</h2><p><strong>Authors:Hongjie Zhu, Xiwei Liu, Rundong Xue, Zeyu Zhang, Yong Xu, Daji Ergu, Ying Cai, Yang Zhao</strong></p>
<p>In the era of information explosion, efficiently leveraging large-scale unlabeled data while minimizing the reliance on high-quality pixel-level annotations remains a critical challenge in the field of medical imaging. Semi-supervised learning (SSL) enhances the utilization of unlabeled data by facilitating knowledge transfer, significantly improving the performance of fully supervised models and emerging as a highly promising research direction in medical image analysis. Inspired by the ability of Vision Foundation Models (e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised SAM-2), a novel approach that leverages SAM-2â€™s robust feature extraction capabilities to uncover latent knowledge in unlabeled medical images, thus effectively enhancing feature support for fully supervised medical image segmentation. Specifically, building upon the single-stream â€œweak-to-strongâ€ consistency regularization framework, this paper introduces a Discriminative Feature Enhancement (DFE) mechanism to further explore the feature discrepancies introduced by various data augmentation strategies across multiple views. By leveraging feature similarity and dissimilarity across multi-scale augmentation techniques, the method reconstructs and models the features, thereby effectively optimizing the salient regions. Furthermore, a prompt generator is developed that integrates Physical Constraints with a Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data, fulfilling SAM-2â€™s requirement for additional prompts. Extensive experiments demonstrate the superiority of the proposed method for semi-supervised medical image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably, SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous state-of-the-art method by +3.65 Dice. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/SSS">https://github.com/AIGeeksGroup/SSS</a>. </p>
<blockquote>
<p>åœ¨ä¿¡æ¯çˆ†ç‚¸çš„æ—¶ä»£ï¼Œå¦‚ä½•åœ¨æœ‰æ•ˆåˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®çš„åŒæ—¶ï¼Œå°½é‡å‡å°‘å¯¹é«˜è´¨é‡åƒç´ çº§æ³¨é‡Šçš„ä¾èµ–ï¼Œä»ç„¶æ˜¯åŒ»å­¦æˆåƒé¢†åŸŸçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é€šè¿‡ä¿ƒè¿›çŸ¥è¯†è¿ç§»ï¼Œæé«˜äº†æ— æ ‡ç­¾æ•°æ®çš„åˆ©ç”¨ç‡ï¼Œæ˜¾è‘—æé«˜äº†å…¨ç›‘ç£æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æˆä¸ºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸä¸€ä¸ªæå…·å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚å—è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAM-2ï¼‰æä¾›ä¸°å¯Œå…ˆéªŒçŸ¥è¯†èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SSSï¼ˆSemi-Supervised SAM-2ï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨SAM-2çš„ç¨³å¥ç‰¹å¾æå–èƒ½åŠ›ï¼Œåœ¨æœªç»æ ‡è®°çš„åŒ»å­¦å›¾åƒä¸­å‘ç°æ½œåœ¨çŸ¥è¯†ï¼Œä»è€Œæœ‰æ•ˆåœ°å¢å¼ºå…¨ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç‰¹å¾æ”¯æŒã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åŸºäºå•æµâ€œå¼±åˆ°å¼ºâ€çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¡†æ¶ï¼Œå¼•å…¥åˆ¤åˆ«ç‰¹å¾å¢å¼ºï¼ˆDFEï¼‰æœºåˆ¶ï¼Œè¿›ä¸€æ­¥æ¢ç´¢ç”±å¤šç§æ•°æ®å¢å¼ºç­–ç•¥åœ¨ä¸åŒè§†å›¾ä¸‹å¼•å…¥çš„ç‰¹å¾å·®å¼‚ã€‚é€šè¿‡åˆ©ç”¨å¤šå°ºåº¦å¢å¼ºæŠ€æœ¯ä¸­çš„ç‰¹å¾ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼Œè¯¥æ–¹æ³•å¯¹ç‰¹å¾è¿›è¡Œé‡å»ºå’Œå»ºæ¨¡ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¼˜åŒ–æ˜¾è‘—åŒºåŸŸã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ä¸ªæç¤ºç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨ç»“åˆäº†ç‰©ç†çº¦æŸä¸æ»‘åŠ¨çª—å£ï¼ˆPCSWï¼‰æœºåˆ¶ï¼Œä¸ºæ— æ ‡ç­¾æ•°æ®ç”Ÿæˆè¾“å…¥æç¤ºï¼Œæ»¡è¶³SAM-2å¯¹é¢å¤–æç¤ºçš„è¦æ±‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ACDCå’ŒBHSDä¸¤ä¸ªå¤šæ ‡ç­¾æ•°æ®é›†ä¸Šè¿›è¡ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ—¶è¡¨ç°ä¼˜è¶Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSSSåœ¨BHSDä¸Šçš„å¹³å‡Diceç³»æ•°ä¸º53.15ï¼Œæ¯”ä¹‹å‰çš„å…ˆè¿›æ–¹æ³•é«˜å‡º+3.65 Diceã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/SSS%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/AIGeeksGroup/SSSä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08949v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œå¦‚ä½•åˆ©ç”¨å¤§é‡æœªæ ‡æ³¨æ•°æ®å¹¶å‡å°‘å¯¹é«˜è´¨é‡åƒç´ çº§æ ‡æ³¨çš„ä¾èµ–ï¼Œæ˜¯åŒ»å­¦å½±åƒé¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é€šè¿‡çŸ¥è¯†è½¬ç§»æé«˜äº†æœªæ ‡æ³¨æ•°æ®çš„åˆ©ç”¨ç‡ï¼Œæå¤§æå‡äº†å…¨ç›‘ç£æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆä¸ºåŒ»å­¦å½±åƒåˆ†æä¸­æå…·å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚å—è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAM-2ï¼‰æä¾›ä¸°å¯Œå…ˆéªŒçŸ¥è¯†èƒ½åŠ›çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†SSSï¼ˆåŸºäºSAM-2çš„åŠç›‘ç£æ–¹æ³•ï¼‰ï¼Œåˆ©ç”¨SAM-2çš„ç¨³å¥ç‰¹å¾æå–èƒ½åŠ›æŒ–æ˜æœªæ ‡æ³¨åŒ»å­¦å½±åƒä¸­çš„æ½œåœ¨çŸ¥è¯†ï¼Œä»è€Œå¢å¼ºå…¨ç›‘ç£åŒ»å­¦å½±åƒåˆ†å‰²çš„ç‰¹å¾æ”¯æŒã€‚é€šè¿‡å¼•å…¥åˆ¤åˆ«ç‰¹å¾å¢å¼ºï¼ˆDFEï¼‰æœºåˆ¶å’Œå¤šå°ºåº¦æ•°æ®å¢å¼ºæŠ€æœ¯çš„ç‰¹å¾ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§é‡å»ºå’Œå»ºæ¨¡ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æ˜¾è‘—åŒºåŸŸã€‚åŒæ—¶å¼€å‘äº†ä¸€ä¸ªæç¤ºç”Ÿæˆå™¨ï¼Œç»“åˆç‰©ç†çº¦æŸå’Œæ»‘åŠ¨çª—å£ï¼ˆPCSWï¼‰æœºåˆ¶ä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆè¾“å…¥æç¤ºï¼Œæ»¡è¶³SAM-2å¯¹é¢å¤–æç¤ºçš„è¦æ±‚ã€‚åœ¨ACDCå’ŒBHSDä¸¤ä¸ªå¤šæ ‡ç­¾æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠç›‘ç£åŒ»å­¦å½±åƒåˆ†å‰²ä¸Šçš„ä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯SSSåœ¨BHSDä¸Šå–å¾—äº†å¹³å‡Diceç³»æ•°53.15çš„é«˜åˆ†ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤§æ•°æ®æ—¶ä»£ï¼Œå¦‚ä½•åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ˜¯åŒ»å­¦å½±åƒåˆ†æçš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­å±•ç°å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æå‡ºSSSæ–¹æ³•ï¼Œç»“åˆSAM-2å’ŒDFEæœºåˆ¶è¿›è¡ŒåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>åˆ©ç”¨å¤šå°ºåº¦æ•°æ®å¢å¼ºæŠ€æœ¯ä¼˜åŒ–æ˜¾è‘—åŒºåŸŸã€‚</li>
<li>å¼€å‘æç¤ºç”Ÿæˆå™¨ä»¥æ»¡è¶³SAM-2çš„é¢å¤–æç¤ºéœ€æ±‚ã€‚</li>
<li>åœ¨ACDCå’ŒBHSDæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†SSSæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-831ccab936dc1d475b89c04f76fa3388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-638d89e25fc2ef54d2512d9854df2753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fc74b469aa219e98d8c357d41f16d7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MIRAGE-Multimodal-foundation-model-and-benchmark-for-comprehensive-retinal-OCT-image-analysis"><a href="#MIRAGE-Multimodal-foundation-model-and-benchmark-for-comprehensive-retinal-OCT-image-analysis" class="headerlink" title="MIRAGE: Multimodal foundation model and benchmark for comprehensive   retinal OCT image analysis"></a>MIRAGE: Multimodal foundation model and benchmark for comprehensive   retinal OCT image analysis</h2><p><strong>Authors:JosÃ© Morano, Botond Fazekas, Emese SÃ¼kei, Ronald Fecso, Taha Emre, Markus Gumpinger, Georg Faustmann, Marzieh Oghbaie, Ursula Schmidt-Erfurth, Hrvoje BogunoviÄ‡</strong></p>
<p>Artificial intelligence (AI) has become a fundamental tool for assisting clinicians in analyzing ophthalmic images, such as optical coherence tomography (OCT). However, developing AI models often requires extensive annotation, and existing models tend to underperform on independent, unseen data. Foundation models (FMs), large AI models trained on vast unlabeled datasets, have shown promise in overcoming these challenges. Nonetheless, available FMs for ophthalmology lack extensive validation, especially for segmentation tasks, and focus on a single imaging modality. In this context, we propose MIRAGE, a novel multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO) images. Additionally, we propose a new evaluation benchmark with OCT&#x2F;SLO classification and segmentation tasks. The comparison with general and specialized FMs and segmentation methods shows the superiority of MIRAGE in both types of tasks, highlighting its suitability as a basis for the development of robust AI systems for retinal OCT image analysis. Both MIRAGE and the evaluation benchmark are publicly available: <a target="_blank" rel="noopener" href="https://github.com/j-morano/MIRAGE">https://github.com/j-morano/MIRAGE</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·²ç»æˆä¸ºè¾…åŠ©ä¸´åºŠåŒ»ç”Ÿåˆ†æçœ¼ç§‘å›¾åƒçš„é‡è¦å·¥å…·ï¼Œå¦‚å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€‚ç„¶è€Œï¼Œå¼€å‘AIæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨ï¼Œç°æœ‰æ¨¡å‹åœ¨æœªè§è¿‡çš„ç‹¬ç«‹æ•°æ®ä¸Šå¾€å¾€è¡¨ç°ä¸ä½³ã€‚é¢„è®­ç»ƒæ¨¡å‹ï¼ˆFMsï¼‰æ˜¯åœ¨å¤§é‡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„åºå¤§AIæ¨¡å‹ï¼Œåœ¨å…‹æœè¿™äº›æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çœ¼ç§‘é¢„è®­ç»ƒæ¨¡å‹ç¼ºä¹å¹¿æ³›éªŒè¯ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†å‰²ä»»åŠ¡æ–¹é¢ï¼Œä¸”åªå…³æ³¨å•ä¸€æˆåƒæ¨¡å¼ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†MIRAGEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåˆ†æOCTå’Œæ‰«ææ¿€å…‰çœ¼ç§‘æ£€æŸ¥ï¼ˆSLOï¼‰å›¾åƒçš„æ–°å‹å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬OCT&#x2F;SLOåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚ä¸ä¸€èˆ¬å’Œä¸“ä¸šçš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼ŒMIRAGEåœ¨è¿™ä¸¤ç§ç±»å‹çš„ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œå‡¸æ˜¾å…¶ä½œä¸ºå¼€å‘ç”¨äºè§†ç½‘è†œOCTå›¾åƒåˆ†æçš„ç¨³å¥AIç³»ç»Ÿçš„åŸºç¡€çš„é€‚ç”¨æ€§ã€‚MIRAGEå’Œè¯„ä¼°åŸºå‡†å‡å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/j-morano/MIRAGE">https://github.com/j-morano/MIRAGE</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08900v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·²æˆä¸ºè¾…åŠ©åŒ»ç”Ÿåˆ†æçœ¼ç§‘å›¾åƒçš„é‡è¦å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰æ–¹é¢ã€‚ç„¶è€Œï¼Œå¼€å‘AIæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼Œç°æœ‰æ¨¡å‹åœ¨æœªè§è¿‡çš„ç‹¬ç«‹æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºé‡‡ç”¨åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰â€”â€”åœ¨å¤§é‡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤§å‹AIæ¨¡å‹â€”â€”æ¥è§£å†³æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çœ¼ç§‘é¢†åŸŸåŸºç¡€æ¨¡å‹ç¼ºä¹å¹¿æ³›éªŒè¯ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œä¸”ä¸»è¦å…³æ³¨å•ä¸€æˆåƒæ¨¡å¼ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹MIRAGEï¼Œç”¨äºåˆ†æOCTå’Œæ‰«ææ¿€å…‰çœ¼ç§‘ï¼ˆSLOï¼‰å›¾åƒã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬OCT&#x2F;SLOåˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚ä¸é€šç”¨å’Œä¸“ç”¨åŸºç¡€æ¨¡å‹åŠåˆ†å‰²æ–¹æ³•çš„æ¯”è¾ƒæ˜¾ç¤ºï¼ŒMIRAGEåœ¨ä¸¤ç±»ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé€‚åˆä½œä¸ºå¼€å‘è§†ç½‘è†œOCTå›¾åƒåˆ†æç¨³å¥AIç³»ç»Ÿçš„åŸºçŸ³ã€‚MIRAGEå’ŒåŸºç¡€æ¨¡å‹è¯„ä¼°åŸºå‡†å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨çœ¼ç§‘å›¾åƒåˆ†æä¸­çš„åº”ç”¨é€æ¸æ™®åŠï¼Œç‰¹åˆ«æ˜¯åœ¨OCTå›¾åƒåˆ†ææ–¹é¢ã€‚</li>
<li>å¼€å‘AIæ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯æ•°æ®æ ‡æ³¨çš„éœ€æ±‚é‡å¤§ä»¥åŠæ¨¡å‹åœ¨ç‹¬ç«‹æ•°æ®ä¸Šçš„è¡¨ç°ä¸ç¨³å®šã€‚</li>
<li>åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ä½œä¸ºä¸€ç§åœ¨å¤§é‡æ— æ ‡ç­¾æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¤§å‹AIæ¨¡å‹ï¼Œæœ‰åŠ©äºè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å½“å‰çœ¼ç§‘é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ç¼ºä¹å¹¿æ³›éªŒè¯ï¼Œå°¤å…¶åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œä¸”ä¸»è¦å…³æ³¨å•ä¸€æˆåƒæ¨¡å¼ã€‚</li>
<li>MIRAGEæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨åˆ†æOCTå’ŒSLOå›¾åƒã€‚</li>
<li>MIRAGEæ¨¡å‹å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e7e5a62c4d71c940101e78917946ec8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d187f9670ed13e6689577b08d2024865.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cea4f9f15bf5ff259db25b1f105b545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2812aa62297a6d60f1989f437ff4a637.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Adapting-Vision-Language-Foundation-Model-for-Next-Generation-Medical-Ultrasound-Image-Analysis"><a href="#Adapting-Vision-Language-Foundation-Model-for-Next-Generation-Medical-Ultrasound-Image-Analysis" class="headerlink" title="Adapting Vision-Language Foundation Model for Next Generation Medical   Ultrasound Image Analysis"></a>Adapting Vision-Language Foundation Model for Next Generation Medical   Ultrasound Image Analysis</h2><p><strong>Authors:Jingguo Qu, Xinyang Han, Tonghuan Xiao, Jia Ai, Juan Wu, Tong Zhao, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying</strong></p>
<p>Medical ultrasonography is an essential imaging technique for examining superficial organs and tissues, including lymph nodes, breast, and thyroid. It employs high-frequency ultrasound waves to generate detailed images of the internal structures of the human body. However, manually contouring regions of interest in these images is a labor-intensive task that demands expertise and often results in inconsistent interpretations among individuals. Vision-language foundation models, which have excelled in various computer vision applications, present new opportunities for enhancing ultrasound image analysis. Yet, their performance is hindered by the significant differences between natural and medical imaging domains. This research seeks to overcome these challenges by developing domain adaptation methods for vision-language foundation models. In this study, we explore the fine-tuning pipeline for vision-language foundation models by utilizing large language model as text refiner with special-designed adaptation strategies and task-driven heads. Our approach has been extensively evaluated on six ultrasound datasets and two tasks: segmentation and classification. The experimental results show that our method can effectively improve the performance of vision-language foundation models for ultrasound image analysis, and outperform the existing state-of-the-art vision-language and pure foundation models. The source code of this study is available at <a target="_blank" rel="noopener" href="https://github.com/jinggqu/NextGen-UIA">https://github.com/jinggqu/NextGen-UIA</a>. </p>
<blockquote>
<p>åŒ»å­¦è¶…å£°æ˜¯æ£€æŸ¥æµ…è¡¨å™¨å®˜å’Œç»„ç»‡ï¼ˆåŒ…æ‹¬æ·‹å·´ç»“ã€ä¹³æˆ¿å’Œç”²çŠ¶è…ºï¼‰çš„é‡è¦æˆåƒæŠ€æœ¯ã€‚å®ƒåˆ©ç”¨é«˜é¢‘è¶…å£°æ³¢ç”Ÿæˆäººä½“å†…éƒ¨ç»“æ„çš„è¯¦ç»†å›¾åƒã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›å›¾åƒä¸­æ‰‹åŠ¨è½®å»“æ„Ÿå…´è¶£åŒºåŸŸæ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¸”åœ¨ä¸åŒä¸ªä½“ä¹‹é—´å¸¸å¸¸å¯¼è‡´è§£é‡Šä¸ä¸€è‡´ã€‚è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨å„ç§è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºå¢å¼ºè¶…å£°å›¾åƒåˆ†ææä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œç”±äºå…¶æ˜¾è‘—çš„è‡ªç„¶å’ŒåŒ»å­¦å½±åƒåŸŸä¹‹é—´çš„å·®å¼‚ï¼Œå…¶æ€§èƒ½å—åˆ°äº†é˜»ç¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼€å‘è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„åŸŸé€‚åº”æ–¹æ³•æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„å¾®è°ƒç®¡é“ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬ç²¾ç‚¼å™¨ï¼Œé‡‡ç”¨ç‰¹æ®Šè®¾è®¡çš„é€‚åº”ç­–ç•¥å’Œä»»åŠ¡é©±åŠ¨å¤´ã€‚æˆ‘ä»¬çš„æ–¹æ³•å·²åœ¨å…­ä¸ªè¶…å£°æ•°æ®é›†å’Œä¸¤ä¸ªä»»åŠ¡ï¼ˆåˆ†å‰²å’Œåˆ†ç±»ï¼‰ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨è¶…å£°å›¾åƒåˆ†æä¸­çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€å’Œçº¯åŸºç¡€æ¨¡å‹ã€‚æœ¬ç ”ç©¶çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jinggqu/NextGen-UIA">https://github.com/jinggqu/NextGen-UIA</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08849v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŒ»å­¦è¶…å£°æˆåƒæŠ€æœ¯åœ¨æµ…è¡¨å™¨å®˜å’Œç»„ç»‡æ£€æŸ¥ä¸­çš„åº”ç”¨ï¼Œå¦‚æ·‹å·´ç»“ã€ä¹³æˆ¿å’Œç”²çŠ¶è…ºã€‚æ‰‹åŠ¨åœ¨è¿™äº›å›¾åƒä¸­æç»˜æ„Ÿå…´è¶£åŒºåŸŸæ˜¯ä¸€é¡¹åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡ï¼Œéœ€æ±‚é«˜åº¦ä¸“ä¸šä¸”å¸¸å¯¼è‡´ä¸åŒäººä¹‹é—´çš„è§£è¯»ä¸ä¸€è‡´ã€‚ç ”ç©¶é‡‡ç”¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡é¢†åŸŸé€‚åº”æ–¹æ³•ä¸ºè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹å¼€å‘é€‚é…ç­–ç•¥ï¼Œåœ¨è¶…å£°å›¾åƒåˆ†æä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆæå‡è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨è¶…å£°å›¾åƒåˆ†æä¸Šçš„æ€§èƒ½ï¼Œå¹¶è¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ¨¡å‹å’Œçº¯åŸºç¡€æ¨¡å‹ã€‚ç›¸å…³ç ”ç©¶ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è¶…å£°æˆåƒæŠ€æœ¯å¹¿æ³›åº”ç”¨äºæµ…è¡¨å™¨å®˜å’Œç»„ç»‡çš„æ£€æŸ¥ï¼Œå¦‚æ·‹å·´ç»“ã€ä¹³æˆ¿å’Œç”²çŠ¶è…ºã€‚</li>
<li>æ‰‹åŠ¨æç»˜è¶…å£°å›¾åƒä¸­çš„æ„Ÿå…´è¶£åŒºåŸŸæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œéœ€è¦ä¸“ä¸šæŠ€èƒ½ä¸”æ˜“å‡ºç°è§£è¯»ä¸ä¸€è‡´ã€‚</li>
<li>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸Šå…·æœ‰æ½œåŠ›ï¼Œä½†å…¶åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸå’Œè‡ªç„¶å›¾åƒé¢†åŸŸçš„å·®å¼‚é™åˆ¶äº†å…¶æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨é¢†åŸŸé€‚åº”ç­–ç•¥æ¥å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ–‡æœ¬ç²¾ç‚¼å™¨ï¼Œå¹¶è®¾è®¡ç‰¹æ®Šé€‚é…ç­–ç•¥å’Œä»»åŠ¡é©±åŠ¨å¤´ã€‚</li>
<li>è¯¥æ–¹æ³•ç»è¿‡åœ¨å…­ä¸ªè¶…å£°æ•°æ®é›†å’Œä¸¤ä¸ªä»»åŠ¡ï¼ˆåˆ†å‰²å’Œåˆ†ç±»ï¼‰ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œçº¯åŸºç¡€æ¨¡å‹ï¼Œåœ¨è¶…å£°å›¾åƒåˆ†æä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-076a8fb72100573ae57993685e593fd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dae3fe7fee91175a0007cb636e21f243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b3d110fff5eda59fe0c5c448c6e06f7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Modern-approach-to-muonic-x-ray-spectroscopy-demonstrated-through-the-measurement-of-stable-Cl-radii"><a href="#Modern-approach-to-muonic-x-ray-spectroscopy-demonstrated-through-the-measurement-of-stable-Cl-radii" class="headerlink" title="Modern approach to muonic x-ray spectroscopy demonstrated through the   measurement of stable Cl radii"></a>Modern approach to muonic x-ray spectroscopy demonstrated through the   measurement of stable Cl radii</h2><p><strong>Authors:K. A. Beyer, T. E. Cocolios, C. Costache, M. Deseyn, P. Demol, A. Doinaki, O. Eizenberg, M. Gorshteyn, M. Heines, A. HerzÃ¡Åˆ, P. Indelicato, K. Kirch, A. Knecht, R. Lica, V. Matousek, E. A. Maugeri, B. Ohayon, N. S. Oreshkina, W. W. M. M. Phyo, R. Pohl, S. Rathi, W. Ryssens, A. Turturica, K. von Schoeler, I. A. Valuev, S. M. Vogiatzi, F. Wauters, A. Zendour</strong></p>
<p>Recent advances in muonic x-ray experiments have reinvigorated efforts in measurements of absolute nuclear charge radii. Here, a modern approach is presented, and demonstrated through determination of the charge radii of the two stable chlorine nuclides $^{35}$Cl and $^{37}$Cl. Knowledge of these radii has implications for fundamental studies in nuclear and atomic physics. For this purpose, a state-of-the-art experiment was performed at the $\pi$E1 beamline in the Paul Scherrer Institute (Switzerland), using a large-scale HPGe detector array in order to extract precise energies of the muonic $^{35}$Cl and $^{37}$Cl $np1s$ transitions. The nuclear charge radius extraction relies on modern calculations for QED effects and nuclear polarization with rigorous uncertainty quantification, including effects that were not accounted for in older studies. Additionally, we established a new method for applying the nuclear shape correction directly from energy density functionals, which are amenable to isotopes for which no high-quality electron scattering experiments are available. The resulting charge radii are $3.3335(23) fm$ for $^{35}$Cl and $3.3445(23) fm$ for $^{37}$Cl, thus improving the uncertainty of the available electron scattering values by a factor of seven. The correlation of several observables was evaluated between the different isotopes in order to produce a more precise value of the differential mean square charge radius $\delta \langle r^2 \rangle^{37, 35}&#x3D;+0.0771(66) fm^{2}$. In this case, improvement of the uncertainty by more than one order of magnitude was achieved compared to the literature value. This precision is sufficient to use this differential as input for isotope shift factor determination. </p>
<blockquote>
<p>è¿‘æœŸÎ¼å­Xå°„çº¿å®éªŒçš„æ–°è¿›å±•é‡æ–°æ¿€å‘äº†ç»å¯¹æ ¸ç”µè·åŠå¾„æµ‹é‡çš„åŠªåŠ›ã€‚åœ¨æ­¤ï¼Œä»‹ç»äº†ä¸€ç§ç°ä»£æ–¹æ³•ï¼Œå¹¶é€šè¿‡æµ‹å®šä¸¤ä¸ªç¨³å®šæ°¯æ ¸ç´ $^{35}$Clå’Œ$^{37}$Clçš„ç”µè·åŠå¾„æ¥å±•ç¤ºã€‚è¿™äº›åŠå¾„çš„çŸ¥è¯†å¯¹æ ¸ç‰©ç†å’ŒåŸå­ç‰©ç†çš„åŸºç¡€ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¸ºæ­¤ï¼Œåœ¨ä¿ç½—è°¢å°”ç ”ç©¶æ‰€ï¼ˆç‘å£«ï¼‰çš„$\pi$E1å…‰æŸçº¿ä¸Šè¿›è¡Œäº†ä¸€é¡¹æœ€å…ˆè¿›çš„å®éªŒï¼Œä½¿ç”¨å¤§è§„æ¨¡HPGeæ¢æµ‹å™¨é˜µåˆ—æå–Î¼å­$^{35}$Clå’Œ$^{37}$Clçš„$np1s$è·ƒè¿çš„ç²¾ç¡®èƒ½é‡ã€‚æ ¸ç”µè·åŠå¾„çš„æå–ä¾èµ–äºå¯¹é‡å­ç”µåŠ¨åŠ›å­¦æ•ˆåº”å’Œæ ¸æåŒ–çš„ç°ä»£è®¡ç®—ï¼Œå¹¶è¿›è¡Œäº†ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ŒåŒ…æ‹¬å¯¹æ—§ç ”ç©¶ä¸­æœªè€ƒè™‘çš„å› ç´ çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç›´æ¥ä»èƒ½é‡å¯†åº¦å‡½æ•°åº”ç”¨æ ¸å½¢çŠ¶æ ¡æ­£ï¼Œè¿™å¯¹äºæ²¡æœ‰é«˜è´¨é‡ç”µå­æ•£å°„å®éªŒçš„åŒä½ç´ æ˜¯å¯è¡Œçš„ã€‚å¾—åˆ°çš„ç”µè·åŠå¾„ä¸º$^{35}$Clçš„$3.3335(23)fm$å’Œ$^{37}$Clçš„$3.3445(23)fm$ï¼Œä»è€Œå°†ç°æœ‰ç”µå­æ•£å°„å€¼çš„ä¸ç¡®å®šæ€§é™ä½äº†ä¸ƒå€ã€‚ä¸ºäº†å¾—åˆ°æ›´ç²¾ç¡®çš„å‡æ–¹ç”µè·åŠå¾„å·®å€¼$\delta \langle r^{2} \rangle^{37, 35}&#x3D;+0.0771(66)fm^{2}$çš„å€¼ï¼Œæˆ‘ä»¬å¯¹ä¸åŒåŒä½ç´ ä¹‹é—´çš„å‡ ä¸ªè§‚æµ‹å€¼è¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸æ–‡çŒ®å€¼ç›¸æ¯”ï¼Œä¸ç¡®å®šåº¦çš„é™ä½å¹…åº¦è¶…è¿‡äº†ä¸€ä¸ªæ•°é‡çº§ã€‚è¿™ç§ç²¾ç¡®åº¦è¶³ä»¥å°†æ­¤å·®å¼‚ç”¨ä½œåŒä½ç´ ä½ç§»å› å­æµ‹å®šçš„è¾“å…¥å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨ç‘å£«ä¿ç½—è°¢å°”ç ”ç©¶æ‰€åˆ©ç”¨å…ˆè¿›æŠ€æœ¯å’Œå¤§å‹HPGeæ¢æµ‹å™¨é˜µåˆ—è¿›è¡Œçš„æœ€æ–°muonic xå°„çº¿å®éªŒçš„æˆæœã€‚è¯¥ç ”ç©¶é€šè¿‡ç²¾å¯†æµ‹é‡æ°¯çš„åŒä½ç´ $^{35}$Clå’Œ$^{37}$Clçš„ç”µè·åŠå¾„ï¼Œæé«˜äº†æ ¸ç‰©ç†å’ŒåŸå­ç‰©ç†åŸºç¡€ç ”ç©¶çš„è®¤è¯†ã€‚é€šè¿‡ä¸¥æ ¼çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè¯¥ç ”ç©¶è€ƒè™‘äº†é‡å­åŠ›å­¦æ•ˆåº”å’Œæ ¸æåŒ–æ•ˆåº”çš„ç°ä»£è®¡ç®—ï¼Œå¹¶å»ºç«‹äº†æ–°çš„æ ¸å½¢çŠ¶æ ¡æ­£æ–¹æ³•ã€‚æœ€ç»ˆå¾—åˆ°çš„ç”µè·åŠå¾„å€¼æé«˜äº†ç”µå­æ•£å°„å®éªŒå€¼çš„ç²¾åº¦ã€‚è¯¥ç ”ç©¶å¯¹åŒä½ç´ é—´å¯è§‚æµ‹é‡çš„ç›¸å…³æ€§è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ºå·®å¼‚å¹³å‡å¹³æ–¹ç”µè·åŠå¾„æä¾›äº†æ›´ç²¾ç¡®çš„å€¼ï¼Œå¹¶å°†ä¸ç¡®å®šæ€§é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ä»¥ä¸Šã€‚è¿™äº›ç²¾ç¡®æ•°æ®å¯ç”¨äºåŒä½ç´ ä½ç§»å› å­çš„ç¡®å®šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°muonic xå°„çº¿å®éªŒé€šè¿‡ç²¾å¯†æµ‹é‡$^{35}$Clå’Œ$^{37}$Clçš„ç”µè·åŠå¾„é‡æ–°æ¿€å‘äº†å¯¹ç»å¯¹æ ¸ç”µè·åŠå¾„æµ‹é‡çš„å…´è¶£ã€‚</li>
<li>åœ¨ç‘å£«ä¿ç½—è°¢å°”ç ”ç©¶æ‰€çš„å…ˆè¿›å®éªŒä½¿ç”¨å¤§å‹HPGeæ¢æµ‹å™¨é˜µåˆ—ç²¾ç¡®æå–äº†muonicæ°¯åŒä½ç´ çš„èƒ½é‡ã€‚</li>
<li>è€ƒè™‘åˆ°é‡å­åŠ›å­¦æ•ˆåº”å’Œæ ¸æåŒ–çš„ç°ä»£è®¡ç®—ï¼Œä¸¥æ ¼åœ°é‡åŒ–äº†ä¸ç¡®å®šæ€§ã€‚</li>
<li>å»ºç«‹äº†æ–°çš„æ–¹æ³•åº”ç”¨æ ¸å½¢çŠ¶æ ¡æ­£ï¼Œç›´æ¥ä»èƒ½é‡å¯†åº¦åŠŸèƒ½ä¸­è·å¾—ï¼Œé€‚ç”¨äºæ²¡æœ‰é«˜è´¨é‡ç”µå­æ•£å°„å®éªŒçš„åŒä½ç´ ã€‚</li>
<li>è·å¾—çš„ç”µè·åŠå¾„å€¼æé«˜äº†ç”µå­æ•£å°„å®éªŒå€¼çš„ç²¾åº¦ï¼Œå¹¶å‡å°‘äº†ä¸ç¡®å®šæ€§çš„å¤§å°ã€‚</li>
<li>é€šè¿‡è¯„ä¼°ä¸åŒåŒä½ç´ ä¹‹é—´çš„å¯è§‚æµ‹é‡çš„ç›¸å…³æ€§ï¼Œå¾—å‡ºäº†æ›´ç²¾ç¡®çš„å·®å¼‚åŒ–å¹³å‡å¹³æ–¹ç”µè·åŠå¾„å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3eb6187c6e9d68cdad53d3662a9fe866.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c12f7a58b8d87dd5f63332714833334a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3983cb8312d5f144deb1c4b7543c051.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a7b21c68fc7a9623da8621cf5a5a2ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae9174bbf91f436bd4cef1b7c7789136.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cadebf74a557517c78057783c8d148f9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Generative-Learning-of-Differentiable-Object-Models-for-Compositional-Interpretation-of-Complex-Scenes"><a href="#Generative-Learning-of-Differentiable-Object-Models-for-Compositional-Interpretation-of-Complex-Scenes" class="headerlink" title="Generative Learning of Differentiable Object Models for Compositional   Interpretation of Complex Scenes"></a>Generative Learning of Differentiable Object Models for Compositional   Interpretation of Complex Scenes</h2><p><strong>Authors:Antoni Nowinowski, Krzysztof Krawiec</strong></p>
<p>This study builds on the architecture of the Disentangler of Visual Priors (DVP), a type of autoencoder that learns to interpret scenes by decomposing the perceived objects into independent visual aspects of shape, size, orientation, and color appearance. These aspects are expressed as latent parameters which control a differentiable renderer that performs image reconstruction, so that the model can be trained end-to-end with gradient using reconstruction loss. In this study, we extend the original DVP so that it can handle multiple objects in a scene. We also exploit the interpretability of its latent by using the decoder to sample additional training examples and devising alternative training modes that rely on loss functions defined not only in the image space, but also in the latent space. This significantly facilitates training, which is otherwise challenging due to the presence of extensive plateaus in the image-space reconstruction loss. To examine the performance of this approach, we propose a new benchmark featuring multiple 2D objects, which subsumes the previously proposed Multi-dSprites dataset while being more parameterizable. We compare the DVP extended in these ways with two baselines (MONet and LIVE) and demonstrate its superiority in terms of reconstruction quality and capacity to decompose overlapping objects. We also analyze the gradients induced by the considered loss functions, explain how they impact the efficacy of training, and discuss the limitations of differentiable rendering in autoencoders and the ways in which they can be addressed. </p>
<blockquote>
<p>æœ¬ç ”ç©¶åŸºäºè§†è§‰å…ˆéªŒè§£æ„å™¨ï¼ˆDVPï¼‰æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç¼–ç å™¨ï¼Œå®ƒé€šè¿‡åˆ†è§£æ„ŸçŸ¥åˆ°çš„å¯¹è±¡ä¸ºå…¶ç‹¬ç«‹çš„è§†è§‰æ–¹é¢ï¼ˆå½¢çŠ¶ã€å¤§å°ã€æ–¹å‘å’Œé¢œè‰²å¤–è§‚ï¼‰æ¥ç†è§£åœºæ™¯ã€‚è¿™äº›æ–¹é¢è¢«è¡¨è¾¾ä¸ºæ½œåœ¨å‚æ•°ï¼Œè¿™äº›å‚æ•°æ§åˆ¶ä¸€ä¸ªå¯å¾®æ¸²æŸ“å™¨è¿›è¡Œå›¾åƒé‡å»ºï¼Œä»è€Œä½¿æ¨¡å‹å¯ä»¥ä½¿ç”¨é‡å»ºæŸå¤±ä¸æ¢¯åº¦è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ‰©å±•äº†åŸå§‹çš„DVPï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†åœºæ™¯ä¸­çš„å¤šä¸ªå¯¹è±¡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è§£ç å™¨é‡‡æ ·é¢å¤–çš„è®­ç»ƒæ ·æœ¬ï¼Œå¹¶è®¾è®¡ä¾èµ–äºä¸ä»…åœ¨å›¾åƒç©ºé—´ä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°è€Œä¸”åœ¨æ½œåœ¨ç©ºé—´ä¸­å®šä¹‰çš„æŸå¤±å‡½æ•°çš„æ›¿ä»£è®­ç»ƒæ¨¡å¼ï¼Œä»è€Œåˆ©ç”¨æ½œåœ¨çš„å¯è§£é‡Šæ€§ã€‚è¿™æå¤§åœ°ä¿ƒè¿›äº†è®­ç»ƒï¼Œå¦åˆ™ç”±äºå›¾åƒç©ºé—´é‡å»ºæŸå¤±ä¸­å­˜åœ¨å¤§é‡çš„å¹³ç¨³æœŸè€Œé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†æ£€éªŒè¿™ç§æ–¹æ³•çš„æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŒ…å«å¤šä¸ªäºŒç»´å¯¹è±¡çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™ä¸ªåŸºå‡†æµ‹è¯•æ¶µç›–äº†ä¹‹å‰æå‡ºçš„Multi-dSpritesæ•°æ®é›†å¹¶ä¸”æ›´åŠ å¯å‚æ•°åŒ–ã€‚æˆ‘ä»¬å°†ä»¥è¿™ç§æ–¹å¼æ‰©å±•çš„DVPä¸ä¸¤ä¸ªåŸºå‡†çº¿ï¼ˆMONetå’ŒLIVEï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨é‡å»ºè´¨é‡å’Œåˆ†è§£é‡å å¯¹è±¡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ‰€è€ƒè™‘çš„æŸå¤±å‡½æ•°å¼•èµ·çš„æ¢¯åº¦ï¼Œè§£é‡Šäº†å®ƒä»¬å¦‚ä½•å½±å“è®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è®¨è®ºäº†è‡ªç¼–ç å™¨ä¸­å¯å¾®åˆ†æ¸²æŸ“çš„å±€é™æ€§ä»¥åŠè§£å†³è¿™äº›é—®é¢˜çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08191v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶åŸºäºè§†è§‰å…ˆéªŒè§£çº ç¼ å™¨ï¼ˆDVPï¼‰æ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç¼–ç å™¨ï¼Œé€šè¿‡åˆ†è§£æ„ŸçŸ¥åˆ°çš„å¯¹è±¡æˆç‹¬ç«‹çš„è§†è§‰æ–¹é¢ï¼ˆå½¢çŠ¶ã€å¤§å°ã€æ–¹å‘å’Œé¢œè‰²å¤–è§‚ï¼‰æ¥è§£é‡Šåœºæ™¯ã€‚é€šè¿‡è¡¨è¾¾è¿™äº›æ–¹é¢ä½œä¸ºæ§åˆ¶å¯å¾®åˆ†æ¸²æŸ“å™¨çš„æ½œåœ¨å‚æ•°ï¼Œè¿›è¡Œå›¾åƒé‡å»ºï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥ä½¿ç”¨æ¢¯åº¦è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒé‡å»ºæŸå¤±ã€‚æœ¬ç ”ç©¶æ‰©å±•äº†åŸå§‹çš„DVPï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†åœºæ™¯ä¸­çš„å¤šä¸ªå¯¹è±¡ã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ©ç”¨æ½œåœ¨çš„å¯è§£é‡Šæ€§ï¼Œä½¿ç”¨è§£ç å™¨å¯¹é¢å¤–çš„è®­ç»ƒæ ·æœ¬è¿›è¡Œé‡‡æ ·å¹¶è®¾è®¡æ›¿ä»£è®­ç»ƒæ¨¡å¼ï¼Œè¿™äº›è®­ç»ƒæ¨¡å¼ä¾èµ–äºä¸ä»…åœ¨å›¾åƒç©ºé—´å®šä¹‰çš„æŸå¤±å‡½æ•°ï¼Œè€Œä¸”åœ¨æ½œåœ¨ç©ºé—´å®šä¹‰çš„æŸå¤±å‡½æ•°ã€‚è¿™æå¤§åœ°ä¿ƒè¿›äº†è®­ç»ƒï¼Œå¦åˆ™ç”±äºå›¾åƒç©ºé—´é‡å»ºæŸå¤±ä¸­å­˜åœ¨å¤§é‡é«˜åŸè€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†æ£€éªŒè¯¥æ–¹æ³•çš„æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«å¤šä¸ªäºŒç»´å¯¹è±¡çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ—¢åŒ…å«äº†å…ˆå‰æå‡ºçš„Multi-dSpritesæ•°æ®é›†åˆæ›´å…·å¯å‚æ•°åŒ–æ€§ã€‚æˆ‘ä»¬å°†æ‰©å±•åçš„DVPä¸ä¸¤ä¸ªåŸºå‡†çº¿ï¼ˆMONetå’ŒLIVEï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œåœ¨é‡å»ºè´¨é‡å’Œåˆ†è§£é‡å å¯¹è±¡æ–¹é¢è¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚åŒæ—¶åˆ†æäº†æ‰€è€ƒè™‘çš„æŸå¤±å‡½æ•°å¼•èµ·çš„æ¢¯åº¦ï¼Œè§£é‡Šäº†å®ƒä»¬å¯¹è®­ç»ƒæ•ˆæœçš„å½±å“ä»¥åŠé’ˆå¯¹è‡ªç¼–ç å™¨ä¸­å¯å¾®åˆ†æ¸²æŸ“çš„å±€é™æ€§åŠå…¶è§£å†³æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åŸºäºDisentangler of Visual Priorsï¼ˆDVPï¼‰æ¶æ„è¿›è¡Œæ‰©å±•ï¼Œä½¿è‡ªç¼–ç å™¨èƒ½å¤Ÿå¤„ç†åœºæ™¯ä¸­çš„å¤šä¸ªå¯¹è±¡ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨DVPçš„æ½œåœ¨å¯è§£é‡Šæ€§ï¼Œé‡‡ç”¨è§£ç å™¨é‡‡æ ·é¢å¤–è®­ç»ƒæ ·æœ¬å¹¶å¼€å‘æ›¿ä»£è®­ç»ƒæ¨¡å¼ã€‚</li>
<li>æŸå¤±å‡½æ•°ä¸ä»…åœ¨å›¾åƒç©ºé—´å®šä¹‰ï¼Œè€Œä¸”åœ¨æ½œåœ¨ç©ºé—´å®šä¹‰ï¼Œä¿ƒè¿›äº†è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŒ…å«å¤šä¸ªäºŒç»´å¯¹è±¡çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ‰©å±•åçš„DVPåœ¨é‡å»ºè´¨é‡å’Œåˆ†è§£é‡å å¯¹è±¡æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>åˆ†æäº†ä¸åŒæŸå¤±å‡½æ•°å¯¹è®­ç»ƒæ•ˆæœçš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08191">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-43266cd0b8ebce06a73cd466935d115a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fb012cfbb4bc3295d898e5d824d862e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be7547e7c7299941166e2b1cf090cdd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f0fb8062870306aa28a19d893c95600.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-System-for-Accurate-Tracking-and-Video-Recordings-of-Rodent-Eye-Movements-using-Convolutional-Neural-Networks-for-Biomedical-Image-Segmentation"><a href="#A-System-for-Accurate-Tracking-and-Video-Recordings-of-Rodent-Eye-Movements-using-Convolutional-Neural-Networks-for-Biomedical-Image-Segmentation" class="headerlink" title="A System for Accurate Tracking and Video Recordings of Rodent Eye   Movements using Convolutional Neural Networks for Biomedical Image   Segmentation"></a>A System for Accurate Tracking and Video Recordings of Rodent Eye   Movements using Convolutional Neural Networks for Biomedical Image   Segmentation</h2><p><strong>Authors:Isha Puri, David Cox</strong></p>
<p>Research in neuroscience and vision science relies heavily on careful measurements of animal subjectâ€™s gaze direction. Rodents are the most widely studied animal subjects for such research because of their economic advantage and hardiness. Recently, video based eye trackers that use image processing techniques have become a popular option for gaze tracking because they are easy to use and are completely noninvasive. Although significant progress has been made in improving the accuracy and robustness of eye tracking algorithms, unfortunately, almost all of the techniques have focused on human eyes, which does not account for the unique characteristics of the rodent eye images, e.g., variability in eye parameters, abundance of surrounding hair, and their small size. To overcome these unique challenges, this work presents a flexible, robust, and highly accurate model for pupil and corneal reflection identification in rodent gaze determination that can be incrementally trained to account for variability in eye parameters encountered in the field. To the best of our knowledge, this is the first paper that demonstrates a highly accurate and practical biomedical image segmentation based convolutional neural network architecture for pupil and corneal reflection identification in eye images. This new method, in conjunction with our automated infrared videobased eye recording system, offers the state of the art technology in eye tracking for neuroscience and vision science research for rodents. </p>
<blockquote>
<p>ç¥ç»ç§‘å­¦å’Œè§†è§‰ç§‘å­¦çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¯¹åŠ¨ç‰©ä¸»ä½“æ³¨è§†æ–¹å‘çš„ç²¾ç¡®æµ‹é‡ã€‚ç”±äºç»æµä¼˜åŠ¿å’Œé€‚åº”æ€§å¼ºçš„ç‰¹ç‚¹ï¼Œå•®é½¿åŠ¨ç‰©æ˜¯æ­¤ç±»ç ”ç©¶ä¸­æœ€ä¸ºå¹¿æ³›ç ”ç©¶çš„åŠ¨ç‰©å¯¹è±¡ã€‚è¿‘æœŸï¼ŒåŸºäºè§†é¢‘çš„çœ¼ç¥è·Ÿè¸ªå™¨å·²å˜æˆä¸€ç§æµè¡Œçš„æ³¨è§†è¿½è¸ªé€‰é¡¹ï¼Œå› ä¸ºå®ƒæ˜“äºä½¿ç”¨å¹¶ä¸”å®Œå…¨æ— åˆ›ã€‚è™½ç„¶çœ¼ç¥è·Ÿè¸ªç®—æ³•çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§å·²å¾—åˆ°æ˜¾è‘—æé«˜ï¼Œä½†é—æ†¾çš„æ˜¯ï¼Œå‡ ä¹æ‰€æœ‰çš„æŠ€æœ¯éƒ½é›†ä¸­åœ¨äººç±»çœ¼ç›ä¸Šï¼Œå¹¶æ²¡æœ‰è€ƒè™‘åˆ°å•®é½¿åŠ¨ç‰©çœ¼ç›å›¾åƒçš„ç‹¬ç‰¹ç‰¹å¾ï¼Œä¾‹å¦‚çœ¼ç›å‚æ•°çš„å˜é‡ã€å‘¨å›´æ¯›å‘çš„ä¸°å¯Œä»¥åŠå®ƒä»¬çš„å°å°ºå¯¸ã€‚ä¸ºäº†å…‹æœè¿™äº›ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªçµæ´»ã€ç¨³å¥å’Œé«˜åº¦å‡†ç¡®çš„æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«å•®é½¿åŠ¨ç‰©ç³å­”å’Œè§’è†œåå°„ä»¥ç¡®å®šå…¶æ³¨è§†æ–¹å‘ï¼Œè¯¥æ¨¡å‹å¯ä»¥é€æ­¥è®­ç»ƒä»¥é€‚åº”ç°åœºé‡åˆ°çš„çœ¼ç›å‚æ•°å˜é‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡å±•ç¤ºåŸºäºå·ç§¯ç¥ç»ç½‘ç»œæ¶æ„çš„é«˜åº¦å‡†ç¡®å’Œå®é™…å¯è¡Œçš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«çœ¼ç›å›¾åƒä¸­çš„ç³å­”å’Œè§’è†œåå°„ã€‚è¿™ç§æ–°æ–¹æ³•ç»“åˆæˆ‘ä»¬è‡ªåŠ¨åŒ–çš„çº¢å¤–è§†é¢‘çœ¼è®°å½•ç³»ç»Ÿï¼Œä¸ºç¥ç»ç§‘å­¦å’Œè§†è§‰ç§‘å­¦ç ”ç©¶ä¸­å•®é½¿åŠ¨ç‰©çš„çœ¼ç¥è·Ÿè¸ªæä¾›äº†æœ€å…ˆè¿›çš„ç§‘æŠ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08183v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å•®é½¿åŠ¨ç‰©çœ¼åŠ¨è¿½è¸ªç ”ç©¶çš„æ–°æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºå›¾åƒå¤„ç†æŠ€æœ¯ï¼Œå…·æœ‰çµæ´»æ€§ã€é²æ£’æ€§å’Œé«˜ç²¾åº¦ï¼Œå¯è¯†åˆ«ç³å­”å’Œè§’è†œåå°„ï¼Œå¹¶èƒ½é€æ­¥è®­ç»ƒä»¥é€‚åº”é‡å¤–é‡åˆ°çš„çœ¼å‚æ•°å˜åŒ–ã€‚è¯¥æ¨¡å‹å…‹æœäº†å•®é½¿åŠ¨ç‰©çœ¼å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚çœ¼å‚æ•°å˜åŒ–å¤§ã€å‘¨å›´æ¯›å‘ä¸°å¯Œå’Œå°ºå¯¸å°ç­‰ã€‚æ­¤æ¨¡å‹çš„æå‡ºï¼Œå°†ä¿ƒè¿›å•®é½¿åŠ¨ç‰©ç¥ç»ç§‘å­¦å’Œè§†è§‰ç§‘å­¦ç ”ç©¶é¢†åŸŸçš„çœ¼åŠ¨è¿½è¸ªæŠ€æœ¯å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ¨¡å‹åŸºäºå›¾åƒå¤„ç†æŠ€æœ¯ï¼Œç”¨äºè¯†åˆ«å•®é½¿åŠ¨ç‰©çš„ç³å­”å’Œè§’è†œåå°„ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰çµæ´»æ€§ã€é²æ£’æ€§å’Œé«˜ç²¾åº¦ï¼Œèƒ½é€‚åº”é‡å¤–çœ¼å‚æ•°çš„å˜åŒ–ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿé€æ­¥è®­ç»ƒï¼Œå…‹æœå•®é½¿åŠ¨ç‰©çœ¼å›¾åƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>æ­¤æ¨¡å‹æ˜¯é¦–ä¸ªé’ˆå¯¹å•®é½¿åŠ¨ç‰©çœ¼å›¾åƒä¸­ç³å­”å’Œè§’è†œåå°„è¯†åˆ«çš„ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
<li>æ­¤æ–°æ–¹æ³•ä¸è‡ªåŠ¨çº¢å¤–è§†é¢‘è®°å½•ç³»ç»Ÿç»“åˆï¼Œä¸ºå•®é½¿åŠ¨ç‰©ç¥ç»ç§‘å­¦å’Œè§†è§‰ç§‘å­¦ç ”ç©¶é¢†åŸŸçš„çœ¼åŠ¨è¿½è¸ªæä¾›äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</li>
<li>æ­¤æ¨¡å‹çš„æå‡ºå°†ä¿ƒè¿›çœ¼åŠ¨è¿½è¸ªæŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e0cb6a4581fa4ddaac42394be23532a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2aa8437df9ffeb7697e6fe0be711807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def4990f2313a76a6876358958b157a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c408ca3aca019287659db3bb4c6da413.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Gaze-based-Volumetric-Medical-Image-Segmentation"><a href="#Zero-Shot-Gaze-based-Volumetric-Medical-Image-Segmentation" class="headerlink" title="Zero-Shot Gaze-based Volumetric Medical Image Segmentation"></a>Zero-Shot Gaze-based Volumetric Medical Image Segmentation</h2><p><strong>Authors:Tatyana Shmykova, Leila Khaertdinova, Ilya Pershin</strong></p>
<p>Accurate segmentation of anatomical structures in volumetric medical images is crucial for clinical applications, including disease monitoring and cancer treatment planning. Contemporary interactive segmentation models, such as Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on manually provided prompts like bounding boxes and mouse clicks. In this study, we introduce eye gaze as a novel informational modality for interactive segmentation, marking the application of eye-tracking for 3D medical image segmentation. We evaluate the performance of using gaze-based prompts with SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to bounding boxes, gaze-based prompts offer a time-efficient interaction approach with slightly lower segmentation quality. Our findings highlight the potential of using gaze as a complementary input modality for interactive 3D medical image segmentation. </p>
<blockquote>
<p>åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒä¸­å¯¹è§£å‰–ç»“æ„è¿›è¡Œå‡†ç¡®çš„åˆ†å‰²å¯¹äºä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ç–¾ç—…ç›‘æµ‹å’Œç™Œç—‡æ²»ç–—è®¡åˆ’ã€‚å½“ä»£çš„äº¤äº’å¼åˆ†å‰²æ¨¡å‹ï¼Œå¦‚Segment Anything Model 2ï¼ˆSAM-2ï¼‰åŠå…¶åŒ»å­¦å˜ä½“ï¼ˆMedSAM-2ï¼‰ï¼Œä¾èµ–äºæ‰‹åŠ¨æä¾›çš„æç¤ºï¼Œå¦‚è¾¹ç•Œæ¡†å’Œé¼ æ ‡ç‚¹å‡»ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥çœ¼åŠ¨è¿½è¸ªä½œä¸ºä¸€ç§æ–°å‹ä¿¡æ¯æ¨¡å¼ï¼Œç”¨äºäº¤äº’å¼åˆ†å‰²ï¼Œæ ‡å¿—ç€çœ¼åŠ¨è¿½è¸ªåœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆå’ŒçœŸå®çœ¼åŠ¨æ•°æ®è¯„ä¼°äº†åŸºäºçœ¼åŠ¨æç¤ºçš„SAM-2å’ŒMedSAM-2çš„æ€§èƒ½ã€‚ä¸è¾¹ç•Œæ¡†ç›¸æ¯”ï¼ŒåŸºäºçœ¼åŠ¨çš„æç¤ºæä¾›äº†ä¸€ç§æ—¶é—´æ•ˆç‡é«˜çš„äº¤äº’æ–¹å¼ï¼Œä½†åˆ†å‰²è´¨é‡ç•¥æœ‰ä¸‹é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨çœ¼åŠ¨ä½œä¸ºä¸€ç§è¡¥å……è¾“å…¥æ¨¡å¼è¿›è¡Œäº¤äº’å¼3DåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15256v2">PDF</a> Accepted to MMFM-BIOMED Workshop @ CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒä¸­å‡†ç¡®åˆ†å‰²è§£å‰–ç»“æ„çš„é‡è¦æ€§åŠå…¶åœ¨ç–¾ç—…ç›‘æµ‹å’Œç™Œç—‡æ²»ç–—è®¡åˆ’ç­‰ä¸´åºŠåº”ç”¨ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å¼•å…¥äº†çœ¼åŠ¨è¿½è¸ªä½œä¸ºä¸€ç§æ–°å‹ä¿¡æ¯æ¨¡æ€ï¼Œç”¨äºäº¤äº’å¼åˆ†å‰²ï¼Œå¹¶è¯„ä¼°äº†å…¶ä¸Segment Anything Model 2ï¼ˆSAM-2ï¼‰å’ŒMedSAM-2ç­‰å½“ä»£äº¤äº’å¼åˆ†å‰²æ¨¡å‹ç»“åˆä½¿ç”¨æ—¶çš„æ€§èƒ½ã€‚ä¸è¾¹ç•Œæ¡†ç›¸æ¯”ï¼ŒåŸºäºçœ¼åŠ¨çš„æç¤ºæä¾›äº†ä¸€ç§æ—¶é—´æ•ˆç‡é«˜çš„äº¤äº’æ–¹å¼ï¼Œå°½ç®¡åˆ†å‰²è´¨é‡ç•¥æœ‰ä¸‹é™ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œçœ¼åŠ¨è¿½è¸ªåœ¨äº¤äº’å¼ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­è§£å‰–ç»“æ„çš„å‡†ç¡®åˆ†å‰²å¯¹äºä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ï¼Œå¦‚ç–¾ç—…ç›‘æµ‹å’Œç™Œç—‡æ²»ç–—è®¡åˆ’ã€‚</li>
<li>å½“å‰äº¤äº’å¼åˆ†å‰²æ¨¡å‹å¦‚SAM-2å’ŒMedSAM-2ä¾èµ–äºæ‰‹åŠ¨æç¤ºï¼Œå¦‚è¾¹ç•Œæ¡†å’Œé¼ æ ‡ç‚¹å‡»ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†çœ¼åŠ¨è¿½è¸ªä½œä¸ºä¸€ç§æ–°å‹ä¿¡æ¯æ¨¡æ€ï¼Œç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„äº¤äº’å¼æ“ä½œã€‚</li>
<li>åŸºäºçœ¼åŠ¨çš„æç¤ºä¸SAM-2å’ŒMedSAM-2ç»“åˆä½¿ç”¨æ—¶ï¼Œè¡¨ç°å‡ºæ—¶é—´æ•ˆç‡é«˜çš„äº¤äº’æ–¹å¼ã€‚</li>
<li>ä¸è¾¹ç•Œæ¡†ç›¸æ¯”ï¼ŒåŸºäºçœ¼åŠ¨çš„æç¤ºè™½ç„¶åˆ†å‰²è´¨é‡ç•¥æœ‰ä¸‹é™ï¼Œä½†ä»å…·æœ‰ä¸€å®šä¼˜åŠ¿ã€‚</li>
<li>åŸºäºçœ¼åŠ¨çš„äº¤äº’å¼ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aedb2d32adcac3eac0eff726b38ee9d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1aa84b6587c1581d0ad63133fe5d6826.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4de44d039eb71d5db4b24c511557e66a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd3defa0c77f076243619f91777fde4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69037c7bed7415d110d220aba388c3a6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Curvature-Tuning-Provable-Training-free-Model-Steering-From-a-Single-Parameter"><a href="#Curvature-Tuning-Provable-Training-free-Model-Steering-From-a-Single-Parameter" class="headerlink" title="Curvature Tuning: Provable Training-free Model Steering From a Single   Parameter"></a>Curvature Tuning: Provable Training-free Model Steering From a Single   Parameter</h2><p><strong>Authors:Leyang Hu, Matteo Gamba, Randall Balestriero</strong></p>
<p>The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a modelâ€™s decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50&#x2F;152 by 7.14%&#x2F;8.46% over linear probing and 4.64%&#x2F;1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark from RobustBench by 1032.64%&#x2F;1494.46%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Leon-Leyang/curvature-tuning">https://github.com/Leon-Leyang/curvature-tuning</a>. </p>
<blockquote>
<p>æ¨¡å‹çš„ç¼©æ”¾å’Œæ•°æ®è§„æ¨¡æ‰©å¤§å·²ç»é‡å¡‘äº†äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå°†å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ç¡®ç«‹ä¸ºè§£å†³ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼Œä¸»æµçš„å¾®è°ƒæ–¹æ³•é€šå¸¸ä¾èµ–äºæƒé‡é€‚åº”ï¼Œå¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”ä¾èµ–äºå¯å‘å¼é€‰æ‹©çš„è¶…å‚æ•°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ä¸åŒçš„è§’åº¦å‡ºå‘ï¼Œå°†ç„¦ç‚¹ä»æƒé‡è½¬å‘æ¿€æ´»å‡½æ•°ï¼Œé€šè¿‡æ ·æ¡ç®—å­çš„è§†è§’æ¥å®¡è§†å®ƒä»¬ã€‚æˆ‘ä»¬æå‡ºäº†æ›²ç‡è°ƒæ•´ï¼ˆCTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯è§£é‡Šä¸”åŸºäºåŸåˆ™çš„æ§åˆ¶æ–¹æ³•ï¼Œé€šè¿‡å‘æ¿€æ´»å‡½æ•°æ³¨å…¥å•ä¸ªè¶…å‚æ•°æ¥è°ƒèŠ‚æ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚æˆ‘ä»¬è¯æ˜äº†CTèƒ½å¤Ÿå¯é åœ°è°ƒæ•´æ¨¡å‹çš„å†³ç­–è¾¹ç•Œæ›²ç‡ï¼Œå¹¶ä¸”æ›´æ ¹æœ¬çš„æ˜¯ï¼Œå®ƒå°†æ¨¡å‹æŠ•å½±åˆ°å¹³æ»‘å‡½æ•°ç©ºé—´ä¸Šï¼Œä»è€Œå¼¥è¡¥äº†å½“å‰å¾®è°ƒæ–¹æ³•ä¸»è¦åœ¨ç‰¹å¾é€‚åº”æ–¹é¢çš„ä¸è¶³ã€‚ä½¿è¿™ä¸ªè¶…å‚æ•°å¯è®­ç»ƒï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç§æ–°å‹ä¸”é«˜åº¦å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚ç»éªŒä¸Šï¼ŒCTæé«˜äº†æ³›åŒ–å’Œç¨³å¥æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨12ä¸ªæ•°æ®é›†ä¸Šï¼Œç›¸å¯¹äºçº¿æ€§æ¢æµ‹å’ŒLoRAï¼Œå®ƒåˆ†åˆ«å°†ResNet-50&#x2F;152çš„ä¸‹æ¸¸ç²¾åº¦æé«˜äº†7.14%&#x2F;8.46%å’Œ4.64%&#x2F;1.70%ï¼Œå¹¶åœ¨RobustBenchçš„lâˆåŸºå‡†ä¸Šæé«˜äº†ç¨³å¥ç²¾åº¦è¾¾1032.64%&#x2F;1494.46%ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Leon-Leyang/curvature-tuning%E3%80%82">https://github.com/Leon-Leyang/curvature-tuningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07783v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ¨¡å‹å’Œæ•°æ®è§„æ¨¡çš„æ‰©å±•å¯¹äººå·¥æ™ºèƒ½é¢†åŸŸçš„å½±å“ï¼Œå¹¶æŒ‡å‡ºå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å·²æˆä¸ºè§£å†³ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡å‡†èŒƒå¼ã€‚ç„¶è€Œï¼Œä¸»æµå¾®è°ƒæ–¹æ³•é€šå¸¸ä¾èµ–äºæƒé‡é€‚åº”ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¹¶ä¾èµ–äºå¯å‘å¼é€‰æ‹©çš„è¶…å‚æ•°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯è°ƒæ›²ç‡æ–¹æ³•â€”â€”æ›²ç‡è°ƒæ•´ï¼ˆCTï¼‰ï¼Œå®ƒé€šè¿‡å‘æ¿€æ´»å‡½æ•°æ³¨å…¥å•ä¸€è¶…å‚æ•°ï¼Œä»¥ä¸€ç§å¯è§£é‡Šå’ŒåŸåˆ™æ€§çš„æ–¹å¼è°ƒèŠ‚æ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚CTå¯è¯æ˜åœ°è°ƒæ•´æ¨¡å‹å†³ç­–è¾¹ç•Œçš„æ›²ç‡ï¼Œå¹¶å°†æ¨¡å‹æŠ•å½±åˆ°å¹³æ»‘å‡½æ•°ç©ºé—´ï¼Œä»è€Œå¼¥è¡¥äº†å½“å‰å¾®è°ƒæ–¹æ³•ä¸»è¦ä¾§é‡äºç‰¹å¾é€‚åº”çš„ä¸è¶³ã€‚é€šè¿‡ä½¿è¿™ä¸ªè¶…å‚æ•°å¯è®­ç»ƒï¼ŒCTæˆä¸ºäº†ä¸€ç§æ–°å‹ä¸”é«˜åº¦å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒCTæé«˜äº†æ¨¡å‹çš„æ³›åŒ–å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»æµå¾®è°ƒæ–¹æ³•ä¸»è¦ä¾èµ–æƒé‡é€‚åº”ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>æ›²ç‡è°ƒæ•´ï¼ˆCTï¼‰æ˜¯ä¸€ç§æ–°å‹çš„å¾®è°ƒæ–¹æ³•ï¼Œä¾§é‡äºé€šè¿‡æ¿€æ´»å‡½æ•°è¿›è¡Œæ¨¡å‹è°ƒæ•´ã€‚</li>
<li>CTé€šè¿‡æ³¨å…¥å•ä¸€è¶…å‚æ•°åˆ°æ¿€æ´»å‡½æ•°ä¸­ï¼Œä»¥å¯è§£é‡Šå’ŒåŸåˆ™æ€§çš„æ–¹å¼è°ƒèŠ‚æ¨¡å‹çš„å†³ç­–è¾¹ç•Œã€‚</li>
<li>CTèƒ½å¤Ÿè¯æ˜åœ°è°ƒæ•´æ¨¡å‹å†³ç­–è¾¹ç•Œçš„æ›²ç‡ï¼Œå¹¶å°†æ¨¡å‹æŠ•å½±åˆ°å¹³æ»‘å‡½æ•°ç©ºé—´ã€‚</li>
<li>ä¸ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCTåœ¨ç‰¹å¾é€‚åº”çš„åŸºç¡€ä¸Šè¿›è¡Œäº†è¡¥å……ã€‚</li>
<li>é€šè¿‡ä½¿è¶…å‚æ•°å¯è®­ç»ƒï¼ŒCTæ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d38bf2249d4c2d0e2fa18002e819536c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d05eb6ed09242af75c1c5ef66d2ed2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edde8e9212f0de6673071e3bdbba09a7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction"><a href="#LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction" class="headerlink" title="LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction"></a>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction</h2><p><strong>Authors:Hrishav Bakul Barua, Kalin Stefanov, Lemuel Lai En Che, Abhinav Dhall, KokSheik Wong, Ganesh Krishnasamy</strong></p>
<p>The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: <a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> </p>
<blockquote>
<p>å°†ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒè½¬æ¢ä¸ºé«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒæ˜¯ä¸€é¡¹é‡è¦çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚è®¸å¤šç ”ç©¶éƒ½åˆ©ç”¨ä¼ ç»Ÿçš„éå­¦ä¹ æ–¹æ³•å’Œç°ä»£çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œä¾§é‡äºä½¿ç”¨å•æ›å…‰å’Œå¤šæ›å…‰çš„LDRæ¥è¿›è¡ŒHDRå›¾åƒé‡å»ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å½“å‰æœ€å‰æ²¿çš„æ–¹æ³•éƒ½éœ€è¦é«˜è´¨é‡æˆå¯¹çš„{LDRï¼ŒHDR}æ•°æ®é›†æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œå…³äºä½¿ç”¨æœªé…å¯¹æ•°æ®é›†è¿›è¡Œæ­¤ä»»åŠ¡çš„æ–‡çŒ®å¾ˆå°‘ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹å­¦ä¹ åŸŸä¹‹é—´çš„æ˜ å°„ï¼Œå³{LDRï¼ŒHDR}ã€‚æœ¬æ–‡æå‡ºäº†LLM-HDRæ–¹æ³•ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ„ŸçŸ¥èƒ½åŠ›èå…¥åˆ°ä¸€ä¸ªç»è¿‡ä¿®æ”¹çš„è¯­ä¹‰å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ¶æ„ä¸­ï¼Œè¯¥æ¶æ„åˆ©ç”¨æœªé…å¯¹çš„{LDRï¼ŒHDR}æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•å¼•å…¥äº†æ–°å‹ä¼ªå½±å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ï¼Œä»¥åŠä¸€ä¸ªç¼–ç å™¨å’ŒæŸå¤±æ¥è§£å†³è¯­ä¹‰ä¸€è‡´æ€§è¿™ä¸€å°šæœªè¢«æ·±å…¥æ¢è®¨çš„é—®é¢˜ã€‚LLM-HDRæ˜¯ç¬¬ä¸€ä¸ªåœ¨è‡ªæˆ‘ç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨LLMè¿›è¡Œ{LDRï¼ŒHDR}ç¿»è¯‘ä»»åŠ¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶é‡å»ºäº†é«˜è´¨é‡çš„HDRå›¾åƒã€‚è¯¥å·¥ä½œçš„å®˜æ–¹ç½‘ç«™å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v3">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹æ„ŸçŸ¥çš„LDRåˆ°HDRå›¾åƒç¿»è¯‘æ–¹æ³•ï¼Œé‡‡ç”¨æ— é…å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡è¯­ä¹‰ä¸€è‡´æ€§å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ€§æ¶æ„å®ç°å›¾åƒé‡å»ºã€‚è¯¥æ–¹æ³•å¼•å…¥æ–°çš„ä¼ªå½±æ„ŸçŸ¥å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨ä»¥è§£å†³è§†è§‰ä¼ªå½±é—®é¢˜ï¼Œå¹¶ä½¿ç”¨ç¼–ç å™¨è§£å†³è¯­ä¹‰ä¸€è‡´æ€§éš¾é¢˜ã€‚LLM-HDRåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå¯é‡å»ºé«˜è´¨é‡HDRå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDRåˆ°HDRå›¾åƒç¿»è¯‘æ˜¯è®¡ç®—æœºè§†è§‰çš„é‡è¦ä»»åŠ¡ï¼Œç°æœ‰æ–¹æ³•å¤§å¤šéœ€è¦é«˜è´¨é‡é…å¯¹æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºLLM-HDRæ–¹æ³•ï¼Œé¦–æ¬¡åœ¨è‡ªç›‘ç£è®¾ç½®ä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒLDRåˆ°HDRçš„ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>LLM-HDRç»“åˆæ„ŸçŸ¥è¯­ä¹‰å’Œå¾ªç¯ä¸€è‡´çš„å¯¹æŠ—æ€§æ¶æ„ï¼Œä½¿ç”¨æ— é…å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ–¹æ³•å¼•å…¥ä¼ªå½±æ„ŸçŸ¥å’Œæ›å…‰æ„ŸçŸ¥ç”Ÿæˆå™¨ä»¥è§£å†³è§†è§‰ä¼ªå½±é—®é¢˜ã€‚</li>
<li>LLM-HDRä½¿ç”¨ç¼–ç å™¨è§£å†³è¯­ä¹‰ä¸€è‡´æ€§éš¾é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªä¹‹å‰æœªè¢«å……åˆ†æ¢ç´¢çš„ä¸»é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œèƒ½å¤Ÿé‡å»ºé«˜è´¨é‡HDRå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0604352e626b0cd673be111849895840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c796c8b662950542046b576ea2142a76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df9558764027549d613a2463ed6c1f1f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="NeRF-CA-Dynamic-Reconstruction-of-X-ray-Coronary-Angiography-with-Extremely-Sparse-views"><a href="#NeRF-CA-Dynamic-Reconstruction-of-X-ray-Coronary-Angiography-with-Extremely-Sparse-views" class="headerlink" title="NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with   Extremely Sparse-views"></a>NeRF-CA: Dynamic Reconstruction of X-ray Coronary Angiography with   Extremely Sparse-views</h2><p><strong>Authors:Kirsten W. H. Maas, Danny Ruijters, Anna Vilanova, Nicola Pezzotti</strong></p>
<p>Dynamic three-dimensional (4D) reconstruction from two-dimensional X-ray coronary angiography (CA) remains a significant clinical problem. Existing CA reconstruction methods often require extensive user interaction or large training datasets. Recently, Neural Radiance Field (NeRF) has successfully reconstructed high-fidelity scenes in natural and medical contexts without these requirements. However, challenges such as sparse-views, intra-scan motion, and complex vessel morphology hinder its direct application to CA data. We introduce NeRF-CA, a first step toward a fully automatic 4D CA reconstruction that achieves reconstructions from sparse coronary angiograms. To the best of our knowledge, we are the first to address the challenges of sparse-views and cardiac motion by decoupling the scene into the moving coronary artery and the static background, effectively translating the problem of motion into a strength. NeRF-CA serves as a first stepping stone for solving the 4D CA reconstruction problem, achieving adequate 4D reconstructions from as few as four angiograms, as required by clinical practice, while significantly outperforming state-of-the-art sparse-view X-ray NeRF. We validate our approach quantitatively and qualitatively using representative 4D phantom datasets and ablation studies. To accelerate research in this domain, we made our codebase public: <a target="_blank" rel="noopener" href="https://github.com/kirstenmaas/NeRF-CA">https://github.com/kirstenmaas/NeRF-CA</a>. </p>
<blockquote>
<p>ä»äºŒç»´Xå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±ï¼ˆCAï¼‰è¿›è¡ŒåŠ¨æ€ä¸‰ç»´ï¼ˆ4Dï¼‰é‡å»ºä»ç„¶æ˜¯ä¸´åºŠä¸Šçš„ä¸€ä¸ªé‡å¤§é—®é¢˜ã€‚ç°æœ‰çš„CAé‡å»ºæ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„ç”¨æˆ·äº¤äº’æˆ–å¤§é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚æœ€è¿‘ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆåŠŸåœ°é‡å»ºäº†è‡ªç„¶å’ŒåŒ»å­¦èƒŒæ™¯ä¸‹çš„é«˜ä¿çœŸåœºæ™¯ï¼Œè€Œæ— éœ€è¿™äº›è¦æ±‚ã€‚ç„¶è€Œï¼Œç¨€ç–è§†å›¾ã€æ‰«æå†…è¿åŠ¨ä»¥åŠå¤æ‚çš„è¡€ç®¡å½¢æ€ç­‰æŒ‘æˆ˜é˜»ç¢äº†å…¶ç›´æ¥åº”ç”¨äºCAæ•°æ®ã€‚æˆ‘ä»¬å¼•å…¥äº†NeRF-CAï¼Œè¿™æ˜¯æœç€å…¨è‡ªåŠ¨4D CAé‡å»ºçš„ç¬¬ä¸€æ­¥ï¼Œå®ç°äº†ä»ç¨€ç–å† çŠ¶åŠ¨è„‰é€ å½±å›¾åƒè¿›è¡Œé‡å»ºã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡å°†åœºæ™¯è§£è€¦ä¸ºç§»åŠ¨çš„å† çŠ¶åŠ¨è„‰å’Œé™æ€èƒŒæ™¯æ¥è§£å†³ç¨€ç–è§†å›¾å’Œå¿ƒè„è¿åŠ¨æŒ‘æˆ˜çš„å›¢é˜Ÿï¼Œæœ‰æ•ˆåœ°å°†è¿åŠ¨é—®é¢˜è½¬åŒ–ä¸ºä¼˜åŠ¿ã€‚NeRF-CAä½œä¸ºè§£å†³4D CAé‡å»ºé—®é¢˜çš„ç¬¬ä¸€æ­¥ï¼Œèƒ½å¤Ÿä»ä¸´åºŠå®è·µä¸­æ‰€éœ€çš„ä»…å››å¼ é€ å½±å›¾åƒå®ç°è¶³å¤Ÿçš„4Dé‡å»ºï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾Xå°„çº¿NeRFã€‚æˆ‘ä»¬ä½¿ç”¨å…·æœ‰ä»£è¡¨æ€§çš„4D Phantomæ•°æ®é›†å’Œæ¶ˆèç ”ç©¶è¿›è¡Œå®šé‡å’Œå®šæ€§çš„éªŒè¯ã€‚ä¸ºäº†åŠ é€Ÿè¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/kirstenmaas/NeRF-CA%E3%80%82">https://github.com/kirstenmaas/NeRF-CAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16355v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯è¿›è¡ŒäºŒç»´Xå…‰å† çŠ¶åŠ¨è„‰é€ å½±ï¼ˆCAï¼‰çš„åŠ¨æ€ä¸‰ç»´ï¼ˆ4Dï¼‰é‡å»ºçš„æ–¹æ³•ã€‚é€šè¿‡è§£å†³ç¨€ç–è§†å›¾å’Œå¿ƒè„è¿åŠ¨ç­‰æŒ‘æˆ˜ï¼ŒNeRF-CAæ–¹æ³•å®ç°äº†ä»å°‘é‡å† çŠ¶åŠ¨è„‰é€ å½±å›¾åƒä¸­çš„é‡å»ºã€‚è¯¥æ–¹æ³•å°†åœºæ™¯åˆ†è§£ä¸ºè¿åŠ¨çš„å† çŠ¶åŠ¨è„‰å’Œé™æ€èƒŒæ™¯ï¼ŒæˆåŠŸè½¬åŒ–è¿åŠ¨é—®é¢˜ä¸ºä¼˜åŠ¿ã€‚ç›¸æ¯”ç°æœ‰çš„ç¨€ç–è§†å›¾Xå…‰NeRFï¼ŒNeRF-CAè¡¨ç°å‡ºæ›´å‡ºè‰²çš„æ€§èƒ½ï¼Œå·²ç»åœ¨ä»£è¡¨æ€§4D Phantomæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®šé‡å’Œå®šæ€§çš„éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF-CAæ˜¯åˆ©ç”¨ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯å®ç°äºŒç»´Xå…‰å† çŠ¶åŠ¨è„‰é€ å½±ï¼ˆCAï¼‰åŠ¨æ€ä¸‰ç»´ï¼ˆ4Dï¼‰é‡å»ºçš„é¦–æ¬¡å°è¯•ã€‚</li>
<li>NeRF-CAè§£å†³äº†ç¨€ç–è§†å›¾å’Œå¿ƒè„è¿åŠ¨ç­‰æŒ‘æˆ˜ï¼Œå®ç°äº†ä»å°‘é‡å† çŠ¶åŠ¨è„‰é€ å½±å›¾åƒä¸­çš„é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å°†åœºæ™¯åˆ†è§£ä¸ºè¿åŠ¨çš„å† çŠ¶åŠ¨è„‰å’Œé™æ€èƒŒæ™¯ï¼ŒæˆåŠŸè½¬åŒ–è¿åŠ¨é—®é¢˜ä¸ºä¼˜åŠ¿ã€‚</li>
<li>NeRF-CAæ–¹æ³•å®ç°äº†ä»ä»…å››ä¸ªå† çŠ¶åŠ¨è„‰é€ å½±å›¾åƒè¿›è¡Œé‡å»ºï¼Œæ»¡è¶³äº†ä¸´åºŠå®è·µçš„éœ€æ±‚ã€‚</li>
<li>ä¸ç°æœ‰çš„ç¨€ç–è§†å›¾Xå…‰NeRFç›¸æ¯”ï¼ŒNeRF-CAæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>NeRF-CAå·²åœ¨ä»£è¡¨æ€§4D Phantomæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®šé‡å’Œå®šæ€§çš„éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b29aa99aea5a720c5158b50923df214a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff73670dcdfb57740ae75686739fcf11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3643ba3d3f43f2bfc1312905a0dca28.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ST-USleepNet-A-Spatial-Temporal-Coupling-Prominence-Network-for-Multi-Channel-Sleep-Staging"><a href="#ST-USleepNet-A-Spatial-Temporal-Coupling-Prominence-Network-for-Multi-Channel-Sleep-Staging" class="headerlink" title="ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for   Multi-Channel Sleep Staging"></a>ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for   Multi-Channel Sleep Staging</h2><p><strong>Authors:Jingying Ma, Qika Lin, Ziyu Jia, Mengling Feng</strong></p>
<p>Sleep staging is critical to assess sleep quality and diagnose disorders. Despite advancements in artificial intelligence enabling automated sleep staging, significant challenges remain: (1) Simultaneously extracting prominent temporal and spatial sleep features from multi-channel raw signals, including characteristic sleep waveforms and salient spatial brain networks. (2) Capturing the spatial-temporal coupling patterns essential for accurate sleep staging. To address these challenges, we propose a novel framework named ST-USleepNet, comprising a spatial-temporal graph construction module (ST) and a U-shaped sleep network (USleepNet). The ST module converts raw signals into a spatial-temporal graph based on signal similarity, temporal, and spatial relationships to model spatial-temporal coupling patterns. The USleepNet employs a U-shaped structure for both the temporal and spatial streams, mirroring its original use in image segmentation to isolate significant targets. Applied to raw sleep signals and graph data from the ST module, USleepNet effectively segments these inputs, simultaneously extracting prominent temporal and spatial sleep features. Testing on three datasets demonstrates that ST-USleepNet outperforms existing baselines, and model visualizations confirm its efficacy in extracting prominent sleep features and temporal-spatial coupling patterns across various sleep stages. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Majy-Yuji/ST-USleepNet">https://github.com/Majy-Yuji/ST-USleepNet</a>. </p>
<blockquote>
<p>ç¡çœ åˆ†æœŸå¯¹äºè¯„ä¼°ç¡çœ è´¨é‡å’Œè¯Šæ–­ç¡çœ éšœç¢è‡³å…³é‡è¦ã€‚å°½ç®¡äººå·¥æ™ºèƒ½çš„è¿›æ­¥å·²ç»å®ç°äº†è‡ªåŠ¨åŒ–çš„ç¡çœ åˆ†æœŸï¼Œä½†ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼š(1) ä»å¤šé€šé“åŸå§‹ä¿¡å·ä¸­åŒæ—¶æå–çªå‡ºçš„æ—¶é—´å’Œç©ºé—´ç¡çœ ç‰¹å¾ï¼ŒåŒ…æ‹¬ç‰¹å¾ç¡çœ æ³¢å½¢å’Œæ˜¾è‘—çš„è„‘ç©ºé—´ç½‘ç»œã€‚(2) æ•æ‰å¯¹å‡†ç¡®ç¡çœ åˆ†æœŸè‡³å…³é‡è¦çš„æ—¶ç©ºè€¦åˆæ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºST-USleepNetçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ—¶ç©ºå›¾æ„å»ºæ¨¡å—ï¼ˆSTï¼‰å’Œä¸€ä¸ªUå‹ç¡çœ ç½‘ç»œï¼ˆUSleepNetï¼‰ã€‚STæ¨¡å—æ ¹æ®ä¿¡å·ç›¸ä¼¼æ€§ã€æ—¶é—´å’Œç©ºç©ºé—´å…³ç³»å°†åŸå§‹ä¿¡å·è½¬æ¢ä¸ºæ—¶ç©ºå›¾ï¼Œä»¥æ¨¡æ‹Ÿæ—¶ç©ºè€¦åˆæ¨¡å¼ã€‚USleepNeté‡‡ç”¨Uå‹ç»“æ„ï¼Œç”¨äºæ—¶é—´å’Œç©ºç©ºé—´æµï¼Œæ¨¡ä»¿å…¶åœ¨å›¾åƒåˆ†å‰²ä¸­çš„åŸå§‹ç”¨é€”ï¼Œä»¥éš”ç¦»é‡è¦ç›®æ ‡ã€‚åº”ç”¨äºæ¥è‡ªSTæ¨¡å—çš„åŸå§‹ç¡çœ ä¿¡å·å’Œå›¾å½¢æ•°æ®ï¼ŒUSleepNetå¯ä»¥æœ‰æ•ˆåœ°å¯¹è¿™äº›è¾“å…¥è¿›è¡Œåˆ†æ®µï¼ŒåŒæ—¶æå–çªå‡ºçš„æ—¶ç©ºç¡çœ ç‰¹å¾ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒST-USleepNetä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ¨¡å‹å¯è§†åŒ–è¯å®äº†å…¶åœ¨æå–å„ç§ç¡çœ é˜¶æ®µçš„çªå‡ºç¡çœ ç‰¹å¾å’Œæ—¶ç©ºè€¦åˆæ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Majy-Yuji/ST-USleepNet%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Majy-Yuji/ST-USleepNetè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11884v3">PDF</a> </p>
<p><strong>Summary</strong><br>     ç¡çœ åˆ†æœŸæ˜¯è¯„ä¼°ç¡çœ è´¨é‡å’Œè¯Šæ–­ç¡çœ éšœç¢çš„å…³é”®ã€‚é’ˆå¯¹äººå·¥æ™ºèƒ½åœ¨è‡ªåŠ¨ç¡çœ åˆ†æœŸä¸Šé¢ä¸´çš„åŒæ—¶æå–é‡è¦æ—¶é—´-ç©ºé—´ç‰¹å¾å’Œæ•æ‰æ—¶ç©ºè€¦åˆæ¨¡å¼ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ST-USleepNetï¼ŒåŒ…æ‹¬æ—¶ç©ºå›¾æ„å»ºæ¨¡å—ï¼ˆSTï¼‰å’ŒUå‹ç¡çœ ç½‘ç»œï¼ˆUSleepNetï¼‰ã€‚è¯¥æ¡†æ¶åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¯æœ‰æ•ˆæå–é‡è¦ç¡çœ ç‰¹å¾å’Œæ—¶ç©ºè€¦åˆæ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¡çœ åˆ†æœŸæ˜¯è¯„ä¼°ç¡çœ è´¨é‡å’Œè¯Šæ–­ç¡çœ éšœç¢çš„å…³é”®ã€‚</li>
<li>ç›®å‰åœ¨è‡ªåŠ¨ç¡çœ åˆ†æœŸä¸Šï¼Œå­˜åœ¨åŒæ—¶æå–é‡è¦æ—¶é—´-ç©ºé—´ç‰¹å¾å’Œæ•æ‰æ—¶ç©ºè€¦åˆæ¨¡å¼çš„æŒ‘æˆ˜ã€‚</li>
<li>ST-USleepNetæ¡†æ¶åŒ…æ‹¬æ—¶ç©ºå›¾æ„å»ºæ¨¡å—ï¼ˆSTï¼‰å’ŒUå‹ç¡çœ ç½‘ç»œï¼ˆUSleepNetï¼‰ã€‚</li>
<li>STæ¨¡å—å°†åŸå§‹ä¿¡å·è½¬æ¢ä¸ºåŸºäºä¿¡å·ç›¸ä¼¼æ€§çš„æ—¶ç©ºå›¾ï¼Œä»¥å»ºæ¨¡æ—¶ç©ºè€¦åˆæ¨¡å¼ã€‚</li>
<li>USleepNeté‡‡ç”¨Uå‹ç»“æ„ï¼ŒåŒæ—¶å¤„ç†æ—¶é—´å’Œç©ºé—´æµï¼Œä»¥éš”ç¦»é‡è¦ç›®æ ‡ã€‚</li>
<li>ST-USleepNetåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b07962c8c78790a883352498136d3ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101241d76a9689ffd24a57b75940a426.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e7c9e275e906033e04e4c5f96cc32b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54db110248e200393f9e310c89698c1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e86128f9b5f011d6d5f8d01c64f009.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90f331e9530737152fb4558104a3bf6e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-83cfad4229de3ef7d528ec8db7f45aaf.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  EmoNet-Voice A Fine-Grained, Expert-Verified Benchmark for Speech   Emotion Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d71fa48e29ad67bf5c0448791478be58.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  Text-Aware Image Restoration with Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
