<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  DGS-LRM Real-Time Deformable 3D Gaussian Reconstruction From Monocular   Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-88769026279271facbce3e30506beb8c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-13-æ›´æ–°"><a href="#2025-06-13-æ›´æ–°" class="headerlink" title="2025-06-13 æ›´æ–°"></a>2025-06-13 æ›´æ–°</h1><h2 id="DGS-LRM-Real-Time-Deformable-3D-Gaussian-Reconstruction-From-Monocular-Videos"><a href="#DGS-LRM-Real-Time-Deformable-3D-Gaussian-Reconstruction-From-Monocular-Videos" class="headerlink" title="DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular   Videos"></a>DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular   Videos</h2><p><strong>Authors:Chieh Hubert Lin, Zhaoyang Lv, Songyin Wu, Zhen Xu, Thu Nguyen-Phuoc, Hung-Yu Tseng, Julian Straub, Numair Khan, Lei Xiao, Ming-Hsuan Yang, Yuheng Ren, Richard Newcombe, Zhao Dong, Zhengqin Li</strong></p>
<p>We introduce the Deformable Gaussian Splats Large Reconstruction Model (DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian splats from a monocular posed video of any dynamic scene. Feed-forward scene reconstruction has gained significant attention for its ability to rapidly create digital replicas of real-world environments. However, most existing models are limited to static scenes and fail to reconstruct the motion of moving objects. Developing a feed-forward model for dynamic scene reconstruction poses significant challenges, including the scarcity of training data and the need for appropriate 3D representations and training paradigms. To address these challenges, we introduce several key technical contributions: an enhanced large-scale synthetic dataset with ground-truth multi-view videos and dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian representation that is easy to learn, supports high-quality dynamic view synthesis, and enables long-range 3D tracking; and a large transformer network that achieves real-time, generalizable dynamic scene reconstruction. Extensive qualitative and quantitative experiments demonstrate that DGS-LRM achieves dynamic scene reconstruction quality comparable to optimization-based methods, while significantly outperforming the state-of-the-art predictive dynamic reconstruction method on real-world examples. Its predicted physically grounded 3D deformation is accurate and can readily adapt for long-range 3D tracking tasks, achieving performance on par with state-of-the-art monocular video 3D tracking methods. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†å¯å˜å½¢é«˜æ–¯æ–‘ç‚¹å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆDGS-LRMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿä»ä»»ä½•åŠ¨æ€åœºæ™¯çš„å•ç›®è§†é¢‘è¿›è¡Œé¢„æµ‹çš„å¯å˜å½¢3Dé«˜æ–¯æ–‘ç‚¹é¢„æµ‹çš„é¦–æ¬¡å‰é¦ˆæ–¹æ³•ã€‚å‰é¦ˆåœºæ™¯é‡å»ºå› å…¶èƒ½å¤Ÿè¿…é€Ÿåˆ›å»ºçœŸå®ä¸–ç•Œç¯å¢ƒçš„æ•°å­—å‰¯æœ¬è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä»…é™äºé™æ€åœºæ™¯ï¼Œæ— æ³•é‡å»ºç§»åŠ¨ç‰©ä½“çš„è¿åŠ¨ã€‚å¼€å‘ç”¨äºåŠ¨æ€åœºæ™¯é‡å»ºçš„å‰é¦ˆæ¨¡å‹é¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºå’Œéœ€è¦é€‚å½“çš„3Dè¡¨ç¤ºå’Œè®­ç»ƒèŒƒå¼ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ é¡¹å…³é”®çš„æŠ€æœ¯è´¡çŒ®ï¼šä¸€ä¸ªå¸¦æœ‰çœŸå®å¤šè§†è§’è§†é¢‘å’Œå¯†é›†3Dåœºæ™¯æµç›‘ç£çš„å¤§å‹åˆæˆæ•°æ®é›†ï¼›ä¸€ç§æ˜“äºå­¦ä¹ çš„åƒç´ çº§å¯å˜å½¢3Dé«˜æ–¯è¡¨ç¤ºï¼Œå®ƒæ”¯æŒé«˜è´¨é‡åŠ¨æ€è§†å›¾åˆæˆï¼Œå¹¶å¯å®ç°è¿œç¨‹3Dè·Ÿè¸ªï¼›ä»¥åŠä¸€ä¸ªå¤§å‹å˜å‹å™¨ç½‘ç»œï¼Œå®ç°å®æ—¶ã€é€šç”¨çš„åŠ¨æ€åœºæ™¯é‡å»ºã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒDGS-LRMçš„åŠ¨æ€åœºæ™¯é‡å»ºè´¨é‡å¯ä¸åŸºäºä¼˜åŒ–çš„æ–¹æ³•ç›¸åª²ç¾ï¼ŒåŒæ—¶åœ¨çœŸå®ä¸–ç•Œç¤ºä¾‹ä¸Šæ˜¾è‘—ä¼˜äºæœ€æ–°çš„é¢„æµ‹åŠ¨æ€é‡å»ºæ–¹æ³•ã€‚å…¶é¢„æµ‹çš„ç‰©ç†åŸºç¡€3Då˜å½¢å‡†ç¡®ï¼Œå¯è½»æ¾é€‚åº”è¿œç¨‹3Dè·Ÿè¸ªä»»åŠ¡ï¼Œæ€§èƒ½ä¸æœ€æ–°çš„å•ç›®è§†é¢‘3Dè·Ÿè¸ªæ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09997v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hubert0527.github.io/dgslrm/">https://hubert0527.github.io/dgslrm/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä»‹ç»äº†ä¸€ç§å¯å˜å½¢é«˜æ–¯æ–‘ç‚¹å¤§é‡å»ºæ¨¡å‹ï¼ˆDGS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹æ˜¯é¦–ä¸ªä»å‰å‘è§†é¢‘é¢„æµ‹å¯å˜å½¢ä¸‰ç»´é«˜æ–¯æ–‘ç‚¹çš„é¢„æµ‹æ–¹æ³•ï¼Œé€‚ç”¨äºä»»ä½•åŠ¨æ€åœºæ™¯ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç³»åˆ—å…³é”®æŠ€æœ¯è´¡çŒ®ï¼ŒåŒ…æ‹¬å¢å¼ºçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ã€åƒç´ çº§çš„å¯å˜å½¢ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºå’Œå¤§å‹å˜å‹å™¨ç½‘ç»œï¼Œå®ç°äº†å®æ—¶ã€é€šç”¨çš„åŠ¨æ€åœºæ™¯é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒDGS-LRMå¯å®ç°ä¸ä¼˜åŒ–æ–¹æ³•ç›¸å½“çš„åŠ¨åŠ›å­¦åœºæ™¯é‡å»ºè´¨é‡ï¼Œä¸”åœ¨çœŸå®ä¸–ç•Œç¤ºä¾‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é¢„æµ‹åŠ¨æ€é‡å»ºæ–¹æ³•ã€‚å…¶é¢„æµ‹çš„åŸºäºç‰©ç†çš„ä¸‰ç»´å˜å½¢å‡†ç¡®ï¼Œæ˜“äºé€‚åº”é•¿æœŸä¸‰ç»´è·Ÿè¸ªä»»åŠ¡ï¼Œè¾¾åˆ°ä¸æœ€æ–°å•ç›®è§†é¢‘ä¸‰ç»´è·Ÿè¸ªæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>DGS-LRMæ˜¯é¦–ä¸ªèƒ½å¤Ÿé¢„æµ‹å¯å˜å½¢ä¸‰ç»´é«˜æ–¯æ–‘ç‚¹çš„åŠ¨æ€åœºæ™¯é‡å»ºæ¨¡å‹çš„é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹è§£å†³äº†ç°æœ‰æ¨¡å‹æ— æ³•é‡å»ºç§»åŠ¨ç‰©ä½“è¿åŠ¨çš„é—®é¢˜ã€‚</li>
<li>æ¨¡å‹å¼•å…¥å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼Œå…·æœ‰çœŸå®å¤šè§†è§’è§†é¢‘å’Œå¯†é›†çš„ä¸‰ç»´åœºæ™¯æµç›‘ç£ã€‚</li>
<li>é‡‡ç”¨åƒç´ çº§çš„å¯å˜å½¢ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼Œæ˜“äºå­¦ä¹ ï¼Œæ”¯æŒé«˜è´¨é‡åŠ¨æ€è§†å›¾åˆæˆï¼Œå¹¶å¯å®ç°é•¿æœŸä¸‰ç»´è·Ÿè¸ªã€‚</li>
<li>å¤§å‹å˜å‹å™¨ç½‘ç»œå®ç°äº†å®æ—¶ã€é€šç”¨çš„åŠ¨æ€åœºæ™¯é‡å»ºã€‚</li>
<li>DGS-LRMçš„é¢„æµ‹ä¸‰ç»´å˜å½¢å‡†ç¡®ï¼Œé€‚ç”¨äºé•¿æœŸä¸‰ç»´è·Ÿè¸ªä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4113ad429c5fe42b87bf9a2cb8189ff6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1273ba9cbc141737751abc2788b5876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc3a4e54eaf4c725ed2ec675295a8f91.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniPre3D-Unified-Pre-training-of-3D-Point-Cloud-Models-with-Cross-Modal-Gaussian-Splatting"><a href="#UniPre3D-Unified-Pre-training-of-3D-Point-Cloud-Models-with-Cross-Modal-Gaussian-Splatting" class="headerlink" title="UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal   Gaussian Splatting"></a>UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal   Gaussian Splatting</h2><p><strong>Authors:Ziyi Wang, Yanran Zhang, Jie Zhou, Jiwen Lu</strong></p>
<p>The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the modelâ€™s focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wangzy22/UniPre3D">https://github.com/wangzy22/UniPre3D</a>. </p>
<blockquote>
<p>ç‚¹äº‘æ•°æ®çš„è§„æ¨¡å¤šæ ·æ€§åœ¨ä¸º3Dè§†è§‰å¼€å‘ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯æ—¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ç›®å‰ï¼Œç»Ÿä¸€çš„3Dæ¨¡å‹å¾ˆå°‘ï¼Œå¹¶ä¸”æ²¡æœ‰ä¸€ç§é¢„è®­ç»ƒæ–¹æ³•èƒ½å¤ŸåŒæ—¶é€‚ç”¨äºå¯¹è±¡çº§å’Œåœºæ™¯çº§çš„ç‚¹äº‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†UniPre3Dï¼Œè¿™æ˜¯ç¬¬ä¸€ç§å¯ä»¥æ— ç¼åº”ç”¨äºä»»ä½•è§„æ¨¡çš„ç‚¹äº‘å’Œä»»ä½•ç»“æ„çš„3Dæ¨¡å‹çš„ç»Ÿä¸€é¢„è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é¢„æµ‹é«˜æ–¯åŸå§‹æ•°æ®ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å¯å¾®é«˜æ–¯æº…å°„æ¥å‘ˆç°å›¾åƒï¼Œä»è€Œå®ç°ç²¾ç¡®çš„åƒç´ çº§ç›‘ç£å’Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚ä¸ºäº†è¿›ä¸€æ­¥è°ƒæ•´é¢„è®­ç»ƒä»»åŠ¡çš„å¤æ‚æ€§å¹¶å¼•å¯¼æ¨¡å‹çš„ç„¦ç‚¹æœå‘å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬æ•´åˆäº†æ¥è‡ªé¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„2Dç‰¹å¾ï¼Œä»¥èå…¥æˆç†Ÿçš„çº¹ç†çŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•åœ¨å…¨èŒƒå›´çš„å¯¹è±¡çº§å’Œåœºæ™¯çº§ä»»åŠ¡ä¸­çš„é€šç”¨æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨å¤šæ ·åŒ–çš„ç‚¹äº‘æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/wangzy22/UniPre3D%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wangzy22/UniPre3Dè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09952v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡ç‚¹äº‘æ•°æ®çš„å¤šæ ·æ€§åœ¨å¼€å‘ç”¨äºä¸‰ç»´è§†è§‰çš„ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯æ–¹é¢å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰å­˜åœ¨è¾ƒå°‘çš„ç»Ÿä¸€ä¸‰ç»´æ¨¡å‹ï¼Œå¹¶ä¸”å°šæ— é¢„è®­ç»ƒæ–¹æ³•èƒ½å¤ŸåŒæ—¶æœ‰æ•ˆåœ°å¤„ç†å¯¹è±¡çº§å’Œåœºæ™¯çº§ç‚¹äº‘ã€‚æœ¬æ–‡æå‡ºUniPre3Dï¼Œè¿™æ˜¯ä¸€ç§å¯æ— ç¼åº”ç”¨äºä»»ä½•è§„æ¨¡ç‚¹äº‘å’Œä»»ä½•æ¶æ„çš„ä¸‰ç»´æ¨¡å‹çš„ç»Ÿä¸€é¢„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•ä»¥é¢„æµ‹é«˜æ–¯åŸºæœ¬ä½“ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨å¯å¾®é«˜æ–¯å¹³é“ºæŠ€æœ¯æ¸²æŸ“å›¾åƒï¼Œå®ç°ç²¾ç¡®åƒç´ çº§ç›‘ç£å’Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç®€åŒ–é¢„è®­ç»ƒä»»åŠ¡çš„å¤æ‚æ€§å¹¶å¼•å¯¼æ¨¡å‹å…³æ³¨å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬é›†æˆäº†æ¥è‡ªé¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„äºŒç»´ç‰¹å¾ï¼Œä»¥èå…¥æˆç†Ÿçš„çº¹ç†çŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•åœ¨ä¸åŒå¯¹è±¡çº§å’Œåœºæ™¯çº§ä»»åŠ¡ä¸­çš„é€šç”¨æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨å„ç§ç‚¹äº‘æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‚¹äº‘æ•°æ®çš„è§„æ¨¡å¤šæ ·æ€§ç»™ä¸‰ç»´è§†è§‰çš„ç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ç¼ºä¹èƒ½åŒæ—¶æœ‰æ•ˆå¤„ç†å¯¹è±¡çº§å’Œåœºæ™¯çº§ç‚¹äº‘çš„ç»Ÿä¸€é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>UniPre3Dæ˜¯ä¸€ç§æ–°çš„ç»Ÿä¸€é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€‚ç”¨äºä»»ä½•è§„æ¨¡ç‚¹äº‘å’Œä¸‰ç»´æ¨¡å‹ã€‚</li>
<li>UniPre3Dé€šè¿‡é¢„æµ‹é«˜æ–¯åŸºæœ¬ä½“ä½œä¸ºé¢„è®­ç»ƒä»»åŠ¡ï¼Œé‡‡ç”¨å¯å¾®é«˜æ–¯å¹³é“ºæŠ€æœ¯å®ç°ç²¾ç¡®åƒç´ çº§ç›‘ç£å’Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚</li>
<li>ä¸ºç®€åŒ–é¢„è®­ç»ƒä»»åŠ¡å¹¶å¼•å¯¼æ¨¡å‹å…³æ³¨å‡ ä½•ç»“æ„ï¼Œé›†æˆäº†é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„äºŒç»´ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜UniPre3Dåœ¨ä¸åŒå¯¹è±¡çº§å’Œåœºæ™¯çº§ä»»åŠ¡ä¸­è¡¨ç°å‡ºé€šç”¨æœ‰æ•ˆæ€§ã€‚</li>
<li>UniPre3Dä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1edeec9b6839615c7dc064d03b4cc7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88769026279271facbce3e30506beb8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb59ef7f7ad7ec7f0f03026c9900bc97.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="The-Less-You-Depend-The-More-You-Learn-Synthesizing-Novel-Views-from-Sparse-Unposed-Images-without-Any-3D-Knowledge"><a href="#The-Less-You-Depend-The-More-You-Learn-Synthesizing-Novel-Views-from-Sparse-Unposed-Images-without-Any-3D-Knowledge" class="headerlink" title="The Less You Depend, The More You Learn: Synthesizing Novel Views from   Sparse, Unposed Images without Any 3D Knowledge"></a>The Less You Depend, The More You Learn: Synthesizing Novel Views from   Sparse, Unposed Images without Any 3D Knowledge</h2><p><strong>Authors:Haoru Wang, Kai Ye, Yangyan Li, Wenzheng Chen, Baoquan Chen</strong></p>
<p>We consider the problem of generalizable novel view synthesis (NVS), which aims to generate photorealistic novel views from sparse or even unposed 2D images without per-scene optimization. This task remains fundamentally challenging, as it requires inferring 3D structure from incomplete and ambiguous 2D observations. Early approaches typically rely on strong 3D knowledge, including architectural 3D inductive biases (e.g., embedding explicit 3D representations, such as NeRF or 3DGS, into network design) and ground-truth camera poses for both input and target views. While recent efforts have sought to reduce the 3D inductive bias or the dependence on known camera poses of input views, critical questions regarding the role of 3D knowledge and the necessity of circumventing its use remain under-explored. In this work, we conduct a systematic analysis on the 3D knowledge and uncover a critical trend: the performance of methods that requires less 3D knowledge accelerates more as data scales, eventually achieving performance on par with their 3D knowledge-driven counterparts, which highlights the increasing importance of reducing dependence on 3D knowledge in the era of large-scale data. Motivated by and following this trend, we propose a novel NVS framework that minimizes 3D inductive bias and pose dependence for both input and target views. By eliminating this 3D knowledge, our method fully leverages data scaling and learns implicit 3D awareness directly from sparse 2D images, without any 3D inductive bias or pose annotation during training. Extensive experiments demonstrate that our model generates photorealistic and 3D-consistent novel views, achieving even comparable performance with methods that rely on posed inputs, thereby validating the feasibility and effectiveness of our data-centric paradigm. Project page: <a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/Less3Depend/">https://pku-vcl-geometry.github.io/Less3Depend/</a> . </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘é€šç”¨æ–°é¢–è§†è§’åˆæˆï¼ˆNVSï¼‰çš„é—®é¢˜ï¼Œå…¶ç›®æ ‡æ˜¯ä»ç¨€ç–ç”šè‡³æœªæ‘†æ”¾çš„2Då›¾åƒç”ŸæˆçœŸå®æ„Ÿçš„æ–°è§†è§’ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä¸€ä»»åŠ¡ä»ç„¶å…·æœ‰æ ¹æœ¬æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦ä»ä¸å®Œæ•´å’Œæ¨¡ç³Šçš„2Dè§‚å¯Ÿä¸­æ¨æ–­3Dç»“æ„ã€‚æ—©æœŸçš„æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå¤§çš„3DçŸ¥è¯†ï¼ŒåŒ…æ‹¬æ¶æ„çš„3Då½’çº³åè§ï¼ˆä¾‹å¦‚ï¼Œå°†æ˜¾å¼NeRFæˆ–3DGSç­‰3Dè¡¨ç¤ºåµŒå…¥ç½‘ç»œè®¾è®¡ï¼‰ä»¥åŠè¾“å…¥å’Œç›®æ ‡è§†è§’çš„çœŸå®ç›¸æœºå§¿æ€ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›è¯•å›¾å‡å°‘3Då½’çº³åè§æˆ–å¯¹è¾“å…¥è§†è§’å·²çŸ¥ç›¸æœºå§¿æ€çš„ä¾èµ–ï¼Œä½†å…³äº3DçŸ¥è¯†çš„ä½œç”¨ä»¥åŠé¿å…ä½¿ç”¨å®ƒçš„å¿…è¦æ€§çš„å…³é”®é—®é¢˜ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹3DçŸ¥è¯†è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªå…³é”®è¶‹åŠ¿ï¼šéœ€è¦è¾ƒå°‘3DçŸ¥è¯†çš„æ–¹æ³•çš„æ€§èƒ½éšç€æ•°æ®è§„æ¨¡çš„å¢åŠ è€ŒåŠ é€Ÿæé«˜ï¼Œæœ€ç»ˆè¾¾åˆ°äº†ä¸ä¾èµ–3DçŸ¥è¯†çš„åŒè¡Œç›¸å½“çš„æ€§èƒ½æ°´å¹³ï¼Œè¿™çªæ˜¾äº†åœ¨å¤§æ•°æ®æ—¶ä»£å‡å°‘å¯¹3DçŸ¥è¯†ä¾èµ–çš„é‡è¦æ€§ã€‚å—æ­¤è¶‹åŠ¿çš„å¯å‘å¹¶éµå¾ªè¿™ä¸€è¶‹åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„NVSæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æœ€å°åŒ–äº†è¾“å…¥å’Œç›®æ ‡è§†è§’çš„3Då½’çº³åè§å’Œå§¿æ€ä¾èµ–æ€§ã€‚é€šè¿‡æ¶ˆé™¤è¿™ç§3DçŸ¥è¯†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å……åˆ†åˆ©ç”¨äº†æ•°æ®è§„æ¨¡ï¼Œç›´æ¥ä»ç¨€ç–çš„2Då›¾åƒå­¦ä¹ éšå«çš„3Dæ„è¯†ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ä»»ä½•3Då½’çº³åè§æˆ–å§¿æ€æ³¨é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆäº†çœŸå®æ„Ÿä¸”ç¬¦åˆ3Dä¸€è‡´æ€§çš„æ–°é¢–è§†è§’ï¼Œå³ä½¿ä¸ä¾èµ–å®šä½è¾“å…¥çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿå–å¾—äº†ç›¸å½“çš„æ€§èƒ½ï¼Œä»è€ŒéªŒè¯äº†æˆ‘ä»¬çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ¨¡å¼çš„æœ‰æ•ˆæ€§å’Œå¯è¡Œæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/Less3Depend/%E3%80%82">https://pku-vcl-geometry.github.io/Less3Depend/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¯æ¨å¹¿çš„æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰é—®é¢˜ï¼Œæ—¨åœ¨ä»ç¨€ç–æˆ–æœªæ‘†æ”¾çš„2Då›¾åƒç”ŸæˆçœŸå®æ„Ÿçš„æ–°è§†è§’ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚æ–‡ç« æŒ‡å‡ºï¼Œéšç€æ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œå‡å°‘å¯¹3DçŸ¥è¯†çš„éœ€æ±‚çš„æ–¹æ³•æ€§èƒ½æå‡æ›´å¿«ï¼Œæœ€ç»ˆè¾¾åˆ°ä¸ä¾èµ–3DçŸ¥è¯†çš„æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚åŸºäºæ­¤è¶‹åŠ¿ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„NVSæ¡†æ¶ï¼Œæœ€å°åŒ–3Då½’çº³åè§å’Œå¯¹è¾“å…¥åŠç›®æ ‡è§†è§’çš„å§¿æ€ä¾èµ–æ€§ã€‚è¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨æ•°æ®è§„æ¨¡ï¼Œç›´æ¥ä»ç¨€ç–çš„2Då›¾åƒå­¦ä¹ éšå¼çš„3Dæ„ŸçŸ¥ï¼Œæ— éœ€åœ¨è®­ç»ƒæœŸé—´æä¾›ä»»ä½•3Då½’çº³åè§æˆ–å§¿æ€æ³¨é‡Šã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„çœŸå®æ„Ÿã€3Dä¸€è‡´çš„æ–°è§†è§’ï¼Œç”šè‡³ä¸ä¾èµ–å®šä½è¾“å…¥çš„æ–¹æ³•ç›¸æ¯”ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é’ˆå¯¹å¯æ¨å¹¿çš„æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰é—®é¢˜ã€‚</li>
<li>NVSä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä»ä¸å®Œæ•´å’Œæ¨¡ç³Šçš„2Dè§‚å¯Ÿä¸­æ¨æ–­3Dç»“æ„ã€‚</li>
<li>éšç€æ•°æ®è§„æ¨¡çš„å¢åŠ ï¼Œå‡å°‘å¯¹3DçŸ¥è¯†ä¾èµ–çš„æ–¹æ³•æ€§èƒ½æå‡æ›´å¿«ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„NVSæ¡†æ¶ï¼Œæœ€å°åŒ–3Då½’çº³åè§å’Œå¯¹å§¿æ€çš„ä¾èµ–ã€‚</li>
<li>æ–¹æ³•ç›´æ¥ä»ç¨€ç–çš„2Då›¾åƒå­¦ä¹ éšå¼çš„3Dæ„ŸçŸ¥ï¼Œæ— éœ€åœ¨è®­ç»ƒæœŸé—´æä¾›3DçŸ¥è¯†å’Œå§¿æ€æ³¨é‡Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33fa067bfd672a3f546d623ba205f368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1895cc5423e9285b465a0937fc760ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bccad08474b6276a4d4b8637806ea9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c072c0874358196d32cbdaa9ecbf30d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-421ebf3a6c4480d41fcbf41acb96e91d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DynaSplat-Dynamic-Static-Gaussian-Splatting-with-Hierarchical-Motion-Decomposition-for-Scene-Reconstruction"><a href="#DynaSplat-Dynamic-Static-Gaussian-Splatting-with-Hierarchical-Motion-Decomposition-for-Scene-Reconstruction" class="headerlink" title="DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion   Decomposition for Scene Reconstruction"></a>DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion   Decomposition for Scene Reconstruction</h2><p><strong>Authors:Junli Deng, Ping Shi, Qipei Li, Jinyang Guo</strong></p>
<p>Reconstructing intricate, ever-changing environments remains a central ambition in computer vision, yet existing solutions often crumble before the complexity of real-world dynamics. We present DynaSplat, an approach that extends Gaussian Splatting to dynamic scenes by integrating dynamic-static separation and hierarchical motion modeling. First, we classify scene elements as static or dynamic through a novel fusion of deformation offset statistics and 2D motion flow consistency, refining our spatial representation to focus precisely where motion matters. We then introduce a hierarchical motion modeling strategy that captures both coarse global transformations and fine-grained local movements, enabling accurate handling of intricate, non-rigid motions. Finally, we integrate physically-based opacity estimation to ensure visually coherent reconstructions, even under challenging occlusions and perspective shifts. Extensive experiments on challenging datasets reveal that DynaSplat not only surpasses state-of-the-art alternatives in accuracy and realism but also provides a more intuitive, compact, and efficient route to dynamic scene reconstruction. </p>
<blockquote>
<p>é‡å»ºå¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä»æ˜¯è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒç›®æ ‡ï¼Œä½†ç°æœ‰è§£å†³æ–¹æ¡ˆå¾€å¾€éš¾ä»¥åº”å¯¹ç°å®ä¸–ç•Œä¸­åŠ¨æ€çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†DynaSplatæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ•´åˆåŠ¨é™åˆ†ç¦»å’Œåˆ†å±‚è¿åŠ¨å»ºæ¨¡ï¼Œå°†é«˜æ–¯Splattingæ‰©å±•åˆ°åŠ¨æ€åœºæ™¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å˜å½¢åç§»ç»Ÿè®¡å’Œ2Dè¿åŠ¨æµä¸€è‡´æ€§èåˆçš„æ–°æ–¹æ³•ï¼Œå°†åœºæ™¯å…ƒç´ åˆ†ç±»ä¸ºé™æ€æˆ–åŠ¨æ€ï¼Œä»è€Œç²¾ç¡®èšç„¦åœ¨è¿åŠ¨å…³é”®åŒºåŸŸæ¥ä¼˜åŒ–æˆ‘ä»¬çš„ç©ºé—´è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚è¿åŠ¨å»ºæ¨¡ç­–ç•¥ï¼Œæ—¢èƒ½æ•æ‰ç²—ç³™çš„å…¨å±€å˜æ¢ï¼Œåˆèƒ½æ•æ‰ç²¾ç»†çš„å±€éƒ¨è¿åŠ¨ï¼Œä»è€Œå®ç°å¤æ‚éåˆšæ€§è¿åŠ¨çš„ç²¾ç¡®å¤„ç†ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“åˆäº†åŸºäºç‰©ç†çš„é€æ˜åº¦ä¼°è®¡ï¼Œä»¥ç¡®ä¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é®æŒ¡å’Œé€è§†å˜åŒ–ä¸‹ä¹Ÿèƒ½å®ç°è§†è§‰è¿è´¯çš„é‡å»ºã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDynaSplatä¸ä»…åœ¨å‡†ç¡®æ€§å’Œé€¼çœŸåº¦ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè€Œä¸”è¿˜ä¸ºåŠ¨æ€åœºæ™¯é‡å»ºæä¾›äº†æ›´ç›´è§‚ã€æ›´ç´§å‡‘ã€æ›´é«˜æ•ˆçš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09836v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDynaSplatçš„æ–°æ–¹æ³•ï¼Œç”¨äºåŠ¨æ€åœºæ™¯çš„é‡å»ºã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŠ¨æ€é™æ€åˆ†ç¦»å’Œå±‚æ¬¡åŒ–è¿åŠ¨å»ºæ¨¡ï¼Œé€šè¿‡å˜å½¢åç§»ç»Ÿè®¡å’ŒäºŒç»´è¿åŠ¨æµä¸€è‡´æ€§èåˆå¯¹åœºæ™¯å…ƒç´ è¿›è¡Œåˆ†ç±»ï¼Œå¹¶å¯¹è¿åŠ¨è¿›è¡Œç²¾å‡†å»ºæ¨¡ï¼Œä»è€Œå®ç°äº†å¯¹å¤æ‚åŠ¨æ€åœºæ™¯çš„ç²¾ç¡®é‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DynaSplatæ‰©å±•äº†é«˜æ–¯SplattingæŠ€æœ¯ï¼Œé€‚ç”¨äºåŠ¨æ€åœºæ™¯çš„é‡å»ºã€‚</li>
<li>é€šè¿‡ç»“åˆåŠ¨æ€é™æ€åˆ†ç¦»æŠ€æœ¯ï¼ŒDynaSplatèƒ½å¤ŸåŒºåˆ†åœºæ™¯ä¸­çš„é™æ€å’ŒåŠ¨æ€å…ƒç´ ã€‚</li>
<li>é‡‡ç”¨äº†å±‚æ¬¡åŒ–è¿åŠ¨å»ºæ¨¡ç­–ç•¥ï¼Œèƒ½æ•æ‰ç²—ç•¥çš„å…¨å±€å˜æ¢å’Œç²¾ç»†çš„å±€éƒ¨è¿åŠ¨ã€‚</li>
<li>å¼•å…¥åŸºäºç‰©ç†çš„é€æ˜åº¦ä¼°è®¡ï¼Œç¡®ä¿åœ¨é®æŒ¡å’Œé€è§†å˜åŒ–ä¸‹çš„è§†è§‰è¿è´¯æ€§é‡å»ºã€‚</li>
<li>DynaSplatåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œç°å®æ„Ÿã€‚</li>
<li>DynaSplatä¸ºåŠ¨æ€åœºæ™¯é‡å»ºæä¾›äº†æ›´ç›´è§‚ã€ç´§å‡‘å’Œé«˜æ•ˆçš„é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b85103bd3f0f72c78f23976925a9abc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04792ab82add08354fa6e154c5fbf1f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b60ef058c0b4e560c328901702fd9428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b89852366365c19d703ed460d7749ed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a4e19c4733a40da8e9fb0f4ae97e30c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Multi-Part-Articulated-Objects-Modeling-via-Deformable-Gaussian-Splatting-and-Progressive-Primitive-Segmentation"><a href="#Self-Supervised-Multi-Part-Articulated-Objects-Modeling-via-Deformable-Gaussian-Splatting-and-Progressive-Primitive-Segmentation" class="headerlink" title="Self-Supervised Multi-Part Articulated Objects Modeling via Deformable   Gaussian Splatting and Progressive Primitive Segmentation"></a>Self-Supervised Multi-Part Articulated Objects Modeling via Deformable   Gaussian Splatting and Progressive Primitive Segmentation</h2><p><strong>Authors:Haowen Wang, Xiaoping Yuan, Zhao Jin, Zhen Zhao, Zhengping Che, Yousong Xue, Jin Tian, Yakun Huang, Jian Tang</strong></p>
<p>Articulated objects are ubiquitous in everyday life, and accurate 3D representations of their geometry and motion are critical for numerous applications. However, in the absence of human annotation, existing approaches still struggle to build a unified representation for objects that contain multiple movable parts. We introduce DeGSS, a unified framework that encodes articulated objects as deformable 3D Gaussian fields, embedding geometry, appearance, and motion in one compact representation. Each interaction state is modeled as a smooth deformation of a shared field, and the resulting deformation trajectories guide a progressive coarse-to-fine part segmentation that identifies distinct rigid components, all in an unsupervised manner. The refined field provides a spatially continuous, fully decoupled description of every part, supporting part-level reconstruction and precise modeling of their kinematic relationships. To evaluate generalization and realism, we enlarge the synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset that pairs RGB captures with accurately reverse-engineered 3D models. Extensive experiments demonstrate that our method outperforms existing methods in both accuracy and stability. </p>
<blockquote>
<p>æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¯åŠ¨ç‰©ä½“æ— å¤„ä¸åœ¨ï¼Œå¯¹å…¶å‡ ä½•å½¢çŠ¶å’Œè¿åŠ¨çš„ç²¾ç¡®ä¸‰ç»´è¡¨ç¤ºå¯¹è®¸å¤šåº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥å¯¹åŒ…å«å¤šä¸ªå¯åŠ¨éƒ¨ä»¶çš„ç‰©ä½“æ„å»ºç»Ÿä¸€è¡¨ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†DeGSSç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒå°†å¯åŠ¨ç‰©ä½“ç¼–ç ä¸ºå¯å˜å½¢ä¸‰ç»´é«˜æ–¯åœºï¼Œå°†å‡ ä½•å½¢çŠ¶ã€å¤–è§‚å’Œè¿åŠ¨åµŒå…¥åˆ°ä¸€ä¸ªç´§å‡‘çš„è¡¨ç¤ºä¸­ã€‚æ¯ä¸ªäº¤äº’çŠ¶æ€éƒ½è¢«å»ºæ¨¡ä¸ºå…±äº«å­—æ®µçš„å¹³æ»‘å˜å½¢ï¼Œå¾—åˆ°çš„å˜å½¢è½¨è¿¹å¼•å¯¼æ¸è¿›çš„ç²—ç»†éƒ¨ä»¶åˆ†å‰²ï¼Œä»¥è¯†åˆ«ä¸åŒçš„åˆšæ€§ç»„ä»¶ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä»¥æ— ç›‘ç£çš„æ–¹å¼è¿›è¡Œã€‚ä¼˜åŒ–åçš„å­—æ®µæä¾›äº†æ¯ä¸ªéƒ¨ä»¶çš„ç©ºé—´è¿ç»­ã€å®Œå…¨è§£è€¦çš„æè¿°ï¼Œæ”¯æŒéƒ¨ä»¶çº§åˆ«çš„é‡å»ºå’Œä»–ä»¬è¿åŠ¨å­¦å…³ç³»çš„ç²¾ç¡®å»ºæ¨¡ã€‚ä¸ºäº†è¯„ä¼°é€šç”¨æ€§å’ŒçœŸå®æ€§ï¼Œæˆ‘ä»¬æ‰©å¤§äº†åˆæˆPartNet-MobilityåŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘å¸ƒäº†RS-Artï¼Œè¿™æ˜¯ä¸€ä¸ªçœŸå®åˆ°æ¨¡æ‹Ÿçš„æ•°æ®é›†ï¼Œå®ƒå°†RGBæ•è·ä¸ç²¾ç¡®çš„é€†å‘å·¥ç¨‹ä¸‰ç»´æ¨¡å‹é…å¯¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09663v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†DeGSSæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¯ç§»åŠ¨ç‰©ä½“ç¼–ç ä¸ºå¯å˜å½¢ä¸‰ç»´é«˜æ–¯åœºï¼Œå°†å‡ ä½•å½¢çŠ¶ã€å¤–è§‚å’Œè¿åŠ¨åµŒå…¥ä¸€ä¸ªç´§å‡‘çš„è¡¨ç¤ºä¸­ã€‚æ¡†æ¶åœ¨æ— ç›‘ç£æƒ…å†µä¸‹æ¨¡æ‹Ÿå¯¹è±¡çš„äº¤äº’çŠ¶æ€ä½œä¸ºå…±äº«åœºçš„å¹³æ»‘å˜å½¢ï¼Œå¹¶æä¾›ç²¾ç»†åŒ–å­—æ®µä»¥æ”¯æŒéƒ¨ä»¶çº§é‡å»ºå’Œç²¾ç¡®çš„åˆšä½“åŠ¨åŠ›å­¦å…³ç³»å»ºæ¨¡ã€‚æå‡ºçš„æ•°æ®é›†PartNet-Mobilityå’Œè¿åŠ¨å­¦æµ‹è¯•æ˜¾ç¤ºå…¶æ–¹æ³•çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DeGSSæ¡†æ¶é‡‡ç”¨å¯å˜å½¢ä¸‰ç»´é«˜æ–¯åœºè¡¨ç¤ºç‰©ä½“å‡ ä½•å’Œè¿åŠ¨ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå…±äº«åœºçš„å¹³æ»‘å˜å½¢æ¥å»ºæ¨¡å¯¹è±¡çš„äº¤äº’çŠ¶æ€ã€‚</li>
<li>åˆ©ç”¨ç²¾ç»†åŒ–åœºå®ç°éƒ¨ä»¶çº§åˆ«çš„é‡å»ºå’Œç²¾ç¡®çš„åˆšä½“åŠ¨åŠ›å­¦å…³ç³»å»ºæ¨¡ã€‚</li>
<li>é€šè¿‡æ— ç›‘ç£å­¦ä¹ è¿›è¡Œç‰©ä½“åˆ†å‰²å’Œè¯†åˆ«ã€‚</li>
<li>æå‡ºæ–°çš„æ•°æ®é›†PartNet-Mobilityå’ŒRS-Artä»¥è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç°å®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a294e2f222f781855089e05e6a3cca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85c0238b4e36841ce7d29d0d893808f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SemanticSplat-Feed-Forward-3D-Scene-Understanding-with-Language-Aware-Gaussian-Fields"><a href="#SemanticSplat-Feed-Forward-3D-Scene-Understanding-with-Language-Aware-Gaussian-Fields" class="headerlink" title="SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware   Gaussian Fields"></a>SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware   Gaussian Fields</h2><p><strong>Authors:Qijing Li, Jingxiang Sun, Liang An, Zhaoqi Su, Hongwen Zhang, Yebin Liu</strong></p>
<p>Holistic 3D scene understanding, which jointly models geometry, appearance, and semantics, is crucial for applications like augmented reality and robotic interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM) are limited to extracting language-based semantics from scenes, failing to achieve holistic scene comprehension. Additionally, they suffer from low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene optimization methods rely on dense input views, which reduces practicality and increases complexity during deployment. In this paper, we propose SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which unifies 3D Gaussians with latent semantic attributes for joint geometry-appearance-semantics modeling. To predict the semantic anisotropic Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a cost volume representation that stores cross-view feature similarities, enhancing coherent and accurate scene comprehension. Leveraging a two-stage distillation framework, SemanticSplat reconstructs a holistic multi-modal semantic feature field from sparse-view images. Experiments demonstrate the effectiveness of our method for 3D scene understanding tasks like promptable and open-vocabulary segmentation. Video results are available at <a target="_blank" rel="noopener" href="https://semanticsplat.github.io/">https://semanticsplat.github.io</a>. </p>
<blockquote>
<p>å…¨æ™¯3Dåœºæ™¯ç†è§£ï¼Œå³å¯¹å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰è¿›è¡Œè”åˆå»ºæ¨¡ï¼Œå¯¹äºå¢å¼ºç°å®å’Œæœºå™¨äººäº¤äº’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å‰é¦ˆ3Dåœºæ™¯ç†è§£æ–¹æ³•ï¼ˆä¾‹å¦‚LSMï¼‰ä»…é™äºä»åœºæ™¯ä¸­æå–åŸºäºè¯­è¨€çš„è¯­ä¹‰ï¼Œæ— æ³•å®ç°å…¨æ™¯åœºæ™¯ç†è§£ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜é¢ä¸´å‡ ä½•é‡å»ºè´¨é‡ä½å’Œå™ªå£°å¹²æ‰°ç­‰é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºåœºæ™¯çš„ä¼˜åŒ–æ–¹æ³•ä¾èµ–äºå¯†é›†çš„è¾“å…¥è§†å›¾ï¼Œè¿™é™ä½äº†å®ç”¨æ€§ï¼Œå¹¶å¢åŠ äº†éƒ¨ç½²æœŸé—´çš„å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SemanticSplatï¼Œä¸€ç§å‰é¦ˆçš„è¯­ä¹‰æ„ŸçŸ¥3Dé‡å»ºæ–¹æ³•ï¼Œå®ƒå°†3Dé«˜æ–¯ä¸æ½œåœ¨è¯­ä¹‰å±æ€§ç›¸ç»“åˆï¼Œè¿›è¡Œè”åˆå‡ ä½•-å¤–è§‚-è¯­ä¹‰å»ºæ¨¡ã€‚ä¸ºäº†é¢„æµ‹è¯­ä¹‰å„å‘å¼‚æ€§é«˜æ–¯ï¼ŒSemanticSplatèåˆäº†ä¸åŒçš„ç‰¹å¾åœºï¼ˆä¾‹å¦‚LSegã€SAMï¼‰ä¸æˆæœ¬ä½“ç§¯è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºå­˜å‚¨è·¨è§†å›¾ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œå¢å¼ºäº†è¿è´¯å’Œå‡†ç¡®çš„åœºæ™¯ç†è§£ã€‚åˆ©ç”¨ä¸¤é˜¶æ®µè’¸é¦æ¡†æ¶ï¼ŒSemanticSplatä»ç¨€ç–è§†å›¾å›¾åƒé‡å»ºå…¨æ™¯å¤šæ¨¡æ€è¯­ä¹‰ç‰¹å¾åœºã€‚å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨å¯æç¤ºå’Œå¼€æ”¾è¯æ±‡åˆ†å‰²ç­‰3Dåœºæ™¯ç†è§£ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚è§†é¢‘ç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://semanticsplat.github.ioæŸ¥çœ‹./">https://semanticsplat.github.ioæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSemanticSplatçš„è¯­ä¹‰æ„ŸçŸ¥çš„3Dé‡å»ºæ–¹æ³•ï¼Œç”¨äºå…¨æ¯3Dåœºæ™¯ç†è§£ã€‚è¯¥æ–¹æ³•ç»“åˆäº†3Dé«˜æ–¯æ½œè¯­ä¹‰å±æ€§ï¼Œå®ç°äº†å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰çš„è”åˆå»ºæ¨¡ã€‚é€šè¿‡èåˆå¤šç§ç‰¹å¾åœºå’Œæˆæœ¬ä½“ç§¯è¡¨ç¤ºï¼ŒSemanticSplatèƒ½å¤Ÿé¢„æµ‹è¯­ä¹‰å„å‘å¼‚æ€§é«˜æ–¯ï¼Œæé«˜åœºæ™¯ç†è§£çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¨€ç–è§†è§’å›¾åƒä¸Šèƒ½æœ‰æ•ˆé‡å»ºå…¨æ¯å¤šæ¨¡æ€è¯­ä¹‰ç‰¹å¾åœºï¼Œé€‚ç”¨äºå¯æç¤ºå’Œå¼€æ”¾è¯æ±‡è¡¨çš„åˆ†å‰²ç­‰3Dåœºæ™¯ç†è§£ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºè°ƒäº†å…¨æ¯3Dåœºæ™¯ç†è§£çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¢å¼ºç°å®å’Œæœºå™¨äººäº¤äº’ç­‰åº”ç”¨ã€‚</li>
<li>æŒ‡å‡ºç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼šæ— æ³•åŒæ—¶å®ç°å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰çš„å…¨é¢å»ºæ¨¡ï¼Œä¸”å­˜åœ¨å‡ ä½•é‡å»ºè´¨é‡ä½å’Œå™ªå£°å¹²æ‰°é—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†SemanticSplatæ–¹æ³•çš„ç‰¹ç‚¹ï¼šç»“åˆäº†è¯­ä¹‰æ„ŸçŸ¥çš„3Dé‡å»ºå’Œå…¨æ¯åœºæ™¯ç†è§£ï¼Œå®ç°äº†å‡ ä½•ã€å¤–è§‚å’Œè¯­ä¹‰çš„è”åˆå»ºæ¨¡ã€‚</li>
<li>é‡‡ç”¨äº†å¤šç§ç‰¹å¾åœºçš„èåˆæŠ€æœ¯å’Œæˆæœ¬ä½“ç§¯è¡¨ç¤ºæ–¹æ³•ï¼Œæé«˜äº†åœºæ™¯ç†è§£çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè’¸é¦æ¡†æ¶ä»ç¨€ç–è§†è§’å›¾åƒä¸­é‡å»ºå…¨æ¯å¤šæ¨¡æ€è¯­ä¹‰ç‰¹å¾åœºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9ea60cc8cfb98be7529f63c524820c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d940b0020137b328d1f5b5db63d859bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a58b6099287136965f7f32e8600f61e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0229dc8e1d913910e5310b9127def0b3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Gaussian-Herding-across-Pens-An-Optimal-Transport-Perspective-on-Global-Gaussian-Reduction-for-3DGS"><a href="#Gaussian-Herding-across-Pens-An-Optimal-Transport-Perspective-on-Global-Gaussian-Reduction-for-3DGS" class="headerlink" title="Gaussian Herding across Pens: An Optimal Transport Perspective on Global   Gaussian Reduction for 3DGS"></a>Gaussian Herding across Pens: An Optimal Transport Perspective on Global   Gaussian Reduction for 3DGS</h2><p><strong>Authors:Tao Wang, Mengyu Li, Geduo Zeng, Cheng Meng, Qiong Zhang</strong></p>
<p>3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering. </p>
<blockquote>
<p>3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰å·²æˆä¸ºè¾å°„åœºæ¸²æŸ“çš„å¼ºå¤§æŠ€æœ¯ï¼Œä½†å®ƒé€šå¸¸éœ€è¦æ•°ä»¥ç™¾ä¸‡è®¡çš„å¤šä½™é«˜æ–¯åŸºæœ¬ä½“ï¼Œä»è€Œæ¶ˆè€—å¤§é‡å†…å­˜å’Œæ¸²æŸ“é¢„ç®—ã€‚ç°æœ‰çš„å‹ç¼©æ–¹æ³•é€šè¿‡åŸºäºå¯å‘å¼é‡è¦æ€§åˆ†æ•°çš„é«˜æ–¯ä¿®å‰ªæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ²¡æœ‰å…¨å±€ä¿çœŸåº¦ä¿è¯ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ä»å…¨æ–°çš„æœ€ä¼˜ä¼ è¾“è§’åº¦æå‡ºå°†3DGSå‹ç¼©è§†ä¸ºå…¨å±€é«˜æ–¯æ··åˆå‡å°‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡KDæ ‘åˆ†åŒºæœ€å°åŒ–å¤åˆä¼ è¾“æ•£åº¦ï¼Œä»¥äº§ç”Ÿç´§å‡‘çš„å‡ ä½•è¡¨ç¤ºï¼Œç„¶åé€šè¿‡å¾®è°ƒé¢œè‰²å’Œé€æ˜åº¦å±æ€§æ¥å°†å¤–è§‚ä¸å‡ ä½•åˆ†ç¦»ï¼Œæ‰€éœ€çš„é«˜æ–¯åŸºæœ¬ä½“æ›´å°‘ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆiï¼‰ä¸ä½¿ç”¨ä»…10%çš„é«˜æ–¯ç›¸æ¯”ï¼Œåœ¨æ¸²æŸ“è´¨é‡ï¼ˆPSNRã€SSIMã€LPIPSï¼‰æ–¹é¢å‡ ä¹æ²¡æœ‰æŸå¤±ï¼›ï¼ˆiiï¼‰å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„3DGSå‹ç¼©æŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºä»»ä½•é˜¶æ®µçš„æ™®é€šæˆ–åŠ é€Ÿ3DGSç®¡é“ï¼Œä¸ºè½»é‡çº§ç¥ç»æ¸²æŸ“æä¾›äº†é«˜æ•ˆä¸”é€šç”¨çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09534v1">PDF</a> 18 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>3DGSæŠ€æœ¯ç”¨äºè¾å°„åœºæ¸²æŸ“æ•ˆæœæ˜¾è‘—ï¼Œä½†å­˜åœ¨å¤§é‡å†—ä½™çš„é«˜æ–¯åŸºæœ¬ä½“ï¼Œå ç”¨å¤§é‡å†…å­˜å’Œæ¸²æŸ“é¢„ç®—ã€‚ç°æœ‰å‹ç¼©æ–¹æ³•åŸºäºå¯å‘å¼é‡è¦æ€§è¯„åˆ†è¿›è¡Œé«˜æ–¯å‰ªæï¼Œæ— æ³•ä¿è¯å…¨å±€ä¿çœŸåº¦ã€‚æœ¬ç ”ç©¶ä»æœ€ä¼˜ä¼ è¾“è§’åº¦æå‡ºæ–°çš„å…¨çƒé«˜æ–¯æ··åˆå‡å°‘æ–¹æ³•ï¼Œä»¥ç¼©å°è¿™ä¸€å·®è·ã€‚é¦–å…ˆï¼Œé€šè¿‡KDæ ‘åˆ†åŒºæœ€å°åŒ–å¤åˆä¼ è¾“æ•£åº¦ï¼Œäº§ç”Ÿç´§å‡‘çš„å‡ ä½•è¡¨ç¤ºï¼›ç„¶åï¼Œé€šè¿‡å¾®è°ƒé¢œè‰²å’Œé€æ˜åº¦å±æ€§ï¼Œå®ç°å¤–è§‚ä¸å‡ ä½•çš„è§£è€¦ï¼Œä½¿ç”¨æ›´å°‘çš„é«˜æ–¯åŸºæœ¬ä½“ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¸²æŸ“è´¨é‡çš„åŒæ—¶ï¼Œå¤§å¤§å‡å°‘äº†é«˜æ–¯åŸºæœ¬ä½“çš„æ•°é‡ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›çš„3DGSå‹ç¼©æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSæŠ€æœ¯åœ¨è¾å°„åœºæ¸²æŸ“ä¸­æ•ˆæœæ˜¾è‘—ï¼Œä½†å­˜åœ¨å¤§é‡å†—ä½™é«˜æ–¯åŸºæœ¬ä½“é—®é¢˜ã€‚</li>
<li>ç°æœ‰å‹ç¼©æ–¹æ³•ä¸»è¦åŸºäºå¯å‘å¼é‡è¦æ€§è¯„åˆ†è¿›è¡Œé«˜æ–¯å‰ªæï¼Œç¼ºä¹å…¨å±€ä¿çœŸåº¦ä¿éšœã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæœ€ä¼˜ä¼ è¾“è§’åº¦çš„å…¨çƒé«˜æ–¯æ··åˆå‡å°‘æ–°æ–¹æ³•ï¼Œå®ç°ç´§å‡‘çš„å‡ ä½•è¡¨ç¤ºã€‚</li>
<li>æ–¹æ³•é¦–å…ˆé€šè¿‡KDæ ‘åˆ†åŒºæœ€å°åŒ–å¤åˆä¼ è¾“æ•£åº¦ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢œè‰²å’Œé€æ˜åº¦å±æ€§ï¼Œå®ç°å¤–è§‚ä¸å‡ ä½•çš„è§£è€¦ï¼Œä½¿ç”¨æ›´å°‘çš„é«˜æ–¯åŸºæœ¬ä½“ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ¸²æŸ“è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†é«˜æ–¯åŸºæœ¬ä½“çš„æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7be3b4245a451ab74b82ff45f1f3472e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7493e9805795e76f44bcf2b559e85d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efe482f0c9a46fa4a76ae52d000e93c2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HAIF-GS-Hierarchical-and-Induced-Flow-Guided-Gaussian-Splatting-for-Dynamic-Scene"><a href="#HAIF-GS-Hierarchical-and-Induced-Flow-Guided-Gaussian-Splatting-for-Dynamic-Scene" class="headerlink" title="HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for   Dynamic Scene"></a>HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for   Dynamic Scene</h2><p><strong>Authors:Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, Yucheng Zhang</strong></p>
<p>Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppresses redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency. </p>
<blockquote>
<p>ä»å•ç›®è§†é¢‘ä¸­é‡å»ºåŠ¨æ€ä¸‰ç»´åœºæ™¯ä»ç„¶æ˜¯ä¸‰ç»´è§†è§‰é¢†åŸŸçš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚è™½ç„¶ä¸‰ç»´é«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰åœ¨é™æ€åœºæ™¯ä¸­å®ç°äº†å®æ—¶æ¸²æŸ“ï¼Œä½†ç”±äºå­¦ä¹ ç»“æ„åŒ–ä¸”æ—¶é—´ä¸Šä¸€è‡´çš„åŠ¨æ€è¡¨ç¤ºçš„éš¾åº¦ï¼Œå°†å…¶æ‰©å±•åˆ°åŠ¨æ€åœºæ™¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨ç°æœ‰æ–¹æ³•ä¸­é€šå¸¸è¡¨ç°ä¸ºä¸‰ä¸ªå±€é™æ€§ï¼šå†—ä½™çš„é«˜æ–¯æ›´æ–°ã€è¿åŠ¨ç›‘ç£ä¸è¶³ä»¥åŠå¤æ‚éåˆšæ€§å˜å½¢çš„å¼±å»ºæ¨¡ã€‚è¿™äº›é—®é¢˜å…±åŒé˜»ç¢äº†è¿è´¯å’Œé«˜æ•ˆçš„åŠ¨æ€é‡å»ºã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HAIF-GSï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç¨€ç–é”šé©±åŠ¨å˜å½¢å®ç°ç»“æ„åŒ–ä¸”ä¸€è‡´åŠ¨æ€å»ºæ¨¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚å®ƒé¦–å…ˆé€šè¿‡é”šç‚¹è¿‡æ»¤å™¨è¯†åˆ«è¿åŠ¨ç›¸å…³åŒºåŸŸï¼Œä»è€ŒæŠ‘åˆ¶é™æ€åŒºåŸŸçš„å†—ä½™æ›´æ–°ã€‚è‡ªç›‘ç£çš„è¯±å¯¼æµå¼•å¯¼å˜å½¢æ¨¡å—åˆ©ç”¨å¤šå¸§ç‰¹å¾èšåˆè¯±å¯¼é”šç‚¹è¿åŠ¨ï¼Œæ— éœ€æ˜ç¡®çš„æµæ ‡ç­¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¤„ç†ç²¾ç»†çš„å˜å½¢ï¼Œåˆ†å±‚é”šç‚¹ä¼ æ’­æœºåˆ¶æ ¹æ®è¿åŠ¨å¤æ‚æ€§æé«˜é”šç‚¹åˆ†è¾¨ç‡å¹¶ä¼ æ’­å¤šçº§å˜æ¢ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯ï¼ŒHAIF-GSåœ¨æ¸²æŸ“è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œé‡å»ºæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŠ¨æ€3DGSæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09518v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŠ¨æ€ä¸‰ç»´åœºæ™¯ä»å•ç›®è§†é¢‘ä¸­é‡å»ºä»æ˜¯ä¸‰ç»´è§†è§‰ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å†—ä½™é«˜æ–¯æ›´æ–°ã€è¿åŠ¨ç›‘ç£ä¸è¶³ä»¥åŠå¤æ‚éåˆšæ€§å˜å½¢çš„å¼±å»ºæ¨¡ç­‰ï¼Œæå‡ºäº†HAIF-GSç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¨€ç–é”šç‚¹é©±åŠ¨å˜å½¢ï¼Œå®ç°ç»“æ„åŒ–ä¸€è‡´æ€§åŠ¨æ€å»ºæ¨¡ã€‚å®éªŒè¯æ˜ï¼ŒHAIF-GSåœ¨æ¸²æŸ“è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œé‡å»ºæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„åŠ¨æ€3DGSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠ¨æ€ä¸‰ç»´åœºæ™¯ä»å•ç›®è§†é¢‘ä¸­é‡å»ºæ˜¯ä¸‰ç»´è§†è§‰é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯é‡å»ºä¸­é¢ä¸´å†—ä½™é«˜æ–¯æ›´æ–°ã€è¿åŠ¨ç›‘ç£ä¸è¶³ä»¥åŠå¤æ‚éåˆšæ€§å˜å½¢çš„å¼±å»ºæ¨¡ç­‰é—®é¢˜ã€‚</li>
<li>HAIF-GSæ¡†æ¶é€šè¿‡ç¨€ç–é”šç‚¹é©±åŠ¨å˜å½¢ï¼Œå®ç°ç»“æ„åŒ–ä¸€è‡´æ€§åŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>Anchor Filterç”¨äºè¯†åˆ«è¿åŠ¨ç›¸å…³åŒºåŸŸï¼ŒæŠ‘åˆ¶é™æ€åŒºåŸŸçš„å†—ä½™æ›´æ–°ã€‚</li>
<li>æå‡ºäº†è‡ªç›‘ç£çš„Induced Flow-Guided Deformationæ¨¡å—ï¼Œåˆ©ç”¨å¤šå¸§ç‰¹å¾èšåˆè¯±å¯¼é”šç‚¹è¿åŠ¨ï¼Œæ— éœ€æ˜ç¡®æµæ ‡ç­¾ã€‚</li>
<li>Hierarchical Anchor Propagationæœºåˆ¶æ ¹æ®è¿åŠ¨å¤æ‚æ€§æé«˜é”šç‚¹åˆ†è¾¨ç‡ï¼Œå¹¶ä¼ æ’­å¤šçº§å˜æ¢ï¼Œä»¥å¤„ç†ç»†å¾®å˜å½¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff6b760590fa71c770ecce8224d61bd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdd402bed2302c1ff1a420dde58bd548.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba7fd88b2c23f0fd77442a549353b0fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TinySplat-Feedforward-Approach-for-Generating-Compact-3D-Scene-Representation"><a href="#TinySplat-Feedforward-Approach-for-Generating-Compact-3D-Scene-Representation" class="headerlink" title="TinySplat: Feedforward Approach for Generating Compact 3D Scene   Representation"></a>TinySplat: Feedforward Approach for Generating Compact 3D Scene   Representation</h2><p><strong>Authors:Zetian Song, Jiaye Fu, Jiaqi Zhang, Xiaohan Lu, Chuanmin Jia, Siwei Ma, Wen Gao</strong></p>
<p>The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a training-free compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time. </p>
<blockquote>
<p>è¿‘æœŸå‘å±•çš„å‰é¦ˆä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ä¸ºé‡å»ºä¸‰ç»´åœºæ™¯æä¾›äº†æ–°çš„èŒƒå¼ã€‚å®ƒä½¿ç”¨åœ¨å¤§è§„æ¨¡å¤šè§†è§’æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œå¯ä»¥ç›´æ¥ä»ç¨€ç–çš„è¾“å…¥è§†è§’æ¨æ–­å‡º3DGSè¡¨ç¤ºã€‚è™½ç„¶å‰é¦ˆæ–¹æ³•å®ç°äº†é«˜é€Ÿé‡å»ºï¼Œä½†å®ƒä»ç„¶é¢ä¸´ç€ä¸‰ç»´é«˜æ–¯å­˜å‚¨æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ç°æœ‰çš„ä¾èµ–äºåœºæ™¯ä¼˜åŒ–çš„3DGSå‹ç¼©æ–¹æ³•ç”±äºæ¶æ„ä¸å…¼å®¹è€Œä¸å¯ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†TinySplatï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå‰é¦ˆç”Ÿæˆç´§å‡‘ä¸‰ç»´åœºæ™¯è¡¨ç¤ºçš„æ–¹æ³•ã€‚TinySplatå»ºç«‹åœ¨æ ‡å‡†å‰é¦ˆ3DGSæ–¹æ³•çš„åŸºç¡€ä¸Šï¼Œé›†æˆäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒå³å¯ä½¿ç”¨çš„å‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°æ¶ˆé™¤äº†å…³é”®æ¥æºçš„å†—ä½™ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§’æŠ•å½±å˜æ¢ï¼ˆVPTï¼‰ï¼Œé€šè¿‡å°†å‡ ä½•å‚æ•°æŠ•å½±åˆ°æ›´ç´§å‡‘çš„ç©ºé—´æ¥å‡å°‘å‡ ä½•å†—ä½™ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åŸºäºå¯è§æ€§çš„åŸºå…ƒå‡å°‘ï¼ˆVABRï¼‰ï¼Œé€šè¿‡å¯¹ä¸»è¦è§‚çœ‹æ–¹å‘ä¸Šçš„ç‰¹å¾èƒ½é‡è¿›è¡ŒåŸºå…ƒå˜æ¢æ¥å‡è½»æ„ŸçŸ¥å†—ä½™ã€‚æœ€åï¼Œé€šè¿‡ç°æˆçš„è§†é¢‘ç¼–è§£ç å™¨è§£å†³ç©ºé—´å†—ä½™é—®é¢˜ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¡¨æ˜ï¼ŒTinySplatå¯¹å‰é¦ˆæ–¹æ³•ç”Ÿæˆçš„ä¸‰ç»´é«˜æ–¯æ•°æ®å®ç°äº†è¶…è¿‡100å€çš„å‹ç¼©ã€‚ä¸æœ€å…ˆè¿›çš„å‹ç¼©æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ä¿æŒç›¸å½“è´¨é‡çš„åŒæ—¶ï¼Œä»…ä½¿ç”¨å…¶6%çš„å­˜å‚¨ç©ºé—´ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„å‹ç¼©æ¡†æ¶ä»…éœ€è¦å…¶ç¼–ç æ—¶é—´çš„25%å’Œè§£ç æ—¶é—´çš„1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå‰é¦ˆçš„3Dé«˜æ–¯æ··åˆï¼ˆ3DGSï¼‰æŠ€æœ¯çš„æ–°å‘å±•ã€‚é€šè¿‡ä½¿ç”¨å¤§è§„æ¨¡å¤šè§†è§’æ•°æ®é›†è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œè¯¥æŠ€æœ¯å¯ä»¥ç›´æ¥ä»ç¨€ç–è¾“å…¥è§†è§’æ¨æ–­å‡º3DGSè¡¨ç¤ºã€‚ç„¶è€Œï¼Œå‰é¦ˆæ–¹æ³•è™½ç„¶å®ç°äº†è¾ƒé«˜çš„é‡å»ºé€Ÿåº¦ï¼Œä½†ä»ç„¶å­˜åœ¨å¤§é‡å­˜å‚¨æˆæœ¬çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•TinySplatï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€å±€é™æ€§ã€‚TinySplatæ˜¯åŸºäºæ ‡å‡†å‰é¦ˆ3DGSæ–¹æ³•çš„å‹ç¼©æ¡†æ¶ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°æ¶ˆé™¤å…³é”®å†—ä½™ä¿¡æ¯æ¥æºã€‚é€šè¿‡å¼•å…¥è§†å›¾æŠ•å½±å˜æ¢ï¼ˆVPTï¼‰å‡å°‘å‡ ä½•å†—ä½™ï¼Œé€šè¿‡å¯è§æ€§æ„ŸçŸ¥åŸºç¡€å‡å°‘ï¼ˆVABRï¼‰å‡è½»æ„ŸçŸ¥å†—ä½™ï¼Œå¹¶é€šè¿‡ç°æˆçš„è§†é¢‘ç¼–ç å™¨è§£å†³ç©ºé—´å†—ä½™é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTinySplatå¯¹å‰é¦ˆæ–¹æ³•ç”Ÿæˆçš„3Dé«˜æ–¯æ•°æ®å®ç°äº†è¶…è¿‡100å€çš„å‹ç¼©æ¯”ï¼Œç›¸è¾ƒäºæœ€æ–°çš„å‹ç¼©æ–¹æ³•è¾¾åˆ°äº†ç›¸å½“çš„å‹ç¼©æ•ˆæœï¼Œå¹¶ä¸”å¤§å¹…åº¦æå‡äº†ç¼–ç å’Œè§£ç æ—¶é—´æ•ˆç‡ã€‚æ€»çš„æ¥è¯´ï¼ŒTinySplatæä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€ç´§å‡‘çš„3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSæŠ€æœ¯å¯ä»¥ç›´æ¥ä»ç¨€ç–è§†è§’æ¨æ–­å‡ºä¸‰ç»´åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>å‰é¦ˆæ–¹æ³•è™½ç„¶é‡å»ºé€Ÿåº¦å¿«ï¼Œä½†å­˜å‚¨æˆæœ¬é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9ce4b9b5b1fd1767294f6dc5d0e69e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba92c06036a14f29f7d9006919d80e37.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cda0d1acab18f0e92b4d904b1e0079b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fac0541c33de3d6951d32dce227f2334.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5f9c06a2e5800bc0a2b2709d6a67e53.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UniForward-Unified-3D-Scene-and-Semantic-Field-Reconstruction-via-Feed-Forward-Gaussian-Splatting-from-Only-Sparse-View-Images"><a href="#UniForward-Unified-3D-Scene-and-Semantic-Field-Reconstruction-via-Feed-Forward-Gaussian-Splatting-from-Only-Sparse-View-Images" class="headerlink" title="UniForward: Unified 3D Scene and Semantic Field Reconstruction via   Feed-Forward Gaussian Splatting from Only Sparse-View Images"></a>UniForward: Unified 3D Scene and Semantic Field Reconstruction via   Feed-Forward Gaussian Splatting from Only Sparse-View Images</h2><p><strong>Authors:Qijian Tian, Xin Tan, Jingyu Gong, Yuan Xie, Lizhuang Ma</strong></p>
<p>We propose a feed-forward Gaussian Splatting model that unifies 3D scene and semantic field reconstruction. Combining 3D scenes with semantic fields facilitates the perception and understanding of the surrounding environment. However, key challenges include embedding semantics into 3D representations, achieving generalizable real-time reconstruction, and ensuring practical applicability by using only images as input without camera parameters or ground truth depth. To this end, we propose UniForward, a feed-forward model to predict 3D Gaussians with anisotropic semantic features from only uncalibrated and unposed sparse-view images. To enable the unified representation of the 3D scene and semantic field, we embed semantic features into 3D Gaussians and predict them through a dual-branch decoupled decoder. During training, we propose a loss-guided view sampler to sample views from easy to hard, eliminating the need for ground truth depth or masks required by previous methods and stabilizing the training process. The whole model can be trained end-to-end using a photometric loss and a distillation loss that leverages semantic features from a pre-trained 2D semantic model. At the inference stage, our UniForward can reconstruct 3D scenes and the corresponding semantic fields in real time from only sparse-view images. The reconstructed 3D scenes achieve high-quality rendering, and the reconstructed 3D semantic field enables the rendering of view-consistent semantic features from arbitrary views, which can be further decoded into dense segmentation masks in an open-vocabulary manner. Experiments on novel view synthesis and novel view segmentation demonstrate that our method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å‰é¦ˆé«˜æ–¯å¹³é“ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»Ÿä¸€äº†ä¸‰ç»´åœºæ™¯å’Œè¯­ä¹‰åœºçš„é‡å»ºã€‚å°†ä¸‰ç»´åœºæ™¯ä¸è¯­ä¹‰åœºç›¸ç»“åˆï¼Œæœ‰åŠ©äºæ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒã€‚ç„¶è€Œï¼Œå…³é”®æŒ‘æˆ˜åŒ…æ‹¬å°†è¯­ä¹‰åµŒå…¥åˆ°ä¸‰ç»´è¡¨ç¤ºä¸­ï¼Œå®ç°å¯æ³›åŒ–çš„å®æ—¶é‡å»ºï¼Œä»¥åŠç¡®ä¿ä»…ä½¿ç”¨å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè€Œä¸ä½¿ç”¨ç›¸æœºå‚æ•°æˆ–çœŸå®æ·±åº¦ä¿¡æ¯çš„å®é™…å¯è¡Œæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UniForwardï¼Œä¸€ç§å‰é¦ˆæ¨¡å‹ï¼Œä»…ä»æœªæ ¡å‡†ä¸”æœªå®šä½ç¨€ç–è§†è§’çš„å›¾åƒä¸­é¢„æµ‹å…·æœ‰å„å‘å¼‚æ€§è¯­ä¹‰ç‰¹å¾çš„ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚ä¸ºäº†ç»Ÿä¸€è¡¨ç¤ºä¸‰ç»´åœºæ™¯å’Œè¯­ä¹‰åœºï¼Œæˆ‘ä»¬å°†è¯­ä¹‰ç‰¹å¾åµŒå…¥åˆ°ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒä¸­ï¼Œå¹¶é€šè¿‡åŒåˆ†æ”¯è§£è€¦è§£ç å™¨è¿›è¡Œé¢„æµ‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŸå¤±å¼•å¯¼è§†å›¾é‡‡æ ·å™¨ï¼Œç”¨äºä»æ˜“åˆ°éš¾åœ°é‡‡æ ·è§†å›¾ï¼Œä»è€Œæ¶ˆé™¤äº†ä»¥å‰æ–¹æ³•æ‰€éœ€çš„çœŸå®æ·±åº¦æˆ–æ©ç çš„éœ€æ±‚ï¼Œå¹¶ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ã€‚æ•´ä¸ªæ¨¡å‹å¯ä»¥ä½¿ç”¨å…‰åº¦æŸå¤±å’Œè’¸é¦æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œè¯¥æŸå¤±åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´è¯­ä¹‰æ¨¡å‹çš„è¯­ä¹‰ç‰¹å¾ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬çš„UniForwardå¯ä»¥ä»…ä»ç¨€ç–è§†è§’å›¾åƒä¸­å®æ—¶é‡å»ºä¸‰ç»´åœºæ™¯å’Œç›¸åº”çš„è¯­ä¹‰åœºã€‚é‡å»ºçš„ä¸‰ç»´åœºæ™¯å®ç°äº†é«˜è´¨é‡æ¸²æŸ“ï¼Œé‡å»ºçš„ä¸‰ç»´è¯­ä¹‰åœºå¯ä»¥ä»ä»»æ„è§†è§’å‘ˆç°ä¸€è‡´çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ä¸”å¯ä»¥è¿›ä¸€æ­¥ä»¥å¼€æ”¾è¯æ±‡è¡¨çš„æ–¹å¼è§£ç ä¸ºå¯†é›†åˆ†å‰²æ©ç ã€‚åœ¨æ–°å‹è§†å›¾åˆæˆå’Œæ–°å‹è§†å›¾åˆ†å‰²æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»Ÿä¸€ä¸‰ç»´åœºæ™¯å’Œè¯­ä¹‰åœºé‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§å‰é¦ˆé«˜æ–¯ç‚¹äº‘æ¨¡å‹ï¼ˆUniForwardï¼‰ï¼Œè¯¥æ¨¡å‹å®ç°äº†ä¸‰ç»´åœºæ™¯å’Œè¯­ä¹‰åœºçš„ç»Ÿä¸€é‡å»ºï¼Œä»…ä½¿ç”¨æœªæ ¡å‡†ã€æœªç»å§¿æ€è®¾å®šçš„ç¨€ç–è§†è§’å›¾åƒä½œä¸ºè¾“å…¥ã€‚æ¨¡å‹é€šè¿‡å°†è¯­ä¹‰ç‰¹å¾åµŒå…¥ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒä¸­è¿›è¡Œé¢„æµ‹ï¼Œå®ç°äº†åœºæ™¯å’Œè¯­ä¹‰åœºçš„ç»Ÿä¸€è¡¨ç¤ºã€‚è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨æŸå¤±å¼•å¯¼è§†å›¾é‡‡æ ·å™¨ï¼Œå¹¶é‡‡ç”¨å…‰åº¦æŸå¤±å’Œè’¸é¦æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå®æ—¶ä»ç¨€ç–è§†è§’å›¾åƒé‡å»ºä¸‰ç»´åœºæ™¯å’Œå¯¹åº”çš„è¯­ä¹‰åœºï¼Œå®ç°é«˜è´¨é‡æ¸²æŸ“å’Œä»»æ„è§†è§’çš„è¯­ä¹‰ç‰¹å¾æ¸²æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å‰é¦ˆé«˜æ–¯ç‚¹äº‘æ¨¡å‹ï¼ˆUniForwardï¼‰ï¼Œå®ç°äº†ä¸‰ç»´åœºæ™¯å’Œè¯­ä¹‰åœºçš„ç»Ÿä¸€é‡å»ºã€‚</li>
<li>ä»…ä½¿ç”¨æœªæ ¡å‡†ã€æœªç»å§¿æ€è®¾å®šçš„ç¨€ç–è§†è§’å›¾åƒä½œä¸ºè¾“å…¥ã€‚</li>
<li>é€šè¿‡å°†è¯­ä¹‰ç‰¹å¾åµŒå…¥ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒè¿›è¡Œé¢„æµ‹ï¼Œå®ç°åœºæ™¯å’Œè¯­ä¹‰åœºçš„ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨æŸå¤±å¼•å¯¼è§†å›¾é‡‡æ ·å™¨ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæ— éœ€ä½¿ç”¨åœ°é¢çœŸå®æ·±åº¦æˆ–æ©ç ã€‚</li>
<li>ä½¿ç”¨å…‰åº¦æŸå¤±å’Œè’¸é¦æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚</li>
<li>èƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µå®æ—¶ä»ç¨€ç–è§†è§’å›¾åƒé‡å»ºä¸‰ç»´åœºæ™¯å’Œå¯¹åº”çš„è¯­ä¹‰åœºã€‚</li>
<li>å®ç°äº†é«˜è´¨é‡æ¸²æŸ“ï¼Œå¹¶èƒ½ä»ä»»æ„è§†è§’æ¸²æŸ“è¯­ä¹‰ç‰¹å¾ï¼Œè¿›ä¸€æ­¥è§£ç ä¸ºå¼€æ”¾è¯æ±‡è¡¨çš„å¯†é›†åˆ†å‰²æ©ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1312e6dccf74af64c4219bde80169922.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c3351832749d01451a7ac7002b57ed3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e8cfd35844d53f29835c35edeb65280.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StreamSplat-Towards-Online-Dynamic-3D-Reconstruction-from-Uncalibrated-Video-Streams"><a href="#StreamSplat-Towards-Online-Dynamic-3D-Reconstruction-from-Uncalibrated-Video-Streams" class="headerlink" title="StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated   Video Streams"></a>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated   Video Streams</h2><p><strong>Authors:Zike Wu, Qi Yan, Xuanyu Yi, Lele Wang, Renjie Liao</strong></p>
<p>Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/nickwzk/StreamSplat">https://github.com/nickwzk/StreamSplat</a>. </p>
<blockquote>
<p>å®æ—¶ä»æœªæ ¡å‡†çš„è§†é¢‘æµä¸­é‡å»ºåŠ¨æ€çš„3Dåœºæ™¯å¯¹äºä¼—å¤šç°å®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶è§£å†³ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰å®æ—¶å¤„ç†æœªæ ¡å‡†çš„è¾“å…¥ï¼Œ2ï¼‰å‡†ç¡®æ¨¡æ‹ŸåŠ¨æ€åœºæ™¯æ¼”å˜ï¼Œä»¥åŠ3ï¼‰ä¿æŒé•¿æœŸç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†StreamSplatï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å‰é¦ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»¥åœ¨çº¿æ–¹å¼å°†ä»»æ„é•¿åº¦çš„æœªæ ¡å‡†è§†é¢‘æµè½¬æ¢ä¸ºåŠ¨æ€çš„3Dé«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰è¡¨ç¤ºï¼Œèƒ½å¤Ÿä»æ—¶é—´å±€éƒ¨è§‚å¯Ÿä¸­æ¢å¤åœºæ™¯åŠ¨æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šé™æ€ç¼–ç å™¨ä¸­çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶ç”¨äº3DGSä½ç½®é¢„æµ‹ï¼Œä»¥åŠåŠ¨æ€è§£ç å™¨ä¸­çš„åŒå‘å˜å½¢åœºï¼Œèƒ½å¤Ÿå®ç°ç¨³å¥å’Œé«˜æ•ˆçš„åŠ¨æ€å»ºæ¨¡ã€‚åœ¨é™æ€å’ŒåŠ¨æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒStreamSplatåœ¨é‡å»ºè´¨é‡å’ŒåŠ¨æ€åœºæ™¯å»ºæ¨¡æ–¹é¢å‡å§‹ç»ˆä¼˜äºå…ˆå‰çš„å·¥ä½œï¼ŒåŒæ—¶ç‹¬ç‰¹åœ°æ”¯æŒä»»æ„é•¿åº¦è§†é¢‘æµçš„åœ¨çº¿é‡å»ºã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nickwzk/StreamSplat%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nickwzk/StreamSplatæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08862v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å®æ—¶ä»æœªæ ¡å‡†çš„è§†é¢‘æµé‡å»ºåŠ¨æ€ä¸‰ç»´åœºæ™¯å¯¹è®¸å¤šå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åŒæ—¶è§£å†³ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰å¤„ç†å®æ—¶æœªæ ¡å‡†è¾“å…¥ï¼›2ï¼‰å‡†ç¡®æ¨¡æ‹ŸåŠ¨æ€åœºæ™¯æ¼”å˜ï¼›ä»¥åŠ3ï¼‰ä¿æŒé•¿æœŸç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†StreamSplatï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„å‰é¦ˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®æ—¶å°†ä»»æ„é•¿åº¦çš„æœªæ ¡å‡†è§†é¢‘æµè½¬æ¢ä¸ºåŠ¨æ€ä¸‰ç»´é«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰è¡¨ç¤ºï¼Œä»æ—¶é—´ä¸Šå±€éƒ¨è§‚æµ‹æ¢å¤åœºæ™¯åŠ¨æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šé™æ€ç¼–ç å™¨ä¸­çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶ç”¨äºé¢„æµ‹3DGSä½ç½®ï¼Œä»¥åŠåŠ¨æ€è§£ç å™¨ä¸­çš„åŒå‘å˜å½¢åœºï¼Œå¯å®ç°ç¨³å¥é«˜æ•ˆçš„åŠ¨åŠ›å­¦å»ºæ¨¡ã€‚åœ¨é™æ€å’ŒåŠ¨æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒStreamSplatåœ¨é‡å»ºè´¨é‡å’ŒåŠ¨æ€åœºæ™¯å»ºæ¨¡æ–¹é¢å‡ä¼˜äºå…ˆå‰çš„å·¥ä½œï¼Œå¹¶ä¸”å”¯ä¸€æ”¯æŒä»»æ„é•¿è§†é¢‘æµçš„åœ¨çº¿é‡å»ºã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nickwzk/StreamSplat%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nickwzk/StreamSplatæ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>StreamSplatæ˜¯é¦–ä¸ªå…¨å‰é¦ˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»ä»»æ„é•¿åº¦çš„æœªæ ¡å‡†è§†é¢‘æµå®æ—¶é‡å»ºåŠ¨æ€ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>é€šè¿‡ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°è§£å†³åŠ¨æ€ä¸‰ç»´åœºæ™¯é‡å»ºçš„ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>é™æ€ç¼–ç å™¨ä¸­çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶ç”¨äºé¢„æµ‹3DGSä½ç½®ã€‚</li>
<li>åŠ¨æ€è§£ç å™¨ä¸­çš„åŒå‘å˜å½¢åœºå®ç°ç¨³å¥é«˜æ•ˆçš„åŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œé‡å»ºè´¨é‡å’ŒåŠ¨æ€åœºæ™¯å»ºæ¨¡å‡ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</li>
<li>æ”¯æŒä»»æ„é•¿è§†é¢‘æµçš„åœ¨çº¿é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-daf4d984c7f82ab7e4bc3937a8a40e9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58083c9b8634118e1ea86e39a6679388.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c65de4e3d0809c69066b701e671832a7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TraGraph-GS-Trajectory-Graph-based-Gaussian-Splatting-for-Arbitrary-Large-Scale-Scene-Rendering"><a href="#TraGraph-GS-Trajectory-Graph-based-Gaussian-Splatting-for-Arbitrary-Large-Scale-Scene-Rendering" class="headerlink" title="TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary   Large-Scale Scene Rendering"></a>TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary   Large-Scale Scene Rendering</h2><p><strong>Authors:Xiaohan Zhang, Sitong Wang, Yushen Yan, Yi Yang, Mingda Xu, Qi Liu</strong></p>
<p>High-quality novel view synthesis for large-scale scenes presents a challenging dilemma in 3D computer vision. Existing methods typically partition large scenes into multiple regions, reconstruct a 3D representation using Gaussian splatting for each region, and eventually merge them for novel view rendering. They can accurately render specific scenes, yet they do not generalize effectively for two reasons: (1) rigid spatial partition techniques struggle with arbitrary camera trajectories, and (2) the merging of regions results in Gaussian overlap to distort texture details. To address these challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable high-precision rendering for arbitrarily large-scale scenes. We present a spatial partitioning method for large-scale scenes based on graphs, which incorporates a regularization constraint to enhance the rendering of textures and distant objects, as well as a progressive rendering strategy to mitigate artifacts caused by Gaussian overlap. Experimental results demonstrate its superior performance both on four aerial and four ground datasets and highlight its remarkable efficiency: our method achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches. </p>
<blockquote>
<p>é«˜è´¨é‡çš„å¤§è§„æ¨¡åœºæ™¯æ–°è§†å›¾åˆæˆæ˜¯ä¸‰ç»´è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†å¤§è§„æ¨¡åœºæ™¯åˆ†å‰²æˆå¤šä¸ªåŒºåŸŸï¼Œä½¿ç”¨é«˜æ–¯é£æº…æŠ€æœ¯ä¸ºæ¯ä¸ªåŒºåŸŸé‡å»ºä¸‰ç»´è¡¨ç¤ºï¼Œå¹¶æœ€ç»ˆå°†å®ƒä»¬åˆå¹¶ä»¥å‘ˆç°æ–°è§†å›¾ã€‚å®ƒä»¬å¯ä»¥å‡†ç¡®åœ°å‘ˆç°ç‰¹å®šåœºæ™¯ï¼Œä½†ç”±äºä¸¤ä¸ªåŸå› ï¼Œå®ƒä»¬å¹¶ä¸èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿ï¼š ï¼ˆ1ï¼‰åˆšæ€§çš„ç©ºé—´åˆ†å‰²æŠ€æœ¯éš¾ä»¥å¤„ç†ä»»æ„çš„ç›¸æœºè½¨è¿¹ï¼›ï¼ˆ2ï¼‰åŒºåŸŸçš„åˆå¹¶å¯¼è‡´é«˜æ–¯é‡å ï¼Œä»è€Œæ‰­æ›²çº¹ç†ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºTraGraph-GSï¼Œåˆ©ç”¨è½¨è¿¹å›¾å®ç°ä»»æ„å¤§è§„æ¨¡åœºæ™¯çš„é«˜ç²¾åº¦æ¸²æŸ“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„å¤§å‹åœºæ™¯ç©ºé—´åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨æ­£åˆ™åŒ–çº¦æŸä»¥å¢å¼ºçº¹ç†å’Œè¿œè·ç¦»ç‰©ä½“çš„æ¸²æŸ“ï¼Œä»¥åŠé‡‡ç”¨æ¸è¿›æ¸²æŸ“ç­–ç•¥æ¥ç¼“è§£å› é«˜æ–¯é‡å è€Œäº§ç”Ÿçš„ä¼ªå½±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªèˆªç©ºå’Œå››ä¸ªåœ°é¢æ•°æ®é›†ä¸Šçš„æ€§èƒ½å“è¶Šï¼Œå¹¶ä¸”æ•ˆç‡æ˜¾è‘—ï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨èˆªç©ºæ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†1.86 dBçš„PSNRï¼Œåœ¨åœ°é¢æ•°æ®é›†ä¸Šå¹³å‡æé«˜äº†1.62 dBï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08704v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯çš„é«˜è´¨é‡æ–°å‹è§†å›¾åˆæˆåœ¨3Dè®¡ç®—æœºè§†è§‰ä¸­çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†å¤§å‹åœºæ™¯åˆ†å‰²æˆå¤šä¸ªåŒºåŸŸï¼Œä¸ºæ¯ä¸ªåŒºåŸŸä½¿ç”¨é«˜æ–¯è´´å›¾æ„å»º3Dè¡¨ç¤ºï¼Œå¹¶æœ€ç»ˆåˆå¹¶å®ƒä»¬ä»¥è¿›è¡Œæ–°å‹è§†å›¾æ¸²æŸ“ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯åˆšæ€§ç©ºé—´åˆ†å‰²æŠ€æœ¯éš¾ä»¥å¤„ç†ä»»æ„çš„ç›¸æœºè½¨è¿¹ï¼ŒäºŒæ˜¯åŒºåŸŸåˆå¹¶å¯¼è‡´é«˜æ–¯é‡å ï¼Œä»è€Œæ‰­æ›²çº¹ç†ç»†èŠ‚ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TraGraph-GSæ–¹æ³•ï¼Œåˆ©ç”¨è½¨è¿¹å›¾å®ç°å¤§è§„æ¨¡åœºæ™¯çš„é«˜ç²¾åº¦æ¸²æŸ“ã€‚è¯¥æ–¹æ³•åŸºäºå›¾è¿›è¡Œç©ºé—´åˆ†å‰²ï¼Œå¹¶å¼•å…¥æ­£åˆ™åŒ–çº¦æŸä»¥æé«˜çº¹ç†å’Œè¿œè·ç¦»ç‰©ä½“çš„æ¸²æŸ“æ•ˆæœï¼ŒåŒæ—¶é‡‡ç”¨æ¸è¿›æ¸²æŸ“ç­–ç•¥æ¥å‡è½»é«˜æ–¯é‡å å¼•èµ·çš„ä¼ªå½±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ç»„èˆªç©ºå’Œå››ç»„åœ°é¢æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨æ•ˆç‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡çš„æ–°å‹è§†å›¾åˆæˆå¯¹äºå¤§è§„æ¨¡åœºæ™¯åœ¨3Dè®¡ç®—æœºè§†è§‰ä¸­æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡åˆ†å‰²åœºæ™¯ã€æ„å»º3Dè¡¨ç¤ºå’Œåˆå¹¶åŒºåŸŸæ¥è¿›è¡Œæ¸²æŸ“ï¼Œä½†å­˜åœ¨å¤„ç†ä»»æ„ç›¸æœºè½¨è¿¹å’Œçº¹ç†ç»†èŠ‚å¤±çœŸé—®é¢˜ã€‚</li>
<li>TraGraph-GSæ–¹æ³•åˆ©ç”¨è½¨è¿¹å›¾å®ç°å¤§è§„æ¨¡åœºæ™¯çš„é«˜ç²¾åº¦æ¸²æŸ“ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºå›¾è¿›è¡Œç©ºé—´åˆ†å‰²ï¼Œå¹¶å¼•å…¥æ­£åˆ™åŒ–çº¦æŸæé«˜çº¹ç†å’Œè¿œè·ç¦»ç‰©ä½“çš„æ¸²æŸ“ã€‚</li>
<li>æ¸è¿›æ¸²æŸ“ç­–ç•¥ç”¨äºå‡è½»é«˜æ–¯é‡å å¼•èµ·çš„ä¼ªå½±ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTraGraph-GSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•å¹³å‡æé«˜äº†PSNRå€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ecfca4c4b61cba8a5c5d6a417417f08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-381ae59900e4c4cc76ab58bdbbce0ae0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbf1144ef346a7b759e7e983371d9bf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ab245a09915fe09f56a32120e9ec707.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6498732edf22ed78ec4acc71e012b8df.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GigaSLAM-Large-Scale-Monocular-SLAM-with-Hierarchical-Gaussian-Splats"><a href="#GigaSLAM-Large-Scale-Monocular-SLAM-with-Hierarchical-Gaussian-Splats" class="headerlink" title="GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats"></a>GigaSLAM: Large-Scale Monocular SLAM with Hierarchical Gaussian Splats</h2><p><strong>Authors:Kai Deng, Yigong Zhang, Jian Yang, Jin Xie</strong></p>
<p>Tracking and mapping in large-scale, unbounded outdoor environments using only monocular RGB input presents substantial challenges for existing SLAM systems. Traditional Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) SLAM methods are typically limited to small, bounded indoor settings. To overcome these challenges, we introduce GigaSLAM, the first RGB NeRF &#x2F; 3DGS-based SLAM framework for kilometer-scale outdoor environments, as demonstrated on the KITTI, KITTI 360, 4 Seasons and A2D2 datasets. Our approach employs a hierarchical sparse voxel map representation, where Gaussians are decoded by neural networks at multiple levels of detail. This design enables efficient, scalable mapping and high-fidelity viewpoint rendering across expansive, unbounded scenes. For front-end tracking, GigaSLAM utilizes a metric depth model combined with epipolar geometry and PnP algorithms to accurately estimate poses, while incorporating a Bag-of-Words-based loop closure mechanism to maintain robust alignment over long trajectories. Consequently, GigaSLAM delivers high-precision tracking and visually faithful rendering on urban outdoor benchmarks, establishing a robust SLAM solution for large-scale, long-term scenarios, and significantly extending the applicability of Gaussian Splatting SLAM systems to unbounded outdoor environments. GitHub: <a target="_blank" rel="noopener" href="https://github.com/DengKaiCQ/GigaSLAM">https://github.com/DengKaiCQ/GigaSLAM</a>. </p>
<blockquote>
<p>ä»…ä½¿ç”¨å•ç›®RGBè¾“å…¥åœ¨å¤§è§„æ¨¡ã€æ— è¾¹ç•Œçš„å®¤å¤–ç¯å¢ƒä¸­è¿›è¡Œè¿½è¸ªå’Œæ˜ å°„ï¼Œç»™ç°æœ‰çš„SLAMç³»ç»Ÿå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯å–·ç»˜ï¼ˆ3DGSï¼‰SLAMæ–¹æ³•é€šå¸¸å±€é™äºå°è§„æ¨¡çš„å®¤å†…ç¯å¢ƒã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GigaSLAMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºRGB NeRF&#x2F;3DGSçš„SLAMæ¡†æ¶ï¼Œé€‚ç”¨äºå…¬é‡Œçº§å®¤å¤–ç¯å¢ƒï¼Œå¦‚åœ¨KITTIã€KITTI 360ã€å››å­£å’ŒA2D2æ•°æ®é›†ä¸Šæ‰€ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åˆ†å±‚ç¨€ç–ä½“ç´ å›¾è¡¨ç¤ºï¼Œå…¶ä¸­é«˜æ–¯æ•°æ®é€šè¿‡å¤šçº§ç»†èŠ‚ç¥ç»ç½‘ç»œè¿›è¡Œè§£ç ã€‚è¿™ç§è®¾è®¡å®ç°äº†é«˜æ•ˆã€å¯æ‰©å±•çš„æ˜ å°„å’Œé«˜ä¿çœŸè§†è§’æ¸²æŸ“ï¼Œé€‚ç”¨äºå¹¿é˜”çš„æ— ç•Œåœºæ™¯ã€‚å¯¹äºå‰ç«¯è·Ÿè¸ªï¼ŒGigaSLAMç»“åˆåº¦é‡æ·±åº¦æ¨¡å‹ã€æçº¿å‡ ä½•å’ŒPnPç®—æ³•æ¥å‡†ç¡®ä¼°è®¡å§¿æ€ï¼ŒåŒæ—¶é‡‡ç”¨åŸºäºè¯è¢‹çš„ç¯è·¯é—­åˆæœºåˆ¶ï¼Œä»¥åœ¨é•¿æœŸè½¨è¿¹ä¸­ä¿æŒç¨³å¥çš„å¯¹é½ã€‚å› æ­¤ï¼ŒGigaSLAMåœ¨åŸå¸‚å®¤å¤–åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜ç²¾åº¦è¿½è¸ªå’Œé«˜ä¿çœŸæ¸²æŸ“ï¼Œä¸ºå¤§è§„æ¨¡ã€é•¿æœŸåœºæ™¯å»ºç«‹äº†ç¨³å¥çš„SLAMè§£å†³æ–¹æ¡ˆï¼Œå¹¶å°†é«˜æ–¯å–·ç»˜SLAMç³»ç»Ÿçš„åº”ç”¨èŒƒå›´å¤§å¤§æ‰©å±•åˆ°äº†æ— ç•Œå®¤å¤–ç¯å¢ƒã€‚GitHubï¼š<a target="_blank" rel="noopener" href="https://github.com/DengKaiCQ/GigaSLAM%E3%80%82">https://github.com/DengKaiCQ/GigaSLAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08071v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤§èŒƒå›´æˆ·å¤–ç¯å¢ƒçš„RGB NeRF&#x2F; 3DGS-åŸºäºSLAMæ¡†æ¶çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œå…¶ç»“åˆäº†å¤šç§æŠ€æœ¯çš„ä¼˜ç‚¹ä»¥å®ç°å¤§èŒƒå›´ç¯å¢ƒä¸­çš„åœ°å›¾åˆ›å»ºä¸è·¯å¾„è·Ÿè¸ªã€‚ä½œè€…é‡‡ç”¨ç¥ç»ç½‘ç»œå’Œå±‚æ¬¡åŒ–çš„ç¨€ç–ä½“ç´ å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œé€šè¿‡ä¸åŒå±‚æ¬¡çš„ç»†èŠ‚è§£ç é«˜æ–¯ä¿¡æ¯ï¼Œå®ç°äº†é«˜æ•ˆã€å¯ä¼¸ç¼©çš„æ˜ å°„å’Œé«˜ä¿çœŸåº¦çš„è§†ç‚¹æ¸²æŸ“ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ·±åº¦æ¨¡å‹ã€æå‡ ä½•å’ŒPnPç®—æ³•è¿›è¡Œå‰ç«¯è·Ÿè¸ªï¼Œå¹¶åˆ©ç”¨åŸºäºBag-of-Wordsçš„é—­ç¯æœºåˆ¶ç»´æŒé•¿æœŸè½¨è¿¹çš„ç¨³å¥å¯¹é½ã€‚å› æ­¤ï¼ŒGigaSLAMä¸ºå¤§è§„æ¨¡æˆ·å¤–ç¯å¢ƒæä¾›äº†ç¨³å¥çš„SLAMè§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æ‰©å±•äº†é«˜æ–¯Splatting SLAMç³»ç»Ÿçš„åº”ç”¨èŒƒå›´ã€‚å…¶GitHubé“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/DengKaiCQ/GigaSLAM">é“¾æ¥åœ°å€</a>ã€‚æ•´ä½“æ¥è¯´ï¼Œè¯¥ç ”ç©¶åœ¨å¤§è§„æ¨¡å®¤å¤–ç¯å¢ƒå®šä½ä¸åœ°å›¾æ„å»ºæ–¹é¢å®ç°äº†æ˜¾è‘—çš„çªç ´å’Œè¿›å±•ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GigaSLAMæ˜¯é¦–ä¸ªé’ˆå¯¹å¤§èŒƒå›´æˆ·å¤–ç¯å¢ƒçš„RGB NeRF &#x2F; 3DGS-åŸºäºSLAMæ¡†æ¶ã€‚é¦–æ¬¡åœ¨å¤§å‹å®¤å¤–åœºæ™¯å¦‚KITTIã€KITTI 360ã€å››å­£å’ŒA2D2æ•°æ®é›†ä¸Šå±•ç¤ºäº†å…¶æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨å±‚æ¬¡åŒ–çš„ç¨€ç–ä½“ç´ å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œç»“åˆç¥ç»ç½‘ç»œè§£ç é«˜æ–¯ä¿¡æ¯ï¼Œå®ç°é«˜æ•ˆæ˜ å°„å’Œé«˜ä¿çœŸåº¦æ¸²æŸ“ã€‚</li>
<li>å‰ç«¯è·Ÿè¸ªç»“åˆäº†æ·±åº¦æ¨¡å‹ã€æå‡ ä½•å’ŒPnPç®—æ³•ï¼Œæé«˜äº†å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨åŸºäºBag-of-Wordsçš„é—­ç¯æœºåˆ¶æ¥ä¿æŒé•¿æœŸè½¨è¿¹çš„ç¨³å¥å¯¹é½ã€‚</li>
<li>GigaSLAMåœ¨é«˜ç²¾åº¦è·Ÿè¸ªå’Œè§†è§‰çœŸå®æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºå¤§è§„æ¨¡ã€é•¿æœŸåœºæ™¯æä¾›äº†ç¨³å¥çš„SLAMè§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æŠ€æœ¯æ˜¾è‘—æ‰©å±•äº†é«˜æ–¯Splatting SLAMç³»ç»Ÿçš„åº”ç”¨èŒƒå›´ï¼Œä½¿å…¶é€‚ç”¨äºå¤§èŒƒå›´æˆ·å¤–ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a5293bf51a0022ae0d13a840ac71619.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-486db14fe17d6c3d439f7935b19a7b04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f7528469ad8852bb9c51c5870c6431a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="4D-Radar-Inertial-Odometry-based-on-Gaussian-Modeling-and-Multi-Hypothesis-Scan-Matching"><a href="#4D-Radar-Inertial-Odometry-based-on-Gaussian-Modeling-and-Multi-Hypothesis-Scan-Matching" class="headerlink" title="4D Radar-Inertial Odometry based on Gaussian Modeling and   Multi-Hypothesis Scan Matching"></a>4D Radar-Inertial Odometry based on Gaussian Modeling and   Multi-Hypothesis Scan Matching</h2><p><strong>Authors:Fernando Amodeo, Luis Merino, Fernando Caballero</strong></p>
<p>4D millimeter-wave (mmWave) radars are sensors that provide robustness against adverse weather conditions (rain, snow, fog, etc.), and as such they are increasingly being used for odometry and SLAM applications. However, the noisy and sparse nature of the returned scan data proves to be a challenging obstacle for existing point cloud matching based solutions, especially those originally intended for more accurate sensors such as LiDAR. Inspired by visual odometry research around 3D Gaussian Splatting, in this paper we propose using freely positioned 3D Gaussians to create a summarized representation of a radar point cloud tolerant to sensor noise, and subsequently leverage its inherent probability distribution function for registration (similar to NDT). Moreover, we propose simultaneously optimizing multiple scan matching hypotheses in order to further increase the robustness of the system against local optima of the function. Finally, we fuse our Gaussian modeling and scan matching algorithms into an EKF radar-inertial odometry system designed after current best practices. Experiments using publicly available 4D radar datasets show that our Gaussian-based odometry is comparable to existing registration algorithms, outperforming them in several sequences. </p>
<blockquote>
<p>å››ç»´æ¯«ç±³æ³¢é›·è¾¾å¯¹æ¶åŠ£å¤©æ°”ï¼ˆå¦‚é›¨ã€é›ªã€é›¾ç­‰ï¼‰å…·æœ‰ç¨³å¥æ€§ï¼Œå› æ­¤è¶Šæ¥è¶Šå¤šåœ°ç”¨äºæµ‹è·å’ŒSLAMåº”ç”¨ã€‚ç„¶è€Œï¼Œè¿”å›æ‰«ææ•°æ®å…·æœ‰å™ªå£°å¤§å’Œç¨€ç–æ€§ç‰¹ç‚¹ï¼Œè¯æ˜å¯¹äºç°æœ‰çš„åŸºäºç‚¹äº‘åŒ¹é…çš„è§£å†³æ–¹æ¡ˆæ„æˆäº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é‚£äº›åŸæœ¬ä¸ºæ¿€å…‰é›·è¾¾ç­‰æ›´ç²¾ç¡®ä¼ æ„Ÿå™¨è®¾è®¡çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å—å›´ç»•ä¸‰ç»´é«˜æ–¯æ‰©æ•£çš„è§†è§‰æµ‹è·ç ”ç©¶å¯å‘ï¼Œæå‡ºä½¿ç”¨è‡ªç”±å®šä½çš„3Dé«˜æ–¯æ¥åˆ›å»ºè€å—ä¼ æ„Ÿå™¨å™ªå£°çš„é›·è¾¾ç‚¹äº‘æ‘˜è¦è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å…¶å›ºæœ‰çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œæ³¨å†Œï¼ˆç±»ä¼¼äºNDTï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåŒæ—¶ä¼˜åŒ–å¤šä¸ªæ‰«æåŒ¹é…å‡è®¾ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç³»ç»Ÿå¯¹å‡½æ•°å±€éƒ¨æœ€ä¼˜è§£çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é«˜æ–¯å»ºæ¨¡å’Œæ‰«æåŒ¹é…ç®—æ³•èåˆåˆ°æ ¹æ®å½“å‰æœ€ä½³å®è·µè®¾è®¡çš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨é›·è¾¾æƒ¯æ€§æµ‹è·ç³»ç»Ÿä¸­ã€‚ä½¿ç”¨å…¬å¼€å¯ç”¨çš„å››ç»´é›·è¾¾æ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºé«˜æ–¯æµ‹è·ä¸ç°æœ‰æ³¨å†Œç®—æ³•ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªåºåˆ—ä¸­è¡¨ç°æ›´ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13639v2">PDF</a> Our code and results can be publicly accessed at:   <a target="_blank" rel="noopener" href="https://github.com/robotics-upo/gaussian-rio-cpp">https://github.com/robotics-upo/gaussian-rio-cpp</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºåˆ©ç”¨è‡ªç”±å®šä½çš„3Dé«˜æ–¯æ¥åˆ›å»ºé›·è¾¾ç‚¹äº‘çš„ç®€åŒ–è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºå¯¹ä¼ æ„Ÿå™¨å™ªå£°å…·æœ‰å®¹å¿æ€§ï¼Œå¹¶åˆ©ç”¨å…¶å†…åœ¨çš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œæ³¨å†Œã€‚é€šè¿‡ä¼˜åŒ–å¤šä¸ªæ‰«æåŒ¹é…å‡è®¾ï¼Œæé«˜äº†ç³»ç»Ÿå¯¹å‡½æ•°å±€éƒ¨æœ€ä¼˜è§£çš„é²æ£’æ€§ã€‚æœ€åï¼Œå°†é«˜æ–¯å»ºæ¨¡å’Œæ‰«æåŒ¹é…ç®—æ³•èåˆåˆ°åŸºäºæ‰©å±•å¡å°”æ›¼æ»¤æ³¢çš„é›·è¾¾æƒ¯æ€§é‡Œç¨‹è®¡ç³»ç»Ÿä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºé«˜æ–¯çš„æ–¹æ³•ä¸ç°æœ‰æ³¨å†Œç®—æ³•ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªåºåˆ—ä¸Šè¡¨ç°æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4Dæ¯«ç±³æ³¢é›·è¾¾åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹å…·æœ‰ç¨³å¥æ€§ï¼Œå¹¶è¶Šæ¥è¶Šå¤šåœ°ç”¨äºæµ‹è·å’ŒSLAMåº”ç”¨ã€‚</li>
<li>é›·è¾¾è¿”å›çš„æ‰«ææ•°æ®å…·æœ‰å™ªå£°å’Œç¨€ç–æ€§ï¼Œç»™ç°æœ‰çš„ç‚¹äº‘åŒ¹é…è§£å†³æ–¹æ¡ˆå¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯åˆ©ç”¨3Dé«˜æ–¯åˆ›å»ºé›·è¾¾ç‚¹äº‘çš„ç®€åŒ–è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•å¯¹ä¼ æ„Ÿå™¨å™ªå£°å…·æœ‰å®¹å¿æ€§ã€‚</li>
<li>ä½¿ç”¨é«˜æ–¯æ¦‚ç‡åˆ†å¸ƒå‡½æ•°è¿›è¡Œæ³¨å†Œï¼Œç±»ä¼¼äºNDTï¼ˆç½‘æ ¼åœ°å›¾é…å‡†æŠ€æœ¯ï¼‰ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–å¤šä¸ªæ‰«æåŒ¹é…å‡è®¾ï¼Œæé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§ï¼Œä»¥åº”å¯¹å‡½æ•°å±€éƒ¨æœ€ä¼˜è§£çš„é—®é¢˜ã€‚</li>
<li>å°†é«˜æ–¯å»ºæ¨¡å’Œæ‰«æåŒ¹é…ç®—æ³•èåˆåˆ°åŸºäºæ‰©å±•å¡å°”æ›¼æ»¤æ³¢çš„é›·è¾¾æƒ¯æ€§é‡Œç¨‹è®¡ç³»ç»Ÿä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae6925ba7b475d1c21cb4111a4032281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b45933110969203bcb92549518bfab09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f35aab572b6efdfeb04bf116d9fa414.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24108851801b1448de6f01710b58b284.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f3997ed2612c2adf6014480f6e2173b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b74b7a5ababb410ffc9c506f00046762.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Video2BEV-Transforming-Drone-Videos-to-BEVs-for-Video-based-Geo-localization"><a href="#Video2BEV-Transforming-Drone-Videos-to-BEVs-for-Video-based-Geo-localization" class="headerlink" title="Video2BEV: Transforming Drone Videos to BEVs for Video-based   Geo-localization"></a>Video2BEV: Transforming Drone Videos to BEVs for Video-based   Geo-localization</h2><p><strong>Authors:Hao Ju, Shaofei Huang, Si Liu, Zhedong Zheng</strong></p>
<p>Existing approaches to drone visual geo-localization predominantly adopt the image-based setting, where a single drone-view snapshot is matched with images from other platforms. Such task formulation, however, underutilizes the inherent video output of the drone and is sensitive to occlusions and viewpoint disparity. To address these limitations, we formulate a new video-based drone geo-localization task and propose the Video2BEV paradigm. This paradigm transforms the video into a Birdâ€™s Eye View (BEV), simplifying the subsequent \textbf{inter-platform} matching process. In particular, we employ Gaussian Splatting to reconstruct a 3D scene and obtain the BEV projection. Different from the existing transform methods, \eg, polar transform, our BEVs preserve more fine-grained details without significant distortion. To facilitate the discriminative \textbf{intra-platform} representation learning, our Video2BEV paradigm also incorporates a diffusion-based module for generating hard negative samples. To validate our approach, we introduce UniV, a new video-based geo-localization dataset that extends the image-based University-1652 dataset. UniV features flight paths at $30^\circ$ and $45^\circ$ elevation angles with increased frame rates of up to 10 frames per second (FPS). Extensive experiments on the UniV dataset show that our Video2BEV paradigm achieves competitive recall rates and outperforms conventional video-based methods. Compared to other competitive methods, our proposed approach exhibits robustness at lower elevations with more occlusions. </p>
<blockquote>
<p>ç°æœ‰çš„æ— äººæœºè§†è§‰åœ°ç†å®šä½æ–¹æ³•ä¸»è¦é‡‡ç”¨åŸºäºå›¾åƒçš„è®¾ç½®ï¼Œå…¶ä¸­å•ä¸ªæ— äººæœºè§†è§’çš„å¿«ç…§ä¸å…¶ä»–å¹³å°çš„å›¾åƒè¿›è¡ŒåŒ¹é…ã€‚ç„¶è€Œï¼Œè¿™ç§ä»»åŠ¡åˆ¶å®šæ–¹å¼æ²¡æœ‰å……åˆ†åˆ©ç”¨æ— äººæœºå›ºæœ‰çš„è§†é¢‘è¾“å‡ºï¼Œå¹¶ä¸”å¯¹é®æŒ¡å’Œè§†ç‚¹å·®å¼‚å¾ˆæ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬åˆ¶å®šäº†åŸºäºè§†é¢‘çš„æ–°å‹æ— äººæœºåœ°ç†å®šä½ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†Video2BEVèŒƒå¼ã€‚è¯¥èŒƒå¼å°†è§†é¢‘è½¬æ¢ä¸ºé¸Ÿç°å›¾ï¼ˆBEVï¼‰ï¼Œç®€åŒ–äº†éšåçš„è·¨å¹³å°åŒ¹é…è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨é«˜æ–¯æ‹¼æ¥æŠ€æœ¯é‡å»ºäº†ä¸‰ç»´åœºæ™¯å¹¶è·å¾—äº†BEVæŠ•å½±ã€‚ä¸ç°æœ‰çš„è½¬æ¢æ–¹æ³•ï¼ˆä¾‹å¦‚æåæ ‡å˜æ¢ï¼‰ä¸åŒï¼Œæˆ‘ä»¬çš„BEVåœ¨ä¿ç•™æ›´å¤šç²¾ç»†ç»†èŠ‚çš„åŒæ—¶ï¼Œé¿å…äº†æ˜¾è‘—çš„å¤±çœŸã€‚ä¸ºäº†ä¿ƒè¿›å¹³å°å†…éƒ¨åˆ¤åˆ«è¡¨ç¤ºçš„å­¦ä¹ ï¼Œæˆ‘ä»¬çš„Video2BEVèŒƒå¼è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¨¡å—æ¥ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniVï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†é¢‘çš„æ–°å‹åœ°ç†å®šä½æ•°æ®é›†ï¼Œæ‰©å±•äº†åŸºäºå›¾åƒçš„University-1652æ•°æ®é›†ã€‚UniVä»¥$30^\circ$å’Œ$45^\circ$çš„é£è¡Œè·¯å¾„ä¸ºç‰¹è‰²ï¼Œå¸§ç‡æœ€é«˜å¯è¾¾æ¯ç§’10å¸§ï¼ˆFPSï¼‰ã€‚åœ¨UniVæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Video2BEVèŒƒå¼å…·æœ‰ç«äº‰åŠ›çš„å¬å›ç‡ï¼Œå¹¶ä¸”ä¼˜äºä¼ ç»Ÿçš„è§†é¢‘æ–¹æ³•ã€‚ä¸å…¶ä»–æœ‰ç«äº‰åŠ›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨ä½æµ·æ‹”ã€æœ‰æ›´å¤šé®æŒ¡çš„æƒ…å†µä¸‹è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13610v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶æ— äººæœºè§†è§‰åœ°ç†å®šä½æ–¹æ³•ï¼Œé’ˆå¯¹ç°æœ‰å›¾åƒå®šä½æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†åŸºäºè§†é¢‘çš„æ— äººæœºåœ°ç†å®šä½ä»»åŠ¡å’Œè§†é¢‘è½¬é¸Ÿç°è§†å›¾ï¼ˆVideo2BEVï¼‰çš„æ¨¡å¼ã€‚è¯¥æ¨¡å¼é€šè¿‡é«˜æ–¯æ‘Šæ¶‚æŠ€æœ¯é‡å»º3Dåœºæ™¯ï¼Œè·å¾—é¸Ÿç°è§†å›¾ï¼Œç®€åŒ–äº†è·¨å¹³å°åŒ¹é…è¿‡ç¨‹ã€‚ä¸ç°æœ‰è½¬æ¢æ–¹æ³•ç›¸æ¯”ï¼Œå…¶é¸Ÿç°å›¾ä¿ç•™äº†æ›´å¤šçš„ç»†èŠ‚ä¸”ä¸å¤±çœŸã€‚ä¸ºæå‡å¹³å°å†…ç‰¹å¾å­¦ä¹ çš„è¾¨åˆ«åŠ›ï¼ŒVideo2BEVæ¨¡å¼è¿˜ç»“åˆäº†åŸºäºæ‰©æ•£çš„æ¨¡å—ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬ã€‚ä¸ºéªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ï¼Œç ”ç©¶å¼•å…¥äº†æ–°çš„è§†é¢‘åœ°ç†å®šä½æ•°æ®é›†UniVï¼Œå…¶åœ¨å›¾åƒæ•°æ®é›†University-1652åŸºç¡€ä¸Šæ‰©å±•ï¼ŒåŒ…å«ä¸åŒé£è¡Œé«˜åº¦å’Œå¸§ç‡çš„è§†é¢‘è·¯å¾„ã€‚å®éªŒè¡¨æ˜ï¼ŒVideo2BEVæ¨¡å¼å…·æœ‰ç«äº‰åŠ›çš„å¬å›ç‡ï¼Œä¸”åœ¨ä½æµ·æ‹”å’Œé®æŒ¡æ¡ä»¶ä¸‹è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰æ— äººæœºè§†è§‰åœ°ç†å®šä½æ–¹æ³•ä¸»è¦åŸºäºå›¾åƒåŒ¹é…ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨æ— äººæœºçš„è§†é¢‘è¾“å‡ºï¼Œä¸”æ˜“å—é®æŒ¡å’Œè§†è§’å·®å¼‚çš„å½±å“ã€‚</li>
<li>æå‡ºäº†Video2BEVæ¨¡å¼ï¼Œå°†è§†é¢‘è½¬æ¢ä¸ºé¸Ÿç°å›¾ï¼ˆBEVï¼‰ï¼Œç®€åŒ–äº†è·¨å¹³å°åŒ¹é…è¿‡ç¨‹ã€‚</li>
<li>é‡‡ç”¨é«˜æ–¯æ‘Šæ¶‚æŠ€æœ¯é‡å»º3Dåœºæ™¯å¹¶è·å¾—BEVæŠ•å½±ï¼Œç›¸æ¯”å…¶ä»–è½¬æ¢æ–¹æ³•ï¼Œå…¶ä¿ç•™äº†æ›´å¤šç»†èŠ‚ä¸”é¿å…å¤±çœŸã€‚</li>
<li>Video2BEVæ¨¡å¼ç»“åˆäº†åŸºäºæ‰©æ•£çš„æ¨¡å—ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬ï¼Œæé«˜äº†å¹³å°å†…ç‰¹å¾å­¦ä¹ çš„è¾¨åˆ«åŠ›ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„è§†é¢‘åœ°ç†å®šä½æ•°æ®é›†UniVï¼Œæ‰©å±•äº†åŸºäºå›¾åƒçš„æ•°æ®é›†University-1652ï¼ŒåŒ…å«ä¸åŒé£è¡Œé«˜åº¦å’Œå¸§ç‡çš„è§†é¢‘è·¯å¾„ã€‚</li>
<li>å®éªŒè¡¨æ˜Video2BEVæ¨¡å¼å…·æœ‰ç«äº‰åŠ›çš„å¬å›ç‡ï¼Œä¸”åœ¨ä½æµ·æ‹”å’Œé®æŒ¡æ¡ä»¶ä¸‹è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-916ce284b58f058d94743c2fef5ca90b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55d00f90b166c7e51616780636213868.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93cf78650a4372eb1f0876907ae80424.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c11b4217c15063674a827c0dab2165f4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FLoD-Integrating-Flexible-Level-of-Detail-into-3D-Gaussian-Splatting-for-Customizable-Rendering"><a href="#FLoD-Integrating-Flexible-Level-of-Detail-into-3D-Gaussian-Splatting-for-Customizable-Rendering" class="headerlink" title="FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting   for Customizable Rendering"></a>FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting   for Customizable Rendering</h2><p><strong>Authors:Yunji Seo, Young Sun Choi, Hyun Seung Son, Youngjung Uh</strong></p>
<p>3D Gaussian Splatting (3DGS) and its subsequent works are restricted to specific hardware setups, either on only low-cost or on only high-end configurations. Approaches aimed at reducing 3DGS memory usage enable rendering on low-cost GPU but compromise rendering quality, which fails to leverage the hardware capabilities in the case of higher-end GPU. Conversely, methods that enhance rendering quality require high-end GPU with large VRAM, making such methods impractical for lower-end devices with limited memory capacity. Consequently, 3DGS-based works generally assume a single hardware setup and lack the flexibility to adapt to varying hardware constraints.   To overcome this limitation, we propose Flexible Level of Detail (FLoD) for 3DGS. FLoD constructs a multi-level 3DGS representation through level-specific 3D scale constraints, where each level independently reconstructs the entire scene with varying detail and GPU memory usage. A level-by-level training strategy is introduced to ensure structural consistency across levels. Furthermore, the multi-level structure of FLoD allows selective rendering of image regions at different detail levels, providing additional memory-efficient rendering options. To our knowledge, among prior works which incorporate the concept of Level of Detail (LoD) with 3DGS, FLoD is the first to follow the core principle of LoD by offering adjustable options for a broad range of GPU settings.   Experiments demonstrate that FLoD provides various rendering options with trade-offs between quality and memory usage, enabling real-time rendering under diverse memory constraints. Furthermore, we show that FLoD generalizes to different 3DGS frameworks, indicating its potential for integration into future state-of-the-art developments. </p>
<blockquote>
<p>3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰åŠå…¶åç»­ä½œå“å—é™äºç‰¹å®šçš„ç¡¬ä»¶é…ç½®ï¼Œåªèƒ½åœ¨ä½ç«¯æˆ–é«˜ç«¯é…ç½®ä¸Šè¿è¡Œã€‚æ—¨åœ¨å‡å°‘3DGSå†…å­˜ä½¿ç”¨çš„æ–¹æ³•å¯ä»¥åœ¨ä½ç«¯GPUä¸Šè¿›è¡Œæ¸²æŸ“ï¼Œä½†ç‰ºç‰²äº†æ¸²æŸ“è´¨é‡ï¼Œåœ¨é«˜ç«¯GPUçš„æƒ…å†µä¸‹æœªèƒ½å……åˆ†åˆ©ç”¨ç¡¬ä»¶åŠŸèƒ½ã€‚ç›¸åï¼Œæé«˜æ¸²æŸ“è´¨é‡çš„æ–¹æ³•éœ€è¦å…·æœ‰å¤§VRAMçš„é«˜ç«¯GPUï¼Œè¿™ä½¿å¾—æ­¤ç±»æ–¹æ³•å¯¹äºå†…å­˜æœ‰é™çš„ä½ç«¯è®¾å¤‡ä¸åˆ‡å®é™…ã€‚å› æ­¤ï¼ŒåŸºäº3DGSçš„ä½œå“é€šå¸¸å‡è®¾å•ä¸€çš„ç¡¬ä»¶é…ç½®ï¼Œå¹¶ä¸”ç¼ºä¹é€‚åº”ä¸åŒç¡¬ä»¶çº¦æŸçš„çµæ´»æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸º3DGSæå‡ºäº†çµæ´»ç»†èŠ‚å±‚æ¬¡ï¼ˆFLoDï¼‰çš„æ–¹æ³•ã€‚FLoDé€šè¿‡ç‰¹å®šçš„ä¸‰ç»´å°ºåº¦çº¦æŸæ„å»ºå¤šå±‚æ¬¡çš„ä¸‰ç»´é«˜æ–¯è´´å›¾è¡¨ç¤ºï¼Œæ¯ä¸ªå±‚æ¬¡éƒ½èƒ½ç‹¬ç«‹åœ°ä»¥ä¸åŒç»†èŠ‚å’ŒGPUå†…å­˜ä½¿ç”¨é‡å»ºæ•´ä¸ªåœºæ™¯ã€‚å¼•å…¥åˆ†å±‚è®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿å„å±‚æ¬¡ä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒFLoDçš„å¤šå±‚æ¬¡ç»“æ„å…è®¸æœ‰é€‰æ‹©åœ°ä»¥ä¸åŒç»†èŠ‚å±‚æ¬¡æ¸²æŸ“å›¾åƒåŒºåŸŸï¼Œæä¾›æ›´èŠ‚çœå†…å­˜çš„æ¸²æŸ“é€‰é¡¹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨ç»“åˆç»†èŠ‚å±‚æ¬¡ï¼ˆLoDï¼‰æ¦‚å¿µçš„å…ˆå‰å·¥ä½œä¸­ï¼ŒFLoDé¦–æ¬¡éµå¾ªLoDçš„æ ¸å¿ƒåŸåˆ™ï¼Œä¸ºå¹¿æ³›çš„GPUè®¾ç½®æä¾›äº†å¯è°ƒé€‰é¡¹ã€‚å®éªŒè¡¨æ˜ï¼ŒFLoDæä¾›äº†å„ç§æ¸²æŸ“é€‰é¡¹ï¼Œåœ¨è´¨é‡å’Œå†…å­˜ä½¿ç”¨ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œèƒ½å¤Ÿåœ¨å„ç§å†…å­˜çº¦æŸä¸‹è¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†FLoDå¯ä»¥åº”ç”¨äºä¸åŒçš„3DGSæ¡†æ¶ï¼Œæ˜¾ç¤ºå‡ºå…¶æœªæ¥èå…¥æœ€æ–°å‘å±•çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12894v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://3dgs-flod.github.io/flod/">https://3dgs-flod.github.io/flod/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†å½“å‰3D Gaussian Splattingï¼ˆ3DGSï¼‰æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜åŠå…¶é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡¬ä»¶é€‚åº”æ€§æ–¹é¢ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†Flexible Level of Detailï¼ˆFLoDï¼‰æ–¹æ¡ˆï¼Œé€šè¿‡æ„å»ºå¤šå±‚æ¬¡çš„3DGSè¡¨ç¤ºï¼Œå®ç°ä¸åŒç¡¬ä»¶çº¦æŸä¸‹çš„çµæ´»é€‚åº”ã€‚FLoDé‡‡ç”¨å±‚çº§ç‰¹å®šçš„3Då°ºåº¦çº¦æŸï¼Œç‹¬ç«‹é‡å»ºåœºæ™¯å¹¶è°ƒæ•´ç»†èŠ‚å’ŒGPUå†…å­˜ä½¿ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒFLoDæä¾›å¤šç§æ¸²æŸ“é€‰é¡¹ï¼Œèƒ½åœ¨ä¿è¯å®æ—¶æ¸²æŸ“çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¹³è¡¡è´¨é‡å’Œå†…å­˜ä½¿ç”¨ï¼Œä¸”å¯åº”ç”¨äºä¸åŒçš„3DGSæ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰3DGSæŠ€æœ¯åœ¨ç¡¬ä»¶é€‚åº”æ€§æ–¹é¢å­˜åœ¨å±€é™ï¼Œéš¾ä»¥æ»¡è¶³å„ç§ç¡¬ä»¶çº¦æŸã€‚</li>
<li>FLoDæ–¹æ¡ˆé€šè¿‡æ„å»ºå¤šå±‚æ¬¡3DGSè¡¨ç¤ºï¼Œå®ç°çµæ´»é€‚åº”ä¸åŒç¡¬ä»¶ã€‚</li>
<li>FLoDé‡‡ç”¨å±‚çº§ç‰¹å®šçš„3Då°ºåº¦çº¦æŸï¼Œç‹¬ç«‹é‡å»ºåœºæ™¯å¹¶è°ƒæ•´ç»†èŠ‚å’ŒGPUå†…å­˜ä½¿ç”¨ã€‚</li>
<li>FLoDæä¾›å¤šç§æ¸²æŸ“é€‰é¡¹ï¼Œèƒ½åœ¨ä¿è¯å®æ—¶æ¸²æŸ“çš„åŒæ—¶ï¼Œæœ‰æ•ˆå¹³è¡¡è´¨é‡å’Œå†…å­˜ä½¿ç”¨ã€‚</li>
<li>FLoDå¯åº”ç”¨äºä¸åŒçš„3DGSæ¡†æ¶ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>FLoDé€šè¿‡å±‚çº§è®­ç»ƒç­–ç•¥ç¡®ä¿ç»“æ„ä¸€è‡´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1898f15ec5a4b48091e9ee4fd3e1fdf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea7298e22b44fde8df8baed2e953de8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6036ef6e0ee5fc65cdae143b5c95a3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7ade680a8310945040bf5c519e9e584.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-13/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f1895cc5423e9285b465a0937fc760ac.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  The Less You Depend, The More You Learn Synthesizing Novel Views from   Sparse, Unposed Images without Any 3D Knowledge
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-13/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-45e982bbb5e881e18505a77319741526.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-13  DGAE Diffusion-Guided Autoencoder for Efficient Latent Representation   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
