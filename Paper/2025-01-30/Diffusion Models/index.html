<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-30  CubeDiff Repurposing Diffusion-Based Image Models for Panorama   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fdf0db8b8061d6603296201be49b6811.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-30
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-30-更新"><a href="#2025-01-30-更新" class="headerlink" title="2025-01-30 更新"></a>2025-01-30 更新</h1><h2 id="CubeDiff-Repurposing-Diffusion-Based-Image-Models-for-Panorama-Generation"><a href="#CubeDiff-Repurposing-Diffusion-Based-Image-Models-for-Panorama-Generation" class="headerlink" title="CubeDiff: Repurposing Diffusion-Based Image Models for Panorama   Generation"></a>CubeDiff: Repurposing Diffusion-Based Image Models for Panorama   Generation</h2><p><strong>Authors:Nikolai Kalischek, Michael Oechsle, Fabian Manhardt, Philipp Henzler, Konrad Schindler, Federico Tombari</strong></p>
<p>We introduce a novel method for generating 360{\deg} panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively. Project page: <a target="_blank" rel="noopener" href="https://cubediff.github.io/">https://cubediff.github.io/</a> </p>
<blockquote>
<p>我们介绍了一种从文字提示或图像生成360°全景的新方法。我们的方法利用3D生成的最新进展，采用多视角扩散模型联合合成立方体贴图的六个面。与以往依赖于处理等距投影或自回归生成的方法不同，我们的方法将每面视为标准透视图像，简化了生成过程，并使得现有多视角扩散模型的使用成为可能。我们证明，这些模型可以适应产生高质量的立方体贴图，而无需对应感知注意力层。我们的模型允许精细的文本控制，生成高分辨率的全景图像，并且在训练集之外也能很好地推广，同时在定性和定量上达到最新水平的结果。项目页面：<a target="_blank" rel="noopener" href="https://cubediff.github.io/">https://cubediff.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17162v1">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种从文本提示或图像生成360°全景图的新方法。该方法利用最新的3D生成技术，通过采用多视角扩散模型联合合成立方体贴图的六个面。不同于以往依赖等距投影或自回归生成的方法，本文方法将每个面视为标准透视图像，简化了生成过程，并允许使用现有的多视角扩散模型。实验证明，这些模型可适应产生高质量立方体贴图，无需对应感知注意力层。该方法具有精细的文本控制功能，可生成高分辨率全景图像，并在训练集之外具有良好的泛化能力，同时达到定性和定量的最佳结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种生成360°全景图的新方法。</li>
<li>利用多视角扩散模型联合合成立方体贴图的六个面。</li>
<li>将每个面视为标准透视图像，简化了生成过程。</li>
<li>方法不需要对应感知注意力层，即可产生高质量立方体贴图。</li>
<li>具有精细的文本控制功能，可生成高分辨率全景图像。</li>
<li>在训练集之外具有良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17162">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cd35aa05915bb6cfa4f8d148d2459519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7959b3d92d204d5be1ee061fbb0fb5a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="IC-Portrait-In-Context-Matching-for-View-Consistent-Personalized-Portrait"><a href="#IC-Portrait-In-Context-Matching-for-View-Consistent-Personalized-Portrait" class="headerlink" title="IC-Portrait: In-Context Matching for View-Consistent Personalized   Portrait"></a>IC-Portrait: In-Context Matching for View-Consistent Personalized   Portrait</h2><p><strong>Authors:Han Yang, Enis Simsar, Sotiris Anagnostidi, Yanlong Zang, Thomas Hofmann, Ziwei Liu</strong></p>
<p>Existing diffusion models show great potential for identity-preserving generation. However, personalized portrait generation remains challenging due to the diversity in user profiles, including variations in appearance and lighting conditions. To address these challenges, we propose IC-Portrait, a novel framework designed to accurately encode individual identities for personalized portrait generation. Our key insight is that pre-trained diffusion models are fast learners (e.g.,100 ~ 200 steps) for in-context dense correspondence matching, which motivates the two major designs of our IC-Portrait framework. Specifically, we reformulate portrait generation into two sub-tasks: 1) Lighting-Aware Stitching: we find that masking a high proportion of the input image, e.g., 80%, yields a highly effective self-supervisory representation learning of reference image lighting. 2) View-Consistent Adaptation: we leverage a synthetic view-consistent profile dataset to learn the in-context correspondence. The reference profile can then be warped into arbitrary poses for strong spatial-aligned view conditioning. Coupling these two designs by simply concatenating latents to form ControlNet-like supervision and modeling, enables us to significantly enhance the identity preservation fidelity and stability. Extensive evaluations demonstrate that IC-Portrait consistently outperforms existing state-of-the-art methods both quantitatively and qualitatively, with particularly notable improvements in visual qualities. Furthermore, IC-Portrait even demonstrates 3D-aware relighting capabilities. </p>
<blockquote>
<p>现有的扩散模型在身份保留生成方面显示出巨大潜力。然而，由于用户资料的多样性，包括外观和光照条件的差异，个性化肖像生成仍然具有挑战性。为了解决这些挑战，我们提出了IC-Portrait，这是一个旨在准确编码个人身份用于个性化肖像生成的新型框架。我们的关键见解是，预训练的扩散模型对于上下文中的密集对应关系匹配是快速学习者（例如，100至200步），这激发了我们IC-Portrait框架的两个主要设计。具体来说，我们将肖像生成重新定义为两个子任务：1）光照感知拼接：我们发现遮挡输入图像的高比例部分（例如80％）可以有效地进行参考图像光照的自监督表示学习。2）视图一致适应：我们利用合成视图一致的轮廓数据集来学习上下文中的对应关系。然后，可以将参考轮廓变形为任意姿势，以实现强大的空间对齐视图条件。通过简单地连接潜在空间以形成类似ControlNet的监督和管理，可以大大增强身份保留的保真度和稳定性。大量评估表明，IC-Portrait在定量和定性方面均始终优于现有最先进的方法，在视觉品质方面尤其取得了显著的改进。此外，IC-Portrait甚至展示了3D感知的重照明能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17159v1">PDF</a> technical report</p>
<p><strong>摘要</strong></p>
<p>本文提出一种名为IC-Portrait的新型框架，旨在解决个性化肖像生成中的身份保留挑战。该框架利用预训练的扩散模型进行快速学习，通过两个主要设计实现准确编码个体身份：一是光照感知拼接，通过遮挡大部分输入图像实现自我监督学习参考图像光照；二是视角一致性适应，利用合成视角一致性轮廓数据集学习上下文对应关系。IC-Portrait通过结合这两个设计，显著提高了身份保留的保真度和稳定性。评估表明，IC-Portrait在定量和定性上均优于现有先进技术，尤其在视觉品质上有显著改进，甚至展示了3D感知的重照明能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>IC-Portrait框架被设计为解决个性化肖像生成中的身份保留挑战。</li>
<li>利用预训练的扩散模型进行快速学习，实现准确编码个体身份。</li>
<li>通过光照感知拼接和视角一致性适应两个主要设计，提高身份保留的保真度和稳定性。</li>
<li>光照感知拼接通过遮挡大部分输入图像实现自我监督学习参考图像光照。</li>
<li>视角一致性适应利用合成视角一致性轮廓数据集学习上下文对应关系。</li>
<li>IC-Portrait在定量和定性评估上优于现有技术，尤其在视觉品质上有显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17159">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-677957cefff1d0534559d2c7db7e89ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fae033f722c11754e250050fab37ce49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-181c421d2e16684151e8b7a5e915cf66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3effc876bcfaae1951eea6ed552d3b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad8455a2766e6494eabe2810c31bf337.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DIRIGENt-End-To-End-Robotic-Imitation-of-Human-Demonstrations-Based-on-a-Diffusion-Model"><a href="#DIRIGENt-End-To-End-Robotic-Imitation-of-Human-Demonstrations-Based-on-a-Diffusion-Model" class="headerlink" title="DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on   a Diffusion Model"></a>DIRIGENt: End-To-End Robotic Imitation of Human Demonstrations Based on   a Diffusion Model</h2><p><strong>Authors:Josua Spisak, Matthias Kerzel, Stefan Wermter</strong></p>
<p>There has been substantial progress in humanoid robots, with new skills continuously being taught, ranging from navigation to manipulation. While these abilities may seem impressive, the teaching methods often remain inefficient. To enhance the process of teaching robots, we propose leveraging a mechanism effectively used by humans: teaching by demonstrating. In this paper, we introduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel end-to-end diffusion approach that directly generates joint values from observing human demonstrations, enabling a robot to imitate these actions without any existing mapping between it and humans. We create a dataset in which humans imitate a robot and then use this collected data to train a diffusion model that enables a robot to imitate humans. The following three aspects are the core of our contribution. First is our novel dataset with natural pairs between human and robot poses, allowing our approach to imitate humans accurately despite the gap between their anatomies. Second, the diffusion input to our model alleviates the challenge of redundant joint configurations, limiting the search space. And finally, our end-to-end architecture from perception to action leads to an improved learning capability. Through our experimental analysis, we show that combining these three aspects allows DIRIGENt to outperform existing state-of-the-art approaches in the field of generating joint values from RGB images. </p>
<blockquote>
<p>人形机器人在技能学习方面取得了巨大进步，不断习得新技能，从导航到操作都有涉及。尽管这些能力令人印象深刻，但教学方法往往效率低下。为了改进机器人教学过程，我们提出利用人类有效使用的一种机制：通过示范进行教学。在本文中，我们介绍了DIRIGENt（直接机器人模仿生成模型），这是一种新型端到端扩散方法，能够直接从人类示范中生成关节值，使机器人能够模仿这些动作，而无需在机器人和人类之间建立现有映射。我们创建了一个数据集，人类在其中模仿机器人，然后使用收集的数据来训练一个扩散模型，使机器人能够模仿人类。我们贡献的核心在于以下三个方面。首先，我们拥有自然的人类和机器人姿势配对数据集，这使得我们的方法即使在他们解剖结构之间存在差异的情况下，也能准确地模仿人类。其次，我们模型的扩散输入减轻了冗余关节配置的挑战，限制了搜索空间。最后，我们从感知到动作的端到端架构提高了学习能力。通过我们的实验分析，我们证明了结合这三个方面可以使DIRIGENt在从RGB图像生成关节值领域超越现有最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16800v1">PDF</a> </p>
<p><strong>Summary</strong><br>     人类型机器人取得显著进展，习得新技能，如导航和操控。为提升机器人教学过程的效率，本文提出借鉴人类常用的教学方式——通过示范进行教学，并介绍DIRIGENt（直接机器人模仿生成模型），这是一种全新的端到端扩散方法，它能从观察人类示范动作中直接生成关节值，让机器人在无需与人类建立映射关系的情况下模仿动作。本文创建了人类模仿机器人的数据集，并用其训练扩散模型，使机器人能模仿人类。本文的核心贡献包括三个方面：自然配对的人类和机器人姿态构成的新数据集，让机器人能够准确模仿人类；模型扩散输入解决了关节配置冗余问题并缩减了搜索空间；端到端的感知到行动架构提高了学习能力。实验证明DIRIGENt在由RGB图像生成关节值方面表现超越现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类型机器人在技能和动作上取得显著进步。</li>
<li>当前机器人教学方式存在效率问题。</li>
<li>借鉴人类通过示范进行教学的方式，提出DIRIGENt模型。</li>
<li>DIRIGENt模型是一种端到端的扩散方法，能从人类示范中直接生成关节值。</li>
<li>创建了人类模仿机器人的数据集用于训练扩散模型。</li>
<li>模型的核心贡献包括新数据集、扩散输入解决关节配置冗余问题以及端到端的感知到行动架构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-420bd86e752028bbddd24aa1ef9d9ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9accd520e2fadac716b720cb36c94ffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b37d013d9a3ca043b3373a35be8bd1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a964ccd5bc2feec9caf79e33a4867a20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-242342f980ce0987dcce8fad9031fe8a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffSplat-Repurposing-Image-Diffusion-Models-for-Scalable-Gaussian-Splat-Generation"><a href="#DiffSplat-Repurposing-Image-Diffusion-Models-for-Scalable-Gaussian-Splat-Generation" class="headerlink" title="DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian   Splat Generation"></a>DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian   Splat Generation</h2><p><strong>Authors:Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, Yadong Mu</strong></p>
<p>Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale text-to-image diffusion models. It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model. To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian splat grids for scalable dataset curation. In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views. The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm. Extensive experiments reveal the superiority of DiffSplat in text- and image-conditioned generation tasks and downstream applications. Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism. </p>
<blockquote>
<p>关于文本或单幅图像生成三维内容的最新进展受限于高质量的三维数据集以及二维多视角生成的不一致性。我们引入了DiffSplat，这是一种新型的三维生成框架，它通过驯服大规模文本到图像扩散模型来原生生成三维高斯光斑。它与之前的三维生成模型不同，能够在统一模型中有效利用网络规模的二维先验知识，同时保持三维一致性。为了启动训练，我们提出了一种轻量级的重建模型，用于即时生成多视角高斯光斑网格，以实现可扩展的数据集整理。结合这些网格上的常规扩散损失，引入了一种三维渲染损失，以促进任意视角下的三维连贯性。其与图像扩散模型的兼容性使得众多图像生成技术能够无缝适应三维领域。大量实验表明，DiffSplat在文本和图像条件生成任务以及下游应用中表现出卓越性能。详尽的消融研究验证了每个关键设计选择的有效性，并深入了解了其内在机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16764v1">PDF</a> Accepted to ICLR 2025; Project page:   <a target="_blank" rel="noopener" href="https://chenguolin.github.io/projects/DiffSplat">https://chenguolin.github.io/projects/DiffSplat</a></p>
<p><strong>Summary</strong></p>
<p>新一代三维内容生成技术面临数据集质量不高和多视角生成不一致的问题。我们推出DiffSplat，一种新型三维生成框架，通过驾驭大规模文本到图像扩散模型，直接生成三维高斯点云。它不同于以往的三维生成模型，能有效利用互联网规模的二维先验知识，同时在统一模型中保持三维一致性。为启动训练，我们提出了一种轻量级重建模型，可立即生成多视角高斯点云网格，便于扩展数据集整理。除了在这些网格上的常规扩散损失，我们还引入了三维渲染损失，以促进任意视角下的三维连贯性。其与图像扩散模型的兼容性使得众多图像生成技术能够无缝适应三维领域。大量实验表明，DiffSplat在文本和图像条件生成任务以及下游应用中表现出卓越性能。彻底的消融研究验证了关键设计选择的有效性，并揭示了其内在机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffSplat是一种新型三维生成框架，能够直接生成三维高斯点云。</li>
<li>与其他三维生成模型不同，DiffSplat有效利用互联网规模的二维先验知识，并维持统一模型中的三维一致性。</li>
<li>通过轻量级重建模型快速生成多视角高斯点云网格，促进数据集整理。</li>
<li>引入三维渲染损失，增强任意视角下的三维连贯性。</li>
<li>DiffSplat与图像扩散模型的兼容性使得图像生成技术能无缝适应三维领域。</li>
<li>大量实验证明DiffSplat在文本和图像条件生成任务以及下游应用中的卓越性能。</li>
<li>消融研究验证了关键设计选择的有效性，揭示了其内在机制和工作原理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16764">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a08db892e6154dea051f9f796e88a32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a92f569f82ed878130b8a4cf061f7152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d325368ed949026d796ea05784910ee.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ITVTON-Virtual-Try-On-Diffusion-Transformer-Model-Based-on-Integrated-Image-and-Text"><a href="#ITVTON-Virtual-Try-On-Diffusion-Transformer-Model-Based-on-Integrated-Image-and-Text" class="headerlink" title="ITVTON:Virtual Try-On Diffusion Transformer Model Based on Integrated   Image and Text"></a>ITVTON:Virtual Try-On Diffusion Transformer Model Based on Integrated   Image and Text</h2><p><strong>Authors:Haifeng Ni</strong></p>
<p>Recent advancements in virtual fitting for characters and clothing have leveraged diffusion models to improve the realism of garment fitting. However, challenges remain in handling complex scenes and poses, which can result in unnatural garment fitting and poorly rendered intricate patterns. In this work, we introduce ITVTON, a novel method that enhances clothing-character interactions by combining clothing and character images along spatial channels as inputs, thereby improving fitting accuracy for the inpainting model. Additionally, we incorporate integrated textual descriptions from multiple images to boost the realism of the generated visual effects. To optimize computational efficiency, we limit training to the attention parameters within a single diffusion transformer (Single-DiT) block. To more rigorously address the complexities of real-world scenarios, we curated training samples from the IGPair dataset, thereby enhancing ITVTON’s performance across diverse environments. Extensive experiments demonstrate that ITVTON outperforms baseline methods both qualitatively and quantitatively, setting a new standard for virtual fitting tasks. </p>
<blockquote>
<p>在字符和服装虚拟适配方面的最新进展已经利用扩散模型提高了服装适配的真实性。然而，在处理复杂场景和姿势时仍存在挑战，这可能导致服装适配不自然，精细图案渲染不佳。在这项工作中，我们引入了ITVTON，这是一种通过结合服装和角色图像作为空间通道输入来增强服装与角色交互的新型方法，从而提高补全模型的适配准确性。此外，我们从多张图像中融入集成的文本描述，以提高生成视觉效果的真实性。为了优化计算效率，我们将训练限制在单个扩散变压器（Single-DiT）块内的注意力参数。为了更严格地解决现实场景的复杂性，我们从IGPair数据集中精选训练样本，从而提高ITVTON在不同环境中的性能。大量实验表明，ITVTON在定性和定量方面都优于基准方法，为虚拟适配任务树立了新的标准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16757v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在虚拟角色服装拟合中的应用，近期取得了显著进展，提升了服装真实感。然而，处理复杂场景和姿势的挑战仍然存在，可能导致服装拟合不自然和精细图案渲染不良。本研究引入ITVTON方法，通过结合服装和角色图像作为空间通道输入，提高衣物与角色的互动，改善填充模型的拟合精度。此外，还融入多张图像的文本描述，增强生成视觉效果的真实感。为优化计算效率，仅在单个扩散变压器（Single-DiT）块内训练注意力参数。通过采集IGPair数据集的训练样本，应对现实场景的复杂性，ITVTON在多种环境下表现出卓越性能。实验证明，ITVTON在定性和定量上均优于基准方法，为虚拟拟合任务树立了新标准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型用于改进虚拟角色服装拟合的逼真度。</li>
<li>处理复杂场景和姿势的挑战仍然突出。</li>
<li>ITVTON方法结合服装和角色图像作为输入，提高衣物与角色的互动。</li>
<li>通过结合多张图像的文本描述，增强生成的视觉效果的真实感。</li>
<li>为优化计算效率，仅在单个扩散变压器块内训练注意力参数。</li>
<li>使用IGPair数据集的训练样本应对现实场景的复杂性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16757">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-11bfa00085039cd8ff1afdf1a84f5a06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c89784a80c90d41cda593cc1b4a3265a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e86fa1f6b1ae716747084f19fb617d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39557d16a1f7c191ea38d9d20c140e8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f291e7077e762a9129efeb3ad3dc527b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d39c9b509de00863c93c6c2384fe1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a44092cc779b330b2080fa08116fd6a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CascadeV-An-Implementation-of-Wurstchen-Architecture-for-Video-Generation"><a href="#CascadeV-An-Implementation-of-Wurstchen-Architecture-for-Video-Generation" class="headerlink" title="CascadeV: An Implementation of Wurstchen Architecture for Video   Generation"></a>CascadeV: An Implementation of Wurstchen Architecture for Video   Generation</h2><p><strong>Authors:Wenfeng Lin, Jiangchuan Wei, Boyuan Liu, Yichen Zhang, Shiyue Yan, Mingyu Guo</strong></p>
<p>Recently, with the tremendous success of diffusion models in the field of text-to-image (T2I) generation, increasing attention has been directed toward their potential in text-to-video (T2V) applications. However, the computational demands of diffusion models pose significant challenges, particularly in generating high-resolution videos with high frame rates. In this paper, we propose CascadeV, a cascaded latent diffusion model (LDM), that is capable of producing state-of-the-art 2K resolution videos. Experiments demonstrate that our cascaded model achieves a higher compression ratio, substantially reducing the computational challenges associated with high-quality video generation. We also implement a spatiotemporal alternating grid 3D attention mechanism, which effectively integrates spatial and temporal information, ensuring superior consistency across the generated video frames. Furthermore, our model can be cascaded with existing T2V models, theoretically enabling a 4$\times$ increase in resolution or frames per second without any fine-tuning. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/bytedance/CascadeV">https://github.com/bytedance/CascadeV</a>. </p>
<blockquote>
<p>最近，随着扩散模型在文本到图像（T2I）生成领域的巨大成功，人们越来越关注其在文本到视频（T2V）应用中的潜力。然而，扩散模型的计算需求构成了重大挑战，特别是在生成高帧率的高分辨率视频时。在本文中，我们提出了CascadeV，这是一种级联的潜在扩散模型（LDM），能够产生最先进的2K分辨率视频。实验表明，我们的级联模型实现了更高的压缩比，大大降低了高质量视频生成的计算挑战。我们还实现了一种时空交替网格3D注意力机制，有效地结合了空间和时间信息，确保了生成视频帧之间的一致性。此外，我们的模型可以与现有的T2V模型级联，理论上可以在不进行微调的情况下实现每秒帧数或分辨率的四倍提升。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/bytedance/CascadeV%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bytedance/CascadeV找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16612v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型在文本到图像生成领域取得了巨大成功，其在文本到视频（T2V）应用中的潜力正受到越来越多的关注。然而，扩散模型在计算需求方面存在挑战，特别是在生成高分辨率和高帧率视频时。本文提出了CascadeV，一种级联潜在扩散模型（LDM），能够生成最先进的2K分辨率视频。实验表明，级联模型实现了更高的压缩比，减少了高质量视频生成的计算挑战。此外，还实现了时空交替网格3D注意力机制，有效整合空间和时间信息，确保生成视频帧之间的一致性。该模型还可以与现有T2V模型级联，理论上可以提高分辨率或帧率而不需微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本到视频生成领域的应用潜力正在受到关注。</li>
<li>级联潜在扩散模型（CascadeV）能够生成高质量的2K分辨率视频。</li>
<li>级联模型实现了高压缩比，降低计算需求。</li>
<li>实现了时空交替网格3D注意力机制，增强视频帧间一致性。</li>
<li>CascadeV模型可以与现有T2V模型级联，提高分辨率或帧率。</li>
<li>该模型理论上的优点包括在不进行微调的情况下提高分辨率或帧率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b273b7f797cfff232a7a43a50e60c696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58a7d8bfb3af09d7cf5ede4a52eac63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-164cae4de0e896a8393ea2fb00213c4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83ac14e54fc138462f3a6d40d5c8efc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdf0db8b8061d6603296201be49b6811.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b18a1dd45678ae22c2706bf1abdb0e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7870065adfaaf13a437ea3037bfccd1e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation"><a href="#Slot-Guided-Adaptation-of-Pre-trained-Diffusion-Models-for-Object-Centric-Learning-and-Compositional-Generation" class="headerlink" title="Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation"></a>Slot-Guided Adaptation of Pre-trained Diffusion Models for   Object-Centric Learning and Compositional Generation</h2><p><strong>Authors:Adil Kaan Akan, Yucel Yemez</strong></p>
<p>We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found at <a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/">https://kaanakan.github.io/SlotAdapt/</a>. </p>
<blockquote>
<p>我们提出了SlotAdapt，这是一种结合插槽注意力和预训练扩散模型的面向对象的学习方法，它通过引入适配器来实现基于插槽的条件。我们的方法保留了预训练扩散模型的生成能力，同时避免了其面向文本的条件偏差。我们还将额外的指导损失纳入我们的架构，以调整适配器层的交叉注意力与插槽注意力。这增强了我们的模型与输入图像中的对象的对齐性，而无需使用外部监督。实验结果表明，我们的方法在多个数据集上的对象发现和图像生成任务中优于最先进的技术，包括那些带有真实图像的数据集。此外，通过试验，我们证明我们的方法在复杂真实图像的合成生成方面表现出色，与文献中的其他基于插槽的生成方法相比具有显著优势。项目页面位于<a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/%E3%80%82">https://kaanakan.github.io/SlotAdapt/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15878v2">PDF</a> Accepted to ICLR2025. Project page:   <a target="_blank" rel="noopener" href="https://kaanakan.github.io/SlotAdapt/">https://kaanakan.github.io/SlotAdapt/</a></p>
<p><strong>Summary</strong></p>
<p>SlotAdapt结合槽位注意力和预训练扩散模型，通过引入适配器进行槽位条件化，实现对象级学习。该方法在保留预训练扩散模型的生成能力的同时，避免了其文本中心化的条件偏差。通过引入额外的指导损失，增强模型与输入图像中对象的对齐度，无需外部监督。实验表明，该方法在多个数据集上的物体发现和图像生成任务上表现优异，尤其擅长处理复杂真实图像的合成生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SlotAdapt结合了槽位注意力和预训练的扩散模型。</li>
<li>通过引入适配器实现槽位条件化，保留扩散模型的生成能力。</li>
<li>方法避免了文本中心化的条件偏差。</li>
<li>通过引入额外的指导损失增强模型与输入图像中对象的对齐。</li>
<li>该方法在不同数据集上的物体发现和图像生成任务上表现优异。</li>
<li>特别擅长处理复杂真实图像的合成生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58a46ebcaa30f6b754313d244022d9b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05a8b9e8034e74bb394fb6eb8b24573d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy"><a href="#Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy" class="headerlink" title="Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy"></a>Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy</h2><p><strong>Authors:Shaoyan Pan, Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</strong></p>
<p>The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation. </p>
<blockquote>
<p>在介入心脏荧光透视视频中，对导线进行准确的分割对于计算机辅助导航任务至关重要。虽然深度学习的方法在导线分割方面已经显示出高准确度和稳健性，但它们需要大规模的标注数据集来实现普遍适用性，这凸显了对大量标注数据的需要，以提高模型性能。为了解决这一挑战，我们提出了“基于分割引导的帧一致性视频扩散模型（SF-VD）”，用于生成大量标记的荧光透视视频，增强导线分割网络的训练数据。SF-VD通过独立建模场景分布和运动分布来利用有限的标注视频。它通过根据指定的输入掩码生成带有导线的二维荧光透视图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，并通过帧一致性策略确保帧与帧之间的连贯性。分割引导机制进一步调整了导线对比度，确保合成图像的可见性范围多样化。在荧光数据集上的评估证实了所生成视频的卓越质量，并显示出导线分割的显著改善。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16050v4">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>在心脏介入手术荧光透视视频中准确分割导丝对于计算机辅助导航任务至关重要。针对深度学习方法需要大量标注数据集以提高导丝分割的普遍性问题，我们提出了基于分割引导的帧一致性视频扩散模型（SF-VD）。该模型能够生成大量标注的荧光透视视频，增强导丝分割网络的训练数据。SF-VD通过独立建模场景分布和运动分布，利用有限标注的视频进行工作。它首先根据指定的输入掩膜生成二维荧光透视图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，确保帧间一致性。分割引导机制进一步调整导丝对比度，确保合成图像的可见性多样性。在荧光数据集上的评估证明了生成视频的高质量，并显示出在导丝分割方面的显著改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>导丝在心脏介入手术荧光透视视频中的准确分割对计算机辅助导航至关重要。</li>
<li>深度学习方法虽能高精度、稳健地进行导丝分割，但需大量标注数据以改善模型性能。</li>
<li>提出基于分割引导的帧一致性视频扩散模型（SF-VD）以生成大量标注的荧光透视视频，增强训练数据。</li>
<li>SF-VD通过独立建模场景分布和运动分布来工作。</li>
<li>模型通过生成二维荧光透视图像采样场景分布，并通过逐步生成后续帧来确保帧间一致性。</li>
<li>分割引导机制调整导丝对比度，确保合成图像的可见性多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-495b0d7f8e63084a5494496228682813.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1812b9f580ab1045f66623b4f6e47d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6dfead2b062aa6a8f15edfc59eb3429.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d47112dbe22818737ff4b9362916e1d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StableMaterials-Enhancing-Diversity-in-Material-Generation-via-Semi-Supervised-Learning"><a href="#StableMaterials-Enhancing-Diversity-in-Material-Generation-via-Semi-Supervised-Learning" class="headerlink" title="StableMaterials: Enhancing Diversity in Material Generation via   Semi-Supervised Learning"></a>StableMaterials: Enhancing Diversity in Material Generation via   Semi-Supervised Learning</h2><p><strong>Authors:Giuseppe Vecchio</strong></p>
<p>We introduce StableMaterials, a novel approach for generating photorealistic physical-based rendering (PBR) materials that integrate semi-supervised learning with Latent Diffusion Models (LDMs). Our method employs adversarial training to distill knowledge from existing large-scale image generation models, minimizing the reliance on annotated data and enhancing the diversity in generation. This distillation approach aligns the distribution of the generated materials with that of image textures from an SDXL model, enabling the generation of novel materials that are not present in the initial training dataset. Furthermore, we employ a diffusion-based refiner model to improve the visual quality of the samples and achieve high-resolution generation. Finally, we distill a latent consistency model for fast generation in just four steps and propose a new tileability technique that removes visual artifacts typically associated with fewer diffusion steps. We detail the architecture and training process of StableMaterials, the integration of semi-supervised training within existing LDM frameworks and show the advantages of our approach. Comparative evaluations with state-of-the-art methods show the effectiveness of StableMaterials, highlighting its potential applications in computer graphics and beyond. StableMaterials is publicly available at <a target="_blank" rel="noopener" href="https://gvecchio.com/stablematerials">https://gvecchio.com/stablematerials</a>. </p>
<blockquote>
<p>我们介绍了StableMaterials，这是一种基于物理渲染（PBR）材料生成的新型方法，它将半监督学习与潜在扩散模型（LDM）相结合。我们的方法采用对抗训练从现有的大规模图像生成模型中提炼知识，减少对标注数据的依赖，并增强生成的多样性。这种提炼方法使生成材料的分布与SDXL模型中的图像纹理分布相一致，能够生成初始训练数据集中不存在的新材料。此外，我们采用基于扩散的细化模型来提高样本的视觉质量，实现高分辨率生成。最后，我们提炼了一个潜在一致性模型，只需四个步骤即可快速生成，并提出了一种新的可平铺技术，消除了由于较少的扩散步骤而通常出现的视觉伪影。我们详细介绍了StableMaterials的架构和训练过程，以及在现有LDM框架内半监督训练的集成，展示了我们的优势。与最新方法的比较评估表明，StableMaterials是有效的，突出了其在计算机图形等领域的应用潜力。StableMaterials可在<a target="_blank" rel="noopener" href="https://gvecchio.com/stablematerials%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://gvecchio.com/stablematerials公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.09293v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>StableMaterials是一种结合半监督学习与潜在扩散模型（Latent Diffusion Models，LDM）生成真实物理渲染（PBR）材料的新方法。该方法采用对抗训练从现有大规模图像生成模型中提炼知识，减少了对标注数据的依赖，提高了生成的多样性。通过蒸馏方法与SDXL模型图像纹理的分布对齐，生成不存在于初始训练集中的新材料。此外，使用基于扩散的细化模型提高样本的视觉质量，实现高分辨率生成。最后，我们提炼了一个具有快速四步生成能力的潜在一致性模型，并提出一种新的去瓦技术，消除了因减少扩散步骤而产生的典型视觉伪影。StableMaterials的优势在于其架构和训练过程的细节，以及其在计算机图形等领域的应用潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StableMaterials结合了半监督学习与潜在扩散模型（LDM）生成真实物理渲染（PBR）材料。</li>
<li>通过对抗训练从大规模图像生成模型中提炼知识，减少标注数据依赖并提高生成多样性。</li>
<li>生成的材料与SDXL模型图像纹理分布对齐，能生成初始训练集中不存在的新材料。</li>
<li>使用基于扩散的细化模型提高样本的视觉质量，实现高分辨率生成。</li>
<li>提炼了一个快速四步生成的潜在一致性模型。</li>
<li>提出新的去瓦技术，消除因减少扩散步骤而产生的视觉伪影。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.09293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-523c9d09a7c66eb768087b2d5c8259a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-effc1adb54fabc2a31187d6324d4ef4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-840a9b20e6b27aa78038176d6b15e57a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df03359706f47abcb521bd41373db8e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e5f30ac883811912e4dba1d2724ef4b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-30/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-30/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-30/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2208dc6c1b22c16cef30efe6649ac00e.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-30  Sensitivity of Quantitative Susceptibility Mapping in Clinical Brain   Research
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-30
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-30/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-71fd8ac1e8219487b111cd2d07631692.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-01-30  LinPrim Linear Primitives for Differentiable Volumetric Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11176.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
