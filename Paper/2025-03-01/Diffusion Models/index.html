<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  FlexVAR Flexible Visual Autoregressive Modeling without Residual   Prediction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5738f829f99e5430b6a8a602598990fa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-01-æ›´æ–°"><a href="#2025-03-01-æ›´æ–°" class="headerlink" title="2025-03-01 æ›´æ–°"></a>2025-03-01 æ›´æ–°</h1><h2 id="FlexVAR-Flexible-Visual-Autoregressive-Modeling-without-Residual-Prediction"><a href="#FlexVAR-Flexible-Visual-Autoregressive-Modeling-without-Residual-Prediction" class="headerlink" title="FlexVAR: Flexible Visual Autoregressive Modeling without Residual   Prediction"></a>FlexVAR: Flexible Visual Autoregressive Modeling without Residual   Prediction</h2><p><strong>Authors:Siyu Jiao, Gengwei Zhang, Yinlong Qian, Jiancheng Huang, Yao Zhao, Humphrey Shi, Lin Ma, Yunchao Wei, Zequn Jie</strong></p>
<p>This work challenges the residual prediction paradigm in visual autoregressive modeling and presents FlexVAR, a new Flexible Visual AutoRegressive image generation paradigm. FlexVAR facilitates autoregressive learning with ground-truth prediction, enabling each step to independently produce plausible images. This simple, intuitive approach swiftly learns visual distributions and makes the generation process more flexible and adaptable. Trained solely on low-resolution images ($\leq$ 256px), FlexVAR can: (1) Generate images of various resolutions and aspect ratios, even exceeding the resolution of the training images. (2) Support various image-to-image tasks, including image refinement, in&#x2F;out-painting, and image expansion. (3) Adapt to various autoregressive steps, allowing for faster inference with fewer steps or enhancing image quality with more steps. Our 1.0B model outperforms its VAR counterpart on the ImageNet 256$\times$256 benchmark. Moreover, when zero-shot transfer the image generation process with 13 steps, the performance further improves to 2.08 FID, outperforming state-of-the-art autoregressive models AiM&#x2F;VAR by 0.25&#x2F;0.28 FID and popular diffusion models LDM&#x2F;DiT by 1.52&#x2F;0.19 FID, respectively. When transferring our 1.0B model to the ImageNet 512$\times$512 benchmark in a zero-shot manner, FlexVAR achieves competitive results compared to the VAR 2.3B model, which is a fully supervised model trained at 512$\times$512 resolution. </p>
<blockquote>
<p>æœ¬æ–‡æŒ‘æˆ˜äº†è§†è§‰è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ®‹å·®é¢„æµ‹èŒƒå¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„çµæ´»è§†è§‰è‡ªå›å½’å›¾åƒç”ŸæˆèŒƒå¼FlexVARã€‚FlexVARé€šè¿‡çœŸå®é¢„æµ‹ä¿ƒè¿›è‡ªå›å½’å­¦ä¹ ï¼Œä½¿æ¯ä¸€æ­¥éƒ½èƒ½ç‹¬ç«‹äº§ç”Ÿåˆç†çš„å›¾åƒã€‚è¿™ç§ç®€å•ç›´è§‚çš„æ–¹æ³•èƒ½å¤Ÿå¿«é€Ÿå­¦ä¹ è§†è§‰åˆ†å¸ƒï¼Œä½¿ç”Ÿæˆè¿‡ç¨‹æ›´åŠ çµæ´»å’Œé€‚åº”æ€§å¼ºã€‚ä»…å¯¹ä½åˆ†è¾¨ç‡å›¾åƒï¼ˆ$\leq$ 256pxï¼‰è¿›è¡Œè®­ç»ƒï¼ŒFlexVARå¯ä»¥ï¼šï¼ˆ1ï¼‰ç”Ÿæˆå„ç§åˆ†è¾¨ç‡å’Œçºµæ¨ªæ¯”çš„å›¾åƒï¼Œç”šè‡³è¶…è¿‡è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡ã€‚ï¼ˆ2ï¼‰æ”¯æŒå„ç§å›¾åƒåˆ°å›¾åƒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒç»†åŒ–ã€ç»˜ç”»å’Œå›¾åƒæ‰©å±•ã€‚ï¼ˆ3ï¼‰é€‚åº”å„ç§è‡ªå›å½’æ­¥éª¤ï¼Œå…è®¸ç”¨æ›´å°‘çš„æ­¥éª¤è¿›è¡Œæ›´å¿«çš„æ¨ç†æˆ–æé«˜å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬çš„1.0Bæ¨¡å‹åœ¨ImageNet 256Ã—256åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å…¶VARå¯¹åº”æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå½“ä»¥é›¶æ­¥è½¬ç§»æ–¹å¼è¿›è¡Œå›¾åƒç”Ÿæˆè¿‡ç¨‹æ—¶ï¼Œä½¿ç”¨13æ­¥ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æé«˜åˆ°2.08 FIDï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹AiM&#x2F;VARï¼ˆåˆ†åˆ«é™ä½äº†0.25&#x2F;0.28 FIDï¼‰å’Œæµè¡Œçš„æ‰©æ•£æ¨¡å‹LDM&#x2F;DiTï¼ˆåˆ†åˆ«é™ä½äº†1.52&#x2F;0.19 FIDï¼‰ã€‚å½“ä»¥é›¶æ­¥è½¬ç§»æ–¹å¼å°†æˆ‘ä»¬çš„1.0Bæ¨¡å‹è½¬ç§»åˆ°ImageNet 512Ã—512åŸºå‡†æµ‹è¯•æ—¶ï¼ŒFlexVARä¸ç»è¿‡512Ã—512åˆ†è¾¨ç‡è®­ç»ƒçš„å®Œå…¨ç›‘ç£æ¨¡å‹VAR 2.3Bæ¨¡å‹ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20313v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æŒ‘æˆ˜äº†è§†è§‰è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ®‹å·®é¢„æµ‹èŒƒå¼ï¼Œå¹¶æå‡ºäº†FlexVARè¿™ä¸€æ–°çš„çµæ´»è§†è§‰è‡ªå›å½’å›¾åƒç”ŸæˆèŒƒå¼ã€‚FlexVARé€šè¿‡ç»“åˆåœ°é¢çœŸå®é¢„æµ‹ä¿ƒè¿›è‡ªå›å½’å­¦ä¹ ï¼Œä½¿æ¯ä¸ªæ­¥éª¤éƒ½èƒ½ç‹¬ç«‹ç”Ÿæˆåˆç†çš„å›¾åƒã€‚è¿™ç§ç®€å•ç›´è§‚çš„æ–¹æ³•å¯ä»¥å¿«é€Ÿå­¦ä¹ è§†è§‰åˆ†å¸ƒï¼Œä½¿ç”Ÿæˆè¿‡ç¨‹æ›´åŠ çµæ´»å’Œé€‚åº”æ€§å¼ºã€‚ä»…å¯¹ä½äºæˆ–ç­‰äº256åƒç´ çš„ä½åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œè®­ç»ƒï¼ŒFlexVARå¯ä»¥ç”Ÿæˆå„ç§åˆ†è¾¨ç‡å’Œæ¯”ä¾‹å°ºçš„å›¾åƒï¼Œç”šè‡³è¶…è¿‡è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡ï¼›æ”¯æŒå„ç§å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒç»†åŒ–ã€å†…å¤–ç»˜ç”»å’Œå›¾åƒæ‰©å±•ï¼›é€‚åº”å„ç§è‡ªå›å½’æ­¥éª¤ï¼Œå…è®¸æ›´å¿«çš„æ¨ç†ä½¿ç”¨è¾ƒå°‘çš„æ­¥éª¤æˆ–æ›´å¤šæ­¥éª¤æ¥æé«˜å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬çš„1.0Bæ¨¡å‹åœ¨ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºVARæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå½“ä»¥é›¶æ­¥è½¬ç§»çš„æ–¹å¼ä»¥åŒ…å«åä¸‰ä¸ªæ­¥éª¤çš„å›¾åƒç”Ÿæˆè¿‡ç¨‹æ—¶ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æé«˜äº†åˆ°æ¥è¿‘å®Œå…¨åˆ›æ–°å¼çš„å…¶ä»–é«˜è´¨é‡äº§å“ä¾‹å¦‚å¦‚å½“ä¸‹çƒ­é—¨çš„æ‰©æ•£æ¨¡å‹ä¹‹æŠ€æœ¯æŒ‡æ ‡äº¦æ˜¯å°–ç«¯å¼ºå¤§ç«äº‰çš„å®åŠ›ä½¼ä½¼è€…ä¸ä»…è¶…è¿‡äº†å½“å‰å…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹AiM&#x2F;VAR 0.25&#x2F;0.28 FIDçš„æˆç»©ä¹Ÿè¶…è¿‡äº†æµè¡Œçš„æ‰©æ•£æ¨¡å‹LDM&#x2F;DiTçš„FIDå¾—åˆ†åˆ†åˆ«ä¸ºé¢†å…ˆä¸šç•Œæ ‡æ†çš„FIDå¾—åˆ†åœ¨é›¶æ­¥è½¬ç§»è‡³ImageNet 512x512åŸºå‡†æµ‹è¯•æ—¶æˆ‘ä»¬çš„1.0Bæ¨¡å‹åœ¨æˆç»©æ–¹é¢èƒ½ä¸ç»è¿‡å……åˆ†ç›‘ç£è®­ç»ƒçš„VAR 2.3Bæ¨¡å‹ç«äº‰å¯è§FlexVARå…·å¤‡é«˜åº¦ç«äº‰æ€§å’Œé€‚åº”å„ç§å¤æ‚ç¯å¢ƒçš„èƒ½åŠ› </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>FlexVARæŒ‘æˆ˜äº†è§†è§‰è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ®‹å·®é¢„æµ‹èŒƒå¼ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>FlexVARé€šè¿‡ç»“åˆåœ°é¢çœŸå®é¢„æµ‹ä¿ƒè¿›è‡ªå›å½’å­¦ä¹ ï¼Œä½¿å›¾åƒç”Ÿæˆæ›´å…·çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥åœ¨å„ç§åˆ†è¾¨ç‡å’Œæ¯”ä¾‹å°ºä¸‹ç”Ÿæˆå›¾åƒï¼Œç”šè‡³è¶…è¿‡è®­ç»ƒå›¾åƒçš„åˆ†è¾¨ç‡ã€‚</li>
<li>FlexVARæ”¯æŒå¤šç§å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒç»†åŒ–ã€å†…å¤–ç»˜ç”»å’Œå›¾åƒæ‰©å±•ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒçš„è‡ªå›å½’æ­¥éª¤è¿›è¡Œè°ƒæ•´ï¼Œå¯å®ç°æ›´å¿«çš„æ¨ç†æˆ–æ›´é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>1.0Bæ¨¡å‹çš„æ€§èƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬ImageNet 256x256å’ŒImageNet 512x512ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d313d3bf3382a3a2fa82d4d20e5d3991.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-232cc635b709a84d8e9682dd524b1826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08994cdb4b0bc1d70bc1f331c48f9840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85927bfd7ab9b2ae1f463d6b79f3d02b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa24980d79716729fd5b3f308b3b124f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7b3ae8c4e2c8113315d6c953f46c7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b43d5f45b8e1f93c83ff8517d1cc61be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Explainable-Multi-modal-Wound-Infection-Classification-from-Images-Augmented-with-Generated-Captions"><a href="#Explainable-Multi-modal-Wound-Infection-Classification-from-Images-Augmented-with-Generated-Captions" class="headerlink" title="Explainable, Multi-modal Wound Infection Classification from Images   Augmented with Generated Captions"></a>Explainable, Multi-modal Wound Infection Classification from Images   Augmented with Generated Captions</h2><p><strong>Authors:Palawat Busaranuvong, Emmanuel Agu, Reza Saadati Fard, Deepak Kumar, Shefalika Gautam, Bengisu Tulu, Diane Strong</strong></p>
<p>Infections in Diabetic Foot Ulcers (DFUs) can cause severe complications, including tissue death and limb amputation, highlighting the need for accurate, timely diagnosis. Previous machine learning methods have focused on identifying infections by analyzing wound images alone, without utilizing additional metadata such as medical notes. In this study, we aim to improve infection detection by introducing Synthetic Caption Augmented Retrieval for Wound Infection Detection (SCARWID), a novel deep learning framework that leverages synthetic textual descriptions to augment DFU images. SCARWID consists of two components: (1) Wound-BLIP, a Vision-Language Model (VLM) fine-tuned on GPT-4o-generated descriptions to synthesize consistent captions from images; and (2) an Image-Text Fusion module that uses cross-attention to extract cross-modal embeddings from an image and its corresponding Wound-BLIP caption. Infection status is determined by retrieving the top-k similar items from a labeled support set. To enhance the diversity of training data, we utilized a latent diffusion model to generate additional wound images. As a result, SCARWID outperformed state-of-the-art models, achieving average sensitivity, specificity, and accuracy of 0.85, 0.78, and 0.81, respectively, for wound infection classification. Displaying the generated captions alongside the wound images and infection detection results enhances interpretability and trust, enabling nurses to align SCARWID outputs with their medical knowledge. This is particularly valuable when wound notes are unavailable or when assisting novice nurses who may find it difficult to identify visual attributes of wound infection. </p>
<blockquote>
<p>ç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰çš„æ„ŸæŸ“å¯å¼•èµ·ä¸¥é‡çš„å¹¶å‘ç—‡ï¼ŒåŒ…æ‹¬ç»„ç»‡æ­»äº¡å’Œè‚¢ä½“æˆªè‚¢ï¼Œè¿™çªæ˜¾äº†å‡†ç¡®åŠæ—¶è¯Šæ–­çš„å¿…è¦æ€§ã€‚ä»¥å¾€çš„æœºå™¨å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾§é‡äºé€šè¿‡åˆ†æä¼¤å£å›¾åƒæ¥è¯†åˆ«æ„ŸæŸ“ï¼Œè€Œæ²¡æœ‰åˆ©ç”¨é¢å¤–çš„å…ƒæ•°æ®ï¼Œå¦‚åŒ»ç–—è®°å½•ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å¼•å…¥åˆæˆå­—å¹•å¢å¼ºæ£€ç´¢ä¼¤å£æ„ŸæŸ“æ£€æµ‹ï¼ˆSCARWIDï¼‰æŠ€æœ¯æ¥æé«˜æ„ŸæŸ“æ£€æµ‹æ°´å¹³ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨åˆæˆæ–‡æœ¬æè¿°æ¥å¢å¼ºDFUå›¾åƒã€‚SCARWIDç”±ä¸¤ä¸ªç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰Wound-BLIPï¼Œä¸€ä¸ªç»è¿‡GPT-4oç”Ÿæˆæè¿°å¾®è°ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œèƒ½å¤Ÿä»å›¾åƒä¸­åˆæˆä¸€è‡´çš„æ ‡é¢˜ï¼›ï¼ˆ2ï¼‰å›¾åƒæ–‡æœ¬èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ä»å›¾åƒåŠå…¶ç›¸åº”çš„Wound-BLIPæ ‡é¢˜ä¸­æå–è·¨æ¨¡æ€åµŒå…¥ã€‚æ„ŸæŸ“çŠ¶æ€æ˜¯é€šè¿‡ä»æ ‡è®°çš„æ”¯æŒé›†ä¸­æ£€ç´¢å‰kä¸ªç›¸ä¼¼é¡¹æ¥ç¡®å®šçš„ã€‚ä¸ºäº†å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆäº†é¢å¤–çš„ä¼¤å£å›¾åƒã€‚å› æ­¤ï¼ŒSCARWIDè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨ä¼¤å£æ„ŸæŸ“åˆ†ç±»æ–¹é¢å–å¾—äº†å¹³å‡çµæ•åº¦ã€ç‰¹å¼‚æ€§å’Œå‡†ç¡®æ€§åˆ†åˆ«ä¸º0.85ã€0.78å’Œ0.81ã€‚å°†ç”Ÿæˆçš„æ ‡é¢˜ä¸ä¼¤å£å›¾åƒå’Œæ„ŸæŸ“æ£€æµ‹ç»“æœä¸€èµ·æ˜¾ç¤ºï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§å’Œä¿¡ä»»åº¦ï¼Œä½¿æŠ¤å£«èƒ½å¤Ÿå°†SCARWIDè¾“å‡ºä¸ä»–ä»¬çš„åŒ»å­¦çŸ¥è¯†ç›¸ç»“åˆã€‚è¿™åœ¨ä¼¤å£è®°å½•ä¸å¯ç”¨æˆ–è¾…åŠ©æ–°æ‰‹æŠ¤å£«æ—¶å°¤å…¶æœ‰ä»·å€¼ï¼Œä»–ä»¬å¯èƒ½éš¾ä»¥è¯†åˆ«ä¼¤å£æ„ŸæŸ“çš„å¯è§†å±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20277v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶SCARWIDï¼Œç”¨äºç³–å°¿ç—…è¶³æºƒç–¡æ„ŸæŸ“æ£€æµ‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åˆæˆæ–‡æœ¬æè¿°æ¥å¢å¼ºDFUå›¾åƒï¼Œé€šè¿‡Wound-BLIPæ¨¡å‹å’Œå›¾åƒ-æ–‡æœ¬èåˆæ¨¡å—ï¼Œå®ç°æ„ŸæŸ“çŠ¶æ€çš„åˆ¤å®šã€‚åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¢å¼ºè®­ç»ƒæ•°æ®å¤šæ ·æ€§ã€‚SCARWIDè¾ƒå…ˆè¿›æ¨¡å‹æœ‰æ›´ä½³è¡¨ç°ï¼Œåˆ†ç±»æ•æ„Ÿåº¦ã€ç‰¹å¼‚åº¦ã€å‡†ç¡®åº¦åˆ†åˆ«ä¸º0.85ã€0.78å’Œ0.81ã€‚æ­¤æ¡†æ¶èƒ½æé«˜è§£è¯»æ€§å’Œä¿¡ä»»åº¦ï¼Œå°¤å…¶åœ¨ç¼ºä¹ä¼¤å£è®°å½•æˆ–è¾…åŠ©æ–°æ‰‹æŠ¤å£«æ—¶æ›´æ˜¾ä»·å€¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SCARWIDæ˜¯é¦–ä¸ªç»“åˆåˆæˆæ–‡æœ¬æè¿°å’ŒDFUå›¾åƒæ¥æ£€æµ‹æ„ŸæŸ“çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>Wound-BLIPæ¨¡å‹ç”¨äºä»å›¾åƒä¸­åˆæˆä¸€è‡´çš„æ–‡æœ¬æè¿°ã€‚</li>
<li>å›¾åƒ-æ–‡æœ¬èåˆæ¨¡å—é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶æå–å›¾åƒå’Œå…¶å¯¹åº”æè¿°çš„å¤šæ¨¡å¼åµŒå…¥ã€‚</li>
<li>åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å¢å¼ºè®­ç»ƒæ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>SCARWIDåœ¨æ„ŸæŸ“åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºé«˜æ•æ„Ÿåº¦ã€ç‰¹å¼‚åº¦å’Œå‡†ç¡®åº¦ã€‚</li>
<li>æ˜¾ç¤ºç”Ÿæˆæè¿°å’Œæ„ŸæŸ“æ£€æµ‹ç»“æœï¼Œèƒ½æé«˜æ¨¡å‹çš„è§£è¯»æ€§å’Œä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cceb6384634992ae1868222a4b453a9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfb08541f49da1e465151a932efd606c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-371bd1359b2de6157fd909963d2843b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ddde4002619ad0c96261c86bba21347.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Attention-Distillation-A-Unified-Approach-to-Visual-Characteristics-Transfer"><a href="#Attention-Distillation-A-Unified-Approach-to-Visual-Characteristics-Transfer" class="headerlink" title="Attention Distillation: A Unified Approach to Visual Characteristics   Transfer"></a>Attention Distillation: A Unified Approach to Visual Characteristics   Transfer</h2><p><strong>Authors:Yang Zhou, Xu Gao, Zichong Chen, Hui Huang</strong></p>
<p>Recent advances in generative diffusion models have shown a notable inherent understanding of image style and semantics. In this paper, we leverage the self-attention features from pretrained diffusion networks to transfer the visual characteristics from a reference to generated images. Unlike previous work that uses these features as plug-and-play attributes, we propose a novel attention distillation loss calculated between the ideal and current stylization results, based on which we optimize the synthesized image via backpropagation in latent space. Next, we propose an improved Classifier Guidance that integrates attention distillation loss into the denoising sampling process, further accelerating the synthesis and enabling a broad range of image generation applications. Extensive experiments have demonstrated the extraordinary performance of our approach in transferring the examplesâ€™ style, appearance, and texture to new images in synthesis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xugao97/AttentionDistillation">https://github.com/xugao97/AttentionDistillation</a>. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºäº†å¯¹å›¾åƒé£æ ¼å’Œè¯­ä¹‰çš„æ˜¾è‘—å†…åœ¨ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£ç½‘ç»œçš„è‡ªæ³¨æ„åŠ›ç‰¹å¾ï¼Œå°†å‚è€ƒå›¾åƒçš„å¯è§†ç‰¹æ€§è½¬ç§»åˆ°ç”Ÿæˆå›¾åƒä¸Šã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œè¿™äº›ç‰¹å¾è¢«ç”¨ä½œå³æ’å³ç”¨çš„å±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç†æƒ³é£æ ¼åŒ–ç»“æœå’Œå½“å‰é£æ ¼åŒ–ç»“æœä¹‹é—´è®¡ç®—çš„æ–°å‹æ³¨æ„åŠ›è’¸é¦æŸå¤±ï¼ˆattention distillation lossï¼‰ï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´çš„åå‘ä¼ æ’­ä¼˜åŒ–åˆæˆå›¾åƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„Classifier Guidanceï¼Œå°†æ³¨æ„åŠ›è’¸é¦æŸå¤±é›†æˆåˆ°å»å™ªé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿåˆæˆè¿‡ç¨‹ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆåº”ç”¨ç¨‹åºã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°†ç¤ºä¾‹çš„é£æ ¼ã€å¤–è§‚å’Œçº¹ç†è½¬ç§»åˆ°åˆæˆå›¾åƒä¸­çš„æ–°å›¾åƒä¸Šçš„è¡¨ç°éå¸¸å‡ºè‰²ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xugao97/AttentionDistillation%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xugao97/AttentionDistillationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20235v1">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://github.com/xugao97/AttentionDistillation">https://github.com/xugao97/AttentionDistillation</a></p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘ï¼Œç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé£æ ¼å’Œè¯­ä¹‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£ç½‘ç»œçš„è‡ªæ³¨æ„åŠ›ç‰¹å¾ï¼Œå°†å‚è€ƒå›¾åƒçš„è§†è§‰ç‰¹å¾è½¬ç§»åˆ°ç”Ÿæˆå›¾åƒä¸Šã€‚ä¸åŒäºä»¥å¾€çš„å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç†æƒ³ä¸ç°å®é£æ ¼åŒ–ç»“æœä¹‹é—´è®¡ç®—æ³¨æ„åŠ›è’¸é¦æŸå¤±çš„æ–¹æ³•ï¼Œé€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–åˆæˆå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¹è¿›äº†åˆ†ç±»å™¨å¼•å¯¼ï¼Œå°†æ³¨æ„åŠ›è’¸é¦æŸå¤±é›†æˆåˆ°å»å™ªé‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿåˆæˆï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå›¾åƒç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆæ–°å›¾åƒæ—¶èƒ½å¤Ÿå‡ºè‰²åœ°è½¬ç§»ç¤ºä¾‹çš„é£æ ¼ã€å¤–è§‚å’Œçº¹ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å±•ç°å‡ºå¯¹å›¾åƒé£æ ¼å’Œè¯­ä¹‰çš„æ·±å…¥ç†è§£ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£ç½‘ç»œçš„è‡ªæ³¨æ„åŠ›ç‰¹å¾è¿›è¡Œå›¾åƒé£æ ¼è¿ç§»ã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›è’¸é¦æŸå¤±æ¥è®¡ç®—ç†æƒ³ä¸ç°å®é£æ ¼åŒ–ç»“æœä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–åˆæˆå›¾åƒã€‚</li>
<li>æ”¹è¿›çš„åˆ†ç±»å™¨å¼•å¯¼é›†æˆæ³¨æ„åŠ›è’¸é¦æŸå¤±ï¼ŒåŠ é€Ÿå›¾åƒåˆæˆã€‚</li>
<li>å¹¿æ³›çš„å›¾åƒç”Ÿæˆåº”ç”¨å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-da7cb95223e30da2dfb754c2b51fdbc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d4f3069c4a89ea3a8b8932c6f0a29aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5063c87481f27b428a037ad80f5827ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de6b81ee8236941c4a9ed58e6a76518f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e1e1d10cdde7b6afc8b15184f0750ec.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multimodal-Representation-Alignment-for-Image-Generation-Text-Image-Interleaved-Control-Is-Easier-Than-You-Think"><a href="#Multimodal-Representation-Alignment-for-Image-Generation-Text-Image-Interleaved-Control-Is-Easier-Than-You-Think" class="headerlink" title="Multimodal Representation Alignment for Image Generation: Text-Image   Interleaved Control Is Easier Than You Think"></a>Multimodal Representation Alignment for Image Generation: Text-Image   Interleaved Control Is Easier Than You Think</h2><p><strong>Authors:Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang</strong></p>
<p>The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX. </p>
<blockquote>
<p>å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸæ­£åœ¨å‡ºç°èåˆäº†å¼ºå¤§æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚CLIPå’ŒT5ï¼‰ä¸Diffusion Transformerä¸»å¹²çš„ç»Ÿä¸€æ¡†æ¶ã€‚å°½ç®¡äººä»¬å·²ç»å°è¯•ä½¿ç”¨Cannyå’Œæ·±åº¦å›¾ç­‰é™„åŠ æ¡ä»¶æ¥æ§åˆ¶è¾“å‡ºå›¾åƒï¼Œä½†å¯¹äºä»»æ„æ–‡æœ¬-å›¾åƒäº¤é”™æ§åˆ¶çš„ç»¼åˆæ¡†æ¶ä»ç„¶ç¼ºä¹ã€‚å½“è¯•å›¾åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åˆå¹¶å¤šä¸ªå›¾åƒçš„æ¦‚å¿µæˆ–è§†è§‰å…ƒç´ æ—¶ï¼Œè¿™ä¸€å·®è·å°¤ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬è¿›è¡Œäº†åˆæ­¥å®éªŒï¼Œå‘ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å…±äº«è¡¨ç¤ºç©ºé—´ï¼Œå…¶ä¸­å›¾åƒå’Œæ–‡æœ¬å¯ä»¥å¾ˆå¥½åœ°å¯¹é½ï¼Œä»¥ä½œä¸ºå¤–éƒ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Dream Engineï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„ä»»æ„æ–‡æœ¬-å›¾åƒäº¤é”™æ§åˆ¶è€Œè®¾è®¡çš„é«˜æ•ˆä¸”ç»Ÿä¸€çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚SD3.5ï¼‰çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡èå…¥é€šç”¨å¤šæ¨¡æ€ä¿¡æ¯ç¼–ç å™¨ï¼ˆå¦‚QwenVLï¼‰æ¥æ›¿æ¢åŸå§‹ä»…æ–‡æœ¬ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬è”åˆæ–‡æœ¬-å›¾åƒå¯¹é½å’Œå¤šæ¨¡æ€äº¤é”™æŒ‡ä»¤è°ƒæ•´ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§è®­ç»ƒæ–¹æ³•æœ‰æ•ˆï¼Œåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°0.69çš„æ•´ä½“åˆ†æ•°ï¼Œä¸SD3.5å’ŒFLUXç­‰å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20172v1">PDF</a> 13 pages, 9 figures, codebase in   <a target="_blank" rel="noopener" href="https://github.com/chenllliang/DreamEngine">https://github.com/chenllliang/DreamEngine</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸçš„æ–°å‘å±•ï¼Œå¼ºè°ƒäº†ç»Ÿä¸€æ¡†æ¶çš„é‡è¦æ€§ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¼ºå¤§çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚CLIPå’ŒT5ï¼‰ä¸Diffusion Transformeréª¨å¹²ç½‘ã€‚æ–‡ç« æŒ‡å‡ºå°½ç®¡å·²æœ‰å°è¯•é€šè¿‡é™„åŠ æ¡ä»¶ï¼ˆå¦‚è¾¹ç¼˜å’Œæ·±åº¦å›¾ï¼‰æ¥æ§åˆ¶è¾“å‡ºå›¾åƒï¼Œä½†ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥å®ç°ä»»æ„çš„æ–‡æœ¬-å›¾åƒäº¤æ›¿æ§åˆ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†Dream Engineæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æä¾›å…±äº«è¡¨ç¤ºç©ºé—´ï¼Œå®ç°å›¾åƒå’Œæ–‡æœ¬çš„ç²¾å‡†å¯¹é½ï¼Œä½œä¸ºå¤–éƒ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è®­ç»ƒæ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œè¾¾åˆ°äº†GenEvalåŸºå‡†æµ‹è¯•çš„æ•´ä½“å¾—åˆ†0.69ï¼Œä¸SD3.5å’ŒFLUXç­‰å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸæ­£åœ¨å‘å±•ç»Ÿä¸€æ¡†æ¶ï¼Œé›†æˆå¼ºå¤§çš„æ–‡æœ¬ç¼–ç å™¨å’ŒDiffusion TransformeræŠ€æœ¯ã€‚</li>
<li>å°½ç®¡æœ‰é™„åŠ æ¡ä»¶çš„è¾“å‡ºå›¾åƒæ§åˆ¶å°è¯•ï¼Œä½†ç¼ºä¹ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥å®ç°ä»»æ„çš„æ–‡æœ¬-å›¾åƒäº¤æ›¿æ§åˆ¶ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æä¾›å…±äº«è¡¨ç¤ºç©ºé—´ï¼Œä½¿å›¾åƒå’Œæ–‡æœ¬å¯¹é½è‰¯å¥½ï¼Œå¯ä½œä¸ºå¤–éƒ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ã€‚</li>
<li>Dream Engineæ¡†æ¶è¢«æå‡ºï¼Œç”¨äºä»»æ„æ–‡æœ¬-å›¾åƒäº¤æ›¿æ§åˆ¶åœ¨å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>Dream Engineæ¡†æ¶å»ºç«‹åœ¨å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚SD3.5ï¼‰ä¹‹ä¸Šï¼Œèå…¥äº†å¤šåŠŸèƒ½å¤šæ¨¡æ€ä¿¡æ¯ç¼–ç å™¨å¦‚QwenVLã€‚</li>
<li>è¯¥è®­ç»ƒæ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œè¾¾åˆ°äº†GenEvalåŸºå‡†æµ‹è¯•çš„æ•´ä½“å¾—åˆ†0.69ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1581d072940f1373180b798472c85dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94a8953428225ab571e72725710184c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e394fa1d9fdd8480802c86c26e672d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7a4f761af24f014afdb099e8a5bbbfc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0129785a01fc209ec03625c3396a34f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-augmentations-for-improved-cardiac-ultrasound-segmentation-using-diffusion-models"><a href="#Generative-augmentations-for-improved-cardiac-ultrasound-segmentation-using-diffusion-models" class="headerlink" title="Generative augmentations for improved cardiac ultrasound segmentation   using diffusion models"></a>Generative augmentations for improved cardiac ultrasound segmentation   using diffusion models</h2><p><strong>Authors:Gilles Van De Vyver, Aksel Try Lenz, Erik Smistad, Sindre Hellum Olaisen, BjÃ¸rnar Grenne, Espen Holte, HÃ¥avard Dalen, Lasse LÃ¸vstakken</strong></p>
<p>One of the main challenges in current research on segmentation in cardiac ultrasound is the lack of large and varied labeled datasets and the differences in annotation conventions between datasets. This makes it difficult to design robust segmentation models that generalize well to external datasets. This work utilizes diffusion models to create generative augmentations that can significantly improve diversity of the dataset and thus the generalisability of segmentation models without the need for more annotated data. The augmentations are applied in addition to regular augmentations. A visual test survey showed that experts cannot clearly distinguish between real and fully generated images. Using the proposed generative augmentations, segmentation robustness was increased when training on an internal dataset and testing on an external dataset with an improvement of over 20 millimeters in Hausdorff distance. Additionally, the limits of agreement for automatic ejection fraction estimation improved by up to 20% of absolute ejection fraction value on out of distribution cases. These improvements come exclusively from the increased variation of the training data using the generative augmentations, without modifying the underlying machine learning model. The augmentation tool is available as an open source Python library at <a target="_blank" rel="noopener" href="https://github.com/GillesVanDeVyver/EchoGAINS">https://github.com/GillesVanDeVyver/EchoGAINS</a>. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹å¿ƒè„è¶…å£°åˆ†å‰²ç ”ç©¶çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„æ ‡è®°æ•°æ®é›†ä»¥åŠä¸åŒæ•°æ®é›†ä¹‹é—´æ ‡æ³¨è§„èŒƒçš„ä¸åŒã€‚è¿™ä½¿å¾—è®¾è®¡èƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°å¤–éƒ¨æ•°æ®é›†çš„ç¨³å¥åˆ†å‰²æ¨¡å‹å˜å¾—å›°éš¾ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆ›å»ºç”Ÿæˆå¢å¼ºæŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œä»è€Œåœ¨ä¸éœ€è¦æ›´å¤šæ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹æé«˜åˆ†å‰²æ¨¡å‹çš„ä¸€èˆ¬æ€§ã€‚é™¤äº†å¸¸è§„å¢å¼ºæŠ€æœ¯å¤–ï¼Œè¿˜åº”ç”¨äº†è¿™äº›å¢å¼ºæŠ€æœ¯ã€‚è§†è§‰æµ‹è¯•è°ƒæŸ¥è¡¨æ˜ï¼Œä¸“å®¶æ— æ³•æ¸…æ¥šåŒºåˆ†çœŸå®å’Œå®Œå…¨ç”Ÿæˆçš„å›¾åƒã€‚åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œä½¿ç”¨æ‰€æå‡ºçš„ç”Ÿæˆå¢å¼ºæŠ€æœ¯æé«˜äº†åˆ†å‰²çš„ç¨³å¥æ€§ï¼Œè±ªæ–¯å¤šå¤«è·ç¦»æé«˜äº†20æ¯«ç±³ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ¡ˆä¾‹ä¸­ï¼Œè‡ªåŠ¨å°„è¡€åˆ†æ•°ä¼°è®¡çš„åè®®é™åˆ¶æé«˜äº†é«˜è¾¾ç»å¯¹å°„è¡€åˆ†æ•°å€¼çš„20%ã€‚è¿™äº›æ”¹è¿›å®Œå…¨æ¥è‡ªäºä½¿ç”¨ç”Ÿæˆå¢å¼ºæŠ€æœ¯åè®­ç»ƒæ•°æ®å¤šæ ·æ€§çš„å¢åŠ ï¼Œè€Œæ²¡æœ‰ä¿®æ”¹åŸºæœ¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å¢å¼ºå·¥å…·å¯ä½œä¸ºå¼€æºPythonåº“åœ¨<a target="_blank" rel="noopener" href="https://github.com/GillesVanDeVyver/EchoGAINS%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GillesVanDeVyver/EchoGAINSä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20100v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å¿ƒè„è¶…å£°åˆ†å‰²ç ”ç©¶ä¸­çš„åº”ç”¨æ‘˜è¦ï¼šå½“å‰å¿ƒè„è¶…å£°åˆ†å‰²ç ”ç©¶é¢ä¸´ç¼ºä¹å¤§è§„æ¨¡å¤šæ ·åŒ–æ ‡æ³¨æ•°æ®é›†å’Œæ ‡æ³¨è§„èŒƒå·®å¼‚çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆ›å»ºç”Ÿæˆå¢å¼ºæŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œä»è€Œæå‡åˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€æ›´å¤šæ ‡æ³¨æ•°æ®ã€‚ç”Ÿæˆçš„å¢å¼ºæŠ€æœ¯ä¸å¸¸è§„å¢å¼ºæŠ€æœ¯ç›¸ç»“åˆåº”ç”¨ã€‚è§†è§‰æµ‹è¯•è°ƒæŸ¥æ˜¾ç¤ºï¼Œä¸“å®¶æ— æ³•æ˜ç¡®åŒºåˆ†çœŸå®ä¸å®Œå…¨ç”Ÿæˆçš„å›¾åƒã€‚é€šè¿‡ä½¿ç”¨æ‰€æå‡ºçš„ç”Ÿæˆå¢å¼ºæŠ€æœ¯ï¼Œåœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œåˆ†å‰²ç¨³å¥æ€§æœ‰æ‰€æé«˜ï¼Œè±ªæ–¯å¤šå¤«è·ç¦»æé«˜äº†è¶…è¿‡20æ¯«ç±³ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨å°„è¡€åˆ†æ•°ä¼°è®¡çš„åè®®é™åˆ¶åœ¨è¶…å‡ºåˆ†å¸ƒçš„æƒ…å†µä¸‹æœ€å¤šæé«˜äº†20%ã€‚è¿™äº›æ”¹è¿›å®Œå…¨æ¥è‡ªäºä½¿ç”¨ç”Ÿæˆå¢å¼ºæŠ€æœ¯å¢åŠ çš„è®­ç»ƒæ•°æ®å¤šæ ·æ€§ï¼Œæ— éœ€ä¿®æ”¹åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¯¥å¢å¼ºå·¥å…·å¯ä½œä¸ºå¼€æºPythonåº“ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¿ƒè„è¶…å£°åˆ†å‰²ç ”ç©¶é¢ä¸´ç¼ºä¹å¤§è§„æ¨¡å¤šæ ·åŒ–æ ‡æ³¨æ•°æ®é›†å’Œæ ‡æ³¨è§„èŒƒå·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºåˆ›å»ºç”Ÿæˆå¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§å’Œåˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å›¾åƒéš¾ä»¥åŒºåˆ†ï¼Œé€šè¿‡è§†è§‰æµ‹è¯•è°ƒæŸ¥å¾—åˆ°äº†è¯å®ã€‚</li>
<li>åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒå¹¶åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šæµ‹è¯•æ—¶ï¼Œåˆ†å‰²ç¨³å¥æ€§æ˜¾è‘—æé«˜ã€‚</li>
<li>è±ªæ–¯å¤šå¤«è·ç¦»æé«˜äº†è¶…è¿‡20æ¯«ç±³ï¼Œè¿™æ˜¯åˆ†å‰²æ€§èƒ½çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ã€‚</li>
<li>è‡ªåŠ¨å°„è¡€åˆ†æ•°ä¼°è®¡çš„åè®®é™åˆ¶åœ¨è¶…å‡ºåˆ†å¸ƒçš„æƒ…å†µä¸‹æœ€å¤šæé«˜äº†20%ï¼Œè¡¨æ˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df3a4b56e3908a74de8bd2fe6a89115f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b6a586a0f1e9a12edc35e255840aabc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc6d42169392e0610a1910d8e7bdeee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07caa2be4010d46afd254b41b49c07a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73a1a953c5370502b1c8f485951ec1e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-843b21b0a92ca0c57c00e4b2a2676f4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9111d72fee06ad054a1d2fe5d82e3440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e4fc36e0a9f8e2072fe26408c6d4108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3001f6139fee3d8b619817c96088acc8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Image-Referenced-Sketch-Colorization-Based-on-Animation-Creation-Workflow"><a href="#Image-Referenced-Sketch-Colorization-Based-on-Animation-Creation-Workflow" class="headerlink" title="Image Referenced Sketch Colorization Based on Animation Creation   Workflow"></a>Image Referenced Sketch Colorization Based on Animation Creation   Workflow</h2><p><strong>Authors:Dingkun Yan, Xinrui Wang, Zhuoru Li, Suguru Saito, Yusuke Iwasawa, Yutaka Matsuo, Jiaxian Guo</strong></p>
<p>Sketch colorization plays an important role in animation and digital illustration production tasks. However, existing methods still meet problems in that text-guided methods fail to provide accurate color and style reference, hint-guided methods still involve manual operation, and image-referenced methods are prone to cause artifacts. To address these limitations, we propose a diffusion-based framework inspired by real-world animation production workflows. Our approach leverages the sketch as the spatial guidance and an RGB image as the color reference, and separately extracts foreground and background from the reference image with spatial masks. Particularly, we introduce a split cross-attention mechanism with LoRA (Low-Rank Adaptation) modules. They are trained separately with foreground and background regions to control the corresponding embeddings for keys and values in cross-attention. This design allows the diffusion model to integrate information from foreground and background independently, preventing interference and eliminating the spatial artifacts. During inference, we design switchable inference modes for diverse use scenarios by changing modules activated in the framework. Extensive qualitative and quantitative experiments, along with user studies, demonstrate our advantages over existing methods in generating high-qualigy artifact-free results with geometric mismatched references. Ablation studies further confirm the effectiveness of each component. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> tellurion-kanata&#x2F;colorizeDiffusion. </p>
<blockquote>
<p>è‰å›¾ç€è‰²åœ¨åŠ¨ç”»å’Œæ•°å­—æ’ç”»åˆ¶ä½œä»»åŠ¡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•æ— æ³•æä¾›å‡†ç¡®çš„é¢œè‰²å’Œé£æ ¼å‚è€ƒï¼Œæç¤ºå¼•å¯¼çš„æ–¹æ³•ä»éœ€è¦æ‰‹åŠ¨æ“ä½œï¼Œè€Œå›¾åƒå‚è€ƒçš„æ–¹æ³•åˆ™å®¹æ˜“äº§ç”Ÿä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå—ç°å®åŠ¨ç”»åˆ¶ä½œå·¥ä½œæµç¨‹å¯å‘çš„åŸºäºæ‰©æ•£çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è‰å›¾ä½œä¸ºç©ºé—´æŒ‡å¯¼ï¼Œå¹¶ä½¿ç”¨RGBå›¾åƒä½œä¸ºé¢œè‰²å‚è€ƒï¼Œä½¿ç”¨ç©ºé—´æ©è†œä»å‚è€ƒå›¾åƒä¸­åˆ†åˆ«æå–å‰æ™¯å’ŒèƒŒæ™¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¸¦æœ‰LoRAï¼ˆä½ç§©é€‚åº”ï¼‰æ¨¡å—çš„åˆ†å‰²äº¤å‰æ³¨æ„æœºåˆ¶ã€‚å®ƒä»¬åˆ†åˆ«ä¸å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸè¿›è¡Œè®­ç»ƒï¼Œä»¥æ§åˆ¶äº¤å‰æ³¨æ„ä¸­çš„é”®å’Œå€¼çš„å¯¹åº”åµŒå…¥ã€‚è¿™ç§è®¾è®¡å…è®¸æ‰©æ•£æ¨¡å‹ç‹¬ç«‹åœ°æ•´åˆå‰æ™¯å’ŒèƒŒæ™¯çš„ä¿¡æ¯ï¼Œé˜²æ­¢å¹²æ‰°ï¼Œæ¶ˆé™¤ç©ºé—´ä¼ªå½±ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ”¹å˜æ¡†æ¶ä¸­æ¿€æ´»çš„æ¨¡å—ï¼Œä¸ºä¸åŒçš„ä½¿ç”¨åœºæ™¯è®¾è®¡äº†å¯åˆ‡æ¢çš„æ¨ç†æ¨¡å¼ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒï¼Œä»¥åŠç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡ã€æ— ä¼ªå½±çš„ç»“æœæ–¹é¢ï¼Œå…·æœ‰å‡ ä½•ä¸åŒ¹é…å¼•ç”¨çš„ä¼˜åŠ¿ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tellurion-kanata/colorizeDiffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tellurion-kanata/colorizeDiffusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19937v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ¨ç”»ä¸æ•°å­—æ’ç”»ç”Ÿäº§ä¸­çš„è‰å›¾å½©è‰²åŒ–æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‰å›¾ä½œä¸ºç©ºé—´æŒ‡å¯¼ï¼ŒRGBå›¾åƒä½œä¸ºé¢œè‰²å‚è€ƒï¼Œé€šè¿‡ç©ºé—´æ©è†œåˆ†åˆ«æå–å‰æ™¯å’ŒèƒŒæ™¯ã€‚å¼•å…¥å¸¦æœ‰LoRAæ¨¡å—çš„åˆ†å‰²äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œåˆ†åˆ«å¯¹å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸè¿›è¡Œè®­ç»ƒï¼Œæ§åˆ¶äº¤å‰æ³¨æ„åŠ›ä¸­çš„é”®å’Œå€¼åµŒå…¥ã€‚æ­¤æ–¹æ³•è®¾è®¡çµæ´»ï¼Œæ”¯æŒå¤šç§åº”ç”¨åœºæ™¯ï¼Œå¹¶é€šè¿‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜äº†å…¶åœ¨å‡ ä½•ä¸åŒ¹é…å‚è€ƒä¸‹ç”Ÿæˆé«˜è´¨é‡ã€æ— ä¼ªå½±ç»“æœçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„è‰å›¾å½©è‰²åŒ–æ–°æ–¹æ³•ã€‚</li>
<li>ç»“åˆçœŸå®åŠ¨ç”»ç”Ÿäº§æµç¨‹å¯å‘è®¾è®¡æ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨è‰å›¾ä½œä¸ºç©ºé—´æŒ‡å¯¼ï¼ŒRGBå›¾åƒä½œä¸ºé¢œè‰²å‚è€ƒï¼Œé‡‡ç”¨ç©ºé—´æ©è†œåˆ†ç¦»å‰æ™¯ä¸èƒŒæ™¯ã€‚</li>
<li>å¼•å…¥å¸¦æœ‰LoRAæ¨¡å—çš„åˆ†å‰²äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®¾è®¡çµæ´»å¤šå˜çš„æ¨ç†æ¨¡å¼ä»¥é€‚åº”ä¸åŒåº”ç”¨åœºæ™¯ã€‚</li>
<li>é€šè¿‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶éªŒè¯äº†æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ã€æ— ä¼ªå½±ç»“æœæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5c9f56999b7c483745be6e83dcf2f3e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5175c9720dab212052f8d8acb343c9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5a05e37b7a71b97e243971a82d3542c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c07252fcee6b11abd39129608c4f9f2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94321b648712c394aede20743d945edc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-Fidelity-Relightable-Monocular-Portrait-Animation-with-Lighting-Controllable-Video-Diffusion-Model"><a href="#High-Fidelity-Relightable-Monocular-Portrait-Animation-with-Lighting-Controllable-Video-Diffusion-Model" class="headerlink" title="High-Fidelity Relightable Monocular Portrait Animation with   Lighting-Controllable Video Diffusion Model"></a>High-Fidelity Relightable Monocular Portrait Animation with   Lighting-Controllable Video Diffusion Model</h2><p><strong>Authors:Mingtao Guo, Guanyu Xing, Yanli Liu</strong></p>
<p>Relightable portrait animation aims to animate a static reference portrait to match the head movements and expressions of a driving video while adapting to user-specified or reference lighting conditions. Existing portrait animation methods fail to achieve relightable portraits because they do not separate and manipulate intrinsic (identity and appearance) and extrinsic (pose and lighting) features. In this paper, we present a Lighting Controllable Video Diffusion model (LCVD) for high-fidelity, relightable portrait animation. We address this limitation by distinguishing these feature types through dedicated subspaces within the feature space of a pre-trained image-to-video diffusion model. Specifically, we employ the 3D mesh, pose, and lighting-rendered shading hints of the portrait to represent the extrinsic attributes, while the reference represents the intrinsic attributes. In the training phase, we employ a reference adapter to map the reference into the intrinsic feature subspace and a shading adapter to map the shading hints into the extrinsic feature subspace. By merging features from these subspaces, the model achieves nuanced control over lighting, pose, and expression in generated animations. Extensive evaluations show that LCVD outperforms state-of-the-art methods in lighting realism, image quality, and video consistency, setting a new benchmark in relightable portrait animation. </p>
<blockquote>
<p>é‡å…‰ç…§è‚–åƒåŠ¨ç”»æ—¨åœ¨ä½¿é™æ€å‚è€ƒè‚–åƒåŠ¨ç”»ä¸é©¾é©¶è§†é¢‘çš„å¤´éƒ¨åŠ¨ä½œå’Œè¡¨æƒ…ç›¸åŒ¹é…ï¼ŒåŒæ—¶é€‚åº”ç”¨æˆ·æŒ‡å®šçš„æˆ–å‚è€ƒçš„å…‰ç…§æ¡ä»¶ã€‚ç°æœ‰çš„è‚–åƒåŠ¨ç”»æ–¹æ³•æ— æ³•å®ç°é‡å…‰ç…§è‚–åƒï¼Œå› ä¸ºå®ƒä»¬ä¸èƒ½åˆ†ç¦»å’Œæ“ä½œå†…åœ¨ï¼ˆèº«ä»½å’Œå¤–è§‚ï¼‰å’Œå¤–åœ¨ï¼ˆå§¿åŠ¿å’Œç…§æ˜ï¼‰ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…‰ç…§å¯æ§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆLCVDï¼‰ï¼Œç”¨äºé«˜ä¿çœŸé‡å…‰ç…§è‚–åƒåŠ¨ç”»ã€‚æˆ‘ä»¬é€šè¿‡åŒºåˆ†ç‰¹å¾ç©ºé—´å†…ç‰¹å®šå­ç©ºé—´å†…çš„è¿™äº›ç‰¹å¾ç±»å‹æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¿™äº›å­ç©ºé—´ç”¨äºè¡¨ç¤ºé¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å†…åœ¨å’Œå¤–åœ¨å±æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨è‚–åƒçš„3Dç½‘æ ¼ã€å§¿åŠ¿å’Œå…‰ç…§æ¸²æŸ“çš„é˜´å½±æç¤ºæ¥è¡¨ç¤ºå¤–åœ¨å±æ€§ï¼Œè€Œå‚è€ƒå›¾åƒåˆ™è¡¨ç¤ºå†…åœ¨å±æ€§ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å‚è€ƒé€‚é…å™¨å°†å‚è€ƒå›¾åƒæ˜ å°„åˆ°å†…åœ¨ç‰¹å¾å­ç©ºé—´ï¼Œå¹¶ä½¿ç”¨ç€è‰²é€‚é…å™¨å°†é˜´å½±æç¤ºæ˜ å°„åˆ°å¤–åœ¨ç‰¹å¾å­ç©ºé—´ã€‚é€šè¿‡åˆå¹¶è¿™äº›å­ç©ºé—´ä¸­çš„ç‰¹å¾ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆçš„åŠ¨ç”»ä¸­å¯¹å…‰ç…§ã€å§¿åŠ¿å’Œè¡¨æƒ…å®ç°äº†å¾®å¦™çš„æ§åˆ¶ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼ŒLCVDåœ¨å…‰ç…§çœŸå®æ€§ã€å›¾åƒè´¨é‡å’Œè§†é¢‘ä¸€è‡´æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸ºå¯é‡å…‰ç…§è‚–åƒåŠ¨ç”»æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLCVDçš„ç¯å…‰å¯æ§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå®ç°é«˜ä¿çœŸã€å¯é‡å…‰ç…§çš„è‚–åƒåŠ¨ç”»ã€‚è¯¥æ¨¡å‹é€šè¿‡åŒºåˆ†é¢„è®­ç»ƒå›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾ç©ºé—´ä¸­çš„ç‰¹å®šå­ç©ºé—´æ¥åŒºåˆ†å†…åœ¨ï¼ˆèº«ä»½å’Œå¤–è§‚ï¼‰å’Œå¤–åœ¨ï¼ˆå§¿åŠ¿å’Œç…§æ˜ï¼‰ç‰¹å¾ï¼Œä»è€Œå®ç°å¯é‡å…‰ç…§çš„è‚–åƒåŠ¨ç”»ã€‚é€šè¿‡åˆå¹¶è¿™äº›å­ç©ºé—´ä¸­çš„ç‰¹å¾ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆçš„åŠ¨ç”»ä¸­å¯¹ç…§æ˜ã€å§¿åŠ¿å’Œè¡¨æƒ…å®ç°äº†å¾®å¦™æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†å¯é‡å…‰ç…§è‚–åƒåŠ¨ç”»çš„ç›®æ ‡ï¼Œå³å®ç°é™æ€å‚è€ƒè‚–åƒçš„åŠ¨ç”»ï¼Œä½¿å…¶ä¸é©±åŠ¨è§†é¢‘çš„å¤´éƒ¨åŠ¨ä½œå’Œè¡¨æƒ…ç›¸åŒ¹é…ï¼ŒåŒæ—¶é€‚åº”ç”¨æˆ·æŒ‡å®šçš„æˆ–å‚è€ƒçš„ç…§æ˜æ¡ä»¶ã€‚</li>
<li>å½“å‰è‚–åƒåŠ¨ç”»æ–¹æ³•çš„å±€é™æ€§åœ¨äºå®ƒä»¬æ— æ³•åˆ†ç¦»å’Œæ“ä½œå†…åœ¨å’Œå¤–åœ¨ç‰¹å¾ï¼Œå› æ­¤æ— æ³•å®ç°å¯é‡å…‰ç…§çš„è‚–åƒã€‚</li>
<li>LCVDæ¨¡å‹é€šè¿‡ä¸“é—¨çš„å­ç©ºé—´æ¥åŒºåˆ†å†…åœ¨å’Œå¤–åœ¨ç‰¹å¾ï¼Œè¿™äº›å­ç©ºé—´ä½äºé¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾ç©ºé—´ä¸­ã€‚</li>
<li>LCVDæ¨¡å‹ä½¿ç”¨3Dç½‘æ ¼ã€å§¿åŠ¿å’Œç…§æ˜æ¸²æŸ“ç€è‰²æç¤ºæ¥è¡¨ç¤ºè‚–åƒçš„å¤–åœ¨å±æ€§ï¼Œè€Œå‚è€ƒå›¾åƒåˆ™ä»£è¡¨å†…åœ¨å±æ€§ã€‚</li>
<li>åœ¨è®­ç»ƒé˜¶æ®µï¼ŒLCVDæ¨¡å‹ä½¿ç”¨å‚è€ƒé€‚é…å™¨å’Œç€è‰²é€‚é…å™¨æ¥æ˜ å°„å‚è€ƒå›¾åƒå’Œç€è‰²æç¤ºåˆ°ç›¸åº”çš„ç‰¹å¾å­ç©ºé—´ã€‚</li>
<li>é€šè¿‡åˆå¹¶è¿™äº›å­ç©ºé—´ä¸­çš„ç‰¹å¾ï¼ŒLCVDæ¨¡å‹å®ç°äº†å¯¹ç”ŸæˆåŠ¨ç”»ä¸­çš„ç…§æ˜ã€å§¿åŠ¿å’Œè¡¨æƒ…çš„å¾®å¦™æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e780f7e868dee52c6b0bbfc4ad668769.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-726f523d5e225c00b529d44658044944.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="C-Drag-Chain-of-Thought-Driven-Motion-Controller-for-Video-Generation"><a href="#C-Drag-Chain-of-Thought-Driven-Motion-Controller-for-Video-Generation" class="headerlink" title="C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation"></a>C-Drag: Chain-of-Thought Driven Motion Controller for Video Generation</h2><p><strong>Authors:Yuhao Li, Mirana Claire Angel, Salman Khan, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan</strong></p>
<p>Trajectory-based motion control has emerged as an intuitive and efficient approach for controllable video generation. However, the existing trajectory-based approaches are usually limited to only generating the motion trajectory of the controlled object and ignoring the dynamic interactions between the controlled object and its surroundings. To address this limitation, we propose a Chain-of-Thought-based motion controller for controllable video generation, named C-Drag. Instead of directly generating the motion of some objects, our C-Drag first performs object perception and then reasons the dynamic interactions between different objects according to the given motion control of the objects. Specifically, our method includes an object perception module and a Chain-of-Thought-based motion reasoning module. The object perception module employs visual language models to capture the position and category information of various objects within the image. The Chain-of-Thought-based motion reasoning module takes this information as input and conducts a stage-wise reasoning process to generate motion trajectories for each of the affected objects, which are subsequently fed to the diffusion model for video synthesis. Furthermore, we introduce a new video object interaction (VOI) dataset to evaluate the generation quality of motion controlled video generation methods. Our VOI dataset contains three typical types of interactions and provides the motion trajectories of objects that can be used for accurate performance evaluation. Experimental results show that C-Drag achieves promising performance across multiple metrics, excelling in object motion control. Our benchmark, codes, and models will be available at <a target="_blank" rel="noopener" href="https://github.com/WesLee88524/C-Drag-Official-Repo">https://github.com/WesLee88524/C-Drag-Official-Repo</a>. </p>
<blockquote>
<p>åŸºäºè½¨è¿¹çš„è¿åŠ¨æ§åˆ¶å·²ä½œä¸ºä¸€ç§ç›´è§‚è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè½¨è¿¹çš„æ–¹æ³•é€šå¸¸ä»…é™äºç”Ÿæˆå—æ§å¯¹è±¡çš„è¿åŠ¨è½¨è¿¹ï¼Œè€Œå¿½ç•¥äº†å—æ§å¯¹è±¡ä¸å…¶å‘¨å›´ç¯å¢ƒä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ€ç»´é“¾çš„è¿åŠ¨æ§åˆ¶å™¨ï¼Œç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆï¼Œåä¸ºC-Dragã€‚ä¸ç›´æ¥ç”ŸæˆæŸäº›å¯¹è±¡çš„è¿åŠ¨ä¸åŒï¼Œæˆ‘ä»¬çš„C-Dragé¦–å…ˆæ‰§è¡Œå¯¹è±¡æ„ŸçŸ¥ï¼Œç„¶åæ ¹æ®ç»™å®šçš„å¯¹è±¡è¿åŠ¨æ§åˆ¶ï¼Œæ¨ç†ä¸åŒå¯¹è±¡ä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªå¯¹è±¡æ„ŸçŸ¥æ¨¡å—å’Œä¸€ä¸ªåŸºäºæ€ç»´é“¾çš„è¿åŠ¨æ¨ç†æ¨¡å—ã€‚å¯¹è±¡æ„ŸçŸ¥æ¨¡å—é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¥æ•è·å›¾åƒä¸­å„ç§å¯¹è±¡çš„ä½ç½®å’Œç±»åˆ«ä¿¡æ¯ã€‚åŸºäºæ€ç»´é“¾çš„è¿åŠ¨æ¨ç†æ¨¡å—ä»¥æ­¤ä¿¡æ¯ä¸ºè¾“å…¥ï¼Œè¿›è¡Œåˆ†é˜¶æ®µæ¨ç†è¿‡ç¨‹ï¼Œä¸ºå—å½±å“å¯¹è±¡ç”Ÿæˆè¿åŠ¨è½¨è¿¹ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘åˆæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è§†é¢‘å¯¹è±¡äº¤äº’ï¼ˆVOIï¼‰æ•°æ®é›†ï¼Œä»¥è¯„ä¼°å—æ§è¿åŠ¨è§†é¢‘ç”Ÿæˆæ–¹æ³•çš„ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„VOIæ•°æ®é›†åŒ…å«ä¸‰ç§å…¸å‹çš„äº¤äº’ç±»å‹ï¼Œå¹¶æä¾›å¯ç”¨äºå‡†ç¡®æ€§èƒ½è¯„ä¼°çš„å¯¹è±¡è¿åŠ¨è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC-Dragåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶åœ¨å¯¹è±¡è¿åŠ¨æ§åˆ¶æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ã€ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/WesLee88524/C-Drag-Official-Repo%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/WesLee88524/C-Drag-Official-Repoä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19868v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºChain-of-Thoughtçš„C-Dragè¿åŠ¨æ§åˆ¶å™¨ï¼Œç”¨äºå¯æ§è§†é¢‘ç”Ÿæˆã€‚ä¸åŒäºç°æœ‰æ–¹æ³•ï¼Œå®ƒå…ˆé€šè¿‡å¯¹è±¡æ„ŸçŸ¥æ•æ‰å›¾åƒä¸­å„ç§å¯¹è±¡çš„ä½ç½®å’Œç±»åˆ«ä¿¡æ¯ï¼Œç„¶åé€šè¿‡åŸºäºChain-of-Thoughtçš„è¿åŠ¨æ¨ç†æ¨¡å—å¯¹è¿™äº›å¯¹è±¡é—´çš„åŠ¨æ€äº¤äº’è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆæ¯ä¸ªå—å½±å“å¯¹è±¡çš„è¿åŠ¨è½¨è¿¹ï¼Œæœ€åè¾“å…¥æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘åˆæˆã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è§†é¢‘å¯¹è±¡äº¤äº’æ•°æ®é›†æ¥è¯„ä¼°è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆæ–¹æ³•çš„è´¨é‡ã€‚å®éªŒç»“æœè¯æ˜äº†C-Dragåœ¨å¤šæŒ‡æ ‡ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨å¯¹è±¡è¿åŠ¨æ§åˆ¶æ–¹é¢è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è½¨è¿¹æ§åˆ¶æ–¹æ³•ä¸»è¦ç”Ÿæˆæ§åˆ¶å¯¹è±¡çš„è¿åŠ¨è½¨è¿¹ï¼Œå¿½ç•¥äº†ä¸å‘¨å›´å¯¹è±¡çš„åŠ¨æ€äº¤äº’ã€‚</li>
<li>æå‡ºçš„C-Dragæ–¹æ³•åŒ…æ‹¬å¯¹è±¡æ„ŸçŸ¥æ¨¡å—å’ŒåŸºäºChain-of-Thoughtçš„è¿åŠ¨æ¨ç†æ¨¡å—ã€‚</li>
<li>å¯¹è±¡æ„ŸçŸ¥æ¨¡å—åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ•æ‰å›¾åƒä¸­å¯¹è±¡çš„ä½ç½®å’Œç±»åˆ«ä¿¡æ¯ã€‚</li>
<li>åŸºäºChain-of-Thoughtçš„è¿åŠ¨æ¨ç†æ¨¡å—æ ¹æ®ç»™å®šçš„è¿åŠ¨æ§åˆ¶è¿›è¡ŒåŠ¨æ€äº¤äº’æ¨ç†ï¼Œç”Ÿæˆå—å½±å“å¯¹è±¡çš„è¿åŠ¨è½¨è¿¹ã€‚</li>
<li>å¼•å…¥æ–°çš„è§†é¢‘å¯¹è±¡äº¤äº’æ•°æ®é›†ç”¨äºè¯„ä¼°è¿åŠ¨æ§åˆ¶è§†é¢‘ç”Ÿæˆæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>C-Dragå®ç°äº†å“è¶Šçš„å¯¹è±¡è¿åŠ¨æ§åˆ¶èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60a49a77a416556b4d752964c1504ff7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5737559a146e66132f64b04bed8d2a57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4d198834c210a1223ccdf93e997cedf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f470a057e7e9fc7b4da7b2d2f5d4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7515306bff8f712e7b3ee8215b4b5116.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="One-for-More-Continual-Diffusion-Model-for-Anomaly-Detection"><a href="#One-for-More-Continual-Diffusion-Model-for-Anomaly-Detection" class="headerlink" title="One-for-More: Continual Diffusion Model for Anomaly Detection"></a>One-for-More: Continual Diffusion Model for Anomaly Detection</h2><p><strong>Authors:Xiaofan Li, Xin Tan, Zhuo Chen, Zhizhong Zhang, Ruixin Zhang, Rizen Guo, Guanna Jiang, Yulong Chen, Yanyun Qu, Lizhuang Ma, Yuan Xie</strong></p>
<p>With the rise of generative models, there is a growing interest in unifying all tasks within a generative framework. Anomaly detection methods also fall into this scope and utilize diffusion models to generate or reconstruct normal samples when given arbitrary anomaly images. However, our study found that the diffusion model suffers from severe <code>faithfulness hallucination&#39;&#39; and </code>catastrophic forgettingâ€™â€™, which canâ€™t meet the unpredictable pattern increments. To mitigate the above problems, we propose a continual diffusion model that uses gradient projection to achieve stable continual learning. Gradient projection deploys a regularization on the model updating by modifying the gradient towards the direction protecting the learned knowledge. But as a double-edged sword, it also requires huge memory costs brought by the Markov process. Hence, we propose an iterative singular value decomposition method based on the transitive property of linear representation, which consumes tiny memory and incurs almost no performance loss. Finally, considering the risk of &#96;&#96;over-fittingâ€™â€™ to normal images of the diffusion model, we propose an anomaly-masked network to enhance the condition mechanism of the diffusion model. For continual anomaly detection, ours achieves first place in 17&#x2F;18 settings on MVTec and VisA. Code is available at <a target="_blank" rel="noopener" href="https://github.com/FuNz-0/One-for-More">https://github.com/FuNz-0/One-for-More</a> </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹ï¼ˆgenerative modelsï¼‰çš„å…´èµ·ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…´è¶£é›†ä¸­åœ¨åœ¨ä¸€ä¸ªç”Ÿæˆæ¡†æ¶å†…ç»Ÿä¸€æ‰€æœ‰ä»»åŠ¡ã€‚å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly detectionï¼‰æ–¹æ³•ä¹Ÿåœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œå½“ç»™å®šä»»æ„å¼‚å¸¸å›¾åƒæ—¶ï¼Œå®ƒä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰æ¥ç”Ÿæˆæˆ–é‡å»ºæ­£å¸¸æ ·æœ¬ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„â€œå¿ å®æ€§å¹»è§‰â€ï¼ˆfaithfulness hallucinationï¼‰å’Œâ€œç¾éš¾æ€§é—å¿˜â€ï¼ˆcatastrophic forgettingï¼‰é—®é¢˜ï¼Œæ— æ³•æ»¡è¶³ä¸å¯é¢„æµ‹çš„æ¨¡å¼å¢é‡éœ€æ±‚ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­æ‰©æ•£æ¨¡å‹ï¼ˆcontinual diffusion modelï¼‰ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æ¢¯åº¦æŠ•å½±ï¼ˆgradient projectionï¼‰æ¥å®ç°ç¨³å®šçš„æŒç»­å­¦ä¹ ã€‚æ¢¯åº¦æŠ•å½±é€šè¿‡åœ¨æ¨¡å‹æ›´æ–°ä¸Šéƒ¨ç½²æ­£åˆ™åŒ–ï¼Œä¿®æ”¹æ¢¯åº¦ä»¥ä¿æŠ¤å·²å­¦çŸ¥è¯†ä¸ºæ–¹å‘ã€‚ä½†ä¸æ­¤åŒæ—¶ï¼Œå®ƒä¹Ÿéœ€è¦ç”±é©¬å°”å¯å¤«è¿‡ç¨‹å¸¦æ¥çš„å·¨å¤§å†…å­˜æˆæœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçº¿æ€§è¡¨ç¤ºä¼ é€’å±æ€§çš„è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•ï¼ˆiterative singular value decomposition methodï¼‰ï¼Œè¯¥æ–¹æ³•æ¶ˆè€—å†…å­˜æå°ä¸”å‡ ä¹ä¸ä¼šé€ æˆæ€§èƒ½æŸå¤±ã€‚æœ€åï¼Œè€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹å¯¹æ­£å¸¸å›¾åƒè¿‡åº¦æ‹Ÿåˆçš„é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼‚å¸¸å±è”½ç½‘ç»œï¼ˆanomaly-masked networkï¼‰ï¼Œä»¥å¢å¼ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æœºåˆ¶ã€‚å¯¹äºæŒç»­çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MVTecå’ŒVisAçš„17&#x2F;18è®¾ç½®ä¸Šå–å¾—äº†ç¬¬ä¸€åã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/FuNz-0/One-for-More">https://github.com/FuNz-0/One-for-More</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19848v1">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹å­˜åœ¨â€œå¿ å®å¹»è§‰â€å’Œâ€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜ï¼Œéš¾ä»¥æ»¡è¶³ä¸å¯é¢„æµ‹çš„æ¨¡å¼å¢é‡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æŒç»­æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨æ¢¯åº¦æŠ•å½±å®ç°ç¨³å®šæŒç»­å­¦ä¹ ã€‚ä¸ºè§£å†³Markovè¿‡ç¨‹å¸¦æ¥çš„å·¨å¤§å†…å­˜æˆæœ¬ï¼Œæå‡ºäº†åŸºäºçº¿æ€§è¡¨ç¤ºä¼ é€’æ€§çš„è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•ã€‚æœ€åï¼Œè€ƒè™‘æ‰©æ•£æ¨¡å‹å¯¹æ­£å¸¸å›¾åƒçš„â€œè¿‡åº¦æ‹Ÿåˆâ€é£é™©ï¼Œæå‡ºäº†ä¸€ç§å¼‚å¸¸æ©è†œç½‘ç»œæ¥å¢å¼ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­è¡¨ç°å‡ºâ€œå¿ å®å¹»è§‰â€å’Œâ€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æŒç»­æ‰©æ•£æ¨¡å‹ï¼Œä½¿ç”¨æ¢¯åº¦æŠ•å½±å®ç°ç¨³å®šæŒç»­å­¦ä¹ ã€‚</li>
<li>æ¢¯åº¦æŠ•å½±é€šè¿‡ä¿®æ”¹æ¨¡å‹æ›´æ–°æ—¶çš„æ¢¯åº¦æ¥ä¿æŠ¤æ‰€å­¦çŸ¥è¯†ã€‚</li>
<li>è§£å†³Markovè¿‡ç¨‹å¸¦æ¥çš„å¤§å†…å­˜æ¶ˆè€—ï¼Œé‡‡ç”¨åŸºäºçº¿æ€§è¡¨ç¤ºä¼ é€’æ€§çš„è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å­˜åœ¨å¯¹æ­£å¸¸å›¾åƒçš„â€œè¿‡åº¦æ‹Ÿåˆâ€é£é™©ã€‚</li>
<li>ä¸ºå¢å¼ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æœºåˆ¶ï¼Œæå‡ºäº†ä¸€ç§å¼‚å¸¸æ©è†œç½‘ç»œã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨MVTecå’ŒVisAçš„17&#x2F;18è®¾ç½®ä¸Šè·å¾—ç¬¬ä¸€åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-382cdaecdaba5e5393cdc7c19dcd049e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07fa0efcb8162a8c22e57942207c222c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb5dd7436d5dd7100a57441806cfd7ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-006c51b098dca875c94f32ada021f73a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41d25693fd02e5a7072a0cbb7b2ff712.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Analyzing-CLIPâ€™s-Performance-Limitations-in-Multi-Object-Scenarios-A-Controlled-High-Resolution-Study"><a href="#Analyzing-CLIPâ€™s-Performance-Limitations-in-Multi-Object-Scenarios-A-Controlled-High-Resolution-Study" class="headerlink" title="Analyzing CLIPâ€™s Performance Limitations in Multi-Object Scenarios: A   Controlled High-Resolution Study"></a>Analyzing CLIPâ€™s Performance Limitations in Multi-Object Scenarios: A   Controlled High-Resolution Study</h2><p><strong>Authors:Reza Abbasi, Ali Nazari, Aminreza Sefid, Mohammadali Banayeeanzade, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable performance in zero-shot classification tasks, yet their efficacy in handling complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIPâ€™s performance limitations in multi-object contexts through controlled experiments. We introduce two custom datasets, SimCO and CompCO, to evaluate CLIPâ€™s image and text encoders in various multi-object configurations. Our findings reveal significant biases in both encoders: the image encoder favors larger objects, while the text encoder prioritizes objects mentioned first in descriptions. We hypothesize these biases originate from CLIPâ€™s training process and provide evidence through analyses of the COCO dataset and CLIPâ€™s training progression. Additionally, we extend our investigation to Stable Diffusion models, revealing that biases in the CLIP text encoder significantly impact text-to-image generation tasks. Our experiments demonstrate how these biases affect CLIPâ€™s performance in image-caption matching and generation tasks, particularly when manipulating object sizes and their order in captions. This work contributes valuable insights into CLIPâ€™s behavior in complex visual environments and highlights areas for improvement in future vision-language models. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†åœ¨å¤„ç†å¤æ‚å¤šç›®æ ‡åœºæ™¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é€šè¿‡æ§åˆ¶å®éªŒå…¨é¢åˆ†æäº†CLIPåœ¨å¤šç›®æ ‡ä¸Šä¸‹æ–‡ä¸­çš„æ€§èƒ½å±€é™æ€§ã€‚ä¸ºäº†è¯„ä¼°CLIPçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨åœ¨å„ç§å¤šç›®æ ‡é…ç½®ä¸­çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªè‡ªå®šä¹‰æ•°æ®é›†SimCOå’ŒCompCOã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸¤ä¸ªç¼–ç å™¨éƒ½å­˜åœ¨æ˜æ˜¾çš„åè§ï¼šå›¾åƒç¼–ç å™¨åçˆ±è¾ƒå¤§çš„ç‰©ä½“ï¼Œè€Œæ–‡æœ¬ç¼–ç å™¨åˆ™ä¼˜å…ˆå¤„ç†æè¿°ä¸­é¦–å…ˆæåˆ°çš„ç‰©ä½“ã€‚æˆ‘ä»¬å‡è®¾è¿™äº›åè§æºäºCLIPçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åˆ†æCOCOæ•°æ®é›†å’ŒCLIPçš„è®­ç»ƒè¿›åº¦æä¾›äº†è¯æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹Stable Diffusionæ¨¡å‹è¿›è¡Œäº†è°ƒæŸ¥ï¼Œå‘ç°CLIPæ–‡æœ¬ç¼–ç å™¨ä¸­çš„åè§å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜äº†è¿™äº›åè§å¦‚ä½•åœ¨å›¾åƒæ ‡é¢˜åŒ¹é…å’Œç”Ÿæˆä»»åŠ¡ä¸­å½±å“CLIPçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ“çºµå¯¹è±¡å¤§å°å’Œæ ‡é¢˜ä¸­ç‰©ä½“çš„é¡ºåºæ—¶ã€‚æœ¬ç ”ç©¶ä¸ºCLIPåœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸­çš„è¡Œä¸ºæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥è§†è§‰è¯­è¨€æ¨¡å‹æ”¹è¿›çš„é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19828v1">PDF</a> Accepted at ECCV 2024 Workshop EVAL-FoMo</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†CLIPæ¨¡å‹åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­çš„æ€§èƒ½å±€é™æ€§ï¼Œå¹¶ä»‹ç»äº†ä¸¤ä¸ªç”¨äºè¯„ä¼°CLIPå›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨åœ¨å¤šç›®æ ‡é…ç½®ä¸­çš„è¡¨ç°çš„æ–°æ•°æ®é›†SimCOå’ŒCompCOã€‚ç ”ç©¶å‘ç°CLIPæ¨¡å‹å­˜åœ¨æ˜¾è‘—åè§ï¼šå›¾åƒç¼–ç å™¨åå‘äºè¯†åˆ«è¾ƒå¤§çš„ç‰©ä½“ï¼Œè€Œæ–‡æœ¬ç¼–ç å™¨åˆ™ä¼˜å…ˆå¤„ç†æè¿°ä¸­é¦–å…ˆæåˆ°çš„ç‰©ä½“ã€‚æœ¬æ–‡å‡è®¾è¿™äº›åè§æºäºCLIPçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡åˆ†æCOCOæ•°æ®é›†å’ŒCLIPçš„è®­ç»ƒè¿›åº¦æä¾›äº†è¯æ®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¯¹Stable Diffusionæ¨¡å‹è¿›è¡Œäº†æ¢ç´¢ï¼Œå‘ç°CLIPæ–‡æœ¬ç¼–ç å™¨çš„åè§å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡äº§ç”Ÿä¸¥é‡å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“æ“ä½œç‰©ä½“çš„å¤§å°åŠå…¶åœ¨æè¿°ä¸­çš„é¡ºåºæ—¶ï¼Œè¿™äº›åè§ä¼šå½±å“CLIPåœ¨å›¾åƒæ ‡é¢˜åŒ¹é…å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æœ¬æ–‡å¯¹äºCLIPåœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸­çš„è¡Œä¸ºæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPæ¨¡å‹åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­å­˜åœ¨æ€§èƒ½å±€é™æ€§ã€‚</li>
<li>å¼•å…¥çš„ä¸¤ä¸ªæ–°æ•°æ®é›†SimCOå’ŒCompCOç”¨äºè¯„ä¼°CLIPåœ¨å¤šç§å¤šç›®æ ‡é…ç½®ä¸­çš„è¡¨ç°ã€‚</li>
<li>CLIPæ¨¡å‹å­˜åœ¨åè§ï¼šå›¾åƒç¼–ç å™¨åå‘äºè¯†åˆ«è¾ƒå¤§çš„ç‰©ä½“ï¼Œæ–‡æœ¬ç¼–ç å™¨ä¼˜å…ˆå¤„ç†æè¿°ä¸­é¦–å…ˆæåˆ°çš„ç‰©ä½“ã€‚</li>
<li>è¿™äº›åè§å¯èƒ½æºäºCLIPçš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>CLIPæ–‡æœ¬ç¼–ç å™¨çš„åè§å¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡æœ‰ä¸¥é‡å½±å“ã€‚</li>
<li>å½“æ“ä½œç‰©ä½“çš„å¤§å°åŠå…¶åœ¨æè¿°ä¸­çš„é¡ºåºæ—¶ï¼Œåè§ä¼šå½±å“CLIPåœ¨å›¾åƒæ ‡é¢˜åŒ¹é…å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e946753eb97de25912c63045a970c893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e122b5ce6d8a5bb7044bd0748856d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18f20aacff2ea79605cb87cadc27810f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-005a6c3f00430e139a8d0978ef6768fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4ef846e8384502bd9e04324fcc8905.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-727cb102ac2cf43de009c6aa0ba6ccc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f123645b7ea88feaa13cf1737e8555f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fdb5ebdfdcbeb597aa203ba61f47602.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UIFace-Unleashing-Inherent-Model-Capabilities-to-Enhance-Intra-Class-Diversity-in-Synthetic-Face-Recognition"><a href="#UIFace-Unleashing-Inherent-Model-Capabilities-to-Enhance-Intra-Class-Diversity-in-Synthetic-Face-Recognition" class="headerlink" title="UIFace: Unleashing Inherent Model Capabilities to Enhance Intra-Class   Diversity in Synthetic Face Recognition"></a>UIFace: Unleashing Inherent Model Capabilities to Enhance Intra-Class   Diversity in Synthetic Face Recognition</h2><p><strong>Authors:Xiao Lin, Yuge Huang, Jianqing Xu, Yuxi Mi, Shuigeng Zhou, Shouhong Ding</strong></p>
<p>Face recognition (FR) stands as one of the most crucial applications in computer vision. The accuracy of FR models has significantly improved in recent years due to the availability of large-scale human face datasets. However, directly using these datasets can inevitably lead to privacy and legal problems. Generating synthetic data to train FR models is a feasible solution to circumvent these issues. While existing synthetic-based face recognition methods have made significant progress in generating identity-preserving images, they are severely plagued by context overfitting, resulting in a lack of intra-class diversity of generated images and poor face recognition performance. In this paper, we propose a framework to Unleash Inherent capability of the model to enhance intra-class diversity for synthetic face recognition, shortened as UIFace. Our framework first trains a diffusion model that can perform sampling conditioned on either identity contexts or a learnable empty context. The former generates identity-preserving images but lacks variations, while the latter exploits the modelâ€™s intrinsic ability to synthesize intra-class-diversified images but with random identities. Then we adopt a novel two-stage sampling strategy during inference to fully leverage the strengths of both types of contexts, resulting in images that are diverse as well as identitypreserving. Moreover, an attention injection module is introduced to further augment the intra-class variations by utilizing attention maps from the empty context to guide the sampling process in ID-conditioned generation. Experiments show that our method significantly surpasses previous approaches with even less training data and half the size of synthetic dataset. The proposed UIFace even achieves comparable performance with FR models trained on real datasets when we further increase the number of synthetic identities. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ€é‡è¦çš„åº”ç”¨ä¹‹ä¸€ã€‚ç”±äºå¤§è§„æ¨¡äººè„¸æ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œè¿‘å¹´æ¥äººè„¸è¯†åˆ«æ¨¡å‹çš„å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®é›†ä¸å¯é¿å…åœ°ä¼šå¯¼è‡´éšç§å’Œæ³•å¾‹é—®é¢˜ã€‚ç”Ÿæˆåˆæˆæ•°æ®æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹æ˜¯è§£å†³è¿™äº›é—®é¢˜çš„å¯è¡Œæ–¹æ³•ã€‚è™½ç„¶åŸºäºåˆæˆçš„ç°æœ‰çš„äººè„¸è¯†åˆ«æ–¹æ³•å·²ç»åœ¨ç”Ÿæˆèº«ä»½ä¿ç•™å›¾åƒæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å®ƒä»¬å—åˆ°ä¸Šä¸‹æ–‡è¿‡æ‹Ÿåˆçš„ä¸¥é‡å›°æ‰°ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒç¼ºä¹ç±»å†…å¤šæ ·æ€§å’Œè¾ƒå·®çš„äººè„¸è¯†åˆ«æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶æ¥é‡Šæ”¾æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ï¼Œä»¥æé«˜åˆæˆäººè„¸è¯†åˆ«çš„ç±»å†…å¤šæ ·æ€§ï¼Œç®€ç§°ä¸ºUIFaceã€‚æˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆè®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨èº«ä»½ä¸Šä¸‹æ–‡æˆ–å¯å­¦ä¹ çš„ç©ºä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹è¿›è¡Œé‡‡æ ·ã€‚å‰è€…å¯ä»¥ç”Ÿæˆèº«ä»½ä¿ç•™çš„å›¾åƒä½†ç¼ºä¹å˜åŒ–ï¼Œè€Œåè€…åˆ™åˆ©ç”¨æ¨¡å‹åˆæˆç±»å†…å¤šæ ·åŒ–å›¾åƒçš„å†…åœ¨èƒ½åŠ›ï¼Œä½†å…·æœ‰éšæœºèº«ä»½ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨äº†æ–°å‹çš„ä¸¤é˜¶æ®µé‡‡æ ·ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨ä¸¤ç§ç±»å‹ä¸Šä¸‹æ–‡çš„ä¼˜ç‚¹ï¼Œä»è€Œç”Ÿæˆæ—¢å¤šæ ·åŒ–åˆèº«ä»½ä¿ç•™çš„å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ³¨æ„åŠ›æ³¨å…¥æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨ç©ºä¸Šä¸‹æ–‡ä¸­çš„æ³¨æ„åŠ›å›¾æ¥æŒ‡å¯¼èº«ä»½æ¡ä»¶ç”Ÿæˆä¸­çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºäº†ç±»å†…å˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³ä½¿åœ¨è®­ç»ƒæ•°æ®æ›´å°‘ã€åˆæˆæ•°æ®é›†è§„æ¨¡å‡åŠçš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¤§å¤§è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚å½“æˆ‘ä»¬è¿›ä¸€æ­¥å¢åŠ åˆæˆèº«ä»½çš„æ•°é‡æ—¶ï¼Œæ‰€æå‡ºçš„UIFaceç”šè‡³å®ç°äº†ä¸åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„FRæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19803v1">PDF</a> ICLR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºUIFaceçš„æ¡†æ¶ï¼Œç”¨äºé‡Šæ”¾æ¨¡å‹å†…åœ¨èƒ½åŠ›ï¼Œæé«˜åˆæˆäººè„¸è¯†åˆ«çš„ç±»å†…å¤šæ ·æ€§ã€‚è¯¥æ¡†æ¶é¦–å…ˆè®­ç»ƒä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨èº«ä»½ä¸Šä¸‹æ–‡æˆ–å¯å­¦ä¹ ç©ºç™½ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œé‡‡æ ·ã€‚åˆ©ç”¨ä¸€ä¸ªä¸¤é˜¶æ®µé‡‡æ ·ç­–ç•¥ï¼Œåœ¨æ¨ç†é˜¶æ®µå……åˆ†åˆ©ç”¨è¿™ä¸¤ç§ç±»å‹çš„ä¸Šä¸‹æ–‡ä¼˜åŠ¿ï¼Œç”Ÿæˆå…·æœ‰èº«ä»½ä¿ç•™å’Œå¤šæ ·åŒ–çš„äººè„¸å›¾åƒã€‚åŒæ—¶å¼•å…¥æ³¨æ„åŠ›æ³¨å…¥æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨ç©ºç™½ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›å›¾æ¥æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†ç±»å†…å˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘è®­ç»ƒæ•°æ®å’Œåˆæˆæ•°æ®é›†å¤§å°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶åœ¨å¢åŠ åˆæˆèº«ä»½æ•°é‡æ—¶å®ç°äº†ä¸åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„FRæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UIFaceæ¡†æ¶æ—¨åœ¨è§£å†³åˆæˆäººè„¸è¯†åˆ«ä¸­ç¼ºä¹ç±»å†…å¤šæ ·æ€§åŠè¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«è®­ç»ƒèƒ½åœ¨èº«ä»½ä¸Šä¸‹æ–‡æˆ–ç©ºç™½ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œé‡‡æ ·ã€‚</li>
<li>ä¸¤é˜¶æ®µé‡‡æ ·ç­–ç•¥ç»“åˆäº†ä¸¤ç§ä¸Šä¸‹æ–‡çš„ä¼˜ç‚¹ï¼Œç”Ÿæˆæ—¢å…·æœ‰å¤šæ ·æ€§åˆä¿ç•™èº«ä»½çš„å›¾åƒã€‚</li>
<li>å¼•å…¥æ³¨æ„åŠ›æ³¨å…¥æ¨¡å—ï¼Œåˆ©ç”¨ç©ºç™½ä¸Šä¸‹æ–‡çš„æ³¨æ„åŠ›å›¾æŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œå¢å¼ºäº†ç±»å†…å˜åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒæ•°æ®å’Œè¾ƒå°çš„åˆæˆæ•°æ®é›†å³å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>UIFaceæ¡†æ¶åœ¨å¢åŠ åˆæˆèº«ä»½æ•°é‡æ—¶ï¼Œè¾¾åˆ°äº†ä¸çœŸå®æ•°æ®é›†è®­ç»ƒçš„FRæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2f0e46830ebecca9c818efc1254d00f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07e98a86720b5244ee20d06b0d43ddd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-611e7bb202bdf6504bf8f310b6a1c80b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc10cc5c8f3ad1361c2fcd99f83d899c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Finding-Local-Diffusion-Schrodinger-Bridge-using-Kolmogorov-Arnold-Network"><a href="#Finding-Local-Diffusion-Schrodinger-Bridge-using-Kolmogorov-Arnold-Network" class="headerlink" title="Finding Local Diffusion SchrÃ¶dinger Bridge using Kolmogorov-Arnold   Network"></a>Finding Local Diffusion SchrÃ¶dinger Bridge using Kolmogorov-Arnold   Network</h2><p><strong>Authors:Xingyu Qiu, Mengying Yang, Xinghua Ma, Fanding Li, Dong Liang, Gongning Luo, Wei Wang, Kuanquan Wang, Shuo Li</strong></p>
<p>In image generation, Schr&quot;odinger Bridge (SB)-based methods theoretically enhance the efficiency and quality compared to the diffusion models by finding the least costly path between two distributions. However, they are computationally expensive and time-consuming when applied to complex image data. The reason is that they focus on fitting globally optimal paths in high-dimensional spaces, directly generating images as next step on the path using complex networks through self-supervised training, which typically results in a gap with the global optimum. Meanwhile, most diffusion models are in the same path subspace generated by weights $f_A(t)$ and $f_B(t)$, as they follow the paradigm ($x_t &#x3D; f_A(t)x_{Img} + f_B(t)\epsilon$). To address the limitations of SB-based methods, this paper proposes for the first time to find local Diffusion Schr&quot;odinger Bridges (LDSB) in the diffusion path subspace, which strengthens the connection between the SB problem and diffusion models. Specifically, our method optimizes the diffusion paths using Kolmogorov-Arnold Network (KAN), which has the advantage of resistance to forgetting and continuous output. The experiment shows that our LDSB significantly improves the quality and efficiency of image generation using the same pre-trained denoising network and the KAN for optimising is only less than 0.1MB. The FID metric is reduced by \textbf{more than 15%}, especially with a reduction of 48.50% when NFE of DDIM is $5$ for the CelebA dataset. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Qiu-XY/LDSB">https://github.com/Qiu-XY/LDSB</a>. </p>
<blockquote>
<p>åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸï¼ŒåŸºäºSchrÃ¶dinger Bridgeï¼ˆSBï¼‰çš„æ–¹æ³•é€šè¿‡å¯»æ‰¾ä¸¤ç§åˆ†å¸ƒä¹‹é—´çš„æœ€ä½æˆæœ¬è·¯å¾„ï¼Œç†è®ºä¸Šæé«˜äº†ä¸æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡å’Œè´¨é‡ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºå¤æ‚çš„å›¾åƒæ•°æ®æ—¶ï¼Œå®ƒä»¬è®¡ç®—é‡å¤§ä¸”è€—æ—¶ã€‚åŸå› æ˜¯å®ƒä»¬å…³æ³¨äºåœ¨é«˜ç»´ç©ºé—´ä¸­æ‹Ÿåˆå…¨å±€æœ€ä¼˜è·¯å¾„ï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒç›´æ¥ä½¿ç”¨å¤æ‚ç½‘ç»œåœ¨ä¸‹ä¸€æ­¥ç”Ÿæˆå›¾åƒï¼Œè¿™é€šå¸¸ä¸å…¨å±€æœ€ä¼˜è§£å­˜åœ¨å·®è·ã€‚åŒæ—¶ï¼Œå¤§å¤šæ•°æ‰©æ•£æ¨¡å‹çš„è·¯å¾„éƒ½å—åˆ°æƒé‡fA(t)å’ŒfB(t)çš„åˆ¶çº¦ï¼Œå®ƒä»¬éµå¾ªx_t &#x3D; fA(t)x_{Img} + fB(t)Îµçš„æ¨¡å¼ã€‚ä¸ºäº†å…‹æœSBæ–¹æ³•çš„å±€é™æ€§ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†åœ¨æ‰©æ•£è·¯å¾„å­ç©ºé—´ä¸­å¯»æ‰¾å±€éƒ¨æ‰©æ•£SchrÃ¶dinger Bridgesï¼ˆLDSBï¼‰ï¼Œå¢å¼ºäº†SBé—®é¢˜ä¸æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„è”ç³»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä¼˜åŒ–æ‰©æ•£è·¯å¾„ï¼Œè¯¥ç½‘ç»œå…·æœ‰æŠµæŠ—é—å¿˜å’Œè¿ç»­è¾“å‡ºçš„ä¼˜åŠ¿ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒé™å™ªç½‘ç»œå’Œä¼˜åŒ–çš„KANï¼Œæˆ‘ä»¬çš„LDSBåœ¨å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ï¼Œä¸”KANçš„ä¼˜åŒ–å¤§å°ä»…å°äº0.1MBã€‚FIDæŒ‡æ ‡é™ä½äº†**è¶…è¿‡15%**ï¼Œç‰¹åˆ«æ˜¯åœ¨CelebAæ•°æ®é›†ä¸Šï¼Œå½“DDIMçš„NFEä¸º5æ—¶ï¼Œé™ä½äº†48.5%ã€‚ä»£ç å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/Qiu-XY/LDSB%E3%80%82">https://github.com/Qiu-XY/LDSBã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19754v1">PDF</a> 16 pages, 10 figures, to be published in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å±€éƒ¨æ‰©æ•£SchrÃ¶dingeræ¡¥ï¼ˆLDSBï¼‰æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å›¾åƒç”Ÿæˆä¸­çš„æ•ˆç‡å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•ä¼˜åŒ–äº†æ‰©æ•£è·¯å¾„ï¼Œåˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å¢å¼ºSBé—®é¢˜ä¸æ‰©æ•£æ¨¡å‹çš„è¿æ¥ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒå»å™ªç½‘ç»œå’Œä¼˜åŒ–çš„KANï¼ŒLDSBå¯ä»¥æ˜¾è‘—æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ã€‚å…¶ä¸­ï¼ŒFIDæŒ‡æ ‡é™ä½äº†è¶…è¿‡15%ï¼Œç‰¹åˆ«æ˜¯åœ¨CelebAæ•°æ®é›†ä¸Šï¼Œå½“DDIMçš„NFEä¸º5æ—¶ï¼Œé™ä½äº†48.5%ã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SchrÃ¶dinger Bridgeï¼ˆSBï¼‰æ–¹æ³•åœ¨å›¾åƒç”Ÿæˆä¸­ç†è®ºä¸Šæé«˜äº†æ•ˆç‡å’Œè´¨é‡ï¼Œä½†åº”ç”¨äºå¤æ‚å›¾åƒæ•°æ®æ—¶è®¡ç®—æˆæœ¬é«˜ä¸”è€—æ—¶ã€‚</li>
<li>SBæ–¹æ³•ä¾§é‡äºåœ¨é«˜ç»´ç©ºé—´ä¸­å¯»æ‰¾å…¨å±€æœ€ä¼˜è·¯å¾„ï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒç›´æ¥ç”Ÿæˆå›¾åƒï¼Œè¿™é€šå¸¸ä¸å…¨å±€æœ€ä¼˜å­˜åœ¨å·®è·ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é€šå¸¸ä½äºç”±æƒé‡fA(t)å’ŒfB(t)ç”Ÿæˆçš„ç›¸åŒè·¯å¾„å­ç©ºé—´ä¸­ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºäº†å±€éƒ¨æ‰©æ•£SchrÃ¶dingeræ¡¥ï¼ˆLDSBï¼‰æ–¹æ³•ï¼Œåœ¨æ‰©æ•£è·¯å¾„å­ç©ºé—´ä¸­æ‰¾åˆ°ä¼˜åŒ–è·¯å¾„ã€‚</li>
<li>LDSBæ–¹æ³•ä½¿ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä¼˜åŒ–æ‰©æ•£è·¯å¾„ï¼Œå…·æœ‰æŠµæŠ—é—å¿˜å’Œè¿ç»­è¾“å‡ºçš„ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLDSBæ–¹æ³•æ˜¾è‘—æé«˜äº†å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ•ˆç‡ï¼Œä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒå»å™ªç½‘ç»œå’Œä¼˜åŒ–çš„KANã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ca2b4606a251aaf312ceaa7adc52024.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be29d00793045dfd7a33836bd0205b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8875da2991255fdc0142fd5057c47e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d5fac5329a8ab3a6c69697fadb1b50c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5738f829f99e5430b6a8a602598990fa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Recent-Advances-on-Generalizable-Diffusion-generated-Image-Detection"><a href="#Recent-Advances-on-Generalizable-Diffusion-generated-Image-Detection" class="headerlink" title="Recent Advances on Generalizable Diffusion-generated Image Detection"></a>Recent Advances on Generalizable Diffusion-generated Image Detection</h2><p><strong>Authors:Qijie Xu, Defang Chen, Jiawei Chen, Siwei Lyu, Can Wang</strong></p>
<p>The rise of diffusion models has significantly improved the fidelity and diversity of generated images. With numerous benefits, these advancements also introduce new risks. Diffusion models can be exploited to create high-quality Deepfake images, which poses challenges for image authenticity verification. In recent years, research on generalizable diffusion-generated image detection has grown rapidly. However, a comprehensive review of this topic is still lacking. To bridge this gap, we present a systematic survey of recent advances and classify them into two main categories: (1) data-driven detection and (2) feature-driven detection. Existing detection methods are further classified into six fine-grained categories based on their underlying principles. Finally, we identify several open challenges and envision some future directions, with the hope of inspiring more research work on this important topic. Reviewed works in this survey can be found at <a target="_blank" rel="noopener" href="https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection">https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å…´èµ·æ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§ã€‚è¿™äº›è¿›æ­¥å¸¦æ¥äº†è®¸å¤šå¥½å¤„ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ–°çš„é£é™©ã€‚æ‰©æ•£æ¨¡å‹å¯ä»¥è¢«ç”¨æ¥åˆ›å»ºé«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ å›¾åƒï¼Œè¿™ç»™å›¾åƒçœŸå®æ€§éªŒè¯å¸¦æ¥äº†æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥ï¼Œå…³äºé€šç”¨æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹çš„ç ”ç©¶å‘å±•è¿…é€Ÿã€‚ç„¶è€Œï¼Œå…³äºè¿™ä¸€è¯é¢˜çš„å…¨é¢ç»¼è¿°ä»ç„¶ç¼ºä¹ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹æœ€è¿‘çš„è¿›å±•è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ï¼Œå¹¶å°†å…¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦ç±»åˆ«ï¼šï¼ˆ1ï¼‰æ•°æ®é©±åŠ¨çš„æ£€æµ‹å’Œï¼ˆ2ï¼‰ç‰¹å¾é©±åŠ¨çš„æ£€æµ‹ã€‚åŸºäºå…¶åŸºæœ¬åŸç†ï¼Œç°æœ‰çš„æ£€æµ‹æ–¹æ³•è¢«è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå…­ä¸ªç»†åˆ†ç±»åˆ«ã€‚æœ€åï¼Œæˆ‘ä»¬ç¡®å®šäº†è‹¥å¹²å¼€æ”¾æ€§çš„æŒ‘æˆ˜ï¼Œå¹¶å±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œå¸Œæœ›åœ¨è¿™ä¸€é‡è¦è¯é¢˜ä¸Šæ¿€å‘æ›´å¤šçš„ç ”ç©¶å·¥ä½œã€‚æœ¬ç»¼è¿°ä¸­çš„å®¡æŸ¥å·¥ä½œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detectionä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19716v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„å…´èµ·æå¤§æå‡äº†ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶ä¹Ÿå¸¦æ¥äº†æ–°çš„é£é™©ã€‚æ‰©æ•£æ¨¡å‹å¯è¢«ç”¨äºåˆ¶ä½œé«˜è´¨é‡æ·±åº¦ä¼ªé€ å›¾åƒï¼Œç»™å›¾åƒçœŸå®æ€§éªŒè¯å¸¦æ¥æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥ï¼Œå…³äºé€šç”¨æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹çš„ç ”ç©¶è¿…é€Ÿå¢é•¿ï¼Œä½†ç¼ºä¹å…¨é¢ç»¼è¿°ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°å›é¡¾äº†è¿‘æœŸè¿›å±•ï¼Œå¹¶å°†å…¶åˆ†ä¸ºä¸¤å¤§ç±»ï¼šï¼ˆ1ï¼‰æ•°æ®é©±åŠ¨æ£€æµ‹ï¼›ï¼ˆ2ï¼‰ç‰¹å¾é©±åŠ¨æ£€æµ‹ã€‚åŸºäºåŸç†ï¼Œç°æœ‰æ£€æµ‹æ–¹æ³•è¢«è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå…­ä¸ªå­ç±»åˆ«ã€‚æœ€åï¼Œæœ¬æ–‡è¯†åˆ«äº†å‡ ä¸ªå¼€æ”¾æŒ‘æˆ˜å¹¶å±•æœ›äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¸Œæœ›æ¿€å‘æ›´å¤šåœ¨è¯¥é¢†åŸŸçš„ç ”ç©¶å·¥ä½œã€‚ç›¸å…³è®ºæ–‡ç»¼è¿°å¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detection%E3%80%82">https://github.com/zju-pi/Awesome-Diffusion-generated-Image-Detectionã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥å¸¦æ¥äº†å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦å’Œå¤šæ ·æ€§æ˜¾è‘—æå‡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¹Ÿå¯¼è‡´äº†æ–°çš„é£é™©ï¼Œå¦‚åˆ¶ä½œé«˜è´¨é‡æ·±åº¦ä¼ªé€ å›¾åƒã€‚</li>
<li>è¿‘æœŸå…³äºé€šç”¨æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹çš„ç ”ç©¶å¢é•¿è¿…é€Ÿã€‚</li>
<li>å½“å‰ç¼ºä¹å…³äºè¯¥è¯é¢˜çš„å…¨é¢ç»¼è¿°ã€‚</li>
<li>è®ºæ–‡ç³»ç»Ÿæ€§å›é¡¾äº†æ‰©æ•£ç”Ÿæˆå›¾åƒæ£€æµ‹çš„è¿‘æœŸè¿›å±•ï¼Œå¹¶åˆ†ä¸ºæ•°æ®é©±åŠ¨å’Œç‰¹å¾é©±åŠ¨ä¸¤å¤§ç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•åŸºäºåŸç†è¢«è¿›ä¸€æ­¥ç»†åˆ†ä¸ºå…­ä¸ªå­ç±»åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91c7f328801bb97ec712af09f0d3833f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-162304c0cc9e50efd8aa60a2cf14f7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad7dc7c9be211cbd58cdb1fdb87808f1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SubZero-Composing-Subject-Style-and-Action-via-Zero-Shot-Personalization"><a href="#SubZero-Composing-Subject-Style-and-Action-via-Zero-Shot-Personalization" class="headerlink" title="SubZero: Composing Subject, Style, and Action via Zero-Shot   Personalization"></a>SubZero: Composing Subject, Style, and Action via Zero-Shot   Personalization</h2><p><strong>Authors:Shubhankar Borse, Kartikeya Bhardwaj, Mohammad Reza Karimi Dastjerdi, Hyojin Park, Shreya Kadambi, Shobitha Shivakumar, Prathamesh Mandke, Ankita Nayak, Harris Teague, Munawar Hayat, Fatih Porikli</strong></p>
<p>Diffusion models are increasingly popular for generative tasks, including personalized composition of subjects and styles. While diffusion models can generate user-specified subjects performing text-guided actions in custom styles, they require fine-tuning and are not feasible for personalization on mobile devices. Hence, tuning-free personalization methods such as IP-Adapters have progressively gained traction. However, for the composition of subjects and styles, these works are less flexible due to their reliance on ControlNet, or show content and style leakage artifacts. To tackle these, we present SubZero, a novel framework to generate any subject in any style, performing any action without the need for fine-tuning. We propose a novel set of constraints to enhance subject and style similarity, while reducing leakage. Additionally, we propose an orthogonalized temporal aggregation scheme in the cross-attention blocks of denoising model, effectively conditioning on a text prompt along with single subject and style images. We also propose a novel method to train customized content and style projectors to reduce content and style leakage. Through extensive experiments, we show that our proposed approach, while suitable for running on-edge, shows significant improvements over state-of-the-art works performing subject, style and action composition. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–çš„ä¸»é¢˜å’Œé£æ ¼ç»„åˆã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆç”¨æˆ·æŒ‡å®šçš„ä¸»é¢˜ï¼Œæ‰§è¡Œæ–‡æœ¬å¼•å¯¼çš„åŠ¨ä½œï¼Œå¹¶ä»¥è‡ªå®šä¹‰é£æ ¼å‘ˆç°ï¼Œä½†å®ƒä»¬éœ€è¦å¾®è°ƒï¼Œå¹¶ä¸é€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°ä¸ªæ€§åŒ–ã€‚å› æ­¤ï¼Œæ— éœ€è°ƒæ•´çš„ä¸ªæ€§åŒ–æ–¹æ³•ï¼Œå¦‚IPé€‚é…å™¨ç­‰ï¼Œé€æ¸å—åˆ°äº†å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨ä¸»é¢˜å’Œé£æ ¼çš„ç»„åˆæ–¹é¢ï¼Œè¿™äº›å·¥ä½œç”±äºå…¶ä¾èµ–ControlNetè€Œç¼ºä¹çµæ´»æ€§ï¼Œæˆ–è€…å‡ºç°å†…å®¹å’Œé£æ ¼æ³„éœ²çš„ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SubZeroï¼Œä¸€ä¸ªæ— éœ€å¾®è°ƒå³å¯ç”Ÿæˆä»»ä½•ä¸»é¢˜ã€ä»»ä½•é£æ ¼ã€æ‰§è¡Œä»»ä½•åŠ¨ä½œçš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç»„æ–°çš„çº¦æŸæ¡ä»¶ï¼Œä»¥æé«˜ä¸»é¢˜å’Œé£æ ¼çš„ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶å‡å°‘æ³„éœ²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å»å™ªæ¨¡å‹çš„è·¨æ³¨æ„å—ä¸­æå‡ºäº†æ­£äº¤åŒ–çš„æ—¶é—´èšåˆæ–¹æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ ¹æ®æ–‡æœ¬æç¤ºä»¥åŠå•ä¸ªä¸»é¢˜å’Œé£æ ¼å›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è®­ç»ƒå®šåˆ¶çš„å†…å®¹å’Œé£æ ¼æŠ•å½±ä»ªï¼Œä»¥å‡å°‘å†…å®¹å’Œé£æ ¼çš„æ³„éœ²ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•é€‚åˆåœ¨è¾¹ç¼˜è¿è¡Œï¼Œå¹¶ä¸”åœ¨ä¸»é¢˜ã€é£æ ¼å’ŒåŠ¨ä½œç»„åˆæ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19673v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹å¯ç”Ÿæˆä¸ªæ€§åŒ–ä¸»ä½“å¹¶æ‰§è¡Œæ–‡æœ¬å¼•å¯¼çš„åŠ¨ä½œï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦ç²¾ç»†è°ƒæ•´å¹¶ä¸é€‚ç”¨äºç§»åŠ¨è®¾å¤‡ä¸Šçš„ä¸ªæ€§åŒ–ã€‚SubZeroæ¡†æ¶å¯ç”Ÿæˆä»»ä½•ä¸»é¢˜ä»¥ä»»ä½•é£æ ¼æ‰§è¡Œä»»ä½•åŠ¨ä½œï¼Œæ— éœ€ç²¾ç»†è°ƒæ•´ã€‚é€šè¿‡çº¦æŸå¢å¼ºä¸»é¢˜å’Œé£æ ¼çš„ç›¸ä¼¼æ€§ï¼Œå‡å°‘æ³„æ¼ã€‚é‡‡ç”¨æ­£äº¤åŒ–æ—¶é—´èšåˆæ–¹æ¡ˆï¼Œæœ‰æ•ˆå¤„ç†æ–‡æœ¬æç¤ºå’Œå•ä¸»é¢˜é£æ ¼å›¾åƒã€‚é€šè¿‡å¹¿æ³›å®éªŒè¯æ˜ï¼ŒSubZeroé€‚åˆè¾¹ç¼˜è¿è¡Œï¼Œåœ¨ä¸»é¢˜ã€é£æ ¼å’ŒåŠ¨ä½œç»„åˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–ä¸»ä½“å’Œé£æ ¼çš„ç»„åˆã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹éœ€è¦ç²¾ç»†è°ƒæ•´ï¼Œä¸é€‚ç”¨äºç§»åŠ¨è®¾å¤‡çš„ä¸ªæ€§åŒ–ã€‚</li>
<li>SubZeroæ¡†æ¶æ— éœ€è°ƒæ•´å³å¯ç”Ÿæˆä»»ä½•ä¸»é¢˜ã€ä»»ä½•é£æ ¼çš„ä»»ä½•åŠ¨ä½œã€‚</li>
<li>SubZeroé€šè¿‡çº¦æŸå¢å¼ºä¸»é¢˜å’Œé£æ ¼çš„ç›¸ä¼¼æ€§ï¼Œå‡å°‘æ³„æ¼ã€‚</li>
<li>SubZeroé‡‡ç”¨æ­£äº¤åŒ–æ—¶é—´èšåˆæ–¹æ¡ˆï¼Œæœ‰æ•ˆå¤„ç†æ–‡æœ¬æç¤ºå’Œå›¾åƒã€‚</li>
<li>SubZeroæå‡ºè®­ç»ƒå®šåˆ¶å†…å®¹å’Œé£æ ¼æŠ•å½±ä»ªçš„æ–°æ–¹æ³•ï¼Œå‡å°‘å†…å®¹å’Œé£æ ¼æ³„æ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-acc6a1c9791947d811561b7f8b3e5046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2c0bdb0ae5852b766b4eb609a33f2f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-016650df9627d97c7734d23a1ff468d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6acaacb4c7b99ad11de709a465fd215b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d27b726fb442b47c6f40c400eecd0f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-605e192895b7bdd37444471b8f857c14.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Diffusion-Models-for-Anomaly-Detection"><a href="#A-Survey-on-Diffusion-Models-for-Anomaly-Detection" class="headerlink" title="A Survey on Diffusion Models for Anomaly Detection"></a>A Survey on Diffusion Models for Anomaly Detection</h2><p><strong>Authors:Jing Liu, Zhenchao Ma, Zepu Wang, Chenxuanyin Zou, Jiayang Ren, Zehua Wang, Liang Song, Bo Hu, Yang Liu, Victor C. M. Leung</strong></p>
<p>Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we review recent advances in DMAD research. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at <a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">https://github.com/fdjingliu/DMAD</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»å´­éœ²å¤´è§’ï¼Œæˆä¸ºä¸€ç±»å¼ºå¤§çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚å®ƒåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ½œåŠ›ï¼Œæ¶µç›–ç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—å’Œåˆ¶é€ ç­‰å¤šä¸ªé¢†åŸŸã€‚è¿™ä¸¤ä¸ªé¢†åŸŸçš„äº¤é›†â€”â€”ç§°ä¸ºç”¨äºå¼‚å¸¸æ£€æµ‹çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMADï¼‰â€”â€”ä¸ºè¯†åˆ«æ—¥ç›Šå¤æ‚å’Œé«˜ç»´æ•°æ®ä¸­çš„åå·®æä¾›äº†å‰æ™¯å¹¿é˜”çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†DMADç ”ç©¶çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»ADå’ŒDMçš„åŸºæœ¬æ¦‚å¿µï¼Œç„¶åç»¼åˆåˆ†æç»å…¸çš„DMæ¶æ„ï¼ŒåŒ…æ‹¬DDPMsã€DDIIMså’ŒScore SDEsã€‚æˆ‘ä»¬å°†ç°æœ‰çš„DMADæ–¹æ³•è¿›ä¸€æ­¥åˆ†ä¸ºåŸºäºé‡å»ºçš„ã€åŸºäºå¯†åº¦çš„å’Œæ··åˆæ–¹æ³•ï¼Œå¹¶å¯¹å…¶æ–¹æ³•åˆ›æ–°è¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†ä¸åŒæ•°æ®æ¨¡æ€çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†å…³é”®æŒ‘æˆ˜å’Œæ–°å…´ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€å¢å¼ºç¨³å¥æ€§ã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆã€‚DMADç ”ç©¶è®ºæ–‡å’Œèµ„æºé›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/fdjingliu/DMADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.11430v5">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ–°å…´å¼ºå¤§ç±»åˆ«ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå¹¿æ³›åº”ç”¨äºç½‘ç»œå®‰å…¨ã€æ¬ºè¯ˆæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œåˆ¶é€ ç­‰é¢†åŸŸã€‚æœ¬æ–‡ç»¼è¿°äº†æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œä»‹ç»äº†å¼‚å¸¸æ£€æµ‹å’Œæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µï¼Œåˆ†æäº†ç»å…¸çš„æ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œå¦‚DDPMsã€DDIIMså’ŒScore SDEsï¼Œå¹¶å°†ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•åˆ†ä¸ºé‡å»ºå‹ã€å¯†åº¦å‹å’Œæ··åˆå‹æ–¹æ³•ï¼Œè¯¦ç»†æ¢è®¨äº†å®ƒä»¬åœ¨æ–¹æ³•è®ºä¸Šçš„åˆ›æ–°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ä¸åŒæ•°æ®æ¨¡æ€çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æã€‚åŒæ—¶ï¼Œè®¨è®ºäº†è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§ã€é²æ£’æ€§å¢å¼ºã€è¾¹ç¼˜äº‘åä½œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆç­‰å…³é”®æŒ‘æˆ˜å’Œæ–°å…´ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³èµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/fdjingliu/DMAD">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œåº”ç”¨èŒƒå›´å¹¿æ³›ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†å¼‚å¸¸æ£€æµ‹å’Œæ‰©æ•£æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µã€‚</li>
<li>ç»å…¸çš„æ‰©æ•£æ¨¡å‹æ¶æ„åŒ…æ‹¬DDPMsã€DDIIMså’ŒScore SDEsã€‚</li>
<li>ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¯åˆ†ä¸ºé‡å»ºå‹ã€å¯†åº¦å‹å’Œæ··åˆå‹æ–¹æ³•ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒã€æ—¶é—´åºåˆ—ã€è§†é¢‘å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æç­‰ä»»åŠ¡ä¸­æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>é¢ä¸´çš„å…³é”®æŒ‘æˆ˜åŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€æ¨¡å‹å¯è§£é‡Šæ€§å’Œé²æ£’æ€§å¢å¼ºç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.11430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6034b1f0d7fc2814afa57960141264f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5757f4748c16ca0b92aef664646e184.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ffb4067f89f8f2fbe695cb0bfaaa79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a59d760f27fd4e8799dcc76376000a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5133c2fff8778a66219d0ec65d750a5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FIRE-Robust-Detection-of-Diffusion-Generated-Images-via-Frequency-Guided-Reconstruction-Error"><a href="#FIRE-Robust-Detection-of-Diffusion-Generated-Images-via-Frequency-Guided-Reconstruction-Error" class="headerlink" title="FIRE: Robust Detection of Diffusion-Generated Images via   Frequency-Guided Reconstruction Error"></a>FIRE: Robust Detection of Diffusion-Generated Images via   Frequency-Guided Reconstruction Error</h2><p><strong>Authors:Beilin Chu, Xuan Xu, Xin Wang, Yufei Zhang, Weike You, Linna Zhou</strong></p>
<p>The rapid advancement of diffusion models has significantly improved high-quality image generation, making generated content increasingly challenging to distinguish from real images and raising concerns about potential misuse. In this paper, we observe that diffusion models struggle to accurately reconstruct mid-band frequency information in real images, suggesting the limitation could serve as a cue for detecting diffusion model generated images. Motivated by this observation, we propose a novel method called Frequency-guided Reconstruction Error (FIRE), which, to the best of our knowledge, is the first to investigate the influence of frequency decomposition on reconstruction error. FIRE assesses the variation in reconstruction error before and after the frequency decomposition, offering a robust method for identifying diffusion model generated images. Extensive experiments show that FIRE generalizes effectively to unseen diffusion models and maintains robustness against diverse perturbations. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå¤§åœ°æé«˜äº†é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ï¼Œä½¿å¾—ç”Ÿæˆçš„å†…å®¹è¶Šæ¥è¶Šéš¾ä»¥ä¸çœŸå®å›¾åƒåŒºåˆ†å¼€ï¼Œå¹¶å¼•å‘äº†å…³äºæ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰©æ•£æ¨¡å‹åœ¨å‡†ç¡®é‡å»ºçœŸå®å›¾åƒçš„ä¸­é¢‘ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™è¡¨æ˜è¿™ä¸€å±€é™å¯ä½œä¸ºæ£€æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„ä¸€ç§çº¿ç´¢ã€‚å—æ­¤è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºé¢‘ç‡å¼•å¯¼é‡å»ºè¯¯å·®ï¼ˆFIREï¼‰çš„æ–°æ–¹æ³•ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç ”ç©¶é¢‘ç‡åˆ†è§£å¯¹é‡å»ºè¯¯å·®çš„å½±å“ã€‚FIREè¯„ä¼°é¢‘ç‡åˆ†è§£å‰åçš„é‡å»ºè¯¯å·®å˜åŒ–ï¼Œä¸ºè¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæä¾›äº†ä¸€ç§ç¨³å¥çš„æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFIREèƒ½æœ‰æ•ˆæ¨å¹¿åˆ°æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¯¹å„ç§æ‰°åŠ¨ä¿æŒç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07140v2">PDF</a> 14 pages, 14 figures</p>
<p><strong>Summary</strong>:<br>æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æå¤§åœ°æé«˜äº†é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ï¼Œä½†ç”Ÿæˆçš„å›¾åƒéš¾ä»¥ä¸çœŸå®å›¾åƒåŒºåˆ†ï¼Œå¼•å‘äº†å…³äºæ½œåœ¨è¯¯ç”¨çš„æ‹…å¿§ã€‚æœ¬æ–‡å‘ç°æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºçœŸå®å›¾åƒçš„ä¸­é¢‘ä¿¡æ¯æ—¶å­˜åœ¨å›°éš¾ï¼Œå¯ä½œä¸ºæ£€æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒçš„ä¾æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”é¢‘ç‡å¼•å¯¼é‡å»ºè¯¯å·®ï¼ˆFIREï¼‰ï¼Œå®ƒæ˜¯é¦–ä¸ªç ”ç©¶é¢‘ç‡åˆ†è§£å¯¹é‡å»ºè¯¯å·®å½±å“çš„æ–¹æ³•ã€‚é€šè¿‡è¯„ä¼°é¢‘ç‡åˆ†è§£å‰åçš„é‡å»ºè¯¯å·®å˜åŒ–ï¼ŒFIREä¸ºè¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒæä¾›äº†å¯é çš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒFIREèƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ï¼Œå¯¹å¤šç§æ‰°åŠ¨ä¿æŒç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆèƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œä½†ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å›¾åƒéš¾ä»¥åŒºåˆ†ï¼Œå­˜åœ¨æ½œåœ¨è¯¯ç”¨é£é™©ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºçœŸå®å›¾åƒçš„ä¸­é¢‘ä¿¡æ¯æ—¶å­˜åœ¨å›°éš¾ï¼Œå¯ä½œä¸ºæ£€æµ‹å…¶ç”Ÿæˆå›¾åƒçš„ä¾æ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”é¢‘ç‡å¼•å¯¼é‡å»ºè¯¯å·®ï¼ˆFIREï¼‰ï¼Œç”¨äºè¯†åˆ«æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>FIREé€šè¿‡è¯„ä¼°é¢‘ç‡åˆ†è§£å‰åçš„é‡å»ºè¯¯å·®å˜åŒ–æ¥è¯†åˆ«ç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>FIREèƒ½æœ‰æ•ˆæ³›åŒ–åˆ°æœªè§è¿‡çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>FIREå¯¹å¤šç§æ‰°åŠ¨ä¿æŒç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c75a0b5262fada8fa917ef8c428336de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eab80f0be924b07517d75bc4fea094d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf259b85903a6ab922adb4a9d58158b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4d2fa6c2a74a52d0c9496549b8bab7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Bag-of-Design-Choices-for-Inference-of-High-Resolution-Masked-Generative-Transformer"><a href="#Bag-of-Design-Choices-for-Inference-of-High-Resolution-Masked-Generative-Transformer" class="headerlink" title="Bag of Design Choices for Inference of High-Resolution Masked Generative   Transformer"></a>Bag of Design Choices for Inference of High-Resolution Masked Generative   Transformer</h2><p><strong>Authors:Shitong Shao, Zikai Zhou, Tian Ye, Lichen Bai, Zhiqiang Xu, Zeke Xie</strong></p>
<p>Text-to-image diffusion models (DMs) develop at an unprecedented pace, supported by thorough theoretical exploration and empirical analysis. Unfortunately, the discrepancy between DMs and autoregressive models (ARMs) complicates the path toward achieving the goal of unified vision and language generation. Recently, the masked generative Transformer (MGT) serves as a promising intermediary between DM and ARM by predicting randomly masked image tokens (i.e., masked image modeling), combining the efficiency of DM with the discrete token nature of ARM. However, we find that the comprehensive analyses regarding the inference for MGT are virtually non-existent, and thus we aim to present positive design choices to fill this gap. We propose and redesign a set of enhanced inference techniques tailored for MGT, providing a detailed analysis of their performance. Additionally, we explore several DM-based approaches aimed at accelerating the sampling process on MGT. Extensive experiments and empirical analyses on the recent SOTA MGT, such as MaskGIT and Meissonic lead to concrete and effective design choices, and these design choices can be merged to achieve further performance gains. For instance, in terms of enhanced inference, we achieve winning rates of approximately 70% compared to vanilla sampling on HPS v2 with Meissonic-1024x1024. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦å‘å±•ï¼Œå¾—ç›Šäºæ·±å…¥çš„ç†è®ºæ¢ç´¢å’Œå®è¯åˆ†æã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰å’Œè‡ªå›å½’æ¨¡å‹ï¼ˆARMï¼‰ä¹‹é—´çš„å·®å¼‚ä½¿å¾—å®ç°ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç”Ÿæˆç›®æ ‡çš„é“è·¯å˜å¾—å¤æ‚ã€‚æœ€è¿‘ï¼Œè¢«æ©ç›–çš„ç”Ÿæˆå¼Transformerï¼ˆMGTï¼‰ä½œä¸ºæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ä¹‹é—´çš„æœ‰å‰é€”çš„ä¸­é—´å±‚ï¼Œé€šè¿‡é¢„æµ‹éšæœºæ©ç›–çš„å›¾åƒä»¤ç‰Œï¼ˆå³æ©ç›–å›¾åƒå»ºæ¨¡ï¼‰ï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡å’Œè‡ªå›å½’æ¨¡å‹çš„ç¦»æ•£ä»¤ç‰Œç‰¹æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å…³äºMGTçš„æ¨ç†ç»¼åˆåˆ†æå‡ ä¹ä¸å­˜åœ¨ï¼Œå› æ­¤æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æå‡ºç§¯æçš„è®¾è®¡é€‰æ‹©æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºå¹¶é‡æ–°è®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹MGTçš„å¢å¼ºæ¨ç†æŠ€æœ¯ï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡ ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åŠ é€ŸMGTé‡‡æ ·è¿‡ç¨‹çš„æ–¹æ³•ã€‚åœ¨æœ€è¿‘çš„SOTA MGTï¼ˆå¦‚MaskGITå’ŒMeissonicï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå’Œå®è¯åˆ†æå¾—å‡ºäº†å…·ä½“æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›è®¾è®¡é€‰æ‹©å¯ä»¥åˆå¹¶ä»¥å®ç°æ›´è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¢å¼ºæ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨HPS v2ä¸Šä¸æ™®é€šé‡‡æ ·ç›¸æ¯”è¾¾åˆ°äº†çº¦70%çš„èƒœç‡ï¼Œä½¿ç”¨Meissonic-1024x1024ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10781v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å‘å±•è¿…é€Ÿï¼Œä½†ä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰ä¹‹é—´çš„å·®å¼‚ä½¿å¾—å®ç°ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç”Ÿæˆçš„ç›®æ ‡å˜å¾—å¤æ‚ã€‚è¿‘æœŸï¼Œè¢«æ©ç›–çš„ç”Ÿæˆå¼Transformerï¼ˆMGTï¼‰ä½œä¸ºDMå’ŒARMä¹‹é—´çš„æ¡¥æ¢å±•ç°å‡ºäº†å·¨å¤§æ½œåŠ›ï¼Œå®ƒé€šè¿‡é¢„æµ‹éšæœºæ©ç›–çš„å›¾åƒæ ‡è®°ï¼ˆå³æ©ç›–å›¾åƒå»ºæ¨¡ï¼‰ç»“åˆäº†DMçš„é«˜æ•ˆç‡å’ŒARMçš„ç¦»æ•£æ ‡è®°ç‰¹æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥å…³äºMGTæ¨ç†åˆ†æçš„ç©ºç™½ï¼Œæå‡ºå¹¶é‡æ–°è®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹MGTçš„å¢å¼ºæ¨ç†æŠ€æœ¯ï¼Œå¹¶å¯¹å®ƒä»¬çš„æ€§èƒ½è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚åŒæ—¶ï¼Œæ¢ç´¢äº†åŠ é€ŸMGTé‡‡æ ·è¿‡ç¨‹çš„åŸºäºDMçš„æ–¹æ³•ã€‚åœ¨æœ€æ–°çš„SOTA MGTä¸Šçš„å¹¿æ³›å®éªŒå’Œç»éªŒåˆ†æå¾—å‡ºäº†å…·ä½“æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›è®¾è®¡é€‰æ‹©å¯ä»¥åˆå¹¶ä»¥å®ç°è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¢å¼ºæ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨HPS v2ä¸Šç›¸å¯¹äºæ™®é€šé‡‡æ ·è¾¾åˆ°äº†çº¦70%çš„èƒœç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å‘å±•è¿…çŒ›ï¼Œä½†ä¸è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä½¿å¾—ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç”Ÿæˆçš„ç›®æ ‡å¤æ‚ã€‚</li>
<li>è¢«æ©ç›–çš„ç”Ÿæˆå¼Transformerï¼ˆMGTï¼‰ä½œä¸ºDMå’ŒARMä¹‹é—´çš„æ¡¥æ¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>MGTé€šè¿‡é¢„æµ‹éšæœºæ©ç›–çš„å›¾åƒæ ‡è®°ï¼ˆæ©ç›–å›¾åƒå»ºæ¨¡ï¼‰ç»“åˆäº†DMå’ŒARMçš„ç‰¹æ€§ã€‚</li>
<li>ç›®å‰å¯¹MGTçš„æ¨ç†åˆ†æå­˜åœ¨ç©ºç™½ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚</li>
<li>ç ”ç©¶æå‡ºå¹¶é‡æ–°è®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹MGTçš„å¢å¼ºæ¨ç†æŠ€æœ¯ï¼Œå¹¶è¿›è¡Œè¯¦ç»†æ€§èƒ½åˆ†æã€‚</li>
<li>æ¢ç´¢äº†åŠ é€ŸMGTé‡‡æ ·è¿‡ç¨‹çš„åŸºäºDMçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9891d95e37e50fcae5aa699ab1d3587.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4a56a7aed9b8e8d44e8fa5e015b329a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2f55d7246a49ddc8ee1007133222a9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96fabcf098d8fd890fe0262faa0c2376.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-574438057c96eb7e7ec7a78e11b10dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a3a73fea82b47f75948331e3529fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04cac1cdb56ce7636c062cdbd428de29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47fc6b4b51a76aaffbeccb4c6d53603a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FreCaS-Efficient-Higher-Resolution-Image-Generation-via-Frequency-aware-Cascaded-Sampling"><a href="#FreCaS-Efficient-Higher-Resolution-Image-Generation-via-Frequency-aware-Cascaded-Sampling" class="headerlink" title="FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware   Cascaded Sampling"></a>FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware   Cascaded Sampling</h2><p><strong>Authors:Zhengqiang Zhang, Ruihuang Li, Lei Zhang</strong></p>
<p>While image generation with diffusion models has achieved a great success, generating images of higher resolution than the training size remains a challenging task due to the high computational cost. Current methods typically perform the entire sampling process at full resolution and process all frequency components simultaneously, contradicting with the inherent coarse-to-fine nature of latent diffusion models and wasting computations on processing premature high-frequency details at early diffusion stages. To address this issue, we introduce an efficient $\textbf{Fre}$quency-aware $\textbf{Ca}$scaded $\textbf{S}$ampling framework, $\textbf{FreCaS}$ in short, for higher-resolution image generation. FreCaS decomposes the sampling process into cascaded stages with gradually increased resolutions, progressively expanding frequency bands and refining the corresponding details. We propose an innovative frequency-aware classifier-free guidance (FA-CFG) strategy to assign different guidance strengths for different frequency components, directing the diffusion model to add new details in the expanded frequency domain of each stage. Additionally, we fuse the cross-attention maps of previous and current stages to avoid synthesizing unfaithful layouts. Experiments demonstrate that FreCaS significantly outperforms state-of-the-art methods in image quality and generation speed. In particular, FreCaS is about 2.86$\times$ and 6.07$\times$ faster than ScaleCrafter and DemoFusion in generating a 2048$\times$2048 image using a pre-trained SDXL model and achieves an FID$_b$ improvement of 11.6 and 3.7, respectively. FreCaS can be easily extended to more complex models such as SD3. The source code of FreCaS can be found at <a target="_blank" rel="noopener" href="https://github.com/xtudbxk/FreCaS">https://github.com/xtudbxk/FreCaS</a>. </p>
<blockquote>
<p>è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ç”Ÿæˆæ¯”è®­ç»ƒè§„æ¨¡æ›´é«˜çš„åˆ†è¾¨ç‡çš„å›¾åƒä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸åœ¨å…¨åˆ†è¾¨ç‡ä¸‹æ‰§è¡Œæ•´ä¸ªé‡‡æ ·è¿‡ç¨‹ï¼Œå¹¶åŒæ—¶å¤„ç†æ‰€æœ‰é¢‘ç‡åˆ†é‡ï¼Œè¿™ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰ä»ç²—åˆ°ç»†çš„ç‰¹è´¨ç›¸çŸ›ç›¾ï¼Œå¹¶åœ¨æ—©æœŸçš„æ‰©æ•£é˜¶æ®µæµªè´¹äº†å¯¹è¿‡æ—©çš„é«˜é¢‘ç»†èŠ‚çš„å¤„ç†è®¡ç®—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡æ„ŸçŸ¥çº§è”é‡‡æ ·æ¡†æ¶ï¼ˆç®€ç§°ä¸ºFreCaSï¼‰ï¼Œç”¨äºç”Ÿæˆæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒã€‚FreCaSå°†é‡‡æ ·è¿‡ç¨‹åˆ†è§£ä¸ºçº§è”çš„é˜¶æ®µï¼Œé€æ­¥å¢åŠ åˆ†è¾¨ç‡ï¼Œé€æ¸æ‰©å±•é¢‘ç‡èŒƒå›´å¹¶ä¼˜åŒ–ç›¸åº”çš„ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„é¢‘ç‡æ„ŸçŸ¥æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼ˆFA-CFGï¼‰ï¼Œä¸ºä¸åŒçš„é¢‘ç‡åˆ†é‡åˆ†é…ä¸åŒçš„å¼•å¯¼å¼ºåº¦ï¼Œå¼•å¯¼æ‰©æ•£æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µçš„æ‰©å±•é¢‘ç‡åŸŸä¸­æ·»åŠ æ–°çš„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬èåˆäº†å‰ä¸€é˜¶æ®µå’Œå½“å‰é˜¶æ®µçš„äº¤å‰æ³¨æ„åŠ›å›¾ï¼Œä»¥é¿å…åˆæˆä¸çœŸå®çš„å¸ƒå±€ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢ï¼ŒFreCaSæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨ä½¿ç”¨é¢„è®­ç»ƒçš„SDXLæ¨¡å‹ç”Ÿæˆä¸€å¼ åˆ†è¾¨ç‡ä¸º2048x2048çš„å›¾åƒæ—¶ï¼ŒFreCaSæ¯”ScaleCrafterå’ŒDemoFusionåˆ†åˆ«å¿«çº¦2.86å€å’Œ6.07å€ã€‚å¹¶ä¸”FreCaSå®ç°äº†FIDbæ”¹å–„åˆ†åˆ«ä¸º11.6å’Œ3.7ã€‚FreCaSå¯ä»¥è½»æ¾æ‰©å±•åˆ°æ›´å¤æ‚çš„æ¨¡å‹ï¼Œå¦‚SD3ã€‚FreCaSçš„æºä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/xtudbxk/FreCaS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xtudbxk/FreCaSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18410v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå·²å–å¾—å·¨å¤§æˆåŠŸï¼Œä½†ç”Ÿæˆé«˜äºè®­ç»ƒå°ºå¯¸çš„æ›´é«˜åˆ†è¾¨ç‡å›¾åƒä»å…·æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„é¢‘ç‡æ„ŸçŸ¥çº§è”é‡‡æ ·æ¡†æ¶FreCaSï¼Œç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚é€šè¿‡çº§è”é‡‡æ ·è¿‡ç¨‹é€æ­¥å¢åŠ åˆ†è¾¨ç‡ï¼Œæ¸è¿›æ‰©å±•é¢‘ç‡å¸¦å¹¶ç»†åŒ–ç›¸åº”ç»†èŠ‚ã€‚åŒæ—¶é‡‡ç”¨é¢‘ç‡æ„ŸçŸ¥çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œä¸ºä¸åŒé¢‘ç‡æˆåˆ†åˆ†é…ä¸åŒçš„å¼•å¯¼å¼ºåº¦ã€‚å®éªŒè¯æ˜ï¼ŒFreCaSåœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­é¢ä¸´ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>FreCaSæ¡†æ¶é€šè¿‡çº§è”é‡‡æ ·è¿‡ç¨‹é€æ­¥å¢åŠ åˆ†è¾¨ç‡ï¼Œè§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>FreCaSé‡‡ç”¨é¢‘ç‡æ„ŸçŸ¥çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œé’ˆå¯¹ä¸åŒé¢‘ç‡æˆåˆ†åˆ†é…ä¸åŒçš„å¼•å¯¼å¼ºåº¦ã€‚</li>
<li>FreCaSèƒ½é¿å…åˆæˆä¸çœŸå®çš„å¸ƒå±€ï¼Œé€šè¿‡èåˆå‰ä¸€é˜¶æ®µå’Œå½“å‰é˜¶æ®µçš„äº¤å‰æ³¨æ„åŠ›å›¾å®ç°ã€‚</li>
<li>å®éªŒè¯æ˜FreCaSåœ¨å›¾åƒè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¸Šä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>FreCaSç”Ÿæˆ2048x2048åƒç´ çš„å›¾åƒé€Ÿåº¦æ¯”ScaleCrafterå’ŒDemoFusionåˆ†åˆ«å¿«2.86å€å’Œ6.07å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f4ba7a50d10de76e8468bee95a7787ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37e51f08e09129b674284b8b84f909b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec91e8fa4e6746749676b5d6197fca03.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Training-free-Diffusion-Model-Alignment-with-Sampling-Demons"><a href="#Training-free-Diffusion-Model-Alignment-with-Sampling-Demons" class="headerlink" title="Training-free Diffusion Model Alignment with Sampling Demons"></a>Training-free Diffusion Model Alignment with Sampling Demons</h2><p><strong>Authors:Po-Hung Yeh, Kuang-Huei Lee, Jun-Cheng Chen</strong></p>
<p>Aligning diffusion models with user preferences has been a key challenge. Existing methods for aligning diffusion models either require retraining or are limited to differentiable reward functions. To address these limitations, we propose a stochastic optimization approach, dubbed Demon, to guide the denoising process at inference time without backpropagation through reward functions or model retraining. Our approach works by controlling noise distribution in denoising steps to concentrate density on regions corresponding to high rewards through stochastic optimization. We provide comprehensive theoretical and empirical evidence to support and validate our approach, including experiments that use non-differentiable sources of rewards such as Visual-Language Model (VLM) APIs and human judgements. To the best of our knowledge, the proposed approach is the first inference-time, backpropagation-free preference alignment method for diffusion models. Our method can be easily integrated with existing diffusion models without further training. Our experiments show that the proposed approach significantly improves the average aesthetics scores for text-to-image generation. Implementation is available at <a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DemonSampling">https://github.com/aiiu-lab/DemonSampling</a>. </p>
<blockquote>
<p>å°†æ‰©æ•£æ¨¡å‹ä¸ç”¨æˆ·åå¥½å¯¹é½ä¸€ç›´æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¯¹é½æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•è¦ä¹ˆéœ€è¦é‡æ–°è®­ç»ƒï¼Œè¦ä¹ˆä»…é™äºå¯å¾®å¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éšæœºä¼˜åŒ–æ–¹æ³•ï¼Œç§°ä¸ºDemonï¼Œå®ƒå¯ä»¥åœ¨æ¨ç†æ—¶æŒ‡å¯¼å»å™ªè¿‡ç¨‹ï¼Œè€Œæ— éœ€é€šè¿‡å¥–åŠ±å‡½æ•°è¿›è¡Œåå‘ä¼ æ’­æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ§åˆ¶å»å™ªæ­¥éª¤ä¸­çš„å™ªå£°åˆ†å¸ƒï¼Œé€šè¿‡éšæœºä¼˜åŒ–å°†å¯†åº¦é›†ä¸­åœ¨å¯¹åº”äºé«˜å¥–åŠ±çš„åŒºåŸŸã€‚æˆ‘ä»¬æä¾›äº†å…¨é¢çš„ç†è®ºå’Œå®è¯è¯æ®æ¥æ”¯æŒå’ŒéªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸å¯å¾®å¥–åŠ±æºçš„å®éªŒï¼Œå¦‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰APIå’Œäººç±»åˆ¤æ–­ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€åå‘ä¼ æ’­çš„æ¨ç†æ—¶é—´åå¥½å¯¹é½æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ¾åœ°ä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹é›†æˆï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¹³å‡ç¾å­¦åˆ†æ•°ã€‚å®ç°å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/DemonSampling%E3%80%82">https://github.com/aiiu-lab/DemonSamplingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05760v2">PDF</a> 35 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDemonçš„éšæœºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨ç†é˜¶æ®µå¼•å¯¼å»å™ªè¿‡ç¨‹ï¼Œæ— éœ€é€šè¿‡å¥–åŠ±å‡½æ•°è¿›è¡Œåå‘ä¼ æ’­æˆ–æ¨¡å‹é‡æ–°è®­ç»ƒï¼Œå³å¯å®ç°å¯¹æ‰©æ•£æ¨¡å‹çš„è°ƒæ•´ä»¥æ»¡è¶³ç”¨æˆ·åå¥½ã€‚è¯¥æ–¹æ³•é€šè¿‡æ§åˆ¶å»å™ªè¿‡ç¨‹ä¸­çš„å™ªå£°åˆ†å¸ƒï¼Œä½¿å¯†åº¦é›†ä¸­äºé«˜å¥–åŠ±å¯¹åº”çš„åŒºåŸŸï¼Œå¹¶é€šè¿‡éšæœºä¼˜åŒ–æ¥å®ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ç”¨äºéå¯å¾®å¥–åŠ±æºï¼Œå¦‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰APIå’Œäººç±»åˆ¤æ–­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¹³å‡ç¾å­¦åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸ç”¨æˆ·åå¥½å¯¹é½æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦é‡è®­æ¨¡å‹æˆ–é™äºå¯å¾®å¥–åŠ±å‡½æ•°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDemonçš„éšæœºä¼˜åŒ–æ–¹æ³•ï¼Œå¯åœ¨æ¨ç†é˜¶æ®µè°ƒæ•´æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Demonæ–¹æ³•é€šè¿‡æ§åˆ¶å™ªå£°åˆ†å¸ƒæ¥å¼•å¯¼å»å™ªè¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½é›†ä¸­äºé«˜å¥–åŠ±å¯¹åº”çš„åŒºåŸŸï¼Œæ— éœ€åå‘ä¼ æ’­æˆ–æ¨¡å‹é‡è®­ã€‚</li>
<li>æ”¯æŒéå¯å¾®å¥–åŠ±æºï¼Œå¦‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰APIå’Œäººç±»åˆ¤æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc7be9daf7846005ed46a920f0cc5890.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0973091250caf07dc20d2c580a558101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d984039879db0aaafe3237794a25c2a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5ef852d7e6d446bb1e5920928bc8ed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c6291506f9c68993c3cd798182c9762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc4d09377d546c0ac7494e0533d09581.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Solving-Video-Inverse-Problems-Using-Image-Diffusion-Models"><a href="#Solving-Video-Inverse-Problems-Using-Image-Diffusion-Models" class="headerlink" title="Solving Video Inverse Problems Using Image Diffusion Models"></a>Solving Video Inverse Problems Using Image Diffusion Models</h2><p><strong>Authors:Taesung Kwon, Jong Chul Ye</strong></p>
<p>Recently, diffusion model-based inverse problem solvers (DIS) have emerged as state-of-the-art approaches for addressing inverse problems, including image super-resolution, deblurring, inpainting, etc. However, their application to video inverse problems arising from spatio-temporal degradation remains largely unexplored due to the challenges in training video diffusion models. To address this issue, here we introduce an innovative video inverse solver that leverages only image diffusion models. Specifically, by drawing inspiration from the success of the recent decomposed diffusion sampler (DDS), our method treats the time dimension of a video as the batch dimension of image diffusion models and solves spatio-temporal optimization problems within denoised spatio-temporal batches derived from each image diffusion model. Moreover, we introduce a batch-consistent diffusion sampling strategy that encourages consistency across batches by synchronizing the stochastic noise components in image diffusion models. Our approach synergistically combines batch-consistent sampling with simultaneous optimization of denoised spatio-temporal batches at each reverse diffusion step, resulting in a novel and efficient diffusion sampling strategy for video inverse problems. Experimental results demonstrate that our method effectively addresses various spatio-temporal degradations in video inverse problems, achieving state-of-the-art reconstructions. Project page: <a target="_blank" rel="noopener" href="https://svi-diffusion.github.io/">https://svi-diffusion.github.io/</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„é€†é—®é¢˜æ±‚è§£å™¨ï¼ˆDISï¼‰å·²ç»ä½œä¸ºè§£å†³é€†é—®é¢˜çš„æœ€å…ˆè¿›æ–¹æ³•å‡ºç°ï¼ŒåŒ…æ‹¬å›¾åƒè¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šã€å›¾åƒä¿®å¤ç­‰ã€‚ç„¶è€Œï¼Œç”±äºå…¶è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå®ƒä»¬åœ¨è§£å†³ç”±æ—¶ç©ºé€€åŒ–äº§ç”Ÿçš„è§†é¢‘é€†é—®é¢˜ä¸Šçš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è§†é¢‘é€†æ±‚è§£å™¨ï¼Œå®ƒåªåˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚å…·ä½“åœ°è¯´ï¼Œé€šè¿‡å€Ÿé‰´æœ€è¿‘çš„åˆ†è§£æ‰©æ•£é‡‡æ ·å™¨ï¼ˆDDSï¼‰çš„æˆåŠŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è§†é¢‘çš„æ—¶é—´ç»´åº¦è§†ä¸ºå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ‰¹é‡ç»´åº¦ï¼Œå¹¶åœ¨æ¥è‡ªæ¯ä¸ªå›¾åƒæ‰©æ•£æ¨¡å‹çš„é™å™ªæ—¶ç©ºæ‰¹æ¬¡å†…è§£å†³æ—¶ç©ºä¼˜åŒ–é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ‰¹é‡ä¸€è‡´æ‰©æ•£é‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡åŒæ­¥å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„éšæœºå™ªå£°æˆåˆ†ï¼Œé¼“åŠ±æ‰¹æ¬¡ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ååŒåœ°å°†æ‰¹é‡ä¸€è‡´é‡‡æ ·ä¸æ¯ä¸ªåå‘æ‰©æ•£æ­¥éª¤ä¸­é™å™ªæ—¶ç©ºæ‰¹é‡çš„åŒæ—¶ä¼˜åŒ–ç›¸ç»“åˆï¼Œä¸ºè§†é¢‘é€†é—®é¢˜æä¾›äº†ä¸€ç§æ–°å‹é«˜æ•ˆçš„æ‰©æ•£é‡‡æ ·ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†è§†é¢‘é€†é—®é¢˜ä¸­çš„å„ç§æ—¶ç©ºé€€åŒ–é—®é¢˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ•ˆæœã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://svi-diffusion.github.io/">https://svi-diffusion.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02574v3">PDF</a> ICLR 2025; 25 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³å›¾åƒè¶…åˆ†è¾¨ç‡ã€å»æ¨¡ç³Šã€è¡¥å…¨ç­‰é€†å‘é—®é¢˜æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå…¶åœ¨è§†é¢‘é€†å‘é—®é¢˜ä¸­çš„åº”ç”¨ä»ç„¶æœªå¾—åˆ°è¶³å¤Ÿç ”ç©¶ã€‚è¿™é‡Œä»‹ç»äº†ä¸€ç§ä»…åˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†é¢‘é€†å‘æ±‚è§£å™¨ã€‚é€šè¿‡å°†æ—¶é—´ç»´åº¦è§†ä¸ºå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ‰¹å¤„ç†ç»´åº¦ï¼Œå¹¶å¼•å…¥åˆ†è§£æ‰©æ•£é‡‡æ ·å™¨ï¼ˆDDSï¼‰çš„æ¦‚å¿µï¼Œè¯¥æ–¹æ³•è§£å†³äº†æ—¶ç©ºä¼˜åŒ–é—®é¢˜ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§æ‰¹ä¸€è‡´æ€§æ‰©æ•£é‡‡æ ·ç­–ç•¥ï¼Œé€šè¿‡åŒæ­¥å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„éšæœºå™ªå£°æˆåˆ†ï¼Œç¡®ä¿å„æ‰¹æ¬¡ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†è§†é¢‘é€†å‘é—®é¢˜çš„å„ç§æ—¶ç©ºé€€åŒ–é—®é¢˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§£å†³å›¾åƒé€†å‘é—®é¢˜çš„æœ€å‰æ²¿æŠ€æœ¯ã€‚</li>
<li>è§†é¢‘é€†å‘é—®é¢˜å› è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜è€Œå°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºå›¾åƒæ‰©æ•£æ¨¡å‹çš„è§†é¢‘é€†å‘æ±‚è§£å™¨ã€‚</li>
<li>è¯¥æ–¹æ³•å°†æ—¶é—´ç»´åº¦è§†ä¸ºå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ‰¹å¤„ç†ç»´åº¦ï¼Œè§£å†³æ—¶ç©ºä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å¼•å…¥åˆ†è§£æ‰©æ•£é‡‡æ ·å™¨çš„æ¦‚å¿µä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>é‡‡ç”¨æ‰¹ä¸€è‡´æ€§æ‰©æ•£é‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿å„æ‰¹æ¬¡ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-994638dca77f9c32e04cded312959762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7feba5768dc1a82f7d182ae84fc4f3c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d428e13df85281e6d1692a58ec83aac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b335287516e58e10fe5622a476ce06f1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eae2d74ff268df4180ff80ef656c65d8.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  M^3Builder A Multi-Agent System for Automated Machine Learning in   Medical Imaging
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bfccef5e5776333f579acfe3ac97362a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  Identity-preserving Distillation Sampling by Fixed-Point Iterator
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
