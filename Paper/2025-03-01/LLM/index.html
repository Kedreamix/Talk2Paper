<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-01  R2-T2 Re-Routing in Test-Time for Multimodal Mixture-of-Experts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6f1411ab09e736020130cdd8d94a5c7e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-01-更新"><a href="#2025-03-01-更新" class="headerlink" title="2025-03-01 更新"></a>2025-03-01 更新</h1><h2 id="R2-T2-Re-Routing-in-Test-Time-for-Multimodal-Mixture-of-Experts"><a href="#R2-T2-Re-Routing-in-Test-Time-for-Multimodal-Mixture-of-Experts" class="headerlink" title="R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts"></a>R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts</h2><p><strong>Authors:Zhongyang Li, Ziyue Li, Tianyi Zhou</strong></p>
<p>In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)’ powerful reasoning capabilities, deterring LMMs’ performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method “Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs’ performance on challenging benchmarks of diverse tasks, without training any base-model parameters. </p>
<blockquote>
<p>在多模态大型模型（LMMs）中，非语言模态（例如视觉表示）的感知通常与大型语言模型（LLMs）的强大推理能力不相匹配，这制约了LMMs在具有挑战性的下游任务上的性能。这一弱点最近得到了缓解，通过将视觉编码器替换为混合专家（MoE），提供了丰富、多粒度和多样化的表示，这些表示被不同的下游任务所需要。多模态MoE的性能在很大程度上取决于其路由器，该路由器会重新加权并混合不同专家的表示以适用于每个输入。然而，我们发现端到端训练的路由器并不总是为每一个测试样本生成最佳的路由权重。为了弥补这一差距，我们提出了一种新的高效方法——“测试时的重新路由（R2-T2）”，该方法在测试时局部优化路由权重向量，将其向测试样本邻域中正确预测样本的向量移动。我们提出了三种具有不同优化目标和邻居搜索空间的R2-T2策略。R2-T2在具有挑战性的基准测试中持续且大大提高了LMMs的性能表现，无需训练任何基础模型的参数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20395v1">PDF</a> </p>
<p><strong>Summary</strong>：在多模态大型模型（LMMs）中，非语言模态（如视觉表示）的感知通常无法与大型语言模型（LLMs）的强大推理能力相匹配，影响了LMMs在挑战性下游任务上的性能。最近的研究通过用混合专家（MoE）替换视觉编码器来减轻这一弱点，为各种下游任务提供丰富、多粒度和多样化的表示。多模态MoE的性能在很大程度上取决于其路由器，它为每个输入重新加权和混合不同专家的表示。然而，我们发现端到端训练的路由器并不总是为每个测试样本生成最佳的路由权重。为了解决这个问题，我们提出了一种新的高效方法“测试时的重新路由（R2-T2）”，它在测试时通过向邻近的正确预测样本的向量移动路由权重向量来进行局部优化。我们提出了三种具有不同优化目标和邻近搜索空间的R2-T2策略。R2-T2在不训练任何基础模型参数的情况下，始终提高了最新LMMs在具有挑战性基准测试上的性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态大型模型（LMMs）在处理非语言模态（如视觉表示）时存在感知能力不足的弱点，影响其性能。</li>
<li>引入混合专家（MoE）可以改善LMMs在非语言模态方面的性能。</li>
<li>多模态MoE的性能取决于其路由器，它能根据每个输入重新加权和混合不同专家的表示。</li>
<li>测试时的重新路由（R2-T2）是一种新的策略，用于在测试时优化路由器的路由权重。</li>
<li>R2-T2通过向邻近的正确预测样本的向量移动路由权重向量来进行局部优化。</li>
<li>R2-T2提出了三种具有不同优化目标和邻近搜索空间的策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed2036d099d4254d59e2d6e78305301f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e28d4d237359a78bfe3c21b45fc79c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87e41725c0af3d247a8e93905df485fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02fd94b217dadaf9abc59cbe73d9265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58931de0e2cf7cd623c47fe5ccc1c4d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Verification-Scaling-Test-Time-Compute-with-Multiple-Verifiers"><a href="#Multi-Agent-Verification-Scaling-Test-Time-Compute-with-Multiple-Verifiers" class="headerlink" title="Multi-Agent Verification: Scaling Test-Time Compute with Multiple   Verifiers"></a>Multi-Agent Verification: Scaling Test-Time Compute with Multiple   Verifiers</h2><p><strong>Authors:Shalev Lifshitz, Sheila A. McIlraith, Yilun Du</strong></p>
<p>By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time. </p>
<blockquote>
<p>利用测试时的更多计算资源，大型语言模型（LLM）可以在不需要额外训练的情况下进行改进。一种常见策略是使用验证器来评估候选输出。在这项工作中，我们为测试时的计算提出了一种新型扩展维度：扩展验证器的数量。我们引入了多代理验证（MAV）作为测试时计算的一种范式，它将多个验证器结合起来以提高性能。我们提出使用方面验证器（AVs）作为MAV系统中验证器的一种可能选择，方面验证器是即插即用的LLM，可提示验证输出的不同方面。方面验证器是MAV的便捷构建块，因为它们可以很容易地结合起来，无需额外的训练。此外，我们介绍了BoN-MAV，这是一种简单的多代理验证算法，它将最佳n采样与多个验证器结合起来。BoN-MAV表现出比自我一致性奖励模型验证更强的扩展模式。我们展示了从弱到强的泛化，其中结合弱验证器甚至可以提高更强的大型语言模型的性能，以及自我改进，其中使用相同的基准模型来生成和验证输出。我们的结果将扩展验证器的数量确立为提高语言模型在测试时性能的有前途的新维度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在测试时利用更多的计算资源可以提升性能，而无需额外的训练。本文提出一种新型测试时计算扩展维度：扩展验证器数量。我们引入了多代理验证（MAV）作为测试时计算范式，通过结合多个验证器来提高性能。提出使用方面验证器（AV）作为MAV系统中验证器的一种可能选择，AV是现成的LLM，经过提示可用于验证输出的不同方面，且无需额外训练，易于结合。此外，我们引入了BoN-MAV多代理验证算法，通过最佳n采样结合多个验证器。BoN-MAV表现出比自我一致性奖励模型验证更强的扩展模式，并展示了从弱到强的泛化能力，即结合弱验证器可以提高甚至更强的LLM性能，以及自我改进能力，即使用同一基础模型来生成和验证输出。我们的结果将扩展验证器数量作为提高语言模型测试性能的新维度，显示出巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用测试时的更多计算资源，大型语言模型（LLM）可以在无需额外训练的情况下提升性能。</li>
<li>提出了一种新的测试时计算扩展维度：扩展验证器数量。</li>
<li>引入了多代理验证（MAV）作为测试时计算范式，能结合多个验证器以提高性能。</li>
<li>方面验证器（AV）是一种可能的验证器选择，是现成的LLM，可验证输出的不同方面，且易于结合。</li>
<li>介绍了BoN-MAV多代理验证算法，它通过最佳n采样结合多个验证器。</li>
<li>BoN-MAV表现出比自我一致性奖励模型验证更强的扩展模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b2b76a62b0fde8b559d0309c285dac75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201dbf6a0def84242a23493acd57d1a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7fad77f7243459384cbd85863a2ab4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1336bd377a42deb455d8003898741945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-693e8166ef1b67b991cfeda3ced6181f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PhantomWiki-On-Demand-Datasets-for-Reasoning-and-Retrieval-Evaluation"><a href="#PhantomWiki-On-Demand-Datasets-for-Reasoning-and-Retrieval-Evaluation" class="headerlink" title="PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation"></a>PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation</h2><p><strong>Authors:Albert Gong, Kamilė Stankevičiūtė, Chao Wan, Anmol Kabra, Raphael Thesmar, Johann Lee, Julius Klenke, Carla P. Gomes, Kilian Q. Weinberger</strong></p>
<p>High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/kilian-group/phantom-wiki">https://github.com/kilian-group/phantom-wiki</a>. </p>
<blockquote>
<p>高质量基准测试对于评估大型语言模型（LLM）的推理和检索能力至关重要。然而，为此目的整理数据集并非长久之计，因为它们容易出现数据泄露和性能结果膨胀的问题。为了解决这些挑战，我们提出了PhantomWiki：一个生成独特、事实一致、文档语料库多样、问题答案对丰富的管道。与以前的工作不同，PhantomWiki既不是固定数据集，也不是基于任何现有数据的。相反，为每个评估都会生成新的PhantomWiki实例。我们改变问题的难度和语料库的大小，以分别解开推理和检索能力，发现PhantomWiki数据集对前沿的LLM模型具有出人意料的挑战性。因此，我们为推理、检索和工具使用能力的脱节评估提供了一个可扩展且防数据泄露的框架。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/kilian-group/phantom-wiki%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kilian-group/phantom-wiki找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20377v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的推理和检索能力评估需要高质量的标准。然而，为这一目的制作数据集并非长久之计，因为它们容易存在数据泄露和性能膨胀的问题。为解决这些挑战，我们提出了PhantomWiki：一个生成独特、事实一致、文档语料库与多样问题答案对的管道。不同于之前的工作，PhantomWiki既不是固定的数据集，也不是基于任何现有数据的。相反，为每个评估都会生成一个新的PhantomWiki实例。我们通过调整问题的难度和语料库的大小来分别解开推理和检索能力，发现PhantomWiki数据集对前沿的LLM模型具有出人意料的挑战性。因此，我们为推理、检索和工具使用能力的脱节评估提供了一个可扩展且抗数据泄露的框架。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/kilian-group/phantom-wiki%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kilian-group/phantom-wiki找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量标准对于评估大型语言模型（LLM）的推理和检索能力至关重要。</li>
<li>当前数据集存在数据泄露和性能膨胀的问题，因此不是长久之计。</li>
<li>PhantomWiki是一个生成独特、事实一致文档语料库与问题答案对的管道，不同于固定或基于现有数据的数据集。</li>
<li>PhantomWiki实例按需生成，适用于每次评估。</li>
<li>通过调整问题难度和语料库大小，可以解开推理和检索能力。</li>
<li>PhantomWiki数据集对前沿LLM模型具有挑战性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20377">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16b641b3f0895a09e444c38dfb5fe5a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4bf820c52b886321e88e0a37d51df0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c0f0e670f9cf43d92edfa383894d773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00c9058534c053a10867796aeb8470a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30cb86e04d0bf6629bf58c1a6491a42f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="M-3Builder-A-Multi-Agent-System-for-Automated-Machine-Learning-in-Medical-Imaging"><a href="#M-3Builder-A-Multi-Agent-System-for-Automated-Machine-Learning-in-Medical-Imaging" class="headerlink" title="M^3Builder: A Multi-Agent System for Automated Machine Learning in   Medical Imaging"></a>M^3Builder: A Multi-Agent System for Automated Machine Learning in   Medical Imaging</h2><p><strong>Authors:Jinghao Feng, Qiaoyu Zheng, Chaoyi Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging. </p>
<blockquote>
<p>医学智能体系统因其能够自主执行复杂任务而受到广泛关注。然而，它们对准备好的工具的依赖限制了它们在医学领域的应用，因为医学领域需要训练专门模型。在本文中，我们做出了三个主要贡献：</p>
</blockquote>
<p>（一）我们提出了M3Builder，这是一个新型多智能体系统，旨在实现医学影像中的机器学习自动化。M3Builder的核心采用了四个专业智能体进行协作，以处理复杂的医学机器学习工作流程，包括自动化数据处理、环境配置、自主调试和模型训练等多个步骤。这些智能体在一个医学影像机器学习工作区内运行，这是一个结构化环境，为智能体提供数据集、训练代码和交互工具的文本描述，以实现无缝沟通和任务执行。</p>
<p>（二）为了评估医学影像机器学习的自动化程度，我们推出了M3Bench，这是一个包含四项通用任务的基准测试，涉及14个训练数据集、五个解剖部位和三种成像模式，涵盖二维和三维数据。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20301v1">PDF</a> 38 pages, 7 figures</p>
<p><strong>摘要</strong><br>M3Builder是一种新型多智能体系统，专为自动化医学影像机器学习而设计。它通过四个专门智能体的协作，可完成从数据自动化处理和环境配置到自主调试和模型训练的复杂多步骤医学影像机器学习工作流程。本文的贡献包括：提出M3Builder系统，建立医学影像机器学习的工作空间，为智能体提供数据集、训练代码和互动工具的自由文本描述，实现无缝沟通和任务执行；建立M3Bench基准测试平台，包含四项一般任务在14个训练数据集上，覆盖五种解剖结构和三种成像模式，涉及二维和三维数据；实验使用七种先进的自然语言模型作为智能体核心，如Claude系列、GPT-4o和DeepSeek-V3。相较于现有的机器学习智能体设计，M3Builder在医学影像机器学习任务上表现优越，使用Claude-3.7-Sonnet作为智能体核心时成功率为94.29%，显示出在全自动医学影像机器学习中的巨大潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>M3Builder是一种多智能体系统，旨在自动化医学影像机器学习流程。</li>
<li>M3Builder包含四个专门智能体，可协作完成复杂的多步骤医学影像机器学习任务。</li>
<li>建立了医学影像机器学习的工作空间，提供智能体所需的数据集和自由文本描述。</li>
<li>提出M3Bench基准测试平台，用于评估自动化医学影像机器学习的进展。</li>
<li>M3Builder实验使用多种自然语言模型作为智能体核心。</li>
<li>M3Builder在医学影像机器学习任务上表现优越，使用Claude-3.7-Sonnet作为智能体核心时成功率为94.29%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20301">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-84d4f38d510f1dbd809b3d5331d556ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d74b3f214371bf5356a15a34d72428ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edbf97479d5a11c6c804e54f4702b477.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Judge-a-Book-by-its-Cover-Investigating-Multi-Modal-LLMs-for-Multi-Page-Handwritten-Document-Transcription"><a href="#Judge-a-Book-by-its-Cover-Investigating-Multi-Modal-LLMs-for-Multi-Page-Handwritten-Document-Transcription" class="headerlink" title="Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page   Handwritten Document Transcription"></a>Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page   Handwritten Document Transcription</h2><p><strong>Authors:Benjamin Gutteridge, Matthew Thomas Jackson, Toni Kukurin, Xiaowen Dong</strong></p>
<p>Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, ‘+first page’, which enhances MLLM transcription by providing the OCR output of the entire document along with just the first page image. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that ‘+first page’ improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page. </p>
<blockquote>
<p>手写文本识别（HTR）仍然是一项具有挑战性的任务，特别是在具有共享常见格式和上下文特征的多页文档中。虽然现代光学字符识别（OCR）引擎在打印文本方面表现出色，但在手写文本方面的性能却受到限制，通常需要昂贵的标记数据来进行微调。在本文中，我们探索了在零样本设置中使用多模态大型语言模型（MLLM）来转录多页手写文档。我们研究了商业OCR引擎和MLLM的各种配置，利用后者作为端到端的转录机和后处理器，并带有和不带有图像组件。我们提出了一种新方法“+首页”，通过提供整个文档的OCR输出以及仅第一页图像来提高MLLM转录效果。这种方法利用共享文档特征，而无需处理所有图像的高成本。在IAM手写数据库的多页版本上的实验表明，“+首页”提高了转录准确性，平衡了成本与性能之间的关系，并且甚至通过从一页中推断格式和OCR错误模式来提高超出样本文本的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20295v1">PDF</a> 11 pages (including references and appendix), 14 figures, accepted at   AAAI-25 Workshop on Document Understanding and Intelligence, non-archival</p>
<p><strong>Summary</strong><br>手写文本识别（HTR）对多页文档而言仍是一项挑战，尤其当这些页面具有共同格式和上下文特征时。现代光学字符识别（OCR）引擎擅长处理打印文本，但对手写文本的识别性能有限，通常需要昂贵的标签数据进行微调。本文探讨了多模态大型语言模型（MLLMs）在无监督设置下对多页手写文档的转录应用。我们研究了商业OCR引擎和MLLMs的各种配置，并尝试将后者用作端到端的转录机和后置处理器，以及有无图像组件。本文提出了一种新方法：“+首页”，该方法在MLLM转录中提供了整个文档的OCR输出以及仅第一页的图像，从而利用共享文档特征，同时避免了处理所有图像的高成本。在IAM手写数据库的多页版本上的实验表明，“+首页”方法提高了转录准确性，平衡了成本与性能之间的关系，并且通过对单个页面的格式和OCR错误模式进行推断，提高了超出样本文本的识别效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>手写文本识别（HTR）对多页文档仍是挑战，尤其在页面具有共同格式和上下文特征时。</li>
<li>现代OCR引擎处理手写文本性能有限，需要昂贵的标签数据进行微调。</li>
<li>多模态大型语言模型（MLLMs）可在无监督环境下用于多页手写文档的转录。</li>
<li>‘+首页’方法通过提供整个文档的OCR输出及仅第一页图像，提高了MLLM的转录准确性。</li>
<li>‘+首页’方法平衡了处理成本与性能之间的关系。</li>
<li>该方法通过从单个页面中推断格式和OCR错误模式，增强了超出样本文本的识别效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0b7ca4adaebecd6496969e4eaf5fd33f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9b2ed4cabfe15f003c3b0041fe24fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc2d96d9acf8b1e1a116eb723a86cfea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08af4b92ac4607b713da990ad704dd69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c98900fe9ba8dab1c1bec1075eeb530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7983e0a59a1b46b8df9d41bd0c51be13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c86b4fb8902d9c419103df6002a7bdea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eb9cb3987e8350b07386c740a807cd7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Natural-Language-Perplexity-Detecting-Dead-Code-Poisoning-in-Code-Generation-Datasets"><a href="#Beyond-Natural-Language-Perplexity-Detecting-Dead-Code-Poisoning-in-Code-Generation-Datasets" class="headerlink" title="Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in   Code Generation Datasets"></a>Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in   Code Generation Datasets</h2><p><strong>Authors:Chichien Tsai, Chiamu Yu, Yingdar Lin, Yusung Wu, Weibin Lee</strong></p>
<p>The increasing adoption of large language models (LLMs) for code-related tasks has raised concerns about the security of their training datasets. One critical threat is dead code poisoning, where syntactically valid but functionally redundant code is injected into training data to manipulate model behavior. Such attacks can degrade the performance of neural code search systems, leading to biased or insecure code suggestions. Existing detection methods, such as token-level perplexity analysis, fail to effectively identify dead code due to the structural and contextual characteristics of programming languages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a novel line-level detection and cleansing method tailored to the structural properties of code. DePA computes line-level perplexity by leveraging the contextual relationships between code lines and identifies anomalous lines by comparing their perplexity to the overall distribution within the file. Our experiments on benchmark datasets demonstrate that DePA significantly outperforms existing methods, achieving 0.14-0.19 improvement in detection F1-score and a 44-65% increase in poisoned segment localization precision. Furthermore, DePA enhances detection speed by 0.62-23x, making it practical for large-scale dataset cleansing. Overall, by addressing the unique challenges of dead code poisoning, DePA provides a robust and efficient solution for safeguarding the integrity of code generation model training datasets. </p>
<blockquote>
<p>大型语言模型（LLM）在代码相关任务中的广泛应用引发了人们对其训练数据集安全性的关注。一个关键威胁是死代码中毒，其中语法有效但功能冗余的代码被注入到训练数据中，以操纵模型行为。这种攻击会降低神经代码搜索系统的性能，导致偏见或不安全的代码建议。现有的检测方法，如基于标记级别的困惑度分析，由于编程语言的结构和上下文特征，无法有效地检测死代码。在本文中，我们提出了针对代码结构特性的全新行级检测与净化方法——DePA（死代码困惑度分析）。DePA通过利用代码行之间的上下文关系计算行级困惑度，并通过比较各行困惑度与文件内总体分布来识别异常行。我们在基准数据集上的实验表明，DePA在检测F1分数上显著优于现有方法，提高了0.14-0.19的F1分数，并且在中毒段定位精度上提高了44-65%。此外，DePA提高了检测速度，达到0.62-23倍，使其成为大规模数据集净化的实用选择。总体而言，DePA通过解决死代码中毒的独特挑战，为保护代码生成模型训练数据集的完整性提供了稳健高效解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20246v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型（LLM）在代码相关任务中的普及引发了对其训练数据集安全的担忧。一项关键威胁是死代码中毒，其中语法有效但功能冗余的代码被注入到训练数据中以操纵模型行为。现有检测方法，如基于标记的困惑度分析，未能有效地识别死代码。本文提出一种新的面向代码结构的死代码困惑度分析（DePA）方法，通过在行级别计算困惑度并利用代码行之间的上下文关系来检测死代码。实验结果表明，DePA在检测F1分数上显著优于现有方法，提高了0.14-0.19的F1分数，并且在中毒段定位精度上提高了44-65%。此外，DePA提高了检测速度，加快了大规模数据集清洁的速度。总体而言，DePA为解决死代码中毒的独特挑战提供了稳健高效的解决方案，保障了代码生成模型训练数据集的安全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在代码相关任务中面临死代码中毒的安全威胁。</li>
<li>死代码中毒是指通过注入语法正确但功能冗余的代码来操纵模型行为。</li>
<li>现有检测方法，如基于标记的困惑度分析，无法有效识别死代码。</li>
<li>DePA是一种新的面向代码结构的死代码检测方法，通过计算行级别的困惑度并利用代码行的上下文关系来检测死代码。</li>
<li>DePA在检测F1分数上显著优于现有方法，提高了定位精度和检测速度。</li>
<li>DePA为死代码中毒提供了稳健高效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20246">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ec64c805fd0fa603658ba1151d7a94f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b574edf6f05791602b23ae675ade54e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4025367f44b9b216f9d6226f573967.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e6e06560c45e730b532231ae5833e24.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multimodal-Representation-Alignment-for-Image-Generation-Text-Image-Interleaved-Control-Is-Easier-Than-You-Think"><a href="#Multimodal-Representation-Alignment-for-Image-Generation-Text-Image-Interleaved-Control-Is-Easier-Than-You-Think" class="headerlink" title="Multimodal Representation Alignment for Image Generation: Text-Image   Interleaved Control Is Easier Than You Think"></a>Multimodal Representation Alignment for Image Generation: Text-Image   Interleaved Control Is Easier Than You Think</h2><p><strong>Authors:Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang</strong></p>
<p>The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX. </p>
<blockquote>
<p>先进文本到图像生成领域正出现融合强大文本编码器（如CLIP和T5）与Diffusion Transformer主干的统一框架。尽管人们已经尝试利用canny和深度图等额外条件来控制输出图像，但对于任意文本-图像交替控制的综合框架仍然缺乏。在尝试将多个图像中的概念或视觉元素合并到生成过程中时，这一差距尤其明显。为了缩小这一差距，我们进行了初步实验，发现大型多模态模型（LMM）提供了一个有效的共享表示空间，其中图像和文本可以很好地对齐，以作为外部扩散模型的条件。基于此发现，我们提出了Dream Engine，这是一个为图像生成模型中的任意文本-图像交替控制设计的高效且统一的框架。我们在强大的文本到图像模型（如SD3.5）的基础上，通过融入多功能多模态信息编码器（如QwenVL）来替换原有的纯文本编码器。我们的方法采用两阶段训练范式，包括联合文本-图像对齐和多模态交替指令调整。实验表明，这种训练方法有效，在GenEval基准测试中达到0.69的整体得分，与SD3.5和FLUX等最先进的文本到图像模型性能相匹配。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20172v1">PDF</a> 13 pages, 9 figures, codebase in   <a target="_blank" rel="noopener" href="https://github.com/chenllliang/DreamEngine">https://github.com/chenllliang/DreamEngine</a></p>
<p><strong>Summary</strong><br>文本介绍了基于大型多模态模型（LMMs）的Dream Engine框架，该框架实现了文本和图像之间的任意交织控制。通过引入多模态信息编码器如QwenVL，并采用两阶段训练模式，该框架在图像生成模型中实现了高效且统一的任意文本与图像交织控制。其性能在GenEval基准测试中获得了0.69的整体评分，与主流的文本到图像模型如SD3.5和FLUX相匹配。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）提供了一个共享表示空间，使图像和文本能够良好对齐，成为外部扩散模型的条件。</li>
<li>Dream Engine框架是一个针对图像生成模型的任意文本与图像交织控制的有效且统一的框架。</li>
<li>框架引入了多模态信息编码器如QwenVL，以增强文本和图像之间的对齐和交织控制。</li>
<li>采用两阶段训练模式，包括联合文本-图像对齐和多模态交织指令调整。</li>
<li>实验结果表明，该训练模式在GenEval基准测试中实现了0.69的整体评分，显示出其有效性。</li>
<li>Dream Engine框架的性能与主流的文本到图像模型如SD3.5和FLUX相匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20172">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1581d072940f1373180b798472c85dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a8953428225ab571e72725710184c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e394fa1d9fdd8480802c86c26e672d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7a4f761af24f014afdb099e8a5bbbfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0129785a01fc209ec03625c3396a34f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-Fusion-and-Encoding-Advancing-Multimodal-Retrieval-from-the-Ground-Up"><a href="#Joint-Fusion-and-Encoding-Advancing-Multimodal-Retrieval-from-the-Ground-Up" class="headerlink" title="Joint Fusion and Encoding: Advancing Multimodal Retrieval from the   Ground Up"></a>Joint Fusion and Encoding: Advancing Multimodal Retrieval from the   Ground Up</h2><p><strong>Authors:Lang Huang, Qiyu Wu, Zhongtao Miao, Toshihiko Yamasaki</strong></p>
<p>Information retrieval is indispensable for today’s Internet applications, yet traditional semantic matching techniques often fall short in capturing the fine-grained cross-modal interactions required for complex queries. Although late-fusion two-tower architectures attempt to bridge this gap by independently encoding visual and textual data before merging them at a high level, they frequently overlook the subtle interplay essential for comprehensive understanding. In this work, we rigorously assess these limitations and introduce a unified retrieval framework that fuses visual and textual cues from the ground up, enabling early cross-modal interactions for enhancing context interpretation. Through a two-stage training process–comprising post-training adaptation followed by instruction tuning–we adapt MLLMs as retrievers using a simple one-tower architecture. Our approach outperforms conventional methods across diverse retrieval scenarios, particularly when processing complex multi-modal inputs. Notably, the joint fusion encoder yields greater improvements on tasks that require modality fusion compared to those that do not, underscoring the transformative potential of early integration strategies and pointing toward a promising direction for contextually aware and effective information retrieval. </p>
<blockquote>
<p>信息检索在当今互联网应用中不可或缺，然而传统的语义匹配技术在处理复杂查询时往往难以捕捉到精细粒度的跨模态交互。尽管采用晚期融合双塔架构的方法试图通过独立编码视觉和文本数据并在高层次进行融合来弥补这一差距，但它们往往忽视了全面的理解所需的微妙互动。在这项工作中，我们严格评估了这些局限性，并引入了一个统一的检索框架，该框架自下而上融合视觉和文本线索，为早期跨模态交互提供可能性，增强上下文解释。通过包括微调训练后的适应和指令调整在内的两阶段训练过程，我们采用简单的单塔架构将MLLM作为检索器。我们的方法在多种检索场景中均优于传统方法，特别是在处理复杂的多模态输入时表现尤为出色。值得注意的是，联合融合编码器在需要模态融合的任务上取得了更大的改进，这突显了早期整合策略的变革潜力，并为上下文感知和有效的信息检索指明了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20008v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文指出传统语义匹配技术在复杂查询中的局限性，无法捕捉精细粒度的跨模态交互。引入一种统一检索框架，从底层融合视觉和文本线索，促进早期跨模态交互，提高上下文理解。通过两阶段训练过程，适应MLLM作为检索器，采用简单的一塔架构，在多样检索场景中表现优异，特别是在处理复杂多模态输入时。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统语义匹配技术在复杂查询中表现不足，无法捕捉精细粒度的跨模态交互。</li>
<li>引入一种统一检索框架，实现视觉和文本信息的早期融合，提高上下文理解。</li>
<li>采用两阶段训练过程，包括训练后适应和指令调整，适应MLLM作为检索器。</li>
<li>采用简单的一塔架构，提高检索效率。</li>
<li>在多样检索场景中表现优异，特别是处理复杂多模态输入时。</li>
<li>联合融合编码器在需要模态融合的任务上表现出更大的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20008">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-970557c6e9c349d1b246b1f7ebc21c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdadd4f87da0b8c85ea2a22e45ba91f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-109972c07ce06e6dfee76a3bae82c7fb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Space-Rotation-with-Basis-Transformation-for-Training-free-Test-Time-Adaptation"><a href="#Space-Rotation-with-Basis-Transformation-for-Training-free-Test-Time-Adaptation" class="headerlink" title="Space Rotation with Basis Transformation for Training-free Test-Time   Adaptation"></a>Space Rotation with Basis Transformation for Training-free Test-Time   Adaptation</h2><p><strong>Authors:Chenhao Ding, Xinyuan Gao, Songlin Dong, Yuhang He, Qiang Wang, Xiang Song, Alex Kot, Yihong Gong</strong></p>
<p>With the development of visual-language models (VLM) in downstream task applications, test-time adaptation methods based on VLM have attracted increasing attention for their ability to address changes distribution in test-time. Although prior approaches have achieved some progress, they typically either demand substantial computational resources or are constrained by the limitations of the original feature space, rendering them less effective for test-time adaptation tasks. To address these challenges, we propose a training-free feature space rotation with basis transformation for test-time adaptation. By leveraging the inherent distinctions among classes, we reconstruct the original feature space and map it to a new representation, thereby enhancing the clarity of class differences and providing more effective guidance for the model during testing. Additionally, to better capture relevant information from various classes, we maintain a dynamic queue to store representative samples. Experimental results across multiple benchmarks demonstrate that our method outperforms state-of-the-art techniques in terms of both performance and efficiency. </p>
<blockquote>
<p>随着视觉语言模型（VLM）在下游任务应用中的发展，基于VLM的测试时间适应方法因其解决测试时分配变化的能力而越来越受到关注。尽管先前的方法已经取得了一些进展，但它们通常要么需要大量的计算资源，要么受到原始特征空间局限性的约束，因而在测试时间适应任务中效果较差。为了应对这些挑战，我们提出了一种无需训练的测试时间特征空间旋转和基变换方法。我们利用类之间的固有差异，重构原始特征空间并将其映射到新的表示形式，从而提高类差异的清晰度，为模型在测试时提供更有效的指导。此外，为了更好地从各类别中提取相关信息，我们采用动态队列来存储代表性样本。在多个基准测试上的实验结果表明，我们的方法在性能和效率方面都优于最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于视觉语言模型（VLM）的测试时自适应方法，通过训练外的特征空间旋转和基变换实现测试时数据分布变化的应对。新方法无需大量计算资源，能克服原有特征空间的限制，通过重构特征空间并映射到新的表示，提高类间差异的清晰度，为测试时的模型提供更有效的指导。同时，通过动态队列存储代表性样本，以更好地捕捉各类相关信息。实验结果表明，该方法在多个基准测试上均表现出优异性能和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种基于视觉语言模型（VLM）的测试时自适应方法，用于应对测试数据分布变化。</li>
<li>通过训练外的特征空间旋转和基变换实现测试时适应，无需大量计算资源。</li>
<li>重构原始特征空间并映射到新的表示，以提高类间差异的清晰度。</li>
<li>通过动态队列存储代表性样本，以优化信息捕捉。</li>
<li>方法能提高模型在测试时的性能。</li>
<li>实验结果表明该方法在多个基准测试上表现出优异性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19946">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-380990f7ae9c68fb8ad163ebc07c0afb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e59df997698bbde3c7766d42504a3d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2d71072d23a6326b1f06b94f149e47d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1803f7a897d221fd5e24aff0d73c388c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e52585bd0674347d191c208d43530af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2ffc25ab0b44a250c292054a303ceef.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Picking-the-Cream-of-the-Crop-Visual-Centric-Data-Selection-with-Collaborative-Agents"><a href="#Picking-the-Cream-of-the-Crop-Visual-Centric-Data-Selection-with-Collaborative-Agents" class="headerlink" title="Picking the Cream of the Crop: Visual-Centric Data Selection with   Collaborative Agents"></a>Picking the Cream of the Crop: Visual-Centric Data Selection with   Collaborative Agents</h2><p><strong>Authors:Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang</strong></p>
<p>To improve Multimodal Large Language Models’ (MLLMs) ability to process images and complex instructions, researchers predominantly curate large-scale visual instruction tuning datasets, which are either sourced from existing vision tasks or synthetically generated using LLMs and image descriptions. However, they often suffer from critical flaws, including misaligned instruction-image pairs and low-quality images. Such issues hinder training efficiency and limit performance improvements, as models waste resources on noisy or irrelevant data with minimal benefit to overall capability. To address this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach via \textbf{A}gents Collaboration (ViSA), which centers on image quality assessment and image-instruction relevance evaluation. Specifically, our approach consists of 1) an image information quantification method via visual agents collaboration to select images with rich visual information, and 2) a visual-centric instruction quality assessment method to select high-quality instruction data related to high-quality images. Finally, we reorganize 80K instruction data from large open-source datasets. Extensive experiments demonstrate that ViSA outperforms or is comparable to current state-of-the-art models on seven benchmarks, using only 2.5% of the original data, highlighting the efficiency of our data selection approach. Moreover, we conduct ablation studies to validate the effectiveness of each component of our method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/ViSA">https://github.com/HITsz-TMG/ViSA</a>. </p>
<blockquote>
<p>为了提高多模态大型语言模型（MLLMs）处理图像和复杂指令的能力，研究者主要创建大规模视觉指令调整数据集，这些数据集来源于现有的视觉任务或使用LLMs和图像描述人工合成。然而，它们经常存在关键缺陷，包括指令与图像不匹配和低质量图像。这些问题阻碍了训练效率，限制了性能提升，因为模型会在噪音或无关数据上浪费资源，对整体能力几乎没有帮助。为了解决这一问题，我们提出了一种以视觉为中心的通过代理协作的选型方法（ViSA），该方法以图像质量评估和图像指令相关性评估为中心。具体来说，我们的方法包括：1）通过视觉代理协作的图像信息量化方法，以选择具有丰富视觉信息的图像；2）以视觉为中心的指令质量评估方法，以选择与高质量图像相关的优质指令数据。最后，我们从大型开源数据集中重新组织了8万条指令数据。大量实验表明，ViSA在七个基准测试上的表现优于或相当于当前的最先进模型，而且只使用了原始数据的2.5%，凸显了我们数据选择方法的高效性。此外，我们进行了剥离研究以验证我们方法的每个组成部分的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/ViSA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HITsz-TMG/ViSA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19917v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>针对多模态大型语言模型在处理图像和复杂指令时面临的挑战，如数据集存在的图像与指令不匹配、图像质量低下等问题，研究者提出了一种视觉中心化的选择方法——ViSA。该方法通过视觉代理协作，选择信息丰富的图像和高质量的指令数据，实现了高效的数据选择。实验证明，ViSA在七个基准测试上的表现优于或相当于当前最先进的模型，且仅使用原始数据的2.5%，突显了数据选择的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在处理图像和复杂指令时面临挑战，如数据集存在图像与指令不匹配、图像质量低下等问题。</li>
<li>ViSA方法通过视觉代理协作，旨在解决这些问题，包括图像信息量化方法和视觉中心的指令质量评估方法。</li>
<li>ViSA方法实现了高效的数据选择，仅使用原始数据的2.5%，在七个基准测试上的表现优于或相当于当前最先进的模型。</li>
<li>ViSA方法包括图像信息量化方法和视觉中心的指令质量评估两个核心部分。</li>
<li>该方法通过选择信息丰富的图像和高质量的指令数据，提高了模型的训练效率和性能。</li>
<li>广泛实验验证了ViSA方法的有效性，包括与当前最先进模型的对比和组件有效性的验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19917">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d286ed633556cfc60487a3cd056756d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b50fa3e3cb47afe140847ebcf87d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9455b97a19ca6841de3abd56d68ae172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c53a62069c782517f743efa6d7f6e3ec.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LeanProgress-Guiding-Search-for-Neural-Theorem-Proving-via-Proof-Progress-Prediction"><a href="#LeanProgress-Guiding-Search-for-Neural-Theorem-Proving-via-Proof-Progress-Prediction" class="headerlink" title="LeanProgress: Guiding Search for Neural Theorem Proving via Proof   Progress Prediction"></a>LeanProgress: Guiding Search for Neural Theorem Proving via Proof   Progress Prediction</h2><p><strong>Authors:Suozhi Huang, Peiyang Song, Robert Joseph George, Anima Anandkumar</strong></p>
<p>Mathematical reasoning remains a significant challenge for Large Language Models (LLMs) due to hallucinations. When combined with formal proof assistants like Lean, these hallucinations can be eliminated through rigorous verification, making theorem proving reliable. However, even with formal verification, LLMs still struggle with long proofs and complex mathematical formalizations. While Lean with LLMs offers valuable assistance with retrieving lemmas, generating tactics, or even complete proofs, it lacks a crucial capability: providing a sense of proof progress. This limitation particularly impacts the overall development efficiency in large formalization projects. We introduce LeanProgress, a method that predicts the progress in the proof. Training and evaluating our models made on a large corpus of Lean proofs from Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we employ data preprocessing and balancing techniques to handle the skewed distribution of proof lengths. Our experiments show that LeanProgress achieves an overall prediction accuracy of 75.1% in predicting the amount of progress and, hence, the remaining number of steps. When integrated into a best-first search framework using Reprover, our method shows a 3.8% improvement on Mathlib4 compared to baseline performances of 41.2%, particularly for longer proofs. These results demonstrate how proof progress prediction can enhance both automated and interactive theorem proving, enabling users to make more informed decisions about proof strategies. </p>
<blockquote>
<p>数学推理对于大型语言模型（LLM）来说仍然是一个巨大的挑战，因为存在幻想。当与Lean等正式证明助手结合时，这些幻想可以通过严格的验证来消除，使定理证明更加可靠。然而，即使有正式的验证，LLM在处理长证明和复杂的数学形式化时仍然会遇到困难。虽然Lean与LLM在检索引理、生成策略或甚至完成证明方面提供了宝贵的帮助，但它缺乏一种关键能力：提供证明进度的感觉。这一局限性特别影响了大型形式化项目的整体开发效率。我们引入了LeanProgress，一种预测证明进度的方法。我们在大量来自Lean Workbook Plus和Mathlib4的Lean证明语料库上进行训练和评估我们的模型，以及完成证明所需的剩余步骤数，我们采用数据预处理和平衡技术来处理证明长度分布的不平衡。我们的实验表明，LeanProgress在预测进度以及剩余步骤数量方面达到了75.1%的整体预测准确率。当使用Reprover在最佳优先搜索框架中进行集成时，我们的方法在Mathlib4上与基线性能相比提高了3.8%，特别是在较长的证明中。这些结果表明，证明进度预测可以增强自动和交互式定理证明，使用户能够做出更多关于证明策略的有根据的决策。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17925v2">PDF</a> </p>
<p><strong>Summary</strong><br>     大型语言模型（LLM）在数学推理方面存在挑战，因易出现幻觉而与形式化证明助手（如Lean）结合后，可通过严格验证消除幻觉，使定理证明更加可靠。然而，即使在形式验证下，LLM在长证明和复杂的数学形式化方面仍存在困难。本研究引入了LeanProgress方法，能够预测证明进度。在大量Lean证明语料库上进行训练和评估，采用数据预处理和平衡技术处理证明长度分布不均的问题。实验表明，LeanProgress在预测进度方面达到75.1%的总体预测精度，与Reprover最佳优先搜索框架集成后，在Mathlib4上相比基线性能提高了3.8%，尤其是长证明的改进更加明显。这证明了预测证明进度对自动化和交互式定理证明的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在数学推理中面临挑战，因为容易出现幻觉。</li>
<li>与形式化证明助手（如Lean）结合可以消除幻觉，提高定理证明的可靠性。</li>
<li>即使是在形式验证下，LLM在处理长证明和复杂数学形式化方面仍存在困难。</li>
<li>引入了LeanProgress方法，能够预测证明的进度。</li>
<li>LeanProgress在预测进度方面达到75.1%的总体预测精度。</li>
<li>与Reprover最佳优先搜索框架集成后，LeanProgress在Mathlib4上的性能有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17925">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4dc954295d8c5b7156a14a0fc8e29dca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112be7941887e09e4d962e266aa2a74a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f1411ab09e736020130cdd8d94a5c7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f9e23847ba94c6c24b80cd2b696d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03f597867456f3c126122967dd085d84.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level “novelty.” Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. </p>
<blockquote>
<p>数据的多样性对于大语言模型的指令调整至关重要。现有研究已经探索了多种意识多样性的数据选择方法，以构建高质量的数据集并增强模型性能。然而，关于精确定义和测量数据多样性的基本问题仍然被探索得不够深入，这对数据工程缺乏明确的指导。为了解决这一问题，我们通过大量微调实验评估了现有的11种多样性测量方法与模型性能的相关性。我们的结果表明，可靠的多样性度量应适当地考虑样本之间的差异以及样本空间中的信息分布。在此基础上，我们提出了基于样本级“新颖性”的NovelSum新多样性指标。在模拟和真实数据上的实验表明，NovelSum能够准确捕捉多样性变化，与指令调整模型性能的相关性达到0.97，突显其在指导数据工程实践中的价值。以NovelSum为优化目标，我们进一步开发了一种贪婪的、面向多样性的数据选择策略，其性能优于现有方法，验证了我们指标的有效性和实际意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v3">PDF</a> 16 pages. The related codes and resources will be released later.   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong></p>
<p>数据多样性对于大语言模型的指令调整至关重要。现有研究已经探索了各种基于多样性的数据选择方法以构建高质量数据集并增强模型性能。然而，关于如何精确定义和衡量数据多样性的基础问题仍然被忽视，导致缺乏明确的数据工程指导。为解决这一问题，本文系统地分析了现有的十一种多样性测量方法，并通过大量的微调实验评估它们与模型性能的相关性。结果表明，可靠的多样性测量应同时考虑样本间的差异和样本空间中的信息分布。基于此，本文提出了基于样本级“新颖性”的NovelSum新多样性度量指标。在模拟数据和真实数据上的实验表明，NovelSum能够准确捕捉多样性变化，与指令调整模型性能的相关性达到0.97，凸显其在指导数据工程实践中的价值。利用NovelSum作为优化目标，我们进一步开发了一种以多样性为导向的数据选择策略，其表现优于现有方法，验证了该指标的实用性和实际意义。该论文的研究有助于更好地理解数据多样性的重要性并推动相关领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据多样性对于大语言模型的性能提升至关重要。</li>
<li>现有研究虽然已经探索了多种基于多样性的数据选择方法，但对数据多样性的定义和衡量仍然存在不足。</li>
<li>可靠的数据多样性测量需要综合考虑样本间的差异以及样本空间中的信息分布。</li>
<li>本文提出了基于样本级“新颖性”的NovelSum新多样性度量指标。</li>
<li>NovelSum在模拟数据和真实数据上的实验表现出良好的性能，与模型性能的相关性达到0.97。</li>
<li>利用NovelSum作为优化目标，开发了一种新的以多样性为导向的数据选择策略，其表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f3e5382f0483bf95efcda506b85d70d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54e13d1819d42fc8227602d59168d40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73708b42657d862618f4d0f17d41eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e43cf27f6eac82e9bd09d3c6ce4f768.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning"><a href="#BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning" class="headerlink" title="BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning"></a>BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning</h2><p><strong>Authors:Haiteng Zhao, Chang Ma, Fangzhi Xu, Lingpeng Kong, Zhi-Hong Deng</strong></p>
<p>The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各类生物领域的应用近来已得到广泛研究，但它们在复杂生物系统（如途径）中的推理能力仍鲜有研究，这对于预测生物现象、提出假设和设计实验至关重要。本研究探讨了LLM在途径推理方面的潜力。我们介绍了BioMaze数据集，其中包含从真实研究中得出的5.1K个复杂途径问题，涉及多种生物背景，包括自然动态变化、干扰、额外的干预条件和多尺度研究目标。我们对CoT和图形增强推理等方法进行的评估表明，LLM在途径推理方面存在困难，特别是在受到干扰的系统方面。为解决这一问题，我们提出了PathSeeker，这是一个LLM代理，它通过基于交互子图的导航增强推理能力，以科学的方式更有效地处理生物系统的复杂性。数据集和代码可通过<a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zhao-ht/BioMaze获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16660v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多个生物领域的应用已被广泛探索，但在生物通路等复杂生物系统中的推理能力仍被较少研究。本文探索了LLM在通路推理中的潜力，并引入了BioMaze数据集，包含5.1K个真实研究中的复杂通路问题。评估显示，LLM在扰动系统中的通路推理方面存在困难。为解决这一问题，本文提出了PathSeeker，一个通过交互式子图导航增强推理的LLM代理，以更有效地处理生物系统的复杂性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在生物通路等复杂生物系统中的推理能力仍待探索。</li>
<li>BioMaze数据集包含真实研究中的复杂生物通路问题。</li>
<li>LLM在扰动系统中的通路推理方面存在困难。</li>
<li>PathSeeker是一个LLM代理，通过交互式子图导航增强推理。</li>
<li>PathSeeker能更有效地处理生物系统的复杂性。</li>
<li>BioMaze数据集和代码已公开分享。</li>
<li>LLM的潜力在于其能够预测生物现象、提出假设和设计实验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16660">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2113727ebf2af4f7b52e56861cd6b88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4205c08bf1f99915760fd89f6f664c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20365646be29255d7065440556d637bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0015393d9eb5a90f1979e22a304189cf.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SuperGPQA-Scaling-LLM-Evaluation-across-285-Graduate-Disciplines"><a href="#SuperGPQA-Scaling-LLM-Evaluation-across-285-Graduate-Disciplines" class="headerlink" title="SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"></a>SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines</h2><p><strong>Authors:M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, Ge Zhang</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope. </p>
<blockquote>
<p>大型语言模型（LLM）在数学、物理和计算机科学等主流学科领域表现出了卓越的专长。然而，人类知识包含超过200个专业领域，远超现有基准测试的范围。在这些专业领域中的许多领域，特别是在轻工业、农业和服务导向型领域，LLM的能力尚未得到充分评估。为了弥补这一差距，我们推出了SuperGPQA基准测试，它旨在评估涵盖285个学科的研究生级知识和推理能力。我们的基准测试采用了一种新型的人机协同过滤机制，通过基于LLM响应和专家反馈的迭代优化来消除琐碎或模糊的问题。我们的实验结果表明，在多种知识领域中，当前最先进的LLM性能仍有很大的提升空间（例如，以推理为重点的DeepSeek-R1在SuperGPQA上达到了最高的61.82%准确率），这突显了当前模型能力与通用人工智能之间的巨大差距。此外，我们还提供了关于管理大规模标注过程的综合见解，涉及超过80名专家标注人员和一个人机协同互动系统，为未来的类似研究提供了宝贵的方法论指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14739v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在主流学科如数学、物理和计算机科学中表现出卓越的能力，但在众多专业领域，尤其是轻工业、农业和服务导向型学科中的能力评估仍不足。为解决此问题，提出了SuperGPQA基准测试，用于评估研究生级别的知识和推理能力在285个学科中的应用。该基准测试采用新型的人机协同过滤机制，通过迭代优化，结合LLM响应和专家反馈来消除琐碎或模糊的问题。实验结果显示，在SuperGPQA上，最先进的LLM性能仍有待提高，如DeepSeek-R1模型最高准确率为61.82%，表明当前模型能力与通用人工智能之间仍存在巨大差距。同时，分享了大规模标注过程中的综合见解，涉及超过80位专家标注师和人机交互系统，为未来类似研究提供方法论指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在主流学科中表现出卓越的能力，但在众多专业领域中的能力评估仍不足。</li>
<li>SuperGPQA是一个新的基准测试，旨在评估LLM在285个学科中的研究生级别知识和推理能力。</li>
<li>SuperGPQA采用人机协同过滤机制，结合LLM响应和专家反馈来优化问题。</li>
<li>当前最先进的LLM在SuperGPQA上的性能有待提高，显示与通用人工智能之间存在巨大差距。</li>
<li>DeepSeek-R1模型在SuperGPQA上的最高准确率为61.82%。</li>
<li>大规模标注过程中的综合见解被分享，涉及超过80位专家标注师和人机交互系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49a95e00e148d1ebb601fc44b62436c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e569dc0664db8655dc71a25ee43e93b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks"><a href="#Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks" class="headerlink" title="Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks"></a>Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks</h2><p><strong>Authors:Chengxin Hu, Hao Li, Yihe Yuan, Zezheng Song, Haixin Wang</strong></p>
<p>Building generalist models has recently demonstrated remarkable capabilities in diverse scientific domains. Within the realm of molecular learning, several studies have explored unifying diverse tasks across diverse domains. However, negative conflicts and interference between molecules and knowledge from different domain may have a worse impact in threefold. First, conflicting molecular representations can lead to optimization difficulties for the models. Second, mixing and scaling up training data across diverse tasks is inherently challenging. Third, the computational cost of refined pretraining is prohibitively high. To address these limitations, this paper presents Omni-Mol, a scalable and unified LLM-based framework for direct instruction tuning. Omni-Mol builds on three key components to tackles conflicts: (1) a unified encoding mechanism for any task input; (2) an active-learning-driven data selection strategy that significantly reduces dataset size; (3) a novel design of the adaptive gradient stabilization module and anchor-and-reconcile MoE framework that ensures stable convergence. Experimentally, Omni-Mol achieves state-of-the-art performance across 15 molecular tasks, demonstrates the presence of scaling laws in the molecular domain, and is supported by extensive ablation studies and analyses validating the effectiveness of its design. The dataset, code and weights of the powerful AI-driven chemistry generalist are open-sourced. </p>
<blockquote>
<p>最近，构建通用模型在不同科学领域表现出了显著的能力。在分子学习领域，多项研究探索了如何在不同领域统一各种任务。然而，分子和来自不同领域的知识之间的负面冲突和干扰可能会产生三倍的负面影响。首先，冲突的分子表征可能导致模型优化困难。其次，在多种任务之间混合和扩大训练数据本身就具有挑战性。第三，精细预训练的计算成本高昂。为了解决这些限制，本文提出了Omni-Mol，一个基于大规模语言模型（LLM）的可扩展统一框架，用于直接指令调整。Omni-Mol通过三个关键组件来解决冲突：（1）任何任务输入的统一编码机制；（2）以主动学习驱动的数据选择策略，可显著减少数据集大小；（3）自适应梯度稳定模块和锚定与和解MoE框架的新颖设计，确保稳定收敛。通过实验，Omni-Mol在15个分子任务上实现了最先进的性能，证明了分子领域的规模法则的存在，并通过广泛的消融研究和分析验证了其设计的有效性。该强大AI驱动化学通用主义的数据集、代码和权重均已开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01074v2">PDF</a> 30 pages, 13 figures, 7 tables, paper under review</p>
<p><strong>Summary</strong>：</p>
<p>近期，通用模型在多个科学领域展现出强大能力。在分子学习领域，多项研究尝试统一不同任务。然而，不同领域分子和知识间的冲突干扰可能带来三倍负面影响。本文提出Omni-Mol框架，以大型语言模型为基础，通过统一编码机制、主动学习驱动的数据选择策略和梯度稳定模块设计，解决冲突问题。实验表明，Omni-Mol在15个分子任务上达到最佳性能，并在分子领域展现规模效应。其数据集、代码和权重均已开源。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>通用模型在多个科学领域表现出强大的能力，特别是在分子学习领域。</li>
<li>不同领域分子和知识间的冲突干扰是分子学习面临的主要问题之一。</li>
<li>Omni-Mol框架通过统一编码机制、数据选择策略和梯度稳定模块设计来解决冲突问题。</li>
<li>Omni-Mol实现了在15个分子任务上的最佳性能。</li>
<li>Omni-Mol在分子领域展现出规模效应，并通过广泛的消融研究和分析验证了其设计的有效性。</li>
<li>Omni-Mol使用的数据集、代码和权重均已开源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01966bee2774dcd85e7d8d85a1555a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130b9b768c64e945f315444fac72ae42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fe495678783958dda53091c6924cab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5e9cf6e84ed1c9667c89b8f2c619f8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Bag-of-Design-Choices-for-Inference-of-High-Resolution-Masked-Generative-Transformer"><a href="#Bag-of-Design-Choices-for-Inference-of-High-Resolution-Masked-Generative-Transformer" class="headerlink" title="Bag of Design Choices for Inference of High-Resolution Masked Generative   Transformer"></a>Bag of Design Choices for Inference of High-Resolution Masked Generative   Transformer</h2><p><strong>Authors:Shitong Shao, Zikai Zhou, Tian Ye, Lichen Bai, Zhiqiang Xu, Zeke Xie</strong></p>
<p>Text-to-image diffusion models (DMs) develop at an unprecedented pace, supported by thorough theoretical exploration and empirical analysis. Unfortunately, the discrepancy between DMs and autoregressive models (ARMs) complicates the path toward achieving the goal of unified vision and language generation. Recently, the masked generative Transformer (MGT) serves as a promising intermediary between DM and ARM by predicting randomly masked image tokens (i.e., masked image modeling), combining the efficiency of DM with the discrete token nature of ARM. However, we find that the comprehensive analyses regarding the inference for MGT are virtually non-existent, and thus we aim to present positive design choices to fill this gap. We propose and redesign a set of enhanced inference techniques tailored for MGT, providing a detailed analysis of their performance. Additionally, we explore several DM-based approaches aimed at accelerating the sampling process on MGT. Extensive experiments and empirical analyses on the recent SOTA MGT, such as MaskGIT and Meissonic lead to concrete and effective design choices, and these design choices can be merged to achieve further performance gains. For instance, in terms of enhanced inference, we achieve winning rates of approximately 70% compared to vanilla sampling on HPS v2 with Meissonic-1024x1024. </p>
<blockquote>
<p>文本到图像扩散模型（DM）以前所未有的速度发展，得益于深入的理论探索和实证分析。然而，DM和自回归模型（ARM）之间的差异使得实现统一视觉和语言生成的目标变得更加复杂。最近，掩码生成式Transformer（MGT）作为DM和ARM之间的有前途的中介，通过预测随机掩码图像令牌（即掩码图像建模），结合了DM的效率与ARM的离散令牌特性。然而，我们发现关于MGT推理的综合分析几乎不存在，因此，我们的目标是提出积极的设计选择来填补这一空白。我们提出并重新设计了一系列针对MGT的增强推理技术，并对其性能进行了详细分析。此外，我们还探索了几种基于DM的方法来加速MGT上的采样过程。在最近的最新MGT（如MaskGIT和Meissonic）上进行的大量实验和实证分析得出了具体有效的设计选择，这些设计选择可以合并以实现进一步的性能提升。例如，在增强推理方面，我们在HPS v2上与香草采样相比达到了约70%的胜率，使用Meissonic-1024x1024。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10781v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>文本到图像扩散模型（DMs）的发展速度空前，得益于深入的理论探索和实证分析。然而，DMs和自回归模型（ARMs）之间的差异使得实现统一视觉和语言生成的目标变得复杂。最近，掩码生成式转换器（MGT）作为DM和ARM之间的有前途的中间方案，通过预测随机掩码图像令牌（即掩码图像建模），结合了DM的效率与ARM的离散令牌特性。然而，关于MGT的推理综合分析几乎不存在，因此，我们旨在提出积极的设计选择来填补这一空白。我们提出并重新设计了一系列针对MGT的增强推理技术，并对其性能进行了详细分析。此外，我们还探索了旨在加速MGT采样过程的几种基于DM的方法。在最新的SOTA MGT（如MaskGIT和Meissonic）上进行的大量实验和实证分析得出了切实有效的设计选择，这些设计选择可以合并以实现进一步的性能提升。例如，在增强推理方面，我们在HPS v2上实现了约70%的胜率，超过了普通采样方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本到图像扩散模型（DMs）的发展迅速，但自回归模型（ARMs）之间的差异影响了统一视觉和语言生成目标的实现。</li>
<li>掩码生成式转换器（MGT）是DM和ARM之间的有前途的中间方案，结合了两者优点。</li>
<li>目前关于MGT的推理综合分析几乎不存在。</li>
<li>提出并重新设计了针对MGT的增强推理技术，并进行详细分析。</li>
<li>探索了旨在加速MGT采样过程的基于DM的方法。</li>
<li>在最新的MGT上进行的大量实验和实证分析表明，某些设计选择能带来显著性能提升。</li>
<li>在增强推理方面取得了显著成果，如在HPS v2上的胜率达到约70%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10781">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e9891d95e37e50fcae5aa699ab1d3587.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4a56a7aed9b8e8d44e8fa5e015b329a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f55d7246a49ddc8ee1007133222a9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96fabcf098d8fd890fe0262faa0c2376.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-574438057c96eb7e7ec7a78e11b10dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a3a73fea82b47f75948331e3529fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04cac1cdb56ce7636c062cdbd428de29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47fc6b4b51a76aaffbeccb4c6d53603a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="S-4-ST-A-Strong-Self-transferable-faSt-and-Simple-Scale-Transformation-for-Transferable-Targeted-Attack"><a href="#S-4-ST-A-Strong-Self-transferable-faSt-and-Simple-Scale-Transformation-for-Transferable-Targeted-Attack" class="headerlink" title="S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale   Transformation for Transferable Targeted Attack"></a>S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale   Transformation for Transferable Targeted Attack</h2><p><strong>Authors:Yongxiang Liu, Bowen Peng, Li Liu, Xiang Li</strong></p>
<p>Transferable Targeted Attacks (TTAs), which aim to deceive black-box models into predicting specific erroneous labels, face significant challenges due to severe overfitting to surrogate models. Although modifying image features to generate robust semantic patterns of the target class is a promising approach, existing methods heavily rely on large-scale additional data. This dependence undermines the fair evaluation of TTA threats, potentially leading to a false sense of security or unnecessary overreactions. In this paper, we introduce two blind measures, surrogate self-alignment and self-transferability, to analyze the effectiveness and correlations of basic transformations, to enhance data-free attacks under strict black-box constraints. Our findings challenge conventional assumptions: (1) Attacking simple scaling transformations uniquely enhances targeted transferability, outperforming other basic transformations and rivaling leading complex methods; (2) Geometric and color transformations exhibit high internal redundancy despite weak inter-category correlations. These insights drive the design and tuning of S4ST (Strong, Self-transferable, faSt, Simple Scale Transformation), which integrates dimensionally consistent scaling, complementary low-redundancy transformations, and block-wise operations. Extensive experiments on the ImageNet-Compatible dataset demonstrate that S4ST achieves a 77.7% average targeted success rate (tSuc), surpassing existing transformations (+17.2% over H-Aug with only 26% computational time) and SOTA TTA solutions (+6.2% over SASD-WS with 1.2M samples for post-training). Notably, it attains 69.6% and 55.3% average tSuc against three commercial APIs and vision-language models, respectively. This work establishes a new SOTA for TTAs, highlights their potential threats, and calls for a reevaluation of the data dependency in achieving targeted transferability. </p>
<blockquote>
<p>可迁移的定向攻击（TTAs）旨在欺骗黑箱模型预测特定的错误标签，但由于对代理模型的过度拟合而面临重大挑战。虽然修改图像特征以生成目标类别的稳健语义模式是一种有前景的方法，但现有方法严重依赖于大规模额外数据。这种依赖破坏了对TTA威胁的公正评估，可能导致错误的安全感或不必要的过度反应。在本文中，我们引入了两种盲测方法，即代理自我对齐和自迁移性，来分析基本转换的有效性及其相关性，以增强在严格黑箱约束下的无数据攻击。我们的研究结果挑战了传统假设：（1）攻击简单的缩放转换可以独特地增强目标迁移性，优于其他基本转换，并与领先的复杂方法相匹敌；（2）尽管跨类别相关性较弱，但几何和颜色转换表现出高度的内部冗余。这些见解推动了S4ST（强、自迁移、快速、简单缩放转换）的设计和调整，它融合了尺寸一致的缩放、互补的低冗余转换和块操作。在ImageNet兼容数据集上的广泛实验表明，S4ST的平均目标成功率（tSuc）达到77.7%，超越了现有转换（与H-Aug相比，仅用时26%的计算时间就提高了17.2%），并超越了最先进的TTA解决方案（在训练后使用120万样本的SASD-WS提高了6.2%）。值得注意的是，它对三种商业API和视觉语言模型的平均tSuc分别达到69.6%和55.3%。这项工作为TTAs设立了新的最先进水平，突显了它们的潜在威胁，并呼吁重新评估实现目标迁移性的数据依赖性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13891v2">PDF</a> 16 pages, 18 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对黑盒模型的可转移目标攻击（TTAs）面临的挑战，包括过度拟合代理模型的问题。通过引入两种盲量措施（代理自我对齐和自转移性）来评估基本转换的有效性，进而提高无数据攻击在严格黑盒约束下的效果。研究发现简单缩放转换能独特地提高目标转移性，超越其他基本转换和领先的复杂方法；几何和颜色转换展现出高内部冗余性，尽管跨类别相关性较弱。基于这些见解，设计出S4ST（强、自转移、快速、简单缩放转换）方法，集成一致性缩放、互补低冗余转换和块操作。在ImageNet兼容数据集上的实验表明，S4ST平均目标成功率（tSuc）达到77.7%，超越现有转换方法（在只有26%的计算时间内，比H-Aug高出17.2%），并超越最先进的TTA解决方案（在1.2M样本后训练时，高出6.2%）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTAs在黑盒模型中面临过度拟合代理模型的挑战。</li>
<li>引入两种盲量措施：代理自我对齐和自转移性，以评估基本转换的有效性。</li>
<li>简单缩放转换能独特提高目标转移性，超越其他基本和复杂方法。</li>
<li>几何和颜色转换具有高内部冗余性，跨类别相关性较弱。</li>
<li>S4ST方法结合一致性缩放、低冗余转换和块操作，实现高效攻击。</li>
<li>S4ST在ImageNet兼容数据集上表现优越，平均目标成功率达77.7%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e2d58b6c0907e4135602dfd11c40a065.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0b13c0361cdd6334fdb0332efedbe9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c05383eeef75afacfe11a2d757dc9da2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab63594edd5f3718bd864b24a556d906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb39e06fea9ee796b1783115153c29ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7da99e0a06ef3621a35a58898f58b95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-861c23cd5face45f2d6733fc06b870a6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-Android-Malware-Detection-The-Influence-of-ChatGPT-on-Decision-centric-Task"><a href="#Enhancing-Android-Malware-Detection-The-Influence-of-ChatGPT-on-Decision-centric-Task" class="headerlink" title="Enhancing Android Malware Detection: The Influence of ChatGPT on   Decision-centric Task"></a>Enhancing Android Malware Detection: The Influence of ChatGPT on   Decision-centric Task</h2><p><strong>Authors:Yao Li, Sen Fang, Tao Zhang, Haipeng Cai</strong></p>
<p>With the rise of large language models, such as ChatGPT, non-decisional models have been applied to various tasks. Moreover, ChatGPT has drawn attention to the traditional decision-centric task of Android malware detection. Despite effective detection methods proposed by scholars, they face low interpretability issues. Specifically, while these methods excel in classifying applications as benign or malicious and can detect malicious behavior, they often fail to provide detailed explanations for the decisions they make. This challenge raises concerns about the reliability of existing detection schemes and questions their true ability to understand complex data. In this study, we investigate the influence of the non-decisional model, ChatGPT, on the traditional decision-centric task of Android malware detection. We choose three state-of-the-art solutions, Drebin, XMAL, and MaMaDroid, conduct a series of experiments on publicly available datasets, and carry out a comprehensive comparison and analysis. Our findings indicate that these decision-driven solutions primarily rely on statistical patterns within datasets to make decisions, rather than genuinely understanding the underlying data. In contrast, ChatGPT, as a non-decisional model, excels in providing comprehensive analysis reports, substantially enhancing interpretability. Furthermore, we conduct surveys among experienced developers. The result highlights developers’ preference for ChatGPT, as it offers in-depth insights and enhances efficiency and understanding of challenges. Meanwhile, these studies and analyses offer profound insights, presenting developers with a novel perspective on Android malware detection–enhancing the reliability of detection results from a non-decisional perspective. </p>
<blockquote>
<p>随着大型语言模型（如ChatGPT）的兴起，非决策模型已应用于各种任务。此外，ChatGPT引起了人们对传统决策导向任务——Android恶意软件检测的关注。尽管学者已经提出了有效的检测方法，但它们存在解释性不足的问题。具体来说，这些方法虽然在将应用程序分类为良性或恶意以及检测恶意行为方面表现出色，但它们往往无法为所做的决策提供详细的解释。这一挑战引发了人们对现有检测方案可靠性的担忧，并对它们真正理解复杂数据的能力提出质疑。在本研究中，我们调查了非决策模型ChatGPT对传统决策导向任务——Android恶意软件检测的影响。我们选择了三种最新解决方案：Drebin、XMAL和MaMaDroid，在公开数据集上进行了一系列实验，并进行了全面的比较和分析。我们的研究结果表明，这些决策驱动解决方案主要依赖于数据集内的统计模式来做出决策，而非真正了解底层数据。相比之下，ChatGPT作为一个非决策模型，在提供综合分析报告方面表现出色，大大提高了可解释性。此外，我们对经验丰富的开发者进行了调查。结果表明，开发者更倾向于使用ChatGPT，因为它提供了深入的见解，提高了对挑战的理解和效率。同时，这些研究和分析为开发者提供了关于Android恶意软件检测的全新视角——从非决策角度提高检测结果可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04352v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型如ChatGPT的兴起，非决策模型已应用于多项任务。本研究探讨了非决策模型ChatGPT对传统的以决策为中心的Android恶意软件检测任务的影响。研究发现在现有决策驱动解决方案（如Drebin，XMAL和MaMaDroid）中，它们主要依赖于数据集中的统计模式来做出决策，而不是真正理解数据。相反，ChatGPT作为一个非决策模型，在提供综合分析报告方面表现出色，大大提高了可解释性。此外，通过对经验丰富的开发者的调查，发现开发者更倾向于使用ChatGPT，因为它提供了深入的见解，提高了对挑战的理解和效率。这项研究为开发者提供了一个新的角度来检测Android恶意软件，从非决策的角度提高了检测结果的可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型如ChatGPT在非决策任务中的应用日益广泛。</li>
<li>传统决策驱动的Android恶意软件检测方法面临低解释性的问题。</li>
<li>现有方法主要依赖于数据集中的统计模式来做出决策，缺乏真正的数据理解。</li>
<li>ChatGPT作为一个非决策模型，在提供综合分析报告方面表现出色，提高了可解释性。</li>
<li>开发者更倾向于使用ChatGPT，因为它提供了深入的见解，提高了对挑战的理解和效率。</li>
<li>研究为开发者提供了一个新的角度来检测Android恶意软件。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-22cce304e839d77c24a7bc9e49cad1b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0a5347726fd8adf949a850fd5b9552e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PixWizard-Versatile-Image-to-Image-Visual-Assistant-with-Open-Language-Instructions"><a href="#PixWizard-Versatile-Image-to-Image-Visual-Assistant-with-Open-Language-Instructions" class="headerlink" title="PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language   Instructions"></a>PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language   Instructions</h2><p><strong>Authors:Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Huan Teng, Junlin Xie, Yu Qiao, Peng Gao, Hongsheng Li</strong></p>
<p>This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an Omni Pixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction templates in natural language, we comprehensively include a large set of diverse vision tasks such as text-to-image generation, image restoration, image grounding, dense image prediction, image editing, controllable generation, inpainting&#x2F;outpainting, and more. Furthermore, we adopt Diffusion Transformers (DiT) as our foundation model and extend its capabilities with a flexible any resolution mechanism, enabling the model to dynamically process images based on the aspect ratio of the input, closely aligning with human perceptual processes. The model also incorporates structure-aware and semantic-aware guidance to facilitate effective fusion of information from the input image. Our experiments demonstrate that PixWizard not only shows impressive generative and understanding abilities for images with diverse resolutions but also exhibits promising generalization capabilities with unseen tasks and human instructions. The code and related resources are available at <a target="_blank" rel="noopener" href="https://github.com/AFeng-x/PixWizard">https://github.com/AFeng-x/PixWizard</a> </p>
<blockquote>
<p>本文介绍了一个通用图像到图像的视觉助手PixWizard，它旨在基于自由的语言指令进行图像生成、操作和翻译。为此，我们将各种视觉任务纳入统一的图像文本到图像生成框架，并创建了一个Omni Pixel-to-Pixel Instruction-Tuning数据集。通过构建自然语言中的详细指令模板，我们全面涵盖了大量不同的视觉任务，如文本到图像生成、图像恢复、图像定位、密集图像预测、图像编辑、可控生成、填充&#x2F;描边等。此外，我们采用扩散变压器（DiT）作为基础模型，并通过灵活的任何分辨率机制扩展其功能，使模型能够根据输入的比例动态处理图像，与人类感知过程紧密对齐。该模型还结合了结构感知和语义感知指导，以促进输入图像中信息的有效融合。我们的实验表明，PixWizard不仅在对具有不同分辨率的图像的理解和生成能力方面表现出色，而且在未知任务和人类指令方面也表现出有希望的泛化能力。相关代码和资源可在<a target="_blank" rel="noopener" href="https://github.com/AFeng-x/PixWizard%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AFeng-x/PixWizard上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15278v4">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/AFeng-x/PixWizard">https://github.com/AFeng-x/PixWizard</a></p>
<p><strong>Summary</strong><br>PixWizard是一款基于自由语言指令的图像到图像视觉助手，用于图像生成、操作和转换。它建立了一个统一的图像-文本到图像生成框架，并采用了Diffusion Transformers作为基础模型。通过详细的自然语言指令模板，它包括各种视觉任务，如文本到图像生成、图像恢复、图像定位等。它采用灵活的分辨率机制和结构与语义感知指导，可有效融合输入图像的信息。实验表明，PixWizard在多样分辨率的图像处理上展现出令人印象深刻的表现能力，且在未见任务和人指令上展现出良好的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PixWizard是一个图像到图像的视觉助手，可以进行图像生成、操作和转换。</li>
<li>它采用统一的图像-文本到图像生成框架处理多种视觉任务。</li>
<li>PixWizard使用Diffusion Transformers作为基础模型，并具备灵活的分辨率机制。</li>
<li>该模型采用自然语言指令模板，涵盖多种视觉任务，如文本到图像生成、图像恢复等。</li>
<li>模型结合结构感知和语义感知指导，有效融合输入图像的信息。</li>
<li>实验显示PixWizard在多样分辨率的图像处理上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-683fbeecf24ceded7e1f02f4064e5070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a165250cac75a46bfd7a0f9aca390e00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f7fc29b12bc5a21314e1b2bf530294d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335b3688fe8b6e6e1ad4b7930b8720f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RRM-Robust-Reward-Model-Training-Mitigates-Reward-Hacking"><a href="#RRM-Robust-Reward-Model-Training-Mitigates-Reward-Hacking" class="headerlink" title="RRM: Robust Reward Model Training Mitigates Reward Hacking"></a>RRM: Robust Reward Model Training Mitigates Reward Hacking</h2><p><strong>Authors:Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh</strong></p>
<p>Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%. </p>
<blockquote>
<p>奖励模型（RM）在将大型语言模型（LLM）与人类偏好对齐方面发挥着至关重要的作用。然而，传统的RM训练依赖于与特定提示相关联的响应对，这使其在区分提示驱动的偏好和提示无关的伪特征（如响应长度和格式）时遇到困难。在这项工作中，我们揭示了当前RM训练方法的基本局限性，即RM在确定偏好时无法有效区分上下文信号和无关伪特征。为了解决这一问题，我们引入了一个独立于这些伪特征的因果框架，并提出了一种旨在消除这些伪特征的新型数据增强技术。大量实验表明，我们的方法成功过滤掉了不必要的伪特征，从而产生了一个更稳健的奖励模型（RRM）。我们的RRM在Gemma-2-9b-it上训练的成对奖励模型的性能在RewardBench上有所提高，准确率从80.61%提高到84.15%。此外，我们使用RM和RRM训练了两个DPO策略，证明RRM显著增强了与DPO对齐的策略，MT-Bench得分从7.27提高到8.31，在AlpacaEval-2中的长度控制胜率从33.46%提高到52.49%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13156v2">PDF</a> Accepted in ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了奖励模型（RMs）在大规模语言模型（LLMs）中的重要性及其面临的挑战。针对传统RM训练无法有效区分上下文信号和无关伪影的问题，本文提出了一种因果框架和一种新的数据增强技术来消除伪影干扰。实验证明，该方法能够成功过滤掉不需要的伪影，产生更稳健的奖励模型（RRM）。RRM提高了基于Gemma-2-9b-it的配对奖励模型在RewardBench上的准确性，从80.61%提高到84.15%。此外，使用RM和RRM训练的两种DPO策略显示，RRM显著提高了与DPO对齐的政策，MT-Bench分数从7.27提高到8.31，在AlpacaEval-2中的长度控制胜率从33.46%提高到52.49%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励模型（RMs）在大规模语言模型（LLMs）中对齐人类偏好方面起着至关重要的作用。</li>
<li>传统RM训练面临无法有效区分上下文信号和无关伪影的挑战。</li>
<li>引入了一种因果框架和新的数据增强技术，以消除伪影并产生更稳健的奖励模型（RRM）。</li>
<li>RRM能提高配对奖励模型在特定任务上的准确性。</li>
<li>RRM显著提高与DPO对齐的政策表现，体现在多个评估指标上的改善。</li>
<li>实验证明RRM能够过滤掉不需要的伪影，提升模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9782e48d12fb701c2003e1ff4c31054c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cebb9064a82bafeddd39eb929d6495c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b67387f1127198eed2003190cb906052.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-84d4f38d510f1dbd809b3d5331d556ad.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-01  Multi-Agent Verification Scaling Test-Time Compute with Multiple   Verifiers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Talking Head Generation/2502.18725v1/page_0_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-02-28  Talking to the brain Using Large Language Models as Proxies to Model   Brain Semantic Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19758k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
