<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  R2-T2 Re-Routing in Test-Time for Multimodal Mixture-of-Experts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6f1411ab09e736020130cdd8d94a5c7e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-01-æ›´æ–°"><a href="#2025-03-01-æ›´æ–°" class="headerlink" title="2025-03-01 æ›´æ–°"></a>2025-03-01 æ›´æ–°</h1><h2 id="R2-T2-Re-Routing-in-Test-Time-for-Multimodal-Mixture-of-Experts"><a href="#R2-T2-Re-Routing-in-Test-Time-for-Multimodal-Mixture-of-Experts" class="headerlink" title="R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts"></a>R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts</h2><p><strong>Authors:Zhongyang Li, Ziyue Li, Tianyi Zhou</strong></p>
<p>In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)â€™ powerful reasoning capabilities, deterring LMMsâ€™ performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method â€œRe-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMsâ€™ performance on challenging benchmarks of diverse tasks, without training any base-model parameters. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­ï¼Œéè¯­è¨€æ¨¡æ€ï¼ˆä¾‹å¦‚è§†è§‰è¡¨ç¤ºï¼‰çš„æ„ŸçŸ¥é€šå¸¸ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ä¸ç›¸åŒ¹é…ï¼Œè¿™åˆ¶çº¦äº†LMMsåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¿™ä¸€å¼±ç‚¹æœ€è¿‘å¾—åˆ°äº†ç¼“è§£ï¼Œé€šè¿‡å°†è§†è§‰ç¼–ç å™¨æ›¿æ¢ä¸ºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰ï¼Œæä¾›äº†ä¸°å¯Œã€å¤šç²’åº¦å’Œå¤šæ ·åŒ–çš„è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºè¢«ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€è¦ã€‚å¤šæ¨¡æ€MoEçš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…¶è·¯ç”±å™¨ï¼Œè¯¥è·¯ç”±å™¨ä¼šé‡æ–°åŠ æƒå¹¶æ··åˆä¸åŒä¸“å®¶çš„è¡¨ç¤ºä»¥é€‚ç”¨äºæ¯ä¸ªè¾“å…¥ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç«¯åˆ°ç«¯è®­ç»ƒçš„è·¯ç”±å™¨å¹¶ä¸æ€»æ˜¯ä¸ºæ¯ä¸€ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆæœ€ä½³çš„è·¯ç”±æƒé‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆæ–¹æ³•â€”â€”â€œæµ‹è¯•æ—¶çš„é‡æ–°è·¯ç”±ï¼ˆR2-T2ï¼‰â€ï¼Œè¯¥æ–¹æ³•åœ¨æµ‹è¯•æ—¶å±€éƒ¨ä¼˜åŒ–è·¯ç”±æƒé‡å‘é‡ï¼Œå°†å…¶å‘æµ‹è¯•æ ·æœ¬é‚»åŸŸä¸­æ­£ç¡®é¢„æµ‹æ ·æœ¬çš„å‘é‡ç§»åŠ¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å…·æœ‰ä¸åŒä¼˜åŒ–ç›®æ ‡å’Œé‚»å±…æœç´¢ç©ºé—´çš„R2-T2ç­–ç•¥ã€‚R2-T2åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¸”å¤§å¤§æé«˜äº†LMMsçš„æ€§èƒ½è¡¨ç°ï¼Œæ— éœ€è®­ç»ƒä»»ä½•åŸºç¡€æ¨¡å‹çš„å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20395v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåœ¨å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­ï¼Œéè¯­è¨€æ¨¡æ€ï¼ˆå¦‚è§†è§‰è¡¨ç¤ºï¼‰çš„æ„ŸçŸ¥é€šå¸¸æ— æ³•ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ç›¸åŒ¹é…ï¼Œå½±å“äº†LMMsåœ¨æŒ‘æˆ˜æ€§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ›¿æ¢è§†è§‰ç¼–ç å™¨æ¥å‡è½»è¿™ä¸€å¼±ç‚¹ï¼Œä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡æä¾›ä¸°å¯Œã€å¤šç²’åº¦å’Œå¤šæ ·åŒ–çš„è¡¨ç¤ºã€‚å¤šæ¨¡æ€MoEçš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…¶è·¯ç”±å™¨ï¼Œå®ƒä¸ºæ¯ä¸ªè¾“å…¥é‡æ–°åŠ æƒå’Œæ··åˆä¸åŒä¸“å®¶çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç«¯åˆ°ç«¯è®­ç»ƒçš„è·¯ç”±å™¨å¹¶ä¸æ€»æ˜¯ä¸ºæ¯ä¸ªæµ‹è¯•æ ·æœ¬ç”Ÿæˆæœ€ä½³çš„è·¯ç”±æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆæ–¹æ³•â€œæµ‹è¯•æ—¶çš„é‡æ–°è·¯ç”±ï¼ˆR2-T2ï¼‰â€ï¼Œå®ƒåœ¨æµ‹è¯•æ—¶é€šè¿‡å‘é‚»è¿‘çš„æ­£ç¡®é¢„æµ‹æ ·æœ¬çš„å‘é‡ç§»åŠ¨è·¯ç”±æƒé‡å‘é‡æ¥è¿›è¡Œå±€éƒ¨ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å…·æœ‰ä¸åŒä¼˜åŒ–ç›®æ ‡å’Œé‚»è¿‘æœç´¢ç©ºé—´çš„R2-T2ç­–ç•¥ã€‚R2-T2åœ¨ä¸è®­ç»ƒä»»ä½•åŸºç¡€æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆæé«˜äº†æœ€æ–°LMMsåœ¨å…·æœ‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤„ç†éè¯­è¨€æ¨¡æ€ï¼ˆå¦‚è§†è§‰è¡¨ç¤ºï¼‰æ—¶å­˜åœ¨æ„ŸçŸ¥èƒ½åŠ›ä¸è¶³çš„å¼±ç‚¹ï¼Œå½±å“å…¶æ€§èƒ½ã€‚</li>
<li>å¼•å…¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¯ä»¥æ”¹å–„LMMsåœ¨éè¯­è¨€æ¨¡æ€æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€MoEçš„æ€§èƒ½å–å†³äºå…¶è·¯ç”±å™¨ï¼Œå®ƒèƒ½æ ¹æ®æ¯ä¸ªè¾“å…¥é‡æ–°åŠ æƒå’Œæ··åˆä¸åŒä¸“å®¶çš„è¡¨ç¤ºã€‚</li>
<li>æµ‹è¯•æ—¶çš„é‡æ–°è·¯ç”±ï¼ˆR2-T2ï¼‰æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œç”¨äºåœ¨æµ‹è¯•æ—¶ä¼˜åŒ–è·¯ç”±å™¨çš„è·¯ç”±æƒé‡ã€‚</li>
<li>R2-T2é€šè¿‡å‘é‚»è¿‘çš„æ­£ç¡®é¢„æµ‹æ ·æœ¬çš„å‘é‡ç§»åŠ¨è·¯ç”±æƒé‡å‘é‡æ¥è¿›è¡Œå±€éƒ¨ä¼˜åŒ–ã€‚</li>
<li>R2-T2æå‡ºäº†ä¸‰ç§å…·æœ‰ä¸åŒä¼˜åŒ–ç›®æ ‡å’Œé‚»è¿‘æœç´¢ç©ºé—´çš„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed2036d099d4254d59e2d6e78305301f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e28d4d237359a78bfe3c21b45fc79c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87e41725c0af3d247a8e93905df485fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02fd94b217dadaf9abc59cbe73d9265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58931de0e2cf7cd623c47fe5ccc1c4d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Verification-Scaling-Test-Time-Compute-with-Multiple-Verifiers"><a href="#Multi-Agent-Verification-Scaling-Test-Time-Compute-with-Multiple-Verifiers" class="headerlink" title="Multi-Agent Verification: Scaling Test-Time Compute with Multiple   Verifiers"></a>Multi-Agent Verification: Scaling Test-Time Compute with Multiple   Verifiers</h2><p><strong>Authors:Shalev Lifshitz, Sheila A. McIlraith, Yilun Du</strong></p>
<p>By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time. </p>
<blockquote>
<p>åˆ©ç”¨æµ‹è¯•æ—¶çš„æ›´å¤šè®¡ç®—èµ„æºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥åœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œæ”¹è¿›ã€‚ä¸€ç§å¸¸è§ç­–ç•¥æ˜¯ä½¿ç”¨éªŒè¯å™¨æ¥è¯„ä¼°å€™é€‰è¾“å‡ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºæµ‹è¯•æ—¶çš„è®¡ç®—æå‡ºäº†ä¸€ç§æ–°å‹æ‰©å±•ç»´åº¦ï¼šæ‰©å±•éªŒè¯å™¨çš„æ•°é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šä»£ç†éªŒè¯ï¼ˆMAVï¼‰ä½œä¸ºæµ‹è¯•æ—¶è®¡ç®—çš„ä¸€ç§èŒƒå¼ï¼Œå®ƒå°†å¤šä¸ªéªŒè¯å™¨ç»“åˆèµ·æ¥ä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨æ–¹é¢éªŒè¯å™¨ï¼ˆAVsï¼‰ä½œä¸ºMAVç³»ç»Ÿä¸­éªŒè¯å™¨çš„ä¸€ç§å¯èƒ½é€‰æ‹©ï¼Œæ–¹é¢éªŒè¯å™¨æ˜¯å³æ’å³ç”¨çš„LLMï¼Œå¯æç¤ºéªŒè¯è¾“å‡ºçš„ä¸åŒæ–¹é¢ã€‚æ–¹é¢éªŒè¯å™¨æ˜¯MAVçš„ä¾¿æ·æ„å»ºå—ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ç»“åˆèµ·æ¥ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†BoN-MAVï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å¤šä»£ç†éªŒè¯ç®—æ³•ï¼Œå®ƒå°†æœ€ä½³né‡‡æ ·ä¸å¤šä¸ªéªŒè¯å™¨ç»“åˆèµ·æ¥ã€‚BoN-MAVè¡¨ç°å‡ºæ¯”è‡ªæˆ‘ä¸€è‡´æ€§å¥–åŠ±æ¨¡å‹éªŒè¯æ›´å¼ºçš„æ‰©å±•æ¨¡å¼ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä»å¼±åˆ°å¼ºçš„æ³›åŒ–ï¼Œå…¶ä¸­ç»“åˆå¼±éªŒè¯å™¨ç”šè‡³å¯ä»¥æé«˜æ›´å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥åŠè‡ªæˆ‘æ”¹è¿›ï¼Œå…¶ä¸­ä½¿ç”¨ç›¸åŒçš„åŸºå‡†æ¨¡å‹æ¥ç”Ÿæˆå’ŒéªŒè¯è¾“å‡ºã€‚æˆ‘ä»¬çš„ç»“æœå°†æ‰©å±•éªŒè¯å™¨çš„æ•°é‡ç¡®ç«‹ä¸ºæé«˜è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶æ€§èƒ½çš„æœ‰å‰é€”çš„æ–°ç»´åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶åˆ©ç”¨æ›´å¤šçš„è®¡ç®—èµ„æºå¯ä»¥æå‡æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ç»´åº¦ï¼šæ‰©å±•éªŒè¯å™¨æ•°é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šä»£ç†éªŒè¯ï¼ˆMAVï¼‰ä½œä¸ºæµ‹è¯•æ—¶è®¡ç®—èŒƒå¼ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªéªŒè¯å™¨æ¥æé«˜æ€§èƒ½ã€‚æå‡ºä½¿ç”¨æ–¹é¢éªŒè¯å™¨ï¼ˆAVï¼‰ä½œä¸ºMAVç³»ç»Ÿä¸­éªŒè¯å™¨çš„ä¸€ç§å¯èƒ½é€‰æ‹©ï¼ŒAVæ˜¯ç°æˆçš„LLMï¼Œç»è¿‡æç¤ºå¯ç”¨äºéªŒè¯è¾“å‡ºçš„ä¸åŒæ–¹é¢ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒï¼Œæ˜“äºç»“åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†BoN-MAVå¤šä»£ç†éªŒè¯ç®—æ³•ï¼Œé€šè¿‡æœ€ä½³né‡‡æ ·ç»“åˆå¤šä¸ªéªŒè¯å™¨ã€‚BoN-MAVè¡¨ç°å‡ºæ¯”è‡ªæˆ‘ä¸€è‡´æ€§å¥–åŠ±æ¨¡å‹éªŒè¯æ›´å¼ºçš„æ‰©å±•æ¨¡å¼ï¼Œå¹¶å±•ç¤ºäº†ä»å¼±åˆ°å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå³ç»“åˆå¼±éªŒè¯å™¨å¯ä»¥æé«˜ç”šè‡³æ›´å¼ºçš„LLMæ€§èƒ½ï¼Œä»¥åŠè‡ªæˆ‘æ”¹è¿›èƒ½åŠ›ï¼Œå³ä½¿ç”¨åŒä¸€åŸºç¡€æ¨¡å‹æ¥ç”Ÿæˆå’ŒéªŒè¯è¾“å‡ºã€‚æˆ‘ä»¬çš„ç»“æœå°†æ‰©å±•éªŒè¯å™¨æ•°é‡ä½œä¸ºæé«˜è¯­è¨€æ¨¡å‹æµ‹è¯•æ€§èƒ½çš„æ–°ç»´åº¦ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æµ‹è¯•æ—¶çš„æ›´å¤šè®¡ç®—èµ„æºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æå‡æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•ç»´åº¦ï¼šæ‰©å±•éªŒè¯å™¨æ•°é‡ã€‚</li>
<li>å¼•å…¥äº†å¤šä»£ç†éªŒè¯ï¼ˆMAVï¼‰ä½œä¸ºæµ‹è¯•æ—¶è®¡ç®—èŒƒå¼ï¼Œèƒ½ç»“åˆå¤šä¸ªéªŒè¯å™¨ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>æ–¹é¢éªŒè¯å™¨ï¼ˆAVï¼‰æ˜¯ä¸€ç§å¯èƒ½çš„éªŒè¯å™¨é€‰æ‹©ï¼Œæ˜¯ç°æˆçš„LLMï¼Œå¯éªŒè¯è¾“å‡ºçš„ä¸åŒæ–¹é¢ï¼Œä¸”æ˜“äºç»“åˆã€‚</li>
<li>ä»‹ç»äº†BoN-MAVå¤šä»£ç†éªŒè¯ç®—æ³•ï¼Œå®ƒé€šè¿‡æœ€ä½³né‡‡æ ·ç»“åˆå¤šä¸ªéªŒè¯å™¨ã€‚</li>
<li>BoN-MAVè¡¨ç°å‡ºæ¯”è‡ªæˆ‘ä¸€è‡´æ€§å¥–åŠ±æ¨¡å‹éªŒè¯æ›´å¼ºçš„æ‰©å±•æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2b76a62b0fde8b559d0309c285dac75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-201dbf6a0def84242a23493acd57d1a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d7fad77f7243459384cbd85863a2ab4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1336bd377a42deb455d8003898741945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-693e8166ef1b67b991cfeda3ced6181f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PhantomWiki-On-Demand-Datasets-for-Reasoning-and-Retrieval-Evaluation"><a href="#PhantomWiki-On-Demand-Datasets-for-Reasoning-and-Retrieval-Evaluation" class="headerlink" title="PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation"></a>PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation</h2><p><strong>Authors:Albert Gong, KamilÄ— StankeviÄiÅ«tÄ—, Chao Wan, Anmol Kabra, Raphael Thesmar, Johann Lee, Julius Klenke, Carla P. Gomes, Kilian Q. Weinberger</strong></p>
<p>High-quality benchmarks are essential for evaluating reasoning and retrieval capabilities of large language models (LLMs). However, curating datasets for this purpose is not a permanent solution as they are prone to data leakage and inflated performance results. To address these challenges, we propose PhantomWiki: a pipeline to generate unique, factually consistent document corpora with diverse question-answer pairs. Unlike prior work, PhantomWiki is neither a fixed dataset, nor is it based on any existing data. Instead, a new PhantomWiki instance is generated on demand for each evaluation. We vary the question difficulty and corpus size to disentangle reasoning and retrieval capabilities respectively, and find that PhantomWiki datasets are surprisingly challenging for frontier LLMs. Thus, we contribute a scalable and data leakage-resistant framework for disentangled evaluation of reasoning, retrieval, and tool-use abilities. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/kilian-group/phantom-wiki">https://github.com/kilian-group/phantom-wiki</a>. </p>
<blockquote>
<p>é«˜è´¨é‡åŸºå‡†æµ‹è¯•å¯¹äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¸ºæ­¤ç›®çš„æ•´ç†æ•°æ®é›†å¹¶éé•¿ä¹…ä¹‹è®¡ï¼Œå› ä¸ºå®ƒä»¬å®¹æ˜“å‡ºç°æ•°æ®æ³„éœ²å’Œæ€§èƒ½ç»“æœè†¨èƒ€çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PhantomWikiï¼šä¸€ä¸ªç”Ÿæˆç‹¬ç‰¹ã€äº‹å®ä¸€è‡´ã€æ–‡æ¡£è¯­æ–™åº“å¤šæ ·ã€é—®é¢˜ç­”æ¡ˆå¯¹ä¸°å¯Œçš„ç®¡é“ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼ŒPhantomWikiæ—¢ä¸æ˜¯å›ºå®šæ•°æ®é›†ï¼Œä¹Ÿä¸æ˜¯åŸºäºä»»ä½•ç°æœ‰æ•°æ®çš„ã€‚ç›¸åï¼Œä¸ºæ¯ä¸ªè¯„ä¼°éƒ½ä¼šç”Ÿæˆæ–°çš„PhantomWikiå®ä¾‹ã€‚æˆ‘ä»¬æ”¹å˜é—®é¢˜çš„éš¾åº¦å’Œè¯­æ–™åº“çš„å¤§å°ï¼Œä»¥åˆ†åˆ«è§£å¼€æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›ï¼Œå‘ç°PhantomWikiæ•°æ®é›†å¯¹å‰æ²¿çš„LLMæ¨¡å‹å…·æœ‰å‡ºäººæ„æ–™çš„æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ºæ¨ç†ã€æ£€ç´¢å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è„±èŠ‚è¯„ä¼°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”é˜²æ•°æ®æ³„éœ²çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kilian-group/phantom-wiki%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kilian-group/phantom-wikiæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20377v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›è¯„ä¼°éœ€è¦é«˜è´¨é‡çš„æ ‡å‡†ã€‚ç„¶è€Œï¼Œä¸ºè¿™ä¸€ç›®çš„åˆ¶ä½œæ•°æ®é›†å¹¶éé•¿ä¹…ä¹‹è®¡ï¼Œå› ä¸ºå®ƒä»¬å®¹æ˜“å­˜åœ¨æ•°æ®æ³„éœ²å’Œæ€§èƒ½è†¨èƒ€çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PhantomWikiï¼šä¸€ä¸ªç”Ÿæˆç‹¬ç‰¹ã€äº‹å®ä¸€è‡´ã€æ–‡æ¡£è¯­æ–™åº“ä¸å¤šæ ·é—®é¢˜ç­”æ¡ˆå¯¹çš„ç®¡é“ã€‚ä¸åŒäºä¹‹å‰çš„å·¥ä½œï¼ŒPhantomWikiæ—¢ä¸æ˜¯å›ºå®šçš„æ•°æ®é›†ï¼Œä¹Ÿä¸æ˜¯åŸºäºä»»ä½•ç°æœ‰æ•°æ®çš„ã€‚ç›¸åï¼Œä¸ºæ¯ä¸ªè¯„ä¼°éƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„PhantomWikiå®ä¾‹ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæ•´é—®é¢˜çš„éš¾åº¦å’Œè¯­æ–™åº“çš„å¤§å°æ¥åˆ†åˆ«è§£å¼€æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›ï¼Œå‘ç°PhantomWikiæ•°æ®é›†å¯¹å‰æ²¿çš„LLMæ¨¡å‹å…·æœ‰å‡ºäººæ„æ–™çš„æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ºæ¨ç†ã€æ£€ç´¢å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›çš„è„±èŠ‚è¯„ä¼°æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æŠ—æ•°æ®æ³„éœ²çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kilian-group/phantom-wiki%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kilian-group/phantom-wikiæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡æ ‡å‡†å¯¹äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ•°æ®é›†å­˜åœ¨æ•°æ®æ³„éœ²å’Œæ€§èƒ½è†¨èƒ€çš„é—®é¢˜ï¼Œå› æ­¤ä¸æ˜¯é•¿ä¹…ä¹‹è®¡ã€‚</li>
<li>PhantomWikiæ˜¯ä¸€ä¸ªç”Ÿæˆç‹¬ç‰¹ã€äº‹å®ä¸€è‡´æ–‡æ¡£è¯­æ–™åº“ä¸é—®é¢˜ç­”æ¡ˆå¯¹çš„ç®¡é“ï¼Œä¸åŒäºå›ºå®šæˆ–åŸºäºç°æœ‰æ•°æ®çš„æ•°æ®é›†ã€‚</li>
<li>PhantomWikiå®ä¾‹æŒ‰éœ€ç”Ÿæˆï¼Œé€‚ç”¨äºæ¯æ¬¡è¯„ä¼°ã€‚</li>
<li>é€šè¿‡è°ƒæ•´é—®é¢˜éš¾åº¦å’Œè¯­æ–™åº“å¤§å°ï¼Œå¯ä»¥è§£å¼€æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>PhantomWikiæ•°æ®é›†å¯¹å‰æ²¿LLMæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16b641b3f0895a09e444c38dfb5fe5a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4bf820c52b886321e88e0a37d51df0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c0f0e670f9cf43d92edfa383894d773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00c9058534c053a10867796aeb8470a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30cb86e04d0bf6629bf58c1a6491a42f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="M-3Builder-A-Multi-Agent-System-for-Automated-Machine-Learning-in-Medical-Imaging"><a href="#M-3Builder-A-Multi-Agent-System-for-Automated-Machine-Learning-in-Medical-Imaging" class="headerlink" title="M^3Builder: A Multi-Agent System for Automated Machine Learning in   Medical Imaging"></a>M^3Builder: A Multi-Agent System for Automated Machine Learning in   Medical Imaging</h2><p><strong>Authors:Jinghao Feng, Qiaoyu Zheng, Chaoyi Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging. </p>
<blockquote>
<p>åŒ»å­¦æ™ºèƒ½ä½“ç³»ç»Ÿå› å…¶èƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¤æ‚ä»»åŠ¡è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å‡†å¤‡å¥½çš„å·¥å…·çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œå› ä¸ºåŒ»å­¦é¢†åŸŸéœ€è¦è®­ç»ƒä¸“é—¨æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼š</p>
</blockquote>
<p>ï¼ˆä¸€ï¼‰æˆ‘ä»¬æå‡ºäº†M3Builderï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°åŒ»å­¦å½±åƒä¸­çš„æœºå™¨å­¦ä¹ è‡ªåŠ¨åŒ–ã€‚M3Builderçš„æ ¸å¿ƒé‡‡ç”¨äº†å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“è¿›è¡Œåä½œï¼Œä»¥å¤„ç†å¤æ‚çš„åŒ»å­¦æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†ã€ç¯å¢ƒé…ç½®ã€è‡ªä¸»è°ƒè¯•å’Œæ¨¡å‹è®­ç»ƒç­‰å¤šä¸ªæ­¥éª¤ã€‚è¿™äº›æ™ºèƒ½ä½“åœ¨ä¸€ä¸ªåŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ å·¥ä½œåŒºå†…è¿è¡Œï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„åŒ–ç¯å¢ƒï¼Œä¸ºæ™ºèƒ½ä½“æä¾›æ•°æ®é›†ã€è®­ç»ƒä»£ç å’Œäº¤äº’å·¥å…·çš„æ–‡æœ¬æè¿°ï¼Œä»¥å®ç°æ— ç¼æ²Ÿé€šå’Œä»»åŠ¡æ‰§è¡Œã€‚</p>
<p>ï¼ˆäºŒï¼‰ä¸ºäº†è¯„ä¼°åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ çš„è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼Œæˆ‘ä»¬æ¨å‡ºäº†M3Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å››é¡¹é€šç”¨ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠ14ä¸ªè®­ç»ƒæ•°æ®é›†ã€äº”ä¸ªè§£å‰–éƒ¨ä½å’Œä¸‰ç§æˆåƒæ¨¡å¼ï¼Œæ¶µç›–äºŒç»´å’Œä¸‰ç»´æ•°æ®ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20301v1">PDF</a> 38 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong><br>M3Builderæ˜¯ä¸€ç§æ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸“ä¸ºè‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ è€Œè®¾è®¡ã€‚å®ƒé€šè¿‡å››ä¸ªä¸“é—¨æ™ºèƒ½ä½“çš„åä½œï¼Œå¯å®Œæˆä»æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†å’Œç¯å¢ƒé…ç½®åˆ°è‡ªä¸»è°ƒè¯•å’Œæ¨¡å‹è®­ç»ƒçš„å¤æ‚å¤šæ­¥éª¤åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ã€‚æœ¬æ–‡çš„è´¡çŒ®åŒ…æ‹¬ï¼šæå‡ºM3Builderç³»ç»Ÿï¼Œå»ºç«‹åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ çš„å·¥ä½œç©ºé—´ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›æ•°æ®é›†ã€è®­ç»ƒä»£ç å’Œäº’åŠ¨å·¥å…·çš„è‡ªç”±æ–‡æœ¬æè¿°ï¼Œå®ç°æ— ç¼æ²Ÿé€šå’Œä»»åŠ¡æ‰§è¡Œï¼›å»ºç«‹M3BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«å››é¡¹ä¸€èˆ¬ä»»åŠ¡åœ¨14ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šï¼Œè¦†ç›–äº”ç§è§£å‰–ç»“æ„å’Œä¸‰ç§æˆåƒæ¨¡å¼ï¼Œæ¶‰åŠäºŒç»´å’Œä¸‰ç»´æ•°æ®ï¼›å®éªŒä½¿ç”¨ä¸ƒç§å…ˆè¿›çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä½“æ ¸å¿ƒï¼Œå¦‚Claudeç³»åˆ—ã€GPT-4oå’ŒDeepSeek-V3ã€‚ç›¸è¾ƒäºç°æœ‰çš„æœºå™¨å­¦ä¹ æ™ºèƒ½ä½“è®¾è®¡ï¼ŒM3Builderåœ¨åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½¿ç”¨Claude-3.7-Sonnetä½œä¸ºæ™ºèƒ½ä½“æ ¸å¿ƒæ—¶æˆåŠŸç‡ä¸º94.29%ï¼Œæ˜¾ç¤ºå‡ºåœ¨å…¨è‡ªåŠ¨åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>M3Builderæ˜¯ä¸€ç§å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ æµç¨‹ã€‚</li>
<li>M3BuilderåŒ…å«å››ä¸ªä¸“é—¨æ™ºèƒ½ä½“ï¼Œå¯åä½œå®Œæˆå¤æ‚çš„å¤šæ­¥éª¤åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚</li>
<li>å»ºç«‹äº†åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ çš„å·¥ä½œç©ºé—´ï¼Œæä¾›æ™ºèƒ½ä½“æ‰€éœ€çš„æ•°æ®é›†å’Œè‡ªç”±æ–‡æœ¬æè¿°ã€‚</li>
<li>æå‡ºM3BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ çš„è¿›å±•ã€‚</li>
<li>M3Builderå®éªŒä½¿ç”¨å¤šç§è‡ªç„¶è¯­è¨€æ¨¡å‹ä½œä¸ºæ™ºèƒ½ä½“æ ¸å¿ƒã€‚</li>
<li>M3Builderåœ¨åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä½¿ç”¨Claude-3.7-Sonnetä½œä¸ºæ™ºèƒ½ä½“æ ¸å¿ƒæ—¶æˆåŠŸç‡ä¸º94.29%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84d4f38d510f1dbd809b3d5331d556ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d74b3f214371bf5356a15a34d72428ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edbf97479d5a11c6c804e54f4702b477.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Judge-a-Book-by-its-Cover-Investigating-Multi-Modal-LLMs-for-Multi-Page-Handwritten-Document-Transcription"><a href="#Judge-a-Book-by-its-Cover-Investigating-Multi-Modal-LLMs-for-Multi-Page-Handwritten-Document-Transcription" class="headerlink" title="Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page   Handwritten Document Transcription"></a>Judge a Book by its Cover: Investigating Multi-Modal LLMs for Multi-Page   Handwritten Document Transcription</h2><p><strong>Authors:Benjamin Gutteridge, Matthew Thomas Jackson, Toni Kukurin, Xiaowen Dong</strong></p>
<p>Handwritten text recognition (HTR) remains a challenging task, particularly for multi-page documents where pages share common formatting and contextual features. While modern optical character recognition (OCR) engines are proficient with printed text, their performance on handwriting is limited, often requiring costly labeled data for fine-tuning. In this paper, we explore the use of multi-modal large language models (MLLMs) for transcribing multi-page handwritten documents in a zero-shot setting. We investigate various configurations of commercial OCR engines and MLLMs, utilizing the latter both as end-to-end transcribers and as post-processors, with and without image components. We propose a novel method, â€˜+first pageâ€™, which enhances MLLM transcription by providing the OCR output of the entire document along with just the first page image. This approach leverages shared document features without incurring the high cost of processing all images. Experiments on a multi-page version of the IAM Handwriting Database demonstrate that â€˜+first pageâ€™ improves transcription accuracy, balances cost with performance, and even enhances results on out-of-sample text by extrapolating formatting and OCR error patterns from a single page. </p>
<blockquote>
<p>æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å…±äº«å¸¸è§æ ¼å¼å’Œä¸Šä¸‹æ–‡ç‰¹å¾çš„å¤šé¡µæ–‡æ¡£ä¸­ã€‚è™½ç„¶ç°ä»£å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¼•æ“åœ¨æ‰“å°æ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰‹å†™æ–‡æœ¬æ–¹é¢çš„æ€§èƒ½å´å—åˆ°é™åˆ¶ï¼Œé€šå¸¸éœ€è¦æ˜‚è´µçš„æ ‡è®°æ•°æ®æ¥è¿›è¡Œå¾®è°ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è½¬å½•å¤šé¡µæ‰‹å†™æ–‡æ¡£ã€‚æˆ‘ä»¬ç ”ç©¶äº†å•†ä¸šOCRå¼•æ“å’ŒMLLMçš„å„ç§é…ç½®ï¼Œåˆ©ç”¨åè€…ä½œä¸ºç«¯åˆ°ç«¯çš„è½¬å½•æœºå’Œåå¤„ç†å™¨ï¼Œå¹¶å¸¦æœ‰å’Œä¸å¸¦æœ‰å›¾åƒç»„ä»¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€œ+é¦–é¡µâ€ï¼Œé€šè¿‡æä¾›æ•´ä¸ªæ–‡æ¡£çš„OCRè¾“å‡ºä»¥åŠä»…ç¬¬ä¸€é¡µå›¾åƒæ¥æé«˜MLLMè½¬å½•æ•ˆæœã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨å…±äº«æ–‡æ¡£ç‰¹å¾ï¼Œè€Œæ— éœ€å¤„ç†æ‰€æœ‰å›¾åƒçš„é«˜æˆæœ¬ã€‚åœ¨IAMæ‰‹å†™æ•°æ®åº“çš„å¤šé¡µç‰ˆæœ¬ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œâ€œ+é¦–é¡µâ€æé«˜äº†è½¬å½•å‡†ç¡®æ€§ï¼Œå¹³è¡¡äº†æˆæœ¬ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”ç”šè‡³é€šè¿‡ä»ä¸€é¡µä¸­æ¨æ–­æ ¼å¼å’ŒOCRé”™è¯¯æ¨¡å¼æ¥æé«˜è¶…å‡ºæ ·æœ¬æ–‡æœ¬çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20295v1">PDF</a> 11 pages (including references and appendix), 14 figures, accepted at   AAAI-25 Workshop on Document Understanding and Intelligence, non-archival</p>
<p><strong>Summary</strong><br>æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰å¯¹å¤šé¡µæ–‡æ¡£è€Œè¨€ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶å½“è¿™äº›é¡µé¢å…·æœ‰å…±åŒæ ¼å¼å’Œä¸Šä¸‹æ–‡ç‰¹å¾æ—¶ã€‚ç°ä»£å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¼•æ“æ“…é•¿å¤„ç†æ‰“å°æ–‡æœ¬ï¼Œä½†å¯¹æ‰‹å†™æ–‡æœ¬çš„è¯†åˆ«æ€§èƒ½æœ‰é™ï¼Œé€šå¸¸éœ€è¦æ˜‚è´µçš„æ ‡ç­¾æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ— ç›‘ç£è®¾ç½®ä¸‹å¯¹å¤šé¡µæ‰‹å†™æ–‡æ¡£çš„è½¬å½•åº”ç”¨ã€‚æˆ‘ä»¬ç ”ç©¶äº†å•†ä¸šOCRå¼•æ“å’ŒMLLMsçš„å„ç§é…ç½®ï¼Œå¹¶å°è¯•å°†åè€…ç”¨ä½œç«¯åˆ°ç«¯çš„è½¬å½•æœºå’Œåç½®å¤„ç†å™¨ï¼Œä»¥åŠæœ‰æ— å›¾åƒç»„ä»¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼šâ€œ+é¦–é¡µâ€ï¼Œè¯¥æ–¹æ³•åœ¨MLLMè½¬å½•ä¸­æä¾›äº†æ•´ä¸ªæ–‡æ¡£çš„OCRè¾“å‡ºä»¥åŠä»…ç¬¬ä¸€é¡µçš„å›¾åƒï¼Œä»è€Œåˆ©ç”¨å…±äº«æ–‡æ¡£ç‰¹å¾ï¼ŒåŒæ—¶é¿å…äº†å¤„ç†æ‰€æœ‰å›¾åƒçš„é«˜æˆæœ¬ã€‚åœ¨IAMæ‰‹å†™æ•°æ®åº“çš„å¤šé¡µç‰ˆæœ¬ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œâ€œ+é¦–é¡µâ€æ–¹æ³•æé«˜äº†è½¬å½•å‡†ç¡®æ€§ï¼Œå¹³è¡¡äº†æˆæœ¬ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”é€šè¿‡å¯¹å•ä¸ªé¡µé¢çš„æ ¼å¼å’ŒOCRé”™è¯¯æ¨¡å¼è¿›è¡Œæ¨æ–­ï¼Œæé«˜äº†è¶…å‡ºæ ·æœ¬æ–‡æœ¬çš„è¯†åˆ«æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹å†™æ–‡æœ¬è¯†åˆ«ï¼ˆHTRï¼‰å¯¹å¤šé¡µæ–‡æ¡£ä»æ˜¯æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨é¡µé¢å…·æœ‰å…±åŒæ ¼å¼å’Œä¸Šä¸‹æ–‡ç‰¹å¾æ—¶ã€‚</li>
<li>ç°ä»£OCRå¼•æ“å¤„ç†æ‰‹å†™æ–‡æœ¬æ€§èƒ½æœ‰é™ï¼Œéœ€è¦æ˜‚è´µçš„æ ‡ç­¾æ•°æ®è¿›è¡Œå¾®è°ƒã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯åœ¨æ— ç›‘ç£ç¯å¢ƒä¸‹ç”¨äºå¤šé¡µæ‰‹å†™æ–‡æ¡£çš„è½¬å½•ã€‚</li>
<li>â€˜+é¦–é¡µâ€™æ–¹æ³•é€šè¿‡æä¾›æ•´ä¸ªæ–‡æ¡£çš„OCRè¾“å‡ºåŠä»…ç¬¬ä¸€é¡µå›¾åƒï¼Œæé«˜äº†MLLMçš„è½¬å½•å‡†ç¡®æ€§ã€‚</li>
<li>â€˜+é¦–é¡µâ€™æ–¹æ³•å¹³è¡¡äº†å¤„ç†æˆæœ¬ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä»å•ä¸ªé¡µé¢ä¸­æ¨æ–­æ ¼å¼å’ŒOCRé”™è¯¯æ¨¡å¼ï¼Œå¢å¼ºäº†è¶…å‡ºæ ·æœ¬æ–‡æœ¬çš„è¯†åˆ«æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0b7ca4adaebecd6496969e4eaf5fd33f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9b2ed4cabfe15f003c3b0041fe24fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc2d96d9acf8b1e1a116eb723a86cfea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08af4b92ac4607b713da990ad704dd69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c98900fe9ba8dab1c1bec1075eeb530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7983e0a59a1b46b8df9d41bd0c51be13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c86b4fb8902d9c419103df6002a7bdea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eb9cb3987e8350b07386c740a807cd7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Natural-Language-Perplexity-Detecting-Dead-Code-Poisoning-in-Code-Generation-Datasets"><a href="#Beyond-Natural-Language-Perplexity-Detecting-Dead-Code-Poisoning-in-Code-Generation-Datasets" class="headerlink" title="Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in   Code Generation Datasets"></a>Beyond Natural Language Perplexity: Detecting Dead Code Poisoning in   Code Generation Datasets</h2><p><strong>Authors:Chichien Tsai, Chiamu Yu, Yingdar Lin, Yusung Wu, Weibin Lee</strong></p>
<p>The increasing adoption of large language models (LLMs) for code-related tasks has raised concerns about the security of their training datasets. One critical threat is dead code poisoning, where syntactically valid but functionally redundant code is injected into training data to manipulate model behavior. Such attacks can degrade the performance of neural code search systems, leading to biased or insecure code suggestions. Existing detection methods, such as token-level perplexity analysis, fail to effectively identify dead code due to the structural and contextual characteristics of programming languages. In this paper, we propose DePA (Dead Code Perplexity Analysis), a novel line-level detection and cleansing method tailored to the structural properties of code. DePA computes line-level perplexity by leveraging the contextual relationships between code lines and identifies anomalous lines by comparing their perplexity to the overall distribution within the file. Our experiments on benchmark datasets demonstrate that DePA significantly outperforms existing methods, achieving 0.14-0.19 improvement in detection F1-score and a 44-65% increase in poisoned segment localization precision. Furthermore, DePA enhances detection speed by 0.62-23x, making it practical for large-scale dataset cleansing. Overall, by addressing the unique challenges of dead code poisoning, DePA provides a robust and efficient solution for safeguarding the integrity of code generation model training datasets. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†äººä»¬å¯¹å…¶è®­ç»ƒæ•°æ®é›†å®‰å…¨æ€§çš„å…³æ³¨ã€‚ä¸€ä¸ªå…³é”®å¨èƒæ˜¯æ­»ä»£ç ä¸­æ¯’ï¼Œå…¶ä¸­è¯­æ³•æœ‰æ•ˆä½†åŠŸèƒ½å†—ä½™çš„ä»£ç è¢«æ³¨å…¥åˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œä»¥æ“çºµæ¨¡å‹è¡Œä¸ºã€‚è¿™ç§æ”»å‡»ä¼šé™ä½ç¥ç»ä»£ç æœç´¢ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå¯¼è‡´åè§æˆ–ä¸å®‰å…¨çš„ä»£ç å»ºè®®ã€‚ç°æœ‰çš„æ£€æµ‹æ–¹æ³•ï¼Œå¦‚åŸºäºæ ‡è®°çº§åˆ«çš„å›°æƒ‘åº¦åˆ†æï¼Œç”±äºç¼–ç¨‹è¯­è¨€çš„ç»“æ„å’Œä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œæ— æ³•æœ‰æ•ˆåœ°æ£€æµ‹æ­»ä»£ç ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹ä»£ç ç»“æ„ç‰¹æ€§çš„å…¨æ–°è¡Œçº§æ£€æµ‹ä¸å‡€åŒ–æ–¹æ³•â€”â€”DePAï¼ˆæ­»ä»£ç å›°æƒ‘åº¦åˆ†æï¼‰ã€‚DePAé€šè¿‡åˆ©ç”¨ä»£ç è¡Œä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»è®¡ç®—è¡Œçº§å›°æƒ‘åº¦ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒå„è¡Œå›°æƒ‘åº¦ä¸æ–‡ä»¶å†…æ€»ä½“åˆ†å¸ƒæ¥è¯†åˆ«å¼‚å¸¸è¡Œã€‚æˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDePAåœ¨æ£€æµ‹F1åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†0.14-0.19çš„F1åˆ†æ•°ï¼Œå¹¶ä¸”åœ¨ä¸­æ¯’æ®µå®šä½ç²¾åº¦ä¸Šæé«˜äº†44-65%ã€‚æ­¤å¤–ï¼ŒDePAæé«˜äº†æ£€æµ‹é€Ÿåº¦ï¼Œè¾¾åˆ°0.62-23å€ï¼Œä½¿å…¶æˆä¸ºå¤§è§„æ¨¡æ•°æ®é›†å‡€åŒ–çš„å®ç”¨é€‰æ‹©ã€‚æ€»ä½“è€Œè¨€ï¼ŒDePAé€šè¿‡è§£å†³æ­»ä»£ç ä¸­æ¯’çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œä¸ºä¿æŠ¤ä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒæ•°æ®é›†çš„å®Œæ•´æ€§æä¾›äº†ç¨³å¥é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20246v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­çš„æ™®åŠå¼•å‘äº†å¯¹å…¶è®­ç»ƒæ•°æ®é›†å®‰å…¨çš„æ‹…å¿§ã€‚ä¸€é¡¹å…³é”®å¨èƒæ˜¯æ­»ä»£ç ä¸­æ¯’ï¼Œå…¶ä¸­è¯­æ³•æœ‰æ•ˆä½†åŠŸèƒ½å†—ä½™çš„ä»£ç è¢«æ³¨å…¥åˆ°è®­ç»ƒæ•°æ®ä¸­ä»¥æ“çºµæ¨¡å‹è¡Œä¸ºã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•ï¼Œå¦‚åŸºäºæ ‡è®°çš„å›°æƒ‘åº¦åˆ†æï¼Œæœªèƒ½æœ‰æ•ˆåœ°è¯†åˆ«æ­»ä»£ç ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„é¢å‘ä»£ç ç»“æ„çš„æ­»ä»£ç å›°æƒ‘åº¦åˆ†æï¼ˆDePAï¼‰æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¡Œçº§åˆ«è®¡ç®—å›°æƒ‘åº¦å¹¶åˆ©ç”¨ä»£ç è¡Œä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»æ¥æ£€æµ‹æ­»ä»£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDePAåœ¨æ£€æµ‹F1åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†0.14-0.19çš„F1åˆ†æ•°ï¼Œå¹¶ä¸”åœ¨ä¸­æ¯’æ®µå®šä½ç²¾åº¦ä¸Šæé«˜äº†44-65%ã€‚æ­¤å¤–ï¼ŒDePAæé«˜äº†æ£€æµ‹é€Ÿåº¦ï¼ŒåŠ å¿«äº†å¤§è§„æ¨¡æ•°æ®é›†æ¸…æ´çš„é€Ÿåº¦ã€‚æ€»ä½“è€Œè¨€ï¼ŒDePAä¸ºè§£å†³æ­»ä»£ç ä¸­æ¯’çš„ç‹¬ç‰¹æŒ‘æˆ˜æä¾›äº†ç¨³å¥é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¿éšœäº†ä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒæ•°æ®é›†çš„å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­é¢ä¸´æ­»ä»£ç ä¸­æ¯’çš„å®‰å…¨å¨èƒã€‚</li>
<li>æ­»ä»£ç ä¸­æ¯’æ˜¯æŒ‡é€šè¿‡æ³¨å…¥è¯­æ³•æ­£ç¡®ä½†åŠŸèƒ½å†—ä½™çš„ä»£ç æ¥æ“çºµæ¨¡å‹è¡Œä¸ºã€‚</li>
<li>ç°æœ‰æ£€æµ‹æ–¹æ³•ï¼Œå¦‚åŸºäºæ ‡è®°çš„å›°æƒ‘åº¦åˆ†æï¼Œæ— æ³•æœ‰æ•ˆè¯†åˆ«æ­»ä»£ç ã€‚</li>
<li>DePAæ˜¯ä¸€ç§æ–°çš„é¢å‘ä»£ç ç»“æ„çš„æ­»ä»£ç æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—è¡Œçº§åˆ«çš„å›°æƒ‘åº¦å¹¶åˆ©ç”¨ä»£ç è¡Œçš„ä¸Šä¸‹æ–‡å…³ç³»æ¥æ£€æµ‹æ­»ä»£ç ã€‚</li>
<li>DePAåœ¨æ£€æµ‹F1åˆ†æ•°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†å®šä½ç²¾åº¦å’Œæ£€æµ‹é€Ÿåº¦ã€‚</li>
<li>DePAä¸ºæ­»ä»£ç ä¸­æ¯’æä¾›äº†ç¨³å¥é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec64c805fd0fa603658ba1151d7a94f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b574edf6f05791602b23ae675ade54e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4025367f44b9b216f9d6226f573967.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e6e06560c45e730b532231ae5833e24.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multimodal-Representation-Alignment-for-Image-Generation-Text-Image-Interleaved-Control-Is-Easier-Than-You-Think"><a href="#Multimodal-Representation-Alignment-for-Image-Generation-Text-Image-Interleaved-Control-Is-Easier-Than-You-Think" class="headerlink" title="Multimodal Representation Alignment for Image Generation: Text-Image   Interleaved Control Is Easier Than You Think"></a>Multimodal Representation Alignment for Image Generation: Text-Image   Interleaved Control Is Easier Than You Think</h2><p><strong>Authors:Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang</strong></p>
<p>The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX. </p>
<blockquote>
<p>å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸæ­£å‡ºç°èåˆå¼ºå¤§æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¦‚CLIPå’ŒT5ï¼‰ä¸Diffusion Transformerä¸»å¹²çš„ç»Ÿä¸€æ¡†æ¶ã€‚å°½ç®¡äººä»¬å·²ç»å°è¯•åˆ©ç”¨cannyå’Œæ·±åº¦å›¾ç­‰é¢å¤–æ¡ä»¶æ¥æ§åˆ¶è¾“å‡ºå›¾åƒï¼Œä½†å¯¹äºä»»æ„æ–‡æœ¬-å›¾åƒäº¤æ›¿æ§åˆ¶çš„ç»¼åˆæ¡†æ¶ä»ç„¶ç¼ºä¹ã€‚åœ¨å°è¯•å°†å¤šä¸ªå›¾åƒä¸­çš„æ¦‚å¿µæˆ–è§†è§‰å…ƒç´ åˆå¹¶åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­æ—¶ï¼Œè¿™ä¸€å·®è·å°¤å…¶æ˜æ˜¾ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬è¿›è¡Œäº†åˆæ­¥å®éªŒï¼Œå‘ç°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å…±äº«è¡¨ç¤ºç©ºé—´ï¼Œå…¶ä¸­å›¾åƒå’Œæ–‡æœ¬å¯ä»¥å¾ˆå¥½åœ°å¯¹é½ï¼Œä»¥ä½œä¸ºå¤–éƒ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Dream Engineï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„ä»»æ„æ–‡æœ¬-å›¾åƒäº¤æ›¿æ§åˆ¶è®¾è®¡çš„é«˜æ•ˆä¸”ç»Ÿä¸€çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚SD3.5ï¼‰çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡èå…¥å¤šåŠŸèƒ½å¤šæ¨¡æ€ä¿¡æ¯ç¼–ç å™¨ï¼ˆå¦‚QwenVLï¼‰æ¥æ›¿æ¢åŸæœ‰çš„çº¯æ–‡æœ¬ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼ŒåŒ…æ‹¬è”åˆæ–‡æœ¬-å›¾åƒå¯¹é½å’Œå¤šæ¨¡æ€äº¤æ›¿æŒ‡ä»¤è°ƒæ•´ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§è®­ç»ƒæ–¹æ³•æœ‰æ•ˆï¼Œåœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°0.69çš„æ•´ä½“å¾—åˆ†ï¼Œä¸SD3.5å’ŒFLUXç­‰æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20172v1">PDF</a> 13 pages, 9 figures, codebase in   <a target="_blank" rel="noopener" href="https://github.com/chenllliang/DreamEngine">https://github.com/chenllliang/DreamEngine</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„Dream Engineæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®ç°äº†æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ä»»æ„äº¤ç»‡æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥å¤šæ¨¡æ€ä¿¡æ¯ç¼–ç å™¨å¦‚QwenVLï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¡†æ¶åœ¨å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­å®ç°äº†é«˜æ•ˆä¸”ç»Ÿä¸€çš„ä»»æ„æ–‡æœ¬ä¸å›¾åƒäº¤ç»‡æ§åˆ¶ã€‚å…¶æ€§èƒ½åœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†0.69çš„æ•´ä½“è¯„åˆ†ï¼Œä¸ä¸»æµçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¦‚SD3.5å’ŒFLUXç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æä¾›äº†ä¸€ä¸ªå…±äº«è¡¨ç¤ºç©ºé—´ï¼Œä½¿å›¾åƒå’Œæ–‡æœ¬èƒ½å¤Ÿè‰¯å¥½å¯¹é½ï¼Œæˆä¸ºå¤–éƒ¨æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶ã€‚</li>
<li>Dream Engineæ¡†æ¶æ˜¯ä¸€ä¸ªé’ˆå¯¹å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä»»æ„æ–‡æœ¬ä¸å›¾åƒäº¤ç»‡æ§åˆ¶çš„æœ‰æ•ˆä¸”ç»Ÿä¸€çš„æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶å¼•å…¥äº†å¤šæ¨¡æ€ä¿¡æ¯ç¼–ç å™¨å¦‚QwenVLï¼Œä»¥å¢å¼ºæ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„å¯¹é½å’Œäº¤ç»‡æ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼ŒåŒ…æ‹¬è”åˆæ–‡æœ¬-å›¾åƒå¯¹é½å’Œå¤šæ¨¡æ€äº¤ç»‡æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è®­ç»ƒæ¨¡å¼åœ¨GenEvalåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†0.69çš„æ•´ä½“è¯„åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>Dream Engineæ¡†æ¶çš„æ€§èƒ½ä¸ä¸»æµçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å¦‚SD3.5å’ŒFLUXç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1581d072940f1373180b798472c85dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a8953428225ab571e72725710184c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e394fa1d9fdd8480802c86c26e672d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7a4f761af24f014afdb099e8a5bbbfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0129785a01fc209ec03625c3396a34f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-Fusion-and-Encoding-Advancing-Multimodal-Retrieval-from-the-Ground-Up"><a href="#Joint-Fusion-and-Encoding-Advancing-Multimodal-Retrieval-from-the-Ground-Up" class="headerlink" title="Joint Fusion and Encoding: Advancing Multimodal Retrieval from the   Ground Up"></a>Joint Fusion and Encoding: Advancing Multimodal Retrieval from the   Ground Up</h2><p><strong>Authors:Lang Huang, Qiyu Wu, Zhongtao Miao, Toshihiko Yamasaki</strong></p>
<p>Information retrieval is indispensable for todayâ€™s Internet applications, yet traditional semantic matching techniques often fall short in capturing the fine-grained cross-modal interactions required for complex queries. Although late-fusion two-tower architectures attempt to bridge this gap by independently encoding visual and textual data before merging them at a high level, they frequently overlook the subtle interplay essential for comprehensive understanding. In this work, we rigorously assess these limitations and introduce a unified retrieval framework that fuses visual and textual cues from the ground up, enabling early cross-modal interactions for enhancing context interpretation. Through a two-stage training processâ€“comprising post-training adaptation followed by instruction tuningâ€“we adapt MLLMs as retrievers using a simple one-tower architecture. Our approach outperforms conventional methods across diverse retrieval scenarios, particularly when processing complex multi-modal inputs. Notably, the joint fusion encoder yields greater improvements on tasks that require modality fusion compared to those that do not, underscoring the transformative potential of early integration strategies and pointing toward a promising direction for contextually aware and effective information retrieval. </p>
<blockquote>
<p>ä¿¡æ¯æ£€ç´¢åœ¨å½“ä»Šäº’è”ç½‘åº”ç”¨ä¸­ä¸å¯æˆ–ç¼ºï¼Œç„¶è€Œä¼ ç»Ÿçš„è¯­ä¹‰åŒ¹é…æŠ€æœ¯åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶å¾€å¾€éš¾ä»¥æ•æ‰åˆ°ç²¾ç»†ç²’åº¦çš„è·¨æ¨¡æ€äº¤äº’ã€‚å°½ç®¡é‡‡ç”¨æ™šæœŸèåˆåŒå¡”æ¶æ„çš„æ–¹æ³•è¯•å›¾é€šè¿‡ç‹¬ç«‹ç¼–ç è§†è§‰å’Œæ–‡æœ¬æ•°æ®å¹¶åœ¨é«˜å±‚æ¬¡è¿›è¡Œèåˆæ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†å…¨é¢çš„ç†è§£æ‰€éœ€çš„å¾®å¦™äº’åŠ¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸¥æ ¼è¯„ä¼°äº†è¿™äº›å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„æ£€ç´¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªä¸‹è€Œä¸Šèåˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œä¸ºæ—©æœŸè·¨æ¨¡æ€äº¤äº’æä¾›å¯èƒ½æ€§ï¼Œå¢å¼ºä¸Šä¸‹æ–‡è§£é‡Šã€‚é€šè¿‡åŒ…æ‹¬å¾®è°ƒè®­ç»ƒåçš„é€‚åº”å’ŒæŒ‡ä»¤è°ƒæ•´åœ¨å†…çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€å•çš„å•å¡”æ¶æ„å°†MLLMä½œä¸ºæ£€ç´¢å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ£€ç´¢åœºæ™¯ä¸­å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€è¾“å…¥æ—¶è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè”åˆèåˆç¼–ç å™¨åœ¨éœ€è¦æ¨¡æ€èåˆçš„ä»»åŠ¡ä¸Šå–å¾—äº†æ›´å¤§çš„æ”¹è¿›ï¼Œè¿™çªæ˜¾äº†æ—©æœŸæ•´åˆç­–ç•¥çš„å˜é©æ½œåŠ›ï¼Œå¹¶ä¸ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæœ‰æ•ˆçš„ä¿¡æ¯æ£€ç´¢æŒ‡æ˜äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20008v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºä¼ ç»Ÿè¯­ä¹‰åŒ¹é…æŠ€æœ¯åœ¨å¤æ‚æŸ¥è¯¢ä¸­çš„å±€é™æ€§ï¼Œæ— æ³•æ•æ‰ç²¾ç»†ç²’åº¦çš„è·¨æ¨¡æ€äº¤äº’ã€‚å¼•å…¥ä¸€ç§ç»Ÿä¸€æ£€ç´¢æ¡†æ¶ï¼Œä»åº•å±‚èåˆè§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œä¿ƒè¿›æ—©æœŸè·¨æ¨¡æ€äº¤äº’ï¼Œæé«˜ä¸Šä¸‹æ–‡ç†è§£ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œé€‚åº”MLLMä½œä¸ºæ£€ç´¢å™¨ï¼Œé‡‡ç”¨ç®€å•çš„ä¸€å¡”æ¶æ„ï¼Œåœ¨å¤šæ ·æ£€ç´¢åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€è¾“å…¥æ—¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿè¯­ä¹‰åŒ¹é…æŠ€æœ¯åœ¨å¤æ‚æŸ¥è¯¢ä¸­è¡¨ç°ä¸è¶³ï¼Œæ— æ³•æ•æ‰ç²¾ç»†ç²’åº¦çš„è·¨æ¨¡æ€äº¤äº’ã€‚</li>
<li>å¼•å…¥ä¸€ç§ç»Ÿä¸€æ£€ç´¢æ¡†æ¶ï¼Œå®ç°è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„æ—©æœŸèåˆï¼Œæé«˜ä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬è®­ç»ƒåé€‚åº”å’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œé€‚åº”MLLMä½œä¸ºæ£€ç´¢å™¨ã€‚</li>
<li>é‡‡ç”¨ç®€å•çš„ä¸€å¡”æ¶æ„ï¼Œæé«˜æ£€ç´¢æ•ˆç‡ã€‚</li>
<li>åœ¨å¤šæ ·æ£€ç´¢åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤æ‚å¤šæ¨¡æ€è¾“å…¥æ—¶ã€‚</li>
<li>è”åˆèåˆç¼–ç å™¨åœ¨éœ€è¦æ¨¡æ€èåˆçš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20008">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-970557c6e9c349d1b246b1f7ebc21c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdadd4f87da0b8c85ea2a22e45ba91f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-109972c07ce06e6dfee76a3bae82c7fb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Space-Rotation-with-Basis-Transformation-for-Training-free-Test-Time-Adaptation"><a href="#Space-Rotation-with-Basis-Transformation-for-Training-free-Test-Time-Adaptation" class="headerlink" title="Space Rotation with Basis Transformation for Training-free Test-Time   Adaptation"></a>Space Rotation with Basis Transformation for Training-free Test-Time   Adaptation</h2><p><strong>Authors:Chenhao Ding, Xinyuan Gao, Songlin Dong, Yuhang He, Qiang Wang, Xiang Song, Alex Kot, Yihong Gong</strong></p>
<p>With the development of visual-language models (VLM) in downstream task applications, test-time adaptation methods based on VLM have attracted increasing attention for their ability to address changes distribution in test-time. Although prior approaches have achieved some progress, they typically either demand substantial computational resources or are constrained by the limitations of the original feature space, rendering them less effective for test-time adaptation tasks. To address these challenges, we propose a training-free feature space rotation with basis transformation for test-time adaptation. By leveraging the inherent distinctions among classes, we reconstruct the original feature space and map it to a new representation, thereby enhancing the clarity of class differences and providing more effective guidance for the model during testing. Additionally, to better capture relevant information from various classes, we maintain a dynamic queue to store representative samples. Experimental results across multiple benchmarks demonstrate that our method outperforms state-of-the-art techniques in terms of both performance and efficiency. </p>
<blockquote>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ä¸­çš„å‘å±•ï¼ŒåŸºäºVLMçš„æµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•å› å…¶è§£å†³æµ‹è¯•æ—¶åˆ†é…å˜åŒ–çš„èƒ½åŠ›è€Œè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚å°½ç®¡å…ˆå‰çš„æ–¹æ³•å·²ç»å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸è¦ä¹ˆéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¦ä¹ˆå—åˆ°åŸå§‹ç‰¹å¾ç©ºé—´å±€é™æ€§çš„çº¦æŸï¼Œå› è€Œåœ¨æµ‹è¯•æ—¶é—´é€‚åº”ä»»åŠ¡ä¸­æ•ˆæœè¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æµ‹è¯•æ—¶é—´ç‰¹å¾ç©ºé—´æ—‹è½¬å’ŒåŸºå˜æ¢æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨ç±»ä¹‹é—´çš„å›ºæœ‰å·®å¼‚ï¼Œé‡æ„åŸå§‹ç‰¹å¾ç©ºé—´å¹¶å°†å…¶æ˜ å°„åˆ°æ–°çš„è¡¨ç¤ºå½¢å¼ï¼Œä»è€Œæé«˜ç±»å·®å¼‚çš„æ¸…æ™°åº¦ï¼Œä¸ºæ¨¡å‹åœ¨æµ‹è¯•æ—¶æä¾›æ›´æœ‰æ•ˆçš„æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ›´å¥½åœ°ä»å„ç±»åˆ«ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨åŠ¨æ€é˜Ÿåˆ—æ¥å­˜å‚¨ä»£è¡¨æ€§æ ·æœ¬ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒå¤–çš„ç‰¹å¾ç©ºé—´æ—‹è½¬å’ŒåŸºå˜æ¢å®ç°æµ‹è¯•æ—¶æ•°æ®åˆ†å¸ƒå˜åŒ–çš„åº”å¯¹ã€‚æ–°æ–¹æ³•æ— éœ€å¤§é‡è®¡ç®—èµ„æºï¼Œèƒ½å…‹æœåŸæœ‰ç‰¹å¾ç©ºé—´çš„é™åˆ¶ï¼Œé€šè¿‡é‡æ„ç‰¹å¾ç©ºé—´å¹¶æ˜ å°„åˆ°æ–°çš„è¡¨ç¤ºï¼Œæé«˜ç±»é—´å·®å¼‚çš„æ¸…æ™°åº¦ï¼Œä¸ºæµ‹è¯•æ—¶çš„æ¨¡å‹æä¾›æ›´æœ‰æ•ˆçš„æŒ‡å¯¼ã€‚åŒæ—¶ï¼Œé€šè¿‡åŠ¨æ€é˜Ÿåˆ—å­˜å‚¨ä»£è¡¨æ€§æ ·æœ¬ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å„ç±»ç›¸å…³ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ï¼Œç”¨äºåº”å¯¹æµ‹è¯•æ•°æ®åˆ†å¸ƒå˜åŒ–ã€‚</li>
<li>é€šè¿‡è®­ç»ƒå¤–çš„ç‰¹å¾ç©ºé—´æ—‹è½¬å’ŒåŸºå˜æ¢å®ç°æµ‹è¯•æ—¶é€‚åº”ï¼Œæ— éœ€å¤§é‡è®¡ç®—èµ„æºã€‚</li>
<li>é‡æ„åŸå§‹ç‰¹å¾ç©ºé—´å¹¶æ˜ å°„åˆ°æ–°çš„è¡¨ç¤ºï¼Œä»¥æé«˜ç±»é—´å·®å¼‚çš„æ¸…æ™°åº¦ã€‚</li>
<li>é€šè¿‡åŠ¨æ€é˜Ÿåˆ—å­˜å‚¨ä»£è¡¨æ€§æ ·æœ¬ï¼Œä»¥ä¼˜åŒ–ä¿¡æ¯æ•æ‰ã€‚</li>
<li>æ–¹æ³•èƒ½æé«˜æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-380990f7ae9c68fb8ad163ebc07c0afb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e59df997698bbde3c7766d42504a3d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2d71072d23a6326b1f06b94f149e47d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1803f7a897d221fd5e24aff0d73c388c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e52585bd0674347d191c208d43530af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2ffc25ab0b44a250c292054a303ceef.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Picking-the-Cream-of-the-Crop-Visual-Centric-Data-Selection-with-Collaborative-Agents"><a href="#Picking-the-Cream-of-the-Crop-Visual-Centric-Data-Selection-with-Collaborative-Agents" class="headerlink" title="Picking the Cream of the Crop: Visual-Centric Data Selection with   Collaborative Agents"></a>Picking the Cream of the Crop: Visual-Centric Data Selection with   Collaborative Agents</h2><p><strong>Authors:Zhenyu Liu, Yunxin Li, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang</strong></p>
<p>To improve Multimodal Large Language Modelsâ€™ (MLLMs) ability to process images and complex instructions, researchers predominantly curate large-scale visual instruction tuning datasets, which are either sourced from existing vision tasks or synthetically generated using LLMs and image descriptions. However, they often suffer from critical flaws, including misaligned instruction-image pairs and low-quality images. Such issues hinder training efficiency and limit performance improvements, as models waste resources on noisy or irrelevant data with minimal benefit to overall capability. To address this issue, we propose a \textbf{Vi}sual-Centric \textbf{S}election approach via \textbf{A}gents Collaboration (ViSA), which centers on image quality assessment and image-instruction relevance evaluation. Specifically, our approach consists of 1) an image information quantification method via visual agents collaboration to select images with rich visual information, and 2) a visual-centric instruction quality assessment method to select high-quality instruction data related to high-quality images. Finally, we reorganize 80K instruction data from large open-source datasets. Extensive experiments demonstrate that ViSA outperforms or is comparable to current state-of-the-art models on seven benchmarks, using only 2.5% of the original data, highlighting the efficiency of our data selection approach. Moreover, we conduct ablation studies to validate the effectiveness of each component of our method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/ViSA">https://github.com/HITsz-TMG/ViSA</a>. </p>
<blockquote>
<p>ä¸ºäº†æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¤„ç†å›¾åƒå’Œå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œç ”ç©¶è€…ä¸»è¦åˆ›å»ºå¤§è§„æ¨¡è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ¥æºäºç°æœ‰çš„è§†è§‰ä»»åŠ¡æˆ–ä½¿ç”¨LLMså’Œå›¾åƒæè¿°äººå·¥åˆæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸å­˜åœ¨å…³é”®ç¼ºé™·ï¼ŒåŒ…æ‹¬æŒ‡ä»¤ä¸å›¾åƒä¸åŒ¹é…å’Œä½è´¨é‡å›¾åƒã€‚è¿™äº›é—®é¢˜é˜»ç¢äº†è®­ç»ƒæ•ˆç‡ï¼Œé™åˆ¶äº†æ€§èƒ½æå‡ï¼Œå› ä¸ºæ¨¡å‹ä¼šåœ¨å™ªéŸ³æˆ–æ— å…³æ•°æ®ä¸Šæµªè´¹èµ„æºï¼Œå¯¹æ•´ä½“èƒ½åŠ›å‡ ä¹æ²¡æœ‰å¸®åŠ©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„é€šè¿‡ä»£ç†åä½œçš„é€‰å‹æ–¹æ³•ï¼ˆViSAï¼‰ï¼Œè¯¥æ–¹æ³•ä»¥å›¾åƒè´¨é‡è¯„ä¼°å’Œå›¾åƒæŒ‡ä»¤ç›¸å…³æ€§è¯„ä¼°ä¸ºä¸­å¿ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼š1ï¼‰é€šè¿‡è§†è§‰ä»£ç†åä½œçš„å›¾åƒä¿¡æ¯é‡åŒ–æ–¹æ³•ï¼Œä»¥é€‰æ‹©å…·æœ‰ä¸°å¯Œè§†è§‰ä¿¡æ¯çš„å›¾åƒï¼›2ï¼‰ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œä»¥é€‰æ‹©ä¸é«˜è´¨é‡å›¾åƒç›¸å…³çš„ä¼˜è´¨æŒ‡ä»¤æ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬ä»å¤§å‹å¼€æºæ•°æ®é›†ä¸­é‡æ–°ç»„ç»‡äº†8ä¸‡æ¡æŒ‡ä»¤æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒViSAåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæˆ–ç›¸å½“äºå½“å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œè€Œä¸”åªä½¿ç”¨äº†åŸå§‹æ•°æ®çš„2.5%ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ•°æ®é€‰æ‹©æ–¹æ³•çš„é«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å‰¥ç¦»ç ”ç©¶ä»¥éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„æ¯ä¸ªç»„æˆéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/ViSA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HITsz-TMG/ViSAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19917v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾åƒå’Œå¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®é›†å­˜åœ¨çš„å›¾åƒä¸æŒ‡ä»¤ä¸åŒ¹é…ã€å›¾åƒè´¨é‡ä½ä¸‹ç­‰é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§è§†è§‰ä¸­å¿ƒåŒ–çš„é€‰æ‹©æ–¹æ³•â€”â€”ViSAã€‚è¯¥æ–¹æ³•é€šè¿‡è§†è§‰ä»£ç†åä½œï¼Œé€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒå’Œé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚å®éªŒè¯æ˜ï¼ŒViSAåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæˆ–ç›¸å½“äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä¸”ä»…ä½¿ç”¨åŸå§‹æ•°æ®çš„2.5%ï¼Œçªæ˜¾äº†æ•°æ®é€‰æ‹©çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å›¾åƒå’Œå¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®é›†å­˜åœ¨å›¾åƒä¸æŒ‡ä»¤ä¸åŒ¹é…ã€å›¾åƒè´¨é‡ä½ä¸‹ç­‰é—®é¢˜ã€‚</li>
<li>ViSAæ–¹æ³•é€šè¿‡è§†è§‰ä»£ç†åä½œï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬å›¾åƒä¿¡æ¯é‡åŒ–æ–¹æ³•å’Œè§†è§‰ä¸­å¿ƒçš„æŒ‡ä»¤è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>ViSAæ–¹æ³•å®ç°äº†é«˜æ•ˆçš„æ•°æ®é€‰æ‹©ï¼Œä»…ä½¿ç”¨åŸå§‹æ•°æ®çš„2.5%ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæˆ–ç›¸å½“äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
<li>ViSAæ–¹æ³•åŒ…æ‹¬å›¾åƒä¿¡æ¯é‡åŒ–æ–¹æ³•å’Œè§†è§‰ä¸­å¿ƒçš„æŒ‡ä»¤è´¨é‡è¯„ä¼°ä¸¤ä¸ªæ ¸å¿ƒéƒ¨åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒå’Œé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®ï¼Œæé«˜äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>å¹¿æ³›å®éªŒéªŒè¯äº†ViSAæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä¸å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„å¯¹æ¯”å’Œç»„ä»¶æœ‰æ•ˆæ€§çš„éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d286ed633556cfc60487a3cd056756d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73b50fa3e3cb47afe140847ebcf87d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9455b97a19ca6841de3abd56d68ae172.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c53a62069c782517f743efa6d7f6e3ec.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LeanProgress-Guiding-Search-for-Neural-Theorem-Proving-via-Proof-Progress-Prediction"><a href="#LeanProgress-Guiding-Search-for-Neural-Theorem-Proving-via-Proof-Progress-Prediction" class="headerlink" title="LeanProgress: Guiding Search for Neural Theorem Proving via Proof   Progress Prediction"></a>LeanProgress: Guiding Search for Neural Theorem Proving via Proof   Progress Prediction</h2><p><strong>Authors:Suozhi Huang, Peiyang Song, Robert Joseph George, Anima Anandkumar</strong></p>
<p>Mathematical reasoning remains a significant challenge for Large Language Models (LLMs) due to hallucinations. When combined with formal proof assistants like Lean, these hallucinations can be eliminated through rigorous verification, making theorem proving reliable. However, even with formal verification, LLMs still struggle with long proofs and complex mathematical formalizations. While Lean with LLMs offers valuable assistance with retrieving lemmas, generating tactics, or even complete proofs, it lacks a crucial capability: providing a sense of proof progress. This limitation particularly impacts the overall development efficiency in large formalization projects. We introduce LeanProgress, a method that predicts the progress in the proof. Training and evaluating our models made on a large corpus of Lean proofs from Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we employ data preprocessing and balancing techniques to handle the skewed distribution of proof lengths. Our experiments show that LeanProgress achieves an overall prediction accuracy of 75.1% in predicting the amount of progress and, hence, the remaining number of steps. When integrated into a best-first search framework using Reprover, our method shows a 3.8% improvement on Mathlib4 compared to baseline performances of 41.2%, particularly for longer proofs. These results demonstrate how proof progress prediction can enhance both automated and interactive theorem proving, enabling users to make more informed decisions about proof strategies. </p>
<blockquote>
<p>æ•°å­¦æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å¹»æƒ³ã€‚å½“ä¸Leanç­‰æ­£å¼è¯æ˜åŠ©æ‰‹ç»“åˆæ—¶ï¼Œè¿™äº›å¹»æƒ³å¯ä»¥é€šè¿‡ä¸¥æ ¼çš„éªŒè¯æ¥æ¶ˆé™¤ï¼Œä½¿å®šç†è¯æ˜æ›´åŠ å¯é ã€‚ç„¶è€Œï¼Œå³ä½¿æœ‰æ­£å¼çš„éªŒè¯ï¼ŒLLMåœ¨å¤„ç†é•¿è¯æ˜å’Œå¤æ‚çš„æ•°å­¦å½¢å¼åŒ–æ—¶ä»ç„¶ä¼šé‡åˆ°å›°éš¾ã€‚è™½ç„¶Leanä¸LLMåœ¨æ£€ç´¢å¼•ç†ã€ç”Ÿæˆç­–ç•¥æˆ–ç”šè‡³å®Œæˆè¯æ˜æ–¹é¢æä¾›äº†å®è´µçš„å¸®åŠ©ï¼Œä½†å®ƒç¼ºä¹ä¸€ç§å…³é”®èƒ½åŠ›ï¼šæä¾›è¯æ˜è¿›åº¦çš„æ„Ÿè§‰ã€‚è¿™ä¸€å±€é™æ€§ç‰¹åˆ«å½±å“äº†å¤§å‹å½¢å¼åŒ–é¡¹ç›®çš„æ•´ä½“å¼€å‘æ•ˆç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†LeanProgressï¼Œä¸€ç§é¢„æµ‹è¯æ˜è¿›åº¦çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å¤§é‡æ¥è‡ªLean Workbook Pluså’ŒMathlib4çš„Leanè¯æ˜è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥åŠå®Œæˆè¯æ˜æ‰€éœ€çš„å‰©ä½™æ­¥éª¤æ•°ï¼Œæˆ‘ä»¬é‡‡ç”¨æ•°æ®é¢„å¤„ç†å’Œå¹³è¡¡æŠ€æœ¯æ¥å¤„ç†è¯æ˜é•¿åº¦åˆ†å¸ƒçš„ä¸å¹³è¡¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLeanProgressåœ¨é¢„æµ‹è¿›åº¦ä»¥åŠå‰©ä½™æ­¥éª¤æ•°é‡æ–¹é¢è¾¾åˆ°äº†75.1%çš„æ•´ä½“é¢„æµ‹å‡†ç¡®ç‡ã€‚å½“ä½¿ç”¨Reproveråœ¨æœ€ä½³ä¼˜å…ˆæœç´¢æ¡†æ¶ä¸­è¿›è¡Œé›†æˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Mathlib4ä¸Šä¸åŸºçº¿æ€§èƒ½ç›¸æ¯”æé«˜äº†3.8%ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒé•¿çš„è¯æ˜ä¸­ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè¯æ˜è¿›åº¦é¢„æµ‹å¯ä»¥å¢å¼ºè‡ªåŠ¨å’Œäº¤äº’å¼å®šç†è¯æ˜ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåšå‡ºæ›´å¤šå…³äºè¯æ˜ç­–ç•¥çš„æœ‰æ ¹æ®çš„å†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17925v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› æ˜“å‡ºç°å¹»è§‰è€Œä¸å½¢å¼åŒ–è¯æ˜åŠ©æ‰‹ï¼ˆå¦‚Leanï¼‰ç»“åˆåï¼Œå¯é€šè¿‡ä¸¥æ ¼éªŒè¯æ¶ˆé™¤å¹»è§‰ï¼Œä½¿å®šç†è¯æ˜æ›´åŠ å¯é ã€‚ç„¶è€Œï¼Œå³ä½¿åœ¨å½¢å¼éªŒè¯ä¸‹ï¼ŒLLMåœ¨é•¿è¯æ˜å’Œå¤æ‚çš„æ•°å­¦å½¢å¼åŒ–æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†LeanProgressæ–¹æ³•ï¼Œèƒ½å¤Ÿé¢„æµ‹è¯æ˜è¿›åº¦ã€‚åœ¨å¤§é‡Leanè¯æ˜è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œé‡‡ç”¨æ•°æ®é¢„å¤„ç†å’Œå¹³è¡¡æŠ€æœ¯å¤„ç†è¯æ˜é•¿åº¦åˆ†å¸ƒä¸å‡çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒLeanProgressåœ¨é¢„æµ‹è¿›åº¦æ–¹é¢è¾¾åˆ°75.1%çš„æ€»ä½“é¢„æµ‹ç²¾åº¦ï¼Œä¸Reproveræœ€ä½³ä¼˜å…ˆæœç´¢æ¡†æ¶é›†æˆåï¼Œåœ¨Mathlib4ä¸Šç›¸æ¯”åŸºçº¿æ€§èƒ½æé«˜äº†3.8%ï¼Œå°¤å…¶æ˜¯é•¿è¯æ˜çš„æ”¹è¿›æ›´åŠ æ˜æ˜¾ã€‚è¿™è¯æ˜äº†é¢„æµ‹è¯æ˜è¿›åº¦å¯¹è‡ªåŠ¨åŒ–å’Œäº¤äº’å¼å®šç†è¯æ˜çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®¹æ˜“å‡ºç°å¹»è§‰ã€‚</li>
<li>ä¸å½¢å¼åŒ–è¯æ˜åŠ©æ‰‹ï¼ˆå¦‚Leanï¼‰ç»“åˆå¯ä»¥æ¶ˆé™¤å¹»è§‰ï¼Œæé«˜å®šç†è¯æ˜çš„å¯é æ€§ã€‚</li>
<li>å³ä½¿æ˜¯åœ¨å½¢å¼éªŒè¯ä¸‹ï¼ŒLLMåœ¨å¤„ç†é•¿è¯æ˜å’Œå¤æ‚æ•°å­¦å½¢å¼åŒ–æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚</li>
<li>å¼•å…¥äº†LeanProgressæ–¹æ³•ï¼Œèƒ½å¤Ÿé¢„æµ‹è¯æ˜çš„è¿›åº¦ã€‚</li>
<li>LeanProgressåœ¨é¢„æµ‹è¿›åº¦æ–¹é¢è¾¾åˆ°75.1%çš„æ€»ä½“é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>ä¸Reproveræœ€ä½³ä¼˜å…ˆæœç´¢æ¡†æ¶é›†æˆåï¼ŒLeanProgressåœ¨Mathlib4ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4dc954295d8c5b7156a14a0fc8e29dca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112be7941887e09e4d962e266aa2a74a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f1411ab09e736020130cdd8d94a5c7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f9e23847ba94c6c24b80cd2b696d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03f597867456f3c126122967dd085d84.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric"><a href="#Measuring-Data-Diversity-for-Instruction-Tuning-A-Systematic-Analysis-and-A-Reliable-Metric" class="headerlink" title="Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric"></a>Measuring Data Diversity for Instruction Tuning: A Systematic Analysis   and A Reliable Metric</h2><p><strong>Authors:Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>Data diversity is crucial for the instruction tuning of large language models. Existing studies have explored various diversity-aware data selection methods to construct high-quality datasets and enhance model performance. However, the fundamental problem of precisely defining and measuring data diversity remains underexplored, limiting clear guidance for data engineering. To address this, we systematically analyze 11 existing diversity measurement methods by evaluating their correlation with model performance through extensive fine-tuning experiments. Our results indicate that a reliable diversity measure should properly account for both inter-sample differences and the information distribution in the sample space. Building on this, we propose NovelSum, a new diversity metric based on sample-level â€œnovelty.â€ Experiments on both simulated and real-world data show that NovelSum accurately captures diversity variations and achieves a 0.97 correlation with instruction-tuned model performance, highlighting its value in guiding data engineering practices. With NovelSum as an optimization objective, we further develop a greedy, diversity-oriented data selection strategy that outperforms existing approaches, validating both the effectiveness and practical significance of our metric. </p>
<blockquote>
<p>æ•°æ®çš„å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å¤šç§æ„è¯†å¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä»¥æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºç²¾ç¡®å®šä¹‰å’Œæµ‹é‡æ•°æ®å¤šæ ·æ€§çš„åŸºæœ¬é—®é¢˜ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ï¼Œè¿™å¯¹æ•°æ®å·¥ç¨‹ç¼ºä¹æ˜ç¡®çš„æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¤§é‡å¾®è°ƒå®éªŒè¯„ä¼°äº†ç°æœ‰çš„11ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§åº¦é‡åº”é€‚å½“åœ°è€ƒè™‘æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumèƒ½å¤Ÿå‡†ç¡®æ•æ‰å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œçªæ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚ä»¥NovelSumä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§è´ªå©ªçš„ã€é¢å‘å¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå…¶æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬æŒ‡æ ‡çš„æœ‰æ•ˆæ€§å’Œå®é™…æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17184v3">PDF</a> 16 pages. The related codes and resources will be released later.   Project page: <a target="_blank" rel="noopener" href="https://github.com/UmeanNever/NovelSum">https://github.com/UmeanNever/NovelSum</a></p>
<p><strong>Summary</strong></p>
<p>æ•°æ®å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†å„ç§åŸºäºå¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©æ–¹æ³•ä»¥æ„å»ºé«˜è´¨é‡æ•°æ®é›†å¹¶å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•ç²¾ç¡®å®šä¹‰å’Œè¡¡é‡æ•°æ®å¤šæ ·æ€§çš„åŸºç¡€é—®é¢˜ä»ç„¶è¢«å¿½è§†ï¼Œå¯¼è‡´ç¼ºä¹æ˜ç¡®çš„æ•°æ®å·¥ç¨‹æŒ‡å¯¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†ç°æœ‰çš„åä¸€ç§å¤šæ ·æ€§æµ‹é‡æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¤§é‡çš„å¾®è°ƒå®éªŒè¯„ä¼°å®ƒä»¬ä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå¯é çš„å¤šæ ·æ€§æµ‹é‡åº”åŒæ—¶è€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚å’Œæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§åº¦é‡æŒ‡æ ‡ã€‚åœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒNovelSumèƒ½å¤Ÿå‡†ç¡®æ•æ‰å¤šæ ·æ€§å˜åŒ–ï¼Œä¸æŒ‡ä»¤è°ƒæ•´æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ï¼Œå‡¸æ˜¾å…¶åœ¨æŒ‡å¯¼æ•°æ®å·¥ç¨‹å®è·µä¸­çš„ä»·å€¼ã€‚åˆ©ç”¨NovelSumä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†è¯¥æŒ‡æ ‡çš„å®ç”¨æ€§å’Œå®é™…æ„ä¹‰ã€‚è¯¥è®ºæ–‡çš„ç ”ç©¶æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ•°æ®å¤šæ ·æ€§çš„é‡è¦æ€§å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®å¤šæ ·æ€§å¯¹äºå¤§è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æå‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶è™½ç„¶å·²ç»æ¢ç´¢äº†å¤šç§åŸºäºå¤šæ ·æ€§çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä½†å¯¹æ•°æ®å¤šæ ·æ€§çš„å®šä¹‰å’Œè¡¡é‡ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¯é çš„æ•°æ®å¤šæ ·æ€§æµ‹é‡éœ€è¦ç»¼åˆè€ƒè™‘æ ·æœ¬é—´çš„å·®å¼‚ä»¥åŠæ ·æœ¬ç©ºé—´ä¸­çš„ä¿¡æ¯åˆ†å¸ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºæ ·æœ¬çº§â€œæ–°é¢–æ€§â€çš„NovelSumæ–°å¤šæ ·æ€§åº¦é‡æŒ‡æ ‡ã€‚</li>
<li>NovelSumåœ¨æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å®éªŒè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä¸æ¨¡å‹æ€§èƒ½çš„ç›¸å…³æ€§è¾¾åˆ°0.97ã€‚</li>
<li>åˆ©ç”¨NovelSumä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼Œå¼€å‘äº†ä¸€ç§æ–°çš„ä»¥å¤šæ ·æ€§ä¸ºå¯¼å‘çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå…¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f3e5382f0483bf95efcda506b85d70d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54e13d1819d42fc8227602d59168d40d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73708b42657d862618f4d0f17d41eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e43cf27f6eac82e9bd09d3c6ce4f768.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning"><a href="#BioMaze-Benchmarking-and-Enhancing-Large-Language-Models-for-Biological-Pathway-Reasoning" class="headerlink" title="BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning"></a>BioMaze: Benchmarking and Enhancing Large Language Models for Biological   Pathway Reasoning</h2><p><strong>Authors:Haiteng Zhao, Chang Ma, Fangzhi Xu, Lingpeng Kong, Zhi-Hong Deng</strong></p>
<p>The applications of large language models (LLMs) in various biological domains have been explored recently, but their reasoning ability in complex biological systems, such as pathways, remains underexplored, which is crucial for predicting biological phenomena, formulating hypotheses, and designing experiments. This work explores the potential of LLMs in pathway reasoning. We introduce BioMaze, a dataset with 5.1K complex pathway problems derived from real research, covering various biological contexts including natural dynamic changes, disturbances, additional intervention conditions, and multi-scale research targets. Our evaluation of methods such as CoT and graph-augmented reasoning, shows that LLMs struggle with pathway reasoning, especially in perturbed systems. To address this, we propose PathSeeker, an LLM agent that enhances reasoning through interactive subgraph-based navigation, enabling a more effective approach to handling the complexities of biological systems in a scientifically aligned manner. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze">https://github.com/zhao-ht/BioMaze</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç±»ç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨è¿‘æ¥å·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚ç”Ÿç‰©ç³»ç»Ÿï¼ˆå¦‚é€”å¾„ï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›ä»é²œæœ‰ç ”ç©¶ï¼Œè¿™å¯¹äºé¢„æµ‹ç”Ÿç‰©ç°è±¡ã€æå‡ºå‡è®¾å’Œè®¾è®¡å®éªŒè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†LLMåœ¨é€”å¾„æ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†BioMazeæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä»çœŸå®ç ”ç©¶ä¸­å¾—å‡ºçš„5.1Kä¸ªå¤æ‚é€”å¾„é—®é¢˜ï¼Œæ¶‰åŠå¤šç§ç”Ÿç‰©èƒŒæ™¯ï¼ŒåŒ…æ‹¬è‡ªç„¶åŠ¨æ€å˜åŒ–ã€å¹²æ‰°ã€é¢å¤–çš„å¹²é¢„æ¡ä»¶å’Œå¤šå°ºåº¦ç ”ç©¶ç›®æ ‡ã€‚æˆ‘ä»¬å¯¹CoTå’Œå›¾å½¢å¢å¼ºæ¨ç†ç­‰æ–¹æ³•è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒLLMåœ¨é€”å¾„æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å—åˆ°å¹²æ‰°çš„ç³»ç»Ÿæ–¹é¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PathSeekerï¼Œè¿™æ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œå®ƒé€šè¿‡åŸºäºäº¤äº’å­å›¾çš„å¯¼èˆªå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä»¥ç§‘å­¦çš„æ–¹å¼æ›´æœ‰æ•ˆåœ°å¤„ç†ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/zhao-ht/BioMaze%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zhao-ht/BioMazeè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16660v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªç”Ÿç‰©é¢†åŸŸçš„åº”ç”¨å·²è¢«å¹¿æ³›æ¢ç´¢ï¼Œä½†åœ¨ç”Ÿç‰©é€šè·¯ç­‰å¤æ‚ç”Ÿç‰©ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ä»è¢«è¾ƒå°‘ç ”ç©¶ã€‚æœ¬æ–‡æ¢ç´¢äº†LLMåœ¨é€šè·¯æ¨ç†ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥äº†BioMazeæ•°æ®é›†ï¼ŒåŒ…å«5.1Kä¸ªçœŸå®ç ”ç©¶ä¸­çš„å¤æ‚é€šè·¯é—®é¢˜ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒLLMåœ¨æ‰°åŠ¨ç³»ç»Ÿä¸­çš„é€šè·¯æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PathSeekerï¼Œä¸€ä¸ªé€šè¿‡äº¤äº’å¼å­å›¾å¯¼èˆªå¢å¼ºæ¨ç†çš„LLMä»£ç†ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç”Ÿç‰©é€šè·¯ç­‰å¤æ‚ç”Ÿç‰©ç³»ç»Ÿä¸­çš„æ¨ç†èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚</li>
<li>BioMazeæ•°æ®é›†åŒ…å«çœŸå®ç ”ç©¶ä¸­çš„å¤æ‚ç”Ÿç‰©é€šè·¯é—®é¢˜ã€‚</li>
<li>LLMåœ¨æ‰°åŠ¨ç³»ç»Ÿä¸­çš„é€šè·¯æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>PathSeekeræ˜¯ä¸€ä¸ªLLMä»£ç†ï¼Œé€šè¿‡äº¤äº’å¼å­å›¾å¯¼èˆªå¢å¼ºæ¨ç†ã€‚</li>
<li>PathSeekerèƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†ç”Ÿç‰©ç³»ç»Ÿçš„å¤æ‚æ€§ã€‚</li>
<li>BioMazeæ•°æ®é›†å’Œä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</li>
<li>LLMçš„æ½œåŠ›åœ¨äºå…¶èƒ½å¤Ÿé¢„æµ‹ç”Ÿç‰©ç°è±¡ã€æå‡ºå‡è®¾å’Œè®¾è®¡å®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2113727ebf2af4f7b52e56861cd6b88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50f71bd991c3c5db24d14612a62d5ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4205c08bf1f99915760fd89f6f664c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20365646be29255d7065440556d637bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0015393d9eb5a90f1979e22a304189cf.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SuperGPQA-Scaling-LLM-Evaluation-across-285-Graduate-Disciplines"><a href="#SuperGPQA-Scaling-LLM-Evaluation-across-285-Graduate-Disciplines" class="headerlink" title="SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines"></a>SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines</h2><p><strong>Authors:M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, Ge Zhang</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦ã€ç‰©ç†å’Œè®¡ç®—æœºç§‘å­¦ç­‰ä¸»æµå­¦ç§‘é¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„ä¸“é•¿ã€‚ç„¶è€Œï¼Œäººç±»çŸ¥è¯†åŒ…å«è¶…è¿‡200ä¸ªä¸“ä¸šé¢†åŸŸï¼Œè¿œè¶…ç°æœ‰åŸºå‡†æµ‹è¯•çš„èŒƒå›´ã€‚åœ¨è¿™äº›ä¸“ä¸šé¢†åŸŸä¸­çš„è®¸å¤šé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨è½»å·¥ä¸šã€å†œä¸šå’ŒæœåŠ¡å¯¼å‘å‹é¢†åŸŸï¼ŒLLMçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SuperGPQAåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ—¨åœ¨è¯„ä¼°æ¶µç›–285ä¸ªå­¦ç§‘çš„ç ”ç©¶ç”Ÿçº§çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„äººæœºååŒè¿‡æ»¤æœºåˆ¶ï¼Œé€šè¿‡åŸºäºLLMå“åº”å’Œä¸“å®¶åé¦ˆçš„è¿­ä»£ä¼˜åŒ–æ¥æ¶ˆé™¤çç¢æˆ–æ¨¡ç³Šçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§çŸ¥è¯†é¢†åŸŸä¸­ï¼Œå½“å‰æœ€å…ˆè¿›çš„LLMæ€§èƒ½ä»æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼ˆä¾‹å¦‚ï¼Œä»¥æ¨ç†ä¸ºé‡ç‚¹çš„DeepSeek-R1åœ¨SuperGPQAä¸Šè¾¾åˆ°äº†æœ€é«˜çš„61.82%å‡†ç¡®ç‡ï¼‰ï¼Œè¿™çªæ˜¾äº†å½“å‰æ¨¡å‹èƒ½åŠ›ä¸é€šç”¨äººå·¥æ™ºèƒ½ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…³äºç®¡ç†å¤§è§„æ¨¡æ ‡æ³¨è¿‡ç¨‹çš„ç»¼åˆè§è§£ï¼Œæ¶‰åŠè¶…è¿‡80åä¸“å®¶æ ‡æ³¨äººå‘˜å’Œä¸€ä¸ªäººæœºååŒäº’åŠ¨ç³»ç»Ÿï¼Œä¸ºæœªæ¥çš„ç±»ä¼¼ç ”ç©¶æä¾›äº†å®è´µçš„æ–¹æ³•è®ºæŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14739v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸»æµå­¦ç§‘å¦‚æ•°å­¦ã€ç‰©ç†å’Œè®¡ç®—æœºç§‘å­¦ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ä¼—å¤šä¸“ä¸šé¢†åŸŸï¼Œå°¤å…¶æ˜¯è½»å·¥ä¸šã€å†œä¸šå’ŒæœåŠ¡å¯¼å‘å‹å­¦ç§‘ä¸­çš„èƒ½åŠ›è¯„ä¼°ä»ä¸è¶³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†SuperGPQAåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç ”ç©¶ç”Ÿçº§åˆ«çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›åœ¨285ä¸ªå­¦ç§‘ä¸­çš„åº”ç”¨ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨æ–°å‹çš„äººæœºååŒè¿‡æ»¤æœºåˆ¶ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ï¼Œç»“åˆLLMå“åº”å’Œä¸“å®¶åé¦ˆæ¥æ¶ˆé™¤çç¢æˆ–æ¨¡ç³Šçš„é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨SuperGPQAä¸Šï¼Œæœ€å…ˆè¿›çš„LLMæ€§èƒ½ä»æœ‰å¾…æé«˜ï¼Œå¦‚DeepSeek-R1æ¨¡å‹æœ€é«˜å‡†ç¡®ç‡ä¸º61.82%ï¼Œè¡¨æ˜å½“å‰æ¨¡å‹èƒ½åŠ›ä¸é€šç”¨äººå·¥æ™ºèƒ½ä¹‹é—´ä»å­˜åœ¨å·¨å¤§å·®è·ã€‚åŒæ—¶ï¼Œåˆ†äº«äº†å¤§è§„æ¨¡æ ‡æ³¨è¿‡ç¨‹ä¸­çš„ç»¼åˆè§è§£ï¼Œæ¶‰åŠè¶…è¿‡80ä½ä¸“å®¶æ ‡æ³¨å¸ˆå’Œäººæœºäº¤äº’ç³»ç»Ÿï¼Œä¸ºæœªæ¥ç±»ä¼¼ç ”ç©¶æä¾›æ–¹æ³•è®ºæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä¸»æµå­¦ç§‘ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ä¼—å¤šä¸“ä¸šé¢†åŸŸä¸­çš„èƒ½åŠ›è¯„ä¼°ä»ä¸è¶³ã€‚</li>
<li>SuperGPQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨285ä¸ªå­¦ç§‘ä¸­çš„ç ”ç©¶ç”Ÿçº§åˆ«çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>SuperGPQAé‡‡ç”¨äººæœºååŒè¿‡æ»¤æœºåˆ¶ï¼Œç»“åˆLLMå“åº”å’Œä¸“å®¶åé¦ˆæ¥ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„LLMåœ¨SuperGPQAä¸Šçš„æ€§èƒ½æœ‰å¾…æé«˜ï¼Œæ˜¾ç¤ºä¸é€šç”¨äººå·¥æ™ºèƒ½ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ã€‚</li>
<li>DeepSeek-R1æ¨¡å‹åœ¨SuperGPQAä¸Šçš„æœ€é«˜å‡†ç¡®ç‡ä¸º61.82%ã€‚</li>
<li>å¤§è§„æ¨¡æ ‡æ³¨è¿‡ç¨‹ä¸­çš„ç»¼åˆè§è§£è¢«åˆ†äº«ï¼Œæ¶‰åŠè¶…è¿‡80ä½ä¸“å®¶æ ‡æ³¨å¸ˆå’Œäººæœºäº¤äº’ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49a95e00e148d1ebb601fc44b62436c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e569dc0664db8655dc71a25ee43e93b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks"><a href="#Omni-Mol-Exploring-Universal-Convergent-Space-for-Omni-Molecular-Tasks" class="headerlink" title="Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks"></a>Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks</h2><p><strong>Authors:Chengxin Hu, Hao Li, Yihe Yuan, Zezheng Song, Haixin Wang</strong></p>
<p>Building generalist models has recently demonstrated remarkable capabilities in diverse scientific domains. Within the realm of molecular learning, several studies have explored unifying diverse tasks across diverse domains. However, negative conflicts and interference between molecules and knowledge from different domain may have a worse impact in threefold. First, conflicting molecular representations can lead to optimization difficulties for the models. Second, mixing and scaling up training data across diverse tasks is inherently challenging. Third, the computational cost of refined pretraining is prohibitively high. To address these limitations, this paper presents Omni-Mol, a scalable and unified LLM-based framework for direct instruction tuning. Omni-Mol builds on three key components to tackles conflicts: (1) a unified encoding mechanism for any task input; (2) an active-learning-driven data selection strategy that significantly reduces dataset size; (3) a novel design of the adaptive gradient stabilization module and anchor-and-reconcile MoE framework that ensures stable convergence. Experimentally, Omni-Mol achieves state-of-the-art performance across 15 molecular tasks, demonstrates the presence of scaling laws in the molecular domain, and is supported by extensive ablation studies and analyses validating the effectiveness of its design. The dataset, code and weights of the powerful AI-driven chemistry generalist are open-sourced. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ„å»ºé€šç”¨æ¨¡å‹åœ¨ä¸åŒç§‘å­¦é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚åœ¨åˆ†å­å­¦ä¹ é¢†åŸŸï¼Œå¤šé¡¹ç ”ç©¶æ¢ç´¢äº†å¦‚ä½•åœ¨ä¸åŒé¢†åŸŸç»Ÿä¸€å„ç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåˆ†å­å’Œæ¥è‡ªä¸åŒé¢†åŸŸçš„çŸ¥è¯†ä¹‹é—´çš„è´Ÿé¢å†²çªå’Œå¹²æ‰°å¯èƒ½ä¼šäº§ç”Ÿä¸‰å€çš„è´Ÿé¢å½±å“ã€‚é¦–å…ˆï¼Œå†²çªçš„åˆ†å­è¡¨å¾å¯èƒ½å¯¼è‡´æ¨¡å‹ä¼˜åŒ–å›°éš¾ã€‚å…¶æ¬¡ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¹‹é—´æ··åˆå’Œæ‰©å¤§è®­ç»ƒæ•°æ®æœ¬èº«å°±å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç¬¬ä¸‰ï¼Œç²¾ç»†é¢„è®­ç»ƒçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†Omni-Molï¼Œä¸€ä¸ªåŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯æ‰©å±•ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç›´æ¥æŒ‡ä»¤è°ƒæ•´ã€‚Omni-Molé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶æ¥è§£å†³å†²çªï¼šï¼ˆ1ï¼‰ä»»ä½•ä»»åŠ¡è¾“å…¥çš„ç»Ÿä¸€ç¼–ç æœºåˆ¶ï¼›ï¼ˆ2ï¼‰ä»¥ä¸»åŠ¨å­¦ä¹ é©±åŠ¨çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¯æ˜¾è‘—å‡å°‘æ•°æ®é›†å¤§å°ï¼›ï¼ˆ3ï¼‰è‡ªé€‚åº”æ¢¯åº¦ç¨³å®šæ¨¡å—å’Œé”šå®šä¸å’Œè§£MoEæ¡†æ¶çš„æ–°é¢–è®¾è®¡ï¼Œç¡®ä¿ç¨³å®šæ”¶æ•›ã€‚é€šè¿‡å®éªŒï¼ŒOmni-Molåœ¨15ä¸ªåˆ†å­ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†åˆ†å­é¢†åŸŸçš„è§„æ¨¡æ³•åˆ™çš„å­˜åœ¨ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œåˆ†æéªŒè¯äº†å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚è¯¥å¼ºå¤§AIé©±åŠ¨åŒ–å­¦é€šç”¨ä¸»ä¹‰çš„æ•°æ®é›†ã€ä»£ç å’Œæƒé‡å‡å·²å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01074v2">PDF</a> 30 pages, 13 figures, 7 tables, paper under review</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸï¼Œé€šç”¨æ¨¡å‹åœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸå±•ç°å‡ºå¼ºå¤§èƒ½åŠ›ã€‚åœ¨åˆ†å­å­¦ä¹ é¢†åŸŸï¼Œå¤šé¡¹ç ”ç©¶å°è¯•ç»Ÿä¸€ä¸åŒä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸åŒé¢†åŸŸåˆ†å­å’ŒçŸ¥è¯†é—´çš„å†²çªå¹²æ‰°å¯èƒ½å¸¦æ¥ä¸‰å€è´Ÿé¢å½±å“ã€‚æœ¬æ–‡æå‡ºOmni-Molæ¡†æ¶ï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºåŸºç¡€ï¼Œé€šè¿‡ç»Ÿä¸€ç¼–ç æœºåˆ¶ã€ä¸»åŠ¨å­¦ä¹ é©±åŠ¨çš„æ•°æ®é€‰æ‹©ç­–ç•¥å’Œæ¢¯åº¦ç¨³å®šæ¨¡å—è®¾è®¡ï¼Œè§£å†³å†²çªé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒOmni-Molåœ¨15ä¸ªåˆ†å­ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨åˆ†å­é¢†åŸŸå±•ç°è§„æ¨¡æ•ˆåº”ã€‚å…¶æ•°æ®é›†ã€ä»£ç å’Œæƒé‡å‡å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é€šç”¨æ¨¡å‹åœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å­å­¦ä¹ é¢†åŸŸã€‚</li>
<li>ä¸åŒé¢†åŸŸåˆ†å­å’ŒçŸ¥è¯†é—´çš„å†²çªå¹²æ‰°æ˜¯åˆ†å­å­¦ä¹ é¢ä¸´çš„ä¸»è¦é—®é¢˜ä¹‹ä¸€ã€‚</li>
<li>Omni-Molæ¡†æ¶é€šè¿‡ç»Ÿä¸€ç¼–ç æœºåˆ¶ã€æ•°æ®é€‰æ‹©ç­–ç•¥å’Œæ¢¯åº¦ç¨³å®šæ¨¡å—è®¾è®¡æ¥è§£å†³å†²çªé—®é¢˜ã€‚</li>
<li>Omni-Molå®ç°äº†åœ¨15ä¸ªåˆ†å­ä»»åŠ¡ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>Omni-Molåœ¨åˆ†å­é¢†åŸŸå±•ç°å‡ºè§„æ¨¡æ•ˆåº”ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶å’Œåˆ†æéªŒè¯äº†å…¶è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Omni-Molä½¿ç”¨çš„æ•°æ®é›†ã€ä»£ç å’Œæƒé‡å‡å·²å¼€æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01966bee2774dcd85e7d8d85a1555a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130b9b768c64e945f315444fac72ae42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fe495678783958dda53091c6924cab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5e9cf6e84ed1c9667c89b8f2c619f8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Bag-of-Design-Choices-for-Inference-of-High-Resolution-Masked-Generative-Transformer"><a href="#Bag-of-Design-Choices-for-Inference-of-High-Resolution-Masked-Generative-Transformer" class="headerlink" title="Bag of Design Choices for Inference of High-Resolution Masked Generative   Transformer"></a>Bag of Design Choices for Inference of High-Resolution Masked Generative   Transformer</h2><p><strong>Authors:Shitong Shao, Zikai Zhou, Tian Ye, Lichen Bai, Zhiqiang Xu, Zeke Xie</strong></p>
<p>Text-to-image diffusion models (DMs) develop at an unprecedented pace, supported by thorough theoretical exploration and empirical analysis. Unfortunately, the discrepancy between DMs and autoregressive models (ARMs) complicates the path toward achieving the goal of unified vision and language generation. Recently, the masked generative Transformer (MGT) serves as a promising intermediary between DM and ARM by predicting randomly masked image tokens (i.e., masked image modeling), combining the efficiency of DM with the discrete token nature of ARM. However, we find that the comprehensive analyses regarding the inference for MGT are virtually non-existent, and thus we aim to present positive design choices to fill this gap. We propose and redesign a set of enhanced inference techniques tailored for MGT, providing a detailed analysis of their performance. Additionally, we explore several DM-based approaches aimed at accelerating the sampling process on MGT. Extensive experiments and empirical analyses on the recent SOTA MGT, such as MaskGIT and Meissonic lead to concrete and effective design choices, and these design choices can be merged to achieve further performance gains. For instance, in terms of enhanced inference, we achieve winning rates of approximately 70% compared to vanilla sampling on HPS v2 with Meissonic-1024x1024. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦å‘å±•ï¼Œå¾—ç›Šäºæ·±å…¥çš„ç†è®ºæ¢ç´¢å’Œå®è¯åˆ†æã€‚ç„¶è€Œï¼ŒDMå’Œè‡ªå›å½’æ¨¡å‹ï¼ˆARMï¼‰ä¹‹é—´çš„å·®å¼‚ä½¿å¾—å®ç°ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç”Ÿæˆçš„ç›®æ ‡å˜å¾—æ›´åŠ å¤æ‚ã€‚æœ€è¿‘ï¼Œæ©ç ç”Ÿæˆå¼Transformerï¼ˆMGTï¼‰ä½œä¸ºDMå’ŒARMä¹‹é—´çš„æœ‰å‰é€”çš„ä¸­ä»‹ï¼Œé€šè¿‡é¢„æµ‹éšæœºæ©ç å›¾åƒä»¤ç‰Œï¼ˆå³æ©ç å›¾åƒå»ºæ¨¡ï¼‰ï¼Œç»“åˆäº†DMçš„æ•ˆç‡ä¸ARMçš„ç¦»æ•£ä»¤ç‰Œç‰¹æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å…³äºMGTæ¨ç†çš„ç»¼åˆåˆ†æå‡ ä¹ä¸å­˜åœ¨ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æå‡ºç§¯æçš„è®¾è®¡é€‰æ‹©æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºå¹¶é‡æ–°è®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹MGTçš„å¢å¼ºæ¨ç†æŠ€æœ¯ï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å‡ ç§åŸºäºDMçš„æ–¹æ³•æ¥åŠ é€ŸMGTä¸Šçš„é‡‡æ ·è¿‡ç¨‹ã€‚åœ¨æœ€è¿‘çš„æœ€æ–°MGTï¼ˆå¦‚MaskGITå’ŒMeissonicï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå’Œå®è¯åˆ†æå¾—å‡ºäº†å…·ä½“æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›è®¾è®¡é€‰æ‹©å¯ä»¥åˆå¹¶ä»¥å®ç°è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¢å¼ºæ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨HPS v2ä¸Šä¸é¦™è‰é‡‡æ ·ç›¸æ¯”è¾¾åˆ°äº†çº¦70%çš„èƒœç‡ï¼Œä½¿ç”¨Meissonic-1024x1024ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10781v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å‘å±•é€Ÿåº¦ç©ºå‰ï¼Œå¾—ç›Šäºæ·±å…¥çš„ç†è®ºæ¢ç´¢å’Œå®è¯åˆ†æã€‚ç„¶è€Œï¼ŒDMså’Œè‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰ä¹‹é—´çš„å·®å¼‚ä½¿å¾—å®ç°ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç”Ÿæˆçš„ç›®æ ‡å˜å¾—å¤æ‚ã€‚æœ€è¿‘ï¼Œæ©ç ç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆMGTï¼‰ä½œä¸ºDMå’ŒARMä¹‹é—´çš„æœ‰å‰é€”çš„ä¸­é—´æ–¹æ¡ˆï¼Œé€šè¿‡é¢„æµ‹éšæœºæ©ç å›¾åƒä»¤ç‰Œï¼ˆå³æ©ç å›¾åƒå»ºæ¨¡ï¼‰ï¼Œç»“åˆäº†DMçš„æ•ˆç‡ä¸ARMçš„ç¦»æ•£ä»¤ç‰Œç‰¹æ€§ã€‚ç„¶è€Œï¼Œå…³äºMGTçš„æ¨ç†ç»¼åˆåˆ†æå‡ ä¹ä¸å­˜åœ¨ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬æ—¨åœ¨æå‡ºç§¯æçš„è®¾è®¡é€‰æ‹©æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬æå‡ºå¹¶é‡æ–°è®¾è®¡äº†ä¸€ç³»åˆ—é’ˆå¯¹MGTçš„å¢å¼ºæ¨ç†æŠ€æœ¯ï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†æ—¨åœ¨åŠ é€ŸMGTé‡‡æ ·è¿‡ç¨‹çš„å‡ ç§åŸºäºDMçš„æ–¹æ³•ã€‚åœ¨æœ€æ–°çš„SOTA MGTï¼ˆå¦‚MaskGITå’ŒMeissonicï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå’Œå®è¯åˆ†æå¾—å‡ºäº†åˆ‡å®æœ‰æ•ˆçš„è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›è®¾è®¡é€‰æ‹©å¯ä»¥åˆå¹¶ä»¥å®ç°è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¢å¼ºæ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨HPS v2ä¸Šå®ç°äº†çº¦70%çš„èƒœç‡ï¼Œè¶…è¿‡äº†æ™®é€šé‡‡æ ·æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å‘å±•è¿…é€Ÿï¼Œä½†è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰ä¹‹é—´çš„å·®å¼‚å½±å“äº†ç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç”Ÿæˆç›®æ ‡çš„å®ç°ã€‚</li>
<li>æ©ç ç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆMGTï¼‰æ˜¯DMå’ŒARMä¹‹é—´çš„æœ‰å‰é€”çš„ä¸­é—´æ–¹æ¡ˆï¼Œç»“åˆäº†ä¸¤è€…ä¼˜ç‚¹ã€‚</li>
<li>ç›®å‰å…³äºMGTçš„æ¨ç†ç»¼åˆåˆ†æå‡ ä¹ä¸å­˜åœ¨ã€‚</li>
<li>æå‡ºå¹¶é‡æ–°è®¾è®¡äº†é’ˆå¯¹MGTçš„å¢å¼ºæ¨ç†æŠ€æœ¯ï¼Œå¹¶è¿›è¡Œè¯¦ç»†åˆ†æã€‚</li>
<li>æ¢ç´¢äº†æ—¨åœ¨åŠ é€ŸMGTé‡‡æ ·è¿‡ç¨‹çš„åŸºäºDMçš„æ–¹æ³•ã€‚</li>
<li>åœ¨æœ€æ–°çš„MGTä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒå’Œå®è¯åˆ†æè¡¨æ˜ï¼ŒæŸäº›è®¾è®¡é€‰æ‹©èƒ½å¸¦æ¥æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨å¢å¼ºæ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¦‚åœ¨HPS v2ä¸Šçš„èƒœç‡è¾¾åˆ°çº¦70%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e9891d95e37e50fcae5aa699ab1d3587.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4a56a7aed9b8e8d44e8fa5e015b329a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f55d7246a49ddc8ee1007133222a9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96fabcf098d8fd890fe0262faa0c2376.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-574438057c96eb7e7ec7a78e11b10dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a3a73fea82b47f75948331e3529fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04cac1cdb56ce7636c062cdbd428de29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47fc6b4b51a76aaffbeccb4c6d53603a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="S-4-ST-A-Strong-Self-transferable-faSt-and-Simple-Scale-Transformation-for-Transferable-Targeted-Attack"><a href="#S-4-ST-A-Strong-Self-transferable-faSt-and-Simple-Scale-Transformation-for-Transferable-Targeted-Attack" class="headerlink" title="S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale   Transformation for Transferable Targeted Attack"></a>S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale   Transformation for Transferable Targeted Attack</h2><p><strong>Authors:Yongxiang Liu, Bowen Peng, Li Liu, Xiang Li</strong></p>
<p>Transferable Targeted Attacks (TTAs), which aim to deceive black-box models into predicting specific erroneous labels, face significant challenges due to severe overfitting to surrogate models. Although modifying image features to generate robust semantic patterns of the target class is a promising approach, existing methods heavily rely on large-scale additional data. This dependence undermines the fair evaluation of TTA threats, potentially leading to a false sense of security or unnecessary overreactions. In this paper, we introduce two blind measures, surrogate self-alignment and self-transferability, to analyze the effectiveness and correlations of basic transformations, to enhance data-free attacks under strict black-box constraints. Our findings challenge conventional assumptions: (1) Attacking simple scaling transformations uniquely enhances targeted transferability, outperforming other basic transformations and rivaling leading complex methods; (2) Geometric and color transformations exhibit high internal redundancy despite weak inter-category correlations. These insights drive the design and tuning of S4ST (Strong, Self-transferable, faSt, Simple Scale Transformation), which integrates dimensionally consistent scaling, complementary low-redundancy transformations, and block-wise operations. Extensive experiments on the ImageNet-Compatible dataset demonstrate that S4ST achieves a 77.7% average targeted success rate (tSuc), surpassing existing transformations (+17.2% over H-Aug with only 26% computational time) and SOTA TTA solutions (+6.2% over SASD-WS with 1.2M samples for post-training). Notably, it attains 69.6% and 55.3% average tSuc against three commercial APIs and vision-language models, respectively. This work establishes a new SOTA for TTAs, highlights their potential threats, and calls for a reevaluation of the data dependency in achieving targeted transferability. </p>
<blockquote>
<p>å¯è¿ç§»çš„å®šå‘æ”»å‡»ï¼ˆTTAsï¼‰æ—¨åœ¨æ¬ºéª—é»‘ç®±æ¨¡å‹é¢„æµ‹ç‰¹å®šçš„é”™è¯¯æ ‡ç­¾ï¼Œä½†ç”±äºå¯¹ä»£ç†æ¨¡å‹çš„è¿‡åº¦æ‹Ÿåˆè€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶ä¿®æ”¹å›¾åƒç‰¹å¾ä»¥ç”Ÿæˆç›®æ ‡ç±»åˆ«çš„ç¨³å¥è¯­ä¹‰æ¨¡å¼æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡é¢å¤–æ•°æ®ã€‚è¿™ç§ä¾èµ–ç ´åäº†å¯¹TTAå¨èƒçš„å…¬æ­£è¯„ä¼°ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯çš„å®‰å…¨æ„Ÿæˆ–ä¸å¿…è¦çš„è¿‡åº¦ååº”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§ç›²æµ‹æ–¹æ³•ï¼Œå³ä»£ç†è‡ªæˆ‘å¯¹é½å’Œè‡ªè¿ç§»æ€§ï¼Œæ¥åˆ†æåŸºæœ¬è½¬æ¢çš„æœ‰æ•ˆæ€§åŠå…¶ç›¸å…³æ€§ï¼Œä»¥å¢å¼ºåœ¨ä¸¥æ ¼é»‘ç®±çº¦æŸä¸‹çš„æ— æ•°æ®æ”»å‡»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†ä¼ ç»Ÿå‡è®¾ï¼šï¼ˆ1ï¼‰æ”»å‡»ç®€å•çš„ç¼©æ”¾è½¬æ¢å¯ä»¥ç‹¬ç‰¹åœ°å¢å¼ºç›®æ ‡è¿ç§»æ€§ï¼Œä¼˜äºå…¶ä»–åŸºæœ¬è½¬æ¢ï¼Œå¹¶ä¸é¢†å…ˆçš„å¤æ‚æ–¹æ³•ç›¸åŒ¹æ•Œï¼›ï¼ˆ2ï¼‰å°½ç®¡è·¨ç±»åˆ«ç›¸å…³æ€§è¾ƒå¼±ï¼Œä½†å‡ ä½•å’Œé¢œè‰²è½¬æ¢è¡¨ç°å‡ºé«˜åº¦çš„å†…éƒ¨å†—ä½™ã€‚è¿™äº›è§è§£æ¨åŠ¨äº†S4STï¼ˆå¼ºã€è‡ªè¿ç§»ã€å¿«é€Ÿã€ç®€å•ç¼©æ”¾è½¬æ¢ï¼‰çš„è®¾è®¡å’Œè°ƒæ•´ï¼Œå®ƒèåˆäº†å°ºå¯¸ä¸€è‡´çš„ç¼©æ”¾ã€äº’è¡¥çš„ä½å†—ä½™è½¬æ¢å’Œå—æ“ä½œã€‚åœ¨ImageNetå…¼å®¹æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒS4STçš„å¹³å‡ç›®æ ‡æˆåŠŸç‡ï¼ˆtSucï¼‰è¾¾åˆ°77.7%ï¼Œè¶…è¶Šäº†ç°æœ‰è½¬æ¢ï¼ˆä¸H-Augç›¸æ¯”ï¼Œä»…ç”¨æ—¶26%çš„è®¡ç®—æ—¶é—´å°±æé«˜äº†17.2%ï¼‰ï¼Œå¹¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„TTAè§£å†³æ–¹æ¡ˆï¼ˆåœ¨è®­ç»ƒåä½¿ç”¨120ä¸‡æ ·æœ¬çš„SASD-WSæé«˜äº†6.2%ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå¯¹ä¸‰ç§å•†ä¸šAPIå’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¹³å‡tSucåˆ†åˆ«è¾¾åˆ°69.6%å’Œ55.3%ã€‚è¿™é¡¹å·¥ä½œä¸ºTTAsè®¾ç«‹äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ï¼Œçªæ˜¾äº†å®ƒä»¬çš„æ½œåœ¨å¨èƒï¼Œå¹¶å‘¼åé‡æ–°è¯„ä¼°å®ç°ç›®æ ‡è¿ç§»æ€§çš„æ•°æ®ä¾èµ–æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13891v2">PDF</a> 16 pages, 18 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é»‘ç›’æ¨¡å‹çš„å¯è½¬ç§»ç›®æ ‡æ”»å‡»ï¼ˆTTAsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿‡åº¦æ‹Ÿåˆä»£ç†æ¨¡å‹çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ä¸¤ç§ç›²é‡æªæ–½ï¼ˆä»£ç†è‡ªæˆ‘å¯¹é½å’Œè‡ªè½¬ç§»æ€§ï¼‰æ¥è¯„ä¼°åŸºæœ¬è½¬æ¢çš„æœ‰æ•ˆæ€§ï¼Œè¿›è€Œæé«˜æ— æ•°æ®æ”»å‡»åœ¨ä¸¥æ ¼é»‘ç›’çº¦æŸä¸‹çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ç®€å•ç¼©æ”¾è½¬æ¢èƒ½ç‹¬ç‰¹åœ°æé«˜ç›®æ ‡è½¬ç§»æ€§ï¼Œè¶…è¶Šå…¶ä»–åŸºæœ¬è½¬æ¢å’Œé¢†å…ˆçš„å¤æ‚æ–¹æ³•ï¼›å‡ ä½•å’Œé¢œè‰²è½¬æ¢å±•ç°å‡ºé«˜å†…éƒ¨å†—ä½™æ€§ï¼Œå°½ç®¡è·¨ç±»åˆ«ç›¸å…³æ€§è¾ƒå¼±ã€‚åŸºäºè¿™äº›è§è§£ï¼Œè®¾è®¡å‡ºS4STï¼ˆå¼ºã€è‡ªè½¬ç§»ã€å¿«é€Ÿã€ç®€å•ç¼©æ”¾è½¬æ¢ï¼‰æ–¹æ³•ï¼Œé›†æˆä¸€è‡´æ€§ç¼©æ”¾ã€äº’è¡¥ä½å†—ä½™è½¬æ¢å’Œå—æ“ä½œã€‚åœ¨ImageNetå…¼å®¹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒS4STå¹³å‡ç›®æ ‡æˆåŠŸç‡ï¼ˆtSucï¼‰è¾¾åˆ°77.7%ï¼Œè¶…è¶Šç°æœ‰è½¬æ¢æ–¹æ³•ï¼ˆåœ¨åªæœ‰26%çš„è®¡ç®—æ—¶é—´å†…ï¼Œæ¯”H-Augé«˜å‡º17.2%ï¼‰ï¼Œå¹¶è¶…è¶Šæœ€å…ˆè¿›çš„TTAè§£å†³æ–¹æ¡ˆï¼ˆåœ¨1.2Mæ ·æœ¬åè®­ç»ƒæ—¶ï¼Œé«˜å‡º6.2%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTAsåœ¨é»‘ç›’æ¨¡å‹ä¸­é¢ä¸´è¿‡åº¦æ‹Ÿåˆä»£ç†æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥ä¸¤ç§ç›²é‡æªæ–½ï¼šä»£ç†è‡ªæˆ‘å¯¹é½å’Œè‡ªè½¬ç§»æ€§ï¼Œä»¥è¯„ä¼°åŸºæœ¬è½¬æ¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç®€å•ç¼©æ”¾è½¬æ¢èƒ½ç‹¬ç‰¹æé«˜ç›®æ ‡è½¬ç§»æ€§ï¼Œè¶…è¶Šå…¶ä»–åŸºæœ¬å’Œå¤æ‚æ–¹æ³•ã€‚</li>
<li>å‡ ä½•å’Œé¢œè‰²è½¬æ¢å…·æœ‰é«˜å†…éƒ¨å†—ä½™æ€§ï¼Œè·¨ç±»åˆ«ç›¸å…³æ€§è¾ƒå¼±ã€‚</li>
<li>S4STæ–¹æ³•ç»“åˆä¸€è‡´æ€§ç¼©æ”¾ã€ä½å†—ä½™è½¬æ¢å’Œå—æ“ä½œï¼Œå®ç°é«˜æ•ˆæ”»å‡»ã€‚</li>
<li>S4STåœ¨ImageNetå…¼å®¹æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹³å‡ç›®æ ‡æˆåŠŸç‡è¾¾77.7%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2d58b6c0907e4135602dfd11c40a065.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0b13c0361cdd6334fdb0332efedbe9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c05383eeef75afacfe11a2d757dc9da2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab63594edd5f3718bd864b24a556d906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb39e06fea9ee796b1783115153c29ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7da99e0a06ef3621a35a58898f58b95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-861c23cd5face45f2d6733fc06b870a6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Enhancing-Android-Malware-Detection-The-Influence-of-ChatGPT-on-Decision-centric-Task"><a href="#Enhancing-Android-Malware-Detection-The-Influence-of-ChatGPT-on-Decision-centric-Task" class="headerlink" title="Enhancing Android Malware Detection: The Influence of ChatGPT on   Decision-centric Task"></a>Enhancing Android Malware Detection: The Influence of ChatGPT on   Decision-centric Task</h2><p><strong>Authors:Yao Li, Sen Fang, Tao Zhang, Haipeng Cai</strong></p>
<p>With the rise of large language models, such as ChatGPT, non-decisional models have been applied to various tasks. Moreover, ChatGPT has drawn attention to the traditional decision-centric task of Android malware detection. Despite effective detection methods proposed by scholars, they face low interpretability issues. Specifically, while these methods excel in classifying applications as benign or malicious and can detect malicious behavior, they often fail to provide detailed explanations for the decisions they make. This challenge raises concerns about the reliability of existing detection schemes and questions their true ability to understand complex data. In this study, we investigate the influence of the non-decisional model, ChatGPT, on the traditional decision-centric task of Android malware detection. We choose three state-of-the-art solutions, Drebin, XMAL, and MaMaDroid, conduct a series of experiments on publicly available datasets, and carry out a comprehensive comparison and analysis. Our findings indicate that these decision-driven solutions primarily rely on statistical patterns within datasets to make decisions, rather than genuinely understanding the underlying data. In contrast, ChatGPT, as a non-decisional model, excels in providing comprehensive analysis reports, substantially enhancing interpretability. Furthermore, we conduct surveys among experienced developers. The result highlights developersâ€™ preference for ChatGPT, as it offers in-depth insights and enhances efficiency and understanding of challenges. Meanwhile, these studies and analyses offer profound insights, presenting developers with a novel perspective on Android malware detectionâ€“enhancing the reliability of detection results from a non-decisional perspective. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰çš„å…´èµ·ï¼Œéå†³ç­–æ¨¡å‹å·²åº”ç”¨äºå„ç§ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒChatGPTå¼•èµ·äº†äººä»¬å¯¹ä¼ ç»Ÿå†³ç­–å¯¼å‘ä»»åŠ¡â€”â€”Androidæ¶æ„è½¯ä»¶æ£€æµ‹çš„å…³æ³¨ã€‚å°½ç®¡å­¦è€…å·²ç»æå‡ºäº†æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•ï¼Œä½†å®ƒä»¬å­˜åœ¨è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™äº›æ–¹æ³•è™½ç„¶åœ¨å°†åº”ç”¨ç¨‹åºåˆ†ç±»ä¸ºè‰¯æ€§æˆ–æ¶æ„ä»¥åŠæ£€æµ‹æ¶æ„è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•ä¸ºæ‰€åšçš„å†³ç­–æä¾›è¯¦ç»†çš„è§£é‡Šã€‚è¿™ä¸€æŒ‘æˆ˜å¼•å‘äº†äººä»¬å¯¹ç°æœ‰æ£€æµ‹æ–¹æ¡ˆå¯é æ€§çš„æ‹…å¿§ï¼Œå¹¶å¯¹å®ƒä»¬çœŸæ­£ç†è§£å¤æ‚æ•°æ®çš„èƒ½åŠ›æå‡ºè´¨ç–‘ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†éå†³ç­–æ¨¡å‹ChatGPTå¯¹ä¼ ç»Ÿå†³ç­–å¯¼å‘ä»»åŠ¡â€”â€”Androidæ¶æ„è½¯ä»¶æ£€æµ‹çš„å½±å“ã€‚æˆ‘ä»¬é€‰æ‹©äº†ä¸‰ç§æœ€æ–°è§£å†³æ–¹æ¡ˆï¼šDrebinã€XMALå’ŒMaMaDroidï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼Œå¹¶è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒå’Œåˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›å†³ç­–é©±åŠ¨è§£å†³æ–¹æ¡ˆä¸»è¦ä¾èµ–äºæ•°æ®é›†å†…çš„ç»Ÿè®¡æ¨¡å¼æ¥åšå‡ºå†³ç­–ï¼Œè€ŒéçœŸæ­£äº†è§£åº•å±‚æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒChatGPTä½œä¸ºä¸€ä¸ªéå†³ç­–æ¨¡å‹ï¼Œåœ¨æä¾›ç»¼åˆåˆ†ææŠ¥å‘Šæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¤§å¤§æé«˜äº†å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ç»éªŒä¸°å¯Œçš„å¼€å‘è€…è¿›è¡Œäº†è°ƒæŸ¥ã€‚ç»“æœè¡¨æ˜ï¼Œå¼€å‘è€…æ›´å€¾å‘äºä½¿ç”¨ChatGPTï¼Œå› ä¸ºå®ƒæä¾›äº†æ·±å…¥çš„è§è§£ï¼Œæé«˜äº†å¯¹æŒ‘æˆ˜çš„ç†è§£å’Œæ•ˆç‡ã€‚åŒæ—¶ï¼Œè¿™äº›ç ”ç©¶å’Œåˆ†æä¸ºå¼€å‘è€…æä¾›äº†å…³äºAndroidæ¶æ„è½¯ä»¶æ£€æµ‹çš„å…¨æ–°è§†è§’â€”â€”ä»éå†³ç­–è§’åº¦æé«˜æ£€æµ‹ç»“æœå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04352v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ChatGPTçš„å…´èµ·ï¼Œéå†³ç­–æ¨¡å‹å·²åº”ç”¨äºå¤šé¡¹ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†éå†³ç­–æ¨¡å‹ChatGPTå¯¹ä¼ ç»Ÿçš„ä»¥å†³ç­–ä¸ºä¸­å¿ƒçš„Androidæ¶æ„è½¯ä»¶æ£€æµ‹ä»»åŠ¡çš„å½±å“ã€‚ç ”ç©¶å‘ç°åœ¨ç°æœ‰å†³ç­–é©±åŠ¨è§£å†³æ–¹æ¡ˆï¼ˆå¦‚Drebinï¼ŒXMALå’ŒMaMaDroidï¼‰ä¸­ï¼Œå®ƒä»¬ä¸»è¦ä¾èµ–äºæ•°æ®é›†ä¸­çš„ç»Ÿè®¡æ¨¡å¼æ¥åšå‡ºå†³ç­–ï¼Œè€Œä¸æ˜¯çœŸæ­£ç†è§£æ•°æ®ã€‚ç›¸åï¼ŒChatGPTä½œä¸ºä¸€ä¸ªéå†³ç­–æ¨¡å‹ï¼Œåœ¨æä¾›ç»¼åˆåˆ†ææŠ¥å‘Šæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¤§å¤§æé«˜äº†å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹ç»éªŒä¸°å¯Œçš„å¼€å‘è€…çš„è°ƒæŸ¥ï¼Œå‘ç°å¼€å‘è€…æ›´å€¾å‘äºä½¿ç”¨ChatGPTï¼Œå› ä¸ºå®ƒæä¾›äº†æ·±å…¥çš„è§è§£ï¼Œæé«˜äº†å¯¹æŒ‘æˆ˜çš„ç†è§£å’Œæ•ˆç‡ã€‚è¿™é¡¹ç ”ç©¶ä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªæ–°çš„è§’åº¦æ¥æ£€æµ‹Androidæ¶æ„è½¯ä»¶ï¼Œä»éå†³ç­–çš„è§’åº¦æé«˜äº†æ£€æµ‹ç»“æœçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ChatGPTåœ¨éå†³ç­–ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ã€‚</li>
<li>ä¼ ç»Ÿå†³ç­–é©±åŠ¨çš„Androidæ¶æ„è½¯ä»¶æ£€æµ‹æ–¹æ³•é¢ä¸´ä½è§£é‡Šæ€§çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæ•°æ®é›†ä¸­çš„ç»Ÿè®¡æ¨¡å¼æ¥åšå‡ºå†³ç­–ï¼Œç¼ºä¹çœŸæ­£çš„æ•°æ®ç†è§£ã€‚</li>
<li>ChatGPTä½œä¸ºä¸€ä¸ªéå†³ç­–æ¨¡å‹ï¼Œåœ¨æä¾›ç»¼åˆåˆ†ææŠ¥å‘Šæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæé«˜äº†å¯è§£é‡Šæ€§ã€‚</li>
<li>å¼€å‘è€…æ›´å€¾å‘äºä½¿ç”¨ChatGPTï¼Œå› ä¸ºå®ƒæä¾›äº†æ·±å…¥çš„è§è§£ï¼Œæé«˜äº†å¯¹æŒ‘æˆ˜çš„ç†è§£å’Œæ•ˆç‡ã€‚</li>
<li>ç ”ç©¶ä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªæ–°çš„è§’åº¦æ¥æ£€æµ‹Androidæ¶æ„è½¯ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22cce304e839d77c24a7bc9e49cad1b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0a5347726fd8adf949a850fd5b9552e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PixWizard-Versatile-Image-to-Image-Visual-Assistant-with-Open-Language-Instructions"><a href="#PixWizard-Versatile-Image-to-Image-Visual-Assistant-with-Open-Language-Instructions" class="headerlink" title="PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language   Instructions"></a>PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language   Instructions</h2><p><strong>Authors:Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Huan Teng, Junlin Xie, Yu Qiao, Peng Gao, Hongsheng Li</strong></p>
<p>This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an Omni Pixel-to-Pixel Instruction-Tuning Dataset. By constructing detailed instruction templates in natural language, we comprehensively include a large set of diverse vision tasks such as text-to-image generation, image restoration, image grounding, dense image prediction, image editing, controllable generation, inpainting&#x2F;outpainting, and more. Furthermore, we adopt Diffusion Transformers (DiT) as our foundation model and extend its capabilities with a flexible any resolution mechanism, enabling the model to dynamically process images based on the aspect ratio of the input, closely aligning with human perceptual processes. The model also incorporates structure-aware and semantic-aware guidance to facilitate effective fusion of information from the input image. Our experiments demonstrate that PixWizard not only shows impressive generative and understanding abilities for images with diverse resolutions but also exhibits promising generalization capabilities with unseen tasks and human instructions. The code and related resources are available at <a target="_blank" rel="noopener" href="https://github.com/AFeng-x/PixWizard">https://github.com/AFeng-x/PixWizard</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé€šç”¨å›¾åƒåˆ°å›¾åƒçš„è§†è§‰åŠ©æ‰‹PixWizardï¼Œå®ƒæ—¨åœ¨åŸºäºè‡ªç”±çš„è¯­è¨€æŒ‡ä»¤è¿›è¡Œå›¾åƒç”Ÿæˆã€æ“ä½œå’Œç¿»è¯‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å„ç§è§†è§‰ä»»åŠ¡çº³å…¥ç»Ÿä¸€çš„å›¾åƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªOmni Pixel-to-Pixel Instruction-Tuningæ•°æ®é›†ã€‚é€šè¿‡æ„å»ºè‡ªç„¶è¯­è¨€ä¸­çš„è¯¦ç»†æŒ‡ä»¤æ¨¡æ¿ï¼Œæˆ‘ä»¬å…¨é¢æ¶µç›–äº†å¤§é‡ä¸åŒçš„è§†è§‰ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒæ¢å¤ã€å›¾åƒå®šä½ã€å¯†é›†å›¾åƒé¢„æµ‹ã€å›¾åƒç¼–è¾‘ã€å¯æ§ç”Ÿæˆã€å¡«å……&#x2F;æè¾¹ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡çµæ´»çš„ä»»ä½•åˆ†è¾¨ç‡æœºåˆ¶æ‰©å±•å…¶åŠŸèƒ½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æ¯”ä¾‹åŠ¨æ€å¤„ç†å›¾åƒï¼Œä¸äººç±»æ„ŸçŸ¥è¿‡ç¨‹ç´§å¯†å¯¹é½ã€‚è¯¥æ¨¡å‹è¿˜ç»“åˆäº†ç»“æ„æ„ŸçŸ¥å’Œè¯­ä¹‰æ„ŸçŸ¥æŒ‡å¯¼ï¼Œä»¥ä¿ƒè¿›è¾“å…¥å›¾åƒä¸­ä¿¡æ¯çš„æœ‰æ•ˆèåˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒPixWizardä¸ä»…åœ¨å¯¹å…·æœ‰ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒçš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”åœ¨æœªçŸ¥ä»»åŠ¡å’Œäººç±»æŒ‡ä»¤æ–¹é¢ä¹Ÿè¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AFeng-x/PixWizard%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AFeng-x/PixWizardä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15278v4">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/AFeng-x/PixWizard">https://github.com/AFeng-x/PixWizard</a></p>
<p><strong>Summary</strong><br>PixWizardæ˜¯ä¸€æ¬¾åŸºäºè‡ªç”±è¯­è¨€æŒ‡ä»¤çš„å›¾åƒåˆ°å›¾åƒè§†è§‰åŠ©æ‰‹ï¼Œç”¨äºå›¾åƒç”Ÿæˆã€æ“ä½œå’Œè½¬æ¢ã€‚å®ƒå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å›¾åƒ-æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå¹¶é‡‡ç”¨äº†Diffusion Transformersä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚é€šè¿‡è¯¦ç»†çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¨¡æ¿ï¼Œå®ƒåŒ…æ‹¬å„ç§è§†è§‰ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒæ¢å¤ã€å›¾åƒå®šä½ç­‰ã€‚å®ƒé‡‡ç”¨çµæ´»çš„åˆ†è¾¨ç‡æœºåˆ¶å’Œç»“æ„ä¸è¯­ä¹‰æ„ŸçŸ¥æŒ‡å¯¼ï¼Œå¯æœ‰æ•ˆèåˆè¾“å…¥å›¾åƒçš„ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒPixWizardåœ¨å¤šæ ·åˆ†è¾¨ç‡çš„å›¾åƒå¤„ç†ä¸Šå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°èƒ½åŠ›ï¼Œä¸”åœ¨æœªè§ä»»åŠ¡å’ŒäººæŒ‡ä»¤ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PixWizardæ˜¯ä¸€ä¸ªå›¾åƒåˆ°å›¾åƒçš„è§†è§‰åŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œå›¾åƒç”Ÿæˆã€æ“ä½œå’Œè½¬æ¢ã€‚</li>
<li>å®ƒé‡‡ç”¨ç»Ÿä¸€çš„å›¾åƒ-æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶å¤„ç†å¤šç§è§†è§‰ä»»åŠ¡ã€‚</li>
<li>PixWizardä½¿ç”¨Diffusion Transformersä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶å…·å¤‡çµæ´»çš„åˆ†è¾¨ç‡æœºåˆ¶ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¨¡æ¿ï¼Œæ¶µç›–å¤šç§è§†è§‰ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒæ¢å¤ç­‰ã€‚</li>
<li>æ¨¡å‹ç»“åˆç»“æ„æ„ŸçŸ¥å’Œè¯­ä¹‰æ„ŸçŸ¥æŒ‡å¯¼ï¼Œæœ‰æ•ˆèåˆè¾“å…¥å›¾åƒçš„ä¿¡æ¯ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºPixWizardåœ¨å¤šæ ·åˆ†è¾¨ç‡çš„å›¾åƒå¤„ç†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-683fbeecf24ceded7e1f02f4064e5070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a165250cac75a46bfd7a0f9aca390e00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f7fc29b12bc5a21314e1b2bf530294d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335b3688fe8b6e6e1ad4b7930b8720f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RRM-Robust-Reward-Model-Training-Mitigates-Reward-Hacking"><a href="#RRM-Robust-Reward-Model-Training-Mitigates-Reward-Hacking" class="headerlink" title="RRM: Robust Reward Model Training Mitigates Reward Hacking"></a>RRM: Robust Reward Model Training Mitigates Reward Hacking</h2><p><strong>Authors:Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh</strong></p>
<p>Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them. Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„RMè®­ç»ƒä¾èµ–äºä¸ç‰¹å®šæç¤ºç›¸å…³è”çš„å“åº”å¯¹ï¼Œè¿™ä½¿å…¶åœ¨åŒºåˆ†æç¤ºé©±åŠ¨çš„åå¥½å’Œæç¤ºæ— å…³çš„ä¼ªç‰¹å¾ï¼ˆå¦‚å“åº”é•¿åº¦å’Œæ ¼å¼ï¼‰æ—¶é‡åˆ°å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰RMè®­ç»ƒæ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼Œå³RMåœ¨ç¡®å®šåå¥½æ—¶æ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸Šä¸‹æ–‡ä¿¡å·å’Œæ— å…³ä¼ªç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹äºè¿™äº›ä¼ªç‰¹å¾çš„å› æœæ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ—¨åœ¨æ¶ˆé™¤è¿™äº›ä¼ªç‰¹å¾çš„æ–°å‹æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸè¿‡æ»¤æ‰äº†ä¸å¿…è¦çš„ä¼ªç‰¹å¾ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªæ›´ç¨³å¥çš„å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰ã€‚æˆ‘ä»¬çš„RRMåœ¨Gemma-2-9b-itä¸Šè®­ç»ƒçš„æˆå¯¹å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½åœ¨RewardBenchä¸Šæœ‰æ‰€æé«˜ï¼Œå‡†ç¡®ç‡ä»80.61%æé«˜åˆ°84.15%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨RMå’ŒRRMè®­ç»ƒäº†ä¸¤ä¸ªDPOç­–ç•¥ï¼Œè¯æ˜RRMæ˜¾è‘—å¢å¼ºäº†ä¸DPOå¯¹é½çš„ç­–ç•¥ï¼ŒMT-Benchå¾—åˆ†ä»7.27æé«˜åˆ°8.31ï¼Œåœ¨AlpacaEval-2ä¸­çš„é•¿åº¦æ§åˆ¶èƒœç‡ä»33.46%æé«˜åˆ°52.49%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13156v2">PDF</a> Accepted in ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¼ ç»ŸRMè®­ç»ƒæ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸Šä¸‹æ–‡ä¿¡å·å’Œæ— å…³ä¼ªå½±çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å› æœæ¡†æ¶å’Œä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæŠ€æœ¯æ¥æ¶ˆé™¤ä¼ªå½±å¹²æ‰°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸè¿‡æ»¤æ‰ä¸éœ€è¦çš„ä¼ªå½±ï¼Œäº§ç”Ÿæ›´ç¨³å¥çš„å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰ã€‚RRMæé«˜äº†åŸºäºGemma-2-9b-itçš„é…å¯¹å¥–åŠ±æ¨¡å‹åœ¨RewardBenchä¸Šçš„å‡†ç¡®æ€§ï¼Œä»80.61%æé«˜åˆ°84.15%ã€‚æ­¤å¤–ï¼Œä½¿ç”¨RMå’ŒRRMè®­ç»ƒçš„ä¸¤ç§DPOç­–ç•¥æ˜¾ç¤ºï¼ŒRRMæ˜¾è‘—æé«˜äº†ä¸DPOå¯¹é½çš„æ”¿ç­–ï¼ŒMT-Benchåˆ†æ•°ä»7.27æé«˜åˆ°8.31ï¼Œåœ¨AlpacaEval-2ä¸­çš„é•¿åº¦æ§åˆ¶èƒœç‡ä»33.46%æé«˜åˆ°52.49%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¯¹é½äººç±»åå¥½æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚</li>
<li>ä¼ ç»ŸRMè®­ç»ƒé¢ä¸´æ— æ³•æœ‰æ•ˆåŒºåˆ†ä¸Šä¸‹æ–‡ä¿¡å·å’Œæ— å…³ä¼ªå½±çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å› æœæ¡†æ¶å’Œæ–°çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä»¥æ¶ˆé™¤ä¼ªå½±å¹¶äº§ç”Ÿæ›´ç¨³å¥çš„å¥–åŠ±æ¨¡å‹ï¼ˆRRMï¼‰ã€‚</li>
<li>RRMèƒ½æé«˜é…å¯¹å¥–åŠ±æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>RRMæ˜¾è‘—æé«˜ä¸DPOå¯¹é½çš„æ”¿ç­–è¡¨ç°ï¼Œä½“ç°åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šçš„æ”¹å–„ã€‚</li>
<li>å®éªŒè¯æ˜RRMèƒ½å¤Ÿè¿‡æ»¤æ‰ä¸éœ€è¦çš„ä¼ªå½±ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9782e48d12fb701c2003e1ff4c31054c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cebb9064a82bafeddd39eb929d6495c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b67387f1127198eed2003190cb906052.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-84d4f38d510f1dbd809b3d5331d556ad.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  Multi-Agent Verification Scaling Test-Time Compute with Multiple   Verifiers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-02-28\./crop_Talking Head Generation/2502.18725v1/page_0_0.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-28  Talking to the brain Using Large Language Models as Proxies to Model   Brain Semantic Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19758k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
