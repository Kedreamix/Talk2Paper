<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  M^3Builder A Multi-Agent System for Automated Machine Learning in   Medical Imaging">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-eae2d74ff268df4180ff80ef656c65d8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-01-æ›´æ–°"><a href="#2025-03-01-æ›´æ–°" class="headerlink" title="2025-03-01 æ›´æ–°"></a>2025-03-01 æ›´æ–°</h1><h2 id="M-3Builder-A-Multi-Agent-System-for-Automated-Machine-Learning-in-Medical-Imaging"><a href="#M-3Builder-A-Multi-Agent-System-for-Automated-Machine-Learning-in-Medical-Imaging" class="headerlink" title="M^3Builder: A Multi-Agent System for Automated Machine Learning in   Medical Imaging"></a>M^3Builder: A Multi-Agent System for Automated Machine Learning in   Medical Imaging</h2><p><strong>Authors:Jinghao Feng, Qiaoyu Zheng, Chaoyi Wu, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging. </p>
<blockquote>
<p>æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿå› å…¶è‡ªä¸»æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹ç²¾å¿ƒå‡†å¤‡çš„å·¥å…·çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ»å­¦é¢†åŸŸéœ€è¦è®­ç»ƒä¸“ä¸šæ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸‰é¡¹è´¡çŒ®ï¼šï¼ˆä¸€ï¼‰æˆ‘ä»¬æå‡ºäº†M3Builderï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–åŒ»å­¦å½±åƒä¸­çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ã€‚M3Builderçš„æ ¸å¿ƒé‡‡ç”¨å››ä¸ªä¸“ä¸šæ™ºèƒ½ä½“è¿›è¡Œåä½œï¼Œä»¥å¤„ç†å¤æ‚çš„ã€å¤šæ­¥éª¤çš„åŒ»å­¦MLå·¥ä½œæµç¨‹ï¼Œä»è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†å’Œç¯å¢ƒé…ç½®åˆ°è‡ªä¸»çš„è‡ªåŠ¨è°ƒè¯•å’Œæ¨¡å‹è®­ç»ƒã€‚è¿™äº›æ™ºèƒ½ä½“åœ¨åŒ»å­¦å½±åƒMLå·¥ä½œç©ºé—´å†…è¿è¡Œï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„åŒ–ç¯å¢ƒï¼Œæ—¨åœ¨ä¸ºæ™ºèƒ½ä½“æä¾›æ•°æ®é›†ã€è®­ç»ƒä»£ç å’Œäº¤äº’å·¥å…·çš„æ–‡æœ¬æè¿°ï¼Œä»è€Œå®ç°æ— ç¼é€šä¿¡å’Œä»»åŠ¡æ‰§è¡Œã€‚ï¼ˆäºŒï¼‰ä¸ºäº†è¯„ä¼°åŒ»å­¦å½±åƒè‡ªåŠ¨åŒ–çš„MLè¿›åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†M3Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å››ä¸ªä¸€èˆ¬ä»»åŠ¡åœ¨14ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šï¼Œæ¶‰åŠäº”ä¸ªè§£å‰–éƒ¨ä½å’Œä¸‰ç§æˆåƒæ¨¡å¼ï¼Œæ¶µç›–2Då’Œ3Dæ•°æ®ã€‚ï¼ˆä¸‰ï¼‰æˆ‘ä»¬å°è¯•äº†ä¸ƒç§æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ä½œä¸ºæˆ‘ä»¬ç³»ç»Ÿçš„æ™ºèƒ½ä½“æ ¸å¿ƒï¼Œå¦‚Claudeç³»åˆ—ã€GPT-4oå’ŒDeepSeek-V3ã€‚ä¸ç°æœ‰çš„MLæ™ºèƒ½ä½“è®¾è®¡ç›¸æ¯”ï¼ŒM3Builderåœ¨å®ŒæˆåŒ»å­¦å½±åƒä¸­çš„MLä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½¿ç”¨Claude-3.7-Sonnetä½œä¸ºæ™ºèƒ½ä½“æ ¸å¿ƒæ—¶ï¼ŒæˆåŠŸç‡è¾¾åˆ°94.29%ï¼Œæ˜¾ç¤ºå‡ºåœ¨åŒ»å­¦å½±åƒå…¨è‡ªåŠ¨æœºå™¨å­¦ä¹ çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20301v1">PDF</a> 38 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†M3Builderï¼Œä¸€ä¸ªé’ˆå¯¹åŒ»å­¦æˆåƒé¢†åŸŸè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç”±å››ä¸ªä¸“é—¨æ™ºèƒ½ä½“ç»„æˆï¼Œå¯åä½œå®Œæˆå¤æ‚çš„åŒ»å­¦MLå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†ã€ç¯å¢ƒé…ç½®ã€è‡ªåŠ¨è°ƒè¯•å’Œæ¨¡å‹è®­ç»ƒç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†M3Benchï¼Œä¸€ä¸ªæ¶µç›–ä¸åŒè§£å‰–éƒ¨ä½å’Œæˆåƒæ¨¡å¼çš„åŒ»å­¦æˆåƒMLåŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼ŒM3Builderåœ¨å®ŒæˆåŒ»å­¦æˆåƒMLä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½¿ç”¨Claude-3.7-Sonnetä½œä¸ºæ ¸å¿ƒæ™ºèƒ½ä½“çš„æˆåŠŸç‡ä¸º94.29%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>M3Builderæ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦æˆåƒè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒåŒ…å«å››ä¸ªä¸“é—¨æ™ºèƒ½ä½“ï¼Œå¯åä½œå®Œæˆå¤æ‚æµç¨‹ã€‚</li>
<li>M3Builderåœ¨åŒ»å­¦æˆåƒMLä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½¿ç”¨Claude-3.7-Sonnetä½œä¸ºæ ¸å¿ƒæ™ºèƒ½ä½“çš„æˆåŠŸç‡ä¸º94.29%ã€‚</li>
<li>M3Builderåœ¨ä¸€ä¸ªç»“æ„åŒ–çš„åŒ»å­¦æˆåƒMLå·¥ä½œç©ºé—´ä¸­è¿è¡Œï¼Œæä¾›æ•°æ®é›†æè¿°ã€è®­ç»ƒä»£ç å’Œäº¤äº’å·¥å…·ã€‚</li>
<li>M3Benchæ˜¯ä¸€ä¸ªæ¶µç›–å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†çš„åŒ»å­¦æˆåƒMLåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨åŒ–åŒ»å­¦æˆåƒMLçš„è¿›å±•ã€‚</li>
<li>M3Builderçš„æ™ºèƒ½ä½“æ ¸å¿ƒå¯ä»¥é‡‡ç”¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claudeç³»åˆ—ã€GPT-4oå’ŒDeepSeek-V3ã€‚</li>
<li>M3Builderå¯¹å®Œå…¨è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒä¸­çš„æ½œåŠ›å…·æœ‰å·¨å¤§å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84d4f38d510f1dbd809b3d5331d556ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d74b3f214371bf5356a15a34d72428ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edbf97479d5a11c6c804e54f4702b477.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DIN-CTS-Low-Complexity-Depthwise-Inception-Neural-Network-with-Contrastive-Training-Strategy-for-Deepfake-Speech-Detection"><a href="#DIN-CTS-Low-Complexity-Depthwise-Inception-Neural-Network-with-Contrastive-Training-Strategy-for-Deepfake-Speech-Detection" class="headerlink" title="DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with   Contrastive Training Strategy for Deepfake Speech Detection"></a>DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with   Contrastive Training Strategy for Deepfake Speech Detection</h2><p><strong>Authors:Lam Pham, Dat Tran, Florian Skopik, Alexander Schindler, Silvia Poletti, Fischinger David, Martin Boyer</strong></p>
<p>In this paper, we propose a deep neural network approach for deepfake speech detection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN) trained with a contrastive training strategy (CTS). In this framework, input audio recordings are first transformed into spectrograms using Short-Time Fourier Transform (STFT) and Linear Filter (LF), which are then used to train the DIN. Once trained, the DIN processes bonafide utterances to extract audio embeddings, which are used to construct a Gaussian distribution representing genuine speech. Deepfake detection is then performed by computing the distance between a test utterance and this distribution to determine whether the utterance is fake or bonafide. To evaluate our proposed systems, we conducted extensive experiments on the benchmark dataset of ASVspoof 2019 LA. The experimental results demonstrate the effectiveness of combining the Depthwise-Inception Network with the contrastive learning strategy in distinguishing between fake and bonafide utterances. We achieved Equal Error Rate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9% respectively using a single, low-complexity DIN with just 1.77 M parameters and 985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed system outperforms the single-system submissions in the ASVspoof 2019 LA challenge, showcasing its potential for real-time applications. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä½å¤æ‚åº¦æ·±åº¦å¯åˆ†ç¦»æ„ŸçŸ¥ç½‘ç»œï¼ˆDINï¼‰å’Œå¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼ˆCTSï¼‰çš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹ï¼ˆDSDï¼‰çš„æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œé¦–å…ˆä½¿ç”¨çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰å’Œçº¿æ€§æ»¤æ³¢å™¨ï¼ˆLFï¼‰å°†è¾“å…¥éŸ³é¢‘è®°å½•è½¬æ¢ä¸ºé¢‘è°±å›¾ï¼Œç„¶åç”¨å®ƒä»¬æ¥è®­ç»ƒDINã€‚è®­ç»ƒå®Œæˆåï¼ŒDINå¤„ç†çœŸå®è¯­éŸ³ç‰‡æ®µä»¥æå–éŸ³é¢‘åµŒå…¥ï¼Œç”¨äºæ„å»ºä»£è¡¨çœŸå®è¯­éŸ³çš„é«˜æ–¯åˆ†å¸ƒã€‚ç„¶åé€šè¿‡è®¡ç®—æµ‹è¯•è¯­éŸ³ç‰‡æ®µä¸æ­¤åˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹ï¼Œä»¥ç¡®å®šè¯­éŸ³ç‰‡æ®µæ˜¯ä¼ªé€ çš„è¿˜æ˜¯çœŸå®çš„ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿï¼Œæˆ‘ä»¬åœ¨ASVspoof 2019 LAçš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¯æ˜äº†æ·±åº¦å¯åˆ†ç¦»æ„ŸçŸ¥ç½‘ç»œä¸å¯¹æ¯”å­¦ä¹ ç­–ç•¥ç›¸ç»“åˆåœ¨åŒºåˆ†ä¼ªé€ å’ŒçœŸå®è¯­éŸ³ç‰‡æ®µæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨å•ä¸€ã€ä½å¤æ‚åº¦çš„DINï¼Œåœ¨çŸ­éŸ³é¢‘ç‰‡æ®µï¼ˆ4ç§’ï¼‰ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†ç­‰è¯¯ç‡ï¼ˆEERï¼‰ã€å‡†ç¡®ç‡ï¼ˆAcc.ï¼‰ã€F1ã€AUCå¾—åˆ†åˆ†åˆ«ä¸º4.6%ã€95.4%ã€97.3%å’Œ98.9%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„ç³»ç»Ÿåœ¨ASVspoof 2019 LAæŒ‘æˆ˜ä¸­çš„å•ç³»ç»Ÿæäº¤ä¸­è¡¨ç°çªå‡ºï¼Œå±•ç¤ºäº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20225v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹ï¼ˆDSDï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä½å¤æ‚åº¦çš„Depthwise-Inceptionç½‘ç»œï¼ˆDINï¼‰å’Œå¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼ˆCTSï¼‰ã€‚éŸ³é¢‘è¾“å…¥é¦–å…ˆé€šè¿‡çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰å’Œçº¿æ€§æ»¤æ³¢å™¨ï¼ˆLFï¼‰è½¬æ¢ä¸ºé¢‘è°±å›¾æ¥è®­ç»ƒDINã€‚è®­ç»ƒåçš„DINå¤„ç†çœŸå®è¯­éŸ³ä»¥æå–éŸ³é¢‘åµŒå…¥ï¼Œæ„å»ºä»£è¡¨çœŸå®è¯­éŸ³çš„é«˜æ–¯åˆ†å¸ƒã€‚é€šè¿‡è®¡ç®—æµ‹è¯•è¯­éŸ³ä¸æ­¤åˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹ï¼Œä»¥ç¡®å®šè¯­éŸ³æ˜¯å¦çœŸå®ã€‚åœ¨ASVspoof 2019 LAåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†Depthwise-Inceptionç½‘ç»œä¸å¯¹æ¯”å­¦ä¹ ç­–ç•¥ç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨å•ä¸€ã€ä½å¤æ‚åº¦çš„DINï¼Œåœ¨çŸ­çŸ­4ç§’çš„éŸ³é¢‘ç‰‡æ®µä¸Šï¼Œæˆ‘ä»¬å–å¾—äº†4.6%çš„ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰ã€95.4%çš„å‡†ç¡®ç‡ï¼ˆAcc.ï¼‰ã€97.3%çš„F1åˆ†æ•°å’Œ98.9%çš„AUCåˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ASVspoof 2019 LAæŒ‘æˆ˜ä¸­çš„å•ç³»ç»Ÿæäº¤ä¸­è¡¨ç°æœ€ä½³ï¼Œå±•ç¤ºäº†å…¶å®æ—¶åº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨ä½å¤æ‚åº¦çš„Depthwise-Inceptionç½‘ç»œï¼ˆDINï¼‰å’Œå¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼ˆCTSï¼‰ã€‚</li>
<li>è¾“å…¥éŸ³é¢‘é€šè¿‡çŸ­æ—¶å‚…é‡Œå¶å˜æ¢å’Œçº¿æ€§æ»¤æ³¢å™¨è½¬æ¢ä¸ºé¢‘è°±å›¾è¿›è¡Œè®­ç»ƒã€‚</li>
<li>DINèƒ½å¤Ÿå¤„ç†çœŸå®è¯­éŸ³å¹¶æå–éŸ³é¢‘åµŒå…¥ï¼Œæ„å»ºä»£è¡¨çœŸå®è¯­éŸ³çš„é«˜æ–¯åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡è®¡ç®—æµ‹è¯•è¯­éŸ³ä¸é«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„è·ç¦»æ¥åˆ¤æ–­å…¶çœŸä¼ªã€‚</li>
<li>åœ¨ASVspoof 2019 LAæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7d4096862f862b177167da28eb7dee85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-826dc50db8dbda50f92984de0cba80b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4431b6ff4ddc715ee9c41de8561ec4fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb6189cacbfb15ea63c3c4048523fed6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b08e3b3ee4a985374bace76e565de1c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-augmentations-for-improved-cardiac-ultrasound-segmentation-using-diffusion-models"><a href="#Generative-augmentations-for-improved-cardiac-ultrasound-segmentation-using-diffusion-models" class="headerlink" title="Generative augmentations for improved cardiac ultrasound segmentation   using diffusion models"></a>Generative augmentations for improved cardiac ultrasound segmentation   using diffusion models</h2><p><strong>Authors:Gilles Van De Vyver, Aksel Try Lenz, Erik Smistad, Sindre Hellum Olaisen, BjÃ¸rnar Grenne, Espen Holte, HÃ¥avard Dalen, Lasse LÃ¸vstakken</strong></p>
<p>One of the main challenges in current research on segmentation in cardiac ultrasound is the lack of large and varied labeled datasets and the differences in annotation conventions between datasets. This makes it difficult to design robust segmentation models that generalize well to external datasets. This work utilizes diffusion models to create generative augmentations that can significantly improve diversity of the dataset and thus the generalisability of segmentation models without the need for more annotated data. The augmentations are applied in addition to regular augmentations. A visual test survey showed that experts cannot clearly distinguish between real and fully generated images. Using the proposed generative augmentations, segmentation robustness was increased when training on an internal dataset and testing on an external dataset with an improvement of over 20 millimeters in Hausdorff distance. Additionally, the limits of agreement for automatic ejection fraction estimation improved by up to 20% of absolute ejection fraction value on out of distribution cases. These improvements come exclusively from the increased variation of the training data using the generative augmentations, without modifying the underlying machine learning model. The augmentation tool is available as an open source Python library at <a target="_blank" rel="noopener" href="https://github.com/GillesVanDeVyver/EchoGAINS">https://github.com/GillesVanDeVyver/EchoGAINS</a>. </p>
<blockquote>
<p>å½“å‰å¿ƒè„è¶…å£°åˆ†å‰²ç ”ç©¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„æ ‡è®°æ•°æ®é›†ä»¥åŠä¸åŒæ•°æ®é›†ä¹‹é—´æ ‡æ³¨è§„èŒƒçš„ä¸åŒã€‚è¿™ä½¿å¾—è®¾è®¡èƒ½å¤Ÿè‰¯å¥½åœ°æ¨å¹¿åˆ°å¤–éƒ¨æ•°æ®é›†çš„ç¨³å¥åˆ†å‰²æ¨¡å‹å˜å¾—å›°éš¾ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆ›å»ºç”Ÿæˆå¢å¼ºæŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œä»è€Œåœ¨æ— éœ€æ›´å¤šæ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹æé«˜åˆ†å‰²æ¨¡å‹çš„å¯æ¨å¹¿æ€§ã€‚é™¤äº†å¸¸è§„å¢å¼ºæŠ€æœ¯å¤–ï¼Œè¿˜åº”ç”¨äº†è¿™äº›ç”Ÿæˆå¢å¼ºæŠ€æœ¯ã€‚è§†è§‰æµ‹è¯•è°ƒæŸ¥è¡¨æ˜ï¼Œä¸“å®¶æ— æ³•æ¸…æ¥šåŒºåˆ†çœŸå®å›¾åƒå’Œå®Œå…¨ç”Ÿæˆçš„å›¾åƒã€‚ä½¿ç”¨æ‰€æå‡ºçš„ç”Ÿæˆå¢å¼ºæŠ€æœ¯ï¼Œåœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œåˆ†å‰²ç¨³å¥æ€§æœ‰æ‰€æé«˜ï¼Œè±ªæ–¯å¤šå¤«è·ç¦»æé«˜äº†è¶…è¿‡20æ¯«ç±³ã€‚æ­¤å¤–ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ¡ˆä¾‹ä¸­ï¼Œè‡ªåŠ¨å°„è¡€åˆ†æ•°ä¼°è®¡çš„åè®®é™åˆ¶æé«˜äº†é«˜è¾¾ç»å¯¹å°„è¡€åˆ†æ•°å€¼çš„20%ã€‚è¿™äº›æ”¹è¿›å®Œå…¨æ¥è‡ªäºä½¿ç”¨ç”Ÿæˆå¢å¼ºæŠ€æœ¯åè®­ç»ƒæ•°æ®å˜å¼‚çš„å¢åŠ ï¼Œè€Œæ²¡æœ‰ä¿®æ”¹åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¯¥å¢å¼ºå·¥å…·å¯ä½œä¸ºå¼€æºPythonåº“åœ¨<a target="_blank" rel="noopener" href="https://github.com/GillesVanDeVyver/EchoGAINS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/GillesVanDeVyver/EchoGAINSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20100v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆ›å»ºç”Ÿæˆæ€§å¢å¼ºæ•°æ®ï¼Œæœ‰æ•ˆæé«˜äº†æ•°æ®é›†å¤šæ ·æ€§ï¼Œä»è€Œæé«˜äº†åˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ›´å¤šæ³¨é‡Šæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ã€‚é€šè¿‡åº”ç”¨å¸¸è§„å¢å¼ºå’Œç”Ÿæˆæ€§å¢å¼ºï¼Œä¸“å®¶æ— æ³•æ˜ç¡®åŒºåˆ†çœŸå®å’Œå®Œå…¨ç”Ÿæˆçš„å›¾åƒã€‚åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œæé«˜äº†åˆ†å‰²çš„ç¨³å¥æ€§ï¼Œè±ªæ–¯æ´›è·ç¦»æé«˜äº†20æ¯«ç±³ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œåœ¨è‡ªåŠ¨å°„è¡€åˆ†æ•°ä¼°è®¡æ–¹é¢ï¼Œå¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ¡ˆä¾‹ï¼Œç»å¯¹å°„è¡€åˆ†æ•°å€¼çš„åŒæ„é™åº¦æé«˜äº†20%ã€‚è¿™äº›æ”¹è¿›å®Œå…¨æ¥è‡ªäºä½¿ç”¨ç”Ÿæˆå¢å¼ºæ•°æ®çš„è®­ç»ƒæ•°æ®å¤šæ ·æ€§çš„å¢åŠ ï¼Œè€Œæ²¡æœ‰ä¿®æ”¹åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¿ƒè„è¶…å£°åˆ†å‰²ç ”ç©¶çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¼ºä¹å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ ‡è®°æ•°æ®é›†ä»¥åŠæ•°æ®é›†ä¹‹é—´æ³¨é‡Šè§„èŒƒçš„ä¸åŒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨æ¥åˆ›å»ºç”Ÿæˆæ€§å¢å¼ºæ•°æ®ï¼Œä»¥æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§å’Œåˆ†å‰²æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆæ€§å¢å¼ºæ•°æ®çš„åº”ç”¨ä½¿å¾—ä¸“å®¶éš¾ä»¥åŒºåˆ†çœŸå®å’Œç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒã€åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šæµ‹è¯•æ—¶ï¼Œä½¿ç”¨ç”Ÿæˆæ€§å¢å¼ºæ•°æ®æé«˜äº†åˆ†å‰²ç¨³å¥æ€§ï¼Œè¡¨ç°åœ¨è±ªæ–¯æ´›è·ç¦»çš„æé«˜ä¸Šã€‚</li>
<li>è‡ªåŠ¨å°„è¡€åˆ†æ•°ä¼°è®¡çš„æ”¹è¿›æ˜¾è‘—ï¼Œå¯¹è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ¡ˆä¾‹ï¼Œç»å¯¹å°„è¡€åˆ†æ•°å€¼çš„åŒæ„é™åº¦æé«˜äº†20%ã€‚</li>
<li>è¿™äº›æ”¹è¿›æ¥æºäºè®­ç»ƒæ•°æ®å¤šæ ·æ€§çš„å¢åŠ ï¼Œè€Œä¸æ˜¯å¯¹åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¿®æ”¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-df3a4b56e3908a74de8bd2fe6a89115f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b6a586a0f1e9a12edc35e255840aabc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc6d42169392e0610a1910d8e7bdeee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07caa2be4010d46afd254b41b49c07a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73a1a953c5370502b1c8f485951ec1e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-843b21b0a92ca0c57c00e4b2a2676f4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9111d72fee06ad054a1d2fe5d82e3440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e4fc36e0a9f8e2072fe26408c6d4108.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3001f6139fee3d8b619817c96088acc8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhanced-Contrastive-Learning-with-Multi-view-Longitudinal-Data-for-Chest-X-ray-Report-Generation"><a href="#Enhanced-Contrastive-Learning-with-Multi-view-Longitudinal-Data-for-Chest-X-ray-Report-Generation" class="headerlink" title="Enhanced Contrastive Learning with Multi-view Longitudinal Data for   Chest X-ray Report Generation"></a>Enhanced Contrastive Learning with Multi-view Longitudinal Data for   Chest X-ray Report Generation</h2><p><strong>Authors:Kang Liu, Zhuoqi Ma, Xiaolu Kang, Yunan Li, Kun Xie, Zhicheng Jiao, Qiguang Miao</strong></p>
<p>Automated radiology report generation offers an effective solution to alleviate radiologistsâ€™ workload. However, most existing methods focus primarily on single or fixed-view images to model current disease conditions, which limits diagnostic accuracy and overlooks disease progression. Although some approaches utilize longitudinal data to track disease progression, they still rely on single images to analyze current visits. To address these issues, we propose enhanced contrastive learning with Multi-view Longitudinal data to facilitate chest X-ray Report Generation, named MLRG. Specifically, we introduce a multi-view longitudinal contrastive learning method that integrates spatial information from current multi-view images and temporal information from longitudinal data. This method also utilizes the inherent spatiotemporal information of radiology reports to supervise the pre-training of visual and textual representations. Subsequently, we present a tokenized absence encoding technique to flexibly handle missing patient-specific prior knowledge, allowing the model to produce more accurate radiology reports based on available prior knowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXR datasets demonstrate that our MLRG outperforms recent state-of-the-art methods, achieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvement on MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR. </p>
<blockquote>
<p>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä»¥å‡è½»æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•è§†å›¾æˆ–å›ºå®šè§†å›¾å›¾åƒæ¥å»ºæ¨¡å½“å‰ç–¾ç—…çŠ¶å†µï¼Œè¿™é™åˆ¶äº†è¯Šæ–­çš„å‡†ç¡®æ€§å¹¶å¿½ç•¥äº†ç–¾ç—…çš„è¿›å±•ã€‚è™½ç„¶æœ‰äº›æ–¹æ³•åˆ©ç”¨çºµå‘æ•°æ®è¿½è¸ªç–¾ç—…è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶ä¾èµ–äºå•å¹…å›¾åƒæ¥åˆ†æå½“å‰å°±è¯Šæƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¤šè§†å›¾çºµå‘æ•°æ®çš„å¢å¼ºå¯¹æ¯”å­¦ä¹ æ¥ä¿ƒè¿›èƒ¸éƒ¨Xå°„çº¿æŠ¥å‘Šç”Ÿæˆï¼Œå‘½åä¸ºMLRGã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šè§†å›¾çºµå‘å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å½“å‰å¤šè§†å›¾å›¾åƒçš„ç©ºé—´ä¿¡æ¯å’Œçºµå‘æ•°æ®çš„æ—¶åºä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•è¿˜åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„å›ºæœ‰æ—¶ç©ºä¿¡æ¯æ¥ç›‘ç£è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„é¢„è®­ç»ƒã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ ‡è®°ç¼ºå¤±ç¼–ç æŠ€æœ¯ï¼Œå¯ä»¥çµæ´»åœ°å¤„ç†ç‰¹å®šäºæ‚£è€…çš„ç¼ºå¤±å…ˆéªŒçŸ¥è¯†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å¯ç”¨çš„å…ˆéªŒçŸ¥è¯†ç”Ÿæˆæ›´å‡†ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚åœ¨MIMIC-CXRã€MIMIC-ABNå’ŒTwo-view CXRæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MLRGä¼˜äºæœ€æ–°çš„å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨MIMIC-CXRä¸ŠBLEU-4æé«˜äº†2.3%ï¼Œåœ¨MIMIC-ABNä¸ŠF1å¾—åˆ†æé«˜äº†5.5%ï¼Œåœ¨Two-view CXRä¸ŠF1 RadGraphæé«˜äº†2.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20056v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–ç”ŸæˆæŠ¥å‘Šèƒ½æœ‰æ•ˆç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€æˆ–å›ºå®šè§†è§’çš„å›¾åƒæ¥å»ºæ¨¡å½“å‰ç–¾ç—…çŠ¶å†µï¼Œé™åˆ¶äº†è¯Šæ–­å‡†ç¡®æ€§å’Œç–¾ç—…è¿›å±•çš„æ´å¯Ÿã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºå¤šè§†è§’çºµå‘æ•°æ®çš„å¯¹æ¯”å­¦ä¹ å¢å¼ºæ–¹æ³•ï¼Œç”¨äºä¿ƒè¿›èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆï¼ˆå‘½åä¸ºMLRGï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å½“å‰å¤šè§†è§’å›¾åƒçš„ç©ºé—´ä¿¡æ¯å’Œçºµå‘æ•°æ®çš„æ—¶åºä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„æ—¶ç©ºä¿¡æ¯å¯¹è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºè¿›è¡Œé¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ ‡è®°ç¼ºå¤±ç¼–ç æŠ€æœ¯ï¼Œå¯çµæ´»å¤„ç†æ‚£è€…ç‰¹å®šå…ˆéªŒçŸ¥è¯†çš„ç¼ºå¤±ï¼Œä½¿æ¨¡å‹åŸºäºå¯ç”¨çš„å…ˆéªŒçŸ¥è¯†äº§ç”Ÿæ›´å‡†ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚å®éªŒè¡¨æ˜ï¼ŒMLRGåœ¨MIMIC-CXRã€MIMIC-ABNå’ŒTwo-view CXRæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨MIMIC-CXRä¸ŠBLEU-4æé«˜äº†2.3%ï¼Œåœ¨MIMIC-ABNä¸ŠF1åˆ†æ•°æé«˜äº†5.5%ï¼Œåœ¨Two-view CXRä¸ŠF1 RadGraphæé«˜äº†2.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç”ŸæˆåŒ»å­¦å›¾åƒæŠ¥å‘Šå¯ä»¥ç¼“è§£æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œç–¾ç—…è¿›å±•æ´å¯Ÿæ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦å…³æ³¨å•ä¸€æˆ–å›ºå®šè§†è§’çš„å›¾åƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†è§’çºµå‘æ•°æ®çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•MLRGï¼Œç»“åˆç©ºé—´å’Œæ—¶åºä¿¡æ¯ã€‚</li>
<li>åˆ©ç”¨æ”¾å°„å­¦æŠ¥å‘Šçš„æ—¶ç©ºä¿¡æ¯å¯¹è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>æå‡ºäº†æ ‡è®°ç¼ºå¤±ç¼–ç æŠ€æœ¯ï¼Œä»¥çµæ´»å¤„ç†æ‚£è€…ç‰¹å®šå…ˆéªŒçŸ¥è¯†çš„ç¼ºå¤±ã€‚</li>
<li>MLRGåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f25975146b9e0e500f1e0dcbc8fc70b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894999edb3d590f6cbf3c061f233a0c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bab4bb81e45af7b8837b1a320e2b694d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-430ea43a3d021a0b6673df744c8be669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a5d289d257d31f2d3fb0da62d195724.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Near-and-Mid-UltraViolet-Observations-of-X-6-3-flare-on-22nd-February-2024-recorded-by-the-Solar-Ultraviolet-Imaging-Telescope-on-board-Aditya-L1"><a href="#Near-and-Mid-UltraViolet-Observations-of-X-6-3-flare-on-22nd-February-2024-recorded-by-the-Solar-Ultraviolet-Imaging-Telescope-on-board-Aditya-L1" class="headerlink" title="Near and Mid UltraViolet Observations of X-6.3 flare on 22nd February   2024 recorded by the Solar Ultraviolet Imaging Telescope on board Aditya-L1"></a>Near and Mid UltraViolet Observations of X-6.3 flare on 22nd February   2024 recorded by the Solar Ultraviolet Imaging Telescope on board Aditya-L1</h2><p><strong>Authors:Soumya Roy, Durgesh Tripathi, Sreejith Padinhatteeri, A. N. Ramaprakash, Abhilash R. Sarwade, V. N. Nived, Janmejoy Sarkar, Rahul Gopalakrishnan, Rushikesh Deogaonkar, K. Sankarasubramanian, Sami K. Solanki, Dibyendu Nandy, Dipankar Banerjee</strong></p>
<p>Solar flares are regularly observed in extreme ultraviolet (EUV), soft X-rays (SXR), and hard X-rays (HXR). However, those in near and mid-UV are sparse. The Solar Ultraviolet Imaging Telescope (SUIT) onboard the Aditya-L1, launched on 2nd September, 2023 provides regular observations in the 200-400 nm wavelength range through eleven filters. Here, we report the observation of the X6.3 flare on Feb 22, 2024 using eight narrow band (NB) filters of SUIT. We have also used co-spatiotemporal observations from SDO&#x2F;AIA, Solar Orbiter&#x2F;STIX, GONG H$\alpha$, Aditya-L1&#x2F;SoLEXS and GOES. We obtained light curves over the flaring region from AIA 1600, 1700 \r{A} and GONG H$\alpha$ and compared them with the disk-integrated lightcurve obtained from GOES and SoLEXS SXR and STIX HXR. We find that the flare peaks in SUIT NB01, NB03, NB04, and NB08 filters simultaneously with HXR, 1600, and 1700 \r{A} along with the peak temperature obtained from SoLEXS. In contrast, in NB02 and NB05, the flare peaks $\sim$ 2 minutes later than the HXR peak, while in NB06 and NB07, the flare peaks $\sim$ 3 minutes after the GOES soft X-ray peak. To the best of our knowledge, this is the first observation of a flare in these wavelengths (except in NB03, NB04 and NB05). Moreover, for the first time, we show the presence of a bright kernel in NB02. These results demonstrate the capabilities of SUIT observations in flare studies. </p>
<blockquote>
<p>å¤ªé˜³è€€æ–‘ç»å¸¸ä¼šåœ¨æç´«å¤–ï¼ˆEUVï¼‰ã€è½¯Xå°„çº¿ï¼ˆSXRï¼‰å’Œç¡¬Xå°„çº¿ï¼ˆHXRï¼‰ä¸­è§‚å¯Ÿåˆ°ã€‚ç„¶è€Œï¼Œåœ¨è¿‘ç´«å¤–å’Œä¸­ç´«å¤–ä¸­çš„è€€æ–‘åˆ™è¾ƒä¸ºç¨€å°‘ã€‚Aditya-L1å·å«æ˜Ÿä¸Šçš„å¤ªé˜³èƒ½ç´«å¤–çº¿æˆåƒæœ›è¿œé•œï¼ˆSUITï¼‰äº2023å¹´9æœˆ2æ—¥å‘å°„å‡ç©ºï¼Œå¯ä»¥é€šè¿‡åä¸€ä¸ªæ»¤é•œæä¾›200-400çº³ç±³æ³¢é•¿èŒƒå›´å†…çš„å®šæœŸè§‚æµ‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨SUITçš„å…«ä¸ªçª„å¸¦ï¼ˆNBï¼‰æ»¤é•œè§‚å¯Ÿåˆ°çš„2024å¹´2æœˆ22æ—¥çš„X6.3çº§è€€æ–‘ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†æ¥è‡ªSDO&#x2F;AIAã€Solar Orbiter&#x2F;STIXã€GONG HÎ±ã€Aditya-L1&#x2F;SoLEXSå’ŒGOESçš„ååŒæ—¶ç©ºè§‚æµ‹æ•°æ®ã€‚æˆ‘ä»¬ä»AIA 1600ã€1700Ã…å’ŒGONG HÎ±è·å¾—äº†è€€æ–‘åŒºçš„å…‰å˜æ›²çº¿ï¼Œå¹¶ä¸æ¥è‡ªGOESå’ŒSoLEXS SXRä»¥åŠSTIX HXRçš„ç›˜é›†æˆå…‰å˜æ›²çº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨SUITçš„NB01ã€NB03ã€NB04å’ŒNB08æ»¤é•œä¸­ï¼Œè€€æ–‘å³°å€¼ä¸HXRã€1600å’Œ1700Ã…åŒæ—¶å‡ºç°ï¼Œå¹¶ä¼´éšç€ç”±SoLEXSè·å¾—çš„å³°å€¼æ¸©åº¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨NB02å’ŒNB05ä¸­ï¼Œè€€æ–‘å³°å€¼æ¯”HXRå³°å€¼æ™šäº†çº¦2åˆ†é’Ÿï¼Œè€Œåœ¨NB06å’ŒNB07ä¸­ï¼Œè€€æ–‘å³°å€¼åˆ™æ¯”GOESè½¯Xå°„çº¿å³°å€¼æ™šäº†çº¦3åˆ†é’Ÿã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ï¼ˆé™¤NB03ã€NB04å’ŒNB05å¤–ï¼‰é¦–æ¬¡åœ¨è¿™äº›æ³¢é•¿ä¸‹è§‚å¯Ÿåˆ°çš„è€€æ–‘ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨NB02ä¸­å‘ç°äº†æ˜äº®çš„å†…æ ¸ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†SUITåœ¨è€€æ–‘ç ”ç©¶ä¸­çš„è§‚æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19795v1">PDF</a> Accepted for publication in ApJL, 9 pages, 5 figures</p>
<p><strong>Summary</strong><br>     å¤ªé˜³è€€æ–‘åœ¨æç´«å¤–æ³¢æ®µï¼ˆEUVï¼‰ã€è½¯Xå°„çº¿ï¼ˆSXRï¼‰å’Œç¡¬Xå°„çº¿ï¼ˆHXRï¼‰ä¸­ç»å¸¸è§‚æµ‹åˆ°ï¼Œä½†åœ¨è¿‘ç´«å¤–å’Œä¸­ç´«å¤–è§‚æµ‹ç¨€å°‘ã€‚åˆ©ç”¨å¤ªé˜³ç´«å¤–æˆåƒæœ›è¿œé•œï¼ˆSUITï¼‰çš„è§‚å¯Ÿï¼Œå¯åœ¨ç‰¹å®šçš„å…«ä¸ªçª„å¸¦æ»¤æ³¢å™¨ä¸­å¯¹ç‰¹å®šè€€æ–‘è¿›è¡Œæ£€æµ‹åˆ†æã€‚ä¸å…¶ä»–è®¾å¤‡çš„ååŒè§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œè€€æ–‘çš„ç‰¹æ€§åœ¨ç‰¹å®šçš„æ»¤å…‰å™¨ä¸­ä¸ç‰¹å®šå…‰æ³¢æ®µä¹‹é—´å­˜åœ¨æŸç§ä¸€è‡´æ€§æˆ–ç‰¹å®šå…³è”ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨è¿™äº›æ³¢é•¿ï¼ˆé™¤NB03ã€NB04å’ŒNB05å¤–ï¼‰è§‚æµ‹åˆ°çš„è€€æ–‘ï¼Œå±•ç¤ºäº†SUITè§‚æµ‹åœ¨è€€æ–‘ç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤ªé˜³è€€æ–‘åœ¨ä¸åŒæ³¢é•¿ä¸‹çš„è§‚æµ‹ç»“æœæœ‰æ‰€ä¸åŒï¼Œå°¤å…¶æ˜¯è¿‘ç´«å¤–å’Œä¸­ç´«å¤–æ³¢æ®µç›¸å¯¹è¾ƒå°‘è¢«è§‚å¯Ÿåˆ°ã€‚</li>
<li>Aditya-L1ä¸Šçš„Solar Ultraviolet Imaging Telescopeï¼ˆSUITï¼‰èƒ½å¤Ÿåœ¨ç‰¹å®šçš„å…«ä¸ªçª„å¸¦æ»¤æ³¢å™¨ä¸­å¯¹å¤ªé˜³è€€æ–‘è¿›è¡Œè¯¦ç»†çš„è§‚å¯Ÿåˆ†æã€‚</li>
<li>åœ¨ç‰¹å®šæ»¤å…‰å™¨ä¸‹çš„è€€æ–‘å³°å€¼ä¸ç‰¹å®šå…‰æ³¢æ®µä¹‹é—´å­˜åœ¨åŒæ­¥ç°è±¡ã€‚ä¾‹å¦‚ï¼ŒæŸäº›æ»¤é•œæ•æ‰åˆ°çš„è€€æ–‘å³°å€¼ä¸ç¡¬Xå°„çº¿å³°å€¼åŒæ­¥å‡ºç°ã€‚</li>
<li>åœ¨æŸäº›æ»¤é•œä¸­ï¼Œå¦‚NB02å’ŒNB05ï¼Œå¤ªé˜³è€€æ–‘å³°å€¼æ¯”ç¡¬Xå°„çº¿å³°å€¼å»¶è¿Ÿçº¦2åˆ†é’Ÿæˆ–æ›´é•¿çš„æ—¶é—´å‡ºç°ã€‚è¿™ç§å·®å¼‚è¡¨æ˜å¤ªé˜³è€€æ–‘åœ¨ä¸åŒæ³¢é•¿ä¸‹çš„åŠ¨æ€è¡Œä¸ºå…·æœ‰å¤æ‚æ€§ã€‚</li>
<li>SUITè§‚æµ‹ç»“æœæ­ç¤ºäº†é¦–æ¬¡è§‚å¯Ÿåˆ°çš„ç‰¹å®šæ³¢é•¿ä¸‹çš„å¤ªé˜³è€€æ–‘ç‰¹æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æŸäº›çª„å¸¦æ»¤æ³¢å™¨ä¸­ã€‚è¿™æ˜¾ç¤ºäº†SUITåœ¨å¤ªé˜³è€€æ–‘ç ”ç©¶ä¸­çš„ç‹¬ç‰¹ä»·å€¼ã€‚</li>
<li>æœ¬æ¬¡è§‚æµ‹è¿˜é¦–æ¬¡å‘ç°äº†åä¸ºâ€œäº®æ ¸â€çš„ç°è±¡å­˜åœ¨äºç‰¹å®šçš„æ»¤å…‰å™¨ä¸­ï¼Œå¦‚NB02ã€‚è¿™ä¸ºå¤ªé˜³è€€æ–‘çš„è¿›ä¸€æ­¥ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’å’Œç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-236c4b9a4c195b3ca98153d539b98c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a7cbc0ab8d117497c982a6b802b36f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4befe32360c63f4af0768c4eb4d3ddae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99a2abb7f313bb4736cdc43f15319588.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b500bcad3111a5f75a2451dc803db17.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-Based-Approach-for-Automatic-2D-and-3D-MRI-Segmentation-of-Gliomas"><a href="#Deep-Learning-Based-Approach-for-Automatic-2D-and-3D-MRI-Segmentation-of-Gliomas" class="headerlink" title="Deep Learning-Based Approach for Automatic 2D and 3D MRI Segmentation of   Gliomas"></a>Deep Learning-Based Approach for Automatic 2D and 3D MRI Segmentation of   Gliomas</h2><p><strong>Authors:Kiranmayee Janardhan, Christy Bobby T</strong></p>
<p>Brain tumor diagnosis is a challenging task for clinicians in the modern world. Among the major reasons for cancer-related death is the brain tumor. Gliomas, a category of central nervous system (CNS) tumors, encompass diverse subregions. For accurate diagnosis of brain tumors, precise segmentation of brain images and quantitative analysis are required. A fully automatic approach to glioma segmentation is required because the manual segmentation process is laborious, prone to mistakes, as well as time-consuming. Modern techniques for segmenting gliomas are based on fully convolutional neural networks (FCNs), which can either use two-dimensional (2D) or three-dimensional (3D) convolutions. Nevertheless, 3D convolutions suffer from computational costs and memory demand, while 2D convolutions cannot fully utilize the spatial insights of volumetric clinical imaging data. To obtain an optimal solution, it is vital to balance the computational efficiency of 2D convolutions along with the spatial accuracy of 3D convolutions. This balance can potentially be realized by developing an advanced model to overcome these challenges. The 2D and 3D models implemented here are based on UNET architecture, Inception, and ResNet models. The research work has been implemented on the BraTS 2018, 2019, and 2020 datasets. The best performer of all the modelsâ€™ evaluations metrics for proposed methodologies offer superior potential in terms of the effective segmentation of gliomas. The ResNet model has resulted in 98.91% accuracy for 3D segmentation and 99.77 for 2D segmentations. The dice scores for 2D and 3D segmentations are 0.8312 and 0.9888, respectively. This model can be applied to various other medical applications with fine-tuning, thereby aiding clinicians in brain tumor analysis and improving the diagnosis process effectively. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤è¯Šæ–­æ˜¯ç°ä»£ä¸´åºŠåŒ»ç”Ÿé¢ä¸´çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è„‘è‚¿ç˜¤æ˜¯å¯¼è‡´ç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚èƒ¶è´¨æ˜¯ä¸­æ¢ç¥ç»ç³»ç»Ÿè‚¿ç˜¤çš„ç±»åˆ«ä¹‹ä¸€ï¼Œæ¶µç›–å¤šç§äºšåŒºã€‚ä¸ºäº†å‡†ç¡®è¯Šæ–­è„‘è‚¿ç˜¤ï¼Œéœ€è¦å¯¹è„‘å›¾åƒè¿›è¡Œç²¾ç¡®åˆ†å‰²å’Œå®šé‡åˆ†æã€‚ç”±äºæ‰‹åŠ¨åˆ†å‰²è¿‡ç¨‹æ—¢ç¹çåˆå®¹æ˜“å‡ºé”™ä¸”è€—æ—¶ï¼Œå› æ­¤éœ€è¦ä¸€ç§å…¨è‡ªåŠ¨çš„èƒ¶è´¨ç˜¤åˆ†å‰²æ–¹æ³•ã€‚ç°ä»£ç”¨äºåˆ†å‰²èƒ¶è´¨ç˜¤çš„æŠ€æœ¯åŸºäºå…¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆFCNsï¼‰ï¼Œå¯ä»¥ä½¿ç”¨äºŒç»´ï¼ˆ2Dï¼‰æˆ–ä¸‰ç»´ï¼ˆ3Dï¼‰å·ç§¯ã€‚ç„¶è€Œï¼Œä¸‰ç»´å·ç§¯å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œå†…å­˜éœ€æ±‚å¤§çš„é—®é¢˜ï¼Œè€ŒäºŒç»´å·ç§¯åˆ™æ— æ³•å……åˆ†åˆ©ç”¨ä½“ç§¯ä¸´åºŠæˆåƒæ•°æ®çš„ç©ºé—´ä¿¡æ¯ã€‚ä¸ºäº†è·å¾—æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œå¹³è¡¡äºŒç»´å·ç§¯çš„è®¡ç®—æ•ˆç‡å’Œä¸‰ç»´å·ç§¯çš„ç©ºé—´ç²¾åº¦è‡³å…³é‡è¦ã€‚è¿™ç§å¹³è¡¡å¯ä»¥é€šè¿‡å¼€å‘ä¸€ç§å…ˆè¿›çš„æ¨¡å‹æ¥å…‹æœè¿™äº›æŒ‘æˆ˜è€Œå®ç°ã€‚è¿™é‡Œå®ç°çš„äºŒç»´å’Œä¸‰ç»´æ¨¡å‹åŸºäºUNETæ¶æ„ã€Inceptionå’ŒResNetæ¨¡å‹ã€‚ç ”ç©¶å·¥ä½œå·²åœ¨BraTS 2018ã€2019å’Œ2020æ•°æ®é›†ä¸Šå¾—åˆ°å®æ–½ã€‚æ‰€æå‡ºæ–¹æ³•è®ºçš„æ¨¡å‹è¯„ä¼°æŒ‡æ ‡çš„æœ€ä½³è¡¨ç°è€…æä¾›äº†å‡ºè‰²çš„æ½œåŠ›ï¼Œåœ¨æœ‰æ•ˆåˆ†å‰²èƒ¶è´¨ç˜¤æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ResNetæ¨¡å‹åœ¨ä¸‰ç»´åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†98.91ï¼…çš„å‡†ç¡®ç‡ï¼Œåœ¨äºŒç»´åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†99.77ï¼…ã€‚äºŒç»´å’Œä¸‰ç»´åˆ†å‰²çš„diceå¾—åˆ†åˆ†åˆ«ä¸º0.8312å’Œ0.9888ã€‚é€šè¿‡å¾®è°ƒï¼Œæ­¤æ¨¡å‹å¯åº”ç”¨äºå…¶ä»–å„ç§åŒ»å­¦åº”ç”¨ï¼Œä»è€Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œè„‘è‚¿ç˜¤åˆ†æå¹¶æœ‰æ•ˆæ”¹è¿›è¯Šæ–­è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19760v1">PDF</a> 20 pages, 11 figures, journal paper</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡è®¨è®ºäº†è„‘è‚¿ç˜¤è¯Šæ–­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯èƒ¶è´¨ç˜¤çš„ç²¾ç¡®åˆ†å‰²å’Œå®šé‡åˆ†æçš„é‡è¦æ€§ã€‚ç”±äºæ‰‹åŠ¨åˆ†å‰²è¿‡ç¨‹ç¹çã€æ˜“å‡ºé”™ä¸”è€—æ—¶ï¼Œéœ€è¦å…¨è‡ªåŠ¨çš„èƒ¶è´¨ç˜¤åˆ†å‰²æ–¹æ³•ã€‚å½“å‰æŠ€æœ¯åŸºäºå…¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆFCNsï¼‰çš„2Då’Œ3Då·ç§¯ï¼Œä½†3Då·ç§¯è®¡ç®—æˆæœ¬é«˜ã€å†…å­˜éœ€æ±‚å¤§ï¼Œè€Œ2Då·ç§¯ä¸èƒ½å……åˆ†åˆ©ç”¨ä½“ç§¯ä¸´åºŠå½±åƒæ•°æ®çš„ç©ºé—´ä¿¡æ¯ã€‚ç ”ç©¶å¹³è¡¡äº†2Då·ç§¯çš„è®¡ç®—æ•ˆç‡å’Œ3Då·ç§¯çš„ç©ºé—´å‡†ç¡®æ€§ï¼Œåœ¨BraTSæ•°æ®é›†ä¸Šå®æ–½ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„åˆ†å‰²æ½œåŠ›ï¼Œå¯æœ‰æ•ˆè¾…åŠ©ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œè„‘è‚¿ç˜¤åˆ†æå’Œè¯Šæ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘è‚¿ç˜¤è¯Šæ–­åœ¨ç°ä»£åŒ»å­¦ä¸­ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œéœ€è¦ç²¾ç¡®çš„å›¾åƒåˆ†å‰²å’Œå®šé‡åˆ†æã€‚</li>
<li>èƒ¶è´¨ç˜¤æ˜¯ä¸­æ¢ç¥ç»ç³»ç»Ÿè‚¿ç˜¤çš„ä¸€ç§ï¼Œå…¶å‡†ç¡®è¯Šæ–­éœ€è¦å…¨è‡ªåŠ¨çš„åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>å½“å‰æŠ€æœ¯ä½¿ç”¨åŸºäºå…¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆFCNsï¼‰çš„2Då’Œ3Då·ç§¯è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>3Då·ç§¯å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œå†…å­˜éœ€æ±‚å¤§çš„é—®é¢˜ï¼Œè€Œ2Då·ç§¯æ— æ³•å……åˆ†åˆ©ç”¨ä½“ç§¯ä¸´åºŠå½±åƒæ•°æ®çš„ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œç©ºé—´å‡†ç¡®æ€§ï¼Œé€šè¿‡å¼€å‘å…ˆè¿›æ¨¡å‹å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨UNETæ¶æ„ã€Inceptionå’ŒResNetæ¨¡å‹çš„2Då’Œ3Dæ¨¡å‹åœ¨BraTSæ•°æ®é›†ä¸Šå®æ–½ï¼Œè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a93a457b379cb04495321191c49dd3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8397cb8f046e81edc41f8bc0e59817bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d9560ad7ae8242763359c516cdaf206.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Weakly-Supervised-Segmentation-Framework-for-Thyroid-Nodule-Based-on-High-confidence-Labels-and-High-rationality-Losses"><a href="#Weakly-Supervised-Segmentation-Framework-for-Thyroid-Nodule-Based-on-High-confidence-Labels-and-High-rationality-Losses" class="headerlink" title="Weakly Supervised Segmentation Framework for Thyroid Nodule Based on   High-confidence Labels and High-rationality Losses"></a>Weakly Supervised Segmentation Framework for Thyroid Nodule Based on   High-confidence Labels and High-rationality Losses</h2><p><strong>Authors:Jianning Chi, Zelan Li, Geng Lin, MingYang Sun, Xiaosheng Yu</strong></p>
<p>Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bluehenglee/MLI-MSC">https://github.com/bluehenglee/MLI-MSC</a>. </p>
<blockquote>
<p>å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å¸¦æœ‰ç²—æ ‡ç­¾çš„è®­ç»ƒæ•°æ®æœ‰æ•ˆåœ°å¯¹è¶…å£°å›¾åƒä¸­çš„ç”²çŠ¶è…ºç»“èŠ‚è¿›è¡Œæç»˜ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š1ï¼‰éµå¾ªæ‹“æ‰‘å…ˆéªŒçš„ä½ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾ï¼Œå¼•å…¥äº†å¤§é‡çš„æ ‡ç­¾å™ªå£°ï¼›2ï¼‰æŸå¤±å‡½æ•°ç¼ºä¹ç†æ€§ï¼ŒåƒµåŒ–åœ°å°†åˆ†å‰²ç»“æœä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œå¿½ç•¥äº†å…·æœ‰å¤šæ ·æ€§å’Œå¤æ‚å½¢çŠ¶çš„ç»“èŠ‚çš„åˆ¤åˆ«ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ˜ç¡®äº†å¼±ç›‘ç£è¶…å£°å›¾åƒåˆ†å‰²çš„ç›®æ ‡å’Œå‚è€ƒï¼Œæå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé‡‡ç”¨é«˜ç½®ä¿¡åº¦ä¼ªæ ‡ç­¾æ¥è¡¨ç¤ºæ‹“æ‰‘å’Œè§£å‰–ä¿¡æ¯ï¼Œä»¥åŠé«˜ç†æ€§æŸå¤±æ¥æ•æ‰å¤šçº§åˆ«åˆ¤åˆ«ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬èåˆäº†å››ç‚¹æ³¨é‡Šçš„å‡ ä½•å˜æ¢å’ŒMedSAMæ¨¡å‹ç»“æœï¼Œç”Ÿæˆäº†é«˜ç½®ä¿¡åº¦çš„æ¡†ã€å‰æ™¯å’ŒèƒŒæ™¯æ ‡ç­¾ã€‚æˆ‘ä»¬çš„é«˜ç†æ€§å­¦ä¹ ç­–ç•¥åŒ…æ‹¬ï¼š1ï¼‰å¯¹é½æŸå¤±ï¼Œæµ‹é‡åˆ†å‰²ä¸æ¡†æ ‡ç­¾ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œä»¥åŠå‰æ™¯æ ‡ç­¾å†…çš„æ‹“æ‰‘è¿ç»­æ€§ï¼ŒæŒ‡å¯¼ç½‘ç»œæ„ŸçŸ¥ç»“èŠ‚ä½ç½®ï¼›2ï¼‰å¯¹æ¯”æŸå¤±ï¼Œä»æ ‡è®°çš„å‰æ™¯åŒºåŸŸä¸­æå–ç‰¹å¾ï¼ŒåŒæ—¶ä»æ ‡è®°çš„å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸä¸­æ¨æŒ½ç‰¹å¾ï¼ŒæŒ‡å¯¼ç½‘ç»œå­¦ä¹ ç»“èŠ‚å’ŒèƒŒæ™¯ç‰¹å¾åˆ†å¸ƒï¼›3ï¼‰åŸå‹å…³è”æŸå¤±ï¼Œæµ‹é‡é€šè¿‡æ¯”è¾ƒç‰¹å¾ä¸å‰æ™¯å’ŒèƒŒæ™¯åŸå‹å¾—åˆ°çš„å…³è”å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œç²¾ç¡®ä¿®æ­£ä¸ç¡®å®šåŒºåŸŸä»¥å¾—åˆ°å‡†ç¡®çš„ç»“èŠ‚è¾¹ç¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨TN3Kå’ŒDDTIæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bluehenglee/MLI-MSC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bluehenglee/MLI-MSCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19707v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼±ç›‘ç£å­¦ä¹ çš„ç”²çŠ¶è…ºè¶…å£°å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•å­˜åœ¨çš„æ ‡ç­¾å™ªå£°å’Œä½ç†æ€§æŸå¤±å‡½æ•°é—®é¢˜ã€‚é€šè¿‡èåˆå››ç‚¹æ ‡æ³¨çš„å‡ ä½•å˜æ¢å’ŒMedSAMæ¨¡å‹ç»“æœï¼Œç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æ ‡ç­¾ã€‚åŒæ—¶ï¼Œå¼•å…¥é«˜ç†æ€§å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å¯¹é½æŸå¤±ã€å¯¹æ¯”æŸå¤±å’ŒåŸå‹å…³è”æŸå¤±ï¼Œä»¥æ•æ‰å¤šå±‚æ¬¡ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨TN3Kå’ŒDDTIæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£åˆ†å‰²æ–¹æ³•èƒ½é«˜æ•ˆåˆ©ç”¨ç²—æ ‡ç­¾æ•°æ®å¯¹ç”²çŠ¶è…ºç»“èŠ‚è¿›è¡Œè¶…å£°å›¾åƒåˆ†å‰²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ ‡ç­¾å™ªå£°é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºä½ç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾å’Œæ‹“æ‰‘å…ˆéªŒå¯¼è‡´çš„ã€‚</li>
<li>èåˆå››ç‚¹æ ‡æ³¨çš„å‡ ä½•å˜æ¢å’ŒMedSAMæ¨¡å‹ç»“æœç”Ÿæˆé«˜ç½®ä¿¡åº¦æ ‡ç­¾ã€‚</li>
<li>æå‡ºé«˜ç†æ€§å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å¯¹é½æŸå¤±ã€å¯¹æ¯”æŸå¤±å’ŒåŸå‹å…³è”æŸå¤±ï¼Œä»¥æ•æ‰å¤šå±‚æ¬¡ç‰¹å¾å¹¶ä¼˜åŒ–ä¸ç¡®å®šåŒºåŸŸã€‚</li>
<li>å¯¹é½æŸå¤±è¡¡é‡åˆ†å‰²ä¸æ¡†é€‰æ ‡ç­¾çš„ç©ºé—´ä¸€è‡´æ€§åŠå‰æ™¯æ ‡ç­¾çš„æ‹“æ‰‘è¿ç»­æ€§ã€‚</li>
<li>å¯¹æ¯”æŸå¤±æ‹‰è¿‘å‰æ™¯åŒºåŸŸçš„ç‰¹å¾ï¼ŒåŒæ—¶æ¨å¼€å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸçš„ç‰¹å¾ï¼Œä½¿ç½‘ç»œå­¦ä¹ ç»“èŠ‚å’ŒèƒŒæ™¯çš„ç‰¹å¾åˆ†å¸ƒã€‚</li>
<li>åŸå‹å…³è”æŸå¤±è¡¡é‡ç‰¹å¾ä¸å‰èƒŒæ™¯åŸå‹é—´çš„å…³è”ä¸€è‡´æ€§ï¼Œä»¥ä¼˜åŒ–ä¸ç¡®å®šåŒºåŸŸè‡³å‡†ç¡®çš„ç»“èŠ‚è¾¹ç¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19707">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60a5ad180ce894f4c311a7146c12c1f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-173c7af79e235243e20d722f22ea481d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-612d90a03e40a9cac2d7801041174e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f12f3ff3d7c229295b236ce83126707.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diagnosing-COVID-19-Severity-from-Chest-X-Ray-Images-Using-ViT-and-CNN-Architectures"><a href="#Diagnosing-COVID-19-Severity-from-Chest-X-Ray-Images-Using-ViT-and-CNN-Architectures" class="headerlink" title="Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN   Architectures"></a>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN   Architectures</h2><p><strong>Authors:Luis Lara, Lucia Eve Berger, Rajesh Raju</strong></p>
<p>The COVID-19 pandemic strained healthcare resources and prompted discussion about how machine learning can alleviate physician burdens and contribute to diagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but few studies predict the severity of a patientâ€™s condition from CXRs. In this study, we produce a large COVID severity dataset by merging three sources and investigate the efficacy of transfer learning using ImageNet- and CXR-pretrained models and vision transformers (ViTs) in both severity regression and classification tasks. A pretrained DenseNet161 model performed the best on the three class severity prediction problem, reaching 80% accuracy overall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases, respectively. The ViT had the best regression results, with a mean absolute error of 0.5676 compared to radiologist-predicted severity scores. The projectâ€™s source code is publicly available. </p>
<blockquote>
<p>COVID-19å¤§æµè¡Œä½¿åŒ»ç–—èµ„æºæ‰¿å—å·¨å¤§å‹åŠ›ï¼Œå¹¶å¼•å‘äº†å…³äºæœºå™¨å­¦ä¹ å¦‚ä½•å‡è½»åŒ»ç”Ÿè´Ÿæ‹…å¹¶ä¸ºè¯Šæ–­åšå‡ºè´¡çŒ®çš„è®¨è®ºã€‚èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRsï¼‰è¢«ç”¨äºè¯Šæ–­COVID-19ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶ä»CXRsé¢„æµ‹æ‚£è€…ç—…æƒ…çš„ä¸¥é‡ç¨‹åº¦ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆå¹¶ä¸‰ä¸ªæ¥æºæ¥åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„COVIDä¸¥é‡ç¨‹åº¦æ•°æ®é›†ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨ImageNetå’ŒCXRé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¿›è¡Œè¿ç§»å­¦ä¹ åœ¨ä¸¥é‡ç¨‹åº¦å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨ä¸‰çº§ä¸¥é‡ç¨‹åº¦é¢„æµ‹é—®é¢˜ä¸Šï¼Œé¢„è®­ç»ƒçš„DenseNet161æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®ç‡è¾¾åˆ°80%ï¼Œå¯¹è½»åº¦ã€ä¸­åº¦å’Œé‡åº¦ç—…ä¾‹çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º77.3%ã€83.9%å’Œ70%ã€‚ViTåœ¨å›å½’æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿé¢„æµ‹çš„ä¸¥é‡ç¨‹åº¦è¯„åˆ†ç›¸æ¯”ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä¸º0.5676ã€‚è¯¥é¡¹ç›®çš„æºä»£ç å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16622v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–°å† è‚ºç‚ç–«æƒ…ä¸‹åŒ»ç–—èµ„æºç´§å¼ çš„é—®é¢˜ï¼Œä»¥åŠæœºå™¨å­¦ä¹ å¦‚ä½•ç¼“è§£åŒ»ç”Ÿè´Ÿæ‹…å¹¶åŠ©åŠ›è¯Šæ–­ã€‚ç ”ç©¶é€šè¿‡åˆå¹¶ä¸‰ä¸ªæ¥æºçš„æ•°æ®é›†ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹çš„COVID-19ç—…æƒ…ä¸¥é‡ç¨‹åº¦æ•°æ®é›†ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜æ¢ç©¶äº†ä½¿ç”¨ImageNetå’ŒCXRé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¿›è¡Œè¿ç§»å­¦ä¹ çš„æ•ˆæœï¼Œåœ¨ç—…æƒ…ä¸¥é‡ç¨‹åº¦å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚å…¶ä¸­ï¼Œé¢„è®­ç»ƒçš„DenseNet161æ¨¡å‹åœ¨ä¸‰ç±»ç—…æƒ…é¢„æµ‹é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º80%ï¼Œå¯¹è½»åº¦ã€ä¸­åº¦å’Œé‡åº¦ç—…ä¾‹çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º77.3%ã€83.9%å’Œ70%ã€‚è€ŒViTåœ¨å›å½’ä»»åŠ¡ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿé¢„æµ‹çš„ä¸¥é‡ç¨‹åº¦åˆ†æ•°çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º0.5676ã€‚è¯¥é¡¹ç›®çš„æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†æœºå™¨å­¦ä¹ åœ¨ç¼“è§£åŒ»ç”Ÿè´Ÿæ‹…å’ŒåŠ©åŠ›COVID-19è¯Šæ–­æ–¹é¢çš„ä½œç”¨ã€‚</li>
<li>é€šè¿‡åˆå¹¶ä¸‰ä¸ªæ¥æºçš„æ•°æ®é›†ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹çš„COVID-19ç—…æƒ…ä¸¥é‡ç¨‹åº¦æ•°æ®é›†ã€‚</li>
<li>ä½¿ç”¨äº†é¢„è®­ç»ƒçš„DenseNet161æ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚</li>
<li>DenseNet161æ¨¡å‹åœ¨ä¸‰ç±»ç—…æƒ…é¢„æµ‹é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º80%ã€‚</li>
<li>ViTåœ¨å›å½’ä»»åŠ¡ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œä¸æ”¾å°„ç§‘åŒ»ç”Ÿé¢„æµ‹çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º0.5676ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f4084c5588aeaba05e941b2733c359fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-864285972ea183e55ca4b5eb4ff1bfe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee189a4eba4b697ea4e614bce8bc44b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f409dde1cb252694bda231d8c806d4ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67dcf86b3ee347124aa6a19496966b98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f883ca57a32061673d1225346c59395.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SegAnyPET-Universal-Promptable-Segmentation-from-Positron-Emission-Tomography-Images"><a href="#SegAnyPET-Universal-Promptable-Segmentation-from-Positron-Emission-Tomography-Images" class="headerlink" title="SegAnyPET: Universal Promptable Segmentation from Positron Emission   Tomography Images"></a>SegAnyPET: Universal Promptable Segmentation from Positron Emission   Tomography Images</h2><p><strong>Authors:Yichi Zhang, Le Xue, Wenbo Zhang, Lanlan Li, Yuchen Liu, Chen Jiang, Yuan Cheng, Yuan Qi</strong></p>
<p>Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patientâ€™s body, which is essential for quantification of therapy response and monitoring treatment progress. However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities. Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data. Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒåœ¨ç°ä»£åŒ»å­¦è¯Šæ–­ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå®ƒèƒ½å¤Ÿæ­ç¤ºæ‚£è€…ä½“å†…çš„ä»£è°¢è¿‡ç¨‹ï¼Œè¿™å¯¹äºé‡åŒ–æ²»ç–—ååº”å’Œç›‘æµ‹æ²»ç–—è¿›å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºPETå›¾åƒçš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¾¹ç•Œä¸å¤Ÿæ¸…æ™°ï¼Œä¸å…¶ä»–ç»“æ„åŒ»å­¦å›¾åƒç›¸æ¯”ï¼Œå…¶åˆ†å‰²é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ€è¿‘çš„åˆ†å‰²åŸºç¡€æ¨¡å‹çš„å‘å±•è¡¨æ˜ï¼Œå®ƒä»¬åœ¨å„ç§è‡ªç„¶å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰å‡ºè‰²çš„å¤šåŠŸèƒ½æ€§ã€‚å°½ç®¡è¿›è¡Œäº†åŒ»å­¦é€‚åº”æ€§çš„åŠªåŠ›ï¼Œä½†è¿™äº›å·¥ä½œä¸»è¦é›†ä¸­åœ¨å…·æœ‰è¯¦ç»†ç”Ÿç†ç»“æ„ä¿¡æ¯çš„ç»“æ„åŒ»å­¦å›¾åƒä¸Šï¼Œè€Œåœ¨é€‚åº”åˆ†å­PETæˆåƒæ—¶è¡¨ç°å‡ºè¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†å’Œæ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„PETåˆ†å‰²æ•°æ®é›†PETS-5kï¼ŒåŒ…å«5731ä¸ªä¸‰ç»´å…¨èº«PETå›¾åƒå’Œè¶…è¿‡130ä¸‡å¼ äºŒç»´å›¾åƒã€‚åŸºäºå»ºç«‹çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†SegAnyPETï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹PETå›¾åƒçš„ç‰¹å®šæ¨¡æ€çš„3DåŸºç¡€æ¨¡å‹ï¼Œç”¨äºé€šç”¨çš„å³æ—¶åˆ†å‰²ã€‚é’ˆå¯¹PETå›¾åƒæ ‡æ³¨è´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å¸¦æœ‰ä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹çš„äº¤å‰æç¤ºç½®ä¿¡å­¦ä¹ ï¼ˆCPCLï¼‰ç­–ç•¥ï¼Œä»é«˜è´¨é‡æ ‡æ³¨æ•°æ®å’Œä½è´¨é‡å™ªå£°æ ‡æ³¨æ•°æ®ä¸­ç¨³å¥åœ°å­¦ä¹ åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSegAnyPETä»…ä½¿ç”¨ä¸€ä¸ªæˆ–å°‘æ•°æç¤ºç‚¹å°±èƒ½æ­£ç¡®åˆ†å‰²å·²è§å’Œæœªè§çš„ç›®æ ‡ï¼Œè¶…è¶Šäº†æœ€æ–°çš„åŸºç¡€æ¨¡å‹å’Œä»»åŠ¡ç‰¹å®šçš„å…¨ç›‘ç£æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ç”¨äºé€šç”¨çš„åˆ†å‰²ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªç”¨äºPETå›¾åƒçš„åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬ç›¸ä¿¡SegAnyPETå°†æ¨åŠ¨å…¶åœ¨åˆ†å­æˆåƒçš„å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14351v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒåœ¨ç°ä»£åŒ»å­¦è¯Šæ–­ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†PETå›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„PETåˆ†å‰²æ•°æ®é›†PETS-5kï¼Œå¹¶åŸºäºæ­¤å¼€å‘äº†SegAnyPETï¼Œä¸€ä¸ªç”¨äºPETå›¾åƒé€šç”¨åˆ†å‰²çš„æ¨¡æ€ç‰¹å®šä¸‰ç»´åŸºç¡€æ¨¡å‹ã€‚ä¸ºè§£å†³PETå›¾åƒæ ‡æ³¨è´¨é‡ä¸ä¸€çš„é—®é¢˜ï¼Œé‡‡ç”¨è·¨æç¤ºç½®ä¿¡å­¦ä¹ ï¼ˆCPCLï¼‰ç­–ç•¥ï¼Œç»“åˆä¸ç¡®å®šæ€§å¼•å¯¼çš„è‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹ï¼Œä»é«˜è´¨é‡å’Œæœ‰å™ªå£°çš„æ ‡ç­¾æ•°æ®ä¸­ç¨³å¥åœ°å­¦ä¹ åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSegAnyPETèƒ½æ­£ç¡®åˆ†å‰²å·²è§å’Œæœªè§ç›®æ ‡ï¼Œåªç”¨ä¸€ä¸¤ä¸ªæç¤ºç‚¹å³å¯è¶…è¶Šç°æœ‰åŸºç¡€æ¨¡å‹å’Œä»»åŠ¡ç‰¹å®šå…¨ç›‘ç£æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„é€šç”¨åˆ†å‰²èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETæˆåƒåœ¨ç°ä»£åŒ»å­¦è¯Šæ–­ä¸­æ­ç¤ºæ‚£è€…ä½“å†…ä»£è°¢è¿‡ç¨‹ï¼Œå¯¹ç–—æ•ˆè¯„ä¼°å’Œè¿½è¸ªæ²»ç–—è¿›å±•è‡³å…³é‡è¦ã€‚</li>
<li>PETå›¾åƒåˆ†å‰²é¢ä¸´ä½å¯¹æ¯”åº¦å’Œè¾¹ç•Œæ¨¡ç³Šçš„æŒ‘æˆ˜ã€‚</li>
<li>PETS-5kæ•°æ®é›†çš„å»ºç«‹è§£å†³äº†PETå›¾åƒåˆ†å‰²çš„ç“¶é¢ˆé—®é¢˜ï¼Œæ˜¯æœ€å¤§çš„PETåˆ†å‰²æ•°æ®é›†ã€‚</li>
<li>SegAnyPETæ˜¯é¦–ä¸ªé’ˆå¯¹PETå›¾åƒçš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºé€šç”¨åˆ†å‰²ã€‚</li>
<li>CPCLç­–ç•¥è§£å†³äº†PETå›¾åƒæ ‡æ³¨è´¨é‡ä¸ä¸€çš„é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>SegAnyPETèƒ½æ­£ç¡®åˆ†å‰²å·²è§å’Œæœªè§ç›®æ ‡ï¼Œåªç”¨å°‘æ•°æç¤ºç‚¹å³å¯å®ç°é«˜æ€§èƒ½åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad41fa6cbb576ae40ee1cafb95ebb0b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05270e8e4cddc0876e187d4eb2511c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae2d74ff268df4180ff80ef656c65d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a39db8fb6f7a21001ae4a3f509839b07.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation"><a href="#Improved-Baselines-with-Synchronized-Encoding-for-Universal-Medical-Image-Segmentation" class="headerlink" title="Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation"></a>Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation</h2><p><strong>Authors:Sihan Yang, Xuande Mi, Jiadong Feng, Haixia Bi, Hai Zhang, Jian Sun</strong></p>
<p>Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. The code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM">https://github.com/Hhankyangg/SyncSAM</a>. </p>
<blockquote>
<p>å¤§å‹åŸºç¡€æ¨¡å‹ä»¥å…¶å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è€Œè‘—ç§°ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¼€å‘ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„åŸºç¡€æ¨¡å‹æ„æˆäº†ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ã€‚è™½ç„¶åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„å¾®è°ƒæŠ€æœ¯å·²ç»è¢«æ¢ç´¢ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨æ‰©å¤§æ•°æ®è§„æ¨¡æˆ–æ”¹è¿›æ¨ç†ç­–ç•¥ï¼Œè€Œæ²¡æœ‰èå…¥é¢†åŸŸç‰¹å®šçš„æ¶æ„è®¾è®¡ï¼Œä»è€Œé™åˆ¶äº†å…¶é›¶æ ·æœ¬æ€§èƒ½ã€‚ä¸ºäº†ä¼˜åŒ–æ ‡å‡†æ¨ç†è®¾ç½®ä¸‹çš„åˆ†å‰²æ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å¼ºå¤§çš„åŸºçº¿ï¼Œæˆ‘ä»¬å¼•å…¥äº†SyncSAMã€‚SyncSAMé‡‡ç”¨åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨ï¼Œä»¥åŒæ­¥æ–¹å¼é›†æˆå·ç§¯å’ŒTransformerç‰¹å¾ä»¥å¢å¼ºåŒ»å­¦å›¾åƒç¼–ç ï¼Œå¹¶é‡‡ç”¨å¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ä»¥ä¿ç•™å›¾åƒç»†èŠ‚ã€‚SyncSAMåœ¨ä¸¤å¤§åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†SA-Med2D-20Må’ŒIMed-361Mä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆäº†ä¸€ç³»åˆ—ç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncSAMä¸ä»…åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”å¯¹æœªè§æ•°æ®é›†è¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Hhankyangg/SyncSAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hhankyangg/SyncSAMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09886v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨é›¶æ ·æœ¬å­¦ä¹ æ–¹é¢çš„ä¼˜åŠ¿ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡çš„åŒæ­¥åŒåˆ†æ”¯æ¨¡å‹SyncSAMã€‚è¯¥æ¨¡å‹ç»“åˆäº†å·ç§¯å’ŒTransformerç‰¹å¾ï¼Œå¹¶é‡‡ç”¨äº†å¤šå°ºåº¦åŒåˆ†æ”¯è§£ç å™¨ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦å›¾åƒç¼–ç çš„æ•ˆæœå¹¶ä¿ç•™å›¾åƒç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncSAMåœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå¹¶å¯¹æœªè§æ•°æ®é›†å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚å…¶é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨äºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´è‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒé¢†åŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>SyncSAMæ¨¡å‹é€šè¿‡ç»“åˆå·ç§¯å’ŒTransformerç‰¹å¾ï¼Œå¢å¼ºåŒ»å­¦å›¾åƒç¼–ç æ•ˆæœã€‚</li>
<li>SyncSAMé‡‡ç”¨åŒæ­¥åŒåˆ†æ”¯ç¼–ç å™¨ç»“æ„ï¼Œæ—¨åœ¨æé«˜é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬SA-Med2D-20Må’ŒIMed-361Mã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜SyncSAMåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
<li>SyncSAMå¯¹æœªè§æ•°æ®é›†å±•ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d6a0e096c01585fabe10ac34c1432dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65aebbf7985c8e9ccab91e3f77f059ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-775c002212b3f8748cc976c4ab39c75c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-545bee0888f2e398ba83261f44f8d9aa.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Completed-Feature-Disentanglement-Learning-for-Multimodal-MRIs-Analysis"><a href="#Completed-Feature-Disentanglement-Learning-for-Multimodal-MRIs-Analysis" class="headerlink" title="Completed Feature Disentanglement Learning for Multimodal MRIs Analysis"></a>Completed Feature Disentanglement Learning for Multimodal MRIs Analysis</h2><p><strong>Authors:Tianling Liu, Hongying Liu, Fanhua Shang, Lequan Yu, Tong Han, Liang Wan</strong></p>
<p>Multimodal MRIs play a crucial role in clinical diagnosis and treatment. Feature disentanglement (FD)-based methods, aiming at learning superior feature representations for multimodal data analysis, have achieved significant success in multimodal learning (MML). Typically, existing FD-based methods separate multimodal data into modality-shared and modality-specific features, and employ concatenation or attention mechanisms to integrate these features. However, our preliminary experiments indicate that these methods could lead to a loss of shared information among subsets of modalities when the inputs contain more than two modalities, and such information is critical for prediction accuracy. Furthermore, these methods do not adequately interpret the relationships between the decoupled features at the fusion stage. To address these limitations, we propose a novel Complete Feature Disentanglement (CFD) strategy that recovers the lost information during feature decoupling. Specifically, the CFD strategy not only identifies modality-shared and modality-specific features, but also decouples shared features among subsets of multimodal inputs, termed as modality-partial-shared features. We further introduce a new Dynamic Mixture-of-Experts Fusion (DMF) module that dynamically integrates these decoupled features, by explicitly learning the local-global relationships among the features. The effectiveness of our approach is validated through classification tasks on three multimodal MRI datasets. Extensive experimental results demonstrate that our approach outperforms other state-of-the-art MML methods with obvious margins, showcasing its superior performance. </p>
<blockquote>
<p>å¤šæ¨¡æ€MRIåœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç‰¹å¾è§£æ„ï¼ˆFDï¼‰æ–¹æ³•æ—¨åœ¨ä¸ºå¤šæ¨¡æ€æ•°æ®åˆ†æå­¦ä¹ ä¼˜è´¨ç‰¹å¾è¡¨ç¤ºï¼Œå·²åœ¨å¤šæ¨¡æ€å­¦ä¹ ï¼ˆMMLï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚é€šå¸¸ï¼Œç°æœ‰çš„FDæ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®åˆ†ä¸ºæ¨¡æ€å…±äº«å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ï¼Œå¹¶ä½¿ç”¨æ‹¼æ¥æˆ–æ³¨æ„åŠ›æœºåˆ¶æ¥æ•´åˆè¿™äº›ç‰¹å¾ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œå½“è¾“å…¥åŒ…å«è¶…è¿‡ä¸¤ç§æ¨¡æ€æ—¶ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä¼šå¯¼è‡´æ¨¡æ€å­é›†ä¹‹é—´å…±äº«ä¿¡æ¯çš„ä¸¢å¤±ï¼Œä¸”æ­¤ç±»ä¿¡æ¯å¯¹é¢„æµ‹ç²¾åº¦è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨èåˆé˜¶æ®µæ²¡æœ‰å……åˆ†è§£é‡Šè§£è€¦ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç‰¹å¾è§£æ„ç­–ç•¥ï¼Œå³å®Œå…¨ç‰¹å¾è§£æ„ï¼ˆCFDï¼‰ï¼Œè¯¥ç­–ç•¥æ¢å¤äº†ç‰¹å¾è§£æ„è¿‡ç¨‹ä¸­ä¸¢å¤±çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒCFDç­–ç•¥ä¸ä»…è¯†åˆ«æ¨¡æ€å…±äº«å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ï¼Œè¿˜è§£è€¦å¤šæ¨¡æ€è¾“å…¥å­é›†ä¹‹é—´çš„å…±äº«ç‰¹å¾ï¼Œç§°ä¸ºæ¨¡æ€éƒ¨åˆ†å…±äº«ç‰¹å¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŠ¨æ€æ··åˆä¸“å®¶èåˆï¼ˆDMFï¼‰æ¨¡å—ï¼Œå®ƒé€šè¿‡æ˜¾å¼å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„å±€éƒ¨å…¨å±€å…³ç³»æ¥åŠ¨æ€æ•´åˆè¿™äº›è§£è€¦çš„ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šçš„åˆ†ç±»ä»»åŠ¡è¿›è¡Œäº†éªŒè¯ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–å…ˆè¿›MMLæ–¹æ³•ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼Œå±•ç¤ºäº†å…¶å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04916v2">PDF</a> Accept by IEEE JBHI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€MRIåœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­çš„é‡è¦æ€§ï¼Œä»¥åŠç‰¹å¾åˆ†è§£ï¼ˆFDï¼‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å­¦ä¹ ï¼ˆMMLï¼‰ä¸­çš„åº”ç”¨ã€‚ç°æœ‰FDæ–¹æ³•å°†å¤šæ¨¡æ€æ•°æ®åˆ†ä¸ºå…±äº«å’Œç‰¹å®šç‰¹å¾ï¼Œå¹¶é€šè¿‡æ‹¼æ¥æˆ–æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œé›†æˆã€‚ç„¶è€Œï¼Œå½“è¾“å…¥åŒ…å«è¶…è¿‡ä¸¤ç§æ¨¡æ€æ—¶ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä¼šä¸¢å¤±éƒ¨åˆ†æ¨¡æ€é—´çš„å…±äº«ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å®Œæ•´ç‰¹å¾åˆ†è§£ï¼ˆCFDï¼‰ç­–ç•¥ï¼Œèƒ½å¤Ÿæ¢å¤åœ¨ç‰¹å¾åˆ†è§£è¿‡ç¨‹ä¸­ä¸¢å¤±çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†åŠ¨æ€æ··åˆä¸“å®¶èåˆï¼ˆDMFï¼‰æ¨¡å—ï¼Œèƒ½å¤ŸåŠ¨æ€åœ°èåˆè¿™äº›åˆ†è§£åçš„ç‰¹å¾ï¼Œå¹¶æ˜¾å¼åœ°å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„å±€éƒ¨-å…¨å±€å…³ç³»ã€‚é€šè¿‡åœ¨ä¸‰ç»„å¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šçš„åˆ†ç±»ä»»åŠ¡éªŒè¯ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„MMLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€MRIåœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>ç‰¹å¾åˆ†è§£ï¼ˆFDï¼‰æ–¹æ³•åœ¨å¤šæ¨¡æ€å­¦ä¹ ï¼ˆMMLï¼‰ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>ç°æœ‰FDæ–¹æ³•åœ¨å¤„ç†è¶…è¿‡ä¸¤ç§æ¨¡æ€çš„è¾“å…¥æ—¶ï¼Œå¯èƒ½ä¼šä¸¢å¤±éƒ¨åˆ†æ¨¡æ€é—´çš„å…±äº«ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„å®Œæ•´ç‰¹å¾åˆ†è§£ï¼ˆCFDï¼‰ç­–ç•¥èƒ½å¤Ÿæ¢å¤åœ¨ç‰¹å¾åˆ†è§£è¿‡ç¨‹ä¸­ä¸¢å¤±çš„ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†åŠ¨æ€æ··åˆä¸“å®¶èåˆï¼ˆDMFï¼‰æ¨¡å—ï¼Œèƒ½åŠ¨æ€èåˆç‰¹å¾å¹¶å­¦ä¹ ç‰¹å¾é—´çš„å±€éƒ¨-å…¨å±€å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¤šæ¨¡æ€MRIæ•°æ®é›†ä¸Šçš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29ee53abf33ddca27259a236b2b1dac5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a44b0c96bf377932fb97937ecdff5bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94bbd773fb71a4b6b7835435b28ffc7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb3c96708e9aca8c6e09a3bee6fcbca8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d0073d0c1ac71b0b3f8b0bbdf24f0209.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  Telephone Surveys Meet Conversational AI Evaluating a LLM-Based   Telephone Survey System at Scale
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-01/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5738f829f99e5430b6a8a602598990fa.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-01  FlexVAR Flexible Visual Autoregressive Modeling without Residual   Prediction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
