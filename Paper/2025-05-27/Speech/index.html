<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Daily-Omni Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b1b41946b7c8ba9d8f0e60d82315dd88.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-27-æ›´æ–°"><a href="#2025-05-27-æ›´æ–°" class="headerlink" title="2025-05-27 æ›´æ–°"></a>2025-05-27 æ›´æ–°</h1><h2 id="Daily-Omni-Towards-Audio-Visual-Reasoning-with-Temporal-Alignment-across-Modalities"><a href="#Daily-Omni-Towards-Audio-Visual-Reasoning-with-Temporal-Alignment-across-Modalities" class="headerlink" title="Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities"></a>Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities</h2><p><strong>Authors:Ziwei Zhou, Rui Wang, Zuxuan Wu</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) achieve promising performance on visual and audio benchmarks independently. However, the ability of these models to process cross-modal information synchronously remains largely unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual Questioning and Answering benchmark comprising 684 videos of daily life scenarios from diverse sources, rich in both audio and visual information, and featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA Generation Pipeline, which includes automatic annotation, QA generation and QA optimization, significantly improves efficiency for human evaluation and scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM) and Automatic Speech Recognition (ASR) model to establish a baseline for this benchmark. The results show that current MLLMs still struggle significantly with tasks requiring audio-visual integration, but combining VLMs and ALMs with simple temporal alignment techniques can achieve substantially better performance. Codes and benchmark are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Lliar-liar/Daily-Omni%7D%7Bhttps://github.com/Lliar-liar/Daily-Omni%7D">https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å’ŒéŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸Šç‹¬ç«‹åœ°å–å¾—äº†æœ‰å‰æ™¯çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¤„ç†è·¨æ¨¡æ€ä¿¡æ¯åŒæ­¥çš„èƒ½åŠ›ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä»¥ä¸‹å†…å®¹ï¼š1ï¼‰Daily-Omniï¼Œä¸€ä¸ªåŒ…å«éŸ³é¢‘è§†è§‰é—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒç”±æ¥è‡ªä¸åŒæºçš„684ä¸ªæ—¥å¸¸ç”Ÿæ´»åœºæ™¯çš„è§†é¢‘ç»„æˆï¼Œè¿™äº›è§†é¢‘å¯Œå«éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œæ¶µç›–äº†6å¤§ç±»ä»»åŠ¡çš„1197ä¸ªå¤šé¡¹é€‰æ‹©é—®ç­”å¯¹ï¼›2ï¼‰Daily-Omnié—®ç­”ç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ ‡æ³¨ã€é—®ç­”ç”Ÿæˆå’Œé—®ç­”ä¼˜åŒ–ï¼Œå®ƒæå¤§åœ°æé«˜äº†äººç±»è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„å¯æ‰©å±•æ€§çš„æ•ˆç‡ï¼›3ï¼‰æ— éœ€è®­ç»ƒçš„Daily-Omni-Agentï¼Œå®ƒåˆ©ç”¨å¼€æºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ¥å»ºç«‹è¿™ä¸ªåŸºå‡†æµ‹è¯•çš„åŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦è§†å¬æ•´åˆçš„ä»»åŠ¡æ—¶ä»å­˜åœ¨è¾ƒå¤§çš„å›°éš¾ï¼Œä½†é€šè¿‡ç»“åˆVLMå’ŒALMä»¥åŠç®€å•çš„æ—¶åºå¯¹é½æŠ€æœ¯ï¼Œå¯ä»¥å–å¾—æ˜¾è‘—æ›´å¥½çš„æ€§èƒ½ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lliar-liar/Daily-Omni%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Lliar-liar/Daily-Omniå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†å¬ä¿¡æ¯çš„é—®ç­”åŸºå‡†æµ‹è¯•â€”â€”Daily-Omniã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å¯Œå«è§†å¬ä¿¡æ¯çš„684ä¸ªæ—¥å¸¸ç”Ÿæ´»åœºæ™¯è§†é¢‘ï¼Œæ¶µç›–6é¡¹ä¸»è¦ä»»åŠ¡ï¼Œå¹¶æä¾›äº†è‡ªåŠ¨æ ‡æ³¨ã€é—®ç­”ç”Ÿæˆå’Œé—®ç­”ä¼˜åŒ–çš„ç”Ÿæˆç®¡é“ï¼Œæé«˜äº†äººç±»è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•çš„æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ— éœ€è®­ç»ƒçš„Daily-Omni-Agentï¼Œå®ƒé€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ã€éŸ³é¢‘è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œä¸ºåŸºå‡†æµ‹è¯•å»ºç«‹äº†åŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦è§†å¬æ•´åˆçš„ä»»åŠ¡ä¸Šä»æœ‰æ˜¾è‘—æŒ‘æˆ˜ï¼Œä½†é€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’ŒéŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„æ—¶åºå¯¹é½æŠ€æœ¯ï¼Œå¯ä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Daily-Omniæ˜¯ä¸€ä¸ªè§†å¬é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸°å¯Œçš„æ—¥å¸¸ç”Ÿæ´»åœºæ™¯è§†é¢‘å’Œå¤šå…ƒé€‰æ‹©é—®ç­”å¯¹ã€‚</li>
<li>Daily-Omniå¼•å…¥äº†é«˜æ•ˆçš„QAç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ ‡æ³¨ã€é—®ç­”ç”Ÿæˆå’Œé—®ç­”ä¼˜åŒ–ã€‚</li>
<li>Daily-Omni-Agentæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¨¡å‹ï¼Œç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ã€éŸ³é¢‘è¯­è¨€æ¨¡å‹å’Œè¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†å¬æ•´åˆä»»åŠ¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’ŒéŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„æ—¶åºå¯¹é½æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡æä¾›äº†ç›¸å…³ä»£ç å’ŒåŸºå‡†æµ‹è¯•é“¾æ¥ï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3513e45d61069adc591c2800ff13baec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e330e0c1b74c3d4e7d35ef6d2f4671cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a941fe17a6ee916562be5b4c44d375c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee355df71c7ca27c26ef0d7714919a85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9607e179f91bc443c06972277ff84f8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9ef3c70b3c003059f206d9d938daef6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-752ce21461af1bf25e166324c83be1f6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training"><a href="#CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training" class="headerlink" title="CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training"></a>CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training</h2><p><strong>Authors:Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye</strong></p>
<p>In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3">https://funaudiollm.github.io/cosyvoice3</a>. </p>
<blockquote>
<p>åœ¨æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾å¯æ‰©å±•çš„æµå¼è¯­éŸ³åˆæˆæ¨¡å‹CosyVoice 2ï¼Œå®ƒé›†æˆäº†ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºç‰‡æ®µæ„ŸçŸ¥çš„æµåŒ¹é…ï¼ˆFMï¼‰æ¨¡å‹ï¼Œå®ç°äº†ä½å»¶è¿Ÿçš„åŒå‘æµå¼è¯­éŸ³åˆæˆå’Œæ¥è¿‘çœŸäººæ°´å¹³çš„è¯­éŸ³è´¨é‡ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†åœ¨è¯­è¨€è¦†ç›–ã€é¢†åŸŸå¤šæ ·æ€§ã€æ•°æ®é‡ã€æ–‡æœ¬æ ¼å¼å’Œè®­ç»ƒåæŠ€æœ¯æ–¹é¢ï¼ŒCosyVoice 2ä»å­˜åœ¨ä¸€å®šå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CosyVoice 3ï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹é‡å¤–é›¶æ ·æœ¬å¤šè¯­ç§è¯­éŸ³åˆæˆçš„æ”¹è¿›æ¨¡å‹ï¼Œåœ¨å†…å®¹ä¸€è‡´æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè¯­è°ƒè‡ªç„¶åº¦æ–¹é¢è¶…è¶Šäº†å…¶å‰èº«ã€‚CosyVoice 3çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š1ï¼‰ä¸€ç§æ–°å‹è¯­éŸ³æ ‡è®°å™¨ï¼Œé€šè¿‡ç›‘ç£å¤šä»»åŠ¡è®­ç»ƒå¼€å‘ï¼Œæ—¨åœ¨æé«˜è¯­è°ƒçš„è‡ªç„¶åº¦ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€è¯­è¨€è¯†åˆ«ã€éŸ³é¢‘äº‹ä»¶æ£€æµ‹å’Œè¯´è¯äººåˆ†æã€‚2ï¼‰ä¸€ç§æ–°çš„å¯å¾®å¥–åŠ±æ¨¡å‹ï¼Œé€‚ç”¨äºè®­ç»ƒåçš„åœºæ™¯ï¼Œä¸ä»…é€‚ç”¨äºCosyVoice 3ï¼Œä¹Ÿé€‚ç”¨äºå…¶ä»–åŸºäºLLMçš„è¯­éŸ³åˆæˆæ¨¡å‹ã€‚3ï¼‰æ•°æ®é›†å¤§å°æ‰©å±•ï¼šè®­ç»ƒæ•°æ®ä»ä¸€ä¸‡å°æ—¶æ‰©å±•åˆ°ä¸€ç™¾ä¸‡å°æ—¶ï¼Œæ¶µç›–9ç§è¯­è¨€å’Œ18ç§ä¸­æ–‡æ–¹è¨€ï¼Œæ¶‰åŠå„ç§é¢†åŸŸå’Œæ–‡æœ¬æ ¼å¼ã€‚4ï¼‰æ¨¡å‹å¤§å°æ‰©å±•ï¼šæ¨¡å‹å‚æ•°ä»0.5äº¿å¢åŠ åˆ°1.5äº¿ï¼Œç”±äºæ¨¡å‹å®¹é‡æ›´å¤§ï¼Œæˆ‘ä»¬çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•æ€§èƒ½æœ‰æ‰€æé«˜ã€‚è¿™äº›è¿›æ­¥ä¸ºé‡å¤–è¯­éŸ³åˆæˆçš„è¿›å±•åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æˆ‘ä»¬é¼“åŠ±è¯»è€…åœ¨<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3%E4%B8%8A%E8%AF%95%E5%90%AC%E6%BC%94%E7%A4%BA%E3%80%82">https://funaudiollm.github.io/cosyvoice3ä¸Šè¯•å¬æ¼”ç¤ºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17589v1">PDF</a> Preprint, work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CosyVoice 3æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯å¯¹å…ˆå‰å·¥ä½œçš„æ”¹è¿›ï¼Œç”¨äºé›¶æ ·æœ¬å¤šè¯­ç§é‡å¤–è¯­éŸ³åˆæˆã€‚ç›¸è¾ƒäºå‰ä»£äº§å“ï¼ŒCosyVoice 3åœ¨å†…å®¹ä¸€è‡´æ€§ã€å‘éŸ³äººç›¸ä¼¼æ€§å’Œè¯­è°ƒè‡ªç„¶åº¦ä¸Šæœ‰æ‰€è¶…è¶Šã€‚å…³é”®ç‰¹æ€§åŒ…æ‹¬ï¼š1ï¼‰é€šè¿‡ç›‘ç£å¤šä»»åŠ¡è®­ç»ƒå¼€å‘çš„æ–°å‹è¯­éŸ³æ ‡è®°å™¨ï¼Œä»¥æé«˜è¯­è°ƒçš„è‡ªç„¶æ€§ï¼›2ï¼‰é€‚ç”¨äºCosyVoice 3å’Œå…¶ä»–åŸºäºLLMçš„è¯­éŸ³åˆæˆæ¨¡å‹çš„æ–°å‹å¯å¾®å¥–åŠ±æ¨¡å‹ï¼›3ï¼‰æ•°æ®é›†å¤§å°æ‰©å±•ï¼šè®­ç»ƒæ•°æ®ä»ä¸€ä¸‡å°æ—¶æ‰©å±•åˆ°ä¸€ç™¾ä¸‡å°æ—¶ï¼Œæ¶µç›–9ç§è¯­è¨€å’Œ18ç§ä¸­æ–‡æ–¹è¨€ï¼Œæ¶‰åŠå„ç§é¢†åŸŸå’Œæ–‡æœ¬æ ¼å¼ï¼›4ï¼‰æ¨¡å‹å¤§å°æ‰©å±•ï¼šæ¨¡å‹å‚æ•°ä»0.5äº¿å¢åŠ åˆ°1.5äº¿ï¼Œç”±äºæ¨¡å‹å®¹é‡æ›´å¤§ï¼Œæˆ‘ä»¬çš„å¤šè¯­ç§åŸºå‡†æµ‹è¯•æ€§èƒ½å¢å¼ºã€‚è¿™äº›è¿›å±•å¯¹é‡å¤–è¯­éŸ³åˆæˆçš„è¿›æ­¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CosyVoice 3æ¨¡å‹æ˜¯å¯¹CosyVoice 2çš„æ”¹è¿›ï¼Œç”¨äºé›¶æ ·æœ¬å¤šè¯­ç§é‡å¤–è¯­éŸ³åˆæˆã€‚</li>
<li>CosyVoice 3åœ¨å†…å®¹ä¸€è‡´æ€§ã€å‘éŸ³äººç›¸ä¼¼æ€§å’Œè¯­è°ƒè‡ªç„¶åº¦ä¸Šæœ‰æ‰€è¶…è¶Šã€‚</li>
<li>æ–°å‹è¯­éŸ³æ ‡è®°å™¨é€šè¿‡ç›‘ç£å¤šä»»åŠ¡è®­ç»ƒæé«˜è¯­è°ƒè‡ªç„¶æ€§ã€‚</li>
<li>å¼•å…¥æ–°å‹å¯å¾®å¥–åŠ±æ¨¡å‹ï¼Œé€‚ç”¨äºCosyVoice 3å’Œå…¶ä»–LLMè¯­éŸ³åˆæˆæ¨¡å‹çš„åæœŸè®­ç»ƒã€‚</li>
<li>è®­ç»ƒæ•°æ®é›†ä»åä¸‡å°æ—¶æ‰©å±•åˆ°ç™¾ä¸‡å°æ—¶ï¼Œæ¶µç›–å¤šç§è¯­è¨€å’Œä¸­æ–‡æ–¹è¨€ï¼Œä»¥åŠå¤šç§é¢†åŸŸå’Œæ–‡æœ¬æ ¼å¼ã€‚</li>
<li>æ¨¡å‹å‚æ•°ä»0.5äº¿å¢åŠ åˆ°1.5äº¿ï¼Œæå‡äº†å¤šè¯­ç§æ€§èƒ½ã€‚</li>
<li>é¼“åŠ±å¬ä¼—é€šè¿‡ç½‘ç«™é“¾æ¥ä½“éªŒDemoã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-02dfe4c632c55f1ba90b2947f7d34d30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a151dcf188a216bc252c52f1eeb9500f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-becc3688ac33167ec55c47a327468b56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c3b320b2d2bf7d701dc185ecaa1938c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reverse-Speech-Finder-A-Neural-Network-Backtracking-Architecture-for-Generating-Alzheimerâ€™s-Disease-Speech-Samples-and-Improving-Diagnosis-Performance"><a href="#Reverse-Speech-Finder-A-Neural-Network-Backtracking-Architecture-for-Generating-Alzheimerâ€™s-Disease-Speech-Samples-and-Improving-Diagnosis-Performance" class="headerlink" title="Reverse-Speech-Finder: A Neural Network Backtracking Architecture for   Generating Alzheimerâ€™s Disease Speech Samples and Improving Diagnosis   Performance"></a>Reverse-Speech-Finder: A Neural Network Backtracking Architecture for   Generating Alzheimerâ€™s Disease Speech Samples and Improving Diagnosis   Performance</h2><p><strong>Authors:Victor OK Li, Yang Han, Jacqueline CK Lam, Lawrence YL Cheung</strong></p>
<p>This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimerâ€™s Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSFâ€™s unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSFâ€™s superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSFâ€™s potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†Reverse-Speech-Finderï¼ˆRSFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§çªç ´æ€§çš„ç¥ç»ç½‘ç»œå›æº¯æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡è¯­éŸ³åˆ†ææé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è¯Šæ–­èƒ½åŠ›ã€‚RSFåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¨åŠ›ï¼Œè¯†åˆ«å’Œåˆ©ç”¨æœ€å¯èƒ½çš„ADç‰¹å¼‚æ€§è¯­éŸ³æ ‡è®°ï¼Œè§£å†³çœŸå®ADè¯­éŸ³æ ·æœ¬ç¨€ç¼ºå’Œç°æœ‰æ¨¡å‹è§£é‡Šæ€§æœ‰é™çš„æŒ‘æˆ˜ã€‚RSFçš„ç‹¬ç‰¹æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ï¼šé¦–å…ˆï¼Œå®ƒåˆ©ç”¨äº†ä¸€ä¸ªè§‚å¯Ÿç»“æœï¼Œå³æœ€å¯èƒ½é¢„æµ‹ADçš„è¯­éŸ³æ ‡è®°ï¼ˆå®šä¹‰ä¸ºMPMï¼‰ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­å¿…é¡»æ¿€æ´»é‚£äº›æœ€å¯èƒ½é¢„æµ‹ADçš„ç¥ç»å…ƒï¼ˆå®šä¹‰ä¸ºMPNï¼‰ã€‚å…¶æ¬¡ï¼Œå®ƒåœ¨è¾“å…¥å±‚ä½¿ç”¨è¯­éŸ³æ ‡è®°è¡¨ç¤ºï¼Œå…è®¸ä»MPNå›æº¯æ¥è¯†åˆ«ADçš„æœ€å¯èƒ½è¯­éŸ³æ ‡è®°ï¼ˆMPTsï¼‰ã€‚æœ€åï¼Œå®ƒå¼€å‘äº†ä¸€ç§åˆ›æ–°çš„å›æº¯æ–¹æ³•ï¼Œä»MPNå›æº¯åˆ°è¾“å…¥å±‚ï¼Œè¯†åˆ«MPTså’Œç›¸åº”çš„MPMsï¼Œå¹¶å·§å¦™åœ°å‘ç°äº†ç”¨äºæ£€æµ‹ADçš„æ–°å‹è¯­éŸ³æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRSFä¼˜äºSHAPå’Œé›†æˆæ¢¯åº¦ç­‰ä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨å‡†ç¡®ç‡ä¸Šæé«˜äº†3.5%ï¼ŒF1åˆ†æ•°æé«˜äº†3.2%ã€‚é€šè¿‡ç”ŸæˆåŒ…å«æ–°å‹æ ‡è®°çš„è¯­éŸ³æ•°æ®ï¼ŒRSFä¸ä»…ç¼“è§£äº†çœŸå®æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼Œè¿˜æ˜¾è‘—æé«˜äº†ADè¯Šæ–­æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†RSFåœ¨åŸºäºè¯­éŸ³çš„ADæ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œä¸ºADç›¸å…³çš„è¯­è¨€ç¼ºé™·æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºæ›´æœ‰æ•ˆçš„éä¾µå…¥æ€§æ—©æœŸå¹²é¢„ç­–ç•¥é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17477v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºä¸€ç§åä¸ºReverse-Speech-Finderï¼ˆRSFï¼‰çš„çªç ´æ€§ç¥ç»ç½‘ç»œå›æº¯æ¶æ„ï¼Œé€šè¿‡è¯­éŸ³åˆ†ææé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚RSFåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½ï¼Œè¯†åˆ«å’Œåˆ©ç”¨æœ€å¯èƒ½çš„ADç‰¹å¼‚æ€§è¯­éŸ³æ ‡è®°ç‰©ï¼Œè§£å†³çœŸå®ADè¯­éŸ³æ ·æœ¬ç¼ºä¹å’Œç°æœ‰æ¨¡å‹è§£é‡Šæ€§æœ‰é™çš„é—®é¢˜ã€‚RSFçš„ç‹¬ç‰¹æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šé¦–å…ˆï¼Œå®ƒåˆ©ç”¨æœ€å¯èƒ½çš„è¯­éŸ³æ ‡è®°ç‰©ï¼ˆMPMsï¼‰å¿…é¡»æ¿€æ´»é‚£äº›æœ€å¯èƒ½é¢„æµ‹ADçš„ç¥ç»å…ƒï¼ˆåœ¨ç¥ç»ç½‘ç»œä¸­å®šä¹‰ä¸ºæœ€å¯èƒ½çš„ç¥ç»å…ƒMPNsï¼‰çš„è§‚ç‚¹ã€‚å…¶æ¬¡ï¼Œå®ƒåœ¨è¾“å…¥å±‚ä½¿ç”¨è¯­éŸ³æ ‡è®°è¡¨ç¤ºï¼Œå…è®¸ä»MPNså›æº¯ä»¥è¯†åˆ«ADçš„æœ€å¯èƒ½çš„è¯­éŸ³æ ‡è®°ç¬¦ï¼ˆMPTsï¼‰ã€‚æœ€åï¼Œå®ƒå¼€å‘äº†ä¸€ç§åˆ›æ–°çš„å›æº¯æ–¹æ³•ï¼Œä»MPNså›æº¯åˆ°è¾“å…¥å±‚ï¼Œè¯†åˆ«MPTså’Œç›¸åº”çš„MPMsï¼Œå¹¶å·§å¦™åœ°æ­ç¤ºç”¨äºADæ£€æµ‹çš„æ–°è¯­éŸ³æ ‡è®°ç‰©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRSFä¼˜äºä¼ ç»Ÿæ–¹æ³•å¦‚SHAPå’Œé›†æˆæ¢¯åº¦æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†3.5%ï¼ŒF1å¾—åˆ†æé«˜äº†3.2%ã€‚é€šè¿‡ç”ŸæˆåŒ…å«æ–°å‹æ ‡è®°çš„è¯­éŸ³æ•°æ®ï¼ŒRSFä¸ä»…ç¼“è§£äº†çœŸå®æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼Œè€Œä¸”å¤§å¤§æé«˜äº†ADè¯Šæ–­æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†RSFåœ¨åŸºäºè¯­éŸ³çš„ADæ£€æµ‹ä¸­çš„æ½œåŠ›ï¼Œä¸ºADç›¸å…³çš„è¯­è¨€ç¼ºé™·æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºæ›´æœ‰æ•ˆçš„éä¾µå…¥æ€§æ—©æœŸå¹²é¢„ç­–ç•¥é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RSFæ˜¯ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„å›æº¯æ¶æ„ï¼Œæ—¨åœ¨é€šè¿‡è¯­éŸ³åˆ†ææé«˜é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è¯Šæ–­ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«å’Œåˆ©ç”¨æœ€å¯èƒ½çš„ADç‰¹å¼‚æ€§è¯­éŸ³æ ‡è®°ç‰©ã€‚</li>
<li>RSFçš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼šåˆ©ç”¨æœ€å¯èƒ½çš„è¯­éŸ³æ ‡è®°ç‰©å’Œç¥ç»å…ƒä¹‹é—´çš„å…³ç³»ï¼Œä½¿ç”¨è¯­éŸ³æ ‡è®°è¡¨ç¤ºåœ¨è¾“å…¥å±‚ï¼Œä»¥åŠå¼€å‘ä¸€ç§åˆ›æ–°çš„å›æº¯æ–¹æ³•æ¥è¯†åˆ«ADçš„æ–°å‹è¯­éŸ³æ ‡è®°ç‰©ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜RSFåœ¨ADæ£€æµ‹æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå‡†ç¡®ç‡å’ŒF1å¾—åˆ†å‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>RSFä¸ä»…ç¼“è§£äº†çœŸå®æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œè¿˜æé«˜äº†ADè¯Šæ–­æ¨¡å‹çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>RSFçš„æ½œåŠ›åœ¨äºä¸ºADç›¸å…³çš„è¯­è¨€ç¼ºé™·æä¾›æ–°çš„è§è§£ï¼Œå¹¶å¯èƒ½ä¸ºæ›´æœ‰æ•ˆçš„éä¾µå…¥æ€§æ—©æœŸå¹²é¢„ç­–ç•¥æä¾›æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6cd5c413307a58eef1cf40fbd7be74f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0911577ca3f5264635626d062f3badc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models"><a href="#Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models" class="headerlink" title="Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models"></a>Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models</h2><p><strong>Authors:Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi</strong></p>
<p>The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed&#x2F;variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding. </p>
<blockquote>
<p>è¯­éŸ³åˆ‡è¯çš„ç›®çš„æ˜¯å°†è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºä¸€ç³»åˆ—ç¦»æ•£è¡¨ç¤ºï¼Œä½œä¸ºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„åŸºç¡€ã€‚è™½ç„¶è¯­éŸ³åˆ‡è¯æœ‰å¾ˆå¤šé€‰æ‹©ï¼Œä½†å®ƒä»¬å¯¹SLMæ€§èƒ½çš„å½±å“ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡ç ”ç©¶äº†è¯­éŸ³åˆ‡è¯çš„ä¸¤ä¸ªæ–¹é¢ï¼šåˆ†æ®µå®½åº¦å’Œç¦»æ•£å•å…ƒç°‡çš„å¤§å°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è¯­éŸ³ä¿¡å·åˆ†æ®µä¸ºå›ºå®š&#x2F;å¯å˜å®½åº¦å’Œæ±‡æ€»è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªç°‡å¤§å°ä¸Šè®­ç»ƒK-meansæ¨¡å‹ã€‚é€šè¿‡å¯¹é›¶æ ·æœ¬å£è¯­ç†è§£åŸºå‡†æµ‹è¯•è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°é€‚åº¦ç²—ç³™çš„åˆ†æ®µå’Œè¾ƒå¤§çš„ç°‡å¤§å°äº§ç”Ÿäº†ç§¯æçš„å½±å“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¸­ï¼Œæœ€æœ‰æ•ˆçš„æ–¹æ³•å®ç°äº†è®­ç»ƒæ•°æ®å‡å°‘50%ï¼Œè®­ç»ƒè¿è¡Œæ—¶é—´å‡å°‘70%ã€‚æˆ‘ä»¬çš„åˆ†æå¼ºè°ƒäº†ç»“åˆå¤šä¸ªä»¤ç‰Œä»¥å¢å¼ºç²¾ç»†ç²’åº¦å£è¯­ç†è§£çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17446v1">PDF</a> Accepted to Interspeech2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯­éŸ³æ ‡è®°åŒ–çš„ç›®çš„æ˜¯å°†è¯­éŸ³ä¿¡å·è½¬åŒ–ä¸ºä¸€ç³»åˆ—ç¦»æ•£è¡¨ç¤ºï¼Œä½œä¸ºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„åŸºç¡€ã€‚æœ¬æ–‡ç ”ç©¶äº†è¯­éŸ³æ ‡è®°åŒ–çš„ä¸¤ä¸ªå…³é”®æ–¹é¢ï¼šåˆ†æ®µå®½åº¦å’Œç¦»æ•£å•å…ƒç°‡çš„å¤§å°ã€‚å®éªŒé€šè¿‡å›ºå®šæˆ–å¯å˜å®½åº¦çš„åˆ†æ®µä»¥åŠæ± åŒ–è¡¨ç¤ºï¼Œè®­ç»ƒäº†ä¸åŒç°‡å¤§å°çš„K-meansæ¨¡å‹ã€‚åœ¨é›¶æ ·æœ¬å£è¯­ç†è§£åŸºå‡†æµ‹è¯•ä¸Šï¼Œæœ¬æ–‡å‘ç°é€‚åº¦ç²—ç³™çš„åˆ†æ®µå’Œè¾ƒå¤§çš„ç°‡å¤§å°å…·æœ‰ç§¯æå½±å“ã€‚æœ€é«˜æ•ˆçš„æ¨¡å‹ç”šè‡³å®ç°äº†è®­ç»ƒæ•°æ®å‡å°‘50%ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘70%ã€‚åˆ†æè¡¨æ˜ï¼Œç»“åˆå¤šä¸ªæ ‡è®°æœ‰åŠ©äºæé«˜ç²¾ç»†å£è¯­çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯­éŸ³æ ‡è®°åŒ–æ˜¯å°†è¯­éŸ³ä¿¡å·è½¬åŒ–ä¸ºç¦»æ•£è¡¨ç¤ºåºåˆ—çš„è¿‡ç¨‹ï¼Œæ˜¯è¯­éŸ³è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚</li>
<li>è¯­éŸ³æ ‡è®°åŒ–çš„ä¸¤ä¸ªå…³é”®æ–¹é¢ä¸ºåˆ†æ®µå®½åº¦å’Œç¦»æ•£å•å…ƒç°‡çš„å¤§å°ã€‚</li>
<li>é€šè¿‡å›ºå®šæˆ–å¯å˜å®½åº¦çš„åˆ†æ®µä»¥åŠæ± åŒ–è¡¨ç¤ºï¼Œè®­ç»ƒäº†K-meansæ¨¡å‹ã€‚</li>
<li>é€‚åº¦ç²—ç³™çš„åˆ†æ®µå’Œè¾ƒå¤§çš„ç°‡å¤§å°å¯¹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æœ‰ç§¯æå½±å“ã€‚</li>
<li>æœ€é«˜æ•ˆçš„æ¨¡å‹å®ç°äº†è®­ç»ƒæ•°æ®å’Œæ—¶é—´çš„æ˜¾è‘—å‡å°‘ã€‚</li>
<li>ç»“åˆå¤šä¸ªæ ‡è®°æœ‰åŠ©äºæé«˜ç²¾ç»†å£è¯­ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33921283ba0600e37ddc490ea666cc71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84ab9efa94e18637e17728fb9d61f995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb2c99cf1566ee39243d28fbf980bee3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd3572a3f66ee384e13d863ccdc96bd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975bc3cccba45a17f24ae9ec37c77142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c76bc3715c57c5168db5014fe616f533.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniTTS-An-end-to-end-TTS-system-without-decoupling-of-acoustic-and-semantic-information"><a href="#UniTTS-An-end-to-end-TTS-system-without-decoupling-of-acoustic-and-semantic-information" class="headerlink" title="UniTTS: An end-to-end TTS system without decoupling of acoustic and   semantic information"></a>UniTTS: An end-to-end TTS system without decoupling of acoustic and   semantic information</h2><p><strong>Authors:Rui Wang, Qianguo Sun, Tianrong Chen, Zhiyun Zeng, Junlong Wu, Jiaxing Zhang</strong></p>
<p>The emergence of multi-codebook neutral audio codecs such as Residual Vector Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These codecs are crucial in separating semantic and acoustic information while efficiently harnessing semantic priors. However, since semantic and acoustic information cannot be fully aligned, a significant drawback of these methods when applied to LLM-based TTS is that large language models may have limited access to comprehensive audio information. To address this limitation, we propose DistilCodec and UniTTS, which collectively offer the following advantages: 1) This method can distill a multi-codebook audio codec into a single-codebook audio codec with 32,768 codes while achieving a near 100% utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a large amount of high-quality unlabeled audio (such as audiobooks with sound effects, songs, etc.) can be incorporated during training, further expanding data diversity and broadening its applicability. 3) Leveraging the comprehensive audio information modeling of DistilCodec, we integrated three key tasks into UniTTSâ€™s pre-training framework: audio modality autoregression, text modality autoregression, and speech-text cross-modal autoregression. This allows UniTTS to accept interleaved text and speech&#x2F;audio prompts while substantially preserving LLMâ€™s text capabilities. 4) UniTTS employs a three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and Alignment. Source code and model checkpoints are publicly available at <a target="_blank" rel="noopener" href="https://github.com/IDEA-Emdoor-Lab/UniTTS">https://github.com/IDEA-Emdoor-Lab/UniTTS</a> and <a target="_blank" rel="noopener" href="https://github.com/IDEA-Emdoor-Lab/DistilCodec">https://github.com/IDEA-Emdoor-Lab/DistilCodec</a>. </p>
<blockquote>
<p>éšç€å¤šç æœ¬ä¸­æ€§éŸ³é¢‘ç¼–è§£ç å™¨ï¼ˆå¦‚æ®‹å·®çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰å’Œç»„çŸ¢é‡é‡åŒ–ï¼ˆGVQï¼‰ï¼‰çš„å‡ºç°ï¼Œæ˜¾è‘—æ¨åŠ¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å‘å±•ã€‚è¿™äº›ç¼–è§£ç å™¨åœ¨åˆ†ç¦»è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨è¯­ä¹‰å…ˆéªŒçŸ¥è¯†æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºè¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯æ— æ³•å®Œå…¨å¯¹é½ï¼Œè¿™äº›æ–¹æ³•åº”ç”¨äºåŸºäºLLMçš„TTSæ—¶çš„ä¸€ä¸ªé‡å¤§ç¼ºç‚¹æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½æ— æ³•è·å¾—å…¨é¢çš„éŸ³é¢‘ä¿¡æ¯ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DistilCodecå’ŒUniTTSï¼Œå®ƒä»¬å…±åŒå…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š1ï¼‰æ­¤æ–¹æ³•å¯ä»¥å°†å¤šç æœ¬éŸ³é¢‘ç¼–è§£ç å™¨è’¸é¦ä¸ºå•ç æœ¬éŸ³é¢‘ç¼–è§£ç å™¨ï¼ŒåŒæ—¶æ‹¥æœ‰32768ä¸ªç æœ¬ï¼Œä¸”åˆ©ç”¨ç‡æ¥è¿‘100%ã€‚2ï¼‰ç”±äºDistilCodecæ²¡æœ‰é‡‡ç”¨è¯­ä¹‰å¯¹é½æ–¹æ¡ˆï¼Œå› æ­¤å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥å¤§é‡é«˜è´¨é‡çš„æ— æ ‡ç­¾éŸ³é¢‘ï¼ˆå¦‚å¸¦æœ‰éŸ³æ•ˆçš„æœ‰å£°è¯»ç‰©ã€æ­Œæ›²ç­‰ï¼‰ï¼Œè¿›ä¸€æ­¥æ‰©å¤§äº†æ•°æ®å¤šæ ·æ€§å’Œæ‹“å®½äº†å…¶é€‚ç”¨æ€§ã€‚3ï¼‰åˆ©ç”¨DistilCodecçš„å…¨é¢éŸ³é¢‘ä¿¡æ¯å»ºæ¨¡ï¼Œæˆ‘ä»¬å°†ä¸‰ä¸ªå…³é”®ä»»åŠ¡æ•´åˆåˆ°UniTTSçš„é¢„è®­ç»ƒæ¡†æ¶ä¸­ï¼šéŸ³é¢‘æ¨¡æ€è‡ªå›å½’ã€æ–‡æœ¬æ¨¡æ€è‡ªå›å½’å’Œè¯­éŸ³-æ–‡æœ¬è·¨æ¨¡æ€è‡ªå›å½’ã€‚è¿™å…è®¸UniTTSåœ¨æ¥å—äº¤ç»‡çš„æ–‡æœ¬å’Œè¯­éŸ³&#x2F;éŸ³é¢‘æç¤ºæ—¶ï¼ŒåŒæ—¶ä¿ç•™LLMçš„æ–‡æœ¬åŠŸèƒ½ã€‚4ï¼‰UniTTSé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯¹é½ã€‚æºä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IDEA-Emdoor-Lab/UniTTS%E5%92%8Chttps://github.com/IDEA-Emdoor-Lab/DistilCodec%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IDEA-Emdoor-Lab/UniTTSå’Œhttps://github.com/IDEA-Emdoor-Lab/DistilCodecä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17426v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šä»£ç æœ¬ä¸­ç«‹éŸ³é¢‘ç¼–ç å™¨çš„å‡ºç°ï¼Œå¦‚æ®‹å·®çŸ¢é‡é‡åŒ–ï¼ˆRVQï¼‰å’Œç»„çŸ¢é‡é‡åŒ–ï¼ˆGVQï¼‰ï¼Œå·²ç»æ˜¾è‘—æ¨è¿›äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨åº”ç”¨äºLLM-based TTSæ—¶å­˜åœ¨è¯­ä¹‰å’ŒéŸ³é¢‘ä¿¡æ¯æ— æ³•å®Œå…¨å¯¹é½çš„å±€é™æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†DistilCodecå’ŒUniTTSï¼Œå®ƒä»¬èƒ½å¤Ÿï¼šå°†å¤šä»£ç æœ¬éŸ³é¢‘ç¼–ç å™¨è½¬åŒ–ä¸ºå•ä»£ç æœ¬éŸ³é¢‘ç¼–ç å™¨ï¼Œå®ç°è¿‘100%çš„åˆ©ç”¨ç‡ï¼›åˆ©ç”¨æ— éœ€è¯­ä¹‰å¯¹é½çš„æ–¹æ³•ï¼Œçº³å…¥å¤§é‡é«˜è´¨é‡æœªæ ‡æ³¨éŸ³é¢‘ï¼Œæé«˜æ•°æ®å¤šæ ·æ€§å’Œé€‚ç”¨æ€§ï¼›æ•´åˆä¸‰ä¸ªå…³é”®ä»»åŠ¡åˆ°UniTTSçš„é¢„è®­ç»ƒæ¡†æ¶ä¸­ï¼Œæ¥å—äº¤æ›¿çš„æ–‡æœ¬å’Œè¯­éŸ³&#x2F;éŸ³é¢‘æç¤ºï¼ŒåŒæ—¶ä¿ç•™LLMçš„æ–‡æœ¬èƒ½åŠ›ï¼›é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»£ç æœ¬ä¸­ç«‹éŸ³é¢‘ç¼–ç å™¨çš„å‡ºç°å·²ç»æ˜¾è‘—æ¨åŠ¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>è¯­ä¹‰å’ŒéŸ³é¢‘ä¿¡æ¯çš„ä¸å®Œå…¨å¯¹é½æ˜¯å½“å‰æ–¹æ³•çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>DistilCodecæ–¹æ³•èƒ½å¤Ÿå°†å¤šä»£ç æœ¬éŸ³é¢‘ç¼–ç å™¨è½¬åŒ–ä¸ºå•ä»£ç æœ¬éŸ³é¢‘ç¼–ç å™¨ï¼Œå¹¶å®ç°è¿‘100%çš„åˆ©ç”¨ç‡ã€‚</li>
<li>ç”±äºä¸éœ€è¦è¯­ä¹‰å¯¹é½ï¼ŒDistilCodecå¯ä»¥çº³å…¥å¤§é‡æœªæ ‡æ³¨çš„éŸ³é¢‘æ•°æ®ã€‚</li>
<li>UniTTSæ•´åˆäº†ä¸‰ä¸ªå…³é”®ä»»åŠ¡åˆ°å…¶é¢„è®­ç»ƒæ¡†æ¶ä¸­ï¼Œä»¥æé«˜ç³»ç»Ÿçš„åŠŸèƒ½æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>UniTTSé‡‡ç”¨äº†åŒ…æ‹¬é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¯¹é½åœ¨å†…çš„ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æºä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-74a08a27af31c110607e809849b629be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e3664303fa1156eb3670bca354977c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95eddf18366a0471250fb1383abb3fb9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-based-Generative-Error-Correction-for-Rare-Words-with-Synthetic-Data-and-Phonetic-Context"><a href="#LLM-based-Generative-Error-Correction-for-Rare-Words-with-Synthetic-Data-and-Phonetic-Context" class="headerlink" title="LLM-based Generative Error Correction for Rare Words with Synthetic Data   and Phonetic Context"></a>LLM-based Generative Error Correction for Rare Words with Synthetic Data   and Phonetic Context</h2><p><strong>Authors:Natsuo Yamashita, Masaaki Yamamoto, Hiroaki Kokubo, Yohei Kawaguchi</strong></p>
<p>Generative error correction (GER) with large language models (LLMs) has emerged as an effective post-processing approach to improve automatic speech recognition (ASR) performance. However, it often struggles with rare or domain-specific words due to limited training data. Furthermore, existing LLM-based GER approaches primarily rely on textual information, neglecting phonetic cues, which leads to over-correction. To address these issues, we propose a novel LLM-based GER approach that targets rare words and incorporates phonetic information. First, we generate synthetic data to contain rare words for fine-tuning the GER model. Second, we integrate ASRâ€™s N-best hypotheses along with phonetic context to mitigate over-correction. Experimental results show that our method not only improves the correction of rare words but also reduces the WER and CER across both English and Japanese datasets. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼é”™è¯¯æ ¡æ­£ï¼ˆGERï¼‰å·²æˆä¸ºæé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½çš„æœ‰æ•ˆåå¤„ç†çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®çš„æœ‰é™ï¼Œå®ƒé€šå¸¸åœ¨å¤„ç†ç¨€æœ‰è¯æˆ–ç‰¹å®šé¢†åŸŸçš„è¯æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºäºLLMçš„GERæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œå¿½ç•¥äº†è¯­éŸ³çº¿ç´¢ï¼Œä»è€Œå¯¼è‡´è¿‡åº¦æ ¡æ­£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºLLMçš„GERæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é’ˆå¯¹ç¨€æœ‰è¯å¹¶èå…¥äº†è¯­éŸ³ä¿¡æ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆåˆæˆæ•°æ®ï¼Œå…¶ä¸­åŒ…å«ç¨€æœ‰è¯ï¼Œä»¥å¾®è°ƒGERæ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç»“åˆASRçš„N-bestå‡è®¾å’Œè¯­éŸ³ç¯å¢ƒï¼Œä»¥å‡è½»è¿‡åº¦æ ¡æ­£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†ç¨€æœ‰è¯çš„æ ¡æ­£ç‡ï¼Œè€Œä¸”é™ä½äº†è‹±è¯­å’Œæ—¥è¯­æ•°æ®é›†çš„è¯é”™è¯¯ç‡å’Œå­—ç¬¦é”™è¯¯ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17410v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong><br>     åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼é”™è¯¯æ›´æ­£ï¼ˆGERï¼‰æ˜¯æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½çš„æœ‰æ•ˆåå¤„ç†æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒå› è®­ç»ƒæ•°æ®æœ‰é™è€Œéš¾ä»¥åº”å¯¹ç¨€æœ‰è¯æˆ–ç‰¹å®šé¢†åŸŸçš„è¯æ±‡ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„LLM-based GERæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œå¿½ç•¥äº†è¯­éŸ³çº¿ç´¢ï¼Œå¯¼è‡´è¿‡åº¦ä¿®æ­£ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹ç¨€æœ‰è¯å¹¶èå…¥è¯­éŸ³ä¿¡æ¯çš„æ–°å‹LLM-based GERæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŒ…å«ç¨€æœ‰è¯ï¼Œå¯¹GERæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç»“åˆASRçš„N-bestå‡è®¾å’Œè¯­éŸ³ç¯å¢ƒæ¥å‡è½»è¿‡åº¦ä¿®æ­£çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†ç¨€æœ‰è¯çš„ä¿®æ­£æ•ˆæœï¼Œè¿˜é™ä½äº†è‹±è¯­å’Œæ—¥è¯­æ•°æ®é›†çš„è¯é”™è¯¯ç‡å’Œå­—ç¬¦é”™è¯¯ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼é”™è¯¯æ›´æ­£ï¼ˆGERï¼‰æ˜¯æå‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½çš„æœ‰æ•ˆåå¤„ç†æ–¹æ³•ã€‚</li>
<li>LLM-based GERæ–¹æ³•é¢ä¸´ç¨€æœ‰è¯å’Œç‰¹å®šé¢†åŸŸè¯æ±‡çš„è¯†åˆ«æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰LLM-based GERæ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬ä¿¡æ¯ï¼Œå¯¼è‡´è¿‡åº¦ä¿®æ­£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹LLM-based GERæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é’ˆå¯¹ç¨€æœ‰è¯å¹¶èå…¥è¯­éŸ³ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®å¯¹GERæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åŒ…å«ç¨€æœ‰è¯ã€‚</li>
<li>ç»“åˆASRçš„N-bestå‡è®¾å’Œè¯­éŸ³ç¯å¢ƒæ¥å‡è½»è¿‡åº¦ä¿®æ­£é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d4807dcb973889b035fea4cc9523850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def4198604d04f20aa606ed2360b8e8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ee4377977be173b207538284bf63f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6e882b8cfdcc9c75c9d147af46bd6f9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language"><a href="#An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language" class="headerlink" title="An End-to-End Approach for Child Reading Assessment in the Xhosa   Language"></a>An End-to-End Approach for Child Reading Assessment in the Xhosa   Language</h2><p><strong>Authors:Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar</strong></p>
<p>Child literacy is a strong predictor of life outcomes at the subsequent stages of an individualâ€™s life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of childrenâ€™s voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained. </p>
<blockquote>
<p>å„¿ç«¥è¯†å­—èƒ½åŠ›æ˜¯ä¸ªäººåç»­ç”Ÿå‘½é˜¶æ®µç”Ÿæ´»æˆæœçš„é‡è¦é¢„æµ‹æŒ‡æ ‡ã€‚è¿™æŒ‡å‡ºäº†é’ˆå¯¹è„†å¼±çš„ä¸­ä½æ”¶å…¥ç¾¤ä½“è¿›è¡Œé’ˆå¯¹æ€§å¹²é¢„çš„å¿…è¦æ€§ï¼Œä»¥å¸®åŠ©ç¼©å°è¿™äº›åœ°åŒºè¯†å­—æ°´å¹³ä¸é«˜æ”¶å…¥åœ°åŒºä¹‹é—´çš„å·®è·ã€‚åœ¨æ­¤ç±»å·¥ä½œä¸­ï¼Œé˜…è¯»è¯„ä¼°æ˜¯è¡¡é‡è¿™äº›é¡¹ç›®æ•ˆæœçš„é‡è¦å·¥å…·ï¼Œäººå·¥æ™ºèƒ½å¯ä»¥ä½œä¸ºæ”¯æŒæ•™è‚²å·¥ä½œè€…å®Œæˆè¿™é¡¹ä»»åŠ¡çš„å¯é ä¸”ç»æµçš„å·¥å…·ã€‚å¯¹äºä½èµ„æºè¯­è¨€ä¸­çš„å„¿ç«¥è¯­éŸ³ï¼Œå¼€å‘å‡†ç¡®çš„è‡ªåŠ¨é˜…è¯»è¯„ä¼°ç³»ç»Ÿé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ•°æ®æœ‰é™ä¸”å„¿ç«¥çš„å£°éŸ³å…·æœ‰ç‹¬ç‰¹çš„å£°å­¦ç‰¹æ€§ã€‚æœ¬ç ”ç©¶ä»¥å—éä½¿ç”¨çš„è¯­è¨€ä¹‹ä¸€â€”â€”ç§‘è¨è¯­ä¸ºä¾‹ï¼Œæ—¨åœ¨æé«˜å„¿ç«¥è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªç”±å„¿ç«¥ç§‘è¨è¯­è¯­éŸ³æ ·æœ¬ç»„æˆçš„æ–°æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å¯åœ¨æ”¶åˆ°è¯·æ±‚åè·å¾—ï¼ŒåŒ…å«åä¸ªå•è¯å’Œå­—æ¯ï¼Œè¿™äº›å•è¯å’Œå­—æ¯æ˜¯æ—©æœŸé˜…è¯»èƒ½åŠ›è¯„ä¼°ï¼ˆEGRAï¼‰ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ã€‚æ¯ä¸ªå½•éŸ³éƒ½é€šè¿‡å¤šä¸ªæ ‡è®°è€…é‡‡ç”¨åœ¨çº¿å’Œä½æˆæœ¬çš„æ–¹å¼è¿›è¡Œæ ‡è®°ï¼Œéƒ¨åˆ†å½•éŸ³è¿˜ç»è¿‡ç‹¬ç«‹çš„EGRAå®¡æ ¸äººå‘˜è¿›è¡ŒéªŒè¯ã€‚è¯¥æ•°æ®é›†ç»è¿‡ä¸‰ç§æœ€æ–°ç«¯åˆ°ç«¯æ¨¡å‹çš„å¾®è°ƒåè¿›è¡Œäº†è¯„ä¼°ï¼šwav2vec 2.0ã€HuBERTå’ŒWhisperã€‚ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹çš„æ€§èƒ½å¯èƒ½ä¼šå—åˆ°å¯ç”¨è®­ç»ƒæ•°æ®çš„æ•°é‡å’Œå¹³è¡¡æ€§çš„æ˜¾è‘—å½±å“ï¼Œè¿™å¯¹äºä½æˆæœ¬å¤§è§„æ¨¡æ•°æ®é›†çš„æ”¶é›†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å¯ç”¨æ ·æœ¬æ•°é‡å—é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸€æ¬¡æ€§è®­ç»ƒå¤šä¸ªç±»åˆ«ä¹Ÿå¯ä»¥æé«˜wav2vec 2.0çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17371v1">PDF</a> Paper accepted on AIED 2025 containing 14 pages, 6 figures and 4   tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨å„¿ç«¥é˜…è¯»èƒ½åŠ›å¯¹å…¶æœªæ¥ç”Ÿæ´»é˜¶æ®µçš„å½±å“ï¼Œå¹¶æŒ‡å‡ºå¯¹ä½æ”¶å…¥å’Œä¸­æ”¶å…¥ç¾¤ä½“çš„è„†å¼±äººç¾¤è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æ˜¯å¿…è¦çš„ã€‚é˜…è¯»è¯„ä¼°æ˜¯è¯„ä¼°è¿™äº›é¡¹ç›®æ•ˆæœçš„é‡è¦å·¥å…·ï¼Œè€Œäººå·¥æ™ºèƒ½å¯ä»¥å¯é ä¸”ç»æµåœ°æ”¯æŒæ•™è‚²è€…å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚æœ¬ç ”ç©¶å…³æ³¨å—éä½¿ç”¨çš„è¯­è¨€â€”â€”ç§‘è¨è¯­ï¼Œä»¥æ¨è¿›å„¿ç«¥è¯­éŸ³è¯†åˆ«èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†ä¸€ä»½æ–°å‹æ•°æ®é›†ï¼ŒåŒ…å«ç§‘è¨è¯­å„¿ç«¥è¯­éŸ³æ ·æœ¬ã€‚è¯¥æ•°æ®é›†ç”±åœ¨çº¿å’Œæˆæœ¬æ•ˆç›Šæ–¹æ³•è¿›è¡Œæ ‡æ³¨ï¼Œéƒ¨åˆ†æ•°æ®ç»è¿‡ç‹¬ç«‹æ—©æœŸé˜…è¯»èƒ½åŠ›è¯„ä¼°ç³»ç»Ÿå®¡æ ¸å‘˜éªŒè¯ã€‚ä½¿ç”¨wav2vec 2.0ã€HuBERTå’ŒWhisperä¸‰ç§å…ˆè¿›ç«¯å¯¹ç«¯æ¨¡å‹è¿›è¡Œè¯„ä¼°çš„ç»“æœè¡¨æ˜ï¼Œå¯ç”¨è®­ç»ƒæ•°æ®çš„æ•°é‡å’Œå¹³è¡¡åº¦ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œè¿™å¯¹äºä½æˆæœ¬å¤§è§„æ¨¡æ•°æ®é›†çš„æ”¶é›†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå®éªŒæ˜¾ç¤ºï¼Œå³ä½¿åœ¨å¯ç”¨æ ·æœ¬æ•°é‡æœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åŒæ—¶è®­ç»ƒå¤šä¸ªç±»åˆ«ä¹Ÿèƒ½æé«˜wav2vec 2.0çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å„¿ç«¥é˜…è¯»èƒ½åŠ›æ˜¯é¢„æµ‹å…¶æœªæ¥ç”Ÿæ´»é˜¶æ®µçš„é‡è¦å› ç´ ï¼Œéœ€è¦å…³æ³¨å¯¹ä½æ”¶å…¥å’Œä¸­æ”¶å…¥è„†å¼±ç¾¤ä½“çš„æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½ã€‚</li>
<li>é˜…è¯»è¯„ä¼°æ˜¯è¯„ä¼°é˜…è¯»æ¨å¹¿é¡¹ç›®æ•ˆæœçš„é‡è¦å·¥å…·ï¼Œäººå·¥æ™ºèƒ½å¯ä»¥æ”¯æŒæ•™è‚²è€…è¿›è¡Œè¿™é¡¹ä»»åŠ¡ã€‚</li>
<li>æœ¬ç ”ç©¶å…³æ³¨ç§‘è¨è¯­çš„å„¿ç«¥è¯­éŸ³è¯†åˆ«èƒ½åŠ›å‘å±•ï¼Œæ¨å‡ºæ–°å‹æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡åœ¨çº¿å’Œæˆæœ¬æ•ˆç›Šæ–¹æ³•è¿›è¡Œæ ‡æ³¨ï¼Œéƒ¨åˆ†æ•°æ®ç»è¿‡ç‹¬ç«‹å®¡æ ¸å‘˜éªŒè¯ã€‚</li>
<li>wav2vec 2.0ã€HuBERTå’ŒWhisperä¸‰ç§æ¨¡å‹è¢«ç”¨äºè¯„ä¼°å„¿ç«¥è¯­éŸ³æ ·æœ¬ã€‚</li>
<li>å¯ç”¨è®­ç»ƒæ•°æ®çš„æ•°é‡å’Œå¹³è¡¡åº¦å¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå¼ºè°ƒä½æˆæœ¬å¤§è§„æ¨¡æ•°æ®é›†æ”¶é›†çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8311e68f96845ff0c0bde5f77af2463.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4e33aa3709d3ae11ca3f52047076849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798f6a03b9ca85b10edafe60ab63b3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b41946b7c8ba9d8f0e60d82315dd88.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation"><a href="#Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation" class="headerlink" title="Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation"></a>Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation</h2><p><strong>Authors:Ofir Yaish, Yehuda Mishaly, Eliya Nachmani</strong></p>
<p>We introduce a new paradigm for active sound modification: Active Speech Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on suppressing external interference, ASE goes further by actively shaping the speech signal â€“ both attenuating unwanted noise components and amplifying speech-relevant frequencies â€“ to improve intelligibility and perceptual quality. To enable this, we propose a novel Transformer-Mamba-based architecture, along with a task-specific loss function designed to jointly optimize interference suppression and signal enrichment. Our method outperforms existing baselines across multiple speech processing tasks â€“ including denoising, dereverberation, and declipping â€“ demonstrating the effectiveness of active, targeted modulation in challenging acoustic environments. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä¸»åŠ¨å£°éŸ³ä¿®æ”¹çš„æ–°èŒƒå¼ï¼šä¸»åŠ¨è¯­éŸ³å¢å¼ºï¼ˆASEï¼‰ã€‚è€Œä¸»åŠ¨é™å™ªï¼ˆANCï¼‰ç®—æ³•ä¸»è¦ä¾§é‡äºæŠ‘åˆ¶å¤–éƒ¨å¹²æ‰°ï¼ŒASEæ›´è¿›ä¸€æ­¥åœ°é€šè¿‡ä¸»åŠ¨å¡‘é€ è¯­éŸ³ä¿¡å·æ¥æ”¹å–„è¯­éŸ³çš„æ¸…æ™°åº¦å’Œæ„ŸçŸ¥è´¨é‡ï¼Œå³è¡°å‡ä¸éœ€è¦çš„å™ªå£°æˆåˆ†å¹¶æ”¾å¤§è¯­éŸ³ç›¸å…³çš„é¢‘ç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformer-Mambaçš„æ–°å‹æ¶æ„ï¼Œä»¥åŠä¸€ç§é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨è”åˆä¼˜åŒ–å¹²æ‰°æŠ‘åˆ¶å’Œä¿¡å·å¢å¼ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬å»å™ªã€å»æ··å“å’Œå»å‰Šæ³¢ï¼Œè¯æ˜äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­è¿›è¡Œä¸»åŠ¨ã€é’ˆå¯¹æ€§è°ƒåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16911v2">PDF</a> </p>
<p><strong>Summary</strong><br>ä¸»åŠ¨å£°éŸ³ä¿®æ”¹é¢†åŸŸå‡ºç°äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼šä¸»åŠ¨è¯­éŸ³å¢å¼ºï¼ˆASEï¼‰ã€‚ä¸ä¼ ç»Ÿçš„ä¸»åŠ¨é™å™ªï¼ˆANCï¼‰ç®—æ³•ä¾§é‡äºæŠ‘åˆ¶å¤–éƒ¨å¹²æ‰°ä¸åŒï¼ŒASEèƒ½å¤Ÿç§¯æå¡‘é€ è¯­éŸ³ä¿¡å·ï¼Œæ—¢å‡å°‘ä¸éœ€è¦çš„å™ªå£°æˆåˆ†ï¼Œåˆæ”¾å¤§è¯­éŸ³ç›¸å…³çš„é¢‘ç‡ï¼Œä»è€Œæé«˜å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºTransformer-Mambaçš„æ–°å‹æ¶æ„ï¼Œä»¥åŠä¸“ä¸ºä¼˜åŒ–å¹²æ‰°æŠ‘åˆ¶å’Œä¿¡å·å¢å¼ºè€Œè®¾è®¡çš„ç‰¹å®šä»»åŠ¡æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒ…æ‹¬å»å™ªã€æ¶ˆé™¤å›å£°å’Œå»å‰ªè¾‘ï¼Œè¯æ˜äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­è¿›è¡Œä¸»åŠ¨ã€é’ˆå¯¹æ€§è°ƒåˆ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸»åŠ¨è¯­éŸ³å¢å¼ºï¼ˆASEï¼‰è¿™ä¸€æ–°çš„å£°éŸ³ä¿®æ”¹èŒƒå¼ã€‚</li>
<li>ä¸ä¸»åŠ¨é™å™ªï¼ˆANCï¼‰ç®—æ³•ä¸åŒï¼ŒASEèƒ½å¤Ÿç§¯æå¡‘é€ è¯­éŸ³ä¿¡å·ã€‚</li>
<li>ASEèƒ½å¤ŸåŒæ—¶å‡å°‘ä¸éœ€è¦çš„å™ªå£°æˆåˆ†å¹¶æ”¾å¤§è¯­éŸ³ç›¸å…³çš„é¢‘ç‡ï¼Œæé«˜è¯­éŸ³çš„å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºTransformer-Mambaçš„æ–°å‹æ¶æ„æ¥å®ç°ASEã€‚</li>
<li>ç‰¹å®šä»»åŠ¡æŸå¤±å‡½æ•°çš„è®¾è®¡èƒ½å¤Ÿè”åˆä¼˜åŒ–å¹²æ‰°æŠ‘åˆ¶å’Œä¿¡å·å¢å¼ºã€‚</li>
<li>åœ¨å¤šä¸ªè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬å»å™ªã€æ¶ˆé™¤å›å£°å’Œå»å‰ªè¾‘ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c0498008061bd418770d6b1297d0be57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a09f095b6413342f788c91eda228206c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-570be815f846e66a308fac4b3fc37fe9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing"><a href="#HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing" class="headerlink" title="HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing"></a>HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing</h2><p><strong>Authors:Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate</strong></p>
<p>Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org/">https://catalog.hausanlp.org</a>), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè±ªè¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå°½ç®¡å…¨çƒæœ‰è¶…è¿‡1.2äº¿çš„ç¬¬ä¸€è¯­è¨€ï¼ˆL1ï¼‰å’Œ8åƒä¸‡çš„ç¬¬äºŒè¯­è¨€ï¼ˆL2ï¼‰ä½¿ç”¨è€…ï¼Œä½†å®ƒä½œä¸ºä¸€ä¸ªèµ„æºè´«ä¹çš„è¯­è¨€ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†ç ”ç©¶ã€‚å°½ç®¡åœ¨é«˜èµ„æºè¯­è¨€æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è±ªè¨NLPä»ç„¶é¢ä¸´æŒç»­æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„å¼€æºæ•°æ®é›†å’Œä¸è¶³çš„æ¨¡å‹è¡¨ç¤ºã€‚æœ¬æ–‡æ¦‚è¿°äº†è±ªè¨NLPçš„å½“å‰çŠ¶æ€ï¼Œç³»ç»Ÿåœ°æ£€æŸ¥äº†åŸºæœ¬NLPä»»åŠ¡çš„ç°æœ‰èµ„æºã€ç ”ç©¶è´¡çŒ®å’Œå·®è·ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®ç­”ã€‚æˆ‘ä»¬ä»‹ç»äº†è±ªè¨NLPï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),è¿™æ˜¯ä¸€ä¸ªèšåˆæ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶å·¥ä½œçš„ç²¾é€‰ç›®å½•,æ—¨åœ¨å¢å¼ºå¯è®¿é—®æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥å‘å±•.æ­¤å¤–,æˆ‘ä»¬è®¨è®ºäº†å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹(llm)ä¸­çš„æŒ‘æˆ˜,è§£å†³æ¬¡ä¼˜åˆ†è¯å’Œæ–¹è¨€å˜åŒ–çš„é—®é¢˜.æœ€å,æˆ‘ä»¬æå‡ºäº†æˆ˜ç•¥ç ”ç©¶æ–¹å‘,å¼ºè°ƒæ‰©å¤§æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œ,ä»¥æ¨åŠ¨è±ªè¨nlpçš„å‘å±•.æˆ‘ä»¬çš„å·¥ä½œä¸ºåŠ é€Ÿè±ªè¨nlpçš„è¿›æ­¥æä¾›äº†åŸºç¡€,å¹¶ä¸ºæ›´å¹¿æ³›çš„å¤šè¯­è¨€nlpç ”ç©¶æä¾›äº†å®è´µçš„è§è§£./">https://catalog.hausanlp.orgï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªèšåˆæ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶å·¥ä½œçš„ç²¾é€‰ç›®å½•ï¼Œæ—¨åœ¨å¢å¼ºå¯è®¿é—®æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œè§£å†³æ¬¡ä¼˜åˆ†è¯å’Œæ–¹è¨€å˜åŒ–çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†æˆ˜ç•¥ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒæ‰©å¤§æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œï¼Œä»¥æ¨åŠ¨è±ªè¨NLPçš„å‘å±•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåŠ é€Ÿè±ªè¨NLPçš„è¿›æ­¥æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„å¤šè¯­è¨€NLPç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14311v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è±ªè¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç°çŠ¶ã€‚å°½ç®¡è±ªè¨è¯­æœ‰è¶…ä¸€äº¿çš„ç¬¬ä¸€è¯­è¨€å’Œç¬¬äºŒè¯­è¨€ä½¿ç”¨è€…ï¼Œä½†ä½œä¸ºä½èµ„æºè¯­è¨€ï¼Œå…¶ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡ç³»ç»Ÿå›é¡¾äº†è±ªè¨NLPçš„ç°æœ‰èµ„æºã€ç ”ç©¶è´¡çŒ®å’Œç©ºç™½é¢†åŸŸï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®ç­”ç­‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†è±ªè¨NLPé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æœ‰é™å¼€æ”¾æ•°æ®é›†å’Œæ¨¡å‹è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ä¸ºå¢å¼ºå¯è®¿é—®æ€§å’Œæ¨åŠ¨è¿›ä¸€æ­¥å‘å±•ï¼Œæœ¬æ–‡æ¨å‡ºäº†è±ªè¨NLPç›®å½•ï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org).æœ¬æ–‡æœ€åæå‡ºäº†æˆ˜ç•¥æ€§çš„ç ”ç©¶æ–¹å‘,å¼ºè°ƒæ•°æ®é›†æ‰©å±•ã€æ”¹è¿›çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåä½œ,ä»¥æ¨åŠ¨è±ªè¨nlpçš„è¿›æ­¥./">https://catalog.hausanlp.orgï¼‰ã€‚æœ¬æ–‡æœ€åæå‡ºäº†æˆ˜ç•¥æ€§çš„ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒæ•°æ®é›†æ‰©å±•ã€æ”¹è¿›çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåä½œï¼Œä»¥æ¨åŠ¨è±ªè¨NLPçš„è¿›æ­¥ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è±ªè¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å°½ç®¡æœ‰å¤§é‡çš„ä½¿ç”¨è€…ï¼Œä½†ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>è¯¥é¢†åŸŸé¢ä¸´æœ‰é™å¼€æ”¾æ•°æ®é›†å’Œæ¨¡å‹è¡¨ç¤ºä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>å­˜åœ¨å¤šä¸ªåŸºç¡€NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚</li>
<li>æ¨å‡ºäº†è±ªè¨NLPç›®å½•ï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),ä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•./">https://catalog.hausanlp.orgï¼‰ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚</a></li>
<li>é›†æˆè±ªè¨è¯­åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œå¦‚æ¬¡ä¼˜çš„åˆ†è¯å’Œæ–¹è¨€å˜åŒ–ã€‚</li>
<li>éœ€è¦æ‰©å±•æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåä½œæ¥æ¨åŠ¨è±ªè¨NLPçš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b44a57b070982a564e95c053fe9cf62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42759b51c5122b9d93aca1f303c8c6fd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding"><a href="#U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding" class="headerlink" title="U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding"></a>U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding</h2><p><strong>Authors:Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie</strong></p>
<p>The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pre-trained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that U-SAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available (<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/">https://github.com/Honee-W/U-SAM/</a>). </p>
<blockquote>
<p>éŸ³é¢‘ä»»åŠ¡æ–‡æœ¬ç”ŸæˆèŒƒå¼ä¸ºç»Ÿä¸€éŸ³é¢‘ç†è§£å¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å®ç°ä¸åŒç±»å‹éŸ³é¢‘çš„å…¨é¢ç†è§£æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚è¯­éŸ³ã€é€šç”¨éŸ³é¢‘äº‹ä»¶å’ŒéŸ³ä¹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹äº¤å‰ç†µæŸå¤±çš„è¿‡åº¦ä¾èµ–å¾€å¾€éš¾ä»¥è¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œå› ä¸ºè¿™ç§æ–¹æ³•å°†æ‰€æœ‰æ ‡è®°ä¸€è§†åŒä»ï¼Œå¿½è§†äº†å†—ä½™çš„éŸ³é¢‘ç‰¹å¾ï¼Œå¯¼è‡´è·¨æ¨¡æ€å¯¹é½è¾ƒå¼±ã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†U-SAMï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå®ƒé›†æˆäº†é’ˆå¯¹è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹çš„ä¸“ç”¨ç¼–ç å™¨ä»¥åŠé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚U-SAMé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±ä»ªè¿›è¡Œä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆï¼ŒåŠ¨æ€è·¯ç”±å¹¶é›†æˆé¢†åŸŸç‰¹å®šç¼–ç å™¨çš„è¾“å‡ºã€‚æ­¤å¤–ï¼ŒU-SAMè¿˜åŒ…å«è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€ç›‘ç£ä¸‹æ˜ç¡®è¯†åˆ«å†—ä½™éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶çº æ­£å…¶è¯­ä¹‰å’Œå…‰è°±è¡¨ç¤ºï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒU-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šå±•ç°å‡ºæ–°å…´èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶æ³›åŒ–æ½œåŠ›ã€‚ä»£ç å¯ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/%EF%BC%89%E3%80%82">https://github.com/Honee-W/U-SAM/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13880v2">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹U-SAMï¼Œå®ƒé’ˆå¯¹éŸ³é¢‘ä»»åŠ¡ä¸­çš„æ–‡æœ¬ç”ŸæˆèŒƒå¼æŒ‘æˆ˜è¿›è¡Œäº†æ”¹è¿›ã€‚U-SAMé€šè¿‡é›†æˆä¸“ä¸šåŒ–çš„è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ç¼–ç å™¨ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³è·¨å¤šç§éŸ³é¢‘ç±»å‹çš„å…¨é¢ç†è§£é—®é¢˜ã€‚å®ƒé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±å™¨è¿›è¡Œä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆï¼Œå¹¶åŠ¨æ€è·¯ç”±å’Œé›†æˆç‰¹å®šé¢†åŸŸçš„ç¼–ç å™¨è¾“å‡ºã€‚æ­¤å¤–ï¼ŒU-SAMè¿˜å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è¯­è¨€çš„ç›‘ç£ä¸‹æ˜ç¡®è¯†åˆ«å†—ä½™éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶çº æ­£å…¶è¯­ä¹‰å’Œå…‰è°±è¡¨ç¤ºï¼Œä»¥æé«˜è·¨æ¨¡æ€å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒU-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­ä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚å®ƒè¿˜å…·æœ‰åœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ–°å…´èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºå…¶é€šç”¨æ€§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-SAMæ˜¯ä¸€ç§å…ˆè¿›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤šæ ·éŸ³é¢‘ç±»å‹ç†è§£ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>U-SAMé›†æˆäº†ä¸“ä¸šåŒ–çš„è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ç¼–ç å™¨ï¼Œç»“åˆé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æŠ•å½±å™¨å®ç°ä»»åŠ¡æ„ŸçŸ¥ç‰¹å¾èåˆå’ŒåŠ¨æ€è·¯ç”±é›†æˆã€‚</li>
<li>U-SAMå¼•å…¥äº†è¯­ä¹‰æ„ŸçŸ¥å¯¹æ¯”æŸå¤±æ¨¡å—ï¼Œä»¥æé«˜è·¨æ¨¡æ€å¯¹é½æ•ˆæœã€‚</li>
<li>å†—ä½™éŸ³é¢‘ç‰¹å¾åœ¨è¯­è¨€çš„ç›‘ç£ä¸‹è¢«è¯†åˆ«å’Œçº æ­£ã€‚</li>
<li>U-SAMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä¸“ä¸šæ¨¡å‹å’Œç°æœ‰éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49834991c11e84dd5ee3e0117efe370a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b367bb8b913388b6cde7df16fdc82eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-145be9705a0d75da8c744da23ca6faa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668509d0ee1f5b8c7d8b6a820b7d8662.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SpeechT-RAG-Reliable-Depression-Detection-in-LLMs-with-Retrieval-Augmented-Generation-Using-Speech-Timing-Information"><a href="#SpeechT-RAG-Reliable-Depression-Detection-in-LLMs-with-Retrieval-Augmented-Generation-Using-Speech-Timing-Information" class="headerlink" title="SpeechT-RAG: Reliable Depression Detection in LLMs with   Retrieval-Augmented Generation Using Speech Timing Information"></a>SpeechT-RAG: Reliable Depression Detection in LLMs with   Retrieval-Augmented Generation Using Speech Timing Information</h2><p><strong>Authors:Xiangyu Zhang, Hexin Liu, Qiquan Zhang, Beena Ahmed, Julien Epps</strong></p>
<p>Large Language Models (LLMs) have been increasingly adopted for health-related tasks, yet their performance in depression detection remains limited when relying solely on text input. While Retrieval-Augmented Generation (RAG) typically enhances LLM capabilities, our experiments indicate that traditional text-based RAG systems struggle to significantly improve depression detection accuracy. This challenge stems partly from the rich depression-relevant information encoded in acoustic speech patterns information that current text-only approaches fail to capture effectively. To address this limitation, we conduct a systematic analysis of temporal speech patterns, comparing healthy individuals with those experiencing depression. Based on our findings, we introduce Speech Timing-based Retrieval-Augmented Generation, SpeechT-RAG, a novel system that leverages speech timing features for both accurate depression detection and reliable confidence estimation. This integrated approach not only outperforms traditional text-based RAG systems in detection accuracy but also enhances uncertainty quantification through a confidence scoring mechanism that naturally extends from the same temporal features. Our unified framework achieves comparable results to fine-tuned LLMs without additional training while simultaneously addressing the fundamental requirements for both accuracy and trustworthiness in mental health assessment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†åœ¨ä»…ä¾èµ–æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„è¡¨ç°ä»ç„¶æœ‰é™ã€‚è™½ç„¶æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šå¸¸å¯ä»¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„RAGç³»ç»Ÿåœ¨æé«˜æŠ‘éƒç—‡æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™ä¸€æŒ‘æˆ˜éƒ¨åˆ†æºäºç¼–ç åœ¨è¯­éŸ³æ¨¡å¼ä¿¡æ¯ä¸­çš„ä¸°å¯ŒæŠ‘éƒç—‡ç›¸å…³ä¿¡æ¯ï¼Œè€Œå½“å‰ä»…ä½¿ç”¨æ–‡æœ¬çš„æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°æ•è·è¿™äº›ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ—¶é—´æ€§è¯­éŸ³æ¨¡å¼è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œæ¯”è¾ƒäº†å¥åº·ä¸ªä½“å’ŒæŠ‘éƒç—‡æ‚£è€…çš„å·®å¼‚ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè¯­éŸ³æ—¶åºçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆSpeechT-RAGï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹ç³»ç»Ÿï¼Œåˆ©ç”¨è¯­éŸ³æ—¶åºç‰¹å¾è¿›è¡Œå‡†ç¡®çš„æŠ‘éƒç—‡æ£€æµ‹å’Œå¯é çš„å¯ä¿¡ä¼°è®¡ã€‚è¿™ç§ç»¼åˆæ–¹æ³•ä¸ä»…ä¼˜äºä¼ ç»Ÿçš„åŸºäºæ–‡æœ¬çš„RAGç³»ç»Ÿåœ¨æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢ï¼Œè€Œä¸”è¿˜é€šè¿‡ç½®ä¿¡åº¦è¯„åˆ†æœºåˆ¶å¢å¼ºäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè¯¥æœºåˆ¶è‡ªç„¶åœ°æ‰©å±•äº†ç›¸åŒçš„æ—¶åºç‰¹å¾ã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°äº†ä¸å¾®è°ƒLLMç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶è§£å†³äº†å¿ƒç†å¥åº·è¯„ä¼°ä¸­å¯¹å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦çš„åŸºæœ¬è¦æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10950v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†åœ¨ä»…ä¾èµ–æ–‡æœ¬è¾“å…¥è¿›è¡ŒæŠ‘éƒç—‡æ£€æµ‹æ—¶ï¼Œå…¶æ€§èƒ½è¡¨ç°ä»æœ‰é™ã€‚å°½ç®¡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯é€šå¸¸èƒ½æå‡LLMçš„èƒ½åŠ›ï¼Œä½†å®éªŒè¡¨æ˜ï¼Œä¼ ç»Ÿçš„æ–‡æœ¬å‹RAGç³»ç»Ÿåœ¨æé«˜æŠ‘éƒç—‡æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢æ•ˆæœå¹¶ä¸æ˜¾è‘—ã€‚è¿™ä¸€æŒ‘æˆ˜éƒ¨åˆ†æºäºè¯­éŸ³æ¨¡å¼ä¸­çš„ä¸°å¯ŒæŠ‘éƒç—‡ç›¸å…³ä¿¡æ¯ï¼Œè¿™æ˜¯å½“å‰ä»…ä¾èµ–æ–‡æœ¬çš„åšæ— æ³•æœ‰æ•ˆæ•è·çš„ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬å¯¹æŠ‘éƒç—‡æ‚£è€…ä¸å¥åº·äººçš„è¯­éŸ³æ—¶é—´æ¨¡å¼è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œå¼•å…¥äº†åŸºäºè¯­éŸ³æ—¶åºç‰¹å¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»ŸSpeechT-RAGã€‚è¯¥ç³»ç»Ÿä¸ä»…åˆ©ç”¨è¯­éŸ³æ—¶åºç‰¹å¾è¿›è¡Œå‡†ç¡®çš„æŠ‘éƒç—‡æ£€æµ‹ï¼Œè¿˜é€šè¿‡ä¿¡å¿ƒè¯„åˆ†æœºåˆ¶å®ç°å¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œè‡ªç„¶åœ°æ‰©å±•äº†ç›¸åŒçš„æ—¶åºç‰¹å¾çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶åœ¨ä¸ç»è¿‡é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°äº†ä¸ç²¾ç»†è°ƒæ•´çš„LLMç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶æ»¡è¶³ç²¾ç¥å¥åº·è¯„ä¼°å¯¹å‡†ç¡®æ€§å’Œå¯é æ€§çš„åŸºæœ¬è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŠ‘éƒç—‡æ£€æµ‹æ–¹é¢çš„æ€§èƒ½å—é™äºä»…ä½¿ç”¨æ–‡æœ¬è¾“å…¥ã€‚</li>
<li>ä¼ ç»Ÿçš„æ–‡æœ¬å‹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨æé«˜æŠ‘éƒç—‡æ£€æµ‹å‡†ç¡®æ€§æ–¹é¢æ•ˆæœæœ‰é™ã€‚</li>
<li>è¯­éŸ³æ¨¡å¼ä¸­çš„æŠ‘éƒç—‡ç›¸å…³ä¿¡æ¯ä¸°å¯Œï¼Œå½“å‰ä»…ä¾èµ–æ–‡æœ¬çš„æ¨¡å‹æ— æ³•æœ‰æ•ˆæ•è·ã€‚</li>
<li>å¯¹æŠ‘éƒç—‡æ‚£è€…ä¸å¥åº·äººçš„è¯­éŸ³æ—¶é—´æ¨¡å¼è¿›è¡Œäº†ç³»ç»Ÿåˆ†æã€‚</li>
<li>å¼•å…¥äº†åŸºäºè¯­éŸ³æ—¶åºç‰¹å¾çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»ŸSpeechT-RAGï¼Œç”¨äºå‡†ç¡®æ£€æµ‹æŠ‘éƒç—‡å¹¶è¿›è¡Œä¿¡å¿ƒè¯„åˆ†ã€‚</li>
<li>SpeechT-RAGç³»ç»Ÿä¸ä»…æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ï¼Œè¿˜é€šè¿‡ä¿¡å¿ƒè¯„åˆ†æœºåˆ¶å®ç°äº†ä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5342be2d3a8efaf0496ba677e0a5dea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-918ebfe71c307500699ccd51975dbb2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79fdde326683b97166c714eaef667449.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b978cc0092b161e86ff5aef63900a4bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f16421704dd0d28f7a84aa709b6afc80.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Low-Resource-Language-and-Instruction-Following-Capabilities-of-Audio-Language-Models"><a href="#Enhancing-Low-Resource-Language-and-Instruction-Following-Capabilities-of-Audio-Language-Models" class="headerlink" title="Enhancing Low-Resource Language and Instruction Following Capabilities   of Audio Language Models"></a>Enhancing Low-Resource Language and Instruction Following Capabilities   of Audio Language Models</h2><p><strong>Authors:Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul</strong></p>
<p>Audio language models process audio inputs using textual prompts for tasks like speech recognition and audio captioning. Although built on multilingual pre-trained components, most are trained primarily on English, limiting their usability for other languages. This paper evaluates audio language models on Thai, a low-resource language, and finds that they lack emergent cross-lingual abilities despite their multilingual foundations. To address this, we explore data mixtures that optimize audio language models for both a target language and English while integrating audio comprehension and speech instruction-following into a unified model. Our experiments provide insights into improving instruction-following in low-resource languages by balancing language-specific and multilingual training data. The proposed model, Typhoon-Audio, significantly outperforms existing open-source models and achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai. </p>
<blockquote>
<p>éŸ³é¢‘è¯­è¨€æ¨¡å‹ä½¿ç”¨æ–‡æœ¬æç¤ºæ¥å¤„ç†éŸ³é¢‘è¾“å…¥ï¼Œç”¨äºè¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘å­—å¹•ç­‰ä»»åŠ¡ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å»ºç«‹åœ¨å¤šè¯­è¨€é¢„è®­ç»ƒç»„ä»¶ä¹‹ä¸Šï¼Œä½†å¤§å¤šæ•°ä¸»è¦ä½¿ç”¨è‹±è¯­è¿›è¡Œè®­ç»ƒï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨å…¶ä»–è¯­è¨€ä¸­çš„å¯ç”¨æ€§ã€‚æœ¬æ–‡è¯„ä¼°äº†éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨æ³°è¯­ï¼ˆä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼‰ä¸­çš„åº”ç”¨ï¼Œå‘ç°å°½ç®¡å®ƒä»¬å…·æœ‰å¤šè¯­è¨€åŸºç¡€ï¼Œä½†ç¼ºä¹è·¨è¯­è¨€çš„åº”æ€¥èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¼˜åŒ–ç›®æ ‡è¯­è¨€å’Œè‹±è¯­éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ•°æ®æ··åˆï¼ŒåŒæ—¶å°†éŸ³é¢‘ç†è§£å’Œè¯­éŸ³æŒ‡ä»¤éµå¾ªé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„å®éªŒé€šè¿‡å¹³è¡¡è¯­è¨€ç‰¹å®šå’Œå¤šè¯­è¨€è®­ç»ƒæ•°æ®ï¼Œä¸ºæ”¹è¿›ä½èµ„æºè¯­è¨€ä¸­çš„æŒ‡ä»¤éµå¾ªæä¾›äº†è§è§£ã€‚æ‰€æå‡ºçš„Typhoon-Audioæ¨¡å‹æ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œåœ¨è‹±è¯­å’Œæ³°è¯­æ–¹é¢çš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„Gemini-1.5-Proç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10999v2">PDF</a> Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ³°è¯­ç­‰ä½èµ„æºè¯­è¨€æ—¶çš„è¡¨ç°ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åŸºäºå¤šè¯­è¨€é¢„è®­ç»ƒç»„ä»¶ï¼Œä½†å¤§å¤šæ•°ä¸»è¦è®­ç»ƒäºè‹±è¯­ï¼Œå¯¹äºå…¶ä»–è¯­è¨€çš„å¯ç”¨æ€§æœ‰é™ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨æ³°è¯­ä¸­çš„è¡¨ç°ï¼Œå¹¶å‘ç°å®ƒä»¬ç¼ºä¹è·¨è¯­è¨€çš„ç»¼åˆèƒ½åŠ›ã€‚ä¸ºäº†æ”¹å–„è¿™ä¸€æƒ…å†µï¼Œç ”ç©¶æ¢ç´¢äº†åŒæ—¶é’ˆå¯¹ç›®æ ‡è¯­è¨€å’Œè‹±è¯­è¿›è¡Œä¼˜åŒ–çš„æ•°æ®æ··åˆæ–¹æ³•ï¼Œå°†éŸ³é¢‘ç†è§£å’Œè¯­éŸ³æŒ‡ä»¤è·Ÿéšé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡å¹³è¡¡è¯­è¨€ç‰¹å®šå’Œå¤šè¯­è¨€è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ”¹å–„ä½èµ„æºè¯­è¨€ä¸­çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚æå‡ºçš„Typhoon-Audioæ¨¡å‹åœ¨æ³°è¯­å’Œè‹±è¯­ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œè¾¾åˆ°äº†ä¸Gemini-1.5-Proç›¸å½“çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸»è¦ç”¨äºå¤„ç†è¯­éŸ³ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘æè¿°ç­‰ã€‚</li>
<li>å°½ç®¡å¤§å¤šæ•°éŸ³é¢‘è¯­è¨€æ¨¡å‹åŸºäºå¤šè¯­è¨€é¢„è®­ç»ƒç»„ä»¶ï¼Œä½†å®ƒä»¬ä¸»è¦è®­ç»ƒäºè‹±è¯­ï¼Œå¯¹å…¶ä»–è¯­è¨€çš„å¯ç”¨æ€§å—é™ã€‚</li>
<li>åœ¨æ³°è¯­ç­‰ä½èµ„æºè¯­è¨€ä¸­ï¼ŒéŸ³é¢‘è¯­è¨€æ¨¡å‹ç¼ºä¹è·¨è¯­è¨€çš„ç»¼åˆèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–æ•°æ®æ··åˆæ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶é’ˆå¯¹ç›®æ ‡è¯­è¨€å’Œè‹±è¯­æ”¹å–„éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>å°†éŸ³é¢‘ç†è§£å’Œè¯­éŸ³æŒ‡ä»¤è·Ÿéšé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¹³è¡¡è¯­è¨€ç‰¹å®šå’Œå¤šè¯­è¨€è®­ç»ƒæ•°æ®èƒ½æ”¹å–„ä½èµ„æºè¯­è¨€ä¸­çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c6f30a7f36d8c83b8cad60583c966ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c433cd717b7ce5187239a3547ffc754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecbf15e6a365b2410f0ff489ba5c12b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b0c68bf0223edec2d73c542d10f4766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e76f8515993781c6771d6a0258a6b508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1c6865316f3d7ac82e12c09b809440c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-292b1664fb42724077ce75daf5ccdf1e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-144cb1abbc86db2ae8d174adad83e0fb.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  CGS-GAN 3D Consistent Gaussian Splatting GANs for High Resolution Human   Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e839f8b0068b9be7112bc5db3bbca7a2.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Semantic segmentation with reward
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
