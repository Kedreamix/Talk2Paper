<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-05-27  Daily-Omni Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b1b41946b7c8ba9d8f0e60d82315dd88.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    49 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-27-更新"><a href="#2025-05-27-更新" class="headerlink" title="2025-05-27 更新"></a>2025-05-27 更新</h1><h2 id="Daily-Omni-Towards-Audio-Visual-Reasoning-with-Temporal-Alignment-across-Modalities"><a href="#Daily-Omni-Towards-Audio-Visual-Reasoning-with-Temporal-Alignment-across-Modalities" class="headerlink" title="Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities"></a>Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities</h2><p><strong>Authors:Ziwei Zhou, Rui Wang, Zuxuan Wu</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) achieve promising performance on visual and audio benchmarks independently. However, the ability of these models to process cross-modal information synchronously remains largely unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual Questioning and Answering benchmark comprising 684 videos of daily life scenarios from diverse sources, rich in both audio and visual information, and featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA Generation Pipeline, which includes automatic annotation, QA generation and QA optimization, significantly improves efficiency for human evaluation and scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM) and Automatic Speech Recognition (ASR) model to establish a baseline for this benchmark. The results show that current MLLMs still struggle significantly with tasks requiring audio-visual integration, but combining VLMs and ALMs with simple temporal alignment techniques can achieve substantially better performance. Codes and benchmark are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Lliar-liar/Daily-Omni%7D%7Bhttps://github.com/Lliar-liar/Daily-Omni%7D">https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}</a>. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLMs）在视觉和音频基准测试上独立地取得了有前景的表现。然而，这些模型处理跨模态信息同步的能力仍然很大程度上未被探索。在本文中，我们介绍了以下内容：1）Daily-Omni，一个包含音频视觉问答的基准测试，它由来自不同源的684个日常生活场景的视频组成，这些视频富含音频和视觉信息，涵盖了6大类任务的1197个多项选择问答对；2）Daily-Omni问答生成管道，包括自动标注、问答生成和问答优化，它极大地提高了人类评估和基准测试的可扩展性的效率；3）无需训练的Daily-Omni-Agent，它利用开源的视觉语言模型（VLM）、音频语言模型（ALM）和自动语音识别（ASR）模型来建立这个基准测试的基线。结果表明，当前的多模态大型语言模型在处理需要视听整合的任务时仍存在较大的困难，但通过结合VLM和ALM以及简单的时序对齐技术，可以取得显著更好的性能。代码和基准测试可在<a target="_blank" rel="noopener" href="https://github.com/Lliar-liar/Daily-Omni%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Lliar-liar/Daily-Omni处获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视听信息的问答基准测试——Daily-Omni。该基准测试包含富含视听信息的684个日常生活场景视频，涵盖6项主要任务，并提供了自动标注、问答生成和问答优化的生成管道，提高了人类评估和基准测试的扩展性。此外，还介绍了无需训练的Daily-Omni-Agent，它通过利用视觉语言模型、音频语言模型和语音识别模型，为基准测试建立了基线。结果表明，当前的多模态大型语言模型在需要视听整合的任务上仍有显著挑战，但通过结合视觉语言模型和音频语言模型，并使用简单的时序对齐技术，可以取得更好的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Daily-Omni是一个视听问答基准测试，包含丰富的日常生活场景视频和多元选择问答对。</li>
<li>Daily-Omni引入了高效的QA生成管道，包括自动标注、问答生成和问答优化。</li>
<li>Daily-Omni-Agent是一个无需训练的模型，结合了视觉语言模型、音频语言模型和语音识别模型。</li>
<li>当前多模态大型语言模型在视听整合任务上存在挑战。</li>
<li>结合视觉语言模型和音频语言模型，并使用简单的时序对齐技术，可以显著提高模型性能。</li>
<li>论文提供了相关代码和基准测试链接，便于公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3513e45d61069adc591c2800ff13baec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e330e0c1b74c3d4e7d35ef6d2f4671cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a941fe17a6ee916562be5b4c44d375c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee355df71c7ca27c26ef0d7714919a85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9607e179f91bc443c06972277ff84f8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9ef3c70b3c003059f206d9d938daef6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-752ce21461af1bf25e166324c83be1f6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training"><a href="#CosyVoice-3-Towards-In-the-wild-Speech-Generation-via-Scaling-up-and-Post-training" class="headerlink" title="CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training"></a>CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and   Post-training</h2><p><strong>Authors:Zhihao Du, Changfeng Gao, Yuxuan Wang, Fan Yu, Tianyu Zhao, Hao Wang, Xiang Lv, Hui Wang, Xian Shi, Keyu An, Guanrou Yang, Yabin Li, Yanni Chen, Zhifu Gao, Qian Chen, Yue Gu, Mengzhe Chen, Yafeng Chen, Shiliang Zhang, Wen Wang, Jieping Ye</strong></p>
<p>In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3">https://funaudiollm.github.io/cosyvoice3</a>. </p>
<blockquote>
<p>在我们之前的工作中，我们推出了一款可扩展的流式语音合成模型CosyVoice 2，它集成了一个大型语言模型（LLM）和基于片段感知的流匹配（FM）模型，实现了低延迟的双向流式语音合成和接近真人水平的语音质量。尽管取得了这些进展，但在语言覆盖、领域多样性、数据量、文本格式和训练后技术方面，CosyVoice 2仍存在一定局限性。在本文中，我们介绍了CosyVoice 3，这是一款针对野外零样本多语种语音合成的改进模型，在内容一致性、说话人相似性和语调自然度方面超越了其前身。CosyVoice 3的主要特点包括：1）一种新型语音标记器，通过监督多任务训练开发，旨在提高语调的自然度，包括自动语音识别、语音情感识别、语言识别、音频事件检测和说话人分析。2）一种新的可微奖励模型，适用于训练后的场景，不仅适用于CosyVoice 3，也适用于其他基于LLM的语音合成模型。3）数据集大小扩展：训练数据从一万小时扩展到一百万小时，涵盖9种语言和18种中文方言，涉及各种领域和文本格式。4）模型大小扩展：模型参数从0.5亿增加到1.5亿，由于模型容量更大，我们的多语种基准测试性能有所提高。这些进步为野外语音合成的进展做出了重大贡献。我们鼓励读者在<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/cosyvoice3%E4%B8%8A%E8%AF%95%E5%90%AC%E6%BC%94%E7%A4%BA%E3%80%82">https://funaudiollm.github.io/cosyvoice3上试听演示。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17589v1">PDF</a> Preprint, work in progress</p>
<p><strong>摘要</strong></p>
<p>本文介绍了CosyVoice 3模型，该模型是对先前工作的改进，用于零样本多语种野外语音合成。相较于前代产品，CosyVoice 3在内容一致性、发音人相似性和语调自然度上有所超越。关键特性包括：1）通过监督多任务训练开发的新型语音标记器，以提高语调的自然性；2）适用于CosyVoice 3和其他基于LLM的语音合成模型的新型可微奖励模型；3）数据集大小扩展：训练数据从一万小时扩展到一百万小时，涵盖9种语言和18种中文方言，涉及各种领域和文本格式；4）模型大小扩展：模型参数从0.5亿增加到1.5亿，由于模型容量更大，我们的多语种基准测试性能增强。这些进展对野外语音合成的进步具有重要意义。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>CosyVoice 3模型是对CosyVoice 2的改进，用于零样本多语种野外语音合成。</li>
<li>CosyVoice 3在内容一致性、发音人相似性和语调自然度上有所超越。</li>
<li>新型语音标记器通过监督多任务训练提高语调自然性。</li>
<li>引入新型可微奖励模型，适用于CosyVoice 3和其他LLM语音合成模型的后期训练。</li>
<li>训练数据集从十万小时扩展到百万小时，涵盖多种语言和中文方言，以及多种领域和文本格式。</li>
<li>模型参数从0.5亿增加到1.5亿，提升了多语种性能。</li>
<li>鼓励听众通过网站链接体验Demo。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-02dfe4c632c55f1ba90b2947f7d34d30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a151dcf188a216bc252c52f1eeb9500f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-becc3688ac33167ec55c47a327468b56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c3b320b2d2bf7d701dc185ecaa1938c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reverse-Speech-Finder-A-Neural-Network-Backtracking-Architecture-for-Generating-Alzheimer’s-Disease-Speech-Samples-and-Improving-Diagnosis-Performance"><a href="#Reverse-Speech-Finder-A-Neural-Network-Backtracking-Architecture-for-Generating-Alzheimer’s-Disease-Speech-Samples-and-Improving-Diagnosis-Performance" class="headerlink" title="Reverse-Speech-Finder: A Neural Network Backtracking Architecture for   Generating Alzheimer’s Disease Speech Samples and Improving Diagnosis   Performance"></a>Reverse-Speech-Finder: A Neural Network Backtracking Architecture for   Generating Alzheimer’s Disease Speech Samples and Improving Diagnosis   Performance</h2><p><strong>Authors:Victor OK Li, Yang Han, Jacqueline CK Lam, Lawrence YL Cheung</strong></p>
<p>This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer’s Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF’s unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF’s superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF’s potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies. </p>
<blockquote>
<p>本研究介绍了Reverse-Speech-Finder（RSF），这是一种突破性的神经网络回溯架构，旨在通过语音分析提高阿尔茨海默病（AD）的诊断能力。RSF利用预训练的大型语言模型的威力，识别和利用最可能的AD特异性语音标记，解决真实AD语音样本稀缺和现有模型解释性有限的挑战。RSF的独特方法包括三个核心创新：首先，它利用了一个观察结果，即最可能预测AD的语音标记（定义为MPM），在神经网络中必须激活那些最可能预测AD的神经元（定义为MPN）。其次，它在输入层使用语音标记表示，允许从MPN回溯来识别AD的最可能语音标记（MPTs）。最后，它开发了一种创新的回溯方法，从MPN回溯到输入层，识别MPTs和相应的MPMs，并巧妙地发现了用于检测AD的新型语音标记。实验结果表明，RSF优于SHAP和集成梯度等传统方法，在准确率上提高了3.5%，F1分数提高了3.2%。通过生成包含新型标记的语音数据，RSF不仅缓解了真实数据稀缺的限制，还显著提高了AD诊断模型的稳健性和准确性。这些发现突显了RSF在基于语音的AD检测中的潜力，为AD相关的语言缺陷提供了新的见解，并为更有效的非侵入性早期干预策略铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17477v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>该研究提出一种名为Reverse-Speech-Finder（RSF）的突破性神经网络回溯架构，通过语音分析提高阿尔茨海默病（AD）诊断的准确性。RSF利用预训练的大型语言模型的强大功能，识别和利用最可能的AD特异性语音标记物，解决真实AD语音样本缺乏和现有模型解释性有限的问题。RSF的独特方法包括三个核心创新点：首先，它利用最可能的语音标记物（MPMs）必须激活那些最可能预测AD的神经元（在神经网络中定义为最可能的神经元MPNs）的观点。其次，它在输入层使用语音标记表示，允许从MPNs回溯以识别AD的最可能的语音标记符（MPTs）。最后，它开发了一种创新的回溯方法，从MPNs回溯到输入层，识别MPTs和相应的MPMs，并巧妙地揭示用于AD检测的新语音标记物。实验结果表明，RSF优于传统方法如SHAP和集成梯度法，准确率提高了3.5%，F1得分提高了3.2%。通过生成包含新型标记的语音数据，RSF不仅缓解了真实数据稀缺的限制，而且大大提高了AD诊断模型的稳健性和准确性。这些发现突显了RSF在基于语音的AD检测中的潜力，为AD相关的语言缺陷提供了新的见解，并为更有效的非侵入性早期干预策略铺平了道路。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RSF是一个基于神经网络的回溯架构，旨在通过语音分析提高阿尔茨海默病（AD）的诊断。</li>
<li>利用预训练的大型语言模型来识别和利用最可能的AD特异性语音标记物。</li>
<li>RSF的核心创新包括：利用最可能的语音标记物和神经元之间的关系，使用语音标记表示在输入层，以及开发一种创新的回溯方法来识别AD的新型语音标记物。</li>
<li>实验结果表明RSF在AD检测方面优于传统方法，准确率和F1得分均有显著提高。</li>
<li>RSF不仅缓解了真实数据稀缺的问题，还提高了AD诊断模型的稳健性和准确性。</li>
<li>RSF的潜力在于为AD相关的语言缺陷提供新的见解，并可能为更有效的非侵入性早期干预策略提供指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17477">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6cd5c413307a58eef1cf40fbd7be74f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0911577ca3f5264635626d062f3badc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models"><a href="#Exploring-the-Effect-of-Segmentation-and-Vocabulary-Size-on-Speech-Tokenization-for-Speech-Language-Models" class="headerlink" title="Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models"></a>Exploring the Effect of Segmentation and Vocabulary Size on Speech   Tokenization for Speech Language Models</h2><p><strong>Authors:Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi</strong></p>
<p>The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed&#x2F;variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding. </p>
<blockquote>
<p>语音切词的目的是将语音信号转换为一系列离散表示，作为语音语言模型（SLM）的基础。虽然语音切词有很多选择，但它们对SLM性能的影响仍不明确。本文研究了语音切词的两个方面：分段宽度和离散单元簇的大小。首先，我们将语音信号分段为固定&#x2F;可变宽度和汇总表示。然后，我们在多个簇大小上训练K-means模型。通过对零样本口语理解基准测试进行评估，我们发现适度粗糙的分段和较大的簇大小产生了积极的影响。值得注意的是，在表现最佳的模型中，最有效的方法实现了训练数据减少50%，训练运行时间减少70%。我们的分析强调了结合多个令牌以增强精细粒度口语理解的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17446v1">PDF</a> Accepted to Interspeech2025</p>
<p><strong>总结</strong></p>
<p>语音标记化的目的是将语音信号转化为一系列离散表示，作为语音语言模型（SLM）的基础。本文研究了语音标记化的两个关键方面：分段宽度和离散单元簇的大小。实验通过固定或可变宽度的分段以及池化表示，训练了不同簇大小的K-means模型。在零样本口语理解基准测试上，本文发现适度粗糙的分段和较大的簇大小具有积极影响。最高效的模型甚至实现了训练数据减少50%，训练时间减少70%。分析表明，结合多个标记有助于提高精细口语的理解能力。</p>
<p><strong>要点</strong></p>
<ol>
<li>语音标记化是将语音信号转化为离散表示序列的过程，是语音语言模型的基础。</li>
<li>语音标记化的两个关键方面为分段宽度和离散单元簇的大小。</li>
<li>通过固定或可变宽度的分段以及池化表示，训练了K-means模型。</li>
<li>适度粗糙的分段和较大的簇大小对语音语言模型的性能有积极影响。</li>
<li>最高效的模型实现了训练数据和时间的显著减少。</li>
<li>结合多个标记有助于提高精细口语理解能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33921283ba0600e37ddc490ea666cc71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84ab9efa94e18637e17728fb9d61f995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb2c99cf1566ee39243d28fbf980bee3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd3572a3f66ee384e13d863ccdc96bd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975bc3cccba45a17f24ae9ec37c77142.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c76bc3715c57c5168db5014fe616f533.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniTTS-An-end-to-end-TTS-system-without-decoupling-of-acoustic-and-semantic-information"><a href="#UniTTS-An-end-to-end-TTS-system-without-decoupling-of-acoustic-and-semantic-information" class="headerlink" title="UniTTS: An end-to-end TTS system without decoupling of acoustic and   semantic information"></a>UniTTS: An end-to-end TTS system without decoupling of acoustic and   semantic information</h2><p><strong>Authors:Rui Wang, Qianguo Sun, Tianrong Chen, Zhiyun Zeng, Junlong Wu, Jiaxing Zhang</strong></p>
<p>The emergence of multi-codebook neutral audio codecs such as Residual Vector Quantization (RVQ) and Group Vector Quantization (GVQ) has significantly advanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These codecs are crucial in separating semantic and acoustic information while efficiently harnessing semantic priors. However, since semantic and acoustic information cannot be fully aligned, a significant drawback of these methods when applied to LLM-based TTS is that large language models may have limited access to comprehensive audio information. To address this limitation, we propose DistilCodec and UniTTS, which collectively offer the following advantages: 1) This method can distill a multi-codebook audio codec into a single-codebook audio codec with 32,768 codes while achieving a near 100% utilization. 2) As DistilCodec does not employ a semantic alignment scheme, a large amount of high-quality unlabeled audio (such as audiobooks with sound effects, songs, etc.) can be incorporated during training, further expanding data diversity and broadening its applicability. 3) Leveraging the comprehensive audio information modeling of DistilCodec, we integrated three key tasks into UniTTS’s pre-training framework: audio modality autoregression, text modality autoregression, and speech-text cross-modal autoregression. This allows UniTTS to accept interleaved text and speech&#x2F;audio prompts while substantially preserving LLM’s text capabilities. 4) UniTTS employs a three-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and Alignment. Source code and model checkpoints are publicly available at <a target="_blank" rel="noopener" href="https://github.com/IDEA-Emdoor-Lab/UniTTS">https://github.com/IDEA-Emdoor-Lab/UniTTS</a> and <a target="_blank" rel="noopener" href="https://github.com/IDEA-Emdoor-Lab/DistilCodec">https://github.com/IDEA-Emdoor-Lab/DistilCodec</a>. </p>
<blockquote>
<p>随着多码本中性音频编解码器（如残差矢量量化（RVQ）和组矢量量化（GVQ））的出现，显著推动了基于大型语言模型（LLM）的文本转语音（TTS）系统的发展。这些编解码器在分离语义和声音信息的同时，有效地利用语义先验知识方面发挥着关键作用。然而，由于语义和声音信息无法完全对齐，这些方法应用于基于LLM的TTS时的一个重大缺点是大型语言模型可能无法获得全面的音频信息。为了克服这一局限性，我们提出了DistilCodec和UniTTS，它们共同具有以下优点：1）此方法可以将多码本音频编解码器蒸馏为单码本音频编解码器，同时拥有32768个码本，且利用率接近100%。2）由于DistilCodec没有采用语义对齐方案，因此可以在训练过程中融入大量高质量的无标签音频（如带有音效的有声读物、歌曲等），进一步扩大了数据多样性和拓宽了其适用性。3）利用DistilCodec的全面音频信息建模，我们将三个关键任务整合到UniTTS的预训练框架中：音频模态自回归、文本模态自回归和语音-文本跨模态自回归。这允许UniTTS在接受交织的文本和语音&#x2F;音频提示时，同时保留LLM的文本功能。4）UniTTS采用三阶段训练过程：预训练、监督微调（SFT）和对齐。源代码和模型检查点可在<a target="_blank" rel="noopener" href="https://github.com/IDEA-Emdoor-Lab/UniTTS%E5%92%8Chttps://github.com/IDEA-Emdoor-Lab/DistilCodec%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IDEA-Emdoor-Lab/UniTTS和https://github.com/IDEA-Emdoor-Lab/DistilCodec上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17426v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多代码本中立音频编码器的出现，如残差矢量量化（RVQ）和组矢量量化（GVQ），已经显著推进了基于大语言模型（LLM）的文本到语音（TTS）系统的发展。然而，这些方法在应用于LLM-based TTS时存在语义和音频信息无法完全对齐的局限性。为解决此问题，提出了DistilCodec和UniTTS，它们能够：将多代码本音频编码器转化为单代码本音频编码器，实现近100%的利用率；利用无需语义对齐的方法，纳入大量高质量未标注音频，提高数据多样性和适用性；整合三个关键任务到UniTTS的预训练框架中，接受交替的文本和语音&#x2F;音频提示，同时保留LLM的文本能力；采用三阶段训练过程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多代码本中立音频编码器的出现已经显著推动了基于大语言模型的文本到语音系统的发展。</li>
<li>语义和音频信息的不完全对齐是当前方法的一个重大挑战。</li>
<li>DistilCodec方法能够将多代码本音频编码器转化为单代码本音频编码器，并实现近100%的利用率。</li>
<li>由于不需要语义对齐，DistilCodec可以纳入大量未标注的音频数据。</li>
<li>UniTTS整合了三个关键任务到其预训练框架中，以提高系统的功能性和灵活性。</li>
<li>UniTTS采用了包括预训练、监督微调和对齐在内的三阶段训练过程。</li>
<li>源代码和模型检查点已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17426">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-74a08a27af31c110607e809849b629be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e3664303fa1156eb3670bca354977c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95eddf18366a0471250fb1383abb3fb9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-based-Generative-Error-Correction-for-Rare-Words-with-Synthetic-Data-and-Phonetic-Context"><a href="#LLM-based-Generative-Error-Correction-for-Rare-Words-with-Synthetic-Data-and-Phonetic-Context" class="headerlink" title="LLM-based Generative Error Correction for Rare Words with Synthetic Data   and Phonetic Context"></a>LLM-based Generative Error Correction for Rare Words with Synthetic Data   and Phonetic Context</h2><p><strong>Authors:Natsuo Yamashita, Masaaki Yamamoto, Hiroaki Kokubo, Yohei Kawaguchi</strong></p>
<p>Generative error correction (GER) with large language models (LLMs) has emerged as an effective post-processing approach to improve automatic speech recognition (ASR) performance. However, it often struggles with rare or domain-specific words due to limited training data. Furthermore, existing LLM-based GER approaches primarily rely on textual information, neglecting phonetic cues, which leads to over-correction. To address these issues, we propose a novel LLM-based GER approach that targets rare words and incorporates phonetic information. First, we generate synthetic data to contain rare words for fine-tuning the GER model. Second, we integrate ASR’s N-best hypotheses along with phonetic context to mitigate over-correction. Experimental results show that our method not only improves the correction of rare words but also reduces the WER and CER across both English and Japanese datasets. </p>
<blockquote>
<p>基于大型语言模型（LLM）的生成式错误校正（GER）已成为提高自动语音识别（ASR）性能的有效后处理的方法。然而，由于训练数据的有限，它通常在处理稀有词或特定领域的词时面临挑战。此外，现有的基于LLM的GER方法主要依赖文本信息，忽略了语音线索，从而导致过度校正。为了解决这些问题，我们提出了一种新型的基于LLM的GER方法，该方法针对稀有词并融入了语音信息。首先，我们生成合成数据，其中包含稀有词，以微调GER模型。其次，我们结合ASR的N-best假设和语音环境，以减轻过度校正。实验结果表明，我们的方法不仅提高了稀有词的校正率，而且降低了英语和日语数据集的词错误率和字符错误率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17410v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>Summary</strong><br>     基于大型语言模型（LLM）的生成式错误更正（GER）是提升自动语音识别（ASR）性能的有效后处理方法。然而，它因训练数据有限而难以应对稀有词或特定领域的词汇。此外，现有的LLM-based GER方法主要依赖文本信息，忽略了语音线索，导致过度修正。为解决这些问题，我们提出了一种针对稀有词并融入语音信息的新型LLM-based GER方法。首先，我们生成合成数据以包含稀有词，对GER模型进行微调。其次，我们结合ASR的N-best假设和语音环境来减轻过度修正的问题。实验结果表明，我们的方法不仅提高了稀有词的修正效果，还降低了英语和日语数据集的词错误率和字符错误率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式错误更正（GER）是提升自动语音识别（ASR）性能的有效后处理方法。</li>
<li>LLM-based GER方法面临稀有词和特定领域词汇的识别挑战。</li>
<li>现有LLM-based GER方法主要依赖文本信息，导致过度修正。</li>
<li>提出了一种新型LLM-based GER方法，该方法针对稀有词并融入语音信息。</li>
<li>通过生成合成数据对GER模型进行微调，以包含稀有词。</li>
<li>结合ASR的N-best假设和语音环境来减轻过度修正问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17410">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3d4807dcb973889b035fea4cc9523850.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def4198604d04f20aa606ed2360b8e8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ee4377977be173b207538284bf63f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6e882b8cfdcc9c75c9d147af46bd6f9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language"><a href="#An-End-to-End-Approach-for-Child-Reading-Assessment-in-the-Xhosa-Language" class="headerlink" title="An End-to-End Approach for Child Reading Assessment in the Xhosa   Language"></a>An End-to-End Approach for Child Reading Assessment in the Xhosa   Language</h2><p><strong>Authors:Sergio Chevtchenko, Nikhil Navas, Rafaella Vale, Franco Ubaudi, Sipumelele Lucwaba, Cally Ardington, Soheil Afshar, Mark Antoniou, Saeed Afshar</strong></p>
<p>Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual’s life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children’s voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained. </p>
<blockquote>
<p>儿童识字能力是个人后续生命阶段生活成果的重要预测指标。这指出了针对脆弱的中低收入群体进行针对性干预的必要性，以帮助缩小这些地区识字水平与高收入地区之间的差距。在此类工作中，阅读评估是衡量这些项目效果的重要工具，人工智能可以作为支持教育工作者完成这项任务的可靠且经济的工具。对于低资源语言中的儿童语音，开发准确的自动阅读评估系统面临着重大挑战，因为数据有限且儿童的声音具有独特的声学特性。本研究以南非使用的语言之一——科萨语为例，旨在提高儿童语音识别能力。我们展示了一个由儿童科萨语语音样本组成的新数据集。该数据集可在收到请求后获得，包含十个单词和字母，这些单词和字母是早期阅读能力评估（EGRA）系统的一部分。每个录音都通过多个标记者采用在线和低成本的方式进行标记，部分录音还经过独立的EGRA审核人员进行验证。该数据集经过三种最新端到端模型的微调后进行了评估：wav2vec 2.0、HuBERT和Whisper。结果表明，这些模型的性能可能会受到可用训练数据的数量和平衡性的显著影响，这对于低成本大规模数据集的收集至关重要。此外，我们的实验表明，即使在可用样本数量受限的情况下，通过一次性训练多个类别也可以提高wav2vec 2.0的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17371v1">PDF</a> Paper accepted on AIED 2025 containing 14 pages, 6 figures and 4   tables</p>
<p><strong>Summary</strong></p>
<p>本文探讨儿童阅读能力对其未来生活阶段的影响，并指出对低收入和中收入群体的脆弱人群进行有针对性的干预是必要的。阅读评估是评估这些项目效果的重要工具，而人工智能可以可靠且经济地支持教育者完成这项任务。本研究关注南非使用的语言——科萨语，以推进儿童语音识别能力。研究团队推出了一份新型数据集，包含科萨语儿童语音样本。该数据集由在线和成本效益方法进行标注，部分数据经过独立早期阅读能力评估系统审核员验证。使用wav2vec 2.0、HuBERT和Whisper三种先进端对端模型进行评估的结果表明，可用训练数据的数量和平衡度会对模型性能产生显著影响，这对于低成本大规模数据集的收集至关重要。此外，实验显示，即使在可用样本数量有限的情况下，通过同时训练多个类别也能提高wav2vec 2.0的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>儿童阅读能力是预测其未来生活阶段的重要因素，需要关注对低收入和中收入脆弱群体的有针对性的干预措施。</li>
<li>阅读评估是评估阅读推广项目效果的重要工具，人工智能可以支持教育者进行这项任务。</li>
<li>本研究关注科萨语的儿童语音识别能力发展，推出新型数据集。</li>
<li>数据集通过在线和成本效益方法进行标注，部分数据经过独立审核员验证。</li>
<li>wav2vec 2.0、HuBERT和Whisper三种模型被用于评估儿童语音样本。</li>
<li>可用训练数据的数量和平衡度对模型性能有显著影响，强调低成本大规模数据集收集的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c8311e68f96845ff0c0bde5f77af2463.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4e33aa3709d3ae11ca3f52047076849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-798f6a03b9ca85b10edafe60ab63b3f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b41946b7c8ba9d8f0e60d82315dd88.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation"><a href="#Active-Speech-Enhancement-Active-Speech-Denoising-Decliping-and-Deveraberation" class="headerlink" title="Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation"></a>Active Speech Enhancement: Active Speech Denoising Decliping and   Deveraberation</h2><p><strong>Authors:Ofir Yaish, Yehuda Mishaly, Eliya Nachmani</strong></p>
<p>We introduce a new paradigm for active sound modification: Active Speech Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on suppressing external interference, ASE goes further by actively shaping the speech signal – both attenuating unwanted noise components and amplifying speech-relevant frequencies – to improve intelligibility and perceptual quality. To enable this, we propose a novel Transformer-Mamba-based architecture, along with a task-specific loss function designed to jointly optimize interference suppression and signal enrichment. Our method outperforms existing baselines across multiple speech processing tasks – including denoising, dereverberation, and declipping – demonstrating the effectiveness of active, targeted modulation in challenging acoustic environments. </p>
<blockquote>
<p>我们介绍了一种主动声音修改的新范式：主动语音增强（ASE）。而主动降噪（ANC）算法主要侧重于抑制外部干扰，ASE更进一步地通过主动塑造语音信号来改善语音的清晰度和感知质量，即衰减不需要的噪声成分并放大语音相关的频率。为此，我们提出了一种基于Transformer-Mamba的新型架构，以及一种针对特定任务的损失函数，旨在联合优化干扰抑制和信号增强。我们的方法在多个语音处理任务上的表现优于现有基线，包括去噪、去混响和去削波，证明了在具有挑战性的声学环境中进行主动、针对性调制的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16911v2">PDF</a> </p>
<p><strong>Summary</strong><br>主动声音修改领域出现了一种新的范式：主动语音增强（ASE）。与传统的主动降噪（ANC）算法侧重于抑制外部干扰不同，ASE能够积极塑造语音信号，既减少不需要的噪声成分，又放大语音相关的频率，从而提高可理解性和感知质量。为实现这一点，我们提出了一种基于Transformer-Mamba的新型架构，以及专为优化干扰抑制和信号增强而设计的特定任务损失函数。我们的方法在多个语音处理任务上的表现优于现有基线，包括去噪、消除回声和去剪辑，证明了在具有挑战性的声学环境中进行主动、针对性调制的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了主动语音增强（ASE）这一新的声音修改范式。</li>
<li>与主动降噪（ANC）算法不同，ASE能够积极塑造语音信号。</li>
<li>ASE能够同时减少不需要的噪声成分并放大语音相关的频率，提高语音的可理解性和感知质量。</li>
<li>提出了一种基于Transformer-Mamba的新型架构来实现ASE。</li>
<li>特定任务损失函数的设计能够联合优化干扰抑制和信号增强。</li>
<li>在多个语音处理任务上，包括去噪、消除回声和去剪辑，该方法表现优于现有基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c0498008061bd418770d6b1297d0be57.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a09f095b6413342f788c91eda228206c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-570be815f846e66a308fac4b3fc37fe9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing"><a href="#HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing" class="headerlink" title="HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing"></a>HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing</h2><p><strong>Authors:Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate</strong></p>
<p>Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org/">https://catalog.hausanlp.org</a>), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research. </p>
<blockquote>
<p>近年来，豪萨自然语言处理（NLP）受到了越来越多的关注，尽管全球有超过1.2亿的第一语言（L1）和8千万的第二语言（L2）使用者，但它作为一个资源贫乏的语言仍然没有得到充分研究。尽管在高资源语言方面取得了重大进展，但豪萨NLP仍然面临持续挑战，包括有限的开源数据集和不足的模型表示。本文概述了豪萨NLP的当前状态，系统地检查了基本NLP任务的现有资源、研究贡献和差距，包括文本分类、机器翻译、命名实体识别、语音识别和问答。我们介绍了豪萨NLP（<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),这是一个聚合数据集、工具和研究工作的精选目录,旨在增强可访问性并推动进一步发展.此外,我们讨论了将豪萨语集成到大型语言模型(llm)中的挑战,解决次优分词和方言变化的问题.最后,我们提出了战略研究方向,强调扩大数据集、改进语言建模方法和加强社区合作,以推动豪萨nlp的发展.我们的工作为加速豪萨nlp的进步提供了基础,并为更广泛的多语言nlp研究提供了宝贵的见解./">https://catalog.hausanlp.org），这是一个聚合数据集、工具和研究工作的精选目录，旨在增强可访问性并推动进一步发展。此外，我们讨论了将豪萨语集成到大型语言模型（LLM）中的挑战，解决次优分词和方言变化的问题。最后，我们提出了战略研究方向，强调扩大数据集、改进语言建模方法和加强社区合作，以推动豪萨NLP的发展。我们的工作为加速豪萨NLP的进步提供了基础，并为更广泛的多语言NLP研究提供了宝贵的见解。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14311v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了豪萨自然语言处理（NLP）的现状。尽管豪萨语有超一亿的第一语言和第二语言使用者，但作为低资源语言，其研究仍然不足。本文系统回顾了豪萨NLP的现有资源、研究贡献和空白领域，如文本分类、机器翻译、命名实体识别、语音识别和问答等。此外，文章还介绍了豪萨NLP面临的挑战，如有限开放数据集和模型表示不足的问题。为增强可访问性和推动进一步发展，本文推出了豪萨NLP目录（<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org).本文最后提出了战略性的研究方向,强调数据集扩展、改进的语言建模方法和加强社区协作,以推动豪萨nlp的进步./">https://catalog.hausanlp.org）。本文最后提出了战略性的研究方向，强调数据集扩展、改进的语言建模方法和加强社区协作，以推动豪萨NLP的进步。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>豪萨自然语言处理（NLP）尽管有大量的使用者，但研究仍然不足。</li>
<li>该领域面临有限开放数据集和模型表示不足的挑战。</li>
<li>存在多个基础NLP任务，包括文本分类、机器翻译、命名实体识别等。</li>
<li>推出了豪萨NLP目录（<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),以推动该领域的发展./">https://catalog.hausanlp.org），以推动该领域的发展。</a></li>
<li>集成豪萨语到大型语言模型（LLM）中面临一些问题，如次优的分词和方言变化。</li>
<li>需要扩展数据集、改进语言建模方法和加强社区协作来推动豪萨NLP的进步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14311">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b44a57b070982a564e95c053fe9cf62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42759b51c5122b9d93aca1f303c8c6fd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding"><a href="#U-SAM-An-audio-language-Model-for-Unified-Speech-Audio-and-Music-Understanding" class="headerlink" title="U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding"></a>U-SAM: An audio language Model for Unified Speech, Audio, and Music   Understanding</h2><p><strong>Authors:Ziqian Wang, Xianjun Xia, Xinfa Zhu, Lei Xie</strong></p>
<p>The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pre-trained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that U-SAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available (<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/">https://github.com/Honee-W/U-SAM/</a>). </p>
<blockquote>
<p>音频任务文本生成范式为统一音频理解开启了新的可能性。然而，现有模型在实现不同类型音频的全面理解方面面临重大挑战，如语音、通用音频事件和音乐。此外，它们对交叉熵损失的过度依赖往往难以达到预期效果，因为这种方法将所有标记一视同仁，忽视了冗余的音频特征，导致跨模态对齐较弱。为了应对上述挑战，本文介绍了U-SAM，这是一种先进的音频语言模型，它集成了针对语音、音频和音乐的专用编码器以及预训练的大型语言模型（LLM）。U-SAM采用混合专家（MoE）投影仪进行任务感知特征融合，动态路由并集成领域特定编码器的输出。此外，U-SAM还包含语义感知对比损失模块，该模块在语言监督下明确识别冗余音频特征，并纠正其语义和光谱表示，以增强跨模态对齐。大量实验表明，U-SAM在多个基准测试中始终优于专业模型和现有音频语言模型。此外，它在未见过的任务上展现出新兴能力，展示了其泛化潜力。代码可用（<a target="_blank" rel="noopener" href="https://github.com/Honee-W/U-SAM/%EF%BC%89%E3%80%82">https://github.com/Honee-W/U-SAM/）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13880v2">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种先进的音频语言模型U-SAM，它针对音频任务中的文本生成范式挑战进行了改进。U-SAM通过集成专业化的语音、音频和音乐编码器，并结合预训练的大型语言模型（LLM）来解决跨多种音频类型的全面理解问题。它采用混合专家（MoE）投影器进行任务感知特征融合，并动态路由和集成特定领域的编码器输出。此外，U-SAM还引入了一个语义感知对比损失模块，该模块在语言的监督下明确识别冗余音频特征，并纠正其语义和光谱表示，以提高跨模态对齐。实验表明，U-SAM在多个基准测试中持续优于专业模型和现有音频语言模型。它还具有在未见任务上的新兴能力，显示出其通用性潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-SAM是一种先进的音频语言模型，解决了现有模型在多样音频类型理解上的挑战。</li>
<li>U-SAM集成了专业化的语音、音频和音乐编码器，结合预训练的大型语言模型（LLM）。</li>
<li>采用混合专家（MoE）投影器实现任务感知特征融合和动态路由集成。</li>
<li>U-SAM引入了语义感知对比损失模块，以提高跨模态对齐效果。</li>
<li>冗余音频特征在语言的监督下被识别和纠正。</li>
<li>U-SAM在多个基准测试中表现优异，优于专业模型和现有音频语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49834991c11e84dd5ee3e0117efe370a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b367bb8b913388b6cde7df16fdc82eee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-145be9705a0d75da8c744da23ca6faa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668509d0ee1f5b8c7d8b6a820b7d8662.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SpeechT-RAG-Reliable-Depression-Detection-in-LLMs-with-Retrieval-Augmented-Generation-Using-Speech-Timing-Information"><a href="#SpeechT-RAG-Reliable-Depression-Detection-in-LLMs-with-Retrieval-Augmented-Generation-Using-Speech-Timing-Information" class="headerlink" title="SpeechT-RAG: Reliable Depression Detection in LLMs with   Retrieval-Augmented Generation Using Speech Timing Information"></a>SpeechT-RAG: Reliable Depression Detection in LLMs with   Retrieval-Augmented Generation Using Speech Timing Information</h2><p><strong>Authors:Xiangyu Zhang, Hexin Liu, Qiquan Zhang, Beena Ahmed, Julien Epps</strong></p>
<p>Large Language Models (LLMs) have been increasingly adopted for health-related tasks, yet their performance in depression detection remains limited when relying solely on text input. While Retrieval-Augmented Generation (RAG) typically enhances LLM capabilities, our experiments indicate that traditional text-based RAG systems struggle to significantly improve depression detection accuracy. This challenge stems partly from the rich depression-relevant information encoded in acoustic speech patterns information that current text-only approaches fail to capture effectively. To address this limitation, we conduct a systematic analysis of temporal speech patterns, comparing healthy individuals with those experiencing depression. Based on our findings, we introduce Speech Timing-based Retrieval-Augmented Generation, SpeechT-RAG, a novel system that leverages speech timing features for both accurate depression detection and reliable confidence estimation. This integrated approach not only outperforms traditional text-based RAG systems in detection accuracy but also enhances uncertainty quantification through a confidence scoring mechanism that naturally extends from the same temporal features. Our unified framework achieves comparable results to fine-tuned LLMs without additional training while simultaneously addressing the fundamental requirements for both accuracy and trustworthiness in mental health assessment. </p>
<blockquote>
<p>大型语言模型（LLM）在健康相关任务中的应用越来越广泛，但在仅依赖文本输入的情况下，它们在抑郁症检测方面的表现仍然有限。虽然检索增强生成（RAG）通常可以增强LLM的能力，但我们的实验表明，传统的基于文本的RAG系统在提高抑郁症检测准确性方面存在困难。这一挑战部分源于编码在语音模式信息中的丰富抑郁症相关信息，而当前仅使用文本的方法无法有效地捕获这些信息。为了解决这个问题，我们对时间性语音模式进行了系统分析，比较了健康个体和抑郁症患者的差异。基于我们的发现，我们引入了基于语音时序的检索增强生成（SpeechT-RAG），这是一个新型系统，利用语音时序特征进行准确的抑郁症检测和可靠的可信估计。这种综合方法不仅优于传统的基于文本的RAG系统在检测准确性方面，而且还通过置信度评分机制增强了不确定性量化，该机制自然地扩展了相同的时序特征。我们的统一框架在不进行额外训练的情况下实现了与微调LLM相当的结果，同时解决了心理健康评估中对准确性和可信度的基本要求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10950v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在健康相关任务中的应用日益广泛，但在仅依赖文本输入进行抑郁症检测时，其性能表现仍有限。尽管检索增强生成（RAG）技术通常能提升LLM的能力，但实验表明，传统的文本型RAG系统在提高抑郁症检测准确性方面效果并不显著。这一挑战部分源于语音模式中的丰富抑郁症相关信息，这是当前仅依赖文本的做无法有效捕获的。为解决这一局限，我们对抑郁症患者与健康人的语音时间模式进行了系统分析。基于我们的发现，引入了基于语音时序特征的检索增强生成系统SpeechT-RAG。该系统不仅利用语音时序特征进行准确的抑郁症检测，还通过信心评分机制实现可靠的不确定性量化，自然地扩展了相同的时序特征的应用。我们的统一框架在不经过额外训练的情况下实现了与精细调整的LLM相当的结果，同时满足精神健康评估对准确性和可靠性的基本要求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在抑郁症检测方面的性能受限于仅使用文本输入。</li>
<li>传统的文本型检索增强生成（RAG）系统在提高抑郁症检测准确性方面效果有限。</li>
<li>语音模式中的抑郁症相关信息丰富，当前仅依赖文本的模型无法有效捕获。</li>
<li>对抑郁症患者与健康人的语音时间模式进行了系统分析。</li>
<li>引入了基于语音时序特征的检索增强生成系统SpeechT-RAG，用于准确检测抑郁症并进行信心评分。</li>
<li>SpeechT-RAG系统不仅提高了检测准确性，还通过信心评分机制实现了不确定性量化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10950">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5342be2d3a8efaf0496ba677e0a5dea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-918ebfe71c307500699ccd51975dbb2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79fdde326683b97166c714eaef667449.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b978cc0092b161e86ff5aef63900a4bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f16421704dd0d28f7a84aa709b6afc80.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Low-Resource-Language-and-Instruction-Following-Capabilities-of-Audio-Language-Models"><a href="#Enhancing-Low-Resource-Language-and-Instruction-Following-Capabilities-of-Audio-Language-Models" class="headerlink" title="Enhancing Low-Resource Language and Instruction Following Capabilities   of Audio Language Models"></a>Enhancing Low-Resource Language and Instruction Following Capabilities   of Audio Language Models</h2><p><strong>Authors:Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul</strong></p>
<p>Audio language models process audio inputs using textual prompts for tasks like speech recognition and audio captioning. Although built on multilingual pre-trained components, most are trained primarily on English, limiting their usability for other languages. This paper evaluates audio language models on Thai, a low-resource language, and finds that they lack emergent cross-lingual abilities despite their multilingual foundations. To address this, we explore data mixtures that optimize audio language models for both a target language and English while integrating audio comprehension and speech instruction-following into a unified model. Our experiments provide insights into improving instruction-following in low-resource languages by balancing language-specific and multilingual training data. The proposed model, Typhoon-Audio, significantly outperforms existing open-source models and achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai. </p>
<blockquote>
<p>音频语言模型使用文本提示来处理音频输入，用于语音识别和音频字幕等任务。尽管这些模型建立在多语言预训练组件之上，但大多数主要使用英语进行训练，从而限制了它们在其他语言中的可用性。本文评估了音频语言模型在泰语（一种资源匮乏的语言）中的应用，发现尽管它们具有多语言基础，但缺乏跨语言的应急能力。为了解决这一问题，我们探索了优化目标语言和英语音频语言模型的数据混合，同时将音频理解和语音指令遵循集成到一个统一模型中。我们的实验通过平衡语言特定和多语言训练数据，为改进低资源语言中的指令遵循提供了见解。所提出的Typhoon-Audio模型显著优于现有开源模型，在英语和泰语方面的性能与最先进的Gemini-1.5-Pro相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10999v2">PDF</a> Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了音频语言模型在处理泰语等低资源语言时的表现。尽管这些模型基于多语言预训练组件，但大多数主要训练于英语，对于其他语言的可用性有限。本研究评估了音频语言模型在泰语中的表现，并发现它们缺乏跨语言的综合能力。为了改善这一情况，研究探索了同时针对目标语言和英语进行优化的数据混合方法，将音频理解和语音指令跟随集成到一个统一模型中。实验结果显示，通过平衡语言特定和多语言训练数据，可以改善低资源语言中的指令跟随能力。提出的Typhoon-Audio模型在泰语和英语上的表现均优于现有开源模型，达到了与Gemini-1.5-Pro相当的水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频语言模型主要用于处理语音任务，如语音识别和音频描述等。</li>
<li>尽管大多数音频语言模型基于多语言预训练组件，但它们主要训练于英语，对其他语言的可用性受限。</li>
<li>在泰语等低资源语言中，音频语言模型缺乏跨语言的综合能力。</li>
<li>通过优化数据混合方法，可以同时针对目标语言和英语改善音频语言模型的表现。</li>
<li>将音频理解和语音指令跟随集成到一个统一模型中有助于提高模型的性能。</li>
<li>实验表明，平衡语言特定和多语言训练数据能改善低资源语言中的指令跟随能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10999">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7c6f30a7f36d8c83b8cad60583c966ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c433cd717b7ce5187239a3547ffc754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecbf15e6a365b2410f0ff489ba5c12b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b0c68bf0223edec2d73c542d10f4766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e76f8515993781c6771d6a0258a6b508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1c6865316f3d7ac82e12c09b809440c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-292b1664fb42724077ce75daf5ccdf1e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-144cb1abbc86db2ae8d174adad83e0fb.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-05-27  CGS-GAN 3D Consistent Gaussian Splatting GANs for High Resolution Human   Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e839f8b0068b9be7112bc5db3bbca7a2.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-05-27  Semantic segmentation with reward
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
