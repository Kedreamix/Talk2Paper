<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Fann or Flop A Multigenre, Multiera Benchmark for Arabic Poetry   Understanding in LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-27-æ›´æ–°"><a href="#2025-05-27-æ›´æ–°" class="headerlink" title="2025-05-27 æ›´æ–°"></a>2025-05-27 æ›´æ–°</h1><h2 id="Fann-or-Flop-A-Multigenre-Multiera-Benchmark-for-Arabic-Poetry-Understanding-in-LLMs"><a href="#Fann-or-Flop-A-Multigenre-Multiera-Benchmark-for-Arabic-Poetry-Understanding-in-LLMs" class="headerlink" title="Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry   Understanding in LLMs"></a>Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry   Understanding in LLMs</h2><p><strong>Authors:Wafa Alghallabi, Ritesh Thawkar, Sara Ghaboura, Ketan More, Omkar Thawakar, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer</strong></p>
<p>Arabic poetry stands as one of the most sophisticated and culturally embedded forms of expression in the Arabic language, known for its layered meanings, stylistic diversity, and deep historical continuity. Although large language models (LLMs) have demonstrated strong performance across languages and tasks, their ability to understand Arabic poetry remains largely unexplored. In this work, we introduce <code>Fann or Flop</code>, the first benchmark designed to assess the comprehension of Arabic poetry by LLMs in twelve historical eras, covering 21 core poetic genres and a variety of metrical forms, from classical structures to contemporary free verse. The benchmark comprises a curated corpus of poems with explanations that assess semantic understanding, metaphor interpretation, prosodic awareness, and cultural context. We argue that poetic comprehension offers a strong indicator for testing how good the LLM is in understanding classical Arabic through the Arabic poetry. Unlike surface-level tasks, this domain demands deeper interpretive reasoning and cultural sensitivity. Our evaluation of state-of-the-art LLMs shows that most models struggle with poetic understanding despite strong results on standard Arabic benchmarks. We release <code>Fann or Flop</code> along with the evaluation suite as an open-source resource to enable rigorous evaluation and advancement for Arabic language models. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/FannOrFlop">https://github.com/mbzuai-oryx/FannOrFlop</a>. </p>
<blockquote>
<p>é˜¿æ‹‰ä¼¯è¯—æ­Œæ˜¯é˜¿æ‹‰ä¼¯è¯­ä¸­æœ€ç²¾è‡´ã€æœ€å¯Œæœ‰æ–‡åŒ–ç‰¹è‰²çš„è¡¨è¾¾å½¢å¼ä¹‹ä¸€ï¼Œä»¥å…¶å¤šå±‚æ¬¡çš„å«ä¹‰ã€é£æ ¼å¤šæ ·æ€§å’Œæ·±åšçš„å†å²è¿ç»­æ€§è€Œé—»åã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤šç§è¯­è¨€å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¯¹é˜¿æ‹‰ä¼¯è¯—æ­Œçš„ç†è§£èƒ½åŠ›ä»åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<code>Fann or Flop&#39;ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMå¯¹é˜¿æ‹‰ä¼¯è¯—æ­Œç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†åäºŒä¸ªå†å²æ—¶æœŸçš„è¯—æ­Œï¼Œæ¶‰åŠ21ä¸ªæ ¸å¿ƒè¯—æ­Œæµæ´¾å’Œå„ç§éŸµå¾‹å½¢å¼ï¼Œä»å¤å…¸ç»“æ„åˆ°å½“ä»£è‡ªç”±è¯—ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬è¯—æ­Œè§£é‡Šï¼Œè¯„ä¼°è¯­ä¹‰ç†è§£ã€éšå–»è§£é‡Šã€éŸµå¾‹æ„è¯†å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¯—æ­Œç†è§£æ˜¯æµ‹è¯•LLMå¯¹å¤å…¸é˜¿æ‹‰ä¼¯è¯­ç†è§£èƒ½åŠ›çš„æœ‰åŠ›æŒ‡æ ‡ã€‚ä¸è¡¨å±‚ä»»åŠ¡ä¸åŒï¼Œè¿™ä¸ªé¢†åŸŸéœ€è¦æ›´æ·±å±‚æ¬¡çš„è§£é‡Šæ¨ç†å’Œæ–‡åŒ–æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„LLMçš„è¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡åœ¨æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­ç»“æœå¼ºåŠ²ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨è¯—æ­Œç†è§£æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬å·²å°†</code>Fann or Flopâ€™åŠå…¶è¯„ä¼°å¥—ä»¶ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä»¥ä¿ƒè¿›é˜¿æ‹‰ä¼¯è¯­è¨€æ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°å’Œè¿›æ­¥ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/FannOrFlop%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mbzuai-oryx/FannOrFlopè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18152v1">PDF</a> Github:<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/FannOrFlop">https://github.com/mbzuai-oryx/FannOrFlop</a>,   Dataset:<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/omkarthawakar/FannOrFlop">https://huggingface.co/datasets/omkarthawakar/FannOrFlop</a></p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œæ˜¯é˜¿æ‹‰ä¼¯è¯­è¡¨è¾¾å½¢å¼ä¸­æœ€ç²¾è‡´ã€æœ€å¯Œæœ‰æ–‡åŒ–å†…æ¶µçš„ä¸€ç§ï¼Œä»¥å…¶å¤šå±‚å«ä¹‰ã€é£æ ¼å¤šæ ·å’Œæ·±åšçš„å†å²è¿ç»­æ€§è€Œè‘—ç§°ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¶ä»–è¯­è¨€å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ç†è§£é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œæ–¹é¢çš„èƒ½åŠ›ä»ç„¶å¾ˆå°‘è¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<code>Fann or Flop&#39;ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMå¯¹é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†åäºŒä¸ªå†å²æ—¶æœŸçš„21ç§æ ¸å¿ƒè¯—æ­Œä½“è£å’Œå¤šç§éŸµå¾‹å½¢å¼ï¼Œä»å¤å…¸ç»“æ„åˆ°å½“ä»£è‡ªç”±è¯—ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬è¯—æ­Œçš„è§£é‡Šï¼Œæ—¨åœ¨è¯„ä¼°è¯­ä¹‰ç†è§£ã€éšå–»è§£é‡Šã€éŸµå¾‹æ„è¯†å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¯—æ­Œç†è§£æ˜¯æµ‹è¯•LLMå¯¹å¤å…¸é˜¿æ‹‰ä¼¯è¯­ç†è§£èƒ½åŠ›çš„æœ‰åŠ›æŒ‡æ ‡ã€‚ä¸åŒäºè¡¨å±‚ä»»åŠ¡ï¼Œè¿™ä¸ªé¢†åŸŸéœ€è¦æ›´æ·±å±‚æ¬¡çš„è§£é‡Šæ¨ç†å’Œæ–‡åŒ–æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„LLMçš„è¯„ä¼°è¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹åœ¨è¯—æ­Œç†è§£æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå°½ç®¡åœ¨æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬å…¬å¼€æ¨å‡ºäº†</code>Fann or Flopâ€™åŠå…¶è¯„ä¼°å¥—ä»¶ä½œä¸ºå¼€æºèµ„æºï¼Œä»¥ä¿ƒè¿›é˜¿æ‹‰ä¼¯è¯­è¨€æ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°å’Œè¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œæ˜¯ä¸€ç§è¡¨è¾¾ä¸°å¯Œã€å†å²æ·±åšçš„è¯­è¨€è‰ºæœ¯å½¢å¼ï¼Œå…·æœ‰å¤šå±‚å«ä¹‰ã€é£æ ¼å¤šæ ·ç­‰ç‰¹ç‚¹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>&#96;Fann or Flopâ€™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°LLMå¯¹é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†å¤šä¸ªå†å²æ—¶æœŸã€è¯—æ­Œä½“è£å’ŒéŸµå¾‹å½¢å¼ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹å¯¹é˜¿æ‹‰ä¼¯è¯­è¯—æ­Œçš„ç†è§£ã€‚</li>
<li>è¯—æ­Œç†è§£éœ€è¦æ·±åšçš„è§£é‡Šæ¨ç†å’Œæ–‡åŒ–æ•æ„Ÿæ€§ï¼Œä¸åŒäºç®€å•çš„è¡¨å±‚ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰LLMåœ¨è¯—æ­Œç†è§£æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°½ç®¡å®ƒä»¬åœ¨æ ‡å‡†é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-08797b122264d95690281c35ec6844aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3abf4aea2924882b94590097781b17b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac0fd03a4bbd705b3c64be25163ef9b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a70c12401a2090fe75eb15c5c367f9db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-687c4e062dd00e0c9d371bc636f0ae38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e419b2e2baffa510145b677ed1672ebb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Lost-in-the-Haystack-Smaller-Needles-are-More-Difficult-for-LLMs-to-Find"><a href="#Lost-in-the-Haystack-Smaller-Needles-are-More-Difficult-for-LLMs-to-Find" class="headerlink" title="Lost in the Haystack: Smaller Needles are More Difficult for LLMs to   Find"></a>Lost in the Haystack: Smaller Needles are More Difficult for LLMs to   Find</h2><p><strong>Authors:Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi</strong></p>
<p>Large language models (LLMs) face significant challenges with needle-in-a-haystack tasks, where relevant information (â€œthe needleâ€) must be drawn from a large pool of irrelevant context (â€œthe haystackâ€). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size has received little attention. We address this gap by systematically studying how variations in gold context length impact LLM performance on long-context question answering tasks. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This pattern holds across three diverse domains (general knowledge, biomedical reasoning, and mathematical reasoning) and seven state-of-the-art LLMs of various sizes and architectures. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†â€œæµ·åº•æé’ˆâ€ä»»åŠ¡æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå³ä»å¤§é‡æ— å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆâ€œå¤§æµ·â€ï¼‰ä¸­æ‰¾å‡ºç›¸å…³ä¿¡æ¯ï¼ˆâ€œé’ˆâ€ï¼‰ã€‚ä»¥å¾€çš„ç ”ç©¶å·²ç»å¼ºè°ƒäº†ä½ç½®åå·®å’Œå¹²æ‰°é¡¹æ•°é‡æ˜¯å½±å“æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œä½†é»„é‡‘ä¸Šä¸‹æ–‡å¤§å°å¯¹æ¨¡å‹çš„å½±å“å´è¢«å¿½è§†ã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶é»„é‡‘ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡é—®ç­”ä»»åŠ¡æ€§èƒ½çš„å½±å“æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚å®éªŒè¡¨æ˜ï¼Œå½“é»„é‡‘ä¸Šä¸‹æ–‡è¾ƒçŸ­æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œå³è¾ƒå°çš„é»„é‡‘ä¸Šä¸‹æ–‡å§‹ç»ˆä¼šé™ä½æ¨¡å‹æ€§èƒ½å¹¶åŠ å‰§ä½ç½®æ•æ„Ÿæ€§ï¼Œè¿™å¯¹å¿…é¡»æ•´åˆåˆ†æ•£ã€ç²¾ç»†çš„å˜é•¿ä¿¡æ¯çš„æ™ºèƒ½ç³»ç»Ÿæå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚è¿™ä¸€æ¨¡å¼åœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸï¼ˆé€šç”¨çŸ¥è¯†ã€ç”Ÿç‰©åŒ»å­¦æ¨ç†å’Œæ•°å­¦æ¨ç†ï¼‰å’Œä¸ƒä¸ªä¸åŒå¤§å°å’Œæ¶æ„çš„å…ˆè¿›å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å‡é€‚ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†æ¸…æ™°çš„è§è§£ï¼Œä¸ºè®¾è®¡ç¨³å¥çš„ã€å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨ç³»ç»Ÿæä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18148v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†â€œä¼—é‡Œå¯»é’ˆâ€çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå³éœ€è¦ä»å¤§é‡æ— å…³å†…å®¹ä¸­æ‰¾å‡ºç›¸å…³ä¿¡æ¯ã€‚è™½ç„¶å‰äººç ”ç©¶å·²æŒ‡å‡ºä½ç½®åå·®å’Œå¹²æ‰°é¡¹æ•°é‡æ˜¯å½±å“æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œä½†é»„é‡‘ä¸Šä¸‹æ–‡è§„æ¨¡çš„å½±å“å´é²œæœ‰ç ”ç©¶ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶é»„é‡‘ä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–å¯¹é•¿ä¸Šä¸‹æ–‡é—®ç­”ä»»åŠ¡ä¸­LLMæ€§èƒ½çš„å½±å“ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚å®éªŒè¡¨æ˜ï¼Œé»„é‡‘ä¸Šä¸‹æ–‡è¾ƒçŸ­æ—¶ï¼ŒLLMæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œå³è¾ƒå°çš„é»„é‡‘ä¸Šä¸‹æ–‡ä¼šä¸€è‡´åœ°é™ä½æ¨¡å‹æ€§èƒ½å¹¶åŠ å‰§ä½ç½®æ•æ„Ÿæ€§ï¼Œè¿™å¯¹äºå¿…é¡»æ•´åˆå„ç§é•¿åº¦åˆ†æ•£ã€ç²¾ç»†ä¿¡æ¯çš„ä»£ç†ç³»ç»Ÿæ˜¯ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ã€‚è¿™ä¸€æ¨¡å¼åœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸï¼ˆé€šç”¨çŸ¥è¯†ã€ç”Ÿç‰©åŒ»å­¦æ¨ç†å’Œæ•°å­¦æ¨ç†ï¼‰å’Œä¸ƒç§ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„å…ˆè¿›LLMä¸­å‡å¾—åˆ°éªŒè¯ã€‚æœ¬ç ”ç©¶ä¸ºè®¾è®¡ç¨³å¥ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„LLMé©±åŠ¨ç³»ç»Ÿæä¾›äº†æ˜ç¡®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†éœ€è¦ä»å¤§é‡æ— å…³å†…å®¹ä¸­æ‰¾å‡ºç›¸å…³ä¿¡æ¯ï¼ˆâ€œä¼—é‡Œå¯»é’ˆâ€ä»»åŠ¡ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>é»„é‡‘ä¸Šä¸‹æ–‡é•¿åº¦å¯¹LLMæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>è¾ƒçŸ­çš„é»„é‡‘ä¸Šä¸‹æ–‡ä¼šå¯¼è‡´LLMæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚</li>
<li>è¾ƒå°çš„é»„é‡‘ä¸Šä¸‹æ–‡ä¼šé™ä½æ¨¡å‹æ€§èƒ½å¹¶åŠ å‰§ä½ç½®æ•æ„Ÿæ€§ã€‚</li>
<li>è¿™ä¸€ç°è±¡åœ¨é€šç”¨çŸ¥è¯†ã€ç”Ÿç‰©åŒ»å­¦æ¨ç†å’Œæ•°å­¦æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸéƒ½å­˜åœ¨ã€‚</li>
<li>ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„å…ˆè¿›LLMéƒ½å—åˆ°äº†è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18148">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8c9fbccaeea956d5925bc852445a2425.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2143d42186150e1dade0d0eb8b955400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-524b67263c34b9510258f61ef9214d8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9a1f15ff73adc9977383edb53dd5479.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de9baecf84f9047833bbf99615e4451c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UNJOIN-Enhancing-Multi-Table-Text-to-SQL-Generation-via-Schema-Simplification"><a href="#UNJOIN-Enhancing-Multi-Table-Text-to-SQL-Generation-via-Schema-Simplification" class="headerlink" title="UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema   Simplification"></a>UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema   Simplification</h2><p><strong>Authors:Poojah Ganesan, Rajat Aayush Jha, Dan Roth, Vivek Gupta</strong></p>
<p>Recent advances in large language models (LLMs) have greatly improved Text-to-SQL performance for single-table queries. But, it remains challenging in multi-table databases due to complex schema and relational operations. Existing methods often struggle with retrieving the right tables and columns, generating accurate JOINs and UNIONs, and generalizing across diverse schemas. To address these issues, we introduce UNJOIN, a two-stage framework that decouples the retrieval of schema elements from SQL logic generation. In the first stage, we merge the column names of all tables in the database into a single-table representation by prefixing each column with its table name. This allows the model to focus purely on accurate retrieval without being distracted by the need to write complex SQL logic. In the second stage, the SQL query is generated on this simplified schema and mapped back to the original schema by reconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and BIRD datasets show that UNJOIN matches or exceeds the state-of-the-art baselines. UNJOIN uses only schema information, which does not require data access or fine-tuning, making it scalable and adaptable across databases. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†å•è¡¨æŸ¥è¯¢çš„æ–‡æœ¬åˆ°SQLæ€§èƒ½ã€‚ç„¶è€Œï¼Œåœ¨å¤šè¡¨æ•°æ®åº“ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå¤æ‚çš„æ¶æ„å’Œå…³ç³»æ“ä½œã€‚ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥æ­£ç¡®æ£€ç´¢è¡¨å’Œæ•°æ®åˆ—ï¼Œç”Ÿæˆå‡†ç¡®çš„JOINså’ŒUNIONsï¼Œå¹¶ä¸”åœ¨å¤šç§æ¶æ„ä¹‹é—´è¿›è¡Œå½’çº³æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UNJOINï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œå®ƒå°†æ¶æ„å…ƒç´ çš„æ£€ç´¢ä¸SQLé€»è¾‘ç”Ÿæˆåˆ†å¼€ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä¸ºæ¯åˆ—æ·»åŠ è¡¨åå‰ç¼€çš„æ–¹å¼ï¼Œå°†æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨ååˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€è¡¨è¡¨ç¤ºå½¢å¼ã€‚è¿™å…è®¸æ¨¡å‹ä¸“æ³¨äºå‡†ç¡®æ£€ç´¢ï¼Œè€Œä¸å—éœ€è¦ç¼–å†™å¤æ‚çš„SQLé€»è¾‘æ‰€å¹²æ‰°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåœ¨æ­¤ç®€åŒ–çš„æ¶æ„ä¸Šç”ŸæˆSQLæŸ¥è¯¢ï¼Œå¹¶é€šè¿‡é‡æ–°æ„å»ºJOINsã€UNIONså’Œå…³ç³»é€»è¾‘å°†å…¶æ˜ å°„å›åŸå§‹æ¶æ„ã€‚åœ¨SPIDERå’ŒBIRDæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒUNJOINè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€æ–°åŸºå‡†æµ‹è¯•æ°´å¹³ã€‚UNJOINä»…ä½¿ç”¨æ¶æ„ä¿¡æ¯ï¼Œæ— éœ€æ•°æ®è®¿é—®æˆ–å¾®è°ƒï¼Œä½¿å…¶å¯è·¨æ•°æ®åº“è¿›è¡Œæ‰©å±•å’Œé€‚åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å‘å•è¡¨æŸ¥è¯¢çš„æ–‡æœ¬è½¬SQLä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨å¤šè¡¨æ•°æ®åº“æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•åœ¨æ£€ç´¢æ­£ç¡®çš„è¡¨å’Œåˆ—ã€ç”Ÿæˆå‡†ç¡®çš„JOINå’ŒUNIONä»¥åŠè·¨ä¸åŒæ¨¡å¼è¿›è¡Œæ³›åŒ–æ–¹é¢çš„å›°éš¾ï¼Œæå‡ºäº†ä¸€ç§åä¸ºUNJOINçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨ååˆ—ååˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€è¡¨è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºå‡†ç¡®çš„æ£€ç´¢ï¼Œè€Œä¸ä¼šå—åˆ°éœ€è¦ç¼–å†™å¤æ‚çš„SQLé€»è¾‘çš„å½±å“ã€‚åœ¨ç®€åŒ–æ¨¡å¼ä¸Šç”ŸæˆSQLæŸ¥è¯¢ï¼Œç„¶åé€šè¿‡é‡å»ºJOINã€UNIONå’Œå…³ç³»é€»è¾‘å°†å…¶æ˜ å°„å›åŸå§‹æ¨¡å¼ã€‚åœ¨SPIDERå’ŒBIRDæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒUNJOINè¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰æœ€æ–°åŸºçº¿ã€‚ç”±äºä»…ä½¿ç”¨æ¨¡å¼ä¿¡æ¯ä¸”æ— éœ€æ•°æ®è®¿é—®æˆ–å¾®è°ƒï¼Œä½¿å…¶å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å‘å•è¡¨æŸ¥è¯¢çš„æ–‡æœ¬è½¬SQLä»»åŠ¡ä¸Šæœ‰æ‰€çªç ´ï¼Œä½†åœ¨å¤šè¡¨æ•°æ®åº“æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šè¡¨æ•°æ®åº“æ—¶é¢ä¸´å›°éš¾ï¼Œå¦‚å‡†ç¡®æ£€ç´¢è¡¨å’Œåˆ—ã€ç”Ÿæˆå¤æ‚çš„JOINå’ŒUNIONæ“ä½œä»¥åŠåœ¨å¤šç§æ¨¡å¼ä¹‹é—´è¿›è¡Œæ³›åŒ–ã€‚</li>
<li>UNJOINæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µæ¥å®ç°ï¼šé¦–å…ˆå°†æ‰€æœ‰è¡¨çš„åˆ—ååˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€è¡¨ç¤ºï¼Œä½¿æ¨¡å‹ä¸“æ³¨äºå‡†ç¡®æ£€ç´¢ï¼›ç„¶ååœ¨ç®€åŒ–æ¨¡å¼ä¸Šç”ŸæˆSQLæŸ¥è¯¢å¹¶æ˜ å°„å›åŸå§‹æ¨¡å¼ã€‚</li>
<li>UNJOINé€šè¿‡å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªé˜¶æ®µå¤„ç†æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>UNJOINåœ¨SPIDERå’ŒBIRDæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°è‰¯å¥½ï¼Œè¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰æœ€æ–°åŸºçº¿ã€‚</li>
<li>UNJOINä»…ä½¿ç”¨æ¨¡å¼ä¿¡æ¯ï¼Œæ— éœ€æ•°æ®è®¿é—®æˆ–å¾®è°ƒï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee3be088021f944a0b46955662271bdf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa7ea29c993d3100ab94fb237b893490.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc25c5ea041489e1bbd6307bb60fa012.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d873d7bb14685d0d5217b13eaf42dcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c43c26418ad6c623baa1e6c114364616.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc22f154d0647686878c733c8a5bb363.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Instructify-Demystifying-Metadata-to-Visual-Instruction-Tuning-Data-Conversion"><a href="#Instructify-Demystifying-Metadata-to-Visual-Instruction-Tuning-Data-Conversion" class="headerlink" title="Instructify: Demystifying Metadata to Visual Instruction Tuning Data   Conversion"></a>Instructify: Demystifying Metadata to Visual Instruction Tuning Data   Conversion</h2><p><strong>Authors:Jacob Hansen, Wei Lin, Junmo Kang, Muhammad Jehanzeb Mirza, Hongyin Luo, Rogerio Feris, Alan Ritter, James Glass, Leonid Karlinsky</strong></p>
<p>Visual Instruction Tuning (VisIT) data, commonly available as human-assistant conversations with images interleaved in the human turns, are currently the most widespread vehicle for aligning strong LLMs to understand visual inputs, converting them to strong LMMs. While many VisIT datasets are available, most are constructed using ad-hoc techniques developed independently by different groups. They are often poorly documented, lack reproducible code, and rely on paid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert image metadata (labels) into VisIT instructions. This leads to high costs and makes it challenging to scale, enhance quality, or generate VisIT data for new datasets. In this work, we address these challenges and propose an open and unified recipe and approach,~\textbf{\method}, for converting available metadata to VisIT instructions using open LLMs. Our multi-stage \method features an efficient framework for metadata grouping, quality control, data and prompt organization, and conversation sampling. We show that our approach can reproduce or enhance the data quality of available VisIT datasets when applied to the same image data and metadata sources, improving GPT-4 generated VisIT instructions by ~3% on average and up to 12% on individual benchmarks using open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our approach enables effective performance scaling - both in quantity and quality - by enhancing the resulting LMM performance across a wide range of benchmarks. We also analyze the impact of various factors, including conversation format, base model selection, and resampling strategies. Our code, which supports the reproduction of equal or higher-quality VisIT datasets and facilities future metadata-to-VisIT data conversion for niche domains, is released at <a target="_blank" rel="noopener" href="https://github.com/jacob-hansen/Instructify">https://github.com/jacob-hansen/Instructify</a>. </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVisITï¼‰æ•°æ®é€šå¸¸ä½œä¸ºäººç±»è¾…åŠ©å¯¹è¯ä¸­å‡ºç°çš„äººæœºäº¤æ›¿å›¾åƒå‘ˆç°ï¼Œæ˜¯ç›®å‰å°†å¼ºå¤§çš„LLMå¯¹é½ä»¥ç†è§£è§†è§‰è¾“å…¥å¹¶å°†å…¶è½¬æ¢ä¸ºå¼ºå¤§çš„LMMçš„æœ€å¹¿æ³›å·¥å…·ã€‚è™½ç„¶æœ‰è®¸å¤šå¯ç”¨çš„VisITæ•°æ®é›†ï¼Œä½†å¤§å¤šæ•°æ˜¯ä½¿ç”¨ä¸åŒå°ç»„ç‹¬ç«‹å¼€å‘çš„ç‰¹å®šæŠ€æœ¯æ„å»ºçš„ã€‚å®ƒä»¬é€šå¸¸æ–‡æ¡£ä¸è¶³ï¼Œç¼ºä¹å¯é‡å¤ä½¿ç”¨çš„ä»£ç ï¼Œå¹¶ä¾èµ–äºä»˜è´¹çš„ã€å°é—­æºä»£ç çš„æ¨¡å‹APIï¼ˆå¦‚GPT-4ã€åŒå­åº§æˆ–Claudeï¼‰å°†å›¾åƒå…ƒæ•°æ®ï¼ˆæ ‡ç­¾ï¼‰è½¬æ¢ä¸ºVisITæŒ‡ä»¤ã€‚è¿™å¯¼è‡´äº†é«˜æ˜‚çš„æˆæœ¬ï¼Œå¹¶ä½¿å¾—æ‰©å±•ã€æé«˜è´¨é‡æˆ–ä¸ºæ–°æ•°æ®é›†ç”ŸæˆVisITæ•°æ®å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶æå‡ºä¸€ç§å¼€æ”¾å’Œç»Ÿä¸€çš„é…æ–¹å’Œæ–¹æ³•â€”â€”\methodï¼Œä½¿ç”¨å¼€æ”¾çš„LLMå°†å¯ç”¨çš„å…ƒæ•°æ®è½¬æ¢ä¸ºVisITæŒ‡ä»¤ã€‚æˆ‘ä»¬çš„å¤šé˜¶æ®µ\methodæ–¹æ³•å…·æœ‰é«˜æ•ˆçš„å…ƒæ•°æ®åˆ†ç»„ã€è´¨é‡æ§åˆ¶ã€æ•°æ®å’Œæç¤ºç»„ç»‡ä»¥åŠå¯¹è¯é‡‡æ ·æ¡†æ¶ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“åº”ç”¨äºç›¸åŒçš„å›¾åƒæ•°æ®å’Œå…ƒæ•°æ®æºæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¤åˆ¶æˆ–æé«˜ç°æœ‰VisITæ•°æ®é›†çš„æ•°æ®è´¨é‡ï¼Œåœ¨ä½¿ç”¨å¼€æ”¾æ¨¡å‹çš„æƒ…å†µä¸‹ï¼ŒGPT-4ç”Ÿæˆçš„VisITæŒ‡ä»¤å¹³å‡æé«˜çº¦3%ï¼Œä¸ªåˆ«åŸºå‡†æµ‹è¯•æœ€é«˜å¯æé«˜12%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æé«˜æ‰€å¾—LMMåœ¨å¹¿æ³›åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ï¼Œå®ç°äº†æ€§èƒ½å’Œè´¨é‡çš„æœ‰æ•ˆæ‰©å±•ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å¯¹è¯æ ¼å¼ã€åŸºç¡€æ¨¡å‹é€‰æ‹©å’Œé‡æ–°é‡‡æ ·ç­–ç•¥ç­‰å„ç§å› ç´ çš„å½±å“ã€‚æˆ‘ä»¬çš„ä»£ç æ”¯æŒå¤åˆ¶åŒç­‰æˆ–æ›´é«˜è´¨é‡çš„VisITæ•°æ®é›†ï¼Œå¹¶ä¸ºä¸“ä¸šé¢†åŸŸæä¾›æœªæ¥å…ƒæ•°æ®åˆ°VisITæ•°æ®çš„è½¬æ¢ï¼Œç°å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jacob-hansen/Instructify%E3%80%82">https://github.com/jacob-hansen/Instructifyã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18115v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºå¤§çš„LLMæ¨¡å‹ç†è§£å’Œæ¥å—è§†è§‰è¾“å…¥çš„èƒ½åŠ›ï¼Œå¯é€šè¿‡VisITæ•°æ®å¯¹å…¶è¿›è¡Œä¼˜åŒ–æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰VisITæ•°æ®é›†å¤šç”±ä¸åŒå›¢é˜Ÿç‹¬ç«‹å¼€å‘æ„å»ºï¼Œå­˜åœ¨æ–‡æ¡£ç¼ºå¤±ã€ç¼ºä¹å¯å¤ç°ä»£ç ã€ä¾èµ–æ”¶è´¹é—­æºæ¨¡å‹APIç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§å¼€æºç»Ÿä¸€çš„æ–¹æ¡ˆâ€”â€”æ–¹æ³•ï¼ˆå…·ä½“æ–¹æ¡ˆåœ¨æ–‡æœ¬ä¸­é˜è¿°ï¼‰ï¼Œåˆ©ç”¨å¼€æºLLMå°†ç°æœ‰å…ƒæ•°æ®è½¬æ¢ä¸ºVisITæŒ‡ä»¤ã€‚è¯¥æ–¹æ³•åŒ…å«å¤šä¸ªé˜¶æ®µï¼ŒåŒ…æ‹¬å…ƒæ•°æ®åˆ†ç»„ã€è´¨é‡æ§åˆ¶ã€æ•°æ®å’Œæç¤ºç»„ç»‡ä»¥åŠå¯¹è¯é‡‡æ ·ç­‰ã€‚åº”ç”¨æ­¤æ–¹æ³•å¯ä»¥æ”¹å–„ç°æœ‰VisITæ•°æ®é›†çš„è´¨é‡ï¼Œä½¿ç”¨å¼€æºæ¨¡å‹æ—¶ï¼Œåœ¨ç›¸åŒå›¾åƒæ•°æ®å’Œå…ƒæ•°æ®æ¥æºä¸Šå¹³å‡æé«˜GPT-4ç”Ÿæˆçš„VisITæŒ‡ä»¤çº¦3%ï¼Œä¸ªåˆ«åŸºå‡†æµ‹è¯•ä¸Šæœ€é«˜å¯æé«˜è¾¾12%ã€‚æ­¤å¤–ï¼Œæ­¤æ–¹æ³•èƒ½æœ‰æ•ˆæå‡æ€§èƒ½è§„æ¨¡ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­æé«˜ç»“æœLLMçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†å¯¹è¯å½¢å¼ã€åŸºç¡€æ¨¡å‹é€‰æ‹©å’Œé‡é‡‡æ ·ç­–ç•¥ç­‰å› ç´ çš„å½±å“ï¼Œå¹¶å…¬å¼€äº†æ”¯æŒå¤ç°æˆ–æ›´é«˜è´¨é‡VisITæ•°æ®é›†çš„å·¥å…·å’Œä»£ç ï¼Œä»¥ä¾¿æœªæ¥è¿›è¡Œå…ƒæ•°æ®åˆ°VisITæ•°æ®çš„è½¬æ¢ï¼Œä»¥é€‚åº”å°ä¼—é¢†åŸŸçš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisITæ•°æ®æ˜¯ä¼˜åŒ–LLMç†è§£å’Œå¤„ç†è§†è§‰è¾“å…¥èƒ½åŠ›çš„ä¸»è¦æ‰‹æ®µã€‚</li>
<li>å½“å‰VisITæ•°æ®é›†å­˜åœ¨æ–‡æ¡£ç¼ºå¤±ã€ç¼ºä¹å¯å¤ç°ä»£ç ã€ä¾èµ–æ”¶è´¹é—­æºæ¨¡å‹ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§å¼€æºç»Ÿä¸€çš„æ–¹æ¡ˆï¼ˆå…·ä½“ç»†èŠ‚åœ¨æ–‡ä¸­é˜è¿°ï¼‰å°†ç°æœ‰å…ƒæ•°æ®è½¬æ¢ä¸ºVisITæŒ‡ä»¤ã€‚</li>
<li>æ­¤æ–¹æ³•å¯ä»¥æ”¹å–„ç°æœ‰VisITæ•°æ®é›†è´¨é‡ï¼Œå¹³å‡æé«˜GPT-4ç”Ÿæˆçš„VisITæŒ‡ä»¤çº¦3%ï¼Œæœ€é«˜å¯è¾¾12%ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æå‡æ€§èƒ½è§„æ¨¡ï¼Œæé«˜LLMåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å¯¹è¯å½¢å¼ã€åŸºç¡€æ¨¡å‹é€‰æ‹©å’Œé‡é‡‡æ ·ç­–ç•¥ç­‰å› ç´ å¯¹è¯¥æ–¹æ³•æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e774b1d61e865fa3cde5ed51eccab3cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-310889755e07d515ca221095741a3c00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4621222bcaa3f347ad2017a727c3408f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e601bbb9f8c2ae38b2d07ec20963172.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff7c108c0d5d2f802282d9d6a355ce13.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ManuSearch-Democratizing-Deep-Search-in-Large-Language-Models-with-a-Transparent-and-Open-Multi-Agent-Framework"><a href="#ManuSearch-Democratizing-Deep-Search-in-Large-Language-Models-with-a-Transparent-and-Open-Multi-Agent-Framework" class="headerlink" title="ManuSearch: Democratizing Deep Search in Large Language Models with a   Transparent and Open Multi-Agent Framework"></a>ManuSearch: Democratizing Deep Search in Large Language Models with a   Transparent and Open Multi-Agent Framework</h2><p><strong>Authors:Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao</strong></p>
<p>Recent advances in web-augmented large language models (LLMs) have exhibited strong performance in complex reasoning tasks, yet these capabilities are mostly locked in proprietary systems with opaque architectures. In this work, we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework designed to democratize deep search for LLMs. ManuSearch decomposes the search and reasoning process into three collaborative agents: (1) a solution planning agent that iteratively formulates sub-queries, (2) an Internet search agent that retrieves relevant documents via real-time web search, and (3) a structured webpage reading agent that extracts key evidence from raw web content. To rigorously evaluate deep reasoning abilities, we introduce \textbf{ORION}, a challenging benchmark focused on open-web reasoning over long-tail entities, covering both English and Chinese. Experimental results show that ManuSearch substantially outperforms prior open-source baselines and even surpasses leading closed-source systems. Our work paves the way for reproducible, extensible research in open deep search systems. We release the data and code in <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ManuSearch">https://github.com/RUCAIBox/ManuSearch</a> </p>
<blockquote>
<p>è¿‘æœŸç½‘ç»œå¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºåŠ²çš„æ€§èƒ½ï¼Œä½†è¿™äº›èƒ½åŠ›å¤§å¤šè¢«é”å®šåœ¨æ¶æ„ä¸é€æ˜çš„ä¸“æœ‰ç³»ç»Ÿä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>ManuSearch</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé€æ˜ä¸”æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°LLMçš„æ·±åº¦æœç´¢æ°‘ä¸»åŒ–ã€‚ManuSearchå°†æœç´¢å’Œæ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªåä½œçš„æ™ºèƒ½ä½“ï¼šï¼ˆ1ï¼‰è§£å†³æ–¹æ¡ˆè§„åˆ’æ™ºèƒ½ä½“ï¼Œå®ƒè¿­ä»£åœ°åˆ¶å®šå­æŸ¥è¯¢ï¼›ï¼ˆ2ï¼‰äº’è”ç½‘æœç´¢æ™ºèƒ½ä½“ï¼Œå®ƒé€šè¿‡å®æ—¶ç½‘é¡µæœç´¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼›ï¼ˆ3ï¼‰ç»“æ„åŒ–ç½‘é¡µé˜…è¯»æ™ºèƒ½ä½“ï¼Œå®ƒä»åŸå§‹ç½‘é¡µå†…å®¹ä¸­æå–å…³é”®è¯æ®ã€‚ä¸ºäº†ä¸¥æ ¼è¯„ä¼°æ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>ORION</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºé•¿å°¾å®ä½“ä¸Šçš„å¼€æ”¾ç½‘é¡µæ¨ç†çš„æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è‹±æ–‡å’Œä¸­æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManuSearchæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å¼€æºåŸºå‡†æµ‹è¯•ï¼Œç”šè‡³è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¼€æ”¾æ·±åº¦æœç´¢ç³»ç»Ÿä¸­çš„å¯é‡å¤å’Œå¯æ‰©å±•ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ManuSearch%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E6%8D%AE%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/RUCAIBox/ManuSearchå…¬å¼€äº†æ•°æ®å’Œä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18105v1">PDF</a> LLM, Complex Search Benchmark</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£ç½‘ç»œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†å¤šæ•°åŠŸèƒ½ä»…é™äºä¸“æœ‰ç³»ç»Ÿä¸”æ¶æ„ä¸é€æ˜ã€‚æœ¬ç ”ç©¶æå‡ºé€æ˜æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ManuSearchï¼Œæ—¨åœ¨å®ç°LLMæ·±åº¦æœç´¢çš„æ°‘ä¸»åŒ–ã€‚ManuSearchå°†æœç´¢å’Œæ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªåä½œæ™ºèƒ½ä½“ï¼šè§£å†³æ–¹æ¡ˆè§„åˆ’æ™ºèƒ½ä½“ã€äº’è”ç½‘æœç´¢æ™ºèƒ½ä½“å’Œç»“æ„åŒ–ç½‘é¡µé˜…è¯»æ™ºèƒ½ä½“ã€‚ä¸ºä¸¥æ ¼è¯„ä¼°æ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥é¢å‘é•¿å°¾å®ä½“çš„å¼€æ”¾ç½‘ç»œæ¨ç†åŸºå‡†æµ‹è¯•ORIONï¼Œæ¶µç›–è‹±æ–‡å’Œä¸­æ–‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒManuSearchæ˜¾è‘—ä¼˜äºå…ˆå‰å¼€æºåŸºå‡†çº¿ï¼Œç”šè‡³è¶…è¶Šé¢†å…ˆé—­æºç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¼€æ”¾æ·±åº¦æœç´¢ç³»ç»Ÿçš„å¯å¤åˆ¶å’Œå¯æ‰©å±•ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§æ€§èƒ½ï¼Œä½†åŠŸèƒ½å±€é™äºä¸“æœ‰ç³»ç»Ÿä¸”æ¶æ„ä¸é€æ˜ã€‚</li>
<li>ManuSearchæ˜¯ä¸€ä¸ªé€æ˜æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°LLMæ·±åº¦æœç´¢çš„æ°‘ä¸»åŒ–ã€‚</li>
<li>ManuSearchåŒ…æ‹¬ä¸‰ä¸ªåä½œæ™ºèƒ½ä½“ï¼šè§£å†³æ–¹æ¡ˆè§„åˆ’ã€äº’è”ç½‘æœç´¢å’Œç»“æ„åŒ–ç½‘é¡µé˜…è¯»æ™ºèƒ½ä½“ã€‚</li>
<li>ä¸ºè¯„ä¼°æ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œå¼•å…¥äº†é¢å‘é•¿å°¾å®ä½“çš„å¼€æ”¾ç½‘ç»œæ¨ç†åŸºå‡†æµ‹è¯•ORIONã€‚</li>
<li>ManuSearchåœ¨å®éªŒä¸­æ˜¾è‘—ä¼˜äºå…ˆå‰å¼€æºåŸºå‡†çº¿ï¼Œç”šè‡³è¶…è¶Šé¢†å…ˆé—­æºç³»ç»Ÿã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºå¼€æ”¾æ·±åº¦æœç´¢ç³»ç»Ÿçš„å¯å¤åˆ¶å’Œå¯æ‰©å±•ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-26ccbfc5522e47e5c20edcd3712ab991.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f60122741de9858cd4ccbbdf19b9ca4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70c31cfd1d18826970e8dacbfbad1abe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="How-Can-I-Publish-My-LLM-Benchmark-Without-Giving-the-True-Answers-Away"><a href="#How-Can-I-Publish-My-LLM-Benchmark-Without-Giving-the-True-Answers-Away" class="headerlink" title="How Can I Publish My LLM Benchmark Without Giving the True Answers Away?"></a>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</h2><p><strong>Authors:Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</strong></p>
<p>Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies. </p>
<blockquote>
<p>åœ¨äº’è”ç½‘ä¸Šå‘å¸ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨æ±¡æŸ“æœªæ¥LLMçš„é£é™©ï¼šåŸºå‡†æµ‹è¯•å¯èƒ½è¢«æ— æ„ï¼ˆæˆ–æ•…æ„ï¼‰ç”¨äºè®­ç»ƒæˆ–é€‰æ‹©æ¨¡å‹ã€‚å¸¸è§çš„ç¼“è§£æ–¹æ³•æ˜¯ä¿æŒåŸºå‡†æµ‹è¯•ç§æœ‰ï¼Œè®©å‚ä¸è€…å‘ç»„ç»‡è€…æäº¤ä»–ä»¬çš„æ¨¡å‹æˆ–é¢„æµ‹ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥éœ€è¦ä¿¡ä»»å•ä¸€ç»„ç»‡ï¼Œå¹¶ä¸”ä»ç„¶å…è®¸é€šè¿‡é‡å¤æŸ¥è¯¢æ¥è¿›è¡Œæµ‹è¯•é›†è¿‡æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‘å¸ƒåŸºå‡†æµ‹è¯•çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€å®Œå…¨å…¬å¼€é—®é¢˜çš„çœŸå®ç­”æ¡ˆï¼ŒåŒæ—¶ä»èƒ½å¤Ÿå…¬å¼€è¯„ä¼°LLMã€‚æˆ‘ä»¬çš„ä¸»è¦æƒ³æ³•æ˜¯é€šè¿‡å‡†å¤‡å¤šä¸ªé€»è¾‘æ­£ç¡®çš„ç­”æ¡ˆå¹¶éšæœºæ³¨å…¥ç­”æ¡ˆï¼Œè€Œåªåœ¨åŸºå‡†æµ‹è¯•ä¸­åŒ…æ‹¬å…¶ä¸­ä¹‹ä¸€ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚è¿™é™ä½äº†åŸºå‡†æµ‹è¯•çš„æœ€ä½³å¯èƒ½å‡†ç¡®åº¦ï¼Œå³è´å¶æ–¯å‡†ç¡®åº¦ã€‚è¿™ä¸ä»…æœ‰åŠ©äºæˆ‘ä»¬é¿å…å…¬å¼€çœŸå®ç­”æ¡ˆï¼Œè€Œä¸”è¿™ç§æ–¹æ³•è¿˜æä¾›äº†ä¸€ç§æ£€æµ‹æ•°æ®æ±¡æŸ“çš„æ–¹æ³•ã€‚åŸåˆ™ä¸Šï¼Œå³ä½¿æ˜¯éå¸¸å®Œå–„çš„æ¨¡å‹ä¹Ÿä¸åº”è¶…è¿‡è´å¶æ–¯å‡†ç¡®åº¦ã€‚å¦‚æœä¸€ä¸ªæ¨¡å‹è¶…è¶Šäº†è¿™ä¸€ä¸Šé™ï¼Œå°½ç®¡æœ‰è¿™æ ·çš„é¢„æœŸï¼Œè¿™ä¹Ÿæ˜¯æ•°æ®æ±¡æŸ“çš„ä¸€ä¸ªå¼ºçƒˆä¿¡å·ã€‚æˆ‘ä»¬æä¾›çš„å®éªŒè¯æ®è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•è®ºä¸­å‡†ç¡®åœ°æ£€æµ‹æ•°æ®æ±¡æŸ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18102v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†åœ¨äº’è”ç½‘ä¸Šå‘å¸ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•çš„é£é™©ï¼Œå¯èƒ½ä¼šæ±¡æŸ“æœªæ¥çš„LLMã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åœ¨åŸºå‡†æµ‹è¯•ä¸­æ³¨å…¥éšæœºæ€§çš„æ–¹æ³•ï¼Œé€šè¿‡å‡†å¤‡å¤šä¸ªé€»è¾‘æ­£ç¡®çš„ç­”æ¡ˆï¼Œåªå°†å…¶ä¸­ä¸€ä¸ªä½œä¸ºåŸºå‡†æµ‹è¯•ä¸­çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤æ–¹æ³•ä¸ä»…æœ‰åŠ©äºä¸é€éœ²çœŸå®ç­”æ¡ˆï¼Œè¿˜æä¾›äº†ä¸€ç§æ£€æµ‹æ•°æ®æ±¡æŸ“çš„æ–¹æ³•ã€‚å®éªŒè¯æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å‡†ç¡®åœ°åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’ŒåŸ¹è®­æ–¹æ³•ä¸Šæ£€æµ‹æ•°æ®æ±¡æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘å¸ƒLLMåŸºå‡†æµ‹è¯•å­˜åœ¨æ±¡æŸ“æœªæ¥æ¨¡å‹çš„é£é™©ã€‚</li>
<li>ä¸€ç§å¸¸è§ç­–ç•¥æ˜¯ä¿æŒåŸºå‡†æµ‹è¯•ç§å¯†ï¼Œè®©å‚ä¸è€…æäº¤æ¨¡å‹æˆ–é¢„æµ‹ç»™ç»„ç»‡è€…ï¼Œä½†éœ€ä¿¡ä»»å•ä¸€ç»„ç»‡å¹¶ä»é¢ä¸´é‡å¤æŸ¥è¯¢å¯¼è‡´æµ‹è¯•é›†è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åœ¨åŸºå‡†æµ‹è¯•ä¸­æ³¨å…¥éšæœºæ€§çš„æ–¹æ³•ï¼Œé€šè¿‡å‡†å¤‡å¤šä¸ªé€»è¾‘æ­£ç¡®ç­”æ¡ˆä¸ºè§£å†³æ–¹æ¡ˆæ¥å…¬å¼€è¯„ä¼°LLMã€‚</li>
<li>è¯¥æ–¹æ³•é™ä½åŸºå‡†æµ‹è¯•çš„æœ€ä½³å‡†ç¡®åº¦ï¼ˆå³è´å¶æ–¯å‡†ç¡®åº¦ï¼‰ï¼Œæœ‰åŠ©äºä¸é€éœ²çœŸå®ç­”æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›æ£€æµ‹æ•°æ®æ±¡æŸ“çš„æ–¹æ³•ï¼Œç†è®ºä¸Šå®Œå…¨æˆç†Ÿçš„æ¨¡å‹ä¸åº”è¶…è¿‡è´å¶æ–¯å‡†ç¡®åº¦ã€‚</li>
<li>è‹¥æ¨¡å‹è¶…è¿‡æ­¤ä¸Šé™ï¼Œåˆ™ä¸ºæ•°æ®æ±¡æŸ“å¼ºçƒˆä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93af4ceb36b386305909028faa3152ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0416b25abb011038b6d2dc07b5ff79af.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding"><a href="#Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding" class="headerlink" title="Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding"></a>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding</h2><p><strong>Authors:Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</strong></p>
<p>Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå…¶åºå¤§çš„æ—¶ç©ºå¤æ‚æ€§å’Œåœ¨è¿™ç§æ‰©å±•èƒŒæ™¯ä¸‹è¿›è¡Œé—®ç­”çš„éš¾åº¦ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘åˆ†æèƒ½åŠ›å’Œé•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†ä¿¡æ¯å¯†é›†ã€æ—¶é•¿ä¸ºä¸€å°æ—¶çš„è§†é¢‘æ—¶ï¼Œå®ƒä»¬ä»ç„¶è¡¨ç°å‡ºä¸€äº›å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦è§†é¢‘å‘ç°ä»£ç†ï¼ˆDVD agentï¼‰ï¼Œé‡‡ç”¨åŸºäºåˆ†å‰²è§†é¢‘ç‰‡æ®µçš„ä»£ç†æœç´¢ç­–ç•¥ã€‚ä¸åŒäºä»¥å‰çš„æ‰‹åŠ¨è®¾è®¡åˆšæ€§å·¥ä½œæµç¨‹çš„è§†é¢‘ä»£ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒäº†ä»£ç†çš„è‡ªä¸»æ€§ã€‚é€šè¿‡åœ¨å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šæä¾›ä¸€ç³»åˆ—ä»¥æœç´¢ä¸ºä¸­å¿ƒçš„å·¥å…·ï¼ŒDVDä»£ç†åˆ©ç”¨LLMçš„é«˜çº§æ¨ç†èƒ½åŠ›æ¥è§„åˆ’å…¶å½“å‰è§‚å¯ŸçŠ¶æ€ï¼Œç­–ç•¥æ€§åœ°é€‰æ‹©å·¥å…·ï¼Œä¸ºè¡ŒåŠ¨åˆ¶å®šé€‚å½“å‚æ•°ï¼Œå¹¶æ ¹æ®æ”¶é›†çš„ä¿¡æ¯è¿­ä»£ä¼˜åŒ–å…¶å†…éƒ¨æ¨ç†ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¯¹ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†æ•´ä¸ªç³»ç»Ÿè®¾è®¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„DVDä»£ç†å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šå¤§å¤§è¶…è¶Šäº†ä»¥å‰çš„å·¥ä½œã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œæ·±å…¥çš„å·¥å…·åˆ†æï¼Œä»¥æ¨åŠ¨é’ˆå¯¹é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„æ™ºèƒ½ä»£ç†çš„è¿›ä¸€æ­¥å‘å±•ã€‚ä»£ç å°†åœ¨ç¨åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18079v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>é•¿è§†é¢‘ç†è§£é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ—¶ç©ºå¤æ‚åº¦å’Œé•¿ä¸Šä¸‹æ–‡ä¸‹çš„é—®é¢˜å›ç­”éš¾åº¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯å¯†é›†å‹é•¿è§†é¢‘æ—¶å±•ç°å‡ºä¼˜åŠ¿ä½†ä»æœ‰é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºDeep Video Discoveryæ™ºèƒ½ä½“è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡çµæ´»çš„ä»£ç†æœç´¢ç­–ç•¥ï¼Œé’ˆå¯¹åˆ†æ®µè§†é¢‘ç‰‡æ®µé‡‡ç”¨è‡ªä¸»å­¦ä¹ ã€‚ä¸ä»¥å¾€æ‰‹åŠ¨è®¾è®¡å·¥ä½œæµç¨‹çš„è§†é¢‘ä»£ç†ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒä»£ç†çš„è‡ªä¸»æ€§ã€‚é€šè¿‡æä¾›å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šçš„æœç´¢ä¸­å¿ƒå·¥å…·é›†ï¼ŒDVDæ™ºèƒ½ä½“åˆ©ç”¨LLMçš„é«˜çº§æ¨ç†èƒ½åŠ›æ¥è§„åˆ’å½“å‰è§‚å¯ŸçŠ¶æ€ï¼Œçµæ´»é€‰æ‹©å·¥å…·ï¼Œåˆ¶å®šé€‚å½“çš„è¡ŒåŠ¨å‚æ•°ï¼Œå¹¶æ ¹æ®æ”¶é›†åˆ°çš„ä¿¡æ¯è¿­ä»£ä¼˜åŒ–å†…éƒ¨æ¨ç†ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†ç³»ç»Ÿè®¾è®¡çš„ä¼˜åŠ¿ã€‚DVDæ™ºèƒ½ä½“è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½æ°´å¹³ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šå¤§å¹…åº¦è¶…è¶Šäº†å…ˆå‰ä½œå“ã€‚åŒæ—¶æä¾›äº†å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œæ·±å…¥çš„å·¥å…·åˆ†æï¼Œä¸ºé’ˆå¯¹é•¿è§†é¢‘ç†è§£ä»»åŠ¡è¿›ä¸€æ­¥å¼€å‘æ™ºèƒ½ä½“æä¾›äº†è§è§£ã€‚ä»£ç å°†éšåå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é•¿è§†é¢‘ç†è§£å­˜åœ¨æ—¶ç©ºå¤æ‚æ€§å’Œé•¿ä¸Šä¸‹æ–‡é—®é¢˜å›ç­”çš„éš¾ç‚¹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯å¯†é›†å‹é•¿è§†é¢‘æ—¶å±•ç°å‡ºä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨é™åˆ¶ã€‚</li>
<li>æå‡ºçš„Deep Video Discoveryæ™ºèƒ½ä½“é‡‡ç”¨çµæ´»çš„ä»£ç†æœç´¢ç­–ç•¥ï¼Œé’ˆå¯¹åˆ†æ®µè§†é¢‘ç‰‡æ®µè¿›è¡Œè‡ªä¸»å­¦ä¹ ã€‚</li>
<li>DVDæ™ºèƒ½ä½“çš„è®¾è®¡å¼ºè°ƒä»£ç†çš„è‡ªä¸»æ€§ï¼Œèƒ½è‡ªä¸»è§„åˆ’è§‚å¯ŸçŠ¶æ€ã€é€‰æ‹©å·¥å…·ã€åˆ¶å®šè¡ŒåŠ¨å‚æ•°å¹¶ä¼˜åŒ–å†…éƒ¨æ¨ç†ã€‚</li>
<li>åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•é›†ä¸Šå…¨é¢è¯„ä¼°ï¼ŒDVDæ™ºèƒ½ä½“è¾¾åˆ°å…ˆè¿›æ€§èƒ½æ°´å¹³ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šï¼ŒDVDæ™ºèƒ½ä½“å¤§å¹…åº¦è¶…è¶Šäº†å…ˆå‰ä½œå“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-57d5cf2b52c28552bab63cb75dcc4d9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-953a2e0735dc1523c356980ffc004edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bb3d4505eeba5bafd3e3b5fe7e6761c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Extended-Inductive-Reasoning-for-Personalized-Preference-Inference-from-Behavioral-Signals"><a href="#Extended-Inductive-Reasoning-for-Personalized-Preference-Inference-from-Behavioral-Signals" class="headerlink" title="Extended Inductive Reasoning for Personalized Preference Inference from   Behavioral Signals"></a>Extended Inductive Reasoning for Personalized Preference Inference from   Behavioral Signals</h2><p><strong>Authors:Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan</strong></p>
<p>Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning\textemdash the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose \textsc{AlignXplore}, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in usersâ€™ interaction histories. We develop \textsc{AlignXplore} by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that \textsc{AlignXplore} achieves substantial improvements over the backbone model by an average of 11.05% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œä¸è¿™äº›ä»¥æ¼”ç»æ¨ç†ä¸ºä¸»çš„ä»»åŠ¡ç›¸æ¯”ï¼Œå…³äºå½’çº³æ¨ç†â€”â€”ä»ä¸å®Œæ•´è¯æ®ä¸­æ¨å¯¼ä¸€èˆ¬è§„åˆ™çš„èƒ½åŠ›â€”â€”çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡ä¸ªæ€§åŒ–åå¥½æ¨æ–­çš„è§†è§’ç ”ç©¶äº†LLMä¸­çš„æ‰©å±•å½’çº³æ¨ç†ï¼Œè¿™æ˜¯LLMå¯¹é½ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºå½“å‰çš„æ–¹æ³•å¾ˆéš¾æ•æ‰å¤šæ ·åŒ–çš„ç”¨æˆ·åå¥½ã€‚è¯¥ä»»åŠ¡éœ€è¦å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå› ä¸ºç”¨æˆ·åå¥½é€šå¸¸éšå«åœ¨å„ç§äº¤äº’å½¢å¼ä¸­ï¼Œéœ€è¦æ¨¡å‹ä»æ•£ä¹±çš„ä¿¡å·ä¸­ç»¼åˆå‡ºä¸€è‡´çš„åå¥½æ¨¡å¼ã€‚\emph{æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º\emph{AlignXplore}çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨æ‰©å±•çš„æ¨ç†é“¾ï¼Œä»ç”¨æˆ·äº¤äº’å†å²çš„è¡Œä¸ºä¿¡å·ä¸­è¿›è¡Œç³»ç»Ÿçš„åå¥½æ¨æ–­ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆåŸºäºåˆæˆæ•°æ®çš„å†·å¯åŠ¨è®­ç»ƒä¸éšåçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¥å¼€å‘\emph{AlignXplore}ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†\emph{AlignXplore}åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†11.05%ï¼ŒåŒæ—¶åœ¨ä¸åŒè¾“å…¥æ ¼å¼å’Œä¸‹æ¸¸æ¨¡å‹ä¸­ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿›ä¸€æ­¥çš„åˆ†æé€šè¿‡ç³»ç»Ÿåœ°æ¯”è¾ƒå¥–åŠ±å»ºæ¨¡ç­–ç•¥ï¼Œå»ºç«‹äº†åå¥½æ¨æ–­å­¦ä¹ çš„æœ€ä½³å®è·µï¼ŒåŒæ—¶æ­ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­äººç±»å¼å½’çº³æ¨ç†æ¨¡å¼çš„å‡ºç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œä¸ä»¥æ¼”ç»æ¨ç†ä¸ºä¸»çš„è¿™äº›ä»»åŠ¡ç›¸æ¯”ï¼Œå…³äºå½’çº³æ¨ç†â€”â€”ä»ä¸å®Œæ•´è¯æ®ä¸­æ¨å¯¼ä¸€èˆ¬è§„åˆ™çš„èƒ½åŠ›â€”â€”çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡ä¸ªæ€§åŒ–åå¥½æ¨ç†çš„è§’åº¦æ¢ç´¢äº†LLMä¸­çš„æ‰©å±•å½’çº³æ¨ç†ï¼Œè¿™æ˜¯LLMå¯¹é½ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰çš„æ–¹æ³•å¾ˆéš¾æ•æ‰ç”¨æˆ·å¤šæ ·åŒ–çš„åå¥½ã€‚ä»»åŠ¡è¦æ±‚æ¨¡å‹å…·å¤‡å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå› ä¸ºç”¨æˆ·åå¥½é€šå¸¸éšå«åœ¨å„ç§äº¤äº’å½¢å¼ä¸­ï¼Œéœ€è¦æ¨¡å‹ä»æ•£ä¹±çš„ä¿¡å·ä¸­åˆæˆä¸€è‡´åå¥½æ¨¡å¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºAlignXploreçš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ‰©å±•æ¨ç†é“¾æ¥å®ç°ä»ç”¨æˆ·äº¤äº’å†å²çš„è¡Œä¸ºä¿¡å·ä¸­è¿›è¡Œç³»ç»Ÿçš„åå¥½æ¨ç†ã€‚æˆ‘ä»¬ç»“åˆåŸºäºåˆæˆæ•°æ®çš„å†·å¯åŠ¨è®­ç»ƒä¸éšåçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¥å¼€å‘AlignXploreã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†AlignXploreç›¸è¾ƒäºåŸºå‡†æ¨¡å‹åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†11.05%çš„æ•ˆæœï¼ŒåŒæ—¶åœ¨ä¸åŒè¾“å…¥æ ¼å¼å’Œä¸‹æ¸¸æ¨¡å‹ä¸­ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿›ä¸€æ­¥çš„åˆ†æå»ºç«‹äº†åå¥½æ¨ç†å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œé€šè¿‡å¥–åŠ±å»ºæ¨¡ç­–ç•¥çš„ç³»ç»Ÿæ¯”è¾ƒï¼Œæ­ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­äººç±»å½’çº³æ¨ç†æ¨¡å¼çš„å‡ºç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å½’çº³æ¨ç†æ–¹é¢ç ”ç©¶ä¸è¶³ã€‚</li>
<li>ä¸ªæ€§åŒ–åå¥½æ¨ç†æ˜¯LLMå¯¹é½çš„å…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰ç”¨æˆ·å¤šæ ·åŒ–çš„åå¥½ã€‚</li>
<li>å½’çº³æ¨ç†åœ¨LLMä¸­å¾ˆé‡è¦ï¼Œå› ä¸ºç”¨æˆ·åå¥½é€šå¸¸éšå«åœ¨å„ç§äº¤äº’å½¢å¼ä¸­ã€‚</li>
<li>æå‡ºäº†AlignXploreæ¨¡å‹ï¼Œç»“åˆå†·å¯åŠ¨è®­ç»ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°ç³»ç»Ÿåå¥½æ¨ç†ã€‚</li>
<li>AlignXploreåœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šè¾ƒåŸºå‡†æ¨¡å‹æœ‰æ˜¾è‘—æé«˜ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºã€‚</li>
<li>æœ€ä½³å®è·µè¡¨æ˜ï¼Œå¥–åŠ±å»ºæ¨¡ç­–ç•¥å¯¹åå¥½æ¨ç†å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94f6381d86d4163715d97d5d80382a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e96b1909409c7753654c68f6342dbe73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0384ceee26fc9099f608e43dc3ce6765.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Emergence-of-Hebbian-Dynamics-in-Regularized-Non-Local-Learners"><a href="#Emergence-of-Hebbian-Dynamics-in-Regularized-Non-Local-Learners" class="headerlink" title="Emergence of Hebbian Dynamics in Regularized Non-Local Learners"></a>Emergence of Hebbian Dynamics in Regularized Non-Local Learners</h2><p><strong>Authors:David Koplow, Tomaso Poggio, Liu Ziyin</strong></p>
<p>Stochastic Gradient Descent (SGD) has emerged as a remarkably effective learning algorithm, underpinning nearly all state-of-the-art machine learning models, from large language models to autonomous vehicles. Despite its practical success, SGD appears fundamentally distinct from biological learning mechanisms. It is widely believed that the biological brain can not implement gradient descent because it is nonlocal, and we have found little (if any) experimental evidence for it. In contrast, the brain is widely thought to learn via local Hebbian learning principles, which have been seen as incompatible with gradient descent. In this paper, we establish a theoretical and empirical connection between the learning signals of neural networks trained using SGD with weight decay and those trained with Hebbian learning near convergence. We show that SGD with regularization can appear to learn according to a Hebbian rule, and SGD with injected noise according to an anti-Hebbian rule. We also provide empirical evidence that Hebbian learning properties can emerge in a network with weight decay from virtually any learning ruleâ€“even random ones. These results may bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles and cautioning against interpreting their presence in neural data as evidence against more complex hetero-synaptic mechanisms. </p>
<blockquote>
<p>éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä½œä¸ºä¸€ç§æå…¶æœ‰æ•ˆçš„å­¦ä¹ ç®—æ³•ï¼Œå·²ç»æˆä¸ºå‡ ä¹æ‰€æœ‰æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„åŸºç¡€ï¼Œä»å¤§å‹è¯­è¨€æ¨¡å‹åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚å°½ç®¡SGDåœ¨å®è·µä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä¼¼ä¹ä¸ç”Ÿç‰©å­¦ä¹ æœºåˆ¶å­˜åœ¨æ ¹æœ¬åŒºåˆ«ã€‚æ™®éè®¤ä¸ºç”Ÿç‰©å¤§è„‘æ— æ³•å®æ–½æ¢¯åº¦ä¸‹é™ï¼Œå› ä¸ºå®ƒæ˜¯éå±€éƒ¨çš„ï¼Œæˆ‘ä»¬å‡ ä¹æ²¡æœ‰ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰å®éªŒè¯æ®å¯ä»¥è¯æ˜è¿™ä¸€ç‚¹ã€‚ç›¸åï¼Œäººä»¬æ™®éè®¤ä¸ºå¤§è„‘é€šè¿‡å±€éƒ¨çš„èµ«å¸ƒå­¦ä¹ åŸç†æ¥å­¦ä¹ ï¼Œè¿™ä¸æ¢¯åº¦ä¸‹é™è¢«è®¤ä¸ºæ˜¯ä¸ç›¸å®¹çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨ç†è®ºä¸Šå’Œå®è¯ä¸Šå»ºç«‹äº†ä½¿ç”¨å¸¦æœ‰æƒé‡è¡°å‡çš„SGDè®­ç»ƒçš„ç¥ç»ç½‘ç»œçš„å­¦ä¹ ä¿¡å·ä¸åœ¨æ”¶æ•›é™„è¿‘ä½¿ç”¨èµ«å¸ƒå­¦ä¹ è®­ç»ƒçš„å­¦ä¹ ä¿¡å·ä¹‹é—´çš„è”ç³»ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¸¦æœ‰æ­£åˆ™åŒ–çš„SGDä¼¼ä¹å¯ä»¥æ ¹æ®èµ«å¸ƒè§„åˆ™æ¥å­¦ä¹ ï¼Œè€Œå¸¦æœ‰æ³¨å…¥å™ªå£°çš„SGDåˆ™æ ¹æ®åèµ«å¸ƒè§„åˆ™æ¥å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜æä¾›å®è¯è¯æ®è¡¨æ˜ï¼Œåœ¨å¸¦æœ‰æƒé‡è¡°å‡çš„ç½‘ç»œä¸­ï¼Œèµ«å¸ƒå­¦ä¹ ç‰¹æ€§å¯ä»¥ä»å‡ ä¹ä»»ä½•å­¦ä¹ è§„åˆ™ä¸­å‡ºç°ï¼Œç”šè‡³æ˜¯éšæœºè§„åˆ™ã€‚è¿™äº›ç»“æœå¯èƒ½å¡«è¡¥äº†äººå·¥æ™ºèƒ½å’Œç”Ÿç‰©å­¦ä¹ ä¹‹é—´çš„é•¿æœŸé¸¿æ²Ÿï¼Œæ­ç¤ºèµ«å¸ƒå±æ€§æ˜¯æ›´æ·±å±‚æ¬¡ä¼˜åŒ–åŸåˆ™çš„å‰¯äº§å“ï¼Œå¹¶è­¦å‘Šäººä»¬ä¸è¦å°†ç¥ç»æ•°æ®ä¸­çš„èµ«å¸ƒå±æ€§è§£é‡Šä¸ºåå¯¹æ›´å¤æ‚çš„å¼‚çªè§¦æœºåˆ¶çš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18069v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰å·²æˆä¸ºä¸€ç§éå¸¸æœ‰æ•ˆçš„å­¦ä¹ ç®—æ³•ï¼Œå‡ ä¹æ”¯æ’‘äº†æ‰€æœ‰æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å°½ç®¡SGDä¸ç”Ÿç‰©å­¦ä¹ æœºåˆ¶å­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œä½†æœ¬æ–‡å»ºç«‹äº†SGDå­¦ä¹ ä¸Hebbianå­¦ä¹ ä¹‹é—´çš„ç†è®ºè”ç³»å’Œå®è¯è”ç³»ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¸¦æœ‰æ­£åˆ™åŒ–çš„SGDä¼¼ä¹éµå¾ªHebbianè§„åˆ™ï¼Œè€Œå¸¦æœ‰æ³¨å…¥å™ªå£°çš„SGDåˆ™éµå¾ªanti-Hebbianè§„åˆ™ã€‚æ­¤å¤–ï¼Œå®è¯è¯æ®è¡¨æ˜ï¼Œå…·æœ‰æƒé‡è¡°å‡çš„ç½‘ç»œå¯ä»¥ä»å‡ ä¹ä»»ä½•å­¦ä¹ è§„åˆ™ä¸­å±•ç°å‡ºHebbianå­¦ä¹ å±æ€§ï¼Œæ­ç¤ºäº†Hebbianå±æ€§æ˜¯æ›´æ·±å±‚æ¬¡ä¼˜åŒ–åŸåˆ™çš„å‰¯äº§å“ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸»è¦å­¦ä¹ ç®—æ³•ï¼Œå¹¿æ³›åº”ç”¨äºå„ç§å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>SGDä¸ç”Ÿç‰©å­¦ä¹ æœºåˆ¶å­˜åœ¨æ ¹æœ¬å·®å¼‚ï¼Œä½†æœ¬æ–‡æ¢è®¨äº†å®ƒä»¬ä¹‹é—´çš„ç†è®ºè”ç³»ã€‚</li>
<li>å¸¦æœ‰æ­£åˆ™åŒ–çš„SGDè¡¨ç°å‡ºçš„å­¦ä¹ ä¸Hebbianå­¦ä¹ è§„åˆ™ç›¸ä¼¼ã€‚</li>
<li>å¸¦æœ‰æ³¨å…¥å™ªå£°çš„SGDè¡¨ç°å‡ºä¸anti-Hebbianè§„åˆ™ç›¸ä¼¼çš„ç‰¹æ€§ã€‚</li>
<li>Hebbianå­¦ä¹ å±æ€§å¯ä»¥åœ¨ç½‘ç»œæƒé‡è¡°å‡è¿‡ç¨‹ä¸­ä»å„ç§å­¦ä¹ è§„åˆ™ä¸­æ˜¾ç°ã€‚</li>
<li>è¿™äº›å‘ç°æ­ç¤ºäº†Hebbianå±æ€§å¯èƒ½æ˜¯æ›´æ·±å±‚æ¬¡ä¼˜åŒ–åŸåˆ™çš„å‰¯äº§å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e8afd5f36a007ba42287ab6607968a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-355f4f7206c274605bd0e40ca32d8fca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86ab5c860fd11bc80056c5949aba4a6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe2e85b7b30139a10ef4cdf2fd8b3f00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58fdea270b0f74a51010448850da60af.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reward-Model-Generalization-for-Compute-Aware-Test-Time-Reasoning"><a href="#Reward-Model-Generalization-for-Compute-Aware-Test-Time-Reasoning" class="headerlink" title="Reward Model Generalization for Compute-Aware Test-Time Reasoning"></a>Reward Model Generalization for Compute-Aware Test-Time Reasoning</h2><p><strong>Authors:Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua</strong></p>
<p>External test-time reasoning enhances large language models (LLMs) by decoupling generation and selection. At inference time, the model generates multiple reasoning paths, and an auxiliary process reward model (PRM) is used to score and select the best one. A central challenge in this setting is test-time compute optimality (TCO), i.e., how to maximize answer accuracy under a fixed inference budget. In this work, we establish a theoretical framework to analyze how the generalization error of the PRM affects compute efficiency and reasoning performance. Leveraging PAC-Bayes theory, we derive generalization bounds and show that a lower generalization error of PRM leads to fewer samples required to find correct answers. Motivated by this analysis, we propose Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically controls search behavior. The actor outputs sampling hyperparameters based on reward distributions and sparsity statistics, while the critic estimates their utility to guide budget allocation. Experiments on the MATH and AIME benchmarks with various LLMs and PRMs demonstrate that CATS consistently outperforms other external TTS methods, validating our theoretical predictions. </p>
<blockquote>
<p>å¤–éƒ¨æµ‹è¯•æ—¶é—´æ¨ç†é€šè¿‡è§£è€¦ç”Ÿæˆå’Œé€‰æ‹©æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶ä½¿ç”¨è¾…åŠ©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æ¥è¯„åˆ†å’Œé€‰æ‹©æœ€ä½³è·¯å¾„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæµ‹è¯•æ—¶é—´è®¡ç®—æœ€ä¼˜åŒ–ï¼ˆTCOï¼‰ï¼Œå³å¦‚ä½•åœ¨å›ºå®šçš„æ¨ç†é¢„ç®—ä¸‹æœ€å¤§åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶æ¥åˆ†æPRMçš„æ³›åŒ–è¯¯å·®å¦‚ä½•å½±å“è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨PAC-Bayesç†è®ºï¼Œæ¨å¯¼å‡ºæ³›åŒ–è¾¹ç•Œï¼Œå¹¶è¯æ˜PRMçš„è¾ƒä½æ³›åŒ–è¯¯å·®ä¼šå¯¼è‡´å¯»æ‰¾æ­£ç¡®ç­”æ¡ˆæ‰€éœ€æ ·æœ¬å‡å°‘ã€‚å—æ­¤åˆ†æå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è®¡ç®—æ„ŸçŸ¥æ ‘æœç´¢ï¼ˆCATSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ§åˆ¶æœç´¢è¡Œä¸ºçš„æ¼”å‘˜è¯„è®ºå®¶æ¡†æ¶ã€‚æ¼”å‘˜æ ¹æ®å¥–åŠ±åˆ†å¸ƒå’Œç¨€ç–ç»Ÿè®¡è¾“å‡ºé‡‡æ ·è¶…å‚æ•°ï¼Œè€Œè¯„è®ºå®¶åˆ™ä¼°è®¡å®ƒä»¬çš„æ•ˆç”¨ä»¥æŒ‡å¯¼é¢„ç®—åˆ†é…ã€‚åœ¨MATHå’ŒAIMEåŸºå‡†æµ‹è¯•ä¸Šå¯¹å¤šç§LLMå’ŒPRMè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCATSå§‹ç»ˆä¼˜äºå…¶ä»–å¤–éƒ¨TTSæ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºé¢„æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18065v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è§£è€¦ç”Ÿæˆå’Œé€‰æ‹©è¿‡ç¨‹å®ç°æµ‹è¯•æ—¶çš„æ¨ç†å¢å¼ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶ä½¿ç”¨è¾…åŠ©å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è¿›è¡Œè¯„åˆ†å’Œé€‰æ‹©æœ€ä½³è·¯å¾„ã€‚æœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œåˆ†æPRMçš„æ³›åŒ–è¯¯å·®å¯¹è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½çš„å½±å“ã€‚åˆ©ç”¨PAC-Bayesç†è®ºï¼Œæˆ‘ä»¬æ¨å¯¼äº†æ³›åŒ–è¾¹ç•Œï¼Œè¡¨æ˜PRMçš„æ³›åŒ–è¯¯å·®é™ä½å¯ä»¥å‡å°‘å¯»æ‰¾æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚å—è¿™ä¸€åˆ†æçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è®¡ç®—æ„ŸçŸ¥æ ‘æœç´¢ï¼ˆCATSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ§åˆ¶æœç´¢è¡Œä¸ºçš„æ¼”å‘˜è¯„è®ºå®¶æ¡†æ¶ã€‚æ¼”å‘˜æ ¹æ®å¥–åŠ±åˆ†å¸ƒå’Œç¨€ç–æ€§ç»Ÿè®¡è¾“å‡ºé‡‡æ ·è¶…å‚æ•°ï¼Œè€Œè¯„è®ºå®¶åˆ™ä¼°è®¡å®ƒä»¬çš„æ•ˆç”¨ï¼Œä»¥æŒ‡å¯¼é¢„ç®—åˆ†é…ã€‚åœ¨MATHå’ŒAIMEåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCATSåœ¨å„ç§LLMå’ŒPRMä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºå…¶ä»–å¤–éƒ¨æµ‹è¯•æ—¶é—´æ¨ç†æ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºé¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤–éƒ¨æµ‹è¯•æ—¶é—´æ¨ç†é€šè¿‡è§£è€¦ç”Ÿæˆå’Œé€‰æ‹©è¿‡ç¨‹å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨è¾…åŠ©å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è¿›è¡Œè¯„åˆ†å’Œé€‰æ‹©æœ€ä½³æ¨ç†è·¯å¾„ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†ç†è®ºæ¡†æ¶åˆ†æPRMæ³›åŒ–è¯¯å·®å¯¹è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åˆ©ç”¨PAC-Bayesç†è®ºæ¨å¯¼æ³›åŒ–è¾¹ç•Œï¼Œæ˜¾ç¤ºé™ä½PRMçš„æ³›åŒ–è¯¯å·®å¯ä»¥å‡å°‘å¯»æ‰¾æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚</li>
<li>æå‡ºè®¡ç®—æ„ŸçŸ¥æ ‘æœç´¢ï¼ˆCATSï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€æ§åˆ¶æœç´¢è¡Œä¸ºï¼ŒåŒ…å«æ¼”å‘˜å’Œè¯„è®ºå®¶çš„æ¡†æ¶è®¾è®¡ã€‚</li>
<li>æ¼”å‘˜åŸºäºå¥–åŠ±åˆ†å¸ƒå’Œç¨€ç–æ€§ç»Ÿè®¡è¾“å‡ºé‡‡æ ·è¶…å‚æ•°ï¼Œè¯„è®ºå®¶åˆ™ä¼°è®¡è¿™äº›è¶…å‚æ•°çš„æ•ˆç”¨ä»¥æŒ‡å¯¼é¢„ç®—åˆ†é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5137683af495b4a3e2635a8dd8530d31.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Assessing-the-performance-of-8-AI-chatbots-in-bibliographic-reference-retrieval-Grok-and-DeepSeek-outperform-ChatGPT-but-none-are-fully-accurate"><a href="#Assessing-the-performance-of-8-AI-chatbots-in-bibliographic-reference-retrieval-Grok-and-DeepSeek-outperform-ChatGPT-but-none-are-fully-accurate" class="headerlink" title="Assessing the performance of 8 AI chatbots in bibliographic reference   retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate"></a>Assessing the performance of 8 AI chatbots in bibliographic reference   retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate</h2><p><strong>Authors:Ãlvaro Cabezas-Clavijo, Pavel Sidorenko-Bautista</strong></p>
<p>This study analyzes the performance of eight generative artificial intelligence chatbots â€“ ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le Chat, and Perplexity â€“ in their free versions, in the task of generating academic bibliographic references within the university context. A total of 400 references were evaluated across the five major areas of knowledge (Health, Engineering, Experimental Sciences, Social Sciences, and Humanities), based on a standardized prompt. Each reference was assessed according to five key components (authorship, year, title, source, and location), along with document type, publication age, and error count. The results show that only 26.5% of the references were fully correct, 33.8% partially correct, and 39.8% were either erroneous or entirely fabricated. Grok and DeepSeek stood out as the only chatbots that did not generate false references, while Copilot, Perplexity, and Claude exhibited the highest hallucination rates. Furthermore, the chatbots showed a greater tendency to generate book references over journal articles, although the latter had a significantly higher fabrication rate. A high degree of overlap was also detected among the sources provided by several models, particularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal structural limitations in current AI models, highlight the risks of uncritical use by students, and underscore the need to strengthen information and critical literacy regarding the use of AI tools in higher education. </p>
<blockquote>
<p>æœ¬ç ”ç©¶åˆ†æäº†å…«ä¸ªç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººâ€”â€”ChatGPTã€Claudeã€Copilotã€DeepSeekã€Geminiã€Grokã€Le Chatå’ŒPerplexityâ€”â€”åœ¨å…¶å…è´¹ç‰ˆæœ¬ä¸­çš„è¡¨ç°ï¼Œåœ¨ç”Ÿæˆå¤§å­¦èƒŒæ™¯ä¸‹çš„å­¦æœ¯å‚è€ƒæ–‡çŒ®çš„ä»»åŠ¡ä¸­ã€‚å…±è¯„ä¼°äº†äº”å¤§é¢†åŸŸï¼ˆå¥åº·ã€å·¥ç¨‹ã€å®éªŒç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡ç§‘å­¦ï¼‰çš„å‚è€ƒæ–‡çŒ®å…±è®¡400ç¯‡ï¼ŒåŸºäºæ ‡å‡†åŒ–æç¤ºè¿›è¡Œè¯„ä¼°ã€‚æ¯ä¸€æ¡å‚è€ƒæ–‡çŒ®æ ¹æ®äº”ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼ˆä½œè€…ã€å¹´ä»½ã€æ ‡é¢˜ã€æ¥æºå’Œä½ç½®ï¼‰ï¼Œä»¥åŠæ–‡æ¡£ç±»å‹ã€å‡ºç‰ˆå¹´ä»£å’Œé”™è¯¯æ•°é‡è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»…æœ‰26.5%çš„å‚è€ƒæ–‡çŒ®å®Œå…¨æ­£ç¡®ï¼Œ33.8%éƒ¨åˆ†æ­£ç¡®ï¼Œè€Œé”™è¯¯æˆ–å®Œå…¨è™šæ„çš„å‚è€ƒæ–‡çŒ®å åˆ°äº†39.8%ã€‚Grokå’ŒDeepSeekæ˜¯å”¯ä¸€ä¸€ä¸ªæ²¡æœ‰ç”Ÿæˆé”™è¯¯å‚è€ƒæ–‡çŒ®çš„èŠå¤©æœºå™¨äººï¼Œè€ŒCopilotã€Perplexityå’ŒClaudeå±•ç°å‡ºæœ€é«˜çš„è™šæ„æŠ¥å‘Šç‡ã€‚æ­¤å¤–ï¼ŒèŠå¤©æœºå™¨äººæ›´å€¾å‘äºç”Ÿæˆä¹¦ç±å‚è€ƒæ–‡çŒ®è€ŒéæœŸåˆŠæ–‡ç« ï¼Œå°½ç®¡åè€…çš„è™šæ„ç‡æ˜¾è‘—æ›´é«˜ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†å¤šä¸ªæ¨¡å‹æä¾›çš„æ¥æºä¹‹é—´å­˜åœ¨å¤§é‡é‡å ï¼Œå°¤å…¶æ˜¯DeepSeekã€Grokã€Geminiå’ŒChatGPTä¹‹é—´ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰äººå·¥æ™ºèƒ½æ¨¡å‹çš„ç»“æ„å±€é™æ€§ï¼Œå‡¸æ˜¾äº†å­¦ç”Ÿæ— æ‰¹åˆ¤æ€§ä½¿ç”¨çš„é£é™©ï¼Œå¹¶å¼ºè°ƒäº†åŠ å¼ºå…³äºé«˜ç­‰æ•™è‚²ä¸­ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·çš„ä¿¡æ¯å’Œæ‰¹åˆ¤æ€§ç´ å…»çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18059v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶åˆ†æäº†å…«ç§å¸¸è§çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººï¼ˆChatGPTã€Claudeã€Copilotã€DeepSeekã€Geminiã€Grokã€Le Chatå’ŒPerplexityï¼‰åœ¨å…¶å…è´¹ç‰ˆæœ¬ä¸­çš„è¡¨ç°ï¼Œä»»åŠ¡æ˜¯åœ¨å¤§å­¦èƒŒæ™¯ä¸‹ç”Ÿæˆå­¦æœ¯å‚è€ƒæ–‡çŒ®ã€‚ç ”ç©¶å¯¹äº”å¤§é¢†åŸŸï¼ˆå¥åº·ã€å·¥ç¨‹ã€å®éªŒç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡ç§‘å­¦ï¼‰çš„400ç¯‡å‚è€ƒæ–‡çŒ®è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŸºäºæ ‡å‡†åŒ–æç¤ºå¯¹æ¯ä¸€å‚è€ƒæ–‡çŒ®çš„ä½œè€…ã€å¹´ä»½ç­‰äº”å¤§ç»„æˆéƒ¨åˆ†è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è¯„ä¼°æ–‡æ¡£ç±»å‹ã€å‡ºç‰ˆå¹´ä»£å’Œé”™è¯¯æ•°é‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»…æœ‰26.5%çš„å‚è€ƒæ–‡çŒ®å®Œå…¨æ­£ç¡®ï¼Œéƒ¨åˆ†æ­£ç¡®å 33.8%ï¼Œé”™è¯¯æˆ–å®Œå…¨è™šæ„çš„å åˆ°äº†39.8%ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä»…æœ‰Grokå’ŒDeepSeekæ²¡æœ‰å‡ºç°é”™è¯¯å‚è€ƒï¼›Copilotç­‰å‡ºç°é«˜å¹»æƒ³é¢‘ç‡ç‡ï¼›å¹¶ä¸”è¡¨ç°å‡ºè¾ƒå¤§çš„åå‘ä¹¦ç±å¼•ç”¨åå¥½è¶…è¿‡æœŸåˆŠæ–‡ç« ã€‚å¦å¤–ï¼Œå‡ ä¸ªæ¨¡å‹é—´å¼•ç”¨çš„æ¥æºæœ‰å¾ˆå¤§é‡å ï¼Œç‰¹åˆ«æ˜¯DeepSeekç­‰æ¨¡å‹é—´ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰äººå·¥æ™ºèƒ½æ¨¡å‹çš„å±€é™æ€§ï¼Œå¼ºè°ƒå­¦ç”Ÿåœ¨ä½¿ç”¨æ—¶éœ€è¦æ³¨æ„æ‰¹åˆ¤æ€§æ€ç»´çš„é‡è¦æ€§ï¼Œä»¥åŠåœ¨é«˜ç­‰æ•™è‚²ä¸­éœ€è¦æé«˜äººå·¥æ™ºèƒ½å·¥å…·çš„ä¿¡æ¯å’Œæ‰¹åˆ¤æ€§ç´ å…»çš„ç´§è¿«æ€§ã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€è¿‘åŠæ•°çš„å­¦æœ¯å‚è€ƒæ–‡çŒ®ç”±AIèŠå¤©æœºå™¨äººç”Ÿæˆçš„ä¿¡æ¯å­˜åœ¨é”™è¯¯æˆ–ä¸å‡†ç¡®ã€‚è¿™äº›é”™è¯¯ä¸»è¦å‘ç”Ÿåœ¨å¼•ç”¨å†…å®¹çš„å„ä¸ªæ–¹é¢å¦‚ä½œè€…åç­‰ä¿¡æ¯çš„å‡†ç¡®åº¦éœ€è¦å¢å¼ºã€‚<br>äºŒã€èŠå¤©æœºå™¨äººç”Ÿæˆçš„å‚è€ƒæ–‡çŒ®ä¸­ä¹¦ç±å¼•ç”¨å¤šäºæœŸåˆŠæ–‡ç« å¼•ç”¨ï¼ŒæœŸåˆŠæ–‡ç« çš„è™šæ„ç‡æ›´é«˜ã€‚è¿™åæ˜ äº†AIæ¨¡å‹åœ¨æ–‡çŒ®ç±»å‹åå¥½ä¸Šçš„å€¾å‘æ€§ã€‚<br>ä¸‰ã€ä¸€äº›AIèŠå¤©æœºå™¨äººåœ¨æ–‡çŒ®ç”Ÿæˆä¸­å±•ç°ç›¸ä¼¼çš„å¼•ç”¨æºä¿¡æ¯å¯èƒ½å­˜åœ¨è¿‡åº¦çš„æ¨¡æ¿åŒ–å’Œå•ä¸€æ•°æ®è®­ç»ƒæƒ…å†µéœ€è¦è¿›ä¸€æ­¥å®Œå–„ç®—æ³•çš„å¤šæ ·æ€§ä»¥æ”¹è¿›æ¨¡å‹ã€‚<br>å››ã€ç›®å‰çš„AIèŠå¤©æ¨¡å‹å¯èƒ½å­˜åœ¨ç»“æ„æ€§çš„é™åˆ¶éœ€è¿›ä¸€æ­¥å¼ºåŒ–æŠ€æœ¯ç ”å‘å’Œæ”¹è¿›ç®—æ³•çš„ç²¾å‡†æ€§å‡å°‘æˆ–é¿å…è‡ªåŠ¨ç”Ÿæˆå‚è€ƒæ–‡çŒ®æ—¶äº§ç”Ÿçš„é”™è¯¯é£é™©ã€‚<br>äº”ã€å­¦ç”Ÿåº”æé«˜åœ¨ä½¿ç”¨AIå·¥å…·æ—¶çš„ä¿¡æ¯æ‰¹åˆ¤ç´ å…»åŠ å¼ºè¯†åˆ«å’Œè¯„ä»·ç”Ÿæˆæ–‡çŒ®çš„å‡†ç¡®æ€§é¿å…è¢«è¯¯å¯¼çš„é£é™©ã€‚<br>å…­ã€åœ¨é«˜æ ¡æ•™è‚²ä¸­åº”åŠ å¼ºå…³äºAIå·¥å…·ä½¿ç”¨çš„æ•™è‚²æŒ‡å¯¼åŸ¹å…»å­¦ç”Ÿçš„æ‰¹åˆ¤æ€§æ€ç»´ç¡®ä¿æ–‡çŒ®è´¨é‡å’Œå­¦æœ¯è¯šä¿¡ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a8fe7b2cf3ce23ec9c68c9ebd733236d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Structured-Thinking-Matters-Improving-LLMs-Generalization-in-Causal-Inference-Tasks"><a href="#Structured-Thinking-Matters-Improving-LLMs-Generalization-in-Causal-Inference-Tasks" class="headerlink" title="Structured Thinking Matters: Improving LLMs Generalization in Causal   Inference Tasks"></a>Structured Thinking Matters: Improving LLMs Generalization in Causal   Inference Tasks</h2><p><strong>Authors:Wentao Sun, Joao Paulo Nogueira, Alonso Silva</strong></p>
<p>Despite remarkable advances in the field, LLMs remain unreliable in distinguishing causation from correlation. Recent results from the Corr2Cause dataset benchmark reveal that state-of-the-art LLMs â€“ such as GPT-4 (F1 score: 29.08) â€“ only marginally outperform random baselines (Random Uniform, F1 score: 20.38), indicating limited capacity of generalization. To tackle this limitation, we propose a novel structured approach: rather than directly answering causal queries, we provide the model with the capability to structure its thinking by guiding the model to build a structured knowledge graph, systematically encoding the provided correlational premises, to answer the causal queries. This intermediate representation significantly enhances the modelâ€™s causal capabilities. Experiments on the test subset of the Corr2Cause dataset benchmark with Qwen3-32B model (reasoning model) show substantial gains over standard direct prompting methods, improving F1 scores from 32.71 to 48.26 (over 47.5% relative increase), along with notable improvements in precision and recall. These results underscore the effectiveness of providing the model with the capability to structure its thinking and highlight its promising potential for broader generalization across diverse causal inference tasks. </p>
<blockquote>
<p>å°½ç®¡è¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒºåˆ†å› æœå…³ç³»ä¸ç›¸å…³æ€§æ–¹é¢ä»å­˜åœ¨ä¸å¯é æ€§ã€‚æ¥è‡ªCorr2Causeæ•°æ®é›†åŸºå‡†æµ‹è¯•çš„æœ€æ–°ç»“æœæ­ç¤ºï¼Œæœ€å…ˆè¿›çš„LLMï¼Œå¦‚GPT-4ï¼ˆF1åˆ†æ•°ï¼š29.08ï¼‰ï¼Œä»…ç•¥å¾®ä¼˜äºéšæœºåŸºçº¿ï¼ˆéšæœºç»Ÿä¸€ï¼ŒF1åˆ†æ•°ï¼š20.38ï¼‰ï¼Œè¿™è¡¨æ˜å…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç»“æ„åŒ–æ–¹æ³•ï¼šæˆ‘ä»¬ä¸æ˜¯ç›´æ¥å›ç­”å› æœæŸ¥è¯¢ï¼Œè€Œæ˜¯ä¸ºæ¨¡å‹æä¾›ç»“æ„æ€è€ƒçš„èƒ½åŠ›ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹æ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼Œç³»ç»Ÿåœ°ç¼–ç æä¾›çš„å…³è”æ€§å‰æï¼Œä»¥å›ç­”å› æœæŸ¥è¯¢ã€‚è¿™ç§ä¸­é—´è¡¨ç¤ºå½¢å¼æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å› æœèƒ½åŠ›ã€‚ä½¿ç”¨Qwen3-32Bæ¨¡å‹ï¼ˆæ¨ç†æ¨¡å‹ï¼‰åœ¨Corr2Causeæ•°æ®é›†åŸºå‡†æµ‹è¯•æµ‹è¯•å­é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„ç›´æ¥æç¤ºæ–¹æ³•ç›¸æ¯”ï¼ŒF1åˆ†æ•°ä»32.71æé«˜åˆ°äº†48.26ï¼ˆç›¸å¯¹å¢å¹…è¶…è¿‡47.5%ï¼‰ï¼Œç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ã€‚è¿™äº›ç»“æœè¯æ˜äº†ä¸ºæ¨¡å‹æä¾›ç»“æ„æ€è€ƒèƒ½åŠ›çš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ›´å¹¿æ³›çš„å› æœæ¨ç†ä»»åŠ¡ä¸­æ³›åŒ–çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18034v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†LLMåœ¨åŒºåˆ†å› æœå…³ç³»å’Œç›¸å…³æ€§æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶åŸºäºCorr2Causeæ•°æ®é›†çš„ç»“æœè¿›è¡Œåˆ†æã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ç»“æ„åŒ–æ–¹æ³•ï¼Œé€šè¿‡æŒ‡å¯¼æ¨¡å‹æ„å»ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±æ¥å›ç­”å› æœæŸ¥è¯¢ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å› æœèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ç›¸è¾ƒäºæ ‡å‡†ç›´æ¥æç¤ºæ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ï¼ŒF1åˆ†æ•°ä»32.71æé«˜åˆ°48.26ï¼Œå¹¶ä¸”ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹Ÿæœ‰æ‰€æé«˜ã€‚è¿™è¡¨æ˜ä¸ºæ¨¡å‹æä¾›ç»“æ„åŒ–æ€è€ƒçš„èƒ½åŠ›æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶å…·æœ‰åœ¨å¹¿æ³›çš„å› æœæ¨ç†ä»»åŠ¡ä¸­æ¨å¹¿çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨åŒºåˆ†å› æœå…³ç³»å’Œç›¸å…³æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Corr2Causeæ•°æ®é›†çš„ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰LLMï¼ˆå¦‚GPT-4ï¼‰åœ¨å› æœæ¨ç†æ–¹é¢çš„è¡¨ç°ä»…ç•¥é«˜äºéšæœºåŸºçº¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç»“æ„åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±æ¥æé«˜æ¨¡å‹çš„å› æœèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•ï¼ˆä½¿ç”¨Qwen3-32Bæ¨¡å‹ï¼‰ç›¸è¾ƒäºç›´æ¥æç¤ºæ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ï¼ŒF1åˆ†æ•°æé«˜47.5%ã€‚</li>
<li>æ–°æ–¹æ³•æé«˜äº†æ¨¡å‹çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚</li>
<li>ä¸ºæ¨¡å‹æä¾›ç»“æ„åŒ–æ€è€ƒçš„èƒ½åŠ›æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13349c2b90ef11bae4995b98bbfa02d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c482bdcf7239ea568139c835e93199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788f1ae082f859244de4e908279be9e3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Modelsâ€™ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight &amp; Knave and MATH datasets demonstrate re-distillationâ€™s surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a> </p>
<blockquote>
<p>åŸºäºR1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€ŒåŸºäºè§„åˆ™çš„RLèƒŒåçš„æœºåˆ¶ä»ç„¶ä¸æ¸…æ¥šã€‚æˆ‘ä»¬å‘ç°å°è§„æ¨¡SFTå¯¹RLæœ‰é‡å¤§å½±å“ï¼Œä½†æ˜¾ç¤ºæ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£é‡Šæˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚å‡è®¾åˆ†æè¡¨æ˜ï¼ŒSFTæ•ˆç‡å—åˆ°è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚åœ¨æˆ‘ä»¬çš„åˆ†ææŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å°è§„æ¨¡è’¸é¦å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ï¼Œè’¸é¦æ¥æºäºRLè®­ç»ƒçš„ç­–ç•¥ã€‚åœ¨Knightå’ŒKnaveä»¥åŠMATHæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å†è’¸é¦çš„æƒŠäººæ•ˆç‡ï¼šå†è’¸é¦çš„æ¨¡å‹ä½¿ç”¨æ›´å°‘çš„æ ·æœ¬å’Œè®¡ç®—é‡åŒ¹é…RLçš„æ€§èƒ½ã€‚ç»éªŒéªŒè¯è¡¨æ˜ï¼Œæ ·æœ¬æ•ˆåº”æ˜¯æ€§èƒ½æ”¹è¿›çš„è‰¯å¥½æŒ‡æ ‡ã€‚å› æ­¤ï¼Œåœ¨Kï¼†Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„å†è’¸é¦Qwen2.5-1.5Bæ¨¡å‹ä»…ä½¿ç”¨1Kä¸ªSFTæ ·æœ¬å°±è¶…è¿‡äº†DeepSeek-V3-0324ã€‚åœ¨MATHä¸Šï¼Œä½¿ç”¨å†è’¸é¦çš„500ä¸ªæ ·æœ¬å¯¹Qwen2.5-1.5Bè¿›è¡Œå¾®è°ƒï¼Œæ— éœ€RLå³å¯åŒ¹é…å…¶æŒ‡ä»¤è°ƒä¼˜ç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£é‡Šäº†R1é£æ ¼RLä¸­çš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v1">PDF</a> 11 figs, 3 table, preprint</p>
<p><strong>Summary</strong></p>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ç ”ç©¶å‘ç°å°è§„æ¨¡çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹RLæœ‰å½±å“ä½†æ•ˆç‡è¾ƒä½ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡è¡¡é‡æ ·æœ¬æ•ˆåº”æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚ç†è®ºåˆ†ææ˜¾ç¤ºSFTæ•ˆç‡å—é™äºè®­ç»ƒæ•°æ®ã€‚åŸºäºåˆ†æï¼Œæå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡å°è§„æ¨¡è’¸é¦æ¥è‡ªRLè®­ç»ƒç­–ç•¥å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜å†è’¸é¦å…·æœ‰å¾ˆé«˜çš„æ•ˆç‡ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬å’Œè®¡ç®—é‡å°±èƒ½è¾¾åˆ°ä¸RLç›¸åŒ¹é…çš„æ€§èƒ½ã€‚å†è’¸é¦æ¨¡å‹åœ¨K&amp;Kæ•°æ®é›†ä¸Šè¶…è¶Šäº†DeepSeek-V3-0324ï¼Œåœ¨MATHæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å†è’¸é¦æŠ€æœ¯è°ƒå‚çš„æ¨¡å‹ä»…éœ€500ä¸ªæ ·æœ¬å°±èƒ½åŒ¹é…æŒ‡ä»¤è°ƒå‚çš„æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°è§„æ¨¡çš„æœ‰ç›‘ç£å¾®è°ƒå¯¹å¼ºåŒ–å­¦ä¹ æœ‰å½±å“ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶æ¥æ¯”è¾ƒæœ‰ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ï¼Œé€šè¿‡è¡¡é‡æ ·æœ¬æ•ˆåº”è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æœ‰ç›‘ç£å¾®è°ƒçš„æ•ˆç‡å—é™äºè®­ç»ƒæ•°æ®ã€‚</li>
<li>æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œèƒ½é«˜æ•ˆåœ°å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŒ¹é…å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>å†è’¸é¦æŠ€æœ¯åœ¨K&amp;Kå’ŒMATHæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55e4e2e8fad8a994a4e5db1c8a677289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bebbc683fc0f91d247784da178d4c77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing"><a href="#HausaNLP-Current-Status-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing" class="headerlink" title="HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing"></a>HausaNLP: Current Status, Challenges and Future Directions for Hausa   Natural Language Processing</h2><p><strong>Authors:Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate</strong></p>
<p>Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org/">https://catalog.hausanlp.org</a>), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè±ªè¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå°½ç®¡ä½œä¸ºèµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œå®ƒä»æœ‰è¶…è¿‡ä¸€äº¿ä¸¤åƒä¸‡çš„ç¬¬ä¸€è¯­è¨€ï¼ˆL1ï¼‰ä½¿ç”¨è€…ä»¥åŠå…«åƒä¸‡çš„ç¬¬äºŒè¯­è¨€ï¼ˆL2ï¼‰ä½¿ç”¨è€…åˆ†å¸ƒåœ¨å…¨çƒå„åœ°ã€‚è™½ç„¶èµ„æºä¸°å¯Œçš„è¯­è¨€å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è±ªè¨NLPä»ç„¶é¢ä¸´ç€æŒç»­æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™å¼€æºæ•°æ®é›†å’Œä¸è¶³çš„æ¨¡å‹è¡¨ç¤ºã€‚æœ¬æ–‡æ¦‚è¿°äº†è±ªè¨NLPçš„å½“å‰çŠ¶æ€ï¼Œç³»ç»Ÿåœ°æ£€æŸ¥äº†ç°æœ‰èµ„æºã€ç ”ç©¶è´¡çŒ®ä»¥åŠè·¨åŸºæœ¬NLPä»»åŠ¡çš„å·®è·ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®ç­”ã€‚æˆ‘ä»¬ä»‹ç»äº†è±ªè¨NLPï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),è¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„ç›®å½•,æ±‡é›†äº†æ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶æˆæœ,ä»¥æé«˜å¯è®¿é—®æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥å‘å±•.æ­¤å¤–,æˆ‘ä»¬è®¨è®ºäº†å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹(llm)ä¸­çš„æŒ‘æˆ˜,è§£å†³æ¬¡ä¼˜åˆ†è¯å’Œæ–¹è¨€å˜åŒ–çš„é—®é¢˜.æœ€å,æˆ‘ä»¬æå‡ºäº†æˆ˜ç•¥ç ”ç©¶æ–¹å‘,å¼ºè°ƒæ•°æ®é›†æ‰©å±•ã€æ”¹è¿›çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œä»¥æ¨åŠ¨è±ªè¨nlpçš„å‘å±•.æˆ‘ä»¬çš„å·¥ä½œä¸ºåŠ é€Ÿè±ªè¨nlpçš„è¿›æ­¥æä¾›äº†åŸºç¡€,å¹¶ä¸ºæ›´å¹¿æ³›çš„å¤šå…ƒè¯­è¨€nlpç ”ç©¶æä¾›äº†å®è´µçš„è§è§£./">https://catalog.hausanlp.orgï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„ç›®å½•ï¼Œæ±‡é›†äº†æ•°æ®é›†ã€å·¥å…·å’Œç ”ç©¶æˆæœï¼Œä»¥æé«˜å¯è®¿é—®æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºäº†å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œè§£å†³æ¬¡ä¼˜åˆ†è¯å’Œæ–¹è¨€å˜åŒ–çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†æˆ˜ç•¥ç ”ç©¶æ–¹å‘ï¼Œå¼ºè°ƒæ•°æ®é›†æ‰©å±•ã€æ”¹è¿›çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œä»¥æ¨åŠ¨è±ªè¨NLPçš„å‘å±•ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºåŠ é€Ÿè±ªè¨NLPçš„è¿›æ­¥æä¾›äº†åŸºç¡€ï¼Œå¹¶ä¸ºæ›´å¹¿æ³›çš„å¤šå…ƒè¯­è¨€NLPç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14311v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†è±ªè¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç°çŠ¶å’ŒæŒ‘æˆ˜ã€‚å°½ç®¡è±ªè¨è¯­æ‹¥æœ‰è¶…è¿‡ä¸€äº¿çš„ç¬¬ä¸€è¯­è¨€å’Œç¬¬äºŒè¯­è¨€ä½¿ç”¨è€…ï¼Œä½†å…¶ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æ¦‚è¿°äº†è±ªè¨NLPçš„ç°æœ‰èµ„æºã€ç ”ç©¶è´¡çŒ®å’Œç¼ºå£ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®ç­”ç­‰åŸºæœ¬çš„NLPä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†è±ªè¨NLPçš„é›†æˆæŒ‘æˆ˜å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºé€šè¿‡æ‰©å……æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œæ¥ä¿ƒè¿›è±ªè¨NLPçš„å‘å±•ï¼ŒåŒæ—¶ä¹Ÿä¸ºæ›´å¹¿æ³›çš„å¤šè¯­è¨€NLPç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬å»ºç«‹äº†è±ªè¨NLPç›®å½•ï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),ä»¥æ¨åŠ¨å…¶å‘å±•å’Œä½¿ç”¨./">https://catalog.hausanlp.orgï¼‰ï¼Œä»¥æ¨åŠ¨å…¶å‘å±•å’Œä½¿ç”¨ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è±ªè¨è¯­æ‹¥æœ‰è¶…è¿‡ä¸€äº¿çš„ä½¿ç”¨è€…ï¼Œä½†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>è±ªè¨NLPé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æœ‰é™çš„å¼€æºæ•°æ®é›†å’Œä¸è¶³çš„æ¨¡å‹è¡¨ç¤ºã€‚</li>
<li>æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œé—®ç­”æ˜¯è±ªè¨NLPé¢ä¸´çš„åŸºæœ¬ä»»åŠ¡æŒ‘æˆ˜ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªè±ªè¨NLPç›®å½•ï¼ˆ<a target="_blank" rel="noopener" href="https://catalog.hausanlp.org),ä»¥æ¨åŠ¨å…¶å‘å±•å’Œä½¿ç”¨./">https://catalog.hausanlp.orgï¼‰ï¼Œä»¥æ¨åŠ¨å…¶å‘å±•å’Œä½¿ç”¨ã€‚</a></li>
<li>åœ¨å°†è±ªè¨è¯­é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ï¼Œå­˜åœ¨æ¬¡ä¼˜çš„åˆ†è¯å’Œæ–¹è¨€å·®å¼‚é—®é¢˜ã€‚</li>
<li>æ‰©å……æ•°æ®é›†ã€æ”¹è¿›è¯­è¨€å»ºæ¨¡æ–¹æ³•å’ŒåŠ å¼ºç¤¾åŒºåˆä½œæ˜¯æ¨è¿›è±ªè¨NLPå‘å±•çš„æˆ˜ç•¥æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b44a57b070982a564e95c053fe9cf62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42759b51c5122b9d93aca1f303c8c6fd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Hogwild-Inference-Parallel-LLM-Generation-via-Concurrent-Attention"><a href="#Hogwild-Inference-Parallel-LLM-Generation-via-Concurrent-Attention" class="headerlink" title="Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"></a>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</h2><p><strong>Authors:Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh</strong></p>
<p>Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM â€œworkersâ€ in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while â€œseeingâ€ each otherâ€™s memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with â€œinstantâ€ access to each otherâ€™s memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºé€šè¿‡é«˜çº§æ¨ç†ã€é•¿å½¢å¼å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨æ¥å¤„ç†æ—¥ç›Šå¤æ‚çš„ä»»åŠ¡çš„èƒ½åŠ›ã€‚è§£å†³è¿™äº›ä»»åŠ¡é€šå¸¸æ¶‰åŠé•¿æ—¶é—´çš„æ¨ç†è®¡ç®—ã€‚åœ¨äººç±»çš„é—®é¢˜è§£å†³ä¸­ï¼ŒåŠ é€Ÿå·¥ä½œçš„å¸¸ç”¨ç­–ç•¥æ˜¯åä½œï¼šå°†é—®é¢˜åˆ’åˆ†ä¸ºå­ä»»åŠ¡ï¼ŒåŒæ—¶æ¢ç´¢ä¸åŒçš„ç­–ç•¥ç­‰ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMä¹Ÿå¯ä»¥é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶ï¼Œå¦‚æŠ•ç¥¨æœºåˆ¶æˆ–åˆ›å»ºå¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„ç‹¬ç«‹å­ä»»åŠ¡ï¼Œæ¥å¹¶è¡Œæ“ä½œã€‚ç„¶è€Œï¼Œè¿™äº›æ¡†æ¶å¯èƒ½å¹¶ä¸é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢å…¶é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸åŒçš„è®¾è®¡æ€è·¯ï¼šæˆ‘ä»¬å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œå…è®¸å®ƒä»¬é€šè¿‡å®æ—¶æ›´æ–°çš„å…³æ³¨ç¼“å­˜è¿›è¡ŒåŒæ­¥ï¼Œå¹¶æç¤ºè¿™äº›å·¥ä½œè€…å†³å®šæœ€ä½³çš„åä½œæ–¹å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸LLMå®ä¾‹é’ˆå¯¹æ‰‹å¤´é—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼ŒåŒæ—¶â€œæŸ¥çœ‹â€å½¼æ­¤åœ¨å¹¶å‘KVç¼“å­˜ä¸­çš„è®°å¿†ã€‚æˆ‘ä»¬é€šè¿‡â€œHogwildï¼æ¨ç†â€å®ç°äº†è¿™ç§æ–¹æ³•ï¼šä¸€ä¸ªå¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå…¶ä¸­å¤šä¸ªç›¸åŒçš„LLMå®ä¾‹å¹¶è¡Œè¿è¡Œï¼Œä½¿ç”¨ç›¸åŒçš„å…³æ³¨ç¼“å­˜ï¼Œå¯ä»¥â€œå³æ—¶â€è®¿é—®å½¼æ­¤çš„ç¼“å­˜ã€‚Hogwildï¼æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥é¿å…é‡æ–°è®¡ç®—ï¼ŒåŒæ—¶æé«˜å¹¶è¡Œç¡¬ä»¶çš„åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°ä»£å…·æœ‰æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨å…±äº«é”®å€¼ç¼“å­˜ä¸­ç›´æ¥è¿›è¡Œæ¨ç†ï¼Œæ— éœ€é¢å¤–çš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06261v3">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é«˜çº§æ¨ç†ã€é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨ã€‚é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶ï¼Œå¦‚æŠ•ç¥¨æœºåˆ¶æˆ–åˆ›å»ºå¯å¹¶è¡Œæ‰§è¡Œçš„ç‹¬ç«‹å­ä»»åŠ¡ï¼ŒLLMä¹Ÿå¯ä»¥è¿›è¡Œå¹¶è¡Œæ“ä½œã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•ï¼Œå³å¹¶è¡Œè¿è¡ŒLLMâ€œå·¥ä½œè€…â€ï¼Œé€šè¿‡åŒæ­¥æ›´æ–°çš„å…³æ³¨ç¼“å­˜æ¥æç¤ºè¿™äº›å·¥ä½œè€…åˆ¶å®šæœ€ä½³çš„åä½œæ–¹å¼ã€‚è¿™ç§æ–¹æ³•å…è®¸LLMå®ä¾‹ä¸ºæ‰‹å¤´é—®é¢˜åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼ŒåŒæ—¶â€œæŸ¥çœ‹â€å½¼æ­¤çš„è®°å¿†ã€‚æˆ‘ä»¬å®ç°äº†è¿™ä¸€æ–¹æ³•ï¼Œé€šè¿‡éœæ ¼å¨å°”ï¼ˆHogwildï¼‰æ¨ç†ï¼šä¸€ç§å¹¶è¡ŒLLMæ¨ç†å¼•æ“ï¼Œå…¶ä¸­åŒä¸€LLMçš„å¤šä¸ªå®ä¾‹å¹¶è¡Œè¿è¡Œï¼Œå…±äº«å…³æ³¨ç¼“å­˜ï¼Œå¯ä»¥å³æ—¶è®¿é—®å½¼æ­¤çš„å­˜å‚¨å™¨ã€‚éœæ ¼å¨å°”æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰é¿å…é‡æ–°è®¡ç®—ï¼Œæé«˜å¹¶è¡Œç¡¬ä»¶åˆ©ç”¨ç‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°ä»£å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMå¯ä»¥ä½¿ç”¨å…±äº«é”®å€¼ç¼“å­˜è¿›è¡Œæ¨ç†ï¼Œæ— éœ€é¢å¤–å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬é«˜çº§æ¨ç†ã€é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆç­‰ã€‚</li>
<li>LLMså¯ä»¥é€šè¿‡å®æ–½æ˜ç¡®çš„åˆä½œæ¡†æ¶è¿›è¡Œå¹¶è¡Œæ“ä½œã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡æ–¹æ³•ï¼Œå…è®¸LLMå®ä¾‹åˆ¶å®šè‡ªå·±çš„åä½œç­–ç•¥ï¼Œå¹¶å…±äº«è®°å¿†ã€‚</li>
<li>è¿™ç§æ–¹æ³•é€šè¿‡éœæ ¼å¨å°”ï¼ˆHogwildï¼‰æ¨ç†å¼•æ“å®ç°ï¼Œå¤šä¸ªLLMå®ä¾‹å¯ä»¥å¹¶è¡Œè¿è¡Œå¹¶å…±äº«å…³æ³¨ç¼“å­˜ã€‚</li>
<li>éœæ ¼å¨å°”æ¨ç†åˆ©ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥æé«˜æ¨ç†æ•ˆç‡å’Œç¡¬ä»¶åˆ©ç”¨ç‡ã€‚</li>
<li>ç°ä»£å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMå¯ä»¥ä½¿ç”¨å…±äº«é”®å€¼ç¼“å­˜è¿›è¡Œæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27714454c97d278149f4ff1ac9408171.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-285d941d2606b5778d8d5d4c9df70989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5890cee5a9d3bdbb776dcfc47b0f3ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ca1b1dc6a31d16a4b3d10aea865d760.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Little-Depth-Goes-a-Long-Way-The-Expressive-Power-of-Log-Depth-Transformers"><a href="#A-Little-Depth-Goes-a-Long-Way-The-Expressive-Power-of-Log-Depth-Transformers" class="headerlink" title="A Little Depth Goes a Long Way: The Expressive Power of Log-Depth   Transformers"></a>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth   Transformers</h2><p><strong>Authors:William Merrill, Ashish Sabharwal</strong></p>
<p>Recent theoretical results show transformers cannot express sequential reasoning problems over long inputs, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformerâ€™s depth affects its expressive power. We address these questions by analyzing transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities and was known to be expressible only by an unconventional, non-uniform model of transformers, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, our detailed experiments designed to bridge the expressivity vs. learnability gap reveal that our theoretical depth requirements for regular language recognition closely match the practical depth requirements for successfully training transformers. Thus, our results clarify how depth affects a transformerâ€™s reasoning capabilities, and provide practical guidance for effective depth selection for sequential reasoning. </p>
<blockquote>
<p>æœ€è¿‘çš„ç†è®ºç»“æœè¡¨æ˜ï¼Œå˜å‹å™¨æ— æ³•è¡¨è¾¾é•¿è¾“å…¥ä¸Šçš„åºåˆ—æ¨ç†é—®é¢˜ï¼Œç›´è§‚ä¸Šæ˜¯å› ä¸ºå…¶è®¡ç®—æ·±åº¦æ˜¯æœ‰é™çš„ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶å°†æ·±åº¦è§†ä¸ºå¸¸æ•°ï¼Œå°šä¸æ˜ç¡®æœ‰é™æ·±åº¦åœ¨è§£å†³çŸ­è¾“å…¥é—®é¢˜ä¸Šèƒ½èµ·åˆ°çš„ç¨‹åº¦ï¼Œæˆ–è€…å¢åŠ å˜å‹å™¨çš„æ·±åº¦å¦‚ä½•å½±å“å…¶è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ææ·±åº¦å¯ä»¥éšä¸Šä¸‹æ–‡é•¿åº¦nå¾®å°å¢é•¿çš„å˜å‹å™¨æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿æ˜¯æ·±åº¦ä¸ºÎ˜(logn)çš„é«˜åº¦å‡åŒ€å˜å‹å™¨ï¼Œä¹Ÿèƒ½è¡¨è¾¾ä¸¤ä¸ªé‡è¦é—®é¢˜ï¼šè¯†åˆ«æ­£åˆ™è¯­è¨€ï¼Œè¿™æ¶‰åŠåˆ°çŠ¶æ€è·Ÿè¸ªèƒ½åŠ›ï¼Œå¹¶ä¸”å·²çŸ¥åªæœ‰éä¼ ç»Ÿçš„éå‡åŒ€å˜å‹å™¨æ¨¡å‹æ‰èƒ½è¡¨è¾¾ï¼›ä»¥åŠå›¾è¿é€šæ€§ï¼Œè¿™æ˜¯å¤šæ­¥æ¨ç†çš„åŸºç¡€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®æ ‡å‡†å¤æ‚æ€§çŒœæƒ³ï¼Œè¿™ä¸¤ä¸ªé—®é¢˜éƒ½ä¸èƒ½ç”±å›ºå®šæ·±åº¦çš„å˜å‹å™¨æ¥è¡¨è¾¾ï¼Œè¿™è¯æ˜äº†å¢é•¿æ·±åº¦åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç†è®ºå®šé‡é¢„æµ‹äº†æ·±åº¦å¿…é¡»å¦‚ä½•éšè¾“å…¥é•¿åº¦å¢é•¿æ‰èƒ½è¡¨è¾¾è¿™äº›é—®é¢˜ï¼Œè¡¨æ˜æ·±åº¦ç¼©æ”¾æ¯”å®½åº¦ç¼©æ”¾æˆ–æ€ç»´é“¾æ­¥éª¤æ›´ä¸ºé«˜æ•ˆã€‚ä»å®è¯è§’åº¦çœ‹ï¼Œæˆ‘ä»¬è®¾è®¡çš„è¯¦ç»†å®éªŒæ—¨åœ¨å¼¥è¾¾è¡¨ç°èƒ½åŠ›ä¸å­¦ä¹ èƒ½åŠ›ä¹‹é—´çš„å·®è·ï¼Œæ­ç¤ºå‡ºç†è®ºä¸Šå¯¹äºæ­£åˆ™è¯­è¨€è¯†åˆ«çš„æ·±åº¦éœ€æ±‚ä¸æˆåŠŸè®­ç»ƒå˜å‹å™¨æ‰€éœ€çš„å®é™…æ·±åº¦éœ€æ±‚ç›¸å»åˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç»“æœæ˜ç¡®äº†æ·±åº¦å¦‚ä½•å½±å“å˜å‹å™¨çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸ºæœ‰æ•ˆçš„æ·±åº¦é€‰æ‹©æä¾›äº†å®é™…æŒ‡å¯¼ï¼Œä»¥è¿›è¡Œåºåˆ—æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03961v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å˜å‹å™¨çš„æ·±åº¦å¯¹å…¶è§£å†³é•¿è¾“å…¥åºåˆ—æ¨ç†é—®é¢˜çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿å¯¹äºæ·±åº¦ä¸ºO(logâ¡n)çš„å‡åŒ€å˜å‹å™¨ï¼Œä¹Ÿèƒ½è¡¨è¾¾ä¸¤ç§é‡è¦é—®é¢˜ï¼šè¯†åˆ«æ­£åˆ™è¯­è¨€å’Œå›¾å½¢è¿æ¥é—®é¢˜ã€‚è¿™è¡¨æ˜å¢åŠ æ·±åº¦æœ‰åŠ©äºæé«˜å˜å‹å™¨çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè€Œä¸”ç†è®ºé¢„æµ‹æ·±åº¦éšè¾“å…¥é•¿åº¦çš„å¢é•¿æ–¹å¼æ¯”å®½åº¦æˆ–æ€ç»´é“¾æ­¥éª¤æ›´æœ‰æ•ˆã€‚å®éªŒè¡¨æ˜ï¼Œç†è®ºæ·±åº¦è¦æ±‚ä¸æˆåŠŸè®­ç»ƒå˜å‹å™¨çš„å®é™…æ·±åº¦è¦æ±‚ç›¸åŒ¹é…ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ˜ç¡®äº†æ·±åº¦å¯¹å˜å‹å™¨æ¨ç†èƒ½åŠ›çš„å½±å“ï¼Œä¸ºå®é™…é€‰æ‹©æœ‰æ•ˆæ·±åº¦æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜å‹å™¨åœ¨å¤„ç†é•¿è¾“å…¥åºåˆ—æ¨ç†é—®é¢˜æ—¶å­˜åœ¨è¡¨è¾¾èƒ½åŠ›çš„å±€é™æ€§ã€‚</li>
<li>å¢åŠ å˜å‹å™¨çš„æ·±åº¦å¯ä»¥æé«˜å…¶è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>å¯¹äºæ·±åº¦ä¸ºO(logâ¡n)çš„å‡åŒ€å˜å‹å™¨ï¼Œèƒ½è¡¨è¾¾è¯†åˆ«æ­£åˆ™è¯­è¨€å’Œå›¾å½¢è¿æ¥ç­‰é‡è¦é—®é¢˜ã€‚</li>
<li>ç†è®ºé¢„æµ‹è¡¨æ˜ï¼Œæ·±åº¦éšè¾“å…¥é•¿åº¦çš„å¢é•¿æ–¹å¼æ¯”å®½åº¦æˆ–æ€ç»´é“¾æ­¥éª¤æ›´æœ‰æ•ˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç†è®ºæ·±åº¦è¦æ±‚ä¸æˆåŠŸè®­ç»ƒå˜å‹å™¨çš„å®é™…æ·±åº¦è¦æ±‚ç›¸åŒ¹é…ã€‚</li>
<li>æœ¬æ–‡æ˜ç¡®äº†æ·±åº¦å¯¹å˜å‹å™¨æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d613a8089f348f817124278076d60f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d5d07ad75b29df63202be29f515e55.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Is-Your-Paper-Being-Reviewed-by-an-LLM-Benchmarking-AI-Text-Detection-in-Peer-Review"><a href="#Is-Your-Paper-Being-Reviewed-by-an-LLM-Benchmarking-AI-Text-Detection-in-Peer-Review" class="headerlink" title="Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection   in Peer Review"></a>Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection   in Peer Review</h2><p><strong>Authors:Sungduk Yu, Man Luo, Avinash Madusu, Vasudev Lal, Phillip Howard</strong></p>
<p>Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews fully written by humans and different state-of-the-art LLMs. Additionally, we explore a context-aware detection method called Anchor, which leverages manuscript content to detect AI-generated reviews, and analyze the sensitivity of detection models to LLM-assisted editing of human-written text. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI. Our dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark">https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark</a>. </p>
<blockquote>
<p>åŒè¡Œè¯„å®¡æ˜¯ç¡®ä¿å·²å‘è¡¨ç§‘å­¦ç ”ç©¶å®Œæ•´æ€§çš„å…³é”®è¿‡ç¨‹ã€‚è¯¥è¿‡ç¨‹çš„ä¿¡å¿ƒå»ºç«‹åœ¨è¿™æ ·ä¸€ä¸ªå‡è®¾ä¹‹ä¸Šï¼Œå³ç›¸å…³é¢†åŸŸçš„ä¸“å®¶ä¼šä»”ç»†è€ƒè™‘æäº¤ç»™åŒè¡Œè¯„å®¡çš„æ‰‹ç¨¿çš„ä¼˜ç‚¹ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼ŒåŒè¡Œè¯„å®¡è¿‡ç¨‹ä¸­å­˜åœ¨ä¸€ç§æ–°é£é™©ï¼Œå³ç–å¿½çš„è¯„å®¡è€…ä¼šä¾èµ–LLMæ‰§è¡Œé€šå¸¸è€—æ—¶è¾ƒé•¿çš„è®ºæ–‡è¯„å®¡è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ç”¨äºè¡¡é‡åŒè¡Œè¯„å®¡é¢†åŸŸä¸­AIæ–‡æœ¬æ£€æµ‹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«æ€»è®¡788984ç¯‡ç”±AIæ’°å†™çš„åŒè¡Œè¯„å®¡æ„è§ä»¥åŠç›¸åº”çš„äººç±»è¯„å®¡æ„è§ï¼Œæ¶µç›–è¿‡å»å…«å¹´å†…æäº¤ç»™ä¸¤ä¸ªé¢†å…ˆçš„äººå·¥æ™ºèƒ½ç ”ç©¶ä¼šè®®ï¼ˆICLRå’ŒNeurIPSï¼‰çš„è®ºæ–‡ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ–°èµ„æºæ¥è¯„ä¼°ç°æœ‰AIæ–‡æœ¬æ£€æµ‹ç®—æ³•çš„èƒ½åŠ›ï¼Œä»¥åŒºåˆ†å®Œå…¨ç”±äººç±»æ’°å†™å’Œç”±æœ€æ–°LLMæ’°å†™çš„åŒè¡Œè¯„å®¡æ„è§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸€ç§åä¸ºAnchorçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰‹ç¨¿å†…å®¹æ¥æ£€æµ‹AIç”Ÿæˆçš„è¯„è®ºï¼Œå¹¶åˆ†ææ£€æµ‹æ¨¡å‹å¯¹LLMè¾…åŠ©ç¼–è¾‘äººç±»æ’°å†™æ–‡æœ¬æ—¶çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†å•ä¸ªåŒè¡Œè¯„å®¡çº§åˆ«ä¸Šè¯†åˆ«AIç”Ÿæˆæ–‡æœ¬çš„å›°éš¾æ€§ï¼Œçªæ˜¾äº†å¼€å‘æ–°å·¥å…·å’Œæ–¹æ³•çš„ç´§è¿«éœ€æ±‚æ¥æ£€æµ‹è¿™ç§ä¸é“å¾·çš„äººå·¥æ™ºèƒ½ç”Ÿæˆè¡Œä¸ºã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark%E3%80%82">https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmarkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19614v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒè¡Œè¯„å®¡è¿‡ç¨‹ä¸­å¯èƒ½å¸¦æ¥çš„é£é™©ï¼Œå› ä¸ºä¸€äº›ä¸“å®¶å¯èƒ½ä¼šä¾èµ–è¿™äº›æ¨¡å‹æ¥å®Œæˆè¯„å®¡è¿‡ç¨‹ã€‚ä¸ºè§£å†³è¿™ä¸€é¢†åŸŸç¼ºä¹è¯„ä¼°AIæ–‡æœ¬æ£€æµ‹çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåŒ…å«AIæ’°å†™ä¸äººç±»æ’°å†™çš„åŒè¡Œè¯„å®¡çš„ç»¼åˆæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ç°æœ‰çš„AIæ–‡æœ¬æ£€æµ‹ç®—æ³•åœ¨åŒºåˆ†äººç±»ä¸AIç”Ÿæˆçš„åŒè¡Œè¯„å®¡æ–¹é¢çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ¢ç´¢äº†ä¸€ç§åˆ©ç”¨æ‰‹ç¨¿å†…å®¹æ£€æµ‹AIç”Ÿæˆçš„åŒè¡Œè¯„å®¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•ï¼Œå¹¶åˆ†æäº†æ£€æµ‹æ¨¡å‹å¯¹LLMè¾…åŠ©ç¼–è¾‘çš„äººç±»æ’°å†™æ–‡æœ¬çš„æ•æ„Ÿæ€§ã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒäº†éœ€è¦åœ¨æ£€æµ‹AIç”Ÿæˆæ–‡æœ¬æ–¹é¢å¼€å‘æ–°çš„å·¥å…·å’Œæ–¹æ³•ï¼Œä»¥åº”å¯¹åŒè¡Œè¯„å®¡ä¸­çš„ä¸é“å¾·ä½¿ç”¨è¡Œä¸ºã€‚æ•°æ®é›†å·²å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯èƒ½å¸¦æ¥çš„æ–°é£é™©ï¼šåŒè¡Œè¯„å®¡è¿‡ç¨‹ä¸­ä¸“å®¶å¯èƒ½ä¾èµ–LLMsæ¥å®Œæˆè€—æ—¶çš„å·¥ä½œï¼Œä½†ç¼ºä¹è¯„ä¼°AIæ–‡æœ¬æ£€æµ‹çš„èµ„æºã€‚</li>
<li>æ–°æ•°æ®é›†ä»‹ç»ï¼šåŒ…å«AIæ’°å†™ä¸äººç±»æ’°å†™çš„åŒè¡Œè¯„å®¡çš„ç»¼åˆæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°AIæ–‡æœ¬æ£€æµ‹ç®—æ³•çš„èƒ½åŠ›ã€‚</li>
<li>AIæ–‡æœ¬æ£€æµ‹ç®—æ³•è¯„ä¼°ï¼šæ•°æ®é›†ç”¨äºè¯„ä¼°ç°æœ‰çš„AIæ–‡æœ¬æ£€æµ‹ç®—æ³•åœ¨åŒºåˆ†äººç±»ä¸ä¸åŒå…ˆè¿›LLMsç”Ÿæˆçš„åŒè¡Œè¯„å®¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æ¢ç´¢æ–°æ–¹æ³•ï¼šä¸€ç§åä¸ºAnchorçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€æµ‹æ–¹æ³•è¢«ç”¨æ¥æ£€æµ‹AIç”Ÿæˆçš„åŒè¡Œè¯„å®¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰‹ç¨¿å†…å®¹è¿›è¡Œæ£€æµ‹ã€‚</li>
<li>å¯¹æ¨¡å‹çš„åˆ†æï¼šå¼ºè°ƒäº†AIç”Ÿæˆçš„æ–‡æœ¬çš„å¤æ‚æ€§ä»¥åŠå¯¹å…¶è¿›è¡Œæ£€æµ‹çš„éš¾åº¦ï¼Œå°¤å…¶æ˜¯åœ¨å•ç‹¬çš„åŒè¡Œè¯„å®¡çº§åˆ«ä¸Šã€‚éœ€è¦æ–°çš„å·¥å…·å’Œæ–¹æ³•æ¥è¯†åˆ«è¿™äº›ä¸é“å¾·çš„ä½¿ç”¨è¡Œä¸ºã€‚ </li>
<li>ç ”ç©¶å‘ç°çš„é‡è¦æ€§ï¼šè¿™é¡¹å·¥ä½œçªæ˜¾äº†å¯¹æ–°å‹å·¥å…·çš„è¿«åˆ‡éœ€æ±‚ï¼Œç”¨äºæ›´å¥½åœ°æ£€æµ‹å’Œåº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨äº§ç”Ÿçš„å­¦æœ¯æ–‡æ¡£çš„æŒ‘æˆ˜æ€§è¡Œä¸ºï¼ˆæ¯”å¦‚è‡ªåŠ¨åŒ–çš„åŒåƒšè®ºæ–‡è¯„è®ºï¼‰ï¼Œä»è€Œåœ¨äººå·¥æ™ºèƒ½ç›¸å…³ç ”ç©¶å–å¾—æ›´æ·±çš„ç†è§£å’ŒæŠ€æœ¯æ¨è¿›å¹¶è§„èŒƒæ–°çš„åº”ç”¨åœºæ™¯å’Œè´£ä»»è€ƒé‡ ã€‚è¯¥ç ”ç©¶ä¸ºæ¨åŠ¨å­¦æœ¯ç ”ç©¶å…¬æ­£æ€§çš„æ·±å…¥å®è·µåšå‡ºå®è´¨æ€§è´¡çŒ® ã€‚éœ€è¦æŒç»­çš„è­¦è§‰ä¸åˆ›æ–°å®è·µå»ç»´æŠ¤å’ŒéªŒè¯AIå’Œæ™ºèƒ½æ–‡çŒ®æ•°æ®çš„ç§‘å­¦é“å¾·ä¸å¯é å¯æŒç»­æ€§ å®æ—¶åæ€ä¸åé¦ˆæœºåˆ¶æ˜¯ç¡®ä¿äººå·¥æ™ºèƒ½ç ”ç©¶å…¬æ­£æ€§çš„å…³é”®è¦ç´  ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16b469691b4de294eb3d8a85f9d81d13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-379364ffc5d8d8a07b8c67be58a4fd5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e24a901a312844c234f8d41d1357b468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bb6bdd8dc58112e52a30bb0c5c0dc73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e218081f4bc65bbdd03dfaf293e2aa0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Edit-Once-Update-Everywhere-A-Simple-Framework-for-Cross-Lingual-Knowledge-Synchronization-in-LLMs"><a href="#Edit-Once-Update-Everywhere-A-Simple-Framework-for-Cross-Lingual-Knowledge-Synchronization-in-LLMs" class="headerlink" title="Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual   Knowledge Synchronization in LLMs"></a>Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual   Knowledge Synchronization in LLMs</h2><p><strong>Authors:Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao</strong></p>
<p>Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings. </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ–°ä¿¡æ¯æˆ–ä¿®æ­£çš„é€‚åº”ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢å†è®­ç»ƒã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•è¯­è¨€ç¼–è¾‘æˆ–åŸºæœ¬çš„å¤šè¯­è¨€ç¼–è¾‘ï¼Œæ— æ³•å®ç°çœŸæ­£çš„è·¨è¯­è¨€çŸ¥è¯†åŒæ­¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•å®ç”¨çš„æœ€æ–°ï¼ˆSOTAï¼‰é…æ–¹â€”â€”è·¨è¯­è¨€çŸ¥è¯†æ°‘ä¸»ç¼–è¾‘ï¼ˆX-KDEï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»ä¸»å¯¼è¯­è¨€ä¼ æ’­åˆ°å…¶ä»–è¯­è¨€ã€‚æˆ‘ä»¬çš„X-KDEåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰è·¨è¯­è¨€ç¼–è¾‘æŒ‡ä»¤å¾®è°ƒï¼ˆXE-ITï¼‰ï¼Œè¯¥é˜¶æ®µåœ¨ä¸€ä¸ªç²¾é€‰çš„å¹¶è¡Œæ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥ä¿®æ”¹èŒƒå›´å†…çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™ä¸ç›¸å…³çš„ä¿¡æ¯ï¼›ï¼ˆiiï¼‰ç›®æ ‡è¯­è¨€åå¥½ä¼˜åŒ–ï¼ˆTL-POï¼‰ï¼Œè¯¥é˜¶æ®µåº”ç”¨å…ˆè¿›çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥ç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§ï¼Œä¿ƒè¿›æ›´æ–°çš„è½¬ç§»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šè¯­è¨€æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºå¢å¼ºè·¨è¯­è¨€çš„çŸ¥è¯†è½¬ç§»ã€‚åœ¨Bi-ZsREå’ŒMzsREåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒX-KDEæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†+8.19%ï¼ŒåŒæ—¶åœ¨å•è¯­è¨€è®¾ç½®ä¸­ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14645v2">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†ç¼–è¾‘å¯ä»¥é«˜æ•ˆåœ°é€‚åº”æ–°ä¿¡æ¯æˆ–ä¿®æ­£ï¼Œè€Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸é›†ä¸­åœ¨å•è¯­è¨€ç¼–è¾‘æˆ–åŸºæœ¬çš„å¤šè¯­è¨€ç¼–è¾‘ä¸Šï¼Œæ— æ³•å®ç°çœŸæ­£çš„è·¨è¯­è¨€çŸ¥è¯†åŒæ­¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•å®ç”¨çš„è·¨è¯­è¨€çŸ¥è¯†æ°‘ä¸»ç¼–è¾‘ï¼ˆX-KDEï¼‰çš„æœ€æ–°é…æ–¹ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°ä»ä¸»å¯¼è¯­è¨€ä¼ æ’­çŸ¥è¯†åˆ°å…¶ä»–è¯­è¨€ã€‚X-KDEåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰è·¨è¯­è¨€ç¼–è¾‘æŒ‡ä»¤å¾®è°ƒï¼ˆXE-ITï¼‰ï¼Œåœ¨ä¸€ä¸ªç²¾é€‰çš„å¹¶è¡Œæ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œä»¥ä¿®æ”¹èŒƒå›´å†…çš„çŸ¥è¯†åŒæ—¶ä¿ç•™æ— å…³ä¿¡æ¯ï¼›ï¼ˆiiï¼‰ç›®æ ‡è¯­è¨€åå¥½ä¼˜åŒ–ï¼ˆTL-POï¼‰ï¼Œé‡‡ç”¨å…ˆè¿›çš„ä¼˜åŒ–æŠ€æœ¯ç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§ï¼Œä¿ƒè¿›æ›´æ–°çš„è½¬ç§»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡ã€è·¨è¯­è¨€çš„æ•°æ®é›†ï¼Œä¸“ä¸ºå¢å¼ºè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»è€Œè®¾è®¡ã€‚åœ¨Bi-ZsREå’ŒMzsREåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒX-KDEæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†8.19%ï¼ŒåŒæ—¶åœ¨å•è¯­è¨€è®¾ç½®ä¸­ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†ç¼–è¾‘å¯é«˜æ•ˆé€‚åº”LLMçš„æ–°ä¿¡æ¯æˆ–ä¿®æ­£ï¼Œæ— éœ€å…¨é¢é‡è®­ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•è¯­è¨€æˆ–å¤šåŸºæœ¬è¯­è¨€ç¼–è¾‘ï¼Œéš¾ä»¥å®ç°çœŸæ­£çš„è·¨è¯­è¨€çŸ¥è¯†åŒæ­¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è·¨è¯­è¨€çŸ¥è¯†æ°‘ä¸»ç¼–è¾‘æ–¹æ³•ï¼ˆX-KDEï¼‰ï¼ŒåŒ…å«è·¨è¯­è¨€ç¼–è¾‘æŒ‡ä»¤å¾®è°ƒï¼ˆXE-ITï¼‰å’Œç›®æ ‡è¯­è¨€åå¥½ä¼˜åŒ–ï¼ˆTL-POï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>XE-ITèƒ½å¤Ÿåœ¨ä¿®æ”¹ç‰¹å®šçŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ— å…³ä¿¡æ¯ã€‚</li>
<li>TL-POé‡‡ç”¨å…ˆè¿›ä¼˜åŒ–æŠ€æœ¯ç¡®ä¿è·¨è¯­è¨€çš„ä¸€è‡´æ€§ï¼Œä¿ƒè¿›çŸ¥è¯†æ›´æ–°åœ¨ä¸åŒè¯­è¨€é—´çš„è½¬ç§»ã€‚</li>
<li>è´¡çŒ®äº†ä¸€ä¸ªä¸“ä¸ºå¢å¼ºè·¨è¯­è¨€çŸ¥è¯†è½¬ç§»è®¾è®¡çš„é«˜è´¨é‡ã€è·¨è¯­è¨€æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-996f57b0577a8b793e3cede1b89344e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-074a671e6e9693fc4c7dc4321f249927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a48675644bf427c2e85523a0caac7fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f84f04769e60f60b91d5542a69b18bdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7c2c8512efc1cc7b7ff560bd77f40e7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PASER-Post-Training-Data-Selection-for-Efficient-Pruned-Large-Language-Model-Recovery"><a href="#PASER-Post-Training-Data-Selection-for-Efficient-Pruned-Large-Language-Model-Recovery" class="headerlink" title="PASER: Post-Training Data Selection for Efficient Pruned Large Language   Model Recovery"></a>PASER: Post-Training Data Selection for Efficient Pruned Large Language   Model Recovery</h2><p><strong>Authors:Bowei He, Lihao Yin, Hui-Ling Zhen, Xiaokun Zhang, Mingxuan Yuan, Chen Ma</strong></p>
<p>Model pruning is an effective approach for compressing large language models (LLMs). However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to recover model performance, existing methods often overlook the uneven deterioration of model capabilities and incur high computational costs. Moreover, some irrelevant instructions may also introduce negative effects to model capacity recovery. To address these challenges, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). PASER aims to identify instructions to recover the most compromised model capacities with a certain data budget. Our approach first applies manifold learning and spectral clustering to group recovery instructions in the semantic space, revealing capability-specific instruction sets. Then, the data budget is adaptively allocated across clusters by the degree of corresponding model capability degradation. In each cluster, we prioritize data samples that lead to the most decline of model performance. To mitigate potential negative tuning effects, we also detect and filter out conflicting or irrelevant recovery data. Extensive experiments demonstrate that PASER significantly outperforms conventional baselines, effectively recovering the general capabilities of pruned LLMs while utilizing merely 4%-20% of the original post-training data. We provide the anonymous code repository in \href{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PASER-E606%7D%7BLink%7D">https://anonymous.4open.science/r/PASER-E606}{Link}</a>. </p>
<blockquote>
<p>æ¨¡å‹å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚è™½ç„¶é€šå¸¸é‡‡ç”¨åè®­ç»ƒæŠ€æœ¯ï¼ˆå¦‚æŒ‡ä»¤å¾®è°ƒï¼‰æ¥æ¢å¤æ¨¡å‹æ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†æ¨¡å‹èƒ½åŠ›çš„ä¸å‡åŒ€é€€åŒ–ï¼Œå¹¶äº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸€äº›æ— å…³çš„æŒ‡ä»¤ä¹Ÿå¯èƒ½å¯¹æ¨¡å‹å®¹é‡çš„æ¢å¤äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æœ‰æ•ˆå‰ªæå¤§å‹è¯­è¨€æ¨¡å‹æ¢å¤çš„åè®­ç»ƒæ•°æ®é€‰æ‹©æ–¹æ³•ï¼ˆPASERï¼‰ã€‚PASERæ—¨åœ¨ä»¥ä¸€å®šçš„æ•°æ®é¢„ç®—è¯†åˆ«æ¢å¤æŒ‡ä»¤ï¼Œä»¥æ¢å¤æœ€å—æŸå®³çš„æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåº”ç”¨æµå½¢å­¦ä¹ å’Œè°±èšç±»æ¥åœ¨è¯­ä¹‰ç©ºé—´ä¸­åˆ†ç»„æ¢å¤æŒ‡ä»¤ï¼Œæ­ç¤ºç‰¹å®šèƒ½åŠ›çš„æŒ‡ä»¤é›†ã€‚ç„¶åï¼Œæ ¹æ®ç›¸åº”æ¨¡å‹èƒ½åŠ›çš„é€€åŒ–ç¨‹åº¦ï¼Œè‡ªé€‚åº”åœ°åœ¨ä¸åŒèšç±»ä¹‹é—´åˆ†é…æ•°æ®é¢„ç®—ã€‚åœ¨æ¯ä¸ªèšç±»ä¸­ï¼Œæˆ‘ä»¬ä¼˜å…ˆå¤„ç†å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™æœ€å¤§çš„æ•°æ®æ ·æœ¬ã€‚ä¸ºäº†å‡è½»æ½œåœ¨çš„è´Ÿé¢è°ƒæ•´æ•ˆæœï¼Œæˆ‘ä»¬è¿˜æ£€æµ‹å’Œè¿‡æ»¤å‡ºå†²çªæˆ–æ— å…³çš„æ¢å¤æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPASERæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œåœ¨åˆ©ç”¨ä»…4%-20%çš„åŸå§‹åè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æ¢å¤äº†å‰ªæLLMçš„ä¸€èˆ¬èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/paser-e606">https://anonymous.4open.science/r/PASER-E606</a>ï¼ˆé“¾æ¥ï¼‰æä¾›äº†åŒ¿åä»£ç ä»“åº“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12594v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†è¿™ä¸€è¿‡ç¨‹å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚ä¸ºæ¢å¤æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸é‡‡ç”¨è®­ç»ƒåæŠ€æœ¯å¦‚æŒ‡ä»¤å¾®è°ƒï¼Œä½†ç°æœ‰æ–¹æ³•å¿½è§†äº†æ¨¡å‹èƒ½åŠ›çš„ä¸å‡è¡¡ä¸‹é™ï¼Œå¹¶äº§ç”Ÿè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸€äº›æ— å…³çš„æŒ‡ä»¤ä¹Ÿå¯èƒ½å¯¹æ¨¡å‹å®¹é‡çš„æ¢å¤äº§ç”Ÿè´Ÿé¢å½±å“ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘é«˜æ•ˆå‰ªæå¤§å‹è¯­è¨€æ¨¡å‹æ¢å¤ï¼ˆPASERï¼‰çš„å‰ªæåæ•°æ®é€‰æ‹©æ–¹æ³•ã€‚PASERæ—¨åœ¨ä»¥æœ‰é™çš„æ•°æ®é¢„ç®—è¯†åˆ«æœ€èƒ½æ¢å¤æ¨¡å‹èƒ½åŠ›çš„æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåº”ç”¨æµå½¢å­¦ä¹ å’Œè°±èšç±»å¯¹æ¢å¤æŒ‡ä»¤è¿›è¡Œè¯­ä¹‰ç©ºé—´åˆ†ç»„ï¼Œæ­ç¤ºç‰¹å®šèƒ½åŠ›çš„æŒ‡ä»¤é›†ã€‚ç„¶åï¼Œæ ¹æ®æ¨¡å‹èƒ½åŠ›ä¸‹é™çš„ä¸¥é‡ç¨‹åº¦è‡ªé€‚åº”åˆ†é…æ•°æ®é¢„ç®—åˆ°ä¸åŒçš„ç¾¤ç»„ä¸­ã€‚åœ¨æ¯ä¸ªç¾¤ç»„ä¸­ï¼Œæˆ‘ä»¬ä¼˜å…ˆä½¿ç”¨å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™æœ€å¤§çš„æ•°æ®æ ·æœ¬ã€‚ä¸ºå‡è½»æ½œåœ¨çš„è´Ÿé¢å¾®è°ƒæ•ˆæœï¼Œæˆ‘ä»¬è¿˜æ£€æµ‹å’Œè¿‡æ»¤å‡ºå†²çªæˆ–æ— å…³çš„å¤åŸæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒPASERæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿æ–¹æ³•ï¼Œåœ¨åˆ©ç”¨ä»…4%-20%åŸå§‹è®­ç»ƒåæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæ¢å¤äº†å‰ªæLLMçš„ä¸€èˆ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ä¼šå¯¼è‡´æ¨¡å‹èƒ½åŠ›ä¸‹é™ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†æ¨¡å‹èƒ½åŠ›çš„ä¸å‡è¡¡ä¸‹é™ï¼Œä¸”è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>PASERæ–¹æ³•æ—¨åœ¨ä»¥æœ‰é™çš„æ•°æ®é¢„ç®—æ¢å¤å‰ªææ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>PASERä½¿ç”¨æµå½¢å­¦ä¹ å’Œè°±èšç±»åœ¨è¯­ä¹‰ç©ºé—´åˆ†ç»„æ¢å¤æŒ‡ä»¤ã€‚</li>
<li>æ•°æ®é¢„ç®—æ ¹æ®æ¨¡å‹èƒ½åŠ›ä¸‹é™çš„ä¸¥é‡ç¨‹åº¦è¿›è¡Œåˆ†é…ã€‚</li>
<li>åœ¨æ¯ä¸ªç¾¤ç»„ä¸­ä¼˜å…ˆä½¿ç”¨å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™æœ€å¤§çš„æ•°æ®æ ·æœ¬è¿›è¡Œæ¢å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d91fecd597020e821d1e4c6bd5be5823.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00344ec0237d2355c89006fd27711c9d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c5c6539abdcc4acee92e22ef3dbeaa45.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  ManuSearch Democratizing Deep Search in Large Language Models with a   Transparent and Open Multi-Agent Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-70c31cfd1d18826970e8dacbfbad1abe.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  One RL to See Them All Visual Triple Unified Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
