<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  One RL to See Them All Visual Triple Unified Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-70c31cfd1d18826970e8dacbfbad1abe.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-27-æ›´æ–°"><a href="#2025-05-27-æ›´æ–°" class="headerlink" title="2025-05-27 æ›´æ–°"></a>2025-05-27 æ›´æ–°</h1><h2 id="One-RL-to-See-Them-All-Visual-Triple-Unified-Reinforcement-Learning"><a href="#One-RL-to-See-Them-All-Visual-Triple-Unified-Reinforcement-Learning" class="headerlink" title="One RL to See Them All: Visual Triple Unified Reinforcement Learning"></a>One RL to See Them All: Visual Triple Unified Reinforcement Learning</h2><p><strong>Authors:Yan Ma, Linge Du, Xuyang Shen, Shaoxiang Chen, Pengfei Li, Qibing Ren, Lizhuang Ma, Yuchao Dai, Pengfei Liu, Junjie Yan</strong></p>
<p>Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI">https://github.com/MiniMax-AI</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æ˜¾è‘—æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé™¤äº†æ¨ç†ä»»åŠ¡ä¹‹å¤–ï¼ŒRLåœ¨æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹å’Œå®šä½ï¼‰ä¸­çš„ä½¿ç”¨ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†V-Triuneï¼Œä¸€ä¸ªè§†è§‰ä¸‰é‡ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œä½¿VLMèƒ½å¤Ÿåœ¨å•ä¸ªè®­ç»ƒç®¡é“ä¸­è”åˆå­¦ä¹ è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚V-TriuneåŒ…å«ä¸‰ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šæ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ï¼ˆä»¥ç»Ÿä¸€å„ç§ä»»åŠ¡è¾“å…¥ï¼‰ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—ï¼ˆé€šè¿‡ä¸“ç”¨éªŒè¯å™¨æä¾›è‡ªå®šä¹‰å¥–åŠ±ï¼‰ï¼Œä»¥åŠæºçº§æŒ‡æ ‡ç›‘æ§ï¼ˆåœ¨æ•°æ®æºçº§åˆ«è¯Šæ–­é—®é¢˜ï¼‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŠ¨æ€IoUå¥–åŠ±ï¼Œä¸ºV-Triuneå¤„ç†çš„æ„ŸçŸ¥ä»»åŠ¡æä¾›è‡ªé€‚åº”ã€æ¸è¿›å’Œæ˜ç¡®çš„åé¦ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨ç°æˆçš„RLè®­ç»ƒæ¡†æ¶ä¸­ä½¿ç”¨å¼€æºçš„7Bå’Œ32Béª¨å¹²æ¨¡å‹æ¥å®ç°çš„ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹è¢«ç§°ä¸ºOrstaï¼ˆä¸€RLè§æ‰€æœ‰ï¼‰ï¼Œåœ¨æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¸€è‡´çš„æ”¹è¿›ã€‚è¿™ç§å¹¿æ³›çš„èƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±å…¶åœ¨å›´ç»•å››ä¸ªä»£è¡¨æ€§è§†è§‰æ¨ç†ä»»åŠ¡ï¼ˆæ•°å­¦ã€æ‹¼å›¾ã€å›¾è¡¨å’Œç§‘å­¦ï¼‰å’Œå››ä¸ªè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ï¼ˆå®šä½ã€æ£€æµ‹ã€è®¡æ•°å’ŒOCRï¼‰æ„å»ºçš„å¤šæ ·åŒ–æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ‰€å¡‘é€ çš„ã€‚éšåï¼ŒOrstaåœ¨MEGA-Bench Coreä¸Šå–å¾—äº†é‡å¤§è¿›å±•ï¼Œåœ¨å…¶å„ç§7Bå’Œ32Bæ¨¡å‹å˜ç§ä¸­ï¼Œæ”¹è¿›èŒƒå›´ä»+2.1åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„+14.1ï¼Œå¹¶ä¸”å¯¹ä¸€ç³»åˆ—ä¸‹æ¸¸ä»»åŠ¡äº§ç”Ÿäº†æ€§èƒ½ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœçªå‡ºæ˜¾ç¤ºäº†æˆ‘ä»¬ç»Ÿä¸€çš„RLæ–¹æ³•å¯¹VLMçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚V-Triuneç³»ç»Ÿä»¥åŠOrstaæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/MiniMax-AIä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18129v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›å·²ç»å¾—åˆ°äº†æ˜¾è‘—çš„æå‡ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§è§†è§‰ä¸‰é‡ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ ç³»ç»ŸV-Triuneï¼Œä½¿VLMèƒ½å¤Ÿåœ¨å•ä¸€è®­ç»ƒç®¡é“ä¸­è”åˆå­¦ä¹ è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚é€šè¿‡æ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—å’Œæºçº§æŒ‡æ ‡ç›‘æ§ä¸‰ä¸ªäº’è¡¥ç»„ä»¶ï¼Œç»“åˆåŠ¨æ€IoUå¥–åŠ±çš„æ–°å‹å¥–åŠ±æœºåˆ¶ï¼Œç ”ç©¶æé«˜äº†VLMå¤„ç†æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡çš„èƒ½åŠ›ã€‚åˆ©ç”¨ç°æˆçš„RLè®­ç»ƒæ¡†æ¶å’Œå¼€æºçš„7Bä¸32Béª¨å¹²æ¨¡å‹ï¼Œæ¨å‡ºçš„Orstaæ¨¡å‹åœ¨æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæŒç»­çš„æ€§èƒ½æå‡ã€‚å…¶åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¹¿æ³›èƒ½åŠ›æ˜¾è‘—å—ç›Šäºå››ä¸ªä»£è¡¨æ€§çš„è§†è§‰æ¨ç†ä»»åŠ¡å’Œå››ä¸ªè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ã€‚æœ€ç»ˆï¼ŒOrstaåœ¨MEGA-Bench Coreä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©æå‡ï¼Œä»+2.1åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„+14.1ä¸ç­‰ã€‚è¯æ˜äº†æœ¬ç ”ç©¶ç»Ÿä¸€RLæ–¹æ³•åœ¨å¤„ç†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>V-Triuneç³»ç»Ÿèƒ½è”åˆå­¦ä¹ è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚</li>
<li>V-TriuneåŒ…å«ä¸‰ä¸ªäº’è¡¥ç»„ä»¶ï¼šæ ·æœ¬çº§æ•°æ®æ ¼å¼åŒ–ã€éªŒè¯å™¨çº§å¥–åŠ±è®¡ç®—å’Œæºçº§æŒ‡æ ‡ç›‘æ§ã€‚</li>
<li>å¼•å…¥åŠ¨æ€IoUå¥–åŠ±æœºåˆ¶ä»¥æ”¹å–„æ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>Orstaæ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬è§†è§‰æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡ã€‚</li>
<li>Orstaåœ¨MEGA-Bench Coreä¸Šçš„æˆç»©æ˜¾è‘—æå‡ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>V-Triuneç³»ç»Ÿå’ŒOrstaæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4d05a68cdb91625c44c8b098812b719e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e47ad083f3e5a83367c3928360994987.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-767019c3b4774de728a6fa03c101a23a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ManuSearch-Democratizing-Deep-Search-in-Large-Language-Models-with-a-Transparent-and-Open-Multi-Agent-Framework"><a href="#ManuSearch-Democratizing-Deep-Search-in-Large-Language-Models-with-a-Transparent-and-Open-Multi-Agent-Framework" class="headerlink" title="ManuSearch: Democratizing Deep Search in Large Language Models with a   Transparent and Open Multi-Agent Framework"></a>ManuSearch: Democratizing Deep Search in Large Language Models with a   Transparent and Open Multi-Agent Framework</h2><p><strong>Authors:Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao</strong></p>
<p>Recent advances in web-augmented large language models (LLMs) have exhibited strong performance in complex reasoning tasks, yet these capabilities are mostly locked in proprietary systems with opaque architectures. In this work, we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework designed to democratize deep search for LLMs. ManuSearch decomposes the search and reasoning process into three collaborative agents: (1) a solution planning agent that iteratively formulates sub-queries, (2) an Internet search agent that retrieves relevant documents via real-time web search, and (3) a structured webpage reading agent that extracts key evidence from raw web content. To rigorously evaluate deep reasoning abilities, we introduce \textbf{ORION}, a challenging benchmark focused on open-web reasoning over long-tail entities, covering both English and Chinese. Experimental results show that ManuSearch substantially outperforms prior open-source baselines and even surpasses leading closed-source systems. Our work paves the way for reproducible, extensible research in open deep search systems. We release the data and code in <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ManuSearch">https://github.com/RUCAIBox/ManuSearch</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç½‘ç»œå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†è¿™äº›èƒ½åŠ›å¤§å¤šè¢«é”å®šåœ¨æ¶æ„ä¸é€æ˜çš„ä¸“æœ‰ç³»ç»Ÿä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>ManuSearch</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé€æ˜ä¸”æ¨¡å—åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°LLMçš„æ·±åº¦æœç´¢æ°‘ä¸»åŒ–ã€‚ManuSearchå°†æœç´¢å’Œæ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºä¸‰ä¸ªåä½œçš„æ™ºèƒ½ä½“ï¼šï¼ˆ1ï¼‰è§£å†³æ–¹æ¡ˆè§„åˆ’æ™ºèƒ½ä½“ï¼Œå®ƒè¿­ä»£åœ°åˆ¶å®šå­æŸ¥è¯¢ï¼›ï¼ˆ2ï¼‰äº’è”ç½‘æœç´¢æ™ºèƒ½ä½“ï¼Œå®ƒé€šè¿‡å®æ—¶ç½‘ç»œæœç´¢æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼›ï¼ˆ3ï¼‰ç»“æ„åŒ–ç½‘é¡µé˜…è¯»æ™ºèƒ½ä½“ï¼Œå®ƒä»åŸå§‹ç½‘é¡µå†…å®¹ä¸­æå–å…³é”®è¯æ®ã€‚ä¸ºäº†ä¸¥æ ¼è¯„ä¼°æ·±åº¦æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>ORION</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥é•¿å°¾å®ä½“ä¸ºé‡ç‚¹çš„å¼€æ”¾ç½‘ç»œæ¨ç†æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–è‹±è¯­å’Œä¸­æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManuSearchæ˜¾è‘—ä¼˜äºå…ˆå‰çš„å¼€æºåŸºå‡†æµ‹è¯•ï¼Œç”šè‡³è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºå¯å¤åˆ¶ã€å¯æ‰©å±•çš„å¼€æ”¾æ·±åº¦æœç´¢ç³»ç»Ÿç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ManuSearch">https://github.com/RUCAIBox/ManuSearch</a>ä¸Šå‘å¸ƒæ•°æ®å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18105v1">PDF</a> LLM, Complex Search Benchmark</p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26ccbfc5522e47e5c20edcd3712ab991.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f60122741de9858cd4ccbbdf19b9ca4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70c31cfd1d18826970e8dacbfbad1abe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CXReasonBench-A-Benchmark-for-Evaluating-Structured-Diagnostic-Reasoning-in-Chest-X-rays"><a href="#CXReasonBench-A-Benchmark-for-Evaluating-Structured-Diagnostic-Reasoning-in-Chest-X-rays" class="headerlink" title="CXReasonBench: A Benchmark for Evaluating Structured Diagnostic   Reasoning in Chest X-rays"></a>CXReasonBench: A Benchmark for Evaluating Structured Diagnostic   Reasoning in Chest X-rays</h2><p><strong>Authors:Hyungyung Lee, Geon Choi, Jung-Oh Lee, Hangyul Yoon, Hyuk Gi Hong, Edward Choi</strong></p>
<p>Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ttumyche/CXReasonBench">https://github.com/ttumyche/CXReasonBench</a> </p>
<blockquote>
<p>åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•ä¸ºåŒ»ç–—ä»»åŠ¡ä¸­çš„åº”ç”¨å¸¦æ¥äº†å¸Œæœ›ï¼Œä¾‹å¦‚æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æœ€ç»ˆçš„è¯Šæ–­ç­”æ¡ˆä¸Šï¼Œå¯¹äºæ¨¡å‹æ˜¯å¦è¿›è¡Œä¸´åºŠåˆç†çš„æ¨ç†æä¾›çš„è§è§£æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CheXStructå’ŒCXReasonBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå»ºç«‹åœ¨å…¬å¼€å¯ç”¨çš„MIMIC-CXR-JPGæ•°æ®é›†ä¸Šçš„ç»“æ„åŒ–ç®¡é“å’ŒåŸºå‡†æµ‹è¯•ã€‚CheXStructç›´æ¥ä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­è‡ªåŠ¨æ¨å¯¼å‡ºä¸­é—´æ¨ç†æ­¥éª¤åºåˆ—ï¼Œå¦‚åˆ†å‰²è§£å‰–åŒºåŸŸã€è·å–è§£å‰–æ ‡å¿—å’Œè¯Šæ–­æµ‹é‡ã€è®¡ç®—è¯Šæ–­æŒ‡æ•°ä»¥åŠåº”ç”¨ä¸´åºŠé˜ˆå€¼ã€‚CXReasonBenchåˆ©ç”¨æ­¤ç®¡é“æ¥è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½æ‰§è¡Œä¸´åºŠæœ‰æ•ˆçš„æ¨ç†æ­¥éª¤ï¼Œä»¥åŠå®ƒä»¬èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä»ç»“æ„åŒ–æŒ‡å¯¼ä¸­å­¦ä¹ ï¼Œä»è€Œå®ç°ç²¾ç»†å’Œé€æ˜çš„è¯Šæ–­æ¨ç†è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«18988ä¸ªé—®ç­”å¯¹ï¼Œæ¶‰åŠ12ä¸ªè¯Šæ–­ä»»åŠ¡å’Œ1200ä¸ªç—…ä¾‹ï¼Œæ¯ä¸ªç—…ä¾‹éƒ½é…æœ‰å¤šè¾¾4ä¸ªè§†è§‰è¾“å…¥ï¼Œå¹¶æ”¯æŒå¤šè·¯å¾„ã€å¤šé˜¶æ®µè¯„ä¼°ï¼ŒåŒ…æ‹¬é€šè¿‡è§£å‰–åŒºåŸŸé€‰æ‹©å’Œè¯Šæ–­æµ‹é‡è¿›è¡Œè§†è§‰å®šä½ã€‚å³ä½¿åœ¨è¯„ä¼°çš„10ä¸ªæœ€å¼ºçš„LVLMsä¸­ï¼Œå®ƒä»¬åœ¨ç»“æ„åŒ–æ¨ç†å’Œæ³›åŒ–æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œå¾€å¾€æ— æ³•å°†æŠ½è±¡çŸ¥è¯†ä¸è§£å‰–ä¸ºåŸºç¡€çš„è§†è§‰è§£é‡Šè”ç³»èµ·æ¥ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ttumyche/CXReasonBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ttumyche/CXReasonBenchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18087v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åŒ»ç–—ä»»åŠ¡ï¼ˆå¦‚æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”ï¼‰ä¸­çš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚ä½†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨æœ€ç»ˆè¯Šæ–­ç­”æ¡ˆï¼Œå¯¹æ¨¡å‹æ˜¯å¦è¿›è¡Œä¸´åºŠåˆç†æ¨ç†çš„æ´å¯Ÿæœ‰é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºCheXStructå’ŒCXReasonBenchï¼ŒåŸºäºå…¬å¼€å¯ç”¨çš„MIMIC-CXR-JPGæ•°æ®é›†æ„å»ºç»“æ„åŒ–ç®¡é“å’ŒåŸºå‡†æµ‹è¯•ã€‚CheXStructç›´æ¥ä»èƒ¸éƒ¨Xå…‰ç‰‡å¯¼å‡ºä¸€ç³»åˆ—ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œå¦‚åˆ†å‰²è§£å‰–åŒºåŸŸã€è·å–è§£å‰–æ ‡å¿—å’Œè¯Šæ–­æµ‹é‡ã€è®¡ç®—è¯Šæ–­æŒ‡æ•°å’Œåº”ç”¨ä¸´åºŠé˜ˆå€¼ã€‚CXReasonBenchåˆ©ç”¨æ­¤ç®¡é“è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½æ‰§è¡Œä¸´åºŠæœ‰æ•ˆçš„æ¨ç†æ­¥éª¤ï¼Œä»¥åŠä»ç»“æ„åŒ–æŒ‡å¯¼ä¸­å­¦ä¹ çš„ç¨‹åº¦ï¼Œå®ç°å¯¹è¯Šæ–­æ¨ç†çš„ç²¾ç»†å’Œé€æ˜è¯„ä¼°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«12ä¸ªè¯Šæ–­ä»»åŠ¡çš„18988ä¸ªé—®é¢˜å¯¹å’Œ1200ä¸ªç—…ä¾‹ï¼Œæ¯ä¸ªç—…ä¾‹é…æœ‰å¤šè¾¾4ä¸ªè§†è§‰è¾“å…¥ï¼Œæ”¯æŒå¤šè·¯å¾„ã€å¤šé˜¶æ®µè¯„ä¼°ï¼ŒåŒ…æ‹¬é€šè¿‡è§£å‰–åŒºåŸŸé€‰æ‹©å’Œè¯Šæ–­æµ‹é‡è¿›è¡Œè§†è§‰å®šä½ã€‚å³ä½¿æ˜¯æœ€å¼ºå¤§çš„10ä¸ªLVLMsåœ¨ç»“æ„æ¨ç†å’Œæ³›åŒ–æ–¹é¢ä¹Ÿé¢ä¸´æŒ‘æˆ˜ï¼Œå¾€å¾€æ— æ³•å°†æŠ½è±¡çŸ¥è¯†ä¸è§£å‰–åŸºç¡€ä¸Šçš„è§†è§‰è§£é‡Šè”ç³»èµ·æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ä»»åŠ¡ä¸­å±•ç°åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶æ˜¯æŠ¥å‘Šç”Ÿæˆå’Œè§†è§‰é—®ç­”é¢†åŸŸã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è¯Šæ–­ç­”æ¡ˆï¼Œå¿½è§†æ¨¡å‹åœ¨ä¸´åºŠæ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>CheXStructå’ŒCXReasonBenchæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ç»“æ„åŒ–ç®¡é“å’ŒåŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹çš„è¯Šæ–­æ¨ç†èƒ½åŠ›ã€‚</li>
<li>CheXStructèƒ½å¤Ÿç›´æ¥ä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­å¯¼å‡ºä¸­é—´æ¨ç†æ­¥éª¤ã€‚</li>
<li>CXReasonBenchæ”¯æŒå¤šè·¯å¾„ã€å¤šé˜¶æ®µè¯„ä¼°ï¼ŒåŒ…æ‹¬è§†è§‰å®šä½ï¼Œä»¥ç²¾ç»†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç»“æ„æ¨ç†å’Œæ³›åŒ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-126d775eaf4ee8cad848e3c6b17d4a7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67558b58bb0d15833ba39abf0236d7ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc7109c1a085221662c37e1a377b5448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26a6f5ddafb5bdde8dd637eae029796e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7600c0113f398bc35328b59617dcc705.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Extended-Inductive-Reasoning-for-Personalized-Preference-Inference-from-Behavioral-Signals"><a href="#Extended-Inductive-Reasoning-for-Personalized-Preference-Inference-from-Behavioral-Signals" class="headerlink" title="Extended Inductive Reasoning for Personalized Preference Inference from   Behavioral Signals"></a>Extended Inductive Reasoning for Personalized Preference Inference from   Behavioral Signals</h2><p><strong>Authors:Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan</strong></p>
<p>Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning\textemdash the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose \textsc{AlignXplore}, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in usersâ€™ interaction histories. We develop \textsc{AlignXplore} by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that \textsc{AlignXplore} achieves substantial improvements over the backbone model by an average of 11.05% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œä¸è¿™äº›ä»¥æ¼”ç»æ¨ç†ä¸ºä¸»çš„ä»»åŠ¡ç›¸æ¯”ï¼Œå…³äºå½’çº³æ¨ç†â€”â€”ä»ä¸å®Œæ•´è¯æ®ä¸­æ¨å¯¼ä¸€èˆ¬è§„åˆ™çš„èƒ½åŠ›â€”â€”çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡é€šè¿‡ä¸ªæ€§åŒ–åå¥½æ¨æ–­çš„è§†è§’ç ”ç©¶LLMsä¸­çš„æ‰©å±•å½’çº³æ¨ç†ï¼Œè¿™æ˜¯LLMå¯¹é½ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºå½“å‰çš„æ–¹æ³•å¾ˆéš¾æ•æ‰å¤šæ ·åŒ–çš„ç”¨æˆ·åå¥½ã€‚è¯¥ä»»åŠ¡éœ€è¦å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå› ä¸ºç”¨æˆ·åå¥½é€šå¸¸éšå«åœ¨å„ç§äº’åŠ¨å½¢å¼ä¸­ï¼Œéœ€è¦æ¨¡å‹ä»æ•£ä¹±çš„ä¿¡å·ä¸­ç»¼åˆå‡ºä¸€è‡´çš„åå¥½æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†\text{AlignXplore}ï¼Œä¸€ä¸ªåˆ©ç”¨æ‰©å±•æ¨ç†é“¾çš„æ¨¡å‹ï¼Œå¯ä»¥ä»ç”¨æˆ·äº¤äº’å†å²çš„è¡Œä¸ºä¿¡å·ä¸­è¿›è¡Œç³»ç»Ÿçš„åå¥½æ¨æ–­ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆåŸºäºåˆæˆæ•°æ®çš„å†·å¯åŠ¨è®­ç»ƒå’Œéšåçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¥å¼€å‘\text{AlignXplore}ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†\text{AlignXplore}åœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†11.05%ï¼ŒåŒæ—¶åœ¨ä¸åŒè¾“å…¥æ ¼å¼å’Œä¸‹æ¸¸æ¨¡å‹ä¸­ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿›ä¸€æ­¥çš„åˆ†æé€šè¿‡ç³»ç»Ÿåœ°æ¯”è¾ƒå¥–åŠ±å»ºæ¨¡ç­–ç•¥ï¼Œå»ºç«‹äº†åå¥½æ¨æ–­å­¦ä¹ çš„æœ€ä½³å®è·µï¼ŒåŒæ—¶æ­ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­äººç±»å¼å½’çº³æ¨ç†æ¨¡å¼çš„å‡ºç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18071v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¦‚æ•°å­¦å’Œç¼–ç¨‹ç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç›¸è¾ƒäºè¿™äº›ä»¥æ¼”ç»æ¨ç†ä¸ºä¸»çš„é¢†åŸŸï¼Œå½’çº³æ¨ç†â€”â€”ä»ä¸å®Œæ•´è¯æ®ä¸­æ¨å¯¼ä¸€èˆ¬è§„åˆ™çš„èƒ½åŠ›ï¼Œåœ¨LLMsä¸­çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ¬æ–‡é€šè¿‡ä¸ªæ€§åŒ–åå¥½æ¨ç†è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œç ”ç©¶LLMsä¸­çš„æ‰©å±•å½’çº³æ¨ç†ã€‚è¯¥ä»»åŠ¡è¦æ±‚å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ï¼Œå› ä¸ºç”¨æˆ·åå¥½é€šå¸¸éšå«åœ¨å„ç§äº’åŠ¨å½¢å¼ä¸­ï¼Œéœ€è¦æ¨¡å‹ä»æ•£ä¹±çš„ä¿¡å·ä¸­ç»¼åˆå‡ºä¸€è‡´åå¥½æ¨¡å¼ã€‚\text{AlignXplore}æ¨¡å‹ç»“åˆäº†åŸºäºåˆæˆæ•°æ®çš„å†·å¯åŠ¨è®­ç»ƒå’Œéšåçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¥å®ç°ç³»ç»Ÿåå¥½æ¨æ–­ï¼Œå¯ä»ç”¨æˆ·äº¤äº’å†å²çš„è¡Œä¸ºä¿¡å·ä¸­è¿›è¡Œæ¨æ–­ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹ï¼Œ\text{AlignXplore}åœ¨é¢†åŸŸå†…å’Œè·¨é¢†åŸŸå¤–çš„åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†11.05%ï¼ŒåŒæ—¶åœ¨ä¸åŒè¾“å…¥æ ¼å¼å’Œä¸‹æ¸¸æ¨¡å‹ä¸­ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿›ä¸€æ­¥çš„åˆ†æå»ºç«‹äº†é€šè¿‡å¥–åŠ±å»ºæ¨¡ç­–ç•¥çš„ç³»ç»Ÿæ€§æ¯”è¾ƒè¿›è¡Œåå¥½æ¨æ–­å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œå¹¶æ­ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­äººç±»èˆ¬çš„å½’çº³æ¨ç†æ¨¡å¼çš„å‡ºç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸï¼Œä½†å½’çº³æ¨ç†çš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚</li>
<li>ä¸ªæ€§åŒ–åå¥½æ¨ç†æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œè¦æ±‚LLMså…·å¤‡å¼ºå¤§çš„å½’çº³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>\text{AlignXplore}æ¨¡å‹ç»“åˆäº†å†·å¯åŠ¨è®­ç»ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»ç”¨æˆ·äº¤äº’å†å²çš„è¡Œä¸ºä¿¡å·ä¸­è¿›è¡Œç³»ç»ŸåŒ–çš„åå¥½æ¨æ–­ã€‚</li>
<li>\text{AlignXplore}ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹åœ¨é¢†åŸŸå†…å’Œè·¨é¢†åŸŸå¤–çš„æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>\text{AlignXplore}åœ¨ä¸åŒè¾“å…¥æ ¼å¼å’Œä¸‹æ¸¸æ¨¡å‹ä¸­ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¥–åŠ±å»ºæ¨¡ç­–ç•¥çš„ç³»ç»Ÿæ€§æ¯”è¾ƒï¼Œå»ºç«‹äº†åå¥½æ¨æ–­å­¦ä¹ çš„æœ€ä½³å®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94f6381d86d4163715d97d5d80382a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e96b1909409c7753654c68f6342dbe73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0384ceee26fc9099f608e43dc3ce6765.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reward-Model-Generalization-for-Compute-Aware-Test-Time-Reasoning"><a href="#Reward-Model-Generalization-for-Compute-Aware-Test-Time-Reasoning" class="headerlink" title="Reward Model Generalization for Compute-Aware Test-Time Reasoning"></a>Reward Model Generalization for Compute-Aware Test-Time Reasoning</h2><p><strong>Authors:Zeen Song, Wenwen Qiang, Siyu Zhao, Changwen Zheng, Gang Hua</strong></p>
<p>External test-time reasoning enhances large language models (LLMs) by decoupling generation and selection. At inference time, the model generates multiple reasoning paths, and an auxiliary process reward model (PRM) is used to score and select the best one. A central challenge in this setting is test-time compute optimality (TCO), i.e., how to maximize answer accuracy under a fixed inference budget. In this work, we establish a theoretical framework to analyze how the generalization error of the PRM affects compute efficiency and reasoning performance. Leveraging PAC-Bayes theory, we derive generalization bounds and show that a lower generalization error of PRM leads to fewer samples required to find correct answers. Motivated by this analysis, we propose Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically controls search behavior. The actor outputs sampling hyperparameters based on reward distributions and sparsity statistics, while the critic estimates their utility to guide budget allocation. Experiments on the MATH and AIME benchmarks with various LLMs and PRMs demonstrate that CATS consistently outperforms other external TTS methods, validating our theoretical predictions. </p>
<blockquote>
<p>å¤–éƒ¨æµ‹è¯•æ—¶é—´æ¨ç†é€šè¿‡è§£è€¦ç”Ÿæˆå’Œé€‰æ‹©æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶ä½¿ç”¨è¾…åŠ©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¯¹å®ƒä»¬è¿›è¡Œè¯„åˆ†å’Œé€‰æ‹©æœ€ä½³è·¯å¾„ã€‚åœ¨æ­¤è®¾ç½®ä¸­ï¼Œæ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæµ‹è¯•æ—¶é—´çš„è®¡ç®—æœ€ä¼˜æ€§ï¼ˆTCOï¼‰ï¼Œå³å¦‚ä½•åœ¨å›ºå®šçš„æ¨ç†é¢„ç®—ä¸‹æœ€å¤§åŒ–ç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œåˆ†æPRMçš„æ³›åŒ–è¯¯å·®å¦‚ä½•å½±å“è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨PAC-Bayesç†è®ºï¼Œæ¨å¯¼å‡ºæ³›åŒ–è¾¹ç•Œï¼Œå¹¶è¡¨æ˜PRMçš„æ³›åŒ–è¯¯å·®é™ä½ä¼šå¯¼è‡´æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆæ‰€éœ€æ ·æœ¬æ•°å‡å°‘ã€‚å—æ­¤åˆ†æå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è®¡ç®—æ„ŸçŸ¥æ ‘æœç´¢ï¼ˆCATSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ§åˆ¶æœç´¢è¡Œä¸ºçš„æ¼”å‘˜-è¯„è®ºå®¶æ¡†æ¶ã€‚æ¼”å‘˜æ ¹æ®å¥–åŠ±åˆ†å¸ƒå’Œç¨€ç–ç»Ÿè®¡è¾“å‡ºé‡‡æ ·è¶…å‚æ•°ï¼Œè€Œè¯„è®ºå®¶åˆ™ä¼°è®¡å®ƒä»¬çš„æ•ˆç”¨ä»¥æŒ‡å¯¼é¢„ç®—åˆ†é…ã€‚åœ¨MATHå’ŒAIMEåŸºå‡†æµ‹è¯•ä¸Šï¼Œå¯¹å„ç§LLMå’ŒPRMè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCATSå§‹ç»ˆä¼˜äºå…¶ä»–å¤–éƒ¨TTSæ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºé¢„æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18065v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è§£è€¦ç”Ÿæˆå’Œé€‰æ‹©é˜¶æ®µå®ç°æµ‹è¯•æ—¶é—´æ¨ç†å¢å¼ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶ä½¿ç”¨è¾…åŠ©å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è¿›è¡Œè¯„åˆ†å’Œé€‰æ‹©æœ€ä½³è·¯å¾„ã€‚æœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œåˆ†æPRMçš„æ³›åŒ–è¯¯å·®å¯¹è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½çš„å½±å“ã€‚åˆ©ç”¨PAC-Bayesç†è®ºï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºæ³›åŒ–è¾¹ç•Œï¼Œå¹¶è¯æ˜PRMçš„æ³›åŒ–è¯¯å·®é™ä½å¯ä»¥å‡å°‘å¯»æ‰¾æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚åŸºäºè¿™ä¸€åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†è®¡ç®—æ„ŸçŸ¥æ ‘æœç´¢ï¼ˆCATSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŠ¨æ€æ§åˆ¶æœç´¢è¡Œä¸ºçš„æ¼”å‘˜-è¯„è®ºå®¶æ¡†æ¶ã€‚æ¼”å‘˜æ ¹æ®å¥–åŠ±åˆ†å¸ƒå’Œç¨€ç–æ€§ç»Ÿè®¡æ•°æ®è¾“å‡ºé‡‡æ ·è¶…å‚æ•°ï¼Œè€Œè¯„è®ºå®¶åˆ™ä¼°è®¡å®ƒä»¬çš„æ•ˆç”¨ä»¥æŒ‡å¯¼é¢„ç®—åˆ†é…ã€‚åœ¨MATHå’ŒAIMEåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCATSæŒç»­ä¼˜äºå…¶ä»–å¤–éƒ¨TTSæ–¹æ³•ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºé¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤–éƒ¨æµ‹è¯•æ—¶é—´æ¨ç†é€šè¿‡è§£è€¦ç”Ÿæˆå’Œé€‰æ‹©é˜¶æ®µå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ¨ç†è·¯å¾„ï¼Œå¹¶ä½¿ç”¨è¾…åŠ©å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è¿›è¡Œè¯„åˆ†å’Œé€‰æ‹©ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹äº†ç†è®ºæ¡†æ¶æ¥åˆ†æPRMçš„æ³›åŒ–è¯¯å·®å¯¹è®¡ç®—æ•ˆç‡å’Œæ¨ç†æ€§èƒ½çš„å½±å“ã€‚</li>
<li>åˆ©ç”¨PAC-Bayesç†è®ºæ¨å¯¼å‡ºæ³›åŒ–è¾¹ç•Œï¼Œæ˜¾ç¤ºPRMçš„æ³›åŒ–è¯¯å·®é™ä½å¯ä»¥å‡å°‘å¯»æ‰¾æ­£ç¡®ç­”æ¡ˆæ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚</li>
<li>æå‡ºäº†è®¡ç®—æ„ŸçŸ¥æ ‘æœç´¢ï¼ˆCATSï¼‰æ–¹æ³•ï¼Œç»“åˆæ¼”å‘˜-è¯„è®ºå®¶æ¡†æ¶ï¼ŒåŠ¨æ€æ§åˆ¶æœç´¢è¡Œä¸ºã€‚</li>
<li>æ¼”å‘˜æ ¹æ®å¥–åŠ±åˆ†å¸ƒå’Œç¨€ç–æ€§ç»Ÿè®¡æ•°æ®è¾“å‡ºé‡‡æ ·è¶…å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5137683af495b4a3e2635a8dd8530d31.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Assessing-the-performance-of-8-AI-chatbots-in-bibliographic-reference-retrieval-Grok-and-DeepSeek-outperform-ChatGPT-but-none-are-fully-accurate"><a href="#Assessing-the-performance-of-8-AI-chatbots-in-bibliographic-reference-retrieval-Grok-and-DeepSeek-outperform-ChatGPT-but-none-are-fully-accurate" class="headerlink" title="Assessing the performance of 8 AI chatbots in bibliographic reference   retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate"></a>Assessing the performance of 8 AI chatbots in bibliographic reference   retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate</h2><p><strong>Authors:Ãlvaro Cabezas-Clavijo, Pavel Sidorenko-Bautista</strong></p>
<p>This study analyzes the performance of eight generative artificial intelligence chatbots â€“ ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le Chat, and Perplexity â€“ in their free versions, in the task of generating academic bibliographic references within the university context. A total of 400 references were evaluated across the five major areas of knowledge (Health, Engineering, Experimental Sciences, Social Sciences, and Humanities), based on a standardized prompt. Each reference was assessed according to five key components (authorship, year, title, source, and location), along with document type, publication age, and error count. The results show that only 26.5% of the references were fully correct, 33.8% partially correct, and 39.8% were either erroneous or entirely fabricated. Grok and DeepSeek stood out as the only chatbots that did not generate false references, while Copilot, Perplexity, and Claude exhibited the highest hallucination rates. Furthermore, the chatbots showed a greater tendency to generate book references over journal articles, although the latter had a significantly higher fabrication rate. A high degree of overlap was also detected among the sources provided by several models, particularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal structural limitations in current AI models, highlight the risks of uncritical use by students, and underscore the need to strengthen information and critical literacy regarding the use of AI tools in higher education. </p>
<blockquote>
<p>æœ¬ç ”ç©¶åˆ†æäº†å…«ä¸ªç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººâ€”â€”ChatGPTã€Claudeã€Copilotã€DeepSeekã€Geminiã€Grokã€Le Chatå’ŒPerplexityâ€”â€”åœ¨å…¶å…è´¹ç‰ˆæœ¬ä¸­çš„è¡¨ç°ï¼Œä»»åŠ¡æ˜¯åœ¨å¤§å­¦èƒŒæ™¯ä¸‹ç”Ÿæˆå­¦æœ¯å‚è€ƒæ–‡çŒ®ã€‚æœ¬ç ”ç©¶åœ¨äº”å¤§çŸ¥è¯†é¢†åŸŸï¼ˆå¥åº·ã€å·¥ç¨‹ã€å®éªŒç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡ç§‘å­¦ï¼‰ä¸­å…±è¯„ä¼°äº†400ç¯‡å‚è€ƒæ–‡çŒ®ï¼ŒåŸºäºæ ‡å‡†åŒ–çš„æç¤ºè¿›è¡Œè¯„ä¼°ã€‚æ¯ä¸ªå‚è€ƒæ–‡çŒ®æ ¹æ®äº”ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼ˆä½œè€…ã€å¹´ä»½ã€æ ‡é¢˜ã€æ¥æºå’Œä½ç½®ï¼‰ï¼Œä»¥åŠæ–‡æ¡£ç±»å‹ã€å‡ºç‰ˆå¹´é¾„å’Œé”™è¯¯è®¡æ•°è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»…æœ‰26.5%çš„å‚è€ƒæ–‡çŒ®å®Œå…¨æ­£ç¡®ï¼Œ33.8%éƒ¨åˆ†æ­£ç¡®ï¼Œå…¶ä½™39.8%çš„å‚è€ƒæ–‡çŒ®å­˜åœ¨é”™è¯¯æˆ–å®Œå…¨è™šæ„ã€‚Grokå’ŒDeepSeekè¡¨ç°å‡ºè‰²ï¼Œæ˜¯å”¯ä¸€æ²¡æœ‰ç”Ÿæˆé”™è¯¯å‚è€ƒæ–‡çŒ®çš„èŠå¤©æœºå™¨äººã€‚Copilotã€Perplexityå’ŒClaudeçš„è™šæ„ç‡æœ€é«˜ã€‚æ­¤å¤–ï¼ŒèŠå¤©æœºå™¨äººæ›´å€¾å‘äºç”Ÿæˆä¹¦ç±å‚è€ƒæ–‡çŒ®è€ŒéæœŸåˆŠæ–‡ç« ï¼Œå°½ç®¡æœŸåˆŠæ–‡ç« çš„è™šæ„ç‡æ˜¾è‘—æ›´é«˜ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°äº†å¤šä¸ªæ¨¡å‹æä¾›çš„æ¥æºä¹‹é—´å­˜åœ¨é«˜åº¦é‡å ï¼Œç‰¹åˆ«æ˜¯DeepSeekã€Grokã€Geminiå’ŒChatGPTä¹‹é—´ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰äººå·¥æ™ºèƒ½æ¨¡å‹çš„ç»“æ„å±€é™æ€§ï¼Œå¼ºè°ƒäº†å­¦ç”Ÿåœ¨ä½¿ç”¨æ—¶ç¼ºä¹æ‰¹åˆ¤æ€§çš„é£é™©ï¼Œå¹¶å¼ºè°ƒäº†åŠ å¼ºå…³äºé«˜ç­‰æ•™è‚²ä¸­ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·çš„ä¿¡æ¯å’Œæ‰¹åˆ¤æ€§ç´ å…»çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18059v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å…«ç§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººï¼ˆChatGPTã€Claudeã€Copilotç­‰ï¼‰åœ¨æ ¡å›­ç¯å¢ƒä¸‹ç”Ÿæˆå­¦æœ¯å‚è€ƒæ–‡çŒ®çš„è¡¨ç°ã€‚é€šè¿‡å¯¹äº”å¤§çŸ¥è¯†é¢†åŸŸï¼ˆå¥åº·ã€å·¥ç¨‹ã€å®éªŒç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡ç§‘å­¦ï¼‰çš„400ç¯‡å‚è€ƒæ–‡çŒ®è¿›è¡Œæ ‡å‡†åŒ–æç¤ºè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºä»…æœ‰26.5%çš„å‚è€ƒæ–‡çŒ®å®Œå…¨æ­£ç¡®ï¼Œéƒ¨åˆ†æ­£ç¡®ç‡ä¸º33.8%ï¼Œé”™è¯¯æˆ–å®Œå…¨è™šæ„çš„å 39.8%ã€‚Grokå’ŒDeepSeekæ˜¯å”¯ä¸€æ²¡æœ‰ç”Ÿæˆé”™è¯¯å‚è€ƒæ–‡çŒ®çš„èŠå¤©æœºå™¨äººï¼Œè€ŒCopilotã€Perplexityå’ŒClaudeçš„å¹»æƒ³ç‡æœ€é«˜ã€‚æ­¤å¤–ï¼ŒèŠå¤©æœºå™¨äººæ›´å€¾å‘äºç”Ÿæˆä¹¦ç±å‚è€ƒæ–‡çŒ®è€ŒéæœŸåˆŠæ–‡ç« ï¼Œå°½ç®¡æœŸåˆŠæ–‡ç« çš„è™šæ„ç‡æ›´é«˜ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰äººå·¥æ™ºèƒ½æ¨¡å‹çš„ç»“æ„å±€é™æ€§ï¼Œæé†’å­¦ç”Ÿéœ€æ‰¹åˆ¤æ€§ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œå¹¶å¼ºè°ƒé«˜ç­‰æ•™è‚²ä¸­åŠ å¼ºå…³äºäººå·¥æ™ºèƒ½å·¥å…·çš„ä¿¡æ¯å’Œæ‰¹åˆ¤æ€§é˜…è¯»çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…«ç§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èŠå¤©æœºå™¨äººåœ¨ç”Ÿæˆå­¦æœ¯å‚è€ƒæ–‡çŒ®æ—¶å­˜åœ¨ä¸åŒç¨‹åº¦çš„é—®é¢˜ï¼Œä»…æœ‰å°‘æ•°èƒ½å®Œå…¨æ­£ç¡®ç”Ÿæˆå‚è€ƒæ–‡çŒ®ã€‚</li>
<li>åœ¨äº”å¤§çŸ¥è¯†é¢†åŸŸçš„å‚è€ƒæ–‡çŒ®æµ‹è¯•ä¸­ï¼Œéƒ¨åˆ†æ­£ç¡®ç‡å’Œé”™è¯¯ç‡è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºäººå·¥æ™ºèƒ½æ¨¡å‹çš„ç»“æ„æ€§å±€é™ã€‚</li>
<li>Grokå’ŒDeepSeekæ˜¯å”¯ä¸€æ²¡æœ‰ç”Ÿæˆé”™è¯¯å‚è€ƒæ–‡çŒ®çš„èŠå¤©æœºå™¨äººã€‚</li>
<li>Copilotã€Perplexityå’ŒClaudeçš„å¹»æƒ³ç‡è¾ƒé«˜ï¼Œéœ€è­¦æƒ•å…¶ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ã€‚</li>
<li>èŠå¤©æœºå™¨äººæ›´å€¾å‘äºç”Ÿæˆä¹¦ç±å‚è€ƒæ–‡çŒ®ï¼Œè€ŒæœŸåˆŠæ–‡ç« çš„è™šæ„ç‡æ›´é«˜ã€‚</li>
<li>å¤šç§èŠå¤©æœºå™¨äººä¹‹é—´å­˜åœ¨æ–‡çŒ®æ¥æºçš„é«˜åº¦é‡å ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a8fe7b2cf3ce23ec9c68c9ebd733236d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Knot-So-Simple-A-Minimalistic-Environment-for-Spatial-Reasoning"><a href="#Knot-So-Simple-A-Minimalistic-Environment-for-Spatial-Reasoning" class="headerlink" title="Knot So Simple: A Minimalistic Environment for Spatial Reasoning"></a>Knot So Simple: A Minimalistic Environment for Spatial Reasoning</h2><p><strong>Authors:Zizhao Chen, Yoav Artzi</strong></p>
<p>We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at <a target="_blank" rel="noopener" href="https://github.com/lil-lab/knotgym">https://github.com/lil-lab/knotgym</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºKnotGymï¼Œä¸€ä¸ªç”¨äºå¤æ‚ç©ºé—´æ¨ç†å’Œæ“ä½œçš„äº¤äº’å¼ç¯å¢ƒã€‚KnotGymåŒ…å«é¢å‘ç›®æ ‡ç»³å­æ“ä½œä»»åŠ¡ï¼Œä»»åŠ¡å¤æ‚åº¦å„å¼‚ï¼Œå‡éœ€ä»çº¯å›¾åƒè§‚å¯Ÿä¸­è¿›è¡Œæ“ä½œã€‚ä»»åŠ¡æŒ‰ç…§ç»“äº¤å‰çš„æ•°é‡è¿™ä¸€æ¸…æ™°ä¸”å¯é‡åŒ–çš„å¤æ‚åº¦è½´çº¿è¿›è¡Œå®šä¹‰ï¼Œä»è€Œå½¢æˆä¸€ä¸ªè‡ªç„¶çš„æ³›åŒ–æµ‹è¯•ã€‚KnotGymå…·æœ‰ç®€å•çš„è§‚å¯Ÿç©ºé—´ï¼Œä¾¿äºè§„æ¨¡åŒ–å¼€å‘ï¼Œä½†å®ƒçªå‡ºäº†åœ¨æ•´åˆæ•é”æ„ŸçŸ¥ã€ç©ºé—´æ¨ç†å’Œå®é™…æ“ä½œæ–¹é¢çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒç±»åˆ«çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¨¡å‹çš„RLã€æ¨¡å‹é¢„æµ‹æ§åˆ¶å’Œæ€ç»´é“¾æ¨ç†ï¼Œå¹¶è¯´æ˜äº†KnotGymæ‰€å‘ˆç°çš„æŒ‘æˆ˜ã€‚å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/lil-lab/knotgym">https://github.com/lil-lab/knotgym</a>è·å–KnotGymã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18028v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªåä¸ºKnotGymçš„äº¤äº’å¼ç¯å¢ƒï¼Œç”¨äºå¤æ‚çš„ç©ºé—´æ¨ç†å’Œæ“ä½œã€‚KnotGymåŒ…å«é¢å‘ç›®æ ‡çš„ä¸åŒå¤æ‚ç¨‹åº¦çš„ç»³ç´¢æ“ä½œä»»åŠ¡ï¼Œä»»åŠ¡åŸºäºç»“ç‚¹æ•°æ²¿æ¸…æ™°çš„é‡åŒ–è½´å®šä¹‰å¤æ‚åº¦ï¼Œæä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„æ³›åŒ–æµ‹è¯•ç¯å¢ƒã€‚æ­¤å¤–ï¼ŒKnotGymå…·æœ‰ç®€å•çš„è§‚å¯Ÿç©ºé—´ï¼Œä¾¿äºè¿›è¡Œè§„æ¨¡åŒ–å¼€å‘ï¼ŒåŒæ—¶çªæ˜¾äº†æ•´åˆæ•é”æ„ŸçŸ¥ã€ç©ºé—´æ¨ç†å’Œå®é™…æ“ä½œçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚æœ¬æ–‡è¯„ä¼°äº†ä¸åŒç±»åˆ«çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¨¡å‹çš„RLã€æ¨¡å‹é¢„æµ‹æ§åˆ¶å’Œé“¾å¼æ€ç»´æ¨ç†ç­‰ï¼Œå¹¶å±•ç¤ºäº†KnotGymæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/lil-lab/knotgym%E8%8E%B7%E5%8F%96%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/lil-lab/knotgymè·å–èµ„æºã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KnotGymæ˜¯ä¸€ä¸ªç”¨äºå¤æ‚ç©ºé—´æ¨ç†å’Œæ“ä½œçš„äº¤äº’å¼ç¯å¢ƒã€‚</li>
<li>å®ƒåŒ…å«ä¸åŒå¤æ‚ç¨‹åº¦çš„é¢å‘ç›®æ ‡çš„ç»³ç´¢æ“ä½œä»»åŠ¡ã€‚</li>
<li>KnotGymçš„ä»»åŠ¡åŸºäºç»“ç‚¹æ•°å®šä¹‰å¤æ‚åº¦ï¼Œæä¾›äº†ä¸€ä¸ªè‡ªç„¶çš„æ³›åŒ–æµ‹è¯•ç¯å¢ƒã€‚</li>
<li>KnotGymå…·æœ‰ç®€å•çš„è§‚å¯Ÿç©ºé—´ï¼Œä¾¿äºå¼€å‘å¹¶å‡¸æ˜¾äº†æ„ŸçŸ¥å’Œç©ºé—´æ¨ç†çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>æ–‡ä¸­è¯„ä¼°äº†ä¸åŒç±»åˆ«çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºæ¨¡å‹çš„RLã€æ¨¡å‹é¢„æµ‹æ§åˆ¶å’Œé“¾å¼æ€ç»´æ¨ç†ç­‰ã€‚</li>
<li>KnotGymé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åœ¨äºæ•´åˆæ•é”æ„ŸçŸ¥ã€ç©ºé—´æ¨ç†å’Œå®é™…æ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8ec1e4f5500fed1f4c98144ab880746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfa7b85b4c9b59b2a05ffbe8b8c09dd4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b86c85e681bad5ff136b73228e8bb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f5139b0c5da716e199246cc26ad9916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d99f0f78f7e67acb6f10567e0d23a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcf0c6d418862603db44195cb70c6fb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf051098cb7b9d41af3756cdd48192ba.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Analyzing-and-Understanding-the-Limitations-of-VAPO-A-Theoretical-Perspective"><a href="#Towards-Analyzing-and-Understanding-the-Limitations-of-VAPO-A-Theoretical-Perspective" class="headerlink" title="Towards Analyzing and Understanding the Limitations of VAPO: A   Theoretical Perspective"></a>Towards Analyzing and Understanding the Limitations of VAPO: A   Theoretical Perspective</h2><p><strong>Authors:Jintian Shao, Yiming Cheng, Hongyi Huang, Beiwen Zhang, Zhiyu Wu, You Shan, Mingkai Zheng</strong></p>
<p>The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization. </p>
<blockquote>
<p>VAPOæ¡†æ¶åœ¨æå‡é•¿é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ä»»åŠ¡ä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡ä¸å¯é æ€§æ–¹é¢ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„å®è¯æˆåŠŸã€‚é€šè¿‡ç³»ç»Ÿè§£å†³ä»·å€¼æ¨¡å‹åè§ã€åºåˆ—é•¿åº¦å¤šæ ·åŒ–å’Œç¨€ç–å¥–åŠ±ä¿¡å·ç­‰æŒ‘æˆ˜ï¼ŒVAPOå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶å…¶å®è·µæ•ˆç›Šæ˜¾è€Œæ˜“è§ï¼Œä½†å¯¹å…¶å†…åœ¨æœºåˆ¶å’Œæ½œåœ¨å±€é™çš„æ·±å…¥ç†è®ºç†è§£å¯¹äºæŒ‡å¯¼æœªæ¥å‘å±•è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨ä»ç†è®ºè§’åº¦æ¢è®¨VAPOï¼Œå¼ºè°ƒå…¶å‡è®¾å¯èƒ½é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä»¥åŠè¿›ä¸€æ­¥è°ƒæŸ¥å¯èƒ½äº§ç”Ÿæ›´ç¨³å¥å’Œå¯æ¨å¹¿çš„æ¨ç†ä»£ç†çš„é¢†åŸŸã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å¤æ‚æ¨ç†ç©ºé—´ä¸­ä»·å€¼å‡½æ•°é€¼è¿‘çš„ç»†å¾®ä¹‹å¤„ã€è‡ªé€‚åº”ä¼˜åŠ¿ä¼°è®¡çš„æœ€ä¼˜æ€§ã€æ ‡è®°çº§ä¼˜åŒ–çš„å½±å“ä»¥åŠæ¢ç´¢å’Œæ¦‚æ‹¬çš„æŒä¹…æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VAPOæ¡†æ¶åœ¨æå‡é•¿é“¾æ€ç»´ä»»åŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ•ˆç‡ä¸å¯é æ€§æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„å®é™…æ•ˆæœã€‚é€šè¿‡ç³»ç»Ÿæ€§è§£å†³ä»·å€¼æ¨¡å‹åè§ã€åºåˆ—é•¿åº¦å·®å¼‚å’Œå¥–åŠ±ä¿¡å·ç¨€ç–ç­‰æŒ‘æˆ˜ï¼ŒVAPOæ¡†æ¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æœ¬æ–‡æ—¨åœ¨ä»ç†è®ºè§’åº¦æ¢è®¨VAPOæ¡†æ¶ï¼Œå¼ºè°ƒå…¶å‡è®¾å¯èƒ½é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶è¿›ä¸€æ­¥ç ”ç©¶èƒ½å¸¦æ¥æ›´å…·ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ¨ç†ä»£ç†çš„é¢†åŸŸã€‚é€šè¿‡æ¢ç´¢ä»·å€¼å‡½æ•°è¿‘ä¼¼åœ¨å¤æ‚æ¨ç†ç©ºé—´ä¸­çš„ç»†å¾®å·®åˆ«ã€è‡ªé€‚åº”ä¼˜åŠ¿ä¼°è®¡çš„æœ€ä¼˜æ€§ã€ä»¤ç‰Œçº§ä¼˜åŒ–çš„å½±å“ä»¥åŠæ¢ç´¢å’Œæ³›åŒ–çš„æŒä¹…æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VAPOæ¡†æ¶åœ¨å¼ºåŒ–å­¦ä¹ é•¿é“¾æ€ç»´ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>VAPOè§£å†³äº†ä»·å€¼æ¨¡å‹åè§ã€åºåˆ—é•¿åº¦å·®å¼‚å’Œå¥–åŠ±ä¿¡å·ç¨€ç–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>VAPOæ¡†æ¶å®ç°å“è¶Šæ€§èƒ½ï¼Œæœ¬æ–‡ä»ç†è®ºè§’åº¦å¯¹å…¶è¿›è¡Œæ¢è®¨ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥æ¢ç´¢VAPOæ¡†æ¶çš„å‡è®¾å¯èƒ½é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>ä»·å€¼å‡½æ•°è¿‘ä¼¼åœ¨å¤æ‚æ¨ç†ç©ºé—´ä¸­çš„ç»†å¾®å·®åˆ«æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>è‡ªé€‚åº”ä¼˜åŠ¿ä¼°è®¡çš„æœ€ä¼˜æ€§å’Œä»¤ç‰Œçº§ä¼˜åŒ–çš„å½±å“ä¹Ÿæ˜¯ç ”ç©¶é‡ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e4bbbf3b445d3854e808917ae64f2d5f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Modelsâ€™ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight &amp; Knave and MATH datasets demonstrate re-distillationâ€™s surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a> </p>
<blockquote>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€ŒåŸºäºè§„åˆ™çš„RLèƒŒåçš„æœºåˆ¶ä»ç„¶ä¸æ¸…æ¥šã€‚æˆ‘ä»¬å‘ç°å°è§„æ¨¡SFTå¯¹RLæœ‰é‡å¤§å½±å“ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£é‡Šæˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚å‡è®¾åˆ†æè¡¨æ˜ï¼ŒSFTçš„æ•ˆç‡å—åˆ°è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚åœ¨æˆ‘ä»¬çš„åˆ†ææŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å°è§„æ¨¡è’¸é¦å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æŠ€æœ¯ï¼Œè’¸é¦æ¥è‡ªRLè®­ç»ƒçš„ç­–ç•¥ã€‚åœ¨Knightå’ŒKnaveä»¥åŠMATHæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å†è’¸é¦çš„æƒŠäººæ•ˆç‡ï¼šå†è’¸é¦æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬å’Œè®¡ç®—èµ„æºå°±èƒ½è¾¾åˆ°RLçš„æ€§èƒ½ã€‚ç»éªŒéªŒè¯è¡¨æ˜ï¼Œæ ·æœ¬æ•ˆåº”æ˜¯æ€§èƒ½æ”¹è¿›çš„è‰¯å¥½æŒ‡æ ‡ã€‚å› æ­¤ï¼Œåœ¨K&amp;Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„å†è’¸é¦Qwen2.5-1.5Bæ¨¡å‹ä»…ä½¿ç”¨1Kä¸ªSFTæ ·æœ¬å°±è¶…è¿‡äº†DeepSeek-V3-0324ã€‚åœ¨MATHä¸Šï¼Œä½¿ç”¨å†è’¸é¦çš„500ä¸ªæ ·æœ¬å¯¹Qwen2.5-1.5Bè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥åŒ¹é…å…¶æœªä½¿ç”¨RLçš„æŒ‡ä»¤è°ƒæ•´å˜ä½“ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£é‡Šäº†R1é£æ ¼RLä¸­çš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v1">PDF</a> 11 figs, 3 table, preprint</p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ç ”ç©¶å‘ç°å°è§„æ¨¡æ ·æœ¬æ›¿ä»£è®­ç»ƒï¼ˆSFTï¼‰å¯¹RLæœ‰å½±å“ä½†æ•ˆç‡ä¸é«˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºåˆ†ææ¡†æ¶å¹¶é€šè¿‡æ ·æœ¬æ•ˆåº”æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚åŸºäºåˆ†æï¼Œæå‡ºå†è’¸é¦æŠ€æœ¯ï¼Œé€šè¿‡ä»RLè®­ç»ƒçš„ç­–ç•¥ä¸­è¿›è¡Œå°è§„æ¨¡è’¸é¦æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¡¨æ˜å†è’¸é¦æŠ€æœ¯éå¸¸é«˜æ•ˆï¼Œå†è’¸é¦æ¨¡å‹ä½¿ç”¨æ›´å°‘çš„æ ·æœ¬å’Œè®¡ç®—å°±èƒ½åŒ¹é…RLæ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å°è§„æ¨¡æ ·æœ¬æ›¿ä»£è®­ç»ƒå¯¹å¼ºåŒ–å­¦ä¹ æœ‰å½±å“ï¼Œä½†æ•ˆç‡ä¸é«˜ã€‚</li>
<li>æå‡ºåˆ†ææ¡†æ¶æ¥æ¯”è¾ƒæ ·æœ¬æ›¿ä»£è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ã€‚</li>
<li>å†è’¸é¦æŠ€æœ¯è¢«æå‡ºï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä»¥åŒ¹é…å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>å†è’¸é¦æŠ€æœ¯æ˜¾è‘—æé«˜æ•ˆç‡ï¼Œä½¿ç”¨æ›´å°‘çš„æ ·æœ¬å’Œè®¡ç®—å°±èƒ½è¾¾åˆ°å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚</li>
<li>æ ·æœ¬æ•ˆåº”æ˜¯è¯„ä¼°æ€§èƒ½æ”¹è¿›çš„è‰¯å¥½æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-55e4e2e8fad8a994a4e5db1c8a677289.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bebbc683fc0f91d247784da178d4c77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Beyond-Distillation-Pushing-the-Limits-of-Medical-LLM-Reasoning-with-Minimalist-Rule-Based-RL"><a href="#Beyond-Distillation-Pushing-the-Limits-of-Medical-LLM-Reasoning-with-Minimalist-Rule-Based-RL" class="headerlink" title="Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with   Minimalist Rule-Based RL"></a>Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with   Minimalist Rule-Based RL</h2><p><strong>Authors:Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci</strong></p>
<p>Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼Œæé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½å¹¶å®ç°å¯è§£é‡Šçš„å†³ç­–éœ€è¦æœ‰æ•ˆçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨ä¸éœ€è¦æ˜‚è´µçš„æ¥è‡ªå°é—­æºæ¨¡å‹çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æƒ…å†µä¸‹ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AlphaMedï¼Œè¿™æ˜¯é¦–ä¸ªæ˜¾ç¤ºå¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å‡ºç°æ¨ç†èƒ½åŠ›çš„åŒ»ç–—LLMã€‚å®ƒåŸºäºå…¬å…±å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”æ•°æ®é›†çš„æœ€å°åŒ–è§„åˆ™å¥–åŠ±ï¼Œä¸ä¾èµ–äºSFTæˆ–è’¸é¦çš„CoTæ•°æ®ã€‚AlphaMedåœ¨å…­ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œä¼˜äºä½¿ç”¨ä¼ ç»ŸSFT+RLç®¡é“è®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚MedXpertï¼‰ä¸­ï¼ŒAlphaMedç”šè‡³è¶…è¶Šäº†æ›´å¤§çš„æˆ–å°é—­æºæ¨¡å‹ï¼Œå¦‚DeepSeek-V3-671Bå’ŒClaude-3.5-Sonnetã€‚ä¸ºäº†äº†è§£è¿™ä¸€æˆåŠŸçš„èƒŒåå› ç´ ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªé—®é¢˜çš„æŒ‡å¯¼ä¸‹è¿›è¡Œäº†å…¨é¢çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„åˆ†æï¼šï¼ˆiï¼‰æ˜¯å¦å¯ä»¥ä»…é€šè¿‡åŸºäºæœ€å°åŒ–è§„åˆ™çš„RLæ¥æ¿€åŠ±æ¨ç†ï¼Œè€Œæ— éœ€è’¸é¦çš„CoTç›‘ç£ï¼Ÿï¼ˆiiï¼‰æ•°æ®é›†çš„æ•°é‡å’Œå¤šæ ·æ€§å¦‚ä½•å½±å“æ¨ç†ï¼Ÿï¼ˆiiiï¼‰é—®é¢˜çš„éš¾åº¦å¦‚ä½•å½±å“æ¨ç†çš„å‡ºç°å’Œæ³›åŒ–ï¼Ÿæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ•°æ®é›†çš„ä¿¡æ¯æ€§æ˜¯æ¨åŠ¨æ¨ç†æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œè€Œåœ¨ä¿¡æ¯ä¸°å¯Œçš„å¤šé¡¹é€‰æ‹©é¢˜é—®ç­”æ•°æ®ä¸Šé‡‡ç”¨åŸºäºæœ€å°åŒ–è§„åˆ™çš„RLå¯ä»¥æœ‰æ•ˆåœ°åœ¨æ²¡æœ‰CoTç›‘ç£çš„æƒ…å†µä¸‹æ¿€å‘æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°ä¸åŒçš„è¶‹åŠ¿ï¼Œè¿™çªæ˜¾äº†å½“å‰è¯„ä¼°çš„å±€é™æ€§ä»¥åŠéœ€è¦æ›´å…·æŒ‘æˆ˜æ€§ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17952v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæä¾›çš„æ–‡æœ¬å†…å®¹ï¼ŒAlphaMedä½œä¸ºä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå±•ç¤ºäº†åœ¨ä¸éœ€è¦ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰æˆ–è’¸é¦æ€ç»´æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚åœ¨å…­ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAlphaMedè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„é«˜æ°´å¹³è¡¨ç°ï¼Œç”šè‡³åœ¨ä¸€äº›æŒ‘æˆ˜è¾ƒå¤§çš„æµ‹è¯•ä¸­è¶…è¿‡äº†æ›´å¤§çš„å°é—­æºæ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œçš„é‡ç‚¹åœ¨è§£å†³å¦‚ä½•é€šè¿‡å…¬å¼€çš„å¤šé€‰é¢˜é—®ç­”æ•°æ®é›†å®ç°çº¯ç²¹çš„åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶æ¥æ¨åŠ¨æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚å¯¹æ­¤èƒŒåçš„æˆåŠŸå› ç´ è¿›è¡Œäº†æ•°æ®ä¸ºä¸­å¿ƒçš„ç»¼åˆåˆ†æï¼ŒåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ æ˜¯å¦èƒ½æ¿€åŠ±æ— è’¸é¦æ€ç»´ç›‘ç£çš„æ¨ç†èƒ½åŠ›çš„å‘å±•ã€æ•°æ®é›†çš„æ•°é‡å’Œå¤šæ ·æ€§å¯¹æ¨ç†çš„å½±å“ä»¥åŠé—®é¢˜éš¾åº¦å¯¹æ¨ç†èƒ½åŠ›çš„å‡ºç°å’Œæ³›åŒ–çš„å½±å“ç­‰ä¸‰ä¸ªå…³é”®é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ•°æ®é›†ä¿¡æ¯æ€§æ˜¯å½±å“æ¨ç†æ€§èƒ½çš„å…³é”®å› ç´ ï¼Œåœ¨å¤šä¸ªé€‰æ‹©é—®ç­”æ•°æ®ä¸Šçš„ç®€åŒ–RLå¯¹äºæ¿€åŠ±æ— ç›‘ç£æ€ç»´çš„æ¨ç†èƒ½åŠ›å…·æœ‰æœ‰æ•ˆæ•ˆæœã€‚æ€»ç»“AlphaMedçš„æ ¸å¿ƒä¼˜ç‚¹ä¸ºåˆ©ç”¨ç®€æ˜“è§„åˆ™å¥–åŠ±æ¥æ¨è¿›LLMæ¨ç†èƒ½åŠ›çš„æå‡ä¸”ä¸éœ€è¦ç‰¹å®šçš„ä»»åŠ¡æ•°æ®æºï¼Œå¦‚åœ¨ä¸´åºŠæƒ…å¢ƒä¸‹å…³äºç—…æƒ…æ²»ç–—æˆ–è€…å…¶å®ƒæƒ…å†µï¼Œéå¸¸å®ç”¨ä¸”å…·æœ‰æ½œåŠ›ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹ŸæŒ‡å‡ºäº†å½“å‰è¯„ä¼°å’ŒåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜æ€§ä¸è¶³ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥éœ€è¦æ›´å¤šæŒ‘æˆ˜æ€§çš„åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ¨ç†èƒ½åŠ›çš„é‡è¦æ€§ã€‚è¿™é¡¹ç ”ç©¶å°†ä¸ºæ¨åŠ¨æ— ç›‘ç£å­¦ä¹ çš„æ¨ç†èƒ½åŠ›å‘å±•åŠä¸´åºŠåº”ç”¨åœºæ™¯å¼€è¾Ÿæ–°çš„å¯èƒ½æ€§å’Œé“è·¯ã€‚è¿™æ˜¯å¯¹æœªæ¥æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡çš„å‘å±•å’Œå…·ä½“é¢†åŸŸåº”ç”¨çš„åŒé‡æ¨åŠ¨åŠ›ã€‚ç›®å‰ç ”ç©¶æˆæœæä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„å¯èƒ½æ€§æ–¹å‘ä¾›æˆ‘ä»¬æ¢ç´¢å’Œæ·±åŒ–ç†è§£å…¶å¯èƒ½æ€§å’Œæ½œåŠ›ã€‚æœªæ¥æœ‰æœ›çœ‹åˆ°æ›´å¤šå…³äºæ— ç›‘ç£å­¦ä¹ æ¨ç†èƒ½åŠ›çš„çªç ´å’Œå®é™…åº”ç”¨ã€‚AlphaMedçš„æ¨å‡ºæ— ç–‘ä¸ºåŒ»å­¦é¢†åŸŸçš„é—®ç­”ç³»ç»Ÿå¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚æ€»çš„æ¥è¯´ï¼ŒAlphaMedä½œä¸ºä¸€ç§æ–°å‹çš„LLMæ¨¡å‹å±•ç°äº†å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ä¸æ½œåŠ›ã€‚æ€»ç»“ä¸è¶³ä¸€ç™¾å­—ä½†æ ¸å¿ƒè¦ç‚¹çªå‡ºã€‚å…³äºå…·ä½“è¡¨ç°å’Œæ•°æ®ç»†èŠ‚å»ºè®®æŸ¥é˜…åŸæ–‡æˆ–ç›¸å…³ç ”ç©¶æŠ¥å‘Šä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„çªç ´ï¼Œä¸ºæœªæ¥çš„åŒ»ç–—AIå‘å±•å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>AlphaMedæ˜¯é¦–ä¸ªå±•ç¤ºçº¯ç²¹é€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°æ¨ç†èƒ½åŠ›çš„å¤§å‹åŒ»ç–—è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å®ƒæ— éœ€ç›‘ç£ç²¾ç»†è°ƒæ•´æˆ–è’¸é¦æ€ç»´æ•°æ®ã€‚</li>
<li>AlphaMedåœ¨å…­ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„é«˜æ°´å¹³è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†æŸäº›å¤§å‹å°é—­æºæ¨¡å‹ã€‚</li>
<li>AlphaMedé€šè¿‡å…¬å¼€çš„å¤šé€‰é¢˜é—®ç­”æ•°æ®é›†å®ç°åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶æ¥æ¨åŠ¨æ¨ç†èƒ½åŠ›çš„å‘å±•ã€‚è¿™ç§æœºåˆ¶æ— éœ€ç‰¹å®šçš„ä»»åŠ¡æ•°æ®æºè¿›è¡Œç›‘ç£è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1c69732f2e2f2e589e277e496fc21aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8f5ff05059e12f644e97402c5142c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d94bed830b4c9942b567d9d3ffb186a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05f8b321deb374050d03a2d182b842b6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="VeriThinker-Learning-to-Verify-Makes-Reasoning-Model-Efficient"><a href="#VeriThinker-Learning-to-Verify-Makes-Reasoning-Model-Efficient" class="headerlink" title="VeriThinker: Learning to Verify Makes Reasoning Model Efficient"></a>VeriThinker: Learning to Verify Makes Reasoning Model Efficient</h2><p><strong>Authors:Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, Xinchao Wang</strong></p>
<p>Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/czg1225/VeriThinker">https://github.com/czg1225/VeriThinker</a> </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œå¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ä¼šè¿‡åº¦æ€è€ƒï¼Œå¯¼è‡´æ¨ç†é“¾è¿‡äºå†—é•¿ï¼Œä»è€Œæ˜¾è‘—å¢åŠ æ¨ç†æˆæœ¬ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VeriThinkerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„CoTå‹ç¼©æ–¹æ³•ã€‚ä¸åŒäºä½¿ç”¨åˆæˆç®€æ´CoTæ•°æ®ç›´æ¥åœ¨åŸå§‹æ¨ç†ä»»åŠ¡ä¸Šå¾®è°ƒLRMsçš„å¸¸è§„æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°ä»…é€šè¿‡è¾…åŠ©éªŒè¯ä»»åŠ¡æ¥å¾®è°ƒæ¨¡å‹ã€‚é€šè¿‡è®­ç»ƒLRMså‡†ç¡®éªŒè¯CoTè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ï¼ŒLRMsèƒ½å¤Ÿæ›´è‡ªç„¶åœ°åˆ¤æ–­åç»­è‡ªæˆ‘åæ€æ­¥éª¤çš„å¿…è¦æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°æŠ‘åˆ¶è¿‡åº¦æ€è€ƒã€‚å¤§é‡å®éªŒéªŒè¯ï¼ŒVeriThinkeråœ¨ä¿æŒæˆ–ç•¥å¾®æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå®è´¨æ€§åœ°å‡å°‘äº†æ¨ç†é“¾çš„é•¿åº¦ã€‚å½“åº”ç”¨äºDeepSeek-R1-Distill-Qwen-7Bæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†MATH500çš„æ¨ç†ä»¤ç‰Œä»3790å‡å°‘åˆ°2125ï¼Œå‡†ç¡®ç‡æé«˜0.8%ï¼ˆä»94.0%æé«˜åˆ°94.8%ï¼‰ï¼Œåœ¨AIME25ä¸Šï¼Œä»¤ç‰Œä»14321å‡å°‘åˆ°10287ï¼Œå‡†ç¡®ç‡æé«˜2.1%ï¼ˆä»38.7%æé«˜åˆ°40.8%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼ŒVeriThinkerå¯ä»¥é›¶æ ·æœ¬æ³›åŒ–åˆ°æ¨æµ‹æ€§æ¨ç†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/czg1225/VeriThinker">https://github.com/czg1225/VeriThinker</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17941v1">PDF</a> Working in progress. Code Repo:   <a target="_blank" rel="noopener" href="https://github.com/czg1225/VeriThinker">https://github.com/czg1225/VeriThinker</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è¿›è¡Œå¤æ‚ä»»åŠ¡è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œå¯¼è‡´æ¨ç†é“¾è¿‡é•¿ï¼Œæ¨ç†æˆæœ¬å¢åŠ ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„CoTå‹ç¼©æ–¹æ³•â€”â€”VeriThinkerã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ç›´æ¥ä½¿ç”¨åˆæˆç®€æ´çš„CoTæ•°æ®å¯¹LRMsè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬åˆ›æ–°åœ°ä»…é€šè¿‡è¾…åŠ©éªŒè¯ä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡è®­ç»ƒLRMså‡†ç¡®éªŒè¯CoTè§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§ï¼Œæ¨¡å‹èƒ½å¤Ÿè‡ªæˆ‘åæ€ï¼Œæœ‰æ•ˆæŠ‘åˆ¶è¿‡åº¦æ€è€ƒã€‚å®éªŒè¯æ˜ï¼ŒVeriThinkeråœ¨å‡å°‘æ¨ç†é“¾é•¿åº¦çš„åŒæ—¶ï¼Œç»´æŒæˆ–ç•¥å¾®æé«˜äº†å‡†ç¡®æ€§ã€‚åº”ç”¨äºDeepSeek-R1-Distill-Qwen-7Bå’ŒAIME25æ•°æ®é›†ä¸Šï¼ŒVeriThinkerå®ç°äº†æ˜¾è‘—çš„æ¨ç†ä»¤ç‰Œå‡å°‘å’Œå‡†ç¡®æ€§æå‡ã€‚æ­¤å¤–ï¼ŒVeriThinkerè¿˜å¯ä»¥é›¶å°„å‡»æ³›åŒ–è‡³æ¨æµ‹æ€§æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨è¿‡åº¦æ€è€ƒå’Œæ¨ç†é“¾è¿‡é•¿çš„é—®é¢˜ã€‚</li>
<li>VeriThinkeræ˜¯ä¸€ç§æ–°å‹çš„CoTå‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡è¾…åŠ©éªŒè¯ä»»åŠ¡è®­ç»ƒæ¨¡å‹ï¼Œæœ‰æ•ˆæŠ‘åˆ¶è¿‡åº¦æ€è€ƒã€‚</li>
<li>VeriThinkerèƒ½å¤Ÿå‡å°‘æ¨ç†é“¾é•¿åº¦ï¼ŒåŒæ—¶ç»´æŒæˆ–æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨DeepSeek-R1-Distill-Qwen-7Bå’ŒAIME25æ•°æ®é›†ä¸Šï¼ŒVeriThinkerå®ç°äº†æ˜¾è‘—çš„æ¨ç†ä»¤ç‰Œå‡å°‘å’Œå‡†ç¡®æ€§æå‡ã€‚</li>
<li>VeriThinkerå…·æœ‰é›¶å°„å‡»æ³›åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºæ¨æµ‹æ€§æ¨ç†ã€‚</li>
<li>VeriThinkerçš„æ–¹æ³•ä¸åŒäºä¼ ç»Ÿç›´æ¥ä½¿ç”¨åˆæˆç®€æ´çš„CoTæ•°æ®å¯¹LRMsè¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b67e10d9d96fbdba7474921801228a4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2ed67125101e713de9f56b06f3d5a2f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LLM-Meeting-Decision-Trees-on-Tabular-Data"><a href="#LLM-Meeting-Decision-Trees-on-Tabular-Data" class="headerlink" title="LLM Meeting Decision Trees on Tabular Data"></a>LLM Meeting Decision Trees on Tabular Data</h2><p><strong>Authors:Hangting Ye, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang</strong></p>
<p>Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data throughough logical decision tree rules as intermediaries, proposes a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of &#96;&#96;errorsâ€™â€™ reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance. </p>
<blockquote>
<p>è¡¨æ ¼æ•°æ®åœ¨åŒ…æ‹¬åŒ»ç–—ã€é‡‘èç­‰ç°å®ä¸–ç•Œçš„å¤šä¸ªé¢†åŸŸä¸­éƒ½å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸæˆåŠŸï¼Œå°†LLMæ‰©å±•åˆ°è¡¨æ ¼æ•°æ®é¢†åŸŸçš„ç ”ç©¶ä¹Ÿå·²ç»å±•å¼€ã€‚å¤§å¤šæ•°åŸºäºLLMçš„æ–¹æ³•é€šå¸¸é¦–å…ˆå°†è¡¨æ ¼æ•°æ®åºåˆ—åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œç„¶åå¯¹LLMè¿›è¡Œè°ƒæ•´æˆ–ç›´æ¥å¯¹è¿™äº›åºåˆ—åŒ–æ•°æ®è¿›è¡Œæ¨æ–­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å†…åœ¨é—®é¢˜ï¼šï¼ˆiï¼‰æ•°æ®è§†è§’ï¼šç°æœ‰çš„æ•°æ®åºåˆ—åŒ–æ–¹æ³•ç¼ºä¹é’ˆå¯¹ç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„é€šç”¨é€‚ç”¨æ€§ï¼Œå¹¶ä¸”å¯èƒ½é€šè¿‡ç›´æ¥çš„æ–‡æœ¬æš´éœ²é€ æˆéšç§é£é™©ï¼›ï¼ˆiiï¼‰æ¨¡å‹è§†è§’ï¼šLLMå¾®è°ƒæ–¹æ³•åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´å›°éš¾ï¼Œè€ŒåŸºäºä¸Šä¸‹æ–‡çš„å­¦ä¹ çš„å¯æ‰©å±•æ€§å—åˆ°è¾“å…¥é•¿åº¦é™åˆ¶ï¼ˆé€‚ç”¨äºå°æ ·æœ¬å­¦ä¹ ï¼‰ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ä¸ªé€šè¿‡é€»è¾‘å†³ç­–æ ‘è§„åˆ™å°†LLMé›†æˆåˆ°è¡¨æ ¼æ•°æ®ä¸­çš„æ–°æ–¹å‘ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºè¡¨æ ¼é¢„æµ‹çš„å¸¦æœ‰LLMè¡ç”Ÿè§„åˆ™çš„å†³ç­–æ ‘å¢å¼ºå™¨DeLTaã€‚æ‰€æå‡ºçš„DeLTaé¿å…äº†è¡¨æ ¼æ•°æ®åºåˆ—åŒ–ï¼Œå¹¶å¯ç”¨äºå…¨æ•°æ®å­¦ä¹ è®¾ç½®ï¼Œæ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥æ”¹è¿›ç»™å®šå†³ç­–æ ‘è§„åˆ™é›†çš„è®¾è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡LLMç”Ÿæˆçš„æ–°è§„åˆ™æä¾›äº†ä¸€ç§å¯¹åŸå§‹å†³ç­–æ ‘è¿›è¡Œæ ¡å‡†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é€¼è¿‘è¯¯å·®æ ¡æ­£å‘é‡æ¥å¼•å¯¼åŸå§‹å†³ç­–æ ‘é¢„æµ‹æœå‡å°‘â€œé”™è¯¯â€çš„æ–¹å‘å‘å±•ã€‚æœ€åï¼Œåœ¨å¤šç§è¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17918v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¡¨æ ¼æ•°æ®åœ¨ç°å®ä¸–ç•Œä¸­çš„å¤šä¸ªé¢†åŸŸï¼ˆå¦‚åŒ»ç–—å’Œé‡‘èï¼‰ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸæˆåŠŸï¼Œå°†LLMæ‰©å±•åˆ°è¡¨æ ¼æ•°æ®é¢†åŸŸçš„ç ”ç©¶å·²ç»å¼€å§‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºLLMçš„æ–¹æ³•é€šå¸¸å…ˆå°†è¡¨æ ¼æ•°æ®åºåˆ—åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œç„¶åè°ƒæ•´LLMæˆ–ç›´æ¥å¯¹è¿™äº›åºåˆ—åŒ–æ•°æ®è¿›è¡Œæ¨æ–­ã€‚è¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæ•°æ®è§†è§’ï¼šç°æœ‰æ•°æ®åºåˆ—åŒ–æ–¹æ³•ç¼ºä¹ç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„é€šç”¨é€‚ç”¨æ€§ï¼Œå¹¶ä¸”å¯èƒ½é€šè¿‡ç›´æ¥æ–‡æœ¬æš´éœ²é€ æˆéšç§é£é™©ï¼›æ¨¡å‹è§†è§’ï¼šLLMå¾®è°ƒæ–¹æ³•åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é¢ä¸´å›°éš¾ï¼Œè€Œä¸Šä¸‹æ–‡å­¦ä¹ çš„å¯æ‰©å±•æ€§å—åˆ°è¾“å…¥é•¿åº¦çº¦æŸçš„é™åˆ¶ï¼ˆé€‚ç”¨äºå°æ ·æœ¬å­¦ä¹ ï¼‰ã€‚æœ¬ç ”ç©¶é€šè¿‡é€»è¾‘å†³ç­–æ ‘è§„åˆ™æ¢ç´¢äº†æ•´åˆLLMä¸è¡¨æ ¼æ•°æ®çš„æ–°æ–¹å‘ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºè¡¨æ ¼é¢„æµ‹çš„å†³ç­–æ ‘å¢å¼ºå™¨DeLTaã€‚DeLTaé¿å…äº†è¡¨æ ¼æ•°æ®åºåˆ—åŒ–ï¼Œå¹¶é€‚ç”¨äºå…¨æ•°æ®å­¦ä¹ è®¾ç½®ï¼Œæ— éœ€LLMå¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥æ”¹è¿›ç»™å®šå†³ç­–æ ‘è§„åˆ™é›†çš„é‡æ„è§„åˆ™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§é€šè¿‡LLMç”Ÿæˆçš„æ–°è§„åˆ™å¯¹åŸå§‹å†³ç­–æ ‘è¿›è¡Œæ ¡å‡†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¿‘ä¼¼è¯¯å·®æ ¡æ­£å‘é‡æ¥å¼•å¯¼åŸå§‹å†³ç­–æ ‘é¢„æµ‹æœå‘å‡å°‘è¯¯å·®çš„æ–¹å‘ã€‚åœ¨å¤šç§è¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆåŠŸåº”ç”¨äºè¡¨æ ¼æ•°æ®å¤„ç†é¢†åŸŸã€‚</li>
<li>å½“å‰åŸºäºLLMçš„æ–¹æ³•é€šå¸¸æ¶‰åŠè¡¨æ ¼æ•°æ®çš„åºåˆ—åŒ–ï¼Œå­˜åœ¨é€šç”¨é€‚ç”¨æ€§å’Œéšç§é£é™©é—®é¢˜ã€‚</li>
<li>é€šè¿‡é€»è¾‘å†³ç­–æ ‘è§„åˆ™æ•´åˆLLMä¸è¡¨æ ¼æ•°æ®æ˜¯ä¸€ç§æ–°å…´ç ”ç©¶æ–¹å‘ã€‚</li>
<li>æå‡ºçš„DeLTaæ–¹æ³•é¿å…äº†è¡¨æ ¼æ•°æ®åºåˆ—åŒ–ï¼Œé€‚ç”¨äºå…¨æ•°æ®å­¦ä¹ ï¼Œæ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚</li>
<li>DeLTaåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ”¹è¿›å†³ç­–æ ‘è§„åˆ™ï¼Œæé«˜è¡¨æ ¼é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>DeLTaè¿˜æä¾›äº†ä¸€ç§æ ¡å‡†åŸå§‹å†³ç­–æ ‘çš„æ–¹æ³•ï¼Œé€šè¿‡LLMç”Ÿæˆçš„æ–°è§„åˆ™æ¥è¿‘ä¼¼è¯¯å·®æ ¡æ­£å‘é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-710472d342124537138aa7e6c928fb24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b98371acf46e30858aa4bf5708542dbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a87a28a6db8113da3de4ecac1e4a95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1982eab8cd493153ea894798af39f07e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mutarjim-Advancing-Bidirectional-Arabic-English-Translation-with-a-Small-Language-Model"><a href="#Mutarjim-Advancing-Bidirectional-Arabic-English-Translation-with-a-Small-Language-Model" class="headerlink" title="Mutarjim: Advancing Bidirectional Arabic-English Translation with a   Small Language Model"></a>Mutarjim: Advancing Bidirectional Arabic-English Translation with a   Small Language Model</h2><p><strong>Authors:Khalil Hennara, Muhammad Hreden, Mohamed Motaism Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan</strong></p>
<p>We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Mutarjimï¼Œè¿™æ˜¯ä¸€æ¬¾ç´§å‡‘è€Œå¼ºå¤§çš„åŒå‘é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ç¿»è¯‘è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡å¤§å‹LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ˆåŒ…æ‹¬æœºå™¨ç¿»è¯‘ï¼‰æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†å°å‹æ¨¡å‹ä»ç„¶å…·æœ‰ä¼˜åŠ¿ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬åŸºäºé¢å‘é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­çš„å®šåˆ¶è¯­è¨€æ¨¡å‹Kuwain-1.5Bå¼€å‘äº†Mutarjimã€‚å°½ç®¡è§„æ¨¡é€‚ä¸­ï¼ŒMutarjimåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæ›´å¤§çš„æ¨¡å‹ï¼Œè¿™æ˜¯é€šè¿‡ä¼˜åŒ–çš„ä¸¤é˜¶æ®µåŸ¹è®­æ–¹æ³•å’Œç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡è®­ç»ƒè¯­æ–™åº“å®ç°çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMutarjimçš„è¡¨ç°å¯ä¸å¤§äºŒåå€çš„æ¨¡å‹ç›¸æŠ—è¡¡ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬å’ŒåŸ¹è®­è¦æ±‚ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†Tarjama-25ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å…‹æœç°æœ‰é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­åŸºå‡†æµ‹è¯•æ•°æ®é›†çš„å±€é™æ€§ï¼Œå¦‚é¢†åŸŸç‹­çª„ã€å¥å­ç®€çŸ­å’Œè‹±è¯­æºåè§ç­‰ã€‚Tarjama-25åŒ…å«ä¸“å®¶å®¡é˜…çš„äº”åƒå¥å¯¹ï¼Œæ¶µç›–å¹¿æ³›çš„é¢†åŸŸï¼Œæä¾›äº†æ›´å…¨é¢ã€æ›´å¹³è¡¡çš„è¯„ä»·æ¡†æ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMutarjimåœ¨Tarjama-25çš„è‹±è¯­åˆ°é˜¿æ‹‰ä¼¯è¯­ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†æ›´å¤§ã€æ›´ä¸“ä¸šçš„æ¨¡å‹å¦‚GPT-4o miniç­‰ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒTarjama-25ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶å¹¶æ¨åŠ¨é˜¿æ‹‰ä¼¯è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ç³»ç»Ÿè¯„ä¼°çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­åŒå‘ç¿»è¯‘è¯­è¨€æ¨¡å‹Mutarjimè¢«æ¨å‡ºï¼Œå®ƒåŸºäºKuwain-1.5Bå¼€å‘ï¼Œå…·æœ‰å¼ºå¤§çš„æ€§èƒ½ä¸”è§„æ¨¡ç´§å‡‘ã€‚é€šè¿‡ä¼˜åŒ–ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œç²¾å¿ƒç­›é€‰çš„é«˜è´¨é‡è®­ç»ƒè¯­æ–™åº“ï¼ŒMutarjimåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è¶…è¶Šå¤§å‹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†æ–°çš„é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­åŸºå‡†æµ‹è¯•Tarjama-25ï¼Œä»¥å…‹æœç°æœ‰åŸºå‡†æµ‹è¯•é›†çš„å±€é™æ€§ã€‚Mutarjimåœ¨Tarjama-25çš„è‹±è¯­åˆ°é˜¿æ‹‰ä¼¯è¯­ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œç”šè‡³è¶…è¶Šäº†ä¸€äº›æ›´å¤§çš„ä¸“æœ‰æ¨¡å‹ã€‚Tarjama-25å·²å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶å’Œæ¨åŠ¨é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ç¿»è¯‘ç³»ç»Ÿçš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mutarjimæ˜¯ä¸€ä¸ªç”¨äºé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­åŒå‘ç¿»è¯‘çš„ç´§å‡‘è€Œå¼ºå¤§çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åŸºäºKuwain-1.5Bå¼€å‘ï¼Œè¯¥æ¨¡å‹é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­è¿›è¡Œå®šåˆ¶ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œé«˜è´¨é‡è®­ç»ƒè¯­æ–™åº“ï¼ŒMutarjimåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Mutarjimåœ¨Tarjama-25çš„è‹±è¯­åˆ°é˜¿æ‹‰ä¼¯è¯­ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</li>
<li>Tarjama-25æ˜¯ä¸€ä¸ªæ–°çš„é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å…‹æœç°æœ‰åŸºå‡†æµ‹è¯•é›†çš„å±€é™æ€§ã€‚</li>
<li>Tarjama-25åŒ…å«5000ä¸ªä¸“å®¶å®¡é˜…çš„å¥å­å¯¹ï¼Œè¦†ç›–å¹¿æ³›é¢†åŸŸï¼Œæä¾›æ›´å…¨é¢å’Œå¹³è¡¡çš„è¯„ä»·æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-675d793c1bce054dbf05da0b13043bf4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-774604620bbf349a02a826077071dce1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Daily-Omni-Towards-Audio-Visual-Reasoning-with-Temporal-Alignment-across-Modalities"><a href="#Daily-Omni-Towards-Audio-Visual-Reasoning-with-Temporal-Alignment-across-Modalities" class="headerlink" title="Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities"></a>Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment   across Modalities</h2><p><strong>Authors:Ziwei Zhou, Rui Wang, Zuxuan Wu</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) achieve promising performance on visual and audio benchmarks independently. However, the ability of these models to process cross-modal information synchronously remains largely unexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual Questioning and Answering benchmark comprising 684 videos of daily life scenarios from diverse sources, rich in both audio and visual information, and featuring 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA Generation Pipeline, which includes automatic annotation, QA generation and QA optimization, significantly improves efficiency for human evaluation and scalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent utilizing open-source Visual Language Model (VLM), Audio Language Model (ALM) and Automatic Speech Recognition (ASR) model to establish a baseline for this benchmark. The results show that current MLLMs still struggle significantly with tasks requiring audio-visual integration, but combining VLMs and ALMs with simple temporal alignment techniques can achieve substantially better performance. Codes and benchmark are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Lliar-liar/Daily-Omni%7D%7Bhttps://github.com/Lliar-liar/Daily-Omni%7D">https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å’ŒéŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸Šç‹¬ç«‹åœ°å–å¾—äº†æœ‰å‰æ™¯çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¤„ç†è·¨æ¨¡æ€ä¿¡æ¯åŒæ­¥çš„èƒ½åŠ›ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ï¼š1ï¼‰Daily-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªéŸ³é¢‘è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«684ä¸ªæ¥è‡ªä¸åŒæ¥æºçš„æ—¥å¸¸ç”Ÿæ´»åœºæ™¯è§†é¢‘ï¼Œå¯Œå«éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œæ¶µç›–6å¤§ç±»ä»»åŠ¡çš„1197ä¸ªå¤šé¡¹é€‰æ‹©é—®ç­”å¯¹ï¼›2ï¼‰Daily-Omni QAç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ³¨é‡Šã€é—®ç­”ç”Ÿæˆå’Œé—®ç­”ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äººç±»è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•å¯æ‰©å±•æ€§çš„æ•ˆç‡ï¼›3ï¼‰Daily-Omni-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„æ— éœ€è®­ç»ƒä»£ç†ï¼Œä¸ºè¿™ä¸ªåŸºå‡†æµ‹è¯•å»ºç«‹åŸºå‡†ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éœ€è¦è§†å¬èåˆçš„ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä½†é€šè¿‡ç»“åˆVLMså’ŒALMsä¸ç®€å•çš„æ—¶åºå¯¹é½æŠ€æœ¯ï¼Œå¯ä»¥å–å¾—æ˜¾è‘—æ›´å¥½çš„æ€§èƒ½ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lliar-liar/Daily-Omni%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Lliar-liar/Daily-Omniè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17862v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰å’ŒéŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†å¯¹è·¨æ¨¡æ€ä¿¡æ¯åŒæ­¥å¤„ç†çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†ï¼š1ï¼‰Daily-Omniè§†å¬é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«684ä¸ªæ—¥å¸¸ç”Ÿæ´»åœºæ™¯çš„è§†é¢‘ï¼Œå¯Œå«è§†å¬ä¿¡æ¯ï¼Œæ¶µç›–6å¤§ç±»ä»»åŠ¡çš„1197ä¸ªé€‰æ‹©é¢˜ï¼›2ï¼‰Daily-Omnié—®ç­”ç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ ‡æ³¨ã€é—®ç­”ç”Ÿæˆå’Œé—®ç­”ä¼˜åŒ–ï¼Œæé«˜äº†åŸºå‡†æµ‹è¯•çš„äººç±»è¯„ä¼°æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼›3) Daily-Omni-Agentï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä»£ç†ï¼Œåˆ©ç”¨å¼€æºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œä¸ºè¿™ä¸ªåŸºå‡†æµ‹è¯•å»ºç«‹äº†åŸºçº¿ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰MLLMsåœ¨å¤„ç†éœ€è¦è§†å¬æ•´åˆçš„ä»»åŠ¡æ—¶ä»å­˜åœ¨è¾ƒå¤§å›°éš¾ï¼Œä½†é€šè¿‡ç»“åˆVLMså’ŒALMsä»¥åŠç®€å•çš„æ—¶é—´å¯¹é½æŠ€æœ¯ï¼Œå¯ä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç‹¬ç«‹å¤„ç†è§†è§‰å’ŒéŸ³é¢‘åŸºå‡†æµ‹è¯•æ—¶è¡¨ç°è‰¯å¥½ã€‚</li>
<li>è·¨æ¨¡æ€ä¿¡æ¯åŒæ­¥å¤„ç†èƒ½åŠ›ä»æ˜¯MLLMsçš„ä¸€ä¸ªæœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚</li>
<li>Daily-Omniæ˜¯ä¸€ä¸ªæ–°çš„è§†å¬é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸°å¯Œå¤šæ ·çš„è§†å¬ä¿¡æ¯ã€‚</li>
<li>Daily-Omnié—®ç­”ç”Ÿæˆç®¡é“é€šè¿‡è‡ªåŠ¨æ ‡æ³¨ã€é—®ç­”ç”Ÿæˆå’Œé—®ç­”ä¼˜åŒ–æé«˜äº†æ•ˆç‡ã€‚</li>
<li>Daily-Omni-Agentåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸ºåŸºå‡†æµ‹è¯•å»ºç«‹äº†åŸºçº¿ã€‚</li>
<li>å½“å‰MLLMsåœ¨å¤„ç†éœ€è¦è§†å¬æ•´åˆçš„ä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3513e45d61069adc591c2800ff13baec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e330e0c1b74c3d4e7d35ef6d2f4671cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a941fe17a6ee916562be5b4c44d375c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee355df71c7ca27c26ef0d7714919a85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9607e179f91bc443c06972277ff84f8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9ef3c70b3c003059f206d9d938daef6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-752ce21461af1bf25e166324c83be1f6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DialogXpert-Driving-Intelligent-and-Emotion-Aware-Conversations-through-Online-Value-Based-Reinforcement-Learning-with-LLM-Priors"><a href="#DialogXpert-Driving-Intelligent-and-Emotion-Aware-Conversations-through-Online-Value-Based-Reinforcement-Learning-with-LLM-Priors" class="headerlink" title="DialogXpert: Driving Intelligent and Emotion-Aware Conversations through   Online Value-Based Reinforcement Learning with LLM Priors"></a>DialogXpert: Driving Intelligent and Emotion-Aware Conversations through   Online Value-Based Reinforcement Learning with LLM Priors</h2><p><strong>Authors:Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria</strong></p>
<p>Large-language-model (LLM) agents excel at reactive dialogue but struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. We introduce DialogXpert, which leverages a frozen LLM to propose a small, high-quality set of candidate actions per turn and employs a compact Q-network over fixed BERT embeddings trained via temporal-difference learning to select optimal moves within this reduced space. By tracking the userâ€™s emotions, DialogXpert tailors each decision to advance the task while nurturing a genuine, empathetic connection. Across negotiation, emotional support, and tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with success rates exceeding 94% and, with a larger LLM prior, pushes success above 97% while markedly improving negotiation outcomes. This framework delivers real-time, strategic, and emotionally intelligent dialogue planning at scale. Code available at <a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialogxpert/">https://github.com/declare-lab/dialogxpert/</a> </p>
<blockquote>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨ååº”å¼å¯¹è¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸»åŠ¨ã€ç›®æ ‡é©±åŠ¨äº¤äº’æ–¹é¢å´é¢ä¸´å›°éš¾ï¼ŒåŸå› åœ¨äºå…¶è§†é‡æœ‰é™çš„è§£ç å’Œé«˜æ˜‚çš„è§„åˆ’æˆæœ¬ã€‚æˆ‘ä»¬æ¨å‡ºäº†DialogXpertï¼Œå®ƒé€šè¿‡åˆ©ç”¨å†»ç»“çš„LLMæå‡ºæ¯ä¸€å›åˆçš„ä¸€ç»„å°çš„é«˜è´¨é‡è¡ŒåŠ¨å€™é€‰ï¼Œå¹¶ä½¿ç”¨ç´§å‡‘çš„Qç½‘ç»œåœ¨å›ºå®šçš„BERTåµŒå…¥ä¸Šè¿›è¡Œæ—¶é—´å·®åˆ†å­¦ä¹ è®­ç»ƒæ¥é€‰æ‹©åœ¨æ­¤ç¼©å‡ç©ºé—´å†…çš„æœ€ä½³è¡ŒåŠ¨ã€‚é€šè¿‡è·Ÿè¸ªç”¨æˆ·çš„æƒ…ç»ªï¼ŒDialogXpertèƒ½å¤Ÿé’ˆå¯¹æ¯ä¸ªå†³ç­–æ¥æ¨è¿›ä»»åŠ¡ï¼ŒåŒæ—¶åŸ¹å…»çœŸè¯šã€å¯Œæœ‰åŒæƒ…å¿ƒçš„è”ç³»ã€‚åœ¨è°ˆåˆ¤ã€æƒ…æ„Ÿæ”¯æŒå’Œè¾…å¯¼ç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDialogXpertå°†å¯¹è¯æ¬¡æ•°æ§åˆ¶åœ¨ä¸åˆ°ä¸‰è½®ä»¥å†…ï¼ŒæˆåŠŸç‡è¶…è¿‡94%ï¼Œå¹¶ä¸”åœ¨æœ‰æ›´å¤§çš„LLMå…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸç‡æå‡è‡³è¶…è¿‡97%ï¼ŒåŒæ—¶æ˜¾è‘—æ”¹å–„äº†è°ˆåˆ¤ç»“æœã€‚æ­¤æ¡†æ¶å®ç°äº†å¤§è§„æ¨¡å®æ—¶ã€æˆ˜ç•¥æ€§å’Œæƒ…æ„Ÿæ™ºèƒ½çš„å¯¹è¯è§„åˆ’ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/declare-lab/dialogxpert/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/declare-lab/dialogxpert/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17795v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ååº”å¼å¯¹è¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç›®æ ‡é©±åŠ¨å‹äº’åŠ¨æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå­˜åœ¨è§£ç è¿‘è§†å’Œè§„åˆ’æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºDialogXpertæŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å†»ç»“çš„LLMç”Ÿæˆæ¯ä¸ªå›åˆçš„ä¸€å°å¥—é«˜è´¨é‡åŠ¨ä½œå€™é€‰é›†ï¼Œå¹¶ä½¿ç”¨ç´§å‡‘çš„Qç½‘ç»œåœ¨å›ºå®šçš„BERTåµŒå…¥ä¸Šè¿›è¡Œæ—¶åºå·®åˆ†å­¦ä¹ è®­ç»ƒæ¥é€‰æ‹©æœ€ä¼˜åŠ¨ä½œã€‚é€šè¿‡è¿½è¸ªç”¨æˆ·çš„æƒ…ç»ªï¼ŒDialogXpertèƒ½åœ¨æ¨è¿›ä»»åŠ¡çš„åŒæ—¶å®šåˆ¶å†³ç­–å¹¶åŸ¹å…»çœŸè¯šã€å¯Œæœ‰åŒæƒ…å¿ƒçš„è”ç³»ã€‚åœ¨è°ˆåˆ¤ã€æƒ…æ„Ÿæ”¯æŒå’Œè¾…å¯¼ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDialogXpertå°†å¯¹è¯é©±åŠ¨åˆ°ä¸åˆ°ä¸‰å›åˆçš„èŒƒå›´å†…ï¼ŒæˆåŠŸç‡è¶…è¿‡ç™¾åˆ†ä¹‹ä¹åå››ã€‚é…åˆå¤§å‹LLMè¿›è¡Œæå‰å­¦ä¹ ï¼ŒæˆåŠŸç‡æ›´æ˜¯æå‡åˆ°ç™¾åˆ†ä¹‹ä¹åä¸ƒä»¥ä¸Šï¼Œæ˜¾è‘—æ”¹å–„è°ˆåˆ¤ç»“æœã€‚è¯¥æŠ€æœ¯æ¡†æ¶å¯å®ç°å®æ—¶ã€ç­–ç•¥æ€§ä¸”æƒ…æ„Ÿæ™ºèƒ½çš„å¯¹è¯è§„åˆ’è§„æ¨¡åŒ–åº”ç”¨ã€‚ç›¸å…³ä»£ç å·²ä¸Šä¼ è‡³GitHubä»“åº“ï¼š[GitHubä»“åº“é“¾æ¥]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ååº”å¼å¯¹è¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç›®æ ‡é©±åŠ¨å‹äº’åŠ¨ä¸Šå­˜åœ¨å±€é™ã€‚</li>
<li>DialogXpertæŠ€æœ¯é€šè¿‡åˆ©ç”¨LLMç”ŸæˆåŠ¨ä½œå€™é€‰é›†å¹¶ç»“åˆç´§å‡‘çš„Qç½‘ç»œé€‰æ‹©æœ€ä¼˜åŠ¨ä½œæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DialogXpertèƒ½å¤Ÿè¿½è¸ªç”¨æˆ·æƒ…ç»ªï¼Œå¹¶æ ¹æ®æƒ…ç»ªå®šåˆ¶å†³ç­–ä»¥æ¨è¿›ä»»åŠ¡å¹¶åŸ¹å…»çœŸè¯šã€å¯Œæœ‰åŒæƒ…å¿ƒçš„äº¤æµã€‚</li>
<li>DialogXpertæŠ€æœ¯åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œèƒ½å°†å¯¹è¯æ§åˆ¶åœ¨ä¸‰å›åˆå†…ï¼Œä¸”æˆåŠŸç‡æé«˜ã€‚</li>
<li>ç»“åˆå¤§å‹LLMè¿›è¡Œæå‰å­¦ä¹ å¯è¿›ä¸€æ­¥æå‡DialogXpertçš„æˆåŠŸç‡ã€‚</li>
<li>DialogXpertæ˜¾è‘—æ”¹å–„è°ˆåˆ¤ç»“æœï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17795">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f10d6c9927e07137b59d7ee4f93b2615.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-415f690dbe51fba82f04b104050cbab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-116a60dcf776202e09cb0ea70393fc3f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RECIPE-TKG-From-Sparse-History-to-Structured-Reasoning-for-LLM-based-Temporal-Knowledge-Graph-Completion"><a href="#RECIPE-TKG-From-Sparse-History-to-Structured-Reasoning-for-LLM-based-Temporal-Knowledge-Graph-Completion" class="headerlink" title="RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based   Temporal Knowledge Graph Completion"></a>RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based   Temporal Knowledge Graph Completion</h2><p><strong>Authors:Ã–mer Faruk AkgÃ¼l, Feiyu Zhu, Yuxin Yang, Rajgopal Kannan, Viktor Prasanna</strong></p>
<p>Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped relations between entities. TKG completion involves forecasting missing or future links, requiring models to reason over time-evolving structure. While LLMs show promise for this task, existing approaches often overemphasize supervised fine-tuning and struggle particularly when historical evidence is limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient framework designed to improve accuracy and generalization in settings with sparse historical context. It combines (1) rule-based multi-hop retrieval for structurally diverse history, (2) contrastive fine-tuning of lightweight adapters to encode relational semantics, and (3) test-time semantic filtering to iteratively refine generations based on embedding similarity. Experiments on four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based approaches, achieving up to 30.6% relative improvement in Hits@10. Moreover, our proposed framework produces more semantically coherent predictions, even for the samples with limited historical context. </p>
<blockquote>
<p>æ—¶åºçŸ¥è¯†å›¾è°±ï¼ˆTKGsï¼‰ä»£è¡¨å®ä½“ä¹‹é—´å¸¦æœ‰æ—¶é—´æˆ³çš„å…³ç³»ä½œä¸ºåŠ¨æ€äº‹å®ã€‚TKGè¡¥å…¨æ¶‰åŠé¢„æµ‹ç¼ºå¤±æˆ–æœªæ¥çš„é“¾æ¥ï¼Œè¦æ±‚æ¨¡å‹å¯¹æ—¶é—´æ¼”åŒ–çš„ç»“æ„è¿›è¡Œæ¨ç†ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€è¿‡åˆ†å¼ºè°ƒç›‘ç£å¾®è°ƒï¼Œå½“å†å²è¯æ®æœ‰é™æˆ–ç¼ºå¤±æ—¶ï¼Œå°¤å…¶ä¼šé‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†RECIPE-TKGï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”æ•°æ®é«˜æ•ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨ç¨€ç–å†å²èƒŒæ™¯çš„æƒ…å¢ƒä¸‹æé«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®ƒç»“åˆäº†ï¼ˆ1ï¼‰åŸºäºè§„åˆ™çš„å¤šè·³æ£€ç´¢ï¼Œä»¥è·å–ç»“æ„å¤šæ ·çš„å†å²ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰å¯¹æ¯”å¾®è°ƒè½»é‡çº§é€‚é…å™¨ä»¥ç¼–ç å…³ç³»è¯­ä¹‰ï¼›ï¼ˆ3ï¼‰æµ‹è¯•æ—¶çš„è¯­ä¹‰è¿‡æ»¤ï¼ŒåŸºäºåµŒå…¥ç›¸ä¼¼æ€§è¿›è¡Œè¿­ä»£ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚åœ¨å››ä¸ªTKGåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRECIPE-TKGçš„æ€§èƒ½ä¼˜äºä»¥å‰åŸºäºLLMçš„æ–¹æ³•ï¼Œå‘½ä¸­ç‡@10æé«˜äº†é«˜è¾¾30.6%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶äº§ç”Ÿäº†æ›´è¯­ä¹‰è¿è´¯çš„é¢„æµ‹ï¼Œå³ä½¿åœ¨å†å²èƒŒæ™¯æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17794v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†Temporal Knowledge Graphsï¼ˆTKGsï¼‰çš„å®Œæˆä»»åŠ¡ï¼Œæå‡ºRECIPE-TKGæ¡†æ¶ä»¥æé«˜åœ¨ç¨€ç–å†å²ä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è§„åˆ™åŸºç¡€çš„å¤šè·³æ£€ç´¢ã€å¯¹æ¯”å¾®è°ƒè½»é‡çº§é€‚é…å™¨å’Œæµ‹è¯•æ—¶çš„è¯­ä¹‰è¿‡æ»¤æŠ€æœ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒRECIPE-TKGåœ¨å››ä¸ªTKGåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„LLMæ–¹æ³•ï¼Œæé«˜äº†é«˜è¾¾30.6%çš„Hits@10ï¼Œå¹¶ä¸”å¯¹äºæœ‰é™å†å²ä¸Šä¸‹æ–‡çš„æ ·æœ¬ä¹Ÿèƒ½äº§ç”Ÿæ›´è¯­ä¹‰è¿è´¯çš„é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TKGsè¡¨ç¤ºåŠ¨æ€äº‹å®ä½œä¸ºå®ä½“ä¹‹é—´çš„æ—¶é—´æˆ³å…³ç³»ã€‚</li>
<li>TKGå®Œæˆéœ€è¦é¢„æµ‹ç¼ºå¤±æˆ–æœªæ¥çš„é“¾æ¥ï¼Œéœ€è¦æ¨¡å‹å¯¹æ—¶é—´æ¼”åŒ–ç»“æ„è¿›è¡Œæ¨ç†ã€‚</li>
<li>LLMsåœ¨TKGå®Œæˆä»»åŠ¡ä¸Šå±•ç°æ½œåŠ›ï¼Œä½†åœ¨å†å²è¯æ®æœ‰é™æˆ–ç¼ºå¤±çš„æƒ…å†µä¸‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>RECIPE-TKGæ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”æ•°æ®é«˜æ•ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åœ¨ç¨€ç–å†å²ä¸Šä¸‹æ–‡ä¸­çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>RECIPE-TKGç»“åˆäº†è§„åˆ™åŸºç¡€çš„å¤šè·³æ£€ç´¢ã€å¯¹æ¯”å¾®è°ƒé€‚é…å™¨å’Œæµ‹è¯•æ—¶çš„è¯­ä¹‰è¿‡æ»¤æŠ€æœ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRECIPE-TKGåœ¨å¤šä¸ªTKGåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–LLMæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b01d39770c869ea7ccd6ba4e95eb963f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15ea4eb60266d049feb20410e986a8ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfdeca9b267a9ad354b4188bf552ba19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-884162a858b3e8fed549c978ec157e3a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TopoPoint-Enhance-Topology-Reasoning-via-Endpoint-Detection-in-Autonomous-Driving"><a href="#TopoPoint-Enhance-Topology-Reasoning-via-Endpoint-Detection-in-Autonomous-Driving" class="headerlink" title="TopoPoint: Enhance Topology Reasoning via Endpoint Detection in   Autonomous Driving"></a>TopoPoint: Enhance Topology Reasoning via Endpoint Detection in   Autonomous Driving</h2><p><strong>Authors:Yanping Fu, Xinyuan Liu, Tianyu Li, Yike Ma, Yucheng Zhang, Feng Dai</strong></p>
<p>Topology reasoning, which unifies perception and structured reasoning, plays a vital role in understanding intersections for autonomous driving. However, its performance heavily relies on the accuracy of lane detection, particularly at connected lane endpoints. Existing methods often suffer from lane endpoints deviation, leading to incorrect topology construction. To address this issue, we propose TopoPoint, a novel framework that explicitly detects lane endpoints and jointly reasons over endpoints and lanes for robust topology reasoning. During training, we independently initialize point and lane query, and proposed Point-Lane Merge Self-Attention to enhance global context sharing through incorporating geometric distances between points and lanes as an attention mask . We further design Point-Lane Graph Convolutional Network to enable mutual feature aggregation between point and lane query. During inference, we introduce Point-Lane Geometry Matching algorithm that computes distances between detected points and lanes to refine lane endpoints, effectively mitigating endpoint deviation. Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoPoint achieves state-of-the-art performance in topology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate endpoint detection, under which our method significantly outperforms existing approaches (52.6 v.s. 45.2 on DET$_p$). The code is released at <a target="_blank" rel="noopener" href="https://github.com/Franpin/TopoPoint">https://github.com/Franpin/TopoPoint</a>. </p>
<blockquote>
<p>æ‹“æ‰‘æ¨ç†åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ç†è§£äº¤å‰ç‚¹æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå®ƒèƒ½å°†æ„ŸçŸ¥å’Œç»“æ„åŒ–æ¨ç†ç»Ÿä¸€èµ·æ¥ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½ä¸¥é‡ä¾èµ–äºè½¦é“æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿æ¥çš„è½¦é“ç«¯ç‚¹å¤„ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å­˜åœ¨è½¦é“ç«¯ç‚¹åå·®çš„é—®é¢˜ï¼Œå¯¼è‡´æ‹“æ‰‘æ„å»ºä¸æ­£ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TopoPointè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå¯æ˜¾å¼æ£€æµ‹è½¦é“ç«¯ç‚¹ï¼Œå¹¶è”åˆå¯¹ç«¯ç‚¹å’Œè½¦é“è¿›è¡Œæ¨ç†ï¼Œä»¥å®ç°ç¨³å¥çš„æ‹“æ‰‘æ¨ç†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç‹¬ç«‹åˆå§‹åŒ–ç‚¹å’Œè½¦é“æŸ¥è¯¢ï¼Œå¹¶æå‡ºPoint-Lane Merge Self-Attentionï¼Œé€šè¿‡èå…¥ç‚¹ä¸è½¦é“ä¹‹é—´çš„å‡ ä½•è·ç¦»ä½œä¸ºæ³¨æ„åŠ›æ©è†œï¼Œä»¥å¢å¼ºå…¨å±€ä¸Šä¸‹æ–‡å…±äº«ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†Point-Lane Graph Convolutional Networkï¼Œä»¥å®ç°ç‚¹å’Œè½¦é“æŸ¥è¯¢ä¹‹é—´çš„ç‰¹å¾ç›¸äº’èšåˆã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Point-Lane Geometry Matchingç®—æ³•ï¼Œè¯¥ç®—æ³•è®¡ç®—æ£€æµ‹åˆ°çš„ç‚¹ä¸è½¦é“ä¹‹é—´çš„è·ç¦»æ¥ä¼˜åŒ–è½¦é“ç«¯ç‚¹ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ç«¯ç‚¹åå·®é—®é¢˜ã€‚åœ¨OpenLane-V2åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTopoPointåœ¨æ‹“æ‰‘æ¨ç†æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆOLSå¾—åˆ†ä¸º48.8ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†DET$_p$æ¥è¯„ä¼°ç«¯ç‚¹æ£€æµ‹ï¼Œåœ¨è¿™ä¸€è¯„ä¼°æ ‡å‡†ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆDET$_p$å¾—åˆ†ä¸º52.6 vs 45.2ï¼‰ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Franpin/TopoPoint%E3%80%82">https://github.com/Franpin/TopoPointã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17771v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTopoPointçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³è‡ªåŠ¨é©¾é©¶ä¸­çš„æ‹“æ‰‘æ¨ç†é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ˜¾å¼æ£€æµ‹è½¦é“ç«¯ç‚¹ï¼Œå¹¶è”åˆç«¯ç‚¹å’Œè½¦é“è¿›è¡Œæ¨ç†ï¼Œä»¥æé«˜æ‹“æ‰‘æ¨ç†çš„ç¨³å¥æ€§ã€‚é€šè¿‡å¼•å…¥ç‚¹è½¦é“åˆå¹¶è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œç‚¹è½¦é“å›¾å·ç§¯ç½‘ç»œï¼Œæé«˜äº†å…¨çƒä¸Šä¸‹æ–‡å…±äº«å’Œç‰¹å¾èšåˆèƒ½åŠ›ã€‚åœ¨OpenLane-V2åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTopoPointåœ¨æ‹“æ‰‘æ¨ç†æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TopoPointæ¡†æ¶è§£å†³äº†è‡ªåŠ¨é©¾é©¶ä¸­æ‹“æ‰‘æ¨ç†çš„é‡è¦é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯è½¦é“ç«¯ç‚¹çš„å‡†ç¡®æ€§å¯¹æ‹“æ‰‘æ¨ç†çš„å½±å“ã€‚</li>
<li>TopoPointèƒ½å¤Ÿæ˜¾å¼æ£€æµ‹è½¦é“ç«¯ç‚¹ï¼Œå¹¶è”åˆç«¯ç‚¹å’Œè½¦é“è¿›è¡Œæ¨ç†ï¼Œæé«˜æ‹“æ‰‘æ¨ç†çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥äº†ç‚¹è½¦é“åˆå¹¶è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡è€ƒè™‘ç‚¹å’Œè½¦é“ä¹‹é—´çš„å‡ ä½•è·ç¦»ï¼Œå¢å¼ºäº†å…¨çƒä¸Šä¸‹æ–‡çš„å…±äº«ã€‚</li>
<li>è®¾è®¡äº†ç‚¹è½¦é“å›¾å·ç§¯ç½‘ç»œï¼Œå®ç°äº†ç‚¹å’Œè½¦é“æŸ¥è¯¢ä¹‹é—´çš„ç›¸äº’ç‰¹å¾èšåˆã€‚</li>
<li>åœ¨æ¨æ–­è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ç‚¹è½¦é“å‡ ä½•åŒ¹é…ç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æ£€æµ‹åˆ°çš„ç‚¹ä¸è½¦é“ä¹‹é—´çš„è·ç¦»æ¥ä¼˜åŒ–è½¦é“ç«¯ç‚¹ï¼Œæœ‰æ•ˆå‡è½»äº†ç«¯ç‚¹åå·®çš„é—®é¢˜ã€‚</li>
<li>åœ¨OpenLane-V2åŸºå‡†æµ‹è¯•ä¸Šï¼ŒTopoPointåœ¨æ‹“æ‰‘æ¨ç†æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d0891abf7fd798a1bc370ec5dabdbd98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c6124187fdae8ea88ce0424a949e135.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac3715b5abcd7ac9af55b0f96a296903.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebd57257461b6cfea83940c91a96f66c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Fast-Quiet-STaR-Thinking-Without-Thought-Tokens"><a href="#Fast-Quiet-STaR-Thinking-Without-Thought-Tokens" class="headerlink" title="Fast Quiet-STaR: Thinking Without Thought Tokens"></a>Fast Quiet-STaR: Thinking Without Thought Tokens</h2><p><strong>Authors:Wei Huang, Yizhe Xiong, Xin Ye, Zhijie Deng, Hui Chen, Zijia Lin, Guiguang Ding</strong></p>
<p>Large Language Models (LLMs) have achieved impressive performance across a range of natural language processing tasks. However, recent advances demonstrate that further gains particularly in complex reasoning tasks require more than merely scaling up model sizes or training data. One promising direction is to enable models to think during the reasoning process. Recently, Quiet STaR significantly improves reasoning by generating token-level thought traces, but incurs substantial inference overhead. In this work, we propose Fast Quiet STaR, a more efficient reasoning framework that preserves the benefits of token-level reasoning while reducing computational cost. Our method introduces a curriculum learning based training strategy that gradually reduces the number of thought tokens, enabling the model to internalize more abstract and concise reasoning processes. We further extend this approach to the standard Next Token Prediction (NTP) setting through reinforcement learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates the need for explicit thought token generation during inference. Experiments on four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an average accuracy improvement of 9% on Mistral 7B and 5.7% on Qwen2.5 7B, while maintaining the same inference latency. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/huangwei200012/Fast-Quiet-STaR">https://github.com/huangwei200012/Fast-Quiet-STaR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¿›å±•è¡¨æ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¿›ä¸€æ­¥çš„æ”¶ç›Šéœ€è¦è¶…è¶Šå•çº¯æ‰©å¤§æ¨¡å‹è§„æ¨¡æˆ–è®­ç»ƒæ•°æ®ã€‚ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„æ–¹å‘æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œæ€è€ƒã€‚æœ€è¿‘ï¼ŒQuiet STaRé€šè¿‡ç”Ÿæˆæ ‡è®°çº§çš„æ€ç»´è½¨è¿¹æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œä½†å¼•èµ·äº†å·¨å¤§çš„æ¨ç†å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Fast Quiet STaRï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´é«˜æ•ˆçš„æ¨ç†æ¡†æ¶ï¼Œæ—¢ä¿ç•™äº†æ ‡è®°çº§æ¨ç†çš„å¥½å¤„ï¼Œåˆé™ä½äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å‡å°‘æ€ç»´æ ‡è®°çš„æ•°é‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åŒ–æ›´æŠ½è±¡ã€æ›´ç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒå°†è¿™ç§æ–¹æ³•æ‰©å±•åˆ°æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰è®¾ç½®ï¼Œä»è€Œå¾—åˆ°Fast Quiet-STaR NTPï¼Œå®ƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€æ˜ç¡®çš„æ€ç»´æ ‡è®°ç”Ÿæˆã€‚åœ¨Mistral 7Bå’ŒQwen2.5 7Bå››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„æ¨ç†æ—¶é—´é¢„ç®—ä¸‹ï¼ŒFast Quiet-STaRå§‹ç»ˆä¼˜äºQuiet-STaRçš„å¹³å‡å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFast Quiet-STaR NTPåœ¨Mistral 7Bä¸Šå¹³å‡å‡†ç¡®ç‡æé«˜äº†9%ï¼Œåœ¨Qwen2.5 7Bä¸Šæé«˜äº†5.7%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸åŒçš„æ¨ç†å»¶è¿Ÿã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/huangwei200012/Fast-Quiet-STaR">https://github.com/huangwei200012/Fast-Quiet-STaR</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17746v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—è¿›ä¸€æ­¥è¿›å±•éœ€è¦æ›´å¤šç­–ç•¥ã€‚Quiet STaRé€šè¿‡ç”Ÿæˆæ ‡è®°çº§çš„æ€ç»´è½¨è¿¹æ¥æ˜¾è‘—æé«˜æ¨ç†èƒ½åŠ›ï¼Œä½†ä¼šå¸¦æ¥è¾ƒå¤§çš„æ¨ç†å¼€é”€ã€‚æœ¬ç ”ç©¶æå‡ºäº†Fast Quiet STaRï¼Œä¸€ä¸ªæ›´é«˜æ•ˆçš„æ¨ç†æ¡†æ¶ï¼Œåœ¨ä¿ç•™æ ‡è®°çº§æ¨ç†çš„ä¼˜ç‚¹çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚å®ƒé€šè¿‡åŸºäºè¯¾ç¨‹å­¦ä¹ çš„è®­ç»ƒç­–ç•¥é€æ­¥å‡å°‘æ€ç»´æ ‡è®°çš„æ•°é‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åŒ–æ›´æŠ½è±¡ã€æ›´ç®€æ´çš„æ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†å…¶æ‰©å±•åˆ°æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰è®¾ç½®ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œä»è€Œæ¶ˆé™¤äº†æ¨ç†è¿‡ç¨‹ä¸­æ˜¾å¼æ€ç»´æ ‡è®°ç”Ÿæˆçš„éœ€è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFast Quiet STaRåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šä¼˜äºåŸå§‹çš„Quiet STaRï¼Œå¹¶ä¸”åœ¨ç›¸åŒçš„æ¨ç†æ—¶é—´é¢„ç®—ä¸‹è¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­éœ€è¦æ›´å¤šç­–ç•¥ã€‚</li>
<li>Quiet STaRé€šè¿‡ç”Ÿæˆæ ‡è®°çº§çš„æ€ç»´è½¨è¿¹æé«˜æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ¨ç†å¼€é”€è¾ƒå¤§çš„é—®é¢˜ã€‚</li>
<li>Fast Quiet STaRæ¡†æ¶æ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿ç•™æ ‡è®°çº§æ¨ç†çš„ä¼˜ç‚¹ã€‚</li>
<li>Fast Quiet STaRé‡‡ç”¨åŸºäºè¯¾ç¨‹å­¦ä¹ çš„è®­ç»ƒç­–ç•¥ï¼Œé€æ­¥å‡å°‘æ€ç»´æ ‡è®°æ•°é‡ã€‚</li>
<li>Fast Quiet STaRæ‰©å±•è‡³æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼ˆNTPï¼‰è®¾ç½®ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€‚</li>
<li>Fast Quiet STaRåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºåŸå§‹çš„Quiet STaRã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3c66f3666a409eb0a31e85ac4a50ddf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ce4aa2505d5d2bf426f39d2e25eb2eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-868d083f71a7bfbe43107adee42be724.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6cf060fcb173c5d323979d2ded8f5fe.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SafeMVDrive-Multi-view-Safety-Critical-Driving-Video-Synthesis-in-the-Real-World-Domain"><a href="#SafeMVDrive-Multi-view-Safety-Critical-Driving-Video-Synthesis-in-the-Real-World-Domain" class="headerlink" title="SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the   Real World Domain"></a>SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the   Real World Domain</h2><p><strong>Authors:Jiawei Zhou, Linye Lyu, Zhuotao Tian, Cheng Zhuo, Yu Li</strong></p>
<p>Safety-critical scenarios are rare yet pivotal for evaluating and enhancing the robustness of autonomous driving systems. While existing methods generate safety-critical driving trajectories, simulations, or single-view videos, they fall short of meeting the demands of advanced end-to-end autonomous systems (E2E AD), which require real-world, multi-view video data. To bridge this gap, we introduce SafeMVDrive, the first framework designed to generate high-quality, safety-critical, multi-view driving videos grounded in real-world domains. SafeMVDrive strategically integrates a safety-critical trajectory generator with an advanced multi-view video generator. To tackle the challenges inherent in this integration, we first enhance scene understanding ability of the trajectory generator by incorporating visual context â€“ which is previously unavailable to such generator â€“ and leveraging a GRPO-finetuned vision-language model to achieve more realistic and context-aware trajectory generation. Second, recognizing that existing multi-view video generators struggle to render realistic collision events, we introduce a two-stage, controllable trajectory generation mechanism that produces collision-evasion trajectories, ensuring both video quality and safety-critical fidelity. Finally, we employ a diffusion-based multi-view video generator to synthesize high-quality safety-critical driving videos from the generated trajectories. Experiments conducted on an E2E AD planner demonstrate a significant increase in collision rate when tested with our generated data, validating the effectiveness of SafeMVDrive in stress-testing planning modules. Our code, examples, and datasets are publicly available at: <a target="_blank" rel="noopener" href="https://zhoujiawei3.github.io/SafeMVDrive/">https://zhoujiawei3.github.io/SafeMVDrive/</a>. </p>
<blockquote>
<p>å®‰å…¨å…³é”®åœºæ™¯è™½ç„¶ç½•è§ï¼Œä½†å¯¹äºè¯„ä¼°å’Œå¢å¼ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç¨³å¥æ€§è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶èƒ½å¤Ÿç”Ÿæˆå®‰å…¨å…³é”®çš„é©¾é©¶è½¨è¿¹ã€æ¨¡æ‹Ÿæˆ–å•è§†è§’è§†é¢‘ï¼Œä½†å®ƒä»¬æ— æ³•æ»¡è¶³é«˜çº§ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆE2E ADï¼‰çš„éœ€æ±‚ï¼Œè¿™äº›ç³»ç»Ÿéœ€è¦åŸºäºç°å®ä¸–ç•Œçš„å¤šè§†è§’è§†é¢‘æ•°æ®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SafeMVDriveï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨ç”ŸæˆåŸºäºçœŸå®ä¸–ç•Œé¢†åŸŸçš„é«˜è´¨é‡ã€å®‰å…¨å…³é”®çš„å¤šè§†è§’é©¾é©¶è§†é¢‘æ¡†æ¶ã€‚SafeMVDriveæˆ˜ç•¥æ€§åœ°æ•´åˆäº†å®‰å…¨å…³é”®è½¨è¿¹ç”Ÿæˆå™¨ä¸å…ˆè¿›çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆå™¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æ•´åˆä¸­çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡èå…¥è§†è§‰ä¸Šä¸‹æ–‡ï¼ˆä¹‹å‰æ­¤ç±»ç”Ÿæˆå™¨æ— æ³•è·å–ï¼‰å¹¶åˆ©ç”¨GRPOå¾®è°ƒåçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºè½¨è¿¹ç”Ÿæˆå™¨çš„åœºæ™¯ç†è§£èƒ½åŠ›ï¼Œä»è€Œå®ç°æ›´çœŸå®ã€æ›´å…·ä¸Šä¸‹æ–‡æ„è¯†çš„è½¨è¿¹ç”Ÿæˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¤è¯†åˆ°ç°æœ‰çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆå™¨åœ¨å‘ˆç°çœŸå®çš„ç¢°æ’äº‹ä»¶æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› æ­¤å¼•å…¥äº†å¯æ§çš„ä¸¤é˜¶æ®µè½¨è¿¹ç”Ÿæˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯äº§ç”Ÿé¿ç¢°è½¨è¿¹ï¼Œç¡®ä¿è§†é¢‘è´¨é‡å’Œå®‰å…¨å…³é”®æ€§ä¿çœŸåº¦ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ‰©æ•£çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆå™¨ï¼Œæ ¹æ®ç”Ÿæˆçš„è½¨è¿¹åˆæˆé«˜è´¨é‡çš„å®‰å…¨å…³é”®é©¾é©¶è§†é¢‘ã€‚åœ¨E2E ADè§„åˆ’å™¨ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬ç”Ÿæˆçš„æ•°æ®è¿›è¡Œæµ‹è¯•æ—¶ç¢°æ’ç‡æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†SafeMVDriveåœ¨å‹åŠ›æµ‹è¯•è§„åˆ’æ¨¡å—æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ã€ç¤ºä¾‹å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://zhoujiawei3.github.io/SafeMVDrive/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://zhoujiawei3.github.io/SafeMVDrive/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17727v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSafeMVDriveçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ç”Ÿæˆé«˜è´¨é‡ã€å®‰å…¨å…³é”®çš„å¤šè§†è§’é©¾é©¶è§†é¢‘ã€‚SafeMVDriveèåˆäº†å®‰å…¨å…³é”®è½¨è¿¹ç”Ÿæˆå™¨ä¸å…ˆè¿›çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆå™¨ï¼Œå¢å¼ºäº†åœºæ™¯ç†è§£èƒ½åŠ›å¹¶å¼•å…¥å¯æ§è½¨è¿¹ç”Ÿæˆæœºåˆ¶ï¼Œç¡®ä¿è§†é¢‘è´¨é‡å’Œå®‰å…¨å…³é”®æ€§ã€‚è¯¥æ¡†æ¶å¯¹ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è§„åˆ’æ¨¡å—è¿›è¡Œäº†æœ‰æ•ˆçš„å‹åŠ›æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SafeMVDriveæ˜¯é¦–ä¸ªé’ˆå¯¹å®‰å…¨å…³é”®çš„å¤šè§†è§’é©¾é©¶è§†é¢‘ç”Ÿæˆè®¾è®¡çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶èåˆäº†å®‰å…¨å…³é”®è½¨è¿¹ç”Ÿæˆå™¨ä¸å¤šè§†è§’è§†é¢‘ç”Ÿæˆå™¨ã€‚</li>
<li>é€šè¿‡å¼•å…¥è§†è§‰ä¸Šä¸‹æ–‡å’ŒGRPOå¾®è°ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå¢å¼ºäº†è½¨è¿¹ç”Ÿæˆå™¨çš„åœºæ™¯ç†è§£èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å¯æ§è½¨è¿¹ç”Ÿæˆæœºåˆ¶ï¼Œç¡®ä¿è§†é¢‘è´¨é‡å’Œå®‰å…¨å…³é”®æ€§ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£å¼å¤šè§†è§’è§†é¢‘ç”Ÿæˆå™¨ï¼Œåˆæˆé«˜è´¨é‡çš„å®‰å…¨å…³é”®é©¾é©¶è§†é¢‘ã€‚</li>
<li>åœ¨ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è§„åˆ’æ¨¡å—ä¸Šè¿›è¡Œäº†æœ‰æ•ˆçš„å‹åŠ›æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-36774e766a0d7bbcfbdfc229ea03525d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da9e73fd8c29c47add80274b936fc848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-feee85bbdf2ed35a8ddabefef95a8c45.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2946dba8a5abe348af53251c17c32cd.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PPO-BR-Dual-Signal-Entropy-Reward-Adaptation-for-Trust-Region-Policy-Optimization"><a href="#PPO-BR-Dual-Signal-Entropy-Reward-Adaptation-for-Trust-Region-Policy-Optimization" class="headerlink" title="PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy   Optimization"></a>PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy   Optimization</h2><p><strong>Authors:Ben Rahman</strong></p>
<p>Despite Proximal Policy Optimization (PPO) dominating policy gradient methods â€“ from robotic control to game AI â€“ its static trust region forces a brittle trade-off: aggressive clipping stifles early exploration, while late-stage updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive RL by fusing exploration and convergence signals into a single bounded trust region â€“ a theoretically grounded innovation that outperforms five SOTA baselines with less than 2% overhead. This work bridges a critical gap in phase-aware learning, enabling real-world deployment in safety-critical systems like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1% faster convergence by combining: (1) entropy-driven expansion (epsilon up) for exploration in high-uncertainty states, and (2) reward-guided contraction (epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo, Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p &lt; 0.001), 2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with only five lines of code change. PPO-BRâ€™s simplicity and theoretical guarantees make it ready-to-deploy in safety-critical domains â€“ from surgical robotics to autonomous drones. In contrast to recent methods such as Group Relative Policy Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism applicable to both language models and general reinforcement learning environments. </p>
<blockquote>
<p>å°½ç®¡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­å æ®ä¸»å¯¼åœ°ä½â€”â€”ä»æœºå™¨äººæ§åˆ¶åˆ°æ¸¸æˆäººå·¥æ™ºèƒ½â€”â€”ä½†å…¶é™æ€ä¿¡ä»»åŒºåŸŸå­˜åœ¨ç¼ºé™·ï¼Œä½¿å¾—åœ¨æ¿€è¿›å‰ªåˆ‡ä¸æ—©æœŸæ¢ç´¢ä¹‹é—´å­˜åœ¨å¾®å¦™çš„æƒè¡¡å…³ç³»ï¼Œè€ŒåæœŸæ›´æ–°çš„å˜åŒ–åˆä¼šå¯¼è‡´æ”¶æ•›ä¸ç¨³å®šã€‚PPO-BRé€šè¿‡è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ é¢†åŸŸæå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œå°†æ¢ç´¢ä¸æ”¶æ•›ä¿¡å·èåˆåˆ°ä¸€ä¸ªæœ‰ç•Œä¿¡ä»»åŒºåŸŸä¸­â€”â€”è¿™æ˜¯å…·æœ‰ç†è®ºæ ¹æ®çš„åˆ›æ–°ç†å¿µã€‚å…¶åœ¨å®é™…åº”ç”¨æ–¹é¢ä¼˜äºäº”ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶å¸¦æ¥çš„å¼€é”€ä¸åˆ°2%ã€‚è¿™é¡¹å·¥ä½œåœ¨é˜¶æ®µæ„ŸçŸ¥å­¦ä¹ æ–¹é¢å¼¥è¡¥äº†å…³é”®ç©ºç™½ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€è‡ªé€‚åº”æœºåˆ¶ä¸‹å®ç°å¦‚æœºå™¨äººæ‰‹æœ¯ç­‰å®‰å…¨å…³é”®ç³»ç»Ÿçš„å®é™…åº”ç”¨éƒ¨ç½²ã€‚PPO-BRé€šè¿‡å°†ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ç›¸ç»“åˆï¼Œå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼šä¸€æ˜¯åœ¨é«˜ä¸ç¡®å®šæ€§çŠ¶æ€ä¸‹é‡‡ç”¨ç†µé©±åŠ¨æ‰©å±•ï¼ˆepsilon upï¼‰è¿›è¡Œæ¢ç´¢ï¼›äºŒæ˜¯é‡‡ç”¨å¥–åŠ±å¼•å¯¼æ”¶ç¼©ï¼ˆepsilon downï¼‰å®ç°æ”¶æ•›ç¨³å®šæ€§ã€‚åœ¨å…­ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ï¼ˆMuJoCoã€Atariã€ç¨€ç–å¥–åŠ±ï¼‰ä¸­ï¼ŒPPO-BRå®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼ˆæé«˜29.1%ï¼Œp &lt; 0.001ï¼‰ï¼Œå¥–åŠ±æ–¹å·®ä½äºPPOçš„2.3å€ï¼ŒåŒæ—¶è¿è¡Œæ—¶é—´å¼€é”€ä¸è¶…è¿‡ç›®å‰çš„æœ€å¥½æ–¹æ³•çš„å°†è¿‘äº”å€çº¿é•¿ä»…ä»…åªéœ€æ”¹å˜çš„ä»£ç è¡Œæ•°ä½äºè¿™äº›æœ€æ–°çš„æˆæœå¹¶å› å…¶ç®€ä¾¿æ€§å’Œç†è®ºä¿éšœä¼˜åŠ¿èƒ½åœ¨è¯¸å¦‚å¤–ç§‘æ‰‹æœ¯æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ— äººæœºç­‰å®‰å…¨å…³é”®é¢†åŸŸæŠ•å…¥ä½¿ç”¨éƒ¨ç½²å¯¹æ¯”æœ€æ–°çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼ˆGRPOï¼‰ï¼ŒPPO-BRé‡‡ç”¨äº†ç»Ÿä¸€çš„ç†µå¥–åŠ±æœºåˆ¶é€‚ç”¨äºè¯­è¨€æ¨¡å‹å’Œä¸€èˆ¬å¼ºåŒ–å­¦ä¹ ç¯å¢ƒã€‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17714v1">PDF</a> This manuscript builds upon an earlier version posted to TechRxiv.   This arXiv version includes an updated comparison with GRPO (Group Relative   Policy Optimization)</p>
<p><strong>Summary</strong>ï¼š<br>PPO-BRä½œä¸ºä¸€ç§æ–°å‹çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”è°ƒæ•´ä¿¡ä»»åŒºåŸŸï¼Œè§£å†³äº†ä¼ ç»Ÿç­–ç•¥ä¼˜åŒ–æ–¹æ³•ä¸­çš„æ—©æœŸæ¢ç´¢ä¸è¶³å’ŒåæœŸæ”¶æ•›ä¸ç¨³å®šçš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ¢ç´¢ä¸æ”¶æ•›ä¿¡å·ï¼Œå½¢æˆå•ä¸€çš„æœ‰ç•Œä¿¡ä»»åŒºåŸŸï¼Œå¡«è¡¥äº†é˜¶æ®µæ„ŸçŸ¥å­¦ä¹ ä¸­çš„å…³é”®ç©ºç™½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒPPO-BRè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¥–åŠ±æ–¹å·®æ›´ä½ã€‚å…¶ç®€æ´æ€§å’Œç†è®ºä¿è¯ä½¿å…¶æˆä¸ºå®‰å…¨å…³é”®é¢†åŸŸï¼ˆå¦‚æ‰‹æœ¯æœºå™¨äººå’Œè‡ªä¸»æ— äººæœºï¼‰çš„éƒ¨ç½²é¦–é€‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>PPO-BRé€šè¿‡è‡ªé€‚åº”è°ƒæ•´ä¿¡ä»»åŒºåŸŸè§£å†³äº†PPOé™æ€ä¿¡ä»»åŒºåŸŸå¸¦æ¥çš„é—®é¢˜ï¼Œå®ç°äº†æ›´çµæ´»çš„ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>PPO-BRç»“åˆäº†æ¢ç´¢ä¸æ”¶æ•›ä¿¡å·ï¼Œå½¢æˆå•ä¸€æœ‰ç•Œä¿¡ä»»åŒºåŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªç†è®ºä¸Šæœ‰åˆ›æ–°çš„æ–¹æ³•ã€‚</li>
<li>PPO-BRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œæ”¶æ•›é€Ÿåº¦æé«˜äº†29.1%ï¼Œå¥–åŠ±æ–¹å·®é™ä½äº†2.3å€ã€‚</li>
<li>PPO-BRçš„è¿è¡Œæ—¶é—´å¼€é”€ä½äº1.8%ï¼Œä¸”ä»…éœ€è¦äº”è¡Œä»£ç çš„æ”¹åŠ¨ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
<li>PPO-BRç®€åŒ–äº†å¤æ‚æ€§å¹¶æä¾›äº†ç†è®ºä¿è¯ï¼Œä½¿å…¶æˆä¸ºå®‰å…¨å…³é”®é¢†åŸŸçš„éƒ¨ç½²é¦–é€‰ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•å¦‚GRPOç›¸æ¯”ï¼ŒPPO-BRæä¾›äº†ç»Ÿä¸€çš„ç†µå¥–åŠ±æœºåˆ¶ï¼Œé€‚ç”¨äºè¯­è¨€æ¨¡å‹å’Œé€šç”¨å¼ºåŒ–å­¦ä¹ ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff86ca63d34a47ddb6c3c3c9487790c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8110e9bcecb40fb8225275817925607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c573358850dd1535da250f92fa537cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1993d16801ace458ce1ef46052710304.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Fann or Flop A Multigenre, Multiera Benchmark for Arabic Poetry   Understanding in LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-26/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c61c772c5fd765f8c1f3570e98718fc8.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-26  NOVER Incentive Training for Language Models via Verifier-Free   Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
