<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Effect of Fluorine doping on the electrocatalytic properties of Nb2O5   for H2O2 electrogeneration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b7fa52599f33945e8fd5fcfa53c2403c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-27-æ›´æ–°"><a href="#2025-05-27-æ›´æ–°" class="headerlink" title="2025-05-27 æ›´æ–°"></a>2025-05-27 æ›´æ–°</h1><h2 id="Effect-of-Fluorine-doping-on-the-electrocatalytic-properties-of-Nb2O5-for-H2O2-electrogeneration"><a href="#Effect-of-Fluorine-doping-on-the-electrocatalytic-properties-of-Nb2O5-for-H2O2-electrogeneration" class="headerlink" title="Effect of Fluorine doping on the electrocatalytic properties of Nb2O5   for H2O2 electrogeneration"></a>Effect of Fluorine doping on the electrocatalytic properties of Nb2O5   for H2O2 electrogeneration</h2><p><strong>Authors:Aline B. Trench, JoÃ£o Paulo C. Moura, Caio Machado Fernandes, Mauro C. Santos</strong></p>
<p>The oxygen reduction reaction (ORR) via the 2-electron mechanism is an efficient way to produce hydrogen peroxide (H2O2) under mild conditions. This study examines the modification of Vulcan XC72 carbon with fluorine (F)-doped niobium oxide (Nb2O5) nanoparticles at varying molar ratios (0, 0.005, 0.01, 0.02). The F-doped Nb2O5 nanoparticles were synthesized using the oxidizing peroxide method and then incorporated into Vulcan XC72 carbon via impregnation. Characterization techniques included X-ray diffraction (XRD), scanning electron microscopy (SEM), transmission electron microscopy (TEM), contact angle measurements, and X-ray photoelectron spectroscopy (XPS). Electrochemical evaluation using the rotating ring disk electrode method revealed that Vulcan XC72 modified with 1.0% F-doped Nb2O5 exhibited the best ORR performance. When used as a gas diffusion electrode, this electrocatalyst produced more H2O2 at all applied potentials than the pure and Nb2O5-modified Vulcan XC72 carbon. At potentials of -0.7 V and -1.3 V, the proposed electrocatalyst achieved H2O2 yields 65% and 98% higher than the Nb2O5-modified electrocatalyst. Furthermore, it presented lower energy consumption and higher current efficiency than the other electrocatalysts compared in this study. The enhanced performance is attributed to F doping, which increased Nb2O5 lattice distortion and disorder, improving electron availability for ORR. Additionally, F-doped electrocatalysts exhibited more oxygenated species and greater hydrophilicity, facilitating O2 adsorption, transport, and electron transfer. These properties significantly enhanced H2O2 electrogeneration efficiency while reducing energy consumption. </p>
<blockquote>
<p>é€šè¿‡ä¸¤ç”µå­æœºåˆ¶è¿›è¡Œçš„æ°§è¿˜åŸååº”ï¼ˆORRï¼‰æ˜¯åœ¨æ¸©å’Œæ¡ä»¶ä¸‹äº§ç”Ÿè¿‡æ°§åŒ–æ°¢ï¼ˆH2O2ï¼‰çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬ç ”ç©¶ä»¥æ°Ÿï¼ˆFï¼‰æºæ‚çš„æ°§åŒ–é“Œï¼ˆNb2O5ï¼‰çº³ç±³é¢—ç²’å¯¹Vulcan XC72ç¢³è¿›è¡Œäº†æ”¹æ€§ï¼Œå¹¶è®¾ç½®äº†ä¸åŒçš„æ‘©å°”æ¯”ï¼ˆ0ã€0.005ã€0.01ã€0.02ï¼‰ã€‚é‡‡ç”¨æ°§åŒ–è¿‡æ°§åŒ–ç‰©æ³•åˆæˆFæºæ‚çš„Nb2O5çº³ç±³é¢—ç²’ï¼Œç„¶åé€šè¿‡æµ¸æ¸æ³•å°†å…¶æºå…¥Vulcan XC72ç¢³ä¸­ã€‚è¡¨å¾æŠ€æœ¯åŒ…æ‹¬Xå°„çº¿è¡å°„ï¼ˆXRDï¼‰ã€æ‰«æç”µå­æ˜¾å¾®é•œï¼ˆSEMï¼‰ã€é€å°„ç”µå­æ˜¾å¾®é•œï¼ˆTEMï¼‰ã€æ¥è§¦è§’æµ‹é‡å’ŒXå°„çº¿å…‰ç”µå­èƒ½è°±ï¼ˆXPSï¼‰ã€‚é‡‡ç”¨æ—‹è½¬åœ†ç›˜ç”µææ³•è¿›è¡Œç”µåŒ–å­¦è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œç”¨1.0% Fæºæ‚Nb2O5æ”¹æ€§çš„Vulcan XC72å…·æœ‰æœ€ä½³çš„ORRæ€§èƒ½ã€‚å½“ç”¨ä½œæ°”ä½“æ‰©æ•£ç”µææ—¶ï¼Œè¯¥ç”µå‚¬åŒ–å‰‚åœ¨æ‰€æœ‰æ–½åŠ çš„ç”µä½ä¸‹äº§ç”Ÿçš„H2O2æ¯”çº¯Vulcan XC72å’ŒNb2O5æ”¹æ€§çš„Vulcan XC72ç¢³æ›´å¤šã€‚åœ¨-0.7Vå’Œ-1.3Vçš„ç”µä½ä¸‹ï¼Œæ‰€æç”µå‚¬åŒ–å‰‚çš„H2O2äº§é‡æ¯”Nb2O5æ”¹æ€§ç”µå‚¬åŒ–å‰‚é«˜å‡º65%å’Œ98%ã€‚æ­¤å¤–ï¼Œä¸æœ¬ç ”ç©¶ä¸­æ¯”è¾ƒçš„å…¶ä»–ç”µå‚¬åŒ–å‰‚ç›¸æ¯”ï¼Œå®ƒè¡¨ç°å‡ºæ›´ä½çš„èƒ½è€—å’Œæ›´é«˜çš„ç”µæµæ•ˆç‡ã€‚æ€§èƒ½æå‡çš„åŸå› æ˜¯Fæºæ‚å¢åŠ äº†Nb2O5çš„æ™¶æ ¼ç•¸å˜å’Œæ— åºæ€§ï¼Œæé«˜äº†ç”¨äºORRçš„ç”µå­å¯ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒFæºæ‚çš„ç”µå‚¬åŒ–å‰‚è¡¨ç°å‡ºæ›´å¤šçš„æ°§åŒ–ç‰©ç§å’Œæ›´å¤§çš„äº²æ°´æ€§ï¼Œæœ‰åˆ©äºO2çš„å¸é™„ã€ä¼ è¾“å’Œç”µå­è½¬ç§»ã€‚è¿™äº›ç‰¹æ€§æ˜¾è‘—æé«˜äº†H2O2çš„ç”µç”Ÿäº§æ•ˆç‡ï¼ŒåŒæ—¶é™ä½äº†èƒ½è€—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18140v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ°Ÿæºæ‚äºŒæ°§åŒ–é“Œæ”¹æ€§çš„Vulcan XC72ç¢³åœ¨æ°§è¿˜åŸååº”ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ°¢è¿‡æ°§åŒ–æ°¢ç”Ÿäº§æ•ˆç‡ã€‚ç ”ç©¶é€šè¿‡å¯¹ä¸åŒæ‘©å°”æ¯”ä¾‹çš„æ°Ÿæºæ‚äºŒæ°§åŒ–é“Œæ”¹æ€§çš„Vulcan XC72ç¢³è¿›è¡Œè¡¨å¾å’Œç”µåŒ–å­¦è¯„ä¼°ï¼Œå‘ç°1.0%æ°Ÿæºæ‚äºŒæ°§åŒ–é“Œæ”¹æ€§çš„Vulcan XC72ç¢³å…·æœ‰æœ€ä½³æ€§èƒ½ã€‚è¯¥å‚¬åŒ–å‰‚èƒ½æ›´æœ‰æ•ˆåœ°äº§ç”ŸH2O2ï¼Œé™ä½èƒ½è€—å¹¶æé«˜ç”µæµæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°Ÿæºæ‚äºŒæ°§åŒ–é“Œæ”¹æ€§çš„Vulcan XC72ç¢³é€šè¿‡2ç”µå­æœºåˆ¶æœ‰æ•ˆåœ°å‚ä¸æ°§è¿˜åŸååº”ï¼Œç”Ÿæˆæ°¢è¿‡æ°§åŒ–æ°¢ã€‚</li>
<li>é€šè¿‡XRDã€SEMã€TEMã€æ¥è§¦è§’æµ‹é‡å’ŒXPSç­‰æŠ€æœ¯å¯¹å‚¬åŒ–å‰‚è¿›è¡Œäº†è¡¨å¾ã€‚</li>
<li>1.0%æ°Ÿæºæ‚Nb2O5æ”¹æ€§çš„Vulcan XC72ç¢³åœ¨æ°§è¿˜åŸååº”ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>ä¸çº¯å’ŒNb2O5æ”¹æ€§çš„Vulcan XC72ç¢³ç›¸æ¯”ï¼Œè¯¥å‚¬åŒ–å‰‚åœ¨ä½œä¸ºæ°”ä½“æ‰©æ•£ç”µæä½¿ç”¨æ—¶äº§ç”Ÿäº†æ›´å¤šçš„H2O2ã€‚</li>
<li>åœ¨-0.7Vå’Œ-1.3Vçš„ç”µä½ä¸‹ï¼Œè¯¥å‚¬åŒ–å‰‚çš„H2O2äº§é‡åˆ†åˆ«æ¯”Nb2O5æ”¹æ€§å‚¬åŒ–å‰‚é«˜65%å’Œ98%ã€‚</li>
<li>æ°Ÿæºæ‚æé«˜äº†Nb2O5çš„æ™¶æ ¼å¤±çœŸå’Œæ— åºæ€§ï¼Œæ”¹å–„äº†ç”µå­åœ¨æ°§è¿˜åŸååº”ä¸­çš„å¯ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a222cbee09ce574adb1837aa1d2f6e8a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Foundation-Model-Framework-for-Multi-View-MRI-Classification-of-Extramural-Vascular-Invasion-and-Mesorectal-Fascia-Invasion-in-Rectal-Cancer"><a href="#A-Foundation-Model-Framework-for-Multi-View-MRI-Classification-of-Extramural-Vascular-Invasion-and-Mesorectal-Fascia-Invasion-in-Rectal-Cancer" class="headerlink" title="A Foundation Model Framework for Multi-View MRI Classification of   Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer"></a>A Foundation Model Framework for Multi-View MRI Classification of   Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer</h2><p><strong>Authors:Yumeng Zhang, Zohaib Salahuddin, Danial Khan, Shruti Atul Mali, Henry C. Woodruff, Sina Amirrajab, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Luis Marti-Bonmati, Philippe Lambin</strong></p>
<p>Background: Accurate MRI-based identification of extramural vascular invasion (EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified management of rectal cancer, yet visual assessment is subjective and vulnerable to inter-institutional variability. Purpose: To develop and externally evaluate a multicenter, foundation-model-driven framework that automatically classifies EVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective study used 331 pre-treatment rectal cancer MRI examinations from three European hospitals. After TotalSegmentator-guided rectal patch extraction, a self-supervised frequency-domain harmonization pipeline was trained to minimize scanner-related contrast shifts. Four classifiers were compared: ResNet50, SeResNet, the universal biomedical pretrained transformer (UMedPT) with a lightweight MLP head, and a logistic-regression variant using frozen UMedPT features (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when axial and sagittal features were fused (AUC &#x3D; 0.82; sensitivity &#x3D; 0.75; F1 score &#x3D; 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC &#x3D; 0.74). The highest MFI performance was attained by UMedPT on axial harmonized images (AUC &#x3D; 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC &#x3D; 0.75). Frequency-domain harmonization improved MFI classification but variably affected EVI performance. Conventional CNNs (ResNet50, SeResNet) underperformed, especially in F1 score and balanced accuracy. Conclusion: These findings demonstrate that combining foundation model features, harmonization, and multi-view fusion significantly enhances diagnostic performance in rectal MRI. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šåŸºäºMRIçš„ç²¾å‡†è…¹è†œå¤–è¡€ç®¡æµ¸æ¶¦ï¼ˆEVIï¼‰å’Œç›´è‚ ç³»è†œç­‹è†œæµ¸æ¶¦ï¼ˆMFIï¼‰è¯†åˆ«å¯¹äºç›´è‚ ç™Œçš„é£é™©åˆ†å±‚ç®¡ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè§†è§‰è¯„ä¼°å…·æœ‰ä¸»è§‚æ€§ï¼Œå®¹æ˜“å—åˆ°æœºæ„é—´å·®å¼‚çš„å½±å“ã€‚</p>
</blockquote>
<p>ç›®çš„ï¼šå¼€å‘å¹¶å¤–éƒ¨è¯„ä¼°ä¸€ä¸ªå¤šä¸­å¿ƒã€åŸºäºåŸºç¡€æ¨¡å‹çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨è½´çŠ¶å’ŒçŸ¢çŠ¶T2åŠ æƒMRIä¸Šè‡ªåŠ¨åˆ†ç±»EVIå’ŒMFIã€‚</p>
<p>æ–¹æ³•ï¼šè¿™é¡¹å›é¡¾æ€§ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ªä¸‰å®¶æ¬§æ´²åŒ»é™¢çš„331ä¾‹ç›´è‚ ç™ŒMRIæ£€æŸ¥ã€‚åœ¨TotalSegmentatorå¼•å¯¼çš„ç›´è‚ è¡¥ä¸æå–åï¼Œè®­ç»ƒäº†ä¸€ç§è‡ªç›‘ç£çš„é¢‘ç‡åŸŸå’Œè°ç®¡é“ï¼Œä»¥æœ€å°åŒ–æ‰«æä»ªç›¸å…³çš„å¯¹æ¯”åº¦å˜åŒ–ã€‚å¯¹æ¯”äº†å››ç§åˆ†ç±»å™¨ï¼šResNet50ã€SeResNetã€å¸¦æœ‰è½»é‡çº§MLPå¤´çš„é€šç”¨ç”Ÿç‰©åŒ»å­¦é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆUMedPTï¼‰ï¼Œä»¥åŠä½¿ç”¨å†»ç»“UMedPTç‰¹å¾çš„é€»è¾‘å›å½’å˜ä½“ï¼ˆUMedPT_LRï¼‰ã€‚</p>
<p>ç»“æœï¼šå½“èåˆè½´çŠ¶å’ŒçŸ¢çŠ¶ç‰¹å¾æ—¶ï¼ŒUMedPT_LRåœ¨EVIæ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼ˆAUC &#x3D; 0.82ï¼›çµæ•åº¦&#x3D; 0.75ï¼›F1åˆ†æ•°&#x3D; 0.73ï¼‰ï¼Œè¶…è¶Šäº†Chaiæœ¬ç§‘æŒ‘æˆ˜èµ›å† å†›ï¼ˆAUC &#x3D; 0.74ï¼‰ã€‚åœ¨è½´çŠ¶å’Œè°å›¾åƒä¸Šï¼ŒUMedPTåœ¨MFIæ–¹é¢è¡¨ç°æœ€ä½³ï¼ˆAUC &#x3D; 0.77ï¼‰ï¼Œä¹Ÿè¶…è¶Šäº†æŒ‘æˆ˜èµ›å† å†›ï¼ˆAUC &#x3D; 0.75ï¼‰ã€‚é¢‘ç‡åŸŸå’Œè°åŒ–æ”¹è¿›äº†MFIåˆ†ç±»ï¼Œä½†å¯¹EVIæ€§èƒ½çš„å½±å“æœ‰æ‰€ä¸åŒã€‚ä¼ ç»Ÿçš„CNNï¼ˆResNet50ï¼ŒSeResNetï¼‰è¡¨ç°è¾ƒå·®ï¼Œå°¤å…¶æ˜¯åœ¨F1åˆ†æ•°å’Œå¹³è¡¡ç²¾åº¦æ–¹é¢ã€‚</p>
<p>ç»“è®ºï¼šè¿™äº›å‘ç°è¡¨æ˜ï¼Œç»“åˆåŸºç¡€æ¨¡å‹ç‰¹å¾ã€å’Œè°åŒ–ä»¥åŠå¤šè§†å›¾èåˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç›´è‚ MRIçš„è¯Šæ–­æ€§èƒ½ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18058v1">PDF</a> 22 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong><br>     åŸºäºMRIçš„è‡ªåŠ¨åˆ†ç±»æŠ€æœ¯å¯¹äºç›´è‚ ç™Œçš„é¢å¤–è¡€ç®¡ä¾µè¢­ï¼ˆEVIï¼‰å’Œç³»è†œç­‹è†œä¾µè¢­ï¼ˆMFIï¼‰çš„å‡†ç¡®è¯†åˆ«è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§å¤šä¸­å¿ƒã€åŸºäºåŸºç¡€æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºåœ¨è½´å‘å’ŒçŸ¢çŠ¶T2åŠ æƒMRIä¸Šè‡ªåŠ¨åˆ†ç±»EVIå’ŒMFIã€‚ç»“æœæ˜¾ç¤ºï¼Œç»“åˆåŸºç¡€æ¨¡å‹ç‰¹å¾ã€è°æ³¢èåˆå’Œå¤šè§†è§’èåˆçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†MRIçš„è¯Šæ–­æ€§èƒ½ã€‚å…¶ä¸­UMedPT_LRæ¨¡å‹åœ¨èåˆè½´å‘å’ŒçŸ¢çŠ¶ç‰¹å¾åï¼ŒEVIæ£€æµ‹æ•ˆæœæœ€ä½³ï¼ˆAUC&#x3D;0.82ï¼‰ï¼Œè¶…è¶ŠChaimeleon Grand Challengeå† å†›æ¨¡å‹ï¼ˆAUC&#x3D;0.74ï¼‰ã€‚è€ŒUMedPTæ¨¡å‹åœ¨è½´å‘è°æ³¢å›¾åƒä¸Šå®ç°äº†æœ€é«˜çš„MFIæ€§èƒ½ï¼ˆAUC&#x3D;0.77ï¼‰ã€‚æ­¤å¤–ï¼Œé¢‘ç‡åŸŸè°æ³¢æŠ€æœ¯æ”¹å–„äº†MFIåˆ†ç±»ï¼Œä½†å¯¹EVIæ€§èƒ½çš„å½±å“å­˜åœ¨å·®å¼‚ã€‚ä¼ ç»Ÿçš„CNNï¼ˆå¦‚ResNet50å’ŒSeResNetï¼‰è¡¨ç°è¾ƒå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨F1åˆ†æ•°å’Œå¹³è¡¡ç²¾åº¦ä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºMRIçš„è‡ªåŠ¨åˆ†ç±»æŠ€æœ¯åœ¨ç›´è‚ ç™Œé£é™©åˆ†å±‚ç®¡ç†ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶å¼€å‘äº†ä¸€ç§å¤šä¸­å¿ƒã€åŸºäºåŸºç¡€æ¨¡å‹çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨åˆ†ç±»EVIå’ŒMFIã€‚</li>
<li>UMedPT_LRæ¨¡å‹åœ¨EVIæ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼ŒUMedPTæ¨¡å‹åœ¨MFIè¯Šæ–­ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>é¢‘ç‡åŸŸè°æ³¢æŠ€æœ¯æœ‰åŠ©äºæé«˜MFIåˆ†ç±»æ€§èƒ½ï¼Œä½†å¯¹EVIçš„å½±å“æœ‰æ‰€å·®å¼‚ã€‚</li>
<li>ç»“åˆåŸºç¡€æ¨¡å‹ç‰¹å¾ã€è°æ³¢æŠ€æœ¯å’Œå¤šè§†è§’èåˆèƒ½æ˜¾è‘—æé«˜MRIè¯Šæ–­æ€§èƒ½ã€‚</li>
<li>ä¼ ç»ŸCNNæ¨¡å‹ï¼ˆå¦‚ResNet50å’ŒSeResNetï¼‰åœ¨EVIå’ŒMFIæ£€æµ‹ä¸­çš„è¡¨ç°ä¸å¦‚é¢„æœŸã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºå¤šä¸­å¿ƒã€åŸºäºåŸºç¡€æ¨¡å‹çš„æ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05d6a7e8a96c55691f826921f6b5a205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74c766723ac68f70a9d9b503eda66c95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4818e76c58e296f95889809681eef5a3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Segment-Anyword-Mask-Prompt-Inversion-for-Open-Set-Grounded-Segmentation"><a href="#Segment-Anyword-Mask-Prompt-Inversion-for-Open-Set-Grounded-Segmentation" class="headerlink" title="Segment Anyword: Mask Prompt Inversion for Open-Set Grounded   Segmentation"></a>Segment Anyword: Mask Prompt Inversion for Open-Set Grounded   Segmentation</h2><p><strong>Authors:Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe, Philip Teare, Huiyu Zhou, Chen Jin</strong></p>
<p>Open-set image segmentation poses a significant challenge because existing methods often demand extensive training or fine-tuning and generally struggle to segment unified objects consistently across diverse text reference expressions. Motivated by this, we propose Segment Anyword, a novel training-free visual concept prompt learning approach for open-set language grounded segmentation that relies on token-level cross-attention maps from a frozen diffusion model to produce segmentation surrogates or mask prompts, which are then refined into targeted object masks. Initial prompts typically lack coherence and consistency as the complexity of the image-text increases, resulting in suboptimal mask fragments. To tackle this issue, we further introduce a novel linguistic-guided visual prompt regularization that binds and clusters visual prompts based on sentence dependency and syntactic structural information, enabling the extraction of robust, noise-tolerant mask prompts, and significant improvements in segmentation accuracy. The proposed approach is effective, generalizes across different open-set segmentation tasks, and achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative to fine-tuned methods) mIoU on GranDf, which is the most complex open-set grounded segmentation task in the field. </p>
<blockquote>
<p>å¼€æ”¾é›†å›¾åƒåˆ†å‰²æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡è®­ç»ƒå’Œå¾®è°ƒï¼Œå¹¶ä¸”åœ¨è·¨ä¸åŒæ–‡æœ¬å‚è€ƒè¡¨è¾¾æ—¶ï¼Œä¸€èˆ¬éš¾ä»¥ä¸€è‡´åœ°åˆ†å‰²ç»Ÿä¸€å¯¹è±¡ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Segment Anywordï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒè§†è§‰æ¦‚å¿µæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¼€æ”¾é›†è¯­è¨€åŸºç¡€åˆ†å‰²ã€‚å®ƒä¾èµ–äºå†»ç»“çš„æ‰©æ•£æ¨¡å‹çš„ä»¤ç‰Œçº§è·¨æ³¨æ„å›¾æ¥ç”Ÿæˆåˆ†å‰²æ›¿ä»£å“æˆ–æ©è†œæç¤ºï¼Œç„¶åå°†å…¶ç»†åŒ–ä¸ºç›®æ ‡å¯¹è±¡æ©è†œã€‚éšç€å›¾åƒæ–‡æœ¬å¤æ‚æ€§çš„å¢åŠ ï¼Œåˆå§‹æç¤ºé€šå¸¸ç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œå¯¼è‡´æ©è†œç‰‡æ®µä¸ä½³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¯­è¨€å¼•å¯¼è§†è§‰æç¤ºæ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®å¥å­ä¾èµ–å…³ç³»å’Œå¥æ³•ç»“æ„ä¿¡æ¯æ¥ç»‘å®šå’Œèšç±»è§†è§‰æç¤ºï¼Œä»è€Œæå–å‡ºç¨³å¥ã€è€å™ªå£°çš„æ©è†œæç¤ºï¼Œå¹¶æ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œå¯åº”ç”¨äºä¸åŒçš„å¼€æ”¾é›†åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶åœ¨Pascal Context 59ä¸Šå®ç°äº†52.5ï¼ˆ+6.8ç›¸å¯¹ï¼‰mIoUï¼Œåœ¨gRefCOCOä¸Šå®ç°äº†67.73ï¼ˆ+25.73ç›¸å¯¹ï¼‰cIoUï¼Œä»¥åŠåœ¨é¢†åŸŸä¸­æœ€å¤æ‚çš„å¼€æ”¾é›†è¯­è¨€åŸºç¡€åˆ†å‰²ä»»åŠ¡GranDfä¸Šå®ç°äº†67.4ï¼ˆ+1.1ç›¸å¯¹äºå¾®è°ƒæ–¹æ³•ï¼‰mIoUçš„ä¸šç•Œæœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹è§†è§‰æ¦‚å¿µæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¼€æ”¾å¼è¯­è¨€åŸºç¡€åˆ†å‰²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†»ç»“çš„æ‰©æ•£æ¨¡å‹çš„tokençº§è·¨æ³¨æ„åŠ›å›¾ç”Ÿæˆåˆ†å‰²æ›¿ä»£æ–¹æ¡ˆæˆ–æ©è†œæç¤ºï¼Œå¹¶è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œå½¢æˆç›®æ ‡å¯¹è±¡æ©è†œã€‚é€šè¿‡å¼•å…¥è¯­è¨€æŒ‡å¯¼çš„è§†è§‰æç¤ºæ­£åˆ™åŒ–ï¼Œæé«˜æ©è†œæç¤ºçš„é²æ£’æ€§å’Œå™ªå£°å®¹å¿åº¦ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ã€‚è¯¥æ–¹æ³•åœ¨å¼€æ”¾å¼åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°æœ‰æ•ˆï¼Œå…·æœ‰è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾é›†å›¾åƒåˆ†å‰²é¢ä¸´æŒ‘æˆ˜ï¼šç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæˆ–å¾®è°ƒï¼Œéš¾ä»¥åœ¨å¤šç§æ–‡æœ¬å‚è€ƒè¡¨è¾¾ä¸­ä¸€è‡´åœ°åˆ†å‰²ç»Ÿä¸€å¯¹è±¡ã€‚</li>
<li>Segment Anywordæ–¹æ³•æ˜¯ä¸€ç§æ–°å‹è®­ç»ƒå…è´¹çš„è§†è§‰æ¦‚å¿µæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¼€æ”¾å¼è¯­è¨€åŸºç¡€åˆ†å‰²ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å†»ç»“çš„æ‰©æ•£æ¨¡å‹çš„tokençº§è·¨æ³¨æ„åŠ›å›¾ç”Ÿæˆåˆ†å‰²æ›¿ä»£æ–¹æ¡ˆæˆ–æ©è†œæç¤ºã€‚</li>
<li>åˆå§‹æç¤ºç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œéšç€å›¾åƒæ–‡æœ¬å¤æ‚æ€§çš„å¢åŠ ï¼Œäº§ç”Ÿæ¬¡ä¼˜çš„æ©è†œç‰‡æ®µã€‚</li>
<li>å¼•å…¥è¯­è¨€æŒ‡å¯¼çš„è§†è§‰æç¤ºæ­£åˆ™åŒ–ï¼Œé€šè¿‡å¥å­ä¾èµ–å’Œå¥æ³•ç»“æ„ä¿¡æ¯ç»‘å®šå’Œèšé›†è§†è§‰æç¤ºï¼Œæé«˜æ©è†œæç¤ºçš„é²æ£’æ€§å’Œå™ªå£°å®¹å¿åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›çš„åˆ†å‰²ç²¾åº¦ï¼ŒåŒ…æ‹¬Pascal Context 59ã€gRefCOCOå’ŒGranDfã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6045f637ee038f28ad102947ca2db8c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-674b8a7ea8688c3efd55a35c183ec17e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb466ac21516f34460e8046a6b024aa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7fa52599f33945e8fd5fcfa53c2403c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-065d6fc8bb36357a7fc385ecb70f6d6f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="To-Glue-or-Not-to-Glue-Classical-vs-Learned-Image-Matching-for-Mobile-Mapping-Cameras-to-Textured-Semantic-3D-Building-Models"><a href="#To-Glue-or-Not-to-Glue-Classical-vs-Learned-Image-Matching-for-Mobile-Mapping-Cameras-to-Textured-Semantic-3D-Building-Models" class="headerlink" title="To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile   Mapping Cameras to Textured Semantic 3D Building Models"></a>To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile   Mapping Cameras to Textured Semantic 3D Building Models</h2><p><strong>Authors:Simone Gaisbauer, Prabin Gyawali, Qilin Zhang, Olaf Wysocki, Boris Jutzi</strong></p>
<p>Feature matching is a necessary step for many computer vision and photogrammetry applications such as image registration, structure-from-motion, and visual localization. Classical handcrafted methods such as SIFT feature detection and description combined with nearest neighbour matching and RANSAC outlier removal have been state-of-the-art for mobile mapping cameras. With recent advances in deep learning, learnable methods have been introduced and proven to have better robustness and performance under complex conditions. Despite their growing adoption, a comprehensive comparison between classical and learnable feature matching methods for the specific task of semantic 3D building camera-to-model matching is still missing. This submission systematically evaluates the effectiveness of different feature-matching techniques in visual localization using textured CityGML LoD2 models. We use standard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets consisting of facade textures and corresponding camera images (terrestrial and drone). For the latter, we evaluate the achievable accuracy of the absolute pose estimated using a Perspective-n-Point (PnP) algorithm, with geometric ground truth derived from geo-referenced trajectory data. The results indicate that the learnable feature matching methods vastly outperform traditional approaches regarding accuracy and robustness on our challenging custom datasets with zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We believe that this work will foster the development of model-based visual localization methods. Link to the code: <a target="_blank" rel="noopener" href="https://github.com/simBauer/To/_Glue/_or/_not/_to/_Glue">https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue</a> </p>
<blockquote>
<p>ç‰¹å¾åŒ¹é…æ˜¯è®¡ç®—æœºè§†è§‰å’Œæ‘„å½±æµ‹é‡å­¦ç­‰è®¸å¤šåº”ç”¨ï¼ˆå¦‚å›¾åƒé…å‡†ã€ä»è¿åŠ¨ä¸­è·å–ç»“æ„ä»¥åŠè§†è§‰å®šä½ï¼‰ä¸­å¿…ä¸å¯å°‘çš„ä¸€æ­¥ã€‚å¯¹äºç§»åŠ¨åœ°å›¾ç›¸æœºè€Œè¨€ï¼Œç»å…¸çš„åŸºäºæ‰‹å·¥çš„æ–¹æ³•ï¼ˆå¦‚SIFTç‰¹å¾æ£€æµ‹å’Œæè¿°ä¸æœ€è¿‘é‚»åŒ¹é…å’ŒRANSACå¼‚å¸¸å€¼å»é™¤ç›¸ç»“åˆï¼‰ä¸€ç›´å¤„äºæœ€å…ˆè¿›çš„çŠ¶æ€ã€‚éšç€æ·±åº¦å­¦ä¹ æœ€è¿‘çš„è¿›æ­¥ï¼Œå­¦ä¹ å‹æ–¹æ³•çš„å¼•å…¥å·²è¢«è¯æ˜åœ¨å¤æ‚æ¡ä»¶ä¸‹å…·æœ‰æ›´å¥½çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚å°½ç®¡å®ƒä»¬çš„é‡‡ç”¨æ­£åœ¨ä¸æ–­å¢åŠ ï¼Œä½†å¯¹äºè¯­ä¹‰3Då»ºç­‘ç›¸æœºä¸æ¨¡å‹åŒ¹é…çš„ç‰¹å®šä»»åŠ¡ï¼Œç»å…¸çš„ç‰¹å¾åŒ¹é…æ–¹æ³•å’Œå¯å­¦ä¹ çš„ç‰¹å¾åŒ¹é…æ–¹æ³•ä¹‹é—´çš„å…¨é¢æ¯”è¾ƒä»ç„¶ç¼ºå¤±ã€‚æœ¬æ¬¡æäº¤ç³»ç»Ÿåœ°è¯„ä¼°äº†ä½¿ç”¨çº¹ç†åŒ–çš„CityGML LoD2æ¨¡å‹è¿›è¡Œè§†è§‰å®šä½çš„ä¸åŒç‰¹å¾åŒ¹é…æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†åŸºå‡†æ•°æ®é›†ï¼ˆHPatchesï¼ŒMegaDepth-1500ï¼‰ä»¥åŠè‡ªå®šä¹‰æ•°æ®é›†ï¼ˆåŒ…æ‹¬å¢™é¢çº¹ç†å’Œç›¸åº”çš„ç›¸æœºå›¾åƒï¼ˆåœ°é¢å’Œæ— äººæœºæ‹æ‘„ï¼‰ï¼‰ã€‚å¯¹äºåè€…ï¼Œæˆ‘ä»¬è¯„ä¼°ä½¿ç”¨é€è§†nç‚¹ï¼ˆPnPï¼‰ç®—æ³•ä¼°è®¡çš„ç»å¯¹å§¿æ€çš„å¯å®ç°ç²¾åº¦ï¼Œè¯¥å‡ ä½•çœŸå®å€¼æ˜¯ä»åœ°ç†å‚è€ƒè½¨è¿¹æ•°æ®å¾—å‡ºçš„ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰é›¶åˆ°12ä¸ªRANSAC-inlierså’Œé›¶åˆ°0.16æ›²çº¿ä¸‹é¢ç§¯çš„æˆ‘ä»¬å…·æœ‰æŒ‘æˆ˜æ€§çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šï¼Œå­¦ä¹ å‹ç‰¹å¾åŒ¹é…æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å¤§å¤§ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†ä¿ƒè¿›åŸºäºæ¨¡å‹çš„è§†è§‰å®šä½æ–¹æ³•çš„å‘å±•ã€‚ä»£ç é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/simBauer/To/_Glue/_or/_not/_to/_Glue">https://github.com/simBauer/To\_Glue\_or\_not\_to\_Glue</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17973v1">PDF</a> Accepted to MMT, Xiamen, China; ISPRS Annals</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç‰¹å¾åŒ¹é…æ˜¯å¾ˆå¤šè®¡ç®—æœºè§†è§‰å’Œæ‘„å½±æµ‹é‡åº”ç”¨ï¼ˆå¦‚å›¾åƒé…å‡†ã€è¿åŠ¨ç»“æ„åˆ†æå’Œè§†è§‰å®šä½ï¼‰ä¸­çš„å¿…è¦æ­¥éª¤ã€‚ä¼ ç»Ÿçš„åŸºäºæ‰‹å·¥è®¾è®¡çš„æ–¹æ³•ï¼ˆå¦‚SIFTç‰¹å¾æ£€æµ‹å’Œæè¿°ï¼Œç»“åˆæœ€è¿‘é‚»åŒ¹é…å’ŒRANSACå¼‚å¸¸å€¼å»é™¤ï¼‰å¯¹äºç§»åŠ¨æ˜ å°„ç›¸æœºè€Œè¨€ä¸€ç›´æ˜¯æœ€å…ˆè¿›çš„ã€‚éšç€æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•ï¼Œå¼•å…¥çš„å­¦ä¹ å‹æ–¹æ³•å·²ç»è¯æ˜åœ¨å¤æ‚æ¡ä»¶ä¸‹å…·æœ‰æ›´å¥½çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚å°½ç®¡å­¦ä¹ å‹ç‰¹å¾åŒ¹é…æ–¹æ³•çš„é‡‡ç”¨æ—¥ç›Šæ™®éï¼Œä½†é’ˆå¯¹è¯­ä¹‰ä¸‰ç»´å»ºç­‘ç›¸æœºä¸æ¨¡å‹åŒ¹é…çš„ç‰¹å®šä»»åŠ¡ï¼Œå¯¹ç»å…¸ä¸å¯å­¦ä¹ ç‰¹å¾åŒ¹é…æ–¹æ³•çš„å…¨é¢æ¯”è¾ƒä»ç„¶ç¼ºå¤±ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸åŒç‰¹å¾åŒ¹é…æŠ€æœ¯åœ¨è§†è§‰å®šä½ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨äº†å¸¦çº¹ç†çš„CityGML LoD2æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†æ•°æ®é›†ï¼ˆHPatchesï¼ŒMegaDepth-1500ï¼‰ä»¥åŠåŒ…å«å¢™é¢çº¹ç†å’Œç›¸åº”ç›¸æœºå›¾åƒï¼ˆåœ°é¢å’Œæ— äººæœºï¼‰çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚å¯¹äºåè€…ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨é€è§†nç‚¹ï¼ˆPnPï¼‰ç®—æ³•ä¼°è®¡çš„ç»å¯¹å§¿æ€çš„ç²¾åº¦ï¼Œå‡ ä½•çœŸå®å€¼æ¥æºäºåœ°ç†å‚è€ƒè½¨è¿¹æ•°æ®ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰é›¶è‡³12ä¸ªRANSAC-inlierså’Œé›¶è‡³0.16æ›²çº¿ä¸‹é¢ç§¯çš„æŒ‘æˆ˜æ€§è‡ªå®šä¹‰æ•°æ®é›†ä¸Šï¼Œå¯å­¦ä¹ çš„ç‰¹å¾åŒ¹é…æ–¹æ³•åœ¨ç²¾åº¦å’Œç¨³å¥æ€§æ–¹é¢å¤§å¤§ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†ä¿ƒè¿›åŸºäºæ¨¡å‹çš„è§†è§‰å®šä½æ–¹æ³•çš„å‘å±•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç ”ç©¶å¯¹å¤šç§ç‰¹å¾åŒ¹é…æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿçš„æ¯”è¾ƒï¼ŒåŒ…æ‹¬ç»å…¸çš„å’Œå¯å­¦ä¹ çš„ç‰¹å¾åŒ¹é…æ–¹æ³•ã€‚</li>
<li>å®éªŒåœ¨è§†è§‰å®šä½ä»»åŠ¡ä¸­è¯„ä¼°äº†è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰ä¸‰ç»´å»ºç­‘ç›¸æœºä¸æ¨¡å‹åŒ¹é…æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†æ ‡å‡†æ•°æ®é›†ä»¥åŠåŒ…å«å¢™é¢çº¹ç†å’Œç›¸åº”ç›¸æœºå›¾åƒçš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚</li>
<li>å¯å­¦ä¹ çš„ç‰¹å¾åŒ¹é…æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>å®éªŒä¸­ä½¿ç”¨äº†é€è§†nç‚¹ï¼ˆPnPï¼‰ç®—æ³•æ¥è¯„ä¼°ç»å¯¹å§¿æ€ä¼°è®¡çš„ç²¾åº¦ã€‚</li>
<li>å‡ ä½•çœŸå®å€¼æ¥æºäºåœ°ç†å‚è€ƒè½¨è¿¹æ•°æ®ï¼Œå¢å¼ºäº†å®éªŒçš„å¯é æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0b7e4d834a871faeb58bc785fc5cc75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91bfe204e38cf008e320dda33f46555d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dcf4fc2ae26e5f41a2be9c4e3d65772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec299db0f1eb58869b18857ce6e59725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3909dbb4aff8492a5c27495bb74d986e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9f61613c6f6cd71bb7f4a24f0fbc543.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-816b5f3b83b2db40406023b82a61d532.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4301e09ae67ca00aee7e64a8e30f1b70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-add749e8f4299b165a58ba027bb39279.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Promptable-cancer-segmentation-using-minimal-expert-curated-data"><a href="#Promptable-cancer-segmentation-using-minimal-expert-curated-data" class="headerlink" title="Promptable cancer segmentation using minimal expert-curated data"></a>Promptable cancer segmentation using minimal expert-curated data</h2><p><strong>Authors:Lynn Karam, Yipei Wang, Veeru Kasivisvanathan, Mirabela Rusu, Yipeng Hu, Shaheer U. Saeed</strong></p>
<p>Automated segmentation of cancer on medical images can aid targeted diagnostic and therapeutic procedures. However, its adoption is limited by the high cost of expert annotations required for training and inter-observer variability in datasets. While weakly-supervised methods mitigate some challenges, using binary histology labels for training as opposed to requiring full segmentation, they require large paired datasets of histology and images, which are difficult to curate. Similarly, promptable segmentation aims to allow segmentation with no re-training for new tasks at inference, however, existing models perform poorly on pathological regions, again necessitating large datasets for training. In this work we propose a novel approach for promptable segmentation requiring only 24 fully-segmented images, supplemented by 8 weakly-labelled images, for training. Curating this minimal data to a high standard is relatively feasible and thus issues with the cost and variability of obtaining labels can be mitigated. By leveraging two classifiers, one weakly-supervised and one fully-supervised, our method refines segmentation through a guided search process initiated by a single-point prompt. Our approach outperforms existing promptable segmentation methods, and performs comparably with fully-supervised methods, for the task of prostate cancer segmentation, while using substantially less annotated data (up to 100X less). This enables promptable segmentation with very minimal labelled data, such that the labels can be curated to a very high standard. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸Šçš„ç™Œç—‡è‡ªåŠ¨åˆ†å‰²æœ‰åŠ©äºé’ˆå¯¹æ€§çš„è¯Šæ–­å’Œæ²»ç–—æ–¹æ³•ã€‚ç„¶è€Œï¼Œå…¶åº”ç”¨å—é™äºè®­ç»ƒæ‰€éœ€ä¸“å®¶æ ‡æ³¨çš„é«˜æˆæœ¬å’Œæ•°æ®é›†ä¹‹é—´çš„è§‚å¯Ÿè€…é—´å·®å¼‚ã€‚è™½ç„¶å¼±ç›‘ç£æ–¹æ³•ç¼“è§£äº†ä¸€äº›æŒ‘æˆ˜ï¼Œå®ƒä»¬éœ€è¦ä½¿ç”¨äºŒå…ƒç»„ç»‡ç—…ç†å­¦æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯éœ€è¦å®Œæ•´çš„åˆ†å‰²ï¼Œä½†å®ƒä»¬éœ€è¦é…å¯¹çš„å¤§é‡ç»„ç»‡ç—…ç†å­¦å›¾åƒæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¾ˆéš¾ç¼–çº‚ã€‚åŒæ ·ï¼Œæç¤ºåˆ†å‰²æ—¨åœ¨å®ç°åœ¨æ¨ç†é˜¶æ®µæ— éœ€é’ˆå¯¹æ–°ä»»åŠ¡è¿›è¡Œé‡æ–°è®­ç»ƒå³å¯è¿›è¡Œåˆ†å‰²ï¼Œç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨ç—…ç†åŒºåŸŸçš„æ€§èƒ½è¾ƒå·®ï¼ŒåŒæ ·éœ€è¦å¤§é‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œä»…éœ€è¦24å¼ å®Œå…¨åˆ†å‰²çš„å›¾åƒå’Œ8å¼ å¼±æ ‡ç­¾å›¾åƒè¿›è¡Œè®­ç»ƒã€‚å°†è¿™äº›æœ€å°‘æ•°æ®æ•´ç†åˆ°é«˜æ ‡å‡†æ˜¯ç›¸å¯¹å¯è¡Œçš„ï¼Œå› æ­¤å¯ä»¥ç¼“è§£è·å–æ ‡ç­¾çš„æˆæœ¬å’Œå·®å¼‚é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨ä¸€ä¸ªå¼±ç›‘ç£å’Œä¸€ä¸ªå®Œå…¨ç›‘ç£çš„åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å•ç‚¹æç¤ºå¯åŠ¨çš„å¼•å¯¼æœç´¢è¿‡ç¨‹æ¥å®Œå–„åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰çš„æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å‰åˆ—è…ºç™Œåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸å®Œå…¨ç›‘ç£çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ‡æ³¨æ•°æ®å¤§å¤§å‡å°‘ï¼ˆæœ€å¤šå‡å°‘100å€ï¼‰ã€‚è¿™èƒ½å¤Ÿå®ç°ç”¨æå°‘é‡çš„æ ‡æ³¨æ•°æ®è¿›è¡Œæç¤ºåˆ†å‰²ï¼Œä½¿æ ‡ç­¾å¯ä»¥ç¼–çº‚åˆ°å¾ˆé«˜çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17915v1">PDF</a> Accepted at Medical Image Understanding and Analysis (MIUA) 2025</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯æœ‰åŠ©äºç²¾å‡†è¯Šæ–­å’Œæ²»ç–—ç™Œç—‡ï¼Œä½†å…¶åº”ç”¨å—é™äºä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜å’Œæ•°æ®é›†é—´è§‚å¯Ÿè€…å·®å¼‚å¤§ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œä»…éœ€å°‘é‡å…¨åˆ†å‰²å›¾åƒå’Œå¼±æ ‡æ³¨å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå¯ç¼“è§£æ ‡æ³¨æˆæœ¬å’Œæ•°æ®å·®å¼‚é—®é¢˜ã€‚é€šè¿‡ä¸¤ä¸ªåˆ†ç±»å™¨ï¼Œå³ä¸€ä¸ªå¼±ç›‘ç£åˆ†ç±»å™¨å’Œä¸€ä¸ªå…¨ç›‘ç£åˆ†ç±»å™¨ï¼Œä»¥å•ç‚¹æç¤ºå¼•å¯¼æœç´¢è¿‡ç¨‹ï¼Œå¯¹åˆ†å‰²è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚è¯¥æ–¹æ³•åœ¨å‰åˆ—è…ºç™Œåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ï¼Œä¸”ä½¿ç”¨çš„æ ‡æ³¨æ•°æ®é‡å¤§å¤§å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²åœ¨ç™Œç—‡è¯Šæ–­ä¸æ²»ç–—ä¸­æœ‰é‡è¦ä½œç”¨ï¼Œä½†é¢ä¸´ä¸“å®¶æ ‡æ³¨æˆæœ¬é«˜æ˜‚å’Œæ•°æ®é›†é—´å·®å¼‚å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼±ç›‘ç£æ–¹æ³•è™½ç„¶æœ‰åŠ©äºç¼“è§£éƒ¨åˆ†é—®é¢˜ï¼Œä½†é…å¯¹æ•°æ®é›†éš¾æ±‚ã€‚</li>
<li>æç¤ºåˆ†å‰²æ–¹æ³•æ—¨åœ¨å®ç°å¯¹æ–°ä»»åŠ¡çš„æ— éœ€å†è®­ç»ƒæ¨ç†ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨ç—…ç†åŒºåŸŸè¡¨ç°æ¬ ä½³ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œä»…éœ€è¦å°‘é‡å…¨åˆ†å‰²å›¾åƒå’Œå¼±æ ‡æ³¨å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªåˆ†ç±»å™¨çš„ç»“åˆï¼Œå³ä¸€ä¸ªå¼±ç›‘ç£åˆ†ç±»å™¨å’Œä¸€ä¸ªå…¨ç›‘ç£åˆ†ç±»å™¨ï¼Œä»¥å•ç‚¹æç¤ºå¯åŠ¨å¼•å¯¼æœç´¢è¿‡ç¨‹ï¼Œä¼˜åŒ–åˆ†å‰²ç»“æœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å‰åˆ—è…ºç™Œåˆ†å‰²ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå¤šæ•°æç¤ºåˆ†å‰²æ–¹æ³•ï¼Œå¹¶ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20fe3f1ab5fd425948f4293cf8759220.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ff48651c6552455f6cab42162e7de2e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UltraBoneUDF-Self-supervised-Bone-Surface-Reconstruction-from-Ultrasound-Based-on-Neural-Unsigned-Distance-Functions"><a href="#UltraBoneUDF-Self-supervised-Bone-Surface-Reconstruction-from-Ultrasound-Based-on-Neural-Unsigned-Distance-Functions" class="headerlink" title="UltraBoneUDF: Self-supervised Bone Surface Reconstruction from   Ultrasound Based on Neural Unsigned Distance Functions"></a>UltraBoneUDF: Self-supervised Bone Surface Reconstruction from   Ultrasound Based on Neural Unsigned Distance Functions</h2><p><strong>Authors:Luohong Wu, Matthias Seibold, Nicola A. Cavalcanti, Giuseppe Loggia, Lisa Reissner, Bastian Sigrist, Jonas Hein, Lilian Calvet, Arnd ViehÃ¶fer, Philipp FÃ¼rnstahl</strong></p>
<p>Background: Bone surface reconstruction plays a critical role in computer-assisted orthopedic surgery. Compared to traditional imaging modalities such as CT and MRI, ultrasound offers a radiation-free, cost-effective, and portable alternative. Continuous bone surface reconstruction can be employed for many clinical applications. However, due to the inherent limitations of ultrasound imaging, B-mode ultrasound typically capture only partial bone surfaces. Existing reconstruction methods struggle with such incomplete data, leading to artifacts and increased reconstruction errors. Effective techniques for accurately reconstructing thin and open bone surfaces from real-world 3D ultrasound volumes remain lacking. Methods: We propose UltraBoneUDF, a self-supervised framework designed for reconstructing open bone surfaces from ultrasound using neural Unsigned Distance Functions. To enhance reconstruction quality, we introduce a novel global feature extractor that effectively fuses ultrasound-specific image characteristics. Additionally, we present a novel loss function based on local tangent plane optimization that substantially improves surface reconstruction quality. UltraBoneUDF and baseline models are extensively evaluated on four open-source datasets. Results: Qualitative results highlight the limitations of the state-of-the-art methods for open bone surface reconstruction and demonstrate the effectiveness of UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms competing methods across all evaluated datasets for both open and closed bone surface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the UltraBones100k dataset (39.6% improvement compared to the SOTA), 0.23 mm on the OpenBoneCT dataset (69.3% improvement), 0.18 mm on the ClosedBoneCT dataset (70.2% improvement), and 0.05 mm on the Prostate dataset (55.3% improvement). </p>
<blockquote>
<p>èƒŒæ™¯ï¼šéª¨è¡¨é¢é‡å»ºåœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸CTå’ŒMRIç­‰ä¼ ç»Ÿæˆåƒæ¨¡å¼ç›¸æ¯”ï¼Œè¶…å£°æä¾›äº†ä¸€ç§æ— è¾å°„ã€æˆæœ¬ä½å»‰ã€ä¾¿äºæºå¸¦çš„æ›¿ä»£æ–¹æ¡ˆã€‚éª¨è¡¨é¢è¿ç»­é‡å»ºå¯ç”¨äºå¤šç§ä¸´åºŠåº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºè¶…å£°æˆåƒçš„å›ºæœ‰å±€é™æ€§ï¼ŒBæ¨¡å¼è¶…å£°é€šå¸¸åªèƒ½æ•è·éƒ¨åˆ†éª¨è¡¨é¢ã€‚ç°æœ‰çš„é‡å»ºæ–¹æ³•åœ¨å¤„ç†æ­¤ç±»ä¸å®Œæ•´æ•°æ®æ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å‡ºç°ä¼ªå½±å’Œé‡å»ºè¯¯å·®å¢åŠ ã€‚ä»ç°å®ä¸–ç•Œä¸­çš„3Dè¶…å£°ä½“ç§¯å‡†ç¡®é‡å»ºè–„è€Œå¼€æ”¾çš„éª¨è¡¨é¢çš„æœ‰æ•ˆæŠ€æœ¯ä»ç„¶ç¼ºä¹ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†UltraBoneUDFï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»è¶…å£°æ•°æ®ä¸­é‡å»ºå¼€æ”¾éª¨è¡¨é¢çš„è‡ªç›‘ç£æ¡†æ¶ï¼Œé‡‡ç”¨ç¥ç»æ— ç¬¦å·è·ç¦»å‡½æ•°ã€‚ä¸ºäº†æé«˜é‡å»ºè´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å…¨å±€ç‰¹å¾æå–å™¨ï¼Œæœ‰æ•ˆåœ°èåˆäº†è¶…å£°ç‰¹å®šçš„å›¾åƒç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºå±€éƒ¨åˆ‡çº¿å¹³é¢ä¼˜åŒ–çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œè¿™æå¤§åœ°æé«˜äº†è¡¨é¢é‡å»ºçš„è´¨é‡ã€‚UltraBoneUDFå’ŒåŸºçº¿æ¨¡å‹åœ¨å››ä¸ªå¼€æºæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœï¼šå®šæ€§ç»“æœçªå‡ºäº†ç°æœ‰æ–¹æ³•åœ¨å¼€æ”¾éª¨è¡¨é¢é‡å»ºæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶è¯æ˜äº†UltraBoneUDFçš„æœ‰æ•ˆæ€§ã€‚åœ¨å®šé‡æ–¹é¢ï¼ŒUltraBoneUDFåœ¨æ‰€æœ‰è¯„ä¼°çš„æ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ— è®ºæ˜¯å¼€æ”¾è¿˜æ˜¯é—­åˆéª¨è¡¨é¢é‡å»ºï¼Œä»¥å¹³å‡Chamferè·ç¦»è¯¯å·®è¡¡é‡ï¼šåœ¨UltraBones100kæ•°æ®é›†ä¸Šä¸º1.10æ¯«ç±³ï¼ˆç›¸æ¯”æœ€æ–°æŠ€æœ¯æé«˜39.6%ï¼‰ï¼Œåœ¨OpenBoneCTæ•°æ®é›†ä¸Šä¸º0.23æ¯«ç±³ï¼ˆæé«˜69.3%ï¼‰ï¼Œåœ¨ClosedBoneCTæ•°æ®é›†ä¸Šä¸º0.18æ¯«ç±³ï¼ˆæé«˜70.2%ï¼‰ï¼Œåœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šä¸º0.05æ¯«ç±³ï¼ˆæé«˜55.3%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17912v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨ç¥ç»ç½‘ç»œ Unsigned Distance Functions æŠ€æœ¯ï¼Œä»è¶…å£°æ•°æ®ä¸­é‡å»ºå¼€æ”¾éª¨è¡¨é¢çš„æ–°æ–¹æ³• UltraBoneUDFã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å…¨çƒç‰¹å¾æå–å™¨å’ŒåŸºäºå±€éƒ¨åˆ‡çº¿å¹³é¢ä¼˜åŒ–çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œæé«˜äº†é‡å»ºè´¨é‡ã€‚åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUltraBoneUDF åœ¨å¼€æ”¾å’Œé—­åˆéª¨è¡¨é¢é‡å»ºæ–¹é¢å‡æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶…å£°éª¨è¡¨é¢é‡å»ºæ•°æ®é›†ä¸Šï¼Œå…¶å¹³å‡ Chamfer è·ç¦»è¯¯å·®é™ä½äº† 39.6%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éª¨è¡¨é¢é‡å»ºåœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚</li>
<li>è¶…å£°æˆåƒä½œä¸ºæ›¿ä»£ä¼ ç»Ÿ CT å’Œ MRI çš„æˆåƒæ–¹å¼ï¼Œå…·æœ‰æ— è¾å°„ã€æˆæœ¬ä½å’Œä¾¿æºçš„ä¼˜ç‚¹ã€‚</li>
<li>Bæ¨¡å¼è¶…å£°é€šå¸¸åªèƒ½æ•è·éƒ¨åˆ†éª¨è¡¨é¢ï¼Œç°æœ‰é‡å»ºæ–¹æ³•åœ¨å¤„ç†ä¸å®Œæ•´æ•°æ®æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>UltraBoneUDF æ˜¯ä¸€ä¸ªåˆ©ç”¨ç¥ç»ç½‘ç»œ Unsigned Distance Functions æŠ€æœ¯ä»è¶…å£°æ•°æ®ä¸­é‡å»ºå¼€æ”¾éª¨è¡¨é¢çš„è‡ªç›‘ç£æ¡†æ¶ã€‚</li>
<li>å…¨çƒç‰¹å¾æå–å™¨çš„å¼•å…¥æœ‰æ•ˆèåˆäº†è¶…å£°ç‰¹æœ‰çš„å›¾åƒç‰¹å¾ï¼Œæé«˜äº†é‡å»ºè´¨é‡ã€‚</li>
<li>åŸºäºå±€éƒ¨åˆ‡çº¿å¹³é¢ä¼˜åŒ–çš„æ–°å‹æŸå¤±å‡½æ•°æ˜¾è‘—æé«˜äº†è¡¨é¢é‡å»ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54cf0b5d66de10a995c3ea2557874ee3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pixels-to-Prognosis-Harmonized-Multi-Region-CT-Radiomics-and-Foundation-Model-Signatures-Across-Multicentre-NSCLC-Data"><a href="#Pixels-to-Prognosis-Harmonized-Multi-Region-CT-Radiomics-and-Foundation-Model-Signatures-Across-Multicentre-NSCLC-Data" class="headerlink" title="Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and   Foundation-Model Signatures Across Multicentre NSCLC Data"></a>Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and   Foundation-Model Signatures Across Multicentre NSCLC Data</h2><p><strong>Authors:Shruti Atul Mali, Zohaib Salahuddin, Danial Khan, Yumeng Zhang, Henry C. Woodruff, Eduardo Ibor-Crespo, Ana Jimenez-Pastor, Luis Marti-Bonmati, Philippe Lambin</strong></p>
<p>Purpose: To evaluate the impact of harmonization and multi-region CT image feature integration on survival prediction in non-small cell lung cancer (NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM) features, and clinical data from a multicenter dataset.   Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604 training, 272 test) across five centers. Features were extracted from the whole lung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium (CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat, reconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox models predicted overall survival; performance was assessed using the concordance index (C-index), 5-year time-dependent area under the curve (t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values explained feature contributions. A consensus model used agreement across top region of interest (ROI) models to stratify patient risk.   Results: TNM staging showed prognostic utility (C-index &#x3D; 0.67; HR &#x3D; 2.70; t-AUC &#x3D; 0.85). The clinical + tumor radiomics model with ComBat achieved a C-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined with clinical data yielded the highest performance (C-index &#x3D; 0.7616; t-AUC &#x3D; 0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142 and t-AUC of 0.7885. The consensus model, covering 78% of valid test cases, achieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.   Conclusion: Harmonization and multi-region feature integration improve survival prediction in multicenter NSCLC data. Combining interpretable radiomics, FM features, and consensus modeling enables robust risk stratification across imaging centers. </p>
<blockquote>
<p>ç›®çš„ï¼šæœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ‰‹å·¥åˆ¶ä½œçš„æ”¾å°„å­¦ç‰¹å¾ã€é¢„è®­ç»ƒçš„åŸºçŸ³æ¨¡å‹ï¼ˆFMï¼‰ç‰¹å¾å’Œæ¥è‡ªå¤šä¸­å¿ƒæ•°æ®é›†çš„ä¸´åºŠæ•°æ®ï¼Œè¯„ä¼°å’Œè°åŒ–ä»¥åŠå¤šåŒºåŸŸCTå›¾åƒç‰¹å¾èåˆåœ¨éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰æ‚£è€…ç”Ÿå­˜é¢„æµ‹ä¸­çš„å½±å“ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬åˆ†æäº†æ¥è‡ªäº”ä¸ªä¸­å¿ƒçš„876ä¾‹NSCLCæ‚£è€…ï¼ˆ604ä¾‹è®­ç»ƒï¼Œ272ä¾‹æµ‹è¯•ï¼‰çš„CTæ‰«æå’Œä¸´åºŠæ•°æ®ã€‚ç‰¹å¾ä»æ•´ä¸ªè‚ºéƒ¨ã€è‚¿ç˜¤ã€çºµéš”èŠ‚ç‚¹ã€å† çŠ¶åŠ¨è„‰å’Œå† çŠ¶åŠ¨è„‰é’™åŒ–ï¼ˆCACï¼‰ä¸­æå–ã€‚ä½¿ç”¨ComBatã€é‡å»ºæ ¸å½’ä¸€åŒ–ï¼ˆRKNï¼‰å’ŒRKN+ComBatå¯¹æ‰‹å·¥åˆ¶ä½œçš„æ”¾å°„å­¦ç‰¹å¾å’ŒFMæ·±åº¦ç‰¹å¾è¿›è¡Œå’Œè°åŒ–å¤„ç†ã€‚æ­£åˆ™åŒ–Coxæ¨¡å‹é¢„æµ‹æ€»ä½“ç”Ÿå­˜ï¼›æ€§èƒ½è¯„ä¼°é‡‡ç”¨ä¸€è‡´æ€§æŒ‡æ•°ï¼ˆC-indexï¼‰ã€5å¹´æ—¶é—´ä¾èµ–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆt-AUCï¼‰å’Œå±é™©æ¯”ï¼ˆHRï¼‰ã€‚SHapley Additive exPlanationsï¼ˆSHAPï¼‰å€¼è§£é‡Šäº†ç‰¹å¾è´¡çŒ®ã€‚å…±è¯†æ¨¡å‹åˆ©ç”¨é¡¶çº§æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰æ¨¡å‹çš„å…±è¯†æ¥åˆ†å±‚æ‚£è€…é£é™©ã€‚ç»“æœï¼šTNMåˆ†æœŸå…·æœ‰é¢„åä½œç”¨ï¼ˆC-index &#x3D; 0.67ï¼›HR &#x3D; 2.70ï¼›t-AUC &#x3D; 0.85ï¼‰ã€‚ä½¿ç”¨ComBatçš„ä¸´åºŠ+è‚¿ç˜¤æ”¾å°„å­¦æ¨¡å‹è¾¾åˆ°C-indexä¸º0.7552å’Œt-AUCä¸º0.8820ã€‚FMç‰¹å¾ï¼ˆ50ä½“ç´ ç«‹æ–¹ä½“ï¼‰ä¸ä¸´åºŠæ•°æ®ç›¸ç»“åˆå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ˆC-index &#x3D; 0.7616ï¼›t-AUC &#x3D; 0.8866ï¼‰ã€‚æ‰€æœ‰ROIå’ŒFMç‰¹å¾çš„ç»„åˆè¾¾åˆ°äº†C-indexä¸º0.7142å’Œt-AUCä¸º0.7885ã€‚å…±è¯†æ¨¡å‹è¦†ç›–78%çš„æœ‰æ•ˆæµ‹è¯•ç—…ä¾‹ï¼Œè¾¾åˆ°t-AUCä¸º0.92ï¼Œæ•æ„Ÿåº¦ä¸º97.6%ï¼Œç‰¹å¼‚åº¦ä¸º66.7%ã€‚ç»“è®ºï¼šå’Œè°åŒ–ä¸å¤šåŒºåŸŸç‰¹å¾èåˆå¯æé«˜å¤šä¸­å¿ƒNSCLCæ•°æ®çš„ç”Ÿå­˜é¢„æµ‹èƒ½åŠ›ã€‚ç»“åˆå¯è§£é‡Šçš„æ”¾å°„å­¦ç‰¹å¾ã€FMç‰¹å¾å’Œå…±è¯†å»ºæ¨¡ï¼Œå¯åœ¨æˆåƒä¸­å¿ƒä¹‹é—´å®ç°ç¨³å¥çš„é£é™©åˆ†å±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17893v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶å¤šä¸­å¿ƒéå°ç»†èƒè‚ºç™Œæ‚£è€…çš„CTå›¾åƒä¸ä¸´åºŠæ•°æ®èåˆï¼Œé€šè¿‡å’Œè°åŒ–åŠå¤šåŒºåŸŸå›¾åƒç‰¹å¾æ•´åˆæé«˜ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ç»“åˆæ‰‹å·¥æ”¾å°„å­¦ç‰¹å¾ã€é¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾å’Œä¸´åºŠæ•°æ®ï¼Œé€šè¿‡å’Œè°åŒ–æ–¹æ³•å¦‚ComBatã€é‡å»ºæ ¸å½’ä¸€åŒ–ï¼ˆRKNï¼‰ç­‰ï¼Œåˆ©ç”¨æ­£è§„åŒ–Coxæ¨¡å‹é¢„æµ‹æ€»ä½“ç”Ÿå­˜æƒ…å†µã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œå¤šåŒºåŸŸç‰¹å¾æ•´åˆå’Œè°åŒ–èƒ½æé«˜éå°ç»†èƒè‚ºç™Œçš„ç”Ÿå­˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œç»“åˆè§£é‡Šæ€§æ”¾å°„å­¦ç‰¹å¾ã€é¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾å’Œå…±è¯†å»ºæ¨¡ï¼Œå¯åœ¨è·¨å½±åƒä¸­å¿ƒå®ç°ç¨³å¥çš„é£é™©åˆ†å±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®çš„ï¼šè¯„ä»·å’Œè°åŒ–åŠå¤šåŒºåŸŸCTå›¾åƒç‰¹å¾æ•´åˆåœ¨éå°ç»†èƒè‚ºç™Œæ‚£è€…ç”Ÿå­˜é¢„æµ‹ä¸­çš„å½±å“ã€‚</li>
<li>ä½¿ç”¨äº†æ‰‹å·¥æ”¾å°„å­¦ç‰¹å¾ã€é¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾å’Œä¸´åºŠæ•°æ®ã€‚</li>
<li>é€šè¿‡ComBatã€RKNåŠå…¶ç»„åˆè¿›è¡Œç‰¹å¾å’Œè°åŒ–ã€‚</li>
<li>åˆ©ç”¨æ­£è§„åŒ–Coxæ¨¡å‹è¿›è¡Œç”Ÿå­˜é¢„æµ‹ï¼Œé‡‡ç”¨C-indexã€t-AUCå’ŒHRè¯„ä¼°æ€§èƒ½ã€‚</li>
<li>TNMåˆ†æœŸå…·æœ‰é¢„åæ•ˆç”¨ã€‚</li>
<li>æœ€ä½³çš„é¢„æµ‹æ¨¡å‹ç»“åˆäº†ä¸´åºŠæ•°æ®ã€è‚¿ç˜¤æ”¾å°„å­¦ç‰¹å¾å’Œé¢„è®­ç»ƒæ¨¡å‹ç‰¹å¾ï¼Œè¾¾åˆ°è¾ƒé«˜çš„C-indexå’Œt-AUCã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-139a3f925e6dcf2741dc0db78cf40380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb3a00e47ad6741787a213fda1f3de1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99cf0c0cda0c7661a555af12c28652bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f949bcc24ceca5e6da34b8ceaf8dc292.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="U2-BENCH-Benchmarking-Large-Vision-Language-Models-on-Ultrasound-Understanding"><a href="#U2-BENCH-Benchmarking-Large-Vision-Language-Models-on-Ultrasound-Understanding" class="headerlink" title="U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound   Understanding"></a>U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound   Understanding</h2><p><strong>Authors:Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</strong></p>
<p>Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging. </p>
<blockquote>
<p>è¶…å£°æ˜¯å…¨çƒåŒ»ç–—ä¸­å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ï¼Œå¯¹å…¶è§£è¯»ä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå…¶åœ¨æ“ä½œè€…ã€å™ªå£°å’Œè§£å‰–ç»“æ„ä¸Šçš„å›¾åƒè´¨é‡å­˜åœ¨å·®å¼‚ã€‚è™½ç„¶åœ¨è‡ªç„¶è¯­è¨€ä¸åŒ»ç–—é¢†åŸŸçš„å¤šæ¨¡æ€èƒ½åŠ›æ–¹é¢ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²ç»è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨è¶…å£°æ–¹é¢çš„è¡¨ç°ä»é²œæœ‰ç ”ç©¶ã€‚æˆ‘ä»¬ä»‹ç»äº†U2-BENCHï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°LVLMåœ¨è¶…å£°ç†è§£æ–¹é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚U2-BENCHæ±‡é›†äº†7241ä¾‹ç—…ä¾‹ï¼Œæ¶‰åŠ15ä¸ªè§£å‰–åŒºåŸŸï¼Œå¹¶å®šä¹‰äº†8ä¸ªå—ä¸´åºŠå¯å‘çš„ä»»åŠ¡ï¼Œå¦‚è¯Šæ–­ã€è§†å›¾è¯†åˆ«ã€ç—…ç¶å®šä½ã€ä¸´åºŠä»·å€¼è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆç­‰ï¼Œæ¶µç›–50ç§è¶…å£°åº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬å¯¹20é¡¹æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºã€é€šç”¨å‹å’ŒåŒ»ç”¨ç‰¹å®šå‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å›¾åƒçº§åˆ«åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œä¸´åºŠè¯­è¨€ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨æŒç»­æŒ‘æˆ˜ã€‚U2-BENCHå»ºç«‹äº†ä¸€ä¸ªä¸¥æ ¼ç»Ÿä¸€çš„æµ‹è¯•å¹³å°ï¼Œä»¥è¯„ä¼°å’ŒåŠ é€ŸåŒ»ç”¨è¶…å£°æˆåƒå¤šæ¨¡æ€é¢†åŸŸçš„LVLMç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17779v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¶…å£°æ˜¯å…¨çƒå«ç”Ÿä¿å¥ä¸­å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ï¼Œä½†å…¶å›¾åƒè´¨é‡å—æ“ä½œäººå‘˜ã€å™ªå£°å’Œè§£å‰–ç»“æ„ç­‰å› ç´ å½±å“ï¼Œè§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†åœ¨è¶…å£°ä¸Šçš„æ€§èƒ½å°šå¾…æ¢ç´¢ã€‚å¼•å…¥U2-BENCHä½œä¸ºè¯„ä¼°LVLMsåœ¨è¶…å£°ç†è§£æ–¹é¢çš„é¦–ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚U2-BENCHåŒ…å«7,241ä¾‹ï¼Œæ¶µç›–15ä¸ªè§£å‰–åŒºåŸŸï¼Œå®šä¹‰8ä¸ªä¸´åºŠä»»åŠ¡ï¼Œå¦‚è¯Šæ–­ã€è§†å›¾è¯†åˆ«ã€ç—…ç¶å®šä½ã€ä¸´åºŠä»·å€¼è¯„ä¼°å’ŒæŠ¥å‘Šç”Ÿæˆç­‰ã€‚è¯„ä¼°äº†20é¡¹å…ˆè¿›çš„LVLMsï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºã€é€šç”¨å’Œä¸“ç”¨çš„æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œå›¾åƒåˆ†ç±»æ€§èƒ½å¼ºï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œä¸´åºŠè¯­è¨€ç”Ÿæˆæ–¹é¢ä»æœ‰æŒ‘æˆ˜ã€‚U2-BENCHä¸ºè¯„ä¼°å’ŒåŠ é€ŸåŒ»å­¦è¶…å£°æˆåƒé¢†åŸŸLVLMç ”ç©¶æä¾›äº†ä¸¥æ ¼ã€ç»Ÿä¸€çš„æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æ˜¯å…¨çƒå«ç”Ÿä¿å¥ä¸­é‡è¦çš„æˆåƒæ–¹å¼ï¼Œä½†å…¶è§£è¯»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦æ›´ç²¾ç¡®çš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè§£è¯»ã€‚</li>
<li>U2-BENCHæ˜¯é¦–ä¸ªé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…å£°ç†è§£æ–¹é¢çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>U2-BENCHæ¶µç›–äº†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹ã€å›å½’å’Œæ–‡æœ¬ç”Ÿæˆç­‰ï¼Œå¹¶å®šä¹‰äº†8ä¸ªä¸´åºŠä»»åŠ¡ã€‚</li>
<li>U2-BENCHåŒ…å«å¤§é‡ç—…ä¾‹æ•°æ®ï¼Œæ¶µç›–å¤šç§è§£å‰–åŒºåŸŸå’Œè¶…å£°åº”ç”¨åœºæ™¯ã€‚</li>
<li>å›¾åƒåˆ†ç±»ä»»åŠ¡åœ¨è¶…å£°ç†è§£æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´æ¨ç†å’Œä¸´åºŠè¯­è¨€ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¶…å£°è§£è¯»æ–¹é¢çš„æ€§èƒ½æœ‰å¾…æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6860afcd51f1da3dc8a02013d1c7ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ee9f732c9dec75ae933e707088f6231.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd5c0ffcdbd3c60b6da8c05668dc6409.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7d4e35f6c8fff3b706799de808d3f2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f8389b7acacf76f1ffb4b87b162eef3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Star-like-thermoresponsive-microgels-a-new-class-of-soft-nanocolloids"><a href="#Star-like-thermoresponsive-microgels-a-new-class-of-soft-nanocolloids" class="headerlink" title="Star-like thermoresponsive microgels: a new class of soft nanocolloids"></a>Star-like thermoresponsive microgels: a new class of soft nanocolloids</h2><p><strong>Authors:Elisa Ballin, Francesco Brasili, Tommaso Papetti, Jacopo Vialetto, Michael Sztucki, Simona Sennato, Marco Laurati, Emanuela Zaccarelli</strong></p>
<p>We provide experimental and numerical evidence of a new class of soft nanocolloids: star-like microgels with thermoresponsive character. This is achieved by using the standard precipitation polymerization synthesis of poly(N-isopropylacrylamide) (PNIPAM) microgels and replacing the usually employed crosslinking agent, N,Nâ€™-methylenebis(acrylamide) (BIS), with ethylene glycol dimethacrylate (EGDMA). The fast reactivity of EGDMA combined with its strong tendency to self-bind produces colloidal networks with a central, crosslinker-rich core, surrounded by a corona of long, crosslinker-free arms. These novel star-like microgels fully retain PNIPAM thermoresponsivity and undergo a volume phase transition at a temperature of 32{\deg}C that is very sharp as compared to standard PNIPAM-BIS microgels, independently of crosslinker content. Dynamic light scattering and small angle X-ray scattering experiments are compared to extensive simulation results, based on ideal star polymers as well as on state-of-the-art monomer-resolved simulations, offering a microscopic evidence of the star-like internal structure of PNIPAM-EGDMA microgels. This can be described by a novel model for the form factors combining star and microgel features. The present work thus bridges the fields of star polymers and microgels, providing the former with the ability to respond to temperature via a facile synthetic route that can be routinely employed, opening the way to exploit these soft particles for a variety of fundamental studies and applicative purposes. </p>
<blockquote>
<p>æˆ‘ä»¬æä¾›ä¸€ç±»æ–°å‹è½¯çº³ç±³èƒ¶ä½“ç‰©è´¨â€”â€”æ˜Ÿå½¢å¾®å‡èƒ¶çš„å®éªŒå’Œæ•°å€¼è¯æ®ï¼Œå®ƒå…·æœ‰çƒ­å“åº”ç‰¹æ€§ã€‚è¿™æ˜¯é€šè¿‡é‡‡ç”¨N-å¼‚ä¸™åŸºä¸™çƒ¯é…°èƒºï¼ˆPNIPAMï¼‰å¾®å‡èƒ¶çš„æ ‡å‡†æ²‰æ·€èšåˆåˆæˆæ–¹æ³•å®ç°çš„ï¼Œç”¨ä¹™äºŒé†‡äºŒç”²ä¸™çƒ¯é…¸é…¯ï¼ˆEGDMAï¼‰æ›¿ä»£é€šå¸¸ä½¿ç”¨çš„äº¤è”å‰‚Nï¼ŒNâ€™-äºšç”²åŸºåŒä¸™çƒ¯é…°èƒºï¼ˆBISï¼‰ã€‚EGDMAçš„å¿«é€Ÿååº”æ€§ä¸å…¶è‡ªèº«å¼ºçƒˆçš„è‡ªç»“åˆè¶‹åŠ¿ç›¸ç»“åˆï¼Œäº§ç”Ÿäº†ä»¥å¯Œå«äº¤è”å‰‚çš„ä¸­å¤®ä¸ºæ ¸å¿ƒï¼Œå‘¨å›´ç¯ç»•ç€é•¿è€Œæ— äº¤è”å‰‚çš„è‡‚çš„èƒ¶ä½“ç½‘ç»œã€‚è¿™äº›æ–°å‹çš„æ˜Ÿå½¢å¾®å‡èƒ¶å®Œå…¨ä¿ç•™äº†PNIPAMçš„çƒ­å“åº”æ€§ï¼Œåœ¨32â„ƒçš„æ¸©åº¦ä¸‹å‘ç”Ÿä½“ç§¯ç›¸å˜ï¼Œä¸æ ‡å‡†çš„PNIPAM-BISå¾®å‡èƒ¶ç›¸æ¯”ï¼Œæ— è®ºäº¤è”å‰‚å«é‡å¦‚ä½•ï¼Œè¿™ç§ç›¸å˜éƒ½éå¸¸å°–é”ã€‚é€šè¿‡åŠ¨æ€å…‰æ•£å°„å’Œå°è§’åº¦Xå°„çº¿æ•£å°„å®éªŒä¸åŸºäºç†æƒ³æ˜Ÿå½¢èšåˆç‰©ä»¥åŠæœ€æ–°çš„å•ä½“è§£ææ¨¡æ‹Ÿçš„å¹¿æ³›æ¨¡æ‹Ÿç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œä¸ºPNIPAM-EGDMAå¾®å‡èƒ¶çš„æ˜Ÿå½¢å†…éƒ¨ç»“æ„æä¾›äº†å¾®è§‚è¯æ®ã€‚è¿™å¯ä»¥é€šè¿‡ç»“åˆæ˜Ÿå½¢å’Œå¾®å‡èƒ¶ç‰¹æ€§çš„æ–°å‹å› å­æ¨¡å‹æ¥æè¿°ã€‚å› æ­¤ï¼Œæœ¬å·¥ä½œæ¶èµ·äº†æ˜Ÿå½¢èšåˆç‰©å’Œå¾®å‡èƒ¶ä¹‹é—´çš„æ¡¥æ¢ï¼Œèµ‹äºˆå‰è€…é€šè¿‡ç®€ä¾¿çš„åˆæˆé€”å¾„å¯¹æ¸©åº¦ä½œå‡ºååº”çš„èƒ½åŠ›ï¼Œä¸ºè¿™äº›è½¯ç²’å­åœ¨åŸºç¡€ç ”ç©¶å’Œåº”ç”¨æ–¹é¢å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17700v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å®éªŒå’Œæ•°å€¼è¯æ˜äº†ä¸€ç§æ–°å‹è½¯çº³ç±³èƒ¶ä½“â€”â€”æ˜ŸçŠ¶å¾®å‡èƒ¶çš„çƒ­æ•ç‰¹æ€§ã€‚é€šè¿‡é‡‡ç”¨æ²‰æ·€èšåˆåˆæˆèšï¼ˆN-å¼‚ä¸™åŸºä¸™çƒ¯é…°èƒºï¼‰ï¼ˆPNIPAMï¼‰å¾®å‡èƒ¶çš„æ ‡å‡†æ–¹æ³•ï¼Œå¹¶æ”¹ç”¨ä¹™äºŒé†‡äºŒç”²é…¸é…¯ï¼ˆEGDMAï¼‰ä½œä¸ºäº¤è”å‰‚ï¼Œå®ç°äº†æ˜ŸçŠ¶å¾®å‡èƒ¶çš„åˆæˆã€‚EGDMAçš„å¿«é€Ÿååº”æ€§å’Œå¼ºçƒˆçš„è‡ªç»“åˆè¶‹åŠ¿ï¼Œå½¢æˆäº†ä»¥äº¤è”å‰‚ä¸°å¯Œçš„æ ¸å¿ƒä¸ºä¸­å¿ƒï¼Œå‘¨å›´ç¯ç»•ç€é•¿è€Œæ— äº¤è”å‰‚çš„è‡‚çš„èƒ¶ä½“ç½‘ç»œã€‚è¿™äº›æ–°å‹çš„æ˜ŸçŠ¶å¾®å‡èƒ¶å®Œå…¨ä¿ç•™äº†PNIPAMçš„çƒ­æ•æ€§ï¼Œåœ¨32â„ƒæ—¶ç»å†äº†ä½“ç§¯ç›¸å˜ï¼Œä¸æ ‡å‡†çš„PNIPAM-BISå¾®å‡èƒ¶ç›¸æ¯”ï¼Œæ— è®ºäº¤è”å‰‚å«é‡å¦‚ä½•ï¼Œè¿™ç§ç›¸å˜éƒ½éå¸¸é”åˆ©ã€‚é€šè¿‡åŠ¨æ€å…‰æ•£å°„å’Œå°è§’åº¦Xå°„çº¿æ•£å°„å®éªŒä¸åŸºäºç†æƒ³æ˜Ÿå½¢èšåˆç‰©å’Œæœ€æ–°å•ä½“è§£ææ¨¡æ‹Ÿçš„å¹¿æ³›æ¨¡æ‹Ÿç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œä»å¾®è§‚ä¸Šè¯æ˜äº†PNIPAM-EGDMAå¾®å‡èƒ¶çš„æ˜Ÿå½¢å†…éƒ¨ç»“æ„ã€‚è¿™å¯ä»¥é€šè¿‡ç»“åˆæ˜Ÿå½¢å’Œå¾®å‡èƒ¶ç‰¹å¾çš„æ–°æ¨¡å‹æ¥æè¿°å…¶å½¢çŠ¶å› å­ã€‚å› æ­¤ï¼Œæœ¬æ–‡ç»“åˆäº†æ˜Ÿå½¢èšåˆç‰©å’Œå¾®å‡èƒ¶é¢†åŸŸï¼Œä¸ºå‰è€…æä¾›äº†ä¸€ç§å¯é€šè¿‡ç®€ä¾¿çš„åˆæˆè·¯çº¿å®ç°å¯¹æ¸©åº¦çš„å“åº”èƒ½åŠ›ï¼Œä¸ºè¿™äº›è½¯ç²’å­åœ¨å„ç§åŸºç¡€ç ”ç©¶å’Œåº”ç”¨ç›®çš„ä¸­çš„åˆ©ç”¨æ‰“å¼€äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æˆåŠŸåˆæˆäº†ä¸€ç§æ–°å‹è½¯çº³ç±³èƒ¶ä½“â€”â€”æ˜ŸçŠ¶å¾®å‡èƒ¶ï¼Œå…·æœ‰çƒ­æ•ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨EGDMAä½œä¸ºäº¤è”å‰‚ï¼Œå®ç°äº†æ˜ŸçŠ¶å¾®å‡èƒ¶çš„ç‹¬ç‰¹å†…éƒ¨ç»“æ„ã€‚</li>
<li>æ˜ŸçŠ¶å¾®å‡èƒ¶åœ¨32â„ƒæ—¶ç»å†å°–é”çš„ä½“ç§¯ç›¸å˜ï¼Œè¡¨ç°å‡ºé«˜åº¦çš„æ¸©åº¦å“åº”æ€§ã€‚</li>
<li>é€šè¿‡åŠ¨æ€å…‰æ•£å°„å’Œå°è§’åº¦Xå°„çº¿æ•£å°„å®éªŒéªŒè¯äº†å¾®å‡èƒ¶çš„æ˜Ÿå½¢å†…éƒ¨ç»“æ„ã€‚</li>
<li>ç»“åˆæ˜Ÿå½¢èšåˆç‰©å’Œå¾®å‡èƒ¶ç‰¹æ€§çš„æ–°æ¨¡å‹æè¿°äº†å…¶å½¢çŠ¶å› å­ã€‚</li>
<li>è¿™ç§åˆæˆæ–¹æ³•ä¸ºåˆ¶å¤‡æ¸©åº¦å“åº”æ€§è½¯ç²’å­æä¾›äº†ä¸€ç§ç®€ä¾¿é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1859cce7152c33f471c29a71bfa52f52.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Medical-Image-Segmentation-via-Dual-Networks"><a href="#Semi-Supervised-Medical-Image-Segmentation-via-Dual-Networks" class="headerlink" title="Semi-Supervised Medical Image Segmentation via Dual Networks"></a>Semi-Supervised Medical Image Segmentation via Dual Networks</h2><p><strong>Authors:Yunyao Lu, Yihang Wu, Reem Kateb, Ahmad Chaddad</strong></p>
<p>Traditional supervised medical image segmentation models require large amounts of labeled data for training; however, obtaining such large-scale labeled datasets in the real world is extremely challenging. Recent semi-supervised segmentation models also suffer from noisy pseudo-label issue and limited supervision in feature space. To solve these challenges, we propose an innovative semi-supervised 3D medical image segmentation method to reduce the dependency on large, expert-labeled datasets. Furthermore, we introduce a dual-network architecture to address the limitations of existing methods in using contextual information and generating reliable pseudo-labels. In addition, a self-supervised contrastive learning strategy is used to enhance the representation of the network and reduce prediction uncertainty by distinguishing between reliable and unreliable predictions. Experiments on clinical magnetic resonance imaging demonstrate that our approach outperforms state-of-the-art techniques. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AIPMLab/Semi-supervised-Segmentation">https://github.com/AIPMLab/Semi-supervised-Segmentation</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿç›‘ç£å¼åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼›ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œä¸­è·å–å¦‚æ­¤å¤§è§„æ¨¡çš„æ ‡è®°æ•°æ®é›†æå…·æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„åŠç›‘ç£åˆ†å‰²æ¨¡å‹è¿˜é¢ä¸´ç€ä¼ªæ ‡ç­¾å™ªå£°é—®é¢˜å’Œç‰¹å¾ç©ºé—´ç›‘ç£æœ‰é™çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œä»¥å‡å°‘å¯¹å¤§è§„æ¨¡ä¸“å®¶æ ‡è®°æ•°æ®é›†çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒç½‘ç»œæ¶æ„ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç”Ÿæˆå¯é ä¼ªæ ‡ç­¾æ–¹é¢çš„å±€é™æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜ç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¹¶é€šè¿‡åŒºåˆ†å¯é å’Œä¸å¯é çš„é¢„æµ‹æ¥å‡å°‘é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚å¯¹ä¸´åºŠç£å…±æŒ¯æˆåƒçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AIPMLab/Semi-supervised-Segmentation">https://github.com/AIPMLab/Semi-supervised-Segmentation</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17690v1">PDF</a> Accepted in ISBI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„åŠç›‘ç£3DåŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå‡å°‘å¯¹å¤§é‡ä¸“å®¶æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚é‡‡ç”¨åŒç½‘ç»œæ¶æ„è§£å†³ç°æœ‰æ–¹æ³•åœ¨ä½¿ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç”Ÿæˆå¯é ä¼ªæ ‡ç­¾æ–¹é¢çš„å±€é™æ€§ã€‚åŒæ—¶ï¼Œä½¿ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ç­–ç•¥å¢å¼ºç½‘ç»œè¡¨ç¤ºï¼Œé€šè¿‡åŒºåˆ†å¯é å’Œä¸å¯é çš„é¢„æµ‹æ¥å‡å°‘é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚åœ¨ä¸´åºŠç£å…±æŒ¯æˆåƒä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•å‡å°‘äº†å¯¹å¤§è§„æ¨¡ä¸“å®¶æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚</li>
<li>å¼•å…¥åŒç½‘ç»œæ¶æ„ï¼Œè§£å†³ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç”Ÿæˆå¯é ä¼ªæ ‡ç­¾æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>é‡‡ç”¨è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæé«˜ç½‘ç»œè¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤ŸåŒºåˆ†å¯é å’Œä¸å¯é çš„é¢„æµ‹ï¼Œå‡å°‘é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åœ¨ä¸´åºŠç£å…±æŒ¯æˆåƒå®éªŒä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²å…¬å¼€æä¾›ï¼Œæ–¹ä¾¿ç ”ç©¶ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-519ab21fbd7214e7b6539abb46b78fef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-946e55674bc6882ed00327a7af868b17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3184db45aecce9bbe1d37006b6134bf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b2104ef29a174b04c6ec4aa5d93d44e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e5aa549a1a76ef6f399d3a66520756c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-806433971d477b6be3069b5a704e2df6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dual-Attention-Residual-U-Net-for-Accurate-Brain-Ultrasound-Segmentation-in-IVH-Detection"><a href="#Dual-Attention-Residual-U-Net-for-Accurate-Brain-Ultrasound-Segmentation-in-IVH-Detection" class="headerlink" title="Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation   in IVH Detection"></a>Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation   in IVH Detection</h2><p><strong>Authors:Dan Yuan, Yi Feng, Ziyun Tang</strong></p>
<p>Intraventricular hemorrhage (IVH) is a severe neurological complication among premature infants, necessitating early and accurate detection from brain ultrasound (US) images to improve clinical outcomes. While recent deep learning methods offer promise for computer-aided diagnosis, challenges remain in capturing both local spatial details and global contextual dependencies critical for segmenting brain anatomies. In this work, we propose an enhanced Residual U-Net architecture incorporating two complementary attention mechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse Attention Layer (SAL). The CBAM improves the modelâ€™s ability to refine spatial and channel-wise features, while the SAL introduces a dual-branch design, sparse attention filters out low-confidence query-key pairs to suppress noise, and dense attention ensures comprehensive information propagation. Extensive experiments on the Brain US dataset demonstrate that our method achieves state-of-the-art segmentation performance, with a Dice score of 89.04% and IoU of 81.84% for ventricle region segmentation. These results highlight the effectiveness of integrating spatial refinement and attention sparsity for robust brain anatomy detection. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/DanYuan001/BrainImgSegment">https://github.com/DanYuan001/BrainImgSegment</a>. </p>
<blockquote>
<p>å®¤å†…å‡ºè¡€ï¼ˆIVHï¼‰æ˜¯æ—©äº§å„¿ä¸­ä¸€ç§ä¸¥é‡çš„ç¥ç»å¹¶å‘ç—‡ï¼Œéœ€è¦é€šè¿‡è„‘éƒ¨è¶…å£°ï¼ˆUSï¼‰å›¾åƒè¿›è¡Œæ—©æœŸå’Œå‡†ç¡®çš„æ£€æµ‹ï¼Œä»¥æ”¹å–„ä¸´åºŠæ²»ç–—æ•ˆæœã€‚è™½ç„¶æœ€è¿‘çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå³æ•è·å¯¹åˆ†å‰²å¤§è„‘ç»“æ„è‡³å…³é‡è¦çš„å±€éƒ¨ç©ºé—´ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„Residual U-Netæ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†ä¸¤ç§äº’è¡¥çš„æ³¨æ„åŠ›æœºåˆ¶ï¼šå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰å’Œç¨€ç–æ³¨æ„åŠ›å±‚ï¼ˆSALï¼‰ã€‚CBAMæé«˜äº†æ¨¡å‹å¯¹ç©ºé—´å’Œé€šé“ç‰¹å¾çš„ç²¾ç»†å¤„ç†èƒ½åŠ›ï¼Œè€ŒSALå¼•å…¥äº†åŒåˆ†æ”¯è®¾è®¡ï¼Œå…¶ä¸­ç¨€ç–æ³¨æ„åŠ›è¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦çš„æŸ¥è¯¢-é”®å¯¹ä»¥æŠ‘åˆ¶å™ªå£°ï¼Œè€Œå¯†é›†æ³¨æ„åŠ›ç¡®ä¿å…¨é¢ä¿¡æ¯ä¼ æ’­ã€‚åœ¨Brain USæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œå¯¹è„‘å®¤åŒºåŸŸåˆ†å‰²çš„Diceå¾—åˆ†ä¸º89.04%ï¼ŒIoUä¸º81.84%ã€‚è¿™äº›ç»“æœçªå‡ºäº†æ•´åˆç©ºé—´ç²¾ç»†åŒ–å’Œæ³¨æ„åŠ›ç¨€ç–æ€§å¯¹äºç¨³å¥çš„å¤§è„‘ç»“æ„æ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/DanYuan001/BrainImgSegment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DanYuan001/BrainImgSegmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17683v1">PDF</a> 10 pages,6 figures and 3 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ‘˜è¦é’ˆå¯¹é¢…å†…å‡ºè¡€çš„æ—©äº§å„¿è¿›è¡Œæ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­ï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ çš„å¢å¼ºResidual U-Netæ¶æ„èåˆå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰å’Œç¨€ç–æ³¨æ„åŠ›å±‚ï¼ˆSALï¼‰å®ç°é«˜æ•ˆè¯Šæ–­ã€‚åœ¨è¶…å£°å›¾åƒä¸Šå–å¾—äº†æ˜¾è‘—åˆ†å‰²æ•ˆæœï¼Œå…·æœ‰89.04%çš„Diceç³»æ•°å’Œ81.84%çš„IoUå¾—åˆ†ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæ•´åˆç©ºé—´ç»†èŠ‚ä¸æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œæé«˜äº†è„‘éƒ¨ç»“æ„æ£€æµ‹çš„ç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ç‰¹å®šé“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹æ—©äº§å„¿é¢…å†…å‡ºè¡€çš„æ—©æœŸå’Œå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>å¢å¼ºResidual U-Netæ¶æ„ç»“åˆäº†å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—ï¼ˆCBAMï¼‰å’Œç¨€ç–æ³¨æ„åŠ›å±‚ï¼ˆSALï¼‰ï¼Œå¢å¼ºäº†è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡åŒé‡åˆ†æ”¯è®¾è®¡å®ç°äº†ç©ºé—´å’Œå…¨å±€ä¾èµ–çš„æ•è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5d4a8cd34464b1f6519cd80f3152d4d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd79653225a3bbcff62057fc26e5f4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9154e211f9bbd636dd8e303d5dde489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-004c7a36e8f7369528eaba9d5eff0522.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EMRA-proxy-Enhancing-Multi-Class-Region-Semantic-Segmentation-in-Remote-Sensing-Images-with-Attention-Proxy"><a href="#EMRA-proxy-Enhancing-Multi-Class-Region-Semantic-Segmentation-in-Remote-Sensing-Images-with-Attention-Proxy" class="headerlink" title="EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote   Sensing Images with Attention Proxy"></a>EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote   Sensing Images with Attention Proxy</h2><p><strong>Authors:Yichun Yu, Yuqing Lan, Zhihuan Xing, Xiaoyi Yang, Tingyue Tang, Dan Yu</strong></p>
<p>High-resolution remote sensing (HRRS) image segmentation is challenging due to complex spatial layouts and diverse object appearances. While CNNs excel at capturing local features, they struggle with long-range dependencies, whereas Transformers can model global context but often neglect local details and are computationally expensive.We propose a novel approach, Region-Aware Proxy Network (RAPNet), which consists of two components: Contextual Region Attention (CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely on grid-based layouts, RAPNet operates at the region level for more flexible segmentation. The CRA module uses a Transformer to capture region-level contextual dependencies, generating a Semantic Region Mask (SRM). The GCR module learns a global class attention map to refine multi-class information, combining the SRM and attention map for accurate segmentation.Experiments on three public datasets show that RAPNet outperforms state-of-the-art methods, achieving superior multi-class segmentation accuracy. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡é¥æ„Ÿï¼ˆHRRSï¼‰å›¾åƒåˆ†å‰²ç”±äºå¤æ‚çš„ç©ºé—´å¸ƒå±€å’Œå¤šæ ·çš„å¯¹è±¡å¤–è§‚è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æ•æ‰å±€éƒ¨ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ—¶å´é‡åˆ°å›°éš¾ï¼Œè€ŒTransformerèƒ½å¤Ÿå»ºæ¨¡å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†å¾€å¾€å¿½ç•¥äº†å±€éƒ¨ç»†èŠ‚ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå³åŒºåŸŸæ„ŸçŸ¥ä»£ç†ç½‘ç»œï¼ˆRAPNetï¼‰ï¼Œå®ƒç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸Šä¸‹æ–‡åŒºåŸŸæ³¨æ„åŠ›ï¼ˆCRAï¼‰å’Œå…¨å±€ç±»åˆ«ç»†åŒ–ï¼ˆGCRï¼‰ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç½‘æ ¼å¸ƒå±€çš„æ–¹æ³•ä¸åŒï¼ŒRAPNetåœ¨åŒºåŸŸçº§åˆ«ä¸Šè¿›è¡Œæ“ä½œï¼Œä»¥å®ç°æ›´çµæ´»çš„åˆ†å‰²ã€‚CRAæ¨¡å—ä½¿ç”¨Transformeræ¥æ•è·åŒºåŸŸçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œç”Ÿæˆè¯­ä¹‰åŒºåŸŸæ©è†œï¼ˆSRMï¼‰ã€‚GCRæ¨¡å—å­¦ä¹ å…¨å±€ç±»åˆ«æ³¨æ„åŠ›å›¾æ¥ç»†åŒ–å¤šç±»åˆ«ä¿¡æ¯ï¼Œç»“åˆSRMå’Œæ³¨æ„åŠ›å›¾è¿›è¡Œç²¾ç¡®åˆ†å‰²ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAPNetä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å¤šç±»åˆ«åˆ†å‰²ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17665v1">PDF</a> Proceedings of the 20th International Conference on Intelligent   Computing (ICIC 2024): Poster Volume I. Tianjin, China, 2024: 538-562</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒåˆ†å‰²æ–¹æ³•â€”â€”Region-Aware Proxy Network (RAPNet)ï¼ŒåŒ…å«Contextual Region Attention (CRA)å’ŒGlobal Class Refinement (GCR)ä¸¤ä¸ªç»„ä»¶ã€‚RAPNetä»¥åŒºåŸŸçº§åˆ«æ“ä½œï¼Œæ›´çµæ´»åœ°å®ç°åˆ†å‰²ï¼Œé€šè¿‡Transformeræ•æ‰åŒºåŸŸçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œç”ŸæˆSemantic Region Mask (SRM)ï¼Œå¹¶ç»“åˆå…¨å±€ç±»æ³¨æ„åŠ›å›¾è¿›è¡Œå¤šç±»ä¿¡æ¯ä¿®æ­£ï¼Œä»è€Œæé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRAPNetçš„å¤šç±»åˆ†å‰²ç²¾åº¦ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒåˆ†å‰²é¢ä¸´å¤æ‚ç©ºé—´å¸ƒå±€å’Œå¤šæ ·ç‰©ä½“å¤–è§‚çš„æŒ‘æˆ˜ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ“…é•¿æ•æ‰å±€éƒ¨ç‰¹å¾ï¼Œä½†éš¾ä»¥å¤„ç†é•¿è·ç¦»ä¾èµ–ï¼›è€ŒTransformerå¯ä»¥å»ºæ¨¡å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†å¿½ç•¥å±€éƒ¨ç»†èŠ‚ä¸”è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>RAPNetæ–¹æ³•åŒ…æ‹¬Contextual Region Attention (CRA)å’ŒGlobal Class Refinement (GCR)ä¸¤ä¸ªç»„ä»¶ã€‚</li>
<li>RAPNetä»¥åŒºåŸŸçº§åˆ«æ“ä½œï¼Œå®ç°æ›´çµæ´»çš„åˆ†å‰²ã€‚</li>
<li>CRAæ¨¡å—ä½¿ç”¨Transformeræ•æ‰åŒºåŸŸçº§åˆ«çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œç”ŸæˆSemantic Region Mask (SRM)ã€‚</li>
<li>GCRæ¨¡å—å­¦ä¹ å…¨å±€ç±»æ³¨æ„åŠ›å›¾ï¼Œç”¨äºä¿®æ­£å¤šç±»ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e839f8b0068b9be7112bc5db3bbca7a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b832d008fd4db3d2c8e020ebc8864a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-274c128e3f0a69e19d8404eea2db5fbf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="4D-CTA-Image-and-geometry-dataset-for-kinematic-analysis-of-abdominal-aortic-aneurysms"><a href="#4D-CTA-Image-and-geometry-dataset-for-kinematic-analysis-of-abdominal-aortic-aneurysms" class="headerlink" title="4D-CTA Image and geometry dataset for kinematic analysis of abdominal   aortic aneurysms"></a>4D-CTA Image and geometry dataset for kinematic analysis of abdominal   aortic aneurysms</h2><p><strong>Authors:Mostafa Jamshidian, Adam Wittek, Saeideh Sekhavat, Farah Alkhatib, Jens Carsten Ritter, Paul M. Parizel, Donatien Le Liepvre, Florian Bernard, Ludovic Minvielle, Antoine FondanÃ¨che, Jane Polce, Christopher Wood, Karol Miller</strong></p>
<p>This article presents a dataset used in the article â€œKinematics of Abdominal Aortic Aneurysmsâ€ [arXiv:2405.13377], published in the Journal of Biomechanics. The dataset is publicly available for download from the Zenodo data repository (<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15477710">https://doi.org/10.5281/zenodo.15477710</a>). The dataset includes time-resolved 3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm (AAA) captured throughout the cardiac cycle from ten patients diagnosed with AAA, along with ten patient-specific AAA geometries extracted from these images. Typically, the 4D-CTA dataset for each patient contains ten electrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac cycle, capturing both the systolic and diastolic phases of the AAA configuration. For method verification, the dataset also includes synthetic ground truth data generated from Patient 1â€™s 3D-CTA AAA image in the diastolic phase. The ground truth data includes the patient-specific finite element (FE) biomechanical model and a synthetic systolic 3D-CTA image. The synthetic systolic image was generated by warping Patient 1â€™s diastolic 3D-CTA image using the realistic displacement field obtained from the AAA biomechanical FE model. The images were acquired at Fiona Stanley Hospital in Western Australia and provided to the researchers at the Intelligent Systems for Medicine Laboratory at The University of Western Australia (ISML-UWA), where image-based AAA kinematic analysis was performed. Our dataset enabled the analysis of AAA wall displacement and strain throughout the cardiac cycle using a non-invasive, in vivo, image registration-based approach. The use of widely adopted, open-source file formats (NRRD for images and STL for geometries) facilitates broad applicability and reusability in AAA biomechanics studies that require patient-specific geometry and information about AAA kinematics during cardiac cycle. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ã€Šç”Ÿç‰©åŠ›å­¦æ‚å¿—ã€‹ä¸Šå‘è¡¨çš„â€œè…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤çš„è¿åŠ¨å­¦â€ä¸€æ–‡ï¼ˆarXiv:2405.13377ï¼‰ä¸­ä½¿ç”¨çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†å¯ä»Zenodoæ•°æ®ä»“åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15477710%EF%BC%89%E5%85%AC%E5%BC%80%E4%B8%8B%E8%BD%BD%E3%80%82%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E9%80%9A%E8%BF%87%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E6%8D%95%E8%8E%B7%E7%9A%84%E8%85%B9%E9%83%A8%E4%B8%BB%E5%8A%A8%E8%84%89%E7%98%A4%EF%BC%88AAA%EF%BC%89%E7%9A%84%E6%97%B6%E9%97%B4%E5%88%86%E8%BE%A8%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E8%A1%80%E7%AE%A1%E9%80%A0%E5%BD%B1%EF%BC%88%E5%9B%9B%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E8%A1%80%E7%AE%A1%E9%80%A0%E5%BD%B1%E5%9B%BE%E5%83%8F%EF%BC%89%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%BB%8E%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E4%B8%AD%E6%8F%90%E5%8F%96%E7%9A%84%E5%8D%81%E4%B8%AA%E6%82%A3%E8%80%85%E7%89%B9%E5%AE%9A%E7%9A%84AAA%E5%87%A0%E4%BD%95%E5%BD%A2%E7%8A%B6%E3%80%82%E9%80%9A%E5%B8%B8%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%82%A3%E8%80%85%E7%9A%84%E5%9B%9B%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E6%89%AB%E6%8F%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E5%8D%81%E4%B8%AA%E5%BF%83%E7%94%B5%E5%9B%BE%E9%97%A8%E6%8E%A7%E7%9A%84%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E6%89%AB%E6%8F%8F%E5%9B%BE%E5%83%8F%E5%B8%A7%EF%BC%8C%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E5%B8%A7%E5%9C%A8%E6%95%B4%E4%B8%AA%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E5%86%85%E6%8D%95%E8%8E%B7AAA%E9%85%8D%E7%BD%AE%E7%9A%84%E6%94%B6%E7%BC%A9%E6%9C%9F%E5%92%8C%E8%88%92%E5%BC%A0%E6%9C%9F%E3%80%82%E4%B8%BA%E4%BA%86%E9%AA%8C%E8%AF%81%E6%96%B9%E6%B3%95%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%98%E5%8C%85%E6%8B%AC%E7%94%B1%E6%82%A3%E8%80%85%E4%B8%80%E5%8F%B7%E5%A4%84%E4%BA%8E%E8%88%92%E5%BC%A0%E6%9C%9F%E7%9A%84%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E6%89%AB%E6%8F%8FAAA%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%9A%84%E5%90%88%E6%88%90%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E3%80%82%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E5%8C%85%E6%8B%AC%E6%82%A3%E8%80%85%E7%89%B9%E5%AE%9A%E7%9A%84%E6%9C%89%E9%99%90%E5%85%83%E7%94%9F%E7%89%A9%E5%8A%9B%E5%AD%A6%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%B8%80%E5%BC%A0%E5%90%88%E6%88%90%E6%94%B6%E7%BC%A9%E6%9C%9F%E7%9A%84%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E6%89%AB%E6%8F%8F%E5%9B%BE%E5%83%8F%E3%80%82%E5%90%88%E6%88%90%E6%94%B6%E7%BC%A9%E6%9C%9F%E5%9B%BE%E5%83%8F%E6%98%AF%E9%80%9A%E8%BF%87%E4%BD%BF%E7%94%A8AAA%E7%94%9F%E7%89%A9%E5%8A%9B%E5%AD%A6%E6%9C%89%E9%99%90%E5%85%83%E6%A8%A1%E5%9E%8B%E8%8E%B7%E5%BE%97%E7%9A%84%E7%8E%B0%E5%AE%9E%E4%BD%8D%E7%A7%BB%E5%9C%BA%E6%9D%A5%E6%89%AD%E6%9B%B2%E6%82%A3%E8%80%85%E4%B8%80%E5%8F%B7%E7%9A%84%E8%88%92%E5%BC%A0%E6%9C%9F%E4%B8%89%E7%BB%B4%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%AD%E5%B1%82%E6%89%AB%E6%8F%8F%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%9A%84%E3%80%82%E8%BF%99%E4%BA%9B%E5%9B%BE%E5%83%8F%E6%98%AF%E5%9C%A8%E8%A5%BF%E6%BE%B3%E5%A4%A7%E5%88%A9%E4%BA%9A%E7%9A%84%E8%8F%B2%E6%AC%A7%E5%A8%9C%E6%96%AF%E5%9D%A6%E5%88%A9%E5%8C%BB%E9%99%A2%E8%8E%B7%E5%BE%97%E7%9A%84%EF%BC%8C%E5%B9%B6%E6%8F%90%E4%BE%9B%E7%BB%99%E8%A5%BF%E6%BE%B3%E5%A4%A7%E5%88%A9%E4%BA%9A%E5%A4%A7%E5%AD%A6%E6%99%BA%E8%83%BD%E5%8C%BB%E5%AD%A6%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%9A%84%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%EF%BC%8C%E5%9C%A8%E9%82%A3%E9%87%8C%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E7%9A%84AAA%E8%BF%90%E5%8A%A8%E5%AD%A6%E5%88%86%E6%9E%90%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E8%83%BD%E5%A4%9F%E4%BD%BF%E7%94%A8%E4%B8%80%E7%A7%8D%E6%97%A0%E5%88%9B%E3%80%81%E6%B4%BB%E4%BD%93%E3%80%81%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E9%85%8D%E5%87%86%E7%9A%84%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90%E6%95%B4%E4%B8%AA%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E5%86%85AAA%E5%A3%81%E4%BD%8D%E7%A7%BB%E5%92%8C%E5%BA%94%E5%8F%98%E3%80%82%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%B3%9B%E9%87%87%E7%94%A8%E7%9A%84%E5%BC%80%E6%BA%90%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%EF%BC%88NRRD%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%92%8CSTL%E7%94%A8%E4%BA%8E%E5%87%A0%E4%BD%95%E5%BD%A2%E7%8A%B6%EF%BC%89%E6%9C%89%E5%8A%A9%E4%BA%8E%E5%9C%A8%E9%9C%80%E8%A6%81%E6%82%A3%E8%80%85%E7%89%B9%E5%AE%9A%E5%87%A0%E4%BD%95%E5%BD%A2%E7%8A%B6%E4%BF%A1%E6%81%AF%E5%92%8C%E5%BF%83%E8%84%8F%E5%91%A8%E6%9C%9F%E5%86%85AAA%E8%BF%90%E5%8A%A8%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%9A%84AAA%E7%94%9F%E7%89%A9%E5%8A%9B%E5%AD%A6%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%B9%BF%E6%B3%9B%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%86%8D%E5%88%A9%E7%94%A8%E3%80%82">https://doi.org/10.5281/zenodo.15477710ï¼‰å…¬å¼€ä¸‹è½½ã€‚æ•°æ®é›†åŒ…å«é€šè¿‡å¿ƒè„å‘¨æœŸæ•è·çš„è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰çš„æ—¶é—´åˆ†è¾¨ä¸‰ç»´è®¡ç®—æœºæ–­å±‚è¡€ç®¡é€ å½±ï¼ˆå››ç»´è®¡ç®—æœºæ–­å±‚è¡€ç®¡é€ å½±å›¾åƒï¼‰ï¼Œä»¥åŠä»è¿™äº›å›¾åƒä¸­æå–çš„åä¸ªæ‚£è€…ç‰¹å®šçš„AAAå‡ ä½•å½¢çŠ¶ã€‚é€šå¸¸ï¼Œæ¯ä¸ªæ‚£è€…çš„å››ç»´è®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®é›†åŒ…å«åä¸ªå¿ƒç”µå›¾é—¨æ§çš„ä¸‰ç»´è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒå¸§ï¼Œè¿™äº›å›¾åƒå¸§åœ¨æ•´ä¸ªå¿ƒè„å‘¨æœŸå†…æ•è·AAAé…ç½®çš„æ”¶ç¼©æœŸå’Œèˆ’å¼ æœŸã€‚ä¸ºäº†éªŒè¯æ–¹æ³•ï¼Œæ•°æ®é›†è¿˜åŒ…æ‹¬ç”±æ‚£è€…ä¸€å·å¤„äºèˆ’å¼ æœŸçš„ä¸‰ç»´è®¡ç®—æœºæ–­å±‚æ‰«æAAAå›¾åƒç”Ÿæˆçš„åˆæˆåŸºå‡†æ•°æ®ã€‚åŸºå‡†æ•°æ®åŒ…æ‹¬æ‚£è€…ç‰¹å®šçš„æœ‰é™å…ƒç”Ÿç‰©åŠ›å­¦æ¨¡å‹å’Œä¸€å¼ åˆæˆæ”¶ç¼©æœŸçš„ä¸‰ç»´è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒã€‚åˆæˆæ”¶ç¼©æœŸå›¾åƒæ˜¯é€šè¿‡ä½¿ç”¨AAAç”Ÿç‰©åŠ›å­¦æœ‰é™å…ƒæ¨¡å‹è·å¾—çš„ç°å®ä½ç§»åœºæ¥æ‰­æ›²æ‚£è€…ä¸€å·çš„èˆ’å¼ æœŸä¸‰ç»´è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒç”Ÿæˆçš„ã€‚è¿™äº›å›¾åƒæ˜¯åœ¨è¥¿æ¾³å¤§åˆ©äºšçš„è²æ¬§å¨œæ–¯å¦åˆ©åŒ»é™¢è·å¾—çš„ï¼Œå¹¶æä¾›ç»™è¥¿æ¾³å¤§åˆ©äºšå¤§å­¦æ™ºèƒ½åŒ»å­¦å®éªŒå®¤çš„ç ”ç©¶äººå‘˜ï¼Œåœ¨é‚£é‡Œè¿›è¡Œäº†åŸºäºå›¾åƒçš„AAAè¿åŠ¨å­¦åˆ†æã€‚æˆ‘ä»¬çš„æ•°æ®é›†èƒ½å¤Ÿä½¿ç”¨ä¸€ç§æ— åˆ›ã€æ´»ä½“ã€åŸºäºå›¾åƒé…å‡†çš„æ–¹æ³•åˆ†ææ•´ä¸ªå¿ƒè„å‘¨æœŸå†…AAAå£ä½ç§»å’Œåº”å˜ã€‚ä½¿ç”¨å¹¿æ³›é‡‡ç”¨çš„å¼€æºæ–‡ä»¶æ ¼å¼ï¼ˆNRRDç”¨äºå›¾åƒå’ŒSTLç”¨äºå‡ ä½•å½¢çŠ¶ï¼‰æœ‰åŠ©äºåœ¨éœ€è¦æ‚£è€…ç‰¹å®šå‡ ä½•å½¢çŠ¶ä¿¡æ¯å’Œå¿ƒè„å‘¨æœŸå†…AAAè¿åŠ¨å­¦ä¿¡æ¯çš„AAAç”Ÿç‰©åŠ›å­¦ç ”ç©¶ä¸­å®ç°å¹¿æ³›çš„åº”ç”¨å’Œå†åˆ©ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17647v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºç ”ç©¶è…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤è¿åŠ¨å­¦çš„æ•°æ®é›†ã€‚æ•°æ®é›†åŒ…å«ä»åä¸ªè¢«è¯Šæ–­ä¸ºè…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤çš„æ‚£è€…èº«ä¸Šæ•è·çš„4D-CTAå›¾åƒï¼Œä»¥åŠä»è¿™äº›å›¾åƒä¸­æå–çš„åä¸ªæ‚£è€…ç‰¹å®šçš„AAAå‡ ä½•å½¢çŠ¶ã€‚æ•°æ®é›†è¿˜åŒ…æ‹¬ç”±æ‚£è€…1çš„èˆ’å¼ æœŸ3D-CTAå›¾åƒç”Ÿæˆçš„åˆæˆåŸºå‡†æ•°æ®ã€‚è¯¥æ•°æ®é›†å¯ä¿ƒè¿›å¯¹AAAå£ä½ç§»å’Œåº”å˜åœ¨æ•´ä¸ªå¿ƒè„å‘¨æœŸå†…çš„åˆ†æï¼Œå¹¶ä½¿ç”¨éä¾µå…¥æ€§ã€ä½“å†…ã€åŸºäºå›¾åƒé…å‡†çš„æ–¹æ³•è¿›è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªå…³äºè…¹éƒ¨ä¸»åŠ¨è„‰ç˜¤ï¼ˆAAAï¼‰çš„å…¬å¼€æ•°æ®é›†ï¼ŒåŒ…å«é€šè¿‡4D-CTAæˆåƒæŠ€æœ¯è·å–çš„æ‚£è€…å›¾åƒå’Œæ‚£è€…ç‰¹å®šçš„AAAå‡ ä½•å½¢çŠ¶æ•°æ®ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ååAAAæ‚£è€…çš„å›¾åƒæ•°æ®ï¼Œæ¯ä¸ªæ‚£è€…åŒ…å«åä¸ªå¿ƒç”µå›¾é—¨æ§çš„3D-CTAå›¾åƒå¸§ï¼Œæ¶µç›–æ•´ä¸ªå¿ƒè„å‘¨æœŸï¼Œå¯åˆ†æAAAé…ç½®åœ¨æ”¶ç¼©æœŸå’Œèˆ’å¼ æœŸçš„ç‰¹ç‚¹ã€‚</li>
<li>æ•°æ®é›†è¿˜åŒ…æ‹¬ç”±æ‚£è€…1çš„èˆ’å¼ æœŸ3D-CTAå›¾åƒç”Ÿæˆçš„åˆæˆåŸºå‡†æ•°æ®ï¼ŒåŒ…æ‹¬æ‚£è€…ç‰¹å®šçš„æœ‰é™å…ƒç”Ÿç‰©åŠ›å­¦æ¨¡å‹å’Œåˆæˆæ”¶ç¼©æœŸ3D-CTAå›¾åƒã€‚</li>
<li>å›¾åƒæ˜¯åœ¨è¥¿æ¾³å¤§åˆ©äºšçš„è²å¥¥å¨œæ–¯å¦åˆ©åŒ»é™¢è·å–çš„ï¼Œå¹¶æä¾›äº†æ™ºèƒ½ç³»ç»ŸåŒ»å­¦å®éªŒå®¤ï¼ˆISML-UWAï¼‰çš„ç ”ç©¶äººå‘˜è¿›è¡Œç ”ç©¶ã€‚</li>
<li>æ•°æ®é›†çš„åˆ†ææ–¹æ³•èƒ½å¤Ÿéä¾µå…¥æ€§åœ°ã€åœ¨æ´»ä½“æƒ…å†µä¸‹ï¼ŒåŸºäºå›¾åƒé…å‡†æ–¹æ³•åˆ†æAAAå£ä½ç§»å’Œåº”å˜åœ¨æ•´ä¸ªå¿ƒè„å‘¨æœŸçš„å˜åŒ–ã€‚</li>
<li>æ•°æ®é›†ä½¿ç”¨å¹¿æ³›é‡‡ç”¨çš„å¼€æ”¾æºæ–‡ä»¶æ ¼å¼ï¼ˆNRRDç”¨äºå›¾åƒå’ŒSTLç”¨äºå‡ ä½•ï¼‰ï¼Œä¾¿äºåœ¨éœ€è¦æ‚£è€…ç‰¹å®šå‡ ä½•ä¿¡æ¯å’Œå¿ƒè„å‘¨æœŸå†…AAAè¿åŠ¨å­¦ä¿¡æ¯çš„AAAç”Ÿç‰©åŠ›å­¦ç ”ç©¶ä¸­ä½¿ç”¨å’Œå¤ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e3dc20c9f142fec809fb181b9a798ce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a7cc56a751e68b506ce12a9f6d67fd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c74e3e13ff7b2197c02547586da1595.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97e50b3f8cfa8755076b76dc5001e8f1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CAS-IQA-Teaching-Vision-Language-Models-for-Synthetic-Angiography-Quality-Assessment"><a href="#CAS-IQA-Teaching-Vision-Language-Models-for-Synthetic-Angiography-Quality-Assessment" class="headerlink" title="CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography   Quality Assessment"></a>CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography   Quality Assessment</h2><p><strong>Authors:Bo Wang, De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Nu-Fang Xiao, Jian-Long Hao, Ming-Yuan Liu, Zeng-Guang Hou</strong></p>
<p>Synthetic X-ray angiographies generated by modern generative models hold great potential to reduce the use of contrast agents in vascular interventional procedures. However, low-quality synthetic angiographies can significantly increase procedural risk, underscoring the need for reliable image quality assessment (IQA) methods. Existing IQA models, however, fail to leverage auxiliary images as references during evaluation and lack fine-grained, task-specific metrics necessary for clinical relevance. To address these limitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based framework that predicts fine-grained quality scores by effectively incorporating auxiliary information from related images. In the absence of angiography datasets, CAS-3K is constructed, comprising 3,565 synthetic angiographies along with score annotations. To ensure clinically meaningful assessment, three task-specific evaluation metrics are defined. Furthermore, a Multi-path featUre fuSion and rouTing (MUST) module is designed to enhance image representations by adaptively fusing and routing visual tokens to metric-specific branches. Extensive experiments on the CAS-3K dataset demonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods by a considerable margin. </p>
<blockquote>
<p>ç”±ç°ä»£ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆXå°„çº¿è¡€ç®¡é€ å½±å›¾åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¯ä»¥å‡å°‘è¡€ç®¡ä»‹å…¥è¿‡ç¨‹ä¸­å¯¹é€ å½±å‰‚çš„ä½¿ç”¨ï¼Œå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä½è´¨é‡çš„åˆæˆè¡€ç®¡é€ å½±å›¾å¯èƒ½ä¼šæ˜¾è‘—å¢åŠ æ‰‹æœ¯é£é™©ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦å¯é çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„IQAæ¨¡å‹åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­æœªèƒ½åˆ©ç”¨è¾…åŠ©å›¾åƒä½œä¸ºå‚è€ƒï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹ä¸´åºŠç›¸å…³æ€§çš„ç²¾ç»†ä»»åŠ¡å’Œç‰¹å®šæŒ‡æ ‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†CAS-IQAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆåœ°ç»“åˆç›¸å…³å›¾åƒçš„è¾…åŠ©ä¿¡æ¯æ¥é¢„æµ‹ç²¾ç»†çš„è´¨é‡åˆ†æ•°ã€‚åœ¨æ²¡æœ‰è¡€ç®¡é€ å½±æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæ„å»ºäº†CAS-3Kï¼Œå®ƒåŒ…æ‹¬3565å¼ åˆæˆè¡€ç®¡é€ å½±å›¾åƒä»¥åŠåˆ†æ•°æ³¨é‡Šã€‚ä¸ºäº†ç¡®ä¿ä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„è¯„ä¼°ï¼Œå®šä¹‰äº†ä¸‰ä¸ªç‰¹å®šä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šè·¯å¾„ç‰¹å¾èåˆå’Œè·¯ç”±ï¼ˆMUSTï¼‰æ¨¡å—ï¼Œé€šè¿‡è‡ªé€‚åº”èåˆå’Œè·¯ç”±è§†è§‰ä»¤ç‰Œåˆ°ç‰¹å®šæŒ‡æ ‡çš„åˆ†æ”¯ï¼Œä»¥å¢å¼ºå›¾åƒè¡¨ç¤ºã€‚åœ¨CAS-3Kæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCAS-IQAæ˜¾è‘—ä¼˜äºæœ€æ–°ä¸€ä»£çš„IQAæ–¹æ³•ï¼Œå·®è·ç›¸å½“æ˜æ˜¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17619v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†åŸºäºç°ä»£ç”Ÿæˆæ¨¡å‹çš„åˆæˆXå°„çº¿è¡€ç®¡é€ å½±å›¾åƒè´¨é‡è¯„ä¼°çš„é—®é¢˜ã€‚ç”±äºä½è´¨é‡çš„åˆæˆè¡€ç®¡é€ å½±å›¾åƒå¯èƒ½å¢åŠ æ‰‹æœ¯é£é™©ï¼Œå› æ­¤éœ€è¦å¯é çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ–¹æ³•ã€‚ä¸ºè§£å†³ç°æœ‰IQAæ¨¡å‹æœªèƒ½åˆ©ç”¨è¾…åŠ©å›¾åƒä½œä¸ºå‚è€ƒä»¥åŠç¼ºä¹é’ˆå¯¹ä¸´åºŠç›¸å…³çš„ç²¾ç»†ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„CAS-IQAæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ç›¸å…³å›¾åƒä¸­çš„è¾…åŠ©ä¿¡æ¯æ¥é¢„æµ‹ç²¾ç»†çš„è´¨é‡åˆ†æ•°ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›è¡Œä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„è¯„ä¼°ï¼Œå®šä¹‰äº†ä¸‰ä¸ªä»»åŠ¡ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¤šè·¯å¾„ç‰¹å¾èåˆå’Œè·¯ç”±ï¼ˆMUSTï¼‰æ¨¡å—æ¥å¢å¼ºå›¾åƒè¡¨ç¤ºã€‚åœ¨CAS-3Kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCAS-IQAæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„IQAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆXå°„çº¿è¡€ç®¡é€ å½±å›¾åƒåœ¨ä¸´åºŠåº”ç”¨ä¸­æœ‰æ½œåŠ›å‡å°‘é€ å½±å‰‚çš„ä½¿ç”¨ï¼Œä½†ä½è´¨é‡å›¾åƒå¯èƒ½å¢åŠ æ‰‹æœ¯é£é™©ã€‚</li>
<li>ç°æœ‰çš„å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ¨¡å‹æœªèƒ½å……åˆ†åˆ©ç”¨è¾…åŠ©å›¾åƒï¼Œç¼ºä¹é’ˆå¯¹ä¸´åºŠä»»åŠ¡çš„å…·ä½“æŒ‡æ ‡ã€‚</li>
<li>CAS-IQAæ¡†æ¶ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆåˆ©ç”¨ç›¸å…³å›¾åƒçš„è¾…åŠ©ä¿¡æ¯é¢„æµ‹ç²¾ç»†è´¨é‡åˆ†æ•°ã€‚</li>
<li>ä¸ºç¡®ä¿ä¸´åºŠç›¸å…³çš„è¯„ä¼°ï¼Œå®šä¹‰äº†ä¸‰ä¸ªä»»åŠ¡ç‰¹å®šçš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>è®¾è®¡äº†MUSTæ¨¡å—æ¥å¢å¼ºå›¾åƒè¡¨ç¤ºï¼Œé€šè¿‡è‡ªé€‚åº”èåˆå’Œè·¯ç”±è§†è§‰æ ‡è®°åˆ°ç‰¹å®šæŒ‡æ ‡åˆ†æ”¯ã€‚</li>
<li>CAS-IQAåœ¨CAS-3Kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°æ˜¾è‘—ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„IQAæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89bd40a3cb9430cab97bd8ef3ec3fa99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9720c72f53dbb9b2a501280cf6a2b788.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cbcf13ce8d383d3a3cef4f3ccb1e6d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d8bfbe3dfd738bf53e5badf6dea15dd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Unified-Multi-Scale-Attention-Based-Network-for-Automatic-3D-Segmentation-of-Lung-Parenchyma-Nodules-In-Thoracic-CT-Images"><a href="#A-Unified-Multi-Scale-Attention-Based-Network-for-Automatic-3D-Segmentation-of-Lung-Parenchyma-Nodules-In-Thoracic-CT-Images" class="headerlink" title="A Unified Multi-Scale Attention-Based Network for Automatic 3D   Segmentation of Lung Parenchyma &amp; Nodules In Thoracic CT Images"></a>A Unified Multi-Scale Attention-Based Network for Automatic 3D   Segmentation of Lung Parenchyma &amp; Nodules In Thoracic CT Images</h2><p><strong>Authors:Muhammad Abdullah, Furqan Shaukat</strong></p>
<p>Lung cancer has been one of the major threats across the world with the highest mortalities. Computer-aided detection (CAD) can help in early detection and thus can help increase the survival rate. Accurate lung parenchyma segmentation (to include the juxta-pleural nodules) and lung nodule segmentation, the primary symptom of lung cancer, play a crucial role in the overall accuracy of the Lung CAD pipeline. Lung nodule segmentation is quite challenging because of the diverse nodule types and other inhibit structures present within the lung lobes. Traditional machine&#x2F;deep learning methods suffer from generalization and robustness. Recent Vision Language Models&#x2F;Foundation Models perform well on the anatomical level, but they suffer on fine-grained segmentation tasks, and their semi-automatic nature limits their effectiveness in real-time clinical scenarios. In this paper, we propose a novel method for accurate 3D segmentation of lung parenchyma and lung nodules. The proposed architecture is an attention-based network with residual blocks at each encoder-decoder state. Max pooling is replaced by strided convolutions at the encoder, and trilinear interpolation is replaced by transposed convolutions at the decoder to maximize the number of learnable parameters. Dilated convolutions at each encoder-decoder stage allow the model to capture the larger context without increasing computational costs. The proposed method has been evaluated extensively on one of the largest publicly available datasets, namely LUNA16, and is compared with recent notable work in the domain using standard performance metrics like Dice score, IOU, etc. It can be seen from the results that the proposed method achieves better performance than state-of-the-art methods. The source code, datasets, and pre-processed data can be accessed using the link: <a target="_blank" rel="noopener" href="https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet">https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet</a>. </p>
<blockquote>
<p>è‚ºç™Œæ˜¯ä¸–ç•Œä¸Šæœ€å¤§çš„å¨èƒä¹‹ä¸€ï¼Œè‡´æ­»ç‡æé«˜ã€‚è®¡ç®—æœºè¾…åŠ©æ£€æµ‹ï¼ˆCADï¼‰æœ‰åŠ©äºæ—©æœŸå‘ç°ï¼Œä»è€Œæé«˜å­˜æ´»ç‡ã€‚ç²¾ç¡®çš„è‚ºå®è´¨åˆ†å‰²ï¼ˆåŒ…æ‹¬èƒ¸è†œä¸‹ç»“èŠ‚ï¼‰å’Œè‚ºç»“èŠ‚åˆ†å‰²ï¼ˆè‚ºç™Œçš„ä¸»è¦ç—‡çŠ¶ï¼‰åœ¨è‚ºCADç®¡é“çš„æ•´ä½“å‡†ç¡®æ€§ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è‚ºç»“èŠ‚åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè‚ºå¶ä¸­å­˜åœ¨å„ç§ç±»å‹çš„ç»“èŠ‚å’Œå…¶ä»–æŠ‘åˆ¶ç»“æ„ã€‚ä¼ ç»Ÿçš„æœºå™¨&#x2F;æ·±åº¦å­¦ä¹ æ–¹æ³•å­˜åœ¨é€šç”¨æ€§å’Œç¨³å¥æ€§é—®é¢˜ã€‚æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹&#x2F;åŸºç¡€æ¨¡å‹åœ¨è§£å‰–æ°´å¹³ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç²¾ç»†åˆ†å‰²ä»»åŠ¡ä¸Šå­˜åœ¨é—®é¢˜ï¼Œå…¶åŠè‡ªåŠ¨æ€§è´¨é™åˆ¶äº†å…¶åœ¨å®æ—¶ä¸´åºŠåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç²¾ç¡®3Dè‚ºå®è´¨å’Œè‚ºç»“èŠ‚åˆ†å‰²çš„æ–°æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ¶æ„æ˜¯ä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„ç½‘ç»œï¼Œæ¯ä¸ªç¼–ç å™¨-è§£ç å™¨çŠ¶æ€éƒ½æœ‰æ®‹å·®å—ã€‚ç¼–ç å™¨å¤„çš„æœ€å¤§æ± åŒ–è¢«æ­¥å¹…å·ç§¯æ‰€æ›¿ä»£ï¼Œè§£ç å™¨å¤„çš„ä¸‰çº¿æ€§æ’å€¼è¢«è½¬ç½®å·ç§¯æ‰€æ›¿ä»£ï¼Œä»¥æœ€å¤§åŒ–å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ã€‚æ¯ä¸ªç¼–ç å™¨-è§£ç å™¨é˜¶æ®µçš„è†¨èƒ€å·ç§¯å…è®¸æ¨¡å‹æ•è·æ›´å¤§çš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚è¯¥æ–¹æ³•å·²åœ¨æœ€å¤§çš„å…¬å¼€æ•°æ®é›†ä¹‹ä¸€LUNA16ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨Diceåˆ†æ•°ã€IOUç­‰æ ‡å‡†æ€§èƒ½æŒ‡æ ‡ä¸é¢†åŸŸä¸­çš„æœ€æ–°æ˜¾è‘—å·¥ä½œè¿›è¡Œäº†æ¯”è¾ƒã€‚ä»ç»“æœå¯ä»¥çœ‹å‡ºï¼Œè¯¥æ–¹æ³•æ¯”æœ€æ–°æŠ€æœ¯å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚æºä»£ç ã€æ•°æ®é›†å’Œé¢„å¤„ç†æ•°æ®å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet">https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17602v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„ä¸‰ç»´åˆ†å‰²æ–¹æ³•ï¼Œç”¨äºå‡†ç¡®åœ°è¿›è¡Œè‚ºå®è´¨å’Œè‚ºç»“èŠ‚åˆ†å‰²ã€‚è¯¥æ–¹æ³•åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç½‘ç»œæ¶æ„ï¼Œå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹å…¬å¼€æ•°æ®é›†LUNA16ä¸Šè¿›è¡Œè¯„ä¼°æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„ç»“æœã€‚è¯¥æ–¹æ³•çš„æºä»£ç ã€æ•°æ®é›†å’Œé¢„å¤„ç†æ•°æ®å‡å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œæ˜¯å…¨çƒä¸»è¦çš„å¥åº·å¨èƒä¹‹ä¸€ï¼Œè®¡ç®—æœºè¾…åŠ©æ£€æµ‹ï¼ˆCADï¼‰æœ‰åŠ©äºæ—©æœŸå‘ç°å’Œå¢åŠ å­˜æ´»ç‡ã€‚</li>
<li>è‚ºå®è´¨å’Œè‚ºç»“èŠ‚çš„å‡†ç¡®åˆ†å‰²åœ¨è‚ºCADæµç¨‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨&#x2F;æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è‚ºç»“èŠ‚åˆ†å‰²ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç»“èŠ‚ç±»å‹å¤šæ ·ä¸”è‚ºå¶å†…å­˜åœ¨å…¶ä»–æŠ‘åˆ¶ç»“æ„ã€‚</li>
<li>æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹&#x2F;åŸºç¡€æ¨¡å‹åœ¨è§£å‰–æ°´å¹³ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç²¾ç»†åˆ†å‰²ä»»åŠ¡ä¸Šå—é™ï¼Œä¸”å…¶åŠè‡ªåŠ¨æ€§è´¨é™åˆ¶äº†å®ƒä»¬åœ¨å®æ—¶ä¸´åºŠåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´åˆ†å‰²æ–¹æ³•ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç½‘ç»œæ¶æ„ï¼Œå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨æ®‹å·®å—å’Œæ›¿æ¢æœ€å¤§æ± åŒ–çš„æ­¥å¹…å·ç§¯ä»¥åŠæ›¿æ¢ä¸‰çº¿æ€§æ’å€¼çš„è½¬ç½®å·ç§¯ç­‰æŠ€æœ¯ï¼Œä»¥æœ€å¤§åŒ–å¯å­¦ä¹ å‚æ•°çš„æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b11a98d8ca43fb81b684282b429f1073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-056556a427311502ab85330c329260e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a745e40bafc92f5c2507dc010b230d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6efea559dcd94de2696a5609eb62f369.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FreqU-FNet-Frequency-Aware-U-Net-for-Imbalanced-Medical-Image-Segmentation"><a href="#FreqU-FNet-Frequency-Aware-U-Net-for-Imbalanced-Medical-Image-Segmentation" class="headerlink" title="FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image   Segmentation"></a>FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image   Segmentation</h2><p><strong>Authors:Ruiqi Xing</strong></p>
<p>Medical image segmentation faces persistent challenges due to severe class imbalance and the frequency-specific distribution of anatomical structures. Most conventional CNN-based methods operate in the spatial domain and struggle to capture minority class signals, often affected by frequency aliasing and limited spectral selectivity. Transformer-based models, while powerful in modeling global dependencies, tend to overlook critical local details necessary for fine-grained segmentation. To overcome these limitations, we propose FreqU-FNet, a novel U-shaped segmentation architecture operating in the frequency domain. Our framework incorporates a Frequency Encoder that leverages Low-Pass Frequency Convolution and Daubechies wavelet-based downsampling to extract multi-scale spectral features. To reconstruct fine spatial details, we introduce a Spatial Learnable Decoder (SLD) equipped with an adaptive multi-branch upsampling strategy. Furthermore, we design a frequency-aware loss (FAL) function to enhance minority class learning. Extensive experiments on multiple medical segmentation benchmarks demonstrate that FreqU-FNet consistently outperforms both CNN and Transformer baselines, particularly in handling under-represented classes, by effectively exploiting discriminative frequency bands. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç€ç”±äºç±»åˆ«ä¸¥é‡ä¸å¹³è¡¡å’Œè§£å‰–ç»“æ„ç‰¹å®šé¢‘ç‡åˆ†å¸ƒè€Œå¸¦æ¥çš„æŒç»­æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ä¼ ç»Ÿçš„åŸºäºCNNçš„æ–¹æ³•åœ¨ç©ºé—´åŸŸä¸­æ“ä½œï¼Œéš¾ä»¥æ•è·å°‘æ•°ç±»ä¿¡å·ï¼Œå¸¸å¸¸å—åˆ°é¢‘ç‡æ··å å’Œæœ‰é™è°±é€‰æ‹©æ€§çš„å½±å“ã€‚è™½ç„¶åŸºäºTransformerçš„æ¨¡å‹åœ¨å»ºæ¨¡å…¨å±€ä¾èµ–æ€§æ–¹é¢éå¸¸å¼ºå¤§ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†ç”¨äºç²¾ç»†åˆ†å‰²æ‰€å¿…éœ€çš„å…³é”®å±€éƒ¨ç»†èŠ‚ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FreqU-FNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é¢‘ç‡åŸŸUå‹åˆ†å‰²æ¶æ„ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†ä¸€ä¸ªé¢‘ç‡ç¼–ç å™¨ï¼Œå®ƒåˆ©ç”¨ä½é€šé¢‘ç‡å·ç§¯å’ŒåŸºäºDaubechieså°æ³¢çš„é™é‡‡æ ·æ¥æå–å¤šå°ºåº¦è°±ç‰¹å¾ã€‚ä¸ºäº†é‡å»ºç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é…å¤‡æœ‰è‡ªé€‚åº”å¤šåˆ†æ”¯ä¸Šé‡‡æ ·ç­–ç•¥çš„Spatial Learnable Decoderï¼ˆSLDï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥æŸå¤±ï¼ˆFALï¼‰å‡½æ•°ï¼Œä»¥å¢å¼ºå¯¹å°‘æ•°ç±»çš„å­¦ä¹ ã€‚åœ¨å¤šä¸ªåŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFreqU-FNetæŒç»­ä¼˜äºCNNå’ŒTransformeråŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¡¨ç¤ºä¸è¶³çš„ç±»åˆ«æ—¶ï¼Œé€šè¿‡æœ‰æ•ˆåœ°åˆ©ç”¨åˆ¤åˆ«é¢‘å¸¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17544v1">PDF</a> 15 pages, 1 figure</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡å’Œç‰¹å®šé¢‘ç‡åˆ†å¸ƒé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹Uå‹é¢‘åŸŸåˆ†å‰²æ¶æ„FreqU-FNetã€‚è¯¥æ¶æ„åŒ…æ‹¬é¢‘ç‡ç¼–ç å™¨ã€ç©ºé—´å¯å­¦ä¹ è§£ç å™¨å’Œé¢‘ç‡æ„ŸçŸ¥æŸå¤±å‡½æ•°ã€‚é¢‘ç‡ç¼–ç å™¨åˆ©ç”¨ä½é€šé¢‘å·ç§¯å’Œå°æ³¢ä¸‹é‡‡æ ·æå–å¤šå°ºåº¦é¢‘è°±ç‰¹å¾ï¼Œè§£ç å™¨é‡‡ç”¨è‡ªé€‚åº”å¤šåˆ†æ”¯ä¸Šé‡‡æ ·ç­–ç•¥ä»¥é‡å»ºç²¾ç»†ç©ºé—´ç»†èŠ‚ã€‚åœ¨å¤šä¸ªåŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šï¼ŒFreqU-FNetè¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä»£è¡¨æ€§ä¸è¶³çš„ç±»åˆ«æ—¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡å’Œç‰¹å®šé¢‘ç‡åˆ†å¸ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿçš„CNNæ–¹æ³•ä¸»è¦åœ¨ç©ºé—´åŸŸæ“ä½œï¼Œéš¾ä»¥æ•æ‰å°‘æ•°ç±»åˆ«çš„ä¿¡å·ï¼Œå¹¶å—åˆ°é¢‘ç‡æ··å å’Œæœ‰é™é¢‘è°±é€‰æ‹©æ€§çš„å½±å“ã€‚</li>
<li>Transformeræ¨¡å‹è™½ç„¶æ“…é•¿å»ºæ¨¡å…¨å±€ä¾èµ–æ€§ï¼Œä½†å¾€å¾€å¿½è§†å…³é”®çš„å±€éƒ¨ç»†èŠ‚ï¼Œè¿™å¯¹äºç²¾ç»†åˆ†å‰²è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹é¢‘åŸŸåˆ†å‰²æ¶æ„FreqU-FNetï¼Œç»“åˆäº†é¢‘ç‡ç¼–ç å™¨å’Œç©ºé—´å¯å­¦ä¹ è§£ç å™¨ã€‚</li>
<li>é¢‘ç‡ç¼–ç å™¨åˆ©ç”¨ä½é€šé¢‘å·ç§¯å’Œå°æ³¢ä¸‹é‡‡æ ·æå–å¤šå°ºåº¦é¢‘è°±ç‰¹å¾ã€‚</li>
<li>ç©ºé—´å¯å­¦ä¹ è§£ç å™¨é€šè¿‡è‡ªé€‚åº”å¤šåˆ†æ”¯ä¸Šé‡‡æ ·ç­–ç•¥é‡å»ºç²¾ç»†ç©ºé—´ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3309ba5b6b1e261a7045f9e06ce2df4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-386f362f4f21c07fee64f4bbb1c2135a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Anatomy-Guided-Multitask-Learning-for-MRI-Based-Classification-of-Placenta-Accreta-Spectrum-and-its-Subtypes"><a href="#Anatomy-Guided-Multitask-Learning-for-MRI-Based-Classification-of-Placenta-Accreta-Spectrum-and-its-Subtypes" class="headerlink" title="Anatomy-Guided Multitask Learning for MRI-Based Classification of   Placenta Accreta Spectrum and its Subtypes"></a>Anatomy-Guided Multitask Learning for MRI-Based Classification of   Placenta Accreta Spectrum and its Subtypes</h2><p><strong>Authors:Hai Jiang, Qiongting Liu, Yuanpin Zhou, Jiawei Pan, Ting Song, Yao Lu</strong></p>
<p>Placenta Accreta Spectrum Disorders (PAS) pose significant risks during pregnancy, frequently leading to postpartum hemorrhage during cesarean deliveries and other severe clinical complications, with bleeding severity correlating to the degree of placental invasion. Consequently, accurate prenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta increta (PI), and placenta percreta (PP)-is crucial. However, existing guidelines and methodologies predominantly focus on the presence of PAS, with limited research addressing subtype recognition. Additionally, previous multi-class diagnostic efforts have primarily relied on inefficient two-stage cascaded binary classification tasks. In this study, we propose a novel convolutional neural network (CNN) architecture designed for efficient one-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic resonance imaging (MRI) slices. Our model features two branches: the main classification branch utilizes a residual block architecture comprising multiple residual blocks, while the second branch integrates anatomical features of the uteroplacental area and the adjacent uterine serous layer to enhance the modelâ€™s attention during classification. Furthermore, we implement a multitask learning strategy to leverage both branches effectively. Experiments conducted on a real clinical dataset demonstrate that our model achieves state-of-the-art performance. </p>
<blockquote>
<p>èƒç›˜æ¤å…¥è°±ç³»ç–¾ç—…ï¼ˆPASï¼‰åœ¨å¦Šå¨ è¿‡ç¨‹ä¸­å­˜åœ¨é‡å¤§é£é™©ï¼Œç»å¸¸å¯¼è‡´å‰–å®«äº§æœ¯åå‡ºè¡€å’Œå…¶ä»–ä¸¥é‡ä¸´åºŠå¹¶å‘ç—‡ï¼Œå‡ºè¡€ä¸¥é‡ç¨‹åº¦ä¸èƒç›˜ä¾µçŠ¯ç¨‹åº¦æœ‰å…³ã€‚å› æ­¤ï¼Œå¯¹èƒå„¿èƒç›˜æ¤å…¥ç—‡åŠå…¶äºšå‹ï¼ˆèƒç›˜æ¤å…¥ï¼ˆPAï¼‰ã€èƒç›˜é»é™„ï¼ˆPIï¼‰å’Œèƒç›˜ç©¿é€ï¼ˆPPï¼‰ï¼‰çš„äº§å‰å‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æŒ‡å—å’Œæ–¹æ³•ä¸»è¦å…³æ³¨PASçš„å­˜åœ¨ï¼Œå¯¹äºšå‹è¯†åˆ«çš„ç ”ç©¶æœ‰é™ã€‚æ­¤å¤–ï¼Œä»¥å‰çš„å¤šç±»è¯Šæ–­å·¥ä½œä¸»è¦ä¾èµ–äºæ•ˆç‡ä½ä¸‹çš„ä¸¤é˜¶æ®µçº§è”äºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„æ—¨åœ¨åŸºäº4140ä¸ªç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ‡ç‰‡ï¼Œå®ç°PASåŠå…¶äºšå‹çš„ä¸€ç«™å¼å¤šç±»è¯Šæ–­ã€‚æˆ‘ä»¬çš„æ¨¡å‹æœ‰ä¸¤ä¸ªåˆ†æ”¯ï¼šä¸»åˆ†ç±»åˆ†æ”¯åˆ©ç”¨åŒ…å«å¤šä¸ªæ®‹å·®å—çš„æ®‹å·®å—æ¶æ„ï¼Œè€Œç¬¬äºŒä¸ªåˆ†æ”¯ç»“åˆäº†å­å®«èƒç›˜åŒºåŸŸå’Œé‚»è¿‘çš„å­å®«æµ†è†œå±‚çš„è§£å‰–ç‰¹å¾ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨åˆ†ç±»è¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨è¿™ä¸¤ä¸ªåˆ†æ”¯ã€‚åœ¨çœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17484v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†èƒç›˜æ¤å…¥ç—‡è°±ï¼ˆPASï¼‰åŠå…¶äºšå‹ï¼ˆèƒç›˜æ¤å…¥ã€èƒç›˜æ¸—å…¥å’Œèƒç›˜å…¨å±‚ä¾µå…¥ï¼‰çš„äº§å‰è¯Šæ–­é—®é¢˜ã€‚ç”±äºPASåœ¨å¦Šå¨ æœŸé—´å­˜åœ¨æ˜¾è‘—é£é™©ï¼Œä¸”å‡ºè¡€ä¸¥é‡ç¨‹åº¦ä¸èƒç›˜ä¾µè¢­ç¨‹åº¦ç›¸å…³ï¼Œå› æ­¤å‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å¤šå…³æ³¨PASçš„å­˜åœ¨ï¼Œå¯¹äºšå‹è¯†åˆ«çš„ç ”ç©¶è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„ï¼Œèƒ½å¤Ÿä¸€æ¬¡æ€§è¿›è¡ŒPASåŠå…¶äºšå‹çš„è¯Šæ–­ï¼ŒåŸºäº4140ä¸ªç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ‡ç‰‡è¿›è¡Œè®­ç»ƒã€‚æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªåˆ†æ”¯ï¼Œä¸»åˆ†ç±»åˆ†æ”¯ä½¿ç”¨æ®‹å·®å—æ¶æ„ï¼Œç¬¬äºŒåˆ†æ”¯æ•´åˆå­å®«èƒç›˜åŒºåŸŸå’Œé‚»è¿‘å­å®«æµ†è†œå±‚çš„è§£å‰–ç‰¹å¾ä»¥å¢å¼ºåˆ†ç±»æ—¶çš„æ³¨æ„åŠ›ã€‚åŒæ—¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸¤ä¸ªåˆ†æ”¯çš„ä¼˜åŠ¿ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒç›˜æ¤å…¥ç—‡è°±ï¼ˆPASï¼‰åŠäºšå‹å¯¹å­•å¦‡å­˜åœ¨æ˜¾è‘—é£é™©ï¼Œå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰ç ”ç©¶å¤šå…³æ³¨PASçš„å­˜åœ¨ï¼Œè€Œå¯¹äºšå‹çš„è¯†åˆ«ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„ï¼Œèƒ½ä¸€æ¬¡æ€§è¿›è¡ŒPASåŠå…¶äºšå‹çš„è¯Šæ–­ã€‚</li>
<li>æ¨¡å‹åŸºäº4140ä¸ªç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ‡ç‰‡è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªåˆ†æ”¯ï¼Œä¸»åˆ†ç±»åˆ†æ”¯ä½¿ç”¨æ®‹å·®å—æ¶æ„ï¼Œç¬¬äºŒåˆ†æ”¯æ•´åˆè§£å‰–ç‰¹å¾ä»¥å¢å¼ºåˆ†ç±»æ³¨æ„åŠ›ã€‚</li>
<li>é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ ç­–ç•¥ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸¤ä¸ªåˆ†æ”¯çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e198b5c328467338c73fa5b3beb69a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b922236222437ae3c925b86ce8ee3911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d10a7a6f53f1b2a84fd2f936e50ef26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d46d9c0b882f10d2033d1098dd47588a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c43c4990ebad3ddd3fc0f91e4167711c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SUFFICIENT-A-scan-specific-unsupervised-deep-learning-framework-for-high-resolution-3D-isotropic-fetal-brain-MRI-reconstruction"><a href="#SUFFICIENT-A-scan-specific-unsupervised-deep-learning-framework-for-high-resolution-3D-isotropic-fetal-brain-MRI-reconstruction" class="headerlink" title="SUFFICIENT: A scan-specific unsupervised deep learning framework for   high-resolution 3D isotropic fetal brain MRI reconstruction"></a>SUFFICIENT: A scan-specific unsupervised deep learning framework for   high-resolution 3D isotropic fetal brain MRI reconstruction</h2><p><strong>Authors:Jiangjie Wu, Lixuan Chen, Zhenghao Li, Xin Li, Saban Ozturk, Lihui Wang, Rongpin Wang, Hongjiang Wei, Yuyao Zhang</strong></p>
<p>High-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D slices is crucial for clinical diagnosis. Reliable slice-to-volume registration (SVR)-based motion correction and super-resolution reconstruction (SRR) methods are essential. Deep learning (DL) has demonstrated potential in enhancing SVR and SRR when compared to conventional methods. However, it requires large-scale external training datasets, which are difficult to obtain for clinical fetal MRI. To address this issue, we propose an unsupervised iterative SVR-SRR framework for isotropic HR volume reconstruction. Specifically, SVR is formulated as a function mapping a 2D slice and a 3D target volume to a rigid transformation matrix, which aligns the slice to the underlying location in the target volume. The function is parameterized by a convolutional neural network, which is trained by minimizing the difference between the volume slicing at the predicted position and the input slice. In SRR, a decoding network embedded within a deep image prior framework is incorporated with a comprehensive image degradation model to produce the high-resolution (HR) volume. The deep image prior framework offers a local consistency prior to guide the reconstruction of HR volumes. By performing a forward degradation model, the HR volume is optimized by minimizing loss between predicted slices and the observed slices. Comprehensive experiments conducted on large-magnitude motion-corrupted simulation data and clinical data demonstrate the superior performance of the proposed framework over state-of-the-art fetal brain reconstruction frameworks. </p>
<blockquote>
<p>é«˜è´¨é‡çš„ä¸‰ç»´èƒå„¿è„‘éƒ¨MRIé‡å»ºä»å—è¿åŠ¨å¹²æ‰°çš„äºŒç»´åˆ‡ç‰‡å¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ã€‚å¯é çš„åŸºäºåˆ‡ç‰‡åˆ°ä½“ç§¯æ³¨å†Œï¼ˆSVRï¼‰çš„è¿åŠ¨æ ¡æ­£å’Œè¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰æ–¹æ³•è‡³å…³é‡è¦ã€‚ä¸å¸¸è§„æ–¹æ³•ç›¸æ¯”ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨æé«˜SVRå’ŒSRRæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒéœ€è¦å¤§é‡å¤–éƒ¨è®­ç»ƒæ•°æ®é›†ï¼Œè¿™å¯¹äºä¸´åºŠèƒå„¿MRIæ¥è¯´å¾ˆéš¾è·å¾—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£è¿­ä»£SVR-SRRæ¡†æ¶ï¼Œç”¨äºè¿›è¡Œç­‰åˆ†è¾¨ç‡çš„é«˜åˆ†è¾¨ç‡ä½“ç§¯é‡å»ºã€‚å…·ä½“è€Œè¨€ï¼ŒSVRè¢«åˆ¶å®šä¸ºä¸€ä¸ªå°†äºŒç»´åˆ‡ç‰‡å’Œä¸‰ç»´ç›®æ ‡ä½“ç§¯æ˜ å°„åˆ°åˆšæ€§å˜æ¢çŸ©é˜µçš„å‡½æ•°ï¼Œè¯¥çŸ©é˜µå°†å¯¹é½åˆ‡ç‰‡åˆ°ç›®æ ‡ä½“ç§¯çš„æ½œåœ¨ä½ç½®ã€‚è¯¥å‡½æ•°ç”±å·ç§¯ç¥ç»ç½‘ç»œå‚æ•°åŒ–ï¼Œé€šè¿‡æœ€å°åŒ–é¢„æµ‹ä½ç½®å¤„çš„ä½“ç§¯åˆ‡ç‰‡ä¸è¾“å…¥åˆ‡ç‰‡ä¹‹é—´çš„å·®å¼‚æ¥è®­ç»ƒç½‘ç»œã€‚åœ¨SRRä¸­ï¼ŒåµŒå…¥æ·±åº¦å›¾åƒå…ˆéªŒæ¡†æ¶ä¸­çš„è§£ç ç½‘ç»œç»“åˆå…¨é¢çš„å›¾åƒé€€åŒ–æ¨¡å‹ï¼Œç”Ÿæˆé«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰ä½“ç§¯ã€‚æ·±åº¦å›¾åƒå…ˆéªŒæ¡†æ¶æä¾›äº†ä¸€ä¸ªå±€éƒ¨ä¸€è‡´æ€§å…ˆéªŒæ¥æŒ‡å¯¼HRä½“ç§¯çš„é‡å»ºã€‚é€šè¿‡æ‰§è¡Œæ­£å‘é€€åŒ–æ¨¡å‹ï¼ŒHRä½“ç§¯é€šè¿‡æœ€å°åŒ–é¢„æµ‹åˆ‡ç‰‡ä¸è§‚å¯Ÿåˆ‡ç‰‡ä¹‹é—´çš„æŸå¤±æ¥è¿›è¡Œä¼˜åŒ–ã€‚å¯¹å¤§é‡è¿åŠ¨å¹²æ‰°çš„æ¨¡æ‹Ÿæ•°æ®å’Œä¸´åºŠæ•°æ®è¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä¼˜äºæœ€æ–°çš„èƒå„¿è„‘éƒ¨é‡å»ºæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17472v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ç”¨äºä»å—è¿åŠ¨å¹²æ‰°çš„äºŒç»´åˆ‡ç‰‡é‡å»ºé«˜è´¨é‡ä¸‰ç»´èƒå„¿è„‘éƒ¨MRIå›¾åƒçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— ç›‘ç£è¿­ä»£åˆ‡ç‰‡åˆ°ä½“ç§¯æ³¨å†Œï¼ˆSVRï¼‰å’Œè¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰æ¡†æ¶ï¼Œä»¥å®ç°å„å‘åŒæ€§é«˜åˆ†è¾¨ç‡ä½“ç§¯é‡å»ºã€‚é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨èƒå„¿è„‘éƒ¨MRIé‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡çš„ä¸‰ç»´èƒå„¿è„‘éƒ¨MRIé‡å»ºå¯¹äºä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>å¯é çš„åˆ‡ç‰‡åˆ°ä½“ç§¯æ³¨å†Œï¼ˆSVRï¼‰å’Œè¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰æ–¹æ³•æ˜¯å…³é”®ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å¢å¼ºSVRå’ŒSRRæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†è·å–å¤§è§„æ¨¡å¤–éƒ¨è®­ç»ƒæ•°æ®é›†å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— ç›‘ç£è¿­ä»£SVR-SRRæ¡†æ¶ï¼Œç”¨äºå„å‘åŒæ€§é«˜åˆ†è¾¨ç‡ä½“ç§¯é‡å»ºã€‚</li>
<li>SVRè¢«åˆ¶å®šä¸ºä¸€ä¸ªå‡½æ•°ï¼Œå°†äºŒç»´åˆ‡ç‰‡å’Œä¸‰ç»´ç›®æ ‡ä½“ç§¯æ˜ å°„åˆ°åˆšæ€§å˜æ¢çŸ©é˜µï¼Œè¯¥çŸ©é˜µå°†åˆ‡ç‰‡ä¸åº•å±‚ç›®æ ‡ä½“ç§¯çš„ä½ç½®å¯¹é½ã€‚</li>
<li>ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œå‚æ•°åŒ–è¯¥å‡½æ•°ï¼Œé€šè¿‡æœ€å°åŒ–é¢„æµ‹ä½ç½®ä½“ç§¯åˆ‡ç‰‡ä¸è¾“å…¥åˆ‡ç‰‡ä¹‹é—´çš„å·®å¼‚æ¥è®­ç»ƒç½‘ç»œã€‚</li>
<li>åœ¨å¤§é‡è¿åŠ¨å¹²æ‰°çš„æ¨¡æ‹Ÿæ•°æ®å’Œä¸´åºŠæ•°æ®ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„èƒå„¿è„‘éƒ¨é‡å»ºæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fe2de42fa029dc24a3a4a3bf4d29ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13982e7a76178966034f3834d92333df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d28dcf7697e91771f9835128f836a3dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7842c8263cfefa2e09dd41b9d0b21bfc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bcd004082a13202f9d393fe8b888a6cb.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CT-OT-Flow-Estimating-Continuous-Time-Dynamics-from-Discrete-Temporal-Snapshots"><a href="#CT-OT-Flow-Estimating-Continuous-Time-Dynamics-from-Discrete-Temporal-Snapshots" class="headerlink" title="CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal   Snapshots"></a>CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal   Snapshots</h2><p><strong>Authors:Keisuke Kawano, Takuro Kutsuna, Naoki Hayashi, Yasushi Esaki, Hidenori Tanaka</strong></p>
<p>In many real-world scenarios, such as single-cell RNA sequencing, data are observed only as discrete-time snapshots spanning finite time intervals and subject to noisy timestamps, with no continuous trajectories available. Recovering the underlying continuous-time dynamics from these snapshots with coarse and noisy observation times is a critical and challenging task. We propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers high-resolution time labels via partial optimal transport and then reconstructs a continuous-time data distribution through a temporal kernel smoothing. This reconstruction enables accurate training of dynamics models such as ODEs and SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic benchmarks and achieves lower reconstruction errors on real scRNA-seq and typhoon-track datasets. Our results highlight the benefits of explicitly modeling temporal discretization and timestamp uncertainty, offering an accurate and general framework for bridging discrete snapshots and continuous-time processes. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œä¸­çš„è®¸å¤šåœºæ™¯ä¸­ï¼Œä¾‹å¦‚å•ç»†èƒRNAæµ‹åºï¼Œæ•°æ®ä»…è¢«è§‚å¯Ÿåˆ°ä½œä¸ºè·¨è¶Šæœ‰é™æ—¶é—´é—´éš”çš„ç¦»æ•£æ—¶é—´å¿«ç…§ï¼Œå¹¶å—åˆ°å™ªå£°æ—¶é—´æˆ³çš„å½±å“ï¼Œæ— æ³•è·å¾—è¿ç»­è½¨è¿¹ã€‚ä»å…·æœ‰ç²—ç³™å’Œå™ªå£°è§‚æµ‹æ—¶é—´çš„è¿™äº›å¿«ç…§ä¸­æ¢å¤æ½œåœ¨çš„è¿ç»­æ—¶é—´åŠ¨åŠ›å­¦æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†è¿ç»­æ—¶é—´æœ€ä¼˜ä¼ è¾“æµï¼ˆCT-OT Flowï¼‰ï¼Œå®ƒé¦–å…ˆé€šè¿‡éƒ¨åˆ†æœ€ä¼˜ä¼ è¾“æ¨æ–­é«˜åˆ†è¾¨ç‡æ—¶é—´æ ‡ç­¾ï¼Œç„¶åé€šè¿‡æ—¶é—´æ ¸å¹³æ»‘é‡å»ºè¿ç»­æ—¶é—´æ•°æ®åˆ†å¸ƒã€‚è¿™ç§é‡å»ºä½¿å¾—åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆå¦‚ODEå’ŒSDEï¼‰çš„è®­ç»ƒæ›´åŠ å‡†ç¡®ã€‚CT-OT Flowåœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶åœ¨çœŸå®scRNA-seqå’Œå°é£è½¨è¿¹æ•°æ®é›†ä¸Šå®ç°äº†æ›´ä½çš„é‡å»ºè¯¯å·®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†æ˜¾å¼å»ºæ¨¡æ—¶é—´ç¦»æ•£åŒ–å’Œæ—¶é—´æˆ³ä¸ç¡®å®šæ€§çš„å¥½å¤„ï¼Œæä¾›äº†ä¸€ä¸ªå‡†ç¡®ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºæ¡¥æ¥ç¦»æ•£å¿«ç…§å’Œè¿ç»­æ—¶é—´è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17354v1">PDF</a> 27 pages, 28 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºContinuous-Time Optimal Transport Flowï¼ˆCT-OT Flowï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»ç¦»æ•£æ—¶é—´å¿«ç…§ä¸­æ¢å¤å‡ºåº•å±‚è¿ç»­æ—¶é—´åŠ¨æ€ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡éƒ¨åˆ†æœ€ä¼˜ä¼ è¾“æ¨æ–­é«˜åˆ†è¾¨ç‡æ—¶é—´æ ‡ç­¾ï¼Œç„¶åé€šè¿‡æ—¶é—´æ ¸å¹³æ»‘é‡å»ºè¿ç»­æ—¶é—´æ•°æ®åˆ†å¸ƒã€‚æ­¤æ–¹æ³•åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’Œå®é™…scRNA-seqå’Œå°é£è½¨è¿¹æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œé™ä½äº†é‡å»ºè¯¯å·®ï¼Œå¹¶å¼ºè°ƒæ˜¾å¼å»ºæ¨¡æ—¶é—´ç¦»æ•£åŒ–å’Œæ—¶é—´æˆ³ä¸ç¡®å®šæ€§çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT-OT Flowèƒ½å¤Ÿä»ç¦»æ•£æ—¶é—´å¿«ç…§ä¸­æ¢å¤åº•å±‚è¿ç»­æ—¶é—´åŠ¨æ€ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé€šè¿‡éƒ¨åˆ†æœ€ä¼˜ä¼ è¾“æ¨æ–­é«˜åˆ†è¾¨ç‡æ—¶é—´æ ‡ç­¾ï¼Œä»¥åŠé€šè¿‡æ—¶é—´æ ¸å¹³æ»‘é‡å»ºè¿ç»­æ—¶é—´æ•°æ®åˆ†å¸ƒã€‚</li>
<li>CT-OT Flowåœ¨åˆæˆåŸºå‡†æµ‹è¯•å’Œå®é™…æ•°æ®é›†ï¼ˆå¦‚scRNA-seqå’Œå°é£è½¨è¿¹æ•°æ®ï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCT-OT Flowé™ä½äº†é‡å»ºè¯¯å·®ã€‚</li>
<li>æ˜¾å¼å»ºæ¨¡æ—¶é—´ç¦»æ•£åŒ–å’Œæ—¶é—´æˆ³ä¸ç¡®å®šæ€§å¯¹æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>CT-OT Flowä¸ºè¿æ¥ç¦»æ•£å¿«ç…§å’Œè¿ç»­æ—¶é—´è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªå‡†ç¡®ä¸”é€šç”¨çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02aa0ac31532ae891ec30c28859e92e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d671262c4f082471c8feb0dc11bf2646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf79107086c63f376075d3b269c0101f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec727533ecddcd70d31edc7afc4621a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis"><a href="#Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis" class="headerlink" title="Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis"></a>Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis</h2><p><strong>Authors:Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab</strong></p>
<p>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon. </p>
<blockquote>
<p>å¯¹äºå—è§„å¾‹å‘¼å¸å½±å“è€Œå‘ç”Ÿçš„åŠ¨æ€å˜åŒ–çš„å»ºæ¨¡å¯¹äºå›¾åƒå¯¼å‘çš„ä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶åºè¿åŠ¨ï¼Œé™¤éåŒæ—¶å­˜åœ¨åŒ…æ‹¬èµ·å§‹å¸§å’Œç»“æŸå¸§åœ¨å†…çš„é«˜å‰‚é‡æˆåƒæ‰«æã€‚ç„¶è€Œï¼Œåœ¨æœ¯å‰æ•°æ®é‡‡é›†é˜¶æ®µï¼Œæ‚£è€…çš„è½»å¾®ç§»åŠ¨å¯èƒ½å¯¼è‡´å‘¼å¸å‘¨æœŸå†…ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§ä¹‹é—´çš„èƒŒæ™¯åŠ¨æ€å˜åŒ–ã€‚è¿™ç§é¢å¤–çš„åå·®å‡ ä¹æ— æ³•é€šè¿‡å›¾åƒæ³¨å†Œæ¥æ¶ˆé™¤ï¼Œå› æ­¤å½±å“äº†æ—¶åºå»ºæ¨¡ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–åˆ›æ€§åœ°é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿè§„å¾‹çš„è¿åŠ¨è¿‡ç¨‹ï¼Œè¯¥æ¡†æ¶ä»¥ç¬¬ä¸€å¸§åŠ¨ç”»é¢„æµ‹ç»™å®šé•¿åº¦çš„æœªæ¥å¸§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶åºä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶åºå·®åˆ†æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆæ—¶åºå·®åˆ†åœºï¼Œè¯¥åœºæµ‹é‡ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸å¯¹å·®åˆ†è¡¨ç¤ºã€‚è®¾è®¡äº†å³æ—¶æ³¨æ„å±‚è¿›è¡Œç²¾ç»†çš„å·®åˆ†åœºå¤„ç†ï¼Œå¹¶é‡‡ç”¨åœºå¢å¼ºå±‚ä»¥æ›´å¥½åœ°å°†è¿™äº›å­—æ®µä¸I2Væ¡†æ¶è¿›è¡Œäº¤äº’ï¼Œä»è€Œä¿ƒè¿›åˆæˆè§†é¢‘çš„æ—¶åºå˜åŒ–æ›´åŠ å‡†ç¡®ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†çš„å¤§é‡ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¨¡æ‹Ÿäº†æ²¿å†…åœ¨è¿åŠ¨è½¨è¿¹çš„4Dè§†é¢‘ï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶åºä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–ç«äº‰æ–¹æ³•ç›¸å½“ã€‚ä»£ç å°†å¾ˆå¿«æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17333v1">PDF</a> early accepted by MICCAI</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡å¼ºè°ƒæ—¶é—´å»ºæ¨¡åœ¨å¸¸è§„å‘¼å¸è¯±å¯¼è¿åŠ¨ä¸­çš„å›¾åƒå¼•å¯¼ä¸´åºŠåº”ç”¨çš„é‡è¦æ€§ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶é—´è¿åŠ¨ï¼Œé™¤éå­˜åœ¨åŒæ—¶åŒ…å«èµ·å§‹å’Œç»“æŸå¸§çš„é«˜å‰‚é‡æˆåƒæ‰«æã€‚é’ˆå¯¹æœ¯å‰æ•°æ®é‡‡é›†é˜¶æ®µæ‚£è€…è½»å¾®ç§»åŠ¨å¯¼è‡´çš„åŠ¨æ€èƒŒæ™¯é—®é¢˜ï¼Œè¯¥æ–‡é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹ï¼Œé€šè¿‡ç¬¬ä¸€å¸§é¢„æµ‹æœªæ¥ç»™å®šé•¿åº¦çš„å¸§ã€‚ä¸ºæé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¼•å…¥æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆæ—¶é—´å·®åˆ†åœºï¼Œå¹¶åˆ©ç”¨å³æ—¶æ³¨æ„åŠ›å±‚è¿›è¡Œç²¾ç»†å·®åˆ†åœºå¤„ç†ï¼Œé€šè¿‡åœºå¢å¼ºå±‚ä¼˜åŒ–è¿™äº›åœºä¸I2Væ¡†æ¶çš„äº’åŠ¨ï¼Œæé«˜åˆæˆè§†é¢‘çš„æ—¶ç©ºå˜åŒ–å‡†ç¡®æ€§ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¨¡æ‹Ÿçš„4Dè§†é¢‘æ²¿ç€å†…åœ¨è¿åŠ¨è½¨è¿¹è¿›è¡Œï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´å»ºæ¨¡åœ¨å›¾åƒå¼•å¯¼çš„ä¸´åºŠåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡æ‹Ÿå¸¸è§„å‘¼å¸è¯±å¯¼è¿åŠ¨æ—¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å—é™äºå¿…é¡»åŒæ—¶æ‹¥æœ‰èµ·å§‹å’Œç»“æŸå¸§çš„é«˜å‰‚é‡æˆåƒæ‰«ææ‰èƒ½è¿›è¡Œæ—¶é—´è¿åŠ¨æ¨¡æ‹Ÿã€‚</li>
<li>æœ¯å‰æ•°æ®é‡‡é›†é˜¶æ®µæ‚£è€…è½»å¾®ç§»åŠ¨å¯èƒ½å¯¼è‡´åŠ¨æ€èƒŒæ™¯ï¼Œå½±å“æ—¶é—´å»ºæ¨¡ã€‚</li>
<li>æå‡ºå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶ï¼Œé€šè¿‡ç¬¬ä¸€å¸§é¢„æµ‹æœªæ¥å¸§ä»¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>ä¸ºæé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œå¼•å…¥æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹å’Œå³æ—¶æ³¨æ„åŠ›å±‚ã€‚</li>
<li>æå‡ºåœºå¢å¼ºå±‚æ¥ä¼˜åŒ–I2Væ¡†æ¶ä¸æ—¶ç©ºåœºçš„äº’åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9ac025c4b8106fc2b23e8bc3ceb1a2e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92c140bceaf0b20e54bd27b61db0bfd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8c3f6b48b9c0afefe10e9c897b446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d31f99851506e854b240ce2d7720b9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-95eddf18366a0471250fb1383abb3fb9.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Reward Model Generalization for Compute-Aware Test-Time Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2e986bc27d4003c899dff64145f2c5c2.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  RestoreVAR Visual Autoregressive Generation for All-in-One Image   Restoration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
