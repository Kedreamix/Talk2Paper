<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-27  RestoreVAR Visual Autoregressive Generation for All-in-One Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2e986bc27d4003c899dff64145f2c5c2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-27-更新"><a href="#2025-05-27-更新" class="headerlink" title="2025-05-27 更新"></a>2025-05-27 更新</h1><h2 id="RestoreVAR-Visual-Autoregressive-Generation-for-All-in-One-Image-Restoration"><a href="#RestoreVAR-Visual-Autoregressive-Generation-for-All-in-One-Image-Restoration" class="headerlink" title="RestoreVAR: Visual Autoregressive Generation for All-in-One Image   Restoration"></a>RestoreVAR: Visual Autoregressive Generation for All-in-One Image   Restoration</h2><p><strong>Authors:Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel</strong></p>
<p>The use of latent diffusion models (LDMs) such as Stable Diffusion has significantly improved the perceptual quality of All-in-One image Restoration (AiOR) methods, while also enhancing their generalization capabilities. However, these LDM-based frameworks suffer from slow inference due to their iterative denoising process, rendering them impractical for time-sensitive applications. To address this, we propose RestoreVAR, a novel generative approach for AiOR that significantly outperforms LDM-based models in restoration performance while achieving over $\mathbf{10\times}$ faster inference. RestoreVAR leverages visual autoregressive modeling (VAR), a recently introduced approach which performs scale-space autoregression for image generation. VAR achieves comparable performance to that of state-of-the-art diffusion transformers with drastically reduced computational costs. To optimally exploit these advantages of VAR for AiOR, we propose architectural modifications and improvements, including intricately designed cross-attention mechanisms and a latent-space refinement module, tailored for the AiOR task. Extensive experiments show that RestoreVAR achieves state-of-the-art performance among generative AiOR methods, while also exhibiting strong generalization capabilities. </p>
<blockquote>
<p>使用如Stable Diffusion之类的潜在扩散模型（LDM）已经显著提高了全合一图像恢复（AiOR）方法的感知质量，并增强了其泛化能力。然而，这些基于LDM的框架由于迭代去噪过程而导致推理速度较慢，对于时间敏感的应用程序来说并不实用。为了解决这一问题，我们提出了RestoreVAR，这是一种新的用于AiOR的生成方法，在恢复性能上显著优于基于LDM的模型，同时实现了超过10倍的更快推理速度。RestoreVAR利用视觉自回归建模（VAR），这是一种最近推出的方法，用于执行尺度空间自回归以生成图像。VAR以大幅降低的计算成本实现了与国家最先进的扩散变压器相当的性能。为了充分利用VAR在AiOR中的这些优势，我们提出了架构修改和改进，包括精心设计用于AiOR任务的交叉注意机制和潜在空间细化模块。大量实验表明，RestoreVAR在生成AiOR方法中实现了最先进的性能，同时表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18047v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://sudraj2002.github.io/restorevarpage/">https://sudraj2002.github.io/restorevarpage/</a></p>
<p><strong>Summary</strong></p>
<p>基于潜在扩散模型（LDM）如Stable Diffusion的应用已显著提高了一站式图像恢复（AiOR）方法的感知质量及其泛化能力。然而，这些LDM框架由于迭代去噪过程而导致推理速度较慢，不适合时间敏感型应用。为解决这一问题，我们提出了RestoreVAR，这是一种新型的AiOR生成方法，在恢复性能方面显著优于LDM模型，同时实现超过10倍加速推理。RestoreVAR利用视觉自回归建模（VAR），这是一种最近推出的图像生成尺度空间自回归方法，以较低的计算成本实现了与国家先进扩散变压器相当的性能。为了充分利用VAR在AiOR任务中的优势，我们提出了结构修改和改进，包括精心设计交叉注意机制和针对AiOR任务的潜在空间细化模块。大量实验表明，RestoreVAR在生成AiOR方法中达到最新技术水平，同时表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDM模型如Stable Diffusion提升了AiOR方法的感知质量和泛化能力。</li>
<li>LDM框架存在推理速度慢的问题，不适合时间敏感型应用。</li>
<li>RestoreVAR是一种新型的AiOR生成方法，显著优于LDM模型，实现超过10倍加速推理。</li>
<li>RestoreVAR利用视觉自回归建模（VAR），以较低的计算成本实现了与国家先进扩散变压器相当的性能。</li>
<li>为充分利用VAR在AiOR中的优势，进行了结构修改和改进，包括交叉注意机制和潜在空间细化模块的设计。</li>
<li>RestoreVAR在生成AiOR方法中表现优秀，达到最新技术水平。</li>
<li>RestoreVAR展现出强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18047">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-203e9d8fc5492a7eba6410c9fbb66588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4d1c4d8bf677c300bbf8b6c6b479c16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45ad78d2364c2e3b5280e518ad0198fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90c8f5f8ac3f183e7f7aee77de302694.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Segment-Anyword-Mask-Prompt-Inversion-for-Open-Set-Grounded-Segmentation"><a href="#Segment-Anyword-Mask-Prompt-Inversion-for-Open-Set-Grounded-Segmentation" class="headerlink" title="Segment Anyword: Mask Prompt Inversion for Open-Set Grounded   Segmentation"></a>Segment Anyword: Mask Prompt Inversion for Open-Set Grounded   Segmentation</h2><p><strong>Authors:Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe, Philip Teare, Huiyu Zhou, Chen Jin</strong></p>
<p>Open-set image segmentation poses a significant challenge because existing methods often demand extensive training or fine-tuning and generally struggle to segment unified objects consistently across diverse text reference expressions. Motivated by this, we propose Segment Anyword, a novel training-free visual concept prompt learning approach for open-set language grounded segmentation that relies on token-level cross-attention maps from a frozen diffusion model to produce segmentation surrogates or mask prompts, which are then refined into targeted object masks. Initial prompts typically lack coherence and consistency as the complexity of the image-text increases, resulting in suboptimal mask fragments. To tackle this issue, we further introduce a novel linguistic-guided visual prompt regularization that binds and clusters visual prompts based on sentence dependency and syntactic structural information, enabling the extraction of robust, noise-tolerant mask prompts, and significant improvements in segmentation accuracy. The proposed approach is effective, generalizes across different open-set segmentation tasks, and achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative to fine-tuned methods) mIoU on GranDf, which is the most complex open-set grounded segmentation task in the field. </p>
<blockquote>
<p>开放集图像分割构成了一个重大挑战，因为现有方法通常需要大量的训练或微调，并且在处理跨不同文本参考表达的一致对象分割时通常遇到困难。为此，我们提出了Segment Anyword，这是一种新型的无训练视觉概念提示学习方法，用于基于开放集语言的分割。该方法依赖于冻结的扩散模型的令牌级跨注意力图来生成分割代理或掩膜提示，然后将其细化为目标对象掩膜。随着图像文本复杂性的增加，初始提示通常缺乏连贯性和一致性，导致掩膜片段不佳。为了解决这个问题，我们进一步引入了一种新型的语言引导的视觉提示正则化方法，该方法根据句子依赖关系和句法结构信息来绑定和聚类视觉提示，从而提取出稳健、抗干扰的掩膜提示，并显著提高分割精度。所提出的方法效果显著，可应用于不同的开放集分割任务，并在Pascal Context 59上达到了52.5（+6.8相对）mIoU，在gRefCOCO上达到了67.73（+25.73相对）cIoU，以及在领域中最复杂的开放集基于语言的分割任务GranDf上达到了67.4（+相对于微调方法的1.1）mIoU。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种无需训练的新型视觉概念提示学习方法——Segment Anyword，用于开放式语言基础分割。该方法利用冻结的扩散模型的令牌级跨注意力图生成分割替代者或掩膜提示，并进行细化以形成目标对象掩膜。为解决初始提示缺乏连贯性和一致性的问题，引入了一种新型的语言引导视觉提示正则化方法，根据句子依赖和句法结构信息对视觉提示进行绑定和聚类，提高了掩膜提示的鲁棒性和噪声耐受性，以及分割精度。该方法有效、通用，并在多个开放式分割任务上达到最佳结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anyword是一种新型训练免费的视觉概念提示学习方法，用于开放式语言基础分割。</li>
<li>该方法利用扩散模型的令牌级跨注意力图生成分割替代者或掩膜提示。</li>
<li>初始提示缺乏连贯性和一致性，因此引入语言引导的视觉提示正则化。</li>
<li>该方法根据句子依赖和句法结构信息对视觉提示进行绑定和聚类。</li>
<li>该方法提高了掩膜提示的鲁棒性和噪声耐受性。</li>
<li>Segment Anyword方法在多个开放式分割任务上达到最佳结果，包括Pascal Context 59、gRefCOCO和GranDf。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17994">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6045f637ee038f28ad102947ca2db8c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-674b8a7ea8688c3efd55a35c183ec17e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb466ac21516f34460e8046a6b024aa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7fa52599f33945e8fd5fcfa53c2403c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-065d6fc8bb36357a7fc385ecb70f6d6f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply"><a href="#Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply" class="headerlink" title="Diffusion Classifiers Understand Compositionality, but Conditions Apply"></a>Diffusion Classifiers Understand Compositionality, but Conditions Apply</h2><p><strong>Authors:Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach</strong></p>
<p>Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality</a>. </p>
<blockquote>
<p>理解视觉场景对人类智能至关重要。虽然判别模型在计算机视觉领域取得了显著进展，但在组合理解方面常常遇到困难。相比之下，最近的文本到图像的生成扩散模型在合成复杂场景方面表现出色，这表明其具有内在的组成能力。基于此，零样本扩散分类器被提出用于将扩散模型重新用于判别任务。虽然先前的工作在判别组合场景方面提供了有前景的结果，但由于基准测试的数量有限以及对模型成功的条件分析相对肤浅，这些结果仍然是初步的。为了解决这一问题，我们对扩散分类器在广泛组合任务上的判别能力进行了综合研究。具体来说，我们的研究涵盖了三个扩散模型（SD 1.5、2.0和首次推出的3-m），涉及10个数据集和30多个任务。此外，我们阐明了目标数据集领域对各自性能的作用；为了隔离领域效应，我们推出了一个新的诊断基准Self-Bench，由扩散模型本身创建的图像组成。最后，我们探讨了时间步长权重的重要性，并揭示了领域差距与时间步长敏感性之间的关系，特别是针对SD3-m。总之，扩散分类器能够理解组合性，但条件适用！相关代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17955v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>视觉场景的理解对人类智能至关重要。虽然判别模型在计算机视觉领域取得了显著进展，但它们通常在组合理解方面遇到困难。相比之下，最新的文本到图像的生成扩散模型在合成复杂场景方面表现出色，表明其固有的组合能力。基于此，零样本扩散分类器被提出来将扩散模型用于判别任务。虽然先前的工作在判别组合场景中提供了有前景的结果，但由于基准测试的数量相对较少，以及对模型成功条件的分析相对肤浅，这些结果仍是初步的。为了解决这一问题，我们对扩散分类器在广泛组合任务上的判别能力进行了深入研究。具体地，我们的研究涵盖了三个扩散模型（SD 1.5、2.0和首次推出的3-m），涉及10个数据集和超过30个任务。此外，我们阐明了目标数据集领域在各自性能中的作用；为了隔离领域效应，我们推出了一个新的诊断基准Self-Bench，由扩散模型本身创建的图像组成。最后，我们探讨了时间步长权重的重要性，并揭示了领域差距与时间步长敏感性之间的关系，特别是针对SD3-m。总之，扩散分类器理解组合性，但条件适用！相关代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality找到。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>扩散模型在计算机视觉领域具有出色的合成复杂场景能力，表现出其固有的组合特性。</li>
<li>零样本扩散分类器被提出用于判别任务，这是基于扩散模型的新的应用方向。</li>
<li>当前研究在判别组合场景上的结果仍是初步的，主要由于基准测试数量少和对模型成功条件的分析不够深入。</li>
<li>本文对扩散分类器的判别能力进行了全面研究，覆盖了多个扩散模型和任务。</li>
<li>深入探讨了目标数据集领域对模型性能的影响，并推出了新的诊断基准Self-Bench。</li>
<li>研究了时间步长权重的重要性，并揭示了领域差距与时间步长敏感性之间的关系。</li>
<li>扩散分类器虽然理解组合性，但应用条件需考虑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1b19fa7fa3b9721299a4b1229b5aa8e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f33c3751b813f53f289a8c1793ac3006.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8392f17a0d70d4f317caefa9c8f12c83.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models"><a href="#T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models" class="headerlink" title="T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models"></a>T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models</h2><p><strong>Authors:Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li</strong></p>
<p>Recent advances in text-to-video (T2V) diffusion models have significantly enhanced the quality of generated videos. However, their ability to produce explicit or harmful content raises concerns about misuse and potential rights violations. Inspired by the success of unlearning techniques in erasing undesirable concepts from text-to-image (T2I) models, we extend unlearning to T2V models and propose a robust and precise unlearning method. Specifically, we adopt negatively-guided velocity prediction fine-tuning and enhance it with prompt augmentation to ensure robustness against LLM-refined prompts. To achieve precise unlearning, we incorporate a localization and a preservation regularization to preserve the model’s ability to generate non-target concepts. Extensive experiments demonstrate that our method effectively erases a specific concept while preserving the model’s generation capability for all other concepts, outperforming existing methods. We provide the unlearned models in \href{<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%7D%7Bhttps://github.com/VDIGPKU/T2VUnlearning.git%7D">https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}</a>. </p>
<blockquote>
<p>近期文本到视频（T2V）扩散模型的进步极大地提高了生成视频的质量。然而，它们产生明确或有害内容的能力引发了关于误用和潜在权利侵犯的担忧。受文本到图像（T2I）模型中消除不良概念的去学习技术成功的启发，我们将去学习扩展到T2V模型，并提出了一种稳健且精确的去学习方法。具体来说，我们采用负导向速度预测微调，并通过提示增强来确保其对大型语言模型优化提示的稳健性。为了实现精确的去学习，我们结合了定位和保存正则化，以保留模型生成非目标概念的能力。大量实验表明，我们的方法在消除特定概念的同时，保留了模型对所有其他概念的生成能力，优于现有方法。我们提供去学习的模型在<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%E3%80%82">https://github.com/VDIGPKU/T2VUnlearning.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17550v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了近期文本到视频（T2V）扩散模型的进展，这些模型生成视频的质量得到了显著提升。然而，它们产生明确或有害内容的能力引发了关于误用和潜在权利侵犯的担忧。受文本到图像（T2I）模型中的去遗忘技术成功的启发，研究团队将去遗忘技术扩展到T2V模型，并提出了一种稳健且精确的去遗忘方法。该方法采用负导向速度预测微调，并通过提示增强技术来提高对大型语言模型优化提示的稳健性。为实现精确去遗忘，研究团队引入了定位和保护正则化来保留模型生成非目标概念的能力。实验证明，该方法在消除特定概念的同时，保留了模型对其他所有概念的生成能力，优于现有方法。研究团队提供了遗忘模型在GitHub上的链接：<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到视频（T2V）扩散模型的最新进展显著提高了生成视频的质量。</li>
<li>T2V模型产生有害内容的能力引发了对误用和潜在权利侵犯的担忧。</li>
<li>受文本到图像（T2I）模型去遗忘技术成功的启发，研究团队将其扩展到T2V模型。</li>
<li>提出了一种新的稳健且精确的去遗忘方法，包括负导向速度预测微调、提示增强、定位和保护正则化等技术。</li>
<li>实验证明该方法在消除特定概念的同时，能够保留模型对其他概念的生成能力。</li>
<li>与现有方法相比，该方法表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17550">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-79957d67b30ca16b1de52071a3de9847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d9a9db4dfd0d331c8779326b3ac5ef8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70b8426958699e1949b39812cae4a40f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffd9caa870bcf2e41323ba58fba76f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70de0b98b44af06b9fd700e8a7cba8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86cdc1ec073f06b8e441d5ab9df478d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Variational-Autoencoding-Discrete-Diffusion-with-Enhanced-Dimensional-Correlations-Modeling"><a href="#Variational-Autoencoding-Discrete-Diffusion-with-Enhanced-Dimensional-Correlations-Modeling" class="headerlink" title="Variational Autoencoding Discrete Diffusion with Enhanced Dimensional   Correlations Modeling"></a>Variational Autoencoding Discrete Diffusion with Enhanced Dimensional   Correlations Modeling</h2><p><strong>Authors:Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang</strong></p>
<p>Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines. </p>
<blockquote>
<p>离散扩散模型最近在模拟复杂离散数据方面显示出巨大潜力，其中遮罩扩散模型（MDM）在质量和生成速度之间提供了令人信服的权衡。MDM通过从完全遮罩的输入中逐步去噪多个维度来实现去噪，但由于对维度间依赖性的建模有限，当使用较少的去噪步骤时，其性能可能会下降。在本文中，我们提出了变分自编码离散扩散（VADD）这一新型框架，它通过潜在变量建模增强离散扩散，以隐含地捕获维度之间的相关性。通过引入辅助识别模型，VADD能够通过变分下界最大化和训练集上的摊销推断来实现稳定训练。我们的方法保留了传统MDM的效率，同时显著提高了样本质量，尤其是当去噪步骤数量较少时。在二维玩具数据、像素级图像生成和文本生成方面的经验结果表明，VADD始终优于MDM基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17384v1">PDF</a> 23 pages, 14 figures</p>
<p><strong>Summary</strong><br>     离散扩散模型在处理复杂离散数据时展现出巨大潜力，其中掩模扩散模型（MDMs）在质量和生成速度之间达到了令人信服的平衡。然而，由于缺少对维度间依赖关系的建模，当使用较少的去噪步骤时，其性能可能会下降。本文提出一种新型框架——变分自编码离散扩散（VADD），通过引入潜在变量建模来捕获维度之间的相关性。此外，通过引入辅助识别模型，VADD能够通过变分下界最大化实现稳定训练，并在训练集上进行摊销推断。该方法保留了传统MDMs的效率，同时显著提高了样本质量，特别是在去噪步骤较少时。实验结果表明，无论是在二维玩具数据、像素级图像生成还是文本生成上，VADD均优于MDMs基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>离散扩散模型在建模复杂离散数据方面展现出巨大潜力。</li>
<li>掩模扩散模型（MDMs）在质量和生成速度之间取得了平衡。</li>
<li>MDMs在较少去噪步骤时性能可能下降，因为缺乏对维度间依赖关系的建模。</li>
<li>变分自编码离散扩散（VADD）框架通过引入潜在变量建模来提高离散扩散的性能。</li>
<li>VADD通过引入辅助识别模型实现了稳定训练，同时通过变分下界最大化进行了摊销推断。</li>
<li>VADD在保留MDMs效率的同时，显著提高了样本质量，特别是在去噪步骤较少的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17384">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69692ca4b032e4ba3b456b8d1fb07ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b25017ef4f6aee9043b18d70ba115d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b2bfd2fe5dd362efe351c9985e93568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92f5dd8818fba3aa6e716895d86f929e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Repurposing-Marigold-for-Zero-Shot-Metric-Depth-Estimation-via-Defocus-Blur-Cues"><a href="#Repurposing-Marigold-for-Zero-Shot-Metric-Depth-Estimation-via-Defocus-Blur-Cues" class="headerlink" title="Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus   Blur Cues"></a>Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus   Blur Cues</h2><p><strong>Authors:Chinmay Talegaonkar, Nikhil Gandudi Suresh, Zachary Novack, Yash Belhe, Priyanka Nagasamudra, Nicholas Antipa</strong></p>
<p>Recent monocular metric depth estimation (MMDE) methods have made notable progress towards zero-shot generalization. However, they still exhibit a significant performance drop on out-of-distribution datasets. We address this limitation by injecting defocus blur cues at inference time into Marigold, a \textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular depth estimation (MDE). Our method effectively turns Marigold into a metric depth predictor in a training-free manner. To incorporate defocus cues, we capture two images with a small and a large aperture from the same viewpoint. To recover metric depth, we then optimize the metric depth scaling parameters and the noise latents of Marigold at inference time using gradients from a loss function based on the defocus-blur image formation model. We compare our method against existing state-of-the-art zero-shot MMDE methods on a self-collected real dataset, showing quantitative and qualitative improvements. </p>
<blockquote>
<p>近期单眼度量深度估计（MMDE）方法在实现零样本泛化方面取得了显著进展。然而，它们在非分布数据集上仍然表现出显著的性能下降。我们通过推理时向Marigold注入失焦模糊线索来解决这一局限性，Marigold是一个用于零样本、尺度不变的单眼深度估计（MDE）的预训练扩散模型。我们的方法有效地以无训练的方式将Marigold转变为度量深度预测器。为了融入失焦线索，我们从同一视角捕获小孔和大孔的两张图像。然后，我们使用基于失焦模糊图像形成模型的损失函数的梯度，在推理时间优化度量深度缩放参数和Marigold的噪声潜在变量以恢复度量深度。我们在自我收集的真实数据集上将我们的方法与现有的最先进的零样本MMDE方法进行比较，从定量和定性两个方面展示了改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期单眼度量深度估计（MMDE）方法在零样本泛化方面取得了显著进展，但在跨分布数据集上性能下降。为解决这一问题，我们通过在推理阶段将散焦模糊线索注入到预先训练的扩散模型Marigold中，实现了零样本、尺度不变的单眼深度估计（MDE）。我们的方法以无需训练的方式使Marigold成为度量深度预测器。为融入散焦线索，我们从同一视角捕获小孔和大孔两张图像，然后使用基于散焦模糊图像形成模型的损失函数在推理阶段优化度量深度缩放参数和Marigold的噪声潜在变量。在自收集的真实数据集上，我们的方法与现有最先进的零样本MMDE方法相比，显示出定量和定性的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMDE方法在零样本泛化方面取得进展，但在跨分布数据集上存在性能下降的问题。</li>
<li>提出一种将散焦模糊线索注入到预先训练的扩散模型Marigold中的方法，以提高深度估计性能。</li>
<li>通过优化度量深度缩放参数和噪声潜在变量，实现了零样本、尺度不变的单眼深度估计。</li>
<li>利用小孔和大孔图像来捕捉散焦线索，以提高深度估计的准确性。</li>
<li>采用基于散焦模糊图像形成模型的损失函数进行优化。</li>
<li>在自收集的真实数据集上进行了实验验证，与现有方法相比取得了定量和定性的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-65570258b1503370d9bfe0d698baad5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c97261da66e2b5d53cec8f5e98c1c62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55b20c39e89eaeee69047f456ba0f4a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-daab8831d6d3080050030272f7de34ae.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Ascent-Diffusion-for-Inverse-Problems"><a href="#Dual-Ascent-Diffusion-for-Inverse-Problems" class="headerlink" title="Dual Ascent Diffusion for Inverse Problems"></a>Dual Ascent Diffusion for Inverse Problems</h2><p><strong>Authors:Minseo Kim, Axel Levy, Gordon Wetzstein</strong></p>
<p>Ill-posed inverse problems are fundamental in many domains, ranging from astrophysics to medical imaging. Emerging diffusion models provide a powerful prior for solving these problems. Existing maximum-a-posteriori (MAP) or posterior sampling approaches, however, rely on different computational approximations, leading to inaccurate or suboptimal samples. To address this issue, we introduce a new approach to solving MAP problems with diffusion model priors using a dual ascent optimization framework. Our framework achieves better image quality as measured by various metrics for image restoration problems, it is more robust to high levels of measurement noise, it is faster, and it estimates solutions that represent the observations more faithfully than the state of the art. </p>
<blockquote>
<p>不适定反问题在许多领域都有广泛应用，从天文物理到医学影像。新兴的扩散模型为解决这些问题提供了强大的先验。然而，现有的最大后验概率（MAP）或后采样方法依赖于不同的计算近似方法，导致样本不准确或次优。为了解决这个问题，我们引入了一种使用双上升优化框架解决带有扩散模型先验的MAP问题的新方法。我们的框架在图像恢复问题上以多种度量标准衡量图像质量更好，对高水平的测量噪声更具鲁棒性，速度更快，并且估计的解决方案比现有技术更真实地代表观察结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17353v1">PDF</a> 23 pages, 15 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型在解决不适定反问题中的应用。通过引入一种新的基于双重上升优化框架的MAP问题解决策略，新方法能够在图像修复等问题中提高图像质量，更好地应对高水平的测量噪声，估计出更为精准的解决方案。此外，该策略的速度也更快，能更好地还原观测结果。 </p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ad09a75ad31368a6d029e57a8fad3f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb729f5f7d6b8bdc70136f47da3bbf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e986bc27d4003c899dff64145f2c5c2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis"><a href="#Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis" class="headerlink" title="Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis"></a>Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis</h2><p><strong>Authors:Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab</strong></p>
<p>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon. </p>
<blockquote>
<p>针对规律性的呼吸引起的运动进行时间建模，对图像引导的临床应用至关重要。现有方法无法模拟时间运动，除非同时存在包括起始帧和结束帧在内的高剂量成像扫描。然而，在术前数据采集阶段，患者轻微的移动可能会导致一个呼吸周期内第一帧和最后一帧之间的动态背景。这种额外的偏差几乎无法通过图像注册去除，从而影响时间建模。为了解决这个问题，我们首创性地通过图像到视频（I2V）合成框架模拟常规运动过程，该框架利用第一帧来预测给定长度的未来帧。此外，为了提升动画视频的时间一致性，我们设计了时间差分扩散模型，以生成时间差分场，它测量相邻帧之间的相对差分表示。设计了即时注意层用于精细的差分场，并采用场增强层来更好地将这些字段与I2V框架进行交互，从而促进合成视频的更加准确的时间变化。在ACDC心脏和4D肺部数据集的大量结果表明，我们的方法能够沿着内在运动轨迹模拟4D视频，在感知相似性和时间一致性方面与其他竞争方法相比具有竞争力。代码将很快可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17333v1">PDF</a> early accepted by MICCAI</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种通过图像到视频（I2V）合成框架模拟常规运动过程的方法，利用首帧预测未来指定长度的帧。为解决动态背景对时间建模的影响，引入时间差分扩散模型生成时间差分场，并设计了即时注意力层进行精细差分场的处理。在ACDC心脏和4D肺部数据集上的大量结果表明，该方法模拟的4D视频沿内在运动轨迹进行，感知相似性和时间一致性方面与其他方法相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>常规呼吸引起的运动在图像引导的临床应用中的时间建模是关键。</li>
<li>现有方法无法模拟时间运动，除非同时存在高剂量成像扫描的起始和结束帧。</li>
<li>患者轻微移动可能导致动态背景，影响时间建模。</li>
<li>提出了一种图像到视频（I2V）合成框架，通过首帧预测未来指定长度的帧来模拟常规运动过程。</li>
<li>引入时间差分扩散模型生成时间差分场，提高动画视频的时间一致性。</li>
<li>设计了即时注意力层来处理精细的差分场。</li>
<li>在ACDC心脏和4D肺部数据集上的实验结果表明，该方法在感知相似性和时间一致性方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17333">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ac025c4b8106fc2b23e8bc3ceb1a2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c140bceaf0b20e54bd27b61db0bfd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8c3f6b48b9c0afefe10e9c897b446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d31f99851506e854b240ce2d7720b9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Revisiting-Adversarial-Perception-Attacks-and-Defense-Methods-on-Autonomous-Driving-Systems"><a href="#Revisiting-Adversarial-Perception-Attacks-and-Defense-Methods-on-Autonomous-Driving-Systems" class="headerlink" title="Revisiting Adversarial Perception Attacks and Defense Methods on   Autonomous Driving Systems"></a>Revisiting Adversarial Perception Attacks and Defense Methods on   Autonomous Driving Systems</h2><p><strong>Authors:Cheng Chen, Yuhong Wang, Nafis S Munir, Xiangwei Zhou, Xugui Zhou</strong></p>
<p>Autonomous driving systems (ADS) increasingly rely on deep learning-based perception models, which remain vulnerable to adversarial attacks. In this paper, we revisit adversarial attacks and defense methods, focusing on road sign recognition and lead object detection and prediction (e.g., relative distance). Using a Level-2 production ADS, OpenPilot by Comma$.$ai, and the widely adopted YOLO model, we systematically examine the impact of adversarial perturbations and assess defense techniques, including adversarial training, image processing, contrastive learning, and diffusion models. Our experiments highlight both the strengths and limitations of these methods in mitigating complex attacks. Through targeted evaluations of model robustness, we aim to provide deeper insights into the vulnerabilities of ADS perception systems and contribute guidance for developing more resilient defense strategies. </p>
<blockquote>
<p>自动驾驶系统（ADS）越来越依赖于基于深度学习的感知模型，而这些模型仍然容易受到对抗性攻击的威胁。本文重新审视了对抗性攻击和防御方法，重点关注道路标志识别以及前方物体的检测和预测（例如相对距离）。我们利用Comma.ai的Level-2生产型ADS和广泛采用的YOLO模型，系统地研究了对抗性扰动的影响，并评估了防御技术，包括对抗性训练、图像处理、对比学习和扩散模型。我们的实验突出了这些方法在缓解复杂攻击方面的优势和局限性。通过对模型稳健性的有针对性的评估，我们旨在深入了解ADS感知系统的漏洞，并为开发更具韧性的防御策略提供指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11532v2">PDF</a> 8 pages, 2 figures, To appear in the 8th Dependable and Secure   Machine Learning Workshop (DSML 2025)</p>
<p><strong>Summary</strong></p>
<p>本文重点探讨了自主驾驶系统（ADS）中的对抗性攻击与防御方法，特别是针对道路标志识别以及前方目标检测和预测。研究使用了Comma.ai的OpenPilot水平2生产型ADS系统和广泛采用的YOLO模型，系统评估了对抗性扰动的影响，并测试了包括对抗性训练、图像处理、对比学习和扩散模型在内的防御技术。实验揭示了这些方法在应对复杂攻击时的优缺点，为深入了解ADS感知系统的脆弱性并为开发更稳健的防御策略提供了指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主驾驶系统（ADS）依赖深度学习感知模型，但易受对抗性攻击影响。</li>
<li>研究重点考察了道路标志识别及前方目标检测和预测中的对抗性攻击。</li>
<li>使用了OpenPilot和YOLO模型进行实验研究。</li>
<li>系统评估了对抗性扰动对模型的影响。</li>
<li>测试了多种防御方法，包括对抗性训练、图像处理、对比学习和扩散模型。</li>
<li>实验揭示了这些方法在应对复杂攻击时的优缺点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11532">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1af0033e1c54b6737c9b2911407548f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23249189506ef6045863491af3d0a89f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b206a60dca123dbd7290124669495ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3b372bad7bcf248bc0ffbc0a7d6137e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8df69946f809ba084a12baa35a7d9936.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8c1ec10e5c9e8f9da501d547b7bc3bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-559890da81bdc10559992c5b46613d3c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models"><a href="#QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models" class="headerlink" title="QVGen: Pushing the Limit of Quantized Video Generative Models"></a>QVGen: Pushing the Limit of Quantized Video Generative Models</h2><p><strong>Authors:Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang</strong></p>
<p>Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\Phi$, we propose a rank-decay strategy that progressively eliminates $\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\mathbf{\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. </p>
<blockquote>
<p>视频扩散模型（DMs）已经能够实现高质量的视频合成。然而，其巨大的计算和内存需求对现实世界的应用部署，即使在高端GPU上，也构成了严重的挑战。作为一种常用的解决方案，量化在降低图像DM的成本方面已经取得了显著的成效，而将其直接应用于视频DM仍然无效。在本文中，我们介绍了QVGen，这是一个专为极端低比特量化（例如4位及以下）下高性能和推理效率高的视频DM定制的量化感知训练（QAT）框架。我们从理论分析开始，证明降低梯度范数是促进QAT收敛的关键。为此，我们引入了辅助模块（Φ）来减轻大量的量化误差，从而显著增强了收敛性。为了消除Φ的推理开销，我们提出了一种排名衰减策略，该策略逐步消除Φ。具体来说，我们反复使用奇异值分解（SVD）和提出的基于排名的正则化γ来识别和衰减低贡献成分。此策略在保持性能的同时消除了推理开销。在4种最先进的视频DMs上的广泛实验，参数大小从1.3B到14B，表明QVGen首次在4位设置下达到全精度可比质量。而且，它显著优于现有方法。例如，我们的3位CogVideoX-2B在VBench上的动态度提高了+25.28，场景一致性提高了+8.43。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11497v2">PDF</a> Our code will be released upon acceptance</p>
<p><strong>摘要</strong><br>    视频扩散模型（DMs）可实现高质量视频合成，但其巨大的计算和内存需求对现实世界的部署构成了严峻挑战，即使在高端GPU上也是如此。量化作为一种常用的解决方案，在降低图像DM的成本方面取得了显著的成效，而直接应用于视频DM则效果不佳。本文提出了QVGen，这是一个针对高性能和推理效率的视频DM量身定制的量化感知训练（QAT）框架，适用于极低比特量化（例如4位及以下）。本文首先进行理论分析，证明降低梯度范数是促进QAT收敛的关键。为此，我们引入了辅助模块（Φ）来缓解量化误差，从而显著提高了收敛性。为了消除Φ的推理开销，我们提出了一种等级衰减策略，该策略逐步消除了Φ。具体来说，我们反复使用奇异值分解（SVD）和提出的基于等级的规则γ来识别和衰减低贡献成分。此策略在保持性能的同时消除了推理开销。在参数规模从1.3B到14B的4种最先进视频DM上进行的广泛实验表明，QVGen在四比特设置下首次达到全精度可比质量，并显著优于现有方法。例如，我们的3位CogVideoX-2B在VBench上的动态度和场景一致性分别提高了+25.28和+8.43。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频扩散模型（DMs）可实现高质量视频合成，但部署挑战在于其高计算和内存需求。</li>
<li>量化作为一种降低图像DM成本的解决方案已经取得了成功，但直接应用于视频DM效果不佳。</li>
<li>QVGen是一个针对视频DM的量化感知训练（QAT）框架，旨在提高高性能和推理效率，适用于极低比特量化。</li>
<li>QVGen通过引入辅助模块和等级衰减策略来提高QAT的收敛性和性能。</li>
<li>辅助模块用于缓解量化误差，而等级衰减策略逐步消除推理开销。</li>
<li>QVGen在多种视频DM上实现了全精度可比质量，特别是在4位设置下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-679eba285d145eba6a48e4b7d3593de0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-264f2b954bbe36dc7229dad9723084b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fabb07ff35e2398ec63e3da43ec7ac01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2c6181fceebd1604e509856bac9a02d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era"><a href="#AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era" class="headerlink" title="AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era"></a>AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era</h2><p><strong>Authors:Chenyang Zhu, Xing Zhang, Yuyang Sun, Ching-Chun Chang, Isao Echizen</strong></p>
<p>Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored-despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods. Dataset and code can be found in <a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/">https://flytweety.github.io/AnimeDL2M/</a>. </p>
<blockquote>
<p>近期图像生成领域的进展，特别是扩散模型，大大降低了创建复杂伪造作品的门槛，使得图像操纵检测与定位（IMDL）越来越具有挑战性。尽管先前在IMDL方面的工作主要集中在自然图像上，但动漫领域仍然鲜有研究——尽管它越来越容易受到AI生成的伪造作品的威胁。将AI生成的图像误表示为手绘艺术品、版权侵犯以及不当内容修改对动漫社区和行业构成严重威胁。为了弥补这一空白，我们提出了AnimeDL-2M，这是首个用于动漫IMDL的大规模基准测试，包含全面的注释。它包含超过两百万张图像，包括真实、部分操纵和完全AI生成的样本。实验表明，在自然图像IMDL数据集上训练的模型在应用于动漫图像时表现不佳，这凸显了动漫和自然图像之间的明显领域差距。为了更好地处理动漫领域的IMDL任务，我们进一步提出了AniXplore，这是一个针对动漫图像视觉特征量身定制的新型模型。广泛评估表明，与现有方法相比，AniXplore实现了卓越的性能。数据集和代码可在<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/%E6%89%BE%E5%88%B0%E3%80%82">https://flytweety.github.io/AnimeDL2M/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11015v2">PDF</a> 8+2 pages; update figure 3,4,5 as adding real images into detection   task tests</p>
<p><strong>Summary</strong><br>     最新进展的图像生成技术，特别是扩散模型，大大降低了创建复杂伪造作品的门槛，使得图像操纵检测与定位（IMDL）越来越具挑战性。尽管IMDL先前的工作主要集中在自然图像上，但动漫领域仍然被忽视，尽管该领域面临AI生成伪造作品的日益增长的脆弱性。我们提出了AnimeDL-2M，这是首个大规模动漫IMDL基准测试集，带有全面注释，包含超过两百万张图像，包括真实、部分操纵和完全AI生成的样本。实验表明，在动漫图像上应用训练有素的自然图像IMDL数据集表现不佳，突显出动漫和自然图像之间的明显领域差距。为了更好地处理动漫领域的IMDL任务，我们进一步提出了AniXplore模型，该模型针对动漫图像的视觉特性量身定制。广泛的评估表明，与现有方法相比，AniXplore实现了卓越的性能。数据集和代码可在<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/%E6%89%BE%E5%88%B0%E3%80%82">https://flytweety.github.io/AnimeDL2M/找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的发展使得图像生成变得更加容易，从而增加了创建复杂伪造作品的难度。</li>
<li>图像操纵检测与定位（IMDL）面临新的挑战，因为动漫领域对AI生成伪造作品的脆弱性日益增加。</li>
<li>当前IMDL数据集主要集中在自然图像上，而动漫领域的数据集仍然缺乏。</li>
<li>动漫图像与自然图像之间存在明显的领域差距。</li>
<li>训练有素的自然图像IMDL数据集在动漫图像上的表现不佳。</li>
<li>提出了首个针对动漫IMDL的大规模基准测试集AnimeDL-2M，包含全面注释和超过两百万张图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11015">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca78bb4620abf9141ca3f5f947fbc5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8992976fc1aa15fc7f78e4d85c26d0d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cac0dc7205cf7fdb6c41056585ab09d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70c499791398dcc15996dee8835fa376.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29f8d14632ade71f6b529587479d3426.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="REG-Rectified-Gradient-Guidance-for-Conditional-Diffusion-Models"><a href="#REG-Rectified-Gradient-Guidance-for-Conditional-Diffusion-Models" class="headerlink" title="REG: Rectified Gradient Guidance for Conditional Diffusion Models"></a>REG: Rectified Gradient Guidance for Conditional Diffusion Models</h2><p><strong>Authors:Zhengqi Gao, Kaiwen Zha, Tianyuan Zhang, Zihui Xue, Duane S. Boning</strong></p>
<p>Guidance techniques are simple yet effective for improving conditional generation in diffusion models. Albeit their empirical success, the practical implementation of guidance diverges significantly from its theoretical motivation. In this paper, we reconcile this discrepancy by replacing the scaled marginal distribution target, which we prove theoretically invalid, with a valid scaled joint distribution objective. Additionally, we show that the established guidance implementations are approximations to the intractable optimal solution under no future foresight constraint. Building on these theoretical insights, we propose rectified gradient guidance (REG), a versatile enhancement designed to boost the performance of existing guidance methods. Experiments on 1D and 2D demonstrate that REG provides a better approximation to the optimal solution than prior guidance techniques, validating the proposed theoretical framework. Extensive experiments on class-conditional ImageNet and text-to-image generation tasks show that incorporating REG consistently improves FID and Inception&#x2F;CLIP scores across various settings compared to its absence. </p>
<blockquote>
<p>指导技术在扩散模型中用于改善条件生成简单而有效。尽管它们在经验上取得了成功，但指导的实际实施与其理论动机存在很大差异。在本文中，我们通过用有效的缩放联合分布目标替换理论上无效的可缩放边际分布目标来解决这一差异。此外，我们证明现有的指导实现是在没有未来预测约束下不可行最优解的近似解。基于这些理论见解，我们提出了修正梯度指导（REG），这是一种通用增强设计，旨在提高现有指导方法的性能。在一维和二维实验表明，相对于之前的指导技术，REG更好地接近最优解，验证了所提出理论框架的正确性。在类条件ImageNet和文本到图像生成任务的广泛实验表明，在各种设置中融入REG与缺少REG相比，FID和Inception&#x2F;CLIP分数均有所提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18865v2">PDF</a> 20 pages, 10 figures; accepted by ICML’25</p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型中指导技术的改进对条件生成的影响。针对现有指导技术实施与理论动机的偏差，本文进行了调和。通过替换理论上无效的比例边际分布目标，采用有效的比例联合分布目标，并证明现有指导实现是在无未来预见约束下对最优解的近似。在此基础上，本文提出了矫正梯度指导（REG）这一通用增强技术，旨在提升现有指导方法的性能。实验证明，REG相较于先前的指导技术，在逼近最优解方面表现更佳。在类条件ImageNet和文本到图像生成任务上的实验显示，引入REG在不同设置下始终提高了FID和Inception&#x2F;CLIP得分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型中的指导技术对于改进条件生成至关重要。</li>
<li>现有指导技术的实施与理论动机存在偏差。</li>
<li>通过采用有效的比例联合分布目标替换理论上无效的比例边际分布目标，解决了这一偏差。</li>
<li>现有指导实现是对于无未来预见约束下最优解的近似。</li>
<li>提出矫正梯度指导（REG）技术，旨在增强现有指导方法的性能。</li>
<li>REG在逼近最优解方面表现优于先前的指导技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18865">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-afb6e5ed2aa148c94ce745d29a0161d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f521233b5e091defc2f39267dc42eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72a7ecbeacfbfb8fba43fec4d7fec69b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints"><a href="#Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints" class="headerlink" title="Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout   Constraints"></a>Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout   Constraints</h2><p><strong>Authors:Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, Ping Tan</strong></p>
<p>Text-driven 3D indoor scene generation is useful for gaming, the film industry, and AR&#x2F;VR applications. However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room. To address these problems, we present Ctrl-Room, which can generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. Our key insight is to separate the modeling of layouts and appearance. Our proposed method consists of two stages: a Layout Generation Stage and an Appearance Generation Stage. The Layout Generation Stage trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the Appearance Generation Stage employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt. We thus achieve a high-quality 3D room generation with convincing layouts and lively textures. Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive edit-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts. </p>
<blockquote>
<p>文本驱动的3D室内场景生成在游戏、电影产业和AR&#x2F;VR应用中非常有用。然而，现有方法无法真实地捕捉房间布局，也不允许对房间中的单个物体进行灵活的编辑。为了解决这些问题，我们提出了Ctrl-Room，它仅通过文本提示就能生成具有设计师风格的布局和高保真纹理的令人信服的3D房间。此外，Ctrl-Room还启用了多样化的交互编辑操作，如调整大小或移动单个家具物品。我们的关键见解是分离布局和外观的建模。我们提出的方法分为两个阶段：布局生成阶段和外观生成阶段。布局生成阶段训练了一个文本条件扩散模型，以学习我们的整体场景代码参数化的布局分布。接下来，外观生成阶段采用经过微调的控制网（ControlNet）生成受3D场景布局和文本提示引导的房间生动全景图像。因此，我们实现了具有令人信服的布局和生动纹理的高质量3D房间生成。得益于场景代码参数化，我们可以轻松通过我们的遮罩引导编辑模块编辑生成的房间模型，而无需进行昂贵的特定编辑训练。在Structured3D数据集上的大量实验表明，我们的方法在根据自然语言提示生成更合理、视角一致且可编辑的3D房间方面优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03602v4">PDF</a> </p>
<p><strong>Summary</strong><br>基于文本驱动的3D室内场景生成技术为游戏、影视、AR&#x2F;VR等领域带来了便捷。现有方法难以真实还原房间布局，也无法灵活编辑室内物件。本研究提出Ctrl-Room技术，仅通过文本提示即可生成具有设计感布局和高保真纹理的3D房间，并实现个体家具的缩放和移动等交互式编辑操作。核心在于布局与外观的建模分离，包括布局生成阶段和外观生成阶段。在Structured3D数据集上的实验表明，Ctrl-Room技术较现有方法更优，能生成更合理、视角一致、可编辑的3D房间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ctrl-Room技术通过文本驱动生成3D室内场景，应用于游戏、影视、AR&#x2F;VR领域。</li>
<li>现有方法难以真实还原房间布局，且缺乏灵活的编辑功能。</li>
<li>Ctrl-Room技术分为布局生成阶段和外观生成阶段，实现布局与外观的建模分离。</li>
<li>采用全景图像生成技术，根据3D场景布局和文本提示生成生动逼真的纹理。</li>
<li>通过场景代码参数化，实现轻松编辑生成的房间模型。</li>
<li>Ctrl-Room技术通过mask-guided编辑模块，无需特定培训即可进行编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.03602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b5e9ffaafb07b45806fbf68a14163425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f1f61286a98a1e543ce3053791ef05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20a8eccfcbdaff38bb22347d9d3ff2f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6d081705047798a5d50c3cfe66d7092.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-529ca47a3574172e74472100559dd62c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b7fa52599f33945e8fd5fcfa53c2403c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-27  Effect of Fluorine doping on the electrocatalytic properties of Nb2O5   for H2O2 electrogeneration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d1506782cf16b887a34a02ca40f435b5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-27  CGS-GAN 3D Consistent Gaussian Splatting GANs for High Resolution Human   Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
