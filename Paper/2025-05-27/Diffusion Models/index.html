<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  RestoreVAR Visual Autoregressive Generation for All-in-One Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2e986bc27d4003c899dff64145f2c5c2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-31
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-27-æ›´æ–°"><a href="#2025-05-27-æ›´æ–°" class="headerlink" title="2025-05-27 æ›´æ–°"></a>2025-05-27 æ›´æ–°</h1><h2 id="RestoreVAR-Visual-Autoregressive-Generation-for-All-in-One-Image-Restoration"><a href="#RestoreVAR-Visual-Autoregressive-Generation-for-All-in-One-Image-Restoration" class="headerlink" title="RestoreVAR: Visual Autoregressive Generation for All-in-One Image   Restoration"></a>RestoreVAR: Visual Autoregressive Generation for All-in-One Image   Restoration</h2><p><strong>Authors:Sudarshan Rajagopalan, Kartik Narayan, Vishal M. Patel</strong></p>
<p>The use of latent diffusion models (LDMs) such as Stable Diffusion has significantly improved the perceptual quality of All-in-One image Restoration (AiOR) methods, while also enhancing their generalization capabilities. However, these LDM-based frameworks suffer from slow inference due to their iterative denoising process, rendering them impractical for time-sensitive applications. To address this, we propose RestoreVAR, a novel generative approach for AiOR that significantly outperforms LDM-based models in restoration performance while achieving over $\mathbf{10\times}$ faster inference. RestoreVAR leverages visual autoregressive modeling (VAR), a recently introduced approach which performs scale-space autoregression for image generation. VAR achieves comparable performance to that of state-of-the-art diffusion transformers with drastically reduced computational costs. To optimally exploit these advantages of VAR for AiOR, we propose architectural modifications and improvements, including intricately designed cross-attention mechanisms and a latent-space refinement module, tailored for the AiOR task. Extensive experiments show that RestoreVAR achieves state-of-the-art performance among generative AiOR methods, while also exhibiting strong generalization capabilities. </p>
<blockquote>
<p>ä½¿ç”¨å¦‚Stable Diffusionä¹‹ç±»çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å·²ç»æ˜¾è‘—æé«˜äº†å…¨åˆä¸€å›¾åƒæ¢å¤ï¼ˆAiORï¼‰æ–¹æ³•çš„æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶å¢å¼ºäº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºLDMçš„æ¡†æ¶ç”±äºè¿­ä»£å»å™ªè¿‡ç¨‹è€Œå¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯¹äºæ—¶é—´æ•æ„Ÿçš„åº”ç”¨ç¨‹åºæ¥è¯´å¹¶ä¸å®ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RestoreVARï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç”¨äºAiORçš„ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨æ¢å¤æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºäºLDMçš„æ¨¡å‹ï¼ŒåŒæ—¶å®ç°äº†è¶…è¿‡10å€çš„æ›´å¿«æ¨ç†é€Ÿåº¦ã€‚RestoreVARåˆ©ç”¨è§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼ˆVARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ€è¿‘æ¨å‡ºçš„æ–¹æ³•ï¼Œç”¨äºæ‰§è¡Œå°ºåº¦ç©ºé—´è‡ªå›å½’ä»¥ç”Ÿæˆå›¾åƒã€‚VARä»¥å¤§å¹…é™ä½çš„è®¡ç®—æˆæœ¬å®ç°äº†ä¸å›½å®¶æœ€å…ˆè¿›çš„æ‰©æ•£å˜å‹å™¨ç›¸å½“çš„æ€§èƒ½ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨VARåœ¨AiORä¸­çš„è¿™äº›ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†æ¶æ„ä¿®æ”¹å’Œæ”¹è¿›ï¼ŒåŒ…æ‹¬ç²¾å¿ƒè®¾è®¡ç”¨äºAiORä»»åŠ¡çš„äº¤å‰æ³¨æ„æœºåˆ¶å’Œæ½œåœ¨ç©ºé—´ç»†åŒ–æ¨¡å—ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRestoreVARåœ¨ç”ŸæˆAiORæ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18047v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://sudraj2002.github.io/restorevarpage/">https://sudraj2002.github.io/restorevarpage/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¦‚Stable Diffusionçš„åº”ç”¨å·²æ˜¾è‘—æé«˜äº†ä¸€ç«™å¼å›¾åƒæ¢å¤ï¼ˆAiORï¼‰æ–¹æ³•çš„æ„ŸçŸ¥è´¨é‡åŠå…¶æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›LDMæ¡†æ¶ç”±äºè¿­ä»£å»å™ªè¿‡ç¨‹è€Œå¯¼è‡´æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œä¸é€‚åˆæ—¶é—´æ•æ„Ÿå‹åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RestoreVARï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„AiORç”Ÿæˆæ–¹æ³•ï¼Œåœ¨æ¢å¤æ€§èƒ½æ–¹é¢æ˜¾è‘—ä¼˜äºLDMæ¨¡å‹ï¼ŒåŒæ—¶å®ç°è¶…è¿‡10å€åŠ é€Ÿæ¨ç†ã€‚RestoreVARåˆ©ç”¨è§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼ˆVARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ€è¿‘æ¨å‡ºçš„å›¾åƒç”Ÿæˆå°ºåº¦ç©ºé—´è‡ªå›å½’æ–¹æ³•ï¼Œä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°äº†ä¸å›½å®¶å…ˆè¿›æ‰©æ•£å˜å‹å™¨ç›¸å½“çš„æ€§èƒ½ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨VARåœ¨AiORä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„ä¿®æ”¹å’Œæ”¹è¿›ï¼ŒåŒ…æ‹¬ç²¾å¿ƒè®¾è®¡äº¤å‰æ³¨æ„æœºåˆ¶å’Œé’ˆå¯¹AiORä»»åŠ¡çš„æ½œåœ¨ç©ºé—´ç»†åŒ–æ¨¡å—ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRestoreVARåœ¨ç”ŸæˆAiORæ–¹æ³•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMæ¨¡å‹å¦‚Stable Diffusionæå‡äº†AiORæ–¹æ³•çš„æ„ŸçŸ¥è´¨é‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LDMæ¡†æ¶å­˜åœ¨æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ï¼Œä¸é€‚åˆæ—¶é—´æ•æ„Ÿå‹åº”ç”¨ã€‚</li>
<li>RestoreVARæ˜¯ä¸€ç§æ–°å‹çš„AiORç”Ÿæˆæ–¹æ³•ï¼Œæ˜¾è‘—ä¼˜äºLDMæ¨¡å‹ï¼Œå®ç°è¶…è¿‡10å€åŠ é€Ÿæ¨ç†ã€‚</li>
<li>RestoreVARåˆ©ç”¨è§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼ˆVARï¼‰ï¼Œä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°äº†ä¸å›½å®¶å…ˆè¿›æ‰©æ•£å˜å‹å™¨ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>ä¸ºå……åˆ†åˆ©ç”¨VARåœ¨AiORä¸­çš„ä¼˜åŠ¿ï¼Œè¿›è¡Œäº†ç»“æ„ä¿®æ”¹å’Œæ”¹è¿›ï¼ŒåŒ…æ‹¬äº¤å‰æ³¨æ„æœºåˆ¶å’Œæ½œåœ¨ç©ºé—´ç»†åŒ–æ¨¡å—çš„è®¾è®¡ã€‚</li>
<li>RestoreVARåœ¨ç”ŸæˆAiORæ–¹æ³•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>RestoreVARå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-203e9d8fc5492a7eba6410c9fbb66588.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4d1c4d8bf677c300bbf8b6c6b479c16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45ad78d2364c2e3b5280e518ad0198fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90c8f5f8ac3f183e7f7aee77de302694.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Segment-Anyword-Mask-Prompt-Inversion-for-Open-Set-Grounded-Segmentation"><a href="#Segment-Anyword-Mask-Prompt-Inversion-for-Open-Set-Grounded-Segmentation" class="headerlink" title="Segment Anyword: Mask Prompt Inversion for Open-Set Grounded   Segmentation"></a>Segment Anyword: Mask Prompt Inversion for Open-Set Grounded   Segmentation</h2><p><strong>Authors:Zhihua Liu, Amrutha Saseendran, Lei Tong, Xilin He, Fariba Yousefi, Nikolay Burlutskiy, Dino Oglic, Tom Diethe, Philip Teare, Huiyu Zhou, Chen Jin</strong></p>
<p>Open-set image segmentation poses a significant challenge because existing methods often demand extensive training or fine-tuning and generally struggle to segment unified objects consistently across diverse text reference expressions. Motivated by this, we propose Segment Anyword, a novel training-free visual concept prompt learning approach for open-set language grounded segmentation that relies on token-level cross-attention maps from a frozen diffusion model to produce segmentation surrogates or mask prompts, which are then refined into targeted object masks. Initial prompts typically lack coherence and consistency as the complexity of the image-text increases, resulting in suboptimal mask fragments. To tackle this issue, we further introduce a novel linguistic-guided visual prompt regularization that binds and clusters visual prompts based on sentence dependency and syntactic structural information, enabling the extraction of robust, noise-tolerant mask prompts, and significant improvements in segmentation accuracy. The proposed approach is effective, generalizes across different open-set segmentation tasks, and achieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal Context 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative to fine-tuned methods) mIoU on GranDf, which is the most complex open-set grounded segmentation task in the field. </p>
<blockquote>
<p>å¼€æ”¾é›†å›¾åƒåˆ†å‰²æ„æˆäº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œå¹¶ä¸”åœ¨å¤„ç†è·¨ä¸åŒæ–‡æœ¬å‚è€ƒè¡¨è¾¾çš„ä¸€è‡´å¯¹è±¡åˆ†å‰²æ—¶é€šå¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Segment Anywordï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒè§†è§‰æ¦‚å¿µæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåŸºäºå¼€æ”¾é›†è¯­è¨€çš„åˆ†å‰²ã€‚è¯¥æ–¹æ³•ä¾èµ–äºå†»ç»“çš„æ‰©æ•£æ¨¡å‹çš„ä»¤ç‰Œçº§è·¨æ³¨æ„åŠ›å›¾æ¥ç”Ÿæˆåˆ†å‰²ä»£ç†æˆ–æ©è†œæç¤ºï¼Œç„¶åå°†å…¶ç»†åŒ–ä¸ºç›®æ ‡å¯¹è±¡æ©è†œã€‚éšç€å›¾åƒæ–‡æœ¬å¤æ‚æ€§çš„å¢åŠ ï¼Œåˆå§‹æç¤ºé€šå¸¸ç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œå¯¼è‡´æ©è†œç‰‡æ®µä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¯­è¨€å¼•å¯¼çš„è§†è§‰æç¤ºæ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®å¥å­ä¾èµ–å…³ç³»å’Œå¥æ³•ç»“æ„ä¿¡æ¯æ¥ç»‘å®šå’Œèšç±»è§†è§‰æç¤ºï¼Œä»è€Œæå–å‡ºç¨³å¥ã€æŠ—å¹²æ‰°çš„æ©è†œæç¤ºï¼Œå¹¶æ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ•ˆæœæ˜¾è‘—ï¼Œå¯åº”ç”¨äºä¸åŒçš„å¼€æ”¾é›†åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶åœ¨Pascal Context 59ä¸Šè¾¾åˆ°äº†52.5ï¼ˆ+6.8ç›¸å¯¹ï¼‰mIoUï¼Œåœ¨gRefCOCOä¸Šè¾¾åˆ°äº†67.73ï¼ˆ+25.73ç›¸å¯¹ï¼‰cIoUï¼Œä»¥åŠåœ¨é¢†åŸŸä¸­æœ€å¤æ‚çš„å¼€æ”¾é›†åŸºäºè¯­è¨€çš„åˆ†å‰²ä»»åŠ¡GranDfä¸Šè¾¾åˆ°äº†67.4ï¼ˆ+ç›¸å¯¹äºå¾®è°ƒæ–¹æ³•çš„1.1ï¼‰mIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹è§†è§‰æ¦‚å¿µæç¤ºå­¦ä¹ æ–¹æ³•â€”â€”Segment Anywordï¼Œç”¨äºå¼€æ”¾å¼è¯­è¨€åŸºç¡€åˆ†å‰²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å†»ç»“çš„æ‰©æ•£æ¨¡å‹çš„ä»¤ç‰Œçº§è·¨æ³¨æ„åŠ›å›¾ç”Ÿæˆåˆ†å‰²æ›¿ä»£è€…æˆ–æ©è†œæç¤ºï¼Œå¹¶è¿›è¡Œç»†åŒ–ä»¥å½¢æˆç›®æ ‡å¯¹è±¡æ©è†œã€‚ä¸ºè§£å†³åˆå§‹æç¤ºç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§çš„é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¯­è¨€å¼•å¯¼è§†è§‰æç¤ºæ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ ¹æ®å¥å­ä¾èµ–å’Œå¥æ³•ç»“æ„ä¿¡æ¯å¯¹è§†è§‰æç¤ºè¿›è¡Œç»‘å®šå’Œèšç±»ï¼Œæé«˜äº†æ©è†œæç¤ºçš„é²æ£’æ€§å’Œå™ªå£°è€å—æ€§ï¼Œä»¥åŠåˆ†å‰²ç²¾åº¦ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆã€é€šç”¨ï¼Œå¹¶åœ¨å¤šä¸ªå¼€æ”¾å¼åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Segment Anywordæ˜¯ä¸€ç§æ–°å‹è®­ç»ƒå…è´¹çš„è§†è§‰æ¦‚å¿µæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¼€æ”¾å¼è¯­è¨€åŸºç¡€åˆ†å‰²ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä»¤ç‰Œçº§è·¨æ³¨æ„åŠ›å›¾ç”Ÿæˆåˆ†å‰²æ›¿ä»£è€…æˆ–æ©è†œæç¤ºã€‚</li>
<li>åˆå§‹æç¤ºç¼ºä¹è¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œå› æ­¤å¼•å…¥è¯­è¨€å¼•å¯¼çš„è§†è§‰æç¤ºæ­£åˆ™åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•æ ¹æ®å¥å­ä¾èµ–å’Œå¥æ³•ç»“æ„ä¿¡æ¯å¯¹è§†è§‰æç¤ºè¿›è¡Œç»‘å®šå’Œèšç±»ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ©è†œæç¤ºçš„é²æ£’æ€§å’Œå™ªå£°è€å—æ€§ã€‚</li>
<li>Segment Anywordæ–¹æ³•åœ¨å¤šä¸ªå¼€æ”¾å¼åˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä½³ç»“æœï¼ŒåŒ…æ‹¬Pascal Context 59ã€gRefCOCOå’ŒGranDfã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6045f637ee038f28ad102947ca2db8c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-674b8a7ea8688c3efd55a35c183ec17e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb466ac21516f34460e8046a6b024aa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7fa52599f33945e8fd5fcfa53c2403c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-065d6fc8bb36357a7fc385ecb70f6d6f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply"><a href="#Diffusion-Classifiers-Understand-Compositionality-but-Conditions-Apply" class="headerlink" title="Diffusion Classifiers Understand Compositionality, but Conditions Apply"></a>Diffusion Classifiers Understand Compositionality, but Conditions Apply</h2><p><strong>Authors:Yujin Jeong, Arnas Uselis, Seong Joon Oh, Anna Rohrbach</strong></p>
<p>Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality">https://github.com/eugene6923/Diffusion-Classifiers-Compositionality</a>. </p>
<blockquote>
<p>ç†è§£è§†è§‰åœºæ™¯å¯¹äººç±»æ™ºèƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶åˆ¤åˆ«æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»„åˆç†è§£æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™è¡¨æ˜å…¶å…·æœ‰å†…åœ¨çš„ç»„æˆèƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œé›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨è¢«æå‡ºç”¨äºå°†æ‰©æ•£æ¨¡å‹é‡æ–°ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œåœ¨åˆ¤åˆ«ç»„åˆåœºæ™¯æ–¹é¢æä¾›äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç”±äºåŸºå‡†æµ‹è¯•çš„æ•°é‡æœ‰é™ä»¥åŠå¯¹æ¨¡å‹æˆåŠŸçš„æ¡ä»¶åˆ†æç›¸å¯¹è‚¤æµ…ï¼Œè¿™äº›ç»“æœä»ç„¶æ˜¯åˆæ­¥çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ‰©æ•£åˆ†ç±»å™¨åœ¨å¹¿æ³›ç»„åˆä»»åŠ¡ä¸Šçš„åˆ¤åˆ«èƒ½åŠ›è¿›è¡Œäº†ç»¼åˆç ”ç©¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†ä¸‰ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆSD 1.5ã€2.0å’Œé¦–æ¬¡æ¨å‡ºçš„3-mï¼‰ï¼Œæ¶‰åŠ10ä¸ªæ•°æ®é›†å’Œ30å¤šä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é˜æ˜äº†ç›®æ ‡æ•°æ®é›†é¢†åŸŸå¯¹å„è‡ªæ€§èƒ½çš„ä½œç”¨ï¼›ä¸ºäº†éš”ç¦»é¢†åŸŸæ•ˆåº”ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„è¯Šæ–­åŸºå‡†Self-Benchï¼Œç”±æ‰©æ•£æ¨¡å‹æœ¬èº«åˆ›å»ºçš„å›¾åƒç»„æˆã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§ä¹‹é—´çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹SD3-mã€‚æ€»ä¹‹ï¼Œæ‰©æ•£åˆ†ç±»å™¨èƒ½å¤Ÿç†è§£ç»„åˆæ€§ï¼Œä½†æ¡ä»¶é€‚ç”¨ï¼ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/eugene6923/Diffusion-Classifiers-Compositionalityè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17955v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰åœºæ™¯çš„ç†è§£å¯¹äººç±»æ™ºèƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶åˆ¤åˆ«æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨ç»„åˆç†è§£æ–¹é¢é‡åˆ°å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€æ–°çš„æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹åœ¨åˆæˆå¤æ‚åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¡¨æ˜å…¶å›ºæœ‰çš„ç»„åˆèƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œé›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨è¢«æå‡ºæ¥å°†æ‰©æ•£æ¨¡å‹ç”¨äºåˆ¤åˆ«ä»»åŠ¡ã€‚è™½ç„¶å…ˆå‰çš„å·¥ä½œåœ¨åˆ¤åˆ«ç»„åˆåœºæ™¯ä¸­æä¾›äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ç”±äºåŸºå‡†æµ‹è¯•çš„æ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼Œä»¥åŠå¯¹æ¨¡å‹æˆåŠŸæ¡ä»¶çš„åˆ†æç›¸å¯¹è‚¤æµ…ï¼Œè¿™äº›ç»“æœä»æ˜¯åˆæ­¥çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ‰©æ•£åˆ†ç±»å™¨åœ¨å¹¿æ³›ç»„åˆä»»åŠ¡ä¸Šçš„åˆ¤åˆ«èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¶µç›–äº†ä¸‰ä¸ªæ‰©æ•£æ¨¡å‹ï¼ˆSD 1.5ã€2.0å’Œé¦–æ¬¡æ¨å‡ºçš„3-mï¼‰ï¼Œæ¶‰åŠ10ä¸ªæ•°æ®é›†å’Œè¶…è¿‡30ä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é˜æ˜äº†ç›®æ ‡æ•°æ®é›†é¢†åŸŸåœ¨å„è‡ªæ€§èƒ½ä¸­çš„ä½œç”¨ï¼›ä¸ºäº†éš”ç¦»é¢†åŸŸæ•ˆåº”ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„è¯Šæ–­åŸºå‡†Self-Benchï¼Œç”±æ‰©æ•£æ¨¡å‹æœ¬èº«åˆ›å»ºçš„å›¾åƒç»„æˆã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§ä¹‹é—´çš„å…³ç³»ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹SD3-mã€‚æ€»ä¹‹ï¼Œæ‰©æ•£åˆ†ç±»å™¨ç†è§£ç»„åˆæ€§ï¼Œä½†æ¡ä»¶é€‚ç”¨ï¼ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/eugene6923/Diffusion-Classifiers-Compositionality%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/eugene6923/Diffusion-Classifiers-Compositionalityæ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå…·æœ‰å‡ºè‰²çš„åˆæˆå¤æ‚åœºæ™¯èƒ½åŠ›ï¼Œè¡¨ç°å‡ºå…¶å›ºæœ‰çš„ç»„åˆç‰¹æ€§ã€‚</li>
<li>é›¶æ ·æœ¬æ‰©æ•£åˆ†ç±»å™¨è¢«æå‡ºç”¨äºåˆ¤åˆ«ä»»åŠ¡ï¼Œè¿™æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°çš„åº”ç”¨æ–¹å‘ã€‚</li>
<li>å½“å‰ç ”ç©¶åœ¨åˆ¤åˆ«ç»„åˆåœºæ™¯ä¸Šçš„ç»“æœä»æ˜¯åˆæ­¥çš„ï¼Œä¸»è¦ç”±äºåŸºå‡†æµ‹è¯•æ•°é‡å°‘å’Œå¯¹æ¨¡å‹æˆåŠŸæ¡ä»¶çš„åˆ†æä¸å¤Ÿæ·±å…¥ã€‚</li>
<li>æœ¬æ–‡å¯¹æ‰©æ•£åˆ†ç±»å™¨çš„åˆ¤åˆ«èƒ½åŠ›è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œè¦†ç›–äº†å¤šä¸ªæ‰©æ•£æ¨¡å‹å’Œä»»åŠ¡ã€‚</li>
<li>æ·±å…¥æ¢è®¨äº†ç›®æ ‡æ•°æ®é›†é¢†åŸŸå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ¨å‡ºäº†æ–°çš„è¯Šæ–­åŸºå‡†Self-Benchã€‚</li>
<li>ç ”ç©¶äº†æ—¶é—´æ­¥é•¿æƒé‡çš„é‡è¦æ€§ï¼Œå¹¶æ­ç¤ºäº†é¢†åŸŸå·®è·ä¸æ—¶é—´æ­¥é•¿æ•æ„Ÿæ€§ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨è™½ç„¶ç†è§£ç»„åˆæ€§ï¼Œä½†åº”ç”¨æ¡ä»¶éœ€è€ƒè™‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b19fa7fa3b9721299a4b1229b5aa8e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f33c3751b813f53f289a8c1793ac3006.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8392f17a0d70d4f317caefa9c8f12c83.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models"><a href="#T2VUnlearning-A-Concept-Erasing-Method-for-Text-to-Video-Diffusion-Models" class="headerlink" title="T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models"></a>T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion   Models</h2><p><strong>Authors:Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li</strong></p>
<p>Recent advances in text-to-video (T2V) diffusion models have significantly enhanced the quality of generated videos. However, their ability to produce explicit or harmful content raises concerns about misuse and potential rights violations. Inspired by the success of unlearning techniques in erasing undesirable concepts from text-to-image (T2I) models, we extend unlearning to T2V models and propose a robust and precise unlearning method. Specifically, we adopt negatively-guided velocity prediction fine-tuning and enhance it with prompt augmentation to ensure robustness against LLM-refined prompts. To achieve precise unlearning, we incorporate a localization and a preservation regularization to preserve the modelâ€™s ability to generate non-target concepts. Extensive experiments demonstrate that our method effectively erases a specific concept while preserving the modelâ€™s generation capability for all other concepts, outperforming existing methods. We provide the unlearned models in \href{<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%7D%7Bhttps://github.com/VDIGPKU/T2VUnlearning.git%7D">https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}</a>. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬äº§ç”Ÿæ˜ç¡®æˆ–æœ‰å®³å†…å®¹çš„èƒ½åŠ›å¼•å‘äº†å…³äºè¯¯ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ã€‚å—æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä¸­æ¶ˆé™¤ä¸è‰¯æ¦‚å¿µçš„å»å­¦ä¹ æŠ€æœ¯æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬å°†å»å­¦ä¹ æ‰©å±•åˆ°T2Væ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¨³å¥ä¸”ç²¾ç¡®çš„å»å­¦ä¹ æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨è´Ÿå¯¼å‘é€Ÿåº¦é¢„æµ‹å¾®è°ƒï¼Œå¹¶é€šè¿‡æç¤ºå¢å¼ºæ¥ç¡®ä¿å…¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æç¤ºçš„ç¨³å¥æ€§ã€‚ä¸ºäº†å®ç°ç²¾ç¡®çš„å»å­¦ä¹ ï¼Œæˆ‘ä»¬ç»“åˆäº†å®šä½å’Œä¿å­˜æ­£åˆ™åŒ–ï¼Œä»¥ä¿ç•™æ¨¡å‹ç”Ÿæˆéç›®æ ‡æ¦‚å¿µçš„èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ï¼Œä¿ç•™äº†æ¨¡å‹å¯¹æ‰€æœ‰å…¶ä»–æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬æä¾›å»å­¦ä¹ çš„æ¨¡å‹åœ¨<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git%E3%80%82">https://github.com/VDIGPKU/T2VUnlearning.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17550v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•ï¼Œè¿™äº›æ¨¡å‹ç”Ÿæˆè§†é¢‘çš„è´¨é‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬äº§ç”Ÿæ˜ç¡®æˆ–æœ‰å®³å†…å®¹çš„èƒ½åŠ›å¼•å‘äº†å…³äºè¯¯ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ã€‚å—æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä¸­çš„å»é—å¿˜æŠ€æœ¯æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿå°†å»é—å¿˜æŠ€æœ¯æ‰©å±•åˆ°T2Væ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç¨³å¥ä¸”ç²¾ç¡®çš„å»é—å¿˜æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è´Ÿå¯¼å‘é€Ÿåº¦é¢„æµ‹å¾®è°ƒï¼Œå¹¶é€šè¿‡æç¤ºå¢å¼ºæŠ€æœ¯æ¥æé«˜å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æç¤ºçš„ç¨³å¥æ€§ã€‚ä¸ºå®ç°ç²¾ç¡®å»é—å¿˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†å®šä½å’Œä¿æŠ¤æ­£åˆ™åŒ–æ¥ä¿ç•™æ¨¡å‹ç”Ÿæˆéç›®æ ‡æ¦‚å¿µçš„èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ï¼Œä¿ç•™äº†æ¨¡å‹å¯¹å…¶ä»–æ‰€æœ‰æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿæä¾›äº†é—å¿˜æ¨¡å‹åœ¨GitHubä¸Šçš„é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/VDIGPKU/T2VUnlearning.git">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚</li>
<li>T2Væ¨¡å‹äº§ç”Ÿæœ‰å®³å†…å®¹çš„èƒ½åŠ›å¼•å‘äº†å¯¹è¯¯ç”¨å’Œæ½œåœ¨æƒåˆ©ä¾µçŠ¯çš„æ‹…å¿§ã€‚</li>
<li>å—æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å»é—å¿˜æŠ€æœ¯æˆåŠŸçš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿå°†å…¶æ‰©å±•åˆ°T2Væ¨¡å‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç¨³å¥ä¸”ç²¾ç¡®çš„å»é—å¿˜æ–¹æ³•ï¼ŒåŒ…æ‹¬è´Ÿå¯¼å‘é€Ÿåº¦é¢„æµ‹å¾®è°ƒã€æç¤ºå¢å¼ºã€å®šä½å’Œä¿æŠ¤æ­£åˆ™åŒ–ç­‰æŠ€æœ¯ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™æ¨¡å‹å¯¹å…¶ä»–æ¦‚å¿µçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17550">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79957d67b30ca16b1de52071a3de9847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d9a9db4dfd0d331c8779326b3ac5ef8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70b8426958699e1949b39812cae4a40f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ffd9caa870bcf2e41323ba58fba76f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70de0b98b44af06b9fd700e8a7cba8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86cdc1ec073f06b8e441d5ab9df478d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Variational-Autoencoding-Discrete-Diffusion-with-Enhanced-Dimensional-Correlations-Modeling"><a href="#Variational-Autoencoding-Discrete-Diffusion-with-Enhanced-Dimensional-Correlations-Modeling" class="headerlink" title="Variational Autoencoding Discrete Diffusion with Enhanced Dimensional   Correlations Modeling"></a>Variational Autoencoding Discrete Diffusion with Enhanced Dimensional   Correlations Modeling</h2><p><strong>Authors:Tianyu Xie, Shuchen Xue, Zijin Feng, Tianyang Hu, Jiacheng Sun, Zhenguo Li, Cheng Zhang</strong></p>
<p>Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines. </p>
<blockquote>
<p>ç¦»æ•£æ‰©æ•£æ¨¡å‹æœ€è¿‘åœ¨æ¨¡æ‹Ÿå¤æ‚ç¦»æ•£æ•°æ®æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œå…¶ä¸­é®ç½©æ‰©æ•£æ¨¡å‹ï¼ˆMDMï¼‰åœ¨è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¹‹é—´æä¾›äº†ä»¤äººä¿¡æœçš„æƒè¡¡ã€‚MDMé€šè¿‡ä»å®Œå…¨é®ç½©çš„è¾“å…¥ä¸­é€æ­¥å»å™ªå¤šä¸ªç»´åº¦æ¥å®ç°å»å™ªï¼Œä½†ç”±äºå¯¹ç»´åº¦é—´ä¾èµ–æ€§çš„å»ºæ¨¡æœ‰é™ï¼Œå½“ä½¿ç”¨è¾ƒå°‘çš„å»å™ªæ­¥éª¤æ—¶ï¼Œå…¶æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å˜åˆ†è‡ªç¼–ç ç¦»æ•£æ‰©æ•£ï¼ˆVADDï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ½œåœ¨å˜é‡å»ºæ¨¡å¢å¼ºç¦»æ•£æ‰©æ•£ï¼Œä»¥éšå«åœ°æ•è·ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©è¯†åˆ«æ¨¡å‹ï¼ŒVADDèƒ½å¤Ÿé€šè¿‡å˜åˆ†ä¸‹ç•Œæœ€å¤§åŒ–å’Œè®­ç»ƒé›†ä¸Šçš„æ‘Šé”€æ¨æ–­æ¥å®ç°ç¨³å®šè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†ä¼ ç»ŸMDMçš„æ•ˆç‡ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ ·æœ¬è´¨é‡ï¼Œå°¤å…¶æ˜¯å½“å»å™ªæ­¥éª¤æ•°é‡è¾ƒå°‘æ—¶ã€‚åœ¨äºŒç»´ç©å…·æ•°æ®ã€åƒç´ çº§å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒVADDå§‹ç»ˆä¼˜äºMDMåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17384v1">PDF</a> 23 pages, 14 figures</p>
<p><strong>Summary</strong><br>     ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç¦»æ•£æ•°æ®æ—¶å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå…¶ä¸­æ©æ¨¡æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰åœ¨è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¹‹é—´è¾¾åˆ°äº†ä»¤äººä¿¡æœçš„å¹³è¡¡ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘å¯¹ç»´åº¦é—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡ï¼Œå½“ä½¿ç”¨è¾ƒå°‘çš„å»å™ªæ­¥éª¤æ—¶ï¼Œå…¶æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶â€”â€”å˜åˆ†è‡ªç¼–ç ç¦»æ•£æ‰©æ•£ï¼ˆVADDï¼‰ï¼Œé€šè¿‡å¼•å…¥æ½œåœ¨å˜é‡å»ºæ¨¡æ¥æ•è·ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥è¾…åŠ©è¯†åˆ«æ¨¡å‹ï¼ŒVADDèƒ½å¤Ÿé€šè¿‡å˜åˆ†ä¸‹ç•Œæœ€å¤§åŒ–å®ç°ç¨³å®šè®­ç»ƒï¼Œå¹¶åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œæ‘Šé”€æ¨æ–­ã€‚è¯¥æ–¹æ³•ä¿ç•™äº†ä¼ ç»ŸMDMsçš„æ•ˆç‡ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ ·æœ¬è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å»å™ªæ­¥éª¤è¾ƒå°‘æ—¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨äºŒç»´ç©å…·æ•°æ®ã€åƒç´ çº§å›¾åƒç”Ÿæˆè¿˜æ˜¯æ–‡æœ¬ç”Ÿæˆä¸Šï¼ŒVADDå‡ä¼˜äºMDMsåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å»ºæ¨¡å¤æ‚ç¦»æ•£æ•°æ®æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ©æ¨¡æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰åœ¨è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚</li>
<li>MDMsåœ¨è¾ƒå°‘å»å™ªæ­¥éª¤æ—¶æ€§èƒ½å¯èƒ½ä¸‹é™ï¼Œå› ä¸ºç¼ºä¹å¯¹ç»´åº¦é—´ä¾èµ–å…³ç³»çš„å»ºæ¨¡ã€‚</li>
<li>å˜åˆ†è‡ªç¼–ç ç¦»æ•£æ‰©æ•£ï¼ˆVADDï¼‰æ¡†æ¶é€šè¿‡å¼•å…¥æ½œåœ¨å˜é‡å»ºæ¨¡æ¥æé«˜ç¦»æ•£æ‰©æ•£çš„æ€§èƒ½ã€‚</li>
<li>VADDé€šè¿‡å¼•å…¥è¾…åŠ©è¯†åˆ«æ¨¡å‹å®ç°äº†ç¨³å®šè®­ç»ƒï¼ŒåŒæ—¶é€šè¿‡å˜åˆ†ä¸‹ç•Œæœ€å¤§åŒ–è¿›è¡Œäº†æ‘Šé”€æ¨æ–­ã€‚</li>
<li>VADDåœ¨ä¿ç•™MDMsæ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ ·æœ¬è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å»å™ªæ­¥éª¤è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69692ca4b032e4ba3b456b8d1fb07ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b25017ef4f6aee9043b18d70ba115d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b2bfd2fe5dd362efe351c9985e93568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92f5dd8818fba3aa6e716895d86f929e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Repurposing-Marigold-for-Zero-Shot-Metric-Depth-Estimation-via-Defocus-Blur-Cues"><a href="#Repurposing-Marigold-for-Zero-Shot-Metric-Depth-Estimation-via-Defocus-Blur-Cues" class="headerlink" title="Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus   Blur Cues"></a>Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus   Blur Cues</h2><p><strong>Authors:Chinmay Talegaonkar, Nikhil Gandudi Suresh, Zachary Novack, Yash Belhe, Priyanka Nagasamudra, Nicholas Antipa</strong></p>
<p>Recent monocular metric depth estimation (MMDE) methods have made notable progress towards zero-shot generalization. However, they still exhibit a significant performance drop on out-of-distribution datasets. We address this limitation by injecting defocus blur cues at inference time into Marigold, a \textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular depth estimation (MDE). Our method effectively turns Marigold into a metric depth predictor in a training-free manner. To incorporate defocus cues, we capture two images with a small and a large aperture from the same viewpoint. To recover metric depth, we then optimize the metric depth scaling parameters and the noise latents of Marigold at inference time using gradients from a loss function based on the defocus-blur image formation model. We compare our method against existing state-of-the-art zero-shot MMDE methods on a self-collected real dataset, showing quantitative and qualitative improvements. </p>
<blockquote>
<p>è¿‘æœŸå•çœ¼åº¦é‡æ·±åº¦ä¼°è®¡ï¼ˆMMDEï¼‰æ–¹æ³•åœ¨å®ç°é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éåˆ†å¸ƒæ•°æ®é›†ä¸Šä»ç„¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬é€šè¿‡æ¨ç†æ—¶å‘Marigoldæ³¨å…¥å¤±ç„¦æ¨¡ç³Šçº¿ç´¢æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼ŒMarigoldæ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬ã€å°ºåº¦ä¸å˜çš„å•çœ¼æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ä»¥æ— è®­ç»ƒçš„æ–¹å¼å°†Marigoldè½¬å˜ä¸ºåº¦é‡æ·±åº¦é¢„æµ‹å™¨ã€‚ä¸ºäº†èå…¥å¤±ç„¦çº¿ç´¢ï¼Œæˆ‘ä»¬ä»åŒä¸€è§†è§’æ•è·å°å­”å’Œå¤§å­”çš„ä¸¤å¼ å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºå¤±ç„¦æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œåœ¨æ¨ç†æ—¶é—´ä¼˜åŒ–åº¦é‡æ·±åº¦ç¼©æ”¾å‚æ•°å’ŒMarigoldçš„å™ªå£°æ½œåœ¨å˜é‡ä»¥æ¢å¤åº¦é‡æ·±åº¦ã€‚æˆ‘ä»¬åœ¨è‡ªæˆ‘æ”¶é›†çš„çœŸå®æ•°æ®é›†ä¸Šå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„é›¶æ ·æœ¬MMDEæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œä»å®šé‡å’Œå®šæ€§ä¸¤ä¸ªæ–¹é¢å±•ç¤ºäº†æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå•çœ¼åº¦é‡æ·±åº¦ä¼°è®¡ï¼ˆMMDEï¼‰æ–¹æ³•åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è·¨åˆ†å¸ƒæ•°æ®é›†ä¸Šæ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨æ¨ç†é˜¶æ®µå°†æ•£ç„¦æ¨¡ç³Šçº¿ç´¢æ³¨å…¥åˆ°é¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹Marigoldä¸­ï¼Œå®ç°äº†é›¶æ ·æœ¬ã€å°ºåº¦ä¸å˜çš„å•çœ¼æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼ä½¿Marigoldæˆä¸ºåº¦é‡æ·±åº¦é¢„æµ‹å™¨ã€‚ä¸ºèå…¥æ•£ç„¦çº¿ç´¢ï¼Œæˆ‘ä»¬ä»åŒä¸€è§†è§’æ•è·å°å­”å’Œå¤§å­”ä¸¤å¼ å›¾åƒï¼Œç„¶åä½¿ç”¨åŸºäºæ•£ç„¦æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹çš„æŸå¤±å‡½æ•°åœ¨æ¨ç†é˜¶æ®µä¼˜åŒ–åº¦é‡æ·±åº¦ç¼©æ”¾å‚æ•°å’ŒMarigoldçš„å™ªå£°æ½œåœ¨å˜é‡ã€‚åœ¨è‡ªæ”¶é›†çš„çœŸå®æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æœ€å…ˆè¿›çš„é›¶æ ·æœ¬MMDEæ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºå®šé‡å’Œå®šæ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMDEæ–¹æ³•åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨è·¨åˆ†å¸ƒæ•°æ®é›†ä¸Šå­˜åœ¨æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§å°†æ•£ç„¦æ¨¡ç³Šçº¿ç´¢æ³¨å…¥åˆ°é¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹Marigoldä¸­çš„æ–¹æ³•ï¼Œä»¥æé«˜æ·±åº¦ä¼°è®¡æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–åº¦é‡æ·±åº¦ç¼©æ”¾å‚æ•°å’Œå™ªå£°æ½œåœ¨å˜é‡ï¼Œå®ç°äº†é›¶æ ·æœ¬ã€å°ºåº¦ä¸å˜çš„å•çœ¼æ·±åº¦ä¼°è®¡ã€‚</li>
<li>åˆ©ç”¨å°å­”å’Œå¤§å­”å›¾åƒæ¥æ•æ‰æ•£ç„¦çº¿ç´¢ï¼Œä»¥æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨åŸºäºæ•£ç„¦æ¨¡ç³Šå›¾åƒå½¢æˆæ¨¡å‹çš„æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>åœ¨è‡ªæ”¶é›†çš„çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å–å¾—äº†å®šé‡å’Œå®šæ€§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65570258b1503370d9bfe0d698baad5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c97261da66e2b5d53cec8f5e98c1c62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55b20c39e89eaeee69047f456ba0f4a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-daab8831d6d3080050030272f7de34ae.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Ascent-Diffusion-for-Inverse-Problems"><a href="#Dual-Ascent-Diffusion-for-Inverse-Problems" class="headerlink" title="Dual Ascent Diffusion for Inverse Problems"></a>Dual Ascent Diffusion for Inverse Problems</h2><p><strong>Authors:Minseo Kim, Axel Levy, Gordon Wetzstein</strong></p>
<p>Ill-posed inverse problems are fundamental in many domains, ranging from astrophysics to medical imaging. Emerging diffusion models provide a powerful prior for solving these problems. Existing maximum-a-posteriori (MAP) or posterior sampling approaches, however, rely on different computational approximations, leading to inaccurate or suboptimal samples. To address this issue, we introduce a new approach to solving MAP problems with diffusion model priors using a dual ascent optimization framework. Our framework achieves better image quality as measured by various metrics for image restoration problems, it is more robust to high levels of measurement noise, it is faster, and it estimates solutions that represent the observations more faithfully than the state of the art. </p>
<blockquote>
<p>ä¸é€‚å®šåé—®é¢˜åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œä»å¤©æ–‡ç‰©ç†åˆ°åŒ»å­¦å½±åƒã€‚æ–°å…´çš„æ‰©æ•£æ¨¡å‹ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†å¼ºå¤§çš„å…ˆéªŒã€‚ç„¶è€Œï¼Œç°æœ‰çš„æœ€å¤§åéªŒæ¦‚ç‡ï¼ˆMAPï¼‰æˆ–åé‡‡æ ·æ–¹æ³•ä¾èµ–äºä¸åŒçš„è®¡ç®—è¿‘ä¼¼æ–¹æ³•ï¼Œå¯¼è‡´æ ·æœ¬ä¸å‡†ç¡®æˆ–æ¬¡ä¼˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½¿ç”¨åŒä¸Šå‡ä¼˜åŒ–æ¡†æ¶è§£å†³å¸¦æœ‰æ‰©æ•£æ¨¡å‹å…ˆéªŒçš„MAPé—®é¢˜çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å›¾åƒæ¢å¤é—®é¢˜ä¸Šä»¥å¤šç§åº¦é‡æ ‡å‡†è¡¡é‡å›¾åƒè´¨é‡æ›´å¥½ï¼Œå¯¹é«˜æ°´å¹³çš„æµ‹é‡å™ªå£°æ›´å…·é²æ£’æ€§ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”ä¼°è®¡çš„è§£å†³æ–¹æ¡ˆæ¯”ç°æœ‰æŠ€æœ¯æ›´çœŸå®åœ°ä»£è¡¨è§‚å¯Ÿç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17353v1">PDF</a> 23 pages, 15 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨è§£å†³ä¸é€‚å®šåé—®é¢˜ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„åŸºäºåŒé‡ä¸Šå‡ä¼˜åŒ–æ¡†æ¶çš„MAPé—®é¢˜è§£å†³ç­–ç•¥ï¼Œæ–°æ–¹æ³•èƒ½å¤Ÿåœ¨å›¾åƒä¿®å¤ç­‰é—®é¢˜ä¸­æé«˜å›¾åƒè´¨é‡ï¼Œæ›´å¥½åœ°åº”å¯¹é«˜æ°´å¹³çš„æµ‹é‡å™ªå£°ï¼Œä¼°è®¡å‡ºæ›´ä¸ºç²¾å‡†çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥çš„é€Ÿåº¦ä¹Ÿæ›´å¿«ï¼Œèƒ½æ›´å¥½åœ°è¿˜åŸè§‚æµ‹ç»“æœã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ad09a75ad31368a6d029e57a8fad3f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edb729f5f7d6b8bdc70136f47da3bbf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e986bc27d4003c899dff64145f2c5c2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis"><a href="#Temporal-Differential-Fields-for-4D-Motion-Modeling-via-Image-to-Video-Synthesis" class="headerlink" title="Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis"></a>Temporal Differential Fields for 4D Motion Modeling via Image-to-Video   Synthesis</h2><p><strong>Authors:Xin You, Minghui Zhang, Hanxiao Zhang, Jie Yang, Nassir Navab</strong></p>
<p>Temporal modeling on regular respiration-induced motions is crucial to image-guided clinical applications. Existing methods cannot simulate temporal motions unless high-dose imaging scans including starting and ending frames exist simultaneously. However, in the preoperative data acquisition stage, the slight movement of patients may result in dynamic backgrounds between the first and last frames in a respiratory period. This additional deviation can hardly be removed by image registration, thus affecting the temporal modeling. To address that limitation, we pioneeringly simulate the regular motion process via the image-to-video (I2V) synthesis framework, which animates with the first frame to forecast future frames of a given length. Besides, to promote the temporal consistency of animated videos, we devise the Temporal Differential Diffusion Model to generate temporal differential fields, which measure the relative differential representations between adjacent frames. The prompt attention layer is devised for fine-grained differential fields, and the field augmented layer is adopted to better interact these fields with the I2V framework, promoting more accurate temporal variation of synthesized videos. Extensive results on ACDC cardiac and 4D Lung datasets reveal that our approach simulates 4D videos along the intrinsic motion trajectory, rivaling other competitive methods on perceptual similarity and temporal consistency. Codes will be available soon. </p>
<blockquote>
<p>é’ˆå¯¹è§„å¾‹æ€§çš„å‘¼å¸å¼•èµ·çš„è¿åŠ¨è¿›è¡Œæ—¶é—´å»ºæ¨¡ï¼Œå¯¹å›¾åƒå¼•å¯¼çš„ä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶é—´è¿åŠ¨ï¼Œé™¤éåŒæ—¶å­˜åœ¨åŒ…æ‹¬èµ·å§‹å¸§å’Œç»“æŸå¸§åœ¨å†…çš„é«˜å‰‚é‡æˆåƒæ‰«æã€‚ç„¶è€Œï¼Œåœ¨æœ¯å‰æ•°æ®é‡‡é›†é˜¶æ®µï¼Œæ‚£è€…è½»å¾®çš„ç§»åŠ¨å¯èƒ½ä¼šå¯¼è‡´ä¸€ä¸ªå‘¼å¸å‘¨æœŸå†…ç¬¬ä¸€å¸§å’Œæœ€åä¸€å¸§ä¹‹é—´çš„åŠ¨æ€èƒŒæ™¯ã€‚è¿™ç§é¢å¤–çš„åå·®å‡ ä¹æ— æ³•é€šè¿‡å›¾åƒæ³¨å†Œå»é™¤ï¼Œä»è€Œå½±å“æ—¶é—´å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–åˆ›æ€§åœ°é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç¬¬ä¸€å¸§æ¥é¢„æµ‹ç»™å®šé•¿åº¦çš„æœªæ¥å¸§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æå‡åŠ¨ç”»è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆæ—¶é—´å·®åˆ†åœºï¼Œå®ƒæµ‹é‡ç›¸é‚»å¸§ä¹‹é—´çš„ç›¸å¯¹å·®åˆ†è¡¨ç¤ºã€‚è®¾è®¡äº†å³æ—¶æ³¨æ„å±‚ç”¨äºç²¾ç»†çš„å·®åˆ†åœºï¼Œå¹¶é‡‡ç”¨åœºå¢å¼ºå±‚æ¥æ›´å¥½åœ°å°†è¿™äº›å­—æ®µä¸I2Væ¡†æ¶è¿›è¡Œäº¤äº’ï¼Œä»è€Œä¿ƒè¿›åˆæˆè§†é¢‘çš„æ›´åŠ å‡†ç¡®çš„æ—¶é—´å˜åŒ–ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†çš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ²¿ç€å†…åœ¨è¿åŠ¨è½¨è¿¹æ¨¡æ‹Ÿ4Dè§†é¢‘ï¼Œåœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–ç«äº‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚ä»£ç å°†å¾ˆå¿«å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17333v1">PDF</a> early accepted by MICCAI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¦–å¸§é¢„æµ‹æœªæ¥æŒ‡å®šé•¿åº¦çš„å¸§ã€‚ä¸ºè§£å†³åŠ¨æ€èƒŒæ™¯å¯¹æ—¶é—´å»ºæ¨¡çš„å½±å“ï¼Œå¼•å…¥æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ—¶é—´å·®åˆ†åœºï¼Œå¹¶è®¾è®¡äº†å³æ—¶æ³¨æ„åŠ›å±‚è¿›è¡Œç²¾ç»†å·®åˆ†åœºçš„å¤„ç†ã€‚åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šçš„å¤§é‡ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ¨¡æ‹Ÿçš„4Dè§†é¢‘æ²¿å†…åœ¨è¿åŠ¨è½¨è¿¹è¿›è¡Œï¼Œæ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢ä¸å…¶ä»–æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸¸è§„å‘¼å¸å¼•èµ·çš„è¿åŠ¨åœ¨å›¾åƒå¼•å¯¼çš„ä¸´åºŠåº”ç”¨ä¸­çš„æ—¶é—´å»ºæ¨¡æ˜¯å…³é”®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•æ¨¡æ‹Ÿæ—¶é—´è¿åŠ¨ï¼Œé™¤éåŒæ—¶å­˜åœ¨é«˜å‰‚é‡æˆåƒæ‰«æçš„èµ·å§‹å’Œç»“æŸå¸§ã€‚</li>
<li>æ‚£è€…è½»å¾®ç§»åŠ¨å¯èƒ½å¯¼è‡´åŠ¨æ€èƒŒæ™¯ï¼Œå½±å“æ—¶é—´å»ºæ¨¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆæ¡†æ¶ï¼Œé€šè¿‡é¦–å¸§é¢„æµ‹æœªæ¥æŒ‡å®šé•¿åº¦çš„å¸§æ¥æ¨¡æ‹Ÿå¸¸è§„è¿åŠ¨è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥æ—¶é—´å·®åˆ†æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ—¶é—´å·®åˆ†åœºï¼Œæé«˜åŠ¨ç”»è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†å³æ—¶æ³¨æ„åŠ›å±‚æ¥å¤„ç†ç²¾ç»†çš„å·®åˆ†åœºã€‚</li>
<li>åœ¨ACDCå¿ƒè„å’Œ4Dè‚ºéƒ¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥ç›¸ä¼¼æ€§å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ac025c4b8106fc2b23e8bc3ceb1a2e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c140bceaf0b20e54bd27b61db0bfd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8c3f6b48b9c0afefe10e9c897b446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d31f99851506e854b240ce2d7720b9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Revisiting-Adversarial-Perception-Attacks-and-Defense-Methods-on-Autonomous-Driving-Systems"><a href="#Revisiting-Adversarial-Perception-Attacks-and-Defense-Methods-on-Autonomous-Driving-Systems" class="headerlink" title="Revisiting Adversarial Perception Attacks and Defense Methods on   Autonomous Driving Systems"></a>Revisiting Adversarial Perception Attacks and Defense Methods on   Autonomous Driving Systems</h2><p><strong>Authors:Cheng Chen, Yuhong Wang, Nafis S Munir, Xiangwei Zhou, Xugui Zhou</strong></p>
<p>Autonomous driving systems (ADS) increasingly rely on deep learning-based perception models, which remain vulnerable to adversarial attacks. In this paper, we revisit adversarial attacks and defense methods, focusing on road sign recognition and lead object detection and prediction (e.g., relative distance). Using a Level-2 production ADS, OpenPilot by Comma$.$ai, and the widely adopted YOLO model, we systematically examine the impact of adversarial perturbations and assess defense techniques, including adversarial training, image processing, contrastive learning, and diffusion models. Our experiments highlight both the strengths and limitations of these methods in mitigating complex attacks. Through targeted evaluations of model robustness, we aim to provide deeper insights into the vulnerabilities of ADS perception systems and contribute guidance for developing more resilient defense strategies. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰è¶Šæ¥è¶Šä¾èµ–äºåŸºäºæ·±åº¦å­¦ä¹ çš„æ„ŸçŸ¥æ¨¡å‹ï¼Œè€Œè¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†å¯¹æŠ—æ€§æ”»å‡»å’Œé˜²å¾¡æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨é“è·¯æ ‡å¿—è¯†åˆ«ä»¥åŠå‰æ–¹ç‰©ä½“çš„æ£€æµ‹å’Œé¢„æµ‹ï¼ˆä¾‹å¦‚ç›¸å¯¹è·ç¦»ï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨Comma.aiçš„Level-2ç”Ÿäº§å‹ADSå’Œå¹¿æ³›é‡‡ç”¨çš„YOLOæ¨¡å‹ï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†å¯¹æŠ—æ€§æ‰°åŠ¨çš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†é˜²å¾¡æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§è®­ç»ƒã€å›¾åƒå¤„ç†ã€å¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒçªå‡ºäº†è¿™äº›æ–¹æ³•åœ¨ç¼“è§£å¤æ‚æ”»å‡»æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚é€šè¿‡å¯¹æ¨¡å‹ç¨³å¥æ€§çš„æœ‰é’ˆå¯¹æ€§çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ—¨åœ¨æ·±å…¥äº†è§£ADSæ„ŸçŸ¥ç³»ç»Ÿçš„æ¼æ´ï¼Œå¹¶ä¸ºå¼€å‘æ›´å…·éŸ§æ€§çš„é˜²å¾¡ç­–ç•¥æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11532v2">PDF</a> 8 pages, 2 figures, To appear in the 8th Dependable and Secure   Machine Learning Workshop (DSML 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é‡ç‚¹æ¢è®¨äº†è‡ªä¸»é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰ä¸­çš„å¯¹æŠ—æ€§æ”»å‡»ä¸é˜²å¾¡æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é“è·¯æ ‡å¿—è¯†åˆ«ä»¥åŠå‰æ–¹ç›®æ ‡æ£€æµ‹å’Œé¢„æµ‹ã€‚ç ”ç©¶ä½¿ç”¨äº†Comma.aiçš„OpenPilotæ°´å¹³2ç”Ÿäº§å‹ADSç³»ç»Ÿå’Œå¹¿æ³›é‡‡ç”¨çš„YOLOæ¨¡å‹ï¼Œç³»ç»Ÿè¯„ä¼°äº†å¯¹æŠ—æ€§æ‰°åŠ¨çš„å½±å“ï¼Œå¹¶æµ‹è¯•äº†åŒ…æ‹¬å¯¹æŠ—æ€§è®­ç»ƒã€å›¾åƒå¤„ç†ã€å¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹åœ¨å†…çš„é˜²å¾¡æŠ€æœ¯ã€‚å®éªŒæ­ç¤ºäº†è¿™äº›æ–¹æ³•åœ¨åº”å¯¹å¤æ‚æ”»å‡»æ—¶çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºæ·±å…¥äº†è§£ADSæ„ŸçŸ¥ç³»ç»Ÿçš„è„†å¼±æ€§å¹¶ä¸ºå¼€å‘æ›´ç¨³å¥çš„é˜²å¾¡ç­–ç•¥æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰ä¾èµ–æ·±åº¦å­¦ä¹ æ„ŸçŸ¥æ¨¡å‹ï¼Œä½†æ˜“å—å¯¹æŠ—æ€§æ”»å‡»å½±å“ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹è€ƒå¯Ÿäº†é“è·¯æ ‡å¿—è¯†åˆ«åŠå‰æ–¹ç›®æ ‡æ£€æµ‹å’Œé¢„æµ‹ä¸­çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚</li>
<li>ä½¿ç”¨äº†OpenPilotå’ŒYOLOæ¨¡å‹è¿›è¡Œå®éªŒç ”ç©¶ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†å¯¹æŠ—æ€§æ‰°åŠ¨å¯¹æ¨¡å‹çš„å½±å“ã€‚</li>
<li>æµ‹è¯•äº†å¤šç§é˜²å¾¡æ–¹æ³•ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§è®­ç»ƒã€å›¾åƒå¤„ç†ã€å¯¹æ¯”å­¦ä¹ å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®éªŒæ­ç¤ºäº†è¿™äº›æ–¹æ³•åœ¨åº”å¯¹å¤æ‚æ”»å‡»æ—¶çš„ä¼˜ç¼ºç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1af0033e1c54b6737c9b2911407548f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23249189506ef6045863491af3d0a89f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b206a60dca123dbd7290124669495ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3b372bad7bcf248bc0ffbc0a7d6137e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8df69946f809ba084a12baa35a7d9936.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8c1ec10e5c9e8f9da501d547b7bc3bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-559890da81bdc10559992c5b46613d3c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models"><a href="#QVGen-Pushing-the-Limit-of-Quantized-Video-Generative-Models" class="headerlink" title="QVGen: Pushing the Limit of Quantized Video Generative Models"></a>QVGen: Pushing the Limit of Quantized Video Generative Models</h2><p><strong>Authors:Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang</strong></p>
<p>Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\Phi$, we propose a rank-decay strategy that progressively eliminates $\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\mathbf{\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ç»èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„è§†é¢‘åˆæˆã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹ç°å®ä¸–ç•Œçš„åº”ç”¨éƒ¨ç½²ï¼Œå³ä½¿åœ¨é«˜ç«¯GPUä¸Šï¼Œä¹Ÿæ„æˆäº†ä¸¥é‡çš„æŒ‘æˆ˜ã€‚ä½œä¸ºä¸€ç§å¸¸ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œé‡åŒ–åœ¨é™ä½å›¾åƒDMçš„æˆæœ¬æ–¹é¢å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œè€Œå°†å…¶ç›´æ¥åº”ç”¨äºè§†é¢‘DMä»ç„¶æ— æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†QVGenï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºæç«¯ä½æ¯”ç‰¹é‡åŒ–ï¼ˆä¾‹å¦‚4ä½åŠä»¥ä¸‹ï¼‰ä¸‹é«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡é«˜çš„è§†é¢‘DMå®šåˆ¶çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬ä»ç†è®ºåˆ†æå¼€å§‹ï¼Œè¯æ˜é™ä½æ¢¯åº¦èŒƒæ•°æ˜¯ä¿ƒè¿›QATæ”¶æ•›çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾…åŠ©æ¨¡å—ï¼ˆÎ¦ï¼‰æ¥å‡è½»å¤§é‡çš„é‡åŒ–è¯¯å·®ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†æ”¶æ•›æ€§ã€‚ä¸ºäº†æ¶ˆé™¤Î¦çš„æ¨ç†å¼€é”€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ’åè¡°å‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ¶ˆé™¤Î¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åå¤ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’Œæå‡ºçš„åŸºäºæ’åçš„æ­£åˆ™åŒ–Î³æ¥è¯†åˆ«å’Œè¡°å‡ä½è´¡çŒ®æˆåˆ†ã€‚æ­¤ç­–ç•¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ¶ˆé™¤äº†æ¨ç†å¼€é”€ã€‚åœ¨4ç§æœ€å…ˆè¿›çš„è§†é¢‘DMsä¸Šçš„å¹¿æ³›å®éªŒï¼Œå‚æ•°å¤§å°ä»1.3Båˆ°14Bï¼Œè¡¨æ˜QVGené¦–æ¬¡åœ¨4ä½è®¾ç½®ä¸‹è¾¾åˆ°å…¨ç²¾åº¦å¯æ¯”è´¨é‡ã€‚è€Œä¸”ï¼Œå®ƒæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„3ä½CogVideoX-2Båœ¨VBenchä¸Šçš„åŠ¨æ€åº¦æé«˜äº†+25.28ï¼Œåœºæ™¯ä¸€è‡´æ€§æé«˜äº†+8.43ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11497v2">PDF</a> Our code will be released upon acceptance</p>
<p><strong>æ‘˜è¦</strong><br>    è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å¯å®ç°é«˜è´¨é‡è§†é¢‘åˆæˆï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚å¯¹ç°å®ä¸–ç•Œçš„éƒ¨ç½²æ„æˆäº†ä¸¥å³»æŒ‘æˆ˜ï¼Œå³ä½¿åœ¨é«˜ç«¯GPUä¸Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚é‡åŒ–ä½œä¸ºä¸€ç§å¸¸ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨é™ä½å›¾åƒDMçš„æˆæœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œè€Œç›´æ¥åº”ç”¨äºè§†é¢‘DMåˆ™æ•ˆæœä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†QVGenï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡çš„è§†é¢‘DMé‡èº«å®šåˆ¶çš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ï¼Œé€‚ç”¨äºæä½æ¯”ç‰¹é‡åŒ–ï¼ˆä¾‹å¦‚4ä½åŠä»¥ä¸‹ï¼‰ã€‚æœ¬æ–‡é¦–å…ˆè¿›è¡Œç†è®ºåˆ†æï¼Œè¯æ˜é™ä½æ¢¯åº¦èŒƒæ•°æ˜¯ä¿ƒè¿›QATæ”¶æ•›çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¾…åŠ©æ¨¡å—ï¼ˆÎ¦ï¼‰æ¥ç¼“è§£é‡åŒ–è¯¯å·®ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ”¶æ•›æ€§ã€‚ä¸ºäº†æ¶ˆé™¤Î¦çš„æ¨ç†å¼€é”€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­‰çº§è¡°å‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ¶ˆé™¤äº†Î¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åå¤ä½¿ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å’Œæå‡ºçš„åŸºäºç­‰çº§çš„è§„åˆ™Î³æ¥è¯†åˆ«å’Œè¡°å‡ä½è´¡çŒ®æˆåˆ†ã€‚æ­¤ç­–ç•¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ¶ˆé™¤äº†æ¨ç†å¼€é”€ã€‚åœ¨å‚æ•°è§„æ¨¡ä»1.3Båˆ°14Bçš„4ç§æœ€å…ˆè¿›è§†é¢‘DMä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQVGenåœ¨å››æ¯”ç‰¹è®¾ç½®ä¸‹é¦–æ¬¡è¾¾åˆ°å…¨ç²¾åº¦å¯æ¯”è´¨é‡ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„3ä½CogVideoX-2Båœ¨VBenchä¸Šçš„åŠ¨æ€åº¦å’Œåœºæ™¯ä¸€è‡´æ€§åˆ†åˆ«æé«˜äº†+25.28å’Œ+8.43ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å¯å®ç°é«˜è´¨é‡è§†é¢‘åˆæˆï¼Œä½†éƒ¨ç½²æŒ‘æˆ˜åœ¨äºå…¶é«˜è®¡ç®—å’Œå†…å­˜éœ€æ±‚ã€‚</li>
<li>é‡åŒ–ä½œä¸ºä¸€ç§é™ä½å›¾åƒDMæˆæœ¬çš„è§£å†³æ–¹æ¡ˆå·²ç»å–å¾—äº†æˆåŠŸï¼Œä½†ç›´æ¥åº”ç”¨äºè§†é¢‘DMæ•ˆæœä¸ä½³ã€‚</li>
<li>QVGenæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†é¢‘DMçš„é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é«˜æ€§èƒ½å’Œæ¨ç†æ•ˆç‡ï¼Œé€‚ç”¨äºæä½æ¯”ç‰¹é‡åŒ–ã€‚</li>
<li>QVGené€šè¿‡å¼•å…¥è¾…åŠ©æ¨¡å—å’Œç­‰çº§è¡°å‡ç­–ç•¥æ¥æé«˜QATçš„æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚</li>
<li>è¾…åŠ©æ¨¡å—ç”¨äºç¼“è§£é‡åŒ–è¯¯å·®ï¼Œè€Œç­‰çº§è¡°å‡ç­–ç•¥é€æ­¥æ¶ˆé™¤æ¨ç†å¼€é”€ã€‚</li>
<li>QVGenåœ¨å¤šç§è§†é¢‘DMä¸Šå®ç°äº†å…¨ç²¾åº¦å¯æ¯”è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨4ä½è®¾ç½®ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-679eba285d145eba6a48e4b7d3593de0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-264f2b954bbe36dc7229dad9723084b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fabb07ff35e2398ec63e3da43ec7ac01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2c6181fceebd1604e509856bac9a02d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era"><a href="#AnimeDL-2M-Million-Scale-AI-Generated-Anime-Image-Detection-and-Localization-in-Diffusion-Era" class="headerlink" title="AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era"></a>AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and   Localization in Diffusion Era</h2><p><strong>Authors:Chenyang Zhu, Xing Zhang, Yuyang Sun, Ching-Chun Chang, Isao Echizen</strong></p>
<p>Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored-despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods. Dataset and code can be found in <a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/">https://flytweety.github.io/AnimeDL2M/</a>. </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒç”Ÿæˆé¢†åŸŸçš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†åˆ›å»ºå¤æ‚ä¼ªé€ ä½œå“çš„é—¨æ§›ï¼Œä½¿å¾—å›¾åƒæ“çºµæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡å…ˆå‰åœ¨IMDLæ–¹é¢çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒä¸Šï¼Œä½†åŠ¨æ¼«é¢†åŸŸä»ç„¶é²œæœ‰ç ”ç©¶â€”â€”å°½ç®¡å®ƒè¶Šæ¥è¶Šå®¹æ˜“å—åˆ°AIç”Ÿæˆçš„ä¼ªé€ ä½œå“çš„å¨èƒã€‚å°†AIç”Ÿæˆçš„å›¾åƒè¯¯è¡¨ç¤ºä¸ºæ‰‹ç»˜è‰ºæœ¯å“ã€ç‰ˆæƒä¾µçŠ¯ä»¥åŠä¸å½“å†…å®¹ä¿®æ”¹å¯¹åŠ¨æ¼«ç¤¾åŒºå’Œè¡Œä¸šæ„æˆä¸¥é‡å¨èƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†AnimeDL-2Mï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŠ¨æ¼«IMDLçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å…¨é¢çš„æ³¨é‡Šã€‚å®ƒåŒ…å«è¶…è¿‡ä¸¤ç™¾ä¸‡å¼ å›¾åƒï¼ŒåŒ…æ‹¬çœŸå®ã€éƒ¨åˆ†æ“çºµå’Œå®Œå…¨AIç”Ÿæˆçš„æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨è‡ªç„¶å›¾åƒIMDLæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨åº”ç”¨äºåŠ¨æ¼«å›¾åƒæ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™å‡¸æ˜¾äº†åŠ¨æ¼«å’Œè‡ªç„¶å›¾åƒä¹‹é—´çš„æ˜æ˜¾é¢†åŸŸå·®è·ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†åŠ¨æ¼«é¢†åŸŸçš„IMDLä»»åŠ¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†AniXploreï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŠ¨æ¼«å›¾åƒè§†è§‰ç‰¹å¾é‡èº«å®šåˆ¶çš„æ–°å‹æ¨¡å‹ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAniXploreå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/%E6%89%BE%E5%88%B0%E3%80%82">https://flytweety.github.io/AnimeDL2M/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11015v2">PDF</a> 8+2 pages; update figure 3,4,5 as adding real images into detection   task tests</p>
<p><strong>Summary</strong><br>     æœ€æ–°è¿›å±•çš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†åˆ›å»ºå¤æ‚ä¼ªé€ ä½œå“çš„é—¨æ§›ï¼Œä½¿å¾—å›¾åƒæ“çºµæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰è¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§ã€‚å°½ç®¡IMDLå…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒä¸Šï¼Œä½†åŠ¨æ¼«é¢†åŸŸä»ç„¶è¢«å¿½è§†ï¼Œå°½ç®¡è¯¥é¢†åŸŸé¢ä¸´AIç”Ÿæˆä¼ªé€ ä½œå“çš„æ—¥ç›Šå¢é•¿çš„è„†å¼±æ€§ã€‚æˆ‘ä»¬æå‡ºäº†AnimeDL-2Mï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡åŠ¨æ¼«IMDLåŸºå‡†æµ‹è¯•é›†ï¼Œå¸¦æœ‰å…¨é¢æ³¨é‡Šï¼ŒåŒ…å«è¶…è¿‡ä¸¤ç™¾ä¸‡å¼ å›¾åƒï¼ŒåŒ…æ‹¬çœŸå®ã€éƒ¨åˆ†æ“çºµå’Œå®Œå…¨AIç”Ÿæˆçš„æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŠ¨æ¼«å›¾åƒä¸Šåº”ç”¨è®­ç»ƒæœ‰ç´ çš„è‡ªç„¶å›¾åƒIMDLæ•°æ®é›†è¡¨ç°ä¸ä½³ï¼Œçªæ˜¾å‡ºåŠ¨æ¼«å’Œè‡ªç„¶å›¾åƒä¹‹é—´çš„æ˜æ˜¾é¢†åŸŸå·®è·ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†åŠ¨æ¼«é¢†åŸŸçš„IMDLä»»åŠ¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†AniXploreæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é’ˆå¯¹åŠ¨æ¼«å›¾åƒçš„è§†è§‰ç‰¹æ€§é‡èº«å®šåˆ¶ã€‚å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAniXploreå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://flytweety.github.io/AnimeDL2M/%E6%89%BE%E5%88%B0%E3%80%82">https://flytweety.github.io/AnimeDL2M/æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å‘å±•ä½¿å¾—å›¾åƒç”Ÿæˆå˜å¾—æ›´åŠ å®¹æ˜“ï¼Œä»è€Œå¢åŠ äº†åˆ›å»ºå¤æ‚ä¼ªé€ ä½œå“çš„éš¾åº¦ã€‚</li>
<li>å›¾åƒæ“çºµæ£€æµ‹ä¸å®šä½ï¼ˆIMDLï¼‰é¢ä¸´æ–°çš„æŒ‘æˆ˜ï¼Œå› ä¸ºåŠ¨æ¼«é¢†åŸŸå¯¹AIç”Ÿæˆä¼ªé€ ä½œå“çš„è„†å¼±æ€§æ—¥ç›Šå¢åŠ ã€‚</li>
<li>å½“å‰IMDLæ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨è‡ªç„¶å›¾åƒä¸Šï¼Œè€ŒåŠ¨æ¼«é¢†åŸŸçš„æ•°æ®é›†ä»ç„¶ç¼ºä¹ã€‚</li>
<li>åŠ¨æ¼«å›¾åƒä¸è‡ªç„¶å›¾åƒä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„é¢†åŸŸå·®è·ã€‚</li>
<li>è®­ç»ƒæœ‰ç´ çš„è‡ªç„¶å›¾åƒIMDLæ•°æ®é›†åœ¨åŠ¨æ¼«å›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªé’ˆå¯¹åŠ¨æ¼«IMDLçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†AnimeDL-2Mï¼ŒåŒ…å«å…¨é¢æ³¨é‡Šå’Œè¶…è¿‡ä¸¤ç™¾ä¸‡å¼ å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca78bb4620abf9141ca3f5f947fbc5a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8992976fc1aa15fc7f78e4d85c26d0d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cac0dc7205cf7fdb6c41056585ab09d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70c499791398dcc15996dee8835fa376.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29f8d14632ade71f6b529587479d3426.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="REG-Rectified-Gradient-Guidance-for-Conditional-Diffusion-Models"><a href="#REG-Rectified-Gradient-Guidance-for-Conditional-Diffusion-Models" class="headerlink" title="REG: Rectified Gradient Guidance for Conditional Diffusion Models"></a>REG: Rectified Gradient Guidance for Conditional Diffusion Models</h2><p><strong>Authors:Zhengqi Gao, Kaiwen Zha, Tianyuan Zhang, Zihui Xue, Duane S. Boning</strong></p>
<p>Guidance techniques are simple yet effective for improving conditional generation in diffusion models. Albeit their empirical success, the practical implementation of guidance diverges significantly from its theoretical motivation. In this paper, we reconcile this discrepancy by replacing the scaled marginal distribution target, which we prove theoretically invalid, with a valid scaled joint distribution objective. Additionally, we show that the established guidance implementations are approximations to the intractable optimal solution under no future foresight constraint. Building on these theoretical insights, we propose rectified gradient guidance (REG), a versatile enhancement designed to boost the performance of existing guidance methods. Experiments on 1D and 2D demonstrate that REG provides a better approximation to the optimal solution than prior guidance techniques, validating the proposed theoretical framework. Extensive experiments on class-conditional ImageNet and text-to-image generation tasks show that incorporating REG consistently improves FID and Inception&#x2F;CLIP scores across various settings compared to its absence. </p>
<blockquote>
<p>æŒ‡å¯¼æŠ€æœ¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”¨äºæ”¹å–„æ¡ä»¶ç”Ÿæˆç®€å•è€Œæœ‰æ•ˆã€‚å°½ç®¡å®ƒä»¬åœ¨ç»éªŒä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†æŒ‡å¯¼çš„å®é™…å®æ–½ä¸å…¶ç†è®ºåŠ¨æœºå­˜åœ¨å¾ˆå¤§å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç”¨æœ‰æ•ˆçš„ç¼©æ”¾è”åˆåˆ†å¸ƒç›®æ ‡æ›¿æ¢ç†è®ºä¸Šæ— æ•ˆçš„å¯ç¼©æ”¾è¾¹é™…åˆ†å¸ƒç›®æ ‡æ¥è§£å†³è¿™ä¸€å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ç°æœ‰çš„æŒ‡å¯¼å®ç°æ˜¯åœ¨æ²¡æœ‰æœªæ¥é¢„æµ‹çº¦æŸä¸‹ä¸å¯è¡Œæœ€ä¼˜è§£çš„è¿‘ä¼¼è§£ã€‚åŸºäºè¿™äº›ç†è®ºè§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿®æ­£æ¢¯åº¦æŒ‡å¯¼ï¼ˆREGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨å¢å¼ºè®¾è®¡ï¼Œæ—¨åœ¨æé«˜ç°æœ‰æŒ‡å¯¼æ–¹æ³•çš„æ€§èƒ½ã€‚åœ¨ä¸€ç»´å’ŒäºŒç»´å®éªŒè¡¨æ˜ï¼Œç›¸å¯¹äºä¹‹å‰çš„æŒ‡å¯¼æŠ€æœ¯ï¼ŒREGæ›´å¥½åœ°æ¥è¿‘æœ€ä¼˜è§£ï¼ŒéªŒè¯äº†æ‰€æå‡ºç†è®ºæ¡†æ¶çš„æ­£ç¡®æ€§ã€‚åœ¨ç±»æ¡ä»¶ImageNetå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§è®¾ç½®ä¸­èå…¥REGä¸ç¼ºå°‘REGç›¸æ¯”ï¼ŒFIDå’ŒInception&#x2F;CLIPåˆ†æ•°å‡æœ‰æ‰€æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18865v2">PDF</a> 20 pages, 10 figures; accepted by ICMLâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ä¸­æŒ‡å¯¼æŠ€æœ¯çš„æ”¹è¿›å¯¹æ¡ä»¶ç”Ÿæˆçš„å½±å“ã€‚é’ˆå¯¹ç°æœ‰æŒ‡å¯¼æŠ€æœ¯å®æ–½ä¸ç†è®ºåŠ¨æœºçš„åå·®ï¼Œæœ¬æ–‡è¿›è¡Œäº†è°ƒå’Œã€‚é€šè¿‡æ›¿æ¢ç†è®ºä¸Šæ— æ•ˆçš„æ¯”ä¾‹è¾¹é™…åˆ†å¸ƒç›®æ ‡ï¼Œé‡‡ç”¨æœ‰æ•ˆçš„æ¯”ä¾‹è”åˆåˆ†å¸ƒç›®æ ‡ï¼Œå¹¶è¯æ˜ç°æœ‰æŒ‡å¯¼å®ç°æ˜¯åœ¨æ— æœªæ¥é¢„è§çº¦æŸä¸‹å¯¹æœ€ä¼˜è§£çš„è¿‘ä¼¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æå‡ºäº†çŸ«æ­£æ¢¯åº¦æŒ‡å¯¼ï¼ˆREGï¼‰è¿™ä¸€é€šç”¨å¢å¼ºæŠ€æœ¯ï¼Œæ—¨åœ¨æå‡ç°æœ‰æŒ‡å¯¼æ–¹æ³•çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒREGç›¸è¾ƒäºå…ˆå‰çš„æŒ‡å¯¼æŠ€æœ¯ï¼Œåœ¨é€¼è¿‘æœ€ä¼˜è§£æ–¹é¢è¡¨ç°æ›´ä½³ã€‚åœ¨ç±»æ¡ä»¶ImageNetå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œå¼•å…¥REGåœ¨ä¸åŒè®¾ç½®ä¸‹å§‹ç»ˆæé«˜äº†FIDå’ŒInception&#x2F;CLIPå¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ä¸­çš„æŒ‡å¯¼æŠ€æœ¯å¯¹äºæ”¹è¿›æ¡ä»¶ç”Ÿæˆè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æŒ‡å¯¼æŠ€æœ¯çš„å®æ–½ä¸ç†è®ºåŠ¨æœºå­˜åœ¨åå·®ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨æœ‰æ•ˆçš„æ¯”ä¾‹è”åˆåˆ†å¸ƒç›®æ ‡æ›¿æ¢ç†è®ºä¸Šæ— æ•ˆçš„æ¯”ä¾‹è¾¹é™…åˆ†å¸ƒç›®æ ‡ï¼Œè§£å†³äº†è¿™ä¸€åå·®ã€‚</li>
<li>ç°æœ‰æŒ‡å¯¼å®ç°æ˜¯å¯¹äºæ— æœªæ¥é¢„è§çº¦æŸä¸‹æœ€ä¼˜è§£çš„è¿‘ä¼¼ã€‚</li>
<li>æå‡ºçŸ«æ­£æ¢¯åº¦æŒ‡å¯¼ï¼ˆREGï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨å¢å¼ºç°æœ‰æŒ‡å¯¼æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>REGåœ¨é€¼è¿‘æœ€ä¼˜è§£æ–¹é¢è¡¨ç°ä¼˜äºå…ˆå‰çš„æŒ‡å¯¼æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-afb6e5ed2aa148c94ce745d29a0161d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f521233b5e091defc2f39267dc42eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72a7ecbeacfbfb8fba43fec4d7fec69b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints"><a href="#Ctrl-Room-Controllable-Text-to-3D-Room-Meshes-Generation-with-Layout-Constraints" class="headerlink" title="Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout   Constraints"></a>Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout   Constraints</h2><p><strong>Authors:Chuan Fang, Yuan Dong, Kunming Luo, Xiaotao Hu, Rakesh Shrestha, Ping Tan</strong></p>
<p>Text-driven 3D indoor scene generation is useful for gaming, the film industry, and AR&#x2F;VR applications. However, existing methods cannot faithfully capture the room layout, nor do they allow flexible editing of individual objects in the room. To address these problems, we present Ctrl-Room, which can generate convincing 3D rooms with designer-style layouts and high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables versatile interactive editing operations such as resizing or moving individual furniture items. Our key insight is to separate the modeling of layouts and appearance. Our proposed method consists of two stages: a Layout Generation Stage and an Appearance Generation Stage. The Layout Generation Stage trains a text-conditional diffusion model to learn the layout distribution with our holistic scene code parameterization. Next, the Appearance Generation Stage employs a fine-tuned ControlNet to produce a vivid panoramic image of the room guided by the 3D scene layout and text prompt. We thus achieve a high-quality 3D room generation with convincing layouts and lively textures. Benefiting from the scene code parameterization, we can easily edit the generated room model through our mask-guided editing module, without expensive edit-specific training. Extensive experiments on the Structured3D dataset demonstrate that our method outperforms existing methods in producing more reasonable, view-consistent, and editable 3D rooms from natural language prompts. </p>
<blockquote>
<p>æ–‡æœ¬é©±åŠ¨çš„3Då®¤å†…åœºæ™¯ç”Ÿæˆåœ¨æ¸¸æˆã€ç”µå½±äº§ä¸šå’ŒAR&#x2F;VRåº”ç”¨ä¸­éå¸¸æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•çœŸå®åœ°æ•æ‰æˆ¿é—´å¸ƒå±€ï¼Œä¹Ÿä¸å…è®¸å¯¹æˆ¿é—´ä¸­çš„å•ä¸ªç‰©ä½“è¿›è¡Œçµæ´»çš„ç¼–è¾‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Ctrl-Roomï¼Œå®ƒä»…é€šè¿‡æ–‡æœ¬æç¤ºå°±èƒ½ç”Ÿæˆå…·æœ‰è®¾è®¡å¸ˆé£æ ¼çš„å¸ƒå±€å’Œé«˜ä¿çœŸçº¹ç†çš„ä»¤äººä¿¡æœçš„3Dæˆ¿é—´ã€‚æ­¤å¤–ï¼ŒCtrl-Roomè¿˜å¯ç”¨äº†å¤šæ ·åŒ–çš„äº¤äº’ç¼–è¾‘æ“ä½œï¼Œå¦‚è°ƒæ•´å¤§å°æˆ–ç§»åŠ¨å•ä¸ªå®¶å…·ç‰©å“ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ†ç¦»å¸ƒå±€å’Œå¤–è§‚çš„å»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå¸ƒå±€ç”Ÿæˆé˜¶æ®µå’Œå¤–è§‚ç”Ÿæˆé˜¶æ®µã€‚å¸ƒå±€ç”Ÿæˆé˜¶æ®µè®­ç»ƒäº†ä¸€ä¸ªæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥å­¦ä¹ æˆ‘ä»¬çš„æ•´ä½“åœºæ™¯ä»£ç å‚æ•°åŒ–çš„å¸ƒå±€åˆ†å¸ƒã€‚æ¥ä¸‹æ¥ï¼Œå¤–è§‚ç”Ÿæˆé˜¶æ®µé‡‡ç”¨ç»è¿‡å¾®è°ƒçš„æ§åˆ¶ç½‘ï¼ˆControlNetï¼‰ç”Ÿæˆå—3Dåœºæ™¯å¸ƒå±€å’Œæ–‡æœ¬æç¤ºå¼•å¯¼çš„æˆ¿é—´ç”ŸåŠ¨å…¨æ™¯å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®ç°äº†å…·æœ‰ä»¤äººä¿¡æœçš„å¸ƒå±€å’Œç”ŸåŠ¨çº¹ç†çš„é«˜è´¨é‡3Dæˆ¿é—´ç”Ÿæˆã€‚å¾—ç›Šäºåœºæ™¯ä»£ç å‚æ•°åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾é€šè¿‡æˆ‘ä»¬çš„é®ç½©å¼•å¯¼ç¼–è¾‘æ¨¡å—ç¼–è¾‘ç”Ÿæˆçš„æˆ¿é—´æ¨¡å‹ï¼Œè€Œæ— éœ€è¿›è¡Œæ˜‚è´µçš„ç‰¹å®šç¼–è¾‘è®­ç»ƒã€‚åœ¨Structured3Dæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆæ›´åˆç†ã€è§†è§’ä¸€è‡´ä¸”å¯ç¼–è¾‘çš„3Dæˆ¿é—´æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03602v4">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬é©±åŠ¨çš„3Då®¤å†…åœºæ™¯ç”ŸæˆæŠ€æœ¯ä¸ºæ¸¸æˆã€å½±è§†ã€AR&#x2F;VRç­‰é¢†åŸŸå¸¦æ¥äº†ä¾¿æ·ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥çœŸå®è¿˜åŸæˆ¿é—´å¸ƒå±€ï¼Œä¹Ÿæ— æ³•çµæ´»ç¼–è¾‘å®¤å†…ç‰©ä»¶ã€‚æœ¬ç ”ç©¶æå‡ºCtrl-RoomæŠ€æœ¯ï¼Œä»…é€šè¿‡æ–‡æœ¬æç¤ºå³å¯ç”Ÿæˆå…·æœ‰è®¾è®¡æ„Ÿå¸ƒå±€å’Œé«˜ä¿çœŸçº¹ç†çš„3Dæˆ¿é—´ï¼Œå¹¶å®ç°ä¸ªä½“å®¶å…·çš„ç¼©æ”¾å’Œç§»åŠ¨ç­‰äº¤äº’å¼ç¼–è¾‘æ“ä½œã€‚æ ¸å¿ƒåœ¨äºå¸ƒå±€ä¸å¤–è§‚çš„å»ºæ¨¡åˆ†ç¦»ï¼ŒåŒ…æ‹¬å¸ƒå±€ç”Ÿæˆé˜¶æ®µå’Œå¤–è§‚ç”Ÿæˆé˜¶æ®µã€‚åœ¨Structured3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCtrl-RoomæŠ€æœ¯è¾ƒç°æœ‰æ–¹æ³•æ›´ä¼˜ï¼Œèƒ½ç”Ÿæˆæ›´åˆç†ã€è§†è§’ä¸€è‡´ã€å¯ç¼–è¾‘çš„3Dæˆ¿é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ctrl-RoomæŠ€æœ¯é€šè¿‡æ–‡æœ¬é©±åŠ¨ç”Ÿæˆ3Då®¤å†…åœºæ™¯ï¼Œåº”ç”¨äºæ¸¸æˆã€å½±è§†ã€AR&#x2F;VRé¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥çœŸå®è¿˜åŸæˆ¿é—´å¸ƒå±€ï¼Œä¸”ç¼ºä¹çµæ´»çš„ç¼–è¾‘åŠŸèƒ½ã€‚</li>
<li>Ctrl-RoomæŠ€æœ¯åˆ†ä¸ºå¸ƒå±€ç”Ÿæˆé˜¶æ®µå’Œå¤–è§‚ç”Ÿæˆé˜¶æ®µï¼Œå®ç°å¸ƒå±€ä¸å¤–è§‚çš„å»ºæ¨¡åˆ†ç¦»ã€‚</li>
<li>é‡‡ç”¨å…¨æ™¯å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œæ ¹æ®3Dåœºæ™¯å¸ƒå±€å’Œæ–‡æœ¬æç¤ºç”Ÿæˆç”ŸåŠ¨é€¼çœŸçš„çº¹ç†ã€‚</li>
<li>é€šè¿‡åœºæ™¯ä»£ç å‚æ•°åŒ–ï¼Œå®ç°è½»æ¾ç¼–è¾‘ç”Ÿæˆçš„æˆ¿é—´æ¨¡å‹ã€‚</li>
<li>Ctrl-RoomæŠ€æœ¯é€šè¿‡mask-guidedç¼–è¾‘æ¨¡å—ï¼Œæ— éœ€ç‰¹å®šåŸ¹è®­å³å¯è¿›è¡Œç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.03602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5e9ffaafb07b45806fbf68a14163425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f1f61286a98a1e543ce3053791ef05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20a8eccfcbdaff38bb22347d9d3ff2f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6d081705047798a5d50c3cfe66d7092.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-529ca47a3574172e74472100559dd62c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b7fa52599f33945e8fd5fcfa53c2403c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Effect of Fluorine doping on the electrocatalytic properties of Nb2O5   for H2O2 electrogeneration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d1506782cf16b887a34a02ca40f435b5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  CGS-GAN 3D Consistent Gaussian Splatting GANs for High Resolution Human   Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19710k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
