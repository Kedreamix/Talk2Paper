<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  Clip4Retrofit Enabling Real-Time Image Labeling on Edge Devices via   Cross-Architecture CLIP Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9290f55bc3e5deba2b123bed327e5c07.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-24
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-27-æ›´æ–°"><a href="#2025-05-27-æ›´æ–°" class="headerlink" title="2025-05-27 æ›´æ–°"></a>2025-05-27 æ›´æ–°</h1><h2 id="Clip4Retrofit-Enabling-Real-Time-Image-Labeling-on-Edge-Devices-via-Cross-Architecture-CLIP-Distillation"><a href="#Clip4Retrofit-Enabling-Real-Time-Image-Labeling-on-Edge-Devices-via-Cross-Architecture-CLIP-Distillation" class="headerlink" title="Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via   Cross-Architecture CLIP Distillation"></a>Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via   Cross-Architecture CLIP Distillation</h2><p><strong>Authors:Li Zhong, Ahmed Ghazal, Jun-Jun Wan, Frederik Zilly, Patrick Mackens, Joachim E. Vollrath, Bogdan Sorin Coseriu</strong></p>
<p>Foundation models like CLIP (Contrastive Language-Image Pretraining) have revolutionized vision-language tasks by enabling zero-shot and few-shot learning through cross-modal alignment. However, their computational complexity and large memory footprint make them unsuitable for deployment on resource-constrained edge devices, such as in-car cameras used for image collection and real-time processing. To address this challenge, we propose Clip4Retrofit, an efficient model distillation framework that enables real-time image labeling on edge devices. The framework is deployed on the Retrofit camera, a cost-effective edge device retrofitted into thousands of vehicles, despite strict limitations on compute performance and memory. Our approach distills the knowledge of the CLIP model into a lightweight student model, combining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to preserve cross-modal alignment while significantly reducing computational requirements. We demonstrate that our distilled model achieves a balance between efficiency and performance, making it ideal for deployment in real-world scenarios. Experimental results show that Clip4Retrofit can perform real-time image labeling and object identification on edge devices with limited resources, offering a practical solution for applications such as autonomous driving and retrofitting existing systems. This work bridges the gap between state-of-the-art vision-language models and their deployment in resource-constrained environments, paving the way for broader adoption of foundation models in edge computing. </p>
<blockquote>
<p>åƒCLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰è¿™æ ·çš„åŸºç¡€æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€å¯¹é½å®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œä»è€Œå½»åº•æ”¹å˜äº†è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®¡ç®—å¤æ‚æ€§å’Œè¾ƒå¤§çš„å†…å­˜å ç”¨ä½¿å¾—å®ƒä»¬ä¸é€‚åˆåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä¾‹å¦‚åœ¨ç”¨äºå›¾åƒé‡‡é›†å’Œå®æ—¶å¤„ç†çš„æ±½è½¦æ‘„åƒå¤´ä¸­ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Clip4Retrofitï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¨¡å‹è’¸é¦æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶å›¾åƒæ ‡ç­¾ã€‚è¯¥æ¡†æ¶éƒ¨ç½²åœ¨Retrofitæ‘„åƒå¤´ï¼ˆä¸€ç§æ”¹è£…åˆ°æ•°åƒè¾†æ±½è½¦ä¸­çš„ç»æµå‹è¾¹ç¼˜è®¾å¤‡ï¼‰ä¸Šï¼Œå°½ç®¡åœ¨è®¡ç®—æ€§èƒ½å’Œå†…å­˜æ–¹é¢å­˜åœ¨ä¸¥æ ¼é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†CLIPæ¨¡å‹çš„çŸ¥è¯†è’¸é¦åˆ°ä¸€ä¸ªè½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œç»“åˆEfficientNet-B3å’Œå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æŠ•å½±å¤´ï¼Œåœ¨ä¿æŒè·¨æ¨¡æ€å¯¹é½çš„åŒæ—¶æ˜¾è‘—é™ä½è®¡ç®—è¦æ±‚ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œä½¿å…¶æˆä¸ºåœ¨ç°å®åœºæ™¯éƒ¨ç½²çš„ç†æƒ³é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒClip4Retrofitå¯ä»¥åœ¨èµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶å›¾åƒæ ‡ç­¾å’Œå¯¹è±¡è¯†åˆ«ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å’Œç°æœ‰ç³»ç»Ÿæ”¹é€ ç­‰åº”ç”¨æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œå¡«è¡¥äº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åŠå…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­éƒ¨ç½²ä¹‹é—´çš„ç©ºç™½ï¼Œä¸ºè¾¹ç¼˜è®¡ç®—ä¸­åŸºç¡€æ¨¡å‹çš„æ›´å¹¿æ³›é‡‡ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18039v1">PDF</a> </p>
<p><strong>Summary</strong><br>    CLIPç­‰æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€å¯¹é½å®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­æ€èµ·é©å‘½ã€‚ä½†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é¢ä¸´è®¡ç®—å¤æ‚å’Œå†…å­˜å ç”¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºClip4Retrofitæ¨¡å‹è’¸é¦æ¡†æ¶ï¼Œèƒ½åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶å›¾åƒæ ‡æ³¨ã€‚è¯¥æ¡†æ¶éƒ¨ç½²åœ¨æ”¹è£…æ‘„åƒå¤´ä¸­ï¼Œå³ä½¿é¢ä¸´ä¸¥æ ¼çš„è®¡ç®—å’Œå†…å­˜é™åˆ¶ï¼Œä¹Ÿèƒ½åœ¨æ•°åƒè¾†æ±½è½¦ä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬é€šè¿‡è’¸é¦CLIPæ¨¡å‹çŸ¥è¯†åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ï¼Œç»“åˆEfficientNet-B3å’Œå¤šå±‚æ„ŸçŸ¥æœºæŠ•å½±å¤´ï¼Œåœ¨é™ä½è®¡ç®—è¦æ±‚çš„åŒæ—¶ä¿æŒè·¨æ¨¡æ€å¯¹é½ã€‚å®éªŒè¯æ˜ï¼Œè’¸é¦æ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œé€‚åˆåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶å’Œç°æœ‰ç³»ç»Ÿæ”¹é€ ç­‰åº”ç”¨æä¾›å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPç­‰æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€å¯¹é½å®ç°é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼Œæ¨åŠ¨è§†è§‰è¯­è¨€ä»»åŠ¡å‘å±•ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé¢ä¸´è®¡ç®—å¤æ‚å’Œå†…å­˜å ç”¨å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>Clip4Retrofitæ¡†æ¶è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œèƒ½åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶å›¾åƒæ ‡æ³¨ã€‚</li>
<li>è¯¥æ¡†æ¶éƒ¨ç½²åœ¨æ”¹è£…æ‘„åƒå¤´ä¸­ï¼Œé€‚ç”¨äºæ•°åƒè¾†æ±½è½¦ã€‚</li>
<li>é€šè¿‡è’¸é¦CLIPæ¨¡å‹çŸ¥è¯†åˆ°è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ï¼Œä¿æŒè·¨æ¨¡æ€å¯¹é½çš„åŒæ—¶é™ä½è®¡ç®—è¦æ±‚ã€‚</li>
<li>å®éªŒè¯æ˜è’¸é¦æ¨¡å‹åœ¨æ•ˆç‡å’Œæ€§èƒ½é—´å–å¾—å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9290f55bc3e5deba2b123bed327e5c07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c1a40b1ed073afeb1bd7bda8931467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab7ea99d1900701a63b1aea4f2bc92f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-009636afbdd1f9dbfbfe3d82f1e21eba.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling"><a href="#Few-Shot-Learning-from-Gigapixel-Images-via-Hierarchical-Vision-Language-Alignment-and-Modeling" class="headerlink" title="Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling"></a>Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language   Alignment and Modeling</h2><p><strong>Authors:Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi</strong></p>
<p>Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate alignment between visual and textual modalities on the same scale. To address these gaps, we propose HiVE-MIL, a hierarchical vision-language framework that constructs a unified graph consisting of (1) parent-child links between coarse (5x) and fine (20x) visual&#x2F;textual nodes to capture hierarchical relationships, and (2) heterogeneous intra-scale edges linking visual and textual nodes on the same scale. To further enhance semantic consistency, HiVE-MIL incorporates a two-stage, text-guided dynamic filtering mechanism that removes weakly correlated patch-text pairs, and introduces a hierarchical contrastive loss to align textual semantics across scales. Extensive experiments on TCGA breast, lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently outperforms both traditional MIL and recent VLM-based MIL approaches, achieving gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate the value of jointly modeling hierarchical structure and multimodal alignment for efficient and scalable learning from limited pathology data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL">https://github.com/bryanwong17/HiVE-MIL</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æœ€è¿‘å·²è¢«çº³å…¥å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰è¿›è¡Œå°‘é‡å¼±ç›‘ç£åˆ†ç±»çš„æŒ‘æˆ˜ã€‚ä¸€ç§å…³é”®è¶‹åŠ¿æ˜¯ï¼Œåˆ©ç”¨å¤šå°ºåº¦ä¿¡æ¯æ¥æ›´å¥½åœ°è¡¨ç¤ºå±‚æ¬¡åŒ–çš„ç»„ç»‡ç»“æ„ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šï¼ˆ1ï¼‰å¯¹åŒä¸€æ¨¡æ€å†…ä¸åŒå°ºåº¦ï¼ˆä¾‹å¦‚5å€å’Œ20å€ï¼‰ä¹‹é—´äº¤äº’çš„å»ºæ¨¡ä¸è¶³ï¼›ï¼ˆ2ï¼‰åœ¨åŒä¸€å°ºåº¦ä¸Šè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´å¯¹é½ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†HiVE-MILï¼Œè¿™æ˜¯ä¸€ç§å±‚æ¬¡åŒ–çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€å›¾ï¼ŒåŒ…æ‹¬ï¼ˆ1ï¼‰ç²—å°ºåº¦ï¼ˆ5å€ï¼‰å’Œç»†å°ºåº¦ï¼ˆ20å€ï¼‰è§†è§‰&#x2F;æ–‡æœ¬èŠ‚ç‚¹ä¹‹é—´çš„çˆ¶å­é“¾æ¥ï¼Œä»¥æ•æ‰å±‚æ¬¡å…³ç³»ï¼Œä»¥åŠï¼ˆ2ï¼‰åœ¨åŒä¸€å°ºåº¦ä¸Šè¿æ¥è§†è§‰å’Œæ–‡æœ¬èŠ‚ç‚¹çš„å¼‚è´¨å†…å°ºåº¦è¾¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§ï¼ŒHiVE-MILé‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥å»é™¤å¼±ç›¸å…³çš„è¡¥ä¸-æ–‡æœ¬å¯¹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å±‚æ¬¡å¯¹æ¯”æŸå¤±ï¼Œä»¥å¯¹é½ä¸åŒå°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰ã€‚åœ¨TCGAä¹³è…ºç™Œã€è‚ºç™Œå’Œè‚¾ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHiVE-MILå§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„MILå’Œæœ€æ–°çš„åŸºäºVLMçš„MILæ–¹æ³•ï¼Œåœ¨16æ¬¡æ‹æ‘„çš„å®è§‚F1å¾—åˆ†æé«˜äº†é«˜è¾¾4.1%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è”åˆå»ºæ¨¡å±‚æ¬¡ç»“æ„å’Œå¤šæ¨¡æ€å¯¹é½å¯¹äºä»æœ‰é™ç—…ç†æ•°æ®ä¸­å®ç°é«˜æ•ˆå’Œå¯æ‰©å±•å­¦ä¹ çš„ä»·å€¼ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bryanwong17/HiVE-MIL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bryanwong17/HiVE-MILæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17982v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸåœ¨å°‘æ•°æ ·æœ¬å’Œå¼±ç›‘ç£æƒ…å†µä¸‹ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰è¿›è¡Œåˆ†ç±»çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åä¸ºHiVE-MILçš„å±‚æ¬¡åŒ–è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºç»Ÿä¸€å›¾æ¨¡å‹å’Œå¤šå°ºåº¦ä¿¡æ¯æ¥ä¼˜åŒ–äº¤äº’å»ºæ¨¡å’Œæ¨¡æ€å¯¹é½é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬çˆ¶æ¯èŠ‚ç‚¹å’Œå­èŠ‚ç‚¹ä¹‹é—´çš„é“¾æ¥ä»¥åŠåŒä¸€å°ºåº¦ä¸Šçš„å¼‚è´¨å†…å°ºåº¦è¾¹ç¼˜ï¼Œå¢å¼ºäº†è¯­ä¹‰ä¸€è‡´æ€§å¹¶å®ç°äº†è·¨å°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰å¯¹é½ã€‚åœ¨ç™Œç—‡æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiVE-MILåœ¨å°‘æ•°æ ·æœ¬ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’ŒåŸºäºVLMçš„æ–¹æ³•ã€‚æ€»ç»“äº†ä½¿ç”¨å¤šæ¨¡æ€å¯¹é½å’Œå±‚æ¬¡ç»“æ„å»ºæ¨¡è¿›è¡Œé«˜æ•ˆã€å¯æ‰©å±•å­¦ä¹ çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»è¢«çº³å…¥å¤šæ¬¡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¡†æ¶ï¼Œç”¨äºè§£å†³å°‘é‡æ ·æœ¬å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒçš„åˆ†ç±»æŒ‘æˆ˜ã€‚ä¸»è¦é—®é¢˜åœ¨äºå¤šå°ºåº¦ä¿¡æ¯çš„æœ‰æ•ˆåˆ©ç”¨å’Œæ¨¡æ€é—´çš„å¯¹é½é—®é¢˜ã€‚</li>
<li>HiVE-MILæ¡†æ¶é€šè¿‡æ„å»ºç»Ÿä¸€å›¾æ¨¡å‹æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…æ‹¬è·¨å°ºåº¦çš„çˆ¶æ¯èŠ‚ç‚¹å’Œå­èŠ‚ç‚¹ä¹‹é—´çš„é“¾æ¥ä»¥åŠåŒä¸€å°ºåº¦ä¸Šçš„å¼‚è´¨å†…å°ºåº¦è¾¹ç¼˜ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ–‡æœ¬å¼•å¯¼åŠ¨æ€è¿‡æ»¤æœºåˆ¶ï¼Œå»é™¤å¼±ç›¸å…³çš„è¡¥ä¸æ–‡æœ¬å¯¹ï¼Œå¹¶ä½¿ç”¨å±‚æ¬¡å¯¹æ¯”æŸå¤±æ¥å¯¹é½è·¨å°ºåº¦çš„æ–‡æœ¬è¯­ä¹‰ã€‚è¿™ç§æœºåˆ¶å¢å¼ºäº†è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92e3487114fcdf284aef82ac852fbcc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97dcfab98be7f160215b7823cf69fa16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9450b1afc48fc6fa3f8e9bbffc4e4c52.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLM-Meeting-Decision-Trees-on-Tabular-Data"><a href="#LLM-Meeting-Decision-Trees-on-Tabular-Data" class="headerlink" title="LLM Meeting Decision Trees on Tabular Data"></a>LLM Meeting Decision Trees on Tabular Data</h2><p><strong>Authors:Hangting Ye, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang</strong></p>
<p>Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data throughough logical decision tree rules as intermediaries, proposes a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of &#96;&#96;errorsâ€™â€™ reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance. </p>
<blockquote>
<p>è¡¨æ ¼æ•°æ®åœ¨çœŸå®ä¸–ç•Œçš„å¤šä¸ªé¢†åŸŸä¸­éƒ½æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼ŒåŒ…æ‹¬åŒ»ç–—ä¿å¥ã€é‡‘èç­‰ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸçš„æˆåŠŸï¼Œå°†LLMæ‰©å±•åˆ°è¡¨æ ¼æ•°æ®é¢†åŸŸçš„æ—©æœŸæ¢ç´¢å·²ç»å¾—åˆ°å‘å±•ã€‚å¤§å¤šæ•°åŸºäºLLMçš„æ–¹æ³•é€šå¸¸å…ˆå°†è¡¨æ ¼æ•°æ®åºåˆ—åŒ–ä¸ºè‡ªç„¶è¯­è¨€æè¿°ï¼Œç„¶åè°ƒæ•´LLMæˆ–ç›´æ¥å¯¹è¿™äº›åºåˆ—åŒ–æ•°æ®è¿›è¡Œæ¨æ–­ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å†…åœ¨é—®é¢˜ï¼š</p>
</blockquote>
<p>ï¼ˆiï¼‰æ•°æ®è§’åº¦ï¼šç°æœ‰æ•°æ®åºåˆ—åŒ–æ–¹æ³•ç¼ºä¹é’ˆå¯¹ç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„é€šç”¨é€‚ç”¨æ€§ï¼Œå¹¶ä¸”å¯èƒ½é€šè¿‡ç›´æ¥æ–‡æœ¬æš´éœ²é€ æˆéšç§é£é™©ï¼›<br>ï¼ˆiiï¼‰æ¨¡å‹è§’åº¦ï¼šLLMå¾®è°ƒæ–¹æ³•åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ—¶é‡åˆ°å›°éš¾ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ çš„å¯æ‰©å±•æ€§å—åˆ°è¾“å…¥é•¿åº¦çº¦æŸçš„é™åˆ¶ï¼ˆé€‚ç”¨äºå°æ ·æœ¬å­¦ä¹ ï¼‰ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17918v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®é¢†åŸŸçš„åº”ç”¨é—®é¢˜ã€‚é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„åºåˆ—åŒ–å¤„ç†æ–¹æ³•å­˜åœ¨ç¼ºä¹é€šç”¨æ€§å’Œéšç§æ³„éœ²é£é™©çš„é—®é¢˜ï¼Œä»¥åŠä¼ ç»Ÿçš„LLMå¾®è°ƒæ–¹æ³•å¤„ç†è¡¨æ ¼æ•°æ®å­˜åœ¨å›°éš¾ç­‰é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡é€»è¾‘å†³ç­–æ ‘è§„åˆ™å°†LLMé›†æˆåˆ°è¡¨æ ¼æ•°æ®ä¸­çš„æ–°æ–¹æ³•ï¼Œå³å†³ç­–æ ‘å¢å¼ºå™¨DeLTaã€‚è¯¥æ–¹æ³•é¿å…äº†è¡¨æ ¼æ•°æ®çš„åºåˆ—åŒ–ï¼Œå¹¶é€‚ç”¨äºå…¨æ•°æ®å­¦ä¹ è®¾ç½®ï¼Œæ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥æ”¹è¿›ç»™å®šå†³ç­–æ ‘è§„åˆ™é›†çš„è®¾è®¡ï¼Œå¹¶ä¸ºåŸå§‹å†³ç­–æ ‘æä¾›äº†ä¸€ç§æ ¡å‡†æ–¹æ³•ï¼Œé€šè¿‡LLMç”Ÿæˆçš„æ–°è§„åˆ™æ¥è¿‘ä¼¼è¯¯å·®æ ¡æ­£å‘é‡ï¼Œä»è€Œå°†åŸå§‹å†³ç­–æ ‘é¢„æµ‹å¯¼å‘å‡å°‘è¯¯å·®çš„æ–¹å‘ã€‚åœ¨å¤šä¸ªè¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¡¨æ ¼æ•°æ®é¢†åŸŸçš„åº”ç”¨å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åºåˆ—åŒ–è¡¨æ ¼æ•°æ®åˆ°è‡ªç„¶è¯­è¨€æè¿°æ¥å¤„ç†ï¼Œä½†å­˜åœ¨ç¼ºä¹é€šç”¨æ€§å’Œéšç§æ³„éœ²é£é™©çš„é—®é¢˜ã€‚</li>
<li>DeLTaæ–¹æ³•é€šè¿‡é€»è¾‘å†³ç­–æ ‘è§„åˆ™é›†æˆLLMså¤„ç†è¡¨æ ¼æ•°æ®ï¼Œé¿å…äº†æ•°æ®åºåˆ—åŒ–ã€‚</li>
<li>DeLTaé€‚ç”¨äºå…¨æ•°æ®å­¦ä¹ è®¾ç½®ï¼Œæ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚</li>
<li>LLMsçš„æ¨ç†èƒ½åŠ›ç”¨äºæ”¹è¿›å†³ç­–æ ‘è§„åˆ™è®¾è®¡ï¼Œå¹¶ä¸”æä¾›å¯¹åŸå§‹å†³ç­–æ ‘çš„æ ¡å‡†æ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡è¿‘ä¼¼è¯¯å·®æ ¡æ­£å‘é‡ï¼Œå¯å¼•å¯¼å†³ç­–æ ‘é¢„æµ‹å‡å°‘è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17918">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-710472d342124537138aa7e6c928fb24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98371acf46e30858aa4bf5708542dbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a87a28a6db8113da3de4ecac1e4a95c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1982eab8cd493153ea894798af39f07e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Object-level-Cross-view-Geo-localization-with-Location-Enhancement-and-Multi-Head-Cross-Attention"><a href="#Object-level-Cross-view-Geo-localization-with-Location-Enhancement-and-Multi-Head-Cross-Attention" class="headerlink" title="Object-level Cross-view Geo-localization with Location Enhancement and   Multi-Head Cross Attention"></a>Object-level Cross-view Geo-localization with Location Enhancement and   Multi-Head Cross Attention</h2><p><strong>Authors:Zheyang Huang, Jagannath Aryal, Saeid Nahavandi, Xuequan Lu, Chee Peng Lim, Lei Wei, Hailing Zhou</strong></p>
<p>Cross-view geo-localization determines the location of a query image, captured by a drone or ground-based camera, by matching it to a geo-referenced satellite image. While traditional approaches focus on image-level localization, many applications, such as search-and-rescue, infrastructure inspection, and precision delivery, demand object-level accuracy. This enables users to prompt a specific object with a single click on a drone image to retrieve precise geo-tagged information of the object. However, variations in viewpoints, timing, and imaging conditions pose significant challenges, especially when identifying visually similar objects in extensive satellite imagery. To address these challenges, we propose an Object-level Cross-view Geo-localization Network (OCGNet). It integrates user-specified click locations using Gaussian Kernel Transfer (GKT) to preserve location information throughout the network. This cue is dually embedded into the feature encoder and feature matching blocks, ensuring robust object-specific localization. Additionally, OCGNet incorporates a Location Enhancement (LE) module and a Multi-Head Cross Attention (MHCA) module to adaptively emphasize object-specific features or expand focus to relevant contextual regions when necessary. OCGNet achieves state-of-the-art performance on a public dataset, CVOGL. It also demonstrates few-shot learning capabilities, effectively generalizing from limited examples, making it suitable for diverse applications (<a target="_blank" rel="noopener" href="https://github.com/ZheyangH/OCGNet">https://github.com/ZheyangH/OCGNet</a>). </p>
<blockquote>
<p>è·¨è§†å›¾åœ°ç†å®šä½ï¼ˆCross-view geo-localizationï¼‰æ˜¯é€šè¿‡å°†æ— äººæœºæˆ–åœ°é¢ç›¸æœºæ•è·çš„æŸ¥è¯¢å›¾åƒä¸åœ°ç†å‚è€ƒå«æ˜Ÿå›¾åƒè¿›è¡ŒåŒ¹é…ï¼Œç¡®å®šå…¶ä½ç½®ã€‚è™½ç„¶ä¼ ç»Ÿçš„æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒçº§åˆ«çš„å®šä½ï¼Œä½†è®¸å¤šåº”ç”¨ï¼ˆå¦‚æœæ•‘ã€åŸºç¡€è®¾æ–½æ£€æŸ¥å’Œç²¾ç¡®é…é€ï¼‰éœ€è¦å¯¹è±¡çº§åˆ«çš„ç²¾ç¡®åº¦ã€‚è¿™ä½¿ç”¨æˆ·å¯ä»¥é€šè¿‡å•å‡»æ— äººæœºå›¾åƒä¸Šçš„ç‰¹å®šå¯¹è±¡æ¥æ£€ç´¢è¯¥å¯¹è±¡çš„ç²¾ç¡®åœ°ç†æ ‡è®°ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè§†è§’ã€æ—¶é—´å’Œæˆåƒæ¡ä»¶çš„å·®å¼‚å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿æ³›çš„å«æ˜Ÿå›¾åƒä¸­è¯†åˆ«è§†è§‰ä¸Šç›¸ä¼¼çš„å¯¹è±¡æ—¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹è±¡çº§åˆ«çš„è·¨è§†å›¾åœ°ç†å®šä½ç½‘ç»œï¼ˆObject-level Cross-view Geo-localization Networkï¼Œç®€ç§°OCGNetï¼‰ã€‚å®ƒé€šè¿‡é«˜æ–¯æ ¸è½¬ç§»ï¼ˆGaussian Kernel Transferï¼Œç®€ç§°GKTï¼‰æ•´åˆç”¨æˆ·æŒ‡å®šçš„ç‚¹å‡»ä½ç½®ï¼Œä»¥åœ¨ç½‘ç»œä¸­ä¿ç•™ä½ç½®ä¿¡æ¯ã€‚è¿™ä¸€çº¿ç´¢è¢«åµŒå…¥åˆ°ç‰¹å¾ç¼–ç å™¨å’Œç‰¹å¾åŒ¹é…å—ä¸­ï¼Œç¡®ä¿ç¨³å¥çš„å¯¹è±¡ç‰¹å®šå®šä½ã€‚æ­¤å¤–ï¼ŒOCGNetè¿˜èå…¥äº†ä¸€ä¸ªä½ç½®å¢å¼ºï¼ˆLocation Enhancementï¼Œç®€ç§°LEï¼‰æ¨¡å—å’Œä¸€ä¸ªå¤šå¤´äº¤å‰æ³¨æ„åŠ›ï¼ˆMulti-Head Cross Attentionï¼Œç®€ç§°MHCAï¼‰æ¨¡å—ï¼Œä»¥è‡ªé€‚åº”åœ°çªå‡ºå¯¹è±¡ç‰¹å®šç‰¹å¾æˆ–åœ¨å¿…è¦æ—¶æ‰©å¤§å…³æ³¨ç›¸å…³ä¸Šä¸‹æ–‡åŒºåŸŸã€‚OCGNetåœ¨å…¬å…±æ•°æ®é›†CVOGLä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†å‡ºè‰²çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»æœ‰é™æ ·æœ¬ä¸­æ³›åŒ–ï¼Œä½¿å…¶é€‚ç”¨äºå„ç§åº”ç”¨ã€‚ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/ZheyangH/OCGNet%EF%BC%89">https://github.com/ZheyangH/OCGNetï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17911v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ— äººæœºæˆ–åœ°é¢ç›¸æœºæ‹æ‘„çš„æŸ¥è¯¢å›¾åƒï¼Œé€šè¿‡ä¸å…¶åŒ¹é…çš„åœ°ç†å‚è€ƒå«æ˜Ÿå›¾åƒè¿›è¡Œè·¨è§†å›¾åœ°ç†å®šä½ï¼Œç¡®å®šå…¶ä½ç½®ã€‚ä¸ºåº”å¯¹æœç´¢ä¸æ•‘æ´ã€åŸºç¡€è®¾æ–½æ£€æµ‹ä»¥åŠç²¾å‡†æŠ•é€’ç­‰åº”ç”¨éœ€æ±‚ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¯¹è±¡çº§åˆ«çš„è·¨è§†å›¾åœ°ç†å®šä½ç½‘ç»œï¼ˆOCGNetï¼‰ã€‚æ­¤æ–¹æ³•æ•´åˆç”¨æˆ·æŒ‡å®šçš„ç‚¹å‡»ä½ç½®ï¼Œå¹¶è¿ç”¨é«˜æ–¯æ ¸è½¬ç§»ï¼ˆGKTï¼‰æŠ€æœ¯ä¿ç•™ä½ç½®ä¿¡æ¯åœ¨ç½‘ç»œä¸­ã€‚æ­¤å¤–ï¼ŒOCGNetè¿˜åŒ…å«å®šä½å¢å¼ºæ¨¡å—å’Œè·¨å¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥è‡ªé€‚åº”çªå‡ºå¯¹è±¡ç‰¹å®šç‰¹å¾æˆ–åœ¨å¿…è¦æ—¶æ‰©å¤§å…³æ³¨ç›¸å…³ä¸Šä¸‹æ–‡åŒºåŸŸã€‚åœ¨å…¬å…±æ•°æ®é›†CVOGLä¸Šï¼ŒOCGNetå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å±•ç¤ºäº†å…¶å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»æœ‰é™çš„ä¾‹å­ä¸­æœ‰æ•ˆæ³›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨è§†å›¾åœ°ç†å®šä½æŠ€æœ¯é€šè¿‡åŒ¹é…æŸ¥è¯¢å›¾åƒä¸åœ°ç†å‚è€ƒå«æ˜Ÿå›¾åƒç¡®å®šä½ç½®ã€‚</li>
<li>å¯¹è±¡çº§åˆ«çš„å®šä½éœ€æ±‚åœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚æœç´¢ä¸æ•‘æ´ã€åŸºç¡€è®¾æ–½æ£€æµ‹åŠç²¾å‡†æŠ•é€’ï¼‰ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>OCGNetç½‘ç»œé€šè¿‡æ•´åˆç”¨æˆ·æŒ‡å®šçš„ç‚¹å‡»ä½ç½®å¹¶åˆ©ç”¨é«˜æ–¯æ ¸è½¬ç§»æŠ€æœ¯ï¼Œå®ç°ä½ç½®ä¿¡æ¯çš„ä¿ç•™ã€‚</li>
<li>OCGNetåŒ…å«å®šä½å¢å¼ºæ¨¡å—å’Œè·¨å¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œèƒ½è‡ªé€‚åº”çªå‡ºå¯¹è±¡ç‰¹å¾æˆ–å…³æ³¨ä¸Šä¸‹æ–‡ã€‚</li>
<li>OCGNetåœ¨å…¬å…±æ•°æ®é›†CVOGLä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…·æœ‰é¢†å…ˆçš„å®šä½ç²¾åº¦ã€‚</li>
<li>OCGNetå…·å¤‡å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿä»æœ‰é™ä¾‹å­ä¸­æœ‰æ•ˆæ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9121658764143cb327079703e4b606c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8f7519fb8d2084aecb9f229571e043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d58b66e9c7ae9006d6a62f5e1b1fa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da2dc6ff87088013f28a476b05a6eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e199531c3934a4378ebc62cd81b0c58.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="C-LoRA-Contextual-Low-Rank-Adaptation-for-Uncertainty-Estimation-in-Large-Language-Models"><a href="#C-LoRA-Contextual-Low-Rank-Adaptation-for-Uncertainty-Estimation-in-Large-Language-Models" class="headerlink" title="C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in   Large Language Models"></a>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in   Large Language Models</h2><p><strong>Authors:Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian</strong></p>
<p>Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ä¸ºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†å…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘é‡æ ·æœ¬ç¯å¢ƒä¸­é€šå¸¸ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå‡ ç§ç»å…¸çš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•å·²è¢«é‡æ–°ç”¨äºå¯æ‰©å±•çš„å…·æœ‰ä¸ç¡®å®šæ€§çš„LoRAå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¿½ç•¥äº†è¾“å…¥ç‰¹å¾å¦‚ä½•å½±å“é¢„æµ‹ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºä¸Šä¸‹æ–‡ä½ç§©é€‚åº”ï¼ˆC-LoRAï¼‰ä½œä¸ºä¸€ç§æ–°å‹çš„å…·æœ‰ä¸ç¡®å®šæ€§çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å¼€å‘é’ˆå¯¹æ¯ä¸ªè¾“å…¥æ•°æ®æ ·æœ¬è¿›è¡Œä¸Šä¸‹æ–‡å®šåˆ¶çš„æ–°å‹è½»é‡åŒ–LoRAæ¨¡å—æ¥åŠ¨æ€é€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚é€šè¿‡å°†æ•°æ®é©±åŠ¨ä¸Šä¸‹æ–‡çº³å…¥å‚æ•°åéªŒåˆ†å¸ƒï¼ŒC-LoRAç¼“è§£äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®ç°äº†æ ¡å‡†è‰¯å¥½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶äº§ç”Ÿäº†ç¨³å¥çš„é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸ç¡®å®šåº¦é‡åŒ–å’Œæ¨¡å‹æ³›åŒ–æ–¹é¢ï¼ŒC-LoRAå§‹ç»ˆä¼˜äºæœ€æ–°çš„ä¸€æµå…·æœ‰ä¸ç¡®å®šæ€§çš„LoRAæ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡æ¨¡å—åœ¨æ•è·æ ·æœ¬ç‰¹å®šä¸ç¡®å®šæ€§æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚C-LoRAä¸ºå°‘æ•°æƒ…å†µä¸‹çš„ç¨³å¥ã€å…·æœ‰ä¸ç¡®å®šæ€§çš„LLMå¾®è°ƒè®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17773v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LoRAæ–¹æ³•åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶å­˜åœ¨è¿‡äºè‡ªä¿¡é¢„æµ‹çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„å°‘é‡æ ·æœ¬ç¯å¢ƒä¸­ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†C-LoRAæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼€å‘æ–°çš„è½»é‡çº§LoRAæ¨¡å—æ¥åŠ¨æ€é€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶å¼•å…¥æ•°æ®é©±åŠ¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥è°ƒæ•´å‚æ•°åéªŒåˆ†å¸ƒã€‚C-LoRAèƒ½æœ‰æ•ˆç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®ç°è‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§ï¼Œäº§ç”Ÿç¨³å¥çš„é¢„æµ‹ã€‚åœ¨ä¸ç¡®å®šåº¦é‡æ¨¡å‹å’Œæ¨¡å‹æ³›åŒ–æ–¹é¢ï¼ŒC-LoRAå‡ä¼˜äºç°æœ‰çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥LoRAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒæä¾›äº†ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨å°‘é‡æ ·æœ¬ç¯å¢ƒä¸­ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚</li>
<li>ä¼ ç»Ÿçš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•åœ¨è§£å†³ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„LoRAå¾®è°ƒæ—¶å¿½ç•¥äº†è¾“å…¥ç‰¹æ€§çš„å½±å“ã€‚</li>
<li>C-LoRAä½œä¸ºä¸€ç§æ–°çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥å’Œå‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å¼€å‘é’ˆå¯¹æ¯ä¸ªè¾“å…¥æ•°æ®æ ·æœ¬çš„è½»é‡çº§LoRAæ¨¡å—æ¥åŠ¨æ€é€‚åº”ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>C-LoRAå¼•å…¥æ•°æ®é©±åŠ¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°å‚æ•°åéªŒåˆ†å¸ƒä¸­ï¼Œæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›ã€‚</li>
<li>C-LoRAèƒ½æœ‰æ•ˆç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®ç°è‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒC-LoRAåœ¨ä¸ç¡®å®šåº¦è¯„ä¼°å’Œæ¨¡å‹æ³›åŒ–æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a85cc3c0e369fe1bb4ba21a227a4175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed2797f8477478a46c05b7085ed946e6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PathoSCOPE-Few-Shot-Pathology-Detection-via-Self-Supervised-Contrastive-Learning-and-Pathology-Informed-Synthetic-Embeddings"><a href="#PathoSCOPE-Few-Shot-Pathology-Detection-via-Self-Supervised-Contrastive-Learning-and-Pathology-Informed-Synthetic-Embeddings" class="headerlink" title="PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive   Learning and Pathology-Informed Synthetic Embeddings"></a>PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive   Learning and Pathology-Informed Synthetic Embeddings</h2><p><strong>Authors:Sinchee Chin, Yinuo Ma, Xiaochen Yang, Jing-Hao Xue, Wenming Yang</strong></p>
<p>Unsupervised pathology detection trains models on non-pathological data to flag deviations as pathologies, offering strong generalizability for identifying novel diseases and avoiding costly annotations. However, building reliable normality models requires vast healthy datasets, as hospitalsâ€™ data is inherently biased toward symptomatic populations, while privacy regulations hinder the assembly of representative healthy cohorts. To address this limitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection framework that requires only a small set of non-pathological samples (minimum 2 shots), significantly improving data efficiency. We introduce Global-Local Contrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the variability of non-pathological embeddings and a Global Contrastive Loss to enhance the discrimination of pathological regions. We also propose a Pathology-informed Embedding Generation (PiEG) module that synthesizes pathological embeddings guided by the global loss, better exploiting the limited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8 datasets, PathoSCOPE achieves state-of-the-art performance among unsupervised methods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS). </p>
<blockquote>
<p>æ— ç›‘ç£ç—…ç†æ£€æµ‹é€šè¿‡å¯¹éç—…ç†æ•°æ®è¿›è¡Œè®­ç»ƒæ¨¡å‹ï¼Œå°†åå·®æ ‡è®°ä¸ºç—…ç†ç‰¹å¾ï¼Œä¸ºè¯†åˆ«æ–°å‹ç–¾ç—…å’Œé¿å…æ˜‚è´µçš„æ ‡æ³¨æä¾›äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå»ºç«‹å¯é çš„æ­£å¸¸æ¨¡å‹éœ€è¦å¤§é‡çš„å¥åº·æ•°æ®é›†ï¼Œå› ä¸ºåŒ»é™¢çš„æ•°æ®æœ¬è´¨ä¸Šåå‘äºæœ‰ç—‡çŠ¶çš„äººç¾¤ï¼Œè€Œéšç§æ³•è§„é˜»ç¢äº†ä»£è¡¨æ€§å¥åº·äººç¾¤æ•°æ®çš„æ±‡é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PathoSCOPEï¼Œè¿™æ˜¯ä¸€ä¸ªå°‘æ•°é•œå¤´æ— ç›‘ç£ç—…ç†æ£€æµ‹æ¡†æ¶ï¼Œåªéœ€è¦ä¸€å°éƒ¨åˆ†éç—…ç†æ ·æœ¬ï¼ˆæœ€å°‘2ä¸ªé•œå¤´ï¼‰ï¼Œå¤§å¤§æé«˜äº†æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬å¼•å…¥äº†å…¨å±€å±€éƒ¨å¯¹æ¯”æŸå¤±ï¼ˆGLCLï¼‰ï¼Œå®ƒç”±å±€éƒ¨å¯¹æ¯”æŸå¤±ç»„æˆï¼Œç”¨äºå‡å°‘éç—…ç†åµŒå…¥çš„å˜å¼‚æ€§ï¼Œä»¥åŠå…¨å±€å¯¹æ¯”æŸå¤±ï¼Œç”¨äºå¢å¼ºç—…ç†åŒºåŸŸçš„è¾¨åˆ«åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ç—…ç†ä¿¡æ¯åµŒå…¥ç”Ÿæˆï¼ˆPiEGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—é€šè¿‡å…¨å±€æŸå¤±å¼•å¯¼åˆæˆç—…ç†åµŒå…¥ï¼Œæ›´å¥½åœ°åˆ©ç”¨æœ‰é™çš„éç—…ç†æ ·æœ¬ã€‚åœ¨BraTS2020å’ŒChestXray8æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒPathoSCOPEåœ¨æ— ç›‘ç£æ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼ˆ2.48 GFLOPsï¼Œæ¯ç§’å¤„ç†å›¾åƒæ•°è¾¾166å¼ ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ— ç›‘ç£ç—…ç†æ£€æµ‹åŸç†ï¼Œè®­ç»ƒæ¨¡å‹åœ¨ä¸å«ç—…ç†çš„æ ·æœ¬ä¸Šèƒ½å¤Ÿæ£€æµ‹åˆ°åå·®è¿›è€Œåˆ¤å®šå‡ºç—…ç†å­¦ç—‡çŠ¶ï¼Œæ˜¾ç¤ºå…¶å¯¹æ–°å…´ç–¾ç—…çš„å¼ºæ³›åŒ–è¯†åˆ«èƒ½åŠ›å’Œé¿å…æ˜‚è´µæ ‡æ³¨çš„æˆæœ¬ä¼˜åŠ¿ã€‚ä½†å»ºç«‹å¯é çš„æ­£å¸¸æ¨¡å‹éœ€è¦æµ·é‡çš„å¥åº·æ•°æ®é›†ï¼ŒåŒ»é™¢çš„èµ„æ–™å€¾å‘äºä»¥ç—‡çŠ¶æ‚£è€…ä¸ºä¸»ï¼Œéšç§æ³•è§„ä¹Ÿé™åˆ¶äº†ä»£è¡¨æ€§å¥åº·äººç¾¤çš„æ±‡é›†ã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæˆ‘ä»¬æå‡ºPathoSCOPEæ–¹æ¡ˆï¼Œåªéœ€å°‘é‡éç—…ç†æ ·æœ¬ï¼ˆæœ€å°‘ä¸¤ä¾‹ï¼‰å³å¯è¿›è¡Œå°‘æ•°æ´¾æ— ç›‘ç£ç—…ç†æ£€æµ‹ï¼Œå¤§å¹…æé«˜æ•°æ®æ•ˆç‡ã€‚ç»“åˆå…¨å±€å±€éƒ¨å¯¹æ¯”æŸå¤±ï¼ˆGLCLï¼‰ï¼Œé€šè¿‡å±€éƒ¨å¯¹æ¯”æŸå¤±é™ä½éç—…ç†åµŒä½“å˜é‡çš„åŒæ—¶ï¼Œä»¥å…¨å±€å¯¹æ¯”æŸå¤±æå‡å¯¹ç—…å˜åŒºåŸŸçš„è¾¨åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç—…ç†å­¦å¼•å¯¼åµŒå…¥ç”Ÿæˆï¼ˆPiEGï¼‰æ¨¡å—ï¼Œç”±å…¨å±€æŸå¤±æŒ‡å¯¼åˆæˆç—…ç†æ€§åµŒä½“ï¼Œæ›´å¥½åˆ©ç”¨æœ‰é™çš„éç—…ç†æ ·æœ¬ã€‚åœ¨BraTS2020å’ŒChestXray8æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPathoSCOPEåœ¨æ— ç›‘ç£æ–¹æ³•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼ˆ2.48 GFLOPsï¼Œ166 FPSï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— ç›‘ç£ç—…ç†æ£€æµ‹åˆ©ç”¨éç—…ç†æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæœ‰æ•ˆè¯†åˆ«æ–°å…´ç–¾ç—…å¹¶é™ä½æˆæœ¬ã€‚</li>
<li>å»ºç«‹å¯é çš„æ­£å¸¸æ¨¡å‹éœ€è¦å¤§é‡å¥åº·æ•°æ®é›†ï¼Œä½†åŒ»é™¢æ•°æ®åå‘ç—‡çŠ¶æ‚£è€…ï¼Œéšç§æ³•è§„é™åˆ¶ä»£è¡¨æ€§å¥åº·äººç¾¤æ±‡é›†ã€‚</li>
<li>PathoSCOPEæ–¹æ¡ˆä»…éœ€è¦å°‘é‡éç—…ç†æ ·æœ¬è¿›è¡Œå°‘æ•°æ´¾æ— ç›‘ç£ç—…ç†æ£€æµ‹ï¼Œæé«˜æ•°æ®æ•ˆç‡ã€‚</li>
<li>æå‡ºå…¨å±€å±€éƒ¨å¯¹æ¯”æŸå¤±ï¼ˆGLCLï¼‰ï¼Œç»“åˆå±€éƒ¨å¯¹æ¯”æŸå¤±å’Œå…¨å±€å¯¹æ¯”æŸå¤±æ¥å¢å¼ºè¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥ç—…ç†å­¦å¼•å¯¼åµŒå…¥ç”Ÿæˆï¼ˆPiEGï¼‰æ¨¡å—ï¼Œåˆæˆç—…ç†æ€§åµŒä½“ä»¥åˆ©ç”¨æœ‰é™çš„éç—…ç†æ ·æœ¬ã€‚</li>
<li>åœ¨BraTS2020å’ŒChestXray8æ•°æ®é›†ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼ŒPathoSCOPEæ€§èƒ½è¾¾åˆ°æ— ç›‘ç£æ–¹æ³•æœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-348d3318f2eb23b5152c2945aa61b8d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0ca06d57d523360c49917ed5ba6d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6163b46c03dfe555258b7db9ba3d9001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f804e7e3337a55845eb5649b8a60b0d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FS-DAG-Few-Shot-Domain-Adapting-Graph-Networks-for-Visually-Rich-Document-Understanding"><a href="#FS-DAG-Few-Shot-Domain-Adapting-Graph-Networks-for-Visually-Rich-Document-Understanding" class="headerlink" title="FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich   Document Understanding"></a>FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich   Document Understanding</h2><p><strong>Authors:Amit Agarwal, Srikant Panda, Kulbhushan Pachauri</strong></p>
<p>In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language&#x2F;vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAGâ€™s capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : <a target="_blank" rel="noopener" href="https://github.com/oracle-samples/fs-dag">https://github.com/oracle-samples/fs-dag</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å°‘æ ·æœ¬é¢†åŸŸè‡ªé€‚åº”å›¾ï¼ˆFS-DAGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå°‘æ ·æœ¬è®¾ç½®ä¸‹è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ï¼ˆVRDUï¼‰çš„å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ¨¡å‹æ¶æ„ã€‚FS-DAGåˆ©ç”¨æ¨¡å—åŒ–æ¡†æ¶ä¸­çš„ç‰¹å®šé¢†åŸŸå’Œç‰¹å®šè¯­è¨€&#x2F;è§†è§‰çš„ä¸»å¹²ç½‘ç»œï¼Œä»¥é€‚åº”å¤šæ ·åŒ–çš„æ–‡æ¡£ç±»å‹ï¼ŒåŒæ—¶æ‰€éœ€æ•°æ®æå°‘ã€‚è¯¥æ¨¡å‹å¯¹å¤„ç†å®é™…æŒ‘æˆ˜å…·æœ‰å¾ˆå¼ºçš„é²æ£’æ€§ï¼Œå¦‚OCRé”™è¯¯ã€æ‹¼å†™é”™è¯¯å’Œé¢†åŸŸå˜åŒ–ç­‰ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­è‡³å…³é‡è¦ã€‚FS-DAGæ€§èƒ½é«˜è¶…ï¼Œå‚æ•°å°‘äº90Mï¼Œéå¸¸é€‚åˆè®¡ç®—èµ„æºæœ‰é™çš„ç°å®ä¸–ç•Œä¿¡æ¯æå–ï¼ˆIEï¼‰ä»»åŠ¡ä¸­çš„å¤æ‚åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„ä¿¡æ¯æå–ä»»åŠ¡å®éªŒå±•ç¤ºäº†FS-DAGçš„èƒ½åŠ›ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜å¼ºè°ƒäº†å¼€å‘æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹çš„ä¸æ–­è¿›æ­¥ï¼Œè¿™äº›æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¹¶æ— å¦¥åã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/oracle-samples/fs-dag">https://github.com/oracle-samples/fs-dag</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17330v1">PDF</a> Published in the Proceedings of the 31st International Conference on   Computational Linguistics (COLING 2025), Industry Track, pages 100-114</p>
<p><strong>Summary</strong><br>å°‘é‡æ ·æœ¬ä¸‹çš„é¢†åŸŸè‡ªé€‚åº”å›¾ï¼ˆFS-DAGï¼‰æ¨¡å‹åœ¨è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ï¼ˆVRDUï¼‰ä¸­çš„ç ”ç©¶ä¸å®ç°ã€‚FS-DAGæå‡ºä¸€ç§æ¨¡å—åŒ–æ¡†æ¶ï¼Œç»“åˆç‰¹å®šé¢†åŸŸå’Œè¯­è¨€è§†è§‰æ”¯æŒæ¨¡å—å¿«é€Ÿé€‚åº”å¤šæ ·æ–‡æ¡£ç±»å‹çš„å°æ ·æœ¬ç¯å¢ƒï¼Œå…‹æœå®é™…åº”ç”¨ä¸­å¦‚OCRé”™è¯¯ã€æ‹¼å†™é”™è¯¯ä»¥åŠé¢†åŸŸå˜åŒ–ç­‰é—®é¢˜ï¼Œå¹¶ä»¥è¾ƒä½è®¡ç®—èµ„æºå’Œé«˜æ•ˆçš„æ€§èƒ½å¤„ç†ä¿¡æ¯æŠ½å–ä»»åŠ¡ã€‚å®éªŒè¯æ˜FS-DAGæ¨¡å‹åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†å‘å±•å°å°ºå¯¸ä½†æ€§èƒ½å¼ºå¤§çš„æ¨¡å‹è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚å…³äºæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯å¯è®¿é—®é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/oracle-samples/fs-dag">https://github.com/oracle-samples/fs-dag</a>ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è§‚ç‚¹çš„æ‘˜è¦åˆ—è¡¨ï¼š</p>
<ul>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„åä¸ºå°‘é‡æ ·æœ¬ä¸‹çš„é¢†åŸŸè‡ªé€‚åº”å›¾ï¼ˆFS-DAGï¼‰ã€‚æ­¤æ¨¡å‹ç‰¹åˆ«é’ˆå¯¹è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ï¼ˆVRDUï¼‰è¿›è¡Œè®¾è®¡å’Œä¼˜åŒ–ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºä¿¡æ¯æŠ½å–ä»»åŠ¡çš„å°æ ·æœ¬ç¯å¢ƒã€‚æ­¤æ¨¡å‹åŒ…å«ç‰¹å®šé¢†åŸŸçš„æ¨¡å—å’Œè¯­è¨€è§†è§‰æ”¯æŒæ¨¡å—ï¼Œä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”ä¸åŒçš„æ–‡æ¡£ç±»å‹ã€‚ </li>
<li>FS-DAGæ¨¡å‹èƒ½å¤Ÿåº”å¯¹å®é™…åº”ç”¨ä¸­çš„å¤šç§æŒ‘æˆ˜ï¼Œå¦‚OCRé”™è¯¯ã€æ‹¼å†™é”™è¯¯å’Œé¢†åŸŸå˜åŒ–ç­‰é—®é¢˜ï¼Œå…·æœ‰å¾ˆé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚è¿™æ„å‘³ç€å®ƒåœ¨è¿›è¡Œæ–‡æœ¬å¤„ç†å’Œè¯†åˆ«ä»»åŠ¡æ—¶å…·æœ‰å¾ˆé«˜çš„ç¨³å¥æ€§ã€‚ </li>
<li>æ­¤æ¨¡å‹çš„è®¡ç®—éœ€æ±‚ä½ä¸”é«˜æ€§èƒ½çš„ç‰¹æ€§ä½¿å…¶åœ¨æœ‰é™çš„è®¡ç®—èµ„æºæ¡ä»¶ä¸‹ä¹Ÿå¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨åœ¨å¤æ‚ç°å®ä¸–ç•Œä¸­ã€‚å…¶å‚æ•°æ•°é‡å°‘äº9äº¿ä¸ªï¼Œç¡®ä¿äº†å…¶åœ¨ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§èƒ½ã€‚ </li>
<li>ç ”ç©¶é€šè¿‡å®éªŒè¯æ˜äº†FS-DAGæ¨¡å‹åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰çš„æŠ€æœ¯æ–¹æ³•ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚è¿™äº›å®éªŒå±•ç¤ºäº†æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ã€‚ </li>
<li>ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¬å¼€çš„æ¨¡å‹ä»£ç é“¾æ¥ï¼Œæ–¹ä¾¿å…¬ä¼—è·å–å’Œè¿›ä¸€æ­¥å¼€å‘ç ”ç©¶FS-DAGæ¨¡å‹ã€‚è¿™æœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•å’Œç ”ç©¶ã€‚ </li>
<li>æ­¤ç ”ç©¶å±•ç¤ºäº†åœ¨å¼€å‘å°å°ºå¯¸ä½†æ€§èƒ½å¼ºå¤§çš„æ¨¡å‹æ–¹é¢çš„ç ”ç©¶è¿›å±•ã€‚è¿™ç§æ¨¡å‹è®¾è®¡è¶‹åŠ¿è¡¨æ˜äº†å¯¹æ•ˆç‡å’Œæ€§èƒ½çš„å¹³è¡¡çš„è¿½æ±‚æ­£åœ¨æˆä¸ºç ”ç©¶çš„é‡è¦æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20e361061fc4185c1f340a8b7532625f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9101fb3c3f874c29e6657c76d479818c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53113ecb9f607c30a13c388497b7dcec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf84298e0d53d28947d72d56d2f849b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f5ad66613c7a58319310d63aa6bef75.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AdaReasoner-Adaptive-Reasoning-Enables-More-Flexible-Thinking"><a href="#AdaReasoner-Adaptive-Reasoning-Enables-More-Flexible-Thinking" class="headerlink" title="AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking"></a>AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking</h2><p><strong>Authors:Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang</strong></p>
<p>LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work â€˜well enoughâ€™ across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œå¦‚æ¸©åº¦å’Œæ¨ç†æ­¥éª¤ï¼Œæ¥å¤„ç†ä»ç¬‘è¯ç”Ÿæˆåˆ°æ•°å­¦æ¨ç†ç­‰éœ€è¦å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æç¤ºæ–¹æ³•é€šå¸¸é‡‡ç”¨é€šç”¨ã€å›ºå®šçš„é…ç½®ï¼Œè¿™äº›é…ç½®åœ¨å„é¡¹ä»»åŠ¡ä¸­â€œè¡¨ç°è¶³å¤Ÿå¥½â€ï¼Œä½†å¾ˆå°‘å®ç°é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AdaReasonerï¼Œè¿™æ˜¯ä¸€æ¬¾é€‚ç”¨äºä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹çš„æ’ä»¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–é€‚åº”éœ€è¦ä¸åŒç±»å‹æ€è€ƒçš„ä»»åŠ¡çš„æ¨ç†é…ç½®ã€‚AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œç»“åˆåˆ†è§£çš„åŠ¨ä½œç©ºé—´å’Œæœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥ï¼Œä»¥åŠé¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹è¿›è¡Œæ¨ç†é…ç½®ï¼Œåªéœ€å°‘æ•°é•œå¤´æŒ‡å¯¼å³å¯ã€‚AdaReasoneræœ‰ç†è®ºä¿è¯å’Œå®éªŒæ”¯æŒå¿«é€Ÿæ”¶æ•›å’Œæ¬¡çº¿æ€§ç­–ç•¥å·®è·ã€‚åœ¨å…­ä¸ªä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç§æ¨ç†ä»»åŠ¡ä¸Šï¼Œå®ƒå§‹ç»ˆè¶…è¶Šæ ‡å‡†åŸºçº¿ï¼Œä¿æŒè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„ç¨³å¥æ€§ï¼Œå¹¶é€šè¿‡é‡èº«å®šåˆ¶çš„æç¤ºåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šå–å¾—æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17312v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é€‚åº”ä¸åŒä»»åŠ¡éœ€æ±‚çš„LLMé…ç½®ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰é€šç”¨é…ç½®éš¾ä»¥è¾¾åˆ°ä»»åŠ¡ç‰¹å®šæœ€ä¼˜çš„é—®é¢˜ï¼Œæå‡ºAdaReasoneræ’ä»¶ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥å®ç°è‡ªé€‚åº”è°ƒæ•´LLMé…ç½®çš„ç›®æ ‡ã€‚é€šè¿‡å°‘æ ·æœ¬å¼•å¯¼è¿›è¡Œä¼˜åŒ–ï¼Œèƒ½åœ¨ä¸åŒç±»å‹ä»»åŠ¡ä¸­å–å¾—è‰¯å¥½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³çš„ä»»åŠ¡æ—¶ï¼Œéœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œå¦‚æ¸©åº¦å’Œæ¨ç†æ­¥éª¤ã€‚</li>
<li>ç°æœ‰æç¤ºæ–¹æ³•é€šå¸¸é‡‡ç”¨é€šç”¨é…ç½®ï¼Œéš¾ä»¥è¾¾åˆ°ä»»åŠ¡ç‰¹å®šæœ€ä¼˜ã€‚</li>
<li>AdaReasoneræ˜¯ä¸€ä¸ªé’ˆå¯¹ä»»ä½•LLMçš„æ’ä»¶ï¼Œç”¨äºè‡ªåŠ¨åŒ–é€‚åº”ä»»åŠ¡éœ€æ±‚çš„æ¨ç†é…ç½®ã€‚</li>
<li>AdaReasoneré‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒï¼Œç»“åˆåŠ¨ä½œç©ºé—´åˆ†è§£ã€ç›®æ ‡æ¢ç´¢ç­–ç•¥å’Œé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>AdaReasoneræœ‰ç†è®ºä¿éšœï¼Œå®éªŒè¯æ˜å…¶å¿«é€Ÿæ”¶æ•›å’Œç­–ç•¥å·®è·å‘ˆäºšçº¿æ€§ã€‚</li>
<li>åœ¨å…­ä¸ªä¸åŒçš„LLMså’Œå„ç§æ¨ç†ä»»åŠ¡ä¸Šï¼ŒAdaReasonerè¡¨ç°ä¼˜äºæ ‡å‡†åŸºçº¿ï¼Œå¹¶å…·æœ‰å¥å£®æ€§å’Œé’ˆå¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å¢ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83829af96ee43c7c4f3b89321848973e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25f1c2a9bd20e546943ad9e5a3952244.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="On-the-Robustness-of-Medical-Vision-Language-Models-Are-they-Truly-Generalizable"><a href="#On-the-Robustness-of-Medical-Vision-Language-Models-Are-they-Truly-Generalizable" class="headerlink" title="On the Robustness of Medical Vision-Language Models: Are they Truly   Generalizable?"></a>On the Robustness of Medical Vision-Language Models: Are they Truly   Generalizable?</h2><p><strong>Authors:Raza Imam, Rufael Marew, Mohammad Yaqub</strong></p>
<p>Medical Vision-Language Models (MVLMs) have achieved par excellence generalization in medical image analysis, yet their performance under noisy, corrupted conditions remains largely untested. Clinical imaging is inherently susceptible to acquisition artifacts and noise; however, existing evaluations predominantly assess generally clean datasets, overlooking robustness â€“ i.e., the modelâ€™s ability to perform under real-world distortions. To address this gap, we first introduce MediMeta-C, a corruption benchmark that systematically applies several perturbations across multiple medical imaging datasets. Combined with MedMNIST-C, this establishes a comprehensive robustness evaluation framework for MVLMs. We further propose RobustMedCLIP, a visual encoder adaptation of a pretrained MVLM that incorporates few-shot tuning to enhance resilience against corruptions. Through extensive experiments, we benchmark 5 major MVLMs across 5 medical imaging modalities, revealing that existing models exhibit severe degradation under corruption and struggle with domain-modality tradeoffs. Our findings highlight the necessity of diverse training and robust adaptation strategies, demonstrating that efficient low-rank adaptation when paired with few-shot tuning, improves robustness while preserving generalization across modalities. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMVLMï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å·²ç»è¾¾åˆ°äº†å“è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œä½†åœ¨å™ªå£°å’ŒæŸåæ¡ä»¶ä¸‹çš„æ€§èƒ½ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†çš„æµ‹è¯•ã€‚ä¸´åºŠæˆåƒæœ¬è´¨ä¸Šå®¹æ˜“å—åˆ°é‡‡é›†ä¼ªå½±å’Œå™ªå£°çš„å½±å“ï¼›ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦ä¾§é‡äºå¯¹é€šå¸¸å¹²å‡€æ•°æ®é›†çš„è¯„ä»·ï¼Œå¿½ç•¥äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå³æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­å¤±çœŸæƒ…å†µä¸‹çš„æ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†MediMeta-Cï¼Œè¿™æ˜¯ä¸€ä¸ªè…è´¥åŸºå‡†æµ‹è¯•ï¼Œå®ƒç³»ç»Ÿåœ°åº”ç”¨äºå¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†çš„å„ç§æ‰°åŠ¨ã€‚ç»“åˆMedMNIST-Cï¼Œè¿™ä¸ºMVLMå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„ç¨³å¥æ€§è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†RobustMedCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰ç¼–ç å™¨é€‚åº”çš„é¢„è®­ç»ƒMVLMï¼Œå®ƒç»“åˆäº†å°‘é‡çš„è°ƒæ•´æ¥å¢å¼ºå¯¹æŸåçš„æŠµæŠ—åŠ›ã€‚é€šè¿‡å¤§é‡çš„å®éªŒï¼Œæˆ‘ä»¬å¯¹äº”å¤§åŒ»å­¦æˆåƒæ¨¡æ€çš„äº”å¤§MVLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨å—åˆ°è…èš€æ—¶ä¼šå‡ºç°ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸæ¨¡æ€è½¬æ¢æ–¹é¢é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†å¤šæ ·è®­ç»ƒå’Œç¨³å¥é€‚åº”ç­–ç•¥çš„å¿…è¦æ€§ï¼Œæ˜¾ç¤ºå½“ä¸å°‘é‡è°ƒæ•´ç›¸ç»“åˆæ—¶ï¼Œæœ‰æ•ˆçš„ä½ç§©é€‚åº”èƒ½æé«˜ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿ç•™è·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15425v2">PDF</a> Dataset and Code is available at   <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/RobustMedCLIP">https://github.com/BioMedIA-MBZUAI/RobustMedCLIP</a> Accepted at: Medical Image   Understanding and Analysis (MIUA) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMVLMsï¼‰åœ¨å™ªå£°ã€å¤±çœŸç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚ä¸ºè¯„ä¼°æ¨¡å‹åœ¨å®é™…æ‰­æ›²æƒ…å†µä¸‹çš„ç¨³å¥æ€§ï¼Œæ–‡ç« å¼•å…¥äº†MediMeta-CåŸºå‡†æµ‹è¯•ï¼Œå¹¶ç»“åˆMedMNIST-Cå»ºç«‹ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚åŒæ—¶ï¼Œæå‡ºäº†RobustMedCLIPæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„MVLMçš„è§†è§‰ç¼–ç å™¨ï¼Œå¢å¼ºå…¶å¯¹æŠ—å¹²æ‰°çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨è…èš€ç¯å¢ƒä¸‹æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œä¸”å­˜åœ¨åŸŸæ¨¡æ€æƒè¡¡é—®é¢˜ã€‚æ–‡ç« å¼ºè°ƒå¤šæ ·è®­ç»ƒå’Œç¨³å¥é€‚åº”ç­–ç•¥çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºä½é˜¶é€‚åº”ç»“åˆå°‘é‡è°ƒä¼˜å¯æœ‰æ•ˆæé«˜ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒè·¨æ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMVLMsï¼‰åœ¨å™ªå£°ã€å¤±çœŸç¯å¢ƒä¸‹çš„æ€§èƒ½å°šå¾…æµ‹è¯•ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨æ¸…æ´æ•°æ®é›†ï¼Œå¿½ç•¥äº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼•å…¥MediMeta-CåŸºå‡†æµ‹è¯•å¹¶å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°MVLMçš„ç¨³å¥æ€§ã€‚</li>
<li>æå‡ºRobustMedCLIPæ–¹æ³•ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒMVLMçš„è§†è§‰ç¼–ç å™¨æ¥æé«˜å…¶å¯¹æŠ—å¹²æ‰°çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºç°æœ‰æ¨¡å‹åœ¨è…èš€ç¯å¢ƒä¸‹æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œå­˜åœ¨åŸŸæ¨¡æ€æƒè¡¡é—®é¢˜ã€‚</li>
<li>å¤šæ ·è®­ç»ƒå’Œç¨³å¥é€‚åº”ç­–ç•¥çš„é‡è¦æ€§è¢«å¼ºè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83e6c290f40b42850d1562ea8dffc1a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eed8e07b65c1ba423b97fb1a37aac847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37de556265d7b3bb6796091b74386055.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdde8173f435948cf26afa4f2bb6a597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b8b98615086f88163b093fb821de12a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs"><a href="#HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs" class="headerlink" title="HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs"></a>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs</h2><p><strong>Authors:Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen</strong></p>
<p>An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªè‡´å‘½å¼±ç‚¹æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿéäº‹å®æ€§çš„é™ˆè¿°ã€‚ç”±äº‹å®å’Œè™šæ„é™ˆè¿°ç»„æˆçš„å›åº”ç»™äººç±»å¸¦æ¥äº†éªŒè¯å’Œå‡†ç¡®åšå‡ºå†³ç­–çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†çªå‡ºé‡ç‚¹æ€è€ƒæç¤ºï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºLLMç”Ÿæˆå¸¦æœ‰XMLæ ‡ç­¾çš„å“åº”çš„æ–¹æ³•ï¼Œè¿™äº›æ ‡ç­¾å°†äº‹å®ä¾æ®ä¸æŸ¥è¯¢ä¸­æä¾›çš„äº‹å®ç›¸ç»“åˆã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥é—®é¢˜ï¼ŒLLMä¼šé¦–å…ˆé‡æ–°æ ¼å¼åŒ–é—®é¢˜ï¼Œæ·»åŠ çªå‡ºå…³é”®äº‹å®çš„XMLæ ‡ç­¾ï¼Œç„¶åç”ŸæˆåŒ…å«ä»è¾“å…¥ä¸­å¼•ç”¨çš„é‡ç‚¹äº‹å®çš„å›åº”ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†ç­‰17é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæ™®é€šçš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚å½“è¦æ±‚äººç±»éªŒè¯LLMçš„å›åº”æ—¶ï¼Œé‡ç‚¹æç¤ºæœ‰åŠ©äºæ—¶é—´æœ‰é™çš„å‚ä¸è€…æ›´å‡†ç¡®ã€é«˜æ•ˆåœ°è¯†åˆ«LLMçš„æ­£ç¡®æ€§ã€‚ç„¶è€Œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå½“LLMé”™è¯¯æ—¶ï¼ŒHoTå¾€å¾€ä½¿ç”¨æˆ·è®¤ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02003v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿéäº‹å®æ€§çš„é™ˆè¿°ï¼Œè¿™ä½¿å¾—å®ƒä»¬çš„å›åº”éš¾ä»¥éªŒè¯å¹¶å‡†ç¡®ä½œä¸ºå†³ç­–ä¾æ®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰XMLæ ‡ç­¾çš„â€œé«˜äº®æ€ç»´é“¾æç¤ºâ€ï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½ä½¿LLMåœ¨å›åº”æ—¶åŸºäºæŸ¥è¯¢ä¸­çš„äº‹å®è¿›è¡Œå›åº”ã€‚åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†çš„17é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºç¡€æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚äººç±»éªŒè¯è€…ä½¿ç”¨é«˜äº®æç¤ºå¯ä»¥æ›´å‡†ç¡®å¿«é€Ÿåœ°åˆ¤æ–­LLMçš„æ­£ç¡®æ€§ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œå½“LLMé”™è¯¯æ—¶ï¼ŒHoTå¾€å¾€ä½¿ç”¨æˆ·è®¤ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨ç”Ÿæˆéäº‹å®æ€§é™ˆè¿°çš„é—®é¢˜ã€‚</li>
<li>Highlighted Chain-of-Thought Promptingï¼ˆHoTï¼‰æŠ€æœ¯é€šè¿‡æ·»åŠ XMLæ ‡ç­¾æ¥å¼•å¯¼LLMåŸºäºäº‹å®è¿›è¡Œå›åº”ã€‚</li>
<li>åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒHoTåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºç¡€æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚</li>
<li>é«˜äº®æç¤ºæœ‰åŠ©äºäººç±»éªŒè¯è€…æ›´å‡†ç¡®å¿«é€Ÿåœ°åˆ¤æ–­LLMçš„æ­£ç¡®æ€§ã€‚</li>
<li>HoTæŠ€æœ¯å¯ä»¥æé«˜LLMå›åº”çš„é€æ˜åº¦ï¼Œä½¿å…¶æ›´æ˜“äºç†è§£ã€‚</li>
<li>å½“LLMç»™å‡ºé”™è¯¯å›åº”æ—¶ï¼ŒHoTæŠ€æœ¯å¯èƒ½ä¼šä½¿ç”¨æˆ·äº§ç”Ÿè¯¯è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-46ee7422215c9421ce8ea974265bfc3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f91ecb309dfd0fe21432eddee402e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8475102ba7cc3184cf0fffcdf6ab2724.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bc37f96774bf4140d12418dacc43844.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be6845bca8effcb1da2fa0efd1248eb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning"><a href="#Defending-Multimodal-Backdoored-Models-by-Repulsive-Visual-Prompt-Tuning" class="headerlink" title="Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning"></a>Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning</h2><p><strong>Authors:Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng</strong></p>
<p>Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, while they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we reveal that CLIPâ€™s vulnerabilities primarily stem from its tendency to encode features beyond in-dataset predictive patterns, compromising its visual feature resistivity to input perturbations. This makes its encoded features highly susceptible to being reshaped by backdoor triggers. To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs deep visual prompt tuning with a specially designed feature-repelling loss. Specifically, RVPT adversarially repels the encoded features from deeper layers while optimizing the standard cross-entropy loss, ensuring that only predictive features in downstream tasks are encoded, thereby enhancing CLIPâ€™s visual feature resistivity against input perturbations and mitigating its susceptibility to backdoor attacks. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27% of the parameters in CLIP, yet it significantly outperforms state-of-the-art defense methods, reducing the attack success rate from 89.70% to 2.76% against the most advanced multimodal attacks on ImageNet and effectively generalizes its defensive capabilities across multiple datasets. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰å¯ä»¥ä»å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸­å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºï¼Œä½†å®ƒä»¬å¯¹åé—¨æ”»å‡»è¡¨ç°å‡ºæ˜¾è‘—è„†å¼±æ€§ï¼Œå¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºCLIPçš„è„†å¼±æ€§ä¸»è¦æºäºå…¶ç¼–ç æ•°æ®é›†å†…é¢„æµ‹æ¨¡å¼ä»¥å¤–ç‰¹å¾çš„å€¾å‘ï¼Œè¿™æŸå®³äº†å…¶å¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾æŠ—æ€§ã€‚è¿™ä½¿å¾—å…¶ç¼–ç çš„ç‰¹å¾å¾ˆå®¹æ˜“è¢«åé—¨è§¦å‘ä¿¡å·é‡å¡‘ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºRepulsive Visual Prompt Tuningï¼ˆRVPTï¼‰çš„æ–°å‹é˜²å¾¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨æ·±åº¦è§†è§‰æç¤ºè°ƒèŠ‚ä¸ç‰¹æ®Šè®¾è®¡çš„ç‰¹å¾æ’æ–¥æŸå¤±ç›¸ç»“åˆã€‚å…·ä½“è€Œè¨€ï¼ŒRVPTåœ¨ä¼˜åŒ–æ ‡å‡†äº¤å‰ç†µæŸå¤±çš„åŒæ—¶ï¼Œå¯¹æŠ—æ€§åœ°æ’æ–¥æ·±å±‚ç¼–ç ç‰¹å¾ï¼Œä»¥ç¡®ä¿ä»…å¯¹ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é¢„æµ‹ç‰¹å¾è¿›è¡Œç¼–ç ï¼Œä»è€Œæé«˜CLIPå¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾æŠ—æ€§ï¼Œå¹¶å‡è½»å…¶å¯¹åé—¨æ”»å‡»çš„æ•æ„Ÿæ€§ã€‚ä¸é€šå¸¸éœ€è¦ä¸­æ¯’æ•°æ®æˆ–æ¶‰åŠå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„å¤šæ¨¡æ€åé—¨é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒRVPTåˆ©ç”¨ä¸‹æ¸¸çš„å°‘é‡æ¸…æ´æ ·æœ¬ï¼Œå¹¶ä¸”ä»…è°ƒæ•´å°‘é‡å‚æ•°ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒRVPTä»…è°ƒæ•´CLIPçš„0.27%å‚æ•°ï¼Œå³å¯æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„é˜²å¾¡æ–¹æ³•ï¼Œå°†æ”»å‡»æˆåŠŸç‡ä»89.70%é™ä½åˆ°2.76%ï¼Œå¯¹æŠ—ImageNetä¸Šæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ”»å‡»ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆæ¨å¹¿å…¶é˜²å¾¡èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20392v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPç­‰å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹çš„å›¾åƒ-æ–‡æœ¬æ•°æ®é›†å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºæ—¶å­˜åœ¨ä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œå®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚æœ¬æ–‡æ­ç¤ºäº†CLIPçš„æ¼æ´ä¸»è¦æºäºå…¶ç¼–ç è¶…å‡ºæ•°æ®é›†é¢„æµ‹æ¨¡å¼ç‰¹å¾çš„è¶‹åŠ¿ï¼Œå¯¼è‡´å…¶è§†è§‰ç‰¹å¾æŠµæŠ—è¾“å…¥æ‰°åŠ¨çš„èƒ½åŠ›å—æŸã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRepulsive Visual Prompt Tuningï¼ˆRVPTï¼‰çš„æ–°å‹é˜²å¾¡æ–¹æ³•ï¼Œé€šè¿‡æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹æ®Šè®¾è®¡çš„ç‰¹å¾æ’æ–¥æŸå¤±æ¥å¯¹æŠ—è¿™ä¸€é—®é¢˜ã€‚RVPTåœ¨ä¼˜åŒ–æ ‡å‡†äº¤å‰ç†µæŸå¤±çš„åŒæ—¶ï¼Œå¯¹æŠ—æ€§åœ°æ’æ–¥ç¼–ç ç‰¹å¾ï¼Œç¡®ä¿ä»…ç¼–ç ä¸‹æ¸¸ä»»åŠ¡çš„é¢„æµ‹ç‰¹å¾ï¼Œä»è€Œæé«˜CLIPå¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾æŠµæŠ—åŠ›ï¼Œå¹¶å‡è½»å…¶å—åˆ°åé—¨æ”»å‡»çš„å½±å“ã€‚ä¸å…¶ä»–å¤šæ¨¡æ€åé—¨é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒRVPTä»…ä½¿ç”¨å°‘é‡ä¸‹æ¸¸æ¸…æ´æ ·æœ¬ï¼Œå¹¶ä¸”åªè°ƒæ•´å°‘é‡å‚æ•°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒRVPTä»…è°ƒæ•´CLIPçš„0.27%å‚æ•°ï¼Œå³å¯æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡æ–¹æ³•ï¼Œå°†æ”»å‡»æˆåŠŸç‡ä»89.70%é™ä½åˆ°2.76%ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆæ¨å¹¿å…¶é˜²å¾¡èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰èƒ½ä»å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸­å­¦ä¹ é«˜è´¨é‡è¡¨ç¤ºï¼Œä½†å­˜åœ¨ä¸¥é‡çš„å®‰å…¨æ¼æ´ï¼Œå®¹æ˜“å—åˆ°åé—¨æ”»å‡»å½±å“ã€‚</li>
<li>CLIPçš„æ¼æ´ä¸»è¦æºäºå…¶ç¼–ç è¶…å‡ºæ•°æ®é›†é¢„æµ‹æ¨¡å¼ç‰¹å¾çš„è¶‹åŠ¿ï¼Œä½¿å…¶å¯¹è¾“å…¥æ‰°åŠ¨å’Œåé—¨æ”»å‡»çš„æŠµæŠ—åŠ›é™ä½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†RVPTæ–¹æ³•ï¼Œé€šè¿‡æ·±åº¦è§†è§‰æç¤ºè°ƒæ•´å’Œç‰¹å¾æ’æ–¥æŸå¤±æ¥æé«˜CLIPå¯¹è¾“å…¥æ‰°åŠ¨çš„è§†è§‰ç‰¹å¾æŠµæŠ—åŠ›ã€‚</li>
<li>RVPTä¼˜åŒ–æ ‡å‡†äº¤å‰ç†µæŸå¤±çš„åŒæ—¶ï¼Œå¯¹æŠ—æ€§åœ°æ’æ–¥ç¼–ç ç‰¹å¾ï¼Œç¡®ä¿ä»…ç¼–ç ä¸‹æ¸¸ä»»åŠ¡çš„é¢„æµ‹ç‰¹å¾ã€‚</li>
<li>RVPTæ–¹æ³•ä»…éœ€ä½¿ç”¨å°‘é‡ä¸‹æ¸¸æ¸…æ´æ ·æœ¬ï¼Œä¸”ä»…è°ƒæ•´æ¨¡å‹å°‘é‡å‚æ•°ï¼Œå³å¯æ˜¾è‘—æé«˜æ¨¡å‹çš„é˜²å¾¡èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRVPTæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆé™ä½æ”»å‡»æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-519783e73c9eca8dffc7dcf6c6d47dfd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47575e65417be877a7a53a4593550957.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53331bec1d434f3b1f60f689cb1ec1e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ece64a5266c1b6a205ace96fc63d46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c52271556565cd48e3a834603f73ff.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-27/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4175e7009052f540131f0ff244ddc1e9.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  FullFront Benchmarking MLLMs Across the Full Front-End Engineering   Workflow
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-27/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c5c6539abdcc4acee92e22ef3dbeaa45.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-27  ManuSearch Democratizing Deep Search in Large Language Models with a   Transparent and Open Multi-Agent Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
