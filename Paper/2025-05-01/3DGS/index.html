<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-05-01  EfficientHuman Efficient Training and Reconstruction of Moving Human   using Articulated 2D Gaussian">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b265422595e4b60ba9e4fde1fb783b57.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    24 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-01-更新"><a href="#2025-05-01-更新" class="headerlink" title="2025-05-01 更新"></a>2025-05-01 更新</h1><h2 id="EfficientHuman-Efficient-Training-and-Reconstruction-of-Moving-Human-using-Articulated-2D-Gaussian"><a href="#EfficientHuman-Efficient-Training-and-Reconstruction-of-Moving-Human-using-Articulated-2D-Gaussian" class="headerlink" title="EfficientHuman: Efficient Training and Reconstruction of Moving Human   using Articulated 2D Gaussian"></a>EfficientHuman: Efficient Training and Reconstruction of Moving Human   using Articulated 2D Gaussian</h2><p><strong>Authors:Hao Tian, Rui Liu, Wen Shen, Yilong Hu, Zhihao Zheng, Xiaolin Qin</strong></p>
<p>3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model’s performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians. </p>
<blockquote>
<p>3D高斯插值（3DGS）已被认为是场景重建和新型视图合成中的一种前沿技术。最近利用3DGS重建3D人体的工作试图利用人体姿态的先验信息来提高渲染质量和训练速度。然而，由于多视图的不一致性和冗余的高斯，它难以有效地拟合动态表面平面。这种不一致性产生的原因在于高斯椭圆体无法准确表示动态物体的表面，这阻碍了动态人体的快速重建。同时，冗余高斯的大量存在意味着这些工作的训练时间对于快速拟合动态人体来说仍然不理想。为了解决这些问题，我们提出了EfficientHuman模型，该模型使用关节式二维高斯快速完成人体动态重建，同时确保高渲染质量。关键创新在于将高斯插值编码为规范空间中的关节式二维高斯曲面，然后通过线性混合蒙皮（LBS）将它们变换到姿态空间，以实现高效的姿态变换。与三维高斯不同，关节式二维高斯曲面可以快速适应动态人体，同时确保视图几何一致性。此外，我们还引入了一个姿态校准模块和一个LBS优化模块，以实现动态人体姿态的精确拟合，提高模型性能。在ZJU-MoCap数据集上的大量实验表明，EfficientHuman实现了快速的三维动态人体重建，平均时间不到一分钟，比当前最先进的方法快20秒，同时减少了冗余高斯的数量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20607v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>摘要</strong></p>
<p>3DGS技术在场景重建和新颖视角合成方面有着开创性的应用。针对利用3DGS技术重建3D人体的问题，现有研究试图利用人体姿态的先验信息来提高渲染质量和训练速度，但仍面临动态表面平面拟合困难的问题。本文提出EfficientHuman模型，通过采用关节式2D高斯技术快速完成动态人体重建，确保高质量渲染。关键创新在于将高斯splat编码为规范空间中的关节式2D高斯surfels，然后通过线性混合蒙皮（LBS）将其转换为姿态空间，以实现有效的姿态转换。与3D高斯相比，关节式2D高斯surfels能更快适应动态人体，同时确保视图一致性的几何结构。此外，还引入了姿态校准模块和LBS优化模块，以实现动态人体姿态的精确匹配，提高模型性能。在ZJU-MoCap数据集上的实验表明，EfficientHuman实现了快速的3D动态人体重建，平均时间不到一分钟，比当前最先进的方法快20秒，同时减少了冗余高斯的数量。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>3DGS技术在场景重建和新颖视角合成中具有开创性应用。</li>
<li>现有研究在利用3DGS技术重建3D人体时面临动态表面平面拟合困难的问题。</li>
<li>EfficientHuman模型通过采用关节式2D高斯技术快速适应动态人体，确保高质量渲染。</li>
<li>EfficientHuman将高斯splat编码为规范空间中的关节式2D高斯surfels，并通过线性混合蒙皮（LBS）转换为姿态空间。</li>
<li>姿态校准模块和LBS优化模块的实现提高了动态人体姿态的精确匹配和模型性能。</li>
<li>EfficientHuman在ZJU-MoCap数据集上的实验实现了快速的3D动态人体重建，平均时间少于一分钟，比现有方法更快速且有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20607">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c73e3c286e2186ca3332e09cd22dcdae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c5621d40ecded7ad80a8030ca783f6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bde3f63b903a7f7165c3b38931219cd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Creating-Your-Editable-3D-Photorealistic-Avatar-with-Tetrahedron-constrained-Gaussian-Splatting"><a href="#Creating-Your-Editable-3D-Photorealistic-Avatar-with-Tetrahedron-constrained-Gaussian-Splatting" class="headerlink" title="Creating Your Editable 3D Photorealistic Avatar with   Tetrahedron-constrained Gaussian Splatting"></a>Creating Your Editable 3D Photorealistic Avatar with   Tetrahedron-constrained Gaussian Splatting</h2><p><strong>Authors:Hanxi Liu, Yifang Men, Zhouhui Lian</strong></p>
<p>Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR&#x2F;VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars. </p>
<blockquote>
<p>个性化3D角色编辑因其用户友好性和在AR&#x2F;VR及虚拟试穿等领域的应用前景而备受关注。尽管前人对3D编辑的可行性进行了探索，但往往在生成视觉效果令人满意的成果方面面临挑战，可能是由于复杂重建场景下的几何和纹理混合优化导致表示学习不稳定。本文旨在提供一种普通用户可访问的解决方案，以创建可编辑的3D角色，具备精确区域定位、几何适应性和逼真的渲染效果。为了应对这一挑战，我们引入了一个精心设计的框架，将编辑过程分解为局部空间适应和真实外观学习，利用混合四面体约束的高斯涂绘（TetGS）作为底层表示。TetGS结合了四面体格子的可控显式结构与3D高斯涂绘的高精度渲染能力，并以分阶段渐进的方式进行优化，包括三个阶段：从现实世界单目视频中实例化3D角色，为TetGS初始化提供准确先验；局部空间适应具有明确分区的四面体，引导高斯内核的重分布；以及基于几何的粗糙到精细激活策略的外观生成。定性和定量实验均表明，我们的方法在生成逼真的3D可编辑角色方面的有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20403v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨个性化3D头像编辑的技术，提出了一种易于用户操作的解决方案，可创建具有精确区域定位、几何适应性和逼真渲染的3D头像。通过采用四面体约束的高斯喷绘技术，将编辑过程分解为空间适应和真实外观学习，并在三个阶段逐步优化，包括从真实世界的单目视频中实例化3D头像、局部空间适应以及基于几何的外观生成。实验证明该方法在生成逼真的3D可编辑头像方面的有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>个性化3D头像编辑具有用户友好性和广泛的应用前景，如AR&#x2F;VR和虚拟试穿。</li>
<li>现有研究在3D编辑中生成视觉上的满意结果方面存在挑战，可能由于复杂的重建场景中的几何和纹理混合优化表示学习的不稳定性。</li>
<li>引入了一种精心设计的框架，将编辑过程分解为局部空间适应和真实外观学习，采用四面体约束的高斯喷绘技术（TetGS）作为底层表示。</li>
<li>TetGS结合了四面体网格的可控显式结构与3D高斯喷绘的高精度渲染能力。</li>
<li>框架的优化包括三个阶段：从真实世界的单目视频中实例化3D头像、局部空间适应以及基于几何学的外观生成。</li>
<li>框架可为普通用户提供精确的3D头像编辑工具，具有精确区域定位、几何适应性和逼真渲染。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1f3780428c176320909ca5db9de41fcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5829b75bc7c326b2855350f082cfefc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0f1254730af6c5d518c0ae960080e25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db1b776067320c0c46da75dab2dfb2b7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sparse2DGS-Geometry-Prioritized-Gaussian-Splatting-for-Surface-Reconstruction-from-Sparse-Views"><a href="#Sparse2DGS-Geometry-Prioritized-Gaussian-Splatting-for-Surface-Reconstruction-from-Sparse-Views" class="headerlink" title="Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface   Reconstruction from Sparse Views"></a>Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface   Reconstruction from Sparse Views</h2><p><strong>Authors:Jiang Wu, Rui Li, Yu Zhu, Rong Guo, Jinqiu Sun, Yanning Zhang</strong></p>
<p>We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\times$ faster than the NeRF-based fine-tuning approach. </p>
<blockquote>
<p>我们提出了一种基于稀疏输入视角的表面重建的高斯拼贴方法。之前依赖于密集视角的方法在利用极端稀疏的运动结构点进行初始化时面临困难。虽然基于学习的多视角立体（MVS）提供了密集的3D点，但直接将其与高斯拼贴相结合会导致次优结果，这是由于稀疏视角几何优化的不适定性。我们提出了Sparse2DGS，这是一种以MVS初始化的高斯拼贴管道，用于实现完整而准确的重建。我们的关键见解是融入几何优先增强方案，实现在不适定条件下的直接和稳健的几何学习。Sparse2DGS在显著边界上优于现有方法，同时比基于NeRF的微调方法快两倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20378v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于高斯涂敷方法（Gaussian Splatting）的稀疏视角表面重建方法。针对以往依赖密集视角的方法对于稀疏结构从运动中的点难以进行初始化的问题，文章结合了学习基础的多元视角立体（MVS）技术，提出Sparse2DGS方法。该方法在具有几何优先增强方案的情况下，实现了在不良条件下的直接和稳健的几何学习。Sparse2DGS在性能上显著优于现有方法，并且相较于基于NeRF的微调方法速度提高了两倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了高斯涂敷方法（Gaussian Splatting）用于表面重建。</li>
<li>现有方法对于稀疏结构从运动中的点难以进行初始化的问题进行了讨论。</li>
<li>结合学习基础的多元视角立体（MVS）技术，提出Sparse2DGS方法来解决上述问题。</li>
<li>Sparse2DGS方法通过引入几何优先增强方案，实现了在不良条件下的直接和稳健的几何学习。</li>
<li>Sparse2DGS在性能上显著优于现有方法。</li>
<li>Sparse2DGS方法的处理速度比基于NeRF的微调方法提高了两倍。</li>
<li>文章提供了一种新的视角来解决表面重建问题，特别是在处理稀疏视角数据时具有显著优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b041d42903ec6cb75979b6d08c60a490.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f111e085fba8141cb9653eacdc5e847e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a065063a8da2c760da4f257c36e0f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd762443eb89a0d1b4a5cbc28e5455c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="IM-Portrait-Learning-3D-aware-Video-Diffusion-for-Photorealistic-Talking-Heads-from-Monocular-Videos"><a href="#IM-Portrait-Learning-3D-aware-Video-Diffusion-for-Photorealistic-Talking-Heads-from-Monocular-Videos" class="headerlink" title="IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic   Talking Heads from Monocular Videos"></a>IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic   Talking Heads from Monocular Videos</h2><p><strong>Authors:Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang</strong></p>
<p>We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data. </p>
<blockquote>
<p>我们提出了一种新型的基于三维感知扩散的方法，直接从单张身份图像和明确的控制信号（如表情）生成逼真的动态视频。我们的方法生成多平面图像（MPIs），确保几何一致性，使其成为理想的沉浸式观看体验，如虚拟现实头盔的双目视频。与通常需要单独阶段或联合优化来重建三维表示（如NeRF或三维高斯）的现有方法不同，我们的方法通过单个去噪过程直接生成最终输出，无需后处理步骤即可有效地渲染新颖视角。为了有效地从单目视频中学习，我们引入了一种训练机制，该机制可以在目标相机空间或参考相机空间中随机重建输出MPI。这种方法使模型能够同时学习清晰的图像细节和潜在的三维信息。大量实验证明了我们方法的有效性，即使在不需要明确的三维重建或高质量的多视角训练数据的情况下，也能达到具有竞争力的角色质量并具备新颖视角的渲染能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19165v2">PDF</a> CVPR2025; project page:   <a target="_blank" rel="noopener" href="https://y-u-a-n-l-i.github.io/projects/IM-Portrait/">https://y-u-a-n-l-i.github.io/projects/IM-Portrait/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的基于三维感知扩散的方法，该方法可从单一身份图像和明确的控制信号（如表情）生成逼真的动态视频图像。该方法生成多平面图像（MPIs），确保几何一致性，适用于虚拟现实头戴设备的沉浸式观看体验。与其他方法不同，我们的方法通过一个去噪过程直接生成最终输出，无需额外的重建阶段或联合优化，从而高效渲染新的视角。通过随机重建目标或参考相机空间中的输出MPI，模型能够同时学习清晰的图像细节和潜在的3D信息。实验证明，该方法在无需明确的3D重建或多视角高质量训练数据的情况下，仍能达到竞争性的角色形象质量和新颖的视角渲染能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新型的基于三维感知扩散的方法，用于从单一身份图像和控制信号生成动态视频图像。</li>
<li>生成的多平面图像（MPIs）确保了几何一致性，适用于虚拟现实等沉浸式观看体验。</li>
<li>方法通过单一去噪过程直接生成最终输出，简化了流程并提高了效率。</li>
<li>模型通过随机重建输出MPI来同时学习图像细节和潜在的三维信息，适应性更强。</li>
<li>方法在无需明确的3D重建或多视角高质量训练数据的情况下，仍能达到优秀的角色形象质量和新颖的视角渲染能力。</li>
<li>与其他方法相比，该方法具有更强的效率和效果，尤其在渲染新视角方面表现突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b265422595e4b60ba9e4fde1fb783b57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d1d8a2d3376eb8bcec01d470d25c2b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a3605fcb2c83be4794d477ce4c0b058.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37b87189576c03cf50993939fe028f40.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HI-SLAM2-Geometry-Aware-Gaussian-SLAM-for-Fast-Monocular-Scene-Reconstruction"><a href="#HI-SLAM2-Geometry-Aware-Gaussian-SLAM-for-Fast-Monocular-Scene-Reconstruction" class="headerlink" title="HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene   Reconstruction"></a>HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene   Reconstruction</h2><p><strong>Authors:Wei Zhang, Qing Cheng, David Skuddis, Niclas Zeller, Daniel Cremers, Norbert Haala</strong></p>
<p>We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at <a target="_blank" rel="noopener" href="https://hi-slam2.github.io/">https://hi-slam2.github.io/</a>. </p>
<blockquote>
<p>我们提出了HI-SLAM2，这是一个感知几何的高斯SLAM系统，它仅使用RGB输入即可实现快速准确的单目场景重建。现有的神经SLAM或基于3DGS的SLAM方法往往需要在渲染质量和几何精度之间进行权衡，我们的研究表明，两者可以同时通过仅使用RGB输入来实现。我们的方法的关键思想是通过结合易于获得的单目先验知识和基于学习的密集SLAM，增强几何估计的能力，然后使用3D高斯平铺作为我们的核心地图表示来有效地建模场景。在环路闭合时，我们的方法通过基于锚定的关键帧更新的高效姿态图捆绑调整和即时地图更新，确保在线全局一致性。此外，我们引入了一种基于网格的尺度对齐策略，以在先前深度中保持改进的尺度一致性，从而获得更精细的深度细节。我们在Replica、ScanNet和ScanNet++上进行了大量实验，证明了与现有神经SLAM方法相比的显著改进，甚至在重建和渲染质量方面超越了基于RGB-D的方法。项目页面和源代码将在[<a target="_blank" rel="noopener" href="https://hi-slam2.github.io/]%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://hi-slam2.github.io/]上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17982v2">PDF</a> Under review process</p>
<p><strong>Summary</strong></p>
<p>本文介绍了HI-SLAM2系统，这是一个几何感知的高斯SLAM系统，可通过仅使用RGB输入实现快速而精确的单目场景重建。该研究通过结合单目先验和基于学习的密集SLAM，使用3D高斯溅面作为核心地图表示来有效地建模场景，从而同时实现渲染质量和几何精度的平衡。HI-SLAM2通过高效的姿态图捆绑调整和即时地图更新，确保闭环时的在线全局一致性。此外，它还引入了一种基于网格的尺度对齐策略，以在先前深度中保持改进的尺度一致性，从而实现更精细的深度细节。在Replica、ScanNet和ScanNet++上的实验表明，该方法在重建和渲染质量方面均显著优于现有Neural SLAM方法，甚至超越了基于RGB-D的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HI-SLAM2是一个几何感知的高斯SLAM系统，能够实现快速且精确的单目场景重建。</li>
<li>该系统通过结合单目先验与基于学习的密集SLAM，提高了对几何估计的能力。</li>
<li>HI-SLAM2使用3D高斯溅面作为核心地图表示，以有效地建模场景。</li>
<li>在闭环时，HI-SLAM2通过高效的姿态图捆绑调整和基于锚定的关键帧更新，确保在线全局一致性。</li>
<li>引入了一种基于网格的尺度对齐策略，以改进先前深度中的尺度一致性，从而实现更精细的深度细节。</li>
<li>实验结果表明，HI-SLAM2在多个数据集上的重建和渲染质量均优于现有的Neural SLAM方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17982">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a271063aeb714890c28fb1fb297349d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f734b21c0bd9ce3fe3b53f8d3e07445a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfe2b3001ad3c371ccb24177a0fef8bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb06e593c804ce9c1b6149a07fd49765.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Generic-Objects-as-Pose-Probes-for-Few-shot-View-Synthesis"><a href="#Generic-Objects-as-Pose-Probes-for-Few-shot-View-Synthesis" class="headerlink" title="Generic Objects as Pose Probes for Few-shot View Synthesis"></a>Generic Objects as Pose Probes for Few-shot View Synthesis</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</strong></p>
<p>Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as “pose probes”. The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \href{<a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/%7D%7Bthis">https://zhirui-gao.github.io/PoseProbe.github.io/}{this</a> https URL} </p>
<blockquote>
<p>辐射场，包括NeRF和3D高斯分布，在高保真渲染和场景重建方面表现出巨大潜力，但它们需要大量的姿态图像作为输入。COLMAP常用于预处理以估计姿态，但需要大量特征匹配才能有效运行，对于特征稀疏、图像间基线大或输入图像数量有限的场景，它表现得较为困难。我们的目标是仅使用3到6张未经姿态预估的场景图像来解决少量视图NeRF重建问题。传统方法通常使用校准板，但在图像中并不常见。我们提出了一种利用常见物体的新颖想法，这些物体既可以在图像中也可以在现实生活中找到，作为“姿态探针”。探针物体通过SAM自动分割，其形状初始化为立方体。我们采用双分支体积渲染优化（对象NeRF和场景NeRF）来约束姿态优化并共同优化几何结构。具体来说，首先通过SDF表示的PnP匹配估计两个视图的物体姿态，作为初始姿态。PnP匹配仅需要几个特征，适用于特征稀疏的场景。然后逐步加入额外的视图，以细化先前视图的姿态。在实验方面，PoseProbe在多数据集上的姿态估计和新型视图合成方面都达到了最先进的性能。我们证明了其在少量视图和大基线场景中的有效性，这正是COLMAP所面临的问题。在消融实验中，使用场景中的不同物体可以获得相当的性能。我们的项目页面可访问于：<a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/">此https URL</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16690v4">PDF</a> Accepted by IEEE TCSVT 2025 Project page:   <a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/">https://zhirui-gao.github.io/PoseProbe.github.io/</a></p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-36780712ba775db95dab5711a104762c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed4311d491f3074bebb975ad7c962af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8799747c2350cd604ee4f88c8422968.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbc2a11938bcee9339085fde599a6093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26b9010d5df8be62ca74eac9b43dfae4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9d1d8a2d3376eb8bcec01d470d25c2b5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-01  Generate more than one child in your co-evolutionary semi-supervised   learning GAN
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a0f1254730af6c5d518c0ae960080e25.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-05-01  Creating Your Editable 3D Photorealistic Avatar with   Tetrahedron-constrained Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
