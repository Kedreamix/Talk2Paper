<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-05-01  AegisLLM Scaling Agentic Systems for Self-Reflective Defense in LLM   Security">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9cc9881c687a8d594277af0c2767a519.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-01-更新"><a href="#2025-05-01-更新" class="headerlink" title="2025-05-01 更新"></a>2025-05-01 更新</h1><h2 id="AegisLLM-Scaling-Agentic-Systems-for-Self-Reflective-Defense-in-LLM-Security"><a href="#AegisLLM-Scaling-Agentic-Systems-for-Self-Reflective-Defense-in-LLM-Security" class="headerlink" title="AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM   Security"></a>AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM   Security</h2><p><strong>Authors:Zikui Cai, Shayan Shabihi, Bang An, Zora Che, Brian R. Bartoldson, Bhavya Kailkhura, Tom Goldstein, Furong Huang</strong></p>
<p>We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zikuicai/aegisllm">https://github.com/zikuicai/aegisllm</a> </p>
<blockquote>
<p>我们介绍了AegisLLM，这是一种对抗敌对攻击和信息泄露的合作多智能体防御系统。在AegisLLM中，自主智能体的结构化工作流程——编排者、偏转器、响应者和评估者——协同工作，确保LLM输出的安全和合规性，同时通过提示优化随时间自我改进。我们表明，通过在测试时扩大智能体推理系统，并结合自动化提示优化（如DSPy），在不损害模型效用的前提下，可以显著提高稳健性。这种测试时防御能力使系统能够实时适应不断变化的攻击，而无需重新训练模型。在关键威胁场景的综合评估中，包括遗忘和越狱场景，都证明了AegisLLM的有效性。在WMDP遗忘基准测试中，AegisLLM仅使用20个训练样本和少于300次的LM调用就实现了近乎完美的遗忘效果。在越狱基准测试中，与基础模型相比，我们在StrongReject上实现了51%的改进，而在PHTest上的错误拒绝率仅为7.9%，相比之下，相似方法的错误拒绝率为18-55%。我们的结果强调了自适应智能体推理相对于静态防御的优势，确立了AegisLLM作为基于模型修改的传统方法的强大运行时替代方案。代码可在<a target="_blank" rel="noopener" href="https://github.com/zikuicai/aegisllm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zikuicai/aegisllm找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20965v1">PDF</a> ICLR 2025 Workshop BuildingTrust</p>
<p><strong>Summary</strong><br>AegisLLM是一个合作多智能体防御系统，旨在抵御对抗性攻击和信息泄露。通过自主智能体（如协调器、偏转器、响应器和评估器）的结构化工作流程，确保安全合规的LLM输出。通过增加智能体角色和自动化提示优化，提高了测试时的鲁棒性，无需重新训练模型即可适应不断发展的攻击。代码在<a href="#">GitHub地址</a>（注：未给出实际链接地址）。通过测试证明了该系统的有效性。关键威胁场景的全面评估，包括遗忘和越狱任务结果都显示了其卓越效果。本方法为自适应的智能体推理提供了新的视角，作为运行时替代传统方法的一种强大选择。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AegisLLM是一个合作多智能体防御系统，用于增强LLM的安全性。</li>
<li>通过智能体的结构化工作流程确保安全合规的LLM输出。</li>
<li>通过增加智能体角色和自动化提示优化，提高了测试时的鲁棒性。</li>
<li>该系统能够适应不断发展的攻击，无需重新训练模型。</li>
<li>系统具有强大的自我改进能力，可通过提示优化来实现自我完善。其通过在不同场景下进行测试证明了其有效性。</li>
<li>AegisLLM在遗忘和越狱任务上表现出卓越的效果，相较于基准模型有显著改进。相较于其他方法，其拒绝率较低。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bdd7cf0e0b310cdf4981d34b34b26ff3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0df34c6faee85e888f8d588d71c76cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7ef1e19cdc145099b25c9a4019a6382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9294e14fd22c081c3689da11c1010893.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73d1b22c96a148b092d13de5cc12e5fb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Modeling-AI-Human-Collaboration-as-a-Multi-Agent-Adaptation"><a href="#Modeling-AI-Human-Collaboration-as-a-Multi-Agent-Adaptation" class="headerlink" title="Modeling AI-Human Collaboration as a Multi-Agent Adaptation"></a>Modeling AI-Human Collaboration as a Multi-Agent Adaptation</h2><p><strong>Authors:Prothit Sen, Sai Mihir Jakkaraju</strong></p>
<p>We develop an agent-based simulation to formalize AI-human collaboration as a function of task structure, advancing a generalizable framework for strategic decision-making in organizations. Distinguishing between heuristic-based human adaptation and rule-based AI search, we model interactions across modular (parallel) and sequenced (interdependent) tasks using an NK model. Our results reveal that in modular tasks, AI often substitutes for humans - delivering higher payoffs unless human expertise is very high, and the AI search space is either narrowly focused or extremely broad. In sequenced tasks, interesting complementarities emerge. When an expert human initiates the search and AI subsequently refines it, aggregate performance is maximized. Conversely, when AI leads, excessive heuristic refinement by the human can reduce payoffs. We also show that even “hallucinatory” AI - lacking memory or structure - can improve outcomes when augmenting low-capability humans by helping escape local optima. These results yield a robust implication: the effectiveness of AI-human collaboration depends less on context or industry, and more on the underlying task structure. By elevating task decomposition as the central unit of analysis, our model provides a transferable lens for strategic decision-making involving humans and an agentic AI across diverse organizational settings. </p>
<blockquote>
<p>我们开发了一种基于代理的模拟方法，将人工智能与人类的协作形式化为任务结构的函数，为组织中的战略决策制定提供了一个通用的框架。我们区分了基于启发式的人类适应和基于规则的AI搜索之间的区别，使用NK模型对模块化（并行）和序列化（相互依赖）任务之间的交互进行建模。我们的结果表明，在模块化任务中，人工智能经常替代人类——除非人类的专业知识非常高，或者AI的搜索空间非常狭窄或极其广泛，否则人工智能会带来更高的回报。在序列化任务中，会出现有趣的互补现象。当专家人类启动搜索并且随后由人工智能进行细化时，总体性能会最大化。相反，当人工智能领先时，人类过多的启发式细化可能会降低回报。我们还表明，即使是“幻觉”人工智能——没有记忆或结构——当增强低能力的人类时，通过帮助逃离局部最优，也可以改善结果。这些结果产生了稳健的启示：人工智能与人类的协作的有效性取决于较少的上下文或行业，而更多地取决于基本任务结构。通过提升任务分解为分析的核心单位，我们的模型为涉及人类和智能体人工智能的战略决策制定提供了一个可转移的透镜，适用于各种组织环境。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20903v1">PDF</a> Manuscript under review for the Special Issue: ‘Can AI Do Strategy?’   at Strategy Science (May 1, 2025)</p>
<p><strong>Summary</strong><br>任务结构的分析是人工智能与人类协作的关键所在。本研究通过构建基于主体的模拟模型，探讨了不同任务结构下人工智能与人类的协作模式。模块化任务中人工智能常常取代人类的高效益部分，除非人类对专业要求极高，否则极易出现任务自动化完成的情况。而在序列任务中，当专家人类启动搜索后由人工智能进行精炼时，总体表现最佳。此外，即使缺乏记忆或结构的人工智能也能在特定情况下提高结果。总之，人工智能与人类的协作效果更多地取决于任务结构而非具体情境或行业背景。本研究为组织中的战略决策提供了新的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>基于主体模拟框架形式化了人工智能与人类的协作，这一协作被视为任务结构的函数。</li>
<li>在模块化任务中，人工智能可以替代人类的高效益部分，除非人类对专业要求极高，或AI搜索空间局限于狭窄领域。</li>
<li>在序列任务中，人机合作显示出更有趣的互补性特点，人类主导的初步搜索及后续的人工智能精炼将实现最优的协作表现。相反地，AI领导的过多启发式精细化可能降低合作收益。</li>
<li>缺乏记忆或结构的人工智能在特定情况下仍能改善结果，例如在辅助低能力人类避免局部最优解方面发挥作用。这表明即使是不完善的人工智能在某些条件下仍可以创造价值。  ​​<br>​​ * 模型的有效性更在于任务结构的性质，而并非协作发生的具体情境或行业背景。这一发现为跨不同组织环境的人工智能与人类协作提供了通用视角。  ​​</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd2f935c52104c75de94239393052c39.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ARCS-Agentic-Retrieval-Augmented-Code-Synthesis-with-Iterative-Refinement"><a href="#ARCS-Agentic-Retrieval-Augmented-Code-Synthesis-with-Iterative-Refinement" class="headerlink" title="ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative   Refinement"></a>ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative   Refinement</h2><p><strong>Authors:Manish Bhattarai, Miguel Cordova, Javier Santos, Dan O’Malley</strong></p>
<p>In supercomputing, efficient and optimized code generation is essential to leverage high-performance systems effectively. We propose Agentic Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate, robust, and efficient code generation, completion, and translation. ARCS integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) reasoning to systematically break down and iteratively refine complex programming tasks. An agent-based RAG mechanism retrieves relevant code snippets, while real-time execution feedback drives the synthesis of candidate solutions. This process is formalized as a state-action search tree optimization, balancing code correctness with editing efficiency. Evaluations on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly outperforms traditional prompting methods in translation and generation quality. By enabling scalable and precise code synthesis, ARCS offers transformative potential for automating and optimizing code development in supercomputing applications, enhancing computational resource utilization. </p>
<blockquote>
<p>在超级计算中，高效优化的代码生成是有效利用高性能系统的关键。我们提出使用Agentic检索增强代码合成（ARCS）这一先进框架，以实现精确、稳健和高效的代码生成、补全和翻译。ARCS将检索增强生成（RAG）与思维链（CoT）推理相结合，系统地分解和迭代优化复杂的编程任务。基于代理的RAG机制检索相关代码片段，而实时执行反馈驱动候选解决方案的合成。这一过程被形式化为状态-动作搜索树优化，在代码正确性与编辑效率之间取得平衡。在Geeks4Geeks和HumanEval基准测试上的评估表明，ARCS在翻译和生成质量方面显著优于传统提示方法。通过实现可伸缩和精确的代码合成，ARCS为超级计算应用程序中的代码开发和自动化优化提供了变革性潜力，提高了计算资源的利用率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20434v1">PDF</a> </p>
<p><strong>Summary</strong><br>高性能计算中，高效优化的代码生成至关重要。我们提出一种名为Agentic检索增强代码合成（ARCS）的高级框架，用于准确、稳健和高效的代码生成、补全和翻译。ARCS将检索增强生成（RAG）与思维链（CoT）推理相结合，系统地分解和迭代优化复杂的编程任务。基于代理的RAG机制检索相关代码片段，而实时执行反馈驱动候选解决方案的合成。这一过程被形式化为状态动作搜索树优化，平衡代码正确性与编辑效率。在Geeks4Geeks和HumanEval基准测试上的评估表明，ARCS在翻译和生成质量方面显著优于传统提示方法。通过实现可伸缩和精确的代码合成，ARCS为自动化和优化高性能计算应用程序中的代码开发提供了变革性潜力，提高了计算资源的利用率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARCS是一个高级框架，用于高效、准确的代码生成、补全和翻译。</li>
<li>ARCS结合了检索增强生成（RAG）与思维链（CoT）推理。</li>
<li>RAG机制通过检索相关代码片段来辅助代码合成。</li>
<li>实时执行反馈用于驱动候选解决方案的合成和优化。</li>
<li>ARCS通过状态动作搜索树优化平衡代码正确性与编辑效率。</li>
<li>在Geeks4Geeks和HumanEval基准测试上，ARCS表现出优异的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20434">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-efa588dfb07bcde911bbb5ffd596be21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92181819bcbe18b18decb16fc7300d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84dafdf2f6d103b363ee7ebd8f509128.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MICE-for-CATs-Model-Internal-Confidence-Estimation-for-Calibrating-Agents-with-Tools"><a href="#MICE-for-CATs-Model-Internal-Confidence-Estimation-for-Calibrating-Agents-with-Tools" class="headerlink" title="MICE for CATs: Model-Internal Confidence Estimation for Calibrating   Agents with Tools"></a>MICE for CATs: Model-Internal Confidence Estimation for Calibrating   Agents with Tools</h2><p><strong>Authors:Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson</strong></p>
<p>Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer’s generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/mice_for_cats">https://github.com/microsoft/mice_for_cats</a>. </p>
<blockquote>
<p>在现实世界中行动的工具使用代理既需要实用又需要安全。经过良好校准的模型置信度可以用来权衡潜在行动的风险与回报，但先前的工作表明许多模型的校准情况并不理想。我们从探索模型内部的解释性文献中获得灵感，提出了一种新型的模型内部置信度估计器（MICE），以在调用工具时更好地评估置信度。MICE首先使用logitLens从语言模型的每个中间层进行解码，然后计算每个生成层与最终输出之间的相似度分数。这些特征被输入到一个经过学习的概率分类器中，以评估解码输出的置信度。在模拟试验和错误（STE）工具调用数据集上使用Llama3模型，我们发现MICE在平滑的预期校准误差上超过了基线或与之匹配。使用MICE置信度来决定是否调用工具，在新的预期工具调用效用指标上显著优于强大的基线。进一步的实验表明，MICE样本效率高，可以零样本泛化到看不见的API，并在不同风险级别的场景中导致更高的工具调用效用。我们的代码是开源的，可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/mice_for_cats%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/microsoft/mice_for_cats找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20168v1">PDF</a> Accepted at NAACL 2025. Code:   <a target="_blank" rel="noopener" href="https://github.com/microsoft/mice_for_cats">https://github.com/microsoft/mice_for_cats</a></p>
<p><strong>Summary</strong><br>模型内外置信度评估在工具调用决策中至关重要。为更好地评估模型置信度，提出一种新型模型内部置信度估计器（MICE）。通过从语言模型的各中间层进行解码并使用logitLens计算生成物与最终输出的相似度分数，再输入到概率分类器中评估解码输出的置信度。实验显示，MICE在工具调用数据集上表现优越，并能有效推广至未见API。此外，MICE能提高在不同风险水平场景下的工具调用效用，代码已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型内外置信度评估对工具调用决策至关重要。</li>
<li>为提高模型置信度评估，提出一种新型模型内部置信度估计器（MICE）。</li>
<li>MICE通过解码语言模型的各中间层并使用logitLens计算相似度分数来评估输出置信度。</li>
<li>在工具调用数据集上，MICE表现优越，且能推广至未见API。</li>
<li>使用MICE的置信度评估能提高工具调用效用。</li>
<li>MICE具有样本效率高的特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-deaa036069a353f0b8499af8b341b162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c906c24778a4e4e13ad2d8b7cd60f3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c8325553d79307261cb13c8de21a460.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-251f19f4d6db343f5f7c0af8313ef9fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35c08f0f23757584016b00af195d3912.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Based-Agents-via-Global-Planning-and-Hierarchical-Execution"><a href="#Enhancing-LLM-Based-Agents-via-Global-Planning-and-Hierarchical-Execution" class="headerlink" title="Enhancing LLM-Based Agents via Global Planning and Hierarchical   Execution"></a>Enhancing LLM-Based Agents via Global Planning and Hierarchical   Execution</h2><p><strong>Authors:Junjie Chen, Haitao Li, Jingli Yang, Yiqun Liu, Qingyao Ai</strong></p>
<p>Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents’ adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct’s potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/cjj826/GoalAct">https://github.com/cjj826/GoalAct</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的智能代理系统在实际应用中显示出巨大的潜力。然而，现有的代理框架在任务规划和执行方面仍面临关键限制，限制了其效果和通用性。具体来说，当前的规划方法往往缺乏明确的全球目标，导致代理陷入局部分支，或产生不可执行的计划。同时，现有的执行机制在平衡复杂性和稳定性方面面临困难，其有限的行动空间限制了处理各种实际任务的能力。为了解决这些限制，我们提出了GoalAct，这是一种新的代理框架，它引入了一种持续更新的全球规划机制，并集成了一种分层执行策略。GoalAct将任务执行分解为高级技能，包括搜索、编码、写作等，从而降低了规划复杂性，同时提高了代理在不同任务场景中的适应性。我们在LegalAgentBench上评估了GoalAct，这是一个包含多种法律任务类型的基准测试，需要使用多种工具。实验结果表明，GoalAct达到了最新技术水平（SOTA），成功率平均提高了12.22%。这些发现突出了GoalAct在推动更先进的智能代理系统发展中的潜力，使它们在复杂的实际应用中更加有效。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/cjj826/GoalAct">https://github.com/cjj826/GoalAct</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16563v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的智能代理系统在实际应用中显示出巨大潜力。然而，现有代理框架在任务规划和执行方面仍存在关键限制，限制了其有效性和通用性。为解决这些问题，提出了GoalAct这一新型代理框架，引入持续更新的全局规划机制并整合分层执行策略。GoalAct将任务执行分解为高级技能，包括搜索、编码、写作等，降低规划复杂性，提高代理在不同任务场景中的适应性。在LegalAgentBench基准测试上，GoalAct实现了卓越性能，平均成功率提高12.22%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>智能代理系统基于大型语言模型（LLM）在实际应用中具有巨大潜力。</li>
<li>现有代理框架在任务规划和执行方面存在限制，影响有效性和通用性。</li>
<li>GoalAct框架引入持续更新的全局规划机制，提高代理适应性。</li>
<li>GoalAct整合分层执行策略，将任务执行分解为高级技能，如搜索、编码和写作。</li>
<li>GoalAct在LegalAgentBench基准测试上实现卓越性能，平均成功率提高12.22%。</li>
<li>GoalAct具有潜力推动更先进的智能代理系统发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16563">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-684f0ffae90bce02fc050df302683a77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec794035fb78867986ebc3928f145f59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cc9881c687a8d594277af0c2767a519.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LocAgent-Graph-Guided-LLM-Agents-for-Code-Localization"><a href="#LocAgent-Graph-Guided-LLM-Agents-for-Code-Localization" class="headerlink" title="LocAgent: Graph-Guided LLM Agents for Code Localization"></a>LocAgent: Graph-Guided LLM Agents for Code Localization</h2><p><strong>Authors:Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang</strong></p>
<p>Code localization–identifying precisely where in a codebase changes need to be made–is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/LocAgent">https://github.com/gersteinlab/LocAgent</a>. </p>
<blockquote>
<p>代码定位——精确识别需要在代码库中做出更改的位置——是软件维护中的一项基本且具有挑战性的任务。现有方法在识别相关代码段时很难有效地遍历复杂的代码库。挑战在于将自然语言问题描述与适当的代码元素建立联系，通常需要跨越层次结构和多个依赖关系进行推理。我们引入了LocAgent，这是一个通过基于图表示来解决代码定位问题的框架。通过将代码库解析为定向异质图，LocAgent创建了一种轻量级的表示方法，能够捕获代码结构（文件、类、函数）及其依赖关系（导入、调用、继承），从而使大型语言模型（LLM）代理能够进行有效的多步推理，搜索并定位相关实体。在现实世界基准测试上的实验结果表明，我们的方法显著提高了代码定位的准确性。值得注意的是，我们的方法与经过fine-tuning的Qwen-2.5-Coder-Instruct-32B模型相结合，在成本大大降低（约降低86%）的情况下实现了与最新专有模型相当的结果，在文件级别定位上达到92.7%的准确性，并在多次尝试中提高了下游GitHub问题解决的成功率（Pass@10）达12%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/gersteinlab/LocAgent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gersteinlab/LocAgent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09089v2">PDF</a> </p>
<p><strong>Summary</strong><br>代码定位是软件维护中的基本且具挑战性的任务，要求精准地确定需要在代码库中做出更改的位置。现有方法在处理复杂的代码库时难以有效地导航并识别相关的代码片段。研究团队引入了LocAgent框架，通过基于图的方法来解决代码定位问题。它通过解析代码库并创建有向的异构图，以捕获代码结构（文件、类、函数）及其依赖关系（导入、调用、继承），使LLM代理能够有效地搜索并定位相关实体，进行强大的多跳推理。实验结果表明，该方法显著提高代码定位的准确性。特别地，使用fine-tuned Qwen-2.5-Coder-Instruct-32B模型的我们的方法在经济成本上大幅降低（约降低了86%），在文件级别的定位准确性上达到了92.7%，并且在多次尝试后提高了GitHub问题解决的成功率达12%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>代码定位是软件维护中的核心挑战，需要精确识别代码修改的位置。</li>
<li>现有方法在处理复杂代码库时存在导航困难，难以识别相关代码片段的问题。</li>
<li>LocAgent框架通过基于图的方法来解决代码定位问题，解析代码库为有向异构图。</li>
<li>LocAgent能够捕获代码结构及其依赖关系，使LLM代理能进行多跳推理以搜索和定位相关实体。</li>
<li>实验结果表明，LocAgent显著提高了代码定位的准确性。</li>
<li>使用fine-tuned Qwen-2.5-Coder-Instruct-32B模型的LocAgent在经济成本上大幅降低，并达到了较高的文件级别定位准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-48e5c6e09fe8e239da7a30f5083eaac2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-891c368cf18d1ae5a743aadccb8e4b5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7465383cc5e8f9d223d2b4d861ffbe6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c95779e38dc998fde48e525acba8df1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="An-LLM-Powered-Agent-for-Physiological-Data-Analysis-A-Case-Study-on-PPG-based-Heart-Rate-Estimation"><a href="#An-LLM-Powered-Agent-for-Physiological-Data-Analysis-A-Case-Study-on-PPG-based-Heart-Rate-Estimation" class="headerlink" title="An LLM-Powered Agent for Physiological Data Analysis: A Case Study on   PPG-based Heart Rate Estimation"></a>An LLM-Powered Agent for Physiological Data Analysis: A Case Study on   PPG-based Heart Rate Estimation</h2><p><strong>Authors:Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani</strong></p>
<p>Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs’ limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI’s GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent’s performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub. </p>
<blockquote>
<p>大型语言模型（LLM）正通过交互式通信改善诊断、患者护理和决策支持来推动医疗保健领域的革命。最近，它们被应用于分析生理时间序列，如可穿戴设备数据，以提取健康洞察。现有方法直接将原始数字序列嵌入提示中，这超过了令牌限制并增加了计算成本。此外，一些研究将时间序列的特征集成到文本提示中，或应用多模式方法。然而，这些方法往往产生通用且不可靠的输出，因为LLM在分析严谨性和解释连续波形方面的有限性和效率低下。在本文中，我们开发了一种用于生理时间序列分析的大型语言模型驱动代理，旨在弥合将LLM与成熟的分析工具相结合的差距。我们的代理建立在开源的大型语言模型驱动框架OpenCHA上，使用OpenAI的GPT-3.5 Turbo模型作为功能强大的引擎，配备一个协调器，可整合用户交互、数据源和分析工具，以生成准确的健康洞察。为了评估其有效性，我们在远程健康监测研究的数据集上实施了一项心率估算的案例研究，该数据集包含光体积描记器（PPG）信号的心电图（ECG）记录。代理的性能以心电图（ECG）作为心率估算的金标准，与OpenAI的GPT-4o Mini和GPT-4o进行了比较。结果表明，我们的代理在达到更低的错误率和更可靠的心率估算方面显著优于基准模型。代理实现已在GitHub上公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12836v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个基于大型语言模型（LLM）的代理，用于生理时间序列分析。该代理能够整合用户交互、数据源和分析工具，生成准确的健康洞察。在心率估算的案例中，该代理表现出显著的优势，相比其他模型有更低的误差率和更可靠的心率估算。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）正在医疗领域发挥重要作用，可用于改善诊断、患者护理和决策支持。</li>
<li>现有方法直接将原始数值序列嵌入提示中，导致超出令牌限制和计算成本增加。</li>
<li>LLM在解析连续波形方面存在局限性，导致输出的通用性和可靠性问题。</li>
<li>本文介绍了一个基于LLM的代理，旨在解决LLM与现有分析工具之间的整合差距。</li>
<li>该代理使用OpenCHA框架和OpenAI的GPT-3.5 Turbo模型构建，具备用户交互、数据源和分析工具整合功能。</li>
<li>在心率估算的案例中，该代理显著优于其他模型，具有更低的误差率和更高的可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c3c8b56ada4a1fd3db10c14498d57255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-225f724544aeb14e81992c72c8fdd18c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4344037c24c267abba21295799bb34ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1876da97ee06ccffc6c8ae4031f60f22.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EgoAgent-A-Joint-Predictive-Agent-Model-in-Egocentric-Worlds"><a href="#EgoAgent-A-Joint-Predictive-Agent-Model-in-Egocentric-Worlds" class="headerlink" title="EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds"></a>EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</h2><p><strong>Authors:Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng</strong></p>
<p>This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, which prevents them from learning from each other. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions within a single transformer. EgoAgent introduces two innovations to learn from the causal and temporally intertwined nature of these abilities: (1) Interleaved sequential modeling of states and actions with the causal attention mechanism, and (2) A joint embedding-action-prediction architecture featuring temporal asymmetric predictor-observer branches. Integrating these designs based on JEPA, EgoAgent unifies these capabilities in a cohesive learning framework. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility. </p>
<blockquote>
<p>本文旨在解决学习一种像人类一样行为的代理模型的任务，该模型可以在以自我为中心的世界中共同感知、预测和行动。以前的方法通常针对这三种能力分别进行建模，这阻止了它们之间的互相学习。在本文中，我们提出了一种联合预测代理模型，名为EgoAgent。它能够在单个变压器内同时学习表示世界、预测未来状态和采取合理行动。EgoAgent通过两项创新来学习这些能力的因果和时间交织的本质：（1）利用因果注意力机制的交错状态与行为的顺序建模；（2）具有时间不对称预测器观察分支的联合嵌入动作预测架构。基于JEPA整合这些设计，EgoAgent在一个协调统一的学习框架中融合了这些能力。在图像分类、以自我为中心的未来状态预测和3D人体运动预测等代表性任务上，对EgoAgent的综合评估证明了我们的方法优越性。我们的代码和训练好的模型将发布，以确保结果的可重复性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05857v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为EgoAgent的联合预测代理模型，该模型能够在单一转换器中同时学习表示世界、预测未来状态和采取合理行动。通过采用因果注意力机制和具有时间不对称预测器观察者分支的联合嵌入动作预测架构，EgoAgent实现了对感知、预测和行动能力的联合学习，并在图像分类、以自我为中心的未来状态预测和3D人类运动预测任务上表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EgoAgent是一个联合预测代理模型，能够同时学习表示世界、预测未来状态和采取合理行动。</li>
<li>该模型通过引入因果注意力机制和具有时间不对称预测器观察者分支的设计，实现了对感知、预测和行动能力的联合学习。</li>
<li>EgoAgent在图像分类、以自我为中心的未来状态预测和3D人类运动预测任务上表现出卓越性能。</li>
<li>模型设计基于JEPA，提供了一个连贯的学习框架。</li>
<li>代码和训练好的模型将被公开，以便于复制和进一步的研究。</li>
<li>EgoAgent通过同时训练感知、预测和行动能力，克服了以往分别训练这些能力的方法的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ecf97f668b689b9efc905c967708c8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7badf2e2ef4ddceb9a598c9b16464b59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a93c24b2eb90f215dc335867ed8ebf68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d65e65b0d7d05b8d9532f3279529c8dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Agentic-AI-The-Era-of-Semantic-Decoding"><a href="#Agentic-AI-The-Era-of-Semantic-Decoding" class="headerlink" title="Agentic AI: The Era of Semantic Decoding"></a>Agentic AI: The Era of Semantic Decoding</h2><p><strong>Authors:Maxime Peyrard, Martin Josifoski, Robert West</strong></p>
<p>Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation. </p>
<blockquote>
<p>最近的工作展示了在大规模语言模型（LLMs）、人类输入和各种工具之间协同合作以克服LLMs固有局限的构想具有巨大潜力。我们提出了一种新的视角，称为语义解码，它将这些协同过程描述为语义空间中的优化过程。具体来说，我们将LLMs概念化为语义处理器，它们处理我们称为语义令牌（已知思想）的具有意义的信息片段。LLMs是一组其他语义处理器中的一部分，包括人类和工具，如搜索引擎或代码执行器。集体来说，语义处理器参与语义令牌的动态交换，以逐步构建高实用性的输出。我们将这些语义处理器之间的协同交互，在语义空间中进行优化和搜索，称为语义解码算法。这个概念与已研究过的句法解码问题直接相对应，句法解码涉及设计算法以最佳方式利用自回归语言模型来提取高实用性的句法令牌序列。通过关注语义层面而忽视句法细节，我们对AI系统的工程构建有了全新的认识，使我们能够想象出具有更大复杂性和能力的系统。在本立场论文中，我们正式将句法令牌过渡到语义令牌以及句法解码和语义解码之间的类比。随后，我们通过语义解码算法探索在语义令牌空间内优化的可能性。我们总结了由此产生的新视角所带来的研究机会和问题。语义解码视角为直接在有意义的概念空间中进行搜索和优化提供了强大的抽象，而语义令牌作为新型计算的基本单位。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.14562v2">PDF</a> 25 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种新的观点，即语义解码，将LLMs与人类输入和各种工具的协作过程视为语义空间中的优化过程。作者将LLMs概念化为语义处理器，与其他语义处理器（如人类、搜索引擎或代码执行器）共同构成动态交换语义标记（即语义令牌），以逐步构建高实用性的输出。这些被称为语义解码算法，与句法解码问题形成对比。通过关注语义层面而忽视句法细节，作者提供了一个关于AI系统工程的全新视角，并探讨了通过语义解码算法在语义令牌空间内优化的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的观点——语义解码，将LLMs和其他语义处理器（如人类和工具）之间的协作视为优化过程。</li>
<li>LLMs被概念化为语义处理器，能够处理有意义的语义令牌。</li>
<li>动态交换的语义令牌在构建高实用性输出方面发挥着作用。</li>
<li>语义解码算法在优化和搜索语义空间中起着关键作用。</li>
<li>语义解码与句法解码形成对比，专注于语义层面而非句法细节。</li>
<li>语义解码提供了一个关于AI系统工程的全新视角，并探讨了优化语义令牌空间的可能性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.14562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b16b75929b4c2f95fc44df4851e5d079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24f1a4578b6fa38bff0c6922e2af5cc9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c30014b434d5cd9900c861baba8a5dc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TOP-Former-A-Multi-Agent-Transformer-Approach-for-the-Team-Orienteering-Problem"><a href="#TOP-Former-A-Multi-Agent-Transformer-Approach-for-the-Team-Orienteering-Problem" class="headerlink" title="TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering   Problem"></a>TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering   Problem</h2><p><strong>Authors:Daniel Fuertes, Carlos R. del-Blanco, Fernando Jaureguizar, Narciso García</strong></p>
<p>Route planning for a fleet of vehicles is an important task in applications such as package delivery, surveillance, or transportation, often integrated within larger Intelligent Transportation Systems (ITS). This problem is commonly formulated as a Vehicle Routing Problem (VRP) known as the Team Orienteering Problem (TOP). Existing solvers for this problem primarily rely on either linear programming, which provides accurate solutions but requires computation times that grow with the size of the problem, or heuristic methods, which typically find suboptimal solutions in a shorter time. In this paper, we introduce TOP-Former, a multi-agent route planning neural network designed to efficiently and accurately solve the Team Orienteering Problem. The proposed algorithm is based on a centralized Transformer neural network capable of learning to encode the scenario (modeled as a graph) and analyze the complete context of all agents to deliver fast, precise, and collaborative solutions. Unlike other neural network-based approaches that adopt a more local perspective, TOP-Former is trained to understand the global situation of the vehicle fleet and generate solutions that maximize long-term expected returns. Extensive experiments demonstrate that the presented system outperforms most state-of-the-art methods in terms of both accuracy and computation speed. </p>
<blockquote>
<p>车辆队伍的路线规划是在包裹配送、监控或运输等应用中一项重要的任务，通常被集成在更大的智能交通系统（ITS）中。这个问题通常被形式化为车辆路径问题（VRP），被称为团队定向问题（TOP）。针对这个问题的现有解决方案主要依赖于线性规划，线性规划提供准确的解决方案，但计算时间随着问题规模的增加而增长，或者启发式方法，通常在较短的时间内找到次优解。在本文中，我们介绍了TOP-Former，这是一个多智能体路线规划神经网络，旨在高效准确地解决团队定向问题。所提出的算法基于集中式Transformer神经网络，能够学习对场景进行编码（建模为图）并分析所有智能体的完整上下文，以提供快速、精确和协作的解决方案。与其他采用更局部视角的神经网络方法不同，TOP-Former经过训练，能够理解车辆队伍的全局情况，并生成最大化长期预期回报的解决方案。大量实验表明，所提出的系统在准确性和计算速度方面均优于大多数最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.18662v3">PDF</a> </p>
<p><strong>Summary</strong><br>车队路线规划是智能运输系统中的重要应用，如快递配送、监控和运输等。该问题通常被表述为团队定向问题（TOP），现有解法主要依赖于线性规划或启发式方法。本文提出了TOP-Former，一个基于多智能体的路线规划神经网络，能高效准确地解决团队定向问题。该算法基于集中式Transformer神经网络，能够学习场景建模并分析所有智能体的完整上下文，以快速准确地提供协同解决方案。实验证明，该系统在准确性和计算速度上均优于大多数最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>路线规划在智能运输系统中至关重要，如快递配送、监控和运输等应用。</li>
<li>团队定向问题（TOP）是车辆路线规划中的常见问题，现有解法存在计算时间长或解决方案不准确的问题。</li>
<li>TOP-Former是一个基于多智能体的路线规划神经网络，旨在解决团队定向问题。</li>
<li>TOP-Former采用集中式Transformer神经网络，能学习和分析场景中的所有智能体上下文。</li>
<li>TOP-Former具有快速、精确和协同解决问题的能力。</li>
<li>实验证明，TOP-Former在准确性和计算速度上均优于大多数最新方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.18662">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8a40ab1bd58d7e9cbcc6f139c4a2792.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0868b88664bf4d7a880a0716ffc46f57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03885c7eb3c9d794095d5a57b757b2ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786f3f9587e20349773283d0a4b1d962.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LLM-Coordination-Evaluating-and-Analyzing-Multi-agent-Coordination-Abilities-in-Large-Language-Models"><a href="#LLM-Coordination-Evaluating-and-Analyzing-Multi-agent-Coordination-Abilities-in-Large-Language-Models" class="headerlink" title="LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination   Abilities in Large Language Models"></a>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination   Abilities in Large Language Models</h2><p><strong>Authors:Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang</strong></p>
<p>Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners’ beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs’ Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at <a target="_blank" rel="noopener" href="https://github.com/eric-ai-lab/llm_coordination">https://github.com/eric-ai-lab/llm_coordination</a>. </p>
<blockquote>
<p>大型语言模型（LLM）已经展现出新兴的常识推理和心智理论（ToM）能力，使其成为开发协调代理的有前途的候选者。本研究介绍了LLM协调基准测试，这是一个新的基准测试，用于在纯协调环境的背景下分析LLM，在此环境中，代理必须相互合作以最大化收益。我们的基准测试通过两个独特的任务来评估LLM。第一个是代理协调，LLM在此作为四种纯协调游戏中的主动参与者。第二个是协调问答（CoordQA），对LLM进行198道选择题测试，以评估三项关键能力：环境理解、心智理论推理和联合规划。Agentic协调实验的结果表明，LLM代理在多代理协调环境中表现出色，其中决策主要依赖于环境变量，但在需要主动考虑合作伙伴的信念和意图的场景中面临挑战。CoordQA实验进一步凸显了LLM的心智理论推理和联合规划能力还有很大的提升空间。零射击协调（ZSC）在Agentic协调环境中的实验表明，LLM代理与RL方法不同，对未见过的伙伴具有稳健性。这些发现表明了LLM作为纯协调设置中的代理的潜力，并强调了改进的领域。代码可用在<a target="_blank" rel="noopener" href="https://github.com/eric-ai-lab/llm_coordination%E3%80%82">https://github.com/eric-ai-lab/llm_coordination。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03903v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）展现出潜在的常识推理和心智理论（ToM）能力，成为开发协调代理的候选者。本研究介绍了LLM协调基准测试，这是一个新的基准测试，用于分析LLMs在纯协调设置中的表现。该基准测试通过两个任务来评估LLMs：首先是代理协调，LLMs在四种纯协调游戏中充当积极参与者；其次是协调问答（CoordQA），测试LLMs在环境理解、心智理论推理和联合规划方面的能力。实验结果显示，LLM代理在多代理协调设置中表现出色，但在需要主动考虑合作伙伴信念和意图的场景中面临挑战。代码可在<a target="_blank" rel="noopener" href="https://github.com/eric-ai-lab/llm_coordination">链接</a>获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型展现出协调代理的潜力。</li>
<li>LLM协调基准测试是一个新的评估LLMs在纯协调设置中的表现的基准。</li>
<li>LLMs在代理协调任务中表现出色，特别是在环境变量决策方面。</li>
<li>LLMs在需要理解合作伙伴信念和意图的情境中面临挑战。</li>
<li>协调问答任务揭示了LLMs在理论思维推理和联合规划方面的不足。</li>
<li>与强化学习方法相比，LLM代理在零射击协调实验中表现出对未见合作伙伴的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.03903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c65065b9b83dad7f548be5163448c20c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c593f86d33be73126d68161a0505cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-449ea9446f467c735e9962923ff4efb1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9dd392ddc7a1947335d6e12dfd0a2924.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-01  AutoP2C An LLM-Based Agent Framework for Code Repository Generation   from Multimodal Content in Academic Papers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-01  X-Fusion Introducing New Modality to Frozen Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
