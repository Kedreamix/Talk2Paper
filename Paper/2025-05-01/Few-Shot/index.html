<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  YoChameleon Personalized Vision and Language Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-dda5a988aba9fe18096120cf4272e38a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-01-æ›´æ–°"><a href="#2025-05-01-æ›´æ–°" class="headerlink" title="2025-05-01 æ›´æ–°"></a>2025-05-01 æ›´æ–°</h1><h2 id="YoChameleon-Personalized-Vision-and-Language-Generation"><a href="#YoChameleon-Personalized-Vision-and-Language-Generation" class="headerlink" title="YoChameleon: Personalized Vision and Language Generation"></a>YoChameleon: Personalized Vision and Language Generation</h2><p><strong>Authors:Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li</strong></p>
<p>Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yoâ€™Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yoâ€™Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yoâ€™Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a &#96;&#96;soft-positiveâ€ image generation approach to enhance image quality in a few-shot setting. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4ã€Geminiã€Chameleonï¼‰å·²ç»è¿›åŒ–æˆæ‹¥æœ‰æ•°ç™¾ä¸‡ç”¨æˆ·çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶æ˜¯é€šç”¨æ¨¡å‹ï¼Œç¼ºä¹ç‰¹å®šç”¨æˆ·çš„ä¸ªæ€§åŒ–çŸ¥è¯†æ¦‚å¿µã€‚ä¹‹å‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†æ–‡æœ¬ç”Ÿæˆçš„ä¸ªæ€§åŒ–ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›æ–¹æ³•å¦‚ä½•é€‚åº”æ–°çš„æ¨¡å¼ï¼Œå¦‚å›¾åƒç”Ÿæˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Yoâ€™Chameleonï¼Œè¿™æ˜¯ç ”ç©¶å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸ªæ€§åŒ–çš„é¦–æ¬¡å°è¯•ã€‚ç»™å®šå…³äºç‰¹å®šæ¦‚å¿µçš„3-5å¼ å›¾åƒï¼ŒYoâ€™Chameleonåˆ©ç”¨è½¯æç¤ºè°ƒæ•´æŠ€æœ¯åµŒå…¥ç‰¹å®šä¸»é¢˜çš„ä¿¡æ¯ï¼Œä»¥ï¼ˆiï¼‰å›ç­”æœ‰å…³è¯¥ä¸»é¢˜çš„é—®é¢˜å’Œï¼ˆiiï¼‰é‡æ–°åˆ›å»ºåƒç´ çº§ç»†èŠ‚ï¼Œä»¥åœ¨æ–°ç¯å¢ƒä¸­ç”Ÿæˆè¯¥ä¸»é¢˜çš„å›¾ç‰‡ã€‚Yoâ€™Chameleoné€šè¿‡ï¼ˆiï¼‰ä¸€ç§è‡ªæˆ‘æç¤ºä¼˜åŒ–æœºåˆ¶æ¥å¹³è¡¡å¤šç§æ¨¡æ€çš„æ€§èƒ½è¡¨ç°ï¼Œä»¥åŠï¼ˆiiï¼‰ä¸€ç§â€œè½¯æ­£â€å›¾åƒç”Ÿæˆæ–¹æ³•æ¥å¢å¼ºå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20998v1">PDF</a> CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://thaoshibe.github.io/YoChameleon">https://thaoshibe.github.io/YoChameleon</a></p>
<p><strong>Summary</strong><br>     å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚GPT-4ã€Geminiã€Chameleonï¼‰å·²å‘å±•æˆä¸ºæ‹¥æœ‰æ•°ç™¾ä¸‡ç”¨æˆ·çš„å¼ºå¤§å·¥å…·ï¼Œä½†ä»ç¼ºä¹é’ˆå¯¹ç‰¹å®šç”¨æˆ·çš„ä¸ªæ€§åŒ–çŸ¥è¯†ã€‚æœ¬æ–‡ä»‹ç»Yoâ€™Chameleonï¼Œé¦–æ¬¡å°è¯•å¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–ç ”ç©¶ã€‚é€šè¿‡3-5å¼ ç‰¹å®šæ¦‚å¿µçš„å›¾ç‰‡ï¼ŒYoâ€™Chameleonåˆ©ç”¨è½¯æç¤ºè°ƒæ•´æŠ€æœ¯åµŒå…¥ä¸»é¢˜ç‰¹å®šä¿¡æ¯ï¼Œä»¥å›ç­”æœ‰å…³ä¸»é¢˜çš„é—®é¢˜å¹¶é‡å»ºåƒç´ çº§ç»†èŠ‚ï¼Œä»¥åœ¨æ–°èƒŒæ™¯ä¸‹ç”Ÿæˆä¸»é¢˜å›¾åƒã€‚Yoâ€™Chameleoné€šè¿‡è‡ªæˆ‘æç¤ºä¼˜åŒ–æœºåˆ¶å’Œâ€œè½¯æ­£â€å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹æé«˜å¤šæ¨¡æ€å¹³è¡¡æ€§èƒ½å’Œå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è™½ç„¶å¼ºå¤§ï¼Œä½†ç¼ºä¹é’ˆå¯¹ç‰¹å®šç”¨æˆ·çš„ä¸ªæ€§åŒ–çŸ¥è¯†ã€‚</li>
<li>Yoâ€™Chameleonæ˜¯é¦–ä¸ªé’ˆå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ä¸ªæ€§åŒ–ç ”ç©¶ã€‚</li>
<li>Yoâ€™Chameleonå¯ä»¥é€šè¿‡3-5å¼ å›¾ç‰‡åµŒå…¥ä¸»é¢˜ç‰¹å®šä¿¡æ¯ï¼Œå›ç­”ç›¸å…³é—®é¢˜å¹¶é‡å»ºåƒç´ çº§ç»†èŠ‚ã€‚</li>
<li>Yoâ€™Chameleoné‡‡ç”¨è‡ªæˆ‘æç¤ºä¼˜åŒ–æœºåˆ¶ï¼Œå¹³è¡¡å¤šæ¨¡æ€æ€§èƒ½ã€‚</li>
<li>Yoâ€™Chameleoné‡‡ç”¨â€œè½¯æ­£â€å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>Yoâ€™Chameleonå¯ä»¥åº”ç”¨äºå›ç­”å…³äºç‰¹å®šä¸»é¢˜çš„é—®é¢˜å’Œå›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5defc54af5965f5b5c89900f5d03c8c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-573995f01a9c9b4a19c9f8a2bbf56b6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-442d15e7bcef05d3bf95bc16c9f576a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94c570dc1a715adc82c111275d3ad55b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Partitioned-Memory-Storage-Inspired-Few-Shot-Class-Incremental-learning"><a href="#Partitioned-Memory-Storage-Inspired-Few-Shot-Class-Incremental-learning" class="headerlink" title="Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning"></a>Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning</h2><p><strong>Authors:Renye Zhang, Yimin Yin, Jinghua Zhang</strong></p>
<p>Current mainstream deep learning techniques exhibit an over-reliance on extensive training data and a lack of adaptability to the dynamic world, marking a considerable disparity from human intelligence. To bridge this gap, Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous learning of new categories with limited samples without forgetting old knowledge. Existing FSCIL studies typically use a single model to learn knowledge across all sessions, inevitably leading to the stability-plasticity dilemma. Unlike machines, humans store varied knowledge in different cerebral cortices. Inspired by this characteristic, our paper aims to develop a method that learns independent models for each session. It can inherently prevent catastrophic forgetting. During the testing stage, our method integrates Uncertainty Quantification (UQ) for model deployment. Our method provides a fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on CIFAR-100 and mini-ImageNet datasets. </p>
<blockquote>
<p>å½“å‰ä¸»æµçš„æ·±åº¦å­¦ä¹ æ–¹æ³•è¿‡äºä¾èµ–å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ç¼ºä¹é€‚åº”åŠ¨æ€ä¸–ç•Œçš„èƒ½åŠ›ï¼Œä¸äººç±»æ™ºåŠ›ç›¸æ¯”å­˜åœ¨å¾ˆå¤§çš„å·®è·ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œå°æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰åº”è¿è€Œç”Ÿï¼Œå®ƒèšç„¦äºåœ¨å°‘é‡æ ·æœ¬ä¸‹å¯¹æ–°ç±»åˆ«çš„æŒç»­å­¦ä¹ ï¼ŒåŒæ—¶ä¸å¿˜æ—§çŸ¥è¯†ã€‚ç°æœ‰çš„FSCILç ”ç©¶é€šå¸¸ä½¿ç”¨å•ä¸€æ¨¡å‹è·¨æ‰€æœ‰ä¼šè¯è¿›è¡Œå­¦ä¹ ï¼Œè¿™ä¸å¯é¿å…åœ°å¯¼è‡´äº†ç¨³å®šæ€§å’Œå¯å¡‘æ€§ä¹‹é—´çš„å›°å¢ƒã€‚ä¸åŒäºæœºå™¨ï¼Œäººç±»åœ¨ä¸åŒçš„è„‘çš®å±‚ä¸­å­˜å‚¨ä¸åŒçš„çŸ¥è¯†ã€‚å—è¿™ä¸€ç‰¹ç‚¹çš„å¯å‘ï¼Œæˆ‘ä»¬çš„è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ç§ä¸ºæ¯ä¸€è¯¾ç‹¬ç«‹å­¦ä¹ æ¨¡å‹çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å†…åœ¨åœ°é˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºFSCILæä¾›äº†å…¨æ–°çš„è§†è§’ï¼Œå¹¶åœ¨CIFAR-100å’Œmini-ImageNetæ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20797v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æŠ€æœ¯ç›®å‰è¿‡åº¦ä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ç¼ºä¹é€‚åº”åŠ¨æ€ä¸–ç•Œçš„èƒ½åŠ›ï¼Œä¸äººç±»æ™ºèƒ½å­˜åœ¨å·¨å¤§å·®è·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼ŒFew-Shot Class-Incremental Learningï¼ˆFSCILï¼‰åº”è¿è€Œç”Ÿï¼Œä¸“æ³¨äºåœ¨æœ‰é™æ ·æœ¬ä¸‹å¯¹æ–°ç±»åˆ«çš„æŒç»­å­¦ä¹ ï¼ŒåŒæ—¶ä¸å¿˜æ—§çŸ¥è¯†ã€‚ç°æœ‰FSCILç ”ç©¶é€šå¸¸ä½¿ç”¨å•ä¸€æ¨¡å‹è¿›è¡Œè·¨æ‰€æœ‰ä¼šè¯çš„çŸ¥è¯†å­¦ä¹ ï¼Œå¯¼è‡´ç¨³å®šä¸å¯å¡‘æ€§ä¹‹é—´çš„ä¸¤éš¾å›°å¢ƒã€‚å—äººç±»åœ¨ä¸åŒè„‘åŒºå­˜å‚¨ä¸åŒçŸ¥è¯†çš„å¯å‘ï¼Œæˆ‘ä»¬çš„è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ç§ä¸ºæ¯ä¸ªä¼šè¯å­¦ä¹ ç‹¬ç«‹æ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½ä»æ ¹æœ¬ä¸Šé˜²æ­¢ç¾éš¾æ€§é—å¿˜ã€‚æµ‹è¯•é˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºFSCILæä¾›äº†å…¨æ–°è§†è§’ï¼Œå¹¶åœ¨CIFAR-100å’Œmini-ImageNetæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯å­˜åœ¨è¿‡åº¦ä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®å’Œç¼ºä¹é€‚åº”åŠ¨æ€ä¸–ç•Œçš„é—®é¢˜ã€‚</li>
<li>FSCILæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°æœ‰é™æ ·æœ¬ä¸‹å¯¹æ–°ç±»åˆ«çš„æŒç»­å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—§çŸ¥è¯†çš„è®°å¿†ã€‚</li>
<li>ç°æœ‰FSCILç ”ç©¶é¢ä¸´ç¨³å®šä¸å¯å¡‘æ€§å›°å¢ƒã€‚</li>
<li>äººç±»åœ¨ä¸åŒè„‘åŒºå­˜å‚¨ä¸åŒçŸ¥è¯†çš„ç‰¹æ€§ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜æä¾›äº†çµæ„Ÿã€‚</li>
<li>è®ºæ–‡æå‡ºçš„æ–¹æ³•ä¸ºæ¯ä¸ªä¼šè¯å­¦ä¹ ç‹¬ç«‹æ¨¡å‹ï¼Œä»¥é¢„é˜²ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>æµ‹è¯•é˜¶æ®µç»“åˆäº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-357486e3cdeab384bcd52d82333676ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdbe43ec9289363123278f9e5822cd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa0b818917b40f1b4649cbd37380f5a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80d3afc520d396cd28ee1e84cf71060.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Advance-Fake-Video-Detection-via-Vision-Transformers"><a href="#Advance-Fake-Video-Detection-via-Vision-Transformers" class="headerlink" title="Advance Fake Video Detection via Vision Transformers"></a>Advance Fake Video Detection via Vision Transformers</h2><p><strong>Authors:Joy Battocchio, Stefano Dellâ€™Anna, Andrea Montibeller, Giulia Boato</strong></p>
<p>Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„å¤šåª’ä½“ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•å·²ç»èƒ½å¤Ÿåˆ›å»ºè¶…é€¼çœŸçš„å›¾åƒå’Œè§†é¢‘ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹å®ƒä»¬å¯èƒ½ç”¨äºä¼ æ’­å‡ä¿¡æ¯çš„æ‹…å¿§ã€‚ç”ŸæˆæŠ€æœ¯çš„æ™®åŠåŠå…¶ä¸æ–­æ”¹è¿›ï¼Œå…è®¸ä»æç¤ºæˆ–ç°æœ‰åª’ä½“ä¸­ç”Ÿæˆè™šå‡å¤šåª’ä½“ï¼Œè¿™ä¹Ÿçªæ˜¾äº†å¯¹é«˜åº¦å‡†ç¡®ä¸”å¯æ¨å¹¿çš„äººå·¥æ™ºèƒ½ç”Ÿæˆåª’ä½“æ£€æµ‹æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ï¼Œæ¬§æ´²æ•°å­—äººå·¥æ™ºèƒ½æ³•æ¡ˆç­‰æ–°çš„æ³•è§„ä¹Ÿå¼ºè°ƒäº†è¿™ä¸€ç‚¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»åŸºäºè§†è§‰Transformerï¼ˆViTï¼‰çš„å‡å›¾åƒæ£€æµ‹ä¸­æ±²å–çµæ„Ÿï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æœ‰æ•ˆæ•´åˆéšæ—¶é—´å˜åŒ–çš„ViTåµŒå…¥æ¥å¢å¼ºæ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”±æœ€æ–°æŠ€æœ¯ä¸­çš„äº”ä¸ªå¼€æºç”ŸæˆæŠ€æœ¯ç”Ÿæˆçš„æ–°çš„å¤§å‹ã€å¤šæ ·è§†é¢‘æ•°æ®é›†ä¸Šä»¥åŠåœ¨åŒ…å«ä¸“æœ‰ç”Ÿæˆæ–¹æ³•äº§ç”Ÿçš„è§†é¢‘çš„æ•°æ®é›†ä¸Šï¼Œå‡æ˜¾ç¤ºå‡ºå…·æœ‰å‰æ™¯çš„å‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨AIç”Ÿæˆå¤šåª’ä½“æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒå’Œè§†é¢‘çš„èƒ½åŠ›ï¼Œå¼•å‘äº†å…³äºå…¶ä¼ æ’­è™šå‡ä¿¡æ¯çš„æ‹…å¿§ã€‚éšç€ç”ŸæˆæŠ€æœ¯çš„æ™®åŠå’Œä¸æ–­å®Œå–„ï¼Œå¯¹å‡†ç¡®ä¸”é€šç”¨çš„AIç”Ÿæˆåª’ä½“æ£€æµ‹æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚æ„ˆå‘æ˜¾ç°ã€‚æ–‡ç« å€Ÿé‰´äº†Vision Transformerï¼ˆViTï¼‰çš„å‡å›¾åƒæ£€æµ‹æŠ€æœ¯ï¼Œå¹¶å°†å…¶æ‰©å±•è‡³è§†é¢‘é¢†åŸŸã€‚æå‡ºä¸€ç§åˆ›æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆViTåµŒå…¥çš„æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œæå‡æ£€æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨æ–°çš„å¤§è§„æ¨¡ã€å¤šæ ·åŒ–è§†é¢‘æ•°æ®é›†ä¸Šå±•ç°å‡ºè‰¯å¥½çš„å‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„å¤šåª’ä½“ç”ŸæˆæŠ€æœ¯å·²å‘å±•åˆ°å¯ç”Ÿæˆè¶…é€¼çœŸå›¾åƒå’Œè§†é¢‘çš„ç¨‹åº¦ï¼Œå¼•å‘å…³äºä¼ æ’­è™šå‡ä¿¡æ¯çš„æ‹…å¿§ã€‚</li>
<li>ç”ŸæˆæŠ€æœ¯çš„æ™®åŠå’Œä¸æ–­å®Œå–„çªæ˜¾äº†å¯¹å‡†ç¡®ä¸”é€šç”¨çš„AIç”Ÿæˆåª’ä½“æ£€æµ‹æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ã€‚</li>
<li>æ–‡ç« å—åˆ°Vision Transformerï¼ˆViTï¼‰å‡å›¾åƒæ£€æµ‹æŠ€æœ¯çš„å¯å‘ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è§†é¢‘é¢†åŸŸã€‚</li>
<li>æå‡ºçš„åˆ›æ–°æ¡†æ¶é€šè¿‡æ•´åˆViTåµŒå…¥çš„æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œå¢å¼ºäº†æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ–°çš„å¤§è§„æ¨¡ã€å¤šæ ·åŒ–è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ‰€ææ–¹æ³•å…·æœ‰ä¼˜ç§€çš„é€šç”¨æ€§å’Œå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d3ac46edb762514e5ea22634763bbf71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-849a6087e708815a163455ac9468c8b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2aec0171cbf75d2678cf4612a60145a4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepAndes-A-Self-Supervised-Vision-Foundation-Model-for-Multi-Spectral-Remote-Sensing-Imagery-of-the-Andes"><a href="#DeepAndes-A-Self-Supervised-Vision-Foundation-Model-for-Multi-Spectral-Remote-Sensing-Imagery-of-the-Andes" class="headerlink" title="DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral   Remote Sensing Imagery of the Andes"></a>DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral   Remote Sensing Imagery of the Andes</h2><p><strong>Authors:Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Siqi Lu, Quan Liu, Ruining Deng, Can Cui, Jialin Yue, Yizhe Lin, Tianyuan Yao, Juming Xiong, Junchao Zhu, Chongyu Qu, Yuechen Yang, Mitchell Wilkes, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, Yuankai Huo</strong></p>
<p>By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on <a target="_blank" rel="noopener" href="https://github.com/geopacha/DeepAndes">https://github.com/geopacha/DeepAndes</a>. </p>
<blockquote>
<p>é€šè¿‡åœ¨å¤§è§„æ¨¡ç«™ç‚¹ä½¿ç”¨é¥æ„Ÿæ•°æ®è¿›è¡Œæµ‹ç»˜ï¼Œè€ƒå¤å­¦å®¶å¯ä»¥ç”Ÿæˆå¯¹é•¿æœŸäººå£è¶‹åŠ¿ã€åŒºåŸŸé—´ç¤¾ä¼šç½‘ç»œå’Œæ°”å€™å˜åŒ–çš„è¿‡å»é€‚åº”çš„ç‹¬ç‰¹è§è§£ã€‚é¥æ„Ÿè°ƒæŸ¥æ˜¯å¯¹ç°åœºæ–¹æ³•çš„ä¸€ç§è¡¥å……ï¼Œå½“ä¸æ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯ç›¸ç»“åˆæ—¶ï¼Œå…¶å½±å“èŒƒå›´å°¤å…¶å¹¿æ³›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æœ‰ç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤§è§„æ¨¡ç²¾ç»†è€ƒå¤ç‰¹å¾æ ‡æ³¨æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å°‘é‡æ³¨é‡Šä¸‹å­¦ä¹ å¤§è§„æ¨¡é¥æ„Ÿæ•°æ®æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å¤§å¤šæ•°ç°æˆçš„è§£å†³æ–¹æ¡ˆæ˜¯é’ˆå¯¹RGBå›¾åƒè®¾è®¡çš„ï¼Œè€Œä¸æ˜¯å¤šå…‰è°±å«æ˜Ÿå›¾åƒï¼Œå¦‚æˆ‘ä»¬ç ”ç©¶ä¸­ä½¿ç”¨çš„8æ³¢æ®µæ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DeepAndesï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºtransformerçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç»è¿‡ä¸‰ç™¾ä¸‡å¼ å¤šå…‰è°±å«æ˜Ÿå›¾åƒçš„è®­ç»ƒï¼Œä¸“ä¸ºå®‰ç¬¬æ–¯è€ƒå¤å®šåˆ¶ã€‚DeepAndesèå…¥äº†ä¸€ä¸ªå®šåˆ¶åŒ–çš„DINOv2è‡ªç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•é’ˆå¯¹8æ³¢æ®µå¤šå…‰è°±å›¾åƒè¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºå®‰ç¬¬æ–¯åœ°åŒºè®¾è®¡çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡ä¸å¹³è¡¡å›¾åƒåˆ†ç±»ã€å›¾åƒå®ä¾‹æ£€ç´¢å’Œåƒç´ çº§è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æ¥è¯„ä¼°å…¶å›¾åƒç†è§£æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepAndesåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­å®ç°äº†è¾ƒé«˜çš„F1åˆ†æ•°ã€å¹³å‡ç²¾åº¦å’ŒDiceç³»æ•°ï¼Œæ˜¾è‘—ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹æˆ–åœ¨è¾ƒå°æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™çªæ˜¾äº†å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒåœ¨è€ƒå¤é¥æ„Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/geopacha/DeepAndes%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/geopacha/DeepAndesä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20303v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é€šè¿‡å¤§è§„æ¨¡é¥æ„Ÿæ•°æ®æ˜ å°„ï¼Œè€ƒå¤å­¦å®¶èƒ½å¤Ÿç”Ÿæˆå…³äºé•¿æœŸäººå£è¶‹åŠ¿ã€åŒºåŸŸç¤¾ä¼šç½‘ç»œå’Œæ°”å€™å˜åŒ–çš„é€‚åº”ç­–ç•¥çš„ç‹¬ç‰¹è§è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ ‡æ³¨ç²¾ç»†çš„è€ƒå¤ç‰¹å¾æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè½¬åŒ–å™¨çš„è§†è§‰åŸºç¡€æ¨¡å‹DeepAndesï¼Œè¯¥æ¨¡å‹ç»è¿‡ä¸‰ç™¾ä¸‡å¼ å¤šå…‰è°±å«æ˜Ÿå›¾åƒçš„è®­ç»ƒï¼Œç‰¹åˆ«é€‚åˆå®‰ç¬¬æ–¯è€ƒå¤ã€‚é€šè¿‡ä¸å¹³è¡¡å›¾åƒåˆ†ç±»ã€å›¾åƒå®ä¾‹æ£€ç´¢å’Œåƒç´ çº§è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼ŒéªŒè¯äº†DeepAndesåœ¨å°‘é‡å­¦ä¹ åœºæ™¯ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒæˆ–åœ¨è¾ƒå°æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚è¿™å‡¸æ˜¾äº†å¤§è§„æ¨¡è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒåœ¨è€ƒå¤é¥æ„Ÿä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡é¥æ„Ÿæ•°æ®å¤§è§„æ¨¡æ˜ å°„ï¼Œè€ƒå¤å­¦å®¶å¯è·å¾—é•¿æœŸäººå£è¶‹åŠ¿ã€åŒºåŸŸç¤¾ä¼šç½‘ç»œå’Œæ°”å€™é€‚åº”ç­–ç•¥çš„ç‹¬ç‰¹è§è§£ã€‚</li>
<li>é¥æ„Ÿè°ƒæŸ¥ä¸ç°åœºè°ƒæŸ¥ç›¸è¾…ç›¸æˆï¼Œç»“åˆæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯å¤§å¤§æé«˜å…¶è¦†ç›–èŒƒå›´ã€‚</li>
<li>ä¼ ç»Ÿç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ ‡æ³¨å¤§è§„æ¨¡ç²¾ç»†è€ƒå¤ç‰¹å¾æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ€è¿‘çš„è§†è§‰åŸºç¡€æ¨¡å‹åœ¨å°‘é‡æ³¨é‡Šçš„æƒ…å†µä¸‹æ˜¾ç¤ºå‡ºäº†å·¨å¤§çš„æˆåŠŸï¼Œä½†ç°æœ‰çš„è§£å†³æ–¹æ¡ˆå¤šæ•°é€‚ç”¨äºRGBå›¾åƒè€Œéå¤šå…‰è°±å«æ˜Ÿå›¾åƒã€‚</li>
<li>DeepAndesæ˜¯ä¸€ç§é’ˆå¯¹å®‰ç¬¬æ–¯è€ƒå¤è®¾è®¡çš„åŸºäºè½¬åŒ–å™¨çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç»è¿‡ä¸‰ç™¾ä¸‡å¼ å¤šå…‰è°±å«æ˜Ÿå›¾åƒè®­ç»ƒã€‚</li>
<li>DeepAndesåœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘é‡å­¦ä¹ åœºæ™¯ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92934f74bd42da9c86a3e79d7c399452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-627056f2db6c6b604fbddd82c5189524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de08edc9a79173909e81edace5df1a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f456207b69b34a411ac6fceadd4be247.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ProFi-Net-Prototype-based-Feature-Attention-with-Curriculum-Augmentation-for-WiFi-based-Gesture-Recognition"><a href="#ProFi-Net-Prototype-based-Feature-Attention-with-Curriculum-Augmentation-for-WiFi-based-Gesture-Recognition" class="headerlink" title="ProFi-Net: Prototype-based Feature Attention with Curriculum   Augmentation for WiFi-based Gesture Recognition"></a>ProFi-Net: Prototype-based Feature Attention with Curriculum   Augmentation for WiFi-based Gesture Recognition</h2><p><strong>Authors:Zhe Cui, Shuxian Zhang, Kangzhi Lou, Le-Nam Tran</strong></p>
<p>This paper presents ProFi-Net, a novel few-shot learning framework for WiFi-based gesture recognition that overcomes the challenges of limited training data and sparse feature representations. ProFi-Net employs a prototype-based metric learning architecture enhanced with a feature-level attention mechanism, which dynamically refines the Euclidean distance by emphasizing the most discriminative feature dimensions. Additionally, our approach introduces a curriculum-inspired data augmentation strategy exclusively on the query set. By progressively incorporating Gaussian noise of increasing magnitude, the model is exposed to a broader range of challenging variations, thereby improving its generalization and robustness to overfitting. Extensive experiments conducted across diverse real-world environments demonstrate that ProFi-Net significantly outperforms conventional prototype networks and other state-of-the-art few-shot learning methods in terms of classification accuracy and training efficiency. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ProFi-Netï¼Œè¿™æ˜¯ä¸€ç§åŸºäºWiFiçš„æ‰‹åŠ¿è¯†åˆ«å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œå…‹æœäº†è®­ç»ƒæ•°æ®æœ‰é™å’Œç‰¹å¾è¡¨ç¤ºç¨€ç–çš„æŒ‘æˆ˜ã€‚ProFi-Neté‡‡ç”¨åŸºäºåŸå‹çš„åº¦é‡å­¦ä¹ æ¶æ„ï¼Œå¹¶è¾…ä»¥ç‰¹å¾çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å¼ºè°ƒæœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾ç»´åº¦æ¥åŠ¨æ€ä¼˜åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ç§å—è¯¾ç¨‹å¯å‘çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä»…åœ¨æŸ¥è¯¢é›†ä¸Šå®æ–½ã€‚é€šè¿‡é€æ­¥èå…¥å¢å¼ºçš„é«˜æ–¯å™ªå£°ï¼Œæ¨¡å‹ä¼šé¢ä¸´æ›´å¹¿æ³›çš„æŒ‘æˆ˜å˜åŒ–ï¼Œä»è€Œæé«˜å…¶æ³›åŒ–èƒ½åŠ›å’Œå¯¹è¿‡åº¦æ‹Ÿåˆçš„é²æ£’æ€§ã€‚åœ¨å¤šç§çœŸå®ç¯å¢ƒä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒProFi-Netåœ¨åˆ†ç±»ç²¾åº¦å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸå‹ç½‘ç»œå’Œå…¶ä»–å…ˆè¿›çš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20193v1">PDF</a> This paper was accepted at The 9th APWeb-WAIM joint International   Conference on Web and Big Data</p>
<p><strong>Summary</strong></p>
<p>ProFi-Netæ˜¯ä¸€ç§ç”¨äºåŸºäºWiFiçš„æ‰‹åŠ¿è¯†åˆ«çš„æ–°å‹å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸå‹åº¦é‡å­¦ä¹ æ¶æ„å’Œç‰¹å¾çº§æ³¨æ„åŠ›æœºåˆ¶è§£å†³äº†è®­ç»ƒæ•°æ®æœ‰é™å’Œç‰¹å¾è¡¨ç¤ºç¨€ç–çš„æŒ‘æˆ˜ã€‚ProFi-Netå¼ºè°ƒæœ€å…·åˆ¤åˆ«åŠ›çš„ç‰¹å¾ç»´åº¦æ¥åŠ¨æ€ä¼˜åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨å¯å‘å¼æ•™å­¦ç­–ç•¥è¿›è¡Œæ•°æ®å¢å¼ºï¼Œå¯¹æŸ¥è¯¢é›†å¼•å…¥æ¸è¿›å¼çš„é«˜æ–¯å™ªå£°ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’ŒæŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒProFi-Netåœ¨åˆ†ç±»ç²¾åº¦å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„åŸå‹ç½‘ç»œå’Œå…¶ä»–å…ˆè¿›å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProFi-Netæ˜¯ä¸€ä¸ªé’ˆå¯¹WiFiæ‰‹åŠ¿è¯†åˆ«çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>å®ƒä½¿ç”¨åŸå‹åº¦é‡å­¦ä¹ æ¶æ„å’Œç‰¹å¾çº§æ³¨æ„åŠ›æœºåˆ¶æ¥å…‹æœè®­ç»ƒæ•°æ®æœ‰é™å’Œç‰¹å¾ç¨€ç–çš„é—®é¢˜ã€‚</li>
<li>ProFi-Neté€šè¿‡åŠ¨æ€ä¼˜åŒ–æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œå¼ºè°ƒæœ€å…·åˆ¤åˆ«åŠ›çš„ç‰¹å¾ç»´åº¦ã€‚</li>
<li>é‡‡ç”¨å¯å‘å¼æ•™å­¦ç­–ç•¥è¿›è¡Œæ•°æ®å¢å¼ºï¼Œé€šè¿‡æ¸è¿›å¼å¼•å…¥é«˜æ–¯å™ªå£°æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ProFi-Netå…·æœ‰ä¼˜ç§€çš„æŠ—è¿‡æ‹Ÿåˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒProFi-Netåœ¨åˆ†ç±»ç²¾åº¦å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dec9ce08357740f2e323cbdf4343984f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23eb316715b847a55183e0dfa42050e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-Bench-Human-Aligned-Video-Generation-Benchmark"><a href="#Video-Bench-Human-Aligned-Video-Generation-Benchmark" class="headerlink" title="Video-Bench: Human-Aligned Video Generation Benchmark"></a>Video-Bench: Human-Aligned Video Generation Benchmark</h2><p><strong>Authors:Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni</strong></p>
<p>Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our frameworkâ€™s assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç¡®ä¿ç”Ÿæˆæ¨¡å‹äº§ç”Ÿè§†è§‰çœŸå®ã€é«˜è´¨é‡çš„è§†é¢‘å¹¶ä¸”ç¬¦åˆäººç±»æœŸæœ›è‡³å…³é‡è¦ã€‚å½“å‰çš„è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼šä¼ ç»ŸåŸºå‡†æµ‹è¯•ä½¿ç”¨æŒ‡æ ‡å’ŒåµŒå…¥æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘è´¨é‡å¤šä¸ªç»´åº¦ï¼Œä½†å¾€å¾€ä¸äººç±»åˆ¤æ–­ç¼ºä¹ä¸€è‡´æ€§ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•è™½ç„¶å…·å¤‡äººç±»æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹è§†é¢‘è´¨é‡æŒ‡æ ‡çš„è·¨æ¨¡æ€ä¸€è‡´æ€§ç†è§£æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶å»ºç«‹ä¸€ä¸ªæ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½çš„åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡ä»‹ç»äº†Video-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸°å¯Œæç¤ºå¥—ä»¶å’Œå¹¿æ³›è¯„ä¼°ç»´åº¦çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ˜¯é¦–æ¬¡å°è¯•åœ¨ç”Ÿæˆæ¨¡å‹ä¸­æ¶‰åŠè§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç›¸å…³ç»´åº¦ä¸Šç³»ç»Ÿåœ°åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚é€šè¿‡ç»“åˆå°‘é‡æ‰“åˆ†å’ŒæŸ¥è¯¢é“¾æŠ€æœ¯ï¼ŒVideo-Benchä¸ºç”Ÿæˆçš„è§†é¢‘è¯„ä¼°æä¾›äº†ä¸€ç§ç»“æ„åŒ–ã€å¯æ‰©å±•çš„æ–¹æ³•ã€‚åœ¨åŒ…æ‹¬Soraçš„é«˜çº§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°äº†ä¸äººç±»åå¥½æ›´ä¼˜è¶Šçš„å¥‘åˆåº¦ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶è¯„ä¼°ä¸äººç±»è¯„ä¼°å­˜åœ¨åˆ†æ­§çš„æƒ…å†µä¸‹ï¼Œå®ƒå§‹ç»ˆæä¾›æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ï¼Œè¿™è¡¨æ˜å®ƒç›¸è¾ƒäºä¼ ç»Ÿçš„äººç±»åˆ¤æ–­å…·æœ‰æ›´å¤§çš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04907v2">PDF</a> Accepted by CVPRâ€™25</p>
<p><strong>Summary</strong><br>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç¡®ä¿ç”Ÿæˆæ¨¡å‹ç”Ÿæˆè§†è§‰çœŸå®ã€é«˜è´¨é‡çš„è§†é¢‘è‡³å…³é‡è¦ï¼ŒåŒæ—¶ç¬¦åˆäººç±»æœŸæœ›ã€‚å½“å‰è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šä¼ ç»ŸåŸºå‡†æµ‹è¯•ä½¿ç”¨æŒ‡æ ‡å’ŒåµŒå…¥æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘è´¨é‡ï¼Œä½†å¾€å¾€ç¼ºä¹ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•è™½ç„¶èƒ½å¤Ÿè¿›è¡Œäººç±»æ¨ç†ï¼Œä½†å¯¹è§†é¢‘è´¨é‡æŒ‡æ ‡å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§çš„ç†è§£æœ‰é™ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜å¹¶å»ºç«‹ä¸€ä¸ªæ›´ç¬¦åˆäººç±»åå¥½çš„åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡ä»‹ç»äº†Video-Benchï¼Œä¸€ä¸ªåŒ…å«ä¸°å¯Œæç¤ºå¥—ä»¶å’Œå¹¿æ³›è¯„ä¼°ç»´åº¦çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚å®ƒé¦–æ¬¡å°è¯•åœ¨è§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç›¸å…³ç»´åº¦ä¸Šç³»ç»Ÿåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡ç»“åˆå°‘æ ·æœ¬è¯„åˆ†å’Œé“¾æŸ¥è¯¢æŠ€æœ¯ï¼ŒVideo-Benchä¸ºç”Ÿæˆçš„è§†é¢‘è¯„ä»·æä¾›äº†ä¸€ç§ç»“æ„åŒ–ã€å¯æ‰©å±•çš„æ–¹æ³•ã€‚åœ¨é«˜çº§æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°äº†ä¸äººç±»åå¥½çš„ä¼˜è¶Šå¯¹é½ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶è¯„ä¼°ä¸äººç±»è¯„ä¼°å­˜åœ¨åˆ†æ­§çš„æƒ…å†µä¸‹ï¼Œå®ƒå§‹ç»ˆæä¾›æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç¡®ä¿ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„è§†é¢‘è´¨é‡è‡³å…³é‡è¦ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘è§†é¢‘çš„è§†è§‰çœŸå®æ€§ã€é«˜è´¨é‡å’Œäººç±»æœŸæœ›çš„ç¬¦åˆåº¦ã€‚</li>
<li>å½“å‰è§†é¢‘ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¼ ç»ŸåŸºå‡†æµ‹è¯•å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ä¸¤ç±»ï¼Œä½†éƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Video-Benchæ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡ä¸°å¯Œçš„æç¤ºå¥—ä»¶å’Œå¹¿æ³›è¯„ä¼°ç»´åº¦æ¥è§£å†³ç°æœ‰æŒ‘æˆ˜ã€‚</li>
<li>Video-Benché¦–æ¬¡åœ¨è§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç»´åº¦ä¸Šç³»ç»Ÿåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>Video-Benchç»“åˆäº†å°‘æ ·æœ¬è¯„åˆ†å’Œé“¾æŸ¥è¯¢æŠ€æœ¯ï¼Œä¸ºç”Ÿæˆçš„è§†é¢‘è¯„ä»·æä¾›äº†ç»“æ„åŒ–ã€å¯æ‰©å±•çš„æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°äº†ä¸äººç±»åå¥½çš„ä¼˜è¶Šå¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9d146286134ed8dda134483b4d894d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e300daea1caa72404c324611dd0a5c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f277eb502f125db24353b45ec8ab5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches"><a href="#A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches" class="headerlink" title="A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches"></a>A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches</h2><p><strong>Authors:Luca Ciampi, Ali Azmoudeh, Elif Ecem Akbaba, Erdi SarÄ±taÅŸ, Ziya Ata YazÄ±cÄ±, HazÄ±m Kemal Ekenel, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories â€“ a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of the architectures of 29 CAC approaches and report their results on gold-standard benchmarks. We compare their performance and discuss their strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions. We believe this survey will be a valuable resource, showcasing CAC advancements and guiding future research. </p>
<blockquote>
<p>è§†è§‰ç‰©ä½“è®¡æ•°æœ€è¿‘å·²ç»è½¬å‘ç±»åˆ«æ— å…³çš„è®¡æ•°ï¼ˆCACï¼‰ï¼Œè¿™è§£å†³äº†è·¨ä»»æ„ç±»åˆ«çš„ç‰©ä½“è®¡æ•°æŒ‘æˆ˜â€”â€”å¯¹äºçµæ´»å’Œå¯æ¨å¹¿çš„è®¡æ•°ç³»ç»Ÿï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„èƒ½åŠ›ã€‚ä¸äººç±»èƒ½å¤Ÿè½»æ¾åœ°è¯†åˆ«å¹¶è®¡ç®—æ¥è‡ªä¸åŒç±»åˆ«çš„å¯¹è±¡è€Œæ— éœ€å…ˆéªŒçŸ¥è¯†ä¸åŒï¼Œå¤§å¤šæ•°ç°æœ‰çš„è®¡æ•°æ–¹æ³•ä»…é™äºè®¡ç®—å·²çŸ¥ç±»åˆ«çš„å®ä¾‹ï¼Œéœ€è¦å¤§è§„æ¨¡çš„æœ‰æ ‡ç­¾æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”åœ¨å¼€æ”¾è¯æ±‡ç¯å¢ƒä¸­è¡¨ç°æŒ£æ‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCACæ—¨åœ¨è®¡ç®—å±äºè®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«çš„å¯¹è±¡ï¼Œåœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œæ“ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹CACæ–¹æ³•è¿›è¡Œäº†é¦–æ¬¡å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»æ³•ï¼Œå°†CACæ–¹æ³•åˆ†ä¸ºä¸‰ç§èŒƒå¼ï¼ŒåŸºäºç›®æ ‡ç±»åˆ«çš„æŒ‡å®šæ–¹å¼ï¼šåŸºäºå‚è€ƒã€æ— å‚è€ƒå’Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼ã€‚åŸºäºå‚è€ƒçš„æ–¹æ³•é€šè¿‡ä¾é èŒƒä¾‹å¼•å¯¼æœºåˆ¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ— å‚è€ƒæ–¹æ³•é€šè¿‡åˆ©ç”¨å›ºæœ‰çš„å›¾åƒæ¨¡å¼æ¥æ¶ˆé™¤èŒƒä¾‹ä¾èµ–æ€§ã€‚æœ€åï¼Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºå®ç°å¯¹è±¡ç±»åˆ«æè¿°ï¼Œæä¾›äº†ä¸€ä¸ªçµæ´»ä¸”æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚åŸºäºè¿™ç§åˆ†ç±»æ³•ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†29ç§CACæ–¹æ³•çš„æ¶æ„ï¼Œå¹¶åœ¨é»„é‡‘æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸ŠæŠ¥å‘Šäº†ä»–ä»¬çš„ç»“æœã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä»–ä»¬çš„æ€§èƒ½ï¼Œå¹¶è®¨è®ºäº†ä»–ä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨FSC-147æ•°æ®é›†ä¸Šå±•ç¤ºäº†ç»“æœï¼Œä½¿ç”¨é»„é‡‘æ ‡å‡†æŒ‡æ ‡è®¾ç½®äº†ä¸€ä¸ªæ’è¡Œæ¦œï¼Œå¹¶åœ¨CARPKæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚æ³¨é‡Šä¾èµ–æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠæœªæ¥æ–¹å‘è¿›è¡Œäº†æ‰¹åˆ¤æ€§è®¨è®ºã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä»½è°ƒæŸ¥æŠ¥å‘Šå°†æˆä¸ºä¸€ä¸ªæœ‰ä»·å€¼çš„èµ„æºï¼Œå±•ç¤ºCACçš„è¿›å±•å¹¶æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19184v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†é¢å‘ç±»åˆ«ä¸å¯çŸ¥çš„è®¡æ•°ï¼ˆCACï¼‰æ–¹æ³•ï¼Œå°†å…¶åˆ†ç±»ä¸ºä¸‰ç§æ¨¡å¼ï¼šåŸºäºå‚è€ƒã€æ— å‚è€ƒå’Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼ã€‚æ–‡ç« ä»‹ç»äº†å„ç§æ–¹æ³•çš„æ¶æ„ï¼Œå¹¶åœ¨é»„é‡‘æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šå¯¹å…¶æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒæ—¶è®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚è¯¥ç»¼è¿°ä¸ºç ”ç©¶è€…æä¾›äº†å®è´µçš„èµ„æºï¼Œå±•ç¤ºäº†CACçš„è¿›å±•å¹¶å¼•å¯¼æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å¯¹è±¡è®¡æ•°æ­£è½¬å‘ç±»åˆ«ä¸å¯çŸ¥çš„è®¡æ•°ï¼ˆCACï¼‰ï¼Œä»¥åº”å¯¹ä»»æ„ç±»åˆ«å¯¹è±¡è®¡æ•°çš„æŒ‘æˆ˜ã€‚</li>
<li>CACæ—¨åœ¨è§£å†³åœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«çš„å¯¹è±¡è®¡æ•°é—®é¢˜ï¼Œå¹¶åœ¨å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹è¿ä½œã€‚</li>
<li>ç°æœ‰çš„CACæ–¹æ³•è¢«åˆ†ç±»ä¸ºä¸‰ç§æ¨¡å¼ï¼šåŸºäºå‚è€ƒã€æ— å‚è€ƒå’Œå¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼ã€‚</li>
<li>åŸºäºå‚è€ƒçš„æ–¹æ³•ä¾èµ–äºç¤ºä¾‹å¼•å¯¼æœºåˆ¶ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>æ— å‚è€ƒæ–¹æ³•åˆ©ç”¨å›¾åƒå†…åœ¨æ¨¡å¼æ¶ˆé™¤äº†å¯¹ç¤ºä¾‹çš„ä¾èµ–ã€‚</li>
<li>å¼€æ”¾ä¸–ç•Œæ–‡æœ¬å¼•å¯¼æ–¹æ³•ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œå¯¹è±¡ç±»åˆ«æè¿°ï¼Œæä¾›çµæ´»å’Œæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c553eb686b576f1546c4575e4575157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1e9f8ecdd011ac664f352237d4111d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-895316421417a297e11f5419ff0f5ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c0c0a439a03da969b4786179ef63383.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Generic-Objects-as-Pose-Probes-for-Few-shot-View-Synthesis"><a href="#Generic-Objects-as-Pose-Probes-for-Few-shot-View-Synthesis" class="headerlink" title="Generic Objects as Pose Probes for Few-shot View Synthesis"></a>Generic Objects as Pose Probes for Few-shot View Synthesis</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</strong></p>
<p>Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as â€œpose probesâ€. The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \href{<a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/%7D%7Bthis">https://zhirui-gao.github.io/PoseProbe.github.io/}{this</a> https URL} </p>
<blockquote>
<p>è¾å°„åœºï¼ŒåŒ…æ‹¬NeRFå’Œ3Dé«˜æ–¯åˆ†å¸ƒï¼Œåœ¨é«˜ä¿çœŸæ¸²æŸ“å’Œåœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„å§¿æ€å›¾åƒä½œä¸ºè¾“å…¥ã€‚COLMAPå¸¸ç”¨äºé¢„å¤„ç†ä»¥ä¼°è®¡å§¿æ€ï¼Œä½†éœ€è¦å¤§é‡ç‰¹å¾åŒ¹é…æ‰èƒ½æœ‰æ•ˆè¿è¡Œï¼Œå¯¹äºç‰¹å¾ç¨€ç–ã€å›¾åƒé—´åŸºçº¿å¤§æˆ–è¾“å…¥å›¾åƒæ•°é‡æœ‰é™çš„åœºæ™¯ï¼Œå®ƒè¡¨ç°å›°éš¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è§£å†³ä»…ä½¿ç”¨3åˆ°6å¼ æœªå®šä½çš„åœºæ™¯å›¾åƒè¿›è¡Œå°‘è§†è§’NeRFé‡å»ºçš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¸¸ä½¿ç”¨æ ‡å®šæ¿ï¼Œä½†åœ¨å›¾åƒä¸­å¹¶ä¸å¸¸è§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨åœ¨å›¾åƒå’Œç°å®ç”Ÿæ´»ä¸­éƒ½å¸¸è§çš„æ—¥å¸¸å¯¹è±¡ä½œä¸ºâ€œå§¿æ€æ¢é’ˆâ€çš„æ–°æ€è·¯ã€‚æ¢é’ˆå¯¹è±¡é€šè¿‡SAMè‡ªåŠ¨åˆ†å‰²ï¼Œå…¶å½¢çŠ¶åˆå§‹åŒ–ä¸ºç«‹æ–¹ä½“ã€‚æˆ‘ä»¬é‡‡ç”¨åŒåˆ†æ”¯ä½“ç§¯æ¸²æŸ“ä¼˜åŒ–ï¼ˆå¯¹è±¡NeRFå’Œåœºæ™¯NeRFï¼‰æ¥çº¦æŸå§¿æ€ä¼˜åŒ–å¹¶å…±åŒä¼˜åŒ–å‡ ä½•ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡PnPåŒ¹é…åœ¨SDFè¡¨ç¤ºä¸­ä¼°è®¡ä¸¤ä¸ªè§†è§’çš„ç‰©ä½“å§¿æ€ï¼Œä½œä¸ºåˆå§‹å§¿æ€ã€‚PnPåŒ¹é…ä»…éœ€è¦å‡ ä¸ªç‰¹å¾ï¼Œé€‚ç”¨äºç‰¹å¾ç¨€ç–çš„åœºæ™¯ã€‚ç„¶åé€æ­¥åŠ å…¥é¢å¤–çš„è§†è§’ï¼Œä»¥ç»†åŒ–å…ˆå‰è§†è§’çš„å§¿æ€ã€‚åœ¨å®éªŒæ–¹é¢ï¼ŒPoseProbeåœ¨å¤šæ•°æ®é›†ä¸Šå®ç°äº†å§¿æ€ä¼°è®¡å’Œæ–°å‹è§†å›¾åˆæˆçš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†å…¶åœ¨å°‘è§†è§’å’Œå¤§åŸºçº¿åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­COLMAPè¡¨ç°å›°éš¾ã€‚åœ¨æ¶ˆèå®éªŒä¸­ï¼Œä½¿ç”¨åœºæ™¯ä¸­çš„ä¸åŒå¯¹è±¡å¯ä»¥è·å¾—ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äºï¼šç‚¹å‡»æ­¤å¤„çš„httpsé“¾æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16690v4">PDF</a> Accepted by IEEE TCSVT 2025 Project page:   <a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/">https://zhirui-gao.github.io/PoseProbe.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PoseProbeé¡¹ç›®åœ¨NeRFé‡å»ºæ–¹é¢çš„åˆ›æ–°åº”ç”¨ã€‚é’ˆå¯¹éœ€è¦å¤§é‡å®šä½å›¾åƒä½œä¸ºè¾“å…¥çš„è¾å°„åœºï¼ˆå¦‚NeRFå’Œ3Dé«˜æ–¯ï¼‰åœ¨é«˜ä¿çœŸæ¸²æŸ“å’Œåœºæ™¯é‡å»ºæ–¹é¢çš„æ½œåŠ›ï¼Œé¡¹ç›®æå‡ºäº†ä¸€ç§åˆ©ç”¨å¸¸è§ç‰©ä½“ä½œä¸ºâ€œå§¿æ€æ¢é’ˆâ€çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä»…ä½¿ç”¨å°‘é‡æœªå®šä½åœºæ™¯å›¾åƒè¿›è¡ŒNeRFé‡å»ºçš„é—®é¢˜ã€‚é€šè¿‡è‡ªåŠ¨åˆ†å‰²æ¢é’ˆå¯¹è±¡ã€åŒåˆ†æ”¯ä½“ç§¯æ¸²æŸ“ä¼˜åŒ–å’Œå§¿æ€ä¼˜åŒ–çº¦æŸï¼ŒPoseProbeåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å§¿æ€ä¼°è®¡å’Œæ–°é¢–è§†å›¾åˆæˆçš„æœ€ä½³æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å¾ç¨€ç–ã€å¤§åŸºçº¿é—´éš”æˆ–è¾“å…¥å›¾åƒæ•°é‡æœ‰é™çš„åœºæ™¯ä¸­æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PoseProbeé¡¹ç›®é’ˆå¯¹NeRFé‡å»ºä¸­çš„å°‘è§†è§’é—®é¢˜æå‡ºäº†åˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åˆ©ç”¨å¸¸è§ç‰©ä½“ä½œä¸ºâ€œå§¿æ€æ¢é’ˆâ€ï¼Œä»…éœ€è¦3åˆ°6ä¸ªæœªå®šä½çš„åœºæ™¯å›¾åƒã€‚</li>
<li>é¡¹ç›®é‡‡ç”¨è‡ªåŠ¨åˆ†å‰²æ¢é’ˆå¯¹è±¡ï¼Œåˆå§‹å½¢çŠ¶è®¾å®šä¸ºç«‹æ–¹ä½“ã€‚</li>
<li>é€šè¿‡åŒåˆ†æ”¯ä½“ç§¯æ¸²æŸ“ä¼˜åŒ–ï¼ˆå¯¹è±¡NeRFå’Œåœºæ™¯NeRFï¼‰æ¥çº¦æŸå§¿æ€ä¼˜åŒ–å¹¶å…±åŒæ”¹è¿›å‡ ä½•ç»“æ„ã€‚</li>
<li>é‡‡ç”¨PnPåŒ¹é…è¿›è¡Œå§¿æ€ä¼°è®¡ï¼Œé€‚ç”¨äºç‰¹å¾ç¨€ç–åœºæ™¯ï¼Œå¹¶å¯ä»¥é€æ­¥åŠ å…¥æ›´å¤šè§†è§’ä»¥ä¼˜åŒ–å§¿æ€ã€‚</li>
<li>PoseProbeåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å§¿æ€ä¼°è®¡å’Œæ–°é¢–è§†å›¾åˆæˆçš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36780712ba775db95dab5711a104762c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed4311d491f3074bebb975ad7c962af8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8799747c2350cd604ee4f88c8422968.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc2a11938bcee9339085fde599a6093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26b9010d5df8be62ca74eac9b43dfae4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dual-Interrelated-Diffusion-Model-for-Few-Shot-Anomaly-Image-Generation"><a href="#Dual-Interrelated-Diffusion-Model-for-Few-Shot-Anomaly-Image-Generation" class="headerlink" title="Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation"></a>Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation</h2><p><strong>Authors:Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Jiafu Wu, Hao Chen, Haoxuan Wang, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang</strong></p>
<p>The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. </p>
<blockquote>
<p>åœ¨å·¥ä¸šåˆ¶é€ ä¸­çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½å—åˆ°å¼‚å¸¸æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…ä»¬å·²ç»å¼€å§‹é‡‡ç”¨å¼‚å¸¸ç”Ÿæˆæ–¹æ³•æ¥å¢åŠ å¼‚å¸¸æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼‚å¸¸ç”Ÿæˆæ–¹æ³•å­˜åœ¨ç”Ÿæˆå¼‚å¸¸å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œå¹¶ä¸”éš¾ä»¥å®ç°ç”Ÿæˆå¼‚å¸¸ä¸åŸå§‹å›¾åƒçš„æ— ç¼èåˆã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„æ©è†œé€šå¸¸ä¸ç”Ÿæˆçš„å¼‚å¸¸ä¸åŒ¹é…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªæ–°çš„è§’åº¦å…‹æœäº†è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶ç”Ÿæˆæ•´ä½“å›¾åƒå’Œç›¸åº”çš„å¼‚å¸¸éƒ¨åˆ†ã€‚æˆ‘ä»¬æå‡ºäº†DualAnoDiffï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨åŒç›¸å…³æ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªç”¨äºç”Ÿæˆæ•´ä½“å›¾åƒï¼Œå¦ä¸€ä¸ªç”¨äºç”Ÿæˆå¼‚å¸¸éƒ¨åˆ†ï¼Œå¯ä»¥ç”Ÿæˆå¤šæ ·ä¸”é€¼çœŸçš„å¼‚å¸¸å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå–èƒŒæ™¯å’Œå½¢çŠ¶ä¿¡æ¯ï¼Œä»¥å‡è½»å°‘æ ·æœ¬å›¾åƒç”Ÿæˆä¸­çš„å¤±çœŸå’Œæ¨¡ç³Šç°è±¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨å¤šæ ·æ€§ã€é€¼çœŸç¨‹åº¦å’Œæ©è†œå‡†ç¡®æ€§æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€å¼‚å¸¸å®šä½å’Œå¼‚å¸¸åˆ†ç±»ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13509v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>     å·¥ä¸šåˆ¶é€ ä¸­çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½å—é™äºå¼‚å¸¸æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…å¼€å§‹é‡‡ç”¨å¼‚å¸¸ç”Ÿæˆæ–¹æ³•æ¥æ‰©å……å¼‚å¸¸æ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„å¼‚å¸¸ç”Ÿæˆå¤šæ ·æ€§æœ‰é™ï¼Œä¸”éš¾ä»¥å°†ç”Ÿæˆçš„å¼‚å¸¸æ— ç¼èåˆåˆ°åŸå§‹å›¾åƒä¸­ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„æ©è†œé€šå¸¸ä¸ç”Ÿæˆçš„å¼‚å¸¸ä¸åŒ¹é…ã€‚æœ¬æ–‡ä»ä¸€ä¸ªæ–°è§†è§’è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒæ—¶ç”Ÿæˆæ•´ä½“å›¾åƒå’Œç›¸åº”çš„å¼‚å¸¸éƒ¨åˆ†ã€‚æå‡ºDualAnoDiffï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨ç›¸äº’å…³è”çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šæ ·ä¸”é€¼çœŸçš„å¼‚å¸¸å›¾åƒï¼Œå…¶ä¸­ä¸€ä¸ªç”¨äºç”Ÿæˆæ•´ä½“å›¾åƒï¼Œå¦ä¸€ä¸ªç”¨äºç”Ÿæˆå¼‚å¸¸éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæå–èƒŒæ™¯å’Œå½¢çŠ¶ä¿¡æ¯ä»¥å‡è½»å°‘æ ·æœ¬å›¾åƒç”Ÿæˆä¸­çš„å¤±çœŸå’Œæ¨¡ç³Šç°è±¡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨å¤šæ ·æ€§ã€é€¼çœŸåº¦å’Œæ©è†œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ä¸‹æ¸¸å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼ˆåŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€å¼‚å¸¸å®šä½å’Œå¼‚å¸¸åˆ†ç±»ä»»åŠ¡ï¼‰çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼‚å¸¸æ£€æµ‹åœ¨å·¥ä¸šåˆ¶é€ ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå¼‚å¸¸æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>ç°æœ‰å¼‚å¸¸ç”Ÿæˆæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹å¤šæ ·æ€§ã€éš¾ä»¥æ— ç¼èåˆåˆ°åŸå§‹å›¾åƒä¸­ä»¥åŠæ©è†œä¸ç”Ÿæˆçš„å¼‚å¸¸ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„å°‘æ ·æœ¬å¼‚å¸¸å›¾åƒç”Ÿæˆæ¨¡å‹DualAnoDiffï¼ŒåŒæ—¶ç”Ÿæˆæ•´ä½“å›¾åƒå’Œå¯¹åº”çš„å¼‚å¸¸éƒ¨åˆ†ã€‚</li>
<li>DualAnoDiffæ¨¡å‹é€šè¿‡ä½¿ç”¨ç›¸äº’å…³è”çš„æ‰©æ•£æ¨¡å‹æ¥æé«˜ç”Ÿæˆçš„å¼‚å¸¸å›¾åƒçš„å¤šæ ·æ€§å’Œé€¼çœŸåº¦ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æå–èƒŒæ™¯å’Œå½¢çŠ¶ä¿¡æ¯æ¥å‡è½»å°‘æ ·æœ¬å›¾åƒç”Ÿæˆä¸­çš„å¤±çœŸå’Œæ¨¡ç³Šç°è±¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDualAnoDiffæ¨¡å‹åœ¨å¤šæ ·æ€§ã€é€¼çœŸåº¦å’Œæ©è†œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d4e2f75e72e92e6a95e88e584a05d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fcedbd118ce6b99cc38f594d194f443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f8b25283daf510456ece8df25d6b6e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fca5a5b93d7b212f4f923ce2d66b5e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dda5a988aba9fe18096120cf4272e38a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae4a19b2ade45e459d2f5945442495cf.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  Consensus Recommendations for Hyperpolarized [1-13C]pyruvate MRI   Multi-center Human Studies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9dd392ddc7a1947335d6e12dfd0a2924.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  AutoP2C An LLM-Based Agent Framework for Code Repository Generation   from Multimodal Content in Academic Papers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
