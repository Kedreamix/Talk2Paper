<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-01  YoChameleon Personalized Vision and Language Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-dda5a988aba9fe18096120cf4272e38a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-01-更新"><a href="#2025-05-01-更新" class="headerlink" title="2025-05-01 更新"></a>2025-05-01 更新</h1><h2 id="YoChameleon-Personalized-Vision-and-Language-Generation"><a href="#YoChameleon-Personalized-Vision-and-Language-Generation" class="headerlink" title="YoChameleon: Personalized Vision and Language Generation"></a>YoChameleon: Personalized Vision and Language Generation</h2><p><strong>Authors:Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li</strong></p>
<p>Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo’Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo’Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo’Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a &#96;&#96;soft-positive” image generation approach to enhance image quality in a few-shot setting. </p>
<blockquote>
<p>大型多模态模型（如GPT-4、Gemini、Chameleon）已经进化成拥有数百万用户的强大工具。然而，它们仍然是通用模型，缺乏特定用户的个性化知识概念。之前的工作已经探索了文本生成的个性化，但尚不清楚这些方法如何适应新的模式，如图像生成。在本文中，我们介绍了Yo’Chameleon，这是研究大型多模态模型个性化的首次尝试。给定关于特定概念的3-5张图像，Yo’Chameleon利用软提示调整技术嵌入特定主题的信息，以（i）回答有关该主题的问题和（ii）重新创建像素级细节，以在新环境中生成该主题的图片。Yo’Chameleon通过（i）一种自我提示优化机制来平衡多种模态的性能表现，以及（ii）一种“软正”图像生成方法来增强少样本环境下的图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20998v1">PDF</a> CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://thaoshibe.github.io/YoChameleon">https://thaoshibe.github.io/YoChameleon</a></p>
<p><strong>Summary</strong><br>     大型多模态模型（如GPT-4、Gemini、Chameleon）已发展成为拥有数百万用户的强大工具，但仍缺乏针对特定用户的个性化知识。本文介绍Yo’Chameleon，首次尝试对大型多模态模型进行个性化研究。通过3-5张特定概念的图片，Yo’Chameleon利用软提示调整技术嵌入主题特定信息，以回答有关主题的问题并重建像素级细节，以在新背景下生成主题图像。Yo’Chameleon通过自我提示优化机制和“软正”图像生成方法，在少量样本情况下提高多模态平衡性能和图像质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型虽然强大，但缺乏针对特定用户的个性化知识。</li>
<li>Yo’Chameleon是首个针对大型多模态模型的个性化研究。</li>
<li>Yo’Chameleon可以通过3-5张图片嵌入主题特定信息，回答相关问题并重建像素级细节。</li>
<li>Yo’Chameleon采用自我提示优化机制，平衡多模态性能。</li>
<li>Yo’Chameleon采用“软正”图像生成方法，提高图像生成质量。</li>
<li>Yo’Chameleon可以应用于回答关于特定主题的问题和图像生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5defc54af5965f5b5c89900f5d03c8c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-573995f01a9c9b4a19c9f8a2bbf56b6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-442d15e7bcef05d3bf95bc16c9f576a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94c570dc1a715adc82c111275d3ad55b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Partitioned-Memory-Storage-Inspired-Few-Shot-Class-Incremental-learning"><a href="#Partitioned-Memory-Storage-Inspired-Few-Shot-Class-Incremental-learning" class="headerlink" title="Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning"></a>Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning</h2><p><strong>Authors:Renye Zhang, Yimin Yin, Jinghua Zhang</strong></p>
<p>Current mainstream deep learning techniques exhibit an over-reliance on extensive training data and a lack of adaptability to the dynamic world, marking a considerable disparity from human intelligence. To bridge this gap, Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous learning of new categories with limited samples without forgetting old knowledge. Existing FSCIL studies typically use a single model to learn knowledge across all sessions, inevitably leading to the stability-plasticity dilemma. Unlike machines, humans store varied knowledge in different cerebral cortices. Inspired by this characteristic, our paper aims to develop a method that learns independent models for each session. It can inherently prevent catastrophic forgetting. During the testing stage, our method integrates Uncertainty Quantification (UQ) for model deployment. Our method provides a fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on CIFAR-100 and mini-ImageNet datasets. </p>
<blockquote>
<p>当前主流的深度学习方法过于依赖大量的训练数据，并且缺乏适应动态世界的能力，与人类智力相比存在很大的差距。为了缩小这一差距，小样本类别增量学习（FSCIL）应运而生，它聚焦于在少量样本下对新类别的持续学习，同时不忘旧知识。现有的FSCIL研究通常使用单一模型跨所有会话进行学习，这不可避免地导致了稳定性和可塑性之间的困境。不同于机器，人类在不同的脑皮层中存储不同的知识。受这一特点的启发，我们的论文旨在开发一种为每一课独立学习模型的方法。这种方法可以内在地防止灾难性遗忘。在测试阶段，我们的方法结合了不确定性量化（UQ）进行模型部署。我们的方法为FSCIL提供了全新的视角，并在CIFAR-100和mini-ImageNet数据集上展示了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20797v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>深度学习技术目前过度依赖大量训练数据，并且缺乏适应动态世界的能力，与人类智能存在巨大差距。为解决这一问题，Few-Shot Class-Incremental Learning（FSCIL）应运而生，专注于在有限样本下对新类别的持续学习，同时不忘旧知识。现有FSCIL研究通常使用单一模型进行跨所有会话的知识学习，导致稳定与可塑性之间的两难困境。受人类在不同脑区存储不同知识的启发，我们的论文旨在开发一种为每个会话学习独立模型的方法，该方法能从根本上防止灾难性遗忘。测试阶段，我们的方法结合了不确定性量化（UQ）进行模型部署。我们的方法为FSCIL提供了全新视角，并在CIFAR-100和mini-ImageNet数据集上实现了卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习技术存在过度依赖大量训练数据和缺乏适应动态世界的问题。</li>
<li>FSCIL旨在解决这一问题，实现有限样本下对新类别的持续学习，同时保持对旧知识的记忆。</li>
<li>现有FSCIL研究面临稳定与可塑性困境。</li>
<li>人类在不同脑区存储不同知识的特性为解决这个问题提供了灵感。</li>
<li>论文提出的方法为每个会话学习独立模型，以预防灾难性遗忘。</li>
<li>测试阶段结合了不确定性量化（UQ）进行模型部署。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20797">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-357486e3cdeab384bcd52d82333676ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcdbe43ec9289363123278f9e5822cd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa0b818917b40f1b4649cbd37380f5a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80d3afc520d396cd28ee1e84cf71060.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Advance-Fake-Video-Detection-via-Vision-Transformers"><a href="#Advance-Fake-Video-Detection-via-Vision-Transformers" class="headerlink" title="Advance Fake Video Detection via Vision Transformers"></a>Advance Fake Video Detection via Vision Transformers</h2><p><strong>Authors:Joy Battocchio, Stefano Dell’Anna, Andrea Montibeller, Giulia Boato</strong></p>
<p>Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods. </p>
<blockquote>
<p>近年来，基于人工智能的多媒体生成技术的最新进展已经能够创建超逼真的图像和视频，这引发了人们对它们可能用于传播假信息的担忧。生成技术的普及及其不断改进，允许从提示或现有媒体中生成虚假多媒体，这也突显了对高度准确且可推广的人工智能生成媒体检测方法的迫切需求，欧洲数字人工智能法案等新的法规也强调了这一点。在本文中，我们从基于视觉Transformer（ViT）的假图像检测中汲取灵感，并将其扩展到视频领域。我们提出了一种创新的框架，它通过有效整合随时间变化的ViT嵌入来增强检测性能。我们的方法在由最新技术中的五个开源生成技术生成的新的大型、多样视频数据集上以及在包含专有生成方法产生的视频的数据集上，均显示出具有前景的准确性、通用性和小样本学习能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注AI生成多媒体技术的最新进展，特别是生成高度逼真的图像和视频的能力，引发了关于其传播虚假信息的担忧。随着生成技术的普及和不断完善，对准确且通用的AI生成媒体检测方法的迫切需求愈发显现。文章借鉴了Vision Transformer（ViT）的假图像检测技术，并将其扩展至视频领域。提出一种创新的框架，通过整合ViT嵌入的时间序列信息，提升检测性能，并在新的大规模、多样化视频数据集上展现出良好的准确性、通用性和小样本学习能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI生成的多媒体生成技术已发展到可生成超逼真图像和视频的程度，引发关于传播虚假信息的担忧。</li>
<li>生成技术的普及和不断完善突显了对准确且通用的AI生成媒体检测方法的迫切需求。</li>
<li>文章受到Vision Transformer（ViT）假图像检测技术的启发，并将其应用到视频领域。</li>
<li>提出的创新框架通过整合ViT嵌入的时间序列信息，增强了检测性能。</li>
<li>该方法在新的大规模、多样化视频数据集上进行了测试，展现出良好的准确性。</li>
<li>所提方法具有优秀的通用性和小样本学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d3ac46edb762514e5ea22634763bbf71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-849a6087e708815a163455ac9468c8b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2aec0171cbf75d2678cf4612a60145a4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepAndes-A-Self-Supervised-Vision-Foundation-Model-for-Multi-Spectral-Remote-Sensing-Imagery-of-the-Andes"><a href="#DeepAndes-A-Self-Supervised-Vision-Foundation-Model-for-Multi-Spectral-Remote-Sensing-Imagery-of-the-Andes" class="headerlink" title="DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral   Remote Sensing Imagery of the Andes"></a>DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral   Remote Sensing Imagery of the Andes</h2><p><strong>Authors:Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Siqi Lu, Quan Liu, Ruining Deng, Can Cui, Jialin Yue, Yizhe Lin, Tianyuan Yao, Juming Xiong, Junchao Zhu, Chongyu Qu, Yuechen Yang, Mitchell Wilkes, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, Yuankai Huo</strong></p>
<p>By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on <a target="_blank" rel="noopener" href="https://github.com/geopacha/DeepAndes">https://github.com/geopacha/DeepAndes</a>. </p>
<blockquote>
<p>通过在大规模站点使用遥感数据进行测绘，考古学家可以生成对长期人口趋势、区域间社会网络和气候变化的过去适应的独特见解。遥感调查是对现场方法的一种补充，当与深度学习和计算机视觉技术相结合时，其影响范围尤其广泛。然而，传统的有监督深度学习方法在大规模精细考古特征标注方面面临挑战。虽然最近的视觉基础模型在少量注释下学习大规模遥感数据方面取得了显著的成功，但大多数现成的解决方案是针对RGB图像设计的，而不是多光谱卫星图像，如我们研究中使用的8波段数据。在本文中，我们介绍了DeepAndes，这是一个基于transformer的视觉基础模型，经过三百万张多光谱卫星图像的训练，专为安第斯考古定制。DeepAndes融入了一个定制化的DINOv2自监督学习算法，该算法针对8波段多光谱图像进行了优化，是第一个专为安第斯地区设计的基础模型。我们通过不平衡图像分类、图像实例检索和像素级语义分割任务来评估其图像理解性能。实验表明，DeepAndes在少样本学习场景中实现了较高的F1分数、平均精度和Dice系数，显著优于从头开始训练的模型或在较小数据集上预训练的模型。这突显了大规模自监督预训练在考古遥感中的有效性。代码将在<a target="_blank" rel="noopener" href="https://github.com/geopacha/DeepAndes%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/geopacha/DeepAndes上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20303v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>通过大规模遥感数据映射，考古学家能够生成关于长期人口趋势、区域社会网络和气候变化的适应策略的独特见解。然而，传统的深度学习方法在标注精细的考古特征时面临挑战。本文介绍了一种基于转化器的视觉基础模型DeepAndes，该模型经过三百万张多光谱卫星图像的训练，特别适合安第斯考古。通过不平衡图像分类、图像实例检索和像素级语义分割任务，验证了DeepAndes在少量学习场景中的卓越性能，显著优于从头开始训练或在较小数据集上预训练的模型。这凸显了大规模自我监督预训练在考古遥感中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过遥感数据大规模映射，考古学家可获得长期人口趋势、区域社会网络和气候适应策略的独特见解。</li>
<li>遥感调查与现场调查相辅相成，结合深度学习和计算机视觉技术可大大提高其覆盖范围。</li>
<li>传统监督深度学习方法在标注大规模精细考古特征时面临挑战。</li>
<li>最近的视觉基础模型在少量注释的情况下显示出了巨大的成功，但现有的解决方案多数适用于RGB图像而非多光谱卫星图像。</li>
<li>DeepAndes是一种针对安第斯考古设计的基于转化器的视觉基础模型，经过三百万张多光谱卫星图像训练。</li>
<li>DeepAndes在图像理解方面表现出卓越性能，特别是在少量学习场景下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20303">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-92934f74bd42da9c86a3e79d7c399452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-627056f2db6c6b604fbddd82c5189524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de08edc9a79173909e81edace5df1a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f456207b69b34a411ac6fceadd4be247.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ProFi-Net-Prototype-based-Feature-Attention-with-Curriculum-Augmentation-for-WiFi-based-Gesture-Recognition"><a href="#ProFi-Net-Prototype-based-Feature-Attention-with-Curriculum-Augmentation-for-WiFi-based-Gesture-Recognition" class="headerlink" title="ProFi-Net: Prototype-based Feature Attention with Curriculum   Augmentation for WiFi-based Gesture Recognition"></a>ProFi-Net: Prototype-based Feature Attention with Curriculum   Augmentation for WiFi-based Gesture Recognition</h2><p><strong>Authors:Zhe Cui, Shuxian Zhang, Kangzhi Lou, Le-Nam Tran</strong></p>
<p>This paper presents ProFi-Net, a novel few-shot learning framework for WiFi-based gesture recognition that overcomes the challenges of limited training data and sparse feature representations. ProFi-Net employs a prototype-based metric learning architecture enhanced with a feature-level attention mechanism, which dynamically refines the Euclidean distance by emphasizing the most discriminative feature dimensions. Additionally, our approach introduces a curriculum-inspired data augmentation strategy exclusively on the query set. By progressively incorporating Gaussian noise of increasing magnitude, the model is exposed to a broader range of challenging variations, thereby improving its generalization and robustness to overfitting. Extensive experiments conducted across diverse real-world environments demonstrate that ProFi-Net significantly outperforms conventional prototype networks and other state-of-the-art few-shot learning methods in terms of classification accuracy and training efficiency. </p>
<blockquote>
<p>本文介绍了ProFi-Net，这是一种基于WiFi的手势识别少样本学习框架，克服了训练数据有限和特征表示稀疏的挑战。ProFi-Net采用基于原型的度量学习架构，并辅以特征级注意力机制，通过强调最具区分性的特征维度来动态优化欧几里得距离。此外，我们的方法还引入了一种受课程启发的数据增强策略，该策略仅在查询集上实施。通过逐步融入增强的高斯噪声，模型会面临更广泛的挑战变化，从而提高其泛化能力和对过度拟合的鲁棒性。在多种真实环境中进行的广泛实验表明，ProFi-Net在分类精度和训练效率方面显著优于传统的原型网络和其他先进的少样本学习方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20193v1">PDF</a> This paper was accepted at The 9th APWeb-WAIM joint International   Conference on Web and Big Data</p>
<p><strong>Summary</strong></p>
<p>ProFi-Net是一种用于基于WiFi的手势识别的新型少样本学习框架，它通过原型度量学习架构和特征级注意力机制解决了训练数据有限和特征表示稀疏的挑战。ProFi-Net强调最具判别力的特征维度来动态优化欧几里得距离。此外，它采用启发式教学策略进行数据增强，对查询集引入渐进式的高斯噪声，提高模型的泛化能力和抗过拟合能力。实验表明，ProFi-Net在分类精度和训练效率方面显著优于传统的原型网络和其他先进少样本学习方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProFi-Net是一个针对WiFi手势识别的少样本学习框架。</li>
<li>它使用原型度量学习架构和特征级注意力机制来克服训练数据有限和特征稀疏的问题。</li>
<li>ProFi-Net通过动态优化欧几里得距离，强调最具判别力的特征维度。</li>
<li>采用启发式教学策略进行数据增强，通过渐进式引入高斯噪声提高模型的泛化能力。</li>
<li>ProFi-Net具有优秀的抗过拟合能力。</li>
<li>实验表明，ProFi-Net在分类精度和训练效率方面优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20193">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dec9ce08357740f2e323cbdf4343984f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23eb316715b847a55183e0dfa42050e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Video-Bench-Human-Aligned-Video-Generation-Benchmark"><a href="#Video-Bench-Human-Aligned-Video-Generation-Benchmark" class="headerlink" title="Video-Bench: Human-Aligned Video Generation Benchmark"></a>Video-Bench: Human-Aligned Video Generation Benchmark</h2><p><strong>Authors:Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni</strong></p>
<p>Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our framework’s assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment. </p>
<blockquote>
<p>视频生成评估对于确保生成模型产生视觉真实、高质量的视频并且符合人类期望至关重要。当前的视频生成基准测试主要分为两大类：传统基准测试使用指标和嵌入来评估生成视频质量多个维度，但往往与人类判断缺乏一致性；而基于大型语言模型（LLM）的基准测试虽然具备人类推理能力，但对视频质量指标的跨模态一致性理解有限。为了解决这些挑战并建立一个更好地符合人类偏好的基准测试，本文介绍了Video-Bench，这是一个包含丰富提示套件和广泛评估维度的综合基准测试。该基准测试是首次尝试在生成模型中涉及视频生成评估的所有相关维度上系统地利用多模态语言模型（MLLMs）。通过结合少量打分和查询链技术，Video-Bench为生成的视频评估提供了一种结构化、可扩展的方法。在包括Sora的高级模型上的实验表明，Video-Bench在所有维度上实现了与人类偏好更优越的契合度。此外，在我们的框架评估与人类评估存在分歧的情况下，它始终提供更客观和准确的见解，这表明它相较于传统的人类判断具有更大的潜在优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04907v2">PDF</a> Accepted by CVPR’25</p>
<p><strong>Summary</strong><br>视频生成评估对于确保生成模型生成视觉真实、高质量的视频至关重要，同时符合人类期望。当前视频生成基准测试主要分为两类：传统基准测试使用指标和嵌入来评估生成视频质量，但往往缺乏与人类判断的对齐；而基于大型语言模型的基准测试虽然能够进行人类推理，但对视频质量指标和跨模态一致性的理解有限。为解决这些挑战并建立一个更符合人类偏好的基准测试，本文介绍了Video-Bench，一个包含丰富提示套件和广泛评估维度的综合基准测试。它首次尝试在视频生成评估的所有相关维度上系统地利用大型语言模型。通过结合少样本评分和链查询技术，Video-Bench为生成的视频评价提供了一种结构化、可扩展的方法。在高级模型上的实验表明，Video-Bench在所有维度上实现了与人类偏好的优越对齐。此外，在我们的框架评估与人类评估存在分歧的情况下，它始终提供更客观和准确的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频生成评估对于确保生成模型生成的视频质量至关重要，需要综合考虑视频的视觉真实性、高质量和人类期望的符合度。</li>
<li>当前视频生成基准测试主要分为传统基准测试和基于大型语言模型的基准测试两类，但都存在局限性。</li>
<li>Video-Bench是一个综合基准测试，通过丰富的提示套件和广泛评估维度来解决现有挑战。</li>
<li>Video-Bench首次在视频生成评估的所有维度上系统地利用大型语言模型。</li>
<li>Video-Bench结合了少样本评分和链查询技术，为生成的视频评价提供了结构化、可扩展的方法。</li>
<li>实验表明，Video-Bench在所有维度上实现了与人类偏好的优越对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9d146286134ed8dda134483b4d894d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e300daea1caa72404c324611dd0a5c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f277eb502f125db24353b45ec8ab5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches"><a href="#A-Survey-on-Class-Agnostic-Counting-Advancements-from-Reference-Based-to-Open-World-Text-Guided-Approaches" class="headerlink" title="A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches"></a>A Survey on Class-Agnostic Counting: Advancements from Reference-Based   to Open-World Text-Guided Approaches</h2><p><strong>Authors:Luca Ciampi, Ali Azmoudeh, Elif Ecem Akbaba, Erdi Sarıtaş, Ziya Ata Yazıcı, Hazım Kemal Ekenel, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories – a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of the architectures of 29 CAC approaches and report their results on gold-standard benchmarks. We compare their performance and discuss their strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions. We believe this survey will be a valuable resource, showcasing CAC advancements and guiding future research. </p>
<blockquote>
<p>视觉物体计数最近已经转向类别无关的计数（CAC），这解决了跨任意类别的物体计数挑战——对于灵活和可推广的计数系统，这是至关重要的能力。与人类能够轻松地识别并计算来自不同类别的对象而无需先验知识不同，大多数现有的计数方法仅限于计算已知类别的实例，需要大规模的有标签数据集进行训练，并且在开放词汇环境中表现挣扎。相比之下，CAC旨在计算属于训练期间未见过的类别的对象，在少量样本的情况下进行操作。在本文中，我们对CAC方法进行了首次全面回顾。我们提出了一个分类法，将CAC方法分为三种范式，基于目标类别的指定方式：基于参考、无参考和开放世界文本引导。基于参考的方法通过依靠范例引导机制实现了最先进的性能。无参考方法通过利用固有的图像模式来消除范例依赖性。最后，开放世界文本引导的方法使用视觉语言模型，通过文本提示实现对象类别描述，提供了一个灵活且有前途的解决方案。基于这种分类法，我们概述了29种CAC方法的架构，并在黄金标准基准测试上报告了他们的结果。我们比较了他们的性能，并讨论了他们的优点和局限性。特别是，我们在FSC-147数据集上展示了结果，使用黄金标准指标设置了一个排行榜，并在CARPK数据集上评估了泛化能力。最后，我们对持续存在的挑战，如注释依赖性和泛化能力，以及未来方向进行了批判性讨论。我们相信这份调查报告将成为一个有价值的资源，展示CAC的进展并指导未来的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19184v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文综述了面向类别不可知的计数（CAC）方法，将其分类为三种模式：基于参考、无参考和开放世界文本引导。文章介绍了各种方法的架构，并在黄金标准基准测试上对其性能进行了评估，同时讨论了它们的优缺点。该综述为研究者提供了宝贵的资源，展示了CAC的进展并引导未来的研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉对象计数正转向类别不可知的计数（CAC），以应对任意类别对象计数的挑战。</li>
<li>CAC旨在解决在训练期间未见过的类别的对象计数问题，并在少量样本的情况下运作。</li>
<li>现有的CAC方法被分类为三种模式：基于参考、无参考和开放世界文本引导。</li>
<li>基于参考的方法依赖于示例引导机制，实现了卓越的性能。</li>
<li>无参考方法利用图像内在模式消除了对示例的依赖。</li>
<li>开放世界文本引导方法使用视觉语言模型，通过文本提示进行对象类别描述，提供灵活和有前途的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c553eb686b576f1546c4575e4575157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1e9f8ecdd011ac664f352237d4111d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-895316421417a297e11f5419ff0f5ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c0c0a439a03da969b4786179ef63383.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Generic-Objects-as-Pose-Probes-for-Few-shot-View-Synthesis"><a href="#Generic-Objects-as-Pose-Probes-for-Few-shot-View-Synthesis" class="headerlink" title="Generic Objects as Pose Probes for Few-shot View Synthesis"></a>Generic Objects as Pose Probes for Few-shot View Synthesis</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu</strong></p>
<p>Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as “pose probes”. The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \href{<a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/%7D%7Bthis">https://zhirui-gao.github.io/PoseProbe.github.io/}{this</a> https URL} </p>
<blockquote>
<p>辐射场，包括NeRF和3D高斯分布，在高保真渲染和场景重建方面表现出巨大潜力，但它们需要大量的姿态图像作为输入。COLMAP常用于预处理以估计姿态，但需要大量特征匹配才能有效运行，对于特征稀疏、图像间基线大或输入图像数量有限的场景，它表现困难。我们的目标是解决仅使用3到6张未定位的场景图像进行少视角NeRF重建的问题。传统方法常使用标定板，但在图像中并不常见。我们提出了一种利用在图像和现实生活中都常见的日常对象作为“姿态探针”的新思路。探针对象通过SAM自动分割，其形状初始化为立方体。我们采用双分支体积渲染优化（对象NeRF和场景NeRF）来约束姿态优化并共同优化几何结构。具体来说，首先通过PnP匹配在SDF表示中估计两个视角的物体姿态，作为初始姿态。PnP匹配仅需要几个特征，适用于特征稀疏的场景。然后逐步加入额外的视角，以细化先前视角的姿态。在实验方面，PoseProbe在多数据集上实现了姿态估计和新型视图合成的最先进的性能。我们证明了其在少视角和大基线场景中的有效性，在这些场景中COLMAP表现困难。在消融实验中，使用场景中的不同对象可以获得相当的性能。我们的项目页面位于：点击此处的https链接。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16690v4">PDF</a> Accepted by IEEE TCSVT 2025 Project page:   <a target="_blank" rel="noopener" href="https://zhirui-gao.github.io/PoseProbe.github.io/">https://zhirui-gao.github.io/PoseProbe.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了PoseProbe项目在NeRF重建方面的创新应用。针对需要大量定位图像作为输入的辐射场（如NeRF和3D高斯）在高保真渲染和场景重建方面的潜力，项目提出了一种利用常见物体作为“姿态探针”的新方法，旨在解决仅使用少量未定位场景图像进行NeRF重建的问题。通过自动分割探针对象、双分支体积渲染优化和姿态优化约束，PoseProbe在多个数据集上实现了姿态估计和新颖视图合成的最佳性能，特别是在特征稀疏、大基线间隔或输入图像数量有限的场景中效果显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PoseProbe项目针对NeRF重建中的少视角问题提出了创新解决方案。</li>
<li>利用常见物体作为“姿态探针”，仅需要3到6个未定位的场景图像。</li>
<li>项目采用自动分割探针对象，初始形状设定为立方体。</li>
<li>通过双分支体积渲染优化（对象NeRF和场景NeRF）来约束姿态优化并共同改进几何结构。</li>
<li>采用PnP匹配进行姿态估计，适用于特征稀疏场景，并可以逐步加入更多视角以优化姿态。</li>
<li>PoseProbe在多个数据集上实现了姿态估计和新颖视图合成的最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-36780712ba775db95dab5711a104762c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed4311d491f3074bebb975ad7c962af8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8799747c2350cd604ee4f88c8422968.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc2a11938bcee9339085fde599a6093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26b9010d5df8be62ca74eac9b43dfae4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dual-Interrelated-Diffusion-Model-for-Few-Shot-Anomaly-Image-Generation"><a href="#Dual-Interrelated-Diffusion-Model-for-Few-Shot-Anomaly-Image-Generation" class="headerlink" title="Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation"></a>Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation</h2><p><strong>Authors:Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Jiafu Wu, Hao Chen, Haoxuan Wang, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang</strong></p>
<p>The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. </p>
<blockquote>
<p>在工业制造中的异常检测性能受到异常数据稀缺的限制。为了克服这一挑战，研究者们已经开始采用异常生成方法来增加异常数据集。然而，现有的异常生成方法存在生成异常多样性有限的问题，并且难以实现生成异常与原始图像的无缝融合。此外，生成的掩膜通常与生成的异常不匹配。在本文中，我们从一个新的角度克服了这些挑战，同时生成整体图像和相应的异常部分。我们提出了DualAnoDiff，这是一种基于扩散的新型少样本异常图像生成模型，通过使用双相关扩散模型，其中一个用于生成整体图像，另一个用于生成异常部分，可以生成多样且逼真的异常图像。此外，我们提取背景和形状信息，以减轻少样本图像生成中的失真和模糊现象。大量实验表明，与最新方法相比，我们提出的模型在多样性、逼真程度和掩膜准确性方面更具优势。总体而言，我们的方法显著提高了下游异常检测任务的性能，包括异常检测、异常定位和异常分类任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13509v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>     工业制造中的异常检测性能受限于异常数据的稀缺性。为应对这一挑战，研究者开始采用异常生成方法来扩充异常数据集。然而，现有方法的异常生成多样性有限，且难以将生成的异常无缝融合到原始图像中。此外，生成的掩膜通常与生成的异常不匹配。本文从一个新视角解决这些挑战，同时生成整体图像和相应的异常部分。提出DualAnoDiff，一种基于扩散的少样本异常图像生成模型，通过使用相互关联的扩散模型生成多样且逼真的异常图像，其中一个用于生成整体图像，另一个用于生成异常部分。此外，提取背景和形状信息以减轻少样本图像生成中的失真和模糊现象。实验表明，与最先进的方法相比，所提出的模型在多样性、逼真度和掩膜准确性方面表现优越。总体而言，我们的方法显著提高了下游异常检测任务（包括异常检测、异常定位和异常分类任务）的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异常检测在工业制造中面临挑战，主要由于异常数据的稀缺性。</li>
<li>现有异常生成方法存在局限性，如缺乏多样性、难以无缝融合到原始图像中以及掩膜与生成的异常不匹配的问题。</li>
<li>本文提出一种基于扩散的少样本异常图像生成模型DualAnoDiff，同时生成整体图像和对应的异常部分。</li>
<li>DualAnoDiff模型通过使用相互关联的扩散模型来提高生成的异常图像的多样性和逼真度。</li>
<li>该模型通过提取背景和形状信息来减轻少样本图像生成中的失真和模糊现象。</li>
<li>实验表明，DualAnoDiff模型在多样性、逼真度和掩膜准确性方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13509">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d4e2f75e72e92e6a95e88e584a05d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fcedbd118ce6b99cc38f594d194f443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f8b25283daf510456ece8df25d6b6e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fca5a5b93d7b212f4f923ce2d66b5e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dda5a988aba9fe18096120cf4272e38a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae4a19b2ade45e459d2f5945442495cf.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-01  Consensus Recommendations for Hyperpolarized [1-13C]pyruvate MRI   Multi-center Human Studies
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9dd392ddc7a1947335d6e12dfd0a2924.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-01  AutoP2C An LLM-Based Agent Framework for Code Repository Generation   from Multimodal Content in Academic Papers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19017.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
