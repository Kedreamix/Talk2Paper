<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  ChestX-Reasoner Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-aca356da45c1ced05c918a5559a74dcb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-01-æ›´æ–°"><a href="#2025-05-01-æ›´æ–°" class="headerlink" title="2025-05-01 æ›´æ–°"></a>2025-05-01 æ›´æ–°</h1><h2 id="ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification"><a href="#ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification" class="headerlink" title="ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification"></a>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification</h2><p><strong>Authors:Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs. </p>
<blockquote>
<p>è¿‘æœŸï¼Œæ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰çš„è¿›æ­¥åœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œä½†åŒ»ç–—AIæ¨¡å‹å¾€å¾€å¿½ç•¥äº†ä¸´åºŠå®è·µä¸­çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChestX-Reasonerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ”¾å°„å­¦è¯Šæ–­çš„MLLMï¼Œæ—¨åœ¨åˆ©ç”¨ç›´æ¥ä»ä¸´åºŠæŠ¥å‘Šä¸­æŒ–æ˜çš„è¿‡ç¨‹ç›‘ç£ï¼Œåæ˜ æ”¾å°„ç§‘åŒ»ç”Ÿéµå¾ªçš„é€æ­¥æ¨ç†ã€‚æˆ‘ä»¬é€šè¿‡ä»å¸¸è§„æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾æ¥æ„å»ºå¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒæ³•å’Œç”±è¿‡ç¨‹å¥–åŠ±å¼•å¯¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œä»¥æ›´å¥½åœ°ä½¿æ¨¡å‹æ¨ç†ä¸ä¸´åºŠæ ‡å‡†ç›¸ç¬¦ã€‚æˆ‘ä»¬å¼•å…¥äº†RadRBench-CXRï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«59Kè§†è§‰é—®ç­”æ ·æœ¬å’Œ301Kä¸´åºŠéªŒè¯æ¨ç†æ­¥éª¤çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œå¹¶æå‡ºäº†RadRScoreï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°æ¨ç†çœŸå®æ€§ã€å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§çš„æŒ‡æ ‡ã€‚ChestX-Reasoneråœ¨è¯Šæ–­å’Œæ²»ç–—å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨MLLMï¼Œåœ¨æ¨ç†èƒ½åŠ›æ–¹é¢ä¸æœ€ä½³åŒ»ç–—MLLMã€æœ€ä½³é€šç”¨MLLMå’ŒåŸºå‡†æ¨¡å‹ç›¸æ¯”åˆ†åˆ«æé«˜äº†16%ã€5.9%å’Œ18%ï¼Œå¹¶åœ¨ç»“æœå‡†ç¡®æ€§æ–¹é¢åˆ†åˆ«æé«˜äº†3.3%ã€24%å’Œ27%ã€‚æ‰€æœ‰èµ„æºå‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›åŒ»ç–—æ¨ç†MLLMçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20930v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     è¿‘æœŸè¿›å±•åœ¨æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ï¼Œä½†åŒ»ç–—äººå·¥æ™ºèƒ½æ¨¡å‹å¸¸å¸¸å¿½ç•¥äº†ä¸´åºŠå®è·µä¸­çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†ChestX-Reasonerï¼Œä¸€ä¸ªç”¨äºæ”¾å°„è¯Šæ–­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åˆ©ç”¨ç›´æ¥ä»ä¸´åºŠæŠ¥å‘Šä¸­æŒ–æ˜çš„æ¨ç†è¿‡ç¨‹ç›‘ç£ï¼Œåæ˜ æ”¾å°„ç§‘åŒ»ç”Ÿéµå¾ªçš„é€æ­¥æ¨ç†ã€‚é€šè¿‡ä»å¸¸è§„æ”¾å°„æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œä»¥è¿‡ç¨‹å¥–åŠ±ä¸ºæŒ‡å¯¼ï¼Œä½¿æ¨¡å‹æ¨ç†æ›´å¥½åœ°ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚æˆ‘ä»¬å¼•å…¥äº†RadRBench-CXRåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«59Kè§†è§‰é—®ç­”æ ·æœ¬å’Œ301Kä¸´åºŠéªŒè¯çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶æå‡ºäº†RadRScoreè¯„ä¼°æŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¨ç†çš„çœŸå®æ€§ã€å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ã€‚ChestX-Reasoneråœ¨è¯Šæ–­å’Œæ¨ç†èƒ½åŠ›ä¸Šå‡ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç›¸è¾ƒäºæœ€ä½³åŒ»ç–—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€æœ€ä½³é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åŠå…¶åŸºç¡€æ¨¡å‹ï¼Œåœ¨æ¨ç†èƒ½åŠ›ä¸Šåˆ†åˆ«æé«˜äº†16%ã€5.9%å’Œ18%ï¼Œåœ¨ç»“æœå‡†ç¡®æ€§ä¸Šåˆ†åˆ«æé«˜äº†3.3%ã€24%å’Œ27%ã€‚æ‰€æœ‰èµ„æºå‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›åŒ»ç–—æ¨ç†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ChestX-Reasoneræ˜¯ä¸€ä¸ªé’ˆå¯¹æ”¾å°„è¯Šæ–­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆä¸´åºŠå®è·µä¸­æ”¾å°„ç§‘åŒ»ç”Ÿçš„é€æ­¥æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡ä»å¸¸è§„æ”¾å°„æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹æ¨ç†æ›´ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚</li>
<li>å¼•å…¥äº†RadRBench-CXRåŸºå‡†æµ‹è¯•å’ŒRadRScoreè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ChestX-Reasoneråœ¨è¯Šæ–­å’Œæ¨ç†èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŒ»ç–—å’Œé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åœ¨æ¨ç†èƒ½åŠ›å’Œç»“æœå‡†ç¡®æ€§æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c13dd6b3d9288c7e374ba6efd6851d26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfdb228ad3ba663f2bf723febf3e77d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aca356da45c1ced05c918a5559a74dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5845baf023f841852911dad79ae8ef7b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Leaderboard-Illusion"><a href="#The-Leaderboard-Illusion" class="headerlink" title="The Leaderboard Illusion"></a>The Leaderboard Illusion</h2><p><strong>Authors:Shivalika Singh, Yiyang Nan, Alex Wang, Daniel Dâ€™Souza, Sayash Kapoor, Ahmet ÃœstÃ¼n, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker</strong></p>
<p>Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arenaâ€™s evaluation framework and promote fairer, more transparent benchmarking for the field </p>
<blockquote>
<p>è¿›å±•æµ‹é‡æ˜¯ä»»ä½•ç§‘å­¦é¢†åŸŸå‘å±•çš„åŸºç¡€ã€‚éšç€åŸºå‡†æµ‹è¯•çš„ä½œç”¨è¶Šæ¥è¶Šé‡è¦ï¼Œå®ƒä»¬ä¹Ÿæ›´å®¹æ˜“å—åˆ°æ‰­æ›²ã€‚èŠå¤©æœºå™¨äººç«æŠ€åœºå·²ç»æ¶Œç°ä¸ºæ’åæœ€å…ˆè¿›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é¦–é€‰æ’è¡Œæ¦œã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†å¯¼è‡´ç«æŠ€ç¯å¢ƒæ‰­æ›²çš„ç³»ç»Ÿæ€§é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœªå…¬å¼€çš„ç§äººæµ‹è¯•å®è·µä½¿å°‘æ•°æä¾›è€…å—ç›Šï¼Œè¿™äº›æä¾›è€…èƒ½å¤Ÿåœ¨å…¬å…±å‘å¸ƒä¹‹å‰æµ‹è¯•å¤šä¸ªå˜ä½“ï¼Œå¹¶åœ¨éœ€è¦æ—¶æ”¶å›åˆ†æ•°ã€‚æˆ‘ä»¬è¯å®ï¼Œè¿™äº›æä¾›è€…é€‰æ‹©æœ€ä½³åˆ†æ•°çš„èƒ½åŠ›ä¼šå¯¼è‡´ç«æŠ€åœºå¾—åˆ†åå‘ï¼Œå› ä¸ºæ€§èƒ½ç»“æœæ˜¯é€‰æ‹©æ€§æŠ«éœ²çš„ã€‚åœ¨æç«¯æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç¡®å®šäº†Metaåœ¨Llama-4å‘å¸ƒå‰æµ‹è¯•çš„27ä¸ªç§äººå¤§å‹è¯­è¨€æ¨¡å‹å˜ä½“ã€‚æˆ‘ä»¬è¿˜å‘ç°ä¸“æœ‰å°é—­æ¨¡å‹çš„é‡‡æ ·ç‡ï¼ˆæˆ˜æ–—æ¬¡æ•°ï¼‰æ›´é«˜ï¼Œä¸å…¬å¼€æƒé‡å’Œå¼€æºæ›¿ä»£æ–¹æ¡ˆç›¸æ¯”ï¼Œä»ç«æŠ€åœºç§»é™¤çš„æ¨¡å‹æ›´å°‘ã€‚è¿™ä¸¤ç§æ”¿ç­–éƒ½å¯¼è‡´äº†éšæ—¶é—´æ¨ç§»çš„æ•°æ®è®¿é—®ä¸å¯¹ç§°ã€‚è°·æ­Œå’ŒOpenAIç­‰æä¾›å•†åˆ†åˆ«è·å¾—äº†ä¼°è®¡çš„19.2%å’Œ20.4%çš„æ‰€æœ‰æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ83ä¸ªå…¬å¼€æƒé‡æ¨¡å‹ä»…è·å¾—äº†ä¼°è®¡çš„29.7%çš„æ€»æ•°æ®ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè®¿é—®èŠå¤©æœºå™¨äººç«æŠ€åœºæ•°æ®å¸¦æ¥å·¨å¤§çš„å¥½å¤„ï¼›å³ä½¿æœ‰é™çš„é¢å¤–æ•°æ®ä¹Ÿå¯èƒ½å¯¼è‡´åŸºäºä¿å®ˆä¼°è®¡çš„ç«æŠ€åœºåˆ†å¸ƒä¸Šçš„ç›¸å¯¹æ€§èƒ½æå‡é«˜è¾¾112%ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›åŠ¨æ€å¯¼è‡´å¯¹ç«æŠ€åœºç‰¹å®šåŠ¨æ€çš„è¿‡åº¦æ‹Ÿåˆï¼Œè€Œä¸æ˜¯ä¸€èˆ¬çš„æ¨¡å‹è´¨é‡ã€‚ç«æŠ€åœºå»ºç«‹åœ¨ç»„ç»‡è€…å’Œä¸€ä¸ªç»´æŠ¤è¿™ä¸ªå®è´µè¯„ä¼°å¹³å°çš„å¼€æ”¾ç¤¾åŒºçš„å¤§é‡åŠªåŠ›ä¹‹ä¸Šã€‚æˆ‘ä»¬æä¾›åˆ‡å®å¯è¡Œçš„å»ºè®®ï¼Œä»¥æ”¹é©èŠå¤©æœºå™¨äººç«æŠ€åœºçš„è¯„ä¼°æ¡†æ¶ï¼Œä¿ƒè¿›è¯¥é¢†åŸŸæ›´å…¬å¹³ã€æ›´é€æ˜çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20879v1">PDF</a> 68 pages, 18 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬è®¨è®ºäº†èŠå¤©æœºå™¨äººç«æŠ€åœºï¼ˆChatbot Arenaï¼‰åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿæ–¹é¢å­˜åœ¨çš„é—®é¢˜ã€‚å‘ç°ç§æœ‰æµ‹è¯•å®è·µã€é€‰æ‹©æ€§æŠ«éœ²æ€§èƒ½ç»“æœã€æ•°æ®è®¿é—®ä¸å¯¹ç§°ç­‰é—®é¢˜å¯¼è‡´ç«æŠ€åœºè¯„ä»·å¤±çœŸã€‚æå‡ºæ”¹é©è¯„ä»·æ¡†æ¶ï¼Œä¿ƒè¿›æ›´å…¬å¹³ã€é€æ˜çš„åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èŠå¤©æœºå™¨äººç«æŠ€åœºï¼ˆChatbot Arenaï¼‰ä½œä¸ºAIç³»ç»Ÿçš„æ’åæ¦œï¼Œå­˜åœ¨ç³»ç»Ÿæ€§çš„é—®é¢˜ï¼Œå¯¼è‡´è¯„ä»·å¤±çœŸã€‚</li>
<li>æœªç»å…¬å¸ƒçš„ç§äººæµ‹è¯•å®è·µä½¿å¾—å°‘æ•°æä¾›è€…èƒ½å¤Ÿåœ¨å…¬å¼€å‘å¸ƒå‰æµ‹è¯•å¤šä¸ªå˜ä½“ï¼Œå¹¶é€‰æ‹©æ€§æ’¤å›åˆ†æ•°ï¼Œå¯¼è‡´ç«æŠ€åœºè¯„åˆ†åå·®ã€‚</li>
<li>æŸäº›æä¾›è€…é€‰æ‹©æœ€ä½³åˆ†æ•°çš„èƒ½åŠ›ï¼Œå› é€‰æ‹©æ€§æŠ«éœ²æ€§èƒ½ç»“æœï¼Œå¯¼è‡´ç«æŠ€åœºè¯„åˆ†ä¸å…¬å¹³ã€‚</li>
<li>å­˜åœ¨æç«¯æƒ…å†µï¼Œå¦‚Metaåœ¨Llama-4å‘å¸ƒå‰æµ‹è¯•äº†27ä¸ªç§æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å˜ä½“ã€‚</li>
<li>ä¸“æœ‰å°é—­æ¨¡å‹è¢«é‡‡æ ·çš„é¢‘ç‡æ›´é«˜ï¼Œä»ç«æŠ€åœºä¸­ç§»é™¤çš„æ¨¡å‹æ›´å°‘ï¼Œå¯¼è‡´æ•°æ®è®¿é—®ä¸å¯¹ç§°ã€‚</li>
<li>å¯¹èŠå¤©æœºå™¨äººç«æŠ€åœºæ•°æ®çš„è®¿é—®å¸¦æ¥äº†å·¨å¤§çš„å¥½å¤„ï¼›å³ä½¿æœ‰é™é¢å¤–çš„æ•°æ®ä¹Ÿå¯èƒ½å¯¼è‡´ç«æŠ€åœºåˆ†å¸ƒçš„ç›¸å¯¹æ€§èƒ½æé«˜112%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0efc8d2094394eb09f432f843d957110.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea9311517b3560b80afba2108a26199c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a0258ddcf1cee3badf8a4aad55a437f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-825f71eb0cb5aaf13e5a4fa58c7cc48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-652365c2ebd005af39e3d995e054d1d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33b9f735cdd35be58df141e7f0ae3694.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AI-GenBench-A-New-Ongoing-Benchmark-for-AI-Generated-Image-Detection"><a href="#AI-GenBench-A-New-Ongoing-Benchmark-for-AI-Generated-Image-Detection" class="headerlink" title="AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection"></a>AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</h2><p><strong>Authors:Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli</strong></p>
<p>The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¿«é€Ÿè¿›æ­¥å·²ç»å½»åº•æ”¹å˜äº†å›¾åƒåˆ›ä½œçš„æ–¹å¼ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œé«˜è´¨é‡åˆæˆï¼ŒåŒæ—¶ä¸ºåª’ä½“çœŸå®æ€§å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†Ai-GenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„éœ€æ±‚ã€‚ä¸åœ¨é™æ€æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„ç°æœ‰è§£å†³æ–¹æ¡ˆä¸åŒï¼ŒAi-GenBenchå¼•å…¥äº†ä¸€ä¸ªä¸´æ—¶è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹æ£€æµ‹æ–¹æ³•è¿›è¡Œå¢é‡è®­ç»ƒï¼Œåœ¨æŒ‰ç”Ÿæˆæ¨¡å‹å†å²é¡ºåºåˆæˆçš„å›¾åƒä¸Šè¿›è¡Œï¼Œä»¥æµ‹è¯•å…¶é€‚åº”æ–°ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åˆ°æ‰©æ•£æ¨¡å‹çš„è¿‡æ¸¡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸“æ³¨äºé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œå¹¶å…‹æœäº†å½“å‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»æ„çš„æ•°æ®é›†åˆ†å‰²ã€ä¸å…¬å¹³çš„æ¯”è¾ƒå’Œè¿‡é«˜çš„è®¡ç®—éœ€æ±‚ã€‚Ai-GenBenchä¸ºç ”ç©¶äººå‘˜å’Œéä¸“å®¶ï¼ˆä¾‹å¦‚è®°è€…ã€äº‹å®æ ¸æŸ¥äººå‘˜ï¼‰æä¾›äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œå¯è®¿é—®çš„å·¥å…·ï¼Œç¡®ä¿äº†å¯é‡å¤æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å®ç”¨çš„åŸ¹è®­è¦æ±‚ã€‚é€šè¿‡åˆ¶å®šæ˜ç¡®çš„è¯„ä¼°è§„åˆ™å’Œå—æ§çš„å¢å¼ºç­–ç•¥ï¼ŒAi-GenBenchä½¿å¾—æ£€æµ‹æ–¹æ³•çš„æ¯”è¾ƒå’Œå¯æ‰©å±•è§£å†³æ–¹æ¡ˆå˜å¾—æ›´æœ‰æ„ä¹‰ã€‚ä»£ç å’Œæ•°æ®å…¬å¼€å¯ç”¨ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§å¹¶æ”¯æŒå¼€å‘ç¨³å¥çš„å–è¯æ£€æµ‹å™¨ï¼Œä»¥è·Ÿä¸Šæ–°çš„åˆæˆç”Ÿæˆå™¨çš„æ­¥ä¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20865v1">PDF</a> 9 pages, 6 figures, 4 tables, code available:   <a target="_blank" rel="noopener" href="https://github.com/MI-BioLab/AI-GenBench">https://github.com/MI-BioLab/AI-GenBench</a></p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆå¼AIçš„å¿«é€Ÿå‘å±•ï¼Œå›¾åƒåˆ›å»ºé¢†åŸŸç»å†äº†é©å‘½æ€§çš„å˜é©ï¼Œè¿™æ—¢ä½¿å¾—æ–‡æœ¬æç¤ºèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼ŒåŒæ—¶ä¹Ÿå¯¹åª’ä½“çœŸå®æ€§æå‡ºäº†ä¸¥å³»æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Ai-GenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„éœ€æ±‚ã€‚ä¸ç°æœ‰ä»…åœ¨é™æ€æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆä¸åŒï¼ŒAi-GenBenché‡‡ç”¨äº†ä¸€ç§ä¸´æ—¶è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æŒ‰ç”Ÿæˆæ¨¡å‹çš„é¡ºåºå¯¹åˆæˆå›¾åƒè¿›è¡Œå†å²æ’åºï¼Œä»¥æµ‹è¯•æ£€æµ‹æ–¹æ³•çš„é€æ­¥è®­ç»ƒèƒ½åŠ›ï¼Œæ—¨åœ¨è¯„ä¼°å…¶é€‚åº”æ–°ç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»GANåˆ°æ‰©æ•£æ¨¡å‹çš„è½¬å˜ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•é‡ç‚¹å…³æ³¨é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ï¼Œå¹¶è§£å†³äº†å½“å‰æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬ä»»æ„æ•°æ®é›†åˆ†å‰²ã€ä¸å…¬å¹³æ¯”è¾ƒå’Œè¿‡é«˜çš„è®¡ç®—éœ€æ±‚ã€‚Ai-GenBenchæä¾›äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ä»¥åŠæ–¹ä¾¿ç ”ç©¶è€…å’Œéä¸“å®¶ï¼ˆå¦‚è®°è€…ã€äº‹å®æ ¸æŸ¥äººå‘˜ï¼‰ä½¿ç”¨çš„å·¥å…·ï¼Œç¡®ä¿å®ç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚é€šè¿‡åˆ¶å®šæ˜ç¡®çš„è¯„ä¼°è§„åˆ™å’Œå—æ§çš„å¢å¼ºç­–ç•¥ï¼ŒAi-GenBenchä¸ºå®ç°æ£€æµ‹æ–¹æ³•çš„æœ‰æ„ä¹‰æ¯”è¾ƒå’Œå¯æ‰©å±•è§£å†³æ–¹æ¡ˆæä¾›äº†å¯èƒ½ã€‚ä»£ç å’Œæ•°æ®å‡å…¬å¼€æä¾›ï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§å’Œæ”¯æŒå¼€å‘å¯é çš„å–è¯æ£€æµ‹å™¨ä»¥é€‚åº”ä¸æ–­å‡ºç°çš„æ–°å‹åˆæˆç”Ÿæˆå™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒåˆ›å»ºé¢†åŸŸçš„å˜é©ã€‚</li>
<li>Ai-GenBenchæ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨è§£å†³ç°å®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„éœ€æ±‚ã€‚</li>
<li>Ai-GenBenché‡‡ç”¨ä¸´æ—¶è¯„ä¼°æ¡†æ¶æ¥æµ‹è¯•æ£€æµ‹æ–¹æ³•çš„é€æ­¥è®­ç»ƒèƒ½åŠ›ã€‚</li>
<li>è¯¥å¹³å°å…³æ³¨é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚</li>
<li>Ai-GenBenchè§£å†³äº†å½“å‰å›¾åƒæ£€æµ‹æ–¹æ³•çš„å¤šä¸ªå…³é”®å±€é™æ€§ã€‚</li>
<li>Ai-GenBenchä¸ºç ”ç©¶è€…å’Œéä¸“å®¶æä¾›äº†ç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œæ–¹ä¾¿ä½¿ç”¨çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de1f46db02d6045bed0238e89bd23c3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d01afa0c1f7ea9d95ed022e36361c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c3c597ba9d9d43689f3d7779b01c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ba876161a9a9a455a9fb21fe5e44811.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db194f98bd94c084f68a365b3bf6c8cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ceb9d0654a8712c1fbe0c3fa606fcec8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Non-Core-Language-Instruction-Following-in-Speech-LLMs-via-Semi-Implicit-Cross-Lingual-CoT-Reasoning"><a href="#Enhancing-Non-Core-Language-Instruction-Following-in-Speech-LLMs-via-Semi-Implicit-Cross-Lingual-CoT-Reasoning" class="headerlink" title="Enhancing Non-Core Language Instruction-Following in Speech LLMs via   Semi-Implicit Cross-Lingual CoT Reasoning"></a>Enhancing Non-Core Language Instruction-Following in Speech LLMs via   Semi-Implicit Cross-Lingual CoT Reasoning</h2><p><strong>Authors:Hongfei Xue, Yufeng Tang, Hexin Liu, Jun Zhang, Xuelong Geng, Lei Xie</strong></p>
<p>Large language models have been extended to the speech domain, leading to the development of speech large language models (SLLMs). While existing SLLMs demonstrate strong performance in speech instruction-following for core languages (e.g., English), they often struggle with non-core languages due to the scarcity of paired speech-text data and limited multilingual semantic reasoning capabilities. To address this, we propose the semi-implicit Cross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates speech-to-text translation into the reasoning process of SLLMs. The XS-CoT generates four types of tokens: instruction and response tokens in both core and non-core languages, enabling cross-lingual transfer of reasoning capabilities. To mitigate inference latency in generating target non-core response tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which progressively compresses the first three types of intermediate reasoning tokens while retaining global reasoning logic during training. By leveraging the robust reasoning capabilities of the core language, XS-CoT improves responses for non-core languages by up to 45% in GPT-4 score when compared to direct supervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN. Moreover, the semi-implicit XS-CoT reduces token delay by more than 50% with a slight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount of high-quality training data for non-core languages by leveraging the reasoning capabilities of core languages. To support training, we also develop a data pipeline and open-source speech instruction-following datasets in Japanese, German, and French. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æ‰©å±•åˆ°è¯­éŸ³é¢†åŸŸï¼Œä»è€Œå‚¬ç”Ÿäº†è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰çš„å‘å±•ã€‚è™½ç„¶ç°æœ‰çš„SLLMåœ¨æ ¸å¿ƒè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰çš„è¯­éŸ³æŒ‡ä»¤æ‰§è¡Œæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹é…å¥—çš„è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œæœ‰é™çš„è·¨è¯­è¨€è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œå®ƒä»¬å¾€å¾€åœ¨éæ ¸å¿ƒè¯­è¨€ä¸Šè¡¨ç°æŒ£æ‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠéšå¼è·¨è¯­è¨€è¯­éŸ³æ€ç»´é“¾ï¼ˆXS-CoTï¼‰æ¡†æ¶ï¼Œå®ƒå°†è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘æ•´åˆåˆ°SLLMçš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚XS-CoTç”Ÿæˆå››ç§ç±»å‹çš„ç¬¦å·ï¼šæ ¸å¿ƒè¯­è¨€å’Œéæ ¸å¿ƒè¯­è¨€ä¸­çš„æŒ‡ä»¤å’Œå“åº”ç¬¦å·ï¼Œå®ç°è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„è¿ç§»ã€‚ä¸ºäº†å‡è½»ç”Ÿæˆç›®æ ‡éæ ¸å¿ƒå“åº”ç¬¦å·æ—¶çš„æ¨ç†å»¶è¿Ÿï¼Œæˆ‘ä»¬åœ¨XS-CoTä¸­èå…¥äº†åŠéšå¼CoTæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‹ç¼©å‰ä¸‰ç§ä¸­é—´æ¨ç†ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™å…¨å±€æ¨ç†é€»è¾‘ã€‚é€šè¿‡åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„ç¨³å¥æ¨ç†èƒ½åŠ›ï¼ŒXS-CoTåœ¨ä¸å…¶ä»–ä¸¤ç§å…·æœ‰ä»£è¡¨æ€§çš„SLLMæ¨¡å‹Qwen2-Audioå’ŒSALMONNè¿›è¡Œå¯¹æ¯”æ—¶ï¼Œå¯¹éæ ¸å¿ƒè¯­è¨€çš„å“åº”æé«˜äº†é«˜è¾¾GPT-4åˆ†æ•°çš„45%ã€‚æ­¤å¤–ï¼ŒåŠéšå¼çš„XS-CoTå‡å°‘äº†è¶…è¿‡ä¸€åŠçš„ç¬¦å·å»¶è¿Ÿï¼ŒåŒæ—¶GPT-4åˆ†æ•°ç•¥æœ‰ä¸‹é™ã€‚é‡è¦çš„æ˜¯ï¼ŒXS-CoTåªéœ€è¦åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ºå°‘é‡é«˜è´¨é‡çš„éæ ¸å¿ƒè¯­è¨€è®­ç»ƒæ•°æ®æä¾›æ”¯æŒã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œå¹¶å…¬å¼€äº†æ—¥è¯­ã€å¾·è¯­å’Œæ³•è¯­ç­‰è¯­éŸ³æŒ‡ä»¤æ‰§è¡Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20835v1">PDF</a> 10 pages, 6 figures, Submitted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æ‰©å±•åˆ°è¯­éŸ³é¢†åŸŸï¼Œå¯¼è‡´è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰çš„å‘å±•ã€‚ç°æœ‰SLLMåœ¨æ ¸å¿ƒè¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰çš„è¯­éŸ³æŒ‡ä»¤æ‰§è¡Œæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éæ ¸å¿ƒè¯­è¨€æ–¹é¢ç”±äºç¼ºå°‘é…å¥—çš„è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œæœ‰é™çš„è·¨è¯­è¨€è¯­ä¹‰æ¨ç†èƒ½åŠ›è€Œç»å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŠéšå¼è·¨è¯­è¨€è¯­éŸ³æ€ç»´é“¾ï¼ˆXS-CoTï¼‰æ¡†æ¶ï¼Œå°†è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘èå…¥SLLMçš„æ¨ç†è¿‡ç¨‹ã€‚XS-CoTç”Ÿæˆå››ç§ç±»å‹çš„ç¬¦å·ï¼šæ ¸å¿ƒå’Œéæ ¸å¿ƒè¯­è¨€ä¸­çš„æŒ‡ä»¤å’Œå“åº”ç¬¦å·ï¼Œå®ç°è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„è½¬ç§»ã€‚ä¸ºå‡å°‘ç”Ÿæˆç›®æ ‡éæ ¸å¿ƒå“åº”ç¬¦å·çš„æ¨ç†å»¶è¿Ÿï¼Œæˆ‘ä»¬åœ¨XS-CoTä¸­çº³å…¥äº†åŠéšå¼CoTæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‹ç¼©å‰ä¸‰ç§ç±»å‹çš„ä¸­é—´æ¨ç†ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™å…¨å±€æ¨ç†é€»è¾‘ã€‚é€šè¿‡åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„ç¨³å¥æ¨ç†èƒ½åŠ›ï¼ŒXS-CoTåœ¨å¯¹æ¯”ä¸¤ä¸ªä»£è¡¨æ€§SLLMæ¨¡å‹Qwen2-Audioå’ŒSALMONNæ—¶ï¼Œå¯¹éæ ¸å¿ƒè¯­è¨€çš„å“åº”æé«˜äº†45%ã€‚æ­¤å¤–ï¼ŒåŠéšå¼çš„XS-CoTå°†ç¬¦å·å»¶è¿Ÿå‡å°‘äº†è¶…è¿‡50%ï¼ŒåŒæ—¶GPT-4å¾—åˆ†ç•¥æœ‰ä¸‹é™ã€‚XS-CoTåªéœ€è¦å°‘é‡é«˜è´¨é‡çš„éæ ¸å¿ƒè¯­è¨€è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„æ¨ç†èƒ½åŠ›å³å¯æ”¯æŒè®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªæ•°æ®ç®¡é“ï¼Œå¹¶å…¬å¼€äº†æ—¥è¯­ã€å¾·è¯­å’Œæ³•è¯­çš„å£°éŸ³æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²æ‰©å±•åˆ°è¯­éŸ³é¢†åŸŸï¼Œä½†éæ ¸å¿ƒè¯­è¨€çš„è¯­éŸ³æŒ‡ä»¤æ‰§è¡Œå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>XS-CoTæ¡†æ¶é€šè¿‡é›†æˆè¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘ï¼Œå¢å¼ºäº†SLLMçš„è·¨è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>XS-CoTç”Ÿæˆå››ç§ç±»å‹çš„ç¬¦å·ï¼Œå®ç°è·¨è¯­è¨€èƒ½åŠ›çš„è½¬ç§»ã€‚</li>
<li>åŠéšå¼CoTæ–¹æ¡ˆå‡å°‘äº†ç”Ÿæˆéæ ¸å¿ƒè¯­è¨€å“åº”çš„æ¨ç†å»¶è¿Ÿã€‚</li>
<li>XS-CoTæé«˜äº†éæ ¸å¿ƒè¯­è¨€çš„å“åº”èƒ½åŠ›ï¼Œä¸ç›´æ¥ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒGPT-4å¾—åˆ†æé«˜äº†45%ã€‚</li>
<li>åŠéšå¼XS-CoTé™ä½äº†ç¬¦å·å»¶è¿Ÿï¼ŒåŒæ—¶ç•¥å¾®é™ä½äº†GPT-4å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6681e18f064e2941ae3214cba61e5ff" align="middle">
<img src="https://pic1.zhimg.com/v2-110126185ccfef4c691b7c4a7011671e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9adf7c8c9c2ca386de1f2df1a569948d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6bff61b3a3dd1399359b4e3161d791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a82d32d625949d627e1e9e269267e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6bbe7df84105ddaa62d3f8f275cec88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-546d67a83a2c5d771e8601d1b4245de7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Turing-Machine-Evaluation-for-Large-Language-Model"><a href="#Turing-Machine-Evaluation-for-Large-Language-Model" class="headerlink" title="Turing Machine Evaluation for Large Language Model"></a>Turing Machine Evaluation for Large Language Model</h2><p><strong>Authors:Haitao Wu, Zongbo Han, Huaxi Huang, Changqing Zhang</strong></p>
<p>With the rapid development and widespread application of Large Language Models (LLMs), rigorous evaluation has become particularly crucial. This research adopts a novel perspective, focusing on evaluating the core computational reasoning ability of LLMs, defined as the capacity of model to accurately understand rules, and execute logically computing operations. This capability assesses the reliability of LLMs as precise executors, and is critical to advanced tasks such as complex code generation and multi-step problem-solving. We propose an evaluation framework based on Universal Turing Machine (UTM) simulation. This framework requires LLMs to strictly follow instructions and track dynamic states, such as tape content and read&#x2F;write head position, during multi-step computations. To enable standardized evaluation, we developed TMBench, a benchmark for systematically studying the computational reasoning capabilities of LLMs. TMBench provides several key advantages, including knowledge-agnostic evaluation, adjustable difficulty, foundational coverage through Turing machine encoding, and unlimited capacity for instance generation, ensuring scalability as models continue to evolve. We find that model performance on TMBench correlates strongly with performance on other recognized reasoning benchmarks (Pearson correlation coefficient is 0.73), clearly demonstrating that computational reasoning is a significant dimension for measuring the deep capabilities of LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/HaitaoWuTJU/Turing-Machine-Bench">https://github.com/HaitaoWuTJU/Turing-Machine-Bench</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨ï¼Œä¸¥æ ¼è¯„ä¼°å˜å¾—å°¤ä¸ºå…³é”®ã€‚æœ¬ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è§†è§’ï¼Œä¸“æ³¨äºè¯„ä¼°LLMsçš„æ ¸å¿ƒè®¡ç®—æ¨ç†èƒ½åŠ›ï¼Œè¿™è¢«å®šä¹‰ä¸ºæ¨¡å‹å‡†ç¡®ç†è§£è§„åˆ™å¹¶æ‰§è¡Œé€»è¾‘è®¡ç®—æ“ä½œçš„èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›è¯„ä¼°äº†LLMsä½œä¸ºç²¾ç¡®æ‰§è¡Œè€…çš„å¯é æ€§ï¼Œå¯¹äºå¤æ‚ä»£ç ç”Ÿæˆå’Œå¤šæ­¥é—®é¢˜è§£å†³ç­‰é«˜çº§ä»»åŠ¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºé€šç”¨å›¾çµæœºï¼ˆUTMï¼‰æ¨¡æ‹Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶è¦æ±‚LLMsåœ¨å¤šæ­¥è®¡ç®—è¿‡ç¨‹ä¸­ä¸¥æ ¼éµå®ˆæŒ‡ä»¤å¹¶è·Ÿè¸ªåŠ¨æ€çŠ¶æ€ï¼Œä¾‹å¦‚ç£å¸¦å†…å®¹ä»¥åŠè¯»å†™å¤´ä½ç½®ã€‚ä¸ºäº†å®ç°æ ‡å‡†åŒ–è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†TMBenchï¼Œè¿™æ˜¯ç”¨äºç³»ç»Ÿç ”ç©¶LLMsè®¡ç®—æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚TMBenchæä¾›äº†å‡ ä¸ªå…³é”®ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬ä¸çŸ¥è¯†æ— å…³çš„è¯„ä»·ã€å¯è°ƒæ•´çš„éš¾åº¦ã€é€šè¿‡å›¾çµæœºç¼–ç çš„åŸºç¡€è¦†ç›–ï¼Œä»¥åŠæ— é™çš„å®ä¾‹ç”Ÿæˆèƒ½åŠ›ï¼Œç¡®ä¿éšç€æ¨¡å‹çš„ä¸æ–­å‘å±•è€Œå…·æœ‰å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¨¡å‹åœ¨TMBenchä¸Šçš„è¡¨ç°ä¸å…¶ä»–å…¬è®¤çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼ˆçš®å°”é€Šç›¸å…³ç³»æ•°è¾¾åˆ°0.73ï¼‰ï¼Œè¿™æ¸…æ¥šåœ°è¡¨æ˜è®¡ç®—æ¨ç†æ˜¯è¡¡é‡LLMsæ·±å±‚èƒ½åŠ›çš„é‡è¦ç»´åº¦ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HaitaoWuTJU/Turing-Machine-Bench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HaitaoWuTJU/Turing-Machine-Benchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20771v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•å’Œå¹¿æ³›åº”ç”¨ï¼Œå¯¹å…¶ä¸¥æ ¼è¯„ä¼°å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬ç ”ç©¶ä»æ–°é¢–çš„è§’åº¦è¯„ä¼°LLMsçš„æ ¸å¿ƒè®¡ç®—æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡å‹å‡†ç¡®ç†è§£è§„åˆ™å’Œæ‰§è¡Œé€»è¾‘è®¡ç®—æ“ä½œçš„èƒ½åŠ›ã€‚ä¸ºæ ‡å‡†åŒ–è¯„ä¼°ï¼Œç ”ç©¶æå‡ºäº†åŸºäºé€šç”¨å›¾çµæœºï¼ˆUTMï¼‰æ¨¡æ‹Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼€å‘äº†TMBenchåŸºå‡†æµ‹è¯•ï¼Œå¯ç³»ç»Ÿåœ°ç ”ç©¶LLMsçš„è®¡ç®—æ¨ç†èƒ½åŠ›ã€‚TMBenchæä¾›çŸ¥è¯†æ— å…³è¯„ä¼°ã€å¯è°ƒéš¾åº¦ã€å›¾çµæœºç¼–ç åŸºç¡€è¦†ç›–å’Œæ— é™å®ä¾‹ç”Ÿæˆèƒ½åŠ›ç­‰ä¼˜ç‚¹ï¼Œç¡®ä¿éšç€æ¨¡å‹å‘å±•å…·æœ‰å¯æ‰©å±•æ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒTMBenchä¸Šçš„æ¨¡å‹æ€§èƒ½ä¸å…¶ä»–å…¬è®¤çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ï¼ˆçš®å°”é€Šç›¸å…³ç³»æ•°0.7in the evaluation of LLMsï¼‰ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HaitaoWuTJU/Turing-Machine-Bench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HaitaoWuTJU/Turing-Machine-Benchæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ ¸å¿ƒè®¡ç®—æ¨ç†èƒ½åŠ›è¯„ä¼°å˜å¾—é‡è¦ã€‚</li>
<li>ç ”ç©¶å…³æ³¨äºLLMså‡†ç¡®ç†è§£è§„åˆ™å’Œæ‰§è¡Œé€»è¾‘è®¡ç®—æ“ä½œçš„èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>æå‡ºåŸºäºé€šç”¨å›¾çµæœºï¼ˆUTMï¼‰æ¨¡æ‹Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>å¼€å‘äº†TMBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿåœ°ç ”ç©¶LLMsçš„è®¡ç®—æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TMBenchæä¾›çŸ¥è¯†æ— å…³è¯„ä¼°ã€å¯è°ƒéš¾åº¦ã€å›¾çµæœºç¼–ç åŸºç¡€è¦†ç›–å’Œæ— é™å®ä¾‹ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>TMBenchä¸Šçš„æ¨¡å‹æ€§èƒ½ä¸å…¶ä»–æ¨ç†åŸºå‡†æµ‹è¯•å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c4e1ca321570ecf48b1369e584c145e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2aa78368cf8e3c3c0b7548b207d2d4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c2c19cb567ba6295864ffea6e11f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8333180c097d779afe43ad4e3d9ccfa5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers"><a href="#Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers" class="headerlink" title="Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers"></a>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers</h2><p><strong>Authors:Roman Abramov, Felix Steinbauer, Gjergji Kasneci</strong></p>
<p>Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models. </p>
<blockquote>
<p>Transformeråœ¨è®¸å¤šNLPä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥éª¤äº‹å®æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•ŒçŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æœ€è¿‘çš„Grokkinï¼ˆä¸€ç§å­¦ä¹ èƒ½åŠ›ï¼‰çš„ç ”ç©¶è¿›å±•è¡¨æ˜ï¼Œä¸€æ—¦ç¥ç»ç½‘ç»œå‘ç°æ½œåœ¨çš„é€»è¾‘æ¨¡å¼ï¼Œå®ƒä»¬å°±å¯ä»¥ä»è®°å¿†è½¬å˜ä¸ºå®Œç¾æ¦‚æ‹¬â€”â€”ä½†è¿™äº›ç ”ç©¶ä¸»è¦ä½¿ç”¨çš„æ˜¯å°è§„æ¨¡çš„åˆæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†Grokkinæ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åˆæˆæ•°æ®å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±æ¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§çš„æŒ‘æˆ˜ï¼Œä»è€Œæé«˜æ¨æ–­äº‹å®ä¸åŸå­äº‹å®çš„æ¯”ä¾‹$\phi_r$ï¼Œä½¿å…¶è¶…è¿‡å®ç°Grokkinæ‰€éœ€çš„é˜ˆå€¼ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯äº‹å®ä¸Šä¸æ­£ç¡®çš„åˆæˆæ•°æ®ä¹Ÿå¯ä»¥åŠ å¼ºæ–°å…´çš„æ¨ç†ç”µè·¯ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒè¿«ä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œä¸æ˜¯è®°å¿†ã€‚åœ¨è¯„ä¼°å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨2WikiMultiHopQAä¸Šè¾¾åˆ°äº†95-100%çš„å‡†ç¡®ç‡â€”â€”è¿™å¤§å¤§è¶…è¿‡äº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•å¹¶åŒ¹é…æˆ–è¶…è¿‡äº†å½“å‰æœ€æ–°çš„ç»“æœã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†å¦‚ä½•æé«˜$\phi_r$åœ¨Transformerå†…éƒ¨å½¢æˆæ¦‚æ‹¬ç”µè·¯çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºGrokkinçš„æ•°æ®å¢å¼ºå¯ä»¥è§£é”éšå¼çš„å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ›´ç¨³å¥å’Œå¯è§£é‡Šçš„äº‹å®æ¨ç†æ‰“å¼€äº†å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20752v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†GrokkingæŠ€æœ¯æ‰©å±•åˆ°çœŸå®ä¸–ç•Œäº‹å®æ•°æ®æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é€šè¿‡å¢åŠ åˆæˆæ•°æ®æ¥å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±çš„æ–¹æ³•ï¼Œä»¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯äº‹å®é”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿèƒ½å¼ºåŒ–æ–°å…´æ¨ç†ç”µè·¯ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹¶åœ¨2WikiMultiHopQAä¸Šå®ç°äº†é«˜è¾¾95-100%çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œé€šè¿‡å¢åŠ $\phi_r$å¯ä»¥æ¨åŠ¨Transformerå†…éƒ¨å½¢æˆé€šç”¨ç”µè·¯ï¼Œè¿™è¡¨æ˜åŸºäºGrokkingçš„æ•°æ®å¢å¼ºå¯ä»¥è§£é”éšå¼å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ›´ç¨³å¥å’Œå¯è§£é‡Šçš„äº‹å®æ¨ç†æ‰“å¼€äº†å¤§é—¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformersåœ¨NLPä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥æ¨ç†æ–¹é¢ä»æœ‰æ‰€æ¬ ç¼ºï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®ä¸–ç•ŒçŸ¥è¯†ç¨€ç–çš„æƒ…å†µä¸‹ã€‚</li>
<li>GrokkingæŠ€æœ¯çš„æ‰©å±•å¯¹äºå¤„ç†çœŸå®ä¸–ç•Œçš„äº‹å®æ•°æ®å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>é€šè¿‡å¢åŠ åˆæˆæ•°æ®æ¥å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±ï¼Œå¯ä»¥è§£å†³æ•°æ®é›†ç¨€ç–æ€§é—®é¢˜ã€‚</li>
<li>å³ä½¿æ˜¯äº‹å®é”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿèƒ½å¼ºåŒ–æ–°å…´æ¨ç†ç”µè·¯ï¼Œæ¨åŠ¨æ¨¡å‹æ›´å¤šåœ°ä¾èµ–å…³ç³»ç»“æ„è€Œéè®°å¿†ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
<li>å¢åŠ $\phi_r$æœ‰åŠ©äºåœ¨Transformerå†…éƒ¨å½¢æˆé€šç”¨ç”µè·¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdb74d5a7f76ff736ad815680153d268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a55d5cbe8ee683dc6e752d80e2a233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b9b7b6e7023119aa898965d466f8aab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e554fd2d70706b64cfe4ff222dedc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76abdaf2dcffe85e82b856b4bbf98e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6591030219d9ec9b02801b6e8e3fc520.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Last-Answer-Your-Reasoning-Trace-Uncovers-More-than-You-Think"><a href="#Beyond-the-Last-Answer-Your-Reasoning-Trace-Uncovers-More-than-You-Think" class="headerlink" title="Beyond the Last Answer: Your Reasoning Trace Uncovers More than You   Think"></a>Beyond the Last Answer: Your Reasoning Trace Uncovers More than You   Think</h2><p><strong>Authors:Hasan Abed Al Kader Hammoud, Hani Itani, Bernard Ghanem</strong></p>
<p>Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the modelâ€™s optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the modelâ€™s confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13% and 10% respectively. Implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/hammoudhasan/SubthoughtReasoner">https://github.com/hammoudhasan/SubthoughtReasoner</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é€æ­¥æ¨ç†æ¥è§£å†³å¤æ‚é—®é¢˜ã€‚æ ‡å‡†çš„è¯„ä¼°å®è·µåŒ…æ‹¬ç”Ÿæˆå®Œæ•´çš„æ¨ç†è¿‡ç¨‹å¹¶è¯„ä¼°æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜æ¥è´¨ç–‘å¯¹æœ€ç»ˆç­”æ¡ˆçš„ä¾èµ–ï¼šæœ€ç»ˆç­”æ¡ˆæ˜¯å¦å¯é åœ°ä»£è¡¨æ¨¡å‹çš„ä¼˜åŒ–ç»“è®ºï¼Ÿæ›¿ä»£æ¨ç†è·¯å¾„ä¼šäº§ç”Ÿä¸åŒçš„ç»“æœå—ï¼Ÿä¸ºäº†å›ç­”è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸­é—´çš„æ¨ç†æ­¥éª¤ï¼Œç§°ä¸ºå­æ€æƒ³ï¼Œå¹¶åŸºäºæˆ‘ä»¬çš„å‘ç°æå‡ºäº†ä¸€ç§æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠæ ¹æ®è¯­è¨€çº¿ç´¢å°†æ¨ç†è¿‡ç¨‹åˆ†å‰²æˆè¿ç»­çš„å­æ€æƒ³ã€‚æˆ‘ä»¬ä»æ¯ä¸ªä¸­é—´å­æ€æƒ³çš„ç»ˆç‚¹å¼€å§‹æç¤ºæ¨¡å‹ç”Ÿæˆè¿ç»­å†…å®¹ã€‚æˆ‘ä»¬ä»ä¸åŒå­æ€æƒ³äº§ç”Ÿçš„æ¯ä¸ªå®Œæ•´è¿ç»­å†…å®¹ä¸­æå–æ½œåœ¨ç­”æ¡ˆã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç­”æ¡ˆï¼ˆä¼—æ•°ï¼‰è¿›è¡Œèšåˆï¼Œé€šå¸¸æ¯”ä»…ä¾èµ–ä»åŸå§‹å®Œæ•´è¿½è¸ªä¸­å¾—å‡ºçš„ç­”æ¡ˆäº§ç”Ÿæ›´é«˜çš„å‡†ç¡®æ€§ã€‚åˆ†æä¸åŒå­æ€æƒ³å¾—å‡ºçš„ç­”æ¡ˆä¹‹é—´çš„ä¸€è‡´æ€§æ­ç¤ºäº†ä¸æ¨¡å‹çš„ä¿¡å¿ƒå’Œæ­£ç¡®æ€§ç›¸å…³çš„ç‰¹å¾ï¼Œè¿™è¡¨æ˜æœ‰æ½œåŠ›è¯†åˆ«å‡ºä¸å¤ªå¯é çš„ç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„LLMå’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼ˆAIME2024å’ŒAIME2025ï¼‰ä¸Šçš„å®éªŒæ˜¾ç¤ºäº†æŒç»­çš„å‡†ç¡®æ€§æ”¹è¿›ï¼Œå¢ç›Šåˆ†åˆ«é«˜è¾¾13%å’Œ10%ã€‚å®æ–½æ–¹æ³•å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/hammoudhasan/SubthoughtReasoner%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hammoudhasan/SubthoughtReasonerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20708v1">PDF</a> Preprint</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é€æ­¥æ¨ç†è§£å†³å¤æ‚é—®é¢˜ã€‚æœ¬æ–‡è´¨ç–‘äº†ä»…ä¾èµ–æœ€ç»ˆç­”æ¡ˆè¯„ä¼°æ¨¡å‹è¡¨ç°çš„æ–¹æ³•ï¼Œæå‡ºåˆ†æä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆç§°ä¸ºå­æ€ç»´ï¼‰çš„æ–¹æ³•ã€‚é€šè¿‡åˆ†å‰²æ¨ç†è½¨è¿¹ï¼Œä»æ¯ä¸ªä¸­é—´å­æ€ç»´çš„ç»ˆç‚¹å¼•å¯¼æ¨¡å‹ç”Ÿæˆå»¶ç»­å†…å®¹ï¼Œå¹¶ä»ä¸­æå–æ½œåœ¨ç­”æ¡ˆã€‚å‘ç°èšåˆè¿™äº›ç­”æ¡ˆä¸­çš„æœ€å¸¸è§ç­”æ¡ˆï¼ˆä¼—æ•°ï¼‰é€šå¸¸æ¯”ä»…ä¾èµ–åŸå§‹å®Œæ•´è½¨è¿¹å¾—å‡ºçš„ç­”æ¡ˆæ›´å‡†ç¡®ã€‚åˆ†æä¸åŒå­æ€ç»´å¾—å‡ºçš„ç­”æ¡ˆä¸€è‡´æ€§å¯æ­ç¤ºä¸æ¨¡å‹è‡ªä¿¡å’Œæ­£ç¡®æ€§ç›¸å…³çš„ç‰¹å¾ï¼Œæœ‰åŠ©äºè¯†åˆ«ä¸å¤ªå¯é çš„ç­”æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šä¸ªLLMå’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æŒç»­æé«˜å‡†ç¡®æ€§ï¼Œæå‡å¹…åº¦é«˜è¾¾13%å’Œ10%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨é€æ­¥æ¨ç†è§£å†³å¤æ‚é—®é¢˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•è¿‡äºä¾èµ–æœ€ç»ˆç­”æ¡ˆï¼Œå¯èƒ½å¿½ç•¥äº†æ¨¡å‹çš„ä¼˜åŒ–ç»“è®ºã€‚</li>
<li>æ–‡ä¸­æå‡ºåˆ†æä¸­é—´æ¨ç†æ­¥éª¤çš„æ–¹æ³•ï¼Œç§°ä¸ºå­æ€ç»´ã€‚</li>
<li>é€šè¿‡å¼•å¯¼æ¨¡å‹ä»æ¯ä¸ªä¸­é—´å­æ€ç»´çš„ç»ˆç‚¹ç”Ÿæˆå»¶ç»­å†…å®¹ï¼Œå¹¶æå–æ½œåœ¨ç­”æ¡ˆï¼Œå‘ç°èšåˆå¸¸è§ç­”æ¡ˆå¯æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>åˆ†æä¸åŒå­æ€ç»´å¾—å‡ºçš„ç­”æ¡ˆä¸€è‡´æ€§æœ‰åŠ©äºè¯†åˆ«æ¨¡å‹çš„è‡ªä¿¡å’Œæ­£ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªLLMå’Œæ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šå®ç°å‡†ç¡®æ€§æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d1c83104e0cb19bf79c03a52ebe0217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b47030bb3937bc3e0b9d2b9d41a833.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ReasonIR-Training-Retrievers-for-Reasoning-Tasks"><a href="#ReasonIR-Training-Retrievers-for-Reasoning-Tasks" class="headerlink" title="ReasonIR: Training Retrievers for Reasoning Tasks"></a>ReasonIR: Training Retrievers for Reasoning Tasks</h2><p><strong>Authors:Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, Luke Zettlemoyer</strong></p>
<p>We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºReasonIR-8Bï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºä¸€èˆ¬æ¨ç†ä»»åŠ¡è®­ç»ƒçš„ç¬¬ä¸€ä¸ªæ£€ç´¢å™¨ã€‚ç°æœ‰æ£€ç´¢å™¨åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ”¶ç›Šæœ‰é™ï¼Œéƒ¨åˆ†åŸå› åœ¨äºç°æœ‰è®­ç»ƒæ•°æ®é›†ä¸“æ³¨äºä¸æ–‡æ¡£ç›´æ¥ç›¸å…³çš„ç®€çŸ­äº‹å®æŸ¥è¯¢ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œè¯¥ç®¡é“é’ˆå¯¹æ¯ç¯‡æ–‡æ¡£ï¼Œåˆ›å»ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œç›¸å…³æ€§çš„æŸ¥è¯¢ï¼Œä»¥åŠä¸€ä¸ªå¯èƒ½ç›¸å…³ä½†æœ€ç»ˆæ— ç”¨çš„ç¡¬è´Ÿæ ·æœ¬ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„åˆæˆæ•°æ®å’Œç°æœ‰å…¬å…±æ•°æ®çš„æ··åˆè®­ç»ƒä¸Šï¼ŒReasonIR-8Båœ¨BRIGHTè¿™ä¸€å¹¿æ³›ä½¿ç”¨çš„æ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ— éœ€é‡æ–°æ’åè€…å³å¯å®ç°29.9çš„nDCG@10ï¼Œä½¿ç”¨é‡æ–°æ’åè€…åˆ™ä¸º36.9çš„nDCG@10ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚å½“åº”ç”¨äºRAGä»»åŠ¡æ—¶ï¼ŒReasonIR-8Bç›¸å¯¹äºå°é—­ä¹¦ç±åŸºçº¿æé«˜äº†MMLUå’ŒGPQAçš„æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†6.4%å’Œ22.6%ï¼Œä¼˜äºå…¶ä»–æ£€ç´¢å™¨å’Œæœç´¢å¼•æ“ã€‚æ­¤å¤–ï¼ŒReasonIR-8Bæ›´æœ‰æ•ˆåœ°ä½¿ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—ï¼šåœ¨BRIGHTä¸Šï¼Œå…¶æ€§èƒ½éšç€æ›´é•¿çš„ä¿¡æ¯ä¸°å¯Œçš„é‡å†™æŸ¥è¯¢è€ŒæŒç»­æé«˜ï¼›å½“ä¸å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°æ’åå™¨ç»“åˆæ—¶ï¼Œå®ƒä»ç„¶è¡¨ç°ä¼˜äºå…¶ä»–æ£€ç´¢å™¨ã€‚æˆ‘ä»¬çš„è®­ç»ƒé…æ–¹æ˜¯é€šç”¨çš„ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°æœªæ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼›ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20595v1">PDF</a> Our code is released at   \url{<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/ReasonIR%7D">https://github.com/facebookresearch/ReasonIR}</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ReasonIR-8Bï¼Œé¦–ä¸ªä¸“é—¨ç”¨äºä¸€èˆ¬æ¨ç†ä»»åŠ¡çš„æ£€ç´¢å™¨ã€‚ç°æœ‰çš„æ£€ç´¢å™¨åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„æ”¶ç›Šæœ‰é™ï¼Œå› ä¸ºç°æœ‰çš„è®­ç»ƒæ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ä¸æ–‡æ¡£ç›´æ¥ç›¸å…³çš„ç®€çŸ­äº‹å®æŸ¥è¯¢ä¸Šã€‚å¼€å‘äº†ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå…·æœ‰æŒ‘æˆ˜æ€§å’Œç›¸å…³æ€§çš„æŸ¥è¯¢ï¼Œä»¥åŠå¯èƒ½ç›¸å…³ä½†æœ€ç»ˆä¸ç›¸å…³çš„ç¡¬è´Ÿæ ·æœ¬ã€‚ReasonIR-8Båœ¨å¹¿æ³›ä½¿ç”¨çš„æ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•BRIGHTä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œæ— æ’åå™¨æ—¶è¾¾åˆ°29.9 nDCG@10ï¼Œæœ‰æ’åå™¨æ—¶è¾¾åˆ°36.9 nDCG@10ã€‚åœ¨RAGä»»åŠ¡ä¸­ï¼ŒReasonIR-8Bç›¸è¾ƒäºå°é—­ä¹¦ç±çš„åŸºçº¿ï¼Œå¯¹MMLUå’ŒGPQAçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†6.4%å’Œ22.6%ã€‚æ­¤å¤–ï¼ŒReasonIR-8Bæ›´æœ‰æ•ˆåœ°ä½¿ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—ï¼šåœ¨BRIGHTä¸Šï¼Œå…¶æ€§èƒ½éšç€æ›´é•¿çš„ä¿¡æ¯ä¸°å¯Œçš„é‡å†™æŸ¥è¯¢è€ŒæŒç»­æé«˜ï¼›å½“ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ’åå™¨ç»“åˆæ—¶ï¼Œå®ƒç»§ç»­ä¼˜äºå…¶ä»–æ£€ç´¢å™¨ã€‚å…¬å¼€äº†ä»£ç ã€æ•°æ®å’Œæ¨¡å‹ï¼Œä»¥ä¾¿æœªæ¥å¤§å‹è¯­è¨€æ¨¡å‹æ‰©å±•ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReasonIR-8Bæ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºä¸€èˆ¬æ¨ç†ä»»åŠ¡çš„æ£€ç´¢å™¨ã€‚</li>
<li>ç°æœ‰æ£€ç´¢å™¨åœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®é›†ä¸»è¦å…³æ³¨ç®€çŸ­çš„äº‹å®æŸ¥è¯¢ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåˆæˆæ•°æ®ç”Ÿæˆç®¡é“ï¼Œä¸ºæ¯ç¯‡æ–‡æ¡£åˆ›å»ºæŒ‘æˆ˜æ€§çš„ç›¸å…³æŸ¥è¯¢å’Œç¡¬è´Ÿæ ·æœ¬ã€‚</li>
<li>ReasonIR-8Båœ¨æ¨ç†å¯†é›†å‹ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆBRIGHTï¼‰ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚</li>
<li>åœ¨RAGä»»åŠ¡ä¸­ï¼ŒReasonIR-8Bçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ£€ç´¢å™¨å’Œæœç´¢å¼•æ“ã€‚</li>
<li>ReasonIR-8Bæ›´æœ‰æ•ˆåœ°ä½¿ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œæ€§èƒ½éšæ›´ä¸°å¯Œçš„æŸ¥è¯¢è€Œæé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9c5759078f514dc56bd9b75c18f0969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bb1c4a433a897a5a4463dbf9cd388dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe019685abccf0b3a5d052be07b55a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebb8ca38c8f18ac50aece700fe04c430.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Independent-Learning-in-Performative-Markov-Potential-Games"><a href="#Independent-Learning-in-Performative-Markov-Potential-Games" class="headerlink" title="Independent Learning in Performative Markov Potential Games"></a>Independent Learning in Performative Markov Potential Games</h2><p><strong>Authors:Rilind Sahitaj, Paulius Sasnauskas, YiÄŸit YalÄ±n, Debmalya Mandal, Goran RadanoviÄ‡</strong></p>
<p>Performative Reinforcement Learning (PRL) refers to a scenario in which the deployed policy changes the reward and transition dynamics of the underlying environment. In this work, we study multi-agent PRL by incorporating performative effects into Markov Potential Games (MPGs). We introduce the notion of a performatively stable equilibrium (PSE) and show that it always exists under a reasonable sensitivity assumption. We then provide convergence results for state-of-the-art algorithms used to solve MPGs. Specifically, we show that independent policy gradient ascent (IPGA) and independent natural policy gradient (INPG) converge to an approximate PSE in the best-iterate sense, with an additional term that accounts for the performative effects. Furthermore, we show that INPG asymptotically converges to a PSE in the last-iterate sense. As the performative effects vanish, we recover the convergence rates from prior work. For a special case of our game, we provide finite-time last-iterate convergence results for a repeated retraining approach, in which agents independently optimize a surrogate objective. We conduct extensive experiments to validate our theoretical findings. </p>
<blockquote>
<p>è¡¨ç°å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰æŒ‡çš„æ˜¯éƒ¨ç½²çš„ç­–ç•¥æ”¹å˜åŸºç¡€ç¯å¢ƒçš„å¥–åŠ±å’Œè¿‡æ¸¡åŠ¨æ€çš„ä¸€ç§åœºæ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†è¡¨ç°æ•ˆæœçº³å…¥é©¬å°”å¯å¤«æ½œåœ¨åšå¼ˆï¼ˆMPGsï¼‰ï¼Œç ”ç©¶äº†å¤šä»£ç†PRLã€‚æˆ‘ä»¬å¼•å…¥äº†è¡¨ç°ç¨³å®šå‡è¡¡ï¼ˆPSEï¼‰çš„æ¦‚å¿µï¼Œå¹¶åœ¨åˆç†çš„æ•æ„Ÿæ€§å‡è®¾ä¸‹è¯æ˜å…¶æ€»æ˜¯å­˜åœ¨ã€‚ç„¶åï¼Œæˆ‘ä»¬æä¾›äº†ç”¨äºè§£å†³MPGsçš„å…ˆè¿›ç®—æ³•çš„æ”¶æ•›ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç‹¬ç«‹ç­–ç•¥æ¢¯åº¦ä¸Šå‡æ³•ï¼ˆIPGAï¼‰å’Œç‹¬ç«‹è‡ªç„¶ç­–ç•¥æ¢¯åº¦æ³•ï¼ˆINPGï¼‰åœ¨æœ€ä½³è¿­ä»£æ„ä¹‰ä¸Šä»¥è¡¨ç°æ•ˆæœä¸ºé¢å¤–é¡¹çš„è¿‘ä¼¼PSEæ”¶æ•›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†INPGæœ€ç»ˆä¼šæ¸è¿‘æ”¶æ•›åˆ°PSEã€‚éšç€è¡¨ç°æ•ˆæœçš„æ¶ˆå¤±ï¼Œæˆ‘ä»¬æ¢å¤äº†ä¹‹å‰çš„æ”¶æ•›é€Ÿç‡ã€‚å¯¹äºæˆ‘ä»¬æ¸¸æˆçš„ç‰¹æ®Šæƒ…å†µï¼Œæˆ‘ä»¬ä¸ºé‡å¤å†è®­ç»ƒçš„æ–¹æ³•æä¾›äº†æœ‰é™çš„æœ€ç»ˆè¿­ä»£æ”¶æ•›ç»“æœï¼Œå…¶ä¸­ä»£ç†ç‹¬ç«‹åœ°ä¼˜åŒ–æ›¿ä»£ç›®æ ‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒæ¥éªŒè¯æˆ‘ä»¬çš„ç†è®ºå‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20593v1">PDF</a> AISTATS 2025, code available at   <a target="_blank" rel="noopener" href="https://github.com/PauliusSasnauskas/performative-mpgs">https://github.com/PauliusSasnauskas/performative-mpgs</a></p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰ä¸­çš„éƒ¨ç½²ç­–ç•¥ä¼šæ”¹å˜åŸºç¡€ç¯å¢ƒçš„å¥–åŠ±å’Œè¿‡æ¸¡åŠ¨æ€ã€‚æœ¬ç ”ç©¶å°†è¡¨æ¼”æ•ˆæœèå…¥é©¬å°”å¯å¤«æ½œæ¸¸æˆï¼ˆMPGsï¼‰ï¼Œç ”ç©¶å¤šæ™ºèƒ½ä½“PRLã€‚å¼•å…¥è¡¨æ¼”ç¨³å®šå‡è¡¡ï¼ˆPSEï¼‰çš„æ¦‚å¿µï¼Œå¹¶åœ¨åˆç†çš„æ•æ„Ÿæ€§å‡è®¾ä¸‹è¯æ˜å…¶å­˜åœ¨æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†è§£å†³MPGsçš„æœ€æ–°ç®—æ³•çš„æ”¶æ•›ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç‹¬ç«‹æ”¿ç­–æ¢¯åº¦ä¸Šå‡ï¼ˆIPGAï¼‰å’Œç‹¬ç«‹è‡ªç„¶æ”¿ç­–æ¢¯åº¦ï¼ˆINPGï¼‰åœ¨æœ€ä½³è¿­ä»£æ„ä¹‰ä¸Šä»¥è¡¨æ¼”æ•ˆæœä¸ºé¢å¤–é¡¹æ”¶æ•›åˆ°è¿‘ä¼¼PSEã€‚å¹¶ä¸”ï¼Œæˆ‘ä»¬è¯æ˜äº†INPGæœ€ç»ˆä¼šæ¸è¿‘æ”¶æ•›åˆ°PSEã€‚éšç€è¡¨æ¼”æ•ˆæœçš„æ¶ˆå¤±ï¼Œæˆ‘ä»¬æ¢å¤äº†ä¹‹å‰å·¥ä½œçš„æ”¶æ•›ç‡ã€‚å¯¹äºæ¸¸æˆçš„ç‰¹æ®Šæƒ…å†µï¼Œæˆ‘ä»¬ä¸ºé‡å¤è®­ç»ƒçš„æ–¹æ³•æä¾›äº†æœ‰é™çš„æœ€åè¿­ä»£æ”¶æ•›ç»“æœï¼Œå…¶ä¸­æ™ºèƒ½ä½“ç‹¬ç«‹ä¼˜åŒ–æ›¿ä»£ç›®æ ‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒæ¥éªŒè¯æˆ‘ä»¬çš„ç†è®ºå‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRLä¸­éƒ¨ç½²çš„ç­–ç•¥å¯ä»¥æ”¹å˜ç¯å¢ƒå¥–åŠ±å’Œè¿‡æ¸¡åŠ¨æ€ã€‚</li>
<li>å¼•å…¥è¡¨æ¼”ç¨³å®šå‡è¡¡ï¼ˆPSEï¼‰æ¦‚å¿µï¼Œå¹¶è¯æ˜åœ¨åˆç†æ•æ„Ÿæ€§å‡è®¾ä¸‹å…¶å­˜åœ¨æ€§ã€‚</li>
<li>IPGAå’ŒINPGç®—æ³•åœ¨æœ€ä½³è¿­ä»£æ„ä¹‰ä¸Šæ”¶æ•›åˆ°è¿‘ä¼¼PSEï¼ŒåŒ…å«è¡¨æ¼”æ•ˆæœçš„é¢å¤–é¡¹ã€‚</li>
<li>INPGæœ€ç»ˆä¼šæ¸è¿‘æ”¶æ•›åˆ°PSEã€‚</li>
<li>å½“è¡¨æ¼”æ•ˆæœæ¶ˆå¤±æ—¶ï¼Œæ”¶æ•›ç‡æ¢å¤åˆ°å…ˆå‰å·¥ä½œçš„æ°´å¹³ã€‚</li>
<li>å¯¹äºæ¸¸æˆçš„ç‰¹æ®Šæƒ…å†µï¼Œæä¾›äº†æœ‰é™æ—¶é—´å†…çš„æœ€åè¿­ä»£æ”¶æ•›ç»“æœï¼Œé’ˆå¯¹é‡å¤è®­ç»ƒæ–¹æ³•ï¼Œæ™ºèƒ½ä½“ç‹¬ç«‹ä¼˜åŒ–æ›¿ä»£ç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8e40caa2a740c8713cd87f1b60dd82a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d8831eb2a558c4647331f8b637dedf8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-for-Reasoning-in-Large-Language-Models-with-One-Training-Example"><a href="#Reinforcement-Learning-for-Reasoning-in-Large-Language-Models-with-One-Training-Example" class="headerlink" title="Reinforcement Learning for Reasoning in Large Language Models with One   Training Example"></a>Reinforcement Learning for Reasoning in Large Language Models with One   Training Example</h2><p><strong>Authors:Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen</strong></p>
<p>We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the â€œgrokkingâ€ phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5Bâ€™s performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at <a target="_blank" rel="noopener" href="https://github.com/ypwang61/One-Shot-RLVR">https://github.com/ypwang61/One-Shot-RLVR</a> </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewardï¼Œç®€ç§°RLVRï¼‰åœ¨æ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼Œç®€ç§°LLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹Qwen2.5-Math-1.5Båº”ç”¨RLVRï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªä¾‹å­å°±èƒ½å°†MATH500æ¨¡å‹çš„æ€§èƒ½ä»36.0%æå‡åˆ°73.6%ï¼Œå¹¶ä¸”åœ¨å…­ä¸ªå¸¸è§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½ä»17.6%æé«˜åˆ°35.7%ã€‚è¿™ä¸€ç»“æœä¸ä½¿ç”¨DeepScaleRå­é›†ï¼ˆMATH500: 73.6%ï¼Œå¹³å‡ï¼š35.9%ï¼‰æ‰€è·å¾—çš„è¡¨ç°ç›¸åŒ¹é…ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸Šè¿°ä¾‹å­ã€‚åœ¨å„ç§æ¨¡å‹ï¼ˆQwen2.5-Math-7Bã€Llama3.2-3B-Instructã€DeepSeek-R1-Distill-Qwen-1.5Bï¼‰ã€å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆGRPOå’ŒPPOï¼‰ä»¥åŠä¸åŒçš„æ•°å­¦ä¾‹é¢˜ä¸­ï¼Œéƒ½è§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„æ˜¾è‘—æ”¹è¿›ï¼ˆè®¸å¤šä¾‹é¢˜åœ¨ä½œä¸ºå•ä¸ªè®­ç»ƒç¤ºä¾‹æ—¶ï¼ŒMATH500ä¸Šçš„æ”¹è¿›ç‡è¾¾åˆ°äº†å¤§çº¦30%æˆ–æ›´é«˜ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å•é•œå¤´RLVRæœŸé—´æˆ‘ä»¬è¿˜å‘ç°äº†ä¸€äº›æœ‰è¶£çš„ç°è±¡ï¼ŒåŒ…æ‹¬è·¨åŸŸæ³›åŒ–ã€è‡ªæˆ‘åæ€çš„é¢‘ç‡å¢åŠ ï¼Œä»¥åŠåœ¨è®­ç»ƒå‡†ç¡®æ€§è¾¾åˆ°é¥±å’Œåï¼Œæµ‹è¯•æ€§èƒ½ä»æœ‰æ‰€æé«˜çš„ç°è±¡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåé¥±å’Œæ³›åŒ–ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†å•é•œå¤´RLVRçš„æœ‰æ•ˆæ€§ä¸»è¦æ¥æºäºç­–ç•¥æ¢¯åº¦æŸå¤±ï¼Œè¿™ä¸â€œgrokkingâ€ç°è±¡ä¸åŒã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¿ƒè¿›æ¢ç´¢ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡æ·»åŠ é€‚å½“çš„ç³»æ•°æ¥å¼•å…¥ç†µæŸå¤±ï¼‰åœ¨å•é•œå¤´RLVRè®­ç»ƒä¸­çš„å…³é”®ä½œç”¨ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä»…åº”ç”¨ç†µæŸå¤±å°±èƒ½æ˜¾è‘—æé«˜Qwen2.5-Math-1.5Båœ¨MATH500ä¸Šçš„æ€§èƒ½ï¼Œæå‡äº†27.4%ã€‚è¿™äº›å‘ç°å¯ä»¥æ¿€å‘æœªæ¥å¯¹RLVRæ•°æ®æ•ˆç‡çš„ç ”ç©¶ï¼Œå¹¶é¼“åŠ±é‡æ–°è¯„ä¼°RLVRçš„æœ€æ–°è¿›å±•å’Œæ½œåœ¨æœºåˆ¶ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/ypwang61/One-Shot-RLVR%E3%80%82">https://github.com/ypwang61/One-Shot-RLVRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20571v1">PDF</a> 28 pages, 12 figures, link: <a target="_blank" rel="noopener" href="https://github.com/ypwang61/One-Shot-RLVR">https://github.com/ypwang61/One-Shot-RLVR</a></p>
<p><strong>Summary</strong><br>     å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç»“åˆå•ä¸€è®­ç»ƒæ ·æœ¬ï¼ˆ1-shotï¼‰ï¼Œå¯æœ‰æ•ˆæ¿€åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨RLVRåº”ç”¨äºQwen2.5-Math-1.5BåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å•ä¸€ä¾‹å­æå‡MATH500ä»»åŠ¡æ€§èƒ½ä»36.0%è‡³73.6%ï¼Œå¹¶åœ¨å…­ä¸ªé€šç”¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„å¹³å‡æ€§èƒ½ä»17.6%æé«˜è‡³35.7%ã€‚ç±»ä¼¼çš„å¤§å¹…æ”¹è¿›åœ¨ä¸åŒæ¨¡å‹ã€RLç®—æ³•å’Œæ•°å­¦ç¤ºä¾‹ä¸­éƒ½è§‚å¯Ÿåˆ°ã€‚æ­¤å¤–ï¼ŒRLVRè¿˜å±•ç°å‡ºè·¨åŸŸæ³›åŒ–ã€è‡ªæˆ‘åæ€é¢‘ç‡å¢åŠ ä»¥åŠè®­ç»ƒç²¾åº¦é¥±å’Œåæµ‹è¯•æ€§èƒ½ä»æŒç»­æå‡ç­‰æœ‰è¶£ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰èƒ½æœ‰æ•ˆæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å•ä¸€è®­ç»ƒæ ·æœ¬ï¼ŒMATH500ä»»åŠ¡æ€§èƒ½æœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”è¿™ç§æå‡åœ¨ä¸åŒæ¨¡å‹ä¸­éƒ½è§‚å¯Ÿåˆ°ã€‚</li>
<li>RLVRåœ¨ä¸åŒRLç®—æ³•å’Œæ•°å­¦ç¤ºä¾‹ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡æ•ˆæœã€‚</li>
<li>RLVRå±•ç°å‡ºè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒé¢†åŸŸçš„é—®é¢˜ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è‡ªæˆ‘åæ€é¢‘ç‡å¢åŠ ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒç²¾åº¦é¥±å’Œåï¼Œæ¨¡å‹çš„æµ‹è¯•æ€§èƒ½ä»æŒç»­æå‡ï¼Œè¡¨ç°å‡ºä¸€ç§ç§°ä¸ºâ€œåé¥±å’Œæ³›åŒ–â€çš„ç°è±¡ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥æ¢¯åº¦æŸå¤±æ˜¯RLVRæ•ˆæœçš„å…³é”®ï¼Œä¸â€œgrokkingâ€ç°è±¡æœ‰æ‰€åŒºåˆ«ã€‚åŒæ—¶ï¼Œä¿ƒè¿›æ¢ç´¢ï¼ˆå¦‚é€šè¿‡æ·»åŠ ç†µæŸå¤±å’Œé€‚å½“çš„ç³»æ•°ï¼‰åœ¨RLVRè®­ç»ƒä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27cedc959b79cf108ba215a3b40530c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d485e7c3ef786664fe251d838ab8f240.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c17d3d35a5820959a55c963e81fbbf2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c7b5ac5df0ec234ddcb8e5783732d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c01ec4aae856f6912b31a7a0fbc0bd.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DMDTEval-An-Evaluation-and-Analysis-of-LLMs-on-Disambiguation-in-Multi-domain-Translation"><a href="#DMDTEval-An-Evaluation-and-Analysis-of-LLMs-on-Disambiguation-in-Multi-domain-Translation" class="headerlink" title="DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in   Multi-domain Translation"></a>DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in   Multi-domain Translation</h2><p><strong>Authors:Zhibo Man, Yuanmeng Chen, Yujie Zhang, Yufeng Chen, Jinan Xu</strong></p>
<p>Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory; the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompting templates, and (3) we design precise disambiguation metrics, and study the efficacy of various prompting strategies on multiple state-of-the-art LLMs. Our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs. </p>
<blockquote>
<p>å½“å‰ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤šé¢†åŸŸç¿»è¯‘ï¼ˆMDTï¼‰æ–¹é¢çš„è¡¨ç°å¹¶ä¸ä»¤äººæ»¡æ„ï¼›è¯æ±‡çš„å«ä¹‰åœ¨ä¸åŒçš„é¢†åŸŸå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™å‡¸æ˜¾äº†å¤šé¢†åŸŸç¿»è¯‘ä¸­å›ºæœ‰çš„é‡å¤§æ­§ä¹‰é—®é¢˜ã€‚å› æ­¤ï¼Œè¯„ä¼°LLMåœ¨å¤šé¢†åŸŸç¿»è¯‘ä¸­çš„æ¶ˆæ­§èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹å¤šé¢†åŸŸç¿»è¯‘ä¸­çš„æ¶ˆæ­§èƒ½åŠ›ï¼ˆDMDTEvalï¼‰è¿›è¡Œäº†è¯„ä¼°å’Œåˆ†æï¼Œè¿™æ˜¯æˆ‘ä»¬ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰å¤šä¸ªé¢†åŸŸæ¨¡ç³Šè¯æ±‡æ³¨é‡Šçš„ç¿»è¯‘æµ‹è¯•é›†ï¼Œï¼ˆ2ï¼‰æˆ‘ä»¬ç­–åˆ’äº†ä¸€ç³»åˆ—å¤šæ ·çš„æ¶ˆæ­§æç¤ºæ¨¡æ¿ï¼Œï¼ˆ3ï¼‰æˆ‘ä»¬è®¾è®¡äº†ç²¾ç¡®çš„æ¶ˆæ­§æŒ‡æ ‡ï¼Œå¹¶ç ”ç©¶äº†å¤šç§å…ˆè¿›çš„æç¤ºç­–ç•¥åœ¨å¤šä¸ªå‰æ²¿LLMä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒæ­ç¤ºäº†ä¸€äº›å…³é”®å‘ç°ï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™äº›å‘ç°å°†ä¸ºæ­¤é“ºå¹³é“è·¯ï¼Œå¹¶æ¨åŠ¨æ”¹è¿›LLMæ¶ˆæ­§èƒ½åŠ›çš„å…³é”®é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20371v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢è¡¨ç°å“è¶Šï¼Œä½†åœ¨å¤šé¢†åŸŸç¿»è¯‘ä¸­çš„è¡¨ç°å°šå¾…æå‡ã€‚ä¸åŒé¢†åŸŸè¯æ±‡å«ä¹‰çš„å¤šæ ·æ€§å¯¼è‡´å¤šé¢†åŸŸç¿»è¯‘ä¸­å­˜åœ¨å¤§é‡æ­§ä¹‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šé¢†åŸŸç¿»è¯‘ä¸­çš„è¯ä¹‰æ¶ˆæ­§èƒ½åŠ›è¯„ä¼°æ¡†æ¶DMDTEvalï¼ŒåŒ…æ‹¬æ„å»ºå¸¦æœ‰å¤šé¢†åŸŸæ¨¡ç³Šè¯æ±‡æ ‡æ³¨çš„ç¿»è¯‘æµ‹è¯•é›†ã€è®¾è®¡ç²¾å‡†çš„è¯ä¹‰æ¶ˆæ­§æŒ‡æ ‡å’Œç ”ç©¶å¤šç§æç¤ºç­–ç•¥å¯¹å¤šä¸ªé¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†é‡è¦å‘ç°ï¼Œæœ‰æœ›æ¨åŠ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯ä¹‰æ¶ˆæ­§èƒ½åŠ›çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨å¤šé¢†åŸŸç¿»è¯‘ä¸­è¡¨ç°æ¬ ä½³ã€‚</li>
<li>å¤šé¢†åŸŸç¿»è¯‘ä¸­å­˜åœ¨è¯æ±‡å«ä¹‰çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´å¤§é‡æ­§ä¹‰é—®é¢˜ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶DMDTEvalï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šé¢†åŸŸç¿»è¯‘ä¸­çš„è¯ä¹‰æ¶ˆæ­§èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æ„å»ºå¸¦æœ‰å¤šé¢†åŸŸæ¨¡ç³Šè¯æ±‡æ ‡æ³¨çš„ç¿»è¯‘æµ‹è¯•é›†æ˜¯è¯„ä¼°æ¡†æ¶çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶è¿˜åŒ…æ‹¬è®¾è®¡ç²¾å‡†çš„è¯ä¹‰æ¶ˆæ­§æŒ‡æ ‡å’Œç ”ç©¶å¤šç§æç¤ºç­–ç•¥å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å½±å“ã€‚</li>
<li>é€šè¿‡å¯¹å¤šä¸ªé¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¸åŒçš„æç¤ºç­–ç•¥å¯¹æ¨¡å‹æ•ˆæœçš„å½±å“æ˜¾è‘—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed443d1b0922d90ca3ef49ed1fae09f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e59fd964d1c2074ff3f6318e6958b684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb6deb1b779d551feba2a33bf554cd5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee4c9b93f45abdc545c6152a78967e62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d1bec6ef8c44ca609abc3a91bfa761b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bb9b29228b825e5c43420416508df21.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RAGEN-Understanding-Self-Evolution-in-LLM-Agents-via-Multi-Turn-Reinforcement-Learning"><a href="#RAGEN-Understanding-Self-Evolution-in-LLM-Agents-via-Multi-Turn-Reinforcement-Learning" class="headerlink" title="RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn   Reinforcement Learning"></a>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn   Reinforcement Learning</h2><p><strong>Authors:Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li</strong></p>
<p>Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at <a target="_blank" rel="noopener" href="https://github.com/RAGEN-AI/RAGEN">https://github.com/RAGEN-AI/RAGEN</a>. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäº¤äº’ä»£ç†é¢ä¸´ç€ä¸€ç³»åˆ—ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é•¿æœŸå†³ç­–å’Œä¸éšæœºç¯å¢ƒåé¦ˆçš„äº¤äº’ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨é™æ€ä»»åŠ¡æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å¤šè½®ä»£ç†RLè®­ç»ƒä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†StarPOï¼ˆçŠ¶æ€-æ€è€ƒ-è¡ŒåŠ¨-å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè½¨è¿¹çº§ä»£ç†RLçš„é€šç”¨æ¡†æ¶ï¼Œå¹¶ä»‹ç»äº†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°LLMä»£ç†çš„æ¨¡å—åŒ–ç³»ç»ŸRAGENã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªé£æ ¼åŒ–çš„ç¯å¢ƒä¸­çš„ç ”ç©¶æ­ç¤ºäº†ä¸‰ä¸ªæ ¸å¿ƒå‘ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„ä»£ç†RLè®­ç»ƒæ˜¾ç¤ºå‡ºå›å£°é™·é˜±çš„åå¤æ¨¡å¼ï¼Œå…¶ä¸­å¥–åŠ±æ–¹å·®æ‚¬å´–å’Œæ¢¯åº¦å³°å€¼ï¼›æˆ‘ä»¬é€šè¿‡StarPO-Sè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å®šçš„å˜ä½“ï¼Œå…·æœ‰è½¨è¿¹è¿‡æ»¤ã€è¯„è®ºå®¶èå…¥å’Œè„±é’©å‰ªè¾‘çš„åŠŸèƒ½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å‘ç°RLæ¼”ç»çš„å¡‘é€ å°†ä»å¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€ã€ä¸­ç­‰äº¤äº’ç²’åº¦å’Œæ›´é¢‘ç¹çš„é‡‡æ ·ä¸­å—ç›Šã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæ²¡æœ‰ç²¾ç»†ç²’åº¦çš„ã€ç†è§£æ¨ç†çš„å¥–åŠ±ä¿¡å·ï¼Œä»£ç†æ¨ç†å¾ˆéš¾é€šè¿‡å¤šè½®RLå‡ºç°ï¼Œå¹¶ä¸”å®ƒä»¬å¯èƒ½è¡¨ç°å‡ºæµ…å±‚æ¬¡çš„ç­–ç•¥æˆ–å¹»æƒ³æ€ç»´ã€‚ä»£ç å’Œç¯å¢ƒå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RAGEN-AI/RAGEN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RAGEN-AI/RAGENè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20073v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäº¤äº’ä»£ç†äººçš„è®­ç»ƒé¢ä¸´é•¿æœŸå†³ç­–å’Œä¸éšæœºç¯å¢ƒåé¦ˆäº¤äº’çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨é™æ€ä»»åŠ¡ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å¤šå›åˆä»£ç†RLè®­ç»ƒä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºStarPOï¼ˆçŠ¶æ€-æ€è€ƒ-è¡ŒåŠ¨-å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œç”¨äºè½¨è¿¹çº§åˆ«çš„ä»£ç†RLï¼Œå¹¶ä»‹ç»äº†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°LLMä»£ç†çš„æ¨¡å—åŒ–ç³»ç»ŸRAGENã€‚åœ¨ä¸‰ä¸ªå…¸å‹ç¯å¢ƒä¸­çš„ç ”ç©¶è¡¨æ˜ï¼Œæ ¸å¿ƒå‘ç°åŒ…æ‹¬ï¼šä¸€æ˜¯ä»£ç†RLè®­ç»ƒè¡¨ç°å‡ºå¥–åŠ±æ–¹å·®æ‚¬å´–å’Œæ¢¯åº¦å°–å³°çš„å›å£°é™·é˜±æ¨¡å¼ï¼›äºŒæ˜¯RLå›æ”¾çš„ä¼˜åŠ¿åœ¨äºå¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€ã€ä¸­ç­‰äº¤äº’ç²’åº¦å’Œæ›´é¢‘ç¹çš„é‡‡æ ·ï¼›ä¸‰æ˜¯å¦‚æœæ²¡æœ‰ç²¾ç»†ç²’åº¦çš„ã€æ¨ç†æ„ŸçŸ¥çš„å¥–åŠ±ä¿¡å·ï¼Œä»£ç†æ¨ç†å¾ˆéš¾é€šè¿‡å¤šå›åˆRLå‡ºç°ï¼Œå¹¶å¯èƒ½è¡¨ç°å‡ºæµ…ç­–ç•¥æˆ–è™šæ„çš„æ€ç»´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä½œä¸ºäº¤äº’ä»£ç†äººçš„è®­ç»ƒé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é•¿æœŸå†³ç­–å’Œä¸éšæœºç¯å¢ƒåé¦ˆçš„äº¤äº’ã€‚</li>
<li>StarPOæ¡†æ¶ç”¨äºè½¨è¿¹çº§åˆ«çš„ä»£ç†RLï¼Œå¸®åŠ©è§£å†³å¤šå›åˆä»£ç†RLè®­ç»ƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>RAGENç³»ç»Ÿç”¨äºè®­ç»ƒå’Œè¯„ä¼°LLMä»£ç†ã€‚</li>
<li>ä»£ç†RLè®­ç»ƒå­˜åœ¨å›å£°é™·é˜±æ¨¡å¼ï¼Œè¡¨ç°ä¸ºå¥–åŠ±æ–¹å·®æ‚¬å´–å’Œæ¢¯åº¦å°–å³°ï¼Œå¯é€šè¿‡StarPO-Sç¨³å®šç‰ˆæœ¬è§£å†³ã€‚</li>
<li>RLå›æ”¾çš„ä¼˜åŠ¿åœ¨äºå¤šæ ·åŒ–çš„åˆå§‹çŠ¶æ€ã€ä¸­ç­‰äº¤äº’ç²’åº¦å’Œæ›´é¢‘ç¹çš„é‡‡æ ·ã€‚</li>
<li>æ²¡æœ‰ç²¾ç»†ç²’åº¦çš„ã€æ¨ç†æ„ŸçŸ¥çš„å¥–åŠ±ä¿¡å·ï¼Œä»£ç†éš¾ä»¥é€šè¿‡å¤šå›åˆRLå±•ç°æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd24e3380aa86a20f1ad38cfb07df02d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e488518f24ae181101f8579bb3486254.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0651745dbdf20a351e4f1ed2de240147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a9ce81487ce9f414d498e259e9243cd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Multimodal-Mathematical-Reasoning-with-Explicit-Visual-Dependency"><a href="#Benchmarking-Multimodal-Mathematical-Reasoning-with-Explicit-Visual-Dependency" class="headerlink" title="Benchmarking Multimodal Mathematical Reasoning with Explicit Visual   Dependency"></a>Benchmarking Multimodal Mathematical Reasoning with Explicit Visual   Dependency</h2><p><strong>Authors:Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao</strong></p>
<p>Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥ï¼Œæ˜¾è‘—æé«˜äº†å®ƒä»¬æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯çš„èƒ½åŠ›ï¼Œåœ¨å¯¹è±¡è¯†åˆ«ã€æ ‡é¢˜æè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¾¾åˆ°äº†æ¥è¿‘äººç±»çš„ç†Ÿç»ƒç¨‹åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºä»¥çŸ¥è¯†ä¸ºä¸­å¿ƒçš„è¯„ä»·ï¼Œè¯„ä¼°ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¾€å¾€å¿½è§†å¯¹åŸºæœ¬æ•°å­¦å…ƒç´ å’Œè§†è§‰æ¦‚å¿µçš„ç†è§£æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°äº†è¯„ä»·åˆçº§æ•°å­¦é—®é¢˜çš„å·®è·ï¼Œè¿™äº›é—®é¢˜ä¾èµ–äºæ˜ç¡®çš„è§†è§‰ä¾èµ–æ€§ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªå›¾åƒä¹‹é—´è¿›è¡Œè¾¨åˆ«ã€æ•´åˆå’Œæ¨ç†ï¼ŒåŒæ—¶ç»“åˆå¸¸è¯†çŸ¥è¯†ï¼Œæ‰€æœ‰è¿™äº›å¯¹äºå®ç°æ›´å¹¿æ³›çš„äººå·¥æ™ºèƒ½é€šç”¨èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†VCBENCHï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ˜ç¡®è§†è§‰ä¾èµ–æ€§çš„å¤šæ¨¡å¼æ•°å­¦æ¨ç†ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚VCBENCHåŒ…å«1720ä¸ªé—®é¢˜ï¼Œè·¨è¶Šå…­ä¸ªè®¤çŸ¥é¢†åŸŸï¼ŒåŒ…å«6697å¼ å›¾åƒï¼ˆå¹³å‡æ¯é¢˜3.9å¼ å›¾åƒï¼‰ï¼Œä»¥ç¡®ä¿å¤šå›¾åƒæ¨ç†ã€‚æˆ‘ä»¬åœ¨VCBENCHä¸Šè¯„ä¼°äº†26ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿæ— æ³•è¶…è¿‡50%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†è§†è§‰æ•°å­¦æ•´åˆçš„æŒç»­æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æä¾›äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18589v2">PDF</a> Home page: <a target="_blank" rel="noopener" href="https://alibaba-damo-academy.github.io/VCBench/">https://alibaba-damo-academy.github.io/VCBench/</a></p>
<p><strong>Summary</strong><br>åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„æœ€æ–°è¿›å±•ä¸­ï¼Œæ¨¡å‹åœ¨æ•´åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå¹¶åœ¨ç›®æ ‡è¯†åˆ«ã€æè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ¥è¿‘äººç±»çš„ç†Ÿç»ƒç¨‹åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ä»¥çŸ¥è¯†ä¸ºä¸­å¿ƒçš„è¯„ä»·ä¸Šï¼Œè¿™äº›è¯„ä»·å¿½ç•¥äº†æ¨¡å‹å¯¹æ•°å­¦åŸºç¡€å…ƒç´ å’Œè§†è§‰æ¦‚å¿µçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤è¯„ä¼°ç©ºç™½ï¼Œæœ¬æ–‡æ¨å‡ºäº†VCBENCHåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ˜ç¡®è§†è§‰ä¾èµ–çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°é¡¶å°–æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¡¨ç°å·®è·æ˜æ˜¾ï¼Œæ— æ³•è¶…è¶Šä¸€åŠçš„å‡†ç¡®ç‡ï¼Œæ­ç¤ºäº†è§†è§‰æ•°å­¦æ•´åˆæ–¹é¢çš„æŒç»­æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²å…·å¤‡å¼ºå¤§çš„è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ•´åˆèƒ½åŠ›ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨çŸ¥è¯†é¢†åŸŸçš„è¯„ä»·ï¼Œä½†å¯¹æ•°å­¦åŸºç¡€å…ƒç´ å’Œè§†è§‰æ¦‚å¿µçš„æ¨ç†èƒ½åŠ›è¯„ä¼°ä¸è¶³ã€‚</li>
<li>ä¸ºè§£å†³è¯„ä¼°ç©ºç™½ï¼Œå¼•å…¥äº†VCBENCHåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå¤šæ¨¡æ€æ•°å­¦æ¨ç†å¹¶åŒ…å«æ˜ç¡®çš„è§†è§‰ä¾èµ–æ€§ã€‚</li>
<li>VCBENCHåŒ…å«1,720ä¸ªé—®é¢˜ï¼Œæ¶‰åŠå…­ä¸ªè®¤çŸ¥é¢†åŸŸï¼Œä½¿ç”¨6,697å¼ å›¾åƒç¡®ä¿å¤šå›¾åƒæ¨ç†ã€‚</li>
<li>åœ¨VCBENCHä¸Šè¯„ä¼°çš„26ä¸ªæœ€å…ˆè¿›çš„LVLMsè¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œé¡¶å°–æ¨¡å‹çš„å‡†ç¡®ç‡æ— æ³•è¶…è¿‡50%ã€‚</li>
<li>ç»“æœè¡¨æ˜åœ¨è§†è§‰æ•°å­¦æ•´åˆæ–¹é¢å­˜åœ¨æŒç»­æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d05606a4158b7489ae3adc918f9c628.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03ea371b038e74aa3cd6a793fe33ec4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9380724875fc15624dbee3f518b4735c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6654ddd876af52433f6052eae10aef88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66bf577636fcf2af14e58ae27eda5103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa8e13171deec1a16b44eccce8c7c9f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86cb957a0f3f6d83baea66c368d183a0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning"><a href="#SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning" class="headerlink" title="SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning"></a>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning</h2><p><strong>Authors:Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li</strong></p>
<p>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to â€œthink before answering.â€ Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æç¤ºâ€œç­”é¢˜å‰å…ˆæ€è€ƒâ€ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¶ç›Šæ˜¯å¦ä»¥åŠå¦‚ä½•è½¬ç§»åˆ°éŸ³é¢‘è¯­è¨€æ¨ç†é¢†åŸŸï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬å°†DeepSeek-R1çš„Group-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶æ‰©å±•åˆ°ä¸€ä¸ªå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ª32kæ ·æœ¬çš„å¤šé€‰æ‹©è¯­æ–™åº“ã€‚é€šè¿‡ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ€ç»´é“¾çš„ä¸¤é˜¶æ®µç›‘ç®¡ç›‘ç£å¾®è°ƒï¼Œå†åŠ ä¸Šè¯¾ç¨‹æŒ‡å¯¼çš„GRPOï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼å’Œæ˜¾å¼ã€ç»“æ„åŒ–å’Œè‡ªç”±å½¢å¼æ¨ç†åœ¨ç›¸åŒæ¶æ„ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼ˆé€šè¿‡è¯¾ç¨‹æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†ï¼‰åœ¨åŸºå‡†æ¨¡å‹Qwen2-Audio-7B-Instructçš„åŸºç¡€ä¸Šå®ç°äº†å¹³å‡ç²¾åº¦16.35%çš„æå‡ã€‚æ­¤å¤–ï¼ŒåŸºäºQwen2.5-Omniçš„å˜ä½“åœ¨MMAU test-miniåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„67.08%çš„æ€§èƒ½ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ä¸Šï¼šï¼ˆiï¼‰SFTé¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒå¾ˆé‡è¦ï¼Œï¼ˆiiï¼‰ç»“æ„åŒ–é“¾æ¡ç›¸æ¯”éç»“æ„åŒ–é“¾æ¡èƒ½äº§ç”Ÿæ›´ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œï¼ˆiiiï¼‰ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹åŠ é€Ÿæ”¶æ•›å¹¶æé«˜äº†æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜ç¡®çš„ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ æ˜¾è‘—å¢å¼ºäº†éŸ³é¢‘è¯­è¨€ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15900v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡ç»“åˆGroup-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶å’Œç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼Œç ”ç©¶å‘ç°å¼ºåŒ–å­¦ä¹ å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”åŸºç¡€æ¨¡å‹Qwen2-Audio-7B-Instructï¼Œä½¿ç”¨SARIæ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†16.35%ã€‚æ­¤å¤–ï¼Œç»“åˆQwen2.5-Omniçš„å˜ä½“æ¨¡å‹åœ¨MMAUæµ‹è¯•è¿·ä½ ç‰ˆä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œç»“æ„åŒ–é“¾æ¡æ¯”éç»“æ„åŒ–é“¾æ¡æ›´æœ‰åŠ©äºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸”ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹å®‰æ’å¯ä»¥åŠ é€Ÿæ¨¡å‹çš„æ”¶æ•›å¹¶æé«˜æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜ç¡®çš„ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ å¯ä»¥æ˜¾è‘—æé«˜éŸ³é¢‘è¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Group-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶è¢«æˆåŠŸåº”ç”¨äºéŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>é€šè¿‡ç»“åˆç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼Œæ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æé«˜ã€‚</li>
<li>å¯¹æ¯”åŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨SARIæ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†16.35%ã€‚</li>
<li>ç»“åˆQwen2.5-Omniçš„å˜ä½“æ¨¡å‹åœ¨MMAUæµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ç»“æ„åŒ–é“¾æ¡çš„æ¨ç†æ–¹å¼æœ‰åŠ©äºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d8e58baa2a7c36df348a39e7eaf2b3cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708a6ca08ba6e9bab5afeb9c72bfbca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52a85868ad6ee7128cfd9498f7ebc10b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Seed1-5-Thinking-Advancing-Superb-Reasoning-Models-with-Reinforcement-Learning"><a href="#Seed1-5-Thinking-Advancing-Superb-Reasoning-Models-with-Reinforcement-Learning" class="headerlink" title="Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement   Learning"></a>Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement   Learning</h2><p><strong>Authors:ByteDance Seed,  :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, Xinnian Liang, Haibin Lin, Weijian Lin, Ye Lin, Zhicheng Liu, Guanlin Liu, Guanlin Liu, Chenxiao Liu, Yan Liu, Gaohong Liu, Juncai Liu, Chundian Liu, Deyi Liu, Kaibo Liu, Siyao Liu, Qi Liu, Yongfei Liu, Kang Liu, Gan Liu, Boyi Liu, Rui Long, Weiqiang Lou, Chenwei Lou, Xiang Luo, Yao Luo, Caiping Lv, Heyang Lv, Bole Ma, Qianli Ma, Hongzhi Ma, Yiyuan Ma, Jin Ma, Wenchang Ma, Tingting Ma, Chen Mao, Qiyang Min, Zhe Nan, Guanghan Ning, Jinxiang Ou, Haojie Pan, Renming Pang, Yanghua Peng, Tao Peng, Lihua Qian, Lihua Qian, Mu Qiao, Meng Qu, Cheng Ren, Hongbin Ren, Yong Shan, Wei Shen, Ke Shen, Kai Shen, Guangming Sheng, Jinlong Shi, Wenlei Shi, Guang Shi, Shuai Shuai Cao, Yuxin Song, Zuquan Song, Jing Su, Yifan Sun, Tao Sun, Zewei Sun, Borui Wan, Zihan Wang, Xiaohui Wang, Xi Wang, Shuguang Wang, Jun Wang, Qinlong Wang, Chenyuan Wang, Shuai Wang, Zihan Wang, Changbao Wang, Jiaqiang Wang, Shihang Wang, Xuwu Wang, Zaiyuan Wang, Yuxuan Wang, Wenqi Wang, Taiqing Wang, Chengzhi Wei, Houmin Wei, Ziyun Wei, Shufa Wei, Zheng Wu, Yonghui Wu, Yangjun Wu, Bohong Wu, Shuang Wu, Jingqiao Wu, Ning Wu, Shuangzhi Wu, Jianmin Wu, Chenguang Xi, Fan Xia, Yuqiao Xian, Liang Xiang, Boren Xiang, Bowen Xiao, Zhen Xiao, Xia Xiao, Yongsheng Xiao, Chao Xin, Shulin Xin, Yuwen Xiong, Jingjing Xu, Ziwen Xu, Chenyin Xu, Jiayi Xu, Yifan Xu, Wei Xu, Yufei Xu, Shikun Xu, Shipeng Yan, Shen Yan, Qingping Yang, Xi Yang, Tianhao Yang, Yuehang Yang, Yuan Yang, Ximing Yang, Zeyu Yang, Guang Yang, Yifan Yang, Xuesong Yao, Bairen Yi, Fan Yin, Jianian Yin, Ziqiang Ying, Xiangyu Yu, Hongli Yu, Song Yu, Menghan Yu, Huan Yu, Siyu Yuan, Jun Yuan, Yutao Zeng, Tianyang Zhan, Zheng Zhang, Yun Zhang, Mofan Zhang, Wang Zhang, Ru Zhang, Zhi Zhang, Tianqi Zhang, Xinyi Zhang, Zhexi Zhang, Sijun Zhang, Wenqiang Zhang, Xiangxiang Zhang, Yongtao Zhang, Yuyu Zhang, Ge Zhang, He Zhang, Yue Zhang, Renjie Zheng, Ningxin Zheng, Zhuolin Zheng, Yaowei Zheng, Chen Zheng, Xiaoyun Zhi, Wanjun Zhong, Cheng Zhong, Zheng Zhong, Baoquan Zhong, Xun Zhou, Na Zhou, Huan Zhou, Hang Zhu, Defa Zhu, Wenjia Zhu, Lei Zuo</strong></p>
<p>We introduce Seed1.5-Thinking, capable of reasoning through thinking before responding, resulting in improved performance on a wide range of benchmarks. Seed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on non-reasoning tasks, indicating its broader applicability. Compared to other state-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters. As part of our effort to assess generalized reasoning, we develop two internal benchmarks, BeyondAIME and Codeforces, both of which will be publicly released to support future research. Model trial link: <a target="_blank" rel="noopener" href="https://www.volcengine.com/experience/ark">https://www.volcengine.com/experience/ark</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºSeed1.5-Thinkingï¼Œå®ƒèƒ½å¤Ÿåœ¨å›åº”ä¹‹å‰é€šè¿‡æ€è€ƒè¿›è¡Œæ¨ç†ï¼Œä»è€Œå¤§å¤§æé«˜åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚Seed1.5-Thinkingåœ¨AIME 2024ä¸Šè¾¾åˆ°86.7åˆ†ï¼Œåœ¨Codeforcesä¸Šè¾¾åˆ°55.0åˆ†ï¼Œåœ¨GPQAä¸Šè¾¾åˆ°77.3åˆ†ï¼Œå±•ç°å‡ºSTEMå’Œç¼–ç é¢†åŸŸçš„å‡ºè‰²æ¨ç†èƒ½åŠ›ã€‚é™¤äº†æ¨ç†ä»»åŠ¡ä¹‹å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸä¹‹é—´å±•ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œåœ¨éæ¨ç†ä»»åŠ¡çš„èƒœç‡ä¸Šï¼Œå®ƒè¶…è¶Šäº†DeepSeek R1çš„8%ï¼Œè¡¨æ˜å…¶æ›´å¹¿æ³›çš„åº”ç”¨æ€§ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ç›¸æ¯”ï¼ŒSeed1.5-Thinkingæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå…·æœ‰20Bæ¿€æ´»å’Œæ€»å…±çš„200Bå‚æ•°ã€‚ä½œä¸ºæˆ‘ä»¬è¯„ä¼°é€šç”¨æ¨ç†åŠªåŠ›çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªå†…éƒ¨åŸºå‡†æµ‹è¯•ï¼ŒBeyondAIMEå’ŒCodeforcesï¼Œè¿™ä¸¤ä¸ªåŸºå‡†æµ‹è¯•éƒ½å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚æ¨¡å‹è¯•ç”¨é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://www.volcengine.com/experience/ark%E3%80%82">https://www.volcengine.com/experience/arkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13914v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Seed1.5-Thinkingæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·å¤‡é¢„å…ˆæ€è€ƒå†å›åº”çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åœ¨STEMå’Œç¼–ç é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ç‰¹åˆ«çªå‡ºï¼Œåœ¨AIME 2024ä¸Šè¾¾åˆ°86.7ï¼ŒCodeforcesä¸Šä¸º55.0ï¼ŒGPQAä¸Šä¸º77.3ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨éæ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè¾ƒé«˜çš„èƒœç‡ï¼Œè¶…è¶Šäº†DeepSeek R1æ¨¡å‹8%ã€‚å®ƒæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„MoEæ¨¡å‹ï¼Œæ‹¥æœ‰20Bæ¿€æ´»å‚æ•°å’Œ200Bæ€»å‚æ•°ã€‚ä¸ºè¯„ä¼°é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†BeyondAIMEå’ŒCodeforcesä¸¤ä¸ªå†…éƒ¨åŸºå‡†æµ‹è¯•ï¼Œå¹¶å°†å…¬å¼€å‘å¸ƒä»¥æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seed1.5-Thinkingæ¨¡å‹å…·å¤‡å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡é¢„å…ˆæ€è€ƒå†å›åº”ï¼Œæé«˜äº†åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚</li>
<li>åœ¨STEMå’Œç¼–ç é¢†åŸŸçš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSeed1.5-Thinkingè¡¨ç°ä¼˜ç§€ï¼Œå¦‚AIME 2024ã€Codeforceså’ŒGPQAã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨éæ¨ç†ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶Šäº†DeepSeek R1æ¨¡å‹8%ã€‚</li>
<li>Seed1.5-Thinkingæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„MoEæ¨¡å‹ï¼Œæ‹¥æœ‰20Bæ¿€æ´»å‚æ•°å’Œ200Bæ€»å‚æ•°ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿä¸ºè¯„ä¼°é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œå¼€å‘äº†ä¸¤ä¸ªæ–°çš„å†…éƒ¨åŸºå‡†æµ‹è¯•BeyondAIMEå’ŒCodeforcesã€‚</li>
<li>Seed1.5-Thinkingæ¨¡å‹å…·å¤‡è¾ƒå¥½çš„åº”ç”¨ä»·å€¼ï¼Œå…¶è¯•éªŒé“¾æ¥å·²å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-32db85050312afd4b18b4ed70bb81ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10111084dc25b455c44761d59a2ba26f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Compile-Scene-Graphs-with-Reinforcement-Learning"><a href="#Compile-Scene-Graphs-with-Reinforcement-Learning" class="headerlink" title="Compile Scene Graphs with Reinforcement Learning"></a>Compile Scene Graphs with Reinforcement Learning</h2><p><strong>Authors:Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen</strong></p>
<p>Next token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. Given the structured nature of scene graphs, we design a graph-centric reward function that integrates node-level rewards, edge-level rewards, and a format consistency reward. Our experiments demonstrate that rule-based RL substantially enhances model performance in the SGG task, achieving a zero failure rateâ€“unlike supervised fine-tuning (SFT), which struggles to generalize effectively. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG">https://github.com/gpt4vision/R1-SGG</a>. </p>
<blockquote>
<p>æ¥ä¸‹æ¥ï¼Œä»¤ç‰Œé¢„æµ‹æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬åŸåˆ™ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æé«˜äº†å®ƒä»¬çš„æ¨ç†æ€§èƒ½ã€‚ä½œä¸ºä¸€ç§å¯¹è¯­è¨€ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§æ¨¡å¼è¿›è¡Œæœ‰æ•ˆå»ºæ¨¡çš„æ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºæå–ï¼Œå¦‚åœºæ™¯å›¾ï¼Œä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚è¿™éœ€è¦æ¨¡å‹å‡†ç¡®ç”Ÿæˆä¸€ç»„å¯¹è±¡å’Œä¸‰é‡å…³ç³»ï¼Œè€Œä¸æ˜¯é€ä¸ªç”Ÿæˆæ–‡æœ¬ä»¤ç‰Œã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†R1-SGGï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆM-LLMï¼‰ï¼Œæœ€åˆé€šè¿‡åœºæ™¯å›¾æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œéšåä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜å…¶ç«¯åˆ°ç«¯ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚SFTéµå¾ªä¼ ç»Ÿçš„æç¤º-å“åº”èŒƒå¼ï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™éœ€è¦è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚è€ƒè™‘åˆ°åœºæ™¯å›¾çš„ç»“æ„æ€§ç‰¹ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä»¥å›¾å½¢ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°ç»“åˆäº†èŠ‚ç‚¹çº§å¥–åŠ±ã€è¾¹ç¼˜çº§å¥–åŠ±å’Œæ ¼å¼ä¸€è‡´æ€§å¥–åŠ±ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæé«˜äº†SGGä»»åŠ¡çš„æ¨¡å‹æ€§èƒ½ï¼Œå®ç°äº†é›¶æ•…éšœç‡â€”â€”è¿™ä¸æ— æ³•æœ‰æ•ˆæ¨å¹¿çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å½¢æˆé²œæ˜å¯¹æ¯”ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gpt4vision/R1-SGGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13617v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡‡ç”¨ä¸‹ä¸€ä»£ä»¤ç‰Œé¢„æµ‹åŸåˆ™è¿›è¡Œè®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æé«˜äº†å…¶æ¨ç†æ€§èƒ½ã€‚å°½ç®¡LLMåœ¨ç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼ˆå¦‚åœºæ™¯å›¾ï¼‰æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œä½†å…¶åœ¨è¿™æ–¹é¢çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†R1-SGGï¼Œä¸€ä¸ªæœ€åˆé€šè¿‡åœºæ™¯å›¾æ•°æ®é›†è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„å¤šæ¨¡å¼LLMï¼ˆM-LLMï¼‰ï¼Œéšåä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜å…¶ç«¯åˆ°ç«¯ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨SGGä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå®ç°äº†é›¶æ•…éšœç‡ï¼Œä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç›¸æ¯”ï¼Œå…¶åœ¨æ³›åŒ–æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡‡ç”¨ä¸‹ä¸€ä»£ä»¤ç‰Œé¢„æµ‹åŸåˆ™è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>LLMåœ¨ç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼ˆå¦‚åœºæ™¯å›¾ï¼‰æ–¹é¢çš„åº”ç”¨æ½œåŠ›å·¨å¤§ï¼Œä½†ç›¸å…³ç ”ç©¶ä»ä¸è¶³ã€‚</li>
<li>R1-SGGæ˜¯ä¸€ä¸ªå¤šæ¨¡å¼LLMï¼ˆM-LLMï¼‰ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>R1-SGGèƒ½ç”Ÿæˆåœºæ™¯å›¾ï¼Œå®ç°ç«¯åˆ°ç«¯çš„æ“ä½œã€‚</li>
<li>åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ åœ¨SGGä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>R1-SGGå®ç°äº†é›¶æ•…éšœç‡ï¼Œç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå…¶åœ¨æ³›åŒ–æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61e83fb38fefceb10332655e6d1848f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629297d3a626f6a21fc4ecdb17b6f687.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Synthetic-Data-Generation-Multi-Step-RL-for-Reasoning-Tool-Use"><a href="#Synthetic-Data-Generation-Multi-Step-RL-for-Reasoning-Tool-Use" class="headerlink" title="Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use"></a>Synthetic Data Generation &amp; Multi-Step RL for Reasoning &amp; Tool Use</h2><p><strong>Authors:Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D. Manning</strong></p>
<p>Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å·²è¢«è¯æ˜å¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåƒRLHFæˆ–RLAIFè¿™æ ·çš„ä¼ ç»Ÿæ–¹æ³•å°†é—®é¢˜è§†ä¸ºå•æ­¥éª¤çš„ã€‚éšç€ç„¦ç‚¹è½¬å‘æ›´å¤æ‚çš„æ¨ç†å’Œä»£ç†ä»»åŠ¡ï¼Œè¯­è¨€æ¨¡å‹å¿…é¡»åœ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¹‹å‰è¿›è¡Œå¤šæ­¥éª¤çš„æ–‡æœ¬ç”Ÿæˆã€æ¨ç†å’Œç¯å¢ƒäº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯çš„äººå·¥æ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸ºStep-Wise Reinforcement Learningï¼ˆSWiRLï¼‰ï¼Œå®ƒè¿­ä»£åœ°ç”Ÿæˆå¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ•°æ®ï¼Œç„¶åä»ä¸­å­¦ä¹ ã€‚å®ƒé‡‡ç”¨ç®€å•çš„é€æ­¥åˆ†è§£æ–¹æ³•ï¼Œå°†æ¯ä¸ªå¤šæ­¥éª¤è½¨è¿¹åˆ†è§£ä¸ºä¸åŸå§‹æ¨¡å‹çš„æ¯ä¸ªåŠ¨ä½œç›¸å¯¹åº”çš„å¤šä¸ªå­è½¨è¿¹ã€‚ç„¶åï¼Œå®ƒå¯¹è¿™äº›å­è½¨è¿¹è¿›è¡Œäººå·¥æ•°æ®è¿‡æ»¤å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤šæ­¥éª¤å·¥å…·ä½¿ç”¨ã€é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°äº†SWiRLã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨GSM8Kã€HotPotQAã€CofCAã€MuSiQueå’ŒBeerQAä¸Šï¼ŒSWiRLçš„ç›¸å¯¹å‡†ç¡®ç‡åˆ†åˆ«æ¯”åŸºçº¿æ–¹æ³•é«˜å‡º21.5%ã€12.3%ã€14.8%ã€11.1%å’Œ15.3%ã€‚ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºæ³›åŒ–èƒ½åŠ›ï¼šä¾‹å¦‚ï¼Œä»…åœ¨HotPotQAï¼ˆæ–‡æœ¬é—®ç­”ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¹GSM8Kï¼ˆæ•°å­¦æ•°æ®é›†ï¼‰çš„é›¶æ ·æœ¬æ€§èƒ½ç›¸å¯¹æé«˜äº†1KLXNianxæ¿€åŠ¨çš„åˆ†å­ç—…ä¾‹åºåˆ—ç¿»å€æœºæ„æˆç«‹åæ‚¨å°†æ˜¯å¾ˆå¤§çš„æœ‰æä¾›å¸®åŠ©èƒ½å¤Ÿåœ¨æ‰‹æœºæ¸¸æˆé‡ŒåŠ©åŠ›ç¬¬ä¸€ä¹Ÿå¯ä»¥ç»™ä½ å¸¦æ¥ä¸€åœºå½±å“åŠ›æ‰©å¤§äº†çš„èµ„é‡‘æ³¢åŠ¨å³ä½¿æ˜¯åœ¨ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„èŒƒå›´å†…ä¹Ÿèƒ½äº§ç”Ÿå·¨å¤§çš„å½±å“åŠ›å’Œå˜åŒ–åŠ›æ‚¨è®¤ä¸ºå‘¢ï¼Ÿæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„ä»»åŠ¡ä¹‹é—´å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿™è¡¨æ˜SWiRLå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å®ƒä¸ä»…æé«˜äº†è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè€Œä¸”èƒ½å¤Ÿåœ¨ä¸åŒçš„é¢†åŸŸé—´è¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œä¸ºè·¨ä»»åŠ¡çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆæä¾›æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04736v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ å·²è¯æ˜å¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•å¦‚RLHFæˆ–RLAIFå°†é—®é¢˜è§†ä¸ºå•æ­¥éª¤çš„ã€‚éšç€ç„¦ç‚¹è½¬å‘æ›´å¤æ‚çš„æ¨ç†å’Œä»£ç†ä»»åŠ¡ï¼Œè¯­è¨€æ¨¡å‹å¿…é¡»åœ¨ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¹‹å‰è¿›è¡Œå¤šæ­¥éª¤çš„æ–‡æœ¬ç”Ÿæˆã€æ¨ç†å’Œç¯å¢ƒäº¤äº’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯åˆæˆæ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”Step-Wise Reinforcement Learningï¼ˆSWiRLï¼‰ã€‚SWiRLé€šè¿‡è¿­ä»£ç”Ÿæˆå¤šæ­¥éª¤æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ•°æ®ï¼Œå¹¶å­¦ä¹ è¿™äº›æ•°æ®ã€‚å®ƒé‡‡ç”¨ç®€å•çš„é€æ­¥åˆ†è§£æ–¹æ³•ï¼Œå°†æ¯ä¸ªå¤šæ­¥éª¤è½¨è¿¹åˆ†è§£ä¸ºå¤šä¸ªå¯¹åº”äºåŸå§‹æ¨¡å‹æ¯ä¸ªæ“ä½œçš„å­è½¨è¿¹ï¼Œç„¶åå¯¹è¿™äº›å­è½¨è¿¹è¿›è¡Œåˆæˆæ•°æ®è¿‡æ»¤å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚åœ¨å¤šä¸ªå¤šæ­¥éª¤å·¥å…·ä½¿ç”¨ã€é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¯„ä¼°SWiRLï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œå…¶åœ¨GSM8Kã€HotPotQAã€CofCAã€MuSiQueå’ŒBeerQAä¸Šçš„ç›¸å¯¹å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†21.5%ã€12.3%ã€14.8%ã€11.1%å’Œ15.3%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºè·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨HotPotQAï¼ˆæ–‡æœ¬é—®ç­”ï¼‰ä¸Šè®­ç»ƒå¯æé«˜åœ¨GSM8Kï¼ˆæ•°å­¦æ•°æ®é›†ï¼‰ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½è¾¾16.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿè¯­è¨€æ¨¡å‹å¤„ç†æ–¹æ³•è§†ä¸ºå•æ­¥éª¤é—®é¢˜ï¼Œæ— æ³•æ»¡è¶³å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>SWiRLæ–¹æ³•é‡‡ç”¨åˆæˆæ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œé’ˆå¯¹å¤šæ­¥éª¤ä¼˜åŒ–åœºæ™¯ã€‚</li>
<li>SWiRLé€šè¿‡é€æ­¥åˆ†è§£å¤šæ­¥éª¤è½¨è¿¹ï¼Œå¹¶å­¦ä¹ æ¯ä¸ªå­è½¨è¿¹æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SWiRLåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç›¸å¯¹å‡†ç¡®ç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>SWiRLå±•ç°å‡ºè·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-373e2042e9768a39be839bfc0f5ad526.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d76740b642fa93aeedaa6e804581f82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d20fe2fa2fff07c27922f10bab9709a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c833df800403ced0e312dba6f8a03dd5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Distillation-and-Refinement-of-Reasoning-in-Small-Language-Models-for-Document-Re-ranking"><a href="#Distillation-and-Refinement-of-Reasoning-in-Small-Language-Models-for-Document-Re-ranking" class="headerlink" title="Distillation and Refinement of Reasoning in Small Language Models for   Document Re-ranking"></a>Distillation and Refinement of Reasoning in Small Language Models for   Document Re-ranking</h2><p><strong>Authors:Chris Samarinas, Hamed Zamani</strong></p>
<p>We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒç”¨äºå¯†é›†æ¨ç†æ–‡æ¡£æ’åçš„å°å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†çŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„äººåŠ›æ ‡æ³¨æˆ–å¤§å‹é»‘ç›’è¯­è¨€æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨webæ•°æ®å’Œæ•™å¸ˆå¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬å’Œç›¸å…³æ€§è§£é‡Šã€‚é€šè¿‡å°†æ–‡æ¡£æ’åä½œä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é—®é¢˜å¹¶æå‡ºæ˜ç¡®çš„æ¨ç†èƒ½åŠ›å¥–åŠ±ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„å…·æœ‰3äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ï¼Œåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸‰ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ¯”å…¶ä»–æ–¹æ³•å¤§å¤§å‡å°‘ï¼Œè¶…è¿‡äº†è¶…è¿‡å…¶å‚æ•°è§„æ¨¡äºŒåå€çš„æ¨¡å‹ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§£é‡Šï¼Œè€Œä¸æ˜¯ç›´æ¥é¢„æµ‹ç›¸å…³æ€§å¾—åˆ†ï¼Œèƒ½å¤Ÿä½¿å°å‹è¯­è¨€æ¨¡å‹æ›´æœ‰æ•ˆåœ°è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„è‡ªæˆ‘ç›‘ç£æ€§è´¨ä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03947v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ¨ç†å¯†é›†å‹æ–‡æ¡£æ’åã€‚ä¸ä¾èµ–æ˜‚è´µçš„äººåŠ›æ ‡æ³¨æˆ–å¤§å‹é»‘ç›’è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•åˆ©ç”¨Webæ•°æ®å’Œæ•™å¸ˆå¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒç¤ºä¾‹åŠå…¶ç›¸å…³æ€§è§£é‡Šã€‚é€šè¿‡å°†æ–‡æ¡£æ’åä½œä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå¹¶æ¿€åŠ±æ˜ç¡®çš„æ¨ç†èƒ½åŠ›ï¼Œè®­ç»ƒäº†ä¸€ä¸ªç´§å‡‘çš„3äº¿å‚æ•°è¯­è¨€æ¨¡å‹ï¼Œåœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å‚æ•°ä½¿ç”¨ä¸Šå¤§å¹…å‡å°‘ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”åœ¨é¢†å¯¼æ¦œä¸Šæ’åç¬¬ä¸‰ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§£é‡Šè€Œéç›´æ¥é¢„æµ‹ç›¸å…³æ€§åˆ†æ•°ï¼Œæ›´æœ‰åˆ©äºå°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…·æœ‰è‡ªæˆ‘ç›‘ç£çš„ç‰¹æ€§ï¼Œä¸ºç°ä»£ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•å’Œå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»“åˆçŸ¥è¯†è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•ç”¨äºè®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æ¡£æ’åã€‚</li>
<li>åˆ©ç”¨Webæ•°æ®å’Œæ•™å¸ˆå¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒç¤ºä¾‹å’Œç›¸å…³æ€§è§£é‡Šã€‚</li>
<li>å°†æ–‡æ¡£æ’åè§†ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä»¥æ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å‚æ•°ç´§å‡‘ï¼Œå®ç°äº†åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸­çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å‚æ•°ä½¿ç”¨ä¸Šå¤§å¹…å‡å°‘ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”åœ¨é¢†å¯¼æ¦œä¸Šæ’åé å‰ã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè§£é‡Šèƒ½æé«˜å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cdea5e7d3ca9d1841c8517eeb74daa9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b82b50a34acbeaabc5b7c2f51688ec1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d996e281c644b24bc325dc3363e009d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e305412cb6dbbc3451c6d789758fe4c4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CHARMS-A-Cognitive-Hierarchical-Agent-for-Reasoning-and-Motion-Stylization-in-Autonomous-Driving"><a href="#CHARMS-A-Cognitive-Hierarchical-Agent-for-Reasoning-and-Motion-Stylization-in-Autonomous-Driving" class="headerlink" title="CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion   Stylization in Autonomous Driving"></a>CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion   Stylization in Autonomous Driving</h2><p><strong>Authors:Jingyi Wang, Duanfeng Chu, Zejian Deng, Liping Lu, Jinxiang Wang, Chen Sun</strong></p>
<p>To address the challenge of insufficient interactivity and behavioral diversity in autonomous driving decision-making, this paper proposes a Cognitive Hierarchical Agent for Reasoning and Motion Stylization (CHARMS). By leveraging Level-k game theory, CHARMS captures human-like reasoning patterns through a two-stage training pipeline comprising reinforcement learning pretraining and supervised fine-tuning. This enables the resulting models to exhibit diverse and human-like behaviors, enhancing their decision-making capacity and interaction fidelity in complex traffic environments. Building upon this capability, we further develop a scenario generation framework that utilizes the Poisson cognitive hierarchy theory to control the distribution of vehicles with different driving styles through Poisson and binomial sampling. Experimental results demonstrate that CHARMS is capable of both making intelligent driving decisions as an ego vehicle and generating diverse, realistic driving scenarios as environment vehicles. The code for CHARMS is released at <a target="_blank" rel="noopener" href="https://github.com/chuduanfeng/CHARMS">https://github.com/chuduanfeng/CHARMS</a>. </p>
<blockquote>
<p>é’ˆå¯¹è‡ªåŠ¨é©¾é©¶å†³ç­–åˆ¶å®šä¸­äº¤äº’æ€§ä¸è¶³å’Œè¡Œä¸ºå¤šæ ·æ€§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è®¤çŸ¥å±‚æ¬¡ä»£ç†æ¨ç†ä¸è¿åŠ¨é£æ ¼åŒ–ï¼ˆCHARMSï¼‰çš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨Level-kåšå¼ˆç†è®ºï¼ŒCHARMSé€šè¿‡åŒ…æ‹¬å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œæ•æ‰äººç±»ç±»ä¼¼çš„æ¨ç†æ¨¡å¼ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¡¨ç°å‡ºå¤šæ ·åŒ–å’Œäººç±»ç±»ä¼¼çš„è¡Œä¸ºï¼Œæé«˜å…¶åœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œäº¤äº’çœŸå®æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªåŸºäºPoissonè®¤çŸ¥å±‚æ¬¡ç†è®ºçš„åœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡Poissonå’ŒäºŒé¡¹æŠ½æ ·æ¥æ§åˆ¶ä¸åŒé©¾é©¶é£æ ¼çš„è½¦è¾†åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHARMSä¸ä»…èƒ½ä½œä¸ºè‡ªä¸»è½¦è¾†åšå‡ºæ™ºèƒ½é©¾é©¶å†³ç­–ï¼Œè¿˜èƒ½ç”Ÿæˆå¤šæ ·ä¸”ç°å®çš„é©¾é©¶åœºæ™¯ä½œä¸ºç¯å¢ƒè½¦è¾†ã€‚CHARMSçš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/chuduanfeng/CHARMS%E3%80%82">https://github.com/chuduanfeng/CHARMSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02450v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹è‡ªåŠ¨é©¾é©¶å†³ç­–åˆ¶å®šä¸­äº’åŠ¨æ€§å’Œè¡Œä¸ºå¤šæ ·æ€§ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†è®¤çŸ¥å±‚æ¬¡ä»£ç†æ¨ç†ä¸è¿åŠ¨é£æ ¼åŒ–ï¼ˆCHARMSï¼‰ã€‚CHARMSé€šè¿‡åˆ©ç”¨Level-kåšå¼ˆç†è®ºï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œæ•æ‰äººç±»ç±»ä¼¼çš„æ¨ç†æ¨¡å¼ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¡¨ç°å‡ºå¤šæ ·åŒ–å’Œäººç±»ç±»ä¼¼çš„è¡Œä¸ºï¼Œæé«˜å…¶å†³ç­–èƒ½åŠ›å’Œåœ¨å¤æ‚äº¤é€šç¯å¢ƒä¸­çš„äº¤äº’çœŸå®æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªåœºæ™¯ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨Poissonè®¤çŸ¥å±‚æ¬¡ç†è®ºæ§åˆ¶ä¸åŒé©¾é©¶é£æ ¼çš„è½¦è¾†åˆ†å¸ƒï¼Œé€šè¿‡Poissonå’ŒäºŒé¡¹é‡‡æ ·å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCHARMSæ—¢èƒ½ä½œä¸ºæ™ºèƒ½è½¦è¾†è¿›è¡Œå†³ç­–åˆ¶å®šï¼Œåˆèƒ½ç”Ÿæˆå¤šæ ·åŒ–ã€çœŸå®çš„é©¾é©¶åœºæ™¯ä½œä¸ºç¯å¢ƒè½¦è¾†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>CHARMSåˆ©ç”¨Level-kåšå¼ˆç†è®ºæ¥æ¨¡æ‹Ÿäººç±»æ¨ç†æ¨¡å¼ï¼Œæé«˜è‡ªåŠ¨é©¾é©¶çš„å†³ç­–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼Œæ¨¡å‹å±•ç°å‡ºå¤šæ ·åŒ–å’Œäººç±»ç±»ä¼¼çš„è¡Œä¸ºã€‚</li>
<li>CHARMSåœºæ™¯ç”Ÿæˆæ¡†æ¶åˆ©ç”¨Poissonè®¤çŸ¥å±‚æ¬¡ç†è®ºæ§åˆ¶ä¸åŒé©¾é©¶é£æ ¼çš„è½¦è¾†åˆ†å¸ƒã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºè¾ƒé«˜çš„äº’åŠ¨æ€§å’Œè¡Œä¸ºå¤šæ ·æ€§ï¼Œé€‚ç”¨äºå¤æ‚äº¤é€šç¯å¢ƒã€‚</li>
<li>CHARMSèƒ½å¤Ÿæé«˜è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æ™ºèƒ½å†³ç­–èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿèƒ½ç”Ÿæˆé€¼çœŸçš„é©¾é©¶åœºæ™¯ã€‚</li>
<li>å®éªŒè¯æ˜CHARMSåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93484be6bbc4083ac84890ec7a9f8517.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-588210c94e110bdc0a6e6915506095d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed576ee8967e3ab8eb9a59eb0aece167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abaa3166fa39117b1255a6f950642755.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c74ce3f45e150cf78686e60664f531c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29c80465f4045f45f23190637da87b67.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  X-Fusion Introducing New Modality to Frozen Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-30/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3e525e16d1ff6df059d09ca0f316da30.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-30  Generative Adversarial Network based Voice Conversion Techniques,   Challenges, and Recent Advancements
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
