<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  X-Fusion Introducing New Modality to Frozen Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-01-æ›´æ–°"><a href="#2025-05-01-æ›´æ–°" class="headerlink" title="2025-05-01 æ›´æ–°"></a>2025-05-01 æ›´æ–°</h1><h2 id="X-Fusion-Introducing-New-Modality-to-Frozen-Large-Language-Models"><a href="#X-Fusion-Introducing-New-Modality-to-Frozen-Large-Language-Models" class="headerlink" title="X-Fusion: Introducing New Modality to Frozen Large Language Models"></a>X-Fusion: Introducing New Modality to Frozen Large Language Models</h2><p><strong>Authors:Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li</strong></p>
<p>We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLMâ€™s parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†X-Fusionæ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶è¯­è¨€åŠŸèƒ½ã€‚X-Fusioné‡‡ç”¨å…·æœ‰æ¨¡æ€ç‰¹å®šæƒé‡çš„åŒå¡”è®¾è®¡ï¼Œåœ¨æ•´åˆè§†è§‰ç‰¹å®šä¿¡æ¯ç”¨äºç†è§£å’Œç”Ÿæˆçš„åŒæ—¶ï¼Œä¿æŒLLMçš„å‚æ•°å†»ç»“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒX-Fusionåœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸Šéƒ½ä¸€è‡´åœ°ä¼˜äºå…¶ä»–æ¶æ„ã€‚æˆ‘ä»¬å‘ç°ï¼Œèå…¥ä»¥ç†è§£ä¸ºé‡ç‚¹çš„æ•°æ®æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œå‡å°‘å›¾åƒæ•°æ®å™ªå£°æé«˜äº†æ•´ä½“æ€§èƒ½ï¼Œç‰¹å¾å¯¹é½åŠ é€Ÿäº†è¾ƒå°æ¨¡å‹çš„æ”¶æ•›ï¼Œä½†å¯¹è¾ƒå¤§æ¨¡å‹çš„å½±å“è¾ƒå°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºé«˜æ•ˆç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20996v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://sichengmo.github.io/XFusion/">https://sichengmo.github.io/XFusion/</a></p>
<p><strong>Summary</strong></p>
<p>X-Fusionæ¡†æ¶æ‰©å±•äº†é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒæ—¶ä¿ç•™å…¶è¯­è¨€åŠŸèƒ½ã€‚X-Fusioné‡‡ç”¨åŒå¡”è®¾è®¡ï¼Œå…·æœ‰æ¨¡æ€ç‰¹å®šæƒé‡ï¼Œå†»ç»“LLMå‚æ•°ï¼ŒåŒæ—¶èå…¥è§†è§‰ç‰¹å®šä¿¡æ¯ï¼Œç”¨äºç†è§£å’Œç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒX-Fusionåœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸Šå‡ä¼˜äºå…¶ä»–æ¶æ„ã€‚èå…¥ç†è§£å‹æ•°æ®å¯æé«˜ç”Ÿæˆè´¨é‡ï¼Œå‡å°‘å›¾åƒæ•°æ®å™ªå£°å¯æé«˜æ•´ä½“æ€§èƒ½ï¼Œç‰¹å¾å¯¹é½å¯åŠ é€Ÿå°å‹æ¨¡å‹çš„æ”¶æ•›ä½†å¯¹å¤§å‹æ¨¡å‹å½±å“è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-Fusionæ¡†æ¶æ‰©å±•äº†LLMï¼Œæ”¯æŒå¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€åŠŸèƒ½ã€‚</li>
<li>X-Fusioné‡‡ç”¨åŒå¡”è®¾è®¡ï¼ŒåŒ…å«æ¨¡æ€ç‰¹å®šæƒé‡ã€‚</li>
<li>å†»ç»“LLMå‚æ•°ï¼ŒåŒæ—¶èå…¥è§†è§‰ä¿¡æ¯ï¼Œç”¨äºç†è§£å’Œç”Ÿæˆã€‚</li>
<li>X-Fusionåœ¨å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¶æ„ã€‚</li>
<li>èå…¥ç†è§£å‹æ•°æ®èƒ½æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>å‡å°‘å›¾åƒæ•°æ®å™ªå£°å¯æé«˜æ•´ä½“æ€§èƒ½ã€‚</li>
<li>ç‰¹å¾å¯¹é½å¯ä»¥åŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œå¯¹å°å‹æ¨¡å‹æ•ˆæœæ›´æ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b90013b0096ed6400b10d2a7d18e901f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76338d7904b4b33be69c61c77dbca164.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e1068541e150f6b5f8eacebd713e34e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a4aaa5e20fb76562315508a9fd95667.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e75ae97555f362c860530eef1602e45e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SetKE-Knowledge-Editing-for-Knowledge-Elements-Overlap"><a href="#SetKE-Knowledge-Editing-for-Knowledge-Elements-Overlap" class="headerlink" title="SetKE: Knowledge Editing for Knowledge Elements Overlap"></a>SetKE: Knowledge Editing for Knowledge Elements Overlap</h2><p><strong>Authors:Yifan Wei, Xiaoyan Yu, Ran Song, Hao Peng, Angsheng Li</strong></p>
<p>Large Language Models (LLMs) excel in tasks such as retrieval and question answering but require updates to incorporate new knowledge and reduce inaccuracies and hallucinations. Traditional updating methods, like fine-tuning and incremental learning, face challenges such as overfitting and high computational costs. Knowledge Editing (KE) provides a promising alternative but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where multiple triplets share common elements, leading to editing conflicts. We identify the prevalence of KEO in existing KE datasets and show its significant impact on current KE methods, causing performance degradation in handling such triplets. To address this, we propose a new formulation, Knowledge Set Editing (KSE), and introduce SetKE, a method that edits sets of triplets simultaneously. Experimental results demonstrate that SetKE outperforms existing methods in KEO scenarios on mainstream LLMs. Additionally, we introduce EditSet, a dataset containing KEO triplets, providing a comprehensive benchmark. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€ç´¢å’Œé—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦æ›´æ–°ä»¥èå…¥æ–°çŸ¥è¯†å¹¶å‡å°‘é”™è¯¯å’Œå¹»è§‰ã€‚ä¼ ç»Ÿçš„æ›´æ–°æ–¹æ³•ï¼Œå¦‚å¾®è°ƒï¼ˆfine-tuningï¼‰å’Œå¢é‡å­¦ä¹ ï¼ˆincremental learningï¼‰ï¼Œé¢ä¸´ç€è¿‡æ‹Ÿåˆå’Œé«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¾€å¾€å¿½è§†äº†çŸ¥è¯†å…ƒç´ é‡å ï¼ˆKEOï¼‰ç°è±¡ï¼Œå³å¤šä¸ªä¸‰å…ƒç»„å…·æœ‰å…±äº«å…ƒç´ ï¼Œä»è€Œå¯¼è‡´ç¼–è¾‘å†²çªã€‚æˆ‘ä»¬ç¡®å®šäº†ç°æœ‰KEæ•°æ®é›†ä¸­KEOçš„æ™®éæ€§å’Œå¯¹ç°æœ‰çš„KEæ–¹æ³•çš„æ˜¾è‘—å½±å“ï¼Œå¯¼è‡´åœ¨å¤„ç†æ­¤ç±»ä¸‰å…ƒç»„æ—¶æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¡¨è¿°æ–¹å¼ï¼Œå³çŸ¥è¯†é›†ç¼–è¾‘ï¼ˆKSEï¼‰ï¼Œå¹¶å¼•å…¥äº†SetKEæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŒæ—¶ç¼–è¾‘å¤šä¸ªä¸‰å…ƒç»„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†ä¸»æµLLMæ—¶çš„KEOåœºæ™¯ä¸­ï¼ŒSetKEåœ¨çŸ¥è¯†é›†ç¼–è¾‘ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†EditSetæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«KEOä¸‰å…ƒç»„ï¼Œä¸ºå…¨é¢è¯„ä¼°æä¾›äº†åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20972v1">PDF</a> The CR version will be updated subsequently</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿äºæ£€ç´¢å’Œé—®ç­”ä»»åŠ¡ï¼Œä½†éœ€è¦æ›´æ–°ä»¥èå…¥æ–°çŸ¥è¯†å¹¶å‡å°‘è¯¯å·®å’Œè™šæ„å†…å®¹ã€‚ä¼ ç»Ÿæ›´æ–°æ–¹æ³•å¦‚å¾®è°ƒä¸å¢é‡å­¦ä¹ é¢ä¸´è¿‡æ‹Ÿåˆå’Œé«˜è®¡ç®—æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰è™½æä¾›æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¿½ç•¥äº†çŸ¥è¯†å…ƒç´ é‡å ï¼ˆKEOï¼‰ç°è±¡ï¼Œå³å¤šä¸ªä¸‰å…ƒç»„å…±äº«å…±åŒå…ƒç´ ï¼Œå¯¼è‡´ç¼–è¾‘å†²çªã€‚ç ”ç©¶å›¢é˜ŸæŒ‡å‡ºäº†KEOåœ¨ç°æœ‰KEæ•°æ®é›†ä¸­çš„æ™®éå­˜åœ¨ï¼Œå¹¶å±•ç¤ºäº†å…¶å¯¹å½“å‰KEæ–¹æ³•çš„æ˜¾è‘—å½±å“ï¼Œåœ¨å¤„ç†è¿™ç±»ä¸‰å…ƒç»„æ—¶æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†æ–°çŸ¥è¯†é›†ç¼–è¾‘ï¼ˆKSEï¼‰çš„æ–°æ„æƒ³ï¼Œå¹¶å¼•å…¥äº†SetKEæ–¹æ³•ï¼Œå¯åŒæ—¶ç¼–è¾‘å¤šä¸ªä¸‰å…ƒç»„é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸»æµLLMçš„KEOåœºæ™¯ä¸­ï¼ŒSetKEè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†åŒ…å«KEOä¸‰å…ƒç»„çš„EditSetæ•°æ®é›†ï¼Œä¸ºè¯„ä¼°æä¾›äº†å…¨é¢åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ£€ç´¢å’Œé—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†éœ€æ›´æ–°ä»¥æ”¹è¿›æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ›´æ–°æ–¹æ³•å¦‚å¾®è°ƒä¸å¢é‡å­¦ä¹ å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚è¿‡æ‹Ÿåˆå’Œé«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰æ–¹æ³•è™½ç„¶æä¾›æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å¿½ç•¥äº†çŸ¥è¯†å…ƒç´ é‡å ï¼ˆKEOï¼‰ç°è±¡ã€‚</li>
<li>KEOåœ¨ç°æœ‰KEæ•°æ®é›†ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹å½“å‰çš„KEæ–¹æ³•äº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†æ–°çŸ¥è¯†é›†ç¼–è¾‘ï¼ˆKSEï¼‰çš„æ–°æ„æƒ³ï¼Œä»¥åº”å¯¹KEOé—®é¢˜ã€‚</li>
<li>SetKEæ–¹æ³•èƒ½å¤ŸåŒæ—¶ç¼–è¾‘å¤šä¸ªä¸‰å…ƒç»„é›†ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨ä¸»æµLLMçš„KEOåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b5ff110d94a842fbea1e625a7ea962bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25217239f04f483d17eeac990507c3f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3188d5b2b7021c30d9466fc3f8ead5fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-351f88497e3bd7ebb0db9668760f7450.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-036fcc64da8a6e4de7777f5de3ff7af9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AegisLLM-Scaling-Agentic-Systems-for-Self-Reflective-Defense-in-LLM-Security"><a href="#AegisLLM-Scaling-Agentic-Systems-for-Self-Reflective-Defense-in-LLM-Security" class="headerlink" title="AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM   Security"></a>AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM   Security</h2><p><strong>Authors:Zikui Cai, Shayan Shabihi, Bang An, Zora Che, Brian R. Bartoldson, Bhavya Kailkhura, Tom Goldstein, Furong Huang</strong></p>
<p>We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at <a target="_blank" rel="noopener" href="https://github.com/zikuicai/aegisllm">https://github.com/zikuicai/aegisllm</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†AegisLLMï¼Œè¿™æ˜¯ä¸€ç§å¯¹æŠ—æ•Œå¯¹æ”»å‡»å’Œä¿¡æ¯æ³„éœ²çš„åˆä½œå¤šæ™ºèƒ½ä½“é˜²å¾¡ç³»ç»Ÿã€‚åœ¨AegisLLMä¸­ï¼Œè‡ªä¸»æ™ºèƒ½ä½“çš„ç»“æ„åŒ–å·¥ä½œæµç¨‹â€”â€”åè°ƒå™¨ã€åè½¬å™¨ã€å“åº”å™¨å’Œè¯„ä¼°å™¨â€”â€”ååŒå·¥ä½œï¼Œä»¥ç¡®ä¿å®‰å…¨ä¸”åˆè§„çš„LLMè¾“å‡ºï¼ŒåŒæ—¶é€šè¿‡æç¤ºä¼˜åŒ–å®ç°éšæ—¶é—´è‡ªæˆ‘æ”¹è¿›ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶æ‰©å¤§æ™ºèƒ½æ¨ç†ç³»ç»Ÿçš„è§„æ¨¡â€”â€”é€šè¿‡å¢åŠ é¢å¤–çš„æ™ºèƒ½ä½“è§’è‰²å’Œåˆ©ç”¨è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–ï¼ˆå¦‚DSPyï¼‰â€”â€”åœ¨ä¸å½±å“æ¨¡å‹æ•ˆç”¨çš„å‰æä¸‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜ç¨³å¥æ€§ã€‚è¿™ç§æµ‹è¯•æ—¶é˜²å¾¡èƒ½åŠ›ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿå®æ—¶é€‚åº”ä¸æ–­å˜åŒ–çš„æ”»å‡»ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚åœ¨å…³é”®å¨èƒåœºæ™¯çš„ç»¼åˆè¯„ä¼°ä¸­ï¼ŒåŒ…æ‹¬é—å¿˜å’Œè¶Šç‹±æµ‹è¯•ï¼Œéƒ½è¯æ˜äº†AegisLLMçš„æœ‰æ•ˆæ€§ã€‚åœ¨WMDPé—å¿˜åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAegisLLMä»…ä½¿ç”¨20ä¸ªè®­ç»ƒæ ·æœ¬å’Œä¸åˆ°300æ¬¡çš„LMè°ƒç”¨å°±å®ç°äº†è¿‘ä¹å®Œç¾çš„é—å¿˜æ•ˆæœã€‚åœ¨è¶Šç‹±åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨StrongRejectä¸Šå®ç°äº†51%çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨PHTestä¸Šçš„æ‹’ç»ç‡ä»…ä¸º7.9%ï¼Œè€Œç›¸ä¼¼æ–¹æ³•çš„æ‹’ç»ç‡ä¸º18-55%ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†è‡ªé€‚åº”æ™ºèƒ½æ¨ç†ç›¸å¯¹äºé™æ€é˜²å¾¡çš„ä¼˜åŠ¿ï¼Œç¡®ç«‹äº†AegisLLMä½œä¸ºåŸºäºæ¨¡å‹ä¿®æ”¹çš„ä¼ ç»Ÿæ–¹æ³•çš„å¼ºå¤§è¿è¡Œæ—¶æ›¿ä»£æ–¹æ¡ˆã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/zikuicai/aegisllm%E3%80%82">https://github.com/zikuicai/aegisllmã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20965v1">PDF</a> ICLR 2025 Workshop BuildingTrust</p>
<p><strong>Summary</strong><br>     æ¨å‡ºAegisLLMï¼Œä¸€ç§å¯¹æŠ—æ”»å‡»å’Œä¿¡æ¯æ³„éœ²çš„åˆä½œå¤šä»£ç†é˜²å¾¡ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªä¸»ä»£ç†çš„ååŒå·¥ä½œï¼Œç¡®ä¿LLMè¾“å‡ºçš„å®‰å…¨æ€§å’Œåˆè§„æ€§ï¼ŒåŒæ—¶é€šè¿‡æç¤ºä¼˜åŒ–å®ç°è‡ªæˆ‘æ”¹è¿›ã€‚æµ‹è¯•æ—¶ï¼Œé€šè¿‡å¢åŠ ä»£ç†è§’è‰²å’Œåˆ©ç”¨è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–ï¼Œæé«˜ç³»ç»Ÿçš„ç¨³å¥æ€§ï¼Œä¸”ä¸å½±å“æ¨¡å‹çš„å®ç”¨æ€§ã€‚AegisLLMèƒ½å¤Ÿå®æ—¶é€‚åº”ä¸æ–­å˜åŒ–çš„æ”»å‡»ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚åœ¨å…³é”®å¨èƒåœºæ™¯ä¸‹çš„å…¨é¢è¯„ä¼°è¯æ˜äº†AegisLLMçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AegisLLMæ˜¯ä¸€ä¸ªåˆä½œå¤šä»£ç†é˜²å¾¡ç³»ç»Ÿï¼Œå¯¹æŠ—æ”»å‡»å’Œä¿¡æ¯æ³„éœ²ã€‚</li>
<li>ç”±å››ä¸ªè‡ªä¸»ä»£ç†ï¼ˆOrchestrator, Deflector, Responder, Evaluatorï¼‰ç»„æˆï¼Œç¡®ä¿LLMè¾“å‡ºå®‰å…¨åˆè§„ã€‚</li>
<li>é€šè¿‡æç¤ºä¼˜åŒ–å®ç°è‡ªæˆ‘æ”¹è¿›ï¼Œæé«˜ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</li>
<li>æµ‹è¯•æ—¶ï¼Œèƒ½å®æ—¶é€‚åº”ä¸æ–­å˜åŒ–çš„æ”»å‡»ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>åœ¨WMDP unlearning benchmarkä¸Šå®ç°äº†è¿‘å®Œç¾çš„é—å¿˜æ•ˆæœã€‚</li>
<li>åœ¨jailbreaking benchmarksä¸Šç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒAegisLLMçš„ä¼˜åŠ¿åœ¨äºè‡ªé€‚åº”çš„ã€åŸºäºä»£ç†çš„æ¨ç†æ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdd7cf0e0b310cdf4981d34b34b26ff3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0df34c6faee85e888f8d588d71c76cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7ef1e19cdc145099b25c9a4019a6382.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9294e14fd22c081c3689da11c1010893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73d1b22c96a148b092d13de5cc12e5fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OSVBench-Benchmarking-LLMs-on-Specification-Generation-Tasks-for-Operating-System-Verification"><a href="#OSVBench-Benchmarking-LLMs-on-Specification-Generation-Tasks-for-Operating-System-Verification" class="headerlink" title="OSVBench: Benchmarking LLMs on Specification Generation Tasks for   Operating System Verification"></a>OSVBench: Benchmarking LLMs on Specification Generation Tasks for   Operating System Verification</h2><p><strong>Authors:Shangyu Li, Juyong Jiang, Tiancheng Zhao, Jiasi Shen</strong></p>
<p>We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/lishangyu-hkust/OSVBench">https://github.com/lishangyu-hkust/OSVBench</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†OSVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä¸æ“ä½œç³»ç»Ÿå†…æ ¸éªŒè¯ä»»åŠ¡ç›¸å…³çš„å®Œæ•´è§„èŒƒä»£ç æ–¹é¢çš„æ€§èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•é¦–å…ˆé€šè¿‡ä¸ºLLMæä¾›ç¼–ç¨‹æ¨¡å‹ï¼Œå°†è§„èŒƒç”Ÿæˆé—®é¢˜å®šä¹‰ä¸ºåœ¨é™å®šè¯­æ³•å’Œè¯­ä¹‰èŒƒå›´å†…çš„ç¨‹åºåˆæˆé—®é¢˜ã€‚LLMéœ€è¦ç†è§£æä¾›çš„éªŒè¯å‡è®¾å’Œæ½œåœ¨çš„è¯­æ³•å’Œè¯­ä¹‰ç©ºé—´è¿›è¡Œæœç´¢ï¼Œç„¶åæ ¹æ®æ“ä½œç³»ç»Ÿçš„é«˜çº§åŠŸèƒ½æè¿°æŒ‡å¯¼ï¼Œä¸ºæ½œåœ¨çš„æœ‰ç¼ºé™·çš„æ“ä½œç³»ç»Ÿä»£ç å®ç°ç”Ÿæˆå®Œæ•´çš„è§„èŒƒã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•æ˜¯å»ºç«‹åœ¨çœŸå®ä¸–ç•Œçš„æ“ä½œç³»ç»Ÿå†…æ ¸Hyperkernelä¹‹ä¸Šï¼Œæ€»å…±æœ‰245ä¸ªå¤æ‚çš„è§„èŒƒç”Ÿæˆä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æ˜¯å¤§çº¦2ä¸‡è‡³3ä¸‡ä¸ªæ ‡è®°çš„é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ã€‚æˆ‘ä»¬å¯¹12ä¸ªLLMçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå½“å‰LLMåœ¨æ“ä½œç³»ç»ŸéªŒè¯è§„èŒƒç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰é™ã€‚ä»–ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å·®å¼‚å‡¸æ˜¾äº†ä»–ä»¬åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä»£ç ç”Ÿæˆä»»åŠ¡èƒ½åŠ›ä¸Šçš„å·®å¼‚ã€‚è¯„ä¼°å·¥å…·å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lishangyu-hkust/OSVBench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lishangyu-hkust/OSVBenchä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20964v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OSVBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä¸æ“ä½œç³»ç»Ÿå†…æ ¸éªŒè¯ä»»åŠ¡ç›¸å…³çš„å®Œæ•´è§„èŒƒä»£ç æ–¹é¢çš„æ€§èƒ½è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•å°†è§„èŒƒç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºç¨‹åºåˆæˆé—®é¢˜ï¼Œåœ¨æœ‰é™çš„è¯­æ³•å’Œè¯­ä¹‰èŒƒå›´å†…ä¸ºLLMæä¾›ç¼–ç¨‹æ¨¡å‹ã€‚LLMéœ€è¦ç†è§£æä¾›çš„éªŒè¯å‡è®¾å’Œå¯èƒ½çš„è¯­æ³•å’Œè¯­ä¹‰ç©ºé—´ï¼Œç„¶åæ ¹æ®æ“ä½œç³»ç»Ÿçš„é«˜çº§åŠŸèƒ½æè¿°ï¼Œä¸ºæ½œåœ¨çš„æ“ä½œç³»ç»Ÿä»£ç å®ç°ç”Ÿæˆå®Œæ•´çš„è§„èŒƒã€‚è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨çœŸå®æ“ä½œç³»ç»Ÿå†…æ ¸Hyperkernelä¸Šï¼ŒåŒ…å«245ä¸ªå¤æ‚çš„è§„èŒƒç”Ÿæˆä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æ˜¯å¤§çº¦2ä¸‡è‡³3ä¸‡ä¸ªæ ‡è®°çš„é•¿æ–‡æœ¬ä»»åŠ¡ã€‚å¯¹12ä¸ªLLMçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå®ƒä»¬åœ¨æ“ä½œç³»ç»ŸéªŒè¯è§„èŒƒç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OSVBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨ç”Ÿæˆä¸æ“ä½œç³»ç»Ÿå†…æ ¸éªŒè¯ä»»åŠ¡ç›¸å…³çš„å®Œæ•´è§„èŒƒä»£ç æ–¹é¢çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•å°†è§„èŒƒç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºç¨‹åºåˆæˆé—®é¢˜ã€‚</li>
<li>LLMéœ€è¦æ ¹æ®æä¾›çš„éªŒè¯å‡è®¾å’Œå¯èƒ½çš„è¯­æ³•å’Œè¯­ä¹‰ç©ºé—´æ¥ç”Ÿæˆå®Œæ•´çš„è§„èŒƒä»£ç ã€‚</li>
<li>åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨çœŸå®æ“ä½œç³»ç»Ÿå†…æ ¸Hyperkernelä¸Šï¼ŒåŒ…å«245ä¸ªå¤æ‚çš„è§„èŒƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯é•¿æ–‡æœ¬ä»»åŠ¡ï¼Œæ¶‰åŠå¤§çº¦2ä¸‡è‡³3ä¸‡ä¸ªæ ‡è®°ã€‚</li>
<li>å¯¹12ä¸ªLLMçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶çš„æ€§èƒ½æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b5ed8685ee1d33848ac4edaba3b9dc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4300e74cbcff7329e268d85a60617fac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d465149bd4915fe2806e58827a56c05.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification"><a href="#ChestX-Reasoner-Advancing-Radiology-Foundation-Models-with-Reasoning-through-Step-by-Step-Verification" class="headerlink" title="ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification"></a>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification</h2><p><strong>Authors:Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåœ¨æ¨ç†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMsï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŒ»ç–—AIæ¨¡å‹å¾€å¾€å¿½ç•¥äº†ä¸´åºŠå®è·µä¸­çš„ç»“æ„åŒ–æ¨ç†è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ChestX-Reasonerï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ”¾å°„å­¦è¯Šæ–­çš„MLLMï¼Œæ—¨åœ¨åˆ©ç”¨ç›´æ¥ä»ä¸´åºŠæŠ¥å‘Šä¸­æŒ–æ˜çš„æµç¨‹ç›‘ç£ï¼Œåæ˜ æ”¾å°„ç§‘åŒ»ç”Ÿéµå¾ªçš„é€æ­¥æ¨ç†ã€‚æˆ‘ä»¬é€šè¿‡ä»å¸¸è§„æ”¾å°„æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾æ¥æ„å»ºå¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒæ³•ç”±æµç¨‹å¥–åŠ±å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹æ¨ç†æ›´å¥½åœ°ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚æˆ‘ä»¬æ¨å‡ºäº†RadRBench-CXRï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«59Kè§†è§‰é—®ç­”æ ·æœ¬çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ‹¥æœ‰ç»è¿‡ä¸´åºŠéªŒè¯çš„301Kæ¨ç†æ­¥éª¤ï¼Œå¹¶æå‡ºäº†RadRScoreï¼Œä¸€ä¸ªè¯„ä¼°æ¨ç†çœŸå®æ€§ã€å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§çš„æŒ‡æ ‡ã€‚ChestX-Reasoneråœ¨è¯Šæ–­å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å‡ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨é¢†åŸŸMLLMsï¼Œä¸æœ€ä½³åŒ»ç–—MLLMã€æœ€ä½³é€šç”¨MLLMå’ŒåŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œåˆ†åˆ«åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢æé«˜äº†16%ã€5.9%å’Œ18%ï¼Œåœ¨ç»“æœå‡†ç¡®æ€§æ–¹é¢åˆ†åˆ«æé«˜äº†3.3%ã€24%å’Œ27%ã€‚æ‰€æœ‰èµ„æºå‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›åŒ»ç–—æ¨ç†MLLMçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ChestX-Reasonerï¼Œä¸€ä¸ªç»“åˆä¸´åºŠæŠ¥å‘Šä¸­çš„è¿‡ç¨‹ç›‘ç£ä¿¡æ¯è®¾è®¡çš„ç”¨äºæ”¾å°„å­¦è¯Šæ–­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ä»å¸¸è§„æ”¾å°„å­¦æŠ¥å‘Šä¸­æå–å’Œç²¾ç‚¼æ¨ç†é“¾æ¥æ„å»ºå¤§å‹æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸ä»¥è¿‡ç¨‹å¥–åŠ±å¼•å¯¼å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹æ¨ç†æ›´ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚åŒæ—¶ï¼Œå¼•å…¥RadRBench-CXRåŸºå‡†æµ‹è¯•å’ŒRadRScoreè¯„ä¼°æŒ‡æ ‡æ¥è¯„ä»·æ¨¡å‹çš„æ¨ç†å’Œè¯Šæ–­æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChestX-Reasoneråœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ä¸Šå‡ä¼˜äºç°æœ‰çš„åŒ»ç–—å’Œé€šç”¨é¢†åŸŸMLLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChestX-Reasoneræ˜¯ä¸€ä¸ªé’ˆå¯¹æ”¾å°„å­¦è¯Šæ–­çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç»“åˆä¸´åºŠæŠ¥å‘Šä¸­è•´å«çš„è¿‡ç¨‹ç›‘ç£ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æå–å’Œç²¾ç‚¼å¸¸è§„æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„æ¨ç†é“¾æ„å»ºå¤§å‹æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œä½¿æ¨¡å‹æ¨ç†ç¬¦åˆä¸´åºŠæ ‡å‡†ã€‚</li>
<li>å¼•å…¥RadRBench-CXRåŸºå‡†æµ‹è¯•å’ŒRadRScoreè¯„ä¼°æŒ‡æ ‡è¯„ä»·æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ChestX-Reasoneråœ¨è¯Šæ–­å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºæœ€ä½³åŒ»ç–—MLLMã€æœ€ä½³é€šç”¨MLLMåŠå…¶åŸºç¡€æ¨¡å‹åˆ†åˆ«æœ‰16%ã€5.9%å’Œ18%çš„æ¨ç†èƒ½åŠ›æå‡ï¼Œä»¥åŠ3.3%ã€24%å’Œ27%çš„è¯Šæ–­ç»“æœå‡†ç¡®æ€§æå‡ã€‚</li>
<li>æ‰€æœ‰èµ„æºå‡å¼€æºï¼Œä¾¿äºåŒ»ç–—æ¨ç†MLLMçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c13dd6b3d9288c7e374ba6efd6851d26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfdb228ad3ba663f2bf723febf3e77d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aca356da45c1ced05c918a5559a74dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5845baf023f841852911dad79ae8ef7b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Classifier-to-Bias-Toward-Unsupervised-Automatic-Bias-Detection-for-Visual-Classifiers"><a href="#Classifier-to-Bias-Toward-Unsupervised-Automatic-Bias-Detection-for-Visual-Classifiers" class="headerlink" title="Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for   Visual Classifiers"></a>Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for   Visual Classifiers</h2><p><strong>Authors:Quentin Guimard, Moreno Dâ€™IncÃ , Massimiliano Mancini, Elisa Ricci</strong></p>
<p>A person downloading a pre-trained model from the web should be aware of its biases. Existing approaches for bias identification rely on datasets containing labels for the task of interest, something that a non-expert may not have access to, or may not have the necessary resources to collect: this greatly limits the number of tasks where model biases can be identified. In this work, we present Classifier-to-Bias (C2B), the first bias discovery framework that works without access to any labeled data: it only relies on a textual description of the classification task to identify biases in the target classification model. This description is fed to a large language model to generate bias proposals and corresponding captions depicting biases together with task-specific target labels. A retrieval model collects images for those captions, which are then used to assess the accuracy of the model w.r.t. the given biases. C2B is training-free, does not require any annotations, has no constraints on the list of biases, and can be applied to any pre-trained model on any classification task. Experiments on two publicly available datasets show that C2B discovers biases beyond those of the original datasets and outperforms a recent state-of-the-art bias detection baseline that relies on task-specific annotations, being a promising first step toward addressing task-agnostic unsupervised bias detection. </p>
<blockquote>
<p>å½“ä¸€ä¸ªäººä»ç½‘ä¸Šä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œåº”è¯¥æ„è¯†åˆ°å…¶å­˜åœ¨çš„åè§ã€‚ç°æœ‰çš„åè§è¯†åˆ«æ–¹æ³•ä¾èµ–äºåŒ…å«ä»»åŠ¡æ ‡ç­¾çš„æ•°æ®é›†ï¼Œéä¸“ä¸šäººå£«å¯èƒ½æ— æ³•è·å–è¿™äº›æ•°æ®é›†ï¼Œæˆ–è€…æ²¡æœ‰æ”¶é›†å¿…è¦èµ„æºçš„æ‰‹æ®µï¼šè¿™æå¤§åœ°é™åˆ¶äº†èƒ½å¤Ÿè¯†åˆ«æ¨¡å‹åè§çš„ä»»åŠ¡æ•°é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Classifier-to-Biasï¼ˆC2Bï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€è®¿é—®ä»»ä½•æ ‡è®°æ•°æ®å³å¯å·¥ä½œçš„åè§å‘ç°æ¡†æ¶ï¼šå®ƒä»…ä¾èµ–äºåˆ†ç±»ä»»åŠ¡çš„æ–‡æœ¬æè¿°æ¥è¯†åˆ«ç›®æ ‡åˆ†ç±»æ¨¡å‹ä¸­çš„åè§ã€‚è¿™ä¸ªæè¿°è¢«è¾“å…¥åˆ°ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆåè§ææ¡ˆå’Œç›¸åº”çš„æ ‡é¢˜ï¼Œè¿™äº›æ ‡é¢˜æç»˜äº†åè§ä»¥åŠä»»åŠ¡ç‰¹å®šçš„ç›®æ ‡æ ‡ç­¾ã€‚æ£€ç´¢æ¨¡å‹ä¼šæ”¶é›†è¿™äº›æ ‡é¢˜çš„å›¾åƒï¼Œç„¶åç”¨äºè¯„ä¼°æ¨¡å‹å¯¹äºç»™å®šåè§çš„å‡†ç¡®æ€§ã€‚C2Bæ— éœ€è®­ç»ƒï¼Œä¸éœ€è¦ä»»ä½•æ³¨é‡Šï¼Œå¯¹åè§åˆ—è¡¨æ²¡æœ‰çº¦æŸï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºä»»ä½•åˆ†ç±»ä»»åŠ¡çš„ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒC2Bèƒ½å¤Ÿå‘ç°è¶…å‡ºåŸå§‹æ•°æ®é›†èŒƒå›´çš„åè§ï¼Œå¹¶ä¸”ä¼˜äºæœ€è¿‘ä¾èµ–ä»»åŠ¡ç‰¹å®šæ³¨é‡Šçš„å…ˆè¿›åè§æ£€æµ‹åŸºçº¿ï¼Œè¿™æ˜¯æœç€è§£å†³ä¸ä»»åŠ¡æ— å…³çš„æ— ç›‘ç£åè§æ£€æµ‹çš„æœ‰å¸Œæœ›çš„ç¬¬ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20902v1">PDF</a> CVPR 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/mardgui/C2B">https://github.com/mardgui/C2B</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºClassifier-to-Biasï¼ˆC2Bï¼‰çš„åè§å‘ç°æ¡†æ¶ï¼Œå®ƒæ— éœ€è®¿é—®ä»»ä½•æ ‡è®°æ•°æ®ï¼Œä»…é€šè¿‡åˆ†ç±»ä»»åŠ¡æ–‡æœ¬æè¿°æ¥è¯†åˆ«ç›®æ ‡åˆ†ç±»æ¨¡å‹ä¸­çš„åè§ã€‚C2Bé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåè§ææ¡ˆå’Œç›¸åº”çš„æè¿°åè§çš„æ ‡é¢˜ï¼Œæ£€ç´¢æ¨¡å‹æ”¶é›†è¿™äº›æ ‡é¢˜çš„å›¾åƒï¼Œç„¶åç”¨äºè¯„ä¼°æ¨¡å‹å¯¹ç»™å®šåè§çš„å‡†ç¡®æ€§ã€‚å®ƒå…·æœ‰è®­ç»ƒå…è´¹ã€æ— éœ€æ³¨é‡Šã€æ— åè§åˆ—è¡¨é™åˆ¶ï¼Œå¯åº”ç”¨äºä»»ä½•åˆ†ç±»ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒC2Bèƒ½å¤Ÿåœ¨å…¬å¼€æ•°æ®é›†ä¸Šå‘ç°è¶…å‡ºåŸå§‹æ•°æ®é›†çš„åè§ï¼Œå¹¶ä¼˜äºä¾èµ–ä»»åŠ¡ç‰¹å®šæ³¨é‡Šçš„æœ€è¿‘å…ˆè¿›åè§æ£€æµ‹åŸºçº¿ï¼Œæ˜¯æœç€ä»»åŠ¡æ— å…³çš„æ— ç›‘ç£åè§æ£€æµ‹è¿ˆå‡ºçš„æœ‰å‰é€”çš„ç¬¬ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Classifier-to-Biasï¼ˆC2Bï¼‰æ˜¯ä¸€ä¸ªæ— éœ€æ ‡è®°æ•°æ®çš„åè§å‘ç°æ¡†æ¶ã€‚</li>
<li>C2Bé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåè§ææ¡ˆå’Œæè¿°åè§çš„æ ‡é¢˜ã€‚</li>
<li>C2Bé€šè¿‡æ£€ç´¢æ¨¡å‹æ”¶é›†æ ‡é¢˜å¯¹åº”çš„å›¾åƒæ¥è¯„ä¼°æ¨¡å‹çš„åè§ã€‚</li>
<li>C2Bå…·æœ‰è®­ç»ƒå…è´¹ã€æ— éœ€æ³¨é‡Šã€æ— åè§åˆ—è¡¨é™åˆ¶çš„ç‰¹ç‚¹ã€‚</li>
<li>C2Bå¯åº”ç”¨äºä»»ä½•åˆ†ç±»ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒC2Båœ¨å…¬å¼€æ•°æ®é›†ä¸Šèƒ½å¤Ÿå‘ç°è¶…å‡ºåŸå§‹æ•°æ®é›†çš„åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9746bbb9958d5949851ffdf1710209aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75935d8e0871b9477f3aeb75c2184031.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7274dbc5c2da90cc3a8636642a581742.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-Non-Core-Language-Instruction-Following-in-Speech-LLMs-via-Semi-Implicit-Cross-Lingual-CoT-Reasoning"><a href="#Enhancing-Non-Core-Language-Instruction-Following-in-Speech-LLMs-via-Semi-Implicit-Cross-Lingual-CoT-Reasoning" class="headerlink" title="Enhancing Non-Core Language Instruction-Following in Speech LLMs via   Semi-Implicit Cross-Lingual CoT Reasoning"></a>Enhancing Non-Core Language Instruction-Following in Speech LLMs via   Semi-Implicit Cross-Lingual CoT Reasoning</h2><p><strong>Authors:Hongfei Xue, Yufeng Tang, Hexin Liu, Jun Zhang, Xuelong Geng, Lei Xie</strong></p>
<p>Large language models have been extended to the speech domain, leading to the development of speech large language models (SLLMs). While existing SLLMs demonstrate strong performance in speech instruction-following for core languages (e.g., English), they often struggle with non-core languages due to the scarcity of paired speech-text data and limited multilingual semantic reasoning capabilities. To address this, we propose the semi-implicit Cross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates speech-to-text translation into the reasoning process of SLLMs. The XS-CoT generates four types of tokens: instruction and response tokens in both core and non-core languages, enabling cross-lingual transfer of reasoning capabilities. To mitigate inference latency in generating target non-core response tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which progressively compresses the first three types of intermediate reasoning tokens while retaining global reasoning logic during training. By leveraging the robust reasoning capabilities of the core language, XS-CoT improves responses for non-core languages by up to 45% in GPT-4 score when compared to direct supervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN. Moreover, the semi-implicit XS-CoT reduces token delay by more than 50% with a slight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount of high-quality training data for non-core languages by leveraging the reasoning capabilities of core languages. To support training, we also develop a data pipeline and open-source speech instruction-following datasets in Japanese, German, and French. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æ‰©å±•åˆ°è¯­éŸ³é¢†åŸŸï¼Œå‚¬ç”Ÿäº†è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰çš„å‘å±•ã€‚å°½ç®¡ç°æœ‰SLLMåœ¨æ ¸å¿ƒè¯­è¨€çš„è¯­éŸ³æŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ˆä¾‹å¦‚è‹±è¯­ï¼‰ï¼Œä½†ç”±äºç¼ºä¹é…å¥—çš„è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œæœ‰é™çš„è·¨è¯­è¨€è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œå®ƒä»¬åœ¨éæ ¸å¿ƒè¯­è¨€ä¸Šç»å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠéšå¼è·¨è¯­è¨€è¯­éŸ³æ€ç»´é“¾ï¼ˆXS-CoTï¼‰æ¡†æ¶ï¼Œå®ƒå°†è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘èå…¥SLLMçš„æ¨ç†è¿‡ç¨‹ä¸­ã€‚XS-CoTç”Ÿæˆå››ç§ç±»å‹çš„æ ‡è®°ï¼šæ ¸å¿ƒè¯­è¨€å’Œéæ ¸å¿ƒè¯­è¨€ä¸­çš„æŒ‡ä»¤å’Œå“åº”æ ‡è®°ï¼Œå®ç°äº†è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„è½¬ç§»ã€‚ä¸ºäº†å‡è½»ç”Ÿæˆç›®æ ‡éæ ¸å¿ƒå“åº”æ ‡è®°æ—¶çš„æ¨ç†å»¶è¿Ÿï¼Œæˆ‘ä»¬åœ¨XS-CoTä¸­çº³å…¥äº†åŠéšå¼CoTæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‹ç¼©å‰ä¸‰ç§ä¸­é—´æ¨ç†æ ‡è®°ï¼ŒåŒæ—¶ä¿ç•™å…¨å±€æ¨ç†é€»è¾‘ã€‚é€šè¿‡åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼ŒXS-CoTåœ¨ä¸¤ç§ä»£è¡¨æ€§SLLMï¼ˆQwen2-Audioå’ŒSALMONNï¼‰ä¸Šï¼Œä¸éæ ¸å¿ƒè¯­è¨€çš„ç›´æ¥ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼Œé€šè¿‡GPT-4è¯„åˆ†æé«˜äº†é«˜è¾¾45%çš„å“åº”èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒåŠéšå¼XS-CoTå‡å°‘äº†ä»¤ç‰Œå»¶è¿Ÿè¶…è¿‡50%ï¼ŒåŒæ—¶GPT-4å¾—åˆ†ç•¥æœ‰ä¸‹é™ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒXS-CoTä»…éœ€è¦å°‘é‡é«˜è´¨é‡çš„éæ ¸å¿ƒè¯­è¨€è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªæ•°æ®ç®¡é“å¹¶å…¬å¼€äº†æ—¥è¯­ã€å¾·è¯­å’Œæ³•è¯­ç­‰è¯­éŸ³æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20835v1">PDF</a> 10 pages, 6 figures, Submitted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æ‰©å±•åˆ°è¯­éŸ³é¢†åŸŸï¼Œå¯¼è‡´è¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰çš„å‘å±•ã€‚ç°æœ‰SLLMåœ¨è‹±è¯­ç­‰æ ¸å¿ƒè¯­è¨€è¯­éŸ³æŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç”±äºç¼ºä¹é…å¥—è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œæœ‰é™çš„å¤šè¯­è¨€è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œå¯¹éæ ¸å¿ƒè¯­è¨€çš„å¤„ç†å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŠéšå¼è·¨è¯­è¨€è¯­éŸ³æ€ç»´é“¾ï¼ˆXS-CoTï¼‰æ¡†æ¶ï¼Œå°†è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘èå…¥SLLMçš„æ¨ç†è¿‡ç¨‹ã€‚XS-CoTç”Ÿæˆå››ç§ç±»å‹çš„ç¬¦å·ï¼šæ ¸å¿ƒå’Œéæ ¸å¿ƒè¯­è¨€ä¸­çš„æŒ‡ä»¤å’Œå“åº”ç¬¦å·ï¼Œå®ç°è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„è½¬ç§»ã€‚é€šè¿‡é‡‡ç”¨åŠéšå¼CoTæ–¹æ¡ˆï¼Œåœ¨ç”Ÿæˆç›®æ ‡éæ ¸å¿ƒå“åº”ç¬¦å·æ—¶å‡è½»æ¨ç†å»¶è¿Ÿï¼Œè¯¥æ–¹æ¡ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‹ç¼©å‰ä¸‰ç§ä¸­é—´æ¨ç†ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™å…¨å±€æ¨ç†é€»è¾‘ã€‚ä¸å¯¹ä¸¤ä¸ªä»£è¡¨æ€§SLLMï¼ˆQwen2-Audioå’ŒSALMONNï¼‰çš„ç›´æ¥ç›‘ç£å¾®è°ƒç›¸æ¯”ï¼ŒXS-CoTåˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œæé«˜éæ ¸å¿ƒè¯­è¨€çš„å“åº”ç‡é«˜è¾¾45%ã€‚æ­¤å¤–ï¼ŒåŠéšå¼XS-CoTå°†ç¬¦å·å»¶è¿Ÿå‡å°‘äº†50%ä»¥ä¸Šï¼ŒåŒæ—¶GPT-4å¾—åˆ†ç•¥æœ‰ä¸‹é™ã€‚é‡è¦çš„æ˜¯ï¼ŒXS-CoTä»…éœ€è¦å°‘é‡é«˜è´¨é‡çš„éæ ¸å¿ƒè¯­è¨€è®­ç»ƒæ•°æ®ï¼Œå°±å¯ä»¥åˆ©ç”¨æ ¸å¿ƒè¯­è¨€çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²æ‰©å±•è‡³è¯­éŸ³é¢†åŸŸï¼Œå½¢æˆè¯­éŸ³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰ã€‚</li>
<li>SLLMåœ¨å¤„ç†éæ ¸å¿ƒè¯­è¨€æ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œæ¨ç†èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºXS-CoTæ¡†æ¶ï¼Œå°†è¯­éŸ³è½¬åŒ–ä¸ºæ–‡æœ¬è¿›è¡Œæ¨ç†ï¼Œæ”¯æŒæ ¸å¿ƒä¸éæ ¸å¿ƒè¯­è¨€ã€‚</li>
<li>XS-CoTé€šè¿‡é€æ­¥å‹ç¼©ä¸­é—´æ¨ç†ç¬¦å·å‡å°‘æ¨ç†å»¶è¿Ÿã€‚</li>
<li>XS-CoTæ”¹è¿›éæ ¸å¿ƒè¯­è¨€çš„å“åº”èƒ½åŠ›ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æé«˜45%ã€‚</li>
<li>åŠéšå¼XS-CoTå‡å°‘ç¬¦å·å»¶è¿Ÿè¶…è¿‡50%ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„GPT-4å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6681e18f064e2941ae3214cba61e5ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-110126185ccfef4c691b7c4a7011671e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9adf7c8c9c2ca386de1f2df1a569948d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c6bff61b3a3dd1399359b4e3161d791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a82d32d625949d627e1e9e269267e36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d6bbe7df84105ddaa62d3f8f275cec88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-546d67a83a2c5d771e8601d1b4245de7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Secure-Coding-with-AI-From-Creation-to-Inspection"><a href="#Secure-Coding-with-AI-From-Creation-to-Inspection" class="headerlink" title="Secure Coding with AI, From Creation to Inspection"></a>Secure Coding with AI, From Creation to Inspection</h2><p><strong>Authors:Vladislav Belozerov, Peter J Barclay, Ashkan Sami</strong></p>
<p>While prior studies have explored security in code generated by ChatGPT and other Large Language Models, they were conducted in controlled experimental settings and did not use code generated or provided from actual developer interactions. This paper not only examines the security of code generated by ChatGPT based on real developer interactions, curated in the DevGPT dataset, but also assesses ChatGPTâ€™s capability to find and fix these vulnerabilities. We analysed 1,586 C, C++, and C# code snippets using static scanners, which detected potential issues in 124 files. After manual analysis, we selected 26 files with 32 confirmed vulnerabilities for further investigation.   We submitted these files to ChatGPT via the OpenAI API, asking it to detect security issues, identify the corresponding Common Weakness Enumeration numbers, and propose fixes. The responses and modified code were manually reviewed and re-scanned for vulnerabilities. ChatGPT successfully detected 18 out of 32 security issues and resolved 17 issues but failed to recognize or fix the remainder. Interestingly, only 10 vulnerabilities were resulted from the user prompts, while 22 were introduced by ChatGPT itself.   We highlight for developers that code generated by ChatGPT is more likely to contain vulnerabilities compared to their own code. Furthermore, at times ChatGPT reports incorrect information with apparent confidence, which may mislead less experienced developers. Our findings confirm previous studies in demonstrating that ChatGPT is not sufficiently reliable for generating secure code nor identifying all vulnerabilities, highlighting the continuing importance of static scanners and manual review. </p>
<blockquote>
<p>è™½ç„¶å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢è®¨äº†ChatGPTå’Œå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç çš„å®‰å…¨æ€§ï¼Œä½†è¿™äº›ç ”ç©¶æ˜¯åœ¨å—æ§å®éªŒç¯å¢ƒä¸­è¿›è¡Œçš„ï¼Œå¹¶æ²¡æœ‰ä½¿ç”¨å®é™…å¼€å‘è€…äº¤äº’ç”Ÿæˆçš„ä»£ç ã€‚æœ¬æ–‡ä¸ä»…åŸºäºDevGPTæ•°æ®é›†çš„å®é™…å¼€å‘è€…äº¤äº’ï¼Œå¯¹ChatGPTç”Ÿæˆçš„ä»£ç å®‰å…¨æ€§è¿›è¡Œäº†è€ƒå¯Ÿï¼Œè¿˜è¯„ä¼°äº†ChatGPTå‘ç°å’Œä¿®å¤è¿™äº›æ¼æ´çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨é™æ€æ‰«æå™¨åˆ†æäº†1586ä¸ªCã€C++å’ŒC#çš„ä»£ç ç‰‡æ®µï¼Œå…¶ä¸­æ£€æµ‹å‡ºæ½œåœ¨é—®é¢˜å­˜åœ¨äº124ä¸ªæ–‡ä»¶ä¸­ã€‚ç»è¿‡æ‰‹åŠ¨åˆ†æåï¼Œæˆ‘ä»¬é€‰æ‹©äº†åŒ…å«å·²ç¡®è®¤çš„32ä¸ªæ¼æ´çš„26ä¸ªæ–‡ä»¶è¿›è¡Œè¿›ä¸€æ­¥è°ƒæŸ¥ã€‚æˆ‘ä»¬é€šè¿‡OpenAI APIå°†è¿™äº›æ–‡ä»¶æäº¤ç»™ChatGPTï¼Œè¦æ±‚å…¶æ£€æµ‹å®‰å…¨é—®é¢˜ï¼Œè¯†åˆ«ç›¸åº”çš„å¸¸è§å¼±ç‚¹æšä¸¾ç¼–å·ï¼Œå¹¶æå‡ºä¿®å¤å»ºè®®ã€‚æˆ‘ä»¬å¯¹ChatGPTçš„å›åº”å’Œä¿®æ”¹åçš„ä»£ç è¿›è¡Œäº†æ‰‹åŠ¨å®¡æŸ¥ï¼Œå¹¶é‡æ–°æ‰«ææ¼æ´ã€‚ChatGPTæˆåŠŸæ£€æµ‹å‡ºäº†å…¶ä¸­çš„18ä¸ªå®‰å…¨é—®é¢˜å¹¶è§£å†³äº†å…¶ä¸­çš„17ä¸ªé—®é¢˜ï¼Œä½†æœªèƒ½è¯†åˆ«æˆ–ä¿®å¤å…¶ä½™çš„é—®é¢˜ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»…æœ‰10ä¸ªæ¼æ´æ˜¯ç”±äºç”¨æˆ·æç¤ºé€ æˆçš„ï¼Œè€Œå‰©ä¸‹çš„åˆ™æ˜¯ChatGPTæœ¬èº«é€ æˆçš„ã€‚æˆ‘ä»¬å‘å¼€å‘è€…å¼ºè°ƒï¼Œç›¸è¾ƒäºä»–ä»¬è‡ªå·±çš„ä»£ç ï¼ŒChatGPTç”Ÿæˆçš„ä»£ç æ›´æœ‰å¯èƒ½åŒ…å«æ¼æ´ã€‚æ­¤å¤–ï¼Œæœ‰æ—¶ChatGPTä¼šè¡¨ç°å‡ºæ˜æ˜¾çš„è‡ªä¿¡è€ŒæŠ¥å‘Šé”™è¯¯çš„ä¿¡æ¯ï¼Œå¯èƒ½ä¼šè¯¯å¯¼ç»éªŒè¾ƒå°‘çš„å¼€å‘è€…ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¯å®äº†ä¹‹å‰çš„ç ”ç©¶ï¼Œè¡¨æ˜ChatGPTåœ¨ç”Ÿæˆå®‰å…¨ä»£ç æˆ–è¯†åˆ«æ‰€æœ‰æ¼æ´æ–¹é¢å°šä¸å¯é ï¼Œè¿™è¿›ä¸€æ­¥å¼ºè°ƒäº†é™æ€æ‰«æå™¨å’Œäººå·¥å®¡æŸ¥çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20814v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºçœŸå®å¼€å‘è€…äº¤äº’çš„ChatGPTç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ã€‚é€šè¿‡åˆ†æDevGPTæ•°æ®é›†ä¸­çš„1,586ä¸ªCã€C++å’ŒC#ä»£ç ç‰‡æ®µï¼Œå‘ç°æ½œåœ¨é—®é¢˜å¹¶ç¡®è®¤å­˜åœ¨32ä¸ªæ¼æ´ã€‚é€šè¿‡OpenAI APIæäº¤ç»™ChatGPTæ£€æµ‹å’Œä¿®å¤è¿™äº›å®‰å…¨é—®é¢˜ï¼ŒChatGPTæˆåŠŸæ£€æµ‹åˆ°å…¶ä¸­18ä¸ªé—®é¢˜å¹¶è§£å†³å…¶ä¸­17ä¸ªï¼Œä½†æœªèƒ½è¯†åˆ«æˆ–ä¿®å¤å…¶ä½™é—®é¢˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåªæœ‰å°‘æ•°æ¼æ´æ¥è‡ªç”¨æˆ·æç¤ºï¼Œè€Œå¤§å¤šæ•°æ˜¯ç”±ChatGPTè‡ªèº«å¼•å…¥çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒChatGPTç”Ÿæˆçš„ä»£ç å¯èƒ½åŒ…å«æ›´å¤šæ¼æ´ï¼Œå¼€å‘è€…åº”è°¨æ…ä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒChatGPTæœ‰æ—¶ä¼šæŠ¥å‘Šé”™è¯¯çš„ä¿¡æ¯ï¼Œå¯èƒ½ä¼šè¯¯å¯¼ç»éªŒä¸è¶³çš„å¼€å‘è€…ã€‚å°½ç®¡ChatGPTæœ‰å…¶å±€é™æ€§ï¼Œä½†é™æ€æ‰«æå™¨å’Œäººå·¥å®¡æŸ¥çš„é‡è¦æ€§ä»ç„¶ä¸å®¹å¿½è§†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†åŸºäºçœŸå®å¼€å‘è€…äº¤äº’çš„ChatGPTç”Ÿæˆä»£ç çš„å®‰å…¨æ€§ã€‚</li>
<li>é€šè¿‡é™æ€æ‰«æå’Œåˆ†æï¼Œå‘ç°äº†å¤§é‡æ½œåœ¨çš„å®‰å…¨é—®é¢˜å¹¶ç¡®è®¤äº†32ä¸ªæ¼æ´ã€‚</li>
<li>ChatGPTæˆåŠŸæ£€æµ‹åˆ°å¹¶ä¿®å¤äº†éƒ¨åˆ†å®‰å…¨é—®é¢˜ï¼Œä½†æœªèƒ½å…¨é¢è¯†åˆ«å’Œä¿®å¤æ‰€æœ‰æ¼æ´ã€‚</li>
<li>ChatGPTç”Ÿæˆçš„ä»£ç å¯èƒ½æ¯”å¼€å‘è€…è‡ªå·±çš„ä»£ç æ›´å®¹æ˜“åŒ…å«æ¼æ´ã€‚</li>
<li>ChatGPTæœ‰æ—¶ä¼šæŠ¥å‘Šé”™è¯¯çš„ä¿¡æ¯ï¼Œå¯¹å¼€å‘è€…é€ æˆè¯¯å¯¼ï¼Œç‰¹åˆ«æ˜¯ç»éªŒä¸è¶³çš„å¼€å‘è€…éœ€æ ¼å¤–è­¦æƒ•ã€‚</li>
<li>å°½ç®¡ChatGPTåœ¨æŸäº›æ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ï¼Œä½†é™æ€æ‰«æå™¨å’Œäººå·¥å®¡æŸ¥ä»ç„¶æ˜¯ç¡®ä¿ä»£ç å®‰å…¨çš„é‡è¦å·¥å…·ã€‚</li>
<li>ç ”ç©¶ç»“æœé‡ç”³äº†å¯¹äºä»£ç å®‰å…¨æ€§å’Œè´¨é‡æ§åˆ¶çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa04dcfd6ece45fe813176a3a0a1f6f5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unlocking-User-oriented-Pages-Intention-driven-Black-box-Scanner-for-Real-world-Web-Applications"><a href="#Unlocking-User-oriented-Pages-Intention-driven-Black-box-Scanner-for-Real-world-Web-Applications" class="headerlink" title="Unlocking User-oriented Pages: Intention-driven Black-box Scanner for   Real-world Web Applications"></a>Unlocking User-oriented Pages: Intention-driven Black-box Scanner for   Real-world Web Applications</h2><p><strong>Authors:Weizhe Wang, Yao Zhang, Kaitai Liang, Guangquan Xu, Hongpeng Bai, Qingyang Yan, Xi Zheng, Bin Wu</strong></p>
<p>Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing test coverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the Large Language Model to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data&#x2F;code is available at <a target="_blank" rel="noopener" href="https://hoyen.tjunsl.com/">https://hoyen.tjunsl.com/</a> </p>
<blockquote>
<p>é»‘ç›’æ‰«æå™¨åœ¨æ£€æµ‹Webåº”ç”¨ç¨‹åºæ¼æ´æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚å½“å‰é»‘ç›’æ‰«æçš„å…³é”®ç„¦ç‚¹æ˜¯æé«˜æµ‹è¯•è¦†ç›–ç‡ï¼ˆå³è®¿é—®æ›´å¤šçš„ç½‘é¡µï¼‰ã€‚ç„¶è€Œï¼Œç”±äºè®¸å¤šWebåº”ç”¨ç¨‹åºé¢å‘ç”¨æˆ·ï¼Œä¸€äº›æ·±å±‚é¡µé¢åªèƒ½é€šè¿‡å¤æ‚çš„ç”¨æˆ·äº¤äº’è®¿é—®ï¼Œè€Œç°æœ‰çš„é»‘ç›’æ‰«æå™¨å¾ˆéš¾è¾¾åˆ°è¿™äº›é¡µé¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä¸€ä¸ªé‡è¦çš„è§è§£æ˜¯ç½‘é¡µåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥å¸®åŠ©ç†è§£æ½œåœ¨çš„ç”¨æˆ·æ„å›¾ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†Hoyenï¼Œä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹ç”¨æˆ·æ„å›¾å¹¶æä¾›æŒ‡å¯¼ä»¥æ‰©å¤§æ‰«æèŒƒå›´çš„çš„é»‘ç›’æ‰«æå™¨ã€‚Hoyenåœ¨12ä¸ªæµè¡Œçš„å¼€æºWebåº”ç”¨ç¨‹åºä¸Šè¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œå¹¶ä¸6ç§ä»£è¡¨æ€§å·¥å…·è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒHoyenå¯¹Webåº”ç”¨ç¨‹åºè¿›è¡Œäº†å…¨é¢æ¢ç´¢ï¼Œæ‰©å¤§äº†æ”»å‡»é¢ï¼Œå¹³å‡è¦†ç›–ç‡æ¯”å…¶ä»–æ‰«æå™¨é«˜å‡ºçº¦2å€ï¼Œè¯·æ±‚ç²¾åº¦é«˜ã€‚æ­¤å¤–ï¼ŒHoyené’ˆå¯¹åº”ç”¨ç¨‹åºæ ¸å¿ƒåŠŸèƒ½çš„è¯·æ±‚è¶…è¿‡90%ï¼Œæ£€æµ‹åˆ°çš„æ¼æ´æ¯”å…¶ä»–æ‰«æå™¨æ›´å¤šï¼ŒåŒ…æ‹¬åœ¨çŸ¥åWebåº”ç”¨ç¨‹åºä¸­çš„ç‹¬ç‰¹æ¼æ´ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://hoyen.tjunsl.com/%E6%89%BE%E5%88%B0%E3%80%82">https://hoyen.tjunsl.com/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20801v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>é»‘ç›’æ‰«æå™¨åœ¨æ£€æµ‹Webåº”ç”¨ç¨‹åºæ¼æ´æ–¹é¢æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å—é™äºæµ‹è¯•è¦†ç›–èŒƒå›´ã€‚ä¸ºè§£å†³è¯¥é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰ä¿¡æ¯çš„é»‘ç›’æ‰«æå™¨â€”â€”Hoyenã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹ç”¨æˆ·æ„å›¾ï¼Œæ‰©å±•æ‰«æèŒƒå›´å¹¶å¢åŠ æµ‹è¯•è¦†ç›–ç‡ã€‚åœ¨å¤šä¸ªå¼€æºWebåº”ç”¨ç¨‹åºä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒHoyenå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹³å‡è¦†ç›–ç‡æ˜¯å…¶ä»–æ‰«æå™¨çš„ä¸¤å€ä»¥ä¸Šï¼Œè¯·æ±‚å‡†ç¡®ç‡é«˜ï¼Œèƒ½æ£€æµ‹æ›´å¤šæ¼æ´ï¼ŒåŒ…æ‹¬çŸ¥åWebåº”ç”¨ç¨‹åºçš„ç‹¬ç‰¹æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é»‘ç›’æ‰«æå™¨åœ¨æ£€æµ‹Webåº”ç”¨ç¨‹åºæ¼æ´ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†æµ‹è¯•è¦†ç›–èŒƒå›´æœ‰é™ã€‚</li>
<li>å½“å‰é»‘ç›’æ‰«æçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯è®¿é—®å¤æ‚çš„ç”¨æˆ·äº¤äº’é¡µé¢ã€‚</li>
<li>åŸºäºè¯­ä¹‰ä¿¡æ¯é¢„æµ‹ç”¨æˆ·æ„å›¾æ˜¯è§£å†³æ­¤æŒ‘æˆ˜çš„ä¸€ç§æ–¹æ³•ã€‚</li>
<li>Hoyenæ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é»‘ç›’æ‰«æå™¨ï¼Œæ—¨åœ¨æ‰©å±•æ‰«æèŒƒå›´å¹¶æé«˜æµ‹è¯•è¦†ç›–ç‡ã€‚</li>
<li>Hoyené€šè¿‡é¢„æµ‹ç”¨æˆ·æ„å›¾æ¥æŒ‡å¯¼æ‰«æè¿‡ç¨‹ï¼Œä»è€Œå®ç°å¯¹Webåº”ç”¨ç¨‹åºçš„å…¨é¢æ¢ç´¢ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒHoyenæ€§èƒ½ä¼˜è¶Šï¼Œä¸å…¶ä»–æ‰«æå™¨ç›¸æ¯”ï¼Œå¹³å‡è¦†ç›–ç‡æé«˜äº†ä¸¤å€ä»¥ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7202c9527cbf16d759acc03f9e3d5353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcb5330797ec48eeb1c38f5244f1ed99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dba1b95fb92e9129aafbc2a31e551f47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-323694235910903f3e397f1dcfdd9b85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acf00a31a02594763225221a133ff42f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b02f474a90ffa49a44576f8c4bb2c3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f09007cf45724ec79177389b3e029cc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bd42ab6beca8060bb282c8a356c36cf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers"><a href="#Grokking-in-the-Wild-Data-Augmentation-for-Real-World-Multi-Hop-Reasoning-with-Transformers" class="headerlink" title="Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers"></a>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers</h2><p><strong>Authors:Roman Abramov, Felix Steinbauer, Gjergji Kasneci</strong></p>
<p>Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models. </p>
<blockquote>
<p>è½¬æ¢å™¨åœ¨ä¼—å¤šNLPä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥éª¤äº‹å®æ¨ç†æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•ŒçŸ¥è¯†ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æœ€è¿‘å¯¹grokkingçš„ç ”ç©¶è¿›å±•è¡¨æ˜ï¼Œä¸€æ—¦ç¥ç»ç½‘ç»œæ£€æµ‹åˆ°æ½œåœ¨çš„é€»è¾‘æ¨¡å¼ï¼Œå®ƒä»¬å°±å¯ä»¥ä»è®°å¿†è¿‡æ¸¡åˆ°å®Œå…¨æ³›åŒ–â€”â€”ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦ä½¿ç”¨çš„æ˜¯å°è§„æ¨¡çš„åˆæˆä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†grokkingæ‰©å±•åˆ°ç°å®ä¸–ç•Œçš„äº‹å®æ•°æ®ï¼Œå¹¶é€šè¿‡å¢åŠ ç²¾å¿ƒè®¾è®¡åˆæˆæ•°æ®æ¥å¢å¼ºç°æœ‰çŸ¥è¯†å›¾è°±ï¼Œä»¥åº”å¯¹æ•°æ®é›†ç¨€ç–æ€§çš„æŒ‘æˆ˜ï¼Œä»è€Œæé«˜æ¨æ–­äº‹å®ä¸åŸå­äº‹å®çš„æ¯”ç‡$\phi_r$ï¼Œè¾¾åˆ°grokkingæ‰€éœ€çš„é˜ˆå€¼ä»¥ä¸Šã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯äº‹å®é”™è¯¯çš„åˆæˆæ•°æ®ä¹Ÿå¯ä»¥åŠ å¼ºæ–°å…´æ¨ç†ç”µè·¯ï¼Œè€Œä¸æ˜¯é™ä½å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒè¿«ä½¿æ¨¡å‹ä¾èµ–å…³ç³»ç»“æ„è€Œä¸æ˜¯è®°å¿†ã€‚åœ¨è¯„ä¼°å¤šè·³æ¨ç†åŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨2WikiMultiHopQAä¸Šè¾¾åˆ°äº†95-100%çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…è¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶è¾¾åˆ°æˆ–è¶…è¿‡äº†å½“å‰æœ€æ–°æŠ€æœ¯æ°´å¹³çš„ç»“æœã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†å¦‚ä½•æé«˜$\phi_r$åœ¨è½¬æ¢å™¨å†…éƒ¨å½¢æˆæ³›åŒ–ç”µè·¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºgrokkingçš„æ•°æ®å¢å¼ºå¯ä»¥è§£é”éšå¼çš„å¤šè·³æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­çš„æ›´ç¨³å¥å’Œå¯è§£é‡Šçš„äº‹å®æ¨ç†æ‰“å¼€äº†å¤§é—¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20752v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç¥ç»ç½‘ç»œåœ¨é¢ä¸´çŸ¥è¯†ç¨€ç¼ºçš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸è¶³ã€‚ç„¶è€Œï¼Œæ–°ç ”ç©¶è¡¨æ˜é€šè¿‡é€»è¾‘æ¨¡å¼çš„æŒæ¡è€Œè¿›è¡Œçš„æ·±åº¦åˆ†æå¯ä½¿æ¨¡å‹åœ¨çœŸå®çš„å›¾è°±æ•°æ®é›†ä¸Šè¾¾æˆæ›´å¥½çš„æ³›åŒ–æ•ˆæœã€‚ç ”ç©¶ä½¿ç”¨åˆæˆæ•°æ®æ¥æ‰©å……ç°æœ‰çŸ¥è¯†å›¾è°±ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ç†è§£å¤æ‚æ¨ç†æ—¶çš„å‡†ç¡®åº¦ã€‚æ­¤å¤–ï¼Œå³ä½¿ä¸çœŸå®æ•°æ®äº‹å®ä¸å®Œå…¨ä¸€è‡´çš„åˆæˆæ•°æ®ä¹Ÿèƒ½å¤Ÿé€šè¿‡è®©æ¨¡å‹æ›´å¤šåœ°ä¾èµ–äºç»“æ„åŒ–å…³ç³»æ¥åŠ å¼ºæ¨¡å‹å¯¹äºé€»è¾‘æ¨ç†èƒ½åŠ›çš„ä¾èµ–æ€§ï¼Œæå‡å…¶åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ¬ç ”ç©¶çš„åˆ†æå‘ç°åˆæˆæ•°æ®çš„ä½¿ç”¨æœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸”åœ¨æ·±å…¥ç ”ç©¶æ•°æ®å¢åŠ æ—¶æ¨ç†ç”µè·¯çš„æ„å»ºè¿‡ç¨‹åï¼Œæ­ç¤ºäº†å¢å¼ºæ³›åŒ–èƒ½åŠ›çš„å…³é”®æœºåˆ¶ã€‚è¿™ä¸€å‘ç°ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç¨³å¥å’Œå¯è§£é‡Šçš„å¤šæ­¥æ¨ç†èƒ½åŠ›æä¾›äº†å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong><br>     1. ç¥ç»ç½‘ç»œåœ¨å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çŸ¥è¯†ç¨€ç¼ºçš„é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ã€‚<br>     2. é€šè¿‡é€»è¾‘æ¨¡å¼çš„æŒæ¡è¿›è¡Œçš„æ·±åº¦åˆ†æå¯ä»¥ä½¿æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾è°±æ•°æ®é›†ä¸Šå®ç°æ›´å¥½çš„æ³›åŒ–æ•ˆæœã€‚é€šè¿‡æ‰©å……ç°æœ‰çŸ¥è¯†å›¾è°±çš„ç ”ç©¶æ­ç¤ºäº†æ–°è§è§£ã€‚<br>     3. ä½¿ç”¨åˆæˆæ•°æ®å¯ä»¥æå‡æ¨¡å‹åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®åº¦ï¼Œå¹¶æå‡æ¨¡å‹å¯¹äºç»“æ„åŒ–å…³ç³»çš„ä¾èµ–ã€‚è¿™æä¾›äº†ä¸€ç§å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚<br>     4. å³ä½¿åŒ…å«é”™è¯¯äº‹å®çš„åˆæˆæ•°æ®ä¹Ÿæœ‰åŠ©äºåŠ å¼ºæ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬ä¿ƒä½¿æ¨¡å‹æ›´ä¾èµ–äºç»“æ„åŒ–çš„å…³ç³»è€Œä¸æ˜¯ç®€å•è®°å¿†çŸ¥è¯†ä¿¡æ¯ï¼Œä»è€Œä½¿å¾—æ¨ç†æ›´å¥å£®ä¸”æ›´åŠ èƒ½å¤Ÿå¯¹æŠ—å¯¹æŠ—åŸºäºè§‚å¯Ÿè€Œéå½’çº³æ³•çš„æ¨¡å‹å±€é™ã€‚<br>     5. é€šè¿‡æ·±å…¥ç ”ç©¶æ•°æ®å¢åŠ æ—¶æ¨ç†ç”µè·¯çš„æ„å»ºè¿‡ç¨‹ï¼Œæ­ç¤ºäº†å¢å¼ºæ³›åŒ–èƒ½åŠ›çš„å…³é”®æœºåˆ¶ã€‚<br>     6. ç ”ç©¶ç»“æœæä¾›äº†ä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œå³é€šè¿‡åˆæˆæ•°æ®å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä½¿å¾—å…¶åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥æ¨ç†ä»»åŠ¡æ—¶æ›´å…·å¯é æ€§ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdb74d5a7f76ff736ad815680153d268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a55d5cbe8ee683dc6e752d80e2a233.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b9b7b6e7023119aa898965d466f8aab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e554fd2d70706b64cfe4ff222dedc21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76abdaf2dcffe85e82b856b4bbf98e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6591030219d9ec9b02801b6e8e3fc520.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="In-Context-Edit-Enabling-Instructional-Image-Editing-with-In-Context-Generation-in-Large-Scale-Diffusion-Transformer"><a href="#In-Context-Edit-Enabling-Instructional-Image-Editing-with-In-Context-Generation-in-Large-Scale-Diffusion-Transformer" class="headerlink" title="In-Context Edit: Enabling Instructional Image Editing with In-Context   Generation in Large Scale Diffusion Transformer"></a>In-Context Edit: Enabling Instructional Image Editing with In-Context   Generation in Large Scale Diffusion Transformer</h2><p><strong>Authors:Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang</strong></p>
<p>Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)â€™ enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our methodâ€™s superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in <a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/">https://river-zhang.github.io/ICEdit-gh-pages/</a>. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°ç¨³å¥çš„å›¾åƒä¿®æ”¹ï¼Œä½†å½“å‰çš„æ–¹æ³•é¢ä¸´ç€ç²¾åº¦ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚å¾®è°ƒæ–¹æ³•éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºå’Œå¤§é‡çš„æ•°æ®é›†ï¼Œè€Œæ— éœ€è®­ç»ƒçš„æŠ€æœ¯åˆ™åœ¨ç†è§£æŒ‡ä»¤å’Œç¼–è¾‘è´¨é‡æ–¹é¢é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡çš„æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å¢å¼ºçš„ç”Ÿæˆèƒ½åŠ›å’Œæœ¬åœ°ä¸Šä¸‹æ–‡æ„è¯†æ¥è§£å†³è¿™ä¸€éš¾é¢˜ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¼•å…¥äº†ä¸‰ä¸ªè´¡çŒ®ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ä¸Šä¸‹æ–‡æç¤ºçš„é›¶å¯åŠ¨æŒ‡ä»¤åˆè§„æ€§çš„ä¸Šä¸‹æ–‡å†…ç¼–è¾‘æ¡†æ¶ï¼Œé¿å…ç»“æ„å˜åŒ–ï¼›ï¼ˆ2ï¼‰LoRA-MoEæ··åˆè°ƒæ•´ç­–ç•¥ï¼Œé€šè¿‡é«˜æ•ˆçš„é€‚åº”æ€§å’ŒåŠ¨æ€ä¸“å®¶è·¯ç”±å¢å¼ºçµæ´»æ€§ï¼Œè€Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒï¼›ï¼ˆ3ï¼‰ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ—©æœŸè¿‡æ»¤æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œä»¥æ›´æ—©åœ°é€‰æ‹©æ›´å¥½çš„åˆå§‹å™ªå£°ï¼Œæé«˜ç¼–è¾‘è´¨é‡ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼šå®ƒä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä»…éœ€è¦0.5%çš„è®­ç»ƒæ•°æ®å’Œ1%çš„å¯è®­ç»ƒå‚æ•°ï¼Œä¸ä¼ ç»Ÿçš„åŸºçº¿ç›¸æ¯”ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ä¸ªæ–°èŒƒå¼ï¼Œèƒ½å¤Ÿå®ç°é«˜ç²¾åº¦ä¸”é«˜æ•ˆçš„æŒ‡ä»¤æŒ‡å¯¼ç¼–è¾‘ã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/%E6%89%BE%E5%88%B0%E3%80%82">https://river-zhang.github.io/ICEdit-gh-pages/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20690v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://river-zhang.github.io/ICEdit-gh-pages/">https://river-zhang.github.io/ICEdit-gh-pages/</a></p>
<p><strong>Summary</strong><br>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°äº†å¼ºå¤§çš„å›¾åƒä¿®æ”¹åŠŸèƒ½ã€‚å½“å‰æ–¹æ³•é¢ä¸´ç²¾ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç²¾ç»†è°ƒæ•´æ–¹æ³•éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œå¤§å‹æ•°æ®é›†ï¼Œè€Œæ— éœ€è®­ç»ƒçš„æŠ€æœ¯åˆ™åœ¨æŒ‡ä»¤ç†è§£å’Œç¼–è¾‘è´¨é‡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡çš„æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰å¢å¼ºçš„ç”Ÿæˆèƒ½åŠ›å’ŒåŸç”Ÿä¸Šä¸‹æ–‡æ„è¯†ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¼•å…¥äº†ä¸‰ä¸ªè´¡çŒ®ï¼šä¸€ã€ä½¿ç”¨ä¸Šä¸‹æ–‡æç¤ºçš„é›¶å°„å‡»æŒ‡ä»¤ç¬¦åˆæ€§çš„ä¸Šä¸‹æ–‡ç¼–è¾‘æ¡†æ¶ï¼Œé¿å…ç»“æ„å˜åŒ–ï¼›äºŒã€ä¸€ç§LoRA-MoEæ··åˆè°ƒæ•´ç­–ç•¥ï¼Œé€šè¿‡æœ‰æ•ˆçš„é€‚åº”å’ŒåŠ¨æ€ä¸“å®¶è·¯ç”±æé«˜çµæ´»æ€§ï¼Œæ— éœ€å¤§é‡é‡æ–°è®­ç»ƒï¼›ä¸‰ã€ä¸€ç§æ—©æœŸè¿‡æ»¤æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€‰æ‹©æ›´å¥½çš„åˆå§‹å™ªå£°æ—©æœŸï¼Œæé«˜ç¼–è¾‘è´¨é‡ã€‚å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä»…éœ€è¦0.5ï¼…çš„è®­ç»ƒæ•°æ®å’Œ1ï¼…çš„å¯è®­ç»ƒå‚æ•°ç›¸æ¯”ä¼ ç»Ÿçš„åŸºå‡†æµ‹è¯•ã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ä¸ªæ–°èŒƒå¼ï¼Œå®ç°äº†é«˜ç²¾åº¦ä¸”é«˜æ•ˆçš„æŒ‡ä»¤æŒ‡å¯¼ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤åŸºç¡€çš„å›¾åƒç¼–è¾‘é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå®ç°å›¾åƒä¿®æ”¹ï¼Œä½†å­˜åœ¨ç²¾ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œæ•°æ®é›†è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œè€Œæ— éœ€è®­ç»ƒçš„æŠ€æœ¯åœ¨ç†è§£å’Œç¼–è¾‘è´¨é‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶äººå‘˜åˆ©ç”¨å¤§è§„æ¨¡çš„æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œå¢å¼ºäº†ç”Ÿæˆèƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§ä¸»è¦è´¡çŒ®ï¼šé›¶å°„å‡»æŒ‡ä»¤ç¬¦åˆæ€§çš„ä¸Šä¸‹æ–‡ç¼–è¾‘æ¡†æ¶ã€LoRA-MoEæ··åˆè°ƒæ•´ç­–ç•¥å’Œæ—©æœŸè¿‡æ»¤æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”åªéœ€è¦è¾ƒå°‘çš„è®­ç»ƒæ•°æ®å’Œå¯è®­ç»ƒå‚æ•°ã€‚</li>
<li>è¯¥ç ”ç©¶å»ºç«‹äº†æ–°çš„èŒƒå¼ï¼Œå®ç°äº†é«˜ç²¾åº¦ä¸”é«˜æ•ˆçš„æŒ‡ä»¤æŒ‡å¯¼ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf5a6ad768019d54d95a9fdbb1a75fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ddf1ca7217ea836ba669e403e05a0e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51686290867e13c1b7a5de78509223d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd1bcc90c5611cc7166a9445207cf0c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fbfde771490be8603809918cbb888f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cdbbcb4990199a9a7bce8c5b5d08a66.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="TF1-EN-3M-Three-Million-Synthetic-Moral-Fables-for-Training-Small-Open-Language-Models"><a href="#TF1-EN-3M-Three-Million-Synthetic-Moral-Fables-for-Training-Small-Open-Language-Models" class="headerlink" title="TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open   Language Models"></a>TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open   Language Models</h2><p><strong>Authors:Mihai Nadas, Laura Diosan, Andrei Piscoran, Andreea Tomescu</strong></p>
<p>Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -&gt; trait -&gt; setting -&gt; conflict -&gt; resolution -&gt; moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (&lt;24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models. </p>
<blockquote>
<p>é“å¾·æ•…äº‹æ˜¯ä¼ é€’ä»·å€¼çš„ç»è¿‡æ—¶é—´æ£€éªŒçš„æœ‰æ•ˆæ‰‹æ®µï¼Œç„¶è€Œç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸç¼ºä¹ä¸€ä¸ªå¤§å‹ç»“æ„åŒ–è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“èƒ½å¤Ÿå°†è¿è´¯çš„å™äº‹ä¸æ˜ç¡®çš„é“å¾·æ•™è®­ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡TF1-EN-3Mæ•°æ®é›†å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼ŒTF1-EN-3Mæ˜¯é¦–ä¸ªå¼€æ”¾çš„ã€ç”±æŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹ç”Ÿæˆçš„ä¸‰ç™¾ä¸‡è‹±è¯­å¯“è¨€æ•°æ®é›†ï¼Œè¿™äº›æ¨¡å‹çš„å‚æ•°è§„æ¨¡å‡ä¸è¶…è¿‡8Bã€‚æ¯ä¸ªæ•…äº‹éƒ½éµå¾ªå…­ä¸ªæ§½ä½ï¼ˆè§’è‰²-&gt;ç‰¹å¾-&gt;èƒŒæ™¯-&gt;å†²çª-&gt;è§£å†³-&gt;é“å¾·ï¼‰çš„æ¶æ„ï¼Œé€šè¿‡ç»„åˆæç¤ºå¼•æ“ç”Ÿæˆï¼Œæ—¢ä¿è¯äº†æ–‡ä½“å¿ å®åº¦åˆè¦†ç›–äº†å¹¿æ³›çš„ä¸»é¢˜ç©ºé—´ã€‚æ··åˆè¯„ä¼°æµç¨‹ç»“åˆäº†ï¼ˆiï¼‰åŸºäºGPTçš„è¯„è®ºå®¶å¯¹è¯­æ³•ã€åˆ›é€ åŠ›ã€é“å¾·æ¸…æ™°åº¦å’Œæ¨¡æ¿éµå¾ªåº¦çš„è¯„åˆ†ï¼Œä»¥åŠï¼ˆiiï¼‰æ— å‚è€ƒçš„å¤šæ ·æ€§å’Œå¯è¯»æ€§æŒ‡æ ‡ã€‚åœ¨åä¸ªå…¬å¼€æƒé‡å€™é€‰æ¨¡å‹ä¸­ï¼Œä¸€ä¸ªè§„æ¨¡ä¸º8Bå‚æ•°çš„Llama-3å˜ä½“æä¾›äº†æœ€ä½³çš„æ€§ä»·æ¯”ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªæ¶ˆè´¹è€…GPUä¸Šï¼ˆ&lt;24GBVRAMï¼‰ç”Ÿäº§é«˜å¾—åˆ†çš„å¯“è¨€æ•…äº‹ï¼Œæ¯ç”Ÿäº§ä¸€åƒä¸ªå¯“è¨€æ•…äº‹çš„æˆæœ¬çº¦ä¸º13.5ç¾åˆ†ã€‚æˆ‘ä»¬æ ¹æ®è®¸å¯åè®®å…¬å¼€å‘å¸ƒæ•°æ®é›†ã€ç”Ÿæˆä»£ç ã€è¯„ä¼°è„šæœ¬å’Œå®Œæ•´å…ƒæ•°æ®ï¼Œä»¥å®ç°ç²¾ç¡®çš„å¯é‡å¤æ€§å’Œæˆæœ¬åŸºå‡†æµ‹è¯•ã€‚TF1-EN-3Mä¸ºç ”ç©¶æŒ‡ä»¤éµå¾ªã€å™äº‹æ™ºèƒ½ã€ä»·å€¼å¯¹é½ä»¥åŠå„¿ç«¥å‹å¥½å‹æ•™è‚²äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸå¼€è¾Ÿäº†é“è·¯ï¼Œè¯æ˜å¤§è§„æ¨¡é“å¾·æ•…äº‹å™è¿°ä¸å†éœ€è¦ä¸“æœ‰å¤§å‹æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20605v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬ç ”ç©¶åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«ä¸‰ç™¾ä¸‡è‹±è¯­å¯“è¨€çš„å¼€æ”¾æ•°æ®é›†TF1-EN-3Mï¼Œè¿™äº›å¯“è¨€ç”±æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ç”Ÿæˆï¼Œæ¯ä¸ªæ•…äº‹éµå¾ªå…­ä¸ªæ§½ä½æ¶æ„ï¼ˆè§’è‰²-&gt;ç‰¹è´¨-&gt;åœºæ™¯-&gt;å†²çª-&gt;è§£å†³-&gt;é“å¾·ï¼‰ï¼Œè¦†ç›–å¹¿æ³›çš„ä¸»é¢˜ç©ºé—´ã€‚è¯¥æ•°æ®é›†å¡«è¡¥äº†ç°ä»£NLPç¼ºä¹å¤§å‹ç»“æ„åŒ–è¯­æ–™åº“çš„ç©ºç™½ï¼Œèƒ½å¤Ÿä¼ è¾“ä»·å€¼è§‚ã€‚ç ”ç©¶å‘å¸ƒæ•°æ®é›†ã€ç”Ÿæˆä»£ç ã€è¯„ä¼°è„šæœ¬å’Œå®Œæ•´å…ƒæ•°æ®ï¼Œå¹¶è¯æ˜äº†å¤§è§„æ¨¡è®²è¿°é“å¾·æ•…äº‹ä¸å†éœ€è¦ä¸“æœ‰å¤§å‹æ¨¡å‹ï¼Œä¸ºæŒ‡ä»¤éµå¾ªã€å™äº‹æ™ºèƒ½ã€ä»·å€¼å¯¹é½å’Œå„¿ç«¥å‹å¥½æ•™è‚²äººå·¥æ™ºèƒ½çš„ç ”ç©¶å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ›å»ºäº†åŒ…å«ä¸‰ç™¾ä¸‡è‹±è¯­å¯“è¨€çš„å¼€æ”¾æ•°æ®é›†TF1-EN-3Mã€‚</li>
<li>æ•°æ®é›†ä¸­çš„æ¯ä¸ªå¯“è¨€éƒ½éµå¾ªå…­ä¸ªæ§½ä½æ¶æ„ï¼Œå…·æœ‰æ˜ç¡®çš„é“å¾·å¯“æ„ã€‚</li>
<li>è¯¥æ•°æ®é›†é€šè¿‡æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ç”Ÿæˆï¼Œä¿è¯æ•…äº‹ä½“è£çš„å¿ å®æ€§å¹¶è¦†ç›–å¹¿æ³›çš„ä¸»é¢˜ç©ºé—´ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ··åˆè¯„ä¼°ç®¡é“ï¼ŒåŒ…æ‹¬åŸºäºGPTçš„è¯„åˆ†å™¨å’Œæ— å‚è€ƒçš„å¤šæ ·æ€§å’Œå¯è¯»æ€§æŒ‡æ ‡ã€‚</li>
<li>8Bå‚æ•°çš„Llama-3å˜ä½“åœ¨è´¨é‡é€Ÿåº¦æƒè¡¡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œèƒ½åœ¨å•ä¸ªæ¶ˆè´¹è€…GPUä¸Šä»¥çº¦13.5ç¾åˆ†çš„æˆæœ¬ç”Ÿäº§é«˜åˆ†å¯“è¨€ã€‚</li>
<li>ç ”ç©¶å‘å¸ƒäº†æ•°æ®é›†ã€ç”Ÿæˆä»£ç ã€è¯„ä¼°è„šæœ¬å’Œå®Œæ•´å…ƒæ•°æ®ï¼Œä»¥å®ç°ç²¾ç¡®çš„å¯é‡å¤æ€§å’Œæˆæœ¬åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6d70db756d81fa9e8c65b3f2bdc2b11c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c057b06ab3da47cd729383127531779c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Video-Bench-Human-Aligned-Video-Generation-Benchmark"><a href="#Video-Bench-Human-Aligned-Video-Generation-Benchmark" class="headerlink" title="Video-Bench: Human-Aligned Video Generation Benchmark"></a>Video-Bench: Human-Aligned Video Generation Benchmark</h2><p><strong>Authors:Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni</strong></p>
<p>Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our frameworkâ€™s assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆè¯„ä¼°å¯¹äºç¡®ä¿ç”Ÿæˆæ¨¡å‹äº§ç”Ÿè§†è§‰çœŸå®ã€é«˜è´¨é‡çš„è§†é¢‘ï¼ŒåŒæ—¶ç¬¦åˆäººç±»æœŸæœ›è‡³å…³é‡è¦ã€‚å½“å‰è§†é¢‘ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼šä¼ ç»ŸåŸºå‡†æµ‹è¯•ä½¿ç”¨æŒ‡æ ‡å’ŒåµŒå…¥æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘è´¨é‡çš„å¤šç»´æ–¹é¢ï¼Œä½†é€šå¸¸ä¸äººç±»åˆ¤æ–­ç¼ºä¹ä¸€è‡´æ€§ï¼›åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•è™½ç„¶å…·å¤‡äººç±»èˆ¬çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹è§†é¢‘è´¨é‡æŒ‡æ ‡çš„æœ‰é™ç†è§£å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨çº¦æŸã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶å»ºç«‹æ›´ç¬¦åˆäººç±»åå¥½çš„åŸºå‡†æµ‹è¯•ï¼Œæœ¬æ–‡ä»‹ç»äº†Video-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œå…·æœ‰ä¸°å¯Œçš„æç¤ºå¥—ä»¶å’Œå¹¿æ³›çš„è¯„ä¼°ç»´åº¦ã€‚è¯¥åŸºå‡†æµ‹è¯•æ˜¯é¦–æ¬¡å°è¯•åœ¨ç”Ÿæˆæ¨¡å‹ä¸­æ¶‰åŠè§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ‰€æœ‰ç›¸å…³ç»´åº¦ä¸Šç³»ç»Ÿåœ°åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚é€šè¿‡é‡‡ç”¨å°‘é‡è¯„åˆ†å’ŒæŸ¥è¯¢é“¾æŠ€æœ¯ï¼ŒVideo-Benchä¸ºç”Ÿæˆçš„è§†é¢‘è¯„ä¼°æä¾›äº†ä¸€ç§ç»“æ„åŒ–ã€å¯æ‰©å±•çš„æ–¹æ³•ã€‚åœ¨åŒ…æ‹¬Soraåœ¨å†…çš„å…ˆè¿›æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVideo-Benchåœ¨æ‰€æœ‰ç»´åº¦ä¸Šå®ç°äº†ä¸äººç±»åå¥½æ›´ä¼˜è¶Šçš„å¯¹é½ã€‚æ­¤å¤–ï¼Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶è¯„ä¼°ä¸äººç±»è¯„ä¼°æ„è§åˆ†æ­§çš„æƒ…å†µä¸‹ï¼Œå®ƒå§‹ç»ˆæä¾›æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ï¼Œè¿™è¡¨æ˜å®ƒå¯èƒ½æ¯”ä¼ ç»Ÿçš„äººç±»åˆ¤æ–­å…·æœ‰æ›´å¤§çš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04907v2">PDF</a> Accepted by CVPRâ€™25</p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£è§†é¢‘ç”Ÿæˆè¯„ä¼°ç³»ç»Ÿâ€”â€”Video-Benchçš„å¼•å…¥è§£å†³äº†ä¼ ç»Ÿè§†é¢‘ç”Ÿæˆè¯„ä¼°æ ‡å‡†çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬ä¸äººå·¥è¯„ä»·çš„ä¸åŒ¹é…ä»¥åŠå¯¹è§†é¢‘è´¨é‡æŒ‡æ ‡å’Œè·¨æ¨¡æ€ä¸€è‡´æ€§çš„æœ‰é™ç†è§£ã€‚Video-Benché€šè¿‡ä¸°å¯Œçš„æç¤ºå¥—ä»¶å’Œå¹¿æ³›çš„è¯„ä¼°ç»´åº¦ï¼Œç³»ç»Ÿåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè§†é¢‘ç”Ÿæˆè¯„ä¼°ï¼Œä»¥å®ç°å¯¹è§†é¢‘è´¨é‡çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶å®ç°ä¸äººç±»åå¥½çš„æ›´é«˜å¯¹é½ã€‚é‡‡ç”¨å°‘é‡è¯„åˆ†å’ŒæŸ¥è¯¢é“¾æŠ€æœ¯ä½¿Video-Benchç»“æ„åŒ–ï¼Œæ›´æ˜“äºç”Ÿæˆè§†é¢‘è¯„ä»·ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚æ¨¡å‹ä¸Šè¿›è¡Œå®éªŒï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„è¯„ä¼°æ€§èƒ½ã€‚åŒæ—¶ï¼Œå½“è¯„ä¼°æ¡†æ¶ä¸äººå·¥è¯„ä»·å­˜åœ¨åˆ†æ­§æ—¶ï¼Œå®ƒæä¾›äº†æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-Benchæ˜¯æ–°ä¸€ä»£è§†é¢‘ç”Ÿæˆè¯„ä¼°ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè¯„ä¼°æ ‡å‡†çš„å±€é™æ€§ã€‚</li>
<li>Video-Benché‡‡ç”¨ä¸°å¯Œçš„æç¤ºå¥—ä»¶å’Œå¹¿æ³›çš„è¯„ä¼°ç»´åº¦è¿›è¡Œè§†é¢‘ç”Ÿæˆè¯„ä¼°ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒVideo-Benchå¯ä»¥æ›´å¥½åœ°ä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>Video-Benché€šè¿‡å°‘é‡è¯„åˆ†å’ŒæŸ¥è¯¢é“¾æŠ€æœ¯æé«˜äº†è§†é¢‘ç”Ÿæˆè¯„ä¼°çš„ç»“æ„æ€§å’Œå¯è¯„ä¼°æ€§ã€‚</li>
<li>å®éªŒè¯æ˜Video-Benchåœ¨å¤æ‚æ¨¡å‹ä¸Šçš„è¯„ä¼°æ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>Video-Benchåœ¨è¯„ä¼°æ¡†æ¶ä¸äººå·¥è¯„ä»·å­˜åœ¨åˆ†æ­§æ—¶ï¼Œæä¾›äº†æ›´å®¢è§‚å’Œå‡†ç¡®çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9d146286134ed8dda134483b4d894d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e300daea1caa72404c324611dd0a5c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f277eb502f125db24353b45ec8ab5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa4297b55e458a34d2c5858076f2106f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LocAgent-Graph-Guided-LLM-Agents-for-Code-Localization"><a href="#LocAgent-Graph-Guided-LLM-Agents-for-Code-Localization" class="headerlink" title="LocAgent: Graph-Guided LLM Agents for Code Localization"></a>LocAgent: Graph-Guided LLM Agents for Code Localization</h2><p><strong>Authors:Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang</strong></p>
<p>Code localizationâ€“identifying precisely where in a codebase changes need to be madeâ€“is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/LocAgent">https://github.com/gersteinlab/LocAgent</a>. </p>
<blockquote>
<p>ä»£ç å®šä½â€”â€”ç²¾ç¡®ç¡®å®šéœ€è¦åœ¨ä»£ç åº“ä¸­åšå‡ºæ›´æ”¹çš„ä½ç½®â€”â€”æ˜¯è½¯ä»¶ç»´æŠ¤ä¸­çš„ä¸€é¡¹åŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•åœ¨è¯†åˆ«ç›¸å…³ä»£ç æ®µæ—¶å¾ˆéš¾æœ‰æ•ˆéå†å¤æ‚çš„ä»£ç åº“ã€‚æŒ‘æˆ˜åœ¨äºå°†è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ä¸é€‚å½“çš„ä»£ç å…ƒç´ è¿›è¡Œæ¡¥æ¢è¿æ¥ï¼Œé€šå¸¸éœ€è¦è·¨è¶Šå±‚æ¬¡ç»“æ„å’Œå¤šä¸ªä¾èµ–å…³ç³»è¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†LocAgentï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å›¾è¡¨ç¤ºæ¥è§£å†³ä»£ç å®šä½é—®é¢˜çš„æ¡†æ¶ã€‚é€šè¿‡å°†ä»£ç åº“è§£æä¸ºå®šå‘å¼‚è´¨å›¾ï¼ŒLocAgentåˆ›å»ºäº†ä¸€ç§è½»é‡çº§çš„è¡¨ç¤ºå½¢å¼ï¼Œèƒ½å¤Ÿæ•è·ä»£ç ç»“æ„ï¼ˆæ–‡ä»¶ã€ç±»ã€å‡½æ•°ï¼‰åŠå…¶ä¾èµ–å…³ç³»ï¼ˆå¯¼å…¥ã€è°ƒç”¨ã€ç»§æ‰¿ï¼‰ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†èƒ½å¤Ÿé€šè¿‡å¼ºå¤§çš„å¤šè·³æ¨ç†æœ‰æ•ˆåœ°æœç´¢å’Œå®šä½ç›¸å…³å®ä½“ã€‚åœ¨ç°å®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»£ç å®šä½çš„å‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨å¾®è°ƒåçš„Qwen-2.5-Coder-Instruct-32Bæ¨¡å‹ï¼Œä»¥å¤§å¹…é™ä½çš„æˆæœ¬ï¼ˆçº¦86%ï¼‰å–å¾—äº†ä¸æœ€å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œæ–‡ä»¶çº§å®šä½å‡†ç¡®æ€§é«˜è¾¾92.7%ï¼Œå¹¶åœ¨å¤šæ¬¡å°è¯•ä¸‹å°†ä¸‹æ¸¸GitHubé—®é¢˜è§£å†³æˆåŠŸç‡æé«˜äº†12%ï¼ˆPass@10ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gersteinlab/LocAgent%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gersteinlab/LocAgentè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09089v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä»£ç å®šä½æ˜¯è½¯ä»¶ç»´æŠ¤ä¸­çš„åŸºæœ¬ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç²¾ç¡®è¯†åˆ«éœ€è¦åœ¨ä»£ç åº“ä¸­è¿›è¡Œæ›´æ”¹çš„ä½ç½®ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„ä»£ç åº“æ—¶éš¾ä»¥æœ‰æ•ˆåœ°å¯¼èˆªå¹¶è¯†åˆ«ç›¸å…³çš„ä»£ç æ®µã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡åŸºäºå›¾çš„è¡¨ç¤ºæ¥è§£å†³ä»£ç å®šä½é—®é¢˜çš„LocAgentæ¡†æ¶ã€‚å®ƒé€šè¿‡å°†æœ‰å‘å¼‚æ„å›¾è§£æä¸ºä»£ç åº“ï¼Œåˆ›å»ºäº†è½»é‡çº§çš„è¡¨ç¤ºï¼Œæ•è·äº†ä»£ç ç»“æ„ï¼ˆæ–‡ä»¶ã€ç±»ã€å‡½æ•°ï¼‰åŠå…¶ä¾èµ–å…³ç³»ï¼ˆå¯¼å…¥ã€è°ƒç”¨ã€ç»§æ‰¿ï¼‰ï¼Œä½¿å¾—LLMä»£ç†å¯ä»¥æœ‰æ•ˆåœ°è¿›è¡Œå¤šè·³æ¨ç†å¹¶å®šä½ç›¸å…³å®ä½“ã€‚åœ¨çœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜ä»£ç å®šä½çš„å‡†ç¡®æ€§ã€‚å°¤å…¶æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç²¾ç»†è°ƒæ•´çš„Qwen-2.5-Coder-Instruct-32Bæ¨¡å‹ç›¸ç»“åˆï¼Œåœ¨å¤§å¤§é™ä½æˆæœ¬ï¼ˆçº¦é™ä½86%ï¼‰çš„æƒ…å†µä¸‹å®ç°äº†ä¸æœ€å…ˆè¿›ä¸“æœ‰æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œæ–‡ä»¶çº§å®šä½å‡†ç¡®æ€§è¾¾åˆ°92.7%ï¼ŒåŒæ—¶æé«˜ä¸‹æ¸¸GitHubé—®é¢˜è§£å†³çš„æˆåŠŸç‡è¾¾åˆ°å¤šæ¬¡å°è¯•çš„12%ï¼ˆPass@10ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç å®šä½æ˜¯è½¯ä»¶ç»´æŠ¤ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œéœ€è¦ç²¾ç¡®è¯†åˆ«æ›´æ”¹ä½ç½®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ä»£ç åº“ä¸­çš„å¯¼èˆªå’Œè¯†åˆ«ç›¸å…³ä»£ç æ®µæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>LocAgentæ¡†æ¶é€šè¿‡åŸºäºå›¾çš„è¡¨ç¤ºè§£å†³äº†ä»£ç å®šä½é—®é¢˜ã€‚</li>
<li>LocAgentå°†ä»£ç åº“è§£æä¸ºæœ‰å‘å¼‚æ„å›¾ï¼Œæ•è·ä»£ç ç»“æ„åŠå…¶ä¾èµ–å…³ç³»ã€‚</li>
<li>LLMä»£ç†é€šè¿‡LocAgentæ¡†æ¶è¿›è¡Œå¤šè·³æ¨ç†ä»¥å®šä½ç›¸å…³å®ä½“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLocAgentæ˜¾è‘—æé«˜äº†ä»£ç å®šä½çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼ŒLocAgentåœ¨é™ä½æˆæœ¬çš„åŒæ—¶å®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-48e5c6e09fe8e239da7a30f5083eaac2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-891c368cf18d1ae5a743aadccb8e4b5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7465383cc5e8f9d223d2b4d861ffbe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c95779e38dc998fde48e525acba8df1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Training-Plug-n-Play-Knowledge-Modules-with-Deep-Context-Distillation"><a href="#Training-Plug-n-Play-Knowledge-Modules-with-Deep-Context-Distillation" class="headerlink" title="Training Plug-n-Play Knowledge Modules with Deep Context Distillation"></a>Training Plug-n-Play Knowledge Modules with Deep Context Distillation</h2><p><strong>Authors:Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan VuliÄ‡, Alessandro Sordoni</strong></p>
<p>Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG. </p>
<blockquote>
<p>åœ¨ï¼ˆå¤§å‹ï¼‰è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒåï¼ŒåŠ¨æ€æ•´åˆæ–°ä¿¡æ¯æˆ–å¿«é€Ÿæ¼”è¿›çš„ä¿¡æ¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯æˆ–å¤„ç†ç§æœ‰å’Œç‰¹æ®Šæ–‡æ¡£æ—¶ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢ä¸´åŒ…æ‹¬é«˜æ¨ç†æˆæœ¬å’Œæ— æ³•æ•è·å…¨å±€æ–‡æ¡£ä¿¡æ¯åœ¨å†…çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è®­ç»ƒæ–‡æ¡£çº§çŸ¥è¯†æ¨¡å—ï¼ˆKMï¼‰æ¥æ¨¡å—åŒ–çŸ¥è¯†çš„æ–¹æ³•ã€‚çŸ¥è¯†æ¨¡å—è¢«å®ç°ä¸ºå‚æ•°é«˜æ•ˆçš„LoRAæ¨¡å—ï¼Œæ˜“äºæŒ‰éœ€æ’å…¥æ¨¡å‹ï¼Œå¹¶è®­ç»ƒä»¥å­˜å‚¨æ–°æ–‡æ¡£çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½œä¸ºçŸ¥è¯†æ¨¡å—çš„è®­ç»ƒç›®æ ‡ï¼Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è¡¨ç°ä¸ä½³ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦ï¼šæˆ‘ä»¬å­¦ä¹ çŸ¥è¯†æ¨¡å—çš„å‚æ•°ï¼Œä»¥æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æ¡£éšè—çŠ¶æ€å’Œé€»è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºæ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œé¢„å…ˆæŒ‡ä»¤è®­ç»ƒæŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†çŸ¥è¯†æ¨¡å—å’ŒRAGä¹‹é—´çš„ååŒä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08727v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è®­ç»ƒæ–‡æ¡£çº§åˆ«çš„çŸ¥è¯†æ¨¡å—ï¼ˆKMsï¼‰æ¥æ¨¡å—åŒ–çŸ¥è¯†çš„æ–¹æ³•ã€‚KMsä½œä¸ºå‚æ•°é«˜æ•ˆçš„LoRAæ¨¡å—å®ç°ï¼Œèƒ½å¤Ÿå­˜å‚¨æ–°æ–‡æ¡£çš„ä¿¡æ¯ï¼Œå¹¶å¯æ ¹æ®éœ€æ±‚è½»æ¾æ’å…¥æ¨¡å‹ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä½¿ç”¨ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ä½œä¸ºè®­ç»ƒç›®æ ‡å¯¹äºKMsæ•ˆæœä¸ä½³ï¼Œå› æ­¤æå‡ºäº†æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å­¦ä¹ KMçš„å‚æ•°ï¼Œä»¥æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„éšè—çŠ¶æ€å’Œé€»è¾‘ã€‚æ­¤æ–¹æ³•åœ¨ä¸¤é¡¹æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºæ ‡å‡†ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œé¢„æŒ‡ä»¤è®­ç»ƒæŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼ºè°ƒäº†çŸ¥è¯†æ¨¡å—ä¸RAGä¹‹é—´çš„ååŒä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä½æ•°æ®åœºæ™¯æˆ–å¤„ç†ç§æœ‰å’Œç‰¹æ®Šæ–‡æ¡£æ—¶ï¼ŒåŠ¨æ€é›†æˆæ–°ä¿¡æ¯æˆ–å¿«é€Ÿæ¼”åŒ–çš„ä¿¡æ¯å¯¹äºè¯­è¨€æ¨¡å‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å¦‚ä¸Šä¸‹æ–‡å­¦ä¹ å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å­˜åœ¨é«˜æ¨ç†æˆæœ¬å’Œæ— æ³•æ•è·å…¨å±€æ–‡æ¡£ä¿¡æ¯çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†çŸ¥è¯†æ¨¡å—ï¼ˆKMsï¼‰çš„æ¦‚å¿µï¼Œä½œä¸ºå‚æ•°é«˜æ•ˆçš„æ¨¡å—å®ç°ï¼Œèƒ½å¤Ÿå­˜å‚¨æ–°æ–‡æ¡£çš„ä¿¡æ¯ï¼Œå¹¶å¯ä»¥æŒ‰éœ€è½»æ¾æ’å…¥æ¨¡å‹ã€‚</li>
<li>æŒ‡å‡ºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ä½œä¸ºKMsçš„è®­ç»ƒç›®æ ‡æ•ˆæœä¸ä½³ï¼Œæå‡ºäº†æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦çš„æ–¹æ³•ã€‚</li>
<li>æ·±åº¦ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•æ¨¡æ‹Ÿäº†æ•™å¸ˆæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­çš„éšè—çŠ¶æ€å’Œé€»è¾‘ï¼Œè¡¨ç°ä¼˜äºæ ‡å‡†ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œé¢„æŒ‡ä»¤è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>çŸ¥è¯†æ¨¡å—ï¼ˆKMsï¼‰ä¸RAGä¹‹é—´å­˜åœ¨ååŒä½œç”¨ï¼Œå¯ä»¥äº’ç›¸è¡¥å……ä¼˜ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14503521455d8304b1cebe4486916122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28f1aa5d00f06e937e561164f70423b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10b1fedcfb47293217a94a2cca7a3cb3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Wanda-Pruning-Large-Language-Models-via-Regional-Gradients"><a href="#Wanda-Pruning-Large-Language-Models-via-Regional-Gradients" class="headerlink" title="Wanda++: Pruning Large Language Models via Regional Gradients"></a>Wanda++: Pruning Large Language Models via Regional Gradients</h2><p><strong>Authors:Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus MÃ¼ller, Jonas M. KÃ¼bler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack FitzGerald, Abhishek Kumar</strong></p>
<p>Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‰ªææ—¨åœ¨å»é™¤æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸é‡è¦æƒé‡ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å°½é‡å‡å°‘æ€§èƒ½å½±å“ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸åœ¨æ²¡æœ‰å…¨æ¨¡å‹ç¨€ç–æ„ŸçŸ¥å¾®è°ƒçš„æƒ…å†µä¸‹å‡ºç°æ€§èƒ½æŸå¤±ã€‚æœ¬æ–‡ä»‹ç»äº†Wanda++ï¼Œä¸€ç§åˆ©ç”¨è§£ç å™¨å—çº§åŒºåŸŸæ¢¯åº¦çš„æ–°å‹å‰ªææ¡†æ¶ï¼Œå®ƒè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒWanda++é¦–æ¬¡åˆ©ç”¨åŒºåŸŸæ¢¯åº¦æ”¹è¿›å‰ªæè¯„åˆ†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åŒºåŸŸä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æœ€å°åŒ–å¯†é›†å’Œç¨€ç–è§£ç å™¨è¾“å‡ºä¹‹é—´ç”±å‰ªæå¼•èµ·çš„è¾“å‡ºå·®å¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ï¼ŒWanda++ç›¸å¯¹äºWandaçš„å›°æƒ‘åº¦æé«˜äº†é«˜è¾¾32%ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°æ¨å¹¿åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚è¿›ä¸€æ­¥çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ç¨€ç–æ„ŸçŸ¥å¾®è°ƒæ­£äº¤ï¼ŒWanda++å¯ä»¥ä¸LoRAå¾®è°ƒç›¸ç»“åˆï¼Œå®ç°ä¸Wandaæ–¹æ³•ç±»ä¼¼çš„å›°æƒ‘åº¦æ”¹è¿›ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¾ˆè½»ä¾¿ï¼Œåœ¨å•ä¸ªNVIDIA H100 GPUä¸Šï¼Œå¯¹7B LLaMAæ¨¡å‹è¿›è¡Œå‰ªæåªéœ€ä¸åˆ°10åˆ†é’Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04992v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‰ªææ—¨åœ¨å»é™¤ä¸é‡è¦æƒé‡ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶å°½é‡å‡å°‘æ€§èƒ½å½±å“ã€‚æœ¬æ–‡æå‡ºäº†Wanda++è¿™ä¸€æ–°çš„å‰ªææ¡†æ¶ï¼Œåˆ©ç”¨è§£ç å™¨å—çº§çš„åŒºåŸŸæ¢¯åº¦æ¥æå‡å‰ªææ•ˆæœï¼Œå¹¶æå‡ºé«˜æ•ˆåŒºåŸŸä¼˜åŒ–æ–¹æ³•ä»¥æœ€å°åŒ–å¯†é›†å’Œç¨€ç–è§£ç å™¨è¾“å‡ºä¹‹é—´çš„å‰ªæå¼•èµ·çš„å·®å¼‚ã€‚Wanda++åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ç›¸è¾ƒäºWandaé™ä½äº†å›°æƒ‘åº¦è¾¾32%ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°æ³›åŒ–åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•ä¸ç¨€ç–æ„ŸçŸ¥å¾®è°ƒæ­£äº¤ï¼Œå¯ç»“åˆLoRAå¾®è°ƒå®ç°ä¸Wandaæ–¹æ³•ç›¸ä¼¼çš„å›°æƒ‘åº¦æ”¹è¿›ã€‚è¯¥æ–¹æ³•æ˜¯è½»é‡çº§çš„ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA H100 GPUä¸Šåœ¨ä¸åˆ°10åˆ†é’Ÿå†…å¯¹7B LLaMAæ¨¡å‹è¿›è¡Œå‰ªæã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå‰ªææ—¨åœ¨æé«˜æ¨ç†é€Ÿåº¦å¹¶å‡å°‘æ€§èƒ½æŸå¤±ã€‚</li>
<li>Wanda++æ˜¯å…¨æ–°çš„å‰ªææ¡†æ¶ï¼Œåˆ©ç”¨è§£ç å™¨å—çº§çš„åŒºåŸŸæ¢¯åº¦æ¥æå‡å‰ªææ•ˆæœã€‚</li>
<li>Wanda++é€šè¿‡é«˜æ•ˆåŒºåŸŸä¼˜åŒ–æ–¹æ³•æœ€å°åŒ–å¯†é›†å’Œç¨€ç–è§£ç å™¨è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>Wanda++åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ç›¸è¾ƒäºå‰ä»£æ–¹æ³•é™ä½äº†å›°æƒ‘åº¦è¾¾32%ã€‚</li>
<li>Wanda++çš„æ–¹æ³•å¯ä»¥æ³›åŒ–åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ç¨€ç–æ„ŸçŸ¥å¾®è°ƒæ­£äº¤ï¼Œå¯ç»“åˆå…¶ä»–æ–¹æ³•å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-481d81eae373dab41cfef3f058661c10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d3c35d49724a9232d4f5aae5c6a2f0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19824102c393f3e49b34f152e3006954.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be9c0dd5acbf850de105c9605220724d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a36d16b07434f03cd990c223c2d6e9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db7dbc192ca17f41f209b5c122dcb217.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="An-LLM-Powered-Agent-for-Physiological-Data-Analysis-A-Case-Study-on-PPG-based-Heart-Rate-Estimation"><a href="#An-LLM-Powered-Agent-for-Physiological-Data-Analysis-A-Case-Study-on-PPG-based-Heart-Rate-Estimation" class="headerlink" title="An LLM-Powered Agent for Physiological Data Analysis: A Case Study on   PPG-based Heart Rate Estimation"></a>An LLM-Powered Agent for Physiological Data Analysis: A Case Study on   PPG-based Heart Rate Estimation</h2><p><strong>Authors:Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani</strong></p>
<p>Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMsâ€™ limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAIâ€™s GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agentâ€™s performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡äº¤äº’å¼é€šä¿¡æ”¹å–„äº†è¯Šæ–­ã€æ‚£è€…æŠ¤ç†å’Œå†³ç­–æ”¯æŒï¼Œæ­£åœ¨å½»åº•æ”¹å˜åŒ»ç–—ä¿å¥é¢†åŸŸã€‚æœ€è¿‘ï¼Œå®ƒä»¬è¢«åº”ç”¨äºåˆ†æç”Ÿç†æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¦‚å¯ç©¿æˆ´è®¾å¤‡æ•°æ®ä¸­çš„å¥åº·æ´å¯Ÿæå–ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥å°†åŸå§‹æ•°å­—åºåˆ—åµŒå…¥æç¤ºä¸­ï¼Œè¿™è¶…å‡ºäº†ä»¤ç‰Œé™åˆ¶å¹¶å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸€äº›ç ”ç©¶å°†æ—¶é—´åºåˆ—ä¸­æå–çš„ç‰¹å¾é›†æˆåˆ°æ–‡æœ¬æç¤ºä¸­ï¼Œæˆ–é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”±äºLLMåœ¨è¿ç»­æ³¢å½¢åˆ†æå’Œè§£é‡Šæ–¹é¢çš„åˆ†æä¸¥è°¨æ€§å’Œæ•ˆç‡æœ‰é™ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€äº§ç”Ÿé€šç”¨ä¸”ä¸å¯é çš„è¾“å‡ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç”¨äºç”Ÿç†æ—¶é—´åºåˆ—åˆ†æçš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨ä»£ç†ï¼Œæ—¨åœ¨å¼¥åˆå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ—¢å®šåˆ†æå·¥å…·ç»“åˆèµ·æ¥çš„å·®è·ã€‚æˆ‘ä»¬çš„ä»£ç†å»ºç«‹åœ¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨æ¡†æ¶OpenCHAä¸Šï¼Œç”±OpenAIçš„GPT-3.5 Turboæ¨¡å‹æä¾›åŠ¨åŠ›ï¼Œå…·æœ‰ä¸€ä¸ªåè°ƒå™¨ï¼Œå¯ä»¥é›†æˆç”¨æˆ·äº¤äº’ã€æ•°æ®æºå’Œåˆ†æå·¥å…·ä»¥ç”Ÿæˆå‡†ç¡®çš„å¥åº·è§è§£ã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨è¿œç¨‹å¥åº·ç›‘æµ‹ç ”ç©¶çš„æ•°æ®é›†ä¸­å®æ–½äº†ä¸€é¡¹å…³äºä»å…‰ä½“ç§¯æè®°ä»ªï¼ˆPPGï¼‰ä¿¡å·ä¼°è®¡å¿ƒç‡ï¼ˆHRï¼‰çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æ•°æ®é›†åŒ…å«PPGå’Œå¿ƒç”µå›¾ï¼ˆECGï¼‰è®°å½•ã€‚è¯¥ä»£ç†çš„æ€§èƒ½ä»¥OpenAI GPT-4o Miniå’ŒGPT-4oä¸ºåŸºå‡†è¿›è¡Œæµ‹è¯•ï¼Œå¿ƒç”µå›¾ä½œä¸ºå¿ƒç‡ä¼°è®¡çš„é‡‘æ ‡å‡†ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå®ç°äº†æ›´ä½çš„é”™è¯¯ç‡å’Œæ›´å¯é çš„å¿ƒç‡ä¼°è®¡ã€‚è¯¥ä»£ç†çš„å®ç°å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12836v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ­£åœ¨å¸¦æ¥å˜é©ï¼Œé€šè¿‡äº¤äº’å¼é€šä¿¡æ”¹è¿›è¯Šæ–­ã€æ‚£è€…æŠ¤ç†å’Œå†³ç­–æ”¯æŒã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ç§åŸºäºLLMçš„ç”Ÿç†æ—¶é—´åºåˆ—åˆ†æä»£ç†ï¼Œæ—¨åœ¨å¼¥åˆLLMä¸ç°æœ‰åˆ†æå·¥å…·ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥ä»£ç†ä½¿ç”¨OpenCHAè¿™ä¸€å¼€æºLLMæ¡†æ¶æ„å»ºï¼Œå¹¶èåˆäº†OpenAIçš„GPT-3.5 Turboæ¨¡å‹ã€‚å…¶å®æ–½äº†ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œåœ¨è¿œç¨‹å¥åº·ç›‘æµ‹ç ”ç©¶ä¸­ä»å…‰ä½“ç§¯è„‰æå›¾ï¼ˆPPGï¼‰ä¿¡å·ä¼°è®¡å¿ƒç‡ï¼ˆHRï¼‰ï¼Œä¸å¿ƒç”µå›¾ï¼ˆECGï¼‰è®°å½•æ•°æ®é›†è¿›è¡Œå¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥ä»£ç†åœ¨å¿ƒç‡ä¼°è®¡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå®ç°äº†æ›´ä½çš„è¯¯å·®ç‡å’Œæ›´å¯é çš„ä¼°è®¡ç»“æœã€‚ä»£ç†çš„å®ç°å·²åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ­£åœ¨åŒ»ç–—é¢†åŸŸå¼•å‘å˜é©ï¼Œé€šè¿‡äº¤äº’å¼é€šä¿¡æ”¹å–„å¤šä¸ªæ–¹é¢ï¼Œå¦‚è¯Šæ–­ã€æ‚£è€…æŠ¤ç†å’Œå†³ç­–æ”¯æŒã€‚</li>
<li>å½“å‰æ–¹æ³•ç›´æ¥å°†åŸå§‹æ•°å­—åºåˆ—åµŒå…¥æç¤ºä¸­ï¼Œå¯¼è‡´ä»¤ç‰Œé™åˆ¶å’Œè®¡ç®—æˆæœ¬å¢åŠ ã€‚</li>
<li>æ•´åˆæ—¶é—´åºåˆ—æ•°æ®çš„ç°æœ‰æ–¹æ³•ç»å¸¸äº§ç”Ÿé€šç”¨å’Œä¸å¯é çš„è¾“å‡ºï¼Œå› ä¸ºLLMsåœ¨è§£æè¿ç»­æ³¢å½¢æ–¹é¢å­˜åœ¨å±€é™æ€§å’Œæ•ˆç‡ä½ä¸‹ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨OpenCHAå¼€æºæ¡†æ¶æ„å»ºäº†ä¸€ä¸ªç”Ÿç†æ—¶é—´åºåˆ—åˆ†æä»£ç†ã€‚</li>
<li>è¯¥ä»£ç†é›†æˆäº†ç”¨æˆ·äº¤äº’ã€æ•°æ®æºå’Œåˆ†æå·¥å…·ï¼Œæ—¨åœ¨ç”Ÿæˆå‡†ç¡®çš„å¥åº·æ´å¯ŸåŠ›ã€‚</li>
<li>æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ä»£ç†åœ¨å¿ƒç‡ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”å®ç°äº†æ›´ä½çš„è¯¯å·®ç‡å’Œæ›´å¯é çš„ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3c8b56ada4a1fd3db10c14498d57255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-225f724544aeb14e81992c72c8fdd18c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4344037c24c267abba21295799bb34ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1876da97ee06ccffc6c8ae4031f60f22.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Semantic-Consistency-for-Assuring-Reliability-of-Large-Language-Models"><a href="#Semantic-Consistency-for-Assuring-Reliability-of-Large-Language-Models" class="headerlink" title="Semantic Consistency for Assuring Reliability of Large Language Models"></a>Semantic Consistency for Assuring Reliability of Large Language Models</h2><p><strong>Authors:Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar</strong></p>
<p>Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„æµç•…æ€§å’Œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒäº†å®ƒä»¬å¯¹è¾“å…¥æç¤ºçš„æ•æ„Ÿæ€§ã€‚ä¸ºäº†ä»¥å®‰å…¨å’Œå¯é çš„æ–¹å¼éƒ¨ç½²LLMï¼Œå½“ä½¿ç”¨è¡¨è¾¾ç›¸åŒæ„ä¹‰æˆ–æ„å›¾çš„æç¤ºæ—¶ï¼Œå®ƒä»¬çš„è¾“å‡ºå¿…é¡»ä¿æŒä¸€è‡´è‡³å…³é‡è¦ã€‚è™½ç„¶ç°æœ‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†æœ€å‰æ²¿çš„LLMå¦‚ä½•è§£å†³æ­¤é—®é¢˜ï¼Œä½†å®ƒä»¬çš„è¯„ä¼°ä»…é™äºè¯„ä¼°å•ä¸€æˆ–å¤šå­—è¯ç­”æ¡ˆçš„è¯æ±‡å¹³ç­‰æ€§ï¼Œè€Œå¿½è§†äº†ç”Ÿæˆæ–‡æœ¬åºåˆ—çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†æ›´å…¨é¢ç†è§£LLMåœ¨å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­ä¹‰ä¸€è‡´æ€§çš„é€šç”¨åº¦é‡æ ‡å‡†ï¼Œå¹¶åˆ¶å®šäº†å¤šä¸ªç‰ˆæœ¬çš„è¿™ä¸€æŒ‡æ ‡æ¥è¯„ä¼°å„ç§LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ææ¡ˆå±•ç¤ºäº†ä¸è¾“å‡ºä¸€è‡´æ€§çš„äººç±»è¯„ä¼°ç›¸æ¯”ï¼Œä¸åŸºäºè¯æ±‡ä¸€è‡´æ€§çš„ä¼ ç»Ÿåº¦é‡æ ‡å‡†ç›¸æ¯”ï¼Œæ›´é«˜çš„ä¸€è‡´æ€§å’Œæ›´å¼ºçš„ç›¸å…³æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºAsk-to-Chooseï¼ˆA2Cï¼‰çš„æ–°å‹æç¤ºç­–ç•¥ï¼Œä»¥æé«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨åŸºäºTruthfulQAåŸºå‡†çš„ç­”æ¡ˆå˜åŒ–çš„å°é—­ä¹¦ç±é—®ç­”è¯„ä¼°ä¸­ï¼ŒA2Cå°†é¢„è®­ç»ƒå’Œå¾®è°ƒLLMçš„å‡†ç¡®åº¦æŒ‡æ ‡æé«˜äº†é«˜è¾¾47%ï¼Œå¹¶å°†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§æŒ‡æ ‡æé«˜äº†é«˜è¾¾7å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09138v2">PDF</a> An updated version of this preprint is available at arXiv:2502.15924,   and has been accepted at the Transactions on Machine Learning Research</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºæƒŠäººçš„æµç•…æ€§å’Œèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒäº†å®ƒä»¬å¯¹è¾“å…¥æç¤ºçš„æ•æ„Ÿæ€§ã€‚ä¸ºç¡®ä¿å®‰å…¨ã€å¯é åœ°éƒ¨ç½²LLMï¼Œå…³é”®åœ¨äºå½“ä½¿ç”¨ç›¸åŒå«ä¹‰æˆ–æ„å›¾çš„æç¤ºè¯æ—¶ï¼Œå®ƒä»¬çš„è¾“å‡ºå¿…é¡»ä¿æŒä¸€è‡´ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§è¯­ä¹‰ä¸€è‡´æ€§çš„é€šç”¨åº¦é‡æ ‡å‡†ï¼Œå¹¶åˆ¶å®šäº†å¤šä¸ªç‰ˆæœ¬çš„æŒ‡æ ‡æ¥è¯„ä¼°ä¸åŒLLMçš„æ€§èƒ½ã€‚ä¸ä¼ ç»ŸåŸºäºè¯æ±‡ä¸€è‡´æ€§çš„æŒ‡æ ‡ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æŒ‡æ ‡ä¸äººç±»å¯¹è¾“å‡ºä¸€è‡´æ€§çš„è¯„ä»·å…·æœ‰æ›´é«˜çš„ç›¸å…³æ€§å’Œä¸€è‡´æ€§ã€‚æœ€åï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAsk-to-Chooseï¼ˆA2Cï¼‰çš„æ–°å‹æç¤ºç­–ç•¥ï¼Œä»¥æé«˜è¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨åŸºäºTruthfulQAåŸºå‡†çš„ç­”æ¡ˆå˜åŒ–å°é—­ä¹¦ç±é—®ç­”ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒA2Cå¯æé«˜é¢„è®­ç»ƒå’Œå¾®è°ƒLLMçš„å‡†ç¡®åº¦æŒ‡æ ‡é«˜è¾¾47%ï¼ŒæŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹çš„è¯­ä¹‰ä¸€è‡´æ€§æŒ‡æ ‡æé«˜é«˜è¾¾7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†è¾“å…¥æç¤ºçš„å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´å…¶è¾“å‡ºäº§ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚</li>
<li>ä¸ºç¡®ä¿LLMçš„å®‰å…¨å’Œå¯é éƒ¨ç½²ï¼Œéœ€è¦ç¡®ä¿åœ¨ç›¸åŒæ„å›¾çš„è¾“å…¥æç¤ºä¸‹å…¶è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¯­ä¹‰ä¸€è‡´æ€§åº¦é‡æ ‡å‡†ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°LLMåœ¨å¼€æ”¾æ–‡æœ¬ç”Ÿæˆåœºæ™¯ä¸­çš„ä¸€è‡´æ€§ã€‚</li>
<li>ä¸ä¼ ç»ŸåŸºäºè¯æ±‡ä¸€è‡´æ€§çš„åº¦é‡æ ‡å‡†ç›¸æ¯”ï¼Œæ–°æå‡ºçš„åº¦é‡æ ‡å‡†ä¸äººç±»è¯„ä»·å…·æœ‰æ›´é«˜çš„ç›¸å…³æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æç¤ºç­–ç•¥Ask-to-Chooseï¼ˆA2Cï¼‰ï¼Œä»¥æé«˜LLMçš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å°é—­ä¹¦ç±é—®ç­”ä»»åŠ¡ä¸­ï¼ŒA2Cç­–ç•¥æ˜¾è‘—æé«˜äº†LLMçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6243e0e59fc16e50034b34dddb8d0d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb8e03f5d084821ec17f705c2b1f76e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-321f84c5463741f2381d5ef6708350b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b84143738f5514c2b9702c320096079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0d5aba5896b31b819164688d55b6c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1a9ac8ae9098b9e6bc8b3ea278a125b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c44b1f91c992f9438e166fac9d72c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69c9cee2c15e7c3cf81547067899e719.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9cc9881c687a8d594277af0c2767a519.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  AegisLLM Scaling Agentic Systems for Self-Reflective Defense in LLM   Security
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-aca356da45c1ced05c918a5559a74dcb.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-01  ChestX-Reasoner Advancing Radiology Foundation Models with Reasoning   through Step-by-Step Verification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
