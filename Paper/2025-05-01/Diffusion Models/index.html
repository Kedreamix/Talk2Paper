<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-01  AI-GenBench A New Ongoing Benchmark for AI-Generated Image Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-21950de2b5b2e07066fd3abf8159ca34.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-01-更新"><a href="#2025-05-01-更新" class="headerlink" title="2025-05-01 更新"></a>2025-05-01 更新</h1><h2 id="AI-GenBench-A-New-Ongoing-Benchmark-for-AI-Generated-Image-Detection"><a href="#AI-GenBench-A-New-Ongoing-Benchmark-for-AI-Generated-Image-Detection" class="headerlink" title="AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection"></a>AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</h2><p><strong>Authors:Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli</strong></p>
<p>The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators. </p>
<blockquote>
<p>生成式人工智能的快速发展为图像创作带来了革命性的变化，它能够通过文本提示实现高质量合成，同时给媒体真实性带来了重大挑战。我们推出了Ai-GenBench，这是一个新型基准测试平台，旨在解决现实场景中检测AI生成图像这一迫切需求。不同于现有模型在静态数据集上进行评估的解决方案，Ai-GenBench引入了一个时间评估框架，该框架按生成模型的顺序逐步训练检测模型生成的图像数据点进行测试它们泛化到新型生成模型的能力如何评估如何，比如从生成对抗网络过渡到扩散模型。我们的基准测试平台侧重于高质量、多样化的视觉内容，并克服了当前方法的关键局限性，包括任意分割数据集、不公平比较和过高的计算需求等。Ai-GenBench为研究人员和非专业人士（例如记者、事实核查人员）提供了一个综合数据集、标准化的评估协议和易于使用的工具，既保证了实用性又满足了实际训练要求。通过制定明确的评估规则和可控的增强策略，Ai-GenBench为检测方法的比较和可扩展解决方案提供了有力支持。代码和数据公开可用，以确保可重复性并支持稳健的取证检测器的发展以应对新合成发生器浪潮的冲击。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20865v1">PDF</a> 9 pages, 6 figures, 4 tables, code available:   <a target="_blank" rel="noopener" href="https://github.com/MI-BioLab/AI-GenBench">https://github.com/MI-BioLab/AI-GenBench</a></p>
<p><strong>Summary</strong><br>     生成式AI的快速发展已引发图像创建领域的革新，能够根据文本提示生成高质量图像，同时带来媒体真实性的重大挑战。为应对现实场景中检测AI生成图像的需求，我们推出Ai-GenBench基准测试平台。该平台采用动态评估框架，通过增量训练检测器来检测合成图像，按生成模型的生成顺序排序，以测试其对新生成模型的泛化能力，如从生成对抗网络到扩散模型的过渡。Ai-GenBench专注于高质量、多样化的视觉内容，并克服当前方法的关键局限性，包括任意数据集分割、不公平比较和过高的计算需求。它为研究人员和非专家（如记者、事实核查人员）提供综合数据集、标准化评估协议和可用工具，确保实用性和可重复性。通过制定明确的评估规则和受控增强策略，Ai-GenBench使得检测方法的比较和可扩展解决方案变得更有意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式AI的发展推动了图像创建领域的革新，能够根据文本提示生成高质量图像。</li>
<li>Ai-GenBench是一个用于检测AI生成图像的新基准测试平台。</li>
<li>Ai-GenBench采用动态评估框架，能够测试检测器对新生成模型的泛化能力。</li>
<li>该平台专注于高质量、多样化的视觉内容。</li>
<li>Ai-GenBench克服了当前方法的关键局限性，如任意数据集分割、不公平比较和过高的计算需求。</li>
<li>Ai-GenBench为研究人员和非专家提供综合数据集、标准化评估协议和可用工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20865">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-de1f46db02d6045bed0238e89bd23c3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76d01afa0c1f7ea9d95ed022e36361c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0c3c597ba9d9d43689f3d7779b01c44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ba876161a9a9a455a9fb21fe5e44811.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db194f98bd94c084f68a365b3bf6c8cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceb9d0654a8712c1fbe0c3fa606fcec8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DDPS-Discrete-Diffusion-Posterior-Sampling-for-Paths-in-Layered-Graphs"><a href="#DDPS-Discrete-Diffusion-Posterior-Sampling-for-Paths-in-Layered-Graphs" class="headerlink" title="DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs"></a>DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs</h2><p><strong>Authors:Hao Luan, See-Kiong Ng, Chun Kai Ling</strong></p>
<p>Diffusion models form an important class of generative models today, accounting for much of the state of the art in cutting edge AI research. While numerous extensions beyond image and video generation exist, few of such approaches address the issue of explicit constraints in the samples generated. In this paper, we study the problem of generating paths in a layered graph (a variant of a directed acyclic graph) using discrete diffusion models, while guaranteeing that our generated samples are indeed paths. Our approach utilizes a simple yet effective representation for paths which we call the padded adjacency-list matrix (PALM). In addition, we show how to effectively perform classifier guidance, which helps steer the sampled paths to specific preferred edges without any retraining of the diffusion model. Our preliminary results show that empirically, our method outperforms alternatives which do not explicitly account for path constraints. </p>
<blockquote>
<p>扩散模型是当今生成模型领域中的重要一类，代表了前沿人工智能研究的最新进展。尽管存在许多图像和视频生成之外的扩展，但很少有这样的方法解决生成样本中的显式约束问题。在本文中，我们研究了使用离散扩散模型在分层图中生成路径（一种有向无环图的变体）的问题，同时保证生成的样本确实是路径。我们的方法为路径采用了一种简单有效的表示形式，称为填充邻接列表矩阵（PALM）。此外，我们还展示了如何有效地执行分类器指导，这有助于将采样路径引导到特定的首选边缘，而无需对扩散模型进行任何重新训练。我们的初步结果表明，从实证角度看，我们的方法在解决路径约束方面优于那些没有明确考虑路径约束的替代方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20754v1">PDF</a> To appear at Frontiers in Probabilistic Inference: Sampling meets   Learning (FPI) workshop at ICLR 2025.   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=DBdkU0Ikzy">https://openreview.net/forum?id=DBdkU0Ikzy</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了使用离散扩散模型在分层图中生成路径的问题，同时保证生成的样本确实是路径。为此，提出了一种简单有效的路径表示方法——填充邻接列表矩阵（PALM）。此外，还展示了如何进行分类器引导，这有助于将采样路径导向特定的首选边缘，而无需重新训练扩散模型。初步结果表明，该方法在实证上优于未明确考虑路径约束的替代方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型是当前的生成模型的重要组成部分，处于前沿AI研究的前沿地位。</li>
<li>该论文研究了在分层图中使用离散扩散模型生成路径的问题。</li>
<li>提出了一种新的路径表示方法——填充邻接列表矩阵（PALM）。</li>
<li>通过分类器引导技术，能够在不重新训练扩散模型的情况下，使采样路径导向特定首选边缘。</li>
<li>初步实验表明，该方法在生成受约束的路径样本方面优于其他方法。</li>
<li>该方法在保证路径约束的同时，具有良好的通用性，可应用于其他类似场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20754">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bf8496698f43d9e9b078564df20f7d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b5d23bb09fe17d38033237aa80c06f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8f474b85ef6a12d70290ab33a45dd86.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LDPoly-Latent-Diffusion-for-Polygonal-Road-Outline-Extraction-in-Large-Scale-Topographic-Mapping"><a href="#LDPoly-Latent-Diffusion-for-Polygonal-Road-Outline-Extraction-in-Large-Scale-Topographic-Mapping" class="headerlink" title="LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in   Large-Scale Topographic Mapping"></a>LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in   Large-Scale Topographic Mapping</h2><p><strong>Authors:Weiqin Jiao, Hao Cheng, George Vosselman, Claudio Persello</strong></p>
<p>Polygonal road outline extraction from high-resolution aerial images is an important task in large-scale topographic mapping, where roads are represented as vectorized polygons, capturing essential geometric features with minimal vertex redundancy. Despite its importance, no existing method has been explicitly designed for this task. While polygonal building outline extraction has been extensively studied, the unique characteristics of roads, such as branching structures and topological connectivity, pose challenges to these methods. To address this gap, we introduce LDPoly, the first dedicated framework for extracting polygonal road outlines from high-resolution aerial images. Our method leverages a novel Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module, enabling the model to simultaneously generate road masks and vertex heatmaps. A tailored polygonization method is then applied to obtain accurate vectorized road polygons with minimal vertex redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which contains detailed polygonal annotations for various topographic objects in several Dutch regions. Our experiments include both in-region and cross-region evaluations, with the latter designed to assess the model’s generalization performance on unseen regions. Quantitative and qualitative results demonstrate that LDPoly outperforms state-of-the-art polygon extraction methods across various metrics, including pixel-level coverage, vertex efficiency, polygon regularity, and road connectivity. We also design two new metrics to assess polygon simplicity and boundary smoothness. Moreover, this work represents the first application of diffusion models for extracting precise vectorized object outlines without redundant vertices from remote-sensing imagery, paving the way for future advancements in this field. </p>
<blockquote>
<p>从高分辨率航空图像中提取多边形道路轮廓是大型地形测绘中的一项重要任务。在此任务中，道路被表示为矢量化的多边形，能够捕捉基本几何特征，并尽量减少顶点的冗余。尽管这项任务非常重要，但现有的方法并没有专门为此设计。虽然多边形建筑轮廓提取已经得到了广泛的研究，但道路的独特特征，如分支结构和拓扑连接性，给这些方法带来了挑战。为了解决这一空白，我们引入了LDPoly，这是第一个专门用于从高分辨率航空图像中提取多边形道路轮廓的框架。我们的方法利用了一种新型的双潜扩散模型，带有通道嵌入融合模块，使模型能够同时生成道路掩膜和顶点热图。然后应用定制的多边形化方法，以获得准确的矢量化道路多边形，并尽量减少顶点的冗余。我们在新的基准数据集Map2ImLas上评估了LDPoly，该数据集包含荷兰几个地区各种地形对象的详细多边形注释。我们的实验包括区域内和跨区域的评估，后者旨在评估模型在未见区域的泛化性能。定量和定性结果表明，LDPoly在各项度量指标上均优于最先进的多边形提取方法，包括像素级覆盖率、顶点效率、多边形规则性和道路连通性。我们还设计了两个新的指标来评估多边形的简洁性和边界平滑度。此外，这项工作代表了扩散模型在遥感影像中提取精确矢量化对象轮廓（无冗余顶点）的首个应用，为这一领域的未来发展铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20645v1">PDF</a> </p>
<p><strong>Summary</strong><br>    针对高分辨率航拍图像的多边形道路轮廓提取在大型地形测绘中具有重要意义。本文提出LDPoly框架，利用双潜扩散模型与通道嵌入融合模块，生成道路掩膜和顶点热图，实现准确、顶点冗余少的道路多边形提取。在Map2ImLas数据集上实验表明，LDPoly在像素级覆盖、顶点效率、多边形规则性和道路连通性等指标上优于现有多边形提取方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDPoly是首个专为从高分辨率航拍图像中提取多边形道路轮廓设计的框架。</li>
<li>该方法结合双潜扩散模型与通道嵌入融合模块，同步生成道路掩膜和顶点热图。</li>
<li>采用定制的多边形化方法，获得顶点冗余少的准确道路多边形。</li>
<li>在Map2ImLas数据集上的实验表明LDPoly在多项指标上优于现有方法。</li>
<li>提出两个新指标评估多边形的简洁性和边界平滑度。</li>
<li>LDPoly是首个将扩散模型应用于遥感图像精确矢量对象轮廓提取的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20645">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7361a3afbe527ccf7dce43b5f34e3dee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c35e3b1a3582970c80a4272a3931357.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dynamic-Attention-Analysis-for-Backdoor-Detection-in-Text-to-Image-Diffusion-Models"><a href="#Dynamic-Attention-Analysis-for-Backdoor-Detection-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Dynamic Attention Analysis for Backdoor Detection in Text-to-Image   Diffusion Models"></a>Dynamic Attention Analysis for Backdoor Detection in Text-to-Image   Diffusion Models</h2><p><strong>Authors:Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen</strong></p>
<p>Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the $&lt;$EOS$&gt;$ token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens’ attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across five representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Robin-WZQ/DAA">https://github.com/Robin-WZQ/DAA</a>. </p>
<blockquote>
<p>最近的研究表明，文本到图像的扩散模型容易受到后门攻击的影响，攻击者会悄悄植入文本触发器来操纵模型输出。之前的后门检测方法主要关注后门样本的静态特征。然而，扩散模型的一个重要特性是它们的固有动态性。本研究引入了一种新的后门检测视角，称为动态注意力分析（DAA），表明这些动态特征对于后门检测是更好的指标。具体来说，通过检查交叉注意力图的动态演变，我们观察到与良性样本相比，后门样本在<EOS>令牌处表现出不同的特征演变模式。为了量化这些动态异常，我们首先引入DAA-I，它将令牌的注意力图视为空间独立，并使用Frobenius范数测量动态特征。此外，为了更好地捕捉注意力图之间的交互并改进特征，我们提出了一种基于动态系统的方法，称为DAA-S。该方法使用基于图的状态方程来制定注意力图之间的空间相关性，并对该方法的全局渐近稳定性进行了理论分析。在五个代表性的后门攻击场景进行的广泛实验表明，我们的方法显著超越了现有的检测方法，平均F1分数达到79.49%，AUC达到87.67%。代码可在<a target="_blank" rel="noopener" href="https://github.com/Robin-WZQ/DAA%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Robin-WZQ/DAA处获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20518v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本至图像扩散模型易受后门攻击影响，攻击者通过植入隐蔽文本触发器操纵模型输出。本研究提出了一种新型后门检测视角——动态注意力分析（DAA），利用扩散模型的动态特性进行检测。通过分析交叉注意力图的动态演变，研究发现在“<EOS>”符号处后门样本与良性样本的特征演化模式不同。为了量化这些动态异常，研究引入了DAA-I和DAA-S两种方法。前者独立测量注意力图的动态特征，后者则基于动态系统方法，使用图状态方程来捕捉注意力图的空间相关性。实验证明，该方法在五种代表性后门攻击场景下显著优于现有检测方法，平均F1分数为79.49%，AUC为87.67%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本至图像扩散模型存在后门攻击问题。</li>
<li>新型后门检测视角——动态注意力分析（DAA）。</li>
<li>后门样本与良性样本在“<EOS>”符号处的特征演化模式不同。</li>
<li>DAA通过两种方法进行动态异常量化：DAA-I和DAA-S。</li>
<li>DAA-I侧重于独立测量注意力图的动态特征。</li>
<li>DAA-S使用动态系统方法和图状态方程捕捉注意力图的空间相关性。</li>
<li>该方法在五种代表性后门攻击场景下的检测效果优异，平均F1分数和AUC均优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76e5f9ad0f1802b23465153ae2d6ed15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a00c98ce898747ebc189a788ba42040.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd7f53168a39ffd65aaa5476e9032f9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9046820e583c45b63b410a9e23a26c52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80453f9a5b8e1b1d3b0b362d41206faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fab3f9b89cf3a9252af4aea48234936.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3DEnhancer-Consistent-Multi-View-Diffusion-for-3D-Enhancement"><a href="#3DEnhancer-Consistent-Multi-View-Diffusion-for-3D-Enhancement" class="headerlink" title="3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement"></a>3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement</h2><p><strong>Authors:Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy</strong></p>
<p>Despite advances in neural rendering, due to the scarcity of high-quality 3D datasets and the inherent limitations of multi-view diffusion models, view synthesis and 3D model generation are restricted to low resolutions with suboptimal multi-view consistency. In this study, we present a novel 3D enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent diffusion model to enhance coarse 3D inputs while preserving multi-view consistency. Our method includes a pose-aware encoder and a diffusion-based denoiser to refine low-quality multi-view images, along with data augmentation and a multi-view attention module with epipolar aggregation to maintain consistent, high-quality 3D outputs across views. Unlike existing video-based approaches, our model supports seamless multi-view enhancement with improved coherence across diverse viewing angles. Extensive evaluations show that 3DEnhancer significantly outperforms existing methods, boosting both multi-view enhancement and per-instance 3D optimization tasks. </p>
<blockquote>
<p>尽管神经网络渲染有所进展，但由于高质量3D数据集稀缺以及多视角扩散模型本身的局限性，视图合成和3D模型生成仍然受限于低分辨率，并且多视角一致性不佳。在本研究中，我们提出了一种新型3D增强流程，名为“3DEnhancer”，它采用多视角潜在扩散模型，能够在保持多视角一致性的同时，增强粗糙的3D输入。我们的方法包括一个姿态感知编码器和一个基于扩散的去噪器，用于细化低质量的多视角图像，还包括数据增强和带有极线聚合的多视角注意力模块，以在不同视角之间保持一致且高质量的三维输出。与现有的基于视频的方法不同，我们的模型支持无缝的多视角增强，并在各种观看角度上提高了连贯性。大量评估表明，3DEnhancer显著优于现有方法，既提升了多视角增强任务的表现，又优化了每个实例的3D任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18565v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://yihangluo.com/projects/3DEnhancer">https://yihangluo.com/projects/3DEnhancer</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为3DEnhancer的新型三维增强管线，利用多视角潜在扩散模型对粗糙三维输入进行增强，同时保持多视角一致性。该方法通过姿态感知编码器、扩散降噪器进行数据增强和多视角注意模块与极坐标聚合，能够在各种视角中维持高质量的三维输出一致性。相比现有视频方法，本模型支持无缝多视角增强，并在多视角增强和每例三维优化任务上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出一种名为3DEnhancer的新型三维增强管线。</li>
<li>采用多视角潜在扩散模型增强粗糙三维输入。</li>
<li>通过姿态感知编码器和扩散降噪器进行数据增强和多视角处理。</li>
<li>采用多视角注意模块与极坐标聚合维持高质量的三维输出一致性。</li>
<li>模型支持无缝多视角增强。</li>
<li>在多视角增强和每例三维优化任务上表现显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-71d103b434a3e90d65abb07eca612904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eeb4495c5206e951f28fe83264a6d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c62989ac30f2c79fe74ff25bed2e824.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. In response to this challenge, we take inspiration from recent successes in generative flow networks (GFlowNets) and propose a reinforcement learning method for diffusion model finetuning, dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), that leverages the rich signal in reward gradients for probabilistic diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>在训练大型扩散模型时，通常的做法是收集目标下游任务的数据集。然而，人们往往希望调整并微调预训练的扩散模型，使其与专家设计的奖励函数或者从小型数据集中学习得到的奖励函数对齐。现有的扩散模型奖励微调的后训练方法通常面临生成样本缺乏多样性、缺乏先验知识保留以及微调收敛缓慢等问题。为了应对这一挑战，我们从最近生成流网络（GFlowNets）的成功中汲取灵感，提出了一种用于扩散模型微调的强化学习方法，被称为Nabla-GFlowNet（简称∇-GFlowNet），该方法利用奖励梯度中的丰富信号进行概率扩散微调。我们展示，所提出的方法能够在不同的真实奖励函数上，快速且保留多样性和先验地对Stable Diffusion（一种大规模文本条件图像扩散模型）进行微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v4">PDF</a> Technical Report (37 pages, 31 figures), Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型的训练问题，尤其是如何通过奖励函数对预训练的扩散模型进行微调。现有方法存在样本多样性不足、先验知识丢失以及微调收敛慢等问题。因此，受到生成流网络（GFlowNets）最新成果的启发，本文提出了一种基于强化学习的扩散模型微调方法，名为Nabla-GFlowNet（简称$\nabla$-GFlowNet）。该方法利用奖励梯度中的丰富信号进行概率扩散微调，能够在不同的真实奖励函数上快速、多样且保留先验地对大型文本条件图像扩散模型Stable Diffusion进行微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的训练通常涉及目标下游任务的数据集收集，但也可以通过奖励函数对预训练模型进行微调。</li>
<li>现有微调方法存在样本多样性不足、先验知识丢失和收敛速度慢的问题。</li>
<li>本文受到生成流网络（GFlowNets）的启发，提出一种基于强化学习的扩散模型微调方法——Nabla-GFlowNet。</li>
<li>Nabla-GFlowNet利用奖励梯度中的丰富信号进行概率扩散微调。</li>
<li>该方法能够在不同的真实奖励函数上实现对大型文本条件图像扩散模型Stable Diffusion的快速、多样且保留先验的微调。</li>
<li>Nabla-GFlowNet方法的引入，提高了扩散模型的性能，尤其是在样本多样性和收敛速度方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7025339bbe673dd6333293dcc8ca8d9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21950de2b5b2e07066fd3abf8159ca34.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Interrelated-Diffusion-Model-for-Few-Shot-Anomaly-Image-Generation"><a href="#Dual-Interrelated-Diffusion-Model-for-Few-Shot-Anomaly-Image-Generation" class="headerlink" title="Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation"></a>Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation</h2><p><strong>Authors:Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Jiafu Wu, Hao Chen, Haoxuan Wang, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang</strong></p>
<p>The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. </p>
<blockquote>
<p>在工业制造中的异常检测性能受到异常数据稀缺的限制。为了克服这一挑战，研究者们已经开始采用异常生成方法来增加异常数据集。然而，现有的异常生成方法存在生成的异常多样性有限的问题，并且难以将生成的异常无缝融合到原始图像中。此外，生成的掩膜通常与生成的异常不匹配。在本文中，我们从一个新的角度克服了这些挑战，同时生成整体图像和对应的异常部分。我们提出了DualAnoDiff，这是一种基于扩散的新颖少样本异常图像生成模型，通过使用双相关扩散模型生成多样且逼真的异常图像，其中一个用于生成整体图像，另一个用于生成异常部分。此外，我们提取背景和形状信息，以减轻少样本图像生成中的失真和模糊现象。大量实验表明，与最先进的方法相比，我们提出的模型在多样性、逼真性和掩膜准确性方面表现优越。总体而言，我们的方法显著提高了下游异常检测任务的性能，包括异常检测、异常定位和异常分类任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.13509v3">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于扩散模型的新型少样本异常图像生成方法——DualAnoDiff。该方法通过生成整体图像和对应的异常部分来解决异常检测中的挑战，提高了异常数据的多样性和现实性，增强了下游异常检测任务的效果。采用双相关扩散模型生成图像和异常部分，提取背景和形状信息缓解少样本图像生成中的失真和模糊现象。实验证明，该方法在多样性、现实性和掩膜准确性方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异常检测在制造业中受到异常数据稀缺的限制。为了解决这个问题，研究者开始采用异常生成方法来增加异常数据集。然而，现有的异常生成方法存在生成的异常多样性有限的问题。</li>
<li>本文提出了一种新的基于扩散模型的少样本异常图像生成方法——DualAnoDiff。通过同时生成整体图像和对应的异常部分来克服现有挑战。</li>
<li>DualAnoDiff使用双相关扩散模型进行图像生成，其中一个用于生成整体图像，另一个用于生成异常部分。这种设计提高了生成的异常图像的多样性和真实性。</li>
<li>通过提取背景和形状信息来减轻少样本图像生成中的失真和模糊现象。这一技术改进有助于提高图像生成的质量。</li>
<li>实验结果显示，DualAnoDiff在多样性、现实性和掩膜准确性方面优于当前的主流方法。</li>
<li>该方法显著提高了下游异常检测任务的效果，包括异常检测、定位和分类任务。这表明DualAnoDiff在实际应用中有良好的应用前景和价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.13509">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d4e2f75e72e92e6a95e88e584a05d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fcedbd118ce6b99cc38f594d194f443.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f8b25283daf510456ece8df25d6b6e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fca5a5b93d7b212f4f923ce2d66b5e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dda5a988aba9fe18096120cf4272e38a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-01/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-224fdbd6db02405895f63495f1389d12.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-01  SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep   Features
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9d1d8a2d3376eb8bcec01d470d25c2b5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-01  Generate more than one child in your co-evolutionary semi-supervised   learning GAN
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
