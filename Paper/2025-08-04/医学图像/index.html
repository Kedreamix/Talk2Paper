<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-04  A New One-Shot Federated Learning Framework for Medical Imaging   Classification with Feature-Guided Rectified Flow and Knowledge Distillation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-829a733622894156e3d6a71c9ca1e6af.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-04-æ›´æ–°"><a href="#2025-08-04-æ›´æ–°" class="headerlink" title="2025-08-04 æ›´æ–°"></a>2025-08-04 æ›´æ–°</h1><h2 id="A-New-One-Shot-Federated-Learning-Framework-for-Medical-Imaging-Classification-with-Feature-Guided-Rectified-Flow-and-Knowledge-Distillation"><a href="#A-New-One-Shot-Federated-Learning-Framework-for-Medical-Imaging-Classification-with-Feature-Guided-Rectified-Flow-and-Knowledge-Distillation" class="headerlink" title="A New One-Shot Federated Learning Framework for Medical Imaging   Classification with Feature-Guided Rectified Flow and Knowledge Distillation"></a>A New One-Shot Federated Learning Framework for Medical Imaging   Classification with Feature-Guided Rectified Flow and Knowledge Distillation</h2><p><strong>Authors:Yufei Ma, Hanwen Zhang, Qiya Yang, Guibo Luo, Yuesheng Zhu</strong></p>
<p>In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted increasing attention due to its low communication overhead, requiring only a single round of transmission. However, existing generative model-based OSFL methods suffer from low training efficiency and potential privacy leakage in the healthcare domain. Additionally, achieving convergence within a single round of model aggregation is challenging under non-Independent and Identically Distributed (non-IID) data. To address these challenges, in this paper a modified OSFL framework is proposed, in which a new Feature-Guided Rectified Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation method are developed. FG-RF on the client side accelerates generative modeling in medical imaging scenarios while preserving privacy by synthesizing feature-level images rather than pixel-level images. To handle non-IID distributions, DLKD enables the global student model to simultaneously mimic the output logits and align the intermediate-layer features of client-side teacher models during aggregation. Experimental results on three non-IID medical imaging datasets show that our new framework and method outperform multi-round federated learning approaches, achieving up to 21.73% improvement, and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our experiments demonstrate that feature-level synthetic images significantly reduce privacy leakage risks compared to pixel-level synthetic images. </p>
<blockquote>
<p>åœ¨å¤šä¸­å¿ƒåœºæ™¯ä¸­ï¼Œç”±äºåªéœ€è¦ä¸€è½®ä¼ è¾“çš„ä½é€šä¿¡å¼€é”€ï¼Œä¸€æ¬¡æ€§è”é‚¦å­¦ä¹ ï¼ˆOSFLï¼‰å·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºç”Ÿæˆæ¨¡å‹çš„OSFLæ–¹æ³•å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸæ½œåœ¨éšç§æ³„éœ²çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œåœ¨éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®ä¸‹ï¼Œåœ¨å•æ¬¡æ¨¡å‹èšåˆä¸­å®ç°æ”¶æ•›æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„OSFLæ¡†æ¶ï¼Œå…¶ä¸­å¼€å‘äº†ä¸€ç§æ–°çš„ç‰¹å¾å¼•å¯¼ä¿®æ­£æµæ¨¡å‹ï¼ˆFG-RFï¼‰å’ŒåŒå±‚çŸ¥è¯†è’¸é¦ï¼ˆDLKDï¼‰èšåˆæ–¹æ³•ã€‚å®¢æˆ·ç«¯çš„FG-RFåŠ é€ŸåŒ»ç–—æˆåƒåœºæ™¯ä¸­çš„ç”Ÿæˆå»ºæ¨¡ï¼Œé€šè¿‡åˆæˆç‰¹å¾çº§å›¾åƒè€Œä¸æ˜¯åƒç´ çº§å›¾åƒæ¥ä¿ç•™éšç§ã€‚ä¸ºäº†å¤„ç†éIIDåˆ†å¸ƒï¼ŒDLKDä½¿å…¨å±€å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿåœ¨èšåˆè¿‡ç¨‹ä¸­åŒæ—¶æ¨¡ä»¿è¾“å‡ºé€»è¾‘å¹¶å¯¹é½å®¢æˆ·ç«¯æ•™å¸ˆæ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾ã€‚åœ¨ä¸‰ä¸ªéIIDåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–°æ¡†æ¶å’Œæ–¹æ³•ä¼˜äºå¤šè½®è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†é«˜è¾¾21.73%çš„æ”¹è¿›ï¼Œå¹¶è¶…è¿‡åŸºçº¿FedISCAå¹³å‡21.75%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸åƒç´ çº§åˆæˆå›¾åƒç›¸æ¯”ï¼Œç‰¹å¾çº§åˆæˆå›¾åƒæ˜¾è‘—é™ä½äº†éšç§æ³„éœ²é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19045v1">PDF</a> Accepted at ECAI 2025</p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›çš„ä¸€ç«™å¼è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ–°çš„ç‰¹å¾å¼•å¯¼ä¿®æ­£æµæ¨¡å‹å’ŒåŒå±‚çŸ¥è¯†è’¸é¦èšåˆæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰ç”Ÿæˆæ¨¡å‹åœ¨ä¸€ç«™å¼è”é‚¦å­¦ä¹ ä¸­é¢ä¸´çš„è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œéšç§æ³„éœ²é—®é¢˜ã€‚æ–°æ¡†æ¶åœ¨åŒ»ç–—æˆåƒåœºæ™¯ä¸­åŠ é€Ÿç”Ÿæˆå»ºæ¨¡ï¼Œé€šè¿‡åˆæˆç‰¹å¾çº§å›¾åƒè€Œéåƒç´ çº§å›¾åƒæ¥ä¿æŠ¤éšç§ã€‚åŒæ—¶ï¼ŒåŒå±‚çŸ¥è¯†è’¸é¦æŠ€æœ¯è§£å†³äº†éç‹¬ç«‹åŒåˆ†å¸ƒæ•°æ®çš„èšåˆé—®é¢˜ï¼Œä½¿å…¨å±€å­¦ç”Ÿæ¨¡å‹åœ¨èšåˆæ—¶èƒ½å¤ŸåŒæ—¶æ¨¡ä»¿è¾“å‡ºé€»è¾‘å’Œä¸å®¢æˆ·ç«¯æ•™å¸ˆæ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾å¯¹é½ã€‚å®éªŒè¯æ˜æ–°æ¡†æ¶å’Œæ–¹æ³•åœ¨éç‹¬ç«‹åŒåˆ†å¸ƒåŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤šè½®è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œéšç§æ³„éœ²é£é™©ä¹Ÿæ˜¾è‘—é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>One-Shot Federated Learning (OSFL)å…·æœ‰ä½é€šä¿¡å¼€é”€çš„ä¼˜ç‚¹ï¼Œä½†åœ¨å¤šä¸­å¿ƒåœºæ™¯ä¸­é¢ä¸´è®­ç»ƒæ•ˆç‡å’Œéšç§æ³„éœ²çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºäºç”Ÿæˆæ¨¡å‹çš„OSFLæ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ä¸€ç«™å¼è”é‚¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶ä¸­å¼•å…¥äº†æ–°çš„ç‰¹å¾å¼•å¯¼ä¿®æ­£æµæ¨¡å‹ï¼ˆFG-RFï¼‰ï¼ŒåŠ é€ŸåŒ»ç–—æˆåƒåœºæ™¯ä¸­çš„ç”Ÿæˆå»ºæ¨¡ã€‚</li>
<li>FG-RFé€šè¿‡åˆæˆç‰¹å¾çº§å›¾åƒè€Œä¸æ˜¯åƒç´ çº§å›¾åƒï¼Œæœ‰åŠ©äºä¿æŠ¤éšç§ã€‚</li>
<li>åŒå±‚çŸ¥è¯†è’¸é¦ï¼ˆDLKDï¼‰æŠ€æœ¯ç”¨äºå¤„ç†éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰æ•°æ®ï¼Œä¿ƒè¿›å…¨å±€å­¦ç”Ÿæ¨¡å‹çš„èšåˆã€‚</li>
<li>å®éªŒè¯æ˜æ–°æ¡†æ¶åœ¨ä¸‰ä¸ªéç‹¬ç«‹åŒåˆ†å¸ƒåŒ»ç–—æˆåƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤šè½®è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e579d641714fd85320dd22775a99be71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73a3fe300000d64fc952d249c8f08af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee25423ca2994f1b8bd4bcd20a4a314.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ab8456bbb510125802f316d2617e3df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ac20713719ab573b2d7c8e1f49668f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-706eeac27213737703adff4aa73d03b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d4db92b7a4c1eda3abb7ab15473d73c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PTCMIL-Multiple-Instance-Learning-via-Prompt-Token-Clustering-for-Whole-Slide-Image-Analysis"><a href="#PTCMIL-Multiple-Instance-Learning-via-Prompt-Token-Clustering-for-Whole-Slide-Image-Analysis" class="headerlink" title="PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole   Slide Image Analysis"></a>PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole   Slide Image Analysis</h2><p><strong>Authors:Beidi Zhao, SangMook Kim, Hao Chen, Chen Zhou, Zu-hua Gao, Gang Wang, Xiaoxiao Li</strong></p>
<p>Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with the complexity and heterogeneity of WSIs. Existing MIL methods face challenges in aggregating diverse patch information into robust WSI representations. While ViTs and clustering-based approaches show promise, they are computationally intensive and fail to capture task-specific and slide-specific variability. To address these limitations, we propose PTCMIL, a novel Prompt Token Clustering-based ViT for MIL aggregation. By introducing learnable prompt tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in an end-to-end manner. It dynamically aligns clustering with downstream tasks, using projection-based clustering tailored to each WSI, reducing complexity while preserving patch heterogeneity. Through token merging and prototype-based pooling, PTCMIL efficiently captures task-relevant patterns. Extensive experiments on eight datasets demonstrate its superior performance in classification and survival analysis tasks, outperforming state-of-the-art methods. Systematic ablation studies confirm its robustness and strong interpretability. The code is released at <a target="_blank" rel="noopener" href="https://github.com/ubc-tea/PTCMIL">https://github.com/ubc-tea/PTCMIL</a>. </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨WSIåˆ†ææ–¹é¢å–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†åœ¨å¤„ç†WSIçš„å¤æ‚æ€§å’Œå¼‚è´¨æ€§æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ç°æœ‰çš„MILæ–¹æ³•åœ¨å°†å„ç§è¡¥ä¸ä¿¡æ¯èšåˆä¸ºç¨³å¥çš„WSIè¡¨ç¤ºæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºViTå’Œèšç±»çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬è®¡ç®—é‡å¤§ï¼Œæœªèƒ½æ•è·ç‰¹å®šä»»åŠ¡å’Œç‰¹å®šå¹»ç¯ç‰‡çš„å˜å¼‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PTCMILï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæç¤ºä»¤ç‰Œèšç±»çš„ViTçš„æ–°å‹MILèšåˆæ–¹æ³•ã€‚é€šè¿‡å°†å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œå¼•å…¥ViTä¸»å¹²ï¼ŒPTCMILä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ç»Ÿä¸€äº†èšç±»ä»»åŠ¡å’Œé¢„æµ‹ä»»åŠ¡ã€‚å®ƒé€šè¿‡é’ˆå¯¹æ¯ä¸ªWSIå®šåˆ¶çš„åŸºäºæŠ•å½±çš„èšç±»æ¥åŠ¨æ€è°ƒæ•´èšç±»ä¸ä¸‹æ¸¸ä»»åŠ¡çš„å¯¹é½æ–¹å¼ï¼Œåœ¨é™ä½å¤æ‚æ€§çš„åŒæ—¶ä¿ç•™è¡¥ä¸çš„å¼‚è´¨æ€§ã€‚é€šè¿‡ä»¤ç‰Œåˆå¹¶å’ŒåŸºäºåŸå‹çš„æ± åŒ–ï¼ŒPTCMILæœ‰æ•ˆåœ°æ•è·äº†ä»»åŠ¡ç›¸å…³çš„æ¨¡å¼ã€‚åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨åˆ†ç±»å’Œç”Ÿå­˜åˆ†æä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚ç³»ç»Ÿçš„æ¶ˆèç ”ç©¶è¯å®äº†å…¶ç¨³å¥æ€§å’Œå¼ºå¤§çš„å¯è§£é‡Šæ€§ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ubc-tea/PTCMIL%E3%80%82">https://github.com/ubc-tea/PTCMILã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18848v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PTCMILæ–¹æ³•åœ¨å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ä¸­å¯¹WSIåˆ†æçš„æ”¹è¿›ã€‚é’ˆå¯¹ç°æœ‰MILæ–¹æ³•åœ¨èšåˆå¤šæ ·è¡¥ä¸ä¿¡æ¯æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒPTCMILå¼•å…¥äº†ä¸€ç§åŸºäºæç¤ºä»¤ç‰Œèšç±»çš„ViTæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œï¼Œå°†èšç±»ä»»åŠ¡å’Œé¢„æµ‹ä»»åŠ¡ç»Ÿä¸€åœ¨ç«¯åˆ°ç«¯çš„æ¡†æ¶å†…ï¼Œå®ç°äº†åŠ¨æ€çš„å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„èšç±»å¯¹é½ã€‚PTCMILé‡‡ç”¨é’ˆå¯¹æ¯ä¸ªWSIé‡èº«å®šåˆ¶çš„æŠ•å½±å¼èšç±»ï¼Œé™ä½äº†å¤æ‚æ€§ï¼ŒåŒæ—¶ä¿ç•™äº†è¡¥ä¸çš„å¼‚è´¨æ€§ã€‚é€šè¿‡ä»¤ç‰Œåˆå¹¶å’ŒåŸºäºåŸå‹çš„æ± åŒ–ï¼ŒPTCMILæœ‰æ•ˆåœ°æ•è·äº†ä»»åŠ¡ç›¸å…³çš„æ¨¡å¼ã€‚åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨åˆ†ç±»å’Œç”Ÿå­˜åˆ†æä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PTCMILæ˜¯ä¸€ç§åŸºäºæç¤ºä»¤ç‰Œèšç±»çš„ViTæ–¹æ³•ï¼Œç”¨äºæ”¹è¿›WSIåˆ†æä¸­çš„å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ã€‚</li>
<li>ç°æœ‰MILæ–¹æ³•åœ¨èšåˆå¤šæ ·è¡¥ä¸ä¿¡æ¯æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒPTCMILæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>PTCMILå¼•å…¥å¯å­¦ä¹ çš„æç¤ºä»¤ç‰Œï¼Œå°†èšç±»ä»»åŠ¡å’Œé¢„æµ‹ä»»åŠ¡ç»“åˆåœ¨ç«¯åˆ°ç«¯çš„æ¡†æ¶å†…ã€‚</li>
<li>PTCMILé‡‡ç”¨æŠ•å½±å¼èšç±»ï¼Œé’ˆå¯¹æ¯ä¸ªWSIè¿›è¡Œå®šåˆ¶ï¼Œä»¥é™ä½å¤æ‚æ€§å¹¶ä¿ç•™è¡¥ä¸çš„å¼‚è´¨æ€§ã€‚</li>
<li>PTCMILé€šè¿‡ä»¤ç‰Œåˆå¹¶å’ŒåŸºäºåŸå‹çš„æ± åŒ–ï¼Œæœ‰æ•ˆæ•è·ä»»åŠ¡ç›¸å…³æ¨¡å¼ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPTCMILåœ¨åˆ†ç±»å’Œç”Ÿå­˜åˆ†æä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>PTCMILå…·æœ‰ç¨³å¥æ€§ã€å¼ºå¯è§£é‡Šæ€§å’Œä¼˜ç§€çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2adb808e53075292ee84217ddf5735e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eb43805976483c40ffc87048899b345.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TextSAM-EUS-Text-Prompt-Learning-for-SAM-to-Accurately-Segment-Pancreatic-Tumor-in-Endoscopic-Ultrasound"><a href="#TextSAM-EUS-Text-Prompt-Learning-for-SAM-to-Accurately-Segment-Pancreatic-Tumor-in-Endoscopic-Ultrasound" class="headerlink" title="TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment   Pancreatic Tumor in Endoscopic Ultrasound"></a>TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment   Pancreatic Tumor in Endoscopic Ultrasound</h2><p><strong>Authors:Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey S. Miller, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao</strong></p>
<p>Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAMâ€™s architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/TextSAM-EUS">https://github.com/HealthX-Lab/TextSAM-EUS</a> . </p>
<blockquote>
<p>èƒ°è…ºç™Œé¢„åä¸è‰¯ï¼Œä¾èµ–äºå†…é•œè¶…å£°ï¼ˆEUSï¼‰è¿›è¡Œé¶å‘æ´»æ£€å’Œæ”¾å°„æ²»ç–—ã€‚ç„¶è€Œï¼Œå†…é•œè¶…å£°çš„æ–‘ç‚¹å™ªå£°ã€ä½å¯¹æ¯”åº¦å’Œéç›´è§‚å¤–è§‚ä½¿å¾—ä½¿ç”¨å…¨ç›‘ç£æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹è¿›è¡Œèƒ°è…ºè‚¿ç˜¤åˆ†å‰²å®¹æ˜“å‡ºç°é”™è¯¯ï¼Œå¹¶ä¸”ä¾èµ–äºå¤§è§„æ¨¡çš„ä¸“ä¸šæ³¨é‡Šæ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TextSAM-EUSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è½»é‡çº§æ–‡æœ¬é©±åŠ¨é€‚åº”æ€§çš„åˆ†æ®µä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€æ‰‹åŠ¨å‡ ä½•æç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†BiomedCLIPæ–‡æœ¬ç¼–ç å™¨çš„æ–‡æœ¬æç¤ºå­¦ä¹ ï¼ˆä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºLoRAçš„SAMæ¶æ„çš„é€‚åº”ï¼Œä»¥å®ç°EUSä¸­çš„è‡ªåŠ¨èƒ°è…ºè‚¿ç˜¤åˆ†å‰²ï¼Œä»…è°ƒæ•´æ€»å‚æ•°çš„0.86%ã€‚åœ¨å…¬å…±èƒ°è…ºå†…é•œè¶…å£°æ•°æ®åº“ä¸Šï¼Œå…·æœ‰è‡ªåŠ¨æç¤ºçš„TextSAM-EUSè·å¾—82.69%çš„Diceç³»æ•°å’Œ85.28%çš„å½’ä¸€åŒ–è¡¨é¢è·ç¦»ï¼ˆNSDï¼‰ï¼Œä½¿ç”¨æ‰‹åŠ¨å‡ ä½•æç¤ºåˆ™è¾¾åˆ°83.10%çš„Diceç³»æ•°å’Œ85.70%çš„NSDï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ï¼ˆä¾‹å¦‚SAMåŠå…¶å˜ä½“ï¼‰ã€‚ä½œä¸ºå°†æç¤ºå­¦ä¹ çº³å…¥SAMåŸºç¡€åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¦–æ¬¡å°è¯•ï¼ŒTextSAM-EUSä¸ºé«˜æ•ˆä¸”ç¨³å¥çš„è‡ªåŠ¨EUSåˆ†å‰²æä¾›äº†å®ç”¨çš„é€‰æ‹©ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/TextSAM-EUS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HealthX-Lab/TextSAM-EUSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18082v3">PDF</a> Accepted to ICCV 2025 Workshop CVAMD</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹èƒ°è…ºç™Œå†…é•œè¶…å£°ï¼ˆEUSï¼‰å›¾åƒåˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§æ–‡æœ¬é©±åŠ¨è‡ªé€‚åº”åˆ†æ®µæ¨¡å‹TextSAM-EUSã€‚è¯¥æ¨¡å‹ç»“åˆæ–‡æœ¬æç¤ºå­¦ä¹ å’ŒåŸºäºLoRAçš„SAMæ¶æ„æ”¹è¿›ï¼Œæ— éœ€æ‰‹åŠ¨å‡ ä½•æç¤ºå³å¯è‡ªåŠ¨è¿›è¡Œèƒ°è…ºè‚¿ç˜¤åˆ†å‰²ã€‚åœ¨å…¬å…±èƒ°è…ºå†…é•œè¶…å£°æ•°æ®åº“ä¸Šï¼ŒTextSAM-EUSçš„è‡ªåŠ¨æç¤ºè¾¾åˆ°82.69%çš„Diceç³»æ•°å’Œ85.28%çš„å½’ä¸€åŒ–è¡¨é¢è·ç¦»ï¼ˆNSDï¼‰ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. TextSAM-EUSæ˜¯ä¸€ä¸ªé’ˆå¯¹å†…é•œè¶…å£°ï¼ˆEUSï¼‰å›¾åƒåˆ†å‰²çš„æ–°å‹æ–‡æœ¬é©±åŠ¨è‡ªé€‚åº”åˆ†æ®µæ¨¡å‹ã€‚
 2. è¯¥æ¨¡å‹é€šè¿‡ç»“åˆæ–‡æœ¬æç¤ºå­¦ä¹ å’ŒåŸºäºLoRAçš„SAMæ¶æ„æ”¹è¿›ï¼Œå¢å¼ºäº†æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ã€‚
 3. TextSAM-EUSæ— éœ€æ‰‹åŠ¨å‡ ä½•æç¤ºå³å¯è‡ªåŠ¨è¿›è¡Œèƒ°è…ºè‚¿ç˜¤åˆ†å‰²ã€‚
 4. åœ¨å…¬å…±èƒ°è…ºå†…é•œè¶…å£°æ•°æ®åº“ä¸Šï¼ŒTextSAM-EUSçš„åˆ†å‰²æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ã€‚
 5. TextSAM-EUSçš„è‡ªåŠ¨æç¤ºè¾¾åˆ°82.69%çš„Diceç³»æ•°å’Œ85.28%çš„NSDï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚
 6. è¯¥æ¨¡å‹ä¸ºèƒ°è…ºè‚¿ç˜¤åˆ†å‰²æä¾›äº†ä¸€ç§å®ç”¨ã€é«˜æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4e843418c491b33a4f54c66b6af930ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adfe6218a1c1a876e50cd44d5cf5bc1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff7c2850b34b55a516c3361fb50b83a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21efa5c56687e325d32ac16ed69db3b3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MLRU-Multiscale-Lightweight-Residual-UNETR-with-Attention-for-Efficient-3D-Medical-Image-Segmentation"><a href="#MLRU-Multiscale-Lightweight-Residual-UNETR-with-Attention-for-Efficient-3D-Medical-Image-Segmentation" class="headerlink" title="MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for   Efficient 3D Medical Image Segmentation"></a>MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for   Efficient 3D Medical Image Segmentation</h2><p><strong>Authors:Nand Kumar Yadav, Rodrigue Rizk, William CW Chen, KC Santosh</strong></p>
<p>Accurate and efficient medical image segmentation is crucial but challenging due to anatomical variability and high computational demands on volumetric data. Recent hybrid CNN-Transformer architectures achieve state-of-the-art results but add significant complexity. In this paper, we propose MLRU++, a Multiscale Lightweight Residual UNETR++ architecture designed to balance segmentation accuracy and computational efficiency. It introduces two key innovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that enhances contextual feature encoding with minimal overhead, and a Multiscale Bottleneck Block (M2B) in the decoder that captures fine-grained details via multi-resolution feature aggregation. Experiments on four publicly available benchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that MLRU++ achieves state-of-the-art performance, with average Dice scores of 87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing leading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and ACDC, respectively, while significantly reducing parameter count and computational cost. Ablation studies evaluating LCBAM and M2B further confirm the effectiveness of the proposed architectural components. Results suggest that MLRU++ offers a practical and high-performing solution for 3D medical image segmentation tasks. Source code is available at: <a target="_blank" rel="noopener" href="https://github.com/1027865/MLRUPP">https://github.com/1027865/MLRUPP</a> </p>
<blockquote>
<p>å‡†ç¡®ä¸”é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²è‡³å…³é‡è¦ï¼Œä½†ç”±äºè§£å‰–ç»“æ„çš„å·®å¼‚æ€§å’Œå¯¹ä½“ç§¯æ•°æ®çš„è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„æ··åˆCNN-Transformeræ¶æ„å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†å¢åŠ äº†å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MLRU++ï¼Œè¿™æ˜¯ä¸€ç§å¤šå°ºåº¦è½»é‡çº§å‰©ä½™UNetr++æ¶æ„ï¼Œæ—¨åœ¨å¹³è¡¡åˆ†å‰²ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚å®ƒå¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šè½»é‡çº§é€šé“å’Œç“¶é¢ˆæ³¨æ„åŠ›æ¨¡å—ï¼ˆLCBAMï¼‰ï¼Œå®ƒåœ¨æœ€å°å¼€é”€çš„æƒ…å†µä¸‹å¢å¼ºäº†ä¸Šä¸‹æ–‡ç‰¹å¾ç¼–ç ï¼›è§£ç å™¨ä¸­çš„å¤šå°ºåº¦ç“¶é¢ˆå—ï¼ˆM2Bï¼‰ï¼Œå®ƒé€šè¿‡å¤šåˆ†è¾¨ç‡ç‰¹å¾èšåˆæ•è·ç²¾ç»†çš„ç»†èŠ‚ã€‚åœ¨å››ä¸ªå…¬å¼€çš„åŸºå‡†æ•°æ®é›†ï¼ˆSynapseã€BTCVã€ACDCå’ŒDecathlon Lungï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMLRU++è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡Diceå¾—åˆ†åˆ†åˆ«ä¸ºSynapseçš„87.57%ã€ACDCçš„93.00%å’ŒLungçš„81.12%ã€‚ä¸ç°æœ‰çš„é¢†å…ˆæ¨¡å‹ç›¸æ¯”ï¼ŒMLRU++åœ¨Synapseå’ŒACDCä¸Šçš„Diceå¾—åˆ†åˆ†åˆ«æé«˜äº†5.38%å’Œ2.12%ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡å’Œè®¡ç®—æˆæœ¬ã€‚å¯¹LCBAMå’ŒM2Bçš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ‰€æå‡ºæ¶æ„ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒMLRU++ä¸ºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡æä¾›äº†å®ç”¨ä¸”é«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/1027865/MLRUPP">https://github.com/1027865/MLRUPP</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16122v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMLRU++çš„å¤šå°ºåº¦è½»é‡çº§æ®‹å·®UNETR++æ¶æ„ï¼Œæ—¨åœ¨å¹³è¡¡åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚å®ƒå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šè½»é‡çº§é€šé“å’Œç“¶é¢ˆæ³¨æ„åŠ›æ¨¡å—ï¼ˆLCBAMï¼‰å’Œå¤šå°ºåº¦ç“¶é¢ˆå—ï¼ˆM2Bï¼‰ã€‚åœ¨å››ä¸ªå…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMLRU++å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡Diceå¾—åˆ†è¾ƒé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLRU++æ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤šå°ºåº¦è½»é‡çº§æ®‹å·®UNETR++æ¶æ„ã€‚</li>
<li>å®ƒé€šè¿‡å¼•å…¥LCBAMå’ŒM2Bä¸¤ä¸ªå…³é”®æ¨¡å—æ¥å¢å¼ºç‰¹å¾ç¼–ç å’Œç»†èŠ‚æ•æ‰ã€‚</li>
<li>LCBAMæ¨¡å—ä»¥æœ€å°çš„é¢å¤–è®¡ç®—æˆæœ¬å¢å¼ºä¸Šä¸‹æ–‡ç‰¹å¾ç¼–ç ã€‚</li>
<li>M2Bæ¨¡å—åœ¨è§£ç å™¨ä¸­æ•è·å¤šå°ºåº¦ç‰¹å¾ï¼Œä»è€Œæ•æ‰æ›´ç²¾ç»†çš„ç»†èŠ‚ã€‚</li>
<li>MLRU++åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒMLRU++åœ¨æé«˜Diceå¾—åˆ†çš„åŒæ—¶ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡å’Œè®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-738e85e4844a7299704f5a62788b11af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-132e1dbe3b2a7cd780d7b8690bcb9185.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-097fe32699428fdfa6561f98e81f231c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="WSI-Agents-A-Collaborative-Multi-Agent-System-for-Multi-Modal-Whole-Slide-Image-Analysis"><a href="#WSI-Agents-A-Collaborative-Multi-Agent-System-for-Multi-Modal-Whole-Slide-Image-Analysis" class="headerlink" title="WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole   Slide Image Analysis"></a>WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole   Slide Image Analysis</h2><p><strong>Authors:Xinheng Lyu, Yuci Liang, Wenting Chen, Meidan Ding, Jiaqi Yang, Guolin Huang, Daokun Zhang, Xiangjian He, Linlin Shen</strong></p>
<p>Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel tissue analysis across various pathological tasks. While recent advancements in multi-modal large language models (MLLMs) allow multi-task WSI analysis through natural language, they often underperform compared to task-specific models. Collaborative multi-agent systems have emerged as a promising solution to balance versatility and accuracy in healthcare, yet their potential remains underexplored in pathology-specific domains. To address these issues, we propose WSI-Agents, a novel collaborative multi-agent system for multi-modal WSI analysis. WSI-Agents integrates specialized functional agents with robust task allocation and verification mechanisms to enhance both task-specific accuracy and multi-task versatility through three components: (1) a task allocation module assigning tasks to expert agents using a model zoo of patch and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through internal consistency checks and external validation using pathology knowledge bases and domain-specific models, and (3) a summary module synthesizing the final summary with visual interpretation maps. Extensive experiments on multi-modal WSI benchmarks show WSI-Agentsâ€™s superiority to current WSI MLLMs and medical agent frameworks across diverse tasks. </p>
<blockquote>
<p>å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWhole Slide Imagesï¼Œç®€ç§°WSIï¼‰åœ¨æ•°å­—ç—…ç†å­¦ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒå¯ä»¥åœ¨ä¸åŒçš„ç—…ç†å­¦ä»»åŠ¡ä¸­å®ç°åƒå…†åƒç´ çº§åˆ«çš„ç»„ç»‡åˆ†æã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Modelsï¼Œç®€ç§°MLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—æˆ‘ä»¬å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œå¤šä»»åŠ¡WSIåˆ†æï¼Œä½†å®ƒä»¬é€šå¸¸ç›¸è¾ƒäºç‰¹å®šä»»åŠ¡çš„æ¨¡å‹è¡¨ç°è¾ƒå·®ã€‚åä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä½œä¸ºä¸€ç§åœ¨åŒ»ç–—é¢†åŸŸä¸­å¹³è¡¡é€šç”¨æ€§å’Œå‡†ç¡®æ€§çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ï¼Œä½†åœ¨ç—…ç†å­¦ç‰¹å®šé¢†åŸŸï¼Œå…¶æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WSI-Agentsï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€WSIåˆ†æçš„æ–°å‹åä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚WSI-Agentsé€šè¿‡é›†æˆä¸“ä¸šåŒ–åŠŸèƒ½æ™ºèƒ½ä½“ä»¥åŠç¨³å¥çš„ä»»åŠ¡åˆ†é…å’ŒéªŒè¯æœºåˆ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸‰ä¸ªç»„ä»¶æ¥æé«˜ä»»åŠ¡ç‰¹å®šçš„å‡†ç¡®æ€§å’Œå¤šä»»åŠ¡é€šç”¨æ€§ï¼šï¼ˆ1ï¼‰ä»»åŠ¡åˆ†é…æ¨¡å—ä½¿ç”¨è¡¥ä¸å’ŒWSIçº§åˆ«çš„MLLMæ¨¡å‹åº“æ¥å°†ä»»åŠ¡åˆ†é…ç»™ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œï¼ˆ2ï¼‰éªŒè¯æœºåˆ¶é€šè¿‡å†…éƒ¨ä¸€è‡´æ€§æ£€æŸ¥å’Œåˆ©ç”¨ç—…ç†å­¦çŸ¥è¯†åº“å’Œç‰¹å®šé¢†åŸŸçš„æ¨¡å‹è¿›è¡Œå¤–éƒ¨éªŒè¯æ¥ç¡®ä¿å‡†ç¡®æ€§ï¼Œï¼ˆ3ï¼‰æ‘˜è¦æ¨¡å—ç»“åˆå¯è§†åŒ–è§£é‡Šåœ°å›¾æ¥æ€»ç»“æœ€ç»ˆç»“æœã€‚åœ¨å¤šæ¨¡æ€WSIåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒWSI-Agentsåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºå½“å‰çš„WSI MLLMå’ŒåŒ»ç–—æ™ºèƒ½ä½“æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14680v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œå…¨æ»‘ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„é‡è¦æ€§ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„åä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿâ€”â€”WSI-Agentsï¼Œç”¨äºå¤šæ¨¡æ€WSIåˆ†æã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆä¸“ä¸šåŠŸèƒ½æ™ºèƒ½ä½“ã€ä»»åŠ¡åˆ†é…æ¨¡å—å’ŒéªŒè¯æœºåˆ¶ï¼Œæé«˜äº†ä»»åŠ¡ç‰¹å®šå‡†ç¡®æ€§å’Œå¤šä»»åŠ¡é€šç”¨æ€§ã€‚é€šè¿‡æ¨¡å‹åº“ä¸­çš„è¡¥ä¸å’ŒWSIçº§åˆ«MLLMsçš„ä»»åŠ¡åˆ†é…æ¨¡å—è¿›è¡Œä»»åŠ¡åˆ†é…ï¼Œé€šè¿‡å†…éƒ¨ä¸€è‡´æ€§æ£€æŸ¥ã€å¤–éƒ¨éªŒè¯ä»¥åŠç—…ç†å­¦çŸ¥è¯†åº“å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹ç¡®ä¿å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡æ‘˜è¦æ¨¡å—æä¾›å¯è§†åŒ–è§£é‡Šå›¾è¿›è¡Œç»¼åˆæ‘˜è¦ã€‚å®éªŒè¯æ˜ï¼ŒWSI-Agentsåœ¨å¤šæ¨¡æ€WSIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>WSIåœ¨æ•°å­—ç—…ç†å­¦ä¸­è‡³å…³é‡è¦ï¼Œç”¨äºè¿›è¡Œå¤šå‰åƒç´ ç»„ç»‡åˆ†æã€‚</li>
<li>å¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”¨äºå¤šä»»åŠ¡WSIåˆ†æï¼Œä½†å¸¸å¸¸æ€§èƒ½ä¸è¶³ã€‚</li>
<li>åä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿå…·æœ‰å¹³è¡¡çµæ´»æ€§å’Œå‡†ç¡®æ€§çš„æ½œåŠ›ï¼Œåœ¨åŒ»ç–—é¢†åŸŸå—åˆ°å…³æ³¨ã€‚</li>
<li>WSI-Agentsæ˜¯ä¸€ä¸ªæ–°é¢–çš„åä½œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸“ä¸ºå¤šæ¨¡æ€WSIåˆ†æè®¾è®¡ã€‚</li>
<li>WSI-Agentsé›†æˆäº†ä¸“ä¸šåŠŸèƒ½æ™ºèƒ½ä½“ï¼Œé‡‡ç”¨ä»»åŠ¡åˆ†é…å’ŒéªŒè¯æœºåˆ¶ç¡®ä¿å‡†ç¡®æ€§ä¸å¤šä»»åŠ¡èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ¨¡å‹åº“ä¸­çš„è¡¥ä¸å’ŒWSIçº§åˆ«MLLMsè¿›è¡Œä»»åŠ¡åˆ†é…ï¼Œç¡®ä¿å‡†ç¡®æ€§å¹¶æå‡æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0f24d69b359703ef991ccd40449bf5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a90624fe4ecb76e7a787259aea17b14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-967c078d72a2c9f9b305983555e8a22d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Software-architecture-and-manual-for-novel-versatile-CT-image-analysis-toolbox-â€“-AnatomyArchive"><a href="#Software-architecture-and-manual-for-novel-versatile-CT-image-analysis-toolbox-â€“-AnatomyArchive" class="headerlink" title="Software architecture and manual for novel versatile CT image analysis   toolbox â€“ AnatomyArchive"></a>Software architecture and manual for novel versatile CT image analysis   toolbox â€“ AnatomyArchive</h2><p><strong>Authors:Lei Xu, Torkel B Brismar</strong></p>
<p>We have developed a novel CT image analysis package named AnatomyArchive, built on top of the recent full body segmentation model TotalSegmentator. It provides automatic target volume selection and deselection capabilities according to user-configured anatomies for volumetric upper- and lower-bounds. It has a knowledge graph-based and time efficient tool for anatomy segmentation mask management and medical image database maintenance. AnatomyArchive enables automatic body volume cropping, as well as automatic arm-detection and exclusion, for more precise body composition analysis in both 2D and 3D formats. It provides robust voxel-based radiomic feature extraction, feature visualization, and an integrated toolchain for statistical tests and analysis. A python-based GPU-accelerated nearly photo-realistic segmentation-integrated composite cinematic rendering is also included. We present here its software architecture design, illustrate its workflow and working principle of algorithms as well provide a few examples on how the software can be used to assist development of modern machine learning models. Open-source codes will be released at <a target="_blank" rel="noopener" href="https://github.com/lxu-medai/AnatomyArchive">https://github.com/lxu-medai/AnatomyArchive</a> for only research and educational purposes. </p>
<blockquote>
<p>æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„CTå›¾åƒåˆ†æè½¯ä»¶åŒ…ï¼Œåä¸ºAnatomyArchiveï¼Œå®ƒå»ºç«‹åœ¨æœ€æ–°çš„å…¨èº«åˆ†å‰²æ¨¡å‹TotalSegmentatorä¹‹ä¸Šã€‚æ ¹æ®ç”¨æˆ·é…ç½®çš„è§£å‰–ç»“æ„ï¼Œå®ƒæä¾›äº†è‡ªåŠ¨ç›®æ ‡ä½“ç§¯é€‰æ‹©å’Œå–æ¶ˆé€‰æ‹©åŠŸèƒ½ï¼Œç”¨äºä½“ç§¯ä¸Šé™å’Œä¸‹é™ã€‚å®ƒæ‹¥æœ‰åŸºäºçŸ¥è¯†å›¾è°±ã€æ—¶é—´é«˜æ•ˆçš„å·¥å…·ï¼Œç”¨äºè§£å‰–åˆ†å‰²æ©æ¨¡ç®¡ç†å’ŒåŒ»å­¦å›¾åƒæ•°æ®åº“ç»´æŠ¤ã€‚AnatomyArchiveèƒ½å¤Ÿå®ç°è‡ªåŠ¨èº«ä½“ä½“ç§¯è£å‰ªï¼Œä»¥åŠè‡ªåŠ¨æ£€æµ‹å’Œæ’é™¤æ‰‹è‡‚ï¼Œä»¥ä¾¿åœ¨2Då’Œ3Dæ ¼å¼ä¸­è¿›è¡Œæ›´ç²¾ç¡®çš„èº«ä½“æˆåˆ†åˆ†æã€‚å®ƒæä¾›äº†ç¨³å¥çš„åŸºäºä½“ç´ çš„æ”¾å°„å­¦ç‰¹å¾æå–ã€ç‰¹å¾å¯è§†åŒ–ä»¥åŠç”¨äºç»Ÿè®¡æµ‹è¯•å’Œåˆ†æçš„ç»¼åˆå·¥å…·é“¾ã€‚è¿˜åŒ…æ‹¬åŸºäºPythonçš„GPUåŠ é€Ÿã€è¿‘ä¹é€¼çœŸçš„åˆ†å‰²é›†æˆå¤åˆç”µå½±æ¸²æŸ“ã€‚æˆ‘ä»¬åœ¨æ­¤ä»‹ç»å…¶è½¯ä»¶æ¶æ„è®¾è®¡ï¼Œè¯´æ˜äº†å…¶å·¥ä½œæµç¨‹å’Œç®—æ³•çš„å·¥ä½œåŸç†ï¼Œå¹¶æä¾›äº†ä¸€äº›å¦‚ä½•ä½¿ç”¨è¯¥è½¯ä»¶è¾…åŠ©å¼€å‘ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç¤ºä¾‹ã€‚å¼€æºä»£ç å°†ä»…åœ¨ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä¸‹å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lxu-medai/AnatomyArchive%E3%80%82">https://github.com/lxu-medai/AnatomyArchiveã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13901v1">PDF</a> 24 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>AnatomyArchiveæ˜¯ä¸€æ¬¾åŸºäºTotalSegmentatorå…¨èº«åˆ†å‰²æ¨¡å‹çš„CTå›¾åƒåˆ†æè½¯ä»¶åŒ…ï¼Œå…·å¤‡è‡ªåŠ¨ç›®æ ‡ä½“ç§¯é€‰æ‹©ã€è§£é™¤é€‰æ‹©åŠŸèƒ½ï¼Œç”¨äºä½“ç§¯ä¸Šä¸‹é™çš„è®¾å®šã€‚å®ƒé‡‡ç”¨çŸ¥è¯†å›¾è°±ä¸ºåŸºç¡€çš„å·¥å…·è¿›è¡Œè§£å‰–åˆ†å‰²æ©æ¨¡ç®¡ç†å’ŒåŒ»å­¦å›¾åƒæ•°æ®åº“ç»´æŠ¤ï¼Œæä¾›è‡ªåŠ¨èº«ä½“ä½“ç§¯è£å‰ªåŠæ‰‹è‡‚è‡ªåŠ¨æ£€æµ‹å’Œæ’é™¤åŠŸèƒ½ï¼Œç”¨äºæ›´ç²¾ç¡®çš„2Då’Œ3Dæ ¼å¼èº«ä½“ç»„æˆåˆ†æã€‚æ­¤å¤–ï¼Œè¯¥è½¯ä»¶è¿˜åŒ…æ‹¬ç¨³å¥çš„åŸºäºä½“ç´ çš„æ”¾å°„å­¦ç‰¹å¾æå–ã€ç‰¹å¾å¯è§†åŒ–ä»¥åŠé›†æˆå·¥å…·é“¾è¿›è¡Œç»Ÿè®¡åˆ†æã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…å«åŸºäºPythonçš„GPUåŠ é€Ÿè¿‘ç…§ç‰‡çº§é€¼çœŸçš„åˆ†å‰²é›†æˆå¤åˆç”µå½±æ¸²æŸ“ã€‚è¯¥è½¯ä»¶æ—¨åœ¨è¾…åŠ©ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnatomyArchiveæ˜¯ä¸€æ¬¾æ–°å‹çš„CTå›¾åƒåˆ†æè½¯ä»¶åŒ…ï¼Œå»ºç«‹åœ¨TotalSegmentatorå…¨èº«åˆ†å‰²æ¨¡å‹ä¹‹ä¸Šã€‚</li>
<li>æä¾›è‡ªåŠ¨ç›®æ ‡ä½“ç§¯é€‰æ‹©å’Œè§£é™¤é€‰æ‹©åŠŸèƒ½ï¼Œå¯è®¾å®šä½“ç§¯ä¸Šä¸‹é™ã€‚</li>
<li>é‡‡ç”¨çŸ¥è¯†å›¾è°±ä¸ºåŸºç¡€çš„å·¥å…·è¿›è¡Œè§£å‰–åˆ†å‰²æ©æ¨¡ç®¡ç†å’ŒåŒ»å­¦å›¾åƒæ•°æ®åº“ç»´æŠ¤ã€‚</li>
<li>èƒ½å¤Ÿè‡ªåŠ¨è¿›è¡Œèº«ä½“ä½“ç§¯è£å‰ªï¼Œå¹¶è‡ªåŠ¨æ£€æµ‹å’Œæ’é™¤æ‰‹è‡‚ï¼Œæé«˜èº«ä½“ç»„æˆåˆ†æçš„ç²¾ç¡®åº¦ã€‚</li>
<li>æä¾›åŸºäºä½“ç´ çš„æ”¾å°„å­¦ç‰¹å¾æå–ã€ç‰¹å¾å¯è§†åŒ–ä»¥åŠé›†æˆå·¥å…·é“¾è¿›è¡Œç»Ÿè®¡åˆ†æã€‚</li>
<li>åŒ…å«åŸºäºPythonçš„GPUåŠ é€Ÿè¿‘ç…§ç‰‡çº§æ¸²æŸ“ï¼Œå¢å¼ºåˆ†å‰²é›†æˆå½±åƒçš„è§†è§‰æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f8b8fe180f5faa44b5d8684df47917b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85eaef059c641d2668763c0a3653f098.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8bea17cf67072530c8697d87c477262.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b20ed7ec4bfa5aac7814862fbdb2e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-147e9693359c8073262e1e05bdadd662.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GLOMIA-Pro-A-Generalizable-Longitudinal-Medical-Image-Analysis-Framework-for-Disease-Progression-Prediction"><a href="#GLOMIA-Pro-A-Generalizable-Longitudinal-Medical-Image-Analysis-Framework-for-Disease-Progression-Prediction" class="headerlink" title="GLOMIA-Pro: A Generalizable Longitudinal Medical Image Analysis   Framework for Disease Progression Prediction"></a>GLOMIA-Pro: A Generalizable Longitudinal Medical Image Analysis   Framework for Disease Progression Prediction</h2><p><strong>Authors:Shuaitong Zhang, Yuchen Sun, Yong Ao, Xuehuan Zhang, Ruoshui Yang, Jiantao Xu, Zuwu Ai, Haike Zhang, Xiang Yang, Yao Xu, Kunwei Li, Duanduan Chen</strong></p>
<p>Longitudinal medical images are essential for monitoring disease progression by capturing spatiotemporal changes associated with dynamic biological processes. While current methods have made progress in modeling spatiotemporal patterns, they face three key limitations: (1) lack of generalizable framework applicable to diverse disease progression prediction tasks; (2) frequent overlook of the ordinal nature inherent in disease staging; (3) susceptibility to representation collapse due to structural similarities between adjacent time points, which can obscure subtle but discriminative progression biomarkers. To address these limitations, we propose a Generalizable LOngitudinal Medical Image Analysis framework for disease Progression prediction (GLOMIA-Pro). GLOMIA-Pro consists of two core components: progression representation extraction and progression-aware fusion. The progression representation extraction module introduces a piecewise orthogonal attention mechanism and employs a novel ordinal progression constraint to disentangle finegrained temporal imaging variations relevant to disease progression. The progression-aware fusion module incorporates a redesigned skip connection architecture which integrates the learned progression representation with current imaging representation, effectively mitigating representation collapse during cross-temporal fusion. Validated on two distinct clinical applications: knee osteoarthritis severity prediction and esophageal cancer treatment response assessment, GLOMIA-Pro consistently outperforms seven state-of-the-art longitudinal analysis methods. Ablation studies further confirm the contribution of individual components, demonstrating the robustness and generalizability of GLOMIA-Pro across diverse clinical scenarios. </p>
<blockquote>
<p>çºµå‘åŒ»å­¦å›¾åƒå¯¹äºæ•æ‰ä¸åŠ¨æ€ç”Ÿç‰©è¿‡ç¨‹ç›¸å…³çš„æ—¶ç©ºå˜åŒ–ä»¥ç›‘æµ‹ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ã€‚å°½ç®¡å½“å‰çš„æ–¹æ³•åœ¨æ¨¡æ‹Ÿæ—¶ç©ºæ¨¡å¼æ–¹é¢å·²æœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬é¢ä¸´ä¸‰å¤§å…³é”®å±€é™ï¼šï¼ˆ1ï¼‰ç¼ºä¹é€‚ç”¨äºå¤šç§ç–¾ç—…è¿›å±•é¢„æµ‹ä»»åŠ¡çš„é€šç”¨æ¡†æ¶ï¼›ï¼ˆ2ï¼‰ç»å¸¸å¿½è§†ç–¾ç—…åˆ†æœŸä¸­å›ºæœ‰çš„æœ‰åºæ€§è´¨ï¼›ï¼ˆ3ï¼‰ç”±äºç›¸é‚»æ—¶é—´ç‚¹ä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå®¹æ˜“å¯¼è‡´è¡¨ç¤ºå´©æºƒï¼Œä»è€Œæ©ç›–äº†å¾®å¦™ä½†å…·æœ‰åŒºåˆ†åŠ›çš„è¿›å±•ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºç–¾ç—…è¿›å±•é¢„æµ‹çš„ä¸€èˆ¬åŒ–çºµå‘åŒ»å­¦å›¾åƒåˆ†ææ¡†æ¶ï¼ˆGLOMIA-Proï¼‰ã€‚GLOMIA-Proç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆï¼šè¿›å±•è¡¨ç¤ºæå–å’Œè¿›å±•æ„ŸçŸ¥èåˆã€‚è¿›å±•è¡¨ç¤ºæå–æ¨¡å—å¼•å…¥äº†ä¸€ç§åˆ†æ®µæ­£äº¤æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°å‹åºè´¯è¿›å±•çº¦æŸæ¥è§£å¼€ä¸ç–¾ç—…è¿›å±•ç›¸å…³çš„ç²¾ç»†æ—¶é—´æˆåƒå˜åŒ–ã€‚è¿›å±•æ„ŸçŸ¥èåˆæ¨¡å—é‡‡ç”¨é‡æ–°è®¾è®¡çš„è·³è·ƒè¿æ¥æ¶æ„ï¼Œå°†å­¦ä¹ åˆ°çš„è¿›å±•è¡¨ç¤ºä¸å½“å‰æˆåƒè¡¨ç¤ºç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å‡è½»äº†è·¨æ—¶é—´èåˆè¿‡ç¨‹ä¸­çš„è¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚åœ¨è†å…³èŠ‚éª¨å…³èŠ‚ç‚ä¸¥é‡ç¨‹åº¦é¢„æµ‹å’Œé£Ÿç®¡ç™Œæ²»ç–—ååº”è¯„ä¼°ä¸¤ä¸ªç‹¬ç«‹ä¸´åºŠåº”ç”¨ä¸Šçš„éªŒè¯è¡¨æ˜ï¼ŒGLOMIA-Proåœ¨æ€§èƒ½ä¸Šå§‹ç»ˆä¼˜äºä¸ƒç§æœ€å…ˆè¿›çš„çºµå‘åˆ†ææ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å„ç»„ä»¶çš„è´¡çŒ®ï¼Œè¯æ˜äº†GLOMIA-Proåœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12500v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>çºµå‘åŒ»å­¦å›¾åƒé€šè¿‡æ•æ‰ä¸åŠ¨æ€ç”Ÿç‰©è¿‡ç¨‹ç›¸å…³çš„æ—¶ç©ºå˜åŒ–ï¼Œå¯¹äºç›‘æµ‹ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ã€‚å½“å‰æ–¹æ³•è™½ç„¶å·²åœ¨å»ºæ¨¡æ—¶ç©ºæ¨¡å¼æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†ä»å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯ä¸é€‚ç”¨äºå¤šç§ç–¾ç—…è¿›å±•é¢„æµ‹ä»»åŠ¡çš„é€šç”¨æ¡†æ¶ï¼›äºŒæ˜¯å¿½è§†äº†ç–¾ç—…åˆ†æœŸä¸­å›ºæœ‰çš„åºæ•°æ€§è´¨ï¼›ä¸‰æ˜¯ç”±äºç›¸é‚»æ—¶é—´ç‚¹çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå®¹æ˜“å¯¼è‡´è¡¨ç¤ºå´©æºƒï¼Œä»è€Œæ©ç›–äº†å¾®å¦™ä½†å…·æœ‰åŒºåˆ†æ€§çš„è¿›å±•ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºç–¾ç—…è¿›å±•é¢„æµ‹çš„é•¿å‘¨æœŸåŒ»å­¦å›¾åƒåˆ†æé€šç”¨æ¡†æ¶ï¼ˆGLOMIA-Proï¼‰ã€‚GLOMIA-ProåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè¿›å±•è¡¨ç¤ºæå–å’Œè¿›å±•æ„ŸçŸ¥èåˆã€‚è¿›å±•è¡¨ç¤ºæå–æ¨¡å—å¼•å…¥äº†ä¸€ç§åˆ†æ®µæ­£äº¤æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§æ–°å‹åºè¿›å±•çº¦æŸæ¥è§£å¼€ä¸ç–¾ç—…è¿›å±•ç›¸å…³çš„ç²¾ç»†æ—¶é—´æˆåƒå˜åŒ–ã€‚è¿›å±•æ„ŸçŸ¥èåˆæ¨¡å—é‡‡ç”¨é‡æ–°è®¾è®¡çš„è·³è¿‡è¿æ¥æ¶æ„ï¼Œå°†å­¦ä¹ åˆ°çš„è¿›å±•è¡¨ç¤ºä¸å½“å‰æˆåƒè¡¨ç¤ºç›¸ç»“åˆï¼Œæœ‰æ•ˆåœ°å‡è½»äº†è·¨æ—¶é—´èåˆæ—¶çš„è¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚åœ¨ä¸¤ç§ç‹¬ç‰¹çš„ä¸´åºŠåº”ç”¨â€”â€”è†å…³èŠ‚éª¨å…³èŠ‚ç‚ä¸¥é‡ç¨‹åº¦é¢„æµ‹å’Œé£Ÿç®¡ç™Œæ²»ç–—ååº”è¯„ä¼°ä¸­ï¼ŒGLOMIA-Proå§‹ç»ˆä¼˜äºä¸ƒç§æœ€å…ˆè¿›çš„çºµå‘åˆ†ææ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å„ç»„ä»¶çš„è´¡çŒ®ï¼Œè¯æ˜äº†GLOMIA-Proåœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œæ™®éæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>1.çºµå‘åŒ»å­¦å›¾åƒå¯¹äºç›‘æµ‹ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ï¼Œæ•æ‰ä¸åŠ¨æ€ç”Ÿç‰©è¿‡ç¨‹ç›¸å…³çš„æ—¶ç©ºå˜åŒ–ã€‚<br>2.å½“å‰æ–¹æ³•è™½ç„¶å–å¾—è¿›å±•ï¼Œä½†å­˜åœ¨ä¸‰å¤§å±€é™æ€§ï¼šç¼ºä¹æ™®é€‚æ€§æ¡†æ¶ã€å¿½è§†ç–¾ç—…åˆ†æœŸçš„åºæ•°æ€§è´¨å’Œè¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚<br>3.GLOMIA-Proé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶â€”â€”è¿›å±•è¡¨ç¤ºæå–å’Œè¿›å±•æ„ŸçŸ¥èåˆï¼Œè§£å†³ä¸Šè¿°å±€é™ã€‚<br>4.è¿›å±•è¡¨ç¤ºæå–æ¨¡å—ä½¿ç”¨åˆ†æ®µæ­£äº¤æ³¨æ„åŠ›æœºåˆ¶å’Œæ–°å‹åºè¿›å±•çº¦æŸï¼Œæ•æ‰ç–¾ç—…è¿›å±•ç›¸å…³çš„ç²¾ç»†æ—¶é—´æˆåƒå˜åŒ–ã€‚<br>5.è¿›å±•æ„ŸçŸ¥èåˆæ¨¡å—é‡‡ç”¨é‡æ–°è®¾è®¡çš„è·³è¿‡è¿æ¥æ¶æ„ï¼Œæœ‰æ•ˆå‡è½»è·¨æ—¶é—´èåˆæ—¶çš„è¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚<br>6.åœ¨è†å…³èŠ‚éª¨å…³èŠ‚ç‚å’Œé£Ÿç®¡ç™Œæ²»ç–—ååº”è¯„ä¼°çš„ä¸´åºŠåº”ç”¨ä¸­ï¼ŒGLOMIA-Proè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bff375893147ce27fce2fc53966fa4be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa9c399273c8c5dbadd129e43725b6bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d5bd19add4fae1b2b480da8c347b7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e4176748a63b5b1e48e3afec2fa0cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95e52e39460e78bbf9d9beb60b171512.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5285c11ed5f3d3507a645f8ba9d158d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SketchDNN-Joint-Continuous-Discrete-Diffusion-for-CAD-Sketch-Generation"><a href="#SketchDNN-Joint-Continuous-Discrete-Diffusion-for-CAD-Sketch-Generation" class="headerlink" title="SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation"></a>SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation</h2><p><strong>Authors:Sathvik Chereddy, John Femiani</strong></p>
<p>We present SketchDNN, a generative model for synthesizing CAD sketches that jointly models both continuous parameters and discrete class labels through a unified continuous-discrete diffusion process. Our core innovation is Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are projected onto the probability simplex via a softmax transformation, facilitating blended class labels for discrete variables. This formulation addresses 2 key challenges, namely, the heterogeneity of primitive parameterizations and the permutation invariance of primitives in CAD sketches. Our approach significantly improves generation quality, reducing Fr&#39;echet Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL) from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch generation on the SketchGraphs dataset. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SketchDNNï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåˆæˆCADè‰å›¾çš„ç”Ÿæˆæ¨¡å‹ã€‚å®ƒé€šè¿‡ç»Ÿä¸€çš„è¿ç»­-ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ï¼Œå¯¹è¿ç»­å‚æ•°å’Œç¦»æ•£ç±»æ ‡ç­¾è¿›è¡Œè”åˆå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°æ˜¯Gaussian-Softmaxæ‰©æ•£ï¼Œé€šè¿‡é«˜æ–¯å™ªå£°æ‰°åŠ¨çš„logitsè¢«æŠ•å½±åˆ°æ¦‚ç‡å•çº¯å½¢ä¸Šï¼Œé€šè¿‡softmaxè½¬æ¢ï¼Œä¸ºç¦»æ•£å˜é‡æä¾›æ··åˆç±»æ ‡ç­¾ã€‚è¿™ç§è¡¨è¿°è§£å†³äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œå³åŸå§‹å‚æ•°åŒ–çš„å¼‚è´¨æ€§å’ŒCADè‰å›¾ä¸­åŸå§‹å›¾å½¢çš„æ’åˆ—ä¸å˜æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œå°†FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ä»16.04é™ä½åˆ°7.80ï¼Œå°†è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ä»84.8é™ä½åˆ°81.33ï¼Œåœ¨SketchGraphsæ•°æ®é›†ä¸Šå»ºç«‹äº†CADè‰å›¾ç”Ÿæˆçš„æ–°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11579v1">PDF</a> 17 pages, 63 figures, Proceedings of the 42nd International   Conference on Machine Learning (ICML2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SketchDNNæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„è¿ç»­-ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ï¼Œå¯¹CADè‰å›¾è¿›è¡Œç”Ÿæˆã€‚å…¶æ ¸å¿ƒåˆ›æ–°ä¸ºé«˜æ–¯-Softmaxæ‰©æ•£ï¼Œé€šè¿‡å°†å¸¦æœ‰é«˜æ–¯å™ªå£°çš„å¯¹æ•°å‡ ç‡æ˜ å°„åˆ°æ¦‚ç‡å•çº¯å½¢ä¸Šï¼Œå®ç°äº†ç¦»æ•£å˜é‡çš„æ··åˆç±»æ ‡ç­¾ã€‚è¯¥æ–¹æ³•è§£å†³äº†CADè‰å›¾å‚æ•°åŒ–çš„å¼‚è´¨æ€§å’ŒåŸå§‹å›¾å½¢çš„æ’åˆ—ä¸å˜æ€§ä¸¤ä¸ªå…³é”®é—®é¢˜ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œå°†FrÃ©chet Inception Distance (FID)ä»16.04é™è‡³7.80ï¼Œå°†è´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ä»84.8é™è‡³81.33ï¼Œåœ¨SketchGraphsæ•°æ®é›†ä¸Šè¾¾åˆ°äº†CADè‰å›¾ç”Ÿæˆçš„æœ€å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SketchDNNæ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºåˆæˆCADè‰å›¾ï¼Œé€šè¿‡ç»Ÿä¸€çš„è¿ç»­-ç¦»æ•£æ‰©æ•£è¿‡ç¨‹å»ºæ¨¡è¿ç»­å‚æ•°å’Œç¦»æ•£ç±»æ ‡ç­¾ã€‚</li>
<li>æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºé«˜æ–¯-Softmaxæ‰©æ•£ï¼Œè§£å†³äº†CADè‰å›¾å‚æ•°åŒ–çš„å¼‚è´¨æ€§é—®é¢˜ã€‚</li>
<li>æ¨¡å‹é€šè¿‡å°†å¸¦æœ‰é«˜æ–¯å™ªå£°çš„å¯¹æ•°å‡ ç‡æ˜ å°„åˆ°æ¦‚ç‡å•çº¯å½¢ä¸Šï¼Œå®ç°äº†ç¦»æ•£å˜é‡çš„æ··åˆç±»æ ‡ç­¾ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†åŸå§‹å›¾å½¢çš„æ’åˆ—ä¸å˜æ€§é—®é¢˜ã€‚</li>
<li>SketchDNNæ˜¾è‘—æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œé™ä½äº†FrÃ©chet Inception Distance (FID)å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶å€¼ï¼ˆNLLï¼‰ã€‚</li>
<li>SketchDNNåœ¨SketchGraphsæ•°æ®é›†ä¸Šçš„è¡¨ç°è¾¾åˆ°äº†CADè‰å›¾ç”Ÿæˆçš„æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2601a2b5d6e7852d2b38b4120b4a45ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07036579544e31f56a1241f861c0c273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fac73b30b9729deb040ebaa29cac19f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Manual-Annotation-A-Human-AI-Collaborative-Framework-for-Medical-Image-Segmentation-Using-Only-â€œBetter-or-Worseâ€-Expert-Feedback"><a href="#Beyond-Manual-Annotation-A-Human-AI-Collaborative-Framework-for-Medical-Image-Segmentation-Using-Only-â€œBetter-or-Worseâ€-Expert-Feedback" class="headerlink" title="Beyond Manual Annotation: A Human-AI Collaborative Framework for Medical   Image Segmentation Using Only â€œBetter or Worseâ€ Expert Feedback"></a>Beyond Manual Annotation: A Human-AI Collaborative Framework for Medical   Image Segmentation Using Only â€œBetter or Worseâ€ Expert Feedback</h2><p><strong>Authors:Yizhe Zhang</strong></p>
<p>Manual annotation of medical images is a labor-intensive and time-consuming process, posing a significant bottleneck in the development and deployment of robust medical imaging AI systems. This paper introduces a novel hands-free Human-AI collaborative framework for medical image segmentation that substantially reduces the annotation burden by eliminating the need for explicit manual pixel-level labeling. The core innovation lies in a preference learning paradigm, where human experts provide minimal, intuitive feedback â€“ simply indicating whether an AI-generated segmentation is better or worse than a previous version. The framework comprises four key components: (1) an adaptable foundation model (FM) for feature extraction, (2) label propagation based on feature similarity, (3) a clicking agent that learns from human better-or-worse feedback to decide where to click and with which label, and (4) a multi-round segmentation learning procedure that trains a state-of-the-art segmentation network using pseudo-labels generated by the clicking agent and FM-based label propagation. Experiments on three public datasets demonstrate that the proposed approach achieves competitive segmentation performance using only binary preference feedback, without requiring experts to directly manually annotate the images. </p>
<blockquote>
<p>æ‰‹åŠ¨æ ‡æ³¨åŒ»å­¦å›¾åƒæ˜¯ä¸€é¡¹åŠ³åŠ¨å¼ºåº¦é«˜ä¸”è€—æ—¶çš„è¿‡ç¨‹ï¼Œè¿™æˆä¸ºäº†ç¨³å¥çš„åŒ»å­¦æˆåƒäººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€å‘å’Œéƒ¨ç½²çš„é‡å¤§ç“¶é¢ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹å…æ‰‹åŠ¨æ“ä½œçš„åŒ»å­¦å›¾åƒåˆ†å‰²äººæœºåä½œæ¡†æ¶ï¼Œé€šè¿‡æ¶ˆé™¤å¯¹æ˜ç¡®çš„æ‰‹åŠ¨åƒç´ çº§æ ‡ç­¾çš„éœ€æ±‚ï¼Œæå¤§åœ°å‡è½»äº†æ ‡æ³¨è´Ÿæ‹…ã€‚æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºåå¥½å­¦ä¹ èŒƒå¼ï¼Œäººç±»ä¸“å®¶æä¾›æœ€å°‘ã€ç›´è§‚çš„åé¦ˆï¼Œåªéœ€æŒ‡ç¤ºäººå·¥æ™ºèƒ½ç”Ÿæˆçš„åˆ†å‰²ç»“æœæ˜¯å¦æ¯”ä¹‹å‰ç‰ˆæœ¬æ›´å¥½æˆ–æ›´å·®ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ç”¨äºç‰¹å¾æå–çš„å¯é€‚åº”åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰ï¼Œï¼ˆ2ï¼‰åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„æ ‡ç­¾ä¼ æ’­ï¼Œï¼ˆ3ï¼‰ç‚¹å‡»ä»£ç†èƒ½å¤Ÿä»äººç±»çš„æ›´å¥½æˆ–æ›´å·®çš„åé¦ˆä¸­å­¦ä¹ ï¼Œä»¥å†³å®šç‚¹å‡»çš„ä½ç½®å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œï¼ˆ4ï¼‰å¤šè½®åˆ†å‰²å­¦ä¹ ç¨‹åºï¼Œä½¿ç”¨ç‚¹å‡»ä»£ç†å’ŒåŸºäºFMçš„æ ‡ç­¾ä¼ æ’­ç”Ÿæˆçš„ä¼ªæ ‡ç­¾æ¥è®­ç»ƒæœ€å…ˆè¿›çš„åˆ†å‰²ç½‘ç»œã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨äºŒå…ƒåå¥½åé¦ˆå°±å®ç°äº†ç«äº‰çš„åˆ†å‰²æ€§èƒ½ï¼Œæ— éœ€ä¸“å®¶ç›´æ¥æ‰‹åŠ¨æ ‡æ³¨å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05815v2">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹å…æ‰‹åŠ¨çš„äººæœºåä½œæ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæ˜¾è‘—å‡è½»äº†æ ‡æ³¨è´Ÿæ‹…ï¼Œæ— éœ€æ˜¾å¼çš„æ‰‹åŠ¨åƒç´ çº§æ ‡æ³¨ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºåå¥½å­¦ä¹ èŒƒå¼ï¼Œäººç±»ä¸“å®¶æä¾›æœ€å°‘ã€ç›´è§‚çš„åé¦ˆï¼Œåªéœ€æŒ‡ç¤ºAIç”Ÿæˆçš„åˆ†å‰²æ˜¯å¦æ¯”ä¹‹å‰ç‰ˆæœ¬æ›´å¥½æˆ–æ›´å·®ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å››ä¸ªå…³é”®ç»„ä»¶ï¼šè‡ªé€‚åº”åŸºç¡€æ¨¡å‹ã€åŸºäºç‰¹å¾ç›¸ä¼¼æ€§çš„æ ‡ç­¾ä¼ æ’­ã€ç‚¹å‡»ä»£ç†å’Œå¤šè½®åˆ†å‰²å­¦ä¹ ç¨‹åºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨äºŒå…ƒåå¥½åé¦ˆå³å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ï¼Œæ— éœ€ä¸“å®¶ç›´æ¥æ‰‹åŠ¨æ ‡æ³¨å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒçš„æ‰‹åŠ¨æ ‡æ³¨æ˜¯åŠ³åŠ¨å¯†é›†ä¸”è€—æ—¶çš„è¿‡ç¨‹ï¼Œé™åˆ¶äº†åŒ»ç–—æˆåƒAIç³»ç»Ÿçš„å‘å±•å’Œéƒ¨ç½²ã€‚</li>
<li>æ–°å‹å…æ‰‹åŠ¨äººæœºåä½œæ¡†æ¶å¼•å…¥ï¼Œæ˜¾è‘—å‡å°‘æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>æ ¸å¿ƒåˆ›æ–°åœ¨äºåå¥½å­¦ä¹ èŒƒå¼ï¼Œåªéœ€äººç±»ä¸“å®¶æä¾›ç®€å•åé¦ˆã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬è‡ªé€‚åº”åŸºç¡€æ¨¡å‹ã€æ ‡ç­¾ä¼ æ’­ã€ç‚¹å‡»ä»£ç†å’Œå¤šè½®åˆ†å‰²å­¦ä¹ ç¨‹åºå››ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…ä½¿ç”¨äºŒå…ƒåå¥½åé¦ˆå³å¯å®ç°å…·æœ‰ç«äº‰åŠ›çš„å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å®éªŒåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f1ff21c5370cf70542a1d02abe0440a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab8ef16cb1f8b8526b2c924cc70b91f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89a335353639dd0bc12ff141a381c853.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DMCIE-Diffusion-Model-with-Concatenation-of-Inputs-and-Errors-to-Improve-the-Accuracy-of-the-Segmentation-of-Brain-Tumors-in-MRI-Images"><a href="#DMCIE-Diffusion-Model-with-Concatenation-of-Inputs-and-Errors-to-Improve-the-Accuracy-of-the-Segmentation-of-Brain-Tumors-in-MRI-Images" class="headerlink" title="DMCIE: Diffusion Model with Concatenation of Inputs and Errors to   Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images"></a>DMCIE: Diffusion Model with Concatenation of Inputs and Errors to   Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images</h2><p><strong>Authors:Sara Yavari, Rahul Nitin Pandya, Jacob Furst</strong></p>
<p>Accurate segmentation of brain tumors in MRI scans is essential for reliable clinical diagnosis and effective treatment planning. Recently, diffusion models have demonstrated remarkable effectiveness in image generation and segmentation tasks. This paper introduces a novel approach to corrective segmentation based on diffusion models. We propose DMCIE (Diffusion Model with Concatenation of Inputs and Errors), a novel framework for accurate brain tumor segmentation in multi-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation mask, from which an error map is generated by identifying the differences between the prediction and the ground truth. The error map, concatenated with the original MRI images, are used to guide a diffusion model. Using multimodal MRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation accuracy by focusing on misclassified regions, guided by the original inputs. Evaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art diffusion-based segmentation methods, achieving a Dice Score of 93.46 and an HD95 of 5.94 mm. These results highlight the effectiveness of error-guided diffusion in producing precise and reliable brain tumor segmentations. </p>
<blockquote>
<p>åœ¨MRIæ‰«æä¸­å¯¹è„‘è‚¿ç˜¤è¿›è¡Œå‡†ç¡®çš„åˆ†å‰²å¯¹äºå¯é çš„ä¸´åºŠè¯Šæ–­å’Œæœ‰æ•ˆçš„æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹ä¿®æ­£åˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†DMCIEï¼ˆè¾“å…¥å’Œè¯¯å·®æ‹¼æ¥çš„æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€MRIæ‰«æä¸­è„‘è‚¿ç˜¤ç²¾ç¡®åˆ†å‰²çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬ä½¿ç”¨3D U-Netç”Ÿæˆåˆå§‹åˆ†å‰²æ©è†œï¼Œé€šè¿‡è¯†åˆ«é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚æ¥ç”Ÿæˆè¯¯å·®å›¾ã€‚è¯¯å·®å›¾ä¸åŸå§‹MRIå›¾åƒæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚ä½¿ç”¨å¤šæ¨¡æ€MRIè¾“å…¥ï¼ˆT1ã€T1ceã€T2ã€FLAIRï¼‰ï¼ŒDMCIEé€šè¿‡å…³æ³¨è¯¯åˆ†ç±»åŒºåŸŸï¼Œåœ¨åŸå§‹è¾“å…¥çš„æŒ‡å¯¼ä¸‹æœ‰æ•ˆåœ°æé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDMCIEä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„åˆ†å‰²æ–¹æ³•ï¼Œå®ç°äº†Diceç³»æ•°ä¸º93.46ï¼ŒHD95ä¸º5.94mmã€‚è¿™äº›ç»“æœçªæ˜¾äº†è¯¯å·®å¼•å¯¼æ‰©æ•£åœ¨äº§ç”Ÿç²¾ç¡®å¯é çš„è„‘è‚¿ç˜¤åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00983v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è„‘è‚¿ç˜¤çŸ«æ­£åˆ†å‰²æ–¹æ³•â€”â€”DMCIEã€‚è¯¥æ–¹æ³•é‡‡ç”¨3D U-Netç”Ÿæˆåˆå§‹åˆ†å‰²æ©è†œï¼Œé€šè¿‡å¯¹æ¯”é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ç”Ÿæˆè¯¯å·®å›¾ï¼Œå¹¶å°†è¯¯å·®å›¾ä¸åŸå§‹MRIå›¾åƒç»“åˆï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„è¿è¡Œã€‚ä½¿ç”¨å¤šæ¨¡æ€MRIè¾“å…¥ï¼ˆT1ã€T1ceã€T2ã€FLAIRï¼‰ï¼ŒDMCIEèƒ½æœ‰æ•ˆæé«˜åˆ†å‰²ç²¾åº¦ï¼Œé‡ç‚¹å…³æ³¨è¯¯åˆ†ç±»åŒºåŸŸï¼Œç”±åŸå§‹è¾“å…¥å¼•å¯¼ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMCIEçš„Diceç³»æ•°ä¸º93.46%ï¼ŒHD95ä¸º5.94mmï¼Œä¼˜äºå…¶ä»–å…ˆè¿›çš„æ‰©æ•£åˆ†å‰²æ–¹æ³•ï¼Œè¯æ˜äº†è¯¯å·®å¼•å¯¼æ‰©æ•£åœ¨è„‘è‚¿ç˜¤ç²¾ç¡®åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DMCIEæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è„‘è‚¿ç˜¤çŸ«æ­£åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨3D U-Netç”Ÿæˆåˆå§‹åˆ†å‰²æ©è†œï¼Œå¹¶ç”Ÿæˆè¯¯å·®å›¾æ¥çº æ­£é¢„æµ‹é”™è¯¯ã€‚</li>
<li>è¯¯å·®å›¾ä¸åŸå§‹MRIå›¾åƒç»“åˆï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›æŒ‡å¯¼ã€‚</li>
<li>å¤šæ¨¡æ€MRIè¾“å…¥ï¼ˆT1ã€T1ceã€T2ã€FLAIRï¼‰ç”¨äºæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>DMCIEåœ¨BraTS2020æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºè‰²ï¼ŒDiceç³»æ•°é«˜è¾¾93.46%ï¼ŒHD95ä¸º5.94mmã€‚</li>
<li>DMCIEæ–¹æ³•ç›¸æ¯”å…¶ä»–å…ˆè¿›çš„æ‰©æ•£åˆ†å‰²æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-225ea534293e27174695f082ca01cc7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba99c782d6346a1a976b5c91b4be5266.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ZeroReg3D-A-Zero-shot-Registration-Pipeline-for-3D-Consecutive-Histopathology-Image-Reconstruction"><a href="#ZeroReg3D-A-Zero-shot-Registration-Pipeline-for-3D-Consecutive-Histopathology-Image-Reconstruction" class="headerlink" title="ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive   Histopathology Image Reconstruction"></a>ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive   Histopathology Image Reconstruction</h2><p><strong>Authors:Juming Xiong, Ruining Deng, Jialin Yue, Siqi Lu, Junlin Guo, Marilyn Lionts, Tianyuan Yao, Can Cui, Junchao Zhu, Chongyu Qu, Mengmeng Yin, Haichun Yang, Yuankai Huo</strong></p>
<p>Histological analysis plays a crucial role in understanding tissue structure and pathology. While recent advancements in registration methods have improved 2D histological analysis, they often struggle to preserve critical 3D spatial relationships, limiting their utility in both clinical and research applications. Specifically, constructing accurate 3D models from 2D slices remains challenging due to tissue deformation, sectioning artifacts, variability in imaging techniques, and inconsistent illumination. Deep learning-based registration methods have demonstrated improved performance but suffer from limited generalizability and require large-scale training data. In contrast, non-deep-learning approaches offer better generalizability but often compromise on accuracy. In this study, we introduced ZeroReg3D, a novel zero-shot registration pipeline tailored for accurate 3D reconstruction from serial histological sections. By combining zero-shot deep learning-based keypoint matching with optimization-based affine and non-rigid registration techniques, ZeroReg3D effectively addresses critical challenges such as tissue deformation, sectioning artifacts, staining variability, and inconsistent illumination without requiring retraining or fine-tuning. The code has been made publicly available at <a target="_blank" rel="noopener" href="https://github.com/hrlblab/ZeroReg3D">https://github.com/hrlblab/ZeroReg3D</a> </p>
<blockquote>
<p>ç»„ç»‡ç—…ç†å­¦åˆ†æåœ¨äº†è§£ç»„ç»‡ç»“æ„å’Œç—…ç†å­¦æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚è™½ç„¶æœ€è¿‘çš„æ³¨å†Œæ–¹æ³•æ”¹è¿›äº†äºŒç»´ç»„ç»‡ç—…ç†å­¦åˆ†æï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥ä¿æŒå…³é”®çš„ä¸‰ç»´ç©ºé—´å…³ç³»ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºç»„ç»‡å˜å½¢ã€åˆ‡ç‰‡ä¼ªå½±ã€æˆåƒæŠ€æœ¯å·®å¼‚å’Œä¸ä¸€è‡´çš„ç…§æ˜ç­‰å› ç´ ï¼Œä»äºŒç»´åˆ‡ç‰‡æ„å»ºå‡†ç¡®çš„ä¸‰ç»´æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ³¨å†Œæ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæ”¹è¿›çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨é€šç”¨æ€§æœ‰é™å’Œéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéæ·±åº¦å­¦ä¹ æ–¹æ³•æä¾›äº†æ›´å¥½çš„é€šç”¨æ€§ï¼Œä½†å¾€å¾€ç‰ºç‰²äº†å‡†ç¡®æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ZeroReg3Dï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬æ³¨å†Œæµç¨‹ï¼Œä¸“ä¸ºä»è¿ç»­ç»„ç»‡åˆ‡ç‰‡è¿›è¡Œå‡†ç¡®çš„ä¸‰ç»´é‡å»ºè€Œé‡èº«å®šåˆ¶ã€‚é€šè¿‡ç»“åˆåŸºäºé›¶æ ·æœ¬æ·±åº¦å­¦ä¹ çš„å…³é”®ç‚¹åŒ¹é…å’ŒåŸºäºä¼˜åŒ–çš„ä»¿å°„å’Œéåˆšæ€§æ³¨å†ŒæŠ€æœ¯ï¼ŒZeroReg3Dæœ‰æ•ˆåœ°è§£å†³äº†ç»„ç»‡å˜å½¢ã€åˆ‡ç‰‡ä¼ªå½±ã€æŸ“è‰²å·®å¼‚å’Œä¸ä¸€è‡´ç…§æ˜ç­‰å…³é”®é—®é¢˜ï¼Œæ— éœ€è¿›è¡Œå†è®­ç»ƒæˆ–å¾®è°ƒã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/hrlblab/ZeroReg3D%E3%80%82">https://github.com/hrlblab/ZeroReg3Dã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21923v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦ç»„ç»‡åˆ†æå¯¹ç†è§£ç»„ç»‡ç»“æ„å’Œç—…ç†å­¦è‡³å…³é‡è¦ã€‚å½“å‰æ³¨å†Œæ–¹æ³•è™½æ”¹è¿›äº†äºŒç»´åˆ†æï¼Œä½†éš¾ä»¥ä¿ç•™é‡è¦çš„ä¸‰ç»´ç©ºé—´å…³ç³»ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠå’Œç ”ç©¶ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æå‡ºZeroReg3Dï¼Œä¸€ç§ç”¨äºä»è¿ç»­ç»„ç»‡åˆ‡ç‰‡è¿›è¡Œå‡†ç¡®ä¸‰ç»´é‡å»ºçš„é›¶å°„å‡»æ³¨å†Œæµç¨‹ã€‚å®ƒç»“åˆäº†åŸºäºæ·±åº¦å­¦ä¹ çš„å…³é”®ç‚¹åŒ¹é…ä¸ä¼˜åŒ–åŸºç¡€çš„ä»¿å°„å’Œéåˆšæ€§æ³¨å†ŒæŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³äº†ç»„ç»‡å˜å½¢ã€åˆ‡ç‰‡ä¼ªå½±ã€æŸ“è‰²å˜å¼‚å’Œç…§æ˜ä¸ä¸€è‡´ç­‰å…³é”®é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦ç»„ç»‡åˆ†æåœ¨ç†è§£ç»„ç»‡ç»“æ„å’Œç—…ç†å­¦æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚</li>
<li>å½“å‰æ³¨å†Œæ–¹æ³•åœ¨äºŒç»´åˆ†ææ–¹é¢æœ‰æ‰€æ”¹è¿›ï¼Œä½†åœ¨å¤„ç†ä¸‰ç»´ç©ºé—´å…³ç³»æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ„å»ºä»äºŒç»´åˆ‡ç‰‡åˆ°å‡†ç¡®ä¸‰ç»´æ¨¡å‹çš„è¿‡ç¨‹æ˜¯å›°éš¾çš„ï¼ŒåŸå› åŒ…æ‹¬ç»„ç»‡å˜å½¢ã€åˆ‡ç‰‡ä¼ªå½±ã€æˆåƒæŠ€æœ¯å·®å¼‚å’Œç…§æ˜ä¸ä¸€è‡´ç­‰ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•è™½ç„¶è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ï¼Œä½†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šæœ‰é™ï¼Œå¹¶éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>éæ·±åº¦å­¦ä¹ æ–¹æ³•å…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ‰€å¦¥åã€‚</li>
<li>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é›¶å°„å‡»æ³¨å†Œæ–¹æ³•ZeroReg3Dï¼Œèƒ½æœ‰æ•ˆè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01ff94b363cad38a5e8d310e5faed856.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa32752e34ce196d9b69f0ecdb36eb05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-245c124996b84594fc6ef19d10862487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8782738e62270607ed16a8f3e84b5b43.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification"><a href="#LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification" class="headerlink" title="LKA: Large Kernel Adapter for Enhanced Medical Image Classification"></a>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</h2><p><strong>Authors:Ziquan Zhu, Si-Yuan Lu, Tianjin Huang, Lu Liu, Zhe Liu</strong></p>
<p>Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets. </p>
<blockquote>
<p>å°½ç®¡å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœå´æœªèƒ½è¾¾åˆ°é¢„æœŸã€‚è¿™ä¸€å±€é™æ€§æºäºä¸¤ä¸ªå…³é”®å› ç´ ï¼šï¼ˆ1ï¼‰åŒ»å­¦å›¾åƒè¡¨ç°å‡ºå¹¿æ³›çš„è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡æ¥æ•æ‰å…³é”®ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ç°æœ‰çš„PEFTæ–¹æ³•æ²¡æœ‰æ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºçš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶æ‰©å¤§æ„Ÿå—é‡ã€‚æ‰€æå‡ºçš„LKAç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å‹å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚é€šè¿‡å¯¹å„ç§æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ›´å¤§çš„å†…æ ¸å°ºå¯¸å¯¹äºå¢å¼ºé¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºçš„å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰åœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¶…è¿‡äº†11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œåœ¨top-1å‡†ç¡®ç‡ä¸Šè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19118v3">PDF</a> The manuscript has been withdrawn in order to revise key technical   components and improve experimental validation. We plan to substantially   update the model design and resubmit after further evaluation.</p>
<p><strong>Summary</strong><br>     å½“å‰å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å„é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœä¸å°½å¦‚äººæ„ã€‚ä¸»è¦å—é™äºä¸¤ç‚¹ï¼šåŒ»å­¦å›¾åƒå­˜åœ¨å¹¿æ³›è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦å¤§æ„Ÿå—é‡æ•æ‰å…³é”®ç‰¹å¾ï¼›ç°æœ‰PEFTæ–¹æ³•æœªæ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºé—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºLarge Kernel Adapterï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨æ‰©å¤§æ„Ÿå—é‡åŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚LKAç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨æ›´å¤§çš„å†…æ ¸å°ºå¯¸å¯¹äºæé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æå‡ºçš„LKAåœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šä¼˜äº11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œåœ¨top-1å‡†ç¡®ç‡ä¸Šè¶…è¿‡ç°æœ‰æŠ€æœ¯3.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒå…·æœ‰å¹¿æ³›è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ç‰¹ç‚¹ï¼Œéœ€è¦å¤§æ„Ÿå—é‡æ•æ‰å…³é”®ç‰¹å¾ã€‚</li>
<li>å½“å‰å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœæœ‰é™ã€‚</li>
<li>LKAï¼ˆLarge Kernel Adapterï¼‰æ—¨åœ¨æ‰©å¤§æ„Ÿå—é‡å¹¶ç»´æŒå‚æ•°æ•ˆç‡ã€‚</li>
<li>LKAç”±ä¸‹æŠ•å½±ã€é€šé“å¤§å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ä¸‰ä¸ªå…³é”®ç»„ä»¶æ„æˆã€‚</li>
<li>å®éªŒè¯æ˜æ›´å¤§å†…æ ¸å°ºå¯¸å¯¹æé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚</li>
<li>LKAåœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–PEFTæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜top-1å‡†ç¡®ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-73619839500ded2ef30b7caf04d24e18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbec7e26dc8fb1af5f39c9886ac4b020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d3822294460fec056aef4a4ced486a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55936770fbc3ceffe9ede2093420c107.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Seed-Selection-for-Human-Oriented-Image-Reconstruction-via-Guided-Diffusion"><a href="#Seed-Selection-for-Human-Oriented-Image-Reconstruction-via-Guided-Diffusion" class="headerlink" title="Seed Selection for Human-Oriented Image Reconstruction via Guided   Diffusion"></a>Seed Selection for Human-Oriented Image Reconstruction via Guided   Diffusion</h2><p><strong>Authors:Yui Tatsumi, Ziyue Zeng, Hiroshi Watanabe</strong></p>
<p>Conventional methods for scalable image coding for humans and machines require the transmission of additional information to achieve scalability. A recent diffusion-based approach avoids this by generating human-oriented images from machine-oriented images without extra bitrate. However, it utilizes a single random seed, which may lead to suboptimal image quality. In this paper, we propose a seed selection method that identifies the optimal seed from multiple candidates to improve image quality without increasing the bitrate. To reduce the computational cost, selection is performed based on intermediate outputs obtained from early steps of the reverse diffusion process. Experimental results demonstrate that our proposed method outperforms the baseline, which uses a single random seed without selection, across multiple evaluation metrics. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å¯ä¼¸ç¼©å›¾åƒç¼–ç æ–¹æ³•å’Œé’ˆå¯¹äººç±»å’Œæœºå™¨çš„ç¼–ç æ–¹æ³•éœ€è¦ä¼ è¾“é¢å¤–çš„ä¿¡æ¯æ¥å®ç°å¯ä¼¸ç¼©æ€§ã€‚æœ€è¿‘å‡ºç°çš„ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šè¿‡ä»é¢å‘æœºå™¨çš„å›¾åƒç”Ÿæˆé¢å‘äººç±»çš„å›¾åƒï¼Œæ— éœ€é¢å¤–çš„æ¯”ç‰¹ç‡ã€‚ç„¶è€Œï¼Œå®ƒä»…ä½¿ç”¨ä¸€ä¸ªéšæœºç§å­ï¼Œè¿™å¯èƒ½å¯¼è‡´å›¾åƒè´¨é‡ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§å­é€‰æ‹©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»å¤šä¸ªå€™é€‰ç§å­ä¸­è¯†åˆ«å‡ºæœ€ä½³ç§å­ï¼Œæ—¨åœ¨æé«˜å›¾åƒè´¨é‡è€Œä¸å¢åŠ æ¯”ç‰¹ç‡ã€‚ä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œæ ¹æ®åå‘æ‰©æ•£è¿‡ç¨‹æ—©æœŸæ­¥éª¤è·å¾—çš„ä¸­é—´è¾“å‡ºæ¥è¿›è¡Œé€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæŒ‡æ ‡è¯„ä»·ä¸­å‡ä¼˜äºä½¿ç”¨å•ä¸€éšæœºç§å­ä¸”ä¸è¿›è¡Œé€‰æ‹©çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05363v3">PDF</a> Accepted by 2025 IEEE 14th Global Conference on Consumer Electronics   (GCCE 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–ç æ–¹æ³•ï¼Œèƒ½å¤Ÿä»æœºå™¨å¯¼å‘çš„å›¾åƒç”Ÿæˆäººç±»å¯¼å‘çš„å›¾åƒï¼Œæ— éœ€é¢å¤–æ¯”ç‰¹ç‡ã€‚é’ˆå¯¹ä½¿ç”¨å•ä¸€éšæœºç§å­å¯èƒ½å¯¼è‡´å›¾åƒè´¨é‡ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç§å­é€‰æ‹©æ–¹æ³•ï¼Œä»å¤šä¸ªå€™é€‰ç§å­ä¸­é€‰æ‹©æœ€ä¼˜ç§å­ä»¥æå‡å›¾åƒè´¨é‡ï¼ŒåŒæ—¶ä¸å¢åŠ æ¯”ç‰¹ç‡ã€‚é€šè¿‡æ—©æœŸåå‘æ‰©æ•£è¿‡ç¨‹çš„ä¸­é—´è¾“å‡ºæ¥é™ä½è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºä½¿ç”¨å•ä¸€éšæœºç§å­è€Œä¸è¿›è¡Œé€‰æ‹©çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºå›¾åƒç¼–ç ï¼Œèƒ½ä»æœºå™¨å¯¼å‘çš„å›¾åƒç”Ÿæˆäººç±»å¯¼å‘çš„å›¾åƒï¼Œæ— éœ€é¢å¤–æ¯”ç‰¹ç‡ã€‚</li>
<li>æå‡ºäº†ç§å­é€‰æ‹©æ–¹æ³•ï¼Œä»å¤šä¸ªå€™é€‰ç§å­ä¸­é€‰æ‹©æœ€ä¼˜ç§å­ä»¥æå‡å›¾åƒè´¨é‡ã€‚</li>
<li>ç§å­é€‰æ‹©åŸºäºæ—©æœŸåå‘æ‰©æ•£è¿‡ç¨‹çš„ä¸­é—´è¾“å‡ºï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºä½¿ç”¨å•ä¸€éšæœºç§å­çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé¿å…ä¼ ç»Ÿå¯æ‰©å±•å›¾åƒç¼–ç æ–¹æ³•éœ€è¦ä¼ è¾“é¢å¤–ä¿¡æ¯æ¥å®ç°å¯æ‰©å±•æ€§çš„éœ€æ±‚ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ç§å­é€‰æ‹©ï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¯”ç‰¹ç‡çš„æƒ…å†µä¸‹æé«˜å›¾åƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2ac89e55088a4ea8f6577b9af0b081f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e3e99bde305fa77ccc7522bbbc605a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e2b886069ef76c3e02039be8ff7856.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a500257d943262a8c243517fc5ade644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d759d0d83cf06ebbb08f0841355a96bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74126865e1166c8020e8115650cfff6b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Humanoids-in-Hospitals-A-Technical-Study-of-Humanoid-Robot-Surrogates-for-Dexterous-Medical-Interventions"><a href="#Humanoids-in-Hospitals-A-Technical-Study-of-Humanoid-Robot-Surrogates-for-Dexterous-Medical-Interventions" class="headerlink" title="Humanoids in Hospitals: A Technical Study of Humanoid Robot Surrogates   for Dexterous Medical Interventions"></a>Humanoids in Hospitals: A Technical Study of Humanoid Robot Surrogates   for Dexterous Medical Interventions</h2><p><strong>Authors:Soofiyan Atar, Xiao Liang, Calvin Joyce, Florian Richter, Wood Ricardo, Charles Goldberg, Preetham Suresh, Michael Yip</strong></p>
<p>The increasing demand for healthcare workers, driven by aging populations and labor shortages, presents a significant challenge for hospitals. Humanoid robots have the potential to alleviate these pressures by leveraging their human-like dexterity and adaptability to assist in medical procedures. This work conducted an exploratory study on the feasibility of humanoid robots performing direct clinical tasks through teleoperation. A bimanual teleoperation system was developed for the Unitree G1 Humanoid Robot, integrating high-fidelity pose tracking, custom grasping configurations, and an impedance controller to safely and precisely manipulate medical tools. The system is evaluated in seven diverse medical procedures, including physical examinations, emergency interventions, and precision needle tasks. Our results demonstrate that humanoid robots can successfully replicate critical aspects of human medical assessments and interventions, with promising quantitative performance in ventilation and ultrasound-guided tasks. However, challenges remain, including limitations in force output for procedures requiring high strength and sensor sensitivity issues affecting clinical accuracy. This study highlights the potential and current limitations of humanoid robots in hospital settings and lays the groundwork for future research on robotic healthcare integration. </p>
<blockquote>
<p>éšç€äººå£è€é¾„åŒ–å’ŒåŠ³åŠ¨åŠ›çŸ­ç¼ºçš„åŠ å‰§ï¼Œå¯¹åŒ»ç–—å·¥ä½œè€…çš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œè¿™ç»™åŒ»é™¢å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ç±»äººæœºå™¨äººå…·æœ‰ç¼“è§£è¿™äº›å‹åŠ›çš„æ½œåŠ›ï¼Œå®ƒä»¬å¯ä»¥åˆ©ç”¨ç±»äººçš„çµå·§æ€§å’Œé€‚åº”æ€§æ¥è¾…åŠ©åŒ»ç–—ç¨‹åºã€‚æœ¬ç ”ç©¶é€šè¿‡é¥æ“ä½œå¯¹äººç±»æœºå™¨äººæ‰§è¡Œç›´æ¥ä¸´åºŠä»»åŠ¡çš„å¯èƒ½æ€§è¿›è¡Œäº†æ¢ç´¢æ€§ç ”ç©¶ã€‚ä¸ºUnitree G1ç±»äººæœºå™¨äººå¼€å‘äº†ä¸€ä¸ªåŒæ‰‹é¥æ“ä½œç³»ç»Ÿï¼Œé›†æˆäº†é«˜ä¿çœŸå§¿æ€è¿½è¸ªã€è‡ªå®šä¹‰æŠ“æ¡é…ç½®å’Œé˜»æŠ—æ§åˆ¶å™¨ï¼Œä»¥å®‰å…¨ç²¾ç¡®åœ°æ“ä½œåŒ»ç–—å·¥å…·ã€‚è¯¥ç³»ç»Ÿåœ¨ä¸ƒç§ä¸åŒçš„åŒ»ç–—ç¨‹åºä¸­è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ä½“æ£€ã€ç´§æ€¥å¹²é¢„å’Œç²¾ç¡®é’ˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç±»äººæœºå™¨äººèƒ½å¤ŸæˆåŠŸå¤åˆ¶äººç±»åŒ»å­¦è¯„ä¼°å’Œå¹²é¢„çš„å…³é”®æ–¹é¢ï¼Œåœ¨é€šæ°”å’Œè¶…å£°å¼•å¯¼ä»»åŠ¡ä¸­çš„å®šé‡æ€§èƒ½å…·æœ‰å‰æ™¯ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŠ›é‡è¾“å‡ºæ–¹é¢çš„å±€é™ï¼Œå¯¹äºéœ€è¦é«˜å¼ºåº¦çš„ç¨‹åºä»¥åŠå½±å“ä¸´åºŠå‡†ç¡®æ€§çš„ä¼ æ„Ÿå™¨çµæ•åº¦é—®é¢˜ã€‚è¯¥ç ”ç©¶çªå‡ºäº†ç±»äººæœºå™¨äººåœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼Œä¸ºæœºå™¨äººåŒ»ç–—ä¿å¥æ•´åˆçš„æœªæ¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12725v2">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>äººå£è€é¾„åŒ–åŠåŠ³åŠ¨åŠ›çŸ­ç¼ºå¯¹åŒ»é™¢å¸¦æ¥å·¨å¤§æŒ‘æˆ˜ï¼Œäººç±»éœ€æ±‚åŒ»ç–—å·¥ä½œè€…çš„æ•°é‡ä¸æ–­å¢åŠ ã€‚äººå½¢æœºå™¨äººå…·æœ‰äººç±»èˆ¬çš„çµå·§æ€§å’Œé€‚åº”æ€§ï¼Œå¯ååŠ©æ‰§è¡ŒåŒ»ç–—ç¨‹åºï¼Œç¼“è§£å‹åŠ›ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†äººå½¢æœºå™¨äººé€šè¿‡é¥æ§æ“ä½œæ‰§è¡Œç›´æ¥ä¸´åºŠä»»åŠ¡çš„å¯èƒ½æ€§ã€‚ä¸ºUnitree G1äººå½¢æœºå™¨äººå¼€å‘äº†ä¸€ä¸ªåŒæ‰‹åŠ¨é¥æ§æ“ä½œç³»ç»Ÿï¼Œé›†æˆäº†é«˜ä¿çœŸå§¿æ€è¿½è¸ªã€è‡ªå®šä¹‰æŠ“æ¡é…ç½®å’Œé˜»æŠ—æ§åˆ¶å™¨ï¼Œå¯å®‰å…¨ç²¾ç¡®åœ°æ“ä½œåŒ»ç–—å·¥å…·ã€‚ç³»ç»Ÿç»è¿‡ä¸ƒç§ä¸åŒçš„åŒ»ç–—ç¨‹åºçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬ä½“æ£€ã€ç´§æ€¥å¹²é¢„å’Œç²¾ç¡®é’ˆå‰‚ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œäººå½¢æœºå™¨äººåœ¨å…³é”®çš„äººç±»åŒ»ç–—è¯„ä¼°å’Œå¹²é¢„æ–¹é¢è¡¨ç°æˆåŠŸï¼Œå¹¶åœ¨é€šæ°”å’Œè¶…å£°å¼•å¯¼ä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰å‰æ™¯çš„å®šé‡è¡¨ç°ã€‚ç„¶è€Œä»å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŠ›é‡è¾“å‡ºé™åˆ¶å’Œé«˜å¼ºåº¦ç¨‹åºæ‰€éœ€çš„ä¼ æ„Ÿå™¨çµæ•åº¦é—®é¢˜å½±å“ä¸´åºŠå‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†äººå½¢æœºå™¨äººåœ¨åŒ»é™¢ç¯å¢ƒä¸­çš„æ½œåŠ›å’Œå½“å‰å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æœºå™¨äººåŒ»ç–—ä¿å¥èåˆçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå£è€é¾„åŒ–å’ŒåŠ³åŠ¨åŠ›çŸ­ç¼ºä½¿åŒ»é™¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼ŒåŒ»ç–—å·¥ä½œè€…éœ€æ±‚å¢åŠ ã€‚</li>
<li>äººå½¢æœºå™¨äººå…·å¤‡è¾…åŠ©æ‰§è¡ŒåŒ»ç–—ç¨‹åºçš„æ½œåŠ›ï¼Œæ‹¥æœ‰ç±»ä¼¼äººç±»çš„çµå·§æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†äººå½¢æœºå™¨äººé€šè¿‡é¥æ§æ“ä½œæ‰§è¡Œç›´æ¥ä¸´åºŠä»»åŠ¡çš„å¯èƒ½æ€§ã€‚</li>
<li>ä¸ºUnitree G1äººå½¢æœºå™¨äººå¼€å‘çš„åŒæ‰‹åŠ¨é¥æ§æ“ä½œç³»ç»Ÿé›†æˆäº†é«˜ä¿çœŸå§¿æ€è¿½è¸ªã€è‡ªå®šä¹‰æŠ“æ¡å’Œé˜»æŠ—æ§åˆ¶ã€‚</li>
<li>ç³»ç»Ÿç»è¿‡å¤šç§åŒ»ç–—ç¨‹åºè¯„ä¼°ï¼ŒåŒ…æ‹¬ä½“æ£€ã€ç´§æ€¥å¹²é¢„å’Œé’ˆå‰‚ä»»åŠ¡ã€‚</li>
<li>äººå½¢æœºå™¨äººåœ¨åŒ»ç–—è¯„ä¼°å’Œå¹²é¢„æ–¹é¢å±•ç°æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨é€šæ°”å’Œè¶…å£°å¼•å¯¼ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4691f75d8798a79d68196c670d75c1ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b85bb91b62243fdfed6cd8cad08983a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-336cfbf6f821f202c29a060db9e7b1ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-faa4f8dc7808c994b0b12144e12aee9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-963f205da1444fb788e3c105daecdf17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57a6b4416400c3fe347a24d9f39225ad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedViT-V2-Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention"><a href="#MedViT-V2-Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention" class="headerlink" title="MedViT V2: Medical Image Classification with KAN-Integrated Transformers   and Dilated Neighborhood Attention"></a>MedViT V2: Medical Image Classification with KAN-Integrated Transformers   and Dilated Neighborhood Attention</h2><p><strong>Authors:Omid Nejati Manzari, Hojat Asgariandehkordi, Taha Koleilat, Yiming Xiao, Hassan Rivaz</strong></p>
<p>Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6% on MedMNIST, 5.8% on NonMNIST, and 13.4% on the MedMNIST-C benchmark. </p>
<blockquote>
<p>å·ç§¯ç½‘ç»œã€å˜å‹å™¨ã€æ··åˆæ¨¡å‹ä»¥åŠåŸºäºMambaçš„æ¶æ„åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†ä½¿ç”¨æ ‡è®°æ•°æ®å¯¹å¹²å‡€å›¾åƒè¿›è¡Œåˆ†ç±»è€Œè®¾è®¡çš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠæ•°æ®é€šå¸¸æ¶‰åŠå¤šä¸­å¿ƒç ”ç©¶ç‹¬ç‰¹çš„å›¾åƒè…èš€ï¼Œå¹¶ä¸”æºäºä¸åŒåˆ¶é€ å•†çš„æˆåƒè®¾å¤‡ä¹‹é—´çš„å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŒ»å­¦è§†è§‰å˜å‹å™¨ï¼ˆMedViTV2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé¦–æ¬¡å°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚èå…¥å˜å‹å™¨æ¶æ„ä¸­ï¼Œæ—¨åœ¨å®ç°é€šç”¨åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„KANå—ï¼Œä»¥å‡å°‘è®¡ç®—è´Ÿè½½å¹¶æé«˜åŸå§‹MedViTçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æŠµæ¶ˆæˆ‘ä»¬MedViTåœ¨æ‰©å¤§è§„æ¨¡æ—¶çš„è„†å¼±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„è†¨èƒ€é‚»åŸŸæ³¨æ„åŠ›ï¼ˆDiNAï¼‰ï¼Œè¿™æ˜¯å¯¹é«˜æ•ˆèåˆç‚¹ç§¯æ³¨æ„åŠ›æ ¸çš„é€‚åº”ï¼Œèƒ½å¤Ÿæ•è·å…¨å±€ä¸Šä¸‹æ–‡å¹¶æ‰©å¤§æ„Ÿå—é‡ï¼Œä»¥æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å¹¶è§£å†³ç‰¹å¾å´©æºƒé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§åˆ†å±‚æ··åˆç­–ç•¥ï¼Œä»¥æœ‰æ•ˆçš„æ–¹å¼å †å æˆ‘ä»¬çš„å±€éƒ¨ç‰¹å¾æ„ŸçŸ¥å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥å—ï¼Œè¿™å¯ä»¥å¹³è¡¡å±€éƒ¨å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥ä»¥æé«˜æ€§èƒ½ã€‚åœ¨17ä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†å’Œ12ä¸ªè¢«è…èš€çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedViTV2åœ¨29æ¬¡å®éªŒä¸­çš„27æ¬¡å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œå¹¶ä¸”è®¡ç®—å¤æ‚åº¦é™ä½ã€‚MedViTV2çš„è®¡ç®—æ•ˆç‡æ¯”å‰ä¸€ä¸ªç‰ˆæœ¬æé«˜äº†44ï¼…ï¼Œå¹¶ä¸”æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œåœ¨MedMNISTä¸Šæé«˜äº†4.6ï¼…ï¼Œåœ¨NonMNISTä¸Šæé«˜äº†5.8ï¼…ï¼Œåœ¨MedMNIST-CåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†13.4ï¼…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13693v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†æ–°æå‡ºçš„åŒ»ç–—å›¾åƒåˆ†ç±»æ¨¡å‹Medical Vision Transformerï¼ˆMedViTV2ï¼‰ï¼Œç»“åˆäº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚å’Œé«˜æ•ˆDilated Neighborhood Attentionï¼ˆDiNAï¼‰æ¨¡å—ã€‚è¯¥æ¨¡å‹å…·æœ‰å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œé«˜æ•ˆçš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤„ç†å­˜åœ¨å›¾åƒè…èš€é—®é¢˜çš„çœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®ã€‚åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMedViTV2åœ¨è®¡ç®—æ•ˆç‡æé«˜çš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬ï¼ŒMedViTV2è®¡ç®—æ•ˆç‡æé«˜äº†44%ï¼Œåœ¨MedMNISTã€NonMNISTå’ŒMedMNIST-CåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†4.6%ã€5.8%å’Œ13.4%ã€‚æ•´ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç°äº†æ›´é«˜æ•ˆæ€§èƒ½å’Œæ›´å…¨é¢å›¾åƒæ„ŸçŸ¥çš„æ–°åŒ»å­¦å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MedViTV2ç»“åˆäº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚è¿›å…¥transformeræ¶æ„ï¼Œæ—¨åœ¨å®ç°é€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚</li>
<li>MedViTV2ä½¿ç”¨é«˜æ•ˆçš„KANå—å‡å°‘è®¡ç®—è´Ÿè½½å¹¶å¢å¼ºå‡†ç¡®æ€§ã€‚</li>
<li>ä¸ºäº†è§£å†³æ¨¡å‹è§„æ¨¡æ‰©å¤§æ—¶çš„è„†å¼±æ€§é—®é¢˜ï¼Œå¼•å…¥äº†å¢å¼ºçš„Dilated Neighborhood Attentionï¼ˆDiNAï¼‰ã€‚</li>
<li>MedViTV2é‡‡ç”¨å±€éƒ¨ç‰¹å¾æ„ŸçŸ¥ä¸å…¨å±€ç‰¹å¾æ„ŸçŸ¥çš„å±‚æ¬¡æ··åˆç­–ç•¥ï¼Œå¹³è¡¡äº†å±€éƒ¨å’Œå…¨å±€ç‰¹å¾æ„ŸçŸ¥ä»¥æå‡æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-980c8ba31a085b167cffefbec85274c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65093cf0573e4d4e02293e38fa2fc882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34be78d7dcefba8d7265a1e2e265150f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-Optimization-Framework-for-Estimating-3D-Brain-Tumor-Infiltration"><a href="#A-Lightweight-Optimization-Framework-for-Estimating-3D-Brain-Tumor-Infiltration" class="headerlink" title="A Lightweight Optimization Framework for Estimating 3D Brain Tumor   Infiltration"></a>A Lightweight Optimization Framework for Estimating 3D Brain Tumor   Infiltration</h2><p><strong>Authors:Jonas Weidner, Michal Balcerak, Ivan Ezhov, AndrÃ© Datchev, Laurin Lux, Lucas Zimmer, Daniel Rueckert, BjÃ¶rn Menze, Benedikt Wiestler</strong></p>
<p>Glioblastoma, the most aggressive primary brain tumor, poses a severe clinical challenge due to its diffuse microscopic infiltration, which remains largely undetected on standard MRI. As a result, current radiotherapy planning employs a uniform 15 mm margin around the resection cavity, failing to capture patient-specific tumor spread. Tumor growth modeling offers a promising approach to reveal this hidden infiltration. However, methods based on partial differential equations or physics-informed neural networks tend to be computationally intensive or overly constrained, limiting their clinical adaptability to individual patients. In this work, we propose a lightweight, rapid, and robust optimization framework that estimates the 3D tumor concentration by fitting it to MRI tumor segmentations while enforcing a smooth concentration landscape. This approach achieves superior tumor recurrence prediction on 192 brain tumor patients across two public datasets, outperforming state-of-the-art baselines while reducing runtime from 30 minutes to less than one minute. Furthermore, we demonstrate the frameworkâ€™s versatility and adaptability by showing its ability to seamlessly integrate additional imaging modalities or physical constraints. </p>
<blockquote>
<p>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯æœ€å…·ä¾µè¢­æ€§çš„åŸå‘æ€§è„‘è‚¿ç˜¤ï¼Œç”±äºå…¶å¼¥æ¼«æ€§çš„å¾®è§‚æµ¸æ¶¦ï¼Œåœ¨æ ‡å‡†MRIä¸Šå¤§å¤šæ— æ³•æ£€æµ‹ï¼Œå¯¹ä¸´åºŠæ²»ç–—æ„æˆäº†ä¸¥å³»æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œç›®å‰çš„æ”¾ç–—è®¡åˆ’é‡‡ç”¨å›´ç»•åˆ‡é™¤è…”å®¤å‡åŒ€15æ¯«ç±³è¾¹ç•Œçš„æ–¹å¼ï¼Œæœªèƒ½æ•è·æ‚£è€…ç‰¹å®šçš„è‚¿ç˜¤æ‰©æ•£æƒ…å†µã€‚è‚¿ç˜¤å¢é•¿å»ºæ¨¡æ˜¯æ­ç¤ºè¿™ç§éšè”½æµ¸æ¶¦çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŸºäºåå¾®åˆ†æ–¹ç¨‹æˆ–ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„æ–¹æ³•å¾€å¾€è®¡ç®—é‡å¤§æˆ–å—åˆ°è¿‡åº¦çº¦æŸï¼Œé™åˆ¶äº†å…¶åœ¨é’ˆå¯¹ä¸ªåˆ«æ‚£è€…çš„ä¸´åºŠé€‚åº”æ€§æ–¹é¢çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»ä¾¿ã€å¿«é€Ÿã€ç¨³å¥çš„ä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ‹ŸåˆMRIè‚¿ç˜¤åˆ†å‰²æ¥ä¼°è®¡3Dè‚¿ç˜¤æµ“åº¦ï¼ŒåŒæ—¶å¼ºåˆ¶å®æ–½å¹³æ»‘çš„æµ“åº¦æ™¯è§‚ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†çš„192ä¾‹è„‘è‚¿ç˜¤æ‚£è€…èº«ä¸Šå®ç°äº†å“è¶Šçš„è‚¿ç˜¤å¤å‘é¢„æµ‹æ•ˆæœï¼Œä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æŠ€æœ¯ï¼Œå¹¶å°†è¿è¡Œæ—¶é—´ä»30åˆ†é’Ÿç¼©çŸ­åˆ°ä¸åˆ°1åˆ†é’Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å±•ç¤ºè¯¥æ¡†æ¶æ— ç¼é›†æˆå…¶ä»–æˆåƒæ¨¡å¼æˆ–ç‰©ç†çº¦æŸçš„èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13811v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å¿«é€Ÿã€ç¨³å¥çš„ä¼˜åŒ–æ¡†æ¶ï¼Œç”¨äºæ ¹æ®MRIè‚¿ç˜¤åˆ†å‰²ç»“æœä¼°è®¡ä¸‰ç»´è‚¿ç˜¤æµ“åº¦ï¼ŒåŒæ—¶å®ç°å¹³æ»‘æµ“åº¦æ™¯è§‚ã€‚è¯¥æ¡†æ¶å…·æœ‰é¢„æµ‹è‚¿ç˜¤å¤å‘çš„ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿé™ä½è¿è¡Œæ—¶é—´å¹¶å±•ç°å‡ºè‰¯å¥½é€‚åº”æ€§ã€‚è¿™ä¸€æ–¹æ³•åœ¨ä¸¤ç§å…¬å…±æ•°æ®é›†ä¸Šé’ˆå¯¹192ä¾‹è„‘è‚¿ç˜¤æ‚£è€…è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚åŒæ—¶å±•ç¤ºäº†æ¡†æ¶çš„çµæ´»æ€§å’Œé€šç”¨æ€§ï¼Œå¯ä»¥æ— ç¼é›†æˆå…¶ä»–æˆåƒæ¨¡å¼å’Œç‰©ç†çº¦æŸã€‚æ­¤ç ”ç©¶æˆæœä¸ºæ”¹è¿›æ”¾å°„æ²»ç–—è®¡åˆ’å’Œåº”å¯¹èƒ¶è´¨æ¯ç»†èƒç˜¤çš„ä¸´åºŠæŒ‘æˆ˜æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>èƒ¶è´¨æ¯ç»†èƒç˜¤å› å…¶å¾®å¦™çš„å¾®è§‚æµ¸æ¶¦ç°è±¡è€Œå¯¹ä¸´åºŠæ²»ç–—æ„æˆä¸¥å³»æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ”¾ç–—è®¡åˆ’ä¾èµ–äºç»Ÿä¸€çš„å®‰å…¨è¾¹ç•Œï¼Œæœªèƒ½å‡†ç¡®æ•æ‰æ‚£è€…ç‰¹å¼‚æ€§è‚¿ç˜¤æ‰©æ•£ã€‚</li>
<li>è‚¿ç˜¤å¢é•¿å»ºæ¨¡æˆä¸ºæ­ç¤ºéšè—æµ¸æ¶¦çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºçš„ä¼˜åŒ–æ¡†æ¶èƒ½è¿…é€Ÿä¼°è®¡ä¸‰ç»´è‚¿ç˜¤æµ“åº¦ï¼Œæé«˜è‚¿ç˜¤å¤å‘é¢„æµ‹å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥æ¡†æ¶é™ä½äº†è®¡ç®—æ—¶é—´å¹¶å±•ç¤ºäº†è‰¯å¥½é€‚åº”æ€§ï¼Œå¯¹æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c66a4534656ab8f6bb563ceda9b712ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acfc35aae5dc64444c84ed3d00328f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34a3f6209924f64d1cf667eb96eadeb2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Synomaly-Noise-and-Multi-Stage-Diffusion-A-Novel-Approach-for-Unsupervised-Anomaly-Detection-in-Medical-Images"><a href="#Synomaly-Noise-and-Multi-Stage-Diffusion-A-Novel-Approach-for-Unsupervised-Anomaly-Detection-in-Medical-Images" class="headerlink" title="Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for   Unsupervised Anomaly Detection in Medical Images"></a>Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for   Unsupervised Anomaly Detection in Medical Images</h2><p><strong>Authors:Yuan Bi, Lucie Huang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas, Nassir Navab, Zhongliang Jiang</strong></p>
<p>Anomaly detection in medical imaging plays a crucial role in identifying pathological regions across various imaging modalities, such as brain MRI, liver CT, and carotid ultrasound (US). However, training fully supervised segmentation models is often hindered by the scarcity of expert annotations and the complexity of diverse anatomical structures. To address these issues, we propose a novel unsupervised anomaly detection framework based on a diffusion model that incorporates a synthetic anomaly (Synomaly) noise function and a multi-stage diffusion process. Synomaly noise introduces synthetic anomalies into healthy images during training, allowing the model to effectively learn anomaly removal. The multi-stage diffusion process is introduced to progressively denoise images, preserving fine details while improving the quality of anomaly-free reconstructions. The generated high-fidelity counterfactual healthy images can further enhance the interpretability of the segmentation models, as well as provide a reliable baseline for evaluating the extent of anomalies and supporting clinical decision-making. Notably, the unsupervised anomaly detection model is trained purely on healthy images, eliminating the need for anomalous training samples and pixel-level annotations. We validate the proposed approach on brain MRI, liver CT datasets, and carotid US. The experimental results demonstrate that the proposed framework outperforms existing state-of-the-art unsupervised anomaly detection methods, achieving performance comparable to fully supervised segmentation models in the US dataset. Ablation studies further highlight the contributions of Synomaly noise and the multi-stage diffusion process in improving anomaly segmentation. These findings underscore the potential of our approach as a robust and annotation-efficient alternative for medical anomaly detection. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒä¸­çš„å¼‚å¸¸æ£€æµ‹åœ¨è¯†åˆ«å„ç§æˆåƒæ¨¡å¼ï¼ˆå¦‚è„‘éƒ¨MRIã€è‚è„CTå’Œé¢ˆåŠ¨è„‰è¶…å£°ï¼‰ä¸­çš„ç—…ç†åŒºåŸŸæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºä¸“å®¶æ ‡æ³¨çš„ç¨€ç¼ºå’Œå¤šç§è§£å‰–ç»“æ„çš„å¤æ‚æ€§ï¼Œå®Œå…¨ç›‘ç£çš„åˆ†å‰²æ¨¡å‹è®­ç»ƒå¸¸å¸¸å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åˆæˆå¼‚å¸¸ï¼ˆSynomalyï¼‰å™ªå£°å‡½æ•°å’Œå¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹ã€‚Synomalyå™ªå£°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘å¥åº·å›¾åƒä¸­å¼•å…¥åˆæˆå¼‚å¸¸ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ å¼‚å¸¸å»é™¤ã€‚å¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹è¢«ç”¨æ¥é€æ­¥å»å™ªå›¾åƒï¼Œåœ¨ä¿ç•™ç²¾ç»†ç»†èŠ‚çš„åŒæ—¶æé«˜æ— å¼‚å¸¸é‡å»ºçš„è´¨é‡ã€‚ç”Ÿæˆçš„é«˜ä¿çœŸåº¦åäº‹å®å¥åº·å›¾åƒå¯ä»¥è¿›ä¸€æ­¥å¢å¼ºåˆ†å‰²æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œå¹¶æä¾›å¯é çš„åŸºçº¿æ¥è¯„ä¼°å¼‚å¸¸çš„ä¸¥é‡ç¨‹åº¦å’Œæ”¯æŒä¸´åºŠå†³ç­–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ— ç›‘ç£çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ä»…ä½¿ç”¨å¥åº·å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å¼‚å¸¸è®­ç»ƒæ ·æœ¬å’Œåƒç´ çº§æ ‡æ³¨ã€‚æˆ‘ä»¬åœ¨è„‘éƒ¨MRIã€è‚è„CTæ•°æ®é›†å’Œé¢ˆåŠ¨è„‰è¶…å£°ä¸Šå¯¹æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œåœ¨è¶…å£°æ•°æ®é›†ä¸­å®ç°äº†ä¸å®Œå…¨ç›‘ç£çš„åˆ†å‰²æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†Synomalyå™ªå£°å’Œå¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹åœ¨æé«˜å¼‚å¸¸åˆ†å‰²ä¸­çš„è´¡çŒ®ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•ä½œä¸ºç¨³å¥ä¸”æ ‡æ³¨æ•ˆç‡é«˜çš„åŒ»ç–—å¼‚å¸¸æ£€æµ‹æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04004v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡åˆæˆå¼‚å¸¸å™ªå£°å‡½æ•°å’Œå¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹æ¥è§£å†³åŒ»å­¦æˆåƒä¸­çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— éœ€å¼‚å¸¸è®­ç»ƒæ ·æœ¬å’Œåƒç´ çº§æ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œå¯¹å¥åº·çš„å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæœ‰æ•ˆå­¦ä¹ å¼‚å¸¸ç§»é™¤ã€‚å¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹èƒ½å¤Ÿé€æ­¥å»å™ªï¼Œä¿ç•™ç»†èŠ‚ï¼Œæé«˜æ— å¼‚å¸¸é‡å»ºçš„è´¨é‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è„‘MRIã€è‚è„CTå’Œé¢ˆåŠ¨è„‰è¶…å£°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¯ä¸å…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ç›¸ï¿½ï¿½inæ¯”æ‹Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒä¸­çš„å¼‚å¸¸æ£€æµ‹å¯¹äºè¯†åˆ«ä¸åŒæˆåƒæ¨¡æ€ä¸‹çš„ç—…ç†åŒºåŸŸè‡³å…³é‡è¦ï¼Œå¦‚è„‘MRIã€è‚è„CTå’Œé¢ˆåŠ¨è„‰è¶…å£°ã€‚</li>
<li>æå‡ºçš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶åŸºäºæ‰©æ•£æ¨¡å‹ï¼ŒåŒ…å«åˆæˆå¼‚å¸¸å™ªå£°å‡½æ•°å’Œå¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>åˆæˆå¼‚å¸¸å™ªå£°ç”¨äºåœ¨å¥åº·å›¾åƒä¸­å¼•å…¥åˆæˆå¼‚å¸¸ï¼Œä½¿æ¨¡å‹æœ‰æ•ˆå­¦ä¹ å¼‚å¸¸ç§»é™¤ã€‚</li>
<li>å¤šé˜¶æ®µæ‰©æ•£è¿‡ç¨‹èƒ½å¤Ÿé€æ­¥å»å™ªï¼Œä¿ç•™ç»†èŠ‚ï¼Œæé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶è®­ç»ƒä»…éœ€è¦å¥åº·å›¾åƒï¼Œæ— éœ€å¼‚å¸¸è®­ç»ƒæ ·æœ¬å’Œåƒç´ çº§æ³¨é‡Šã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ¡†æ¶æ€§èƒ½ä¼˜è¶Šï¼Œå¯ä¸å…¨ç›‘ç£åˆ†å‰²æ¨¡å‹ç›¸æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b090621b0b6fa0ad01e6921c600fe72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-829a733622894156e3d6a71c9ca1e6af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4b317b17c0cd1cdca978588b9104ae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05af3fbfa68ed1c6bbe9c0435588cc46.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Understanding-implementation-pitfalls-of-distance-based-metrics-for-image-segmentation"><a href="#Understanding-implementation-pitfalls-of-distance-based-metrics-for-image-segmentation" class="headerlink" title="Understanding implementation pitfalls of distance-based metrics for   image segmentation"></a>Understanding implementation pitfalls of distance-based metrics for   image segmentation</h2><p><strong>Authors:Gasper Podobnik, Tomaz Vrtovec</strong></p>
<p>Distance-based metrics, such as the Hausdorff distance (HD), are widely used to validate segmentation performance in (bio)medical imaging. However, their implementation is complex, and critical differences across open-source tools remain largely unrecognized by the community. These discrepancies undermine benchmarking efforts, introduce bias in biomarker calculations, and potentially distort medical device development and clinical commissioning. In this study, we systematically dissect 11 open-source tools that implement distance-based metric computation by performing both a conceptual analysis of their computational steps and an empirical analysis on representative two- and three-dimensional image datasets. Alarmingly, we observed deviations in HD exceeding 100 mm and identified multiple statistically significant differences between tools - demonstrating that statistically significant improvements on the same set of segmentations can be achieved simply by selecting a particular implementation. These findings cast doubts on the validity of prior comparisons of results across studies without accounting for the differences in metric implementations. To address this, we provide practical recommendations for tool selection; additionally, our conceptual analysis informs about the future evolution of implementing open-source tools. </p>
<blockquote>
<p>åŸºäºè·ç¦»çš„åº¦é‡ï¼Œå¦‚è±ªæ–¯å¤šå¤«è·ç¦»ï¼ˆHDï¼‰ï¼Œåœ¨ï¼ˆç”Ÿç‰©ï¼‰åŒ»å­¦æˆåƒä¸­å¹¿æ³›ç”¨äºéªŒè¯åˆ†å‰²æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å®ç°è¿‡ç¨‹å¤æ‚ï¼Œå¼€æºå·¥å…·ä¹‹é—´çš„å…³é”®å·®å¼‚åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°ç¤¾åŒºçš„è®¤å¯ã€‚è¿™äº›å·®å¼‚ç ´åäº†åŸºå‡†æµ‹è¯•å·¥ä½œï¼Œå¼•å…¥äº†ç”Ÿç‰©æ ‡å¿—ç‰©è®¡ç®—ä¸­çš„åè§ï¼Œå¹¶å¯èƒ½æ‰­æ›²åŒ»ç–—è®¾å¤‡å¼€å‘å’Œä¸´åºŠå§”æ‰˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹å…¶è®¡ç®—æ­¥éª¤çš„æ¦‚å¿µåˆ†æå’Œå¯¹ä»£è¡¨æ€§äºŒç»´å’Œä¸‰ç»´å›¾åƒæ•°æ®é›†çš„ç»éªŒåˆ†æï¼Œç³»ç»Ÿåœ°åˆ†æäº†å®ç°åŸºäºè·ç¦»çš„åº¦é‡è®¡ç®—çš„11ä¸ªå¼€æºå·¥å…·ã€‚ä»¤äººæ‹…å¿§çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°HDè¶…è¿‡100æ¯«ç±³çš„åå·®ï¼Œå¹¶åœ¨å·¥å…·ä¹‹é—´å‘ç°äº†å¤šä¸ªç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å·®å¼‚â€”â€”è¿™è¡¨æ˜é€šè¿‡é€‰æ‹©ç‰¹å®šçš„å®ç°æ–¹å¼ï¼Œå¯ä»¥åœ¨åŒä¸€ç»„åˆ†å‰²ä¸Šå®ç°ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹å–„ã€‚è¿™äº›å‘ç°å¯¹æœªè€ƒè™‘åº¦é‡å®ç°å·®å¼‚çš„ç ”ç©¶ç»“æœä¹‹é—´çš„æ¯”è¾ƒçš„æœ‰æ•ˆæ€§æå‡ºäº†è´¨ç–‘ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†å·¥å…·é€‰æ‹©çš„å®ç”¨å»ºè®®ï¼›æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¦‚å¿µåˆ†æä¸ºå¼€æºå·¥å…·çš„æœªæ¥å®æ–½æ¼”å˜æä¾›äº†ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦è®¨è®ºäº†è·ç¦»åº¦é‡åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒé¢†åŸŸä¸­çš„ä½¿ç”¨ï¼Œç‰¹åˆ«æŒ‡å‡ºäº†è·ç¦»åº¦é‡å®æ–½ä¸­çš„å¤æ‚æ€§å’Œå¼€æºå·¥å…·é—´çš„å·®å¼‚ã€‚è¿™äº›å·®å¼‚å¯¹åŸºå‡†æµ‹è¯•ã€ç”Ÿç‰©æ ‡å¿—ç‰©è®¡ç®—ä»¥åŠåŒ»ç–—è®¾å¤‡ç ”å‘å’Œä¸´åºŠå§”æ‰˜äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚ç ”ç©¶é€šè¿‡åˆ†æå’Œå®è¯æµ‹è¯•äº†11ä¸ªå¼€æºå·¥å…·çš„è·ç¦»åº¦é‡è®¡ç®—åŠŸèƒ½ï¼Œå¹¶æŒ‡å‡ºåœ¨é€‰æ‹©å®ç°å·¥å…·æ—¶æ‰€å¸¦æ¥çš„å·¨å¤§å½±å“ï¼Œæå‡ºå·¥å…·é€‰æ‹©çš„å®ç”¨å»ºè®®ä»¥åŠå¯¹æœªæ¥å·¥å…·å®æ–½å‘å±•çš„çœ‹æ³•ã€‚å­˜åœ¨çš„é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´ä»¥å‰çš„ç»“æœå¯¹æ¯”å˜å¾—æ— æ•ˆï¼Œå½±å“ç›¸å…³ç ”ç©¶å’Œå†³ç­–çš„åˆ¶å®šã€‚è¿™äº›é—®é¢˜åœ¨åŒ»ç–—è¡Œä¸šç ”ç©¶ä¸­æ˜¯æå…·è­¦ç¤ºæ„ä¹‰çš„å…³æ³¨ç‚¹ã€‚é‡è¦çš„æ˜¯åœ¨å®è·µä¸­è¦é¿å…ä¸æ­£ç¡®çš„æŒ‡æ ‡è®¡ç®—å®æ–½å¼•èµ·çš„å¤±è¯¯ï¼Œä»¥å‡å°‘ä¸ç²¾ç¡®çš„ç ”ç©¶ç»“è®ºå¯¼è‡´çš„è¯¯è§£æˆ–æ½œåœ¨æŸå®³ã€‚åœ¨å®é™…çš„åŒ»ç–—ç¯å¢ƒä¸­ï¼Œåº”é‡‡å–é¢å¤–çš„æªæ–½æ¥ç¡®ä¿æ‰€é€‰æŒ‡æ ‡å®æ–½çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚è¿™é¡¹ç ”ç©¶çš„ç»“æœå’Œæ¨èå¯ä»¥ä¸ºç”Ÿç‰©åŒ»å­¦æˆåƒçš„ç§‘ç ”å’Œä»ä¸šè€…æä¾›å®è´µçš„æŒ‡å¯¼ã€‚æ€»ä½“æ¥è¯´ï¼Œç ”ç©¶äººå‘˜åº”å¯¹åœ¨åˆ©ç”¨è¿™äº›å¼€æºå·¥å…·è¿›è¡Œè®¡ç®—æ—¶çš„å¾®å¦™å·®å¼‚è¿›è¡Œæ›´å¤šæ¢è®¨ã€‚å®æ–½å·¥å…·é€‰æ‹©è‡³å…³é‡è¦ï¼Œè¿™ä¸ä»…å½±å“äº†æ¯”è¾ƒçš„å…¬å¹³æ€§ï¼Œä¹Ÿå¯èƒ½å¯¹æ•´ä¸ªç ”ç©¶ç»“æœé€ æˆåè§å’Œè¯¯å·®çš„å½±å“ã€‚ä¸ºäº†å®ç°æ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœï¼Œç ”ç©¶äººå‘˜éœ€è¦æ›´åŠ å…³æ³¨å·¥å…·é€‰æ‹©å’Œä½¿ç”¨çš„å‡†ç¡®æ€§é—®é¢˜ã€‚åŒæ—¶ï¼Œå¯¹äºæœªæ¥çš„ç ”ç©¶æ¥è¯´ï¼Œéœ€è¦å…³æ³¨å¦‚ä½•æ”¹è¿›ç°æœ‰å·¥å…·å¹¶å¼€å‘æ–°çš„å·¥å…·æ¥å‡å°‘è¿™ç§å·®å¼‚çš„å½±å“ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹å·¥ä½œæ—¨åœ¨æ¨åŠ¨å¯¹è·ç¦»åº¦é‡å®æ–½ä¸€è‡´æ€§çš„é‡è§†å’Œè®¨è®ºã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥å’Œç ”ç©¶çš„æ·±å…¥ï¼Œå¯¹è·ç¦»åº¦é‡çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§çš„è¦æ±‚ä¹Ÿè¶Šæ¥è¶Šé«˜ã€‚é€šè¿‡è¯†åˆ«ç°æœ‰å·¥å…·ä¹‹é—´çš„å·®å¼‚å¹¶åˆ¶å®šç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥æ¨åŠ¨ç”Ÿç‰©åŒ»å­¦æˆåƒé¢†åŸŸçš„è¿›æ­¥å’Œå‘å±•ã€‚è¿™å°†æœ‰åŠ©äºæ¨åŠ¨æ›´å‡†ç¡®ã€æ›´å¯é çš„ç”Ÿç‰©åŒ»å­¦æˆåƒç ”ç©¶çš„å¼€å±•ã€‚æ­¤å¤–ï¼Œè¿™ä¹Ÿå¼ºè°ƒäº†ç ”ç©¶äººå‘˜åœ¨è¿›è¡Œç›¸å…³å®éªŒæ—¶å¿…é¡»å¯†åˆ‡å…³æ³¨å·¥å…·é€‰æ‹©å’Œä½¿ç”¨çš„ç»†èŠ‚é—®é¢˜çš„é‡è¦æ€§ã€‚è¿™æ˜¯ä¿è¯å®éªŒç»“æœçš„å‡†ç¡®æ€§å’Œå¯é æ€§çš„å…³é”®æ­¥éª¤ä¹‹ä¸€ã€‚è¿™ä¹Ÿå‡¸æ˜¾äº†åœ¨ä¸´åºŠå†³ç­–å’Œç ”ç©¶ä¸­éœ€è¦è€ƒè™‘è·¨å¤šä¸ªå¼€æºå·¥å…·å®ç°çš„ä¸€è‡´æ€§è¦æ±‚çš„é‡è¦æ€§é—®é¢˜çš„é‡è¦æ€§ï¼Œè¿™å¯¹äºåŒ»å­¦å›¾åƒçš„å‡†ç¡®æ€§å’Œç²¾ç¡®åº¦è‡³å…³é‡è¦ã€‚<strong>Key Takeaways</strong>ï¼šå…³é”®ç‚¹æ¦‚è¿°ï¼š</p>
<p>ä¸€ã€è·ç¦»åº¦é‡åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„é‡è¦æ€§åŠå…¶å¹¿æ³›ç”¨é€”ï¼šç”¨äºéªŒè¯åˆ†å‰²æ€§èƒ½ï¼›ç„¶è€Œå­˜åœ¨å®ç°å¤æ‚æ€§å’Œå·¥å…·é—´å·®å¼‚çš„é—®é¢˜ã€‚<br>äºŒã€å¼€æºå·¥å…·é—´å·®å¼‚çš„å½±å“ï¼šå½±å“åŸºå‡†æµ‹è¯•ã€ç”Ÿç‰©æ ‡å¿—ç‰©è®¡ç®—ä»¥åŠåŒ»ç–—è®¾å¤‡ç ”å‘å’Œä¸´åºŠå§”æ‰˜ï¼›å¯èƒ½å¯¼è‡´å…ˆå‰ç ”ç©¶ç»“æœå¯¹æ¯”å¤±æ•ˆã€‚<br>ä¸‰ã€ç ”ç©¶æ–¹æ³•å’Œå‘ç°ï¼šåˆ†æå¹¶å®è¯æµ‹è¯•äº†å¤šä¸ªå¼€æºå·¥å…·çš„è·ç¦»åº¦é‡è®¡ç®—åŠŸèƒ½ï¼›è§‚å¯Ÿåˆ°è¶…è¿‡100æ¯«ç±³çš„Hausdorffè·ç¦»åå·®ï¼›ä¸åŒå·¥å…·é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚<br>å››ã€å…³äºé€‰æ‹©æ­£ç¡®å·¥å…·çš„å®ç”¨å»ºè®®ï¼šæå‡ºå…·ä½“å»ºè®®ä»¥è§£å†³å½“å‰çš„é—®é¢˜ï¼›å¼ºè°ƒäº†å·¥å…·é€‰æ‹©çš„é‡è¦æ€§ä»¥åŠå®ƒå¯¹ç»“æœçš„å½±å“ã€‚<br>äº”ã€æœªæ¥ç ”ç©¶æ–¹å‘ï¼šæ”¹è¿›ç°æœ‰å·¥å…·å’Œå¼€å‘æ–°å·¥å…·çš„éœ€æ±‚ï¼›ç¡®ä¿æœªæ¥ç ”ç©¶çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§å¯¹äºå‡†ç¡®è¯„ä¼°è‡³å…³é‡è¦ã€‚æ¦‚å¿µåˆ†æä¸ºæœªæ¥çš„å¼€æ”¾æºå·¥å…·å‘å±•æä¾›äº†æŒ‡å¯¼æ–¹å‘ã€‚<br>å…­ã€æé†’é‡è§†ä¸€è‡´æ€§è®¨è®ºçš„é‡è¦æ€§ï¼šå¼ºè°ƒæŠ€æœ¯è¿›æ­¥å’Œç ”ç©¶æ·±å…¥å¯¹è·ç¦»åº¦é‡å‡†ç¡®æ€§å’Œä¸€è‡´æ€§çš„è¦æ±‚è¶Šæ¥è¶Šé«˜ï¼›æ¨åŠ¨è¡Œä¸šè¿›æ­¥å’Œå‘å±•éœ€è¦å…³æ³¨è¿™ä¸ªé—®é¢˜ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98f706119d18c0bcf7125bfcc0164a27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8913769c6ed52bd35807674f47684d60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07f6f0a1dd406bf94cdf92c21feb6edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea8353355d877d1c30fd7380b7fd5ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d2381400f8b331440faa98aa01d71b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="An-Effective-UNet-Using-Feature-Interaction-and-Fusion-for-Organ-Segmentation-in-Medical-Image"><a href="#An-Effective-UNet-Using-Feature-Interaction-and-Fusion-for-Organ-Segmentation-in-Medical-Image" class="headerlink" title="An Effective UNet Using Feature Interaction and Fusion for Organ   Segmentation in Medical Image"></a>An Effective UNet Using Feature Interaction and Fusion for Organ   Segmentation in Medical Image</h2><p><strong>Authors:Xiaolin Gou, Chuanlin Liao, Jizhe Zhou, Fengshuo Ye, Yi Lin</strong></p>
<p>Nowadays, pre-trained encoders are widely used in medical image segmentation due to their strong capability in extracting rich and generalized feature representations. However, existing methods often fail to fully leverage these features, limiting segmentation performance. In this work, a novel U-shaped model is proposed to address the above issue, including three plug-and-play modules. A channel spatial interaction module is introduced to improve the quality of skip connection features by modeling inter-stage interactions between the encoder and decoder. A channel attention-based module integrating squeeze-and-excitation mechanisms with convolutional layers is employed in the decoder blocks to strengthen the representation of critical features while suppressing irrelevant ones. A multi-level fusion module is designed to aggregate multi-scale decoder features, improving spatial detail and consistency in the final prediction. Comprehensive experiments on the synapse multi-organ segmentation dataset and automated cardiac diagnosis challenge dataset demonstrate that the proposed model outperforms existing state-of-the-art methods, achieving the highest average Dice score of 86.05% and 92.58%, yielding improvements of 1.15% and 0.26%, respectively. In addition, the proposed model provides a balance between accuracy and computational complexity, with only 86.91 million parameters and 23.26 giga floating-point operations. </p>
<blockquote>
<p>ç›®å‰ï¼Œç”±äºé¢„è®­ç»ƒç¼–ç å™¨åœ¨æå–ä¸°å¯Œä¸”é€šç”¨çš„ç‰¹å¾è¡¨ç¤ºæ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œå®ƒå·²å¹¿æ³›åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨è¿™äº›ç‰¹å¾ï¼Œä»è€Œé™åˆ¶äº†åˆ†å‰²æ€§èƒ½ã€‚é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„Uå½¢æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ã€‚å¼•å…¥é€šé“ç©ºé—´äº¤äº’æ¨¡å—ï¼Œé€šè¿‡å»ºæ¨¡ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„é˜¶æ®µé—´äº¤äº’ï¼Œæé«˜è·³è¿‡è¿æ¥ç‰¹å¾çš„è´¨é‡ã€‚åœ¨è§£ç å™¨å—ä¸­é‡‡ç”¨ç»“åˆå‹ç¼©å’Œæ¿€å‘æœºåˆ¶ä¸å·ç§¯å±‚çš„é€šé“æ³¨æ„æ¨¡å—ï¼Œä»¥åŠ å¼ºå…³é”®ç‰¹å¾çš„è¡¨è¾¾å¹¶æŠ‘åˆ¶æ— å…³ç‰¹å¾ã€‚è®¾è®¡äº†ä¸€ç§å¤šå±‚æ¬¡èåˆæ¨¡å—ï¼Œç”¨äºèšåˆå¤šå°ºåº¦è§£ç å™¨ç‰¹å¾ï¼Œæé«˜æœ€ç»ˆé¢„æµ‹ä¸­çš„ç©ºé—´ç»†èŠ‚å’Œä¸€è‡´æ€§ã€‚åœ¨çªè§¦å¤šå™¨å®˜åˆ†å‰²æ•°æ®é›†å’Œè‡ªåŠ¨å¿ƒè„è¯Šæ–­æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå–å¾—äº†æœ€é«˜çš„å¹³å‡Diceå¾—åˆ†ï¼Œåˆ†åˆ«ä¸º86.05%å’Œ92.58%ï¼Œåˆ†åˆ«æé«˜äº†1.15%å’Œ0.26%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸è®¡ç®—å¤æ‚åº¦ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œä»…æœ‰8691ä¸‡ä¸ªå‚æ•°å’Œ23.26ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05324v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹Uå‹æ¨¡å‹ï¼Œç”¨äºè§£å†³ç°æœ‰åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨æå–çš„ç‰¹å¾çš„é—®é¢˜ã€‚æ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼Œé€šè¿‡å¼•å…¥é€šé“ç©ºé—´äº¤äº’æ¨¡å—ã€é€šé“æ³¨æ„åŠ›æ¨¡å—å’Œå¤šçº§èåˆæ¨¡å—ï¼Œæé«˜äº†ç‰¹å¾è¡¨ç¤ºçš„ä¸°å¯Œæ€§å’Œè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå…·æœ‰è¾ƒé«˜çš„å¹³å‡Diceå¾—åˆ†ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹å…·æœ‰è®¡ç®—å¤æ‚åº¦ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒç¼–ç å™¨åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å…¶ç‰¹å¾ã€‚</li>
<li>æ–°å‹Uå‹æ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼Œæ—¨åœ¨è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>å¼•å…¥é€šé“ç©ºé—´äº¤äº’æ¨¡å—ï¼Œæ”¹è¿›äº†è·³è¿‡è¿æ¥ç‰¹å¾çš„è´¨é‡ã€‚</li>
<li>é€šé“æ³¨æ„åŠ›æ¨¡å—ç»“åˆæŒ¤å‹å’Œæ¿€åŠ±æœºåˆ¶ï¼ŒåŠ å¼ºå…³é”®ç‰¹å¾çš„è¡¨ç¤ºï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸ç›¸å…³ç‰¹å¾ã€‚</li>
<li>å¤šçº§èåˆæ¨¡å—æ—¨åœ¨èåˆå¤šå°ºåº¦è§£ç å™¨ç‰¹å¾ï¼Œæé«˜æœ€ç»ˆé¢„æµ‹çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æœ€é«˜çš„å¹³å‡Diceå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e19f6af0ed9d6b535cf3033fc9d8e7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14f58dec5c2a1eae3b7fedae4419714e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3af91f0eb7cb8ef268f8742cf250a383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eec398edcbf2c6b4584dd2546aa2df6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfca07f2e00541cde4336faa0123757c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SMAFormer-Synergistic-Multi-Attention-Transformer-for-Medical-Image-Segmentation"><a href="#SMAFormer-Synergistic-Multi-Attention-Transformer-for-Medical-Image-Segmentation" class="headerlink" title="SMAFormer: Synergistic Multi-Attention Transformer for Medical Image   Segmentation"></a>SMAFormer: Synergistic Multi-Attention Transformer for Medical Image   Segmentation</h2><p><strong>Authors:Fuchen Zheng, Xuhang Chen, Weihuang Liu, Haolun Li, Yingtie Lei, Jiahui He, Chi-Man Pun, Shounjun Zhou</strong></p>
<p>In medical image segmentation, specialized computer vision techniques, notably transformers grounded in attention mechanisms and residual networks employing skip connections, have been instrumental in advancing performance. Nonetheless, previous models often falter when segmenting small, irregularly shaped tumors. To this end, we introduce SMAFormer, an efficient, Transformer-based architecture that fuses multiple attention mechanisms for enhanced segmentation of small tumors and organs. SMAFormer can capture both local and global features for medical image segmentation. The architecture comprises two pivotal components. First, a Synergistic Multi-Attention (SMA) Transformer block is proposed, which has the benefits of Pixel Attention, Channel Attention, and Spatial Attention for feature enrichment. Second, addressing the challenge of information loss incurred during attention mechanism transitions and feature fusion, we design a Feature Fusion Modulator. This module bolsters the integration between the channel and spatial attention by mitigating reshaping-induced information attrition. To evaluate our method, we conduct extensive experiments on various medical image segmentation tasks, including multi-organ, liver tumor, and bladder tumor segmentation, achieving state-of-the-art results. Code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/CXH-Research/SMAFormer">https://github.com/CXH-Research/SMAFormer</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œç‰¹æ®Šçš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨å’Œé‡‡ç”¨è·³è·ƒè¿æ¥çš„æ®‹å·®ç½‘ç»œï¼Œå¯¹äºæå‡æ€§èƒ½èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ¨¡å‹åœ¨åˆ†å‰²å°è€Œå½¢çŠ¶ä¸è§„åˆ™çš„è‚¿ç˜¤æ—¶ç»å¸¸ä¼šé‡åˆ°å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SMAFormerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åŸºäºå˜å‹å™¨çš„æ¶æ„ï¼Œèåˆäº†å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæ”¹è¿›å°è‚¿ç˜¤å’Œå™¨å®˜çš„åˆ†å‰²ã€‚SMAFormerå¯ä»¥æ•è·åŒ»å­¦å›¾åƒåˆ†å‰²çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚è¯¥æ¶æ„åŒ…å«ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæå‡ºäº†ååŒå¤šæ³¨æ„åŠ›ï¼ˆSMAï¼‰å˜å‹å™¨å—ï¼Œå…·æœ‰åƒç´ æ³¨æ„åŠ›ã€é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›çš„ä¼˜ç‚¹ï¼Œä»¥ä¸°å¯Œç‰¹å¾ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è§£å†³æ³¨æ„åŠ›æœºåˆ¶è½¬æ¢å’Œç‰¹å¾èåˆè¿‡ç¨‹ä¸­ä¿¡æ¯æŸå¤±çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹å¾èåˆè°ƒåˆ¶å™¨ã€‚è¯¥æ¨¡å—é€šè¿‡å‡è½»é‡å¡‘å¼•èµ·çš„ä¿¡æ¯è¡°å‡ï¼ŒåŠ å¼ºäº†é€šé“å’Œç©ºé—´æ³¨æ„åŠ›ä¹‹é—´çš„æ•´åˆã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å„ç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬å¤šå™¨å®˜ã€è‚è„è‚¿ç˜¤å’Œè†€èƒ±è‚¿ç˜¤åˆ†å‰²ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CXH-Research/SMAFormer%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CXH-Research/SMAFormerè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00346v4">PDF</a> Accepted by IEEE BIBM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹æ®Šè®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œå¦‚å˜å‹å™¨å’Œæ®‹å·®ç½‘ç»œï¼Œå·²ç»æ¨åŠ¨äº†æ€§èƒ½çš„æå‡ã€‚ç„¶è€Œï¼Œä»¥å¾€æ¨¡å‹åœ¨åˆ†å‰²å°å‹ã€å½¢çŠ¶ä¸è§„åˆ™çš„è‚¿ç˜¤æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SMAFormerï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åŸºäºå˜å‹å™¨çš„æ¶æ„ï¼Œèåˆäº†å¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºæ”¹è¿›å°å‹è‚¿ç˜¤å’Œå™¨å®˜çš„åˆ†å‰²ã€‚SMAFormerèƒ½å¤Ÿæ•æ‰åŒ»ç–—å›¾åƒåˆ†å‰²çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚å…¶æ¶æ„åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€æ˜¯ååŒå¤šæ³¨æ„åŠ›ï¼ˆSMAï¼‰å˜å‹å™¨å—ï¼Œå…·æœ‰åƒç´ æ³¨æ„åŠ›ã€é€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›çš„ä¼˜ç‚¹ï¼Œç”¨äºç‰¹å¾ä¸°å¯Œï¼›äºŒæ˜¯è§£å†³æ³¨æ„åŠ›æœºåˆ¶è½¬æ¢å’Œç‰¹å¾èåˆè¿‡ç¨‹ä¸­ä¿¡æ¯æŸå¤±çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å¾èåˆè°ƒåˆ¶å™¨ã€‚è¯¥æ¨¡å—é€šè¿‡ç¼“è§£é‡å¡‘å¼•èµ·çš„ä¿¡æ¯è¡°å‡ï¼ŒåŠ å¼ºäº†é€šé“å’Œç©ºé—´æ³¨æ„åŠ›ä¹‹é—´çš„é›†æˆã€‚åœ¨å¤šé¡¹åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬å¤šå™¨å®˜ã€è‚è„è‚¿ç˜¤å’Œè†€èƒ±è‚¿ç˜¤åˆ†å‰²ï¼Œå–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯æå‡äº†æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åˆ†å‰²å°å‹ã€å½¢çŠ¶ä¸è§„åˆ™çš„è‚¿ç˜¤æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥SMAFormeræ¶æ„ï¼Œèåˆå¤šç§æ³¨æ„åŠ›æœºåˆ¶æ”¹è¿›å°å‹è‚¿ç˜¤å’Œå™¨å®˜åˆ†å‰²ã€‚</li>
<li>SMAFormeråŒ…æ‹¬ååŒå¤šæ³¨æ„åŠ›ï¼ˆSMAï¼‰å˜å‹å™¨å—å’Œç‰¹å¾èåˆè°ƒåˆ¶å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>SMAå˜å‹å™¨å—å…·æœ‰åƒç´ ã€é€šé“å’Œç©ºé—´æ³¨æ„åŠ›çš„ä¼˜ç‚¹ã€‚</li>
<li>ç‰¹å¾èåˆè°ƒåˆ¶å™¨è§£å†³äº†ä¿¡æ¯æŸå¤±çš„é—®é¢˜ï¼Œå¢å¼ºäº†é€šé“ä¸ç©ºé—´æ³¨æ„åŠ›çš„é›†æˆã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šå–å¾—æœ€æ–°æˆæœï¼ŒåŒ…æ‹¬å¤šå™¨å®˜ã€è‚è„å’Œè†€èƒ±è‚¿ç˜¤çš„åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6f599618df9e29de52e756ca40d3df6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f24c3e2127a3318cd3a77d49288da59d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-665b56326451ef026c2753f3eabe9bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de89789ce56b5bcd333a9e53ee1a43ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6283d32b2744229b38adfb7888a46841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c146f6d09e87c96140d3dee586938d73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbd35af1843c621d6d41f7f25552ea06.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-04/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-89c4cfb4cd796460584be61d70764f53.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  SpA2V Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware   Video Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-04/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-119000b54380062648311e542c5c54f0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-04  ZS-VCOS Zero-Shot Video Camouflaged Object Segmentation By Optical Flow   and Open Vocabulary Object Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
