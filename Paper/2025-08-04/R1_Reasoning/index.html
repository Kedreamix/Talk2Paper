<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-04  Trustworthy Reasoning Evaluating and Enhancing Factual Accuracy in LLM   Intermediate Thought Processes">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-28cf34600faaa38049e50db9297885ea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-04-æ›´æ–°"><a href="#2025-08-04-æ›´æ–°" class="headerlink" title="2025-08-04 æ›´æ–°"></a>2025-08-04 æ›´æ–°</h1><h2 id="Trustworthy-Reasoning-Evaluating-and-Enhancing-Factual-Accuracy-in-LLM-Intermediate-Thought-Processes"><a href="#Trustworthy-Reasoning-Evaluating-and-Enhancing-Factual-Accuracy-in-LLM-Intermediate-Thought-Processes" class="headerlink" title="Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM   Intermediate Thought Processes"></a>Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM   Intermediate Thought Processes</h2><p><strong>Authors:Rui Jiao, Yue Zhang, Jinku Li</strong></p>
<p>We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy for Confidence Enhancement), a novel framework addressing a critical vulnerability in Large Language Models (LLMs): the prevalence of factual inaccuracies within intermediate reasoning steps despite correct final answers. This phenomenon poses substantial risks in high-stakes domains including healthcare, legal analysis, and scientific research, where erroneous yet confidently presented reasoning can mislead users into dangerous decisions. Our framework integrates three core components: (1) a specialized fact-checking classifier trained on counterfactually augmented data to detect subtle factual inconsistencies within reasoning chains; (2) a Group Relative Policy Optimization (GRPO) reinforcement learning approach that balances factuality, coherence, and structural correctness through multi-dimensional rewards; and (3) a mechanistic interpretability module examining how factuality improvements manifest in model activations during reasoning processes. Extensive evaluation across ten state-of-the-art models reveals concerning patterns: even leading models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual robustness (up to 49.90% improvement) while maintaining or improving performance on challenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore, our activation-level analysis provides actionable insights into how factual enhancements reshape reasoning trajectories within model architectures, establishing foundations for future training methodologies that explicitly target factual robustness through activation-guided optimization. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†RELIANCEï¼ˆåŸºäºé€»è¾‘å®Œæ•´æ€§å’Œå‡†ç¡®æ€§çš„æ¨ç†è¯„ä¼°ç½®ä¿¡å¢å¼ºæ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¸€ä¸ªå…³é”®æ¼æ´ï¼šå°½ç®¡æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ï¼Œä½†ä¸­é—´æ¨ç†æ­¥éª¤ä¸­å­˜åœ¨å¤§é‡äº‹å®é”™è¯¯ã€‚è¿™ç§ç°è±¡åœ¨é«˜é£é™©é¢†åŸŸï¼ˆåŒ…æ‹¬åŒ»ç–—ä¿å¥ã€æ³•å¾‹åˆ†æå’Œç§‘å­¦ç ”ç©¶ï¼‰ä¸­æ„æˆäº†é‡å¤§é£é™©ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé”™è¯¯ä½†è‡ªä¿¡çš„æ¨ç†å¯èƒ½ä¼šè¯¯å¯¼ç”¨æˆ·åšå‡ºå±é™©å†³ç­–ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªä¸“é—¨çš„äº‹å®æ£€æŸ¥åˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨åœ¨é€šè¿‡åäº‹å®å¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥æ£€æµ‹æ¨ç†é“¾ä¸­çš„ç»†å¾®äº‹å®ä¸ä¸€è‡´ï¼›ï¼ˆ2ï¼‰ä¸€ç§é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å¤šç»´å¥–åŠ±å¹³è¡¡äº‹å®æ€§ã€è¿è´¯æ€§å’Œç»“æ„æ­£ç¡®æ€§ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªæœºæ¢°è§£é‡Šæ¨¡å—ï¼Œç”¨äºæ£€æŸ¥äº‹å®æ”¹è¿›åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ¨¡å‹æ¿€æ´»ä¸­çš„è¡¨ç°ã€‚å¯¹åç§æœ€æ–°æŠ€æœ¯çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºå‡ºäº†ä»¤äººæ‹…å¿§çš„æ¨¡å¼ï¼šå³ä½¿æ˜¯é¢†å…ˆçš„æ¨¡å‹ï¼Œå¦‚Claude-3.7å’ŒGPT-o1ï¼Œå…¶æ¨ç†äº‹å®å‡†ç¡®ç‡ä¹Ÿä»…ä¸º81.93%å’Œ82.57%ã€‚RELIANCEåœ¨ä¿æŒæˆ–æé«˜Math-500ã€AIME-2024å’ŒGPQAç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†äº‹å®ç¨³å¥æ€§ï¼ˆæœ€å¤šå¯æé«˜49.90%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¿€æ´»å±‚é¢åˆ†ææä¾›äº†å…³äºäº‹å®æ”¹è¿›å¦‚ä½•æ”¹å˜æ¨¡å‹æ¶æ„å†…æ¨ç†è½¨è¿¹çš„å¯æ“ä½œè§è§£ï¼Œä¸ºæœªæ¥çš„è®­ç»ƒæ–¹æ³•è®ºå¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›æ–¹æ³•è®ºé€šè¿‡æ¿€æ´»å¼•å¯¼ä¼˜åŒ–æ˜ç¡®é’ˆå¯¹äº‹å®ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22940v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RELIANCEæ¡†æ¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å…³é”®æ¼æ´ï¼šå°½ç®¡æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ï¼Œä½†ä¸­é—´æ¨ç†æ­¥éª¤ä¸­æ™®éå­˜åœ¨äº‹å®é”™è¯¯ã€‚è¿™ç§ç°è±¡åœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹åˆ†æå’Œç§‘å­¦ç ”ç©¶ï¼‰å…·æœ‰é‡å¤§é£é™©ï¼Œå¯èƒ½ä¼šè¯¯å¯¼ç”¨æˆ·åšå‡ºå±é™©å†³ç­–ã€‚RELIANCEæ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ£€æµ‹æ¨ç†é“¾ä¸­å¾®å¦™äº‹å®ä¸ä¸€è‡´çš„ä¸“ç”¨äº‹å®æ£€æŸ¥åˆ†ç±»å™¨ã€å¹³è¡¡äº‹å®æ€§ã€è¿è´¯æ€§å’Œç»“æ„æ­£ç¡®æ€§çš„é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥åŠè€ƒå¯Ÿäº‹å®æ€§æ”¹è¿›åœ¨æ¨¡å‹æ¿€æ´»è¿‡ç¨‹ä¸­å¦‚ä½•ä½“ç°åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æœºæ¢°è§£é‡Šæ€§æ¨¡å—ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒRELIANCEæ¡†æ¶æ˜¾è‘—æé«˜äº†äº‹å®ç¨³å¥æ€§ï¼Œå¹¶åœ¨æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ä¿æŒäº†æ€§èƒ½æˆ–æœ‰æ‰€æé«˜ã€‚åŒæ—¶ï¼Œæ¿€æ´»å±‚é¢çš„åˆ†ææä¾›äº†å…³äºäº‹å®æ”¹è¿›å¦‚ä½•é‡å¡‘æ¨¡å‹æ¶æ„å†…æ¨ç†è½¨è¿¹çš„è§è§£ï¼Œä¸ºæœªæ¥çš„è®­ç»ƒæ–¹æ³•è®ºæä¾›äº†åŸºç¡€ï¼Œæ—¨åœ¨é€šè¿‡æ¿€æ´»å¼•å¯¼ä¼˜åŒ–æ˜ç¡®é’ˆå¯¹äº‹å®ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RELIANCEæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­äº‹å®å‡†ç¡®æ€§é—®é¢˜çš„æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶è§£å†³äº†LLMsåœ¨æ¨ç†æ­¥éª¤ä¸­å³ä½¿æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ä¹Ÿå­˜åœ¨äº‹å®é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>RELIANCEåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šäº‹å®æ£€æŸ¥åˆ†ç±»å™¨ã€GRPOå¼ºåŒ–å­¦ä¹ æ–¹æ³•å’Œæœºæ¢°è§£é‡Šæ€§æ¨¡å—ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒRELIANCEæé«˜äº†äº‹å®ç¨³å¥æ€§ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿæ”¹å–„é¢†å…ˆæ¨¡å‹ï¼ˆå¦‚Claude-3.7å’ŒGPT-o1ï¼‰çš„æ¨ç†äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>æ¿€æ´»å±‚é¢çš„åˆ†ææä¾›äº†å…³äºäº‹å®æ”¹è¿›å¦‚ä½•å½±å“æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„å…·ä½“è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66026833224392ebbcf3eb7a6d9a6020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d3ff2ecbc3ef12e94347541d33272aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dd61334f25d1898bd42650c97890854.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eaeb646e6c2e5e965fd9102adf92af0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CIMR-Contextualized-Iterative-Multimodal-Reasoning-for-Robust-Instruction-Following-in-LVLMs"><a href="#CIMR-Contextualized-Iterative-Multimodal-Reasoning-for-Robust-Instruction-Following-in-LVLMs" class="headerlink" title="CIMR: Contextualized Iterative Multimodal Reasoning for Robust   Instruction Following in LVLMs"></a>CIMR: Contextualized Iterative Multimodal Reasoning for Robust   Instruction Following in LVLMs</h2><p><strong>Authors:Yangshu Yuan, Heng Chen, Xinyi Jiang, Christian Ng, Kexin Qiu</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) has enhanced our ability to process and generate human language and visual information. However, these models often struggle with complex, multi-step multi-modal instructions that require logical reasoning, dynamic feedback integration, and iterative self-correction. To address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a novel framework that introduces a context-aware iterative reasoning and self-correction module. CIMR operates in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. A dynamic fusion module deeply integrates textual, visual, and contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy, outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5 (78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the efficacy of its iterative reasoning and self-correction capabilities in complex tasks. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬å¤„ç†å’Œç”Ÿæˆäººç±»è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„èƒ½åŠ›å¾—åˆ°äº†æå‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†éœ€è¦é€»è¾‘æ¨ç†ã€åŠ¨æ€åé¦ˆé›†æˆå’Œè¿­ä»£è‡ªæˆ‘çº æ­£çš„å¤æ‚å¤šæ­¥éª¤å¤šæ¨¡å¼æŒ‡ä»¤æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CIMRï¼šæƒ…å¢ƒåŒ–è¿­ä»£å¤šæ¨¡å¼æ¨ç†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼•å…¥ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¿­ä»£æ¨ç†å’Œè‡ªæˆ‘çº æ­£æ¨¡å—çš„æ–°å‹æ¡†æ¶ã€‚CIMRåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆå§‹æ¨ç†å’Œå“åº”ç”Ÿæˆï¼Œç„¶åä½¿ç”¨è§£æçš„å¤šæ¨¡å¼åé¦ˆè¿›è¡Œè¿­ä»£æ”¹è¿›ã€‚åŠ¨æ€èåˆæ¨¡å—åœ¨æ¯ä¸€æ­¥éƒ½æ·±åº¦æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œä¸Šä¸‹æ–‡ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰æ•°æ®é›†ä¸Šå¯¹LLaVA-1.5-7Bè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶åœ¨æ–°å¼•å…¥çš„å¤šæ¨¡å¼åŠ¨ä½œè§„åˆ’ï¼ˆMAPï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°äº†CIMRã€‚CIMRçš„å‡†ç¡®ç‡ä¸º91.5%ï¼Œè¶…è¿‡äº†GPT-4Vï¼ˆ89.2%ï¼‰ã€LLaVA-1.5ï¼ˆ78.5%ï¼‰ã€MiniGPT-4ï¼ˆ75.3%ï¼‰å’ŒInstructBLIPï¼ˆ72.8%ï¼‰ç­‰å…ˆè¿›æ¨¡å‹çš„è¡¨ç°ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¿­ä»£æ¨ç†å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22074v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¿«é€Ÿå‘å±•å¢å¼ºäº†æˆ‘ä»¬å¤„ç†å’Œç”Ÿæˆäººç±»è¯­è¨€å’Œè§†è§‰ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä½†å¯¹äºéœ€è¦é€»è¾‘ã€åŠ¨æ€åé¦ˆè¿­ä»£æ ¡æ­£çš„å¤šæ­¥éª¤å¤šæ¨¡å¼æŒ‡ä»¤å¸¸å¸¸å¤„ç†ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºCIMRï¼šè¯­å¢ƒåŒ–è¿­ä»£å¤šæ¨¡å¼æ¨ç†æ¡†æ¶ï¼Œå¼•å…¥è¯­å¢ƒæ„ŸçŸ¥è¿­ä»£æ¨ç†å’Œè‡ªæˆ‘æ ¡æ­£æ¨¡å—ã€‚CIMRåˆ†ä¸ºåˆå§‹æ¨ç†å’Œå“åº”ç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œç„¶åä½¿ç”¨è§£æçš„å¤šæ¨¡å¼åé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚åŠ¨æ€èåˆæ¨¡å—åœ¨æ¯ä¸€æ­¥æ·±å…¥æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œè¯­å¢ƒç‰¹å¾ã€‚æˆ‘ä»¬åœ¨Visual Instruction Tuningï¼ˆVITï¼‰æ•°æ®é›†ä¸Šå¾®è°ƒLLaVA-1.5-7Bæ¨¡å‹ï¼Œå¹¶åœ¨æ–°å¼•å…¥çš„å¤šæ¨¡å¼åŠ¨ä½œè§„åˆ’ï¼ˆMAPï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°CIMRã€‚ç»“æœæ˜¾ç¤ºï¼ŒCIMRè¾¾åˆ°91.5%çš„å‡†ç¡®æ€§ï¼Œä¼˜äºGPT-4Vï¼ˆ89.2%ï¼‰ã€LLaVA-1.5ï¼ˆ78.5%ï¼‰ã€MiniGPT-4ï¼ˆ75.3%ï¼‰å’ŒInstructBLIPï¼ˆ72.8%ï¼‰ï¼Œè¯æ˜äº†å…¶è¿­ä»£æ¨ç†å’Œè‡ªæˆ‘æ ¡æ­£èƒ½åŠ›åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†å’Œç”Ÿæˆè¯­è¨€åŠè§†è§‰ä¿¡æ¯æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨é¢å¯¹å¤æ‚ã€å¤šæ­¥éª¤å’Œå¤šæ¨¡å¼æŒ‡ä»¤æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œéœ€è¦é€»è¾‘æ¨ç†ã€åŠ¨æ€åé¦ˆé›†æˆå’Œè¿­ä»£è‡ªæˆ‘æ ¡æ­£ã€‚</li>
<li>CIMRæ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…å«åˆå§‹æ¨ç†å’Œå“åº”ç”Ÿæˆï¼Œç„¶åé€šè¿‡è§£æçš„å¤šæ¨¡å¼åé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
<li>CIMRæ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªåŠ¨æ€èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ·±å…¥æ•´åˆæ–‡æœ¬ã€è§†è§‰å’Œè¯­å¢ƒç‰¹å¾ã€‚</li>
<li>LLaVA-1.5-7Bæ¨¡å‹åœ¨Visual Instruction Tuningï¼ˆVITï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚</li>
<li>CIMRåœ¨Multi-modal Action Planningï¼ˆMAPï¼‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§è¾¾åˆ°91.5%ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œå¦‚GPT-4Vã€LLaVA-1.5ã€MiniGPT-4å’ŒInstructBLIPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0b1cfe005bc65caaa6f7fa96e611f991.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40303621e46dbc7cb6875b174bab979c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a124f5e48d75338c9b0db208afb9a02e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40e2148752719c0af824f9fb4421cccc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2d158833c73b1bea7b24ced9e143726.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MoL-RL-Distilling-Multi-Step-Environmental-Feedback-into-LLMs-for-Feedback-Independent-Reasoning"><a href="#MoL-RL-Distilling-Multi-Step-Environmental-Feedback-into-LLMs-for-Feedback-Independent-Reasoning" class="headerlink" title="MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for   Feedback-Independent Reasoning"></a>MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for   Feedback-Independent Reasoning</h2><p><strong>Authors:Kang Yang, Jingxue Chen, Qingkun Tang, Tianxiang Zhang, Qianchun Lu</strong></p>
<p>Large language models (LLMs) face significant challenges in effectively leveraging sequential environmental feedback (EF) signals, such as natural language evaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either convert EF into scalar rewards, losing rich contextual information, or employ refinement datasets, failing to exploit the multi-step and discrete nature of EF interactions. To address these limitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into LLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses) continual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and general language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-training to distill sequential EF interactions into single-step inferences. This synergy enables robust feedback-independent reasoning without relying on external feedback loops. Experimental results on mathematical reasoning (MATH-500, AIME24&#x2F;AIME25) and code generation (CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art performance with the Qwen3-8B model, while maintaining strong generalization across model scales (Qwen3-4B). This work provides a promising approach for leveraging multi-step textual feedback to enhance LLMsâ€™ reasoning capabilities in diverse domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœ‰æ•ˆåˆ©ç”¨åºåˆ—ç¯å¢ƒåé¦ˆï¼ˆEFï¼‰ä¿¡å·ï¼Œå¦‚è‡ªç„¶è¯­è¨€è¯„ä¼°ï¼Œè¿›è¡Œæ— åé¦ˆçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå°†ç¯å¢ƒåé¦ˆè½¬åŒ–ä¸ºæ ‡é‡å¥–åŠ±ï¼Œä»è€Œå¤±å»ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¦ä¹ˆä½¿ç”¨ç²¾ç‚¼æ•°æ®é›†ï¼Œæ— æ³•åˆ©ç”¨ç¯å¢ƒåé¦ˆäº¤äº’çš„å¤šæ­¥ç¦»æ•£æ€§è´¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MoL-RLè¿™ä¸€æ–°å‹è®­ç»ƒèŒƒå¼ï¼Œå®ƒé€šè¿‡åŒç›®æ ‡ä¼˜åŒ–æ¡†æ¶å°†å¤šæ­¥ç¯å¢ƒåé¦ˆä¿¡å·é›†æˆåˆ°LLMä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†MoLï¼ˆæ··åˆæŸå¤±ï¼‰æŒç»­è®­ç»ƒï¼Œå®ƒå°†é¢†åŸŸç‰¹å®šçš„ç¯å¢ƒåé¦ˆä¿¡å·ï¼ˆé€šè¿‡äº¤å‰ç†µæŸå¤±è¿›è¡Œä¼˜åŒ–ï¼‰å’Œä¸€èˆ¬è¯­è¨€èƒ½åŠ›ï¼ˆé€šè¿‡Kullback-Leibleræ•£åº¦è¿›è¡Œä¿ç•™ï¼‰è§£è€¦ï¼Œä»¥åŠä¸åŸºäºGRPOçš„åæœŸè®­ç»ƒç›¸ç»“åˆï¼Œå°†åºåˆ—ç¯å¢ƒåé¦ˆäº¤äº’è’¸é¦ä¸ºå•æ­¥æ¨æ–­ã€‚è¿™ç§ååŒä½œç”¨å®ç°äº†æ— éœ€å¤–éƒ¨åé¦ˆå¾ªç¯çš„ç¨³å¥çš„æ— åé¦ˆæ¨ç†ã€‚åœ¨æ•°å­¦æ¨ç†ï¼ˆMATH-500ã€AIME24&#x2F;AIME25ï¼‰å’Œä»£ç ç”Ÿæˆï¼ˆCodeAgent-Testï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMoL-RLåœ¨Qwen3-8Bæ¨¡å‹ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨¡å‹è§„æ¨¡ï¼ˆQwen3-4Bï¼‰ä¸Šä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªåˆ©ç”¨å¤šæ­¥æ–‡æœ¬åé¦ˆæ¥æå‡LLMåœ¨å¤šä¸ªé¢†åŸŸä¸­çš„æ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20278v1">PDF</a> 12pages,3figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨åºåˆ—ç¯å¢ƒåé¦ˆï¼ˆEFï¼‰ä¿¡å·è¿›è¡Œæ— åé¦ˆé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä¸¢å¤±ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æˆ–æœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ­¥ç¦»æ•£çš„ç¯å¢ƒåé¦ˆäº¤äº’ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºMoL-RLè¿™ä¸€æ–°å‹è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åŒé‡ç›®æ ‡ä¼˜åŒ–æ¡†æ¶å°†å¤šæ­¥ç¯å¢ƒåé¦ˆä¿¡å·èå…¥LLMsã€‚è¯¥æ–¹æ³•ç»“åˆMoLï¼ˆæ··åˆæŸå¤±ï¼‰æŒç»­è®­ç»ƒï¼Œå°†é¢†åŸŸç‰¹å®šçš„ç¯å¢ƒåé¦ˆä¿¡å·ï¼ˆé€šè¿‡äº¤å‰ç†µæŸå¤±ä¼˜åŒ–ï¼‰ä¸ä¸€èˆ¬è¯­è¨€èƒ½åŠ›ï¼ˆé€šè¿‡Kullback-Leibleræ•£åº¦ä¿æŒï¼‰è§£è€¦ï¼Œä¸åŸºäºGRPOçš„åæœŸè®­ç»ƒç›¸ç»“åˆï¼Œå°†åºåˆ—ç¯å¢ƒåé¦ˆäº’åŠ¨è’¸é¦ä¸ºå•æ­¥æ¨æ–­ã€‚è¿™ç§ååŒä½œç”¨å®ç°äº†æ— éœ€å¤–éƒ¨åé¦ˆå¾ªç¯çš„ç¨³å¥æ— åé¦ˆæ¨ç†ã€‚åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šï¼ŒMoL-RLåœ¨Qwen3-8Bæ¨¡å‹ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨¡å‹è§„æ¨¡ï¼ˆQwen3-4Bï¼‰ä¸Šä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªåˆ©ç”¨å¤šæ­¥æ–‡æœ¬åé¦ˆå¢å¼ºLLMsåœ¨å¤šä¸ªé¢†åŸŸæ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ©ç”¨åºåˆ—ç¯å¢ƒåé¦ˆï¼ˆEFï¼‰è¿›è¡Œæ¨ç†æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è½¬æ¢ç¯å¢ƒåé¦ˆæ—¶ä¸¢å¤±äº†ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ–æœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ­¥ç¦»æ•£çš„ç¯å¢ƒåé¦ˆäº¤äº’ã€‚</li>
<li>MoL-RLæ˜¯ä¸€ç§æ–°å‹è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡åŒé‡ç›®æ ‡ä¼˜åŒ–æ¡†æ¶é›†æˆå¤šæ­¥ç¯å¢ƒåé¦ˆä¿¡å·åˆ°LLMsä¸­ã€‚</li>
<li>MoL-RLç»“åˆMoLæŒç»­è®­ç»ƒå’ŒåŸºäºGRPOçš„åæœŸè®­ç»ƒï¼Œå®ç°æ— éœ€å¤–éƒ¨åé¦ˆçš„ç¨³å¥æ¨ç†ã€‚</li>
<li>MoL-RLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆã€‚</li>
<li>MoL-RLåœ¨Qwen3-8Bæ¨¡å‹ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆå¦‚Qwen3-4Bï¼‰ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d723df994e96b99bef8b1caca5672222.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-539a2f10b4f19cc7cf20b6b845558c3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e5a9fc2b35a767a36cba1cf16751a7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1641a46a411d380f3f3b5643f55763ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94544d645eb5dca8ce318dfe35ab896b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0721696aec4934363da42122df37697e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diversity-Enhanced-Reasoning-for-Subjective-Questions"><a href="#Diversity-Enhanced-Reasoning-for-Subjective-Questions" class="headerlink" title="Diversity-Enhanced Reasoning for Subjective Questions"></a>Diversity-Enhanced Reasoning for Subjective Questions</h2><p><strong>Authors:Yumeng Wang, Zhiyuan Fan, Jiayu Liu, Yi R. Fung</strong></p>
<p>Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1â€™s effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs. </p>
<blockquote>
<p>å…·æœ‰é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰èƒ½åŠ›çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰åœ¨å®¢è§‚ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†å’Œç¼–ç ï¼‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¸»è§‚é—®é¢˜ä¸Šçš„æ•ˆæœï¼Œç”±äºä¸åŒè§’åº¦å¯èƒ½äº§ç”Ÿä¸åŒç­”æ¡ˆï¼Œä»ç„¶å—é™äºåŒè´¨æ¨ç†çš„è¶‹åŠ¿ï¼Œè¿™ç§è¶‹åŠ¿æ˜¯ç”±ç›‘ç£å¾®è°ƒä¸­å¯¹å•ä¸€äº‹å®çœŸç›¸çš„ä¾èµ–ä»¥åŠå¼ºåŒ–å­¦ä¹ ä¸­çš„å¯éªŒè¯å¥–åŠ±æ‰€å¼•å…¥çš„ã€‚é€šè¿‡ç ”ç©¶å‘ç°ï¼Œå¢åŠ è§’è‰²è§†è§’å¯ä»¥æŒç»­æ”¹å–„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†MultiRole-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å¤šé‡è§’è‰²è§†è§’çš„å¤šæ ·æ€§å¢å¼ºæ¡†æ¶ï¼Œä»¥æé«˜ä¸»è§‚æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚MultiRole-R1çš„ç‰¹ç‚¹æ˜¯æ— ç›‘ç£æ•°æ®æ„å»ºç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆèå…¥å¤šç§è§’è‰²è§†è§’çš„æ¨ç†é“¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶é€šè¿‡å¥–åŠ±å¡‘å½¢ï¼Œå°†å¤šæ ·æ€§ä½œä¸ºé™¤å¯éªŒè¯å¥–åŠ±ä¹‹å¤–çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡ä¸“é—¨è®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œæˆ‘ä»¬æˆåŠŸä¿ƒè¿›äº†è§†è§’å¤šæ ·æ€§å’Œè¯æ±‡å¤šæ ·æ€§ï¼Œå‘ç°äº†æ¨ç†å¤šæ ·æ€§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æ­£ç›¸å…³å…³ç³»ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†MultiRole-R1åœ¨å¢å¼ºä¸»è§‚å’Œå®¢è§‚æ¨ç†çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œå±•ç¤ºäº†å¤šæ ·æ€§å¢å¼ºè®­ç»ƒåœ¨å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20187v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰åœ¨å®¢è§‚ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¦‚æ•°å­¦æ¨ç†å’Œç¼–ç¨‹ã€‚ç„¶è€Œï¼Œå¯¹äºå¯èƒ½æœ‰ä¸åŒç­”æ¡ˆçš„ä¸»è§‚é—®é¢˜ï¼ŒLRMå—é™äºå•ä¸€è§†è§’çš„åŒè´¨åŒ–æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMultiRole-R1ï¼Œä¸€ä¸ªå¼•å…¥å¤šé‡è§’è‰²è§†è§’çš„å¤šæ ·åŒ–å¢å¼ºæ¡†æ¶ï¼Œæé«˜ä¸»è§‚æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡é‡‡ç”¨æ— ç›‘ç£æ•°æ®æ„å»ºç®¡é“ç”ŸæˆåŒ…å«å¤šç§è§’è‰²è§†è§’çš„æ¨ç†é“¾ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸­çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œä»¥å¤šæ ·æ€§å’Œå¯éªŒè¯å¥–åŠ±ä¸ºä¿¡å·è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒMultiRole-R1èƒ½æœ‰æ•ˆæé«˜ä¸»è§‚å’Œå®¢è§‚æ¨ç†èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºå¤šæ ·æ€§å¢å¼ºè®­ç»ƒåœ¨LRMä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰åœ¨å®¢è§‚ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†ä¸»è§‚é—®é¢˜æ—¶å—é™äºå•ä¸€è§†è§’çš„åŒè´¨åŒ–æ¨ç†ã€‚</li>
<li>MultiRole-R1æ¡†æ¶å¼•å…¥å¤šé‡è§’è‰²è§†è§’ï¼Œæ—¨åœ¨æé«˜ä¸»è§‚æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>MultiRole-R1é‡‡ç”¨æ— ç›‘ç£æ•°æ®æ„å»ºç®¡é“ç”ŸæˆåŒ…å«å¤šç§è§’è‰²è§†è§’çš„æ¨ç†é“¾ã€‚</li>
<li>ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸­çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¥–åŠ±å¡‘é€ æœºåˆ¶ï¼Œä»¥å¤šæ ·æ€§å’Œå¯éªŒè¯å¥–åŠ±ä¸ºä¿¡å·è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜MultiRole-R1èƒ½æé«˜ä¸»è§‚å’Œå®¢è§‚æ¨ç†èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºå¤šæ ·æ€§å¢å¼ºè®­ç»ƒåœ¨LRMä¸­çš„æ½œåŠ›ã€‚</li>
<li>MultiRole-R1æ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–ç±»å‹çš„æ¨ç†ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20187">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b566663807f098e51428d819651433e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aebfa2a045bf2b2f94ff4ac534f7ed72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a3fc54e70ffd45124c0ac37cda7fb4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NIRS-An-Ontology-for-Non-Invasive-Respiratory-Support-in-Acute-Care"><a href="#NIRS-An-Ontology-for-Non-Invasive-Respiratory-Support-in-Acute-Care" class="headerlink" title="NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care"></a>NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care</h2><p><strong>Authors:Md Fantacher Islam, Jarrod Mosier, Vignesh Subbian</strong></p>
<p>Objective: Develop a Non Invasive Respiratory Support (NIRS) ontology to support knowledge representation in acute care settings.   Materials and Methods: We developed the NIRS ontology using Web Ontology Language (OWL) semantics and Protege to organize clinical concepts and relationships. To enable rule-based clinical reasoning beyond hierarchical structures, we added Semantic Web Rule Language (SWRL) rules. We evaluated logical reasoning by adding 17 hypothetical patient clinical scenarios. We used SPARQL queries and data from the Electronic Intensive Care Unit (eICU) Collaborative Research Database to retrieve and test targeted inferences.   Results: The ontology has 132 classes, 12 object properties, and 17 data properties across 882 axioms that establish concept relationships. To standardize clinical concepts, we added 350 annotations, including descriptive definitions based on controlled vocabularies. SPARQL queries successfully validated all test cases (rules) by retrieving appropriate patient outcomes, for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours due to acute respiratory failure may avoid endotracheal intubation.   Discussion: The NIRS ontology formally represents domain-specific concepts, including ventilation modalities, patient characteristics, therapy parameters, and outcomes. SPARQL query evaluations on clinical scenarios confirmed the ability of the ontology to support rule based reasoning and therapy recommendations, providing a foundation for consistent documentation practices, integration into clinical data models, and advanced analysis of NIRS outcomes.   Conclusion: We unified NIRS concepts into an ontological framework and demonstrated its applicability through the evaluation of hypothetical patient scenarios and alignment with standardized vocabularies. </p>
<blockquote>
<p>ç›®æ ‡ï¼šå¼€å‘éä¾µå…¥æ€§å‘¼å¸æ”¯æŒï¼ˆNIRSï¼‰æœ¬ä½“ï¼Œä»¥æ”¯æŒæ€¥æ€§æŠ¤ç†ç¯å¢ƒä¸­çš„çŸ¥è¯†è¡¨ç¤ºã€‚ææ–™ä¸æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨Webæœ¬ä½“è¯­è¨€ï¼ˆOWLï¼‰è¯­ä¹‰å’ŒProtegeæ¥ç»„ç»‡ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»ï¼Œå¼€å‘äº†NIRSæœ¬ä½“ã€‚ä¸ºäº†è¶…è¶Šå±‚æ¬¡ç»“æ„å®ç°åŸºäºè§„åˆ™çš„ä¸´åºŠæ¨ç†ï¼Œæˆ‘ä»¬æ·»åŠ äº†è¯­ä¹‰ç½‘è§„åˆ™è¯­è¨€ï¼ˆSWRLï¼‰è§„åˆ™ã€‚æˆ‘ä»¬é€šè¿‡æ·»åŠ 17ä¸ªå‡è®¾çš„æ‚£è€…ä¸´åºŠæƒ…æ™¯æ¥è¯„ä¼°é€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨SPARQLæŸ¥è¯¢å’Œç”µå­é‡ç—‡ç›‘æŠ¤å®¤ï¼ˆeICUï¼‰åä½œç ”ç©¶æ•°æ®åº“ä¸­çš„æ•°æ®æ¥æ£€ç´¢å’Œæµ‹è¯•ç›®æ ‡æ¨æ–­ã€‚ç»“æœï¼šè¯¥æœ¬ä½“æœ‰132ä¸ªç±»ï¼Œ12ä¸ªå¯¹è±¡å±æ€§å’Œ17ä¸ªæ•°æ®å±æ€§ï¼Œè·¨è¶Š882æ¡å»ºç«‹æ¦‚å¿µå…³ç³»çš„å…¬ç†ã€‚ä¸ºäº†æ ‡å‡†åŒ–ä¸´åºŠæ¦‚å¿µï¼Œæˆ‘ä»¬æ·»åŠ äº†350ä¸ªæ³¨é‡Šï¼ŒåŒ…æ‹¬åŸºäºå—æ§è¯æ±‡çš„æè¿°æ€§å®šä¹‰ã€‚SPARQLæŸ¥è¯¢æˆåŠŸéªŒè¯äº†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼ˆè§„åˆ™ï¼‰ï¼Œé€šè¿‡æ£€ç´¢é€‚å½“çš„ç—…äººç»“æœï¼Œä¾‹å¦‚ï¼Œç”±äºæ€¥æ€§å‘¼å¸è¡°ç«­æ¥å—é«˜é¢‘é¼»å¯¼ç®¡æ²»ç–—2å°æ—¶çš„ç—…äººå¯èƒ½ä¼šé¿å…æ°”ç®¡æ’ç®¡ã€‚è®¨è®ºï¼šNIRSæœ¬ä½“æ­£å¼è¡¨ç¤ºç‰¹å®šé¢†åŸŸçš„æ¦‚å¿µï¼ŒåŒ…æ‹¬é€šæ°”æ¨¡å¼ã€æ‚£è€…ç‰¹å¾ã€æ²»ç–—å‚æ•°å’Œç»“æœã€‚å¯¹ä¸´åºŠæƒ…æ™¯çš„SPARQLæŸ¥è¯¢è¯„ä¼°è¯å®äº†æœ¬ä½“æ”¯æŒåŸºäºè§„åˆ™æ¨ç†å’Œæ²»ç–—å»ºè®®çš„èƒ½åŠ›ï¼Œä¸ºä¸€è‡´çš„æ–‡æ¡£å®è·µã€æ•´åˆåˆ°ä¸´åºŠæ•°æ®æ¨¡å‹å’ŒNIRSç»“æœçš„å…ˆè¿›åˆ†ææä¾›äº†åŸºç¡€ã€‚ç»“è®ºï¼šæˆ‘ä»¬å°†NIRSæ¦‚å¿µç»Ÿä¸€åˆ°æœ¬ä½“æ¡†æ¶ä¸­ï¼Œå¹¶é€šè¿‡è¯„ä¼°å‡è®¾çš„æ‚£è€…æƒ…æ™¯ä»¥åŠä¸æ ‡å‡†åŒ–è¯æ±‡çš„å¯¹ç…§æ¥è¯æ˜äº†å…¶é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19992v1">PDF</a> Submitted to the Journal of the American Medical Informatics   Association (JAMIA)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§éä¾µå…¥æ€§å‘¼å¸æ”¯æŒï¼ˆNIRSï¼‰æœ¬ä½“ï¼Œä»¥æ”¯æŒæ€¥æ€§æŠ¤ç†ç¯å¢ƒä¸­çš„çŸ¥è¯†è¡¨ç¤ºã€‚é‡‡ç”¨Webæœ¬ä½“è¯­è¨€ï¼ˆOWLï¼‰è¯­ä¹‰å’ŒProtegeå·¥å…·å¯¹ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»è¿›è¡Œç»„ç»‡ï¼Œå¹¶æ·»åŠ è¯­ä¹‰Webè§„åˆ™è¯­è¨€ï¼ˆSWRLï¼‰è§„åˆ™ï¼Œå®ç°åŸºäºè§„åˆ™çš„æ¨ç†ã€‚é€šè¿‡æ·»åŠ 17ä¸ªå‡è®¾çš„æ‚£è€…ä¸´åºŠæƒ…æ™¯ï¼Œå¯¹é€»è¾‘æ¨ç†è¿›è¡Œäº†è¯„ä¼°ã€‚ä½¿ç”¨SPARQLæŸ¥è¯¢å’Œç”µå­é‡ç—‡ç›‘æŠ¤å®¤ï¼ˆeICUï¼‰åä½œç ”ç©¶æ•°æ®åº“ä¸­çš„æ•°æ®æ¥æ£€ç´¢å’Œæµ‹è¯•ç›®æ ‡æ¨ç†ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æœ¬ä½“å…·æœ‰132ä¸ªç±»ã€12ä¸ªå¯¹è±¡å±æ€§å’Œ17ä¸ªæ•°æ®å±æ€§ï¼Œå»ºç«‹äº†æ¦‚å¿µå…³ç³»ã€‚æ ‡å‡†åŒ–ä¸´åºŠæ¦‚å¿µæ—¶æ·»åŠ äº†350ä¸ªæ³¨é‡Šã€‚SPARQLæŸ¥è¯¢æˆåŠŸéªŒè¯äº†æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹ï¼ŒæˆåŠŸæ£€ç´¢äº†é€‚å½“çš„æ‚£è€…ç»“æœã€‚NIRSæœ¬ä½“å¯ä»¥æ”¯æŒåŸºäºè§„åˆ™çš„æ¨ç†å’Œç–—æ³•å»ºè®®ï¼Œä¸ºä¸€è‡´çš„æ–‡æ¡£å®è·µã€é›†æˆåˆ°ä¸´åºŠæ•°æ®æ¨¡å‹å’ŒNIRSç»“æœçš„å…ˆè¿›åˆ†ææä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘äº†éä¾µå…¥æ€§å‘¼å¸æ”¯æŒï¼ˆNIRSï¼‰æœ¬ä½“ï¼Œä»¥æ”¯æŒæ€¥æ€§æŠ¤ç†ç¯å¢ƒçš„çŸ¥è¯†è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨Webæœ¬ä½“è¯­è¨€ï¼ˆOWLï¼‰å’ŒProtegeå·¥å…·æ„å»ºNIRSæœ¬ä½“ï¼Œç»„ç»‡ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»ã€‚</li>
<li>æ·»åŠ Semantic Web Rule Language (SWRL)è§„åˆ™ï¼Œå®ç°æ›´å¤æ‚çš„ä¸´åºŠæ¨ç†ã€‚</li>
<li>é€šè¿‡æ·»åŠ 17ä¸ªå‡è®¾çš„æ‚£è€…ä¸´åºŠæƒ…æ™¯ï¼Œå¯¹é€»è¾‘æ¨ç†è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨SPARQLæŸ¥è¯¢éªŒè¯æœ¬ä½“çš„èƒ½åŠ›ï¼ŒæˆåŠŸæ£€ç´¢é€‚å½“çš„æ‚£è€…ç»“æœã€‚</li>
<li>NIRSæœ¬ä½“æœ‰åŠ©äºæ ‡å‡†åŒ–ä¸´åºŠæ¦‚å¿µï¼Œå¹¶ä¸ºä¸€è‡´çš„æ–‡æ¡£å®è·µã€é›†æˆåˆ°ä¸´åºŠæ•°æ®æ¨¡å‹å’ŒNIRSç»“æœåˆ†ææä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-717d26d49d06e3bf28cdf3fe36811c04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcc2ff2c69677fc7f2a30faef2d6746c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d514b35817018b13b942fbbc65a2220.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Agentic-Reinforced-Policy-Optimization"><a href="#Agentic-Reinforced-Policy-Optimization" class="headerlink" title="Agentic Reinforced Policy Optimization"></a>Agentic Reinforced Policy Optimization</h2><p><strong>Authors:Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou</strong></p>
<p>Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the modelsâ€™ intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPOâ€™s superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at <a target="_blank" rel="noopener" href="https://github.com/dongguanting/ARPO">https://github.com/dongguanting/ARPO</a> </p>
<blockquote>
<p>å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå•è½®æ¨ç†ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå…¶æœ‰æ•ˆæ€§ã€‚åœ¨çœŸå®çš„æ¨ç†åœºæ™¯ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ç»å¸¸å¯ä»¥åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥è¾…åŠ©ä»»åŠ¡è§£å†³è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ— æ³•å¹³è¡¡æ¨¡å‹å†…åœ¨çš„é•¿æœŸæ¨ç†èƒ½åŠ›åŠå…¶åœ¨å¤šè½®å·¥å…·äº¤äº’æ–¹é¢çš„ç†Ÿç»ƒåº¦ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘å¤šè½®å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†è®­ç»ƒçš„æ™ºèƒ½å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆARPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ™ºèƒ½å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚é€šè¿‡åˆæ­¥å®éªŒï¼Œæˆ‘ä»¬å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸å¤–éƒ¨å·¥å…·äº¤äº’åï¼Œå¾€å¾€ä¼šè¡¨ç°å‡ºé«˜åº¦ä¸ç¡®å®šçš„è¡Œä¸ºï¼Œè¡¨ç°ä¸ºç”Ÿæˆä»£å¸çš„ç†µåˆ†å¸ƒå¢åŠ ã€‚ARPOå—åˆ°è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œèå…¥äº†åŸºäºç†µçš„è‡ªé€‚åº”é‡‡æ ·æœºåˆ¶ï¼ŒåŠ¨æ€å¹³è¡¡å…¨å±€è½¨è¿¹é‡‡æ ·å’Œæ­¥éª¤çº§é‡‡æ ·ï¼Œä»è€Œä¿ƒè¿›åœ¨å·¥å…·ä½¿ç”¨åå‡ºç°é«˜åº¦ä¸ç¡®å®šæ€§çš„æ­¥éª¤çš„æ¢ç´¢ã€‚é€šè¿‡æ•´åˆä¼˜åŠ¿å½’å› ä¼°è®¡ï¼ŒARPOä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå†…åŒ–å·¥å…·ä½¿ç”¨äº¤äº’ä¸­çš„ä¼˜åŠ¿å·®å¼‚ã€‚æˆ‘ä»¬åœ¨è®¡ç®—æ¨ç†ã€çŸ¥è¯†æ¨ç†å’Œæ·±åº¦æœç´¢é¢†åŸŸçš„13ä¸ªæŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†ARPOç›¸å¯¹äºè½¨è¿¹çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒARPOä»…ä½¿ç”¨ç°æœ‰æ–¹æ³•æ‰€éœ€å·¥å…·ä½¿ç”¨é¢„ç®—çš„ä¸€åŠå°±å®ç°äº†æ€§èƒ½æå‡ï¼Œä¸ºä¸å®æ—¶åŠ¨æ€ç¯å¢ƒå¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æä¾›äº†å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/dongguanting/ARPO%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/dongguanting/ARPOä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19849v1">PDF</a> Working on progress</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå•è½®æ¨ç†ä»»åŠ¡æ–¹é¢å±•ç°å‡ºå…¶æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®çš„æ¨ç†åœºæ™¯ä¸­ï¼ŒLLMå¸¸å€ŸåŠ©å¤–éƒ¨å·¥å…·æ¥è¾…åŠ©å®Œæˆä»»åŠ¡ï¼Œä½†å½“å‰å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¹³è¡¡æ¨¡å‹å†…åœ¨çš„é•¿è¿œæ¨ç†èƒ½åŠ›ä¸å¤šè½®å·¥å…·äº¤äº’çš„ç†Ÿç»ƒåº¦æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¤šè½®LLMä»£ç†çš„è®­ç»ƒå®šåˆ¶çš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”Agenticå¼ºåŒ–æ”¿ç­–ä¼˜åŒ–ï¼ˆARPOï¼‰ã€‚å®éªŒè§‚å¯Ÿå‘ç°ï¼ŒLLMåœ¨ä¸å¤–éƒ¨å·¥å…·äº¤äº’åï¼Œç”Ÿæˆä»£å¸çš„ç†µåˆ†å¸ƒä¼šå¢åŠ ï¼Œè¡¨ç°å‡ºé«˜åº¦ä¸ç¡®å®šçš„è¡Œä¸ºã€‚ARPOåŸºäºæ­¤è§‚å¯Ÿï¼Œèå…¥åŸºäºç†µçš„è‡ªé€‚åº”å±•å¼€æœºåˆ¶ï¼ŒåŠ¨æ€å¹³è¡¡å…¨å±€è½¨è¿¹é‡‡æ ·å’Œæ­¥éª¤çº§é‡‡æ ·ï¼Œä»¥åœ¨å·¥å…·ä½¿ç”¨åçš„ä¸ç¡®å®šæ­¥éª¤ä¿ƒè¿›æ¢ç´¢ã€‚åŒæ—¶ï¼Œé€šè¿‡ä¼˜åŠ¿å½’å› ä¼°è®¡ï¼Œä½¿LLMå†…åŒ–å·¥å…·ä½¿ç”¨äº¤äº’ä¸­çš„ä¼˜åŠ¿å·®å¼‚ã€‚å®éªŒè¡¨æ˜ï¼ŒARPOåœ¨13é¡¹è®¡ç®—æ¨ç†ã€çŸ¥è¯†æ¨ç†å’Œæ·±åº¦æœç´¢é¢†åŸŸçš„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºè½¨è¿¹çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸”åœ¨ä»…ä½¿ç”¨ä¸€åŠå·¥å…·ä½¿ç”¨é¢„ç®—çš„æƒ…å†µä¸‹å³å®ç°å“è¶Šæ€§èƒ½ï¼Œä¸ºLLMä»£ç†ä¸å®æ—¶åŠ¨æ€ç¯å¢ƒå¯¹é½æä¾›å¯ä¼¸ç¼©è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å•è½®æ¨ç†ä»»åŠ¡ä¸­é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>LLMså€ŸåŠ©å¤–éƒ¨å·¥å…·è¿›è¡Œä»»åŠ¡è¾…åŠ©åœ¨çœŸå®æ¨ç†åœºæ™¯ä¸­å¾ˆå¸¸è§ã€‚</li>
<li>å½“å‰å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¹³è¡¡é•¿è¿œæ¨ç†ä¸å¤šè½®å·¥å…·äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ARPOæ˜¯ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¸“ä¸ºè®­ç»ƒå¤šè½®LLMä»£ç†è€Œè®¾è®¡ã€‚</li>
<li>LLMåœ¨ä¸å¤–éƒ¨å·¥å…·äº¤äº’åè¡¨ç°å‡ºé«˜åº¦ä¸ç¡®å®šçš„è¡Œä¸ºã€‚</li>
<li>ARPOé€šè¿‡èå…¥åŸºäºç†µçš„è‡ªé€‚åº”å±•å¼€æœºåˆ¶å’Œä¼˜åŠ¿å½’å› ä¼°è®¡ï¼Œæé«˜LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6465f9a9ac6b30780a7f4cbe7da1762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-498567795d5e5059c754e9ba9ac17093.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fdfff17979de0427f3ae2d16d1ec1cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94ed0de70352ea8454fd6ff11ea50e22.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Solve-ASP-Problems-Insights-from-a-Benchmarking-Study-Extended-Version"><a href="#Can-LLMs-Solve-ASP-Problems-Insights-from-a-Benchmarking-Study-Extended-Version" class="headerlink" title="Can LLMs Solve ASP Problems? Insights from a Benchmarking Study   (Extended Version)"></a>Can LLMs Solve ASP Problems? Insights from a Benchmarking Study   (Extended Version)</h2><p><strong>Authors:Lin Ren, Guohui Xiao, Guilin Qi, Yishuai Geng, Haohan Xue</strong></p>
<p>Answer Set Programming (ASP) is a powerful paradigm for non-monotonic reasoning. Recently, large language models (LLMs) have demonstrated promising capabilities in logical reasoning. Despite this potential, current evaluations of LLM capabilities in ASP are often limited. Existing works normally employ overly simplified ASP programs, do not support negation, disjunction, or multiple answer sets. Furthermore, there is a lack of benchmarks that introduce tasks specifically designed for ASP solving. To bridge this gap, we introduce ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks: ASP entailment, answer set verification, and answer set computation. Our extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs, including \emph{deepseek-r1}, \emph{o4-mini}, and \emph{gemini-2.5-flash-thinking}, perform relatively well on the first two simpler tasks, they struggle with answer set computation, which is the core of ASP solving. These findings offer insights into the current limitations of LLMs in ASP solving. This highlights the need for new approaches that integrate symbolic reasoning capabilities more effectively. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/HomuraT/ASPBench">https://github.com/HomuraT/ASPBench</a>. </p>
<blockquote>
<p>å›ç­”é›†ç¼–ç¨‹ï¼ˆASPï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„éå•è°ƒæ¨ç†èŒƒå¼ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€»è¾‘æ¨ç†æ–¹é¢è¡¨ç°å‡ºäº†æœ‰å‰é€”çš„èƒ½åŠ›ã€‚å°½ç®¡æ½œåŠ›å·¨å¤§ï¼Œä½†å½“å‰å¯¹LLMåœ¨ASPæ–¹é¢çš„èƒ½åŠ›è¯„ä¼°å¾€å¾€æœ‰é™ã€‚ç°æœ‰å·¥ä½œé€šå¸¸é‡‡ç”¨è¿‡äºç®€åŒ–çš„ASPç¨‹åºï¼Œä¸æ”¯æŒå¦å®šã€æå–æˆ–å¤šé‡ç­”æ¡ˆé›†ã€‚æ­¤å¤–ï¼Œç¼ºä¹ä¸“é—¨ç”¨äºASPè§£å†³çš„ä»»åŠ¡å¼•å…¥çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬ä»‹ç»äº†ASPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ASPåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªç‰¹å®šçš„ASPä»»åŠ¡ï¼šASPè•´æ¶µã€ç­”æ¡ˆé›†éªŒè¯å’Œç­”æ¡ˆé›†è®¡ç®—ã€‚æˆ‘ä»¬å¯¹ASPBenchçš„å¹¿æ³›è¯„ä¼°å‘ç°ï¼Œè™½ç„¶14ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬deepseek-r1ã€o4-miniå’Œgemini-2.5-flash-thinkingç­‰ï¼Œåœ¨å‰ä¸¤ä¸ªè¾ƒç®€å•çš„ä»»åŠ¡ä¸Šè¡¨ç°ç›¸å¯¹è¾ƒå¥½ï¼Œä½†åœ¨æ ¸å¿ƒä»»åŠ¡ç­”æ¡ˆé›†è®¡ç®—ä¸Šå´é‡åˆ°äº†å›°éš¾ï¼Œè¿™æ˜¯ASPè§£å†³çš„æ ¸å¿ƒã€‚è¿™äº›å‘ç°æ­ç¤ºäº†LLMåœ¨ASPè§£å†³æ–¹é¢çš„å½“å‰å±€é™æ€§ã€‚è¿™å¼ºè°ƒäº†éœ€è¦é‡‡ç”¨æ›´æœ‰æ•ˆåœ°æ•´åˆç¬¦å·æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HomuraT/ASPBench">https://github.com/HomuraT/ASPBench</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19749v1">PDF</a> Accepted for publication at the 22nd International Conference on   Principles of Knowledge Representation and Reasoning (KR 2025). The code is   available at <a target="_blank" rel="noopener" href="https://github.com/HomuraT/ASPBench">https://github.com/HomuraT/ASPBench</a></p>
<p><strong>Summary</strong></p>
<p>ASPï¼ˆAnswer Set Programmingï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„éå•è°ƒæ¨ç†èŒƒå¼ã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€»è¾‘æ¨ç†æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºLLMsåœ¨ASPæ–¹é¢çš„èƒ½åŠ›è¯„ä¼°å­˜åœ¨è¯¸å¤šé™åˆ¶ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•é€šå¸¸ä½¿ç”¨è¿‡äºç®€åŒ–çš„ASPç¨‹åºï¼Œä¸æ”¯æŒå¦å®šã€æå–æˆ–å¤šé‡ç­”æ¡ˆé›†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºASPBenchï¼Œä¸€ä¸ªå…¨é¢çš„ASPåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰ä¸ªç‰¹å®šä»»åŠ¡ï¼šASPè•´å«ã€ç­”æ¡ˆé›†éªŒè¯å’Œç­”æ¡ˆé›†è®¡ç®—ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡LLMsåœ¨å‰ä¸¤ä¸ªä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ ¸å¿ƒä»»åŠ¡ç­”æ¡ˆé›†è®¡ç®—ä¸­å´é‡åˆ°å›°éš¾ã€‚è¿™çªæ˜¾äº†LLMsåœ¨ASPæ±‚è§£æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†é›†æˆç¬¦å·æ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§å’Œè¿«åˆ‡æ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€»è¾‘ç†è§£æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ASPï¼ˆAnswer Set Programmingï¼‰é¢†åŸŸã€‚</li>
<li>å½“å‰å¯¹äºLLMsåœ¨ASPæ–¹é¢çš„èƒ½åŠ›è¯„ä¼°å­˜åœ¨è¯¸å¤šé™åˆ¶ï¼Œä¸»è¦é—®é¢˜åœ¨äºä½¿ç”¨çš„ASPç¨‹åºè¿‡äºç®€åŒ–ï¼Œç¼ºä¹è¶³å¤Ÿçš„å¤æ‚æ€§ã€‚</li>
<li>ASPBenchæ˜¯ä¸€ä¸ªå…¨æ–°çš„å…¨é¢ASPåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¸‰ä¸ªç‰¹å®šä»»åŠ¡ï¼šASPè•´å«ã€ç­”æ¡ˆé›†éªŒè¯å’Œç­”æ¡ˆé›†è®¡ç®—ã€‚</li>
<li>LLMsåœ¨å‰ä¸¤ä¸ªç›¸å¯¹ç®€å•çš„ASPä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ ¸å¿ƒä»»åŠ¡ç­”æ¡ˆé›†è®¡ç®—ä¸­é‡åˆ°å›°éš¾ã€‚</li>
<li>LLMsåœ¨ASPæ±‚è§£æ–¹é¢çš„å±€é™æ€§çªæ˜¾ï¼Œéœ€è¦æ–°çš„æ–¹æ³•é›†æˆç¬¦å·æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç›®å‰çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›LLMsåœ¨ASPé¢†åŸŸçš„å¿…è¦æ€§å’Œè¿«åˆ‡æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-174864976a9b7f6850b3184ce29ca7bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d18dbcfc77509c99f906ebf2c90b2f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07077a9bf86af906a277d256cb6fd33f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Bias-Analysis-for-Synthetic-Face-Detection-A-Case-Study-of-the-Impact-of-Facial-Attributes"><a href="#Bias-Analysis-for-Synthetic-Face-Detection-A-Case-Study-of-the-Impact-of-Facial-Attributes" class="headerlink" title="Bias Analysis for Synthetic Face Detection: A Case Study of the Impact   of Facial Attributes"></a>Bias Analysis for Synthetic Face Detection: A Case Study of the Impact   of Facial Attributes</h2><p><strong>Authors:Asmae Lamsaf, Lucia Cascone, Hugo ProenÃ§a, JoÃ£o Neves</strong></p>
<p>Bias analysis for synthetic face detection is bound to become a critical topic in the coming years. Although many detection models have been developed and several datasets have been released to reliably identify synthetic content, one crucial aspect has been largely overlooked: these models and training datasets can be biased, leading to failures in detection for certain demographic groups and raising significant social, legal, and ethical issues. In this work, we introduce an evaluation framework to contribute to the analysis of bias of synthetic face detectors with respect to several facial attributes. This framework exploits synthetic data generation, with evenly distributed attribute labels, for mitigating any skew in the data that could otherwise influence the outcomes of bias analysis. We build on the proposed framework to provide an extensive case study of the bias level of five state-of-the-art detectors in synthetic datasets with 25 controlled facial attributes. While the results confirm that, in general, synthetic face detectors are biased towards the presence&#x2F;absence of specific facial attributes, our study also sheds light on the origins of the observed bias through the analysis of the correlations with the balancing of facial attributes in the training sets of the detectors, and the analysis of detectors activation maps in image pairs with controlled attribute modifications. </p>
<blockquote>
<p>åˆæˆäººè„¸æ£€æµ‹ä¸­çš„åè§åˆ†æåœ¨æ¥ä¸‹æ¥å‡ å¹´ä¸­å¿…å°†æˆä¸ºå…³é”®è¯é¢˜ã€‚å°½ç®¡å·²ç»å¼€å‘äº†è®¸å¤šæ£€æµ‹æ¨¡å‹ï¼Œå¹¶ä¸”å·²ç»å‘å¸ƒäº†å‡ ä¸ªæ•°æ®é›†æ¥å¯é åœ°è¯†åˆ«åˆæˆå†…å®¹ï¼Œä½†ä¸€ä¸ªè‡³å…³é‡è¦çš„æ–¹é¢å´è¢«å¿½è§†äº†ï¼šè¿™äº›æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¯èƒ½å­˜åœ¨åè§ï¼Œå¯¼è‡´å¯¹æŸäº›äººç¾¤çš„æ£€æµ‹å¤±è´¥ï¼Œå¹¶å¼•å‘é‡å¤§çš„ç¤¾ä¼šã€æ³•å¾‹å’Œä¼¦ç†é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä»¥é’ˆå¯¹å‡ ä¸ªé¢éƒ¨å±æ€§åˆ†æåˆæˆäººè„¸æ£€æµ‹å™¨çš„åè§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åˆæˆæ•°æ®ç”Ÿæˆï¼Œé€šè¿‡å‡åŒ€åˆ†å¸ƒçš„å±æ€§æ ‡ç­¾ï¼Œå‡è½»æ•°æ®ä¸­çš„ä»»ä½•åå·®ï¼Œå¦åˆ™å¯èƒ½ä¼šå½±å“åè§åˆ†æçš„ç»“æœã€‚æˆ‘ä»¬åœ¨æ‰€æå‡ºçš„æ¡†æ¶åŸºç¡€ä¸Šï¼Œå¯¹äº”ä¸ªæœ€æ–°æ£€æµ‹å™¨åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„åè§ç¨‹åº¦è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œæ¶‰åŠ25ä¸ªå—æ§é¢éƒ¨å±æ€§ã€‚è™½ç„¶ç»“æœè¯å®ï¼Œåˆæˆäººè„¸æ£€æµ‹å™¨é€šå¸¸åå‘äºç‰¹å®šé¢éƒ¨å±æ€§çš„å­˜åœ¨ä¸å¦ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶è¿˜é€šè¿‡åˆ†æä¸è®­ç»ƒé›†é¢éƒ¨å±æ€§çš„å¹³è¡¡ä»¥åŠåˆ†æå›¾åƒå¯¹ä¸­å…·æœ‰å—æ§å±æ€§ä¿®æ”¹çš„æ£€æµ‹å™¨æ¿€æ´»å›¾ï¼Œæ­ç¤ºäº†æ‰€è§‚å¯Ÿåˆ°çš„åè§çš„åŸå› ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19705v2">PDF</a> Accepted at IJCB2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡ç« çš„ä¸»é¢˜æ˜¯åˆæˆé¢éƒ¨æ£€æµ‹ä¸­çš„åè§åˆ†æã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å·²ç»å¼€å‘äº†è®¸å¤šæ£€æµ‹æ¨¡å‹å¹¶å‘å¸ƒäº†æ•°æ®é›†æ¥å¯é åœ°è¯†åˆ«åˆæˆå†…å®¹ï¼Œä½†ä¸€ä¸ªå…³é”®æ–¹é¢å´è¢«å¿½è§†äº†ï¼šè¿™äº›æ¨¡å‹å’Œè®­ç»ƒæ•°æ®é›†å¯èƒ½å­˜åœ¨åè§ï¼Œå¯¼è‡´å¯¹æŸäº›äººç¾¤çš„æ£€æµ‹å¤±è´¥ï¼Œå¹¶å¼•å‘é‡å¤§çš„ç¤¾ä¼šã€æ³•å¾‹å’Œé“å¾·é—®é¢˜ã€‚å› æ­¤ï¼Œä½œè€…ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨åˆ†æåˆæˆé¢éƒ¨æ£€æµ‹å™¨ä¸é¢éƒ¨å±æ€§ç›¸å…³çš„åè§é—®é¢˜ã€‚é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆï¼Œä»¥å‡åŒ€åˆ†å¸ƒçš„é¢éƒ¨å±æ€§æ ‡ç­¾ï¼Œä»¥å‡å°‘å¯èƒ½å½±å“åè§åˆ†æç»“æœçš„æ•°æ®åæ–œã€‚è¯¥è¯„ä¼°æ¡†æ¶çš„ç ”ç©¶é’ˆå¯¹äº”ä¸ªæœ€å…ˆè¿›åˆæˆé¢éƒ¨æ£€æµ‹å™¨çš„åè§ç¨‹åº¦è¿›è¡Œäº†å…¨é¢çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå…±æ¶‰åŠäºŒåäº”ä¸ªå—æ§é¢éƒ¨å±æ€§ã€‚ç ”ç©¶ç»“æœç¡®è®¤äº†åˆæˆé¢éƒ¨æ£€æµ‹å™¨å¯¹ç‰¹å®šé¢éƒ¨å±æ€§çš„å­˜åœ¨ä¸å¦æœ‰åè§çš„é—®é¢˜ï¼Œå¹¶å¯¹åŸ¹è®­é›†å±æ€§å¹³è¡¡çš„åˆ†æä»¥åŠå›¾åƒé…å¯¹æ¿€æ´»å›¾çš„è¿›ä¸€æ­¥åˆ†ææ­ç¤ºäº†è§‚æµ‹åè§çš„æ¥æºã€‚æ–‡ç« è¡¨æ˜å½“å‰é¢ä¸´çš„æ˜¯ä¸€ä¸ªå¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡é¢†åŸŸã€‚åœ¨æœªæ¥å‡ å¹´ä¸­åˆæˆé¢éƒ¨æ£€æµ‹çš„åè§åˆ†æè‚¯å®šä¼šæˆä¸ºå¤‡å—å…³æ³¨çš„ä¸»é¢˜ã€‚æ­¤é¢†åŸŸçš„æˆåŠŸè§£å†³æ–¹æ¡ˆä¸ä»…èƒ½å¤Ÿæ­ç¤ºé—®é¢˜çœŸç›¸ä¸æ•°æ®é—®é¢˜äº§ç”ŸèƒŒåçš„æœºåˆ¶åŒæ—¶ä¹Ÿèƒ½å¤Ÿä¸ºç ”ç©¶äººå‘˜å’Œå·¥ä¸šä»ä¸šè€…æå‡ºæ˜ç¡®ã€å®¢è§‚çš„è¯„ä¼°å’Œå…¬æ­£å»ºæ¨¡æ¡†æ¶æå‡ºå¯èƒ½æ€§ä¸æ¨è¿›å®è·µå·¥ä½œçš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£åˆ—è¡¨ï¼š</p>
<ul>
<li>åˆæˆé¢éƒ¨æ£€æµ‹ä¸­çš„åè§åˆ†ææ˜¯æœªæ¥å‡ å¹´çš„å…³é”®è®®é¢˜ã€‚å°½ç®¡å·²æœ‰å¤§é‡æ£€æµ‹æ¨¡å‹å’Œæ•°æ®é›†ç”¨äºè¯†åˆ«åˆæˆå†…å®¹ï¼Œä½†å¾€å¾€å¿½è§†äº†å…¶å­˜åœ¨çš„åè§é—®é¢˜ã€‚è¿™å¯èƒ½å¯¼è‡´å¯¹æŸäº›äººç¾¤çš„æ£€æµ‹å¤±è´¥ï¼Œå¼•å‘ç¤¾ä¼šã€æ³•å¾‹å’Œé“å¾·é—®é¢˜ã€‚è¿™ä¸€é¢†åŸŸéœ€è¦æ›´å¤šçš„å…³æ³¨å’Œæ·±å…¥ç ”ç©¶ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨åˆ†æåˆæˆé¢éƒ¨æ£€æµ‹å™¨ä¸å¤šä¸ªé¢éƒ¨å±æ€§ç›¸å…³çš„åè§é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œé€šè¿‡å‡åŒ€åˆ†å¸ƒçš„é¢éƒ¨å±æ€§æ ‡ç­¾æ¥å‡å°‘æ•°æ®åæ–œçš„å½±å“ã€‚è¿™ä¸ºè¯„ä¼°åˆæˆé¢éƒ¨æ£€æµ‹å™¨çš„åè§ç¨‹åº¦æä¾›äº†æœ‰åŠ›çš„å·¥å…·ã€‚</li>
<li>ç ”ç©¶ç»“æœæ˜¾ç¤ºåˆæˆé¢éƒ¨æ£€æµ‹å™¨å¯¹ç‰¹å®šé¢éƒ¨å±æ€§çš„å­˜åœ¨ä¸å¦å­˜åœ¨åè§ã€‚è¿™æ­ç¤ºäº†å½“å‰åˆæˆé¢éƒ¨æ£€æµ‹å™¨åœ¨åº”å¯¹ä¸åŒé¢éƒ¨å±æ€§æ—¶çš„å±€é™æ€§ã€‚ä¸ºäº†æ›´å¥½åœ°åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œéœ€è¦å¼€å‘æ›´åŠ å…¬æ­£å’Œå‡†ç¡®çš„æ£€æµ‹æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21629f143147e03c7b1eca7f103e3413.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35a6acbf5edd9b28c6b076b7e9f894ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d80c5f84b5e9021f38c426286694c183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a478c5aaf9107355e7ae07416b39236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b2a168dafbc82f1a25f68fc9d6b3d4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb26d4680e7e16b7d05c6852147e4c9f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GEPA-Reflective-Prompt-Evolution-Can-Outperform-Reinforcement-Learning"><a href="#GEPA-Reflective-Prompt-Evolution-Can-Outperform-Reinforcement-Learning" class="headerlink" title="GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning"></a>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</h2><p><strong>Authors:Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab</strong></p>
<p>Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPAâ€™s design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä½†é€šå¸¸éœ€è¦æ•°åƒæ¬¡çš„æ»šåŠ¨æ‰èƒ½å­¦ä¹ æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸ç¨€ç–æ ‡é‡å¥–åŠ±äº§ç”Ÿçš„ç­–ç•¥æ¢¯åº¦ç›¸æ¯”ï¼Œè¯­è¨€çš„å¯è§£é‡Šæ€§é€šå¸¸å¯ä»¥ä¸ºLLMæä¾›æ›´ä¸°å¯Œçš„å­¦ä¹ åª’ä»‹ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†GEPAï¼ˆé—ä¼ -å¸•ç´¯æ‰˜ï¼‰ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºä¼˜åŒ–å™¨ï¼Œå®ƒé€šè¿‡è‡ªç„¶è¯­è¨€åæ€æ¥å½»åº•å­¦ä¹ é«˜çº§è§„åˆ™ã€‚å¯¹äºåŒ…å«ä¸€ç§æˆ–å¤šç§LLMæç¤ºçš„ä»»ä½•AIç³»ç»Ÿï¼ŒGEPAä¼šå¯¹ç³»ç»Ÿçº§çš„è½¨è¿¹ï¼ˆä¾‹å¦‚æ¨ç†ã€å·¥å…·è°ƒç”¨å’Œå·¥å…·è¾“å‡ºï¼‰è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ç”¨è‡ªç„¶è¯­è¨€å¯¹å…¶è¿›è¡Œåæ€ï¼Œä»¥è¯Šæ–­é—®é¢˜ã€æå‡ºå¹¶æµ‹è¯•æç¤ºæ›´æ–°ï¼Œå¹¶ç»“åˆå…¶è‡ªèº«å°è¯•çš„å¸•ç´¯æ‰˜å‰æ²¿çš„äº’è¡¥ç»éªŒã€‚ç”±äºGEPAçš„è®¾è®¡ï¼Œå®ƒé€šå¸¸å¯ä»¥å°†å‡ æ¬¡æ»šåŠ¨è½¬åŒ–ä¸ºå·¨å¤§çš„è´¨é‡æå‡ã€‚åœ¨å››é¡¹ä»»åŠ¡ä¸­ï¼ŒGEPAçš„å¹³å‡æ€§èƒ½æ¯”GRPOé«˜å‡º10%ï¼Œæœ€é«˜å¯è¾¾20%ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ»šåŠ¨æ¬¡æ•°å‡å°‘äº†é«˜è¾¾35å€ã€‚åœ¨ä¸¤é¡¹LLMä»»åŠ¡ä¸­ï¼ŒGEPAä¹Ÿä¼˜äºé¢†å…ˆçš„æç¤ºä¼˜åŒ–å™¨MIPROv2ï¼Œè¶…è¿‡10%ï¼Œå¹¶ä½œä¸ºä»£ç ä¼˜åŒ–çš„æ¨ç†æ—¶é—´æœç´¢ç­–ç•¥æ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19457v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€åé¦ˆçš„é—ä¼ -å¸•ç´¯æ‰˜ä¼˜åŒ–æ–¹æ³•ï¼ˆGEPAï¼‰ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚GEPAèƒ½å¤Ÿä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ é«˜å±‚æ¬¡çš„è§„åˆ™ï¼Œå¹¶é€šè¿‡è‡ªç„¶è¯­è¨€åé¦ˆè¿›è¡Œä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†LLMçš„æ€§èƒ½ã€‚åœ¨å››ä¸ªä»»åŠ¡ä¸­ï¼ŒGEPAç›¸è¾ƒäºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå‡å°‘äº†æ‰€éœ€çš„è®­ç»ƒå›åˆæ•°ã€‚åŒæ—¶ï¼ŒGEPAä¹Ÿåœ¨ä¸¤ä¸ªLLMä¸Šè¶…è¶Šäº†ç°æœ‰çš„æç¤ºä¼˜åŒ–å™¨MIPROv2ï¼Œå¹¶å±•ç°å‡ºä½œä¸ºä»£ç ä¼˜åŒ–çš„æ¨ç†æ—¶é—´æœç´¢ç­–ç•¥çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€åé¦ˆè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>GEPAæ˜¯ä¸€ç§åŸºäºé—ä¼ ç®—æ³•çš„æç¤ºä¼˜åŒ–å™¨ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œåæ€ï¼Œä»è¯•éªŒå’Œé”™è¯¯ä¸­å­¦ä¹ é«˜çº§è§„åˆ™ã€‚</li>
<li>GEPAèƒ½å¤Ÿå‡å°‘æ‰€éœ€çš„è®­ç»ƒå›åˆæ•°ï¼Œæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å››ä¸ªä»»åŠ¡ä¸­ï¼ŒGEPAç›¸è¾ƒäºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰å¹³å‡æé«˜äº†10%çš„æ€§èƒ½ï¼Œæœ€é«˜å¯è¾¾20%ã€‚</li>
<li>GEPAä½¿ç”¨çš„è®­ç»ƒå›åˆæ•°å‡å°‘äº†é«˜è¾¾35å€ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼ŒGEPAè¶…è¶Šäº†ç°æœ‰çš„æç¤ºä¼˜åŒ–å™¨MIPROv2ï¼Œå¹³å‡æ€§èƒ½æé«˜äº†è¶…è¿‡10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7ce3683b3f588bdfa3d180d317cf996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a627b572b5cc4cae758feb655efe7ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9e283c3f681f08308885b88756ed9a9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PurpCode-Reasoning-for-Safer-Code-Generation"><a href="#PurpCode-Reasoning-for-Safer-Code-Generation" class="headerlink" title="PurpCode: Reasoning for Safer Code Generation"></a>PurpCode: Reasoning for Safer Code Generation</h2><p><strong>Authors:Jiawei Liu, Nirav Diwan, Zhe Wang, Haoyu Zhai, Xiaona Zhou, Kiet A. Nguyen, Tianjiao Yu, Muntasir Wahed, Yinlin Deng, Hadjer Benkraouda, Yuxiang Wei, Lingming Zhang, Ismini Lourentzou, Gang Wang</strong></p>
<p>We introduce PurpCode, the first post-training recipe for training safe code reasoning models towards generating secure code and defending against malicious cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety and preserves model utility through diverse, multi-objective reward mechanisms. To empower the training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our alignment method decreases the model overrefusal rates in both general and cybersafety-specific scenarios, while preserving model utility in both code generation and common security knowledge. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºPurpCodeï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å®‰å…¨ä»£ç æ¨ç†æ¨¡å‹è®­ç»ƒçš„åç»­è®­ç»ƒé…æ–¹ï¼Œæ—¨åœ¨ç”Ÿæˆå®‰å…¨ä»£ç å¹¶é˜²å¾¡æ¶æ„ç½‘ç»œæ´»åŠ¨ã€‚PurpCodeåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ¨ç†æ¨¡å‹ï¼šï¼ˆiï¼‰è§„åˆ™å­¦ä¹ ï¼Œæ˜ç¡®æ•™å¯¼æ¨¡å‹å‚è€ƒç½‘ç»œå®‰å…¨è§„åˆ™æ¥ç”Ÿæˆæ— æ¼æ´ä»£ç å¹¶é¿å…ååŠ©æ¶æ„ç½‘ç»œæ´»åŠ¨ï¼›ï¼ˆiiï¼‰å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¤šæ ·åŒ–çš„å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ¨¡å‹çš„å®‰å…¨æ€§å¹¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚ä¸ºäº†ä½¿ç”¨å…¨é¢çš„ç½‘ç»œå®‰å…¨æ•°æ®èµ‹èƒ½è®­ç»ƒç®¡é“ï¼Œæˆ‘ä»¬è¿›è¡Œå†…éƒ¨çº¢é˜Ÿåˆä½œï¼ŒåŸºäºç°å®ä¸–ç•Œä»»åŠ¡åˆæˆå…¨é¢ä¸”é«˜è¦†ç›–ç‡çš„æç¤ºï¼Œä»¥è¯±å¯¼æ¨¡å‹ä¸­çš„ä¸å®‰å…¨ç½‘ç»œæ´»åŠ¨ã€‚åŸºäºPurpCodeï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºæ¨ç†çš„ç¼–ç æ¨¡å‹ï¼Œåä¸ºPurpCode-32Bï¼Œå®ƒå±•ç¤ºäº†æœ€å…ˆè¿›çš„ç½‘ç»œå®‰å…¨æ€§èƒ½ï¼Œè¶…è¶Šäº†å„ç§å‰æ²¿æ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ ¡å‡†æ–¹æ³•é™ä½äº†æ™®é€šå’Œç½‘ç»œå®‰å…¨ç‰¹å®šåœºæ™¯ä¸­çš„æ¨¡å‹æ‹’ç»ç‡ï¼ŒåŒæ—¶ä¿æŒäº†ä»£ç ç”Ÿæˆå’Œå¸¸è§å®‰å…¨çŸ¥è¯†çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19060v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>PurpCodeæ˜¯é¦–ä¸ªé’ˆå¯¹å®‰å…¨ä»£ç æ¨ç†æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆå®‰å…¨ä»£ç å¹¶é˜²å¾¡æ¶æ„ç½‘ç»œæ´»åŠ¨ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ¨¡å‹ï¼šè§„åˆ™å­¦ä¹ é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚è§„åˆ™å­¦ä¹ é˜¶æ®µæ•™æˆæ¨¡å‹å‚è€ƒç½‘ç»œå®‰å…¨è§„åˆ™ç”Ÿæˆæ— æ¼æ´ä»£ç å¹¶é¿å…ä¿ƒè¿›æ¶æ„ç½‘ç»œæ´»åŠ¨ï¼›å¼ºåŒ–å­¦ä¹ é˜¶æ®µé€šè¿‡å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ¨¡å‹çš„å®‰å…¨æ€§å’Œä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚é€šè¿‡å†…éƒ¨çº¢é˜Ÿæµ‹è¯•åˆæˆåŸºäºç°å®ä»»åŠ¡çš„ç»¼åˆä¸”é«˜è¦†ç›–ç‡çš„æç¤ºæ¥ä¸°å¯Œè®­ç»ƒç®¡é“çš„å®‰å…¨æ•°æ®ã€‚åŸºäºPurpCodeï¼Œå¼€å‘å‡ºåä¸ºPurpCode-32Bçš„æ¨ç†å‹ç¼–ç æ¨¡å‹ï¼Œå±•ç°äº†å…ˆè¿›çš„ç½‘ç»œå®‰å…¨æ€§èƒ½ï¼Œè¶…è¶Šäº†å„ç§å‰æ²¿æ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„å¯¹é½æ–¹æ³•é™ä½äº†æ¨¡å‹å’Œç½‘ç»œå®‰å…¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¿‡åº¦æ‹’ç»ç‡ï¼ŒåŒæ—¶ä¿æŒäº†ä»£ç ç”Ÿæˆå’Œå¸¸è§å®‰å…¨çŸ¥è¯†çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PurpCodeæ˜¯é¦–ä¸ªç”¨äºè®­ç»ƒå®‰å…¨ä»£ç æ¨ç†æ¨¡å‹çš„è®­ç»ƒæ–¹æ¡ˆã€‚</li>
<li>PurpCodeåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè§„åˆ™å­¦ä¹ é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>è§„åˆ™å­¦ä¹ é˜¶æ®µæ•™æˆæ¨¡å‹å‚è€ƒç½‘ç»œå®‰å…¨è§„åˆ™ï¼Œç”Ÿæˆæ— æ¼æ´ä»£ç å¹¶é¿å…æ¶æ„ç½‘ç»œæ´»åŠ¨ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ é˜¶æ®µä½¿ç”¨å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ä¼˜åŒ–æ¨¡å‹å®‰å…¨æ€§å’Œä¿æŒæ¨¡å‹å®ç”¨æ€§ã€‚</li>
<li>é€šè¿‡å†…éƒ¨çº¢é˜Ÿæµ‹è¯•åˆæˆç»¼åˆä¸”é«˜è¦†ç›–ç‡çš„æç¤ºæ¥ä¸°å¯Œè®­ç»ƒæ•°æ®ã€‚</li>
<li>PurpCode-32Bæ¨¡å‹å±•ç°äº†å…ˆè¿›çš„ç½‘ç»œå®‰å…¨æ€§èƒ½ï¼Œè¶…è¶Šå¤šç§å‰æ²¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6e51e033c7f493e0141c1130324cbb85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bacc6a7c2b2180c15cd671720cf2c207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835ecda665639ed96b4e3ed9c75f5558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75a921ae275cc3cbf2e224df7a0ede1a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AQuilt-Weaving-Logic-and-Self-Inspection-into-Low-Cost-High-Relevance-Data-Synthesis-for-Specialist-LLMs"><a href="#AQuilt-Weaving-Logic-and-Self-Inspection-into-Low-Cost-High-Relevance-Data-Synthesis-for-Specialist-LLMs" class="headerlink" title="AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance   Data Synthesis for Specialist LLMs"></a>AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance   Data Synthesis for Specialist LLMs</h2><p><strong>Authors:Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang</strong></p>
<p>Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at <a target="_blank" rel="noopener" href="https://github.com/Krueske/AQuilt">https://github.com/Krueske/AQuilt</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸå´å¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ•°æ®åˆæˆæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®æ•è·ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ï¼Œä»è€Œå¾—åˆ°æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆå­˜åœ¨æ€§èƒ½ä¸Šçš„å±€é™ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AQuiltæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»ç›¸åº”çš„æ— æ ‡ç­¾æ•°æ®ä¸­æ„å»ºç”¨äºä»»ä½•ä¸“ä¸šé¢†åŸŸçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ŒåŒ…æ‹¬ç­”æ¡ˆã€é—®é¢˜ã€æ— æ ‡ç­¾æ•°æ®ã€æ£€æŸ¥ã€é€»è¾‘å’Œä»»åŠ¡ç±»å‹ã€‚é€šè¿‡èå…¥é€»è¾‘å’Œæ£€æŸ¥ï¼Œæˆ‘ä»¬é¼“åŠ±æ¨ç†è¿‡ç¨‹å’Œè‡ªæˆ‘æ£€æŸ¥ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯å®šåˆ¶çš„ä»»åŠ¡æŒ‡ä»¤èƒ½å¤Ÿå®ç°ä»»ä½•ä»»åŠ¡çš„é«˜è´¨é‡æ•°æ®ç”Ÿæˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«703kä¸ªç¤ºä¾‹çš„æ•°æ®é›†æ¥è®­ç»ƒå¼ºå¤§çš„æ•°æ®åˆæˆæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAQuiltä¸DeepSeek-V3ç›¸å½“ï¼ŒåŒæ—¶ä»…ä½¿ç”¨äº†17%çš„ç”Ÿäº§æˆæœ¬ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„æ•°æ®ä¸ä¸‹æ¸¸ä»»åŠ¡çš„ç›¸å…³æ€§æ›´é«˜ã€‚ç›¸å…³æºä»£ç ã€æ¨¡å‹å’Œè„šæœ¬å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Krueske/AQuilt%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Krueske/AQuiltè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18584v1">PDF</a> 32 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºAQuiltçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä»å¯¹åº”çš„æ— æ ‡ç­¾æ•°æ®ä¸­æ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œé€šè¿‡é€»è¾‘å’Œæ£€æŸ¥æ¥é¼“åŠ±æ¨ç†è¿‡ç¨‹å’Œè‡ªæˆ‘æ£€æŸ¥ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAQuiltæ¡†æ¶å¯ä»¥ç”Ÿæˆå®šåˆ¶çš„ä»»åŠ¡æŒ‡ä»¤æ¥é€‚åº”ä»»ä½•ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸DeepSeek-V3ç›¸æ¯”ï¼ŒAQuiltçš„æ•°æ®é›†èƒ½é«˜æ•ˆäº§å‡ºè´¨é‡æ›´é«˜çš„æ•°æ®ä¸”ç”Ÿäº§æˆæœ¬æ›´ä½ã€‚æœ€åï¼Œå…¬å¼€äº†æºä»£ç ã€æ¨¡å‹å’Œè„šæœ¬ä»¥ä¾›ä¸‹è½½ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸå­˜åœ¨æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡æ•°æ®åˆæˆæ–¹æ³•æ¥æé«˜æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½è¡¨ç°ã€‚è¿™äº›æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€æ€§èƒ½å±€é™æˆ–è·¨ä»»åŠ¡æ³›åŒ–ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>AQuiltæ¡†æ¶èƒ½å¤Ÿä»å¯¹åº”çš„æ— æ ‡ç­¾æ•°æ®ä¸­æ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚å®ƒç»“åˆäº†é€»è¾‘å’Œæ£€æŸ¥ç¯èŠ‚ï¼Œé¼“åŠ±æ¨ç†è¿‡ç¨‹å’Œè‡ªæˆ‘æ£€æŸ¥ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>AQuiltæ¡†æ¶æ”¯æŒç”Ÿæˆå®šåˆ¶çš„ä»»åŠ¡æŒ‡ä»¤æ¥é€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†AQuiltæ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ï¼Œå®ƒä¸ä»…èƒ½å¤Ÿå®ç°é«˜æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶ç”Ÿäº§æˆæœ¬çš„æ•ˆç‡ä¹Ÿè¾ƒé«˜ã€‚</li>
<li>AQuiltå…¬å¼€äº†æºä»£ç ã€æ¨¡å‹å’Œè„šæœ¬ä¾›å…¬ä¼—ä¸‹è½½å’Œä½¿ç”¨ã€‚è¿™å¯¹äºç ”ç©¶è€…å’Œå¼€å‘è€…æ¥è¯´æ˜¯æå…¶æ–¹ä¾¿çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78806b5f0bf56d9f12b5642810a41601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90fe1a92dad6b6e2a32f2367d5e6d0ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4467f67d9d91fc74408c5334e8749783.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a38dcb6ab9dc0f9a5282ab15fa686e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea1f28de9cb3cae330639840cf7ab7cc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DiagR1-A-Vision-Language-Model-Trained-via-Reinforcement-Learning-for-Digestive-Pathology-Diagnosis"><a href="#DiagR1-A-Vision-Language-Model-Trained-via-Reinforcement-Learning-for-Digestive-Pathology-Diagnosis" class="headerlink" title="DiagR1: A Vision-Language Model Trained via Reinforcement Learning for   Digestive Pathology Diagnosis"></a>DiagR1: A Vision-Language Model Trained via Reinforcement Learning for   Digestive Pathology Diagnosis</h2><p><strong>Authors:Minxi Ouyang, Lianghui Zhu, Yaqing Bao, Qiang Huang, Jingli Ouyang, Tian Guan, Xitong Ling, Jiawen Li, Song Duan, Wenbin Dai, Li Zheng, Xuemei Zhang, Yonghong He</strong></p>
<p>Multimodal large models have shown great potential in automating pathology image analysis. However, current multimodal models for gastrointestinal pathology are constrained by both data quality and reasoning transparency: pervasive noise and incomplete annotations in public datasets predispose vision language models to factual hallucinations when generating diagnostic text, while the absence of explicit intermediate reasoning chains renders the outputs difficult to audit and thus less trustworthy in clinical practice. To address these issues, we construct a large scale gastrointestinal pathology dataset containing both microscopic descriptions and diagnostic conclusions, and propose a prompt argumentation strategy that incorporates lesion classification and anatomical site information. This design guides the model to better capture image specific features and maintain semantic consistency in generation. Furthermore, we employ a post training pipeline that combines supervised fine tuning with Group Relative Policy Optimization (GRPO) to improve reasoning quality and output structure. Experimental results on real world pathology report generation tasks demonstrate that our approach significantly outperforms state of the art open source and proprietary baselines in terms of generation quality, structural completeness, and clinical relevance. Our solution outperforms state of the art models with 18.7% higher clinical relevance, 32.4% improved structural completeness, and 41.2% fewer diagnostic errors, demonstrating superior accuracy and clinical utility compared to existing solutions. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹åœ¨è‡ªåŠ¨ç—…ç†å­¦å›¾åƒåˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰ç”¨äºèƒƒè‚ é“ç—…ç†å­¦çš„å¤šæ¨¡æ€æ¨¡å‹å—åˆ°æ•°æ®è´¨é‡å’Œæ¨ç†é€æ˜åº¦çš„é™åˆ¶ï¼šå…¬å…±æ•°æ®é›†ä¸­æ™®éå­˜åœ¨çš„å™ªå£°å’Œæ ‡æ³¨ä¸å®Œæ•´ä¼šä½¿è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆè¯Šæ–­æ–‡æœ¬æ—¶å€¾å‘äºäº§ç”Ÿå¹»è§‰ï¼Œè€Œç¼ºä¹æ˜ç¡®çš„ä¸­é—´æ¨ç†é“¾ä½¿å¾—è¾“å‡ºéš¾ä»¥å®¡æ ¸ï¼Œå› æ­¤åœ¨ä¸´åºŠå®è·µä¸­å¯ä¿¡åº¦è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡èƒƒè‚ é“ç—…ç†å­¦æ•°æ®é›†ï¼ŒåŒ…å«å¾®è§‚æè¿°å’Œè¯Šæ–­ç»“è®ºï¼Œå¹¶æå‡ºäº†ä¸€ç§èåˆç—…å˜åˆ†ç±»å’Œè§£å‰–éƒ¨ä½ä¿¡æ¯çš„æç¤ºè®ºè¯ç­–ç•¥ã€‚è¿™ç§è®¾è®¡å¯ä»¥å¼•å¯¼æ¨¡å‹æ›´å¥½åœ°æ•æ‰å›¾åƒç‰¹å®šç‰¹å¾ï¼Œå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åè®­ç»ƒç®¡é“ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥æé«˜æ¨ç†è´¨é‡å’Œè¾“å‡ºç»“æ„ã€‚åœ¨ç°å®ä¸–ç•Œç—…ç†å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ã€ç»“æ„å®Œæ•´æ€§å’Œä¸´åºŠç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°å¼€æºå’Œä¸“æœ‰åŸºçº¿ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆä¼˜äºç°æœ‰æœ€å…ˆè¿›æ¨¡å‹ï¼Œä¸´åºŠç›¸å…³æ€§æé«˜18.7%ï¼Œç»“æ„å®Œæ•´æ€§æé«˜32.4%ï¼Œè¯Šæ–­é”™è¯¯å‡å°‘41.2%ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18433v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ç—…ç†å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œå¹¶é’ˆå¯¹èƒƒè‚ é“ç—…ç†é¢†åŸŸçš„ç‰¹å®šæŒ‘æˆ˜æå‡ºè§£å†³æ–¹æ¡ˆã€‚ä¸ºè§£å†³å…¬å…±æ•°æ®é›†ä¸­çš„å™ªå£°å’Œæ ‡æ³¨ä¸å®Œæ•´é—®é¢˜ï¼Œå›¢é˜Ÿæ„å»ºäº†å¤§è§„æ¨¡èƒƒè‚ é“ç—…ç†æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç»“åˆç—…å˜åˆ†ç±»å’Œè§£å‰–éƒ¨ä½ä¿¡æ¯çš„æç¤ºè®ºè¯ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒä¸é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åè®­ç»ƒç®¡é“ï¼Œä»¥æé«˜æ¨ç†è´¨é‡å’Œè¾“å‡ºç»“æ„ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°å®ä¸–ç•Œç—…ç†æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰å¼€æºå’Œä¸“æœ‰åŸºçº¿ï¼Œä¸´åºŠç›¸å…³æ€§æé«˜18.7%ï¼Œç»“æ„å®Œæ•´æ€§æé«˜32.4%ï¼Œè¯Šæ–­é”™è¯¯å‡å°‘41.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–ç—…ç†å›¾åƒåˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>èƒƒè‚ é“ç—…ç†é¢†åŸŸé¢ä¸´æ•°æ®è´¨é‡å’Œæ¨ç†é€æ˜åº¦ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>å…¬å…±æ•°æ®é›†ä¸­çš„å™ªå£°å’Œæ ‡æ³¨ä¸å®Œæ•´å¯èƒ½å¯¼è‡´è§†è§‰è¯­è¨€æ¨¡å‹äº§ç”Ÿäº‹å®å¹»è§‰ã€‚</li>
<li>æ„å»ºå¤§è§„æ¨¡èƒƒè‚ é“ç—…ç†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³æ•°æ®è´¨é‡é—®é¢˜ã€‚</li>
<li>å¼•å…¥æç¤ºè®ºè¯ç­–ç•¥ï¼Œç»“åˆç—…å˜åˆ†ç±»å’Œè§£å‰–éƒ¨ä½ä¿¡æ¯ï¼ŒæŒ‡å¯¼æ¨¡å‹æ•æ‰å›¾åƒç‰¹å®šç‰¹å¾å¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨ç»“åˆç›‘ç£å¾®è°ƒå’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„åè®­ç»ƒç®¡é“ï¼Œæé«˜æ¨ç†è´¨é‡å’Œè¾“å‡ºç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aaa2feeaf1ad9252e6ab5c01b21aae74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c19b8789e5365f06b4c8b0ec48cc0d64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1efa87e6cba76bcdc8a2075f28b30587.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-056c923676c859a31324f6e2302cd4c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5e8b993e72b5273117bc81113961ebb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Datasets-and-Recipes-for-Video-Temporal-Grounding-via-Reinforcement-Learning"><a href="#Datasets-and-Recipes-for-Video-Temporal-Grounding-via-Reinforcement-Learning" class="headerlink" title="Datasets and Recipes for Video Temporal Grounding via Reinforcement   Learning"></a>Datasets and Recipes for Video Temporal Grounding via Reinforcement   Learning</h2><p><strong>Authors:Ruizhe Chen, Zhiting Fan, Tianze Luo, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, Huaijian Zhang</strong></p>
<p>Video Temporal Grounding (VTG) aims to localize relevant temporal segments in videos given natural language queries. Despite recent progress with large vision-language models (LVLMs) and instruction-tuning, existing approaches often suffer from limited temporal awareness and poor generalization. In this work, we introduce a two-stage training framework that integrates supervised fine-tuning with reinforcement learning (RL) to improve both the accuracy and robustness of VTG models. Our approach first leverages high-quality curated cold start data for SFT initialization, followed by difficulty-controlled RL to further enhance temporal localization and reasoning abilities. Comprehensive experiments on multiple VTG benchmarks demonstrate that our method consistently outperforms existing models, particularly in challenging and open-domain scenarios. We conduct an in-depth analysis of training strategies and dataset curation, highlighting the importance of both high-quality cold start data and difficulty-controlled RL. To facilitate further research and industrial adoption, we release all intermediate datasets, models, and code to the community. </p>
<blockquote>
<p>è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„ç›¸å…³æ—¶åºç‰‡æ®µã€‚å°½ç®¡æœ€è¿‘çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å’ŒæŒ‡ä»¤å¾®è°ƒå–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨æ—¶åºæ„ŸçŸ¥èƒ½åŠ›æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œä»¥æé«˜VTGæ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨é«˜è´¨é‡çš„ç²¾é€‰å†·å¯åŠ¨æ•°æ®è¿›è¡ŒSFTåˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡éš¾åº¦æ§åˆ¶çš„RLæ¥è¿›ä¸€æ­¥å¢å¼ºæ—¶åºå®šä½å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªVTGåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€ç›´ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¼€æ”¾åŸŸçš„åœºæ™¯ä¸­ã€‚æˆ‘ä»¬å¯¹è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†æ•´ç†è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¼ºè°ƒäº†é«˜è´¨é‡å†·å¯åŠ¨æ•°æ®å’Œéš¾åº¦æ§åˆ¶RLçš„é‡è¦æ€§ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ï¼Œæˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒäº†æ‰€æœ‰ä¸­é—´æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18100v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰çš„ç›®æ ‡æ˜¯åˆ©ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢åœ¨è§†é¢‘ä¸­æ‰¾åˆ°ç›¸å…³çš„æ—¶åºç‰‡æ®µã€‚å°½ç®¡æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å’ŒæŒ‡ä»¤å¾®è°ƒï¼ˆinstruction-tuningï¼‰çš„è¿‘æœŸè¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»å­˜åœ¨æ—¶é—´æ„ŸçŸ¥æœ‰é™å’Œæ³›åŒ–æ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜VTGæ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨é«˜è´¨é‡ç²¾é€‰çš„å†·å¯åŠ¨æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒåˆå§‹åŒ–ï¼Œéšåé‡‡ç”¨éš¾åº¦æ§åˆ¶çš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æ”¹å–„æ—¶åºå®šä½å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªVTGåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¼€æ”¾é¢†åŸŸçš„åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æœ¬æ–‡è¿˜æ·±å…¥åˆ†æäº†è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†æ•´ç†çš„é‡è¦æ€§ï¼Œå¹¶å…¬å¼€äº†æ‰€æœ‰ä¸­é—´æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨åˆ©ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢åœ¨è§†é¢‘ä¸­æ‰¾åˆ°ç›¸å…³çš„æ—¶åºç‰‡æ®µã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ—¶é—´æ„ŸçŸ¥å’Œæ³›åŒ–æ€§èƒ½æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥æé«˜VTGæ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨é«˜è´¨é‡ç²¾é€‰çš„å†·å¯åŠ¨æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒåˆå§‹åŒ–ã€‚</li>
<li>éš¾åº¦æ§åˆ¶çš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æ”¹å–„äº†æ—¶åºå®šä½å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¼€æ”¾é¢†åŸŸçš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13ffb93c299197274e3bcf6e6c5ce32e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e1d41472feccaaefcf4d72b652623f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbec2c7fc47ac250444f7b1e948b503f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa697dc5b06c1ab6c61ba375bede2fef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4a7165d5fcfffcac1f9b30f633d70e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f89ad8fc466917d0139356833e8e0529.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Performance-of-AI-Text-Detectors-Few-Shot-and-Chain-of-Thought-Prompting-Using-DeepSeek-Generated-Text"><a href="#Evaluating-the-Performance-of-AI-Text-Detectors-Few-Shot-and-Chain-of-Thought-Prompting-Using-DeepSeek-Generated-Text" class="headerlink" title="Evaluating the Performance of AI Text Detectors, Few-Shot and   Chain-of-Thought Prompting Using DeepSeek Generated Text"></a>Evaluating the Performance of AI Text Detectors, Few-Shot and   Chain-of-Thought Prompting Using DeepSeek Generated Text</h2><p><strong>Authors:Hulayyil Alshammari, Praveen Rao</strong></p>
<p>Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectorsâ€™ ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools â€“ AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero â€“ can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others â€“ particularly AI Text Classifier and GPT-2 â€“ showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%). </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿…é€Ÿæ”¹å˜äº†ä¹¦é¢ææ–™çš„åˆ›ä½œæ–¹å¼ã€‚LLMså¼•å‘äº†å…³äºå†™ä½œå®Œæ•´æ€§çš„é—®é¢˜ï¼Œä»è€Œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚å¯¹æŠ—æ€§æ”»å‡»ï¼Œå¦‚æ ‡å‡†å’ŒäººåŒ–çš„æ”¹è¿°ï¼ŒæŠ‘åˆ¶äº†æ£€æµ‹å™¨æ£€æµ‹æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ã€‚ä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ChatGPTå’Œå…¶ä»–çŸ¥åLLMsä¸Šï¼Œæ˜¾ç¤ºå‡ºæ£€æµ‹å™¨ä¹‹é—´çš„å‡†ç¡®ç‡å·®å¼‚ã€‚ç„¶è€Œï¼Œå…³äºæœ€è¿‘å‘å¸ƒçš„DeepSeekçš„æ–‡çŒ®ä¸­å­˜åœ¨æ˜æ˜¾çš„ç©ºç™½ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…­ç§é€šç”¨çš„AIæ£€æµ‹å·¥å…·ï¼Œå³AIæ–‡æœ¬åˆ†ç±»å™¨ã€å†…å®¹æ£€æµ‹AIã€Copyleaksã€QuillBotã€GPT-2å’ŒGPTZeroï¼Œæ˜¯å¦èƒ½ä¸€è‡´åœ°è¯†åˆ«DeepSeekç”Ÿæˆçš„æ–‡æœ¬ã€‚æ£€æµ‹å™¨å—åˆ°äº†ä¸Šè¿°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚æˆ‘ä»¬è¿˜è€ƒè™‘å°†DeepSeekä½œä¸ºæ£€æµ‹å™¨ï¼Œé€šè¿‡è¿›è¡Œå°‘é•œå¤´æç¤ºå’Œæ€ç»´é“¾æ¨ç†ï¼ˆCoTï¼‰æ¥åˆ†ç±»AIå’Œäººç±»æ’°å†™çš„æ–‡æœ¬ã€‚æˆ‘ä»¬ä»LLMæ—¶ä»£ä¹‹å‰æ”¶é›†äº†49ä¸ªäººç±»é—®ç­”å¯¹ï¼Œå¹¶ä½¿ç”¨DeepSeek-v3ç”Ÿæˆäº†åŒ¹é…çš„å›ç­”ï¼Œäº§ç”Ÿäº†49ä¸ªAIç”Ÿæˆçš„æ ·æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨äº†è¯¸å¦‚æ”¹è¿°å’ŒäººåŒ–ä¹‹ç±»çš„å¯¹æŠ—æŠ€æœ¯æ¥æ·»åŠ 196ä¸ªé¢å¤–çš„æ ·æœ¬ã€‚è¿™äº›è¢«ç”¨æ¥æŒ‘æˆ˜æ£€æµ‹å™¨çš„ç¨³å¥æ€§å¹¶è¯„ä¼°å‡†ç¡®æ€§çš„å½±å“ã€‚è™½ç„¶QuillBotå’ŒCopyleaksåœ¨åŸå§‹å’Œæ”¹è¿°çš„DeepSeekæ–‡æœ¬ä¸Šè¡¨ç°å‡ºè¿‘ä¹å®Œç¾çš„æ€§èƒ½ï¼Œä½†å…¶ä»–æ£€æµ‹å™¨ï¼Œç‰¹åˆ«æ˜¯AIæ–‡æœ¬åˆ†ç±»å™¨å’ŒGPT-2ï¼Œç»“æœå´ä¸å°½ä¸€è‡´ã€‚æœ€æœ‰æ•ˆçš„æ”»å‡»æ˜¯äººåŒ–ï¼Œå°†Copyleaksçš„å‡†ç¡®ç‡é™ä½åˆ°71%ï¼ŒQuillBotçš„å‡†ç¡®ç‡é™ä½åˆ°58%ï¼ŒGPTZeroçš„å‡†ç¡®ç‡é™ä½åˆ°52%ã€‚å°‘é•œå¤´å’ŒCoTæç¤ºæ˜¾ç¤ºé«˜å‡†ç¡®ç‡ï¼Œæœ€ä½³äº”é•œå¤´ç»“æœä»…è¯¯åˆ¤ä¸€ä¸ªæ ·æœ¬ï¼ˆAIå¬å›ç‡ä¸º96%ï¼Œäººç±»å¬å›ç‡ä¸º100%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17944v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¶Œç°å¼•å‘äº†å¯¹å†™ä½œçœŸå®æ€§çš„å…³æ³¨ï¼Œå¹¶æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚ç„¶è€Œï¼Œå¯¹æŠ—æ€§æ”»å‡»ï¼Œå¦‚æ ‡å‡†å’ŒäººåŒ–çš„æ”¹è¿°ï¼Œé™åˆ¶äº†æ£€æµ‹å™¨æ£€æµ‹æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ã€‚æœ¬æ–‡ç ”ç©¶äº†å…­ç§å¸¸ç”¨çš„AIæ£€æµ‹å·¥å…·æ˜¯å¦èƒ½å‡†ç¡®è¯†åˆ«ç”±DeepSeekç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¹¶æµ‹è¯•äº†è¿™äº›æ£€æµ‹å™¨åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼ŒQuillBotå’ŒCopyleaksåœ¨åº”å¯¹åŸå§‹å’Œæ”¹è¿°çš„DeepSeekæ–‡æœ¬æ—¶è¡¨ç°å‡ºè¿‘ä¹å®Œç¾çš„æ€§èƒ½ï¼Œè€Œå…¶ä»–æ£€æµ‹å™¨åˆ™è¡¨ç°ä¸ä¸€ã€‚æœ€æœ‰æ•ˆçš„æ”»å‡»æ˜¯äººåŒ–æ”»å‡»ï¼Œå®ƒé™ä½äº†æ£€æµ‹å™¨çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°‘é‡æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰çš„æ–¹æ³•åœ¨åŒºåˆ†AIå’Œäººç±»å†™ä½œæ–¹é¢è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•å¼•å‘äº†å¯¹å†™ä½œçœŸå®æ€§çš„å…³æ³¨ï¼Œä¿ƒè¿›äº†AIæ£€æµ‹æŠ€æœ¯çš„äº§ç”Ÿã€‚</li>
<li>å¯¹æŠ—æ€§æ”»å‡»ï¼Œå¦‚æ”¹è¿°ï¼Œèƒ½æŠ‘åˆ¶æ£€æµ‹å™¨è¯†åˆ«æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰çš„AIæ£€æµ‹å·¥å…·åœ¨é¢å¯¹DeepSeekç”Ÿæˆçš„æ–‡æœ¬æ—¶è¡¨ç°ä¸ä¸€ï¼ŒQuillBotå’ŒCopyleaksè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>äººåŒ–æ”»å‡»æ˜¯æœ€æœ‰æ•ˆçš„å¯¹æŠ—æ–¹å¼ï¼Œèƒ½æ˜¾è‘—é™ä½æ£€æµ‹å™¨çš„å‡†ç¡®ç‡ã€‚</li>
<li>é€šè¿‡å°‘é‡æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰çš„æ–¹æ³•åœ¨åŒºåˆ†AIå’Œäººç±»å†™ä½œæ–¹é¢éå¸¸æœ‰æ•ˆã€‚</li>
<li>ç›®å‰å¯¹äºDeepSeekè¿™ä¸€LLMçš„æ£€æµ‹ç ”ç©¶å­˜åœ¨ç©ºç™½ï¼Œæœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35bf13cba6903e56ff3638bb88ef0bfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cf7941de4cabe740787e573d78936a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e2e12c686a2adaadfa1b4c2a873dcaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-634aa6588f69da1ed7d8aeca20dd35ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58dfac29b0a7282a1cf82bd64ee3e3e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60370fd9f6aed50c1907e5a19d724b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee69a69b1dda142f303ac9787ea6bee1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c50d3d9b3fdceff5931f0fbf74682b5d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CodeReasoner-Enhancing-the-Code-Reasoning-Ability-with-Reinforcement-Learning"><a href="#CodeReasoner-Enhancing-the-Code-Reasoning-Ability-with-Reinforcement-Learning" class="headerlink" title="CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement   Learning"></a>CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement   Learning</h2><p><strong>Authors:Lingxiao Tang, He Ye, Zhongxin Liu, Xiaoxue Ren, Lingfeng Bao</strong></p>
<p>Code reasoning is a fundamental capability for large language models (LLMs) in the code domain. It involves understanding and predicting a programâ€™s execution behavior, such as determining the output for a given input or whether a specific statement will be executed. This capability is essential for downstream tasks like debugging, code generation, and program repair. Prior approaches mainly rely on supervised fine-tuning to improve performance in code reasoning tasks. However, they often show limited gains and fail to generalize across diverse scenarios. We argue this is due to two core issues: the low quality of training data and the limitations of supervised fine-tuning, which struggles to teach general reasoning skills. To address these challenges, we propose CodeReasoner, a framework that spans both dataset construction and a two-stage training process. First, we introduce a method to construct datasets that focus on the core execution logic of Python programs. Next, we apply instruction tuning to inject execution-specific knowledge distilled from a powerful teacher model. We then enhance reasoning and generalization through GRPO reinforcement learning on top of the fine-tuned model. Experiments on three widely-used code reasoning benchmarks show that CodeReasoner improves performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the 7B model matches GPT-4o on key tasks like input&#x2F;output and coverage prediction. When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks. Ablation studies confirm the effectiveness of each training stage and highlight the importance of reasoning chains. </p>
<blockquote>
<p>ä»£ç æ¨ç†æ˜¯ä»£ç åŸŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€é¡¹åŸºæœ¬èƒ½åŠ›ã€‚å®ƒæ¶‰åŠç†è§£å’Œé¢„æµ‹ç¨‹åºçš„æ‰§è¡Œè¡Œä¸ºï¼Œå¦‚ç¡®å®šç»™å®šè¾“å…¥çš„è¾“å²€æˆ–ç‰¹å®šè¯­å¥æ˜¯å¦ä¼šæ‰§è¡Œã€‚å¯¹äºè°ƒè¯•ã€ä»£ç ç”Ÿæˆå’Œç¨‹åºä¿®å¤ç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿™é¡¹èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒæ¥æé«˜ä»£ç æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸å¢ç›Šæœ‰é™ï¼Œæ— æ³•åœ¨å¤šç§åœºæ™¯ä¸­å®ç°æ³›åŒ–ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ç”±äºä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜å¯¼è‡´çš„ï¼šè®­ç»ƒæ•°æ®è´¨é‡ä½ä¸‹å’Œç›‘ç£å¾®è°ƒçš„å±€é™æ€§ï¼Œåè€…éš¾ä»¥æ•™æˆé€šç”¨æ¨ç†æŠ€èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CodeReasoneræ¡†æ¶ï¼Œå®ƒæ¶µç›–äº†æ•°æ®é›†æ„å»ºå’Œä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ„å»ºæ•°æ®é›†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸“æ³¨äºPythonç¨‹åºçš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŒ‡ä»¤å¾®è°ƒåº”ç”¨äºä»å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦æ‰§è¡Œç‰¹å®šçŸ¥è¯†çš„æ³¨å…¥ã€‚ç„¶åæˆ‘ä»¬åœ¨å¾®è°ƒæ¨¡å‹ä¹‹ä¸Šåº”ç”¨GRPOå¼ºåŒ–å­¦ä¹ ä»¥å¢å¼ºæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¸¸ç”¨çš„ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCodeReasonerä½¿ç”¨7Bæ¨¡å‹åœ¨å…ˆå‰æ–¹æ³•çš„åŸºç¡€ä¸Šæé«˜äº†27.1%è‡³40.2%çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ7Bæ¨¡å‹åœ¨è¾“å…¥&#x2F;è¾“å²€å’Œè¦†ç›–é¢„æµ‹ç­‰å…³é”®ä»»åŠ¡ä¸ŠåŒ¹é…GPT-4oçš„è¡¨ç°ã€‚å½“æ‰©å±•åˆ°14Bæ—¶ï¼ŒCodeReasoneråœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äºGPT-4oã€‚æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªè®­ç»ƒé˜¶æ®µçš„æœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†æ¨ç†é“¾çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17548v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»£ç æ¨ç†ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç é¢†åŸŸçš„åŸºæœ¬èƒ½åŠ›çš„é‡è¦æ€§ï¼Œæ¶‰åŠç†è§£å’Œé¢„æµ‹ç¨‹åºçš„æ‰§è¡Œè¡Œä¸ºã€‚å…ˆå‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒæ¥æé«˜ä»£ç æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CodeReasoneræ¡†æ¶ï¼Œæ¶µç›–æ•°æ®é›†æ„å»ºå’Œä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥ä¸“æ³¨äºPythonç¨‹åºæ ¸å¿ƒæ‰§è¡Œé€»è¾‘çš„æ•°æ®é›†æ„å»ºæ–¹æ³•ï¼Œåº”ç”¨æŒ‡ä»¤è°ƒä¼˜æ³¨å…¥æ‰§è¡Œç‰¹å®šçŸ¥è¯†ï¼Œå¹¶é€šè¿‡GRPOå¼ºåŒ–å­¦ä¹ å¢å¼ºæ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒCodeReasoneråœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜äº†27.1%è‡³40.2%ï¼Œå¹¶ä¸”åœ¨æ‰©å±•åˆ°14Bæ—¶å…¨é¢è¶…è¶Šäº†GPT-4oã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç é¢†åŸŸçš„åŸºæœ¬èƒ½åŠ›ï¼Œæ¶‰åŠç†è§£å’Œé¢„æµ‹ç¨‹åºæ‰§è¡Œè¡Œä¸ºã€‚</li>
<li>å…ˆå‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒæ¥æé«˜ä»£ç æ¨ç†ä»»åŠ¡æ€§èƒ½ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>CodeReasoneræ¡†æ¶åŒ…æ‹¬æ•°æ®é›†æ„å»ºå’Œä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>CodeReasoneré€šè¿‡å¼•å…¥ä¸“æ³¨äºPythonç¨‹åºæ ¸å¿ƒæ‰§è¡Œé€»è¾‘çš„æ•°æ®é›†ã€åº”ç”¨æŒ‡ä»¤è°ƒä¼˜å’ŒGRPOå¼ºåŒ–å­¦ä¹ ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCodeReasoneråœ¨ä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>CodeReasoneråœ¨æ‰©å±•åˆ°æ›´å¤§æ¨¡å‹æ—¶ï¼Œæ€§èƒ½è¡¨ç°æ›´åŠ å‡ºè‰²ï¼Œå…¨é¢è¶…è¶Šäº†GPT-4oã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58bb40455c97418ea07c56e2a6ca2a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28cf34600faaa38049e50db9297885ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f441194b8e27d690ee280a365b02ab42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6e930319fb9623f44937fdad1d1b76b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2a816e429e1c4aa0f56ff11887c0d17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdb79011b8bd94cc056b1a8ef526604b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation"><a href="#InstructVLA-Vision-Language-Action-Instruction-Tuning-from-Understanding-to-Manipulation" class="headerlink" title="InstructVLA: Vision-Language-Action Instruction Tuning from   Understanding to Manipulation"></a>InstructVLA: Vision-Language-Action Instruction Tuning from   Understanding to Manipulation</h2><p><strong>Authors:Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, Jiangmiao Pang</strong></p>
<p>To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLAâ€™s potential for bridging intuitive and steerable human-robot interaction with efficient policy learning. </p>
<blockquote>
<p>ä¸ºäº†åœ¨ç°å®ä¸–ç•Œä¸­è¿›è¡Œæœ‰æ•ˆçš„æ“ä½œï¼Œæœºå™¨äººå¿…é¡»å°†å¤šæ¨¡æ€æ¨ç†ä¸ç²¾ç¡®çš„åŠ¨ä½œç”Ÿæˆç»“åˆèµ·æ¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å¾€å¾€é¡¾æ­¤å¤±å½¼ï¼Œå±€é™äºç‰¹å®šä»»åŠ¡çš„æ“çºµæ•°æ®ï¼Œå¹¶é­å—é¢„è®­ç»ƒè§†è§‰-è¯­è¨€èƒ½åŠ›çš„ç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†InstructVLAï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„VLAæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¿ç•™å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„çµæ´»æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæä¾›é¢†å…ˆçš„æ“çºµæ€§èƒ½ã€‚InstructVLAå¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼â€”â€”è§†è§‰-è¯­è¨€-åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´ï¼ˆVLA-ITï¼‰ï¼Œå®ƒé‡‡ç”¨å¤šæ¨¡æ€è®­ç»ƒä¸æ··åˆä¸“å®¶é€‚åº”ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥è”åˆä¼˜åŒ–æ–‡æœ¬æ¨ç†å’ŒåŠ¨ä½œç”Ÿæˆï¼Œåœ¨æ ‡å‡†VLMè¯­æ–™åº“å’Œå®šåˆ¶çš„65ä¸‡æ ·æœ¬VLA-ITæ•°æ®é›†ä¸Šè¿›è¡Œæ“ä½œã€‚åœ¨åŸŸå†…çš„SimplerEnvä»»åŠ¡ä¸Šï¼ŒInstructVLAè¾ƒSpatialVLAæé«˜äº†30.5%ã€‚ä¸ºäº†è¯„ä¼°æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†SimplerEnv-Instructï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«80ä¸ªä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚é—­ç¯æ§åˆ¶å’Œé«˜çº§æŒ‡ä»¤ç†è§£ï¼ŒInstructVLAçš„è¡¨ç°è¶…è¶Šäº†å¾®è°ƒåçš„OpenVLAï¼ˆæå‡äº†92%ï¼‰ï¼Œä»¥åŠç”±GPT-4oè¾…åŠ©çš„åŠ¨ä½œä¸“å®¶ï¼ˆæå‡äº†29%ï¼‰ã€‚æ­¤å¤–ï¼ŒInstructVLAåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¶…è¶Šäº†åŸºæœ¬çš„VLMï¼Œå¹¶å€ŸåŠ©æ–‡æœ¬æ¨ç†æé«˜äº†æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸‹çš„æ“ä½œæ€§èƒ½ï¼Œå®ç°äº†æ¨ç†æ—¶é—´çš„ç¼©æ”¾ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†InstructVLAåœ¨æ¡¥æ¥ç›´è§‚å’Œå¯å¼•å¯¼çš„äººæœºäº¤äº’ä¸é«˜æ•ˆç­–ç•¥å­¦ä¹ æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17520v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†InstructVLAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨æ¡¥æ¥ç°å®ä¸–ç•Œä¸­æœºå™¨äººæ“ä½œçš„è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œèƒ½åŠ›ã€‚å®ƒé‡‡ç”¨ä¸€ç§æ–°å‹è®­ç»ƒèŒƒå¼VLA-ITï¼Œç»“åˆäº†å¤šæ¨¡å¼è®­ç»ƒä¸æ··åˆä¸“å®¶é€‚åº”ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æ–‡æœ¬æ¨ç†å’ŒåŠ¨ä½œç”Ÿæˆã€‚åœ¨ç‰¹å®šä»»åŠ¡ç¯å¢ƒä¸‹ï¼ŒInstructVLAç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒInstructVLAè¿˜åˆ©ç”¨æ–‡æœ¬æ¨ç†æå‡äº†æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸‹çš„æ“ä½œæ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨äººæœºäº¤äº’é¢†åŸŸçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InstructVLAæ˜¯ä¸€ä¸ªç«¯å¯¹ç«¯çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººåœ¨ç°å®ä¸–ç•Œä¸­æ“ä½œæ—¶çš„å¤šæ¨¡å¼æ¨ç†å’Œç²¾ç¡®åŠ¨ä½œç”Ÿæˆé—®é¢˜ã€‚</li>
<li>ç°æœ‰VLAæ¨¡å‹å¸¸åœ¨è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œæ–¹é¢æœ‰æ‰€å–èˆï¼Œèƒ½åŠ›å±€é™äºç‰¹å®šä»»åŠ¡æ•°æ®ï¼Œå¹¶å®¹æ˜“å¿˜è®°é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€èƒ½åŠ›ã€‚</li>
<li>InstructVLAå¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼VLA-ITï¼Œé€šè¿‡å¤šæ¨¡å¼è®­ç»ƒå’Œæ··åˆä¸“å®¶é€‚åº”ç­–ç•¥æ¥ä¼˜åŒ–æ–‡æœ¬æ¨ç†å’ŒåŠ¨ä½œç”Ÿæˆã€‚</li>
<li>InstructVLAåœ¨ç‰¹å®šä»»åŠ¡ç¯å¢ƒä¸‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œç›¸è¾ƒäºSpatialVLAæœ‰30.5%çš„æ”¹è¿›ã€‚</li>
<li>InstructVLAå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æ–°çš„80ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>InstructVLAåˆ©ç”¨æ–‡æœ¬æ¨ç†æå‡äº†åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸‹çš„æ“ä½œæ€§èƒ½ï¼Œå®ç°æ¨ç†æ—¶é—´çš„æ¨æ–­æ‰©å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9bf674084c7570db443b3dbd92064a89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9304b49a598ae6f4e788b3c1a749bdf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7922c1bc531ec9e3d8ea8bc3f3c60360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2da37bf785014353ea6496d0735e1a7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-285f52a38a4551f0bab66a9c2665fb46.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="URPO-A-Unified-Reward-Policy-Optimization-Framework-for-Large-Language-Models"><a href="#URPO-A-Unified-Reward-Policy-Optimization-Framework-for-Large-Language-Models" class="headerlink" title="URPO: A Unified Reward &amp; Policy Optimization Framework for Large   Language Models"></a>URPO: A Unified Reward &amp; Policy Optimization Framework for Large   Language Models</h2><p><strong>Authors:Songshuo Lu, Hua Wang, Zhi Chen, Yaohua Tang</strong></p>
<p>Large-scale alignment pipelines typically pair a policy model with a separately trained reward model whose parameters remain frozen during reinforcement learning (RL). This separation creates a complex, resource-intensive pipeline and suffers from a performance ceiling due to a static reward signal. We propose a novel framework, Unified Reward &amp; Policy Optimization (URPO), that unifies instruction-following (â€œplayerâ€) and reward modeling (â€œrefereeâ€) within a single model and a single training phase. Our method recasts all alignment data-including preference pairs, verifiable reasoning, and open-ended instructions-into a unified generative format optimized by a single Group-Relative Policy Optimization (GRPO) loop. This enables the model to learn from ground-truth preferences and verifiable logic while simultaneously generating its own rewards for open-ended tasks. Experiments on the Qwen2.5-7B model demonstrate URPOâ€™s superiority. Our unified model significantly outperforms a strong baseline using a separate generative reward model, boosting the instruction-following score on AlpacaEval from 42.24 to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore, URPO cultivates a superior internal evaluator as a byproduct of training, achieving a RewardBench score of 85.15 and surpassing the dedicated reward model it replaces (83.55). By eliminating the need for a separate reward model and fostering a co-evolutionary dynamic between generation and evaluation, URPO presents a simpler, more efficient, and more effective path towards robustly aligned language models. </p>
<blockquote>
<p>å¤§è§„æ¨¡å¯¹é½ç®¡é“é€šå¸¸ä¼šå°†ç­–ç•¥æ¨¡å‹ä¸å•ç‹¬è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹é…å¯¹ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­ï¼Œå¥–åŠ±æ¨¡å‹çš„å‚æ•°ä¿æŒå†»ç»“ã€‚è¿™ç§åˆ†ç¦»å¯¼è‡´äº†ä¸€ä¸ªå¤æ‚ä¸”èµ„æºå¯†é›†å‹çš„ç®¡é“ï¼Œå¹¶ç”±äºé™æ€å¥–åŠ±ä¿¡å·è€Œå—åˆ°æ€§èƒ½ä¸Šé™çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå³ç»Ÿä¸€å¥–åŠ±ä¸ç­–ç•¥ä¼˜åŒ–ï¼ˆURPOï¼‰ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹å’Œå•ä¸€è®­ç»ƒé˜¶æ®µå†…ç»Ÿä¸€äº†æŒ‡ä»¤éµå¾ªï¼ˆâ€œç©å®¶â€ï¼‰å’Œå¥–åŠ±å»ºæ¨¡ï¼ˆâ€œè£åˆ¤â€ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ‰€æœ‰å¯¹é½æ•°æ®â€”â€”åŒ…æ‹¬åå¥½å¯¹ã€å¯éªŒè¯çš„æ¨ç†å’Œå¼€æ”¾æ€§æŒ‡ä»¤â€”â€”é‡æ–°è½¬æ¢ä¸ºç»Ÿä¸€çš„ç”Ÿæˆæ ¼å¼ï¼Œé€šè¿‡å•ä¸ªç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¾ªç¯è¿›è¡Œä¼˜åŒ–ã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿä»çœŸå®åå¥½å’Œå¯éªŒè¯é€»è¾‘ä¸­å­¦ä¹ ï¼ŒåŒæ—¶ä¸ºå…¶è‡ªèº«å¼€æ”¾ä»»åŠ¡ç”Ÿæˆå¥–åŠ±ã€‚åœ¨Qwen2.5-7Bæ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜äº†URPOçš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¨¡å‹æ˜¾è‘—ä¼˜äºä½¿ç”¨å•ç‹¬ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„å¼ºå¤§åŸºçº¿ï¼Œåœ¨AlpacaEvalä¸Šå°†æŒ‡ä»¤éµå¾ªåˆ†æ•°ä»42.24æé«˜åˆ°44.84ï¼Œå¤åˆæ¨ç†å¹³å‡åˆ†ä»32.66æé«˜åˆ°35.66ã€‚æ­¤å¤–ï¼ŒURPOä½œä¸ºè®­ç»ƒçš„å‰¯äº§å“ï¼ŒåŸ¹å…»äº†ä¸€ä¸ªä¼˜è¶Šçš„å†…éƒ¨è¯„ä¼°è€…ï¼Œå…¶RewardBenchåˆ†æ•°è¾¾åˆ°85.15ï¼Œè¶…è¶Šäº†å…¶æ‰€æ›¿ä»£çš„ä¸“ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆ83.55ï¼‰ã€‚é€šè¿‡æ¶ˆé™¤å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ï¼Œå¹¶åœ¨ç”Ÿæˆå’Œè¯„ä¼°ä¹‹é—´ä¿ƒè¿›ååŒè¿›åŒ–åŠ¨æ€ï¼ŒURPOä¸ºç¨³å¥å¯¹é½è¯­è¨€æ¨¡å‹æä¾›äº†æ›´ç®€å•ã€æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17515v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶Unified Reward &amp; Policy Optimization (URPO)ï¼Œå®ƒå°†æŒ‡ä»¤éµå¾ªçš„â€œç©å®¶â€å’Œå¥–åŠ±å»ºæ¨¡çš„â€œè£åˆ¤â€ç»Ÿä¸€åœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹å’Œå•ä¸€è®­ç»ƒé˜¶æ®µä¸­ã€‚URPOé€šè¿‡ä¼˜åŒ–ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆæ ¼å¼ï¼Œèåˆäº†åå¥½å¯¹ã€å¯éªŒè¯æ¨ç†å’Œå¼€æ”¾å¼æŒ‡ä»¤ï¼Œé‡‡ç”¨å•ä¸€Group-Relative Policy Optimization (GRPO)å¾ªç¯è¿›è¡Œå­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒURPOåœ¨Qwen2.5-7Bæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æŒ‡ä»¤éµå¾ªå’Œå¤åˆæ¨ç†çš„å¹³å‡å¾—åˆ†ã€‚æ­¤å¤–ï¼ŒURPOä½œä¸ºè®­ç»ƒçš„å‰¯äº§å“ï¼ŒåŸ¹å…»å‡ºå“è¶Šçš„å†…éƒ¨è¯„ä¼°è€…ï¼Œè¶…è¶Šä¸“ç”¨çš„å¥–åŠ±æ¨¡å‹ã€‚é€šè¿‡æ¶ˆé™¤å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ï¼Œä¿ƒè¿›ç”Ÿæˆå’Œè¯„ä¼°ä¹‹é—´çš„ååŒè¿›åŒ–åŠ¨æ€ï¼ŒURPOä¸ºæ„å»ºç¨³å¥å¯¹é½çš„è¯­è¨€æ¨¡å‹æä¾›äº†æ›´ç®€å•ã€æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>URPOæ¡†æ¶å°†æŒ‡ä»¤éµå¾ªå’Œå¥–åŠ±å»ºæ¨¡ç»Ÿä¸€åœ¨å•ä¸€æ¨¡å‹å’Œå•ä¸€è®­ç»ƒé˜¶æ®µä¸­ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>URPOé‡‡ç”¨ç”Ÿæˆæ ¼å¼ä¼˜åŒ–ï¼Œèåˆäº†å¤šç§æ•°æ®ï¼Œå¦‚åå¥½å¯¹ã€å¯éªŒè¯æ¨ç†å’Œå¼€æ”¾å¼æŒ‡ä»¤ã€‚</li>
<li>é€šè¿‡Group-Relative Policy Optimization (GRPO)å¾ªç¯ï¼Œæ¨¡å‹èƒ½åŒæ—¶ä»çœŸå®åå¥½å’Œå¯éªŒè¯é€»è¾‘ä¸­å­¦ä¹ ï¼Œå¹¶ç”Ÿæˆå¼€æ”¾å¼ä»»åŠ¡å¥–åŠ±ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºURPOåœ¨Qwen2.5-7Bæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæé«˜äº†æŒ‡ä»¤éµå¾ªå’Œå¤åˆæ¨ç†å¾—åˆ†ã€‚</li>
<li>URPOåŸ¹å…»äº†ä¸€ä¸ªå“è¶Šçš„å†…éƒ¨è¯„ä¼°è€…ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>URPOæ¶ˆé™¤äº†å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹çš„éœ€æ±‚ï¼Œä¿ƒè¿›äº†ç”Ÿæˆå’Œè¯„ä¼°ä¹‹é—´çš„ååŒè¿›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-640c2952fc6cf4b4c81d095b87f0e669.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f43a0f273507af30fd63b4aab0d2b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef50edfad08cd1ae8cd2a62b24ca634a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CLARIFID-Improving-Radiology-Report-Generation-by-Reinforcing-Clinically-Accurate-Impressions-and-Enforcing-Detailed-Findings"><a href="#CLARIFID-Improving-Radiology-Report-Generation-by-Reinforcing-Clinically-Accurate-Impressions-and-Enforcing-Detailed-Findings" class="headerlink" title="CLARIFID: Improving Radiology Report Generation by Reinforcing   Clinically Accurate Impressions and Enforcing Detailed Findings"></a>CLARIFID: Improving Radiology Report Generation by Reinforcing   Clinically Accurate Impressions and Enforcing Detailed Findings</h2><p><strong>Authors:Kyeongkyu Lee, Seonghwan Yoon, Hongki Lim</strong></p>
<p>Automatic generation of radiology reports has the potential to alleviate radiologistsâ€™ significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) enforces reasoning-aware decoding that completes â€œFindingsâ€ before synthesizing the â€œImpressionâ€, and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a reasoning-aware next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive Findings section before synthesizing the Impression and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on both standard NLG metrics and clinically aware scores. </p>
<blockquote>
<p>è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šæœ‰æ½œåŠ›å‡è½»æ”¾å°„ç§‘åŒ»å¸ˆå·¨å¤§çš„å·¥ä½œé‡ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨ä¸´åºŠå¯é ç»“è®ºçš„ç”Ÿæˆä¸Šä»å­˜åœ¨å›°éš¾ã€‚å°¤å…¶æ˜¯ï¼Œå¤§å¤šæ•°æ—©æœŸçš„æ–¹æ³•é›†ä¸­åœ¨ç”Ÿæˆæµç•…æ–‡æœ¬ä¸Šï¼Œå¹¶æœªæœ‰æ•ˆç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”é€šå¸¸ä¾èµ–äºå•è§†å›¾å›¾åƒï¼Œé™åˆ¶äº†è¯Šæ–­çš„å…¨é¢æ€§ã€‚æˆ‘ä»¬æå‡ºäº†CLARIFIDè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿä¸“å®¶çš„ä¸¤æ­¥å·¥ä½œæµç¨‹æ¥ç›´æ¥ä¼˜åŒ–è¯Šæ–­çš„æ­£ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒCLARIFIDï¼ˆ1ï¼‰é€šè¿‡åˆ†æ®µé¢„è®­ç»ƒå­¦ä¹ ä»æ£€æŸ¥ç»“æœåˆ°å°è±¡çš„é€»è¾‘æµç¨‹ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è¿›è¡Œå¾®è°ƒï¼Œå…¶ä¸­å°è±¡éƒ¨åˆ†çš„CheXbert F1åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œï¼ˆ3ï¼‰å¼ºåˆ¶å®æ–½æ¨ç†æ„ŸçŸ¥è§£ç ï¼Œåœ¨å®Œæˆâ€œæ£€æŸ¥ç»“æœâ€åå†åˆæˆâ€œå°è±¡â€ï¼Œï¼ˆ4ï¼‰é€šè¿‡åŸºäºè§†è§‰å˜å‹å™¨çš„å¤šè§†å›¾ç¼–ç å™¨èåˆå¤šä¸ªèƒ¸éƒ¨Xå…‰ç‰‡è§†å›¾ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ¨ç†æ„ŸçŸ¥çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œå¼ºåˆ¶ç­–ç•¥ï¼Œç„¶åè¿›è¡ŒæŠ¥å‘Šçº§åˆ«çš„é‡æ–°æ’åºï¼Œç¡®ä¿æ¨¡å‹é¦–å…ˆç”Ÿæˆå…¨é¢çš„æ£€æŸ¥ç»“æœéƒ¨åˆ†ï¼Œç„¶åå†åˆæˆå°è±¡ï¼Œä»è€Œä¿æŒè¿è´¯çš„ä¸´åºŠæ¨ç†ã€‚åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†ä¼˜è¶Šçš„ä¸´åºŠæ•ˆæœï¼Œåœ¨æ ‡å‡†è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å’Œä¸´åºŠæ„è¯†å¾—åˆ†ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17234v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCLARIFIDçš„æ–°æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šï¼Œä»¥å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡ä»¿ä¸“å®¶ä¸¤æ­¥å·¥ä½œæµç¨‹ç›´æ¥ä¼˜åŒ–è¯Šæ–­æ­£ç¡®æ€§ï¼ŒåŒ…æ‹¬å­¦ä¹ é€»è¾‘æµç¨‹ã€ç²¾ç»†è°ƒæ•´æ”¿ç­–ä¼˜åŒ–ã€å®æ–½æ¨ç†æ„ŸçŸ¥è§£ç å’Œèåˆå¤šè§†è§’çš„èƒ¸éƒ¨Xå…‰ç‰‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„ä¸´åºŠæ•ˆæœï¼Œä¼˜äºç°æœ‰åŸºçº¿æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šå…·æœ‰å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œè´Ÿæ‹…çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨ä¸´åºŠå¯é æ€§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ç¡®ä¿æŠ¥å‘Šçš„äº‹å®æ­£ç¡®æ€§ã€‚</li>
<li>CLARIFIDæ¡†æ¶é€šè¿‡æ¨¡ä»¿ä¸“å®¶å·¥ä½œæµç¨‹ç›´æ¥ä¼˜åŒ–è¯Šæ–­æ­£ç¡®æ€§ã€‚</li>
<li>CLARIFIDæ¡†æ¶åŒ…æ‹¬å­¦ä¹ é€»è¾‘æµç¨‹ã€ç²¾ç»†è°ƒæ•´æ”¿ç­–ä¼˜åŒ–ã€æ¨ç†æ„ŸçŸ¥è§£ç å’Œå¤šè§†è§’èåˆç­‰æŠ€æœ¯ç¯èŠ‚ã€‚</li>
<li>CLARIFIDåœ¨MIMIC-CXRæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„ä¸´åºŠæ•ˆæœã€‚</li>
<li>ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼ŒCLARIFIDåœ¨æ ‡å‡†è‡ªç„¶è¯­è¨€ç”ŸæˆæŒ‡æ ‡å’Œä¸´åºŠæ„è¯†è¯„åˆ†ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c7c947de03d17afa8cb0d3edffaa887.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-604b1c2a8cca8604badcf7c7b6fc72c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a748d2100842c2587b7b4a26b739a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ad177557a9207816355e6a82994c42d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RAVine-Reality-Aligned-Evaluation-for-Agentic-Search"><a href="#RAVine-Reality-Aligned-Evaluation-for-Agentic-Search" class="headerlink" title="RAVine: Reality-Aligned Evaluation for Agentic Search"></a>RAVine: Reality-Aligned Evaluation for Agentic Search</h2><p><strong>Authors:Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao</strong></p>
<p>Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine â€“ a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines modelâ€™s interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/SwordFaith/RAVine">https://github.com/SwordFaith/RAVine</a>. </p>
<blockquote>
<p>ä»£ç†æœç´¢ä½œä¸ºä¸€ç§æ›´åŠ è‡ªä¸»å’Œé€‚åº”æ€§çš„æ£€ç´¢å¢å¼ºèŒƒå¼ï¼Œæ­£åœ¨æ¨åŠ¨æ™ºèƒ½æœç´¢ç³»ç»Ÿçš„è¿›åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸ä»£ç†æœç´¢çš„ç›®æ ‡å¹¶ä¸å®Œå…¨å¥‘åˆã€‚é¦–å…ˆï¼Œå½“å‰åŸºå‡†æµ‹è¯•ä¸­å¸¸ç”¨çš„å¤æ‚æŸ¥è¯¢å¸¸å¸¸åç¦»ç°å®ç”¨æˆ·æœç´¢åœºæ™¯ã€‚å…¶æ¬¡ï¼Œå…ˆå‰çš„æ–¹æ³•åœ¨æå–ç«¯åˆ°ç«¯è¯„ä¼°çš„åŸºå‡†çœŸå®æ•°æ®æ—¶å¾€å¾€å¼•å…¥å™ªå£°ï¼Œå¯¼è‡´ç²¾ç»†çº§åˆ«çš„è¯„ä¼°å¤±çœŸã€‚ç¬¬ä¸‰ï¼Œå¤§å¤šæ•°å½“å‰æ¡†æ¶åªå…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„è´¨é‡ï¼Œå¿½è§†äº†ä»£ç†æœç´¢æ‰€å›ºæœ‰çš„è¿­ä»£è¿‡ç¨‹çš„è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†RAVineâ€”â€”ä¸€ä¸ªä¸ç°å®å¯¹é½çš„ä»£ç†LLMæœç´¢è¯„ä¼°æ¡†æ¶ã€‚RAVineæ—¨åœ¨é’ˆå¯¹å¤šç‚¹æŸ¥è¯¢å’Œé•¿ç­”æ¡ˆï¼Œä»¥æ›´å¥½åœ°åæ˜ ç”¨æˆ·æ„å›¾ï¼Œå¹¶å¼•å…¥å¯å½’å±çš„åŸºå‡†çœŸå®æ•°æ®æ„å»ºç­–ç•¥ï¼Œä»¥æé«˜ç²¾ç»†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒRAVineè¿˜ç ”ç©¶äº†æ¨¡å‹åœ¨æœç´¢å·¥å…·è¿­ä»£è¿‡ç¨‹ä¸­çš„äº¤äº’ä½œç”¨ä»¥åŠæ•ˆç‡å› ç´ ã€‚æˆ‘ä»¬ä½¿ç”¨RAVineå¯¹ä¸€ç³»åˆ—æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è·å¾—äº†è‹¥å¹²è§è§£ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™äº›è§è§£èƒ½ä¸ºæ¨åŠ¨ä»£ç†æœç´¢ç³»ç»Ÿçš„å‘å±•åšå‡ºè´¡çŒ®ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SwordFaith/RAVine%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SwordFaith/RAVineæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16725v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Agenticæœç´¢ä½œä¸ºæ›´è‡ªä¸»å’Œé€‚åº”æ€§çš„æ£€ç´¢å¢å¼ºèŒƒå¼ï¼Œæ­£æ¨åŠ¨æ™ºèƒ½æœç´¢ç³»ç»Ÿçš„è¿›åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸agenticæœç´¢çš„ç›®æ ‡å¹¶ä¸å»åˆã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RAVineè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é’ˆå¯¹å¤šè¦ç‚¹æŸ¥è¯¢å’Œé•¿ç­”æ¡ˆï¼Œæ›´å¥½åœ°åæ˜ ç”¨æˆ·æ„å›¾ï¼Œå¹¶å¼•å…¥å¯å½’å±çš„åœ°é¢çœŸå®æ„å»ºç­–ç•¥ï¼Œæé«˜ç²¾ç»†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒRAVineè¿˜å…³æ³¨æ¨¡å‹åœ¨æœç´¢å·¥å…·ä¸­çš„äº¤äº’è¿‡ç¨‹å¹¶è€ƒè™‘æ•ˆç‡å› ç´ ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ¡†æ¶å¯¹ä¸€ç³»åˆ—æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è·å¾—äº†æœ‰åŠ©äºæ¨åŠ¨agenticæœç´¢ç³»ç»Ÿå‘å±•çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agenticæœç´¢æ˜¯æ™ºèƒ½æœç´¢ç³»ç»Ÿçš„é‡è¦å‘å±•æ–¹å‘ï¼Œå¼ºè°ƒè‡ªä¸»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸agenticæœç´¢ç›®æ ‡ä¸å»åˆï¼Œå­˜åœ¨å¯¹ç°å®ç”¨æˆ·æœç´¢åœºæ™¯çš„åç¦»ã€è¯„ä¼°å™ªå£°å’Œå¿½è§†è¿­ä»£è¿‡ç¨‹çš„é—®é¢˜ã€‚</li>
<li>RAVineè¯„ä¼°æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæ³¨é‡å¤šè¦ç‚¹æŸ¥è¯¢å’Œé•¿ç­”æ¡ˆçš„è¯„ä¼°ï¼Œæ›´æ¥è¿‘ç”¨æˆ·çœŸå®æ„å›¾ã€‚</li>
<li>RAVineå¼•å…¥å¯å½’å±çš„åœ°é¢çœŸå®æ„å»ºç­–ç•¥ï¼Œæé«˜ç²¾ç»†è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>RAVineå…³æ³¨æ¨¡å‹åœ¨æœç´¢å·¥å…·ä¸­çš„äº¤äº’è¿‡ç¨‹ï¼Œå¹¶è€ƒè™‘æ•ˆç‡å› ç´ ã€‚</li>
<li>é€šè¿‡RAVineæ¡†æ¶å¯¹ä¸€ç³»åˆ—æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œè·å¾—äº†æœ‰åŠ©äºæ¨åŠ¨agenticæœç´¢ç³»ç»Ÿå‘å±•çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16814c3cf4c26068ff32a4201ab4bc10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4b461a8c160f75536f69f3313205c1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aba94c0a064ccde75da931f300685949.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de6bb1178d0b02e0db77ac8dd92351b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea32e214dc7a37c4d9c3ac194594e5f3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Being-H0-Vision-Language-Action-Pretraining-from-Large-Scale-Human-Videos"><a href="#Being-H0-Vision-Language-Action-Pretraining-from-Large-Scale-Human-Videos" class="headerlink" title="Being-H0: Vision-Language-Action Pretraining from Large-Scale Human   Videos"></a>Being-H0: Vision-Language-Action Pretraining from Large-Scale Human   Videos</h2><p><strong>Authors:Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu</strong></p>
<p>We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources â€“ including motion capture, VR, and RGB-only videos â€“ into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at <a target="_blank" rel="noopener" href="https://beingbeyond.github.io/Being-H0">https://beingbeyond.github.io/Being-H0</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Being-H0ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤§è§„æ¨¡äººç±»è§†é¢‘ä¸Šè®­ç»ƒçš„çµå·§è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ã€‚ç°æœ‰çš„VLAåœ¨å¤„ç†éœ€è¦é«˜åº¦çµæ´»æ€§çš„å¤æ‚æ“ä½œä»»åŠ¡æ—¶é‡åˆ°å›°éš¾ï¼Œå¹¶ä¸”åœ¨æ–°å‹åœºæ™¯å’Œä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºå­˜åœ¨æ¨¡æ‹Ÿåˆ°ç°å®å·®è·çš„åˆæˆæ•°æ®ï¼Œæˆ–è€…ç¼ºä¹è§„æ¨¡å’Œå¤šæ ·æ€§çš„é¥æ§æ¼”ç¤ºã€‚ä¸ºäº†è§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨äººç±»çš„æ‰‹ä½œä¸ºåŸºç¡€æ“çºµå™¨ï¼Œåˆ©ç”¨ç½‘ç»œæ•°æ®çš„ä¸°å¯Œçµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ç‰©ç†æŒ‡ä»¤å¾®è°ƒä¸ºæ ¸å¿ƒï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†äººç±»è§†é¢‘çš„å¤§è§„æ¨¡VLAé¢„è®­ç»ƒã€ç‰©ç†ç©ºé—´å¯¹é½è¿›è¡Œ3Dæ¨ç†å’Œé’ˆå¯¹æœºå™¨äººä»»åŠ¡çš„åæœŸè®­ç»ƒé€‚åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éƒ¨åˆ†çº§åˆ«çš„è¿åŠ¨æ ‡è®°åŒ–æ–¹æ³•ï¼Œå®ç°æ¯«ç±³çº§çš„é‡å»ºç²¾åº¦ï¼Œä»¥æ¨¡æ‹Ÿç²¾ç¡®çš„æ‰‹éƒ¨è½¨è¿¹è¿›è¡ŒåŠ¨ä½œå­¦ä¹ ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬æå‡ºçš„èŒƒå¼ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªç»¼åˆæ•°æ®æ•´ç†ç®¡é“ï¼Œè¯¥ç®¡é“æ•´åˆäº†åŒ…æ‹¬åŠ¨ä½œæ•æ‰ã€è™šæ‹Ÿç°å®å’Œä»…RGBè§†é¢‘åœ¨å†…çš„å„ç§æ¥æºï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ•°ç™¾ä¸‡åŸºäºåŠ¨ä½œçš„æ•™å­¦å®ä¾‹ã€‚æˆ‘ä»¬ä»å®è¯ä¸Šå±•ç¤ºäº†Being-H0åœ¨æ‰‹éƒ¨è¿åŠ¨ç”Ÿæˆå’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”å®ƒéšç€æ¨¡å‹å’Œæ•°æ®è§„æ¨¡çš„æ‰©å¤§è€Œè¡¨ç°è‰¯å¥½ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨ç°å®ä¸–ç•Œæœºå™¨äººæ“ä½œåº”ç”¨Being-H0åäº§ç”Ÿäº†é¢„æœŸçš„æ•ˆæœæå‡ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://beingbeyond.github.io/Being-H0%E3%80%82">https://beingbeyond.github.io/Being-H0ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15597v1">PDF</a> 37 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡äººç±»è§†é¢‘æ•°æ®ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºâ€œBeing-H0â€çš„çµå·§è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ã€‚ç°æœ‰VLAsåœ¨å¤„ç†éœ€è¦é«˜çµå·§æ€§çš„å¤æ‚æ“ä½œä»»åŠ¡æ—¶è¡¨ç°æŒ£æ‰ï¼Œå¯¹äºæ–°åœºæ™¯å’Œä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºå­˜åœ¨æ˜¾è‘—æ¨¡æ‹Ÿåˆ°ç°å®å·®è·çš„åˆæˆæ•°æ®æˆ–ç¼ºä¹è§„æ¨¡å’Œå¤šæ ·æ€§çš„é¥æ§æ¼”ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨äººç±»æ‰‹éƒ¨ä½œä¸ºåŸºç¡€æ“çºµå™¨ï¼Œå€ŸåŠ©ç½‘ç»œæ•°æ®çš„ä¸°å¯Œçµå·§æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥ç‰©ç†æŒ‡ä»¤å¾®è°ƒä¸ºä¸­å¿ƒï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„è®­ç»ƒèŒƒå¼ï¼Œå®ƒç»“åˆäº†äººç±»è§†é¢‘çš„å¤§è§„æ¨¡VLAé¢„è®­ç»ƒã€ç”¨äº3Dæ¨ç†çš„ç‰©ç†ç©ºé—´å¯¹é½å’Œç”¨äºæœºå™¨äººä»»åŠ¡çš„åæœŸè®­ç»ƒé€‚åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§éƒ¨åˆ†çº§è¿åŠ¨æ ‡è®°æ–¹æ³•ï¼Œå®ç°äº†æ¯«ç±³çº§çš„é‡å»ºç²¾åº¦ï¼Œä»¥æ¨¡æ‹Ÿæ‰‹éƒ¨è½¨è¿¹è¿›è¡ŒåŠ¨ä½œå­¦ä¹ ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„æè®®èŒƒå¼ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªç»¼åˆæ•°æ®æ•´ç†ç®¡é“ï¼Œè¯¥ç®¡é“æ•´åˆäº†åŠ¨ä½œæ•æ‰ã€è™šæ‹Ÿç°å®å’Œä»…RGBè§†é¢‘ç­‰å¼‚è´¨æ¥æºï¼Œå½¢æˆä¸€ä¸ªåŒ…å«æ•°åƒä¸‡ä¸ªåŠ¨ä½œæŒ‡ä»¤å®ä¾‹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨æ‰‹éƒ¨è¿åŠ¨ç”Ÿæˆå’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢å±•ç¤ºäº†Being-H0çš„å“è¶Šè¡¨ç°ï¼Œå®ƒåœ¨æ¨¡å‹å’Œæ•°æ®å¤§å°æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚åœ¨ç°å®ä¸–ç•Œæœºå™¨äººæ“ä½œåº”ç”¨ç‰©ç†æŒ‡ä»¤å¾®è°ƒåï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°Being-H0çš„ä¼˜å¼‚è¡¨ç°ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è§[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Being-H0æ˜¯ä¸€ä¸ªåŸºäºå¤§è§„æ¨¡äººç±»è§†é¢‘çš„çµå·§è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ã€‚</li>
<li>ç°æœ‰VLAsåœ¨å¤„ç†å¤æ‚æ“ä½œä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ï¼Œæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>åˆ©ç”¨äººç±»æ‰‹éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒåŸºç¡€ï¼Œç»“åˆç½‘ç»œæ•°æ®çš„ä¸°å¯Œæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æå‡ºç‰©ç†æŒ‡ä»¤å¾®è°ƒè®­ç»ƒèŒƒå¼ï¼Œç»“åˆé¢„è®­ç»ƒã€ç‰©ç†ç©ºé—´å¯¹é½å’ŒåæœŸè®­ç»ƒé€‚åº”ã€‚</li>
<li>å¼•å…¥éƒ¨åˆ†çº§è¿åŠ¨æ ‡è®°æ–¹æ³•ï¼Œå®ç°ç²¾ç¡®çš„æ‰‹éƒ¨è½¨è¿¹æ¨¡æ‹Ÿã€‚</li>
<li>å¼€å‘ç»¼åˆæ•°æ®æ•´ç†ç®¡é“ï¼Œæ•´åˆå¤šç§æ¥æºæ•°æ®å½¢æˆå¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b85c162aa759df5f2e0d84e45d44bfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdfaf8078b2a7875c7d3973289baea68.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-04/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-04/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d5ba4411c3480909084ccb331b6501c9.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-04  Probing then Editing Response Personality of Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-40549d220dcc4048b5af741c3260e1b6.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  SpeechFake A Large-Scale Multilingual Speech Deepfake Dataset   Incorporating Cutting-Edge Generation Methods
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
