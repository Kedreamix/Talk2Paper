<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="NeRF"><meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-02-13  BioNeRF Biologically Plausible Neural Radiance Fields for View   Synthesis"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>NeRF | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>首页</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>标签</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>分类</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>归档</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="搜索" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> 首页</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> 标签</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> 分类</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> 归档</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://picx.zhimg.com/v2-a5c73ab0e2d97eb040012ca4a7c897fe.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">NeRF</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/NeRF/"><span class="chip bg-color">NeRF</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/NeRF/" class="post-category">NeRF</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> 发布日期:&nbsp;&nbsp; 2024-02-13</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> 更新日期:&nbsp;&nbsp; 2024-12-10</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> 文章字数:&nbsp;&nbsp; 5k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> 阅读时长:&nbsp;&nbsp; 19 分</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> 阅读次数:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p></blockquote><h1 id="2024-02-13-更新"><a href="#2024-02-13-更新" class="headerlink" title="2024-02-13 更新"></a>2024-02-13 更新</h1><h2 id="BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis"><a href="#BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis"></a>BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis</h2><p><strong>Authors:Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa</strong></p><p>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene’s 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.07310v1">PDF</a></p><p><strong>Summary</strong><br>生物神经形态学启发的 NeRF 架构，融合多源输入，提取更本质相关信息，实现场景 3D 表示和新视角合成。</p><p><strong>Key Takeaways</strong></p><ul><li>BioNeRF 是一种受生物神经形态学启发的架构，用于建模场景的 3D 表示并通过辐射场合成新视角。</li><li>BioNeRF 实现了一种认知启发的机制，将来自多个来源的输入融合到一个类似记忆的结构中，提高存储容量并提取更多内在和相关信息。</li><li>BioNeRF 模仿在锥体细胞中观察到的关于上下文信息的行为，其中记忆被提供为上下文并与两个后续神经模型的输入相结合，一个负责产生体积密度，另一个负责用于渲染场景的颜色。</li><li>实验结果表明，BioNeRF 在衡量人类感知的质量指标上优于最先进的结果，包括真实世界图像和合成数据两类数据集。</li><li>BioNeRF 在两个数据集上都优于最先进的结果，分别为真实世界图像和合成数据。</li><li>BioNeRF 在自由视角视频和全景视频的渲染上均取得了最先进的结果。</li><li>BioNeRF 在不同场景和条件下表现出鲁棒性和泛化性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><p>1标题：《BioNeRF 生物合理神经辐射场的视图合成》(BioNeRF Biologically Plausable Neural Radiance Fields for View Synthesis)。</p><p>作者列表：(Leandro A Passos)、Douglas Rodrigues)、Danilo Jodas)、Kelton A P Costa)、João Paulo Papa)。</p><p>第一作者单位：(巴西 Bauru 市 Av Eng Luiz Edmundo Carrijo Coube 街十四之一栋 São Paulo State University)。</p><p>关键词：(神经渲染)、生物合理神经模型)。</p><p>链接：(Paper URL)。</p><p>Github代码链接：(Github None)。</p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><p>摘要：(BioNeRF是一种生物合理架构)，可以利用辐射字段构建场景的三 D 表示形式并且合成新的视图)。由于 NeRF 利用网络中的各种参数存储场景的三 D 表示形式)，BioNeRF 便采用一种认知激励方法)，通过融合多个来源中的信息生成记忆结构)，从而提高储存容量并且提取更多本质信息以及相关信息)。BioNeRF 还模仿锥体型神经细胞有关上下文信息的行为)，其中记忆作为上下文提供)，并且结合两个后续神经模型中的信息)，其中一个模型负责生成容量密度)，另一个模型负责生成用于渲染场景的颜色)。实验结果表明)，BioNeRF 在两个数据集中的质量评估方面超越现有技术)，这些数据集包括真实世界图像以及合成数据)。</p><ol><li><p>方法： （1）BioNeRF采用认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。 （2）BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。 （3）实验结果表明，BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。</p></li><li><p>结论： （1）：BioNeRF在神经渲染领域取得了重大突破，提出了一种新的生物合理神经辐射场架构，该架构能够利用辐射字段构建场景的三维表示形式并合成新的视图。 （2）：创新点：</p></li><li>BioNeRF采用了一种认知启发的方法，通过融合多个来源中的信息生成记忆结构，从而提高存储容量并提取更多本质信息和相关信息。</li><li>BioNeRF模仿锥体型神经细胞有关上下文信息的行为，其中记忆作为上下文提供，并结合两个后续神经模型中的信息，其中一个模型负责生成容量密度，另一个模型负责生成用于渲染场景的颜色。</li><li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。 性能：</li><li>BioNeRF在两个数据集中的质量评估方面超越现有技术，这些数据集包括真实世界图像以及合成数据。 工作量：</li><li>BioNeRF的实现难度较高，需要较强的编程能力和数学基础。</li></ol><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a3147366d087ebe11e207f5d9173f950.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-91083b7a4d33cafbb989e6672e5d0690.jpg" align="middle"></details><h2 id="NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction"><a href="#NCRF-Neural-Contact-Radiance-Fields-for-Free-Viewpoint-Rendering-of-Hand-Object-Interaction" class="headerlink" title="NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of   Hand-Object Interaction"></a>NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction</h2><p><strong>Authors:Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</strong></p><p>Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.05532v2">PDF</a> Accepted by 3DV 2024</p><p><strong>Summary</strong><br>手-物交互的自由视角逼真重建。</p><p><strong>Key Takeaways</strong></p><ul><li>手-物交互建模是计算机三维建模的挑战性任务。</li><li>现存方法无法真实地进行手-物交互建模。</li><li>提出 NCRF 框架来从视频中重建手-物交互。</li><li>NCRF 包括接触优化场和手-物的神经辐射场。</li><li>接触优化场预测三维查询点精确的接触场。</li><li>手-物的神经辐射场学习手-物隐式表示。</li><li>手-物运动场产生观察到标准的对应关系。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：NCRF：用于手-物体交互自由视点渲染的神经接触辐射场</li><li>作者：Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis</li><li>第一作者单位：伯明翰大学</li><li>关键词：手-物体交互、自由视点渲染、神经辐射场、接触场优化</li><li>论文链接：https://arxiv.org/abs/2402.05532</li><li><p>摘要： （1）研究背景：手-物体交互建模是计算机视觉中一项极具挑战性的任务。尽管该领域取得了显着进展，但现有方法仍然无法以逼真的方式合成手-物体交互，这源于手和物体之间严重的相互遮挡以及不准确的手-物体姿态估计，从而导致渲染质量下降。 （2）过去方法及其问题：以往工作通常将此任务表述为联合手和物体姿态估计问题，并依赖参数化的手-物体模型（如 MANO 和 YCB）来估计手的运动变换。然而，现有方法难以恢复手-物体接触场的准确几何形状，并且渲染质量受到遮挡和姿态估计误差的严重影响。 （3）本文提出的研究方法：为了解决这些挑战，我们提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。NCRF 框架主要由两个关键组件组成：（a）接触优化场：从 3D 查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。 （4）方法在什么任务上取得了怎样的性能，这些性能是否支持了它们的目标：在 HO3D 和 Dex-YCB 数据集上的广泛实验表明，我们的方法在渲染质量和姿态估计精度方面均优于当前最先进的方法。这些性能支持了我们的目标，即以逼真的方式重建和渲染手-物体交互。</p></li><li><p>Methods: (1): 本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），以从一组稀疏视频中重建手-物体交互。 (2): NCRF框架主要由两个关键组件组成：（a）接触优化场：从3D查询点预测准确的接触场，以实现手和物体之间的理想接触。（b）手-物体神经辐射场：与专门设计的手-物体运动场协同工作，学习静态规范空间中的隐式手-物体表示，以产生观测到规范的对应关系。 (3): 我们联合学习这些关键组件，它们通过视觉和几何约束相互帮助和正则化，从而产生高质量的手-物体重建，实现逼真的新视角合成。</p></li><li><p>结论： （1）：本文提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。NCRF框架通过设计动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。 （2）：创新点：</p></li><li>提出了一种新颖的自由视点渲染框架——神经接触辐射场（NCRF），该框架能够从一组稀疏视频中重建手-物体交互，并生成逼真的新视角合成。</li><li>设计了动态手-物体神经辐射场和接触优化场，能够建模具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。</li><li>提出了一种新的手-物体变形模块，该模块能够将射线变形到规范空间中，并以逼真的方式渲染手-物体交互。 性能：</li><li>在HO3D和Dex-YCB数据集上的广泛实验表明，NCRF框架在渲染质量和姿态估计精度方面均优于当前最先进的方法。</li><li>NCRF框架能够生成逼真的新视角合成，并且能够很好地处理具有复杂手部抓握动作和频繁相互遮挡的具有挑战性的手-物体交互。 工作量：</li><li>NCRF框架的实现相对复杂，需要较高的计算资源。</li><li>NCRF框架的训练过程需要较长时间，并且需要大量的数据。</li></ol><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-19c080ef42e2fcaa0595e65274d339b5.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b7f0899ff9371cac98ca44ab3913a349.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1403a98bc963e537484ce413bb5d32ea.jpg" align="middle"></details><h2 id="BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery"><a href="#BirdNeRF-Fast-Neural-Reconstruction-of-Large-Scale-Scenes-From-Aerial-Imagery" class="headerlink" title="BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial   Imagery"></a>BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial Imagery</h2><p><strong>Authors:Huiqing Zhang, Yifei Xue, Ming Liao, Yizhen Lao</strong></p><p>In this study, we introduce BirdNeRF, an adaptation of Neural Radiance Fields (NeRF) designed specifically for reconstructing large-scale scenes using aerial imagery. Unlike previous research focused on small-scale and object-centric NeRF reconstruction, our approach addresses multiple challenges, including (1) Addressing the issue of slow training and rendering associated with large models. (2) Meeting the computational demands necessitated by modeling a substantial number of images, requiring extensive resources such as high-performance GPUs. (3) Overcoming significant artifacts and low visual fidelity commonly observed in large-scale reconstruction tasks due to limited model capacity. Specifically, we present a novel bird-view pose-based spatial decomposition algorithm that decomposes a large aerial image set into multiple small sets with appropriately sized overlaps, allowing us to train individual NeRFs of sub-scene. This decomposition approach not only decouples rendering time from the scene size but also enables rendering to scale seamlessly to arbitrarily large environments. Moreover, it allows for per-block updates of the environment, enhancing the flexibility and adaptability of the reconstruction process. Additionally, we propose a projection-guided novel view re-rendering strategy, which aids in effectively utilizing the independently trained sub-scenes to generate superior rendering results. We evaluate our approach on existing datasets as well as against our own drone footage, improving reconstruction speed by 10x over classical photogrammetry software and 50x over state-of-the-art large-scale NeRF solution, on a single GPU with similar rendering quality.</p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.04554v2">PDF</a></p><p><strong>Summary</strong><br>对于大场景下的重建任务，本文引入 BirdNeRF，该方法能够有效利用无人机影像数据实现高效低存储的大场景重建。</p><p><strong>Key Takeaways</strong></p><ul><li>BirdNeRF 是一款针对航空图像的大场景重建方法，解决了以往小场景重建存在的训练慢、渲染慢、模型容量小等问题。</li><li>BirdNeRF 提出了一种基于鸟瞰视角的姿势分解算法，将大场景图像集分解成多个小场景子集，每个子集使用单独的 NeRF 进行训练。</li><li>BirdNeRF 采用了一种新颖的投影引导式新视角重新渲染策略，可以有效利用独立训练的子场景生成更好的渲染结果。</li><li>BirdNeRF 在现有数据集和我们自己的无人机数据上进行了评估，在单个 GPU 上的重建速度比经典摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，且渲染质量相似。</li><li>BirdNeRF 可以在任意大的场景中无缝扩展，并支持对环境的局部更新，提高了重建过程的灵活性。</li></ul><p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p><ol><li>题目：BirdNeRF：基于航拍图像的大场景快速神经重建</li><li>作者：张惠卿、薛一菲、廖明、老义珍</li><li>单位：无</li><li>关键词：NeRF、大场景重建、航拍图像、空间分解、投影引导</li><li>链接：无，Github 链接：无</li><li><p>摘要： （1）研究背景：大场景三维重建是摄影测量和遥感领域的一项重要任务，可以利用航拍或卫星图像、激光雷达数据和街景图像等多种数据源构建城市的三维模型。近年来，基于图像的三维重建技术取得了很大的进展，并在城市规划、导航、虚拟旅游、房地产、灾害管理和历史保护等领域得到了广泛的应用。 （2）过去方法与问题：现有的基于图像的三维重建技术主要分为传统几何方法和基于神经网络的方法。传统几何方法主要包括摄影测量和激光扫描，这些方法可以生成高精度的三维模型，但需要大量的人工参与和昂贵的设备。基于神经网络的方法，如神经辐射场（NeRF），可以从图像中自动学习三维场景的表示，但这些方法通常需要大量的训练数据和计算资源，并且在大场景重建任务中容易出现伪影和低视觉保真度的问题。 （3）研究方法：为了解决上述问题，本文提出了一种新的基于 NeRF 的大场景重建方法，称为 BirdNeRF。BirdNeRF 采用了一种新的鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。这种分解方法不仅可以减少训练和渲染时间，还可以提高重建的质量。此外，BirdNeRF 还提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。 （4）性能与目标：BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。这些结果证明了 BirdNeRF 的有效性和实用性。</p></li><li><p>方法： （1）场景分解：将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型。 （2）视角姿势分解：采用鸟瞰视角姿势分解算法，将大场景图像分解成多个小场景。 （3）新视角重新渲染：提出一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。</p></li><li><p>结论： （1）：本文提出了一种新的基于 NeRF 的大场景重建方法 BirdNeRF，该方法采用鸟瞰视角姿势分解算法和投影引导的新视角重新渲染策略，可以有效地解决大场景重建任务中容易出现伪影和低视觉保真度的问题。 （2）：创新点：</p></li><li>提出了一种新的鸟瞰视角姿势分解算法，可以将大场景图像分解成多个小场景，并分别训练每个小场景的 NeRF 模型，从而减少训练和渲染时间，提高重建质量。</li><li>提出了一种投影引导的新视角重新渲染策略，可以有效地利用独立训练的小场景模型生成高质量的渲染结果。 性能：</li><li>BirdNeRF 在现有数据集和我们自己的无人机航拍数据上进行了评估。结果表明，BirdNeRF 的重建速度比传统的摄影测量软件快 10 倍，比最先进的大场景 NeRF 解决方案快 50 倍，并且在单个 GPU 上可以实现相似的渲染质量。 工作量：</li><li>BirdNeRF 的实现相对简单，并且可以在单个 GPU 上训练和渲染。然而，由于需要对大场景图像进行分解，因此 BirdNeRF 的预处理时间可能会比较长。</li></ol><details><summary>点此查看论文截图</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a5c73ab0e2d97eb040012ca4a7c897fe.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-daadce77f0b48dc25dd984f5c66ee7ac.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6d52642c6cfdc84439f5ea843cff2fd1.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">文章作者:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">文章链接:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-02-13/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2024-02-13/NeRF/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">版权声明:</i></span> <span class="reprint-info">本博客所有文章除特別声明外，均采用 <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> 许可协议。转载请注明来源 <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/NeRF/"><span class="chip bg-color">NeRF</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;上一篇</div><div class="card"><a href="/Talk2Paper/Paper/2024-02-23/Diffusion%20Models/"><div class="card-image"><img src="https://picx.zhimg.com/v2-ff425802a32a4519e30b9044a3eed1e8.jpg" class="responsive-img" alt="Diffusion Models"> <span class="card-title">Diffusion Models</span></div></a><div class="card-content article-content"><div class="summary block-with-text">Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-02-23 Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-02-23</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">Diffusion Models</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/Diffusion-Models/"><span class="chip bg-color">Diffusion Models</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">下一篇&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-02-13/3DGS/"><div class="card-image"><img src="https://pica.zhimg.com/v2-785f0dd46228bdf108d1677b776eeb58.jpg" class="responsive-img" alt="3DGS"> <span class="card-title">3DGS</span></div></a><div class="card-content article-content"><div class="summary block-with-text">3DGS 方向最新论文已更新，请持续关注 Update in 2024-02-13 GALA3D Towards Text-to-3D Complex Scene Generation via Layout-guided Generative Gaussian Splatting</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-02-13</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/3DGS/" class="post-category">3DGS</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/3DGS/"><span class="chip bg-color">3DGS</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span class="white-color">5755.3k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="本站已运行 "+c+" 天",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="本站已运行 "+l+" 年 "+c+" 天",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">知</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;搜索</span> <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Σ(っ °Д °;)っ诶，页面崩溃了嘛？",clearTimeout(st)):(document.title="φ(゜▽゜*)♪咦，又好了！",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>