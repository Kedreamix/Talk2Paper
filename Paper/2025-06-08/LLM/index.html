<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  SparseMM Head Sparsity Emerges from Visual Concept Responses in MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-31593c9a93ac5fd04bd000716e4cef24.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-08-æ›´æ–°"><a href="#2025-06-08-æ›´æ–°" class="headerlink" title="2025-06-08 æ›´æ–°"></a>2025-06-08 æ›´æ–°</h1><h2 id="SparseMM-Head-Sparsity-Emerges-from-Visual-Concept-Responses-in-MLLMs"><a href="#SparseMM-Head-Sparsity-Emerges-from-Visual-Concept-Responses-in-MLLMs" class="headerlink" title="SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"></a>SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs</h2><p><strong>Authors:Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu</strong></p>
<p>Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at <a target="_blank" rel="noopener" href="https://github.com/CR400AF-A/SparseMM">https://github.com/CR400AF-A/SparseMM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šå¸¸æ˜¯é€šè¿‡åœ¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºç¡€ä¸Šå¢å¼ºå…¶è§†è§‰èƒ½åŠ›è€Œå¾—åˆ°çš„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†ææ³¨æ„åŠ›æœºåˆ¶æ¥ç ”ç©¶MLLMså¦‚ä½•å¤„ç†è§†è§‰è¾“å…¥ã€‚æˆ‘ä»¬æ­ç¤ºäº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„ç¨€ç–ç°è±¡ï¼šåœ¨LLMsä¸­ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†ï¼ˆå¤§çº¦ä¸åˆ°5%ï¼‰çš„æ³¨æ„åŠ›å¤´å¯¹è§†è§‰ç†è§£æœ‰ç§¯æè´¡çŒ®ï¼Œè¿™äº›è¢«ç§°ä¸ºè§†è§‰å¤´ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è¯†åˆ«è¿™äº›å¤´éƒ¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ— è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å“åº”åˆ†æé‡åŒ–å¤´éƒ¨çº§åˆ«çš„è§†è§‰ç›¸å…³æ€§ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†SparseMMï¼Œè¿™æ˜¯ä¸€ç§KV-Cacheä¼˜åŒ–ç­–ç•¥ï¼Œæ ¹æ®è§†è§‰åˆ†æ•°ä¸ºLLMsä¸­çš„å¤´éƒ¨åˆ†é…ä¸å¯¹ç§°çš„è®¡ç®—é¢„ç®—ï¼Œåˆ©ç”¨è§†è§‰å¤´éƒ¨çš„ç¨€ç–æ€§æ¥åŠ é€ŸMLLMsçš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ä¹‹å‰å¿½ç•¥è§†è§‰ç‰¹æ€§çš„KV-CacheåŠ é€Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSparseMMåœ¨è§£ç è¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘å‹åŠ›å’Œä¿ç•™è§†è§‰è¯­ä¹‰ã€‚åœ¨ä¸»æµçš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSparseMMå®ç°äº†å“è¶Šçš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒSparseMMå®ç°äº†1.38å€çš„å®æ—¶åŠ é€Ÿå’Œ52%çš„å†…å­˜å‡å°‘ï¼ŒåŒæ—¶åœ¨æ•ˆç‡æµ‹è¯•ä¸Šä¿æŒäº†æ€§èƒ½ä¸Šçš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„é¡¹ç›®å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/CR400AF-A/SparseMM%E3%80%82">https://github.com/CR400AF-A/SparseMMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05344v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚ä½•å¤„ç†è§†è§‰è¾“å…¥ï¼Œé€šè¿‡å¯¹å…¶æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œåˆ†æï¼Œæ­ç¤ºäº†ä¸€ç§æƒŠäººçš„ç¨€ç–ç°è±¡ï¼šåªæœ‰ä¸€å°éƒ¨åˆ†ï¼ˆå¤§çº¦å°‘äº5%ï¼‰çš„æ³¨æ„åŠ›å¤´å¯¹è§†è§‰ç†è§£æœ‰ç§¯æè´¡çŒ®ï¼Œè¢«ç§°ä¸ºè§†è§‰å¤´ã€‚åŸºäºæ­¤å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºKV-Cacheä¼˜åŒ–çš„SparseMMç­–ç•¥ï¼Œæ ¹æ®è§†è§‰åˆ†æ•°å¯¹LLMä¸­çš„å¤´è¿›è¡Œä¸å¯¹ç§°è®¡ç®—é¢„ç®—åˆ†é…ï¼Œåˆ©ç”¨è§†è§‰å¤´çš„ç¨€ç–æ€§åŠ é€ŸMLLMçš„æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒSparseMMåœ¨ä¸»æµçš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ï¼Œå®ç°äº†1.38å€çš„å®æ—¶åŠ é€Ÿå’Œ52%çš„å†…å­˜å‡å°‘ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ä¸å˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯é€šè¿‡æ‰©å±•é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¹¶åŠ å…¥è§†è§‰èƒ½åŠ›å½¢æˆçš„ã€‚</li>
<li>MLLMså¤„ç†è§†è§‰è¾“å…¥æ—¶ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†æ³¨æ„åŠ›å¤´ï¼ˆè§†è§‰å¤´ï¼‰å¯¹è§†è§‰ç†è§£æœ‰è´¡çŒ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„åŸ¹è®­æ¡†æ¶ï¼Œé€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å“åº”åˆ†ææ¥é‡åŒ–å¤´éƒ¨çº§åˆ«çš„è§†è§‰ç›¸å…³æ€§ã€‚</li>
<li>åŸºäºè¿™ä¸€å‘ç°ï¼Œå¼•å…¥äº†SparseMMç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§KV-Cacheä¼˜åŒ–ç­–ç•¥ï¼Œæ ¹æ®è§†è§‰åˆ†æ•°ä¸ºLLMä¸­çš„å¤´åˆ†é…ä¸å¯¹ç§°çš„è®¡ç®—é¢„ç®—ã€‚</li>
<li>SparseMMåˆ©ç”¨è§†è§‰å¤´çš„ç¨€ç–æ€§æ¥åŠ é€ŸMLLMçš„æ¨ç†ã€‚</li>
<li>ä¸å¿½ç•¥è§†è§‰ç‰¹æ€§çš„KV-CacheåŠ é€Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒSparseMMåœ¨è§£ç è¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘å‹åŠ›å¹¶ä¿æŒè§†è§‰è¯­ä¹‰çš„ä¿ç•™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94b4af2a8ec6667d9e77844228f14e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c0948d8004001be27bc612e7ae61472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0e48b37aae6834874fb7b87056d9906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f972cb95a84bf8a1d75596e74aeb485.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VideoMolmo-Spatio-Temporal-Grounding-Meets-Pointing"><a href="#VideoMolmo-Spatio-Temporal-Grounding-Meets-Pointing" class="headerlink" title="VideoMolmo: Spatio-Temporal Grounding Meets Pointing"></a>VideoMolmo: Spatio-Temporal Grounding Meets Pointing</h2><p><strong>Authors:Ghazi Shazan Ahmad, Ahmed Heakl, Hanan Gani, Abdelrahman Shaker, Zhiqiang Shen, Ranjay Krishna, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/VideoMolmo">https://github.com/mbzuai-oryx/VideoMolmo</a>. </p>
<blockquote>
<p>æ—¶ç©ºå®šä½åœ¨å¤šä¸ªé¢†åŸŸï¼ˆä»ç”Ÿç‰©ç ”ç©¶åˆ°è‡ªä¸»å¯¼èˆªå’Œäº¤äº’å¼ç•Œé¢ï¼‰çš„ç²¾ç¡®äº¤äº’ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å°½ç®¡å½“å‰åŸºäºè§†é¢‘çš„æ–¹æ³•åœ¨è·Ÿè¸ªæ–¹é¢éå¸¸ç†Ÿç»ƒï¼Œä½†å®ƒä»¬ç¼ºä¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ä¸Šä¸‹æ–‡ç†è§£å’Œæ³›åŒ–æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†VideoMolmoï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¸“ä¸ºç²¾ç»†çš„æ—¶ç©ºå®šä½è€Œè®¾è®¡ï¼Œè¯¥å®šä½åŸºäºæ–‡æœ¬æè¿°ã€‚VideoMolmoå»ºç«‹åœ¨Molmoæ¶æ„ä¹‹ä¸Šï¼Œå¹¶åŠ å…¥äº†ä¸€ä¸ªæ—¶é—´æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶å°†æ¯ä¸€å¸§ç½®äºå…ˆå‰çš„å¸§ä¸Šï¼Œç¡®ä¿æ—¶é—´çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ–°é¢–çš„æ—¶é—´æ©è†œèåˆç®¡é“é‡‡ç”¨SAM2è¿›è¡ŒåŒå‘ç‚¹ä¼ æ’­ï¼Œæ˜¾è‘—æé«˜äº†è§†é¢‘åºåˆ—ä¹‹é—´çš„è¿è´¯æ€§ã€‚è¿™ç§ä¸¤æ­¥åˆ†è§£æ³•ï¼Œå³é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç²¾ç¡®çš„æŒ‡å‘åæ ‡ï¼Œç„¶åä¾èµ–äºé¡ºåºæ©è†œèåˆæ¨¡å—æ¥äº§ç”Ÿè¿è´¯çš„åˆ†å‰²ï¼Œä¸ä»…ç®€åŒ–äº†è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ï¼Œè¿˜æé«˜äº†å¯è§£é‡Šæ€§ã€‚ç”±äºç¼ºä¹åˆé€‚çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«7.2ä¸‡å¯¹è§†é¢‘å­—å¹•å’Œæ ‡æ³¨çš„10ä¸‡ä¸ªç›®æ ‡ç‚¹ã€‚ä¸ºäº†è¯„ä¼°VideoMolmoçš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†VPoS-Benchè¿™ä¸€é¢‡å…·æŒ‘æˆ˜æ€§çš„åˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•å¹³å°ï¼Œå®ƒæ¶µç›–äº”ç§ç°å®åœºæ™¯ï¼šç»†èƒè¿½è¸ªã€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§‰ã€è‡ªåŠ¨é©¾é©¶ã€è§†é¢‘ç•Œé¢äº¤äº’å’Œæœºå™¨äººæŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆRefer-VOSï¼‰å’Œæ¨ç†VOSä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒVideoMolmoå¤§å¤§æé«˜äº†æ—¶ç©ºå®šä½ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/VideoMolmo%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mbzuai-oryx/VideoMolmoå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05336v1">PDF</a> 20 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VideoMolmoï¼Œä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¸“ä¸ºç²¾ç»†æ—¶ç©ºå®šä½è€Œè®¾è®¡ï¼Œå¯åŸºäºæ–‡æœ¬æè¿°è¿›è¡Œç²¾ç¡®äº’åŠ¨ã€‚å®ƒé€šè¿‡ç»“åˆè¯­è¨€æ¨¡å‹å’Œæ—¶åºæ¨¡å—ï¼Œæé«˜äº†è§†é¢‘ä¸­çš„æ—¶ç©ºå®šä½ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå…¶ç‹¬ç‰¹çš„æ—¶é—´æ©è†œèåˆç®¡é“å¢å¼ºäº†è§†é¢‘åºåˆ—çš„è¿è´¯æ€§ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°è¯¥æ¨¡å‹ï¼Œä½œè€…åˆ›å»ºäº†ä¸€ä¸ªå¤§å‹è§†é¢‘æ ‡æ³¨æ•°æ®é›†VPoS-Benchå’Œä¸€ä¸ªæŒ‘æˆ˜æ€§çš„è·¨åŸŸåŸºå‡†æµ‹è¯•ã€‚æ€»ä½“ä¸Šï¼ŒVideoMolmoæ˜¾è‘—æé«˜äº†æ—¶ç©ºå®šä½ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoMolmoæ˜¯ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºç²¾ç»†çš„æ—¶ç©ºå®šä½ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†æ–‡æœ¬æè¿°è¿›è¡Œæ—¶ç©ºå®šä½ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>VideoMolmoé‡‡ç”¨äº†æ–°é¢–çš„æ—¶ç©ºæ©è†œèåˆç®¡é“ä»¥æé«˜è§†é¢‘åºåˆ—è¿è´¯æ€§ã€‚</li>
<li>ä¸ºè®­ç»ƒå’Œè¯„ä¼°è¯¥æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«å¤§é‡è§†é¢‘æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†VPoS-Benchã€‚</li>
<li>VideoMolmoæ¨¡å‹å¼•å…¥äº†æ–°çš„è·¨åŸŸåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å…¶æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹å¯å¹¿æ³›åº”ç”¨äºä¸åŒé¢†åŸŸå¦‚ç”Ÿç‰©å­¦ç ”ç©¶ã€è‡ªåŠ¨é©¾é©¶å’Œäº¤äº’å¼ç•Œé¢ç­‰ã€‚</li>
<li>VideoMolmoåœ¨æ—¶ç©ºå®šä½ç²¾åº¦å’Œæ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c9b98b6441c68dc42b2b1c7f19ac475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab323253ebe927af8707dc9cbc00af0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c88b08092c6cc1cf856169e3f8c97e91.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Search-Arena-Analyzing-Search-Augmented-LLMs"><a href="#Search-Arena-Analyzing-Search-Augmented-LLMs" class="headerlink" title="Search Arena: Analyzing Search-Augmented LLMs"></a>Search Arena: Analyzing Search-Augmented LLMs</h2><p><strong>Authors:Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez</strong></p>
<p>Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the modelâ€™s parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/lmarena/search-arena">https://github.com/lmarena/search-arena</a>. </p>
<blockquote>
<p>æœç´¢å¢å¼ºå‹è¯­è¨€æ¨¡å‹ç»“åˆäº†ç½‘ç»œæœç´¢ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥æé«˜å“åº”çš„çœŸå®æ€§å’Œå®æ—¶æ€§ã€‚ç„¶è€Œï¼Œåˆ†æè¿™äº›ç³»ç»Ÿä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šç°æœ‰æ•°æ®é›†è§„æ¨¡æœ‰é™ï¼ŒèŒƒå›´ç‹­çª„ï¼Œé€šå¸¸ä»…é™äºé™æ€ã€å•è½®çš„äº‹å®æ ¸æŸ¥é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Search Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€ä¼—æºçš„äººç±»åå¥½æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡24,000ä¸ªä¸æœç´¢å¢å¼ºå‹LLMçš„å¤šè½®ç”¨æˆ·äº¤äº’é…å¯¹ã€‚è¯¥æ•°æ®é›†æ¶µç›–å¤šæ ·çš„æ„å›¾å’Œè¯­è¨€ï¼ŒåŒ…å«çº¦12,000æ¬¡äººç±»åå¥½æŠ•ç¥¨çš„å®Œæ•´ç³»ç»Ÿè·Ÿè¸ªè®°å½•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œç”¨æˆ·åå¥½å—åˆ°å¼•ç”¨æ•°é‡çš„å½±å“ï¼Œå³ä½¿å¼•ç”¨çš„å†…å®¹å¹¶æ²¡æœ‰ç›´æ¥æ”¯æŒæ‰€å½’å±çš„ä¸»å¼ ï¼Œè¿™æ­ç¤ºäº†æ„ŸçŸ¥å¯ä¿¡åº¦å’Œå®é™…å¯ä¿¡åº¦ä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œä¸åŒæ¥æºçš„ç”¨æˆ·åå¥½å„ä¸ç›¸åŒï¼Œè¡¨æ˜ç¤¾åŒºé©±åŠ¨çš„å¹³å°é€šå¸¸æ›´å—æ¬¢è¿ï¼Œè€Œé™æ€ç™¾ç§‘å…¨ä¹¦æ¥æºå¹¶ä¸æ€»æ˜¯é€‚å½“å’Œå¯é ã€‚ä¸ºäº†è¯„ä¼°ä¸åŒç¯å¢ƒä¸‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹è¯•æœç´¢å¢å¼ºå‹LLMåœ¨é€šç”¨èŠå¤©ç¯å¢ƒå’Œä¼ ç»ŸLLMåœ¨æœç´¢å¯†é›†å‹ç¯å¢ƒä¸­çš„è¡¨ç°æ¥è¿›è¡Œè·¨é¢†åŸŸåˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨éæœç´¢ç¯å¢ƒä¸­ï¼Œç½‘ç»œæœç´¢å¹¶ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æé«˜æ€§èƒ½ï¼›ç„¶è€Œï¼Œå¦‚æœåœ¨ä»…ä¾èµ–æ¨¡å‹çš„å‚æ•°çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæœç´¢ç¯å¢ƒä¸­çš„è´¨é‡ä¼šæ˜¾è‘—å—åˆ°å½±å“ã€‚æˆ‘ä»¬å…¬å¼€äº†æ•°æ®é›†ï¼Œä»¥æ”¯æŒæœªæ¥åœ¨è¿™æ–¹é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lmarena/search-arena%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lmarena/search-arenaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05334v1">PDF</a> Preprint. Code: <a target="_blank" rel="noopener" href="https://github.com/lmarena/search-arena">https://github.com/lmarena/search-arena</a>. Dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lmarena-ai/search-arena-24k">https://huggingface.co/datasets/lmarena-ai/search-arena-24k</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ï¼ˆSearch-augmented LLMsï¼‰çš„ç ”ç©¶ã€‚ä¸ºæé«˜å“åº”çš„æ‰å®æ€§å’Œæ–°é¢–æ€§ï¼Œç ”ç©¶è€…å°†ç½‘é¡µæœç´¢ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆã€‚ä¸ºåˆ†æè¿™äº›ç³»ç»Ÿï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†ä¸€æ¬¾åä¸ºSearch Arenaçš„å¤§å‹ã€äººç¾¤å‚ä¸çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–äº†è¶…è¿‡2ä¸‡å››åƒæ¬¡é…å¯¹çš„å¤šè½®ç”¨æˆ·ä¸æœç´¢å¢å¼ºLLMsçš„äº’åŠ¨ã€‚åˆ†ææ˜¾ç¤ºï¼Œç”¨æˆ·åå¥½å—å¼•ç”¨æ•°é‡å½±å“ï¼Œå³ä½¿å¼•ç”¨çš„å†…å®¹å¹¶æœªç›´æ¥æ”¯æŒæ‰€å£°æ˜çš„è§‚ç‚¹ï¼Œæ„ŸçŸ¥å¯ä¿¡åº¦ä¸å®é™…å¯ä¿¡åº¦ä¹‹é—´å­˜åœ¨å·®è·ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯¹å¼•ç”¨æ¥æºçš„åå¥½å„å¼‚ï¼Œç¤¾åŒºé©±åŠ¨å¹³å°æ™®éå—æ¬¢è¿ï¼Œè€Œé™æ€ç™¾ç§‘å…¨ä¹¦æ¥æºå¹¶éå§‹ç»ˆé€‚å½“å’Œå¯é ã€‚è¯„ä¼°ä¸åŒè®¾ç½®ä¸­çš„æ€§èƒ½æ—¶ï¼Œç ”ç©¶å‘ç°ç½‘é¡µæœç´¢åœ¨éæœç´¢ç¯å¢ƒä¸­å¹¶ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æé«˜æ€§èƒ½ï¼›ä½†å¦‚æœä»…ä¾èµ–æ¨¡å‹çš„å‚æ•°çŸ¥è¯†ï¼Œåˆ™åœ¨æœç´¢ç¯å¢ƒä¸­çš„è´¨é‡ä¼šæ˜æ˜¾å—åˆ°å½±å“ã€‚æ•°æ®é›†å·²å¼€æºï¼Œæ”¯æŒæœªæ¥ç›¸å…³ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ç»“åˆäº†ç½‘é¡µæœç´¢å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥æé«˜å“åº”çš„æ‰å®æ€§å’Œæ–°é¢–æ€§ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’ŒèŒƒå›´ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå¸¸å±€é™äºé™æ€ã€å•è½®ã€æŸ¥è¯äº‹å®çš„é—®é¢˜ã€‚</li>
<li>æ¨å‡ºåä¸ºSearch Arenaçš„å¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2ä¸‡å››åƒæ¬¡é…å¯¹çš„å¤šè½®ç”¨æˆ·ä¸æœç´¢å¢å¼ºLLMsçš„äº’åŠ¨è®°å½•ã€‚</li>
<li>ç”¨æˆ·åå¥½å—å¼•ç”¨æ•°é‡å½±å“ï¼Œå³ä½¿å¼•ç”¨çš„å†…å®¹æœªç›´æ¥æ”¯æŒè§‚ç‚¹ï¼Œä¹Ÿå­˜åœ¨æ„ŸçŸ¥å¯ä¿¡åº¦ä¸å®é™…å¯ä¿¡åº¦çš„å·®è·ã€‚</li>
<li>ç”¨æˆ·å¯¹å¼•ç”¨æ¥æºçš„åå¥½ä¸åŒï¼Œç¤¾åŒºé©±åŠ¨å¹³å°æ›´å—æ¬¢è¿ï¼Œé™æ€ç™¾ç§‘å…¨ä¹¦æ¥æºå¹¶éå§‹ç»ˆå¯é ã€‚</li>
<li>ç½‘é¡µæœç´¢åœ¨éæœç´¢ç¯å¢ƒä¸­ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æé«˜æ€§èƒ½ï¼›ä½†åœ¨æœç´¢ç¯å¢ƒä¸­ï¼Œä»…ä¾èµ–æ¨¡å‹çš„å‚æ•°çŸ¥è¯†ä¼šå½±å“è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05430d139604a753f03bf8efcdf96cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1343e16ceedd5c8cf039766c3b2e50b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf03a2e5dac535e8c8bb9d5f0090cdb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eada50c04826377a4b06a565b4a92e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bcd0bc61c0e712d944b9bbb67af91f9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning"><a href="#MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning" class="headerlink" title="MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning"></a>MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning</h2><p><strong>Authors:Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li</strong></p>
<p>Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a> </p>
<blockquote>
<p>â€œé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¿æ³›æé«˜äº†æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†å°†å…¶æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰å·¥ä½œè¦ä¹ˆé‡‡ç”¨ç±»ä¼¼çš„æ–‡æœ¬æ¨ç†è¿›è¡Œå›¾åƒè¾“å…¥ï¼Œè¦ä¹ˆå¯»æ±‚å°†è§†è§‰ä¿¡å·èå…¥æ•°å­¦CoTã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢é¢ä¸´ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šä¾èµ–ç²—ç²’åº¦çš„æ¡†çŠ¶å›¾åƒåŒºåŸŸã€è§†è§‰ç¼–ç å™¨å¯¹æ•°å­¦å†…å®¹çš„æ„ŸçŸ¥æœ‰é™ï¼Œä»¥åŠä¾èµ–å¤–éƒ¨èƒ½åŠ›è¿›è¡Œè§†è§‰ä¿®æ”¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºMINT-CoTï¼Œå¼•å…¥ç”¨äºé“¾å¼æ€ç»´è§†è§‰æ¨ç†çš„æ•°å­¦äº¤ç»‡ä»¤ç‰Œï¼ˆMathematical INterleaved Tokensï¼‰ã€‚MINT-CoTé€šè¿‡äº¤ç»‡ä»¤ç‰Œè‡ªé€‚åº”åœ°å°†ç›¸å…³è§†è§‰ä»¤ç‰Œäº¤ç»‡åˆ°æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸­ï¼Œè¯¥ä»¤ç‰ŒåŠ¨æ€é€‰æ‹©æ•°å­¦å›¾å½¢å†…çš„ä»»ä½•å½¢çŠ¶çš„è§†è§‰åŒºåŸŸã€‚ä¸ºäº†æ”¯æŒæ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†MINT-CoTæ•°æ®é›†ï¼ŒåŒ…å«54Kä¸ªæ•°å­¦é—®é¢˜ï¼Œæ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½ä¸ä»¤ç‰Œçº§åˆ«çš„è§†è§‰åŒºåŸŸå¯¹é½ï¼Œå¹¶é…æœ‰ä¸¥æ ¼çš„æ•°æ®ç”Ÿæˆæµç¨‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„MINT-CoTè®­ç»ƒç­–ç•¥ï¼Œé€æ­¥ç»“åˆçº¯æ–‡æœ¬CoTSFTã€äº¤ç»‡CoTSFTå’Œäº¤ç»‡CoTRLï¼Œä»è€Œè¡ç”Ÿå‡ºæˆ‘ä»¬çš„MINT-CoT-7Bæ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°å­¦é¢†åŸŸè¿›è¡Œæœ‰æ•ˆçš„è§†è§‰äº¤ç»‡æ¨ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒMINT-CoT-7Båœ¨MathVistaä¸Šè¶…è¶ŠåŸºå‡†æ¨¡å‹+34.08%ï¼Œåœ¨GeoQAä¸Š+28.78%ï¼Œåœ¨MMStarä¸Š+23.2%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xinyan-cxy/MINT-CoTæ‰¾åˆ°ã€‚</a>â€œ</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05331v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ•°å­¦é¢†åŸŸä¸­ï¼ŒChain-of-Thoughtï¼ˆCoTï¼‰åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„åº”ç”¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¾èµ–ç²—ç²’åº¦å›¾åƒåŒºåŸŸã€å¯¹æ•°å­¦é—®é¢˜è§†è§‰ç¼–ç çš„è®¤çŸ¥å—é™ä»¥åŠå¯¹è§†è§‰ä¿®æ”¹çš„å¤–éƒ¨èƒ½åŠ›ä¾èµ–ç­‰ä¸‰ä¸ªå…³é”®å±€é™æ€§ã€‚æœ¬æ–‡æå‡ºäº†MINT-CoTï¼Œå¼•å…¥äº†æ•°å­¦äº¤ç»‡ä»¤ç‰Œï¼ˆMathematical INterleaved Tokensï¼‰è¿›è¡Œè§†è§‰æ¨ç†ã€‚MINT-CoTè‡ªé€‚åº”åœ°å°†ç›¸å…³è§†è§‰ä»¤ç‰Œæ’å…¥åˆ°æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸­ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©æ•°å­¦å›¾å½¢å†…çš„ä»»ä½•å½¢çŠ¶è§†è§‰åŒºåŸŸæ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†æ”¯æŒæ­¤åŠŸèƒ½ï¼Œæ„å»ºäº†åŒ…å«54Kæ•°å­¦é—®é¢˜çš„MINT-CoTæ•°æ®é›†ï¼Œæ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½ä¸è§†è§‰åŒºåŸŸåœ¨ä»¤ç‰Œçº§åˆ«å¯¹é½ã€‚åŒæ—¶ä»‹ç»äº†ä¸‰ä¸ªé˜¶æ®µçš„MINT-CoTè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬çº¯æ–‡æœ¬CoTçš„SFTã€äº¤ç»‡CoTçš„SFTå’Œäº¤ç»‡CoTçš„RLã€‚å®éªŒç»“æœè¯æ˜äº†MINT-CoTçš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­MINT-CoT-7Bæ¨¡å‹åœ¨MathVistaã€GeoQAå’ŒMMStarä¸Šçš„æ€§èƒ½åˆ†åˆ«ä¼˜äºåŸºçº¿æ¨¡å‹+34.08%ã€+28.78%å’Œ+23.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MINT-CoTè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å°†Chain-of-Thoughtï¼ˆCoTï¼‰åº”ç”¨äºå¤šæ¨¡æ€é¢†åŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä¸­å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šä¾èµ–ç²—ç²’åº¦å›¾åƒåŒºåŸŸã€å¯¹æ•°å­¦é—®é¢˜è§†è§‰ç¼–ç çš„è®¤çŸ¥å—é™ä»¥åŠå¯¹è§†è§‰ä¿®æ”¹çš„å¤–éƒ¨èƒ½åŠ›ä¾èµ–ã€‚</li>
<li>MINT-CoTå¼•å…¥äº†æ•°å­¦äº¤ç»‡ä»¤ç‰Œï¼ˆMathematical INterleaved Tokensï¼‰è¿›è¡Œè§†è§‰æ¨ç†ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°å°†ç›¸å…³è§†è§‰ä»¤ç‰Œæ’å…¥æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸­ã€‚</li>
<li>MINT-CoTä½¿ç”¨åŠ¨æ€é€‰æ‹©æ•°å­¦å›¾å½¢å†…ä»»ä½•å½¢çŠ¶è§†è§‰åŒºåŸŸçš„Interleave TokenæŠ€æœ¯ã€‚</li>
<li>MINT-CoTæ•°æ®é›†åŒ…å«54Kæ•°å­¦é—®é¢˜ï¼Œæ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½ä¸è§†è§‰åŒºåŸŸåœ¨ä»¤ç‰Œçº§åˆ«å¯¹é½ã€‚</li>
<li>MINT-CoTè®­ç»ƒç­–ç•¥é‡‡ç”¨ä¸‰ä¸ªé˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬çº¯æ–‡æœ¬CoTçš„SFTã€äº¤ç»‡CoTçš„SFTå’Œäº¤ç»‡CoTçš„RLã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e0e517d4f8b10aa9ca2f54ac60ba249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ac84bdb90b912b32fbef78fa3d2fff9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e516bcb6bcba5b8144ca314a595fcedb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568fe4ddee8c914dbbeabe58cb026b37.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Constrained-Entropic-Unlearning-A-Primal-Dual-Framework-for-Large-Language-Models"><a href="#Constrained-Entropic-Unlearning-A-Primal-Dual-Framework-for-Large-Language-Models" class="headerlink" title="Constrained Entropic Unlearning: A Primal-Dual Framework for Large   Language Models"></a>Constrained Entropic Unlearning: A Primal-Dual Framework for Large   Language Models</h2><p><strong>Authors:Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab</strong></p>
<p>Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­è¶Šæ¥è¶Šéœ€è¦é—å¿˜æ•æ„Ÿã€è¿‡æ—¶æˆ–ä¸“æœ‰ä¿¡æ¯ã€‚ç°æœ‰çš„é—å¿˜æ–¹æ³•é€šå¸¸å°†é—å¿˜å’Œä¿ç•™åˆ¶å®šä¸ºè§„èŒƒåŒ–çš„æƒè¡¡ï¼Œå°†ä¸¤ä¸ªç›®æ ‡ç»“åˆæˆä¸€ä¸ªå•ä¸€çš„æ ‡é‡åŒ–æŸå¤±ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šï¼Œä»¥åŠåœ¨ä¿ç•™æ•°æ®ä¸Šçš„æ€§èƒ½ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨å¼ºçƒˆçš„é—å¿˜æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„LLMé—å¿˜å…¬å¼ï¼Œå°†å…¶è§†ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼šé€šè¿‡æ–°çš„logit-marginå¹³å¦æŸå¤±å¼ºåˆ¶æ‰§è¡Œé—å¿˜ï¼Œè¯¥æŸå¤±æ˜¾å¼åœ°å°†è¾“å‡ºåˆ†å¸ƒæ¨å‘æŒ‡å®šé—å¿˜é›†ä¸Šçš„å‡åŒ€åˆ†å¸ƒï¼ŒåŒæ—¶é€šè¿‡åœ¨å•ç‹¬çš„ä¿ç•™é›†ä¸Šè®¾ç½®ç¡¬çº¦æŸæ¥ä¿ç•™ä¿ç•™ä¿¡æ¯ã€‚ä¸åŸºäºç†µçš„ç›®æ ‡ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æŸå¤±æ— softmaxï¼Œæ•°å€¼ç¨³å®šï¼Œä¿æŒéé›¶æ¢¯åº¦ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆå’Œç¨³å¥çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨å¯æ‰©å±•çš„åŸå¯¹å¶ç®—æ³•è§£å†³çº¦æŸé—®é¢˜ï¼Œé€šè¿‡åŒå˜é‡åŠ¨æ€æš´éœ²é—å¿˜å’Œä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚åœ¨TOFUå’ŒMUSEåŸºå‡†æµ‹è¯•ä¸Šå¯¹å¤šç§LLMæ¶æ„çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆåŒ¹é…æˆ–è¶…è¿‡æœ€æ–°åŸºçº¿ï¼Œåœ¨å»é™¤ç›®æ ‡ä¿¡æ¯çš„åŒæ—¶ä¿æŒä¸‹æ¸¸å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05314v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨ç°å®ä¸–ç•Œä¸­æ—¶ï¼Œéœ€è¦å»é™¤æ•æ„Ÿã€è¿‡æ—¶æˆ–ä¸“æœ‰ä¿¡æ¯ã€‚ç°æœ‰å»å­¦ä¹ æ–¹æ³•é€šå¸¸å°†é—å¿˜å’Œä¿ç•™è¡¨è¿°ä¸ºæ­£åˆ™åŒ–çš„æƒè¡¡é—®é¢˜ï¼Œå¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®šå’Œå¯¹ä¿ç•™æ•°æ®çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå°†å¤§è¯­è¨€æ¨¡å‹å»å­¦ä¹ ä½œä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜æ¥è¡¨è¿°ï¼Œé€šè¿‡æ–°é¢–çš„logit-marginå¹³å¦æŸå¤±å¼ºåˆ¶è¾“å‡ºåˆ†å¸ƒå‡åŒ€åŒ–ä»¥å®ç°é—å¿˜ï¼ŒåŒæ—¶é€šè¿‡ç¡¬çº¦æŸåœ¨ä¿ç•™é›†ä¸Šä¿ç•™ä¿ç•™ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æŸå¤±å‡½æ•°æ— éœ€softmaxï¼Œæ•°å€¼ç¨³å®šï¼Œä¿æŒéé›¶æ¢¯åº¦ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„ä¼˜åŒ–ã€‚é‡‡ç”¨å¯æ‰©å±•çš„åŸç”Ÿå¯¹å¶ç®—æ³•è§£å†³çº¦æŸé—®é¢˜ï¼Œé€šè¿‡å¯¹å¶å˜é‡çš„åŠ¨æ€æ­ç¤ºé—å¿˜å’Œä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚åœ¨TOFUå’ŒMUSEåŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¾¾åˆ°æˆ–è¶…è¿‡æœ€æ–°åŸºçº¿æ°´å¹³ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤ç›®æ ‡ä¿¡æ¯çš„åŒæ—¶ä¿ç•™ä¸‹æ¸¸å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­éœ€è¦å»é™¤æ•æ„Ÿã€è¿‡æ—¶æˆ–ä¸“æœ‰ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰å»å­¦ä¹ æ–¹æ³•å­˜åœ¨ä¼˜åŒ–ä¸ç¨³å®šå’Œå¯¹ä¿ç•™æ•°æ®æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºå°†å¤§è¯­è¨€æ¨¡å‹å»å­¦ä¹ è¡¨è¿°ä¸ºçº¦æŸä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡logit-marginå¹³å¦æŸå¤±å®ç°é—å¿˜ã€‚</li>
<li>æŸå¤±å‡½æ•°æ— éœ€softmaxï¼Œæ•°å€¼ç¨³å®šï¼Œä¿æŒéé›¶æ¢¯åº¦ï¼Œä¼˜åŒ–æ›´é«˜æ•ˆã€‚</li>
<li>é‡‡ç”¨å¯æ‰©å±•çš„åŸç”Ÿå¯¹å¶ç®—æ³•è§£å†³çº¦æŸé—®é¢˜ï¼Œå±•ç°é—å¿˜å’Œä¿ç•™ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>æ–¹æ³•åœ¨TOFUå’ŒMUSEåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æœ‰æ•ˆå»é™¤ç›®æ ‡ä¿¡æ¯åŒæ—¶ä¿ç•™ä¸‹æ¸¸å®ç”¨æ€§ã€‚</li>
<li>è¿™ç§æ–¹æ³•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å»å­¦ä¹ æä¾›äº†ä¸€ä¸ªæ–°çš„ã€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ed8ade66aa6778db83427d265502b20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f8840208ef224443c14a4b03f14a66.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sample-Complexity-and-Representation-Ability-of-Test-time-Scaling-Paradigms"><a href="#Sample-Complexity-and-Representation-Ability-of-Test-time-Scaling-Paradigms" class="headerlink" title="Sample Complexity and Representation Ability of Test-time Scaling   Paradigms"></a>Sample Complexity and Representation Ability of Test-time Scaling   Paradigms</h2><p><strong>Authors:Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao</strong></p>
<p>Test-time scaling paradigms have significantly advanced the capabilities of large language models (LLMs) on complex tasks. Despite their empirical success, theoretical understanding of the sample efficiency of various test-time strategies â€“ such as self-consistency, best-of-$n$, and self-correction â€“ remains limited. In this work, we first establish a separation result between two repeated sampling strategies: self-consistency requires $\Theta(1&#x2F;\Delta^2)$ samples to produce the correct answer, while best-of-$n$ only needs $\Theta(1&#x2F;\Delta)$, where $\Delta &lt; 1$ denotes the probability gap between the correct and second most likely answers. Next, we present an expressiveness result for the self-correction approach with verifier feedback: it enables Transformers to simulate online learning over a pool of experts at test time. Therefore, a single Transformer architecture can provably solve multiple tasks without prior knowledge of the specific task associated with a user query, extending the representation theory of Transformers from single-task to multi-task settings. Finally, we empirically validate our theoretical results, demonstrating the practical effectiveness of self-correction methods. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾èŒƒå¼å·²æ˜¾ç€æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬åœ¨ç»éªŒä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å…³äºå„ç§æµ‹è¯•æ—¶é—´ç­–ç•¥ï¼ˆä¾‹å¦‚è‡ªæ´½æ€§ã€best-of-$n$ä»¥åŠè‡ªæˆ‘æ ¡æ­£ï¼‰çš„æ ·æœ¬æ•ˆç‡çš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šäº†ä¸¤ç§é‡å¤é‡‡æ ·ç­–ç•¥ä¹‹é—´çš„åˆ†ç¦»ç»“æœï¼šè‡ªæ´½æ€§éœ€è¦$\Theta(1&#x2F;\Delta^2)$æ ·æœ¬æ‰èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œè€Œbest-of-$n$ä»…éœ€è¦$\Theta(1&#x2F;\Delta)$æ ·æœ¬ï¼Œå…¶ä¸­$\Delta &lt; 1$è¡¨ç¤ºæ­£ç¡®å’Œç¬¬äºŒå¤§å¯èƒ½çš„ç­”æ¡ˆä¹‹é—´çš„æ¦‚ç‡å·®è·ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸ºå¸¦æœ‰éªŒè¯å™¨åé¦ˆçš„è‡ªæˆ‘æ ¡æ­£æ–¹æ³•æä¾›äº†è¡¨ç°åŠ›ç»“æœï¼šå®ƒä½¿Transformerèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶æ¨¡æ‹Ÿä¸“å®¶æ± ä¸­çš„åœ¨çº¿å­¦ä¹ ã€‚å› æ­¤ï¼Œå•ä¸ªTransformeræ¶æ„å¯ä»¥åœ¨ä¸äº†è§£ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„ç‰¹å®šä»»åŠ¡çš„æƒ…å†µä¸‹è§£å†³å¤šä¸ªä»»åŠ¡ï¼Œä»è€Œå°†Transformerçš„è¡¨ç¤ºç†è®ºä»å•ä»»åŠ¡æ‰©å±•åˆ°å¤šä»»åŠ¡è®¾ç½®ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œè¯æ˜äº†è‡ªæˆ‘æ ¡æ­£æ–¹æ³•çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æµ‹è¯•æ—¶é—´ç¼©æ”¾èŒƒå¼åœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚è™½ç„¶å®ƒä»¬åœ¨ç»éªŒä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å¯¹äºå„ç§æµ‹è¯•æ—¶é—´ç­–ç•¥çš„ç†è®ºæ ·æœ¬æ•ˆç‡ï¼ˆå¦‚è‡ªæˆ‘ä¸€è‡´æ€§ã€æœ€ä¼˜é€‰æ‹©æ–¹æ³•å’Œè‡ªæˆ‘ä¿®æ­£ç­‰ï¼‰çš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ ·æœ¬ç­–ç•¥çš„åˆ†ç¦»ç»“æœï¼šè‡ªæˆ‘ä¸€è‡´æ€§éœ€è¦æ›´å¤šçš„æ ·æœ¬æ‰èƒ½äº§ç”Ÿæ­£ç¡®çš„ç­”æ¡ˆï¼Œè€Œæœ€ä¼˜é€‰æ‹©æ–¹æ³•åˆ™éœ€è¦è¾ƒå°‘çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»™å‡ºäº†è‡ªæˆ‘ä¿®æ­£æ–¹æ³•ä¸éªŒè¯å™¨åé¦ˆçš„è¡¨è¾¾æ€§ç»“æœï¼šå®ƒä½¿Transformerèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶æ¨¡æ‹Ÿåœ¨çº¿å­¦ä¹ å¹¶å…±äº«ä¸“å®¶æ„è§æ± çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå•ä¸€Transformeræ¶æ„å¯è¯æ˜èƒ½å¤Ÿåœ¨ä¸äº‹å…ˆäº†è§£ä¸ç”¨æˆ·çš„æŸ¥è¯¢ç›¸å…³è”çš„å…·ä½“ä»»åŠ¡çš„æƒ…å†µä¸‹è§£å†³å¤šä»»åŠ¡é—®é¢˜ï¼Œä»è€Œä»å•ä»»åŠ¡æ‰©å±•äº†Transformerçš„è¡¨ç¤ºç†è®ºè‡³å¤šä»»åŠ¡åœºæ™¯ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®è¯éªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºç»“æœï¼Œè¯æ˜äº†è‡ªæˆ‘ä¿®æ­£æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾èŒƒå¼å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ ·æœ¬æ•ˆç‡å¯¹äºä¸åŒçš„æµ‹è¯•æ—¶é—´ç­–ç•¥å­˜åœ¨ç†è®ºå·®å¼‚ã€‚</li>
<li>è‡ªæˆ‘ä¸€è‡´æ€§ç­–ç•¥å’Œæœ€ä¼˜é€‰æ‹©æ–¹æ³•çš„æ ·æœ¬éœ€æ±‚ä¸åŒã€‚</li>
<li>è‡ªæˆ‘ä¿®æ­£æ–¹æ³•é€šè¿‡éªŒè¯å™¨åé¦ˆå¢å¼ºäº†Transformerçš„å¤šä»»åŠ¡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>Transformerèƒ½å¤Ÿåœ¨ä¸é¢„å…ˆäº†è§£ç‰¹å®šä»»åŠ¡çš„æƒ…å†µä¸‹è§£å†³å¤šä»»åŠ¡é—®é¢˜ã€‚</li>
<li>è‡ªæˆ‘ä¿®æ­£æ–¹æ³•å®ç°äº†Transformerä»å•ä»»åŠ¡è¡¨ç¤ºç†è®ºå‘å¤šä»»åŠ¡è¡¨ç¤ºç†è®ºçš„æ‰©å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8519edea215d36d25ff49a17cac14cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb13e38be03ddaf79b92a7cfafac4715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68e0b14b77f93cadf44ac2c8cf362b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-352ba0fa2e4b7369aa7b520a54ed93f4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning"><a href="#Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning" class="headerlink" title="Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning"></a>Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning</h2><p><strong>Authors:Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿé€šå¸¸é¢ä¸´çŸ¥è¯†å†²çªçš„é—®é¢˜ï¼Œå³æ£€ç´¢åˆ°çš„å¤–éƒ¨çŸ¥è¯†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…åœ¨å‚æ•°çŸ¥è¯†ç›¸çŸ›ç›¾ã€‚è¿™ä¼šå¯¹é—®ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€è¯•å›¾é€šè¿‡å¹¶æ’æ¯”è¾ƒä¸¤ç§çŸ¥è¯†æ¥æºæ¥å‡è½»å†²çªï¼Œä½†è¿™å¯èƒ½ä¼šä½¿è¯­è¨€æ¨¡å‹é¢ä¸´è¿‡å¤šæˆ–å†—é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæœ€ç»ˆé˜»ç¢å…¶è¯†åˆ«å’Œç¼“è§£ä¸ä¸€è‡´çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Micro-Actæ¡†æ¶ï¼Œå®ƒå…·æœ‰åˆ†å±‚åŠ¨ä½œç©ºé—´ï¼Œå¯è‡ªåŠ¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡å¤æ‚æ€§ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†æ¯ä¸ªçŸ¥è¯†æºåˆ†è§£ä¸ºä¸€ç³»åˆ—ç²¾ç»†çš„æ¯”è¾ƒã€‚è¿™äº›æ¯”è¾ƒè¡¨ç°ä¸ºå¯æ“ä½œçš„æ­¥éª¤ï¼Œèƒ½å¤Ÿè¿›è¡Œè¶…è¶Šè¡¨é¢ä¸Šä¸‹æ–‡çš„æ¨ç†ã€‚åœ¨äº”ç»„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMicro-Actåœ¨æ‰€æœ‰äº”ä¸ªæ•°æ®é›†å’Œä¸‰ç§å†²çªç±»å‹ä¸Šï¼Œå‡è¾ƒæœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹åœ¨é—®ç­”å‡†ç¡®æ€§æ–¹é¢æœ‰æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´å’Œè¯­ä¹‰ç±»å‹æ–¹é¢ï¼Œæ‰€æœ‰åŸºçº¿æ¨¡å‹å‡è¡¨ç°ä¸ä½³ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMicro-Actåœ¨éå†²çªé—®é¢˜ä¸Šä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•ŒRAGåº”ç”¨ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05278v1">PDF</a> Accepted by ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Retrieval-Augmented Generationï¼ˆRAGï¼‰ç³»ç»Ÿä¸­å¸¸è§çš„çŸ¥è¯†å†²çªé—®é¢˜ï¼Œå³æ£€ç´¢çš„å¤–éƒ¨çŸ¥è¯†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…åœ¨å‚æ•°çŸ¥è¯†ç›¸çŸ›ç›¾ã€‚è¿™ä¼šå¯¹é—®ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸é€šè¿‡å¹¶æ’æ¯”è¾ƒä¸¤ç§çŸ¥è¯†æºæ¥å‡è½»å†²çªï¼Œä½†è¿™å¯èƒ½ä½¿LLMé¢ä¸´ç¹çæˆ–å†—é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæœ€ç»ˆé˜»ç¢å…¶è¯†åˆ«å’Œç¼“è§£ä¸ä¸€è‡´çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Micro-Actæ¡†æ¶ï¼Œå®ƒå…·æœ‰åˆ†å±‚åŠ¨ä½œç©ºé—´ï¼Œå¯è‡ªåŠ¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡å¤æ‚æ€§ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†æ¯ä¸ªçŸ¥è¯†æºåˆ†è§£æˆä¸€ç³»åˆ—ç²¾ç»†çš„æ¯”è¾ƒã€‚è¿™äº›æ¯”è¾ƒè¡¨ç°ä¸ºå¯æ“ä½œçš„æ­¥éª¤ï¼Œä½¿æ¨ç†è¶…è¶Šäº†è¡¨é¢ä¸Šä¸‹æ–‡ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMicro-Actåœ¨æ‰€æœ‰æ•°æ®é›†å’Œä¸‰ç§å†²çªç±»å‹ä¸Šï¼Œå°¤å…¶æ˜¯åœ¨æ—¶é—´å’Œè¯­ä¹‰ç±»å‹ä¸Šï¼Œç›¸è¾ƒäºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œé—®ç­”å‡†ç¡®æ€§æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMicro-Actåœ¨éå†²çªé—®é¢˜ä¸Šä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå‡¸æ˜¾å…¶åœ¨ç°å®ä¸–ç•Œçš„RAGåº”ç”¨ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGç³»ç»Ÿé¢ä¸´çŸ¥è¯†å†²çªé—®é¢˜ï¼Œå³å¤–éƒ¨æ£€ç´¢çŸ¥è¯†ä¸LLMçš„å†…åœ¨çŸ¥è¯†ç›¸çŸ›ç›¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å¹¶æ’æ¯”è¾ƒçŸ¥è¯†æºæ¥å‡è½»å†²çªï¼Œä½†å¯èƒ½ä½¿LLMé¢ä¸´ç¹ççš„ä¸Šä¸‹æ–‡ã€‚</li>
<li>Micro-Actæ¡†æ¶å…·æœ‰åˆ†å±‚åŠ¨ä½œç©ºé—´ï¼Œå¯è‡ªåŠ¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡å¤æ‚æ€§å¹¶åˆ†è§£çŸ¥è¯†æºã€‚</li>
<li>Micro-Acté€šè¿‡ç²¾ç»†çš„æ¯”è¾ƒå’Œå¯æ“ä½œçš„æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œè¶…è¶Šè¡¨é¢ä¸Šä¸‹æ–‡ã€‚</li>
<li>Micro-Actåœ¨å¤šä¸ªæ•°æ®é›†å’Œä¸‰ç§å†²çªç±»å‹ä¸Šæ˜¾è‘—æé«˜äº†é—®ç­”å‡†ç¡®æ€§ã€‚</li>
<li>Micro-Actåœ¨éå†²çªé—®é¢˜ä¸Šä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-827152fce07163683ee4731b5081d264.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c41fbee446036f82e6d35bef820a5cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb7fa84605926f84e3840f592cd3cd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16b53a70df398ac3e002d9d2ed33389e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d870c33b971ca6264a6abf75e66f1b84.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LeanPO-Lean-Preference-Optimization-for-Likelihood-Alignment-in-Video-LLMs"><a href="#LeanPO-Lean-Preference-Optimization-for-Likelihood-Alignment-in-Video-LLMs" class="headerlink" title="LeanPO: Lean Preference Optimization for Likelihood Alignment in   Video-LLMs"></a>LeanPO: Lean Preference Optimization for Likelihood Alignment in   Video-LLMs</h2><p><strong>Authors:Xiaodong Wang, Jinfa Huang, Li Yuan, Peixi Peng</strong></p>
<p>Most Video Large Language Models (Video-LLMs) adopt preference alignment techniques, e.g., DPO~\citep{rafailov2024dpo}, to optimize the reward margin between a winning response ($y_w$) and a losing response ($y_l$). However, the likelihood displacement observed in DPO indicates that both $\log \pi_\theta (y_w\mid x)$ and $\log \pi_\theta (y_l\mid x) $ often decrease during training, inadvertently boosting the probabilities of non-target responses. In this paper, we systematically revisit this phenomenon from LLMs to Video-LLMs, showing that it intensifies when dealing with the redundant complexity of video content. To alleviate the impact of this phenomenon, we propose \emph{Lean Preference Optimization} (LeanPO), a reference-free approach that reformulates the implicit reward as the average likelihood of the response with respect to the policy model. A key component of LeanPO is the reward-trustworthiness correlated self-generated preference data pipeline, which carefully infuses relevant prior knowledge into the model while continuously refining the preference data via self-reflection. This allows the policy model to obtain high-quality paired data and accurately estimate the newly defined reward, thus mitigating the unintended drop. In addition, we introduce a dynamic label smoothing strategy that mitigates the impact of noise in responses from diverse video content, preventing the model from overfitting to spurious details. Extensive experiments demonstrate that LeanPO significantly enhances the performance of state-of-the-art Video-LLMs, consistently boosting baselines of varying capacities with minimal additional training overhead. Moreover, LeanPO offers a simple yet effective solution for aligning Video-LLM preferences with human trustworthiness, paving the way toward the reliable and efficient Video-LLMs. </p>
<blockquote>
<p>å¤§å¤šæ•°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰é‡‡ç”¨åå¥½å¯¹é½æŠ€æœ¯ï¼Œä¾‹å¦‚DPO~\citep{rafailov2024dpo}ï¼Œä»¥ä¼˜åŒ–è·èƒœå“åº”ï¼ˆywï¼‰å’Œå¤±è´¥å“åº”ï¼ˆylï¼‰ä¹‹é—´çš„å¥–åŠ±å·®è·ã€‚ç„¶è€Œï¼ŒDPOä¸­è§‚å¯Ÿåˆ°çš„å¯èƒ½æ€§ä½ç§»è¡¨æ˜ï¼ŒlogÏ€Î¸(ywâˆ£x)å’ŒlogÏ€Î¸(ylâˆ£x)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€ä¼šå‡å°‘ï¼Œè¿™æ— æ„ä¸­æå‡äº†éç›®æ ‡å“åº”çš„æ¦‚ç‡ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾äº†ä»å¤§è¯­è¨€æ¨¡å‹åˆ°è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¿™ç§ç°è±¡ï¼Œè¡¨æ˜åœ¨å¤„ç†è§†é¢‘å†…å®¹çš„å†—ä½™å¤æ‚æ€§æ—¶ï¼Œè¿™ç§ç°è±¡ä¼šåŠ å‰§ã€‚ä¸ºäº†ç¼“è§£è¿™ç§ç°è±¡çš„å½±å“ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€å‚è€ƒçš„åå¥½ä¼˜åŒ–ï¼ˆLeanPOï¼‰ï¼Œå°†éšæ€§å¥–åŠ±é‡æ–°å®šä¹‰ä¸ºå…³äºç­–ç•¥æ¨¡å‹çš„å“åº”çš„å¹³å‡å¯èƒ½æ€§ã€‚LeanPOçš„å…³é”®ç»„ä»¶æ˜¯å¥–åŠ±å¯ä¿¡åº¦ç›¸å…³çš„è‡ªæˆ‘ç”Ÿæˆçš„åå¥½æ•°æ®ç®¡é“ï¼Œå®ƒè°¨æ…åœ°å°†ç›¸å…³å…ˆéªŒçŸ¥è¯†èå…¥æ¨¡å‹ä¸­ï¼ŒåŒæ—¶é€šè¿‡è‡ªæˆ‘åæ€ä¸æ–­ç²¾ç‚¼åå¥½æ•°æ®ã€‚è¿™ä½¿å¾—ç­–ç•¥æ¨¡å‹èƒ½å¤Ÿè·å¾—é«˜è´¨é‡é…å¯¹æ•°æ®å¹¶å‡†ç¡®ä¼°è®¡æ–°å®šä¹‰çš„å¥–åŠ±ï¼Œä»è€Œç¼“è§£æ— æ„ä¸­çš„ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œå‡è½»äº†æ¥è‡ªä¸åŒè§†é¢‘å†…å®¹ä¸­çš„å“åº”å™ªå£°çš„å½±å“ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆäºè™šå‡ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLeanPOæ˜¾è‘—æé«˜äº†æœ€å…ˆè¿›çš„Video-LLMsçš„æ€§èƒ½ï¼Œå§‹ç»ˆå¦‚ä¸€åœ°æé«˜äº†ä¸åŒå®¹é‡çš„åŸºçº¿æ€§èƒ½ï¼Œå¹¶ä¸”é¢å¤–çš„è®­ç»ƒå¼€é”€æœ€å°ã€‚æ­¤å¤–ï¼ŒLeanPOæä¾›äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½¿Video-LLMçš„åå¥½ä¸äººç±»å¯ä¿¡åº¦ä¿æŒä¸€è‡´ï¼Œä¸ºå¯é é«˜æ•ˆçš„Video-LLMsé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05260v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/Wang-Xiaodong1899/LeanPO">https://github.com/Wang-Xiaodong1899/LeanPO</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨ä¼˜åŒ–å¥–åŠ±è¾¹é™…æ—¶é¢ä¸´çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚DPOåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€å¯¼è‡´èƒœæ–¹å›åº”ï¼ˆywï¼‰å’Œè´¥æ–¹å›åº”ï¼ˆylï¼‰çš„æ¦‚ç‡å¯¹æ•°ä¸‹é™ï¼Œä»è€Œå¯èƒ½æå‡éç›®æ ‡å›åº”çš„æ¦‚ç‡ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åœ°ä»LLMsé‡æ–°å®¡è§†è¿™ä¸€ç°è±¡ï¼Œå¹¶æŒ‡å‡ºåœ¨å¤„ç†è§†é¢‘å†…å®¹çš„å†—ä½™å¤æ‚æ€§æ—¶ï¼Œè¯¥é—®é¢˜æ›´åŠ ä¸¥é‡ã€‚ä¸ºç¼“è§£è¿™ä¸€ç°è±¡ï¼Œæœ¬æ–‡æå‡ºäº†æ— å‚è€ƒçš„LeanPOæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†éšå¼å¥–åŠ±é‡æ–°å®šä¹‰ä¸ºå“åº”ç›¸å¯¹äºç­–ç•¥æ¨¡å‹çš„å¹³å‡æ¦‚ç‡ã€‚LeanPOçš„å…³é”®ç»„ä»¶æ˜¯å¥–åŠ±ä¿¡ä»»åº¦ç›¸å…³çš„è‡ªæˆ‘ç”Ÿæˆåå¥½æ•°æ®ç®¡é“ï¼Œå®ƒé€šè¿‡ä¸æ–­è‡ªæˆ‘åæ€æ¥ç²¾ç»†åœ°æ³¨å…¥ç›¸å…³å…ˆéªŒçŸ¥è¯†å¹¶æŒç»­ç²¾ç‚¼åå¥½æ•°æ®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†åŠ¨æ€æ ‡ç­¾å¹³æ»‘ç­–ç•¥ï¼Œä»¥ç¼“è§£æ¥è‡ªå„ç§è§†é¢‘å†…å®¹ä¸­å“åº”çš„å™ªå£°å½±å“ï¼Œé˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒLeanPOæ˜¾è‘—æé«˜äº†å…ˆè¿›Video-LLMçš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ä¸åŒå®¹é‡çš„åŸºçº¿ç›¸æ¯”å…·æœ‰æœ€å°çš„é¢å¤–è®­ç»ƒå¼€é”€ã€‚æ­¤å¤–ï¼ŒLeanPOä¸ºå¯é é«˜æ•ˆçš„Video-LLMæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åå¥½å¯¹é½æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Video-LLMsåœ¨ä¼˜åŒ–å¥–åŠ±è¾¹é™…æ—¶é‡‡ç”¨åå¥½å¯¹é½æŠ€æœ¯ï¼Œå¦‚DPOï¼Œä½†å­˜åœ¨æ¦‚ç‡ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>åœ¨å¤„ç†è§†é¢‘å†…å®¹çš„å†—ä½™å¤æ‚æ€§æ—¶ï¼Œè¯¥é—®é¢˜æ›´ä¸ºä¸¥é‡ã€‚</li>
<li>æå‡ºLeanPOæ–¹æ³•ï¼Œé€šè¿‡å¹³å‡æ¦‚ç‡é‡æ–°å®šä¹‰äº†éšå¼å¥–åŠ±çš„æ¦‚å¿µã€‚</li>
<li>LeanPOåˆ©ç”¨å¥–åŠ±ä¿¡ä»»åº¦ç›¸å…³çš„è‡ªæˆ‘ç”Ÿæˆåå¥½æ•°æ®ç®¡é“æ¥ç²¾ç»†åœ°æ³¨å…¥ç›¸å…³å…ˆéªŒçŸ¥è¯†å¹¶ç²¾ç‚¼åå¥½æ•°æ®ã€‚</li>
<li>åŠ¨æ€æ ‡ç­¾å¹³æ»‘ç­–ç•¥ç”¨äºç¼“è§£æ¥è‡ªå„ç§è§†é¢‘å†…å®¹å“åº”çš„å™ªå£°å½±å“ã€‚</li>
<li>LeanPOæ˜¾è‘—æé«˜Video-LLMæ€§èƒ½å¹¶æœ‰æ•ˆå¯¹é½äººç±»ä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3594ef21f128de8a5db3d912b7b5040f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f123c11c2212d6ee3668197ee80ae98d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db7161cc2848fa1a1d3a2e70894dca4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-First-Search-Self-Guided-Exploration-of-the-Solution-Space"><a href="#LLM-First-Search-Self-Guided-Exploration-of-the-Solution-Space" class="headerlink" title="LLM-First Search: Self-Guided Exploration of the Solution Space"></a>LLM-First Search: Self-Guided Exploration of the Solution Space</h2><p><strong>Authors:Nathan Herr, Tim RocktÃ¤schel, Roberta Raileanu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable improvements in reasoning and planning through increased test-time compute, often by framing problem-solving as a search process. While methods like Monte Carlo Tree Search (MCTS) have proven effective in some domains, their reliance on fixed exploration hyperparameters limits their adaptability across tasks of varying difficulty, rendering them impractical or expensive in certain settings. In this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM Self-Guided Search} method that removes the need for pre-defined search strategies by empowering the LLM to autonomously control the search process via self-guided exploration. Rather than relying on external heuristics or hardcoded policies, the LLM evaluates whether to pursue the current search path or explore alternative branches based on its internal scoring mechanisms. This enables more flexible and context-sensitive reasoning without requiring manual tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku against three classic widely-used search algorithms, Tree-of-Thoughtsâ€™ Breadth First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which have been used to achieve SotA results on a range of challenging reasoning tasks. We found that LFS (1) performs better on more challenging tasks without additional tuning, (2) is more computationally efficient compared to the other methods, especially when powered by a stronger model, (3) scales better with stronger models, due to its LLM-First design, and (4) scales better with increased compute budget. Our code is publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/NathanHerr/LLM-First-Search%7D%7BLLM-First-Search%7D">https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¢åŠ æµ‹è¯•æ—¶çš„è®¡ç®—é‡ï¼Œåœ¨æ¨ç†å’Œè§„åˆ’æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œé€šå¸¸é€šè¿‡å°†é—®é¢˜è§£å†³æ¡†æ¶è®¾å®šä¸ºæœç´¢è¿‡ç¨‹ã€‚è™½ç„¶è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç­‰æ–¹æ³•åœ¨æŸäº›é¢†åŸŸå·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»¬å¯¹å›ºå®šæ¢ç´¢è¶…å‚æ•°çš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒéš¾åº¦ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ï¼Œå› æ­¤åœ¨æŸäº›ç¯å¢ƒä¸­å®ƒä»¬æ˜¯ä¸åˆ‡å®é™…çš„æˆ–æ˜‚è´µçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{LLM-First Searchï¼ˆLFSï¼‰}ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„\textit{LLMè‡ªæˆ‘å¼•å¯¼æœç´¢}æ–¹æ³•ï¼Œå®ƒé€šè¿‡èµ‹äºˆLLMè‡ªä¸»æ§åˆ¶æœç´¢è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹é¢„å…ˆå®šä¹‰çš„æœç´¢ç­–ç•¥çš„éœ€æ±‚ï¼Œå®ç°è‡ªæˆ‘å¼•å¯¼çš„æ¢ç´¢ã€‚LLMä¸éœ€è¦ä¾èµ–å¤–éƒ¨å¯å‘å¼æˆ–ç¡¬ç¼–ç çš„ç­–ç•¥ï¼Œè€Œæ˜¯æ ¹æ®å…¶å†…éƒ¨è¯„åˆ†æœºåˆ¶æ¥åˆ¤æ–­æ˜¯ç»§ç»­å½“å‰æœç´¢è·¯å¾„è¿˜æ˜¯æ¢ç´¢æ›¿ä»£åˆ†æ”¯ã€‚è¿™å®ç°äº†æ›´çµæ´»ã€æ›´ä¾èµ–äºä¸Šä¸‹æ–‡çš„æ¨ç†ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´æˆ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é€‚åº”ã€‚æˆ‘ä»¬åœ¨å€’è®¡æ—¶å’Œæ•°ç‹¬ä¸Šè¯„ä¼°äº†LFSï¼Œå¹¶ä¸ä¸‰ç§ç»å…¸çš„å¹¿æ³›ä½¿ç”¨çš„æœç´¢ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼šæ€ç»´æ ‘å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆToT-BFSï¼‰ã€æœ€ä½³ä¼˜å…ˆæœç´¢ï¼ˆBestFSï¼‰å’ŒMCTSï¼Œè¿™äº›ç®—æ³•å·²åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚æˆ‘ä»¬å‘ç°LFSï¼ˆ1ï¼‰åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œæ— éœ€é¢å¤–è°ƒæ•´ï¼Œï¼ˆ2ï¼‰ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œå°¤å…¶æ˜¯å½“ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹æ—¶ï¼Œï¼ˆ3ï¼‰ç”±äºå…¶LLM-Firstè®¾è®¡ï¼Œéšç€æ›´å¼ºæ¨¡å‹çš„å¢å¼ºè€Œæ›´å¥½åœ°æ‰©å±•ï¼Œï¼ˆ4ï¼‰éšç€è®¡ç®—é¢„ç®—çš„å¢åŠ è€Œæ›´å¥½åœ°æ‰©å±•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨\href{<a target="_blank" rel="noopener" href="https://github.com/NathanHerr/LLM-First-Search%7D%7BLLM-First-Search%7D%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}ä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05213v1">PDF</a> 9 main pages, 2 figures, 2 tables, 36 appendix pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ‹è¯•æ—¶é—´çš„è®¡ç®—å¢åŠ ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œé€šå¸¸é€šè¿‡å°†é—®é¢˜è§£å†³è§†ä¸ºæœç´¢è¿‡ç¨‹æ¥å®ç°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„LLMè‡ªå¼•å¯¼æœç´¢æ–¹æ³•â€”â€”LLM-First Searchï¼ˆLFSï¼‰ï¼Œå®ƒæ¶ˆé™¤äº†å¯¹é¢„å®šä¹‰æœç´¢ç­–ç•¥çš„éœ€æ±‚ï¼Œé€šè¿‡èµ‹äºˆLLMè‡ªä¸»æ§åˆ¶æœç´¢è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œå®ç°è‡ªæˆ‘å¼•å¯¼çš„æ¢ç´¢ã€‚æˆ‘ä»¬åœ¨å€’è®¡æ—¶å’Œæ•°ç‹¬ä»»åŠ¡ä¸Šè¯„ä¼°äº†LFSï¼Œå¹¶ä¸ä¸‰ç§ç»å…¸çš„æœç´¢ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼šæ ‘çŠ¶æ€ç»´å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆToT-BFSï¼‰ã€æœ€ä½³ä¼˜å…ˆæœç´¢ï¼ˆBestFSï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ã€‚æˆ‘ä»¬å‘ç°LFSåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹æ—¶ï¼Œå¹¶ä¸”ç”±äºå…¶LLMä¼˜å…ˆè®¾è®¡è€Œå…·æœ‰æ›´å¥½çš„æ¨¡å‹æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé€šè¿‡å¢åŠ æµ‹è¯•æ—¶é—´çš„è®¡ç®—ï¼Œæé«˜äº†æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ã€‚</li>
<li>LLM-First Search (LFS)æ˜¯ä¸€ç§æ–°å‹çš„LLMè‡ªå¼•å¯¼æœç´¢æ–¹æ³•ï¼Œæ— éœ€é¢„å®šä¹‰çš„æœç´¢ç­–ç•¥ã€‚</li>
<li>LFSé€šè¿‡LLMçš„è‡ªæˆ‘å¼•å¯¼æ¢ç´¢ï¼Œå¢å¼ºäº†æœç´¢çš„çµæ´»æ€§å’Œä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€‚</li>
<li>LFSåœ¨å€’è®¡æ—¶å’Œæ•°ç‹¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–ä¸‰ç§ç»å…¸çš„æœç´¢ç®—æ³•ã€‚</li>
<li>LFSåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</li>
<li>LFSåœ¨ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹æ—¶å…·æœ‰æ›´å¥½çš„æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b649e9ff8374e127987e754fada7791b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8afbaf563411bca0acae182c903e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a248bf0fc0b8e996e8d3de8b2c659b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e4128e7aaad5c8e3aeabc1a53efeb3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FPTQuant-Function-Preserving-Transforms-for-LLM-Quantization"><a href="#FPTQuant-Function-Preserving-Transforms-for-LLM-Quantization" class="headerlink" title="FPTQuant: Function-Preserving Transforms for LLM Quantization"></a>FPTQuant: Function-Preserving Transforms for LLM Quantization</h2><p><strong>Authors:Boris van Breugel, Yelysei Bondarenko, Paul Whatmough, Markus Nagel</strong></p>
<p>Large language models (LLMs) require substantial compute, and thus energy, at inference time. While quantizing weights and activations is effective at improving efficiency, naive quantization of LLMs can significantly degrade performance due to large magnitude outliers. This paper describes FPTQuant, which introduces four novel, lightweight, and expressive function-preserving transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable pre-RoPE transform for queries and keys, (2) a mergeable transform for values, (3) a mergeable scaling transform within the MLP block, and (4) a cheap, dynamic scaling transform. By leveraging the equivariances and independencies inherent to canonical transformer operation, we designed these FPTs to maintain the modelâ€™s function while shaping the intermediate activation distributions to be more quantization friendly. FPTQuant requires no custom kernels and adds virtually no overhead during inference. The FPTs are trained both locally to reduce outliers, and end-to-end such that the outputs of the quantized and full-precision models match. FPTQuant enables static INT4 quantization with minimal overhead and shows SOTA speed-up of up to 3.9 times over FP. Empirically, FPTQuant has an excellent accuracy-speed trade-off â€“ it is performing on par or exceeding most prior work and only shows slightly lower accuracy compared to a method that is up to 29% slower. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ—¶éœ€è¦å¤§é‡çš„è®¡ç®—å’Œèƒ½æºã€‚è™½ç„¶é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼èƒ½æœ‰æ•ˆæé«˜æ•ˆç‡ï¼Œä½†å¯¹LLMè¿›è¡Œç®€å•çš„é‡åŒ–ä¼šç”±äºè¾ƒå¤§çš„å¹…åº¦å¼‚å¸¸å€¼è€Œå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚æœ¬æ–‡ä»‹ç»äº†FPTQuantï¼Œå®ƒå¼•å…¥äº†å››ç§æ–°é¢–ã€è½»ä¾¿ã€è¡¨è¾¾æ€§å¼ºçš„ä¿åŠŸèƒ½å˜æ¢ï¼ˆFPTsï¼‰ï¼Œä»¥ä¿ƒè¿›å˜å‹å™¨çš„é‡åŒ–ï¼šï¼ˆ1ï¼‰ç”¨äºæŸ¥è¯¢å’Œé”®çš„å¯åˆå¹¶é¢„RoPEå˜æ¢ï¼Œï¼ˆ2ï¼‰ç”¨äºå€¼çš„å¯åˆå¹¶å˜æ¢ï¼Œï¼ˆ3ï¼‰MLPå—å†…çš„å¯åˆå¹¶ç¼©æ”¾å˜æ¢ï¼Œä»¥åŠï¼ˆ4ï¼‰ä¸€ç§å»‰ä»·ã€åŠ¨æ€ç¼©æ”¾å˜æ¢ã€‚æˆ‘ä»¬åˆ©ç”¨è§„èŒƒå˜æ¢å™¨æ“ä½œæ‰€å›ºæœ‰çš„ç­‰å˜æ€§å’Œç‹¬ç«‹æ€§æ¥è®¾è®¡è¿™äº›FPTsï¼Œä»¥ç»´æŒæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ä¸­é—´æ¿€æ´»å€¼åˆ†å¸ƒæ›´åˆ©äºé‡åŒ–ã€‚FPTQuantä¸éœ€è¦è‡ªå®šä¹‰å†…æ ¸ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ ä¹ä¸ä¼šå¢åŠ å¼€é”€ã€‚FPTsæ—¢å¯ä»¥åœ¨æœ¬åœ°è¿›è¡Œè®­ç»ƒä»¥å‡å°‘å¼‚å¸¸å€¼ï¼Œä¹Ÿå¯ä»¥ç«¯åˆ°ç«¯è¿›è¡Œè®­ç»ƒï¼Œä»¥ä½¿é‡åŒ–æ¨¡å‹å’Œå…¨ç²¾åº¦æ¨¡å‹çš„è¾“å‡ºç›¸åŒ¹é…ã€‚FPTQuantå®ç°äº†é™æ€INT4é‡åŒ–ï¼Œå…·æœ‰æœ€å°çš„å¼€é”€ï¼Œå¹¶æ˜¾ç¤ºå‡ºç›¸å¯¹äºæµ®ç‚¹æ•°çš„æœ€é«˜è¾¾3.9å€çš„é€Ÿåº¦æå‡ã€‚ç»éªŒä¸Šï¼ŒFPTQuantå…·æœ‰å‡ºè‰²çš„ç²¾åº¦-é€Ÿåº¦æƒè¡¡â€”â€”å®ƒçš„æ€§èƒ½ä¸å¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œç›¸å½“æˆ–è¶…è¿‡ï¼Œå¹¶ä¸”ä¸ä¸€ç§æ…¢è‡³å¤šè¾¾29%çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…¶ç²¾åº¦ä»…ç•¥æœ‰ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04985v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ—¶éœ€è¦å¤§é‡çš„è®¡ç®—å’Œèƒ½æºã€‚è™½ç„¶é‡åŒ–æƒé‡å’Œæ¿€æ´»å¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†LLMçš„é‡åŒ–æ–¹æ³•å¯èƒ½å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯é¢ä¸´å¤§å‹å¹…åº¦å¼‚å¸¸å€¼é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»çš„FPTQuantæ–¹æ³•å¼•å…¥äº†å››ç§æ–°é¢–ã€è½»ä¾¿ä¸”è¡¨ç°æ€§å¼ºçš„å‡½æ•°ä¿æŒå˜æ¢ï¼ˆFPTï¼‰ï¼Œæ—¨åœ¨è§£å†³transformerçš„é‡åŒ–é—®é¢˜ï¼šï¼ˆ1ï¼‰å¯åˆå¹¶çš„é¢„RoPEå˜æ¢ç”¨äºæŸ¥è¯¢å’Œé”®ï¼›ï¼ˆ2ï¼‰å¯åˆå¹¶çš„å˜æ¢ç”¨äºå€¼ï¼›ï¼ˆ3ï¼‰MLPå—å†…çš„å¯åˆå¹¶ç¼©æ”¾å˜æ¢ï¼›ï¼ˆ4ï¼‰å»‰ä»·ã€åŠ¨æ€ç¼©æ”¾å˜æ¢ã€‚è¿™äº›FPTçš„è®¾è®¡æ—¨åœ¨ä¿æŒæ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ä¸­é—´æ¿€æ´»åˆ†å¸ƒæ›´é€‚åˆé‡åŒ–ã€‚FPTQuantæ— éœ€è‡ªå®šä¹‰å†…æ ¸ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ ä¹ä¸ä¼šå¢åŠ å¼€é”€ã€‚FPTæ—¢å¯ç”¨äºå±€éƒ¨è®­ç»ƒä»¥å‡å°‘å¼‚å¸¸å€¼ï¼Œä¹Ÿå¯ç”¨äºç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œä»¥ä½¿é‡åŒ–æ¨¡å‹å’Œå…¨ç²¾åº¦æ¨¡å‹çš„è¾“å‡ºç›¸åŒ¹é…ã€‚FPTQuantå¯å®ç°é™æ€INT4é‡åŒ–ï¼Œå…·æœ‰æœ€å°çš„é¢å¤–å¼€é”€ï¼Œå¹¶æ˜¾ç¤ºå‡ºæœ€é«˜è¾¾3.9å€çš„é€Ÿåº¦æå‡ã€‚ç»éªŒè¡¨æ˜ï¼ŒFPTQuantåœ¨å‡†ç¡®æ€§-é€Ÿåº¦æ–¹é¢çš„æƒè¡¡è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½ä¸å¤§å¤šæ•°å…ˆå‰å·¥ä½œç›¸å½“æˆ–è¡¨ç°æ›´å¥½ï¼Œä»…åœ¨å°‘æ•°æƒ…å†µä¸‹å‡†ç¡®åº¦ç•¥æœ‰ä¸‹é™ç›¸è¾ƒäºé€Ÿåº¦è¾ƒæ…¢çš„æ–¹æ³•è€Œè¨€ï¼Œå…·æœ‰ä¼˜åŠ¿çš„æ˜¯ä»…éœ€å¤§çº¦æå‡æ¨¡å‹çš„è¿è¡Œé€Ÿåº¦çš„æµ‹è¯•æƒ…å†µä¸‹æ‰æœ‰ç›¸å¯¹è¾ƒä½çš„å‡†ç¡®åº¦æŸå¤±ã€‚æ€»ä½“è€Œè¨€ï¼ŒFPTQuantä¸ºLLMçš„é‡åŒ–æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å‡†ç¡®çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†éœ€è¦å·¨å¤§çš„è®¡ç®—å’Œèƒ½æºèµ„æºã€‚</li>
<li>é‡åŒ–LLMå¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯é¢ä¸´å¤§å‹å¹…åº¦å¼‚å¸¸å€¼é—®é¢˜ã€‚</li>
<li>FPTQuantå¼•å…¥å››ç§å‡½æ•°ä¿æŒå˜æ¢ï¼ˆFPTï¼‰æ¥æ”¹è¿›LLMçš„é‡åŒ–ã€‚è¿™äº›FPTé’ˆå¯¹transformerçš„ç»“æ„è®¾è®¡è€Œæˆä»¥æ›´æœ‰æ•ˆåœ°é‡åŒ–å¤„ç†æŸ¥è¯¢ã€é”®ã€å€¼å’ŒMLPå—çš„ä¸­é—´æ¿€æ´»åˆ†å¸ƒã€‚</li>
<li>FPTQuantæ–¹æ³•æ— éœ€è‡ªå®šä¹‰å†…æ ¸ä¸”å‡ ä¹ä¸å¢åŠ æ¨ç†è¿‡ç¨‹ä¸­çš„å¼€é”€ã€‚å®ƒé€šè¿‡åˆ©ç”¨æ¨¡å‹çš„å†…åœ¨ç­‰ä»·æ€§å’Œç‹¬ç«‹æ€§æ¥å®ç°é«˜æ•ˆé‡åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-317b3229c2bfa6e0b2152064d09adb9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b957eb28816347e4a3fa6ba357af98.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Video-Transformers-for-Word-Level-Bangla-Sign-Language-A-Comparative-Analysis-for-Classification-Tasks"><a href="#Fine-Tuning-Video-Transformers-for-Word-Level-Bangla-Sign-Language-A-Comparative-Analysis-for-Classification-Tasks" class="headerlink" title="Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A   Comparative Analysis for Classification Tasks"></a>Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A   Comparative Analysis for Classification Tasks</h2><p><strong>Authors:Jubayer Ahmed Bhuiyan Shawon, Hasan Mahmud, Kamrul Hasan</strong></p>
<p>Sign Language Recognition (SLR) involves the automatic identification and classification of sign gestures from images or video, converting them into text or speech to improve accessibility for the hearing-impaired community. In Bangladesh, Bangla Sign Language (BdSL) serves as the primary mode of communication for many individuals with hearing impairments. This study fine-tunes state-of-the-art video transformer architectures â€“ VideoMAE, ViViT, and TimeSformer â€“ on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset with 60 frequent signs. We standardized the videos to 30 FPS, resulting in 9,307 user trial clips. To evaluate scalability and robustness, the models were also fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401 sign classes. Additionally, we benchmark performance against public datasets, including LSA64 and WLASL. Data augmentation techniques such as random cropping, horizontal flipping, and short-side scaling were applied to improve model robustness. To ensure balanced evaluation across folds during model selection, we employed 10-fold stratified cross-validation on the training set, while signer-independent evaluation was carried out using held-out test data from unseen users U4 and U8. Results show that video transformer models significantly outperform traditional machine learning and deep learning approaches. Performance is influenced by factors such as dataset size, video quality, frame distribution, frame rate, and model architecture. Among the models, the VideoMAE variant (MCG-NJU&#x2F;videomae-base-finetuned-kinetics) achieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60 dataset and 81.04% on the front-facing signs of BdSLW401 â€“ demonstrating strong potential for scalable and accurate BdSL recognition. </p>
<blockquote>
<p>æ‰‹åŠ¿è¯­è¨€è¯†åˆ«ï¼ˆSLRï¼‰æ¶‰åŠä»å›¾åƒæˆ–è§†é¢‘ä¸­è‡ªåŠ¨è¯†åˆ«å’Œåˆ†ç±»æ‰‹åŠ¿ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºæ–‡æœ¬æˆ–è¯­éŸ³ï¼Œä»¥æé«˜å¬åŠ›éšœç¢è€…çš„å¯è®¿é—®æ€§ã€‚åœ¨å­ŸåŠ æ‹‰å›½ï¼Œå­ŸåŠ æ‹‰æ‰‹è¯­ï¼ˆBdSLï¼‰æ˜¯è®¸å¤šå¬åŠ›éšœç¢è€…ä¸»è¦çš„äº¤æµæ–¹å¼ã€‚æœ¬ç ”ç©¶å¯¹æœ€å‰æ²¿çš„è§†é¢‘è½¬æ¢å™¨æ¶æ„â€”â€”VideoMAEã€ViViTå’ŒTimeSformerè¿›è¡Œäº†å¾®è°ƒï¼Œåº”ç”¨äºBdSLW60ï¼ˆarXiv:2402.08635ï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«60ç§å¸¸è§æ‰‹åŠ¿çš„å°è§„æ¨¡BdSLæ•°æ®é›†ã€‚æˆ‘ä»¬å°†è§†é¢‘æ ‡å‡†åŒ–è‡³30FPSï¼Œç”Ÿæˆäº†9307ä¸ªç”¨æˆ·è¯•éªŒç‰‡æ®µã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ‰©å±•æ€§å’Œç¨³å¥æ€§ï¼Œæˆ‘ä»¬è¿˜åœ¨å¤§è§„æ¨¡çš„BdSLW401æ•°æ®é›†ï¼ˆarXiv:2503.02360ï¼‰ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«401ä¸ªæ‰‹åŠ¿ç±»åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸LSA64å’ŒWLASLç­‰å…¬å…±æ•°æ®é›†è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬åº”ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¦‚éšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬å’ŒçŸ­è¾¹ç¼©æ”¾ã€‚åœ¨æ¨¡å‹é€‰æ‹©è¿‡ç¨‹ä¸­ï¼Œä¸ºäº†ç¡®ä¿è·¨æŠ˜å çš„è¯„ä¼°å¹³è¡¡ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒé›†é‡‡ç”¨äº†10æŠ˜åˆ†å±‚äº¤å‰éªŒè¯ï¼Œè€Œå¯¹æ¥è‡ªæœªè§ç”¨æˆ·çš„U4å’ŒU8çš„ä¿ç•™æµ‹è¯•æ•°æ®è¿›è¡Œäº†ç­¾åäººç‹¬ç«‹è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè§†é¢‘è½¬æ¢å™¨æ¨¡å‹æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚æ€§èƒ½å—åˆ°æ•°æ®é›†å¤§å°ã€è§†é¢‘è´¨é‡ã€å¸§åˆ†å¸ƒã€å¸§ç‡å’Œæ¨¡å‹æ¶æ„ç­‰å› ç´ çš„å½±å“ã€‚åœ¨å¸§é€Ÿç‡æ ¡æ­£çš„BdSLW60æ•°æ®é›†ä¸Šï¼ŒVideoMAEå˜ä½“ï¼ˆMCG-NJU&#x2F;videomae-base-finetuned-kineticsï¼‰å–å¾—äº†æœ€é«˜å‡†ç¡®ç‡ï¼Œè¾¾åˆ°95.5%ï¼Œåœ¨é¢å‘æ­£é¢çš„BdSLW401æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º81.04%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¯æ‰©å±•å’Œå‡†ç¡®è¯†åˆ«BdSLçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04367v1">PDF</a> 16 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè§†é¢‘å›¾åƒçš„æ‰‹è¯­è¯†åˆ«æŠ€æœ¯ï¼ˆSLRï¼‰åœ¨å­ŸåŠ æ‹‰å›½çš„åº”ç”¨ã€‚ç ”ç©¶è€…é’ˆå¯¹å­ŸåŠ æ‹‰æ‰‹è¯­ï¼ˆBdSLï¼‰çš„æ•°æ®é›†è¿›è¡Œäº†å…ˆè¿›çš„è§†é¢‘æ¨¡å‹æ¶æ„çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚æ¨¡å‹æ€§èƒ½é€šè¿‡åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ¥ä½“ç°ï¼Œç»“æœè¡¨æ˜è§†é¢‘æ¨¡å‹æ˜æ˜¾ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å…¶ä¸­ï¼ŒVideoMAEæ¨¡å‹åœ¨ä¿®æ­£å¸§ç‡çš„BdSLW60æ•°æ®é›†ä¸Šå‡†ç¡®ç‡é«˜è¾¾95.5%ï¼Œåœ¨é¢å‘å‰æ–¹çš„BdSLW401æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º81.04%ï¼Œå±•ç°å‡ºåœ¨å¯ä¼¸ç¼©æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹è¯­è¯†åˆ«ï¼ˆSLRï¼‰æ˜¯å°†å›¾åƒæˆ–è§†é¢‘ä¸­çš„æ‰‹åŠ¿è‡ªåŠ¨è¯†åˆ«å’Œåˆ†ç±»ï¼Œå¹¶è½¬æ¢ä¸ºæ–‡å­—æˆ–è¯­éŸ³ï¼Œä»¥æé«˜å¬éšœäººå£«çš„è®¿é—®æ€§ã€‚</li>
<li>åœ¨å­ŸåŠ æ‹‰å›½ï¼Œå­ŸåŠ æ‹‰æ‰‹è¯­ï¼ˆBdSLï¼‰æ˜¯å¬éšœäººå£«çš„ä¸»è¦æ²Ÿé€šæ–¹å¼ã€‚</li>
<li>ç ”ç©¶è€…ä½¿ç”¨å…ˆè¿›çš„è§†é¢‘æ¨¡å‹æ¶æ„ï¼ˆå¦‚VideoMAE, ViViTå’ŒTimeSformerï¼‰å¯¹BdSLæ•°æ®é›†è¿›è¡Œäº†å¾®è°ƒã€‚</li>
<li>æ•°æ®å¢å¼ºæŠ€æœ¯å¦‚éšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬å’ŒçŸ­è¾¹ç¼©æ”¾è¢«ç”¨æ¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>è§†é¢‘æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>VideoMAEæ¨¡å‹åœ¨ä¿®æ­£å¸§ç‡çš„BdSLW60æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°95.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13a733a02451db909724c8ec8e37c34b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73c84ce91fc91a9da14e45187600ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99b8af36d6119db8f7bc4b4c62350e5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7817529756a96bec3b9727b7e617b48c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48d6948d1593df7ca2a33ebea527d0ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7911a06acf614aef6cd7c1a9137b682.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models"><a href="#Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models" class="headerlink" title="Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models"></a>Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models</h2><p><strong>Authors:Michael Li, Nishant Subramani</strong></p>
<p>Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand todayâ€™s language models, we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) represent lexical identity and inflectional morphology. We train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features. We discover that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout the layers. Further analysis reveals that these models encode inflectional morphology through generalizable abstractions, but rely predominantly on memorization to encode lexical identity. Remarkably, these patterns emerge across all 16 models we test, despite differences in architecture, size, and training regime (including pretrained and instruction-tuned variants). This consistency suggests that, despite substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties could be fundamental for next token prediction and are learned early during pretraining. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing">https://github.com/ml5885/model_internal_sleuthing</a> </p>
<blockquote>
<p>ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸»è¦ç”±å¤§å‹åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ä¸»å¯¼ï¼Œç„¶è€Œæˆ‘ä»¬å¯¹å®ƒä»¬å¦‚ä½•ç¼–ç è¯­è¨€ä¿¡æ¯çš„ç†è§£ä»åŸºäºæ—©æœŸæ¨¡å‹ï¼Œå¦‚BERTå’ŒGPT-2ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£å½“ä»Šçš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç»å…¸æ¶æ„ï¼ˆBERTã€DeBERTaã€GPT-2ï¼‰å’Œå½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPythiaã€OLMo-2ã€Gemma-2ã€Qwen2.5ã€Llama-3.1ï¼‰å¦‚ä½•è¡¨ç¤ºè¯æ±‡èº«ä»½å’Œå±ˆæŠ˜å½¢æ€ã€‚æˆ‘ä»¬å¯¹åˆ†å±‚æ¿€æ´»è®­ç»ƒäº†çº¿æ€§å’Œéçº¿æ€§åˆ†ç±»å™¨ï¼Œä»¥é¢„æµ‹è¯ç´ å’Œå±ˆæŠ˜ç‰¹å¾ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¨¡å‹åœ¨æ—©æœŸå±‚æ¬¡ä¸­ä»¥çº¿æ€§æ–¹å¼é›†ä¸­è¯æ±‡ä¿¡æ¯ï¼Œåœ¨åæœŸå±‚æ¬¡ä¸­åˆ™è¶Šæ¥è¶Šä»¥éçº¿æ€§æ–¹å¼é›†ä¸­ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒå±ˆæŠ˜ä¿¡æ¯åœ¨æ•´ä¸ªå±‚æ¬¡ä¸­å‡åŒ€å¯è®¿é—®ä¸”å¯çº¿æ€§åˆ†ç¦»ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡å¯æ¨å¹¿çš„æŠ½è±¡æ¥ç¼–ç å±ˆæŠ˜å½¢æ€ï¼Œä½†ä¸»è¦ä¾èµ–äºè®°å¿†æ¥ç¼–ç è¯æ±‡èº«ä»½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æˆ‘ä»¬æµ‹è¯•çš„16ä¸ªæ¨¡å‹ä¸­ï¼Œå°½ç®¡å®ƒä»¬åœ¨æ¶æ„ã€è§„æ¨¡å’Œè®­ç»ƒæ–¹æ¡ˆï¼ˆåŒ…æ‹¬é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´å˜ä½“ï¼‰ä¸Šå­˜åœ¨å·®å¼‚ï¼Œä½†è¿™äº›æ¨¡å¼ä»ç„¶å‡ºç°ã€‚è¿™ç§ä¸€è‡´æ€§è¡¨æ˜ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†Transformeræ¨¡å‹ä»¥ç›¸ä¼¼çš„æ–¹å¼ç»„ç»‡è¯­è¨€ä¿¡æ¯ï¼Œè¿™è¡¨æ˜è¿™äº›å±æ€§å¯¹äºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è‡³å…³é‡è¦ï¼Œå¹¶ä¸”åœ¨é¢„è®­ç»ƒæ—©æœŸå°±å·²ç»å­¦ä¹ ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ml5885/model_internal_sleuthingä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02132v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œæ— è®ºæ¨¡å‹æ¶æ„ã€å¤§å°æˆ–è®­ç»ƒæœºåˆ¶å¦‚ä½•ï¼Œå¤§å‹åŸºäºtransformerçš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åœ¨æ—©æœŸå±‚çº§ä¸­ä¸»è¦é›†ä¸­å¤„ç†è¯æ±‡ä¿¡æ¯ï¼Œè€Œåå±‚æ›´å¤šåœ°æ¶‰åŠéçº¿æ€§çš„ä¿¡æ¯å¤„ç†ï¼Œå¹¶ä¿æŒäº†è¯æ±‡ä¿¡æ¯çš„ä¸€è‡´æ€§ã€‚è™½ç„¶å®ƒä»¬åœ¨ç»“æ„å’ŒåŠŸèƒ½ä¸Šæœ‰æ‰€å‘å±•ï¼Œä½†å®ƒä»¬éƒ½å€¾å‘äºé€šè¿‡å¯æ¦‚æ‹¬çš„æŠ½è±¡æ¥ç¼–ç å½¢æ€å˜åŒ–ä¿¡æ¯ï¼Œä½†ä¸»è¦ä¾èµ–è®°å¿†æ¥ç¼–ç è¯æ±‡èº«ä»½ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ç°ä»£NLPæ¨¡å‹åœ¨å¤„ç†è¯­è¨€ä¿¡æ¯æ—¶çš„æ ¸å¿ƒæœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹åŸºäºtransformerçš„è¯­è¨€æ¨¡å‹åœ¨ç¼–ç è¯­è¨€ä¿¡æ¯æ—¶è¡¨ç°å‡ºä¸€è‡´æ€§ï¼Œæ— è®ºæ¨¡å‹æ¶æ„ã€å¤§å°æˆ–è®­ç»ƒæœºåˆ¶å¦‚ä½•ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨æ—©æœŸå±‚çº§é›†ä¸­å¤„ç†è¯æ±‡ä¿¡æ¯ï¼Œå¹¶åœ¨åç»­å±‚çº§ä¸­æ¶‰åŠæ›´å¤æ‚çš„éçº¿æ€§ä¿¡æ¯å¤„ç†ã€‚</li>
<li>è¯æ±‡ä¿¡æ¯åœ¨æ¨¡å‹ä¸­ä¿æŒä¸€è‡´æ€§ï¼Œè€Œå½¢æ€å˜åŒ–ä¿¡æ¯åˆ™é€šè¿‡å¯æ¦‚æ‹¬çš„æŠ½è±¡è¿›è¡Œç¼–ç ã€‚</li>
<li>æ¨¡å‹ä¸»è¦ä¾èµ–è®°å¿†æ¥ç¼–ç è¯æ±‡èº«ä»½ã€‚</li>
<li>è®­ç»ƒåˆ†ç±»å™¨å¯ä»¥åœ¨ä¸åŒå±‚çº§é¢„æµ‹å•è¯è¯å…ƒå’Œå½¢æ€å˜åŒ–ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹ç»„ç»‡å’Œå¤„ç†è¯­è¨€ä¿¡æ¯çš„æ–¹å¼å¯èƒ½ä¸ä¸‹ä¸€ä¸ªå•è¯çš„é¢„æµ‹æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc386a4af271b55dee38fd1cbd148a23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49d85bac18845d71791b208ec53ef6d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf376edceef05628fb07c4a9a080815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-743165dd7584cd28f3d8e3bd51542824.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af21719a18d431a3191a862f1cdf446b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c149157610ffd0baddfdafd79edbf674.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Protocol-Unveiling-Attack-Vectors-in-the-Model-Context-Protocol-Ecosystem"><a href="#Beyond-the-Protocol-Unveiling-Attack-Vectors-in-the-Model-Context-Protocol-Ecosystem" class="headerlink" title="Beyond the Protocol: Unveiling Attack Vectors in the Model Context   Protocol Ecosystem"></a>Beyond the Protocol: Unveiling Attack Vectors in the Model Context   Protocol Ecosystem</h2><p><strong>Authors:Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, Jiachi Chen</strong></p>
<p>The Model Context Protocol (MCP) is an emerging standard designed to enable seamless interaction between Large Language Model (LLM) applications and external tools or resources. Within a short period, thousands of MCP services have already been developed and deployed. However, the client-server integration architecture inherent in MCP may expand the attack surface against LLM Agent systems, introducing new vulnerabilities that allow attackers to exploit by designing malicious MCP servers. In this paper, we present the first systematic study of attack vectors targeting the MCP ecosystem. Our analysis identifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources. To evaluate the feasibility of these attacks, we conduct experiments following the typical steps of launching an attack through malicious MCP servers: upload-download-attack. Specifically, we first construct malicious MCP servers and successfully upload them to three widely used MCP aggregation platforms. The results indicate that current audit mechanisms are insufficient to identify and prevent the proposed attack methods. Next, through a user study and interview with 20 participants, we demonstrate that users struggle to identify malicious MCP servers and often unknowingly install them from aggregator platforms. Finally, we demonstrate that these attacks can trigger harmful behaviors within the userâ€™s local environment-such as accessing private files or controlling devices to transfer digital assets-by deploying a proof-of-concept (PoC) framework against five leading LLMs. Additionally, based on interview results, we discuss four key challenges faced by the current security ecosystem surrounding MCP servers. These findings underscore the urgent need for robust security mechanisms to defend against malicious MCP servers. </p>
<blockquote>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ˜¯ä¸€ç§æ–°å…´æ ‡å‡†ï¼Œæ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ç¨‹åºä¸å¤–éƒ¨å·¥å…·æˆ–èµ„æºä¹‹é—´çš„æ— ç¼äº¤äº’ã€‚åœ¨å¾ˆçŸ­çš„æ—¶é—´å†…ï¼Œå·²ç»å¼€å‘å¹¶éƒ¨ç½²äº†æˆåƒä¸Šä¸‡ä¸ªMCPæœåŠ¡ã€‚ç„¶è€Œï¼ŒMCPæ‰€å›ºæœ‰çš„å®¢æˆ·ç«¯-æœåŠ¡å™¨é›†æˆæ¶æ„å¯èƒ½ä¼šæ‰©å¤§é’ˆå¯¹LLMä»£ç†ç³»ç»Ÿçš„æ”»å‡»é¢ï¼Œå¼•å…¥æ–°çš„æ¼æ´ï¼Œæ”»å‡»è€…å¯åˆ©ç”¨è¿™äº›æ¼æ´é€šè¿‡è®¾è®¡æ¶æ„çš„MCPæœåŠ¡å™¨è¿›è¡Œæ”»å‡»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹é’ˆå¯¹MCPç”Ÿæ€ç³»ç»Ÿçš„æ”»å‡»å‘é‡è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†å››ç±»æ”»å‡»ï¼Œå³å·¥å…·ä¸­æ¯’æ”»å‡»ã€å‚€å„¡æ”»å‡»ã€åœ°æ¯¯å¼æ”»å‡»å’Œé€šè¿‡æ¶æ„å¤–éƒ¨èµ„æºçš„åˆ©ç”¨æ”»å‡»ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›æ”»å‡»çš„å¯è¡Œæ€§ï¼Œæˆ‘ä»¬é€šè¿‡æ¶æ„MCPæœåŠ¡å™¨å‘åŠ¨æ”»å‡»çš„å…¸å‹æ­¥éª¤è¿›è¡Œäº†å®éªŒï¼šä¸Šä¼ -ä¸‹è½½-æ”»å‡»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†æ¶æ„MCPæœåŠ¡å™¨ï¼Œå¹¶æˆåŠŸå°†å…¶ä¸Šä¼ åˆ°ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„MCPèšåˆå¹³å°ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„å®¡è®¡æœºåˆ¶ä¸è¶³ä»¥è¯†åˆ«å’Œé¢„é˜²æ‰€æå‡ºçš„æ”»å‡»æ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡å¯¹20åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶å’Œé‡‡è®¿ï¼Œæˆ‘ä»¬è¯æ˜äº†ç”¨æˆ·å¾ˆéš¾è¯†åˆ«æ¶æ„MCPæœåŠ¡å™¨ï¼Œå¹¶ä¸”ç»å¸¸æ— æ„ä¸­ä»èšåˆå¹³å°å®‰è£…å®ƒä»¬ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡é’ˆå¯¹äº”æ¬¾é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²æ¦‚å¿µéªŒè¯ï¼ˆPoCï¼‰æ¡†æ¶ï¼Œè¯æ˜è¿™äº›æ”»å‡»å¯ä»¥è§¦å‘ç”¨æˆ·æœ¬åœ°ç¯å¢ƒä¸­çš„æœ‰å®³è¡Œä¸ºï¼Œä¾‹å¦‚è®¿é—®ç§äººæ–‡ä»¶æˆ–æ§åˆ¶è®¾å¤‡è½¬ç§»æ•°å­—èµ„äº§ã€‚æ­¤å¤–ï¼ŒåŸºäºè®¿è°ˆç»“æœï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰MCPæœåŠ¡å™¨å®‰å…¨ç”Ÿæ€ç³»ç»Ÿæ‰€é¢ä¸´çš„å››å¤§æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†äºŸéœ€æ„å»ºç¨³å¥çš„å®‰å…¨æœºåˆ¶æ¥é˜²èŒƒæ¶æ„MCPæœåŠ¡å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02040v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ˜¯ä¸€ç§æ–°å…´æ ‡å‡†ï¼Œæ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ç¨‹åºä¸å¤–éƒ¨å·¥å…·æˆ–èµ„æºä¹‹é—´çš„æ— ç¼äº¤äº’ã€‚ç„¶è€Œï¼ŒMCPæ‰€å›ºæœ‰çš„å®¢æˆ·ç«¯-æœåŠ¡å™¨é›†æˆæ¶æ„å¯èƒ½ä¼šæ‰©å¤§é’ˆå¯¹LLMä»£ç†ç³»ç»Ÿçš„æ”»å‡»é¢ï¼Œå¼•å…¥å…è®¸æ”»å‡»è€…é€šè¿‡è®¾è®¡æ¶æ„MCPæœåŠ¡å™¨è¿›è¡Œåˆ©ç”¨çš„æ–°æ¼æ´ã€‚æœ¬æ–‡å¯¹é’ˆå¯¹MCPç”Ÿæ€ç³»ç»Ÿçš„æ”»å‡»å‘é‡è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶åˆ†æï¼Œè¯†åˆ«å‡ºå››ç§æ”»å‡»ç±»åˆ«ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›æ”»å‡»çš„å¯è¡Œæ€§ï¼Œæˆ‘ä»¬é€šè¿‡æ¶æ„MCPæœåŠ¡å™¨å‘åŠ¨æ”»å‡»çš„å…¸å‹æ­¥éª¤è¿›è¡Œå®éªŒï¼šä¸Šä¼ -ä¸‹è½½-æ”»å‡»ã€‚æˆ‘ä»¬å‘ç°å½“å‰å®¡è®¡æœºåˆ¶ä¸è¶³ä»¥è¯†åˆ«å¹¶é¢„é˜²æ‰€æå‡ºçš„æ”»å‡»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹20åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶å’Œè®¿è°ˆï¼Œæˆ‘ä»¬è¯æ˜äº†ç”¨æˆ·éš¾ä»¥è¯†åˆ«æ¶æ„MCPæœåŠ¡å™¨ï¼Œå¹¶ç»å¸¸æ— æ„ä¸­ä»èšåˆå¹³å°å®‰è£…å®ƒä»¬ã€‚æœ€åï¼Œé€šè¿‡ä¸€ä¸ªé’ˆå¯¹äº”å¤§é¢†å…ˆLLMçš„æ¦‚å¿µéªŒè¯æ¡†æ¶ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™äº›æ”»å‡»å¯ä»¥è§¦å‘ç”¨æˆ·æœ¬åœ°ç¯å¢ƒä¸­çš„æœ‰å®³è¡Œä¸ºï¼Œå¦‚è®¿é—®ç§äººæ–‡ä»¶æˆ–æ§åˆ¶è®¾å¤‡è½¬ç§»æ•°å­—èµ„äº§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ˜¯ä¸€ç§æ—¨åœ¨ä¿ƒè¿›LLMä¸å¤–éƒ¨å·¥å…·æ— ç¼äº¤äº’çš„æ–°å…´æ ‡å‡†ï¼Œä½†å­˜åœ¨å®‰å…¨æ¼æ´ã€‚</li>
<li>è¯†åˆ«äº†å››ç§é’ˆå¯¹MCPçš„æ”»å‡»ç±»åˆ«ï¼šå·¥å…·ä¸­æ¯’æ”»å‡»ã€æœ¨å¶æ”»å‡»ã€æ‹‰ç»³æ”»å‡»å’Œé€šè¿‡æ¶æ„å¤–éƒ¨èµ„æºçš„åˆ©ç”¨ã€‚</li>
<li>é€šè¿‡å®éªŒè¯æ˜ï¼Œå½“å‰å®¡è®¡æœºåˆ¶ä¸è¶³ä»¥è¯†åˆ«å’Œé¢„é˜²æå‡ºçš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>ç”¨æˆ·éš¾ä»¥è¯†åˆ«æ¶æ„MCPæœåŠ¡å™¨ï¼Œç»å¸¸ä»èšåˆå¹³å°æ— æ„ä¸­å®‰è£…å®ƒä»¬ã€‚</li>
<li>æ”»å‡»å¯ä»¥è§¦å‘ç”¨æˆ·æœ¬åœ°ç¯å¢ƒä¸­çš„æœ‰å®³è¡Œä¸ºï¼Œå¦‚è®¿é—®ç§äººæ–‡ä»¶æˆ–æ§åˆ¶è®¾å¤‡è½¬ç§»æ•°å­—èµ„äº§ã€‚</li>
<li>ç›®å‰MCpæœåŠ¡å™¨çš„å®‰å…¨ç”Ÿæ€ç³»ç»Ÿé¢ä¸´å››å¤§æŒ‘æˆ˜ã€‚</li>
<li>æ€¥éœ€å»ºç«‹ç¨³å¥çš„å®‰å…¨æœºåˆ¶æ¥é˜²èŒƒæ¶æ„MCPæœåŠ¡å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-762a0d1ed36d64ac735ce45311f56e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94bdb429b9683666a2370fbb64fe81d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09880901e0b719188ce71e9e82ec1c48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2b80be259e1e76fc4ed6fdcc9a0610c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e38d289fc371205e54e9a9c5a0a6b719.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Analysis-of-LLM-Bias-Chinese-Propaganda-Anti-US-Sentiment-in-DeepSeek-R1-vs-ChatGPT-o3-mini-high"><a href="#Analysis-of-LLM-Bias-Chinese-Propaganda-Anti-US-Sentiment-in-DeepSeek-R1-vs-ChatGPT-o3-mini-high" class="headerlink" title="Analysis of LLM Bias (Chinese Propaganda &amp; Anti-US Sentiment) in   DeepSeek-R1 vs. ChatGPT o3-mini-high"></a>Analysis of LLM Bias (Chinese Propaganda &amp; Anti-US Sentiment) in   DeepSeek-R1 vs. ChatGPT o3-mini-high</h2><p><strong>Authors:PeiHsuan Huang, ZihWei Lin, Simon Imbot, WenCheng Fu, Ethan Tu</strong></p>
<p>Large language models (LLMs) increasingly shape public understanding and civic decisions, yet their ideological neutrality is a growing concern. While existing research has explored various forms of LLM bias, a direct, cross-lingual comparison of models with differing geopolitical alignments-specifically a PRC-system model versus a non-PRC counterpart-has been lacking. This study addresses this gap by systematically evaluating DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus of 1,200 de-contextualized, reasoning-oriented questions derived from Chinese-language news, presented in Simplified Chinese, Traditional Chinese, and English. Answers from both models (7,200 total) were assessed using a hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human annotation. Our findings reveal significant model-level and language-dependent biases. DeepSeek-R1 consistently exhibited substantially higher proportions of both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which remained largely free of anti-U.S. sentiment and showed lower propaganda levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias rates; these diminished in Traditional Chinese and were nearly absent in English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to Traditional Chinese queries and amplified existing PRC-aligned terms in its Chinese answers, demonstrating an â€œinvisible loudspeakerâ€ effect. Furthermore, such biases were not confined to overtly political topics but also permeated cultural and lifestyle content, particularly in DeepSeek-R1. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå½±å“å…¬ä¼—ç†è§£å’Œå…¬æ°‘å†³ç­–ï¼Œä½†å®ƒä»¬çš„æ„è¯†å½¢æ€ä¸­ç«‹æ€§å´ä»¤äººæ—¥ç›Šæ‹…å¿§ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†LLMåè§çš„å„ç§å½¢å¼ï¼Œä½†é’ˆå¯¹å…·æœ‰ä¸åŒåœ°ç¼˜æ”¿æ²»å¯¹é½æ–¹å¼çš„æ¨¡å‹è¿›è¡Œç›´æ¥ã€è·¨è¯­è¨€çš„æ¯”è¾ƒâ€”â€”ç‰¹åˆ«æ˜¯PRCç³»ç»Ÿæ¨¡å‹ä¸éPRCæ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒâ€”â€”ä»ç„¶ç¼ºä¹ã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°DeepSeek-R1ï¼ˆPRCå¯¹é½ï¼‰ä¸ChatGPT o3-mini-highï¼ˆéPRCï¼‰çš„ä¸­æ–‡å›½å®¶å®£ä¼ å’Œåç¾æƒ…ç»ªï¼Œæ¥å¼¥è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç”±1200ä¸ªå»è¯­å¢ƒåŒ–ã€ä»¥æ¨ç†ä¸ºå¯¼å‘çš„é—®é¢˜ç»„æˆçš„æ–°å‹è¯­æ–™åº“ï¼Œè¿™äº›é—®é¢˜æ¥è‡ªç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡å’Œè‹±æ–‡çš„ä¸­æ–‡è¯­è¨€æ–°é—»ã€‚å¯¹ä¸¤ä¸ªæ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆï¼ˆå…±7200ä¸ªï¼‰è¿›è¡Œäº†è¯„ä¼°ï¼Œé‡‡ç”¨äº†ä¸€ç§ç»“åˆåŸºäºè§„åˆ™çš„GPT-4oè¯„åˆ†ä¸äººç±»æ³¨é‡Šçš„æ··åˆè¯„ä¼°ç®¡é“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†æ¨¡å‹çº§åˆ«å’Œä¾èµ–äºè¯­è¨€çš„åè§ã€‚ä¸ChatGPT o3-mini-highç›¸æ¯”ï¼ŒDeepSeek-R1å§‹ç»ˆè¡¨ç°å‡ºæ›´é«˜æ¯”ä¾‹çš„å®£ä¼ å’Œåç¾åè§ï¼Œè€ŒChatGPT o3-mini-highåˆ™åŸºæœ¬ä¸å­˜åœ¨åç¾æƒ…ç»ªï¼Œå®£ä¼ æ°´å¹³ä¹Ÿè¾ƒä½ã€‚å¯¹äºDeepSeek-R1æ¥è¯´ï¼Œç®€ä½“ä¸­æ–‡æŸ¥è¯¢å¼•å‘çš„åè§ç‡æœ€é«˜ï¼›åœ¨ç¹ä½“ä¸­æ–‡ä¸­è¿™äº›åè§æœ‰æ‰€å‡å°‘ï¼Œè‹±æ–‡ä¸­åˆ™å‡ ä¹ä¸å­˜åœ¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDeepSeek-R1æœ‰æ—¶ä¼šç”¨ç®€ä½“ä¸­æ–‡å›ç­”ç¹ä½“ä¸­æ–‡æŸ¥è¯¢ï¼Œå¹¶åœ¨å…¶ä¸­æ–‡ç­”æ¡ˆä¸­æ”¾å¤§å·²æœ‰çš„PRCå¯¹é½æœ¯è¯­ï¼Œè¡¨ç°å‡ºä¸€ç§â€œéšå½¢æ‰¬å£°å™¨â€æ•ˆåº”ã€‚æ­¤å¤–ï¼Œè¿™ç§åè§ä¸ä»…é™äºæ”¿æ²»è¯é¢˜ï¼Œä¹Ÿæ¸—é€åˆ°æ–‡åŒ–å’Œç”Ÿæ´»æ–¹å¼çš„å†…å®¹ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨DeepSeek-R1ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¡‘é€ å…¬ä¼—ç†è§£å’Œå…¬æ°‘å†³ç­–æ–¹é¢æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œä½†å…¶æ„è¯†å½¢æ€ä¸­ç«‹æ€§å¼•å‘å…³æ³¨ã€‚ç°æœ‰ç ”ç©¶å·²æ¢ç´¢äº†LLMçš„åè§å½¢å¼ï¼Œä½†ç¼ºä¹å¯¹ä¸åŒåœ°ç¼˜æ”¿æ²»ç«‹åœºæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯PRCç³»ç»Ÿæ¨¡å‹ä¸éPRCæ¨¡å‹ï¼‰çš„ç›´æ¥è·¨è¯­è¨€æ¯”è¾ƒã€‚æœ¬ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯„ä¼°DeepSeek-R1ï¼ˆPRCå¯¹é½ï¼‰ä¸ChatGPT o3-mini-highï¼ˆéPRCï¼‰çš„ä¸­æ–‡å›½å®¶å®£ä¼ ä¸åç¾æƒ…ç»ªï¼Œå¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚ç ”ç©¶å‘ç°æ¨¡å‹çº§åˆ«å’Œè¯­è¨€ä¾èµ–çš„åè§ã€‚DeepSeek-R1è¾ƒChatGPT o3-mini-highè¡¨ç°å‡ºæ›´é«˜çš„å®£ä¼ ä¸åç¾åè§ï¼Œåè€…åŸºæœ¬æ— åç¾æƒ…ç»ªä¸”å®£ä¼ ç¨‹åº¦è¾ƒä½ã€‚DeepSeek-R1åœ¨å›ç­”ç®€ä½“ä¸­æ–‡æŸ¥è¯¢æ—¶è¡¨ç°å‡ºæœ€é«˜çš„åè§ç‡ï¼Œç¹ä½“ä¸­æ–‡æŸ¥è¯¢ä¸­åè§ç‡é™ä½ï¼Œè‹±æ–‡å›ç­”ä¸­åè§å‡ ä¹ä¸å­˜åœ¨ã€‚æ­¤å¤–ï¼ŒDeepSeek-R1åœ¨å›ç­”ç¹ä½“ä¸­æ–‡æŸ¥è¯¢æ—¶ä¼šä»¥ç®€ä½“ä¸­æ–‡å›åº”ï¼Œå¹¶åœ¨å…¶ä¸­æ–‡ç­”æ¡ˆä¸­æ”¾å¤§å·²æœ‰PRCç«‹åœºæœ¯è¯­ï¼Œè¡¨ç°å‡ºâ€œéšå½¢æ‰¬å£°å™¨â€æ•ˆåº”ã€‚è¿™ç§åè§ä¸ä»…å­˜åœ¨äºæ”¿æ²»è¯é¢˜ä¸­ï¼Œä¹Ÿæ¸—é€åˆ°æ–‡åŒ–ä¸ç”Ÿæ´»æ–¹å¼å†…å®¹ä¸­ï¼Œå°¤å…¶åœ¨DeepSeek-R1ä¸­æ›´ä¸ºæ˜æ˜¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…¬ä¼—ç†è§£å’Œå…¬æ°‘å†³ç­–ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œå¼•å‘å¯¹å…¶æ„è¯†å½¢æ€ä¸­ç«‹æ€§çš„å…³æ³¨ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†DeepSeek-R1ï¼ˆPRCå¯¹é½ï¼‰ä¸ChatGPT o3-mini-highï¼ˆéPRCï¼‰çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>DeepSeek-R1è¾ƒChatGPT o3-mini-highå­˜åœ¨æ›´é«˜çš„å®£ä¼ ä¸åç¾åè§ã€‚</li>
<li>æ¨¡å‹åè§å­˜åœ¨è¯­è¨€ä¾èµ–æ€§ï¼ŒDeepSeek-R1åœ¨ç®€ä½“ä¸­æ–‡æŸ¥è¯¢ä¸­è¡¨ç°å‡ºæœ€é«˜åè§ç‡ï¼Œè€Œåœ¨è‹±æ–‡ä¸­å‡ ä¹æ— åè§ã€‚</li>
<li>DeepSeek-R1åœ¨å›åº”æ—¶å­˜åœ¨è¯­è¨€è½¬æ¢ç°è±¡ï¼Œå¹¶ä¼šæ”¾å¤§å·²æœ‰PRCç«‹åœºæœ¯è¯­ã€‚</li>
<li>æ¨¡å‹åè§ä¸ä»…å­˜åœ¨äºæ”¿æ²»è¯é¢˜ï¼Œä¹Ÿæ¸—é€åˆ°æ–‡åŒ–ä¸ç”Ÿæ´»æ–¹å¼å†…å®¹ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1026a8bd34fedaa0cd00e6ca0112d40c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31593c9a93ac5fd04bd000716e4cef24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-441032e73d1a810be8de6d1d54759ff4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a321e6cb798ba4382a4ccead303cc739.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Compress-Gather-and-Recompute-REFORMing-Long-Context-Processing-in-Transformers"><a href="#Compress-Gather-and-Recompute-REFORMing-Long-Context-Processing-in-Transformers" class="headerlink" title="Compress, Gather, and Recompute: REFORMing Long-Context Processing in   Transformers"></a>Compress, Gather, and Recompute: REFORMing Long-Context Processing in   Transformers</h2><p><strong>Authors:Woomin Song, Sai Muralidhar Jayanthi, Srikanth Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati</strong></p>
<p>As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the modelâ€™s pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„æ™®åŠåº¦ä¸æ–­æé«˜ï¼Œå¤„ç†æé•¿çš„ä¸Šä¸‹æ–‡å†…å®¹â€”â€”é€šå¸¸è¶…å‡ºæ¨¡å‹çš„é¢„è®­ç»ƒä¸Šä¸‹æ–‡é™åˆ¶â€”â€”å·²ç»æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚è™½ç„¶ç°æœ‰çš„é«˜æ•ˆé•¿ä¸Šä¸‹æ–‡å¤„ç†æ–¹æ³•æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åŸºäºå¾ªç¯å‹ç¼©çš„æ–¹æ³•åœ¨ä¿¡æ¯ä¿ç•™æ–¹é¢é‡åˆ°å›°éš¾ï¼Œè€Œéšæœºè®¿é—®æ–¹æ³•åˆ™éœ€è¦å¤§é‡å†…å­˜èµ„æºã€‚æˆ‘ä»¬å¼•å…¥äº†REFORMï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•æœ‰æ•ˆåœ°å¤„ç†é•¿ä¸Šä¸‹æ–‡ã€‚é¦–å…ˆï¼Œå®ƒå¢é‡å¤„ç†è¾“å…¥å—ï¼ŒåŒæ—¶ç»´æŠ¤å‹ç¼©çš„KVç¼“å­˜ï¼Œæ„å»ºè·¨å±‚ä¸Šä¸‹æ–‡åµŒå…¥ï¼Œå¹¶åˆ©ç”¨æå‰é€€å‡ºç­–ç•¥æé«˜æ•ˆç‡ã€‚å…¶æ¬¡ï¼Œå®ƒé€šè¿‡ç›¸ä¼¼åº¦åŒ¹é…æ¥è¯†åˆ«å’Œæ”¶é›†å…³é”®ä»¤ç‰Œï¼Œå¹¶é€‰æ‹©æ€§é‡æ–°è®¡ç®—KVç¼“å­˜ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼ŒREFORMåœ¨RULERå’ŒBABILongä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†50%å’Œ27%ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º1Mã€‚å®ƒåœ¨Infinite-Benchå’ŒMM-NIAHä¸Šä¹Ÿè¡¨ç°ä¼˜äºåŸºçº¿ï¼Œè¯æ˜äº†åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸçš„çµæ´»æ€§ã€‚æ­¤å¤–ï¼ŒREFORMå°†æ¨ç†æ—¶é—´å‡å°‘äº†30%ï¼Œå³°å€¼å†…å­˜ä½¿ç”¨ç‡é™ä½äº†5%ï¼Œå®ç°äº†æ•ˆç‡å’Œæ€§èƒ½çš„åŒé‡æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01215v1">PDF</a> </p>
<p><strong>Summary</strong><br>é•¿è¯­å¢ƒå¤„ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ä¸­é€æ¸æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†REFORMè¿™ä¸€æ–°é¢–æ¨ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µå¤„ç†é•¿è¯­å¢ƒï¼šä¸€ã€é€šè¿‡å¢é‡å¤„ç†è¾“å…¥ç‰‡æ®µã€ç»´æŒå‹ç¼©KVç¼“å­˜ã€æ„å»ºè·¨å±‚è¯­å¢ƒåµŒå…¥å¹¶é‡‡ç”¨æ—©æœŸé€€å‡ºç­–ç•¥æé«˜æ•ˆç‡ï¼›äºŒã€é€šè¿‡ç›¸ä¼¼æ€§åŒ¹é…è¯†åˆ«å’Œæ”¶é›†å…³é”®ä»¤ç‰Œï¼Œå¹¶é€‰æ‹©æ€§é‡æ–°è®¡ç®—KVç¼“å­˜ã€‚REFORMåœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸéƒ½å±•ç°å‡ºçµæ´»æ€§ï¼ŒåŒæ—¶æé«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®åº”ç”¨ä¸­é¢ä¸´å¤„ç†é•¿è¯­å¢ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚å‹ç¼©å’Œéšæœºè®¿é—®åœ¨å¤„ç†é•¿è¯­å¢ƒæ—¶å­˜åœ¨ä¿¡æ¯ä¿ç•™å’Œå†…å­˜éœ€æ±‚é—®é¢˜ã€‚</li>
<li>REFORMæ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•å¤„ç†é•¿è¯­å¢ƒï¼Œç»“åˆå¢é‡å¤„ç†ã€KVç¼“å­˜ã€è·¨å±‚è¯­å¢ƒåµŒå…¥å’Œæ—©æœŸé€€å‡ºç­–ç•¥æé«˜æ•ˆç‡ã€‚</li>
<li>REFORMé€šè¿‡ç›¸ä¼¼æ€§åŒ¹é…è¯†åˆ«å’Œæ”¶é›†å…³é”®ä»¤ç‰Œï¼Œé€‰æ‹©æ€§é‡æ–°è®¡ç®—KVç¼“å­˜ã€‚</li>
<li>REFORMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸçš„çµæ´»æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d335fc555f8d77d6af56ab512ce8361c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027d85810321b529dc70e4bd1d9724a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b67cc8beaacc6d0391fe70ad135bf7a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ba5babd67b9fc2102e36b6e04d1422.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72c28fd6113ee3496b72e722c07c3fae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ca9fd4ae2672fad48b7082853caeb84.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TRiMM-Transformer-Based-Rich-Motion-Matching-for-Real-Time-multi-modal-Interaction-in-Digital-Humans"><a href="#TRiMM-Transformer-Based-Rich-Motion-Matching-for-Real-Time-multi-modal-Interaction-in-Digital-Humans" class="headerlink" title="TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans"></a>TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans</h2><p><strong>Authors:Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, Fan Zhang</strong></p>
<p>Large Language Model (LLM)-driven digital humans have sparked a series of recent studies on co-speech gesture generation systems. However, existing approaches struggle with real-time synthesis and long-text comprehension. This paper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel multi-modal framework for real-time 3D gesture generation. Our method incorporates three modules: 1) a cross-modal attention mechanism to achieve precise temporal alignment between speech and gestures; 2) a long-context autoregressive model with a sliding window mechanism for effective sequence modeling; 3) a large-scale gesture matching system that constructs an atomic action library and enables real-time retrieval. Additionally, we develop a lightweight pipeline implemented in the Unreal Engine for experimentation. Our approach achieves real-time inference at 120 fps and maintains a per-sentence latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive subjective and objective evaluations on the ZEGGS, and BEAT datasets demonstrate that our model outperforms current state-of-the-art methods. TRiMM enhances the speed of co-speech gesture generation while ensuring gesture quality, enabling LLM-driven digital humans to respond to speech in real time and synthesize corresponding gestures. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching">https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching</a> </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­—äººå¼•å‘äº†ä¸€ç³»åˆ—å…³äºååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆç³»ç»Ÿçš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å®æ—¶åˆæˆå’Œé•¿æ–‡æœ¬ç†è§£æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformerçš„ä¸°å¯ŒåŠ¨ä½œåŒ¹é…ï¼ˆTRiMMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå®æ—¶3Dæ‰‹åŠ¿ç”Ÿæˆçš„æ–°å‹å¤šæ¨¡æ€æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¸‰ä¸ªæ¨¡å—ï¼š1ï¼‰è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°è¯­éŸ³å’Œæ‰‹åŠ¿ä¹‹é—´çš„ç²¾ç¡®æ—¶é—´å¯¹é½ï¼›2ï¼‰å…·æœ‰æ»‘åŠ¨çª—å£æœºåˆ¶çš„é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºæœ‰æ•ˆçš„åºåˆ—å»ºæ¨¡ï¼›3ï¼‰å¤§è§„æ¨¡æ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿï¼Œæ„å»ºåŸå­åŠ¨ä½œåº“ï¼Œå®ç°å®æ—¶æ£€ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨Unreal Engineä¸­å¼€å‘äº†ä¸€ä¸ªè½»é‡çº§çš„å®éªŒç®¡é“ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°å®æ—¶æ¨ç†é€Ÿåº¦ä¸ºæ¯ç§’å¤„ç†é«˜è¾¾æ¯ç§’ç”ŸæˆåŠ¨ä½œçš„å¸§æ•°ä¸ºé«˜è¾¾æ¯ç§’å¤„ç†å¸§æ•°å¯ä»¥è¾¾åˆ°æƒŠäººçš„æ¯å°æ—¶ç™¾ä¸‡å¸§çš„æ°´å¹³120å¸§ï¼ˆæ¯ç§’çš„å¸§é€Ÿç‡å¯è¾¾è¾¾åˆ°FPSï¼‰ã€‚å¹¶ä¸”ç»´æŒå»¶è¿Ÿä¿æŒåœ¨ä½çº§åˆ«GPUä¸Šæ¯å¥å»¶è¿Ÿä»…ä¸º0.15ç§’ï¼ˆGeforce RTX 3060ï¼‰ã€‚åœ¨ZEGGSå’ŒBEATæ•°æ®é›†ä¸Šçš„ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºå½“å‰çš„æœ€å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚TRiMMæé«˜äº†ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆçš„é€Ÿåº¦ï¼ŒåŒæ—¶ç¡®ä¿æ‰‹åŠ¿è´¨é‡ï¼Œä½¿LLMé©±åŠ¨çš„è™šæ‹Ÿæ•°å­—äººèƒ½å¤Ÿå®æ—¶å“åº”è¯­éŸ³å¹¶åˆæˆç›¸åº”çš„æ‰‹åŠ¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matchingè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01077v1">PDF</a> 24 pages,12 figures</p>
<p><strong>Summary</strong><br>åŸºäºTransformerçš„ä¸°å¯ŒåŠ¨ä½œåŒ¹é…ï¼ˆTRiMMï¼‰æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºå®æ—¶3Dæ‰‹åŠ¿ç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ã€é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹å’Œå¤§è§„æ¨¡æ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿï¼Œå®ç°äº†ç²¾ç¡®çš„æ—¶é—´å¯¹é½å’Œé«˜è´¨é‡çš„æ‰‹åŠ¿ç”Ÿæˆã€‚è¯¥æ¨¡å‹åœ¨ZEGGSå’ŒBEATæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœä¼˜äºå½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚TRiMMåŠ å¿«äº†ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆçš„é€Ÿåº¦ï¼Œä½¿å¾—LLMé©±åŠ¨çš„æ•°å­—äººèƒ½å¤Ÿå®æ—¶å“åº”è¯­éŸ³å¹¶åˆæˆç›¸åº”çš„æ‰‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TRiMMæ˜¯ä¸€ç§ç”¨äºå®æ—¶3Dæ‰‹åŠ¿ç”Ÿæˆçš„å¤šæ¨¡æ€æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ã€é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹å’Œå¤§è§„æ¨¡æ‰‹åŠ¿åŒ¹é…ç³»ç»Ÿã€‚</li>
<li>TRiMMå®ç°äº†ç²¾ç¡®çš„æ—¶é—´å¯¹é½ï¼Œä½¿æ‰‹åŠ¿ä¸è¯­éŸ³åŒæ­¥ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ZEGGSå’ŒBEATæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>TRiMMåŠ å¿«äº†ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆçš„å®æ—¶æ¨ç†é€Ÿåº¦ï¼Œè¾¾åˆ°æ¯ç§’å¤„ç†å¸§æ•°é«˜è¾¾120å¸§ã€‚</li>
<li>TRiMMç¡®ä¿äº†æ‰‹åŠ¿è´¨é‡ï¼Œä½¿å¾—LLMé©±åŠ¨çš„æ•°å­—äººèƒ½å®æ—¶å“åº”è¯­éŸ³å¹¶åˆæˆç›¸åº”æ‰‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82afd41e3d4bd1b369159061c3d5a52b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dc7d1337bb4d522e2d57c1ce947daa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28f550c263171b52ab5b84df1ea7d54.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="How-do-Transformer-Embeddings-Represent-Compositions-A-Functional-Analysis"><a href="#How-do-Transformer-Embeddings-Represent-Compositions-A-Functional-Analysis" class="headerlink" title="How do Transformer Embeddings Represent Compositions? A Functional   Analysis"></a>How do Transformer Embeddings Represent Compositions? A Functional   Analysis</h2><p><strong>Authors:Aishik Nagar, Ishaan Singh Rawal, Mansi Dhanania, Cheston Tan</strong></p>
<p>Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality. </p>
<blockquote>
<p>ç»„åˆæ€§æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼Œå¯¹äºæ¨ç†å’Œæ³›åŒ–è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹å·²æˆä¸ºè®¸å¤šè¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„é»˜è®¤æ ‡å‡†ï¼Œä½†å¯¹äºå®ƒä»¬å¦‚ä½•è¡¨ç¤ºå¤åˆè¯ä»¥åŠè¿™äº›è¡¨ç¤ºæ˜¯å¦ç»„åˆæ€§ï¼Œäººä»¬çš„äº†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨Mistralã€OpenAI Largeå’ŒGoogleåµŒå…¥æ¨¡å‹ä¸­æµ‹è¯•äº†ç»„åˆæ€§ï¼Œå¹¶å°†å…¶ä¸BERTè¿›è¡Œäº†æ¯”è¾ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æ£€æŸ¥å…­ç§ä¸åŒçš„ç»„åˆæ€§æ¨¡å‹ï¼ˆåŠ æ³•ã€ä¹˜æ³•ã€è†¨èƒ€ã€å›å½’ç­‰ï¼‰æ¥è¯„ä¼°è¡¨ç¤ºä¸­çš„ç»„åˆæ€§ã€‚æˆ‘ä»¬å‘ç°å²­å›å½’è™½ç„¶çº¿æ€§ï¼Œä½†æœ€èƒ½è§£é‡Šç»„åˆæ€§ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ç»å…¸çš„å‘é‡åŠ æ³•æ¨¡å‹çš„æ€§èƒ½å‡ ä¹ä¸å…¶ä»–ä»»ä½•æ¨¡å‹ä¸€æ ·å¥½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éªŒè¯å¤§å¤šæ•°åµŒå…¥æ¨¡å‹å…·æœ‰å¾ˆé«˜çš„ç»„åˆæ€§ï¼Œè€ŒBERTçš„ç»„åˆæ€§åˆ™è¾ƒå·®ã€‚æˆ‘ä»¬é€šè¿‡åŒ…å«å®Œå…¨é€æ˜çš„å½¢å®¹è¯-åè¯ç»„åˆçš„åˆæˆæ•°æ®é›†æ¥éªŒè¯å¹¶å¯è§†åŒ–æˆ‘ä»¬çš„å‘ç°ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹ç»„åˆæ€§è¿›è¡Œäº†å½»åº•çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00914v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ¨¡å‹åœ¨è¡¨ç°ç»„åˆè¯æ–¹é¢çš„èƒ½åŠ›ï¼Œå³æ‰€è°“çš„â€œç»„åˆæ€§â€ã€‚é€šè¿‡æµ‹è¯•Mistralã€OpenAI Largeã€GoogleåµŒå…¥æ¨¡å‹ä»¥åŠBERTæ¨¡å‹çš„ç»„åˆæ€§è¡¨ç°ï¼Œå‘ç°å¤§å¤šæ•°åµŒå…¥æ¨¡å‹å…·æœ‰è‰¯å¥½çš„ç»„åˆæ€§ï¼Œè€ŒBERTçš„è¡¨ç°è¾ƒå·®ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†å…­ç§ä¸åŒçš„ç»„åˆæ€§æ¨¡å‹æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°å²­å›å½’è™½ç„¶åœ¨ç»„åˆæ€§æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œä½†ç»å…¸çš„å‘é‡åŠ æ³•æ¨¡å‹åŒæ ·è¡¨ç°è‰¯å¥½ã€‚é€šè¿‡åˆæˆæ•°æ®é›†éªŒè¯äº†å½¢å®¹è¯å’Œåè¯ç»„åˆç»“æ„çš„å¯è§†åŒ–ç»“æœã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶å¯¹ç»„åˆæ€§è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†æ¨¡å‹åœ¨è¡¨ç°ç»„åˆè¯æ–¹é¢çš„èƒ½åŠ›ï¼Œå³ç»„åˆæ€§ã€‚</li>
<li>æµ‹è¯•äº†Mistralã€OpenAI Largeã€GoogleåµŒå…¥æ¨¡å‹å’ŒBERTæ¨¡å‹çš„ç»„åˆæ€§è¡¨ç°ã€‚</li>
<li>é€šè¿‡å…­ç§ä¸åŒçš„ç»„åˆæ€§æ¨¡å‹è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°å²­å›å½’å’Œå‘é‡åŠ æ³•æ¨¡å‹è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>å¤§å¤šæ•°åµŒå…¥æ¨¡å‹å…·æœ‰è‰¯å¥½çš„ç»„åˆæ€§ï¼Œè€ŒBERTè¡¨ç°è¾ƒå·®ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®é›†éªŒè¯äº†å½¢å®¹è¯å’Œåè¯ç»„åˆç»“æ„çš„å¯è§†åŒ–ç»“æœã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç»„åˆæ€§æ˜¯è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å…³é”®æ–¹é¢ï¼Œå¯¹äºæ¨ç†å’Œæ³›åŒ–å¾ˆé‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fb409fe0d20f9f536623271e4ffdb81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b93549e4b02c2c33d5f15de07dd7699a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38b5a2f893a8678ca9bfa0be698d9444.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daf1cc1c43288206ea0eab0787625c36.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Probing-the-Geometry-of-Truth-Consistency-and-Generalization-of-Truth-Directions-in-LLMs-Across-Logical-Transformations-and-Question-Answering-Tasks"><a href="#Probing-the-Geometry-of-Truth-Consistency-and-Generalization-of-Truth-Directions-in-LLMs-Across-Logical-Transformations-and-Question-Answering-Tasks" class="headerlink" title="Probing the Geometry of Truth: Consistency and Generalization of Truth   Directions in LLMs Across Logical Transformations and Question Answering   Tasks"></a>Probing the Geometry of Truth: Consistency and Generalization of Truth   Directions in LLMs Across Logical Transformations and Question Answering   Tasks</h2><p><strong>Authors:Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Zhengwen Feng, Hao Peng, Jianwei Yin</strong></p>
<p>Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the â€œtruth directionâ€, which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts. Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation. Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources. Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs. These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs. Our code is public at <a target="_blank" rel="noopener" href="https://github.com/colored-dye/truthfulness_probe_generalization">https://github.com/colored-dye/truthfulness_probe_generalization</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åœ¨åŒ…å«å¤§é‡ä¸–ç•ŒçŸ¥è¯†çš„å¹¿æ³›æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è¾“å‡ºé€šå¸¸åŒ…æ‹¬è‡ªä¿¡çš„ä¸å‡†ç¡®ä¹‹å¤„ã€‚æ—©æœŸçš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMå°†çœŸå®æ€§ç¼–ç ä¸ºä¸€ä¸ªç‹¬ç‰¹çš„çº¿æ€§ç‰¹å¾ï¼Œç§°ä¸ºâ€œçœŸå®æ–¹å‘â€ï¼Œå¯ä»¥å¯é åœ°åˆ†ç±»çœŸå®æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹å…³äºçœŸå®æ–¹å‘çš„å‡ ä¸ªå¼€æ”¾æ€§é—®é¢˜è¿›è¡Œæ¢è®¨ï¼šï¼ˆiï¼‰LLMæ˜¯å¦æ™®éè¡¨ç°å‡ºä¸€è‡´çš„çœŸå®æ–¹å‘ï¼›ï¼ˆiiï¼‰æ˜¯å¦éœ€è¦å¤æ‚çš„æ¢æµ‹æŠ€æœ¯æ¥ç¡®å®šçœŸå®æ–¹å‘ï¼›ï¼ˆiiiï¼‰çœŸå®æ–¹å‘å¦‚ä½•åœ¨ä¸åŒçš„è¯­å¢ƒä¸­é€šç”¨åŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå¹¶éæ‰€æœ‰LLMéƒ½è¡¨ç°å‡ºä¸€è‡´çš„çœŸå®æ–¹å‘ï¼Œåœ¨æ›´å¼ºå¤§çš„æ¨¡å‹ä¸­è§‚å¯Ÿåˆ°æ›´å¼ºçƒˆçš„è¡¨ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨é€»è¾‘å¦å®šçš„èƒŒæ™¯ä¸‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨é™ˆè¿°æ€§åŸå­è¯­å¥ä¸Šè®­ç»ƒçš„çœŸå®æ€§æ¢æµ‹å™¨å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°é€»è¾‘è½¬æ¢ã€é—®ç­”ä»»åŠ¡ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¤–éƒ¨çŸ¥è¯†æºã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†çœŸå®æ€§æ¢æµ‹å™¨åœ¨é€‰æ‹©æ€§é—®ç­”ä¸­çš„å®é™…åº”ç”¨ï¼Œè¯´æ˜äº†å®ƒä»¬åœ¨æé«˜ç”¨æˆ·å¯¹LLMè¾“å‡ºçš„ä¿¡ä»»æ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›ç»“æœæ¨åŠ¨äº†æˆ‘ä»¬å¯¹äºçœŸå®æ–¹å‘çš„ç†è§£ï¼Œå¹¶ä¸ºLLMä¿¡å¿µçš„å†…éƒ¨è¡¨ç¤ºæä¾›äº†æ–°çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/colored-dye/truthfulness_probe_generalization">https://github.com/colored-dye/truthfulness_probe_generalization</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00823v1">PDF</a> 19 pages, 16 figures; accepted to Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½åŸºäºå¹¿æ³›æ•°æ®é›†åŸ¹è®­ï¼ŒåŒ…å«ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†ï¼Œä½†å…¶è¾“å‡ºå¸¸å«è‡ªä¿¡é”™è¯¯ã€‚å…ˆå‰ç ”ç©¶æå‡ºLLMå†…å­˜åœ¨ä¸€ä¸ªç§°ä¸ºâ€œçœŸå®æ–¹å‘â€çš„çº¿æ€§ç‰¹å¾ï¼Œå¯å¯é åœ°åˆ†ç±»çœŸå®æ€§ã€‚æœ¬æ–‡æ¢è®¨å…³äºçœŸå®æ–¹å‘çš„å‡ ä¸ªå¼€æ”¾é—®é¢˜ï¼šLLMæ˜¯å¦æ™®éå±•ç°ä¸€è‡´çš„çœŸå®æ–¹å‘ï¼›æ˜¯å¦éœ€é«˜çº§æ¢æµ‹æŠ€æœ¯æ¥è¯†åˆ«çœŸå®æ–¹å‘ï¼›ä»¥åŠçœŸå®æ–¹å‘å¦‚ä½•åœ¨ä¸åŒè¯­å¢ƒä¸­æ™®åŠã€‚ç ”ç©¶å‘ç°ï¼Œå¹¶éæ‰€æœ‰LLMéƒ½å±•ç°ä¸€è‡´çœŸå®æ–¹å‘ï¼Œæ›´å¼ºå¤§çš„æ¨¡å‹åœ¨é€»è¾‘å¦å®šè¯­å¢ƒä¸‹è¡¨ç°æ›´æ˜æ˜¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜åœ¨é™ˆè¿°åŸå­è¯­å¥ä¸Šè®­ç»ƒçš„çœŸå®æ€§æ¢æµ‹å™¨å¯æœ‰æ•ˆæ¨å¹¿åˆ°é€»è¾‘è½¬æ¢ã€é—®ç­”ä»»åŠ¡ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¤–éƒ¨çŸ¥è¯†æºã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†çœŸå®æ€§æ¢æµ‹å™¨åœ¨é€‰æ‹©æ€§é—®ç­”ä¸­çš„å®é™…åº”ç”¨ï¼Œå±•ç¤ºäº†æé«˜ç”¨æˆ·å¯¹LLMè¾“å‡ºçš„ä¿¡ä»»åº¦çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¸¸è¾“å‡ºè‡ªä¿¡ä½†å¯èƒ½ä¸å‡†ç¡®çš„ç­”æ¡ˆã€‚</li>
<li>â€œçœŸå®æ–¹å‘â€æ˜¯LLMsä¸­ç”¨äºåˆ†ç±»çœŸå®æ€§çš„çº¿æ€§ç‰¹å¾ã€‚</li>
<li>ä¸åŒLLMå¯èƒ½å±•ç°ä¸åŒçš„çœŸå®æ–¹å‘ä¸€è‡´æ€§ã€‚</li>
<li>æ›´å…ˆè¿›çš„LLMåœ¨é€»è¾‘å¦å®šæ–¹é¢è¡¨ç°å‡ºæ›´å¼ºå¤§çš„çœŸå®æ–¹å‘ã€‚</li>
<li>çœŸå®æ€§æ¢æµ‹å™¨å¯åœ¨å¤šç§è¯­å¢ƒå’Œä»»åŠ¡ä¸­æœ‰æ•ˆæ¨å¹¿ã€‚</li>
<li>çœŸå®æ€§æ¢æµ‹å™¨æœ‰åŠ©äºæé«˜ç”¨æˆ·å¯¹LLMè¾“å‡ºçš„ä¿¡ä»»åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3445b1f2eab2828e52ca74c9623ca2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-601708bfa67fcc91b6fe6773c8b1fa43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-132081f5ff35cc02b7378fcac919b469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa8d034da06041f563ff8b56880ab8c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a70b737cb2affa3870496d2b9edfe9d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration"><a href="#MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration" class="headerlink" title="MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration"></a>MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration</h2><p><strong>Authors:Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung</strong></p>
<p>In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢é¢ä¸´å›ºæœ‰çš„æŒ‘æˆ˜ï¼Œè¿™éœ€è¦å¤šå±‚æ¬¡ï¼ˆä¾‹å¦‚æ„ŸçŸ¥ã€æ¨ç†ï¼‰å’Œå¤šç²’åº¦ï¼ˆä¾‹å¦‚å¤šæ­¥æ¨ç†é“¾ï¼‰çš„é«˜çº§æ¨æ–­ã€‚ä»¥å¾€å…³äºä¼°è®¡æ¨¡å‹ä¿¡å¿ƒçš„å·¥ä½œå¾€å¾€é›†ä¸­åœ¨è®­ç»ƒå’Œæ ¡å‡†çš„æ•´ä½“å“åº”ä¸Šï¼Œä½†æœªèƒ½è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒï¼Œå¯¼è‡´å‡ºç°ä¸å¸Œæœ›çœ‹åˆ°çš„å¹»è§‰ç´¯ç§¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MMBoundaryï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†ï¼Œæé«˜äº†MLLMsçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æè®®ç»“åˆè¡¥å……æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªæˆ‘å¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒã€‚é™¤äº†ä½¿ç”¨è‡ªæˆ‘å¥–åŠ±çš„ä¿¡å¿ƒä¼°è®¡ä¿¡å·å¯¹MLLMè¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼Œä»¥è¿›è¡Œåˆå§‹ä¿¡å¿ƒè¡¨è¾¾çš„é¢„çƒ­ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä½¿ç”¨å¤šä¸ªå¥–åŠ±å‡½æ•°æ¥è¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å¹¶æ ¡å‡†æ¯ä¸€æ­¥çš„ä¿¡å¿ƒï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒMMBoundaryåœ¨è·¨ä¸åŒé¢†åŸŸçš„æ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¤šæ¨¡æ€ä¿¡å¿ƒæ ¡å‡†è¯¯å·®å¹³å‡å‡å°‘äº†7.5%ï¼Œä»»åŠ¡æ€§èƒ½æé«˜äº†é«˜è¾¾8.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23224v2">PDF</a> 18 pages, ACL 2025</p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦å¤šçº§åˆ«å’Œå¤šç²’åº¦çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ¨¡å‹ä¿¡å¿ƒè¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æ•´ä½“å“åº”çš„åŸ¹è®­å’Œæ ¡å‡†ï¼Œè€Œå¿½è§†äº†å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒè¯„ä¼°ï¼Œå¯¼è‡´å‡ºç°ä¸å¿…è¦çš„å¹»è§‰ç´¯ç§¯ã€‚æœ¬ç ”ç©¶æå‡ºMMBoundaryæ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†æé«˜MLLMçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚ç»“åˆè¡¥å……æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªå¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒï¼Œå¹¶å¼•å…¥å¼ºåŒ–å­¦ä¹ é˜¶æ®µè¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å’Œæ ¡å‡†ä¿¡å¿ƒï¼Œæé«˜æ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚å®è¯ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒMMBoundaryåœ¨å¤šä¸ªé¢†åŸŸæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å‡å°‘7.5%çš„å¤šæ¨¡æ€ä¿¡å¿ƒæ ¡å‡†é”™è¯¯ï¼Œä»»åŠ¡æ€§èƒ½æé«˜8.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æé«˜å¤šçº§åˆ«å’Œå¤šç²’åº¦çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¿¡å¿ƒè¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æ•´ä½“å“åº”çš„åŸ¹è®­å’Œæ ¡å‡†ï¼Œç¼ºä¹å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒè¯„ä¼°ã€‚</li>
<li>MMBoundaryæ¡†æ¶é€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†æé«˜MLLMçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚</li>
<li>MMBoundaryç»“åˆè¡¥å……æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªå¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒã€‚</li>
<li>MMBoundaryå¼•å…¥å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œè¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å¹¶æ ¡å‡†ä¿¡å¿ƒã€‚</li>
<li>MMBoundaryèƒ½æé«˜æ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4e84b70fafdec136fbf20f34fc34b8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98fa6945fb647e0c77a6f1f4b4455683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724e70cad27d2693c6cc005ac8896228.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling"><a href="#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling" class="headerlink" title="Curse of High Dimensionality Issue in Transformer for Long-context   Modeling"></a>Curse of High Dimensionality Issue in Transformer for Long-context   Modeling</h2><p><strong>Authors:Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</strong></p>
<p>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at <a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œä»è€Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢ä¸´é‡å¤§çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼šè™½ç„¶æ³¨æ„åŠ›æƒé‡é€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œä½†æ‰€æœ‰ä»¤ç‰Œéƒ½æ¶ˆè€—ç€å¹³ç­‰çš„è®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„æ¦‚ç‡åºåˆ—å»ºæ¨¡é‡æ–°å®šä¹‰ä¸ºâ€œç›‘ç£å­¦ä¹ ä»»åŠ¡â€ï¼Œè¿™èƒ½å¤ŸåŒºåˆ†ç›¸å…³å’Œä¸ç›¸å…³çš„ä»¤ç‰Œï¼Œå¹¶æä¾›å¯¹å†—ä½™çš„æ›´æ¸…æ™°ç†è§£ã€‚åŸºäºè¿™ç§é‡æ–°å®šä¹‰ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œæ­ç¤ºåªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹äº§ç”Ÿäº†é‡å¤§è´¡çŒ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›ä¼˜åŒ–åˆ¶å®šä¸ºçº¿æ€§ç¼–ç é—®é¢˜ï¼Œå¹¶æå‡ºâ€œåˆ†ç»„ç¼–ç ç­–ç•¥â€ï¼Œä»ç†è®ºä¸Šå±•ç¤ºäº†å…¶æé«˜å¯¹æŠ—éšæœºå™ªå£°çš„ç¨³å¥æ€§å’Œæé«˜å­¦ä¹ æ•ˆç‡çš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåŠ¨æ€ç»„æ³¨æ„åŠ›â€ï¼ˆDGAï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†ç»„ç¼–ç æ¥é€šè¿‡èšåˆä¸å¤ªé‡è¦çš„ä»¤ç‰Œåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æ˜ç¡®å‡å°‘å†—ä½™ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DGAåœ¨ä¿æŒç«äº‰åŠ›æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention%E3%80%82">https://github.com/bolixinyu/DynamicGroupAttentionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22107v2">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ï¼Œå°¤å…¶æ˜¯é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé•¿æ–‡æœ¬å»ºæ¨¡å­˜åœ¨è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦æ˜¯ç”±äºæ³¨æ„åŠ›è®¡ç®—çš„å†—ä½™æ€§ã€‚å¯¹æ­¤ï¼Œæ–‡ç« é€šè¿‡é‡æ–°æ„å»ºæ¦‚ç‡åºåˆ—æ¨¡å‹ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œåˆ†ç¦»é‡è¦ä¸éé‡è¦ä¿¡æ¯ä»¥è¯†åˆ«å†—ä½™ï¼Œå¯¹æ³¨æ„åŠ›ç¨€ç–æ€§è¿›è¡Œäº†ç†è®ºåˆ†æã€‚æ–‡ç« æå‡ºä¸€ç§åŸºäºçº¿æ€§ç¼–ç çš„æ³¨æ„åŠ›ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶æ®æ­¤æå‡ºåŠ¨æ€åˆ†ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰æœºåˆ¶ï¼Œé€šè¿‡èšåˆæ¬¡è¦ä¿¡æ¯è¿›è¡Œæ›´æ˜ç¡®çš„è®¡ç®—ä»¥å‡å°‘å†—ä½™æ€§ã€‚ç»éªŒè¡¨æ˜ï¼Œè¯¥æœºåˆ¶åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMs æ“…é•¿æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚</li>
<li>é•¿æ–‡æœ¬å»ºæ¨¡é¢ä¸´è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œä¸»è¦ç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚</li>
<li>é€šè¿‡å°†æ¦‚ç‡åºåˆ—æ¨¡å‹é‡æ–°æ„å»ºä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå¯åˆ†ç¦»é‡è¦ä¸éé‡è¦ä¿¡æ¯ï¼Œæ›´å¥½åœ°ç†è§£å†—ä½™ã€‚</li>
<li>å¯¹æ³¨æ„åŠ›ç¨€ç–æ€§çš„ç†è®ºåˆ†ææ˜¾ç¤ºï¼Œåªæœ‰å°‘æ•°æ ‡è®°å¯¹é¢„æµ‹æœ‰é‡è¦è´¡çŒ®ã€‚</li>
<li>çº¿æ€§ç¼–ç ç­–ç•¥çš„æå‡ºæ˜¯ä¸ºäº†ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºçš„åŠ¨æ€åˆ†ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰å¯ä»¥å‡å°‘å†—ä½™æ€§ã€‚</li>
<li>åŠ¨æ€åˆ†ç»„æ³¨æ„åŠ›æœºåˆ¶åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67b46fe3fb47c0d865dac23ea0cfcce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eca6ec960b889a2e81e3ec88cc965ea2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96d12c51275a45c5dadcb236c0b4f035.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  Truly Self-Improving Agents Require Intrinsic Metacognitive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05d7ca8eb66922610ad3cf28a702c0b5.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  VideoMathQA Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
