<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-08  SparseMM Head Sparsity Emerges from Visual Concept Responses in MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-31593c9a93ac5fd04bd000716e4cef24.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    88 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-08-更新"><a href="#2025-06-08-更新" class="headerlink" title="2025-06-08 更新"></a>2025-06-08 更新</h1><h2 id="SparseMM-Head-Sparsity-Emerges-from-Visual-Concept-Responses-in-MLLMs"><a href="#SparseMM-Head-Sparsity-Emerges-from-Visual-Concept-Responses-in-MLLMs" class="headerlink" title="SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"></a>SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs</h2><p><strong>Authors:Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu</strong></p>
<p>Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at <a target="_blank" rel="noopener" href="https://github.com/CR400AF-A/SparseMM">https://github.com/CR400AF-A/SparseMM</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）通常是通过在预训练的大型语言模型（LLMs）的基础上增强其视觉能力而得到的。在这项研究中，我们通过分析注意力机制来研究MLLMs如何处理视觉输入。我们揭示了一个令人惊讶的稀疏现象：在LLMs中，只有一小部分（大约不到5%）的注意力头对视觉理解有积极贡献，这些被称为视觉头。为了有效地识别这些头部，我们设计了一个无训练框架，该框架通过有针对性的响应分析量化头部级别的视觉相关性。基于这一发现，我们引入了SparseMM，这是一种KV-Cache优化策略，根据视觉分数为LLMs中的头部分配不对称的计算预算，利用视觉头部的稀疏性来加速MLLMs的推理过程。与之前忽略视觉特性的KV-Cache加速方法相比，SparseMM在解码过程中优先考虑压力和保留视觉语义。在主流的多模态基准测试上的广泛评估表明，SparseMM实现了卓越的准确性-效率权衡。值得注意的是，在生成过程中，SparseMM实现了1.38倍的实时加速和52%的内存减少，同时在效率测试上保持了性能上的平衡。我们的项目已开源在<a target="_blank" rel="noopener" href="https://github.com/CR400AF-A/SparseMM%E3%80%82">https://github.com/CR400AF-A/SparseMM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05344v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了多模态大型语言模型（MLLMs）如何处理视觉输入，通过对其注意力机制进行分析，揭示了一种惊人的稀疏现象：只有一小部分（大约少于5%）的注意力头对视觉理解有积极贡献，被称为视觉头。基于此发现，本文提出了一种基于KV-Cache优化的SparseMM策略，根据视觉分数对LLM中的头进行不对称计算预算分配，利用视觉头的稀疏性加速MLLM的推理。实验表明，SparseMM在主流的多模态基准测试中实现了卓越的准确性-效率权衡，实现了1.38倍的实时加速和52%的内存减少，同时保持性能不变。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）是通过扩展预训练的大型语言模型（LLMs）并加入视觉能力形成的。</li>
<li>MLLMs处理视觉输入时，只有一小部分注意力头（视觉头）对视觉理解有贡献。</li>
<li>提出了一种无需训练的培训框架，通过有针对性的响应分析来量化头部级别的视觉相关性。</li>
<li>基于这一发现，引入了SparseMM策略，这是一种KV-Cache优化策略，根据视觉分数为LLM中的头分配不对称的计算预算。</li>
<li>SparseMM利用视觉头的稀疏性来加速MLLM的推理。</li>
<li>与忽略视觉特性的KV-Cache加速方法相比，SparseMM在解码过程中优先考虑压力并保持视觉语义的保留。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05344">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-94b4af2a8ec6667d9e77844228f14e45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c0948d8004001be27bc612e7ae61472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0e48b37aae6834874fb7b87056d9906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f972cb95a84bf8a1d75596e74aeb485.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VideoMolmo-Spatio-Temporal-Grounding-Meets-Pointing"><a href="#VideoMolmo-Spatio-Temporal-Grounding-Meets-Pointing" class="headerlink" title="VideoMolmo: Spatio-Temporal Grounding Meets Pointing"></a>VideoMolmo: Spatio-Temporal Grounding Meets Pointing</h2><p><strong>Authors:Ghazi Shazan Ahmad, Ahmed Heakl, Hanan Gani, Abdelrahman Shaker, Zhiqiang Shen, Ranjay Krishna, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/VideoMolmo">https://github.com/mbzuai-oryx/VideoMolmo</a>. </p>
<blockquote>
<p>时空定位在多个领域（从生物研究到自主导航和交互式界面）的精确交互中起着至关重要的作用。尽管当前基于视频的方法在跟踪方面非常熟练，但它们缺乏大型语言模型的复杂推理能力，从而限制了其在上下文理解和泛化方面的能力。我们引入了VideoMolmo，这是一个大型多模态模型，专为精细的时空定位而设计，该定位基于文本描述。VideoMolmo建立在Molmo架构之上，并加入了一个时间模块，该模块利用注意力机制将每一帧置于先前的帧上，确保时间的一致性。此外，我们新颖的时间掩膜融合管道采用SAM2进行双向点传播，显著提高了视频序列之间的连贯性。这种两步分解法，即首先使用大型语言模型生成精确的指向坐标，然后依赖于顺序掩膜融合模块来产生连贯的分割，不仅简化了语言模型的任务，还提高了可解释性。由于缺乏合适的数据集，我们整理了一个综合数据集，包含7.2万对视频字幕和标注的10万个目标点。为了评估VideoMolmo的泛化能力，我们引入了VPoS-Bench这一颇具挑战性的分布外基准测试平台，它涵盖五种现实场景：细胞追踪、以自我为中心的视觉、自动驾驶、视频界面交互和机器人技术。我们还评估了我们的模型在引用视频对象分割（Refer-VOS）和推理VOS任务上的表现。与现有模型相比，VideoMolmo大大提高了时空定位精度和推理能力。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/VideoMolmo%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/mbzuai-oryx/VideoMolmo公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05336v1">PDF</a> 20 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了VideoMolmo，一个大型多模态模型，专为精细时空定位而设计，可基于文本描述进行精确互动。它通过结合语言模型和时序模块，提高了视频中的时空定位精度和推理能力。此外，其独特的时间掩膜融合管道增强了视频序列的连贯性。为了训练和评估该模型，作者创建了一个大型视频标注数据集VPoS-Bench和一个挑战性的跨域基准测试。总体上，VideoMolmo显著提高了时空定位精度和推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoMolmo是一个大型多模态模型，专门用于精细的时空定位任务。</li>
<li>该模型结合了文本描述进行时空定位，增强了模型的理解和推理能力。</li>
<li>VideoMolmo采用了新颖的时空掩膜融合管道以提高视频序列连贯性。</li>
<li>为训练和评估该模型，创建了一个包含大量视频标注的大规模数据集VPoS-Bench。</li>
<li>VideoMolmo模型引入了新的跨域基准测试来评估其泛化能力。该模型可广泛应用于不同领域如生物学研究、自动驾驶和交互式界面等。</li>
<li>VideoMolmo在时空定位精度和推理能力方面显著优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05336">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7c9b98b6441c68dc42b2b1c7f19ac475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab323253ebe927af8707dc9cbc00af0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c88b08092c6cc1cf856169e3f8c97e91.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Search-Arena-Analyzing-Search-Augmented-LLMs"><a href="#Search-Arena-Analyzing-Search-Augmented-LLMs" class="headerlink" title="Search Arena: Analyzing Search-Augmented LLMs"></a>Search Arena: Analyzing Search-Augmented LLMs</h2><p><strong>Authors:Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez</strong></p>
<p>Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model’s parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/lmarena/search-arena">https://github.com/lmarena/search-arena</a>. </p>
<blockquote>
<p>搜索增强型语言模型结合了网络搜索与大型语言模型（LLM），以提高响应的真实性和实时性。然而，分析这些系统仍然具有挑战性：现有数据集规模有限，范围狭窄，通常仅限于静态、单轮的事实核查问题。在这项工作中，我们介绍了Search Arena，这是一个大规模、众源的人类偏好数据集，包含超过24,000个与搜索增强型LLM的多轮用户交互配对。该数据集涵盖多样的意图和语言，包含约12,000次人类偏好投票的完整系统跟踪记录。我们的分析表明，用户偏好受到引用数量的影响，即使引用的内容并没有直接支持所归属的主张，这揭示了感知可信度和实际可信度之间的差距。此外，不同来源的用户偏好各不相同，表明社区驱动的平台通常更受欢迎，而静态百科全书来源并不总是适当和可靠。为了评估不同环境下的性能，我们通过测试搜索增强型LLM在通用聊天环境和传统LLM在搜索密集型环境中的表现来进行跨领域分析。我们发现，在非搜索环境中，网络搜索并不会降低性能，甚至可能提高性能；然而，如果在仅依赖模型的参数知识的情况下，搜索环境中的质量会显著受到影响。我们公开了数据集，以支持未来在这方面的研究。我们的数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/lmarena/search-arena%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lmarena/search-arena找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05334v1">PDF</a> Preprint. Code: <a target="_blank" rel="noopener" href="https://github.com/lmarena/search-arena">https://github.com/lmarena/search-arena</a>. Dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lmarena-ai/search-arena-24k">https://huggingface.co/datasets/lmarena-ai/search-arena-24k</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了搜索增强语言模型（Search-augmented LLMs）的研究。为提高响应的扎实性和新颖性，研究者将网页搜索与大型语言模型（LLMs）结合。为分析这些系统，研究团队推出了一款名为Search Arena的大型、人群参与的大规模数据集，涵盖了超过2万四千次配对的多轮用户与搜索增强LLMs的互动。分析显示，用户偏好受引用数量影响，即使引用的内容并未直接支持所声明的观点，感知可信度与实际可信度之间存在差距。此外，用户对引用来源的偏好各异，社区驱动平台普遍受欢迎，而静态百科全书来源并非始终适当和可靠。评估不同设置中的性能时，研究发现网页搜索在非搜索环境中并不会降低性能，甚至可能提高性能；但如果仅依赖模型的参数知识，则在搜索环境中的质量会明显受到影响。数据集已开源，支持未来相关研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>搜索增强语言模型结合了网页搜索和大型语言模型（LLMs），以提高响应的扎实性和新颖性。</li>
<li>现有数据集在规模和范围上存在局限性，常局限于静态、单轮、查证事实的问题。</li>
<li>推出名为Search Arena的大型数据集，包含超过2万四千次配对的多轮用户与搜索增强LLMs的互动记录。</li>
<li>用户偏好受引用数量影响，即使引用的内容未直接支持观点，也存在感知可信度与实际可信度的差距。</li>
<li>用户对引用来源的偏好不同，社区驱动平台更受欢迎，静态百科全书来源并非始终可靠。</li>
<li>网页搜索在非搜索环境中不会降低性能，甚至可能提高性能；但在搜索环境中，仅依赖模型的参数知识会影响质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05334">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-05430d139604a753f03bf8efcdf96cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1343e16ceedd5c8cf039766c3b2e50b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf03a2e5dac535e8c8bb9d5f0090cdb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eada50c04826377a4b06a565b4a92e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bcd0bc61c0e712d944b9bbb67af91f9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning"><a href="#MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning" class="headerlink" title="MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning"></a>MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning</h2><p><strong>Authors:Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li</strong></p>
<p>Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a> </p>
<blockquote>
<p>“链式思维（Chain-of-Thought，简称CoT）已在大型语言模型（LLM）中广泛提高了数学推理能力，但将其扩展到多模态领域仍然具有挑战性。现有工作要么采用类似的文本推理进行图像输入，要么寻求将视觉信号融入数学CoT。然而，它们在解决数学问题方面面临三个主要局限性：依赖粗粒度的框状图像区域、视觉编码器对数学内容的感知有限，以及依赖外部能力进行视觉修改。在本文中，我们提出MINT-CoT，引入用于链式思维视觉推理的数学交织令牌（Mathematical INterleaved Tokens）。MINT-CoT通过交织令牌自适应地将相关视觉令牌交织到文本推理步骤中，该令牌动态选择数学图形内的任何形状的视觉区域。为了支持此功能，我们构建了MINT-CoT数据集，包含54K个数学问题，每个推理步骤都与令牌级别的视觉区域对齐，并配有严格的数据生成流程。我们还提出了一个三阶段的MINT-CoT训练策略，逐步结合纯文本CoTSFT、交织CoTSFT和交织CoTRL，从而衍生出我们的MINT-CoT-7B模型。大量实验表明，我们的方法在数学领域进行有效的视觉交织推理方面的有效性，MINT-CoT-7B在MathVista上超越基准模型+34.08%，在GeoQA上+28.78%，在MMStar上+23.2%。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xinyan-cxy/MINT-CoT找到。</a>“</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05331v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了在数学领域中，Chain-of-Thought（CoT）在多模态领域的应用挑战。现有方法存在依赖粗粒度图像区域、对数学问题视觉编码的认知受限以及对视觉修改的外部能力依赖等三个关键局限性。本文提出了MINT-CoT，引入了数学交织令牌（Mathematical INterleaved Tokens）进行视觉推理。MINT-CoT自适应地将相关视觉令牌插入到文本推理步骤中，通过动态选择数学图形内的任何形状视觉区域来实现这一点。为了支持此功能，构建了包含54K数学问题的MINT-CoT数据集，每个推理步骤都与视觉区域在令牌级别对齐。同时介绍了三个阶段的MINT-CoT训练策略，包括纯文本CoT的SFT、交织CoT的SFT和交织CoT的RL。实验结果证明了MINT-CoT的有效性，其中MINT-CoT-7B模型在MathVista、GeoQA和MMStar上的性能分别优于基线模型+34.08%、+28.78%和+23.2%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MINT-CoT解决了现有方法在将Chain-of-Thought（CoT）应用于多模态领域的挑战。</li>
<li>现有方法在数学问题求解中存在三个关键局限性：依赖粗粒度图像区域、对数学问题视觉编码的认知受限以及对视觉修改的外部能力依赖。</li>
<li>MINT-CoT引入了数学交织令牌（Mathematical INterleaved Tokens）进行视觉推理，可以自适应地将相关视觉令牌插入文本推理步骤中。</li>
<li>MINT-CoT使用动态选择数学图形内任何形状视觉区域的Interleave Token技术。</li>
<li>MINT-CoT数据集包含54K数学问题，每个推理步骤都与视觉区域在令牌级别对齐。</li>
<li>MINT-CoT训练策略采用三个阶段的训练方法，包括纯文本CoT的SFT、交织CoT的SFT和交织CoT的RL。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9e0e517d4f8b10aa9ca2f54ac60ba249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ac84bdb90b912b32fbef78fa3d2fff9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e516bcb6bcba5b8144ca314a595fcedb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568fe4ddee8c914dbbeabe58cb026b37.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Constrained-Entropic-Unlearning-A-Primal-Dual-Framework-for-Large-Language-Models"><a href="#Constrained-Entropic-Unlearning-A-Primal-Dual-Framework-for-Large-Language-Models" class="headerlink" title="Constrained Entropic Unlearning: A Primal-Dual Framework for Large   Language Models"></a>Constrained Entropic Unlearning: A Primal-Dual Framework for Large   Language Models</h2><p><strong>Authors:Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab</strong></p>
<p>Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility. </p>
<blockquote>
<p>大型语言模型（LLM）在现实世界的部署中越来越需要遗忘敏感、过时或专有信息。现有的遗忘方法通常将遗忘和保留制定为规范化的权衡，将两个目标结合成一个单一的标量化损失。这通常会导致优化不稳定，以及在保留数据上的性能下降，尤其是在强烈的遗忘情况下。我们提出了一种新的LLM遗忘公式，将其视为约束优化问题：通过新的logit-margin平坦损失强制执行遗忘，该损失显式地将输出分布推向指定遗忘集上的均匀分布，同时通过在单独的保留集上设置硬约束来保留保留信息。与基于熵的目标相比，我们的损失无softmax，数值稳定，保持非零梯度，能够实现更高效和稳健的优化。我们使用可扩展的原对偶算法解决约束问题，通过双变量动态暴露遗忘和保留之间的权衡。在TOFU和MUSE基准测试上对多种LLM架构的评估表明，我们的方法始终匹配或超过最新基线，在去除目标信息的同时保持下游实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05314v1">PDF</a> </p>
<p><strong>Summary</strong><br>大语言模型部署在现实世界中时，需要去除敏感、过时或专有信息。现有去学习方法通常将遗忘和保留表述为正则化的权衡问题，导致优化不稳定和对保留数据的性能下降。为此，我们提出将大语言模型去学习作为约束优化问题来表述，通过新颖的logit-margin平坦损失强制输出分布均匀化以实现遗忘，同时通过硬约束在保留集上保留保留信息。我们的损失函数无需softmax，数值稳定，保持非零梯度，实现了更高效的优化。采用可扩展的原生对偶算法解决约束问题，通过对偶变量的动态揭示遗忘和保留之间的权衡。在TOFU和MUSE基准上的评估表明，我们的方法能够达到或超过最新基线水平，有效地消除目标信息的同时保留下游实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型在现实应用中需要去除敏感、过时或专有信息。</li>
<li>现有去学习方法存在优化不稳定和对保留数据性能下降的问题。</li>
<li>提出将大语言模型去学习表述为约束优化问题，通过logit-margin平坦损失实现遗忘。</li>
<li>损失函数无需softmax，数值稳定，保持非零梯度，优化更高效。</li>
<li>采用可扩展的原生对偶算法解决约束问题，展现遗忘和保留之间的权衡。</li>
<li>方法在TOFU和MUSE基准上表现优异，能有效去除目标信息同时保留下游实用性。</li>
<li>这种方法为大型语言模型的去学习提供了一个新的、有效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ed8ade66aa6778db83427d265502b20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59f8840208ef224443c14a4b03f14a66.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sample-Complexity-and-Representation-Ability-of-Test-time-Scaling-Paradigms"><a href="#Sample-Complexity-and-Representation-Ability-of-Test-time-Scaling-Paradigms" class="headerlink" title="Sample Complexity and Representation Ability of Test-time Scaling   Paradigms"></a>Sample Complexity and Representation Ability of Test-time Scaling   Paradigms</h2><p><strong>Authors:Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao</strong></p>
<p>Test-time scaling paradigms have significantly advanced the capabilities of large language models (LLMs) on complex tasks. Despite their empirical success, theoretical understanding of the sample efficiency of various test-time strategies – such as self-consistency, best-of-$n$, and self-correction – remains limited. In this work, we first establish a separation result between two repeated sampling strategies: self-consistency requires $\Theta(1&#x2F;\Delta^2)$ samples to produce the correct answer, while best-of-$n$ only needs $\Theta(1&#x2F;\Delta)$, where $\Delta &lt; 1$ denotes the probability gap between the correct and second most likely answers. Next, we present an expressiveness result for the self-correction approach with verifier feedback: it enables Transformers to simulate online learning over a pool of experts at test time. Therefore, a single Transformer architecture can provably solve multiple tasks without prior knowledge of the specific task associated with a user query, extending the representation theory of Transformers from single-task to multi-task settings. Finally, we empirically validate our theoretical results, demonstrating the practical effectiveness of self-correction methods. </p>
<blockquote>
<p>测试时缩放范式已显着提升大型语言模型（LLM）在复杂任务上的能力。尽管它们在经验上取得了成功，但关于各种测试时间策略（例如自洽性、best-of-$n$以及自我校正）的样本效率的理论理解仍然有限。在这项工作中，我们首先确定了两种重复采样策略之间的分离结果：自洽性需要$\Theta(1&#x2F;\Delta^2)$样本才能得出正确答案，而best-of-$n$仅需要$\Theta(1&#x2F;\Delta)$样本，其中$\Delta &lt; 1$表示正确和第二大可能的答案之间的概率差距。接下来，我们为带有验证器反馈的自我校正方法提供了表现力结果：它使Transformer能够在测试时模拟专家池中的在线学习。因此，单个Transformer架构可以在不了解与用户查询相关的特定任务的情况下解决多个任务，从而将Transformer的表示理论从单任务扩展到多任务设置。最后，我们通过实验验证了我们的理论结果，证明了自我校正方法的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>测试时间缩放范式在复杂任务中显著提升了大型语言模型的能力。虽然它们在经验上取得了成功，但对于各种测试时间策略的理论样本效率（如自我一致性、最优选择方法和自我修正等）的理论理解仍然有限。本文中，我们建立了一个样本策略的分离结果：自我一致性需要更多的样本才能产生正确的答案，而最优选择方法则需要较少的样本。此外，我们还给出了自我修正方法与验证器反馈的表达性结果：它使Transformer能够在测试时模拟在线学习并共享专家意见池的能力。因此，单一Transformer架构可证明能够在不事先了解与用户的查询相关联的具体任务的情况下解决多任务问题，从而从单任务扩展了Transformer的表示理论至多任务场景。最后，我们通过实证验证了我们的理论结果，证明了自我修正方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>测试时间缩放范式增强了大型语言模型在复杂任务上的表现。</li>
<li>样本效率对于不同的测试时间策略存在理论差异。</li>
<li>自我一致性策略和最优选择方法的样本需求不同。</li>
<li>自我修正方法通过验证器反馈增强了Transformer的多任务学习能力。</li>
<li>Transformer能够在不预先了解特定任务的情况下解决多任务问题。</li>
<li>自我修正方法实现了Transformer从单任务表示理论向多任务表示理论的扩展。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8519edea215d36d25ff49a17cac14cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb13e38be03ddaf79b92a7cfafac4715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68e0b14b77f93cadf44ac2c8cf362b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-352ba0fa2e4b7369aa7b520a54ed93f4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning"><a href="#Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning" class="headerlink" title="Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning"></a>Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning</h2><p><strong>Authors:Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications. </p>
<blockquote>
<p>检索增强生成（RAG）系统通常面临知识冲突的问题，即检索到的外部知识与大型语言模型（LLM）的内在参数知识相矛盾。这会对问答等下游任务性能产生不利影响。现有方法往往试图通过并排比较两种知识来源来减轻冲突，但这可能会使语言模型面临过多或冗长的上下文，最终阻碍其识别和缓解不一致的能力。为了解决这一问题，我们提出了Micro-Act框架，它具有分层动作空间，可自动感知上下文复杂性，并自适应地将每个知识源分解为一系列精细的比较。这些比较表现为可操作的步骤，能够进行超越表面上下文的推理。在五组基准数据集上进行的大量实验表明，Micro-Act在所有五个数据集和三种冲突类型上，均较最先进的基线模型在问答准确性方面有显著提高，特别是在时间和语义类型方面，所有基线模型均表现不佳。更重要的是，Micro-Act在非冲突问题上也表现出稳健的性能，这凸显了其在现实世界RAG应用中的实用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05278v1">PDF</a> Accepted by ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Retrieval-Augmented Generation（RAG）系统中常见的知识冲突问题，即检索的外部知识与大型语言模型（LLM）的内在参数知识相矛盾。这会对问答等下游任务性能产生负面影响。现有方法常常通过并排比较两种知识源来减轻冲突，但这可能使LLM面临繁琐或冗长的上下文，最终阻碍其识别和缓解不一致的能力。为解决这一问题，本文提出了Micro-Act框架，它具有分层动作空间，可自动感知上下文复杂性，并自适应地将每个知识源分解成一系列精细的比较。这些比较表现为可操作的步骤，使推理超越了表面上下文。在五个基准数据集上的广泛实验表明，Micro-Act在所有数据集和三种冲突类型上，尤其是在时间和语义类型上，相较于最新基线技术，问答准确性有了显著的提升。更重要的是，Micro-Act在非冲突问题上也表现出稳健的性能，凸显其在现实世界的RAG应用中的实用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG系统面临知识冲突问题，即外部检索知识与LLM的内在知识相矛盾。</li>
<li>现有方法通过并排比较知识源来减轻冲突，但可能使LLM面临繁琐的上下文。</li>
<li>Micro-Act框架具有分层动作空间，可自动感知上下文复杂性并分解知识源。</li>
<li>Micro-Act通过精细的比较和可操作的步骤进行推理，超越表面上下文。</li>
<li>Micro-Act在多个数据集和三种冲突类型上显著提高了问答准确性。</li>
<li>Micro-Act在非冲突问题上也表现出稳健的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-827152fce07163683ee4731b5081d264.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c41fbee446036f82e6d35bef820a5cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cb7fa84605926f84e3840f592cd3cd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16b53a70df398ac3e002d9d2ed33389e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d870c33b971ca6264a6abf75e66f1b84.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LeanPO-Lean-Preference-Optimization-for-Likelihood-Alignment-in-Video-LLMs"><a href="#LeanPO-Lean-Preference-Optimization-for-Likelihood-Alignment-in-Video-LLMs" class="headerlink" title="LeanPO: Lean Preference Optimization for Likelihood Alignment in   Video-LLMs"></a>LeanPO: Lean Preference Optimization for Likelihood Alignment in   Video-LLMs</h2><p><strong>Authors:Xiaodong Wang, Jinfa Huang, Li Yuan, Peixi Peng</strong></p>
<p>Most Video Large Language Models (Video-LLMs) adopt preference alignment techniques, e.g., DPO~\citep{rafailov2024dpo}, to optimize the reward margin between a winning response ($y_w$) and a losing response ($y_l$). However, the likelihood displacement observed in DPO indicates that both $\log \pi_\theta (y_w\mid x)$ and $\log \pi_\theta (y_l\mid x) $ often decrease during training, inadvertently boosting the probabilities of non-target responses. In this paper, we systematically revisit this phenomenon from LLMs to Video-LLMs, showing that it intensifies when dealing with the redundant complexity of video content. To alleviate the impact of this phenomenon, we propose \emph{Lean Preference Optimization} (LeanPO), a reference-free approach that reformulates the implicit reward as the average likelihood of the response with respect to the policy model. A key component of LeanPO is the reward-trustworthiness correlated self-generated preference data pipeline, which carefully infuses relevant prior knowledge into the model while continuously refining the preference data via self-reflection. This allows the policy model to obtain high-quality paired data and accurately estimate the newly defined reward, thus mitigating the unintended drop. In addition, we introduce a dynamic label smoothing strategy that mitigates the impact of noise in responses from diverse video content, preventing the model from overfitting to spurious details. Extensive experiments demonstrate that LeanPO significantly enhances the performance of state-of-the-art Video-LLMs, consistently boosting baselines of varying capacities with minimal additional training overhead. Moreover, LeanPO offers a simple yet effective solution for aligning Video-LLM preferences with human trustworthiness, paving the way toward the reliable and efficient Video-LLMs. </p>
<blockquote>
<p>大多数视频大语言模型（Video-LLMs）采用偏好对齐技术，例如DPO~\citep{rafailov2024dpo}，以优化获胜响应（yw）和失败响应（yl）之间的奖励差距。然而，DPO中观察到的可能性位移表明，logπθ(yw∣x)和logπθ(yl∣x)在训练过程中往往会减少，这无意中提升了非目标响应的概率。本文系统地回顾了从大语言模型到视频大语言模型中的这种现象，表明在处理视频内容的冗余复杂性时，这种现象会加剧。为了缓解这种现象的影响，我们提出了无需参考的偏好优化（LeanPO），将隐性奖励重新定义为关于策略模型的响应的平均可能性。LeanPO的关键组件是奖励可信度相关的自我生成的偏好数据管道，它谨慎地将相关先验知识融入模型中，同时通过自我反思不断精炼偏好数据。这使得策略模型能够获得高质量配对数据并准确估计新定义的奖励，从而缓解无意中的下降。此外，我们引入了一种动态标签平滑策略，减轻了来自不同视频内容中的响应噪声的影响，防止模型过度拟合于虚假细节。大量实验表明，LeanPO显著提高了最先进的Video-LLMs的性能，始终如一地提高了不同容量的基线性能，并且额外的训练开销最小。此外，LeanPO提供了一个简单而有效的解决方案，使Video-LLM的偏好与人类可信度保持一致，为可靠高效的Video-LLMs铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05260v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/Wang-Xiaodong1899/LeanPO">https://github.com/Wang-Xiaodong1899/LeanPO</a></p>
<p><strong>摘要</strong></p>
<p>本文主要探讨了视频大语言模型（Video-LLMs）在优化奖励边际时面临的问题。现有方法如DPO在训练过程中往往导致胜方回应（yw）和败方回应（yl）的概率对数下降，从而可能提升非目标回应的概率。本文系统性地从LLMs重新审视这一现象，并指出在处理视频内容的冗余复杂性时，该问题更加严重。为缓解这一现象，本文提出了无参考的LeanPO方法，该方法将隐式奖励重新定义为响应相对于策略模型的平均概率。LeanPO的关键组件是奖励信任度相关的自我生成偏好数据管道，它通过不断自我反思来精细地注入相关先验知识并持续精炼偏好数据。此外，本文还引入了动态标签平滑策略，以缓解来自各种视频内容中响应的噪声影响，防止模型过度拟合细节。实验表明，LeanPO显著提高了先进Video-LLM的性能，并且与不同容量的基线相比具有最小的额外训练开销。此外，LeanPO为可靠高效的Video-LLM提供了一种简单有效的偏好对齐方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Video-LLMs在优化奖励边际时采用偏好对齐技术，如DPO，但存在概率下降的问题。</li>
<li>在处理视频内容的冗余复杂性时，该问题更为严重。</li>
<li>提出LeanPO方法，通过平均概率重新定义了隐式奖励的概念。</li>
<li>LeanPO利用奖励信任度相关的自我生成偏好数据管道来精细地注入相关先验知识并精炼偏好数据。</li>
<li>动态标签平滑策略用于缓解来自各种视频内容响应的噪声影响。</li>
<li>LeanPO显著提高Video-LLM性能并有效对齐人类信任度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05260">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3594ef21f128de8a5db3d912b7b5040f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f123c11c2212d6ee3668197ee80ae98d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db7161cc2848fa1a1d3a2e70894dca4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LLM-First-Search-Self-Guided-Exploration-of-the-Solution-Space"><a href="#LLM-First-Search-Self-Guided-Exploration-of-the-Solution-Space" class="headerlink" title="LLM-First Search: Self-Guided Exploration of the Solution Space"></a>LLM-First Search: Self-Guided Exploration of the Solution Space</h2><p><strong>Authors:Nathan Herr, Tim Rocktäschel, Roberta Raileanu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable improvements in reasoning and planning through increased test-time compute, often by framing problem-solving as a search process. While methods like Monte Carlo Tree Search (MCTS) have proven effective in some domains, their reliance on fixed exploration hyperparameters limits their adaptability across tasks of varying difficulty, rendering them impractical or expensive in certain settings. In this paper, we propose \textbf{LLM-First Search (LFS)}, a novel \textit{LLM Self-Guided Search} method that removes the need for pre-defined search strategies by empowering the LLM to autonomously control the search process via self-guided exploration. Rather than relying on external heuristics or hardcoded policies, the LLM evaluates whether to pursue the current search path or explore alternative branches based on its internal scoring mechanisms. This enables more flexible and context-sensitive reasoning without requiring manual tuning or task-specific adaptation. We evaluate LFS on Countdown and Sudoku against three classic widely-used search algorithms, Tree-of-Thoughts’ Breadth First Search (ToT-BFS), Best First Search (BestFS), and MCTS, each of which have been used to achieve SotA results on a range of challenging reasoning tasks. We found that LFS (1) performs better on more challenging tasks without additional tuning, (2) is more computationally efficient compared to the other methods, especially when powered by a stronger model, (3) scales better with stronger models, due to its LLM-First design, and (4) scales better with increased compute budget. Our code is publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/NathanHerr/LLM-First-Search%7D%7BLLM-First-Search%7D">https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）通过增加测试时的计算量，在推理和规划方面取得了显著的改进，通常通过将问题解决框架设定为搜索过程。虽然蒙特卡洛树搜索（MCTS）等方法在某些领域已经证明是有效的，但它们对固定探索超参数的依赖限制了它们在不同难度任务中的适应性，因此在某些环境中它们是不切实际的或昂贵的。在本文中，我们提出了\textbf{LLM-First Search（LFS）}，这是一种新型的\textit{LLM自我引导搜索}方法，它通过赋予LLM自主控制搜索过程的能力，从而消除了对预先定义的搜索策略的需求，实现自我引导的探索。LLM不需要依赖外部启发式或硬编码的策略，而是根据其内部评分机制来判断是继续当前搜索路径还是探索替代分支。这实现了更灵活、更依赖于上下文的推理，而无需手动调整或针对特定任务的适应。我们在倒计时和数独上评估了LFS，并与三种经典的广泛使用的搜索算法进行了比较：思维树广度优先搜索（ToT-BFS）、最佳优先搜索（BestFS）和MCTS，这些算法已在各种具有挑战性的推理任务上取得了最新成果。我们发现LFS（1）在更具挑战性的任务上表现更好，无需额外调整，（2）与其他方法相比具有更高的计算效率，尤其是当使用更强大的模型时，（3）由于其LLM-First设计，随着更强模型的增强而更好地扩展，（4）随着计算预算的增加而更好地扩展。我们的代码可在\href{<a target="_blank" rel="noopener" href="https://github.com/NathanHerr/LLM-First-Search%7D%7BLLM-First-Search%7D%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/NathanHerr/LLM-First-Search}{LLM-First-Search}上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05213v1">PDF</a> 9 main pages, 2 figures, 2 tables, 36 appendix pages</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在测试时间的计算增加中展现出令人印象深刻的推理和规划能力，通常通过将问题解决视为搜索过程来实现。本文提出了一种新型的LLM自引导搜索方法——LLM-First Search（LFS），它消除了对预定义搜索策略的需求，通过赋予LLM自主控制搜索过程的能力，实现自我引导的探索。我们在倒计时和数独任务上评估了LFS，并与三种经典的搜索算法进行了比较：树状思维广度优先搜索（ToT-BFS）、最佳优先搜索（BestFS）和蒙特卡洛树搜索（MCTS）。我们发现LFS在更具挑战性的任务上表现更好，计算效率更高，尤其是在使用更强大的模型时，并且由于其LLM优先设计而具有更好的模型扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM通过增加测试时间的计算，提高了推理和规划能力。</li>
<li>LLM-First Search (LFS)是一种新型的LLM自引导搜索方法，无需预定义的搜索策略。</li>
<li>LFS通过LLM的自我引导探索，增强了搜索的灵活性和上下文敏感性。</li>
<li>LFS在倒计时和数独任务上的表现优于其他三种经典的搜索算法。</li>
<li>LFS在更具挑战性的任务上表现更好，并且计算效率更高。</li>
<li>LFS在使用更强大的模型时具有更好的扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b649e9ff8374e127987e754fada7791b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8afbaf563411bca0acae182c903e3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a248bf0fc0b8e996e8d3de8b2c659b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70e4128e7aaad5c8e3aeabc1a53efeb3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FPTQuant-Function-Preserving-Transforms-for-LLM-Quantization"><a href="#FPTQuant-Function-Preserving-Transforms-for-LLM-Quantization" class="headerlink" title="FPTQuant: Function-Preserving Transforms for LLM Quantization"></a>FPTQuant: Function-Preserving Transforms for LLM Quantization</h2><p><strong>Authors:Boris van Breugel, Yelysei Bondarenko, Paul Whatmough, Markus Nagel</strong></p>
<p>Large language models (LLMs) require substantial compute, and thus energy, at inference time. While quantizing weights and activations is effective at improving efficiency, naive quantization of LLMs can significantly degrade performance due to large magnitude outliers. This paper describes FPTQuant, which introduces four novel, lightweight, and expressive function-preserving transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable pre-RoPE transform for queries and keys, (2) a mergeable transform for values, (3) a mergeable scaling transform within the MLP block, and (4) a cheap, dynamic scaling transform. By leveraging the equivariances and independencies inherent to canonical transformer operation, we designed these FPTs to maintain the model’s function while shaping the intermediate activation distributions to be more quantization friendly. FPTQuant requires no custom kernels and adds virtually no overhead during inference. The FPTs are trained both locally to reduce outliers, and end-to-end such that the outputs of the quantized and full-precision models match. FPTQuant enables static INT4 quantization with minimal overhead and shows SOTA speed-up of up to 3.9 times over FP. Empirically, FPTQuant has an excellent accuracy-speed trade-off – it is performing on par or exceeding most prior work and only shows slightly lower accuracy compared to a method that is up to 29% slower. </p>
<blockquote>
<p>大型语言模型（LLM）在推理时需要大量的计算和能源。虽然量化权重和激活值能有效提高效率，但对LLM进行简单的量化会由于较大的幅度异常值而导致性能显著下降。本文介绍了FPTQuant，它引入了四种新颖、轻便、表达性强的保功能变换（FPTs），以促进变压器的量化：（1）用于查询和键的可合并预RoPE变换，（2）用于值的可合并变换，（3）MLP块内的可合并缩放变换，以及（4）一种廉价、动态缩放变换。我们利用规范变换器操作所固有的等变性和独立性来设计这些FPTs，以维持模型的性能，同时使中间激活值分布更利于量化。FPTQuant不需要自定义内核，在推理过程中几乎不会增加开销。FPTs既可以在本地进行训练以减少异常值，也可以端到端进行训练，以使量化模型和全精度模型的输出相匹配。FPTQuant实现了静态INT4量化，具有最小的开销，并显示出相对于浮点数的最高达3.9倍的速度提升。经验上，FPTQuant具有出色的精度-速度权衡——它的性能与大多数先前的工作相当或超过，并且与一种慢至多达29%的方法相比，其精度仅略有下降。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04985v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在推理时需要大量的计算和能源。虽然量化权重和激活可以提高效率，但LLM的量化方法可能导致性能显著下降，特别是面临大型幅度异常值问题。本文介绍的FPTQuant方法引入了四种新颖、轻便且表现性强的函数保持变换（FPT），旨在解决transformer的量化问题：（1）可合并的预RoPE变换用于查询和键；（2）可合并的变换用于值；（3）MLP块内的可合并缩放变换；（4）廉价、动态缩放变换。这些FPT的设计旨在保持模型的性能，同时使中间激活分布更适合量化。FPTQuant无需自定义内核，在推理过程中几乎不会增加开销。FPT既可用于局部训练以减少异常值，也可用于端到端的训练，以使量化模型和全精度模型的输出相匹配。FPTQuant可实现静态INT4量化，具有最小的额外开销，并显示出最高达3.9倍的速度提升。经验表明，FPTQuant在准确性-速度方面的权衡表现出色，性能与大多数先前工作相当或表现更好，仅在少数情况下准确度略有下降相较于速度较慢的方法而言，具有优势的是仅需大约提升模型的运行速度的测试情况下才有相对较低的准确度损失。总体而言，FPTQuant为LLM的量化提供了一种高效且准确的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型推理需要巨大的计算和能源资源。</li>
<li>量化LLM可以提高效率，但可能导致性能下降，特别是面临大型幅度异常值问题。</li>
<li>FPTQuant引入四种函数保持变换（FPT）来改进LLM的量化。这些FPT针对transformer的结构设计而成以更有效地量化处理查询、键、值和MLP块的中间激活分布。</li>
<li>FPTQuant方法无需自定义内核且几乎不增加推理过程中的开销。它通过利用模型的内在等价性和独立性来实现高效量化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-317b3229c2bfa6e0b2152064d09adb9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45b957eb28816347e4a3fa6ba357af98.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Video-Transformers-for-Word-Level-Bangla-Sign-Language-A-Comparative-Analysis-for-Classification-Tasks"><a href="#Fine-Tuning-Video-Transformers-for-Word-Level-Bangla-Sign-Language-A-Comparative-Analysis-for-Classification-Tasks" class="headerlink" title="Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A   Comparative Analysis for Classification Tasks"></a>Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A   Comparative Analysis for Classification Tasks</h2><p><strong>Authors:Jubayer Ahmed Bhuiyan Shawon, Hasan Mahmud, Kamrul Hasan</strong></p>
<p>Sign Language Recognition (SLR) involves the automatic identification and classification of sign gestures from images or video, converting them into text or speech to improve accessibility for the hearing-impaired community. In Bangladesh, Bangla Sign Language (BdSL) serves as the primary mode of communication for many individuals with hearing impairments. This study fine-tunes state-of-the-art video transformer architectures – VideoMAE, ViViT, and TimeSformer – on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset with 60 frequent signs. We standardized the videos to 30 FPS, resulting in 9,307 user trial clips. To evaluate scalability and robustness, the models were also fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401 sign classes. Additionally, we benchmark performance against public datasets, including LSA64 and WLASL. Data augmentation techniques such as random cropping, horizontal flipping, and short-side scaling were applied to improve model robustness. To ensure balanced evaluation across folds during model selection, we employed 10-fold stratified cross-validation on the training set, while signer-independent evaluation was carried out using held-out test data from unseen users U4 and U8. Results show that video transformer models significantly outperform traditional machine learning and deep learning approaches. Performance is influenced by factors such as dataset size, video quality, frame distribution, frame rate, and model architecture. Among the models, the VideoMAE variant (MCG-NJU&#x2F;videomae-base-finetuned-kinetics) achieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60 dataset and 81.04% on the front-facing signs of BdSLW401 – demonstrating strong potential for scalable and accurate BdSL recognition. </p>
<blockquote>
<p>手势语言识别（SLR）涉及从图像或视频中自动识别和分类手势，将它们转换为文本或语音，以提高听力障碍者的可访问性。在孟加拉国，孟加拉手语（BdSL）是许多听力障碍者主要的交流方式。本研究对最前沿的视频转换器架构——VideoMAE、ViViT和TimeSformer进行了微调，应用于BdSLW60（arXiv:2402.08635）数据集，这是一个包含60种常见手势的小规模BdSL数据集。我们将视频标准化至30FPS，生成了9307个用户试验片段。为了评估模型的扩展性和稳健性，我们还在大规模的BdSLW401数据集（arXiv:2503.02360）上对模型进行了微调，该数据集包含401个手势类别。此外，我们还与LSA64和WLASL等公共数据集进行了基准测试。为了提高模型的稳健性，我们应用了数据增强技术，如随机裁剪、水平翻转和短边缩放。在模型选择过程中，为了确保跨折叠的评估平衡，我们对训练集采用了10折分层交叉验证，而对来自未见用户的U4和U8的保留测试数据进行了签名人独立评估。结果表明，视频转换器模型显著优于传统的机器学习和深度学习方法。性能受到数据集大小、视频质量、帧分布、帧率和模型架构等因素的影响。在帧速率校正的BdSLW60数据集上，VideoMAE变体（MCG-NJU&#x2F;videomae-base-finetuned-kinetics）取得了最高准确率，达到95.5%，在面向正面的BdSLW401数据集上的准确率为81.04%，显示出强大的可扩展和准确识别BdSL的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04367v1">PDF</a> 16 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于视频图像的手语识别技术（SLR）在孟加拉国的应用。研究者针对孟加拉手语（BdSL）的数据集进行了先进的视频模型架构的调整和优化。模型性能通过在不同的数据集上进行评估来体现，结果表明视频模型明显优于传统的机器学习和深度学习模型。其中，VideoMAE模型在修正帧率的BdSLW60数据集上准确率高达95.5%，在面向前方的BdSLW401数据集上的准确率为81.04%，展现出在可伸缩性和准确性方面的强大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>手语识别（SLR）是将图像或视频中的手势自动识别和分类，并转换为文字或语音，以提高听障人士的访问性。</li>
<li>在孟加拉国，孟加拉手语（BdSL）是听障人士的主要沟通方式。</li>
<li>研究者使用先进的视频模型架构（如VideoMAE, ViViT和TimeSformer）对BdSL数据集进行了微调。</li>
<li>数据增强技术如随机裁剪、水平翻转和短边缩放被用来提高模型的稳健性。</li>
<li>视频模型在性能上显著优于传统的机器学习和深度学习模型。</li>
<li>VideoMAE模型在修正帧率的BdSLW60数据集上的准确率最高，达到95.5%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-13a733a02451db909724c8ec8e37c34b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73c84ce91fc91a9da14e45187600ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99b8af36d6119db8f7bc4b4c62350e5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7817529756a96bec3b9727b7e617b48c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48d6948d1593df7ca2a33ebea527d0ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7911a06acf614aef6cd7c1a9137b682.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models"><a href="#Model-Internal-Sleuthing-Finding-Lexical-Identity-and-Inflectional-Morphology-in-Modern-Language-Models" class="headerlink" title="Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models"></a>Model Internal Sleuthing: Finding Lexical Identity and Inflectional   Morphology in Modern Language Models</h2><p><strong>Authors:Michael Li, Nishant Subramani</strong></p>
<p>Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand today’s language models, we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) represent lexical identity and inflectional morphology. We train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features. We discover that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout the layers. Further analysis reveals that these models encode inflectional morphology through generalizable abstractions, but rely predominantly on memorization to encode lexical identity. Remarkably, these patterns emerge across all 16 models we test, despite differences in architecture, size, and training regime (including pretrained and instruction-tuned variants). This consistency suggests that, despite substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties could be fundamental for next token prediction and are learned early during pretraining. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing">https://github.com/ml5885/model_internal_sleuthing</a> </p>
<blockquote>
<p>现代自然语言处理领域主要由大型基于Transformer的语言模型主导，然而我们对它们如何编码语言信息的理解仍基于早期模型，如BERT和GPT-2。为了更好地理解当今的语言模型，我们研究了经典架构（BERT、DeBERTa、GPT-2）和当代大型语言模型（Pythia、OLMo-2、Gemma-2、Qwen2.5、Llama-3.1）如何表示词汇身份和屈折形态。我们对分层激活训练了线性和非线性分类器，以预测词素和屈折特征。我们发现，模型在早期层次中以线性方式集中词汇信息，在后期层次中则越来越以非线性方式集中信息，同时保持屈折信息在整个层次中均匀可访问且可线性分离。进一步的分析表明，这些模型通过可推广的抽象来编码屈折形态，但主要依赖于记忆来编码词汇身份。值得注意的是，在我们测试的16个模型中，尽管它们在架构、规模和训练方案（包括预训练和指令调整变体）上存在差异，但这些模式仍然出现。这种一致性表明，尽管大型语言模型技术取得了重大进展，但Transformer模型以相似的方式组织语言信息，这表明这些属性对于下一个令牌预测至关重要，并且在预训练早期就已经学习。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/ml5885/model_internal_sleuthing%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ml5885/model_internal_sleuthing上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02132v2">PDF</a> </p>
<p><strong>Summary</strong>：近期研究表明，无论模型架构、大小或训练机制如何，大型基于transformer的自然语言处理模型在早期层级中主要集中处理词汇信息，而后层更多地涉及非线性的信息处理，并保持了词汇信息的一致性。虽然它们在结构和功能上有所发展，但它们都倾向于通过可概括的抽象来编码形态变化信息，但主要依赖记忆来编码词汇身份。这些发现揭示了现代NLP模型在处理语言信息时的核心机制。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型基于transformer的语言模型在编码语言信息时表现出一致性，无论模型架构、大小或训练机制如何。</li>
<li>这些模型在早期层级集中处理词汇信息，并在后续层级中涉及更复杂的非线性信息处理。</li>
<li>词汇信息在模型中保持一致性，而形态变化信息则通过可概括的抽象进行编码。</li>
<li>模型主要依赖记忆来编码词汇身份。</li>
<li>训练分类器可以在不同层级预测单词词元和形态变化特征。</li>
<li>模型组织和处理语言信息的方式可能与下一个单词的预测有关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02132">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fc386a4af271b55dee38fd1cbd148a23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49d85bac18845d71791b208ec53ef6d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faf376edceef05628fb07c4a9a080815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-743165dd7584cd28f3d8e3bd51542824.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af21719a18d431a3191a862f1cdf446b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c149157610ffd0baddfdafd79edbf674.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Protocol-Unveiling-Attack-Vectors-in-the-Model-Context-Protocol-Ecosystem"><a href="#Beyond-the-Protocol-Unveiling-Attack-Vectors-in-the-Model-Context-Protocol-Ecosystem" class="headerlink" title="Beyond the Protocol: Unveiling Attack Vectors in the Model Context   Protocol Ecosystem"></a>Beyond the Protocol: Unveiling Attack Vectors in the Model Context   Protocol Ecosystem</h2><p><strong>Authors:Hao Song, Yiming Shen, Wenxuan Luo, Leixin Guo, Ting Chen, Jiashui Wang, Beibei Li, Xiaosong Zhang, Jiachi Chen</strong></p>
<p>The Model Context Protocol (MCP) is an emerging standard designed to enable seamless interaction between Large Language Model (LLM) applications and external tools or resources. Within a short period, thousands of MCP services have already been developed and deployed. However, the client-server integration architecture inherent in MCP may expand the attack surface against LLM Agent systems, introducing new vulnerabilities that allow attackers to exploit by designing malicious MCP servers. In this paper, we present the first systematic study of attack vectors targeting the MCP ecosystem. Our analysis identifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources. To evaluate the feasibility of these attacks, we conduct experiments following the typical steps of launching an attack through malicious MCP servers: upload-download-attack. Specifically, we first construct malicious MCP servers and successfully upload them to three widely used MCP aggregation platforms. The results indicate that current audit mechanisms are insufficient to identify and prevent the proposed attack methods. Next, through a user study and interview with 20 participants, we demonstrate that users struggle to identify malicious MCP servers and often unknowingly install them from aggregator platforms. Finally, we demonstrate that these attacks can trigger harmful behaviors within the user’s local environment-such as accessing private files or controlling devices to transfer digital assets-by deploying a proof-of-concept (PoC) framework against five leading LLMs. Additionally, based on interview results, we discuss four key challenges faced by the current security ecosystem surrounding MCP servers. These findings underscore the urgent need for robust security mechanisms to defend against malicious MCP servers. </p>
<blockquote>
<p>模型上下文协议（MCP）是一种新兴标准，旨在实现大型语言模型（LLM）应用程序与外部工具或资源之间的无缝交互。在很短的时间内，已经开发并部署了成千上万个MCP服务。然而，MCP所固有的客户端-服务器集成架构可能会扩大针对LLM代理系统的攻击面，引入新的漏洞，攻击者可利用这些漏洞通过设计恶意的MCP服务器进行攻击。在本文中，我们对针对MCP生态系统的攻击向量进行了首次系统研究。我们的分析确定了四类攻击，即工具中毒攻击、傀儡攻击、地毯式攻击和通过恶意外部资源的利用攻击。为了评估这些攻击的可行性，我们通过恶意MCP服务器发动攻击的典型步骤进行了实验：上传-下载-攻击。具体来说，我们首先构建了恶意MCP服务器，并成功将其上传到三个广泛使用的MCP聚合平台。结果表明，当前的审计机制不足以识别和预防所提出的攻击方法。接下来，通过对20名参与者的用户研究和采访，我们证明了用户很难识别恶意MCP服务器，并且经常无意中从聚合平台安装它们。最后，我们通过针对五款领先的大型语言模型部署概念验证（PoC）框架，证明这些攻击可以触发用户本地环境中的有害行为，例如访问私人文件或控制设备转移数字资产。此外，基于访谈结果，我们讨论了当前MCP服务器安全生态系统所面临的四大挑战。这些发现强调了亟需构建稳健的安全机制来防范恶意MCP服务器。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02040v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>模型上下文协议（MCP）是一种新兴标准，旨在实现大型语言模型（LLM）应用程序与外部工具或资源之间的无缝交互。然而，MCP所固有的客户端-服务器集成架构可能会扩大针对LLM代理系统的攻击面，引入允许攻击者通过设计恶意MCP服务器进行利用的新漏洞。本文对针对MCP生态系统的攻击向量进行了系统的研究分析，识别出四种攻击类别。为了评估这些攻击的可行性，我们通过恶意MCP服务器发动攻击的典型步骤进行实验：上传-下载-攻击。我们发现当前审计机制不足以识别并预防所提出的攻击方法。此外，通过对20名参与者的用户研究和访谈，我们证明了用户难以识别恶意MCP服务器，并经常无意中从聚合平台安装它们。最后，通过一个针对五大领先LLM的概念验证框架，我们证明了这些攻击可以触发用户本地环境中的有害行为，如访问私人文件或控制设备转移数字资产。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>模型上下文协议（MCP）是一种旨在促进LLM与外部工具无缝交互的新兴标准，但存在安全漏洞。</li>
<li>识别了四种针对MCP的攻击类别：工具中毒攻击、木偶攻击、拉绳攻击和通过恶意外部资源的利用。</li>
<li>通过实验证明，当前审计机制不足以识别和预防提出的攻击方法。</li>
<li>用户难以识别恶意MCP服务器，经常从聚合平台无意中安装它们。</li>
<li>攻击可以触发用户本地环境中的有害行为，如访问私人文件或控制设备转移数字资产。</li>
<li>目前MCp服务器的安全生态系统面临四大挑战。</li>
<li>急需建立稳健的安全机制来防范恶意MCP服务器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-762a0d1ed36d64ac735ce45311f56e55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94bdb429b9683666a2370fbb64fe81d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09880901e0b719188ce71e9e82ec1c48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2b80be259e1e76fc4ed6fdcc9a0610c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e38d289fc371205e54e9a9c5a0a6b719.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Analysis-of-LLM-Bias-Chinese-Propaganda-Anti-US-Sentiment-in-DeepSeek-R1-vs-ChatGPT-o3-mini-high"><a href="#Analysis-of-LLM-Bias-Chinese-Propaganda-Anti-US-Sentiment-in-DeepSeek-R1-vs-ChatGPT-o3-mini-high" class="headerlink" title="Analysis of LLM Bias (Chinese Propaganda &amp; Anti-US Sentiment) in   DeepSeek-R1 vs. ChatGPT o3-mini-high"></a>Analysis of LLM Bias (Chinese Propaganda &amp; Anti-US Sentiment) in   DeepSeek-R1 vs. ChatGPT o3-mini-high</h2><p><strong>Authors:PeiHsuan Huang, ZihWei Lin, Simon Imbot, WenCheng Fu, Ethan Tu</strong></p>
<p>Large language models (LLMs) increasingly shape public understanding and civic decisions, yet their ideological neutrality is a growing concern. While existing research has explored various forms of LLM bias, a direct, cross-lingual comparison of models with differing geopolitical alignments-specifically a PRC-system model versus a non-PRC counterpart-has been lacking. This study addresses this gap by systematically evaluating DeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for Chinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus of 1,200 de-contextualized, reasoning-oriented questions derived from Chinese-language news, presented in Simplified Chinese, Traditional Chinese, and English. Answers from both models (7,200 total) were assessed using a hybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human annotation. Our findings reveal significant model-level and language-dependent biases. DeepSeek-R1 consistently exhibited substantially higher proportions of both propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which remained largely free of anti-U.S. sentiment and showed lower propaganda levels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias rates; these diminished in Traditional Chinese and were nearly absent in English. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to Traditional Chinese queries and amplified existing PRC-aligned terms in its Chinese answers, demonstrating an “invisible loudspeaker” effect. Furthermore, such biases were not confined to overtly political topics but also permeated cultural and lifestyle content, particularly in DeepSeek-R1. </p>
<blockquote>
<p>大型语言模型（LLM）越来越影响公众理解和公民决策，但它们的意识形态中立性却令人日益担忧。尽管现有研究已经探索了LLM偏见的各种形式，但针对具有不同地缘政治对齐方式的模型进行直接、跨语言的比较——特别是PRC系统模型与非PRC模型之间的比较——仍然缺乏。本研究通过系统地评估DeepSeek-R1（PRC对齐）与ChatGPT o3-mini-high（非PRC）的中文国家宣传和反美情绪，来弥补这一空白。我们开发了一个由1200个去语境化、以推理为导向的问题组成的新型语料库，这些问题来自简体中文、繁体中文和英文的中文语言新闻。对两个模型给出的答案（共7200个）进行了评估，采用了一种结合基于规则的GPT-4o评分与人类注释的混合评估管道。我们的研究结果揭示了模型级别和依赖于语言的偏见。与ChatGPT o3-mini-high相比，DeepSeek-R1始终表现出更高比例的宣传和反美偏见，而ChatGPT o3-mini-high则基本不存在反美情绪，宣传水平也较低。对于DeepSeek-R1来说，简体中文查询引发的偏见率最高；在繁体中文中这些偏见有所减少，英文中则几乎不存在。值得注意的是，DeepSeek-R1有时会用简体中文回答繁体中文查询，并在其中文答案中放大已有的PRC对齐术语，表现出一种“隐形扬声器”效应。此外，这种偏见不仅限于政治话题，也渗透到文化和生活方式的内容中，特别是在DeepSeek-R1中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在塑造公众理解和公民决策方面扮演着越来越重要的角色，但其意识形态中立性引发关注。现有研究已探索了LLM的偏见形式，但缺乏对不同地缘政治立场模型（特别是PRC系统模型与非PRC模型）的直接跨语言比较。本研究通过系统评估DeepSeek-R1（PRC对齐）与ChatGPT o3-mini-high（非PRC）的中文国家宣传与反美情绪，填补这一空白。研究发现模型级别和语言依赖的偏见。DeepSeek-R1较ChatGPT o3-mini-high表现出更高的宣传与反美偏见，后者基本无反美情绪且宣传程度较低。DeepSeek-R1在回答简体中文查询时表现出最高的偏见率，繁体中文查询中偏见率降低，英文回答中偏见几乎不存在。此外，DeepSeek-R1在回答繁体中文查询时会以简体中文回应，并在其中文答案中放大已有PRC立场术语，表现出“隐形扬声器”效应。这种偏见不仅存在于政治话题中，也渗透到文化与生活方式内容中，尤其在DeepSeek-R1中更为明显。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在公众理解和公民决策中扮演重要角色，引发对其意识形态中立性的关注。</li>
<li>研究对比了DeepSeek-R1（PRC对齐）与ChatGPT o3-mini-high（非PRC）的语言模型。</li>
<li>DeepSeek-R1较ChatGPT o3-mini-high存在更高的宣传与反美偏见。</li>
<li>模型偏见存在语言依赖性，DeepSeek-R1在简体中文查询中表现出最高偏见率，而在英文中几乎无偏见。</li>
<li>DeepSeek-R1在回应时存在语言转换现象，并会放大已有PRC立场术语。</li>
<li>模型偏见不仅存在于政治话题，也渗透到文化与生活方式内容中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1026a8bd34fedaa0cd00e6ca0112d40c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31593c9a93ac5fd04bd000716e4cef24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-441032e73d1a810be8de6d1d54759ff4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a321e6cb798ba4382a4ccead303cc739.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Compress-Gather-and-Recompute-REFORMing-Long-Context-Processing-in-Transformers"><a href="#Compress-Gather-and-Recompute-REFORMing-Long-Context-Processing-in-Transformers" class="headerlink" title="Compress, Gather, and Recompute: REFORMing Long-Context Processing in   Transformers"></a>Compress, Gather, and Recompute: REFORMing Long-Context Processing in   Transformers</h2><p><strong>Authors:Woomin Song, Sai Muralidhar Jayanthi, Srikanth Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin, Aram Galstyan, Shubham Katiyar, Sravan Babu Bodapati</strong></p>
<p>As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model’s pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 50% and 27% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance. </p>
<blockquote>
<p>随着大型语言模型在真实世界应用中的普及度不断提高，处理极长的上下文内容——通常超出模型的预训练上下文限制——已经成为一项关键挑战。虽然现有的高效长上下文处理方法显示出潜力，但基于循环压缩的方法在信息保留方面遇到困难，而随机访问方法则需要大量内存资源。我们引入了REFORM，这是一种新的推理框架，通过两阶段方法有效地处理长上下文。首先，它增量处理输入块，同时维护压缩的KV缓存，构建跨层上下文嵌入，并利用提前退出策略提高效率。其次，它通过相似度匹配来识别和收集关键令牌，并选择性重新计算KV缓存。与基线相比，REFORM在RULER和BABILong上的性能分别提高了50%和27%，上下文长度为1M。它在Infinite-Bench和MM-NIAH上也表现优于基线，证明了在不同任务和领域的灵活性。此外，REFORM将推理时间减少了30%，峰值内存使用率降低了5%，实现了效率和性能的双重提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01215v1">PDF</a> </p>
<p><strong>Summary</strong><br>长语境处理在大型语言模型中的应用中逐渐成为一大挑战。本文提出了REFORM这一新颖推理框架，它通过两个阶段处理长语境：一、通过增量处理输入片段、维持压缩KV缓存、构建跨层语境嵌入并采用早期退出策略提高效率；二、通过相似性匹配识别和收集关键令牌，并选择性重新计算KV缓存。REFORM在不同任务和领域都展现出灵活性，同时提高性能和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在真实应用中面临处理长语境的挑战。</li>
<li>现有方法如压缩和随机访问在处理长语境时存在信息保留和内存需求问题。</li>
<li>REFORM框架采用两阶段方法处理长语境，结合增量处理、KV缓存、跨层语境嵌入和早期退出策略提高效率。</li>
<li>REFORM通过相似性匹配识别和收集关键令牌，选择性重新计算KV缓存。</li>
<li>REFORM在多个基准测试中实现了显著的性能提升，并展示了在不同任务和领域的灵活性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01215">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d335fc555f8d77d6af56ab512ce8361c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027d85810321b529dc70e4bd1d9724a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b67cc8beaacc6d0391fe70ad135bf7a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ba5babd67b9fc2102e36b6e04d1422.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72c28fd6113ee3496b72e722c07c3fae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ca9fd4ae2672fad48b7082853caeb84.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TRiMM-Transformer-Based-Rich-Motion-Matching-for-Real-Time-multi-modal-Interaction-in-Digital-Humans"><a href="#TRiMM-Transformer-Based-Rich-Motion-Matching-for-Real-Time-multi-modal-Interaction-in-Digital-Humans" class="headerlink" title="TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans"></a>TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal   Interaction in Digital Humans</h2><p><strong>Authors:Yueqian Guo, Tianzhao Li, Xin Lyu, Jiehaolin Chen, Zhaohan Wang, Sirui Xiao, Yurun Chen, Yezi He, Helin Li, Fan Zhang</strong></p>
<p>Large Language Model (LLM)-driven digital humans have sparked a series of recent studies on co-speech gesture generation systems. However, existing approaches struggle with real-time synthesis and long-text comprehension. This paper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel multi-modal framework for real-time 3D gesture generation. Our method incorporates three modules: 1) a cross-modal attention mechanism to achieve precise temporal alignment between speech and gestures; 2) a long-context autoregressive model with a sliding window mechanism for effective sequence modeling; 3) a large-scale gesture matching system that constructs an atomic action library and enables real-time retrieval. Additionally, we develop a lightweight pipeline implemented in the Unreal Engine for experimentation. Our approach achieves real-time inference at 120 fps and maintains a per-sentence latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive subjective and objective evaluations on the ZEGGS, and BEAT datasets demonstrate that our model outperforms current state-of-the-art methods. TRiMM enhances the speed of co-speech gesture generation while ensuring gesture quality, enabling LLM-driven digital humans to respond to speech in real time and synthesize corresponding gestures. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching">https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching</a> </p>
<blockquote>
<p>基于大型语言模型（LLM）的数字人引发了一系列关于协同语音手势生成系统的研究。然而，现有方法在实时合成和长文本理解方面遇到了困难。本文介绍了基于Transformer的丰富动作匹配（TRiMM），这是一种用于实时3D手势生成的新型多模态框架。我们的方法结合了三个模块：1）跨模态注意力机制，实现语音和手势之间的精确时间对齐；2）具有滑动窗口机制的长上下文自回归模型，用于有效的序列建模；3）大规模手势匹配系统，构建原子动作库，实现实时检索。此外，我们在Unreal Engine中开发了一个轻量级的实验管道。我们的方法实现实时推理速度为每秒处理高达每秒生成动作的帧数为高达每秒处理帧数可以达到惊人的每小时百万帧的水平120帧（每秒的帧速率可达达到FPS）。并且维持延迟保持在低级别GPU上每句延迟仅为0.15秒（Geforce RTX 3060）。在ZEGGS和BEAT数据集上的主观和客观评估表明，我们的模型优于当前的最先进技术方法。TRiMM提高了协同语音手势生成的速度，同时确保手势质量，使LLM驱动的虚拟数字人能够实时响应语音并合成相应的手势。我们的代码可通过<a target="_blank" rel="noopener" href="https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01077v1">PDF</a> 24 pages,12 figures</p>
<p><strong>Summary</strong><br>基于Transformer的丰富动作匹配（TRiMM）是一种新型的多模态框架，用于实时3D手势生成。该方法结合了跨模态注意力机制、长上下文自回归模型和大规模手势匹配系统，实现了精确的时间对齐和高质量的手势生成。该模型在ZEGGS和BEAT数据集上的评估结果优于当前的最先进方法。TRiMM加快了协同语音手势生成的速度，使得LLM驱动的数字人能够实时响应语音并合成相应的手势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TRiMM是一种用于实时3D手势生成的多模态框架。</li>
<li>该方法结合了跨模态注意力机制、长上下文自回归模型和大规模手势匹配系统。</li>
<li>TRiMM实现了精确的时间对齐，使手势与语音同步。</li>
<li>该模型在ZEGGS和BEAT数据集上的表现优于当前的最先进方法。</li>
<li>TRiMM加快了协同语音手势生成的实时推理速度，达到每秒处理帧数高达120帧。</li>
<li>TRiMM确保了手势质量，使得LLM驱动的数字人能实时响应语音并合成相应手势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-82afd41e3d4bd1b369159061c3d5a52b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dc7d1337bb4d522e2d57c1ce947daa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28f550c263171b52ab5b84df1ea7d54.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="How-do-Transformer-Embeddings-Represent-Compositions-A-Functional-Analysis"><a href="#How-do-Transformer-Embeddings-Represent-Compositions-A-Functional-Analysis" class="headerlink" title="How do Transformer Embeddings Represent Compositions? A Functional   Analysis"></a>How do Transformer Embeddings Represent Compositions? A Functional   Analysis</h2><p><strong>Authors:Aishik Nagar, Ishaan Singh Rawal, Mansi Dhanania, Cheston Tan</strong></p>
<p>Compositionality is a key aspect of human intelligence, essential for reasoning and generalization. While transformer-based models have become the de facto standard for many language modeling tasks, little is known about how they represent compound words, and whether these representations are compositional. In this study, we test compositionality in Mistral, OpenAI Large, and Google embedding models, and compare them with BERT. First, we evaluate compositionality in the representations by examining six diverse models of compositionality (addition, multiplication, dilation, regression, etc.). We find that ridge regression, albeit linear, best accounts for compositionality. Surprisingly, we find that the classic vector addition model performs almost as well as any other model. Next, we verify that most embedding models are highly compositional, while BERT shows much poorer compositionality. We verify and visualize our findings with a synthetic dataset consisting of fully transparent adjective-noun compositions. Overall, we present a thorough investigation of compositionality. </p>
<blockquote>
<p>组合性是人工智能的一个重要方面，对于推理和泛化至关重要。虽然基于转换器的模型已成为许多语言建模任务的默认标准，但对于它们如何表示复合词以及这些表示是否组合性，人们的了解仍然有限。在这项研究中，我们在Mistral、OpenAI Large和Google嵌入模型中测试了组合性，并将其与BERT进行了比较。首先，我们通过检查六种不同的组合性模型（加法、乘法、膨胀、回归等）来评估表示中的组合性。我们发现岭回归虽然线性，但最能解释组合性。令人惊讶的是，我们发现经典的向量加法模型的性能几乎与其他任何模型一样好。接下来，我们验证大多数嵌入模型具有很高的组合性，而BERT的组合性则较差。我们通过包含完全透明的形容词-名词组合的合成数据集来验证并可视化我们的发现。总的来说，我们对组合性进行了彻底的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00914v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了模型在表现组合词方面的能力，即所谓的“组合性”。通过测试Mistral、OpenAI Large、Google嵌入模型以及BERT模型的组合性表现，发现大多数嵌入模型具有良好的组合性，而BERT的表现较差。研究中使用了六种不同的组合性模型来评估模型的表现，发现岭回归虽然在组合性方面表现优秀，但经典的向量加法模型同样表现良好。通过合成数据集验证了形容词和名词组合结构的可视化结果。总之，本研究对组合性进行了全面调查。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究探讨了模型在表现组合词方面的能力，即组合性。</li>
<li>测试了Mistral、OpenAI Large、Google嵌入模型和BERT模型的组合性表现。</li>
<li>通过六种不同的组合性模型评估模型的表现，发现岭回归和向量加法模型表现较好。</li>
<li>大多数嵌入模型具有良好的组合性，而BERT表现较差。</li>
<li>通过合成数据集验证了形容词和名词组合结构的可视化结果。</li>
<li>研究强调了组合性是语言建模任务的关键方面，对于推理和泛化很重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00914">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8fb409fe0d20f9f536623271e4ffdb81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b93549e4b02c2c33d5f15de07dd7699a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38b5a2f893a8678ca9bfa0be698d9444.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daf1cc1c43288206ea0eab0787625c36.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Probing-the-Geometry-of-Truth-Consistency-and-Generalization-of-Truth-Directions-in-LLMs-Across-Logical-Transformations-and-Question-Answering-Tasks"><a href="#Probing-the-Geometry-of-Truth-Consistency-and-Generalization-of-Truth-Directions-in-LLMs-Across-Logical-Transformations-and-Question-Answering-Tasks" class="headerlink" title="Probing the Geometry of Truth: Consistency and Generalization of Truth   Directions in LLMs Across Logical Transformations and Question Answering   Tasks"></a>Probing the Geometry of Truth: Consistency and Generalization of Truth   Directions in LLMs Across Logical Transformations and Question Answering   Tasks</h2><p><strong>Authors:Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Zhengwen Feng, Hao Peng, Jianwei Yin</strong></p>
<p>Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the “truth direction”, which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts. Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation. Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources. Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs. These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs. Our code is public at <a target="_blank" rel="noopener" href="https://github.com/colored-dye/truthfulness_probe_generalization">https://github.com/colored-dye/truthfulness_probe_generalization</a> </p>
<blockquote>
<p>大型语言模型（LLM）是在包含大量世界知识的广泛数据集上进行训练的。然而，它们的输出通常包括自信的不准确之处。早期的研究表明，LLM将真实性编码为一个独特的线性特征，称为“真实方向”，可以可靠地分类真实性。我们针对关于真实方向的几个开放性问题进行探讨：（i）LLM是否普遍表现出一致的真实方向；（ii）是否需要复杂的探测技术来确定真实方向；（iii）真实方向如何在不同的语境中通用化。我们的研究发现，并非所有LLM都表现出一致的真实方向，在更强大的模型中观察到更强烈的表示，特别是在逻辑否定的背景下。此外，我们证明，在陈述性原子语句上训练的真实性探测器可以有效地推广到逻辑转换、问答任务、上下文学习和外部知识源。最后，我们探索了真实性探测器在选择性问答中的实际应用，说明了它们在提高用户对LLM输出的信任方面的潜力。这些结果推动了我们对于真实方向的理解，并为LLM信念的内部表示提供了新的见解。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/colored-dye/truthfulness_probe_generalization">https://github.com/colored-dye/truthfulness_probe_generalization</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00823v1">PDF</a> 19 pages, 16 figures; accepted to Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）虽基于广泛数据集培训，包含丰富世界知识，但其输出常含自信错误。先前研究提出LLM内存在一个称为“真实方向”的线性特征，可可靠地分类真实性。本文探讨关于真实方向的几个开放问题：LLM是否普遍展现一致的真实方向；是否需高级探测技术来识别真实方向；以及真实方向如何在不同语境中普及。研究发现，并非所有LLM都展现一致真实方向，更强大的模型在逻辑否定语境下表现更明显。此外，我们证明在陈述原子语句上训练的真实性探测器可有效推广到逻辑转换、问答任务、上下文学习和外部知识源。最后，我们探索了真实性探测器在选择性问答中的实际应用，展示了提高用户对LLM输出的信任度的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs常输出自信但可能不准确的答案。</li>
<li>“真实方向”是LLMs中用于分类真实性的线性特征。</li>
<li>不同LLM可能展现不同的真实方向一致性。</li>
<li>更先进的LLM在逻辑否定方面表现出更强大的真实方向。</li>
<li>真实性探测器可在多种语境和任务中有效推广。</li>
<li>真实性探测器有助于提高用户对LLM输出的信任度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3445b1f2eab2828e52ca74c9623ca2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-601708bfa67fcc91b6fe6773c8b1fa43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-132081f5ff35cc02b7378fcac919b469.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa8d034da06041f563ff8b56880ab8c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a70b737cb2affa3870496d2b9edfe9d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration"><a href="#MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration" class="headerlink" title="MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration"></a>MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration</h2><p><strong>Authors:Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung</strong></p>
<p>In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance. </p>
<blockquote>
<p>近年来，多模态大型语言模型（MLLMs）取得了显著进展，但仍在多模态推理方面面临固有的挑战，这需要多层次（例如感知、推理）和多粒度（例如多步推理链）的高级推断。以往关于估计模型信心的工作往往集中在训练和校准的整体响应上，但未能评估每个推理步骤的信心，导致出现不希望看到的幻觉累积。在这项工作中，我们提出了MMBoundary，这是一个新型框架，通过推理步骤的信心校准，提高了MLLMs的知识边界意识。为实现这一目标，我们提议结合补充文本和跨模态自我奖励信号来估计MLLM推理过程中每一步的信心。除了使用自我奖励的信心估计信号对MLLM进行有监督的微调，以进行初始信心表达的预热，我们还引入了一个强化学习阶段，使用多个奖励函数来进一步对齐模型知识并校准每一步的信心，增强推理链的自我校正能力。实证结果表明，MMBoundary在跨不同领域的数据集和指标上显著优于现有方法，多模态信心校准误差平均减少了7.5%，任务性能提高了高达8.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23224v2">PDF</a> 18 pages, ACL 2025</p>
<p><strong>Summary</strong></p>
<p>MLLM在多模态推理方面存在挑战，需要多级别和多粒度的推理能力。现有模型信心评估方法主要关注整体响应的培训和校准，而忽视了对每个推理步骤的信心评估，导致出现不必要的幻觉累积。本研究提出MMBoundary框架，通过推理步骤的信心校准提高MLLM的知识边界意识。结合补充文本和跨模态自奖励信号来估计MLLM推理过程中每一步的信心，并引入强化学习阶段进一步对齐模型知识和校准信心，提高推理链的自我校正能力。实证研究结果显示，MMBoundary在多个领域数据集和指标上的表现优于现有方法，平均减少7.5%的多模态信心校准错误，任务性能提高8.3%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在多模态推理方面存在挑战，需要进一步提高多级别和多粒度的推理能力。</li>
<li>现有模型信心评估方法主要关注整体响应的培训和校准，缺乏对每个推理步骤的信心评估。</li>
<li>MMBoundary框架通过推理步骤的信心校准提高MLLM的知识边界意识。</li>
<li>MMBoundary结合补充文本和跨模态自奖励信号来估计MLLM推理过程中每一步的信心。</li>
<li>MMBoundary引入强化学习阶段，进一步对齐模型知识并校准信心。</li>
<li>MMBoundary能提高推理链的自我校正能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4e84b70fafdec136fbf20f34fc34b8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98fa6945fb647e0c77a6f1f4b4455683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724e70cad27d2693c6cc005ac8896228.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling"><a href="#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling" class="headerlink" title="Curse of High Dimensionality Issue in Transformer for Long-context   Modeling"></a>Curse of High Dimensionality Issue in Transformer for Long-context   Modeling</h2><p><strong>Authors:Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</strong></p>
<p>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at <a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>. </p>
<blockquote>
<p>基于Transformer的大型语言模型（LLM）通过自注意力机制捕捉长程依赖关系，从而在自然语言处理任务中表现出色。然而，由于冗余的注意力计算，长上下文建模面临重大的计算效率低下问题：虽然注意力权重通常是稀疏的，但所有令牌都消耗着平等的计算资源。在本文中，我们将传统的概率序列建模重新定义为“监督学习任务”，这能够区分相关和不相关的令牌，并提供对冗余的更清晰理解。基于这种重新定义，我们从理论上分析了注意力稀疏性，揭示只有少数令牌对预测产生了重大贡献。在此基础上，我们将注意力优化制定为线性编码问题，并提出“分组编码策略”，从理论上展示了其提高对抗随机噪声的稳健性和提高学习效率的能力。受此启发，我们提出了“动态组注意力”（DGA），它利用分组编码来通过聚合不太重要的令牌在注意力计算中明确减少冗余。经验结果表明，我们的DGA在保持竞争力性能的同时，显著降低了计算成本。代码可用在<a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention%E3%80%82">https://github.com/bolixinyu/DynamicGroupAttention。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22107v2">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于Transformer的大型语言模型（LLM）在自然语言处理任务中的优异表现，尤其是通过自注意力机制捕捉长距离依赖关系的能力。然而，长文本建模存在计算效率问题，主要是由于注意力计算的冗余性。对此，文章通过重新构建概率序列模型为监督学习任务，分离重要与非重要信息以识别冗余，对注意力稀疏性进行了理论分析。文章提出一种基于线性编码的注意力优化策略，并据此提出动态分组注意力（DGA）机制，通过聚合次要信息进行更明确的计算以减少冗余性。经验表明，该机制在降低计算成本的同时保持了竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMs 擅长捕捉长距离依赖关系。</li>
<li>长文本建模面临计算效率问题，主要由于冗余的注意力计算。</li>
<li>通过将概率序列模型重新构建为监督学习任务，可分离重要与非重要信息，更好地理解冗余。</li>
<li>对注意力稀疏性的理论分析显示，只有少数标记对预测有重要贡献。</li>
<li>线性编码策略的提出是为了优化注意力机制。在此基础上提出的动态分组注意力（DGA）可以减少冗余性。</li>
<li>动态分组注意力机制在降低计算成本的同时保持了良好的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67b46fe3fb47c0d865dac23ea0cfcce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eca6ec960b889a2e81e3ec88cc965ea2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96d12c51275a45c5dadcb236c0b4f035.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-08  Truly Self-Improving Agents Require Intrinsic Metacognitive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-05d7ca8eb66922610ad3cf28a702c0b5.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-08  VideoMathQA Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
