<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  VideoMathQA Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-05d7ca8eb66922610ad3cf28a702c0b5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-08-æ›´æ–°"><a href="#2025-06-08-æ›´æ–°" class="headerlink" title="2025-06-08 æ›´æ–°"></a>2025-06-08 æ›´æ–°</h1><h2 id="VideoMathQA-Benchmarking-Mathematical-Reasoning-via-Multimodal-Understanding-in-Videos"><a href="#VideoMathQA-Benchmarking-Mathematical-Reasoning-via-Multimodal-Understanding-in-Videos" class="headerlink" title="VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos"></a>VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos</h2><p><strong>Authors:Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, Fahad Khan</strong></p>
<p>Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over $920$ man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: <a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/VideoMathQA">https://mbzuai-oryx.github.io/VideoMathQA</a> </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œçš„è§†é¢‘ç¯å¢ƒä¸­è¿›è¡Œæ•°å­¦æ¨ç†ä¸é™æ€å›¾åƒæˆ–æ–‡æœ¬ä¸­çš„æŒ‘æˆ˜å­˜åœ¨æ ¹æœ¬æ€§çš„ä¸åŒã€‚å®ƒè¦æ±‚è§£é‡Šç»†å¾®çš„è§†è§‰ä¿¡æ¯ï¼Œå‡†ç¡®é˜…è¯»æ‰‹å†™æˆ–æ•°å­—æ–‡æœ¬ï¼Œå¹¶æ•´åˆå£è¯­çº¿ç´¢ï¼Œè¿™äº›çº¿ç´¢é€šå¸¸éšæ—¶é—´åˆ†æ•£å¹¶éçº¿æ€§åˆ†å¸ƒã€‚åœ¨è¿™ç§å¤šæ¨¡æ€æƒ…å¢ƒä¸­ï¼ŒæˆåŠŸçš„å…³é”®ä¸ä»…åœ¨äºæ„ŸçŸ¥ï¼Œè¿˜åœ¨äºä»ä¸°å¯Œä¸”å˜ˆæ‚çš„å†…å®¹æµä¸­æœ‰é€‰æ‹©åœ°è¯†åˆ«å’Œæ•´åˆæ­£ç¡®çš„ä¸Šä¸‹æ–‡ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoMathQAåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ä¸Šæ‰§è¡Œè¿™ç§æ—¶é—´æ‰©å±•çš„è·¨æ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†10ä¸ªå¤šæ ·åŒ–çš„æ•°å­¦é¢†åŸŸï¼Œæ¶‰åŠçš„è§†é¢‘æ—¶é•¿ä»10ç§’åˆ°è¶…è¿‡1å°æ—¶ä¸ç­‰ã€‚å®ƒè¦æ±‚æ¨¡å‹è§£é‡Šç»“æ„åŒ–çš„è§†è§‰å†…å®¹ï¼Œç†è§£è¯´æ˜æ€§å™è¿°ï¼Œå¹¶åœ¨è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´å…±åŒå»ºç«‹æ¦‚å¿µã€‚æˆ‘ä»¬è˜è¯·äº†ç ”ç©¶ç”Ÿæ°´å¹³çš„ä¸“å®¶ä»¥ç¡®ä¿é«˜è´¨é‡ï¼Œæ€»æ ‡æ³¨æ—¶é•¿è¶…è¿‡920å°æ—¶ã€‚ä¸ºäº†åæ˜ çœŸå®ä¸–ç•Œåœºæ™¯ï¼Œé—®é¢˜å›´ç»•ä¸‰ä¸ªæ ¸å¿ƒæ¨ç†æŒ‘æˆ˜è¿›è¡Œè®¾è®¡ï¼šç›´æ¥é—®é¢˜è§£å†³ï¼Œç­”æ¡ˆåŸºäºæ‰€æå‡ºçš„é—®é¢˜ï¼›æ¦‚å¿µè¿ç§»ï¼Œè¦æ±‚å°†æ‰€å­¦æ–¹æ³•åº”ç”¨äºæ–°é—®é¢˜ï¼›ä»¥åŠæ·±åº¦æŒ‡ä»¤ç†è§£ï¼Œæ¶‰åŠå¯¹å¤šæ­¥éª¤è§£é‡Šçš„é•¿æœŸæ¨ç†å’Œéƒ¨åˆ†è§£å†³æ–¹æ¡ˆã€‚æ¯ä¸ªé—®é¢˜éƒ½åŒ…å«å¤šæ­¥éª¤æ¨ç†æ³¨é‡Šï¼Œå¯ä»¥ç²¾ç»†è¯Šæ–­æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡è¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸ºå¿…é¡»åœ¨æ—¶é—´å»¶é•¿å’Œæ¨¡å¼ä¸°å¯Œçš„æ•°å­¦é—®é¢˜ç¯å¢ƒä¸­è¿›è¡Œæ¨ç†çš„æ¨¡å‹å»ºç«‹äº†ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/VideoMathQA">https://mbzuai-oryx.github.io/VideoMathQA</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05349v1">PDF</a> VideoMathQA Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VideoMathQAåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ä¸Šè¿›è¡Œæ—¶é—´æ‰©å±•è·¨æ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«10ä¸ªæ•°å­¦é¢†åŸŸçš„å¤šæ ·åŒ–è§†é¢‘å†…å®¹ï¼Œæ—¶é•¿ä»10ç§’åˆ°è¶…è¿‡1å°æ—¶ä¸ç­‰ã€‚å®ƒè¦æ±‚æ¨¡å‹è§£è¯»ç»“æ„åŒ–è§†è§‰å†…å®¹ã€ç†è§£æ•™å­¦å™è¿°ï¼Œå¹¶åœ¨è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´å…±åŒå»ºç«‹æ¦‚å¿µã€‚é€šè¿‡æ­¤åŸºå‡†æµ‹è¯•ï¼Œçªæ˜¾äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸ºéœ€è¦åœ¨æ—¶é—´æ‰©å±•å’Œæ¨¡æ€ä¸°å¯Œçš„æ•°å­¦é—®é¢˜è§£å†³ç¯å¢ƒä¸­è¿›è¡Œæ¨ç†çš„æ¨¡å‹å»ºç«‹äº†ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoMathQAæ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†é¢‘ä¸Šè¿›è¡Œè·¨æ¨¡æ€æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æµ‹è¯•æ¶‰åŠ10ä¸ªæ•°å­¦é¢†åŸŸçš„å¤šæ ·åŒ–è§†é¢‘å†…å®¹ï¼Œæ—¶é•¿ä¸ä¸€ã€‚</li>
<li>æ¨¡å‹éœ€è§£è¯»ç»“æ„åŒ–è§†è§‰å†…å®¹ã€ç†è§£æ•™å­¦å™è¿°ï¼Œå¹¶åœ¨å„æ¨¡å¼é—´å»ºç«‹æ¦‚å¿µã€‚</li>
<li>æµ‹è¯•åŒ…å«ä¸‰ç§æ ¸å¿ƒæ¨ç†æŒ‘æˆ˜ï¼šç›´æ¥é—®é¢˜è§£å†³ã€æ¦‚å¿µè¿ç§»å’Œæ·±åº¦æ•™å­¦ç†è§£ã€‚</li>
<li>æ¯ä¸ªé—®é¢˜åŒ…å«å¤šæ­¥éª¤æ¨ç†æ³¨é‡Šï¼Œèƒ½ç²¾ç»†è¯Šæ–­æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„å±€é™æ€§åœ¨æµ‹è¯•ä¸­çªæ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf491dc0235f11fd4346ca0e416f280a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b350e0428e0a3e219e41243d2e00837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4e2cffc9746b9eff55e4d003f59bf73.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning"><a href="#MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning" class="headerlink" title="MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning"></a>MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning</h2><p><strong>Authors:Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li</strong></p>
<p>Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a> </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¹¿æ³›æé«˜äº†æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†å°†å…¶æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰å·¥ä½œè¦ä¹ˆé‡‡ç”¨ç±»ä¼¼çš„æ–‡æœ¬æ¨ç†è¿›è¡Œå›¾åƒè¾“å…¥ï¼Œè¦ä¹ˆå¯»æ±‚å°†è§†è§‰ä¿¡å·èå…¥æ•°å­¦æ€ç»´é“¾ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢é¢ä¸´ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šä¾èµ–ç²—ç²’åº¦çš„ç›’çŠ¶å›¾åƒåŒºåŸŸã€è§†è§‰ç¼–ç å™¨å¯¹æ•°å­¦å†…å®¹çš„æ„ŸçŸ¥æœ‰é™ï¼Œä»¥åŠä¾èµ–å¤–éƒ¨èƒ½åŠ›è¿›è¡Œè§†è§‰ä¿®æ”¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºMINT-CoTï¼Œå¼•å…¥ç”¨äºæ€ç»´é“¾è§†è§‰æ¨ç†çš„æ•°å­¦äº¤å‰ä»¤ç‰Œã€‚MINT-CoTé€šè¿‡è‡ªé€‚åº”åœ°å°†ç›¸å…³è§†è§‰ä»¤ç‰Œæ’å…¥æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸­ï¼Œå®ç°æ€ç»´é“¾ä¸­çš„è§†è§‰ä¸æ–‡æœ¬äº¤å‰èåˆã€‚å®ƒé€šè¿‡åŠ¨æ€é€‰æ‹©æ•°å­¦å›¾å½¢å†…çš„ä»»ä½•å½¢çŠ¶è§†è§‰åŒºåŸŸæ¥å®ç°è¿™ä¸€åŠŸèƒ½ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€åŠŸèƒ½ï¼Œæˆ‘ä»¬æ„å»ºäº†MINT-CoTæ•°æ®é›†ï¼ŒåŒ…å«54Kä¸ªæ•°å­¦é—®é¢˜ï¼Œæ¯ä¸ªæ¨ç†æ­¥éª¤éƒ½ä¸ä»¤ç‰Œçº§åˆ«çš„è§†è§‰åŒºåŸŸå¯¹é½ï¼Œå¹¶é…å¤‡ä¸¥æ ¼çš„æ•°æ®ç”Ÿæˆæµç¨‹ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„MINT-CoTè®­ç»ƒç­–ç•¥ï¼Œé€æ­¥ç»“åˆçº¯æ–‡æœ¬æ€ç»´é“¾SFTã€äº¤å‰æ€ç»´é“¾SFTå’Œäº¤å‰æ€ç»´é“¾RLï¼Œä»è€Œå¾—åˆ°æˆ‘ä»¬çš„MINT-CoT-7Bæ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°å­¦é¢†åŸŸçš„æœ‰æ•ˆè§†è§‰äº¤å‰æ¨ç†ä¸­éå¸¸æœ‰æ•ˆï¼ŒMINT-CoT-7Båœ¨MathVistaä¸Šä¼˜äºåŸºçº¿æ¨¡å‹+34.08%ï¼Œåœ¨GeoQAä¸Šä¼˜äºåŸºçº¿æ¨¡å‹+28.78%ï¼Œåœ¨MMStarä¸Šä¼˜äºåŸºçº¿æ¨¡å‹+23.2%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xinyan-cxy/MINT-CoTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05331v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a></p>
<p><strong>Summary</strong><br>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰å·²ç»å¹¿æ³›åº”ç”¨äºæ•°å­¦æ¨ç†é¢†åŸŸã€‚ç„¶è€Œï¼Œå°†å…¶æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆé‡‡ç”¨ç±»ä¼¼çš„æ–‡æœ¬æ¨ç†è¿›è¡Œå›¾åƒè¾“å…¥ï¼Œè¦ä¹ˆå¯»æ±‚å°†è§†è§‰ä¿¡å·èå…¥æ•°å­¦CoTã€‚ä½†å®ƒä»¬åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢é¢ä¸´ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šä¾èµ–ç²—ç²’åº¦çš„æ¡†çŠ¶å›¾åƒåŒºåŸŸã€è§†è§‰ç¼–ç å™¨å¯¹æ•°å­¦å†…å®¹çš„æ„ŸçŸ¥æœ‰é™ï¼Œä»¥åŠä¾èµ–å¤–éƒ¨èƒ½åŠ›è¿›è¡Œè§†è§‰ä¿®æ”¹ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºMINT-CoTæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ•°å­¦äº¤é”™ä»¤ç‰Œï¼ˆMathematical INterleaved Tokensï¼‰è¿›è¡Œé“¾å¼æ€ç»´è§†è§‰æ¨ç†ã€‚MINT-CoTè‡ªé€‚åº”åœ°å°†ç›¸å…³è§†è§‰ä»¤ç‰Œæ’å…¥æ–‡æœ¬æ¨ç†æ­¥éª¤ä¸­ï¼Œé€šè¿‡åŠ¨æ€é€‰æ‹©æ•°å­¦å›¾å½¢å†…çš„è§†è§‰åŒºåŸŸæ¥å¢å¼ºè¿™ç§èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ„å»ºäº†MINT-CoTæ•°æ®é›†å’Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæœ€ç»ˆæ¨å‡ºMINT-CoT-7Bæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨MathVistaã€GeoQAå’ŒMMStarä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¦ç»†ä¿¡æ¯å¯é€šè¿‡é“¾æ¥æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a>ã€‚ç®€æ´ç†è§£å³ä¸ºæœ¬æ–‡ä¸»è¦æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è§†è§‰åŒ–æ¨ç†æŠ€æœ¯æ–¹æ³•ä»¥åŠå¯¹åº”çš„æ•°æ®é›†ä¸æ¨¡å‹ä¼˜åŒ–è®­ç»ƒç­–ç•¥ç­‰ä»¥ä¼˜åŒ–å¯¹æ•°å­¦æ¨ç†èƒ½åŠ›çš„æå‡ä¸åº”ç”¨è½åœ°æ€§å¹¶å®é™…å–å¾—è¶…è¶Šå…¶ä»–ç°æœ‰æ–¹æ¡ˆçš„ä¼˜ç§€æ•ˆæœæ€»ç»“æ‘˜è¦å†…å®¹ä¸ºæé«˜å¤šæ¨¡æ€é¢†åŸŸå†…çš„æ•°å­¦æ¨ç†èƒ½åŠ›ç ”ç©¶ç°çŠ¶åŠæ‰€é¢ä¸´çš„å›°éš¾ç‚¹æŒ‘æˆ˜æ®æ­¤å±•å¼€æ·±å…¥ç ”ç©¶ä»è€Œå¼•å‡ºæ‰€åˆ›æ–°æ€§åœ°æå‡ºMINTç»“åˆå®ä½“ç†è®ºåˆ†æè¯´æ˜äº†åˆ›æ–°æ–¹æ¡ˆçš„æ˜¾è‘—ä¼˜åŠ¿æ€»ç»“æ–¹æ³•ä»¥åŠå…¶æ€§èƒ½ç‰¹ç‚¹åŠåº”ç”¨æ¨å¹¿ä¿¡æ¯æ˜ç¡®å…·ä½“çš„è¡Œä¸šæ„ä¹‰åº”ç”¨æ½œåŠ›å’Œå¸‚åœºå‰æ™¯ä¸ºè¯¥é¢†åŸŸçš„æŒç»­å‘å±•ä¸æå‡æä¾›äº†ä¸€ç§åˆ›æ–°æ€§å’Œå®è·µæ€§çš„æŠ€æœ¯æ–¹æ³•å…³é”®ç‰¹ç‚¹åœ¨äºå°†è§†è§‰æ¨ç†ä¸æ•°å­¦æ¨ç†ç´§å¯†ç»“åˆå®ç°äº†æ›´é«˜æ•ˆæ›´å‡†ç¡®çš„æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ›æ¦‚æ‹¬ç®€æ´å‡†ç¡®çªå‡ºä¸»é¢˜åŠäº®ç‚¹æ— å†—ä½™å†…å®¹<strong>Key Takeaways</strong>:</p>
<ol>
<li>MINT-CoTè§£å†³äº†å°†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸçš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¾èµ–äºç²—ç²’åº¦å›¾åƒåŒºåŸŸã€ç¼ºä¹æ·±åº¦æ„ŸçŸ¥æ•°å­¦å†…å®¹ç­‰é™åˆ¶ã€‚</li>
<li>MINT-CoTå¼•å…¥æ•°å­¦äº¤é”™ä»¤ç‰Œï¼Œå®ç°è‡ªé€‚åº”åœ°å°†è§†è§‰ä¿¡æ¯ä¸æ–‡æœ¬æ¨ç†ç»“åˆï¼Œæå‡æ•°å­¦é—®é¢˜çš„æ„ŸçŸ¥å’Œè§£å†³æ–¹æ¡ˆå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e0e517d4f8b10aa9ca2f54ac60ba249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ac84bdb90b912b32fbef78fa3d2fff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e516bcb6bcba5b8144ca314a595fcedb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-568fe4ddee8c914dbbeabe58cb026b37.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AV-Reasoner-Improving-and-Benchmarking-Clue-Grounded-Audio-Visual-Counting-for-MLLMs"><a href="#AV-Reasoner-Improving-and-Benchmarking-Clue-Grounded-Audio-Visual-Counting-for-MLLMs" class="headerlink" title="AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual   Counting for MLLMs"></a>AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual   Counting for MLLMs</h2><p><strong>Authors:Lidong Lu, Guo Chen, Zhiqi Li, Yicheng Liu, Tong Lu</strong></p>
<p>Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve modelâ€™s counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on <a target="_blank" rel="noopener" href="https://av-reasoner.github.io/">https://av-reasoner.github.io</a>. </p>
<blockquote>
<p>å°½ç®¡è§†é¢‘ç†è§£é¢†åŸŸå·²ç»å–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†å½“å‰çš„å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è®¡æ•°ä»»åŠ¡ä¸Šä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•å—é™äºçŸ­è§†é¢‘ã€é™å®šæŸ¥è¯¢èŒƒå›´ã€ç¼ºä¹çº¿ç´¢æ ‡æ³¨å’Œå¼±å¤šæ¨¡å¼è¦†ç›–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CG-AV-Countingï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„åŸºäºçº¿ç´¢çš„è®¡æ•°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1027ä¸ªå¤šæ¨¡å¼é—®é¢˜å’Œ5845ä¸ªæ ‡æ³¨çº¿ç´¢ï¼Œæ¶µç›–497ä¸ªé•¿è§†é¢‘ã€‚å®ƒæ”¯æŒé»‘ç®±å’Œç™½ç®±è¯„ä¼°ï¼Œæ˜¯ç«¯åˆ°ç«¯å’ŒåŸºäºæ¨ç†çš„è®¡æ•°çš„ç»¼åˆæµ‹è¯•å¹³å°ã€‚ä¸ºäº†æ¢ç´¢æé«˜æ¨¡å‹è®¡æ•°èƒ½åŠ›çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†AV-Reasonerï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡GRPOå’Œè¯¾ç¨‹å­¦ä¹ è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿä»ç›¸å…³ä»»åŠ¡ä¸­æ¨å¹¿è®¡æ•°èƒ½åŠ›ã€‚AV-Reasoneråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œè¯æ˜äº†å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼Œåœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯­è¨€ç©ºé—´ä¸­çš„æ¨ç†æ— æ³•å¸¦æ¥æ€§èƒ½æå‡ã€‚ç›¸å…³ä»£ç å’ŒåŸºå‡†æµ‹è¯•å·²åœ¨<a target="_blank" rel="noopener" href="https://av-reasoner.github.ioå‘å¸ƒ./">https://av-reasoner.github.ioå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05328v1">PDF</a> 21 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘ç†è§£é¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è®¡æ•°ä»»åŠ¡æ—¶å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†CG-AV-CountingåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1,027ä¸ªå¤šæ¨¡æ€é—®é¢˜å’Œ5,845ä¸ªé’ˆå¯¹497ä¸ªé•¿è§†é¢‘çš„æ³¨é‡Šçº¿ç´¢ã€‚æ­¤å¤–ï¼Œä¸ºæå‡æ¨¡å‹çš„è®¡æ•°èƒ½åŠ›ï¼Œæ–‡ç« è¿˜æå‡ºäº†AV-Reasoneræ¨¡å‹ï¼Œé€šè¿‡é€šç”¨åŒºåŸŸä¼˜å…ˆï¼ˆGRPOï¼‰å’Œè¯¾ç¨‹å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå…·å¤‡ä»ç›¸å…³ä»»åŠ¡ä¸­æ³›åŒ–è®¡æ•°çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼Œåœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯­è¨€ç©ºé—´çš„æ¨ç†å¹¶æœªå¸¦æ¥æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£é¢†åŸŸçš„è®¡æ•°ä»»åŠ¡ä¸Šä»æœ‰å›°éš¾ã€‚</li>
<li>CG-AV-CountingåŸºå‡†æµ‹è¯•è¢«å¼•å…¥ï¼ŒåŒ…å«å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„çº¿ç´¢ï¼Œæ”¯æŒé»‘ç®±å’Œç™½ç®±è¯„ä¼°ï¼Œæ˜¯ç«¯åˆ°ç«¯å’Œæ¨ç†è®¡æ•°æ–¹æ³•çš„ç»¼åˆæµ‹è¯•å¹³å°ã€‚</li>
<li>AV-Reasoneræ¨¡å‹é€šè¿‡é€šç”¨åŒºåŸŸä¼˜å…ˆï¼ˆGRPOï¼‰å’Œè¯¾ç¨‹å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰æ³›åŒ–è®¡æ•°èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡æ¨¡å‹è®¡æ•°èƒ½åŠ›ä¸Šå±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯­è¨€ç©ºé—´æ¨ç†å¹¶æœªå¸¦æ¥æ€§èƒ½æå‡ã€‚</li>
<li>ä»£ç å’ŒåŸºå‡†æµ‹è¯•å·²åœ¨<a target="_blank" rel="noopener" href="https://av-reasoner.github.ioä¸Šå‘å¸ƒ./">https://av-reasoner.github.ioä¸Šå‘å¸ƒã€‚</a></li>
<li>ç°æœ‰è§†é¢‘è®¡æ•°ä»»åŠ¡é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬çŸ­è§†é¢‘ã€é™å®šæŸ¥è¯¢ã€ç¼ºä¹çº¿ç´¢æ³¨é‡Šå’Œå¼±å¤šæ¨¡æ€è¦†ç›–ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a4579d79fb55fb8e08662919c927c310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8844e3bbc0e3a812c6dfb8a1fad711ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4df35e8c10b38aca84cc0bf811c8750b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef501bffe39fc643dce8dc89296d9fcd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning"><a href="#Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning" class="headerlink" title="Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning"></a>Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning</h2><p><strong>Authors:Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿé€šå¸¸é¢ä¸´çŸ¥è¯†å†²çªé—®é¢˜ï¼Œå…¶ä¸­æ£€ç´¢åˆ°çš„å¤–éƒ¨çŸ¥è¯†ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å›ºæœ‰å‚æ•°çŸ¥è¯†ç›¸çŸ›ç›¾ã€‚è¿™å¯¹é—®ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€é€šè¿‡å¹¶æ’æ¯”è¾ƒä¸¤ç§çŸ¥è¯†æºæ¥å‡è½»å†²çªï¼Œä½†è¿™å¯èƒ½ä½¿LLMé¢ä¸´è¿‡å¤šæˆ–å†—é•¿çš„ä¸Šä¸‹æ–‡ï¼Œæœ€ç»ˆé˜»ç¢å…¶è¯†åˆ«å’Œç¼“è§£ä¸ä¸€è‡´çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Micro-Actæ¡†æ¶ï¼Œå®ƒå…·æœ‰åˆ†å±‚åŠ¨ä½œç©ºé—´ï¼Œå¯è‡ªåŠ¨æ„ŸçŸ¥ä¸Šä¸‹æ–‡å¤æ‚æ€§ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†æ¯ä¸ªçŸ¥è¯†æºåˆ†è§£ä¸ºä¸€ç³»åˆ—ç²¾ç»†çš„æ¯”è¾ƒã€‚è¿™äº›æ¯”è¾ƒè¡¨ç°ä¸ºå¯æ“ä½œçš„æ­¥éª¤ï¼Œä½¿æ¨ç†è¶…è¶Šè¡¨å±‚ä¸Šä¸‹æ–‡ã€‚é€šè¿‡åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒMicro-Actåœ¨æ‰€æœ‰äº”ä¸ªæ•°æ®é›†å’Œä¸‰ç§å†²çªç±»å‹ä¸Šå§‹ç»ˆå®ç°äº†é—®ç­”å‡†ç¡®åº¦çš„æ˜¾è‘—æé«˜ï¼Œå°¤å…¶åœ¨æ—¶é—´å’Œè¯­ä¹‰ç±»å‹æ–¹é¢ï¼Œæ‰€æœ‰åŸºçº¿éƒ½å‡ºç°äº†æ˜¾è‘—å¤±è´¥ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMicro-Actåœ¨éå†²çªé—®é¢˜ä¸Šä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œè¿™çªå‡ºäº†å…¶åœ¨ç°å®ä¸–ç•ŒRAGåº”ç”¨ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05278v1">PDF</a> Accepted by ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Retrieval-Augmented Generationï¼ˆRAGï¼‰ç³»ç»Ÿä¸­å¸¸è§çš„çŸ¥è¯†å†²çªé—®é¢˜ï¼Œå³æ£€ç´¢çš„å¤–éƒ¨çŸ¥è¯†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å›ºæœ‰å‚æ•°çŸ¥è¯†ä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥é—®é¢˜å¯¹é—®ç­”ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸é€šè¿‡å¹¶æ’æ¯”è¾ƒä¸¤ç§çŸ¥è¯†æºæ¥å‡è½»å†²çªï¼Œä½†è¿™å¯èƒ½ä½¿LLMé¢ä¸´è¿‡å¤šå¤æ‚æˆ–å†—é•¿çš„è¯­å¢ƒï¼Œä»è€Œéš¾ä»¥è¯†åˆ«å’Œç¼“è§£ä¸ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºMicro-Actæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰å±‚æ¬¡åŒ–çš„åŠ¨ä½œç©ºé—´ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ„ŸçŸ¥è¯­å¢ƒå¤æ‚æ€§ï¼Œå¹¶è‡ªé€‚åº”åœ°å°†æ¯ä¸ªçŸ¥è¯†æºåˆ†è§£æˆä¸€ç³»åˆ—ç²¾ç»†çš„æ¯”è¾ƒã€‚è¿™äº›æ¯”è¾ƒè¡¨ç°ä¸ºå¯æ“ä½œçš„æ­¥éª¤ï¼Œä½¿æ¨ç†è¶…è¶Šäº†è‚¤æµ…è¯­å¢ƒã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMicro-Actåœ¨æ‰€æœ‰æ•°æ®é›†å’Œä¸‰ç§å†²çªç±»å‹ä¸Šå‡å®ç°äº†å¯¹æœ€æ–°åŸºå‡†çº¿çš„é—®ç­”å‡†ç¡®åº¦æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—¶é—´å’Œè¯­ä¹‰ç±»å‹çš„å†²çªä¸Šã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒMicro-Actåœ¨éå†²çªé—®é¢˜ä¸Šä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå‡¸æ˜¾å…¶åœ¨ç°å®RAGåº”ç”¨ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGç³»ç»Ÿé¢ä¸´çŸ¥è¯†å†²çªé—®é¢˜ï¼Œå³å¤–éƒ¨æ£€ç´¢çŸ¥è¯†ä¸LLMå›ºæœ‰çŸ¥è¯†ä¹‹é—´çš„çŸ›ç›¾ã€‚</li>
<li>çŸ¥è¯†å†²çªå¯¹ä¸‹æ¸¸ä»»åŠ¡å¦‚é—®ç­”çš„æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å¹¶æ’æ¯”è¾ƒçŸ¥è¯†æºæ¥å‡è½»å†²çªï¼Œä½†å¯èƒ½ä½¿LLMé¢ä¸´å¤æ‚æˆ–å†—é•¿çš„è¯­å¢ƒã€‚</li>
<li>Micro-Actæ¡†æ¶å…·æœ‰å±‚æ¬¡åŒ–çš„åŠ¨ä½œç©ºé—´ï¼Œèƒ½è‡ªåŠ¨æ„ŸçŸ¥è¯­å¢ƒå¤æ‚æ€§å¹¶åˆ†è§£çŸ¥è¯†æºã€‚</li>
<li>Micro-Acté€šè¿‡ç²¾ç»†çš„æ¯”è¾ƒå’Œå¯æ“ä½œçš„æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œè¶…è¶Šè‚¤æµ…è¯­å¢ƒã€‚</li>
<li>Micro-Actåœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤šç§å†²çªç±»å‹ä¸Šå®ç°äº†å¯¹æœ€æ–°æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-827152fce07163683ee4731b5081d264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c41fbee446036f82e6d35bef820a5cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cb7fa84605926f84e3840f592cd3cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b53a70df398ac3e002d9d2ed33389e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d870c33b971ca6264a6abf75e66f1b84.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Counterfactual-reasoning-an-analysis-of-in-context-emergence"><a href="#Counterfactual-reasoning-an-analysis-of-in-context-emergence" class="headerlink" title="Counterfactual reasoning: an analysis of in-context emergence"></a>Counterfactual reasoning: an analysis of in-context emergence</h2><p><strong>Authors:Moritz Miller, Bernhard SchÃ¶lkopf, Siyuan Guo</strong></p>
<p>Large-scale neural language models (LMs) exhibit remarkable performance in in-context learning: the ability to learn and reason the input context on the fly without parameter update. This work studies in-context counterfactual reasoning in language models, that is, to predict the consequences of changes under hypothetical scenarios. We focus on studying a well-defined synthetic setup: a linear regression task that requires noise abduction, where accurate prediction is based on inferring and copying the contextual noise from factual observations. We show that language models are capable of counterfactual reasoning in this controlled setup and provide insights that counterfactual reasoning for a broad class of functions can be reduced to a transformation on in-context observations; we find self-attention, model depth, and data diversity in pre-training drive performance in Transformers. More interestingly, our findings extend beyond regression tasks and show that Transformers can perform noise abduction on sequential data, providing preliminary evidence on the potential for counterfactual story generation. Our code is available under <a target="_blank" rel="noopener" href="https://github.com/moXmiller/counterfactual-reasoning.git">https://github.com/moXmiller/counterfactual-reasoning.git</a> . </p>
<blockquote>
<p>å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼šèƒ½å¤Ÿåœ¨ä¸æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå³æ—¶å­¦ä¹ å’Œæ¨ç†è¾“å…¥ä¸Šä¸‹æ–‡ã€‚æœ¬ç ”ç©¶å…³æ³¨è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡åäº‹å®æ¨ç†ï¼Œå³é¢„æµ‹å‡è®¾åœºæ™¯å˜åŒ–åçš„ç»“æœã€‚æˆ‘ä»¬ä¸“æ³¨äºç ”ç©¶ä¸€ä¸ªå®šä¹‰æ˜ç¡®çš„åˆæˆè®¾ç½®ï¼šéœ€è¦è¿›è¡Œå™ªå£°æ¨æ–­çš„çº¿æ€§å›å½’ä»»åŠ¡ï¼Œå‡†ç¡®é¢„æµ‹åŸºäºä»å®é™…è§‚å¯Ÿä¸­æ¨æ–­å’Œå¤åˆ¶ä¸Šä¸‹æ–‡å™ªå£°ã€‚æˆ‘ä»¬è¯æ˜è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨è¿™ç§å—æ§è®¾ç½®ä¸­è¿›è¡Œåäº‹å®æ¨ç†ï¼Œå¹¶æä¾›è§è§£ï¼Œå³ä¸€ç±»å¹¿æ³›çš„å‡½æ•°çš„åäº‹å®æ¨ç†å¯ä»¥å½’ç»“ä¸ºå¯¹ä¸Šä¸‹æ–‡è§‚å¯Ÿçš„è½¬æ¢ï¼›æˆ‘ä»¬å‘ç°è‡ªæ³¨æ„åŠ›ã€æ¨¡å‹æ·±åº¦ä»¥åŠé¢„è®­ç»ƒä¸­çš„æ•°æ®å¤šæ ·æ€§æ˜¯é©±åŠ¨Transformeræ€§èƒ½çš„å…³é”®å› ç´ ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¶…è¶Šäº†å›å½’ä»»åŠ¡ï¼Œè¡¨æ˜Transformerå¯ä»¥å¯¹é¡ºåºæ•°æ®è¿›è¡Œå™ªå£°æ¨æ–­ï¼Œä¸ºæ½œåœ¨çš„åäº‹å®æ•…äº‹ç”Ÿæˆæä¾›äº†åˆæ­¥è¯æ®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/moXmiller/counterfactual-reasoning.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/moXmiller/counterfactual-reasoning.gitæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05188v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå³æ— éœ€æ›´æ–°å‚æ•°å°±èƒ½åœ¨è¯­å¢ƒä¸­å³æ—¶å­¦ä¹ å’Œæ¨ç†ã€‚æœ¬ç ”ç©¶æ¢è®¨è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡åäº‹å®æ¨ç†ï¼Œå³é¢„æµ‹å‡è®¾åœºæ™¯å˜åŒ–åçš„ç»“æœã€‚ç ”ç©¶é‡ç‚¹æ˜¯ä¸€ä¸ªæ˜ç¡®çš„åˆæˆè®¾ç½®ï¼šéœ€è¦è¿›è¡Œå™ªå£°æ¨æ–­çš„çº¿æ€§å›å½’ä»»åŠ¡ï¼Œå‡†ç¡®é¢„æµ‹åŸºäºä»å®é™…è§‚å¯Ÿä¸­æ¨æ–­å’Œå¤åˆ¶ä¸Šä¸‹æ–‡å™ªå£°ã€‚ç ”ç©¶å‘ç°è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨è¿™ç§æ§åˆ¶è®¾ç½®ä¸­è¿›è¡Œåäº‹å®æ¨ç†ï¼Œå¹¶æä¾›è§è§£ï¼Œå³åäº‹å®æ¨ç†å¯ä»¥ç®€åŒ–ä¸ºå¯¹ä¸Šä¸‹æ–‡è§‚å¯Ÿçš„è½¬æ¢ï¼›æˆ‘ä»¬å‘ç°è‡ªæ³¨æ„åŠ›ã€æ¨¡å‹æ·±åº¦å’Œé¢„è®­ç»ƒæ•°æ®å¤šæ ·æ€§æ˜¯æ¨åŠ¨å˜å‹å™¨æ€§èƒ½çš„å…³é”®ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¶…å‡ºäº†å›å½’ä»»åŠ¡çš„èŒƒå›´ï¼Œæ˜¾ç¤ºå˜å‹å™¨å¯ä»¥åœ¨åºåˆ—æ•°æ®ä¸Šè¿›è¡Œå™ªå£°æ¨æ–­ï¼Œä¸ºåäº‹å®æ•…äº‹ç”Ÿæˆçš„æ½œåŠ›æä¾›äº†åˆæ­¥è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>è¯­è¨€æ¨¡å‹å…·å¤‡åäº‹å®æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ§åˆ¶ç¯å¢ƒä¸‹å¯è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>åäº‹å®æ¨ç†å¯ç®€åŒ–ä¸ºå¯¹ä¸Šä¸‹æ–‡è§‚å¯Ÿçš„è½¬æ¢ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›ã€æ¨¡å‹æ·±åº¦å’Œé¢„è®­ç»ƒæ•°æ®å¤šæ ·æ€§å¯¹å˜å‹å™¨æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>è¯­è¨€æ¨¡å‹ä¸ä»…èƒ½åœ¨å›å½’ä»»åŠ¡ä¸­è¿›è¡Œå™ªå£°æ¨æ–­ï¼Œè¿˜èƒ½åœ¨åºåˆ—æ•°æ®ä¸Šè¿›è¡Œã€‚</li>
<li>ç ”ç©¶ä¸ºåäº‹å®æ•…äº‹ç”Ÿæˆçš„æ½œåŠ›æä¾›äº†åˆæ­¥è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e38aa240d9f01787ea9e94cdee7c7c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a36eb44ac8dd3f87a8e846f883366c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c79a71c539518c8d288f4349a32489b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbe6920623c737b38cc262512456c0d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TreeRPO-Tree-Relative-Policy-Optimization"><a href="#TreeRPO-Tree-Relative-Policy-Optimization" class="headerlink" title="TreeRPO: Tree Relative Policy Optimization"></a>TreeRPO: Tree Relative Policy Optimization</h2><p><strong>Authors:Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, Jing Tang</strong></p>
<p>Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce \textbf{\name}, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, \name directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, \name innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows \name to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our \name algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0% to 35.5%. Furthermore, \name significantly outperforms GRPO by 2.9% in performance while simultaneously reducing the average response length by 18.1%, showcasing its effectiveness and efficiency. Our code will be available at \href{<a target="_blank" rel="noopener" href="https://github.com/yangzhch6/TreeRPO%7D%7Bhttps://github.com/yangzhch6/TreeRPO%7D">https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„å…³é”®å±€é™æ€§åœ¨äºï¼Œåœ¨å®Œæ•´è½¨è¿¹å±‚é¢å®šä¹‰çš„å¥–åŠ±æ— æ³•ä¸ºæ¨ç†è¿‡ç¨‹ä¸­ä¼˜åŒ–ä¸­é—´æ­¥éª¤æä¾›è¶³å¤Ÿçš„æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•â€”â€”æ ‘é‡‡æ ·æœŸæœ›å¥–åŠ±ç®—æ³•ï¼ˆæš‚ä¸”ç§°ä¸ºXXç®—æ³•ï¼‰ã€‚è¯¥ç®—æ³•é€šè¿‡æ ‘é‡‡æ ·ä¼°è®¡ä¸åŒæ¨ç†æ­¥éª¤çš„å¥–åŠ±çš„æ•°å­¦æœŸæœ›ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–äºå•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ï¼ŒXXç®—æ³•é€šè¿‡é‡‡æ ·è¿‡ç¨‹ç›´æ¥ä¼°è®¡è¿™äº›å¥–åŠ±ã€‚åŸºäºç›¸å¯¹å¥–åŠ±è®­ç»ƒæœºåˆ¶çš„GRPOç®—æ³•ï¼ŒXXç®—æ³•åˆ›æ–°åœ°æ ¹æ®æ ‘é‡‡æ ·è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ­¥éª¤çº§åˆ«ç»„è®¡ç®—å¥–åŠ±ã€‚è¿™ä¸€è¿›å±•ä½¿å¾—XXç®—æ³•èƒ½å¤Ÿäº§ç”Ÿç²¾ç»†ä¸”å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œæå¤§åœ°æé«˜äº†å­¦ä¹ è¿‡ç¨‹å’ŒLLMçš„æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„XXç®—æ³•åœ¨æµ‹è¯•åŸºå‡†ä¸Šæ˜¾è‘—æé«˜äº†Qwen-2.5-Mathçš„å¹³å‡Pass@1å‡†ç¡®ç‡ï¼Œä»19.0%æé«˜åˆ°35.5%ã€‚æ­¤å¤–ï¼ŒXXç®—æ³•åœ¨æ€§èƒ½ä¸Šæ¯”GRPOé«˜å‡º2.9%ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†18.1%ï¼Œæ˜¾ç¤ºäº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/yangzhch6/TreeRPO">https://github.com/yangzhch6/TreeRPO</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05183v1">PDF</a> 13pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ–¹æ³•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•çš„ä¸€ä¸ªå…³é”®å±€é™æ˜¯ï¼Œåœ¨å®Œæ•´è½¨è¿¹å±‚é¢å®šä¹‰çš„å¥–åŠ±æ— æ³•ä¸ºä¼˜åŒ–æ¨ç†è¿‡ç¨‹çš„ä¸­é—´æ­¥éª¤æä¾›è¶³å¤ŸæŒ‡å¯¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°æ–¹æ³•\nameï¼Œé€šè¿‡æ ‘é‡‡æ ·ä¼°è®¡æ¨ç†æ­¥éª¤ä¸­ä¸åŒå¥–åŠ±çš„æ•°å­¦æœŸæœ›ã€‚ä¸åŒäºä¾èµ–å•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„å…ˆå‰æ–¹æ³•ï¼Œ\nameé€šè¿‡é‡‡æ ·è¿‡ç¨‹ç›´æ¥ä¼°è®¡è¿™äº›å¥–åŠ±ã€‚å»ºç«‹åœ¨ç›¸å¯¹å¥–åŠ±è®­ç»ƒæœºåˆ¶çš„GRPOåŸºç¡€ä¸Šï¼Œ\nameæ ¹æ®æ ‘é‡‡æ ·è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ­¥éª¤çº§åˆ«ç»„è®¡ç®—å¥–åŠ±ã€‚è¿™ä¸€è¿›å±•äº§ç”Ÿäº†ç²¾ç»†ä¸”å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œæ˜¾è‘—å¢å¼ºäº†å­¦ä¹ è¿‡ç¨‹å’ŒLLMsçš„æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„\nameç®—æ³•åœ¨æµ‹è¯•åŸºå‡†ä¸Šå¤§å¹…æé«˜äº†Qwen-2.5-Mathçš„å¹³å‡Pass@1å‡†ç¡®ç‡ï¼Œä»19.0%æå‡è‡³35.5%ã€‚æ­¤å¤–ï¼Œ\nameåœ¨æ€§èƒ½ä¸Šè¾ƒGRPOé«˜å‡º2.9%ï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†18.1%ï¼Œå±•ç°å‡ºå…¶æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¥–åŠ±å®šä¹‰åœ¨å®Œæ•´è½¨è¿¹å±‚é¢ï¼Œç¼ºä¹å¯¹ä¸­é—´æ­¥éª¤ä¼˜åŒ–çš„æŒ‡å¯¼ã€‚</li>
<li>\nameæ–¹æ³•é€šè¿‡æ ‘é‡‡æ ·ä¼°è®¡æ¨ç†æ­¥éª¤ä¸­ä¸åŒå¥–åŠ±çš„æ•°å­¦æœŸæœ›ã€‚</li>
<li>\nameç›´æ¥é€šè¿‡é‡‡æ ·è¿‡ç¨‹ä¼°è®¡å¥–åŠ±ï¼Œä¸åŒäºä¾èµ–å•ç‹¬æ­¥éª¤å¥–åŠ±æ¨¡å‹çš„å…ˆå‰æ–¹æ³•ã€‚</li>
<li>\nameå»ºç«‹åœ¨ç›¸å¯¹å¥–åŠ±è®­ç»ƒæœºåˆ¶çš„GRPOåŸºç¡€ä¸Šï¼Œæ ¹æ®æ­¥éª¤çº§åˆ«ç»„è®¡ç®—å¥–åŠ±ã€‚</li>
<li>\nameç®—æ³•æ˜¾è‘—æé«˜Qwen-2.5-Mathçš„Pass@1å‡†ç¡®ç‡ï¼Œä»19.0%æå‡è‡³35.5%ã€‚</li>
<li>\nameåœ¨æ€§èƒ½ä¸Šè¾ƒGRPOæœ‰æ‰€æå‡ï¼ŒåŒæ—¶å‡å°‘å¹³å‡å“åº”é•¿åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4fe78c01f7eae517a79dcece73bed4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4acbebde56aa146aafa7f9ceabd5c4a9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiCoRe-Enhancing-Zero-shot-Event-Detection-via-Divergent-Convergent-LLM-Reasoning"><a href="#DiCoRe-Enhancing-Zero-shot-Event-Detection-via-Divergent-Convergent-LLM-Reasoning" class="headerlink" title="DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM   Reasoning"></a>DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM   Reasoning</h2><p><strong>Authors:Tanmay Parekh, Kartik Mehta, Ninareh Mehrabi, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline â€“ establishing DiCoRe as a strong zero-shot ED framework. </p>
<blockquote>
<p>é›¶äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰æ˜¯åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­çš„äº‹ä»¶æåŠçš„ä»»åŠ¡ï¼Œå¯¹äºç‰¹å®šé¢†åŸŸæ–‡æ¡£ç†è§£è‡³å…³é‡è¦ã€‚ç†è§£å¤æ‚çš„äº‹ä»¶æœ¬ä½“è®ºï¼Œä»æ®µè½ä¸­æå–ç‰¹å®šé¢†åŸŸçš„è§¦å‘è¯ï¼Œå¹¶é€‚å½“åœ°è¿›è¡Œç»“æ„åŒ–è®¾ç½®ï¼Œå¢åŠ äº†é›¶äº‹ä»¶æ£€æµ‹å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®ç”¨æ€§å’Œå±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DiCoReï¼Œä¸€ä¸ªå‘æ•£æ”¶æ•›æ¨ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡Dreamerå’ŒGrounderå°†äº‹ä»¶æ£€æµ‹ä»»åŠ¡è§£è€¦ã€‚Dreameré€šè¿‡å¼€æ”¾å¼äº‹ä»¶å‘ç°é¼“åŠ±å‘æ•£æ¨ç†ï¼Œæœ‰åŠ©äºæé«˜äº‹ä»¶è¦†ç›–ç‡ã€‚ç›¸åï¼ŒGrounderå¼•å…¥æ”¶æ•›æ¨ç†ï¼Œä»¥æœ‰é™çŠ¶æ€æœºå¼•å¯¼çš„æœ‰çº¦æŸè§£ç æ¥ä½¿è‡ªç”±å½¢å¼çš„é¢„æµ‹ä¸ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤å¯¹é½ã€‚æ­¤å¤–ï¼ŒLLM-JudgeéªŒè¯æœ€ç»ˆè¾“å‡ºä»¥ç¡®ä¿é«˜ç²¾ç¡®åº¦ã€‚é€šè¿‡å¯¹äº”ä¸ªé¢†åŸŸçš„å…­ä¸ªæ•°æ®é›†å’Œä¹ä¸ªLLMçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†DiCoReå¦‚ä½•å§‹ç»ˆä¼˜äºå…ˆå‰çš„é›¶æ ·æœ¬è¿ç§»å­¦ä¹ å’Œæ¨ç†åŸºå‡†æµ‹è¯•ï¼Œåœ¨æœ€ä½³åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¹³å‡F1å¾—åˆ†4-7%çš„æå‡â€”â€”ç¡®ç«‹äº†DiCoReä½œä¸ºå¼ºå¤§çš„é›¶æ ·æœ¬äº‹ä»¶æ£€æµ‹æ¡†æ¶çš„åœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05128v1">PDF</a> Submitted at ACL ARR May 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬ç†è§£ä»»åŠ¡çš„éœ€è¦ï¼Œç ”ç©¶æå‡ºäº†é›¶æ ·æœ¬äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰çš„é‡è¦æ€§ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é›¶æ ·æœ¬äº‹ä»¶æ£€æµ‹æ—¶å­˜åœ¨äº‹ä»¶è¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å‘æ•£æ”¶æ•›æ¨ç†æ¡†æ¶DiCoReï¼Œé€šè¿‡Dreamerè¿›è¡Œå‘æ•£æ¨ç†å®ç°å¼€æ”¾å¼äº‹ä»¶å‘ç°ï¼Œå¹¶é€šè¿‡Grounderå¼•å…¥æ”¶æ•›æ¨ç†å°†è‡ªç”±å½¢å¼çš„é¢„æµ‹ä¸ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤å¯¹é½ã€‚æ­¤å¤–ï¼ŒLLM-JudgeéªŒè¯äº†æœ€ç»ˆè¾“å‡ºç»“æœçš„ç²¾ç¡®åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒDiCoReæ¡†æ¶åœ¨å„ç§æ•°æ®é›†å’Œå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹³å‡F1å¾—åˆ†æ¯”æœ€ä½³åŸºçº¿é«˜å‡º4-7%ï¼Œæˆä¸ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬äº‹ä»¶æ£€æµ‹æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰åœ¨ç‰¹å®šé¢†åŸŸæ–‡æ¡£ç†è§£ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é›¶æ ·æœ¬äº‹ä»¶æ£€æµ‹æ—¶å­˜åœ¨äº‹ä»¶è¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DiCoReæ¡†æ¶ç»“åˆäº†å‘æ•£æ¨ç†å’Œæ”¶æ•›æ¨ç†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>Dreameré€šè¿‡å¼€æ”¾å¼äº‹ä»¶å‘ç°é¼“åŠ±å‘æ•£æ¨ç†ã€‚</li>
<li>Grounderé€šè¿‡æœ‰é™çŠ¶æ€æœºå¼•å¯¼çº¦æŸè§£ç æ¥å®ç°æ”¶æ•›æ¨ç†ã€‚</li>
<li>LLM-JudgeéªŒè¯æœ€ç»ˆè¾“å‡ºçš„ç²¾ç¡®åº¦ä»¥ç¡®ä¿é«˜ç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-45deb2d05f87c810e14938ef5aadbcbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458ea81d0f0ef0f86c4e1ae18ab4898d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc33850d5d56fceba6395036a2baab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd09bf1c6e26884ebe35899d760a7cb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05aa89c1b637896cfb1d73be07e1683b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05d7ca8eb66922610ad3cf28a702c0b5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reason-to-Recommend-Using-Interaction-of-Thought-Reasoning-to-Enhance-LLM-Recommendation"><a href="#Reason-to-Recommend-Using-Interaction-of-Thought-Reasoning-to-Enhance-LLM-Recommendation" class="headerlink" title="Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance   LLM Recommendation"></a>Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance   LLM Recommendation</h2><p><strong>Authors:Keyu Zhao, Fengli Xu, Yong Li</strong></p>
<p>Driven by advances in Large Language Models (LLMs), integrating them into recommendation tasks has gained interest due to their strong semantic understanding and prompt flexibility. Prior work encoded user-item interactions or metadata into prompts for recommendations. In parallel, LLM reasoning, boosted by test-time scaling and reinforcement learning, has excelled in fields like mathematics and code, where reasoning traces and correctness signals are clear, enabling high performance and interpretability. However, directly applying these reasoning methods to recommendation is ineffective because user feedback is implicit and lacks reasoning supervision. To address this, we propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that samples interaction chains from the user-item graph and converts them into structured interaction-of-thoughts via a progressive masked prompting strategy, with each thought representing stepwise reasoning grounded in interaction context. This allows LLMs to simulate step-by-step decision-making based on implicit patterns. We design a two-stage training pipeline: supervised fine-tuning teaches basic reasoning from high-quality traces, and reinforcement learning refines reasoning via reward signals, alleviating sparse explicit supervision. Experiments on three real-world datasets show R2Rec outperforms classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore, the explicit reasoning chains enhance interpretability by revealing the decision process. Our code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2Rec-7C5D">https://anonymous.4open.science/r/R2Rec-7C5D</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå°†å…¶é›†æˆåˆ°æ¨èä»»åŠ¡ä¸­å·²å¼•èµ·å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œæç¤ºçµæ´»æ€§ã€‚æ—©æœŸå·¥ä½œå°†ç”¨æˆ·-é¡¹ç›®äº’åŠ¨æˆ–å…ƒæ•°æ®ç¼–ç ä¸ºæ¨èæç¤ºã€‚ä¸æ­¤åŒæ—¶ï¼Œå€ŸåŠ©æµ‹è¯•æ—¶ç¼©æ”¾å’Œå¼ºåŒ–å­¦ä¹ ï¼ŒLLMæ¨ç†åœ¨æ•°å­¦å’Œä»£ç ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œè¿™äº›é¢†åŸŸçš„æ¨ç†è½¨è¿¹å’Œæ­£ç¡®æ€§ä¿¡å·æ¸…æ™°å¯è§ï¼Œèƒ½å¤Ÿå®ç°é«˜æ€§èƒ½å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œç›´æ¥å°†è¿™äº›æ¨ç†æ–¹æ³•åº”ç”¨äºæ¨èæ˜¯æ— æ•ˆçš„ï¼Œå› ä¸ºç”¨æˆ·åé¦ˆæ˜¯éšå¼çš„ï¼Œç¼ºä¹æ¨ç†ç›‘ç£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05069v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ï¼Œå°†å…¶æ•´åˆåˆ°æ¨èä»»åŠ¡ä¸­å·²å¼•èµ·å…³æ³¨ã€‚è¿‡å»çš„å·¥ä½œå°†ç”¨æˆ·-é¡¹ç›®äº’åŠ¨æˆ–å…ƒæ•°æ®ç¼–ç åˆ°æç¤ºä¸­ä»¥ä¸ºæ¨èæä¾›æ”¯æŒã€‚åŒæ—¶ï¼ŒLLMæ¨ç†åœ¨æµ‹è¯•æ—¶é€šè¿‡æ¯”ä¾‹å°ºè°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ å¾—åˆ°åŠ å¼ºï¼Œåœ¨æ•°å­¦å’Œä»£ç ç­‰é¢†åŸŸè¡¨ç°å“è¶Šï¼Œè¿™äº›é¢†åŸŸçš„æ¨ç†ç—•è¿¹å’Œæ­£ç¡®æ€§ä¿¡å·æ¸…æ™°ã€‚ç„¶è€Œï¼Œç›´æ¥å°†è¿™äº›æ¨ç†æ–¹æ³•åº”ç”¨äºæ¨èæ˜¯ä¸æœ‰æ•ˆçš„ï¼Œå› ä¸ºç”¨æˆ·åé¦ˆæ˜¯éšå¼çš„ï¼Œç¼ºä¹æ¨ç†ç›‘ç£ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§å¢å¼ºæ¨ç†çš„æ¨èæ¡†æ¶R2Recï¼Œä»ç”¨æˆ·-é¡¹ç›®å›¾ä¸­é‡‡æ ·äº’åŠ¨é“¾ï¼Œå¹¶é€šè¿‡æ¸è¿›å¼é®ç½©æç¤ºç­–ç•¥å°†å…¶è½¬åŒ–ä¸ºç»“æ„åŒ–æ€ç»´ã€‚æ¯ä¸ªæ€ç»´ä»£è¡¨åŸºäºäº’åŠ¨ä¸Šä¸‹æ–‡çš„é€æ­¥æ¨ç†ã€‚è¿™å…è®¸LLMsæ¨¡æ‹ŸåŸºäºéšå¼æ¨¡å¼çš„é€æ­¥å†³ç­–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šç›‘ç£å¾®è°ƒä»é«˜è´¨é‡è½¨è¿¹ä¸­å­¦ä¹ åŸºæœ¬æ¨ç†ï¼Œå¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨ç†ï¼Œç¼“è§£ç¨€ç–çš„æ˜¾å¼ç›‘ç£ã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒR2Recåœ¨HitRatio@1æŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†10.48%ï¼Œåœ¨åŸå§‹LLMåŸºç¡€ä¸Šæé«˜äº†131.81%ã€‚æ­¤å¤–ï¼Œæ˜ç¡®çš„æ¨ç†é“¾é€šè¿‡æ­ç¤ºå†³ç­–è¿‡ç¨‹å¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2Rec-7C5D">åŒ¿åé“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨èä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œå› å…¶å¼ºå¤§çš„è¯­ä¹‰ç†è§£å’Œæç¤ºçµæ´»æ€§ã€‚</li>
<li>ç”¨æˆ·åé¦ˆçš„éšå¼æ€§å’Œç¼ºä¹æ¨ç†ç›‘ç£æ˜¯ç›´æ¥å°†LLMåº”ç”¨äºæ¨èçš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>R2Recæ¡†æ¶æ—¨åœ¨é€šè¿‡é‡‡æ ·äº’åŠ¨é“¾å¹¶å°†å…¶è½¬åŒ–ä¸ºç»“æ„åŒ–æ€ç»´æ¥å¢å¼ºæ¨ç†ï¼Œä½¿LLMsèƒ½å¤Ÿæ¨¡æ‹ŸåŸºäºéšå¼æ¨¡å¼çš„é€æ­¥å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>R2Recé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒç”¨äºå­¦ä¹ åŸºæœ¬æ¨ç†å’Œå¼ºåŒ–å­¦ä¹ ä»¥é€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜R2Recåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿå’ŒLLMåŸºçº¿æ–¹æ³•ã€‚</li>
<li>R2Recæé«˜äº†æ¨èçš„è§£é‡Šæ€§ï¼Œé€šè¿‡æ­ç¤ºæ¨ç†é“¾æ˜¾ç¤ºå†³ç­–è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75d96050ade5d95c15f45b9ccf3de6f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72bde7e4a204171837b240d008eb8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10fc519bb44d8ceb84f07b7987d6c274.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4408501d434c99080d4589f583aaaaa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mathematical-Reasoning-for-Unmanned-Aerial-Vehicles-A-RAG-Based-Approach-for-Complex-Arithmetic-Reasoning"><a href="#Mathematical-Reasoning-for-Unmanned-Aerial-Vehicles-A-RAG-Based-Approach-for-Complex-Arithmetic-Reasoning" class="headerlink" title="Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based   Approach for Complex Arithmetic Reasoning"></a>Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based   Approach for Complex Arithmetic Reasoning</h2><p><strong>Authors:Mehdi Azarafza, Mojtaba Nayyeri, Faezeh Pasandideh, Steffen Staab, Achim Rettberg</strong></p>
<p>Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1&#x2F;Turbo, Llama-3.2&#x2F;3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem set comprising 20 UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5% with RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, question and answer are publicly available. </p>
<blockquote>
<p>è‡ªä¸»æ— äººæœºçš„æ“ä½œéœ€è¦è¿›è¡Œå¯é çš„æ•°å­¦æ¨ç†ï¼Œä»¥å®Œæˆè½¨è¿¹è§„åˆ’å’Œç”µæºç®¡ç†ç­‰ä»»åŠ¡ã€‚è™½ç„¶ä¼ ç»Ÿçš„é£è¡Œæ§åˆ¶ä¾èµ–äºç¡¬ç¼–ç çš„æ–¹ç¨‹ï¼Œä½†æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ›´çµæ´»çš„è§£å†³é—®é¢˜æä¾›äº†æ½œåŠ›ï¼Œä½†åœ¨å¯é é€‰æ‹©å’Œåº”ç”¨æ­£ç¡®çš„æ•°å­¦å…¬å¼ä»¥åŠæ‰§è¡Œç²¾ç¡®çš„å¤šæ­¥éª¤ç®—æœ¯æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†RAG-UAVï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºæ£€ç´¢ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æä¾›ç›¸å…³çš„é¢†åŸŸæ–‡çŒ®ï¼Œæé«˜å¤šç§LLMï¼ˆåŒ…æ‹¬GPT o1&#x2F;Turboã€Llama 3.2&#x2F;3.3ã€Mistralå’ŒDeepSeek R1ï¼‰åœ¨æ— äººæœºç‰¹å®šä¸Šä¸‹æ–‡ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è¿›è¡Œåˆæ­¥è¯„ä¼°ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UAV-Math-Benchï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«20ä¸ªä»¥æ— äººæœºä¸ºä¸­å¿ƒçš„æ•°å­¦é—®é¢˜çš„å°é—®é¢˜é›†ï¼Œåˆ†ä¸ºå››ä¸ªéš¾åº¦çº§åˆ«ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŠ å…¥æ£€ç´¢åŠŸèƒ½å¯ä»¥æ˜¾è‘—æé«˜ç²¾ç¡®ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼ˆåœ¨o1çš„æƒ…å†µä¸‹è¾¾åˆ°äº†75%ï¼‰ï¼Œå‡å°‘äº†é”™è¯¯å…¬å¼é€‰æ‹©çš„æƒ…å†µï¼ˆä»æ²¡æœ‰RAGæ—¶çš„25%å‡å°‘åˆ°ä½¿ç”¨RAGæ—¶çš„5%ï¼‰ï¼Œå‡å°‘äº†æ•°å€¼é”™è¯¯ï¼Œå¯¹äºè¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰é™ä½äº†å¤šä¸ªæ•°é‡çº§ã€‚è¿™é¡¹åˆæ­¥ç ”ç©¶è¡¨æ˜ï¼ŒRAGå¯ä»¥ä½¿é€šç”¨LLMæ›´åŠ å¯é åœ°ç”¨äºå·¥ç¨‹åˆ†æï¼Œä½†ç›´æ¥çš„å®æ—¶é£è¡Œæ§åˆ¶éœ€è¦è¿›ä¸€æ­¥çš„å¤§è§„æ¨¡ç ”ç©¶å’ŒéªŒè¯ã€‚æ‰€æœ‰åŸºå‡†æ•°æ®ã€é—®é¢˜å’Œç­”æ¡ˆéƒ½æ˜¯å…¬å¼€å¯ç”¨çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04998v1">PDF</a> 15 pages, 7 figures, 4 appendix subsections</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç« æ¢è®¨äº†è‡ªä¸»æ— äººæœºæ“ä½œå¯¹æ•°å­¦æ¨ç†çš„å¯é æ€§éœ€æ±‚ï¼Œå¹¶æŒ‡å‡ºäº†ä¼ ç»Ÿé£è¡Œæ§åˆ¶æ–¹æ³•ä¸ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºRAG-UAVçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æä¾›ç›¸å…³é¢†åŸŸæ–‡çŒ®çš„æ£€ç´¢ï¼Œæ—¨åœ¨æé«˜LLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºåˆæ­¥è¯„ä¼°è¯¥æ¡†æ¶çš„æ•ˆæœï¼Œæ–‡ç« è¿˜è®¾è®¡äº†ä¸€ä¸ªåŒ…å«20ä¸ªæ— äººæœºä¸­å¿ƒæ•°å­¦é—®é¢˜çš„UAV-Math-Benché—®é¢˜é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆæ£€ç´¢æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜ç²¾ç¡®ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå‡å°‘é”™è¯¯çš„å…¬å¼é€‰æ‹©ï¼Œé™ä½æ•°å€¼è¯¯å·®ã€‚å°½ç®¡è¿˜éœ€è¦åœ¨æ›´å¤§è§„æ¨¡ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’ŒéªŒè¯ï¼Œä½†è¿™é¡¹åˆæ­¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRAGå¯ä»¥ä½¿é€šç”¨LLMæ›´å¯é åœ°ç”¨äºå·¥ç¨‹åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»æ— äººæœºæ“ä½œéœ€è¦å¯é çš„æ•°å­¦æ¨ç†æ¥å®Œæˆä»»åŠ¡ï¼Œå¦‚è½¨è¿¹è§„åˆ’å’Œç”µæºç®¡ç†ã€‚</li>
<li>ä¼ ç»Ÿé£è¡Œæ§åˆ¶ä¾èµ–ç¡¬ç¼–ç æ–¹ç¨‹ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›æ›´ä¸ºçµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LLMåœ¨é€‰æ‹©å’Œåº”ç”¨æ­£ç¡®çš„æ•°å­¦å…¬å¼ä»¥åŠæ‰§è¡Œç²¾ç¡®çš„å¤šæ­¥ç®—æœ¯è¿ç®—æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>RAG-UAVæ¡†æ¶æ—¨åœ¨é€šè¿‡æä¾›ç›¸å…³é¢†åŸŸæ–‡çŒ®çš„æ£€ç´¢ï¼Œæé«˜LLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>UAV-Math-Benché—®é¢˜é›†ç”¨äºåˆæ­¥è¯„ä¼°RAG-UAVçš„æ•ˆæœï¼ŒåŒ…å«20ä¸ªéš¾åº¦ä¸åŒçš„æ— äººæœºä¸­å¿ƒæ•°å­¦é—®é¢˜ã€‚</li>
<li>ç»“åˆæ£€ç´¢æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜ç²¾ç¡®ç­”æ¡ˆçš„å‡†ç¡®æ€§ï¼Œå‡å°‘é”™è¯¯çš„å…¬å¼é€‰æ‹©å’Œæ•°å€¼è¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bff509a6297ca62456f6ab69d97c9b7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fce262e74d9aa9cad09dc64c99a5bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35801ea526d023ba78ee2ed6c03531d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dff3de08a62ded728053e424b81c3dd0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Objects-to-Anywhere-A-Holistic-Benchmark-for-Multi-level-Visual-Grounding-in-3D-Scenes"><a href="#From-Objects-to-Anywhere-A-Holistic-Benchmark-for-Multi-level-Visual-Grounding-in-3D-Scenes" class="headerlink" title="From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual   Grounding in 3D Scenes"></a>From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual   Grounding in 3D Scenes</h2><p><strong>Authors:Tianxu Wang, Zhuofan Zhang, Ziyu Zhu, Yue Fan, Jing Xiong, Pengxiang Li, Xiaojian Ma, Qing Li</strong></p>
<p>3D visual grounding has made notable progress in localizing objects within complex 3D scenes. However, grounding referring expressions beyond objects in 3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a holistic 3D visual grounding benchmark consisting of 2,632 referring expression-3D bounding box pairs spanning four different grounding levels: human-activity areas, unoccupied space beyond objects, objects in the scene, and fine-grained object parts. We assess a range of state-of-the-art 3D visual grounding methods alongside large language models (LLMs) and multimodal LLMs (MLLMs) on Anywhere3D-Bench. Experimental results reveal that space-level and part-level visual grounding pose the greatest challenges: space-level tasks require a more comprehensive spatial reasoning ability, for example, modeling distances and spatial relations within 3D space, while part-level tasks demand fine-grained perception of object composition. Even the best performance model, OpenAI o4-mini, achieves only 23.57% accuracy on space-level tasks and 33.94% on part-level tasks, significantly lower than its performance on area-level and object-level tasks. These findings underscore a critical gap in current modelsâ€™ capacity to understand and reason about 3D scene beyond object-level semantics. </p>
<blockquote>
<p>åœ¨å¤æ‚çš„3Dåœºæ™¯ä¸­å®šä½ç‰©ä½“æ–¹é¢ï¼Œ3Dè§†è§‰å®šä½å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨3Dåœºæ™¯ä¸­å®šä½å¯¹è±¡ä¹‹å¤–çš„å‚ç…§è¡¨è¾¾ä»ç„¶æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Anywhere3D-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„3Dè§†è§‰å®šä½åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«2632ä¸ªå‚ç…§è¡¨è¾¾-3Dè¾¹ç•Œæ¡†å¯¹ï¼Œè·¨è¶Šå››ä¸ªä¸åŒçš„å®šä½çº§åˆ«ï¼šäººç±»æ´»åŠ¨åŒºåŸŸã€å¯¹è±¡ä¹‹å¤–çš„æœªå ç”¨ç©ºé—´ã€åœºæ™¯ä¸­çš„å¯¹è±¡ä»¥åŠç²¾ç»†ç²’åº¦çš„å¯¹è±¡éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨Anywhere3D-Benchä¸Šè¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„3Dè§†è§‰å®šä½æ–¹æ³•ï¼Œä»¥åŠä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€LLMsï¼ˆMLLMsï¼‰çš„ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç©ºé—´çº§åˆ«å’Œéƒ¨ä»¶çº§åˆ«çš„è§†è§‰å®šä½æ„æˆäº†æœ€å¤§çš„æŒ‘æˆ˜ï¼šç©ºé—´çº§åˆ«çš„ä»»åŠ¡éœ€è¦æ›´å…¨é¢çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚å»ºæ¨¡è·ç¦»å’Œä¸‰ç»´ç©ºé—´å†…çš„ç©ºé—´å…³ç³»ï¼Œè€Œéƒ¨ä»¶çº§åˆ«çš„ä»»åŠ¡åˆ™è¦æ±‚å¯¹å¯¹è±¡ç»„æˆè¿›è¡Œç²¾ç»†çš„æ„ŸçŸ¥ã€‚å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹OpenAI o4-miniåœ¨ç©ºé—´çº§åˆ«ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰23.57%ï¼Œåœ¨éƒ¨ä»¶çº§åˆ«ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸º33.94%ï¼Œè¿œä½äºå…¶åœ¨åŒºåŸŸçº§åˆ«å’Œå¯¹è±¡çº§åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å½“å‰æ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†è¶…å‡ºå¯¹è±¡çº§åˆ«è¯­ä¹‰çš„3Dåœºæ™¯æ–¹é¢çš„èƒ½åŠ›ä¸Šå­˜åœ¨å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04897v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨å¤æ‚çš„ä¸‰ç»´åœºæ™¯ä¸­å®šä½ç‰©ä½“ï¼Œä¸‰ç»´è§†è§‰å®šä½æŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹å…¨æ–°çš„æŒ‘æˆ˜â€”â€”è¶…è¶Šç‰©ä½“å±‚é¢çš„ä¸‰ç»´åœºæ™¯å‚ç…§è¡¨è¾¾å®šä½ã€‚æˆ‘ä»¬æ¨å‡ºäº†Anywhere3D-BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«2632ä¸ªå‚ç…§è¡¨è¾¾ä¸ä¸‰ç»´è¾¹ç•Œæ¡†é…å¯¹ï¼Œè¦†ç›–å››ä¸ªä¸åŒçš„å®šä½çº§åˆ«ã€‚è¯„ä¼°ç»“æœæŒ‡å‡ºï¼Œç©ºé—´çº§åˆ«å’Œéƒ¨ä»¶çº§åˆ«çš„è§†è§‰å®šä½æœ€å…·æŒ‘æˆ˜æ€§ã€‚ç©ºé—´çº§åˆ«çš„ä»»åŠ¡éœ€è¦å…¨é¢çš„ç©ºé—´æ¨ç†èƒ½åŠ›ï¼Œå¦‚å»ºæ¨¡è·ç¦»å’Œç©ºé—´å…³ç³»ï¼›éƒ¨ä»¶çº§åˆ«çš„ä»»åŠ¡åˆ™éœ€è¦ç²¾ç»†çš„ç‰©ä½“æ„æˆæ„ŸçŸ¥ã€‚å³ä½¿è¡¨ç°æœ€ä½³çš„OpenAI o4-miniæ¨¡å‹åœ¨ç©ºé—´çº§åˆ«å’Œéƒ¨ä»¶çº§åˆ«çš„ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰23.57%å’Œ33.94%ï¼Œæ˜æ˜¾ä½äºå…¶åœ¨åŒºåŸŸçº§åˆ«å’Œç‰©ä½“çº§åˆ«çš„è¡¨ç°ã€‚è¿™çªæ˜¾äº†å½“å‰æ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†è¶…è¶Šç‰©ä½“å±‚é¢çš„ä¸‰ç»´åœºæ™¯æ–¹é¢çš„å·¨å¤§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dè§†è§‰å®šä½æŠ€æœ¯åœ¨ç‰©ä½“å®šä½æ–¹é¢å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¶…è¶Šç‰©ä½“å±‚é¢çš„ä¸‰ç»´åœºæ™¯å‚ç…§è¡¨è¾¾å®šä½ä»å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>Anywhere3D-Benchæ˜¯ä¸€ä¸ªå…¨æ–°çš„ä¸‰ç»´è§†è§‰å®šä½åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«å¤šç§ä¸åŒçº§åˆ«çš„å®šä½ä»»åŠ¡ã€‚</li>
<li>ç©ºé—´çº§åˆ«å’Œéƒ¨ä»¶çº§åˆ«çš„è§†è§‰å®šä½ä»»åŠ¡æœ€å…·æŒ‘æˆ˜æ€§ï¼Œéœ€è¦é«˜çº§çš„ç©ºé—´æ¨ç†å’Œç²¾ç»†ç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ç†è§£å’Œæ¨ç†è¶…è¶Šç‰©ä½“å±‚é¢çš„ä¸‰ç»´åœºæ™¯æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œåœ¨ç©ºé—´çº§åˆ«å’Œéƒ¨ä»¶çº§åˆ«çš„ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿè¾ƒä½ã€‚</li>
<li>è¿™ä¸€ç ”ç©¶å¼ºè°ƒäº†æœªæ¥æ¨¡å‹éœ€è¦æå‡çš„ç©ºé—´æ¨ç†å’Œç²¾ç»†ç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-158f3cdfecadc5cef902a7cb7cac580a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24d762b104491c5082deae1cac1be995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5e019e5dd0bfbdc21beaf68ea8d6ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d6ef1054f46d7fbbd3299b0010acb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b9d2704ae70c233e1fcc09b2ae1066f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ICPC-Eval-Probing-the-Frontiers-of-LLM-Reasoning-with-Competitive-Programming-Contests"><a href="#ICPC-Eval-Probing-the-Frontiers-of-LLM-Reasoning-with-Competitive-Programming-Contests" class="headerlink" title="ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive   Programming Contests"></a>ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive   Programming Contests</h2><p><strong>Authors:Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a> </p>
<blockquote>
<p>éšç€å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚ç¼–ç å’Œæ¨ç†ä»»åŠ¡æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼ˆå¦‚LiveCodeBenchå’ŒCodeEloï¼‰ä¸è¶³ä»¥åœ¨çœŸå®ç«äº‰ç¯å¢ƒä¸­è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼–ç èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç›®å‰çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚Pass@Kï¼‰æ— æ³•æ•æ‰åˆ°æ¨ç†æ¨¡å‹çš„åå°„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>ICPC-Eval</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé¡¶çº§ç«æŠ€ç¼–ç åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¢ç´¢LLMæ¨ç†çš„å‰æ²¿ã€‚ICPC-EvalåŒ…å«äº†118ä¸ªç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ¥è‡ªè¿‘11å¹´æ¥åœ¨ä¸–ç•Œå„åœ°ä¸¾åŠçš„ICPCæ¯”èµ›ï¼Œæä¾›äº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼š1ï¼‰æ¨¡æ‹Ÿç°å®çš„ICPCç«èµ›åœºæ™¯ï¼Œå…¶é—®é¢˜ç±»å‹å’Œéš¾åº¦åˆ†å¸ƒä¸çœŸå®ç«èµ›ä¸€è‡´ï¼›2ï¼‰ä¸€ç§ç¨³å¥çš„æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆæ–¹æ³•å’Œç›¸åº”çš„æœ¬åœ°è¯„ä¼°å·¥å…·åŒ…ï¼Œå¯å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„æœ¬åœ°è¯„ä¼°ï¼›3ï¼‰ä¸€ä¸ªæœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾è¯„ä¼°æŒ‡æ ‡Refine@Kï¼Œå®ƒå…è®¸æ ¹æ®æ‰§è¡Œåé¦ˆè¿›è¡Œè§£å†³æ–¹æ¡ˆçš„è¿­ä»£ä¿®å¤ã€‚ç»“æœå¼ºè°ƒè¯„ä¼°å¤æ‚æ¨ç†èƒ½åŠ›æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼šé¡¶çº§æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰åœ¨ä¸éæ¨ç†æ¨¡å‹ç›¸æ¯”æ—¶ï¼Œé€šå¸¸éœ€è¦å¤šæ¬¡ä»£ç åé¦ˆæ¥å……åˆ†å‘æŒ¥å…¶ä¸Šä¸‹æ–‡æ¨ç†æ½œåŠ›ã€‚å°½ç®¡ä»£ç ç”Ÿæˆæ–¹é¢æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶è½åäºé¡¶çº§äººç±»å›¢é˜Ÿã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%AD%A4%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E3%80%82">https://github.com/RUCAIBox/Slow_Thinking_with_LLMsä¸Šå‘å¸ƒäº†æ­¤åŸºå‡†æµ‹è¯•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤æ‚ç¼–ç å’Œæ¨ç†ä»»åŠ¡æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†å¦‚LiveCodeBenchå’ŒCodeEloä¸è¶³ä»¥åœ¨çœŸå®ç«èµ›ç¯å¢ƒä¸­è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼–ç èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå½“å‰çš„è¯„ä¼°æŒ‡æ ‡å¦‚Pass@Kæ— æ³•æ•æ‰æ¨ç†æ¨¡å‹çš„åæ€èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºICPC-Evalè¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«æ¥è‡ªä¸–ç•Œå„åœ°è¿‘æœŸä¸¾åŠçš„ICPCç«èµ›ä¸­çš„ç²¾å¿ƒæŒ‘é€‰çš„118ä¸ªé—®é¢˜ï¼Œæä¾›ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šä¸€æ˜¯æ¨¡æ‹ŸçœŸå®çš„ICPCç«èµ›åœºæ™¯ï¼›äºŒæ˜¯æä¾›ç¨³å¥çš„æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆæ–¹æ³•å’Œç›¸åº”çš„æœ¬åœ°è¯„ä¼°å·¥å…·åŒ…ï¼›ä¸‰æ˜¯æ¨å‡ºæœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾è¯„ä¼°æŒ‡æ ‡Refine@Kï¼Œå…è®¸åŸºäºæ‰§è¡Œåé¦ˆè¿›è¡Œè§£å†³æ–¹æ¡ˆçš„è¿­ä»£ä¿®å¤ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯„ä¼°å¤æ‚æ¨ç†èƒ½åŠ›å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œé¡¶çº§æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1åœ¨å¤šè½®ä»£ç åé¦ˆæ–¹é¢çš„ä¾èµ–åº¦é«˜ï¼Œä¸”åœ¨ä»£ç ç”Ÿæˆæ–¹é¢ä»éœ€èµ¶è¶…é¡¶å°–äººç±»å›¢é˜Ÿã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ç¼–ç å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„è¿›å±•æ˜¾è‘—ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†ä¸è¶³ä»¥åœ¨çœŸå®ç«èµ›ç¯å¢ƒä¸­å…¨é¢è¯„ä»·å…¶æ€§èƒ½ã€‚</li>
<li>ICPC-Evalæ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†ï¼Œæ¨¡æ‹ŸçœŸå®çš„ICPCç«èµ›åœºæ™¯ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ICPC-EvalåŒ…å«æ¥è‡ªä¸–ç•Œå„åœ°è¿‘æœŸICPCç«èµ›çš„é—®é¢˜ï¼Œæä¾›ç¨³å¥çš„æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆå’Œæœ¬åœ°è¯„ä¼°å·¥å…·åŒ…ã€‚</li>
<li>æ¨å‡ºæ–°çš„è¯„ä¼°æŒ‡æ ‡Refine@Kï¼Œå…è®¸åŸºäºæ‰§è¡Œåé¦ˆè¿›è¡Œè§£å†³æ–¹æ¡ˆçš„è¿­ä»£ä¿®å¤ã€‚</li>
<li>é¡¶çº§æ¨ç†æ¨¡å‹å¦‚DeepSeek-R1ä¾èµ–å¤šè½®ä»£ç åé¦ˆæ¥å……åˆ†å‘æŒ¥å…¶æ¨ç†æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-57cb1519d1a29a166136659236eb8506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd34d81ea122abf780291ff20bdd7d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c124707f584f9566dc35195070ea8c77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66f41f14ad042f410f5875fd5bcfa65e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee08eb41910023cc948ea2c5efc89d8f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Joint-Evaluation-of-Answer-and-Reasoning-Consistency-for-Hallucination-Detection-in-Large-Reasoning-Models"><a href="#Joint-Evaluation-of-Answer-and-Reasoning-Consistency-for-Hallucination-Detection-in-Large-Reasoning-Models" class="headerlink" title="Joint Evaluation of Answer and Reasoning Consistency for Hallucination   Detection in Large Reasoning Models"></a>Joint Evaluation of Answer and Reasoning Consistency for Hallucination   Detection in Large Reasoning Models</h2><p><strong>Authors:Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu</strong></p>
<p>Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, making them a new source of hallucination that is difficult to detect. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the modelâ€™s reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the modelâ€™s decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. This joint analysis enables fine-grained hallucination detection even when the final answer appears correct. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/bebr2/RACE">https://github.com/bebr2/RACE</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡æ˜ç¡®çš„ã€å¤šæ­¥éª¤çš„æ¨ç†è½¨è¿¹æ‰©å±•äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œä»è€Œå¢å¼ºäº†åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„é€æ˜åº¦å’Œæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†è½¨è¿¹å¯èƒ½æ˜¯å†—ä½™æˆ–é€»è¾‘ä¸ä¸€è‡´çš„ï¼Œæˆä¸ºéš¾ä»¥æ£€æµ‹çš„ä¸€ç§æ–°çš„å¹»è§‰æ¥æºã€‚ç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨ç­”æ¡ˆçº§åˆ«çš„ä¸ç¡®å®šæ€§ï¼Œå¾€å¾€æ— æ³•æ£€æµ‹ç”±æ¨¡å‹æ¨ç†è½¨è¿¹äº§ç”Ÿçš„å¹»è§‰æˆ–é€»è¾‘ä¸ä¸€è‡´ã€‚è¿™ç§ç–å¿½å¯¹äºLRMæ¥è¯´å°¤ä¸ºä¸¥é‡ï¼Œå…¶ä¸­æ˜ç¡®çš„æ€ç»´è½¨è¿¹ä¸ä»…æ˜¯æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„é‡è¦æ”¯æŒï¼Œä¹Ÿæ˜¯æ½œåœ¨å¹»è§‰çš„å…³é”®æ¥æºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹LRMä¸­å¹»è§‰æ£€æµ‹çš„RACEï¼ˆæ¨ç†ä¸ç­”æ¡ˆä¸€è‡´æ€§è¯„ä¼°ï¼‰æ–°æ¡†æ¶ã€‚RACEé€šè¿‡æå–å…³é”®çš„æ¨ç†æ­¥éª¤å¹¶è®¡ç®—å››ä¸ªè¯Šæ–­ä¿¡å·ï¼šæ¨ç†è½¨è¿¹çš„æ ·æœ¬é—´ä¸€è‡´æ€§ã€åŸºäºç†µçš„ç­”æ¡ˆä¸ç¡®å®šæ€§ã€æ¨ç†ä¸ç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰å¯¹é½ä»¥åŠæ¨ç†çš„å†…éƒ¨è¿è´¯æ€§ã€‚è¿™ç§è”åˆåˆ†æå³ä½¿åœ¨æœ€ç»ˆç­”æ¡ˆçœ‹ä¼¼æ­£ç¡®çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å®ç°ç²¾ç»†çš„å¹»è§‰æ£€æµ‹ã€‚åœ¨ä¸åŒæ•°æ®é›†å’Œä¸åŒLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRACEä¼˜äºç°æœ‰çš„å¹»è§‰æ£€æµ‹åŸºçº¿ï¼Œä¸ºè¯„ä¼°LRMæä¾›äº†ç¨³å¥å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/bebr2/RACE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bebr2/RACEè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04832v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼Œæ—¨åœ¨æé«˜å¤æ‚ä»»åŠ¡çš„é€æ˜åº¦å’Œæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†è½¨è¿¹å¯èƒ½å†—ä½™æˆ–é€»è¾‘ä¸ä¸€è‡´ï¼Œæˆä¸ºæ–°çš„éš¾ä»¥æ£€æµ‹çš„å¹»æƒ³æ¥æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹LRMä¸­å¹»è§‰æ£€æµ‹çš„æ–°æ¡†æ¶â€”â€”RACEï¼ˆæ¨ç†ä¸ç­”æ¡ˆä¸€è‡´æ€§è¯„ä¼°ï¼‰ã€‚RACEé€šè¿‡æå–å…³é”®æ¨ç†æ­¥éª¤å¹¶è®¡ç®—å››ç§è¯Šæ–­ä¿¡å·ï¼ˆæ¨ç†è½¨è¿¹çš„è·¨æ ·æœ¬ä¸€è‡´æ€§ã€åŸºäºç†µçš„ç­”æ¡ˆä¸ç¡®å®šæ€§ã€æ¨ç†ä¸ç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰å¯¹é½ä»¥åŠæ¨ç†çš„å†…éƒ¨è¿è´¯æ€§ï¼‰æ¥è¿›è¡Œç²¾ç»†çš„å¹»è§‰æ£€æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒRACEåœ¨æ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å¹»è§‰æ£€æµ‹åŸºçº¿ï¼Œä¸ºè¯„ä¼°LRMæä¾›äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ˜ç¡®çš„æ¨ç†æ­¥éª¤ï¼Œæ—¨åœ¨å¢å¼ºå¤æ‚ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>LRMsä¸­çš„æ¨ç†è½¨è¿¹å¯èƒ½å†—ä½™æˆ–é€»è¾‘ä¸ä¸€è‡´ï¼Œæˆä¸ºæ–°çš„å¹»è§‰æ¥æºã€‚</li>
<li>ç°æœ‰å¹»è§‰æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨ç­”æ¡ˆçº§åˆ«çš„ä¸ç¡®å®šæ€§ï¼Œå¾€å¾€æ— æ³•æ£€æµ‹åˆ°ç”±æ¨¡å‹æ¨ç†è½¨è¿¹äº§ç”Ÿçš„å¹»è§‰æˆ–é€»è¾‘ä¸ä¸€è‡´ã€‚</li>
<li>RACEæ¡†æ¶é€šè¿‡æå–å…³é”®æ¨ç†æ­¥éª¤å’Œè®¡ç®—å››ç§è¯Šæ–­ä¿¡å·è¿›è¡Œç²¾ç»†çš„å¹»è§‰æ£€æµ‹ã€‚</li>
<li>RACEæ¡†æ¶åŒ…æ‹¬è·¨æ ·æœ¬çš„æ¨ç†è½¨è¿¹ä¸€è‡´æ€§ã€ç­”æ¡ˆçš„ä¸ç¡®å®šæ€§ã€æ¨ç†ä¸ç­”æ¡ˆçš„è¯­ä¹‰å¯¹é½ä»¥åŠæ¨ç†çš„å†…éƒ¨è¿è´¯æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRACEåœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¹»è§‰æ£€æµ‹åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0fcd32d5fa2d99918a8a60e82dab7280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5bd5fbc4669756f09163786b0b8cef4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MMSU-A-Massive-Multi-task-Spoken-Language-Understanding-and-Reasoning-Benchmark"><a href="#MMSU-A-Massive-Multi-task-Spoken-Language-Understanding-and-Reasoning-Benchmark" class="headerlink" title="MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark"></a>MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark</h2><p><strong>Authors:Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, Helen Meng</strong></p>
<p>Speech inherently contains rich acoustic information that extends far beyond the textual language. In real-world spoken language understanding, effective interpretation often requires integrating semantic meaning (e.g., content), paralinguistic features (e.g., emotions, speed, pitch) and phonological characteristics (e.g., prosody, intonation, rhythm), which are embedded in speech. While recent multimodal Speech Large Language Models (SpeechLLMs) have demonstrated remarkable capabilities in processing audio information, their ability to perform fine-grained perception and complex reasoning in natural speech remains largely unexplored. To address this gap, we introduce MMSU, a comprehensive benchmark designed specifically for understanding and reasoning in spoken language. MMSU comprises 5,000 meticulously curated audio-question-answer triplets across 47 distinct tasks. To ground our benchmark in linguistic theory, we systematically incorporate a wide range of linguistic phenomena, including phonetics, prosody, rhetoric, syntactics, semantics, and paralinguistics. Through a rigorous evaluation of 14 advanced SpeechLLMs, we identify substantial room for improvement in existing models, highlighting meaningful directions for future optimization. MMSU establishes a new standard for comprehensive assessment of spoken language understanding, providing valuable insights for developing more sophisticated human-AI speech interaction systems. MMSU benchmark is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ddwang2000/MMSU">https://huggingface.co/datasets/ddwang2000/MMSU</a>. Evaluation Code is available at <a target="_blank" rel="noopener" href="https://github.com/dingdongwang/MMSU_Bench">https://github.com/dingdongwang/MMSU_Bench</a>. </p>
<blockquote>
<p>è¯­éŸ³æœ¬èº«åŒ…å«ä¸°å¯Œçš„å£°éŸ³ä¿¡æ¯ï¼Œè¿œè¿œè¶…å‡ºæ–‡æœ¬è¯­è¨€çš„èŒƒå›´ã€‚åœ¨ç°å®ä¸–ç•Œçš„å£è¯­ç†è§£ä¸­ï¼Œæœ‰æ•ˆçš„è§£é‡Šé€šå¸¸éœ€è¦æ•´åˆè¯­ä¹‰ï¼ˆä¾‹å¦‚å†…å®¹ï¼‰ã€å‰¯è¯­è¨€ç‰¹å¾ï¼ˆä¾‹å¦‚æƒ…æ„Ÿã€è¯­é€Ÿã€éŸ³è°ƒï¼‰å’Œè¯­éŸ³ç‰¹å¾ï¼ˆä¾‹å¦‚éŸµå¾‹ã€è¯­è°ƒã€èŠ‚å¥ï¼‰ï¼Œè¿™äº›ç‰¹å¾éƒ½åµŒå…¥åœ¨è¯­éŸ³ä¸­ã€‚è™½ç„¶æœ€è¿‘çš„å¤šåª’ä½“è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰åœ¨å¤„ç†éŸ³é¢‘ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨è‡ªç„¶è¯­éŸ³çš„ç²¾ç»†æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MMSUï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå£è¯­ç†è§£å’Œæ¨ç†è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚MMSUåŒ…å«5000ä¸ªç²¾å¿ƒæŒ‘é€‰çš„éŸ³é¢‘-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œæ¶µç›–47ä¸ªä¸åŒçš„ä»»åŠ¡ã€‚ä¸ºäº†åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­èå…¥è¯­è¨€å­¦ç†è®ºï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°èå…¥äº†ä¸€ç³»åˆ—çš„è¯­è¨€ç°è±¡ï¼ŒåŒ…æ‹¬è¯­éŸ³å­¦ã€éŸµå¾‹ã€ä¿®è¾ã€å¥æ³•ã€è¯­ä¹‰å’Œå‰¯è¯­è¨€å­¦ã€‚é€šè¿‡å¯¹14ä¸ªå…ˆè¿›çš„SpeechLLMçš„ä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°äº†ç°æœ‰æ¨¡å‹çš„æ”¹è¿›ç©ºé—´å¾ˆå¤§ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ä¼˜åŒ–çš„æœ‰æ„ä¹‰æ–¹å‘ã€‚MMSUä¸ºå£è¯­ç†è§£çš„å…¨é¢è¯„ä¼°å»ºç«‹äº†æ–°æ ‡å‡†ï¼Œä¸ºå¼€å‘æ›´å¤æ‚çš„äººç±»-äººå·¥æ™ºèƒ½è¯­éŸ³äº¤äº’ç³»ç»Ÿæä¾›äº†å®è´µçš„è§è§£ã€‚MMSUåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ddwang2000/MMSU">https://huggingface.co/datasets/ddwang2000/MMSU</a>è·å–ã€‚è¯„ä¼°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dingdongwang/MMSU_Bench">https://github.com/dingdongwang/MMSU_Bench</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04779v1">PDF</a> MMSU benchmark is available at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ddwang2000/MMSU">https://huggingface.co/datasets/ddwang2000/MMSU</a>. Evaluation Code is available   at <a target="_blank" rel="noopener" href="https://github.com/dingdongwang/MMSU_Bench">https://github.com/dingdongwang/MMSU_Bench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³åŒ…å«ä¸°å¯Œçš„å£°éŸ³ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯è¿œè¿œè¶…å‡ºæ–‡æœ¬è¯­è¨€ã€‚åœ¨ç°å®ä¸–ç•Œçš„å£è¯­ç†è§£ä¸­ï¼Œæœ‰æ•ˆçš„è§£é‡Šå¾€å¾€éœ€è¦æ•´åˆè¯­ä¹‰ï¼ˆä¾‹å¦‚å†…å®¹ï¼‰ã€å‰¯è¯­è¨€ç‰¹å¾ï¼ˆä¾‹å¦‚æƒ…æ„Ÿã€è¯­é€Ÿã€éŸ³è°ƒï¼‰å’Œè¯­éŸ³ç‰¹å¾ï¼ˆä¾‹å¦‚éŸµå¾‹ã€è¯­è°ƒã€èŠ‚å¥ï¼‰ï¼Œè¿™äº›ç‰¹å¾éƒ½åµŒå…¥åœ¨è¯­éŸ³ä¸­ã€‚ä¸ºäº†ç†è§£å’Œæ¨ç†å£è¯­ï¼Œæˆ‘ä»¬å¼•å…¥äº†MMSUï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå£è¯­ç†è§£è€Œè®¾è®¡çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚MMSUåŒ…å«5000ä¸ªç²¾å¿ƒç­–åˆ’çš„éŸ³é¢‘-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œè·¨è¶Š47ä¸ªä¸åŒçš„ä»»åŠ¡ã€‚é€šè¿‡ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œä¸ºæœªæ¥çš„ä¼˜åŒ–æŒ‡æ˜äº†æœ‰æ„ä¹‰çš„æ–¹å‘ã€‚MMSUä¸ºå…¨é¢è¯„ä¼°å£è¯­ç†è§£å»ºç«‹äº†æ–°çš„æ ‡å‡†ï¼Œä¸ºå¼€å‘æ›´å¤æ‚çš„äººæœºè¯­éŸ³äº¤äº’ç³»ç»Ÿæä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åŒ…å«ä¸°å¯Œçš„å£°éŸ³ä¿¡æ¯ï¼Œè¶…è¶Šæ–‡æœ¬è¯­è¨€ã€‚</li>
<li>æœ‰æ•ˆçš„å£è¯­ç†è§£éœ€è¦æ•´åˆè¯­ä¹‰ã€å‰¯è¯­è¨€ç‰¹å¾å’Œè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>MMSUæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå£è¯­ç†è§£çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>MMSUåŒ…å«5000ä¸ªéŸ³é¢‘-é—®é¢˜-ç­”æ¡ˆä¸‰å…ƒç»„ï¼Œè·¨è¶Š47ä¸ªä»»åŠ¡ã€‚</li>
<li>ç°æœ‰è¯­éŸ³LLMæ¨¡å‹åœ¨ç²¾ç»†æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚</li>
<li>MMSUä¸ºå£è¯­ç†è§£å»ºç«‹äº†æ–°çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04779">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83775bd894dd31699e9bdff1eeec3af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ebe57b946e07556d641ee02d32671a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac749dfa9b08441fdb1d9b3f7aac5c48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4839781e6418b3c55cc01d1cd83a62f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fb57d31a4d5387cc09aed126635fb13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d81217bed9cda7ae4ffa963153163e6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Truth-in-the-Few-High-Value-Data-Selection-for-Efficient-Multi-Modal-Reasoning"><a href="#Truth-in-the-Few-High-Value-Data-Selection-for-Efficient-Multi-Modal-Reasoning" class="headerlink" title="Truth in the Few: High-Value Data Selection for Efficient Multi-Modal   Reasoning"></a>Truth in the Few: High-Value Data Selection for Efficient Multi-Modal   Reasoning</h2><p><strong>Authors:Shenshen Li, Kaiyuan Deng, Lei Wang, Hao Yang, Chong Peng, Peng Yan, Fumin Shen, Heng Tao Shen, Xing Xu</strong></p>
<p>While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sampleâ€™s potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Leo-ssl/RAP">https://github.com/Leo-ssl/RAP</a>. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†äººä»¬æ™®éè®¤ä¸ºï¼Œä¸ºäº†æé«˜å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè¿™ä¸å¯é¿å…åœ°å¯¼è‡´äº†æ•°æ®å†—ä½™å’Œå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œè¾ƒå°çš„é«˜ä»·å€¼æ•°æ®é›†æ˜¯å¦èƒ½åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä¸å®Œæ•´è¯­æ–™åº“ç›¸åŒ¹é…ç”šè‡³è¡¨ç°æ›´å¥½ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€é¡¹å…³é”®è§‚å¯Ÿç»“æœæ¥æŒ‘æˆ˜è¿™ä¸€å‡è®¾ï¼šæœ‰æ„ä¹‰çš„å¤šæ¨¡æ€æ¨ç†ä»…ç”±ä¸€å°éƒ¨åˆ†è®­ç»ƒæ ·æœ¬è§¦å‘ï¼Œè¿™äº›æ ·æœ¬è¢«ç§°ä¸ºè®¤çŸ¥æ ·æœ¬ï¼Œè€Œå¤§å¤šæ•°æ ·æœ¬è´¡çŒ®ç”šå¾®ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©èŒƒå¼ï¼Œç§°ä¸ºâ€œæ¨ç†æ¿€æ´»æ½œåŠ›â€ï¼ˆRAPï¼‰ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ä¼°è®¡å™¨æ¥è¯†åˆ«è®¤çŸ¥æ ·æœ¬ï¼š1ï¼‰åŸºäºæ½œåœ¨ç»“æœæ¨¡å‹åŸç†çš„å› æœå·®å¼‚ä¼°è®¡å™¨ï¼ˆCDEï¼‰ï¼Œé€šè¿‡æ¯”è¾ƒå¤šæ¨¡æ€å’Œæ–‡æœ¬è¾“å…¥çš„è¾“å‡ºï¼Œæ¶ˆé™¤è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒçš„æ ·æœ¬ï¼›2ï¼‰æ³¨æ„åŠ›ç½®ä¿¡ä¼°è®¡å™¨ï¼ˆACEï¼‰ï¼Œåˆ©ç”¨æ ‡è®°çº§åˆ«çš„è‡ªæˆ‘æ³¨æ„åŠ›æ¥ä¸¢å¼ƒåœ¨æ¨ç†é˜¶æ®µè¢«æ— å…³ä½†è¿‡åº¦å¼ºè°ƒçš„æ ‡è®°æ‰€ä¸»å¯¼çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥æ›¿æ¢æ¨¡å—ï¼ˆDRMï¼‰ï¼Œä»¥è®¤çŸ¥ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä¾‹æ›¿æ¢æ‰ç®€å•çš„å®ä¾‹ï¼Œä»è€Œç¡®ä¿å¤æ‚æ€§ä»¥å®ç°ç¨³å¥çš„å¤šæ¨¡æ€æ¨ç†ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„RAPæ–¹æ³•ä»…ä½¿ç”¨9.3%çš„è®­ç»ƒæ•°æ®å°±èƒ½æŒç»­å®ç°ä¼˜è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬è¶…è¿‡43%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Leo-ssl/RAP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Leo-ssl/RAPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‘æˆ˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­å¿…é¡»ä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®çš„ä¼ ç»Ÿè§‚å¿µã€‚é€šè¿‡å…³é”®è§‚å¯Ÿå‘ç°ï¼Œæœ‰æ„ä¹‰çš„å¤šæ¨¡æ€æ¨ç†ä»…ç”±è®­ç»ƒæ ·æœ¬ä¸­çš„ç¨€ç–å­é›†ï¼ˆç§°ä¸ºè®¤çŸ¥æ ·æœ¬ï¼‰è§¦å‘ï¼Œå…¶ä½™å¤§éƒ¨åˆ†æ ·æœ¬è´¡çŒ®ç”šå¾®ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©èŒƒå¼â€”â€”æ¨ç†æ¿€æ´»æ½œåŠ›ï¼ˆRAPï¼‰ï¼Œé€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„è¯„ä¼°å™¨æ¥è¯†åˆ«è®¤çŸ¥æ ·æœ¬ï¼š1ï¼‰å› æœå·®å¼‚è¯„ä¼°å™¨ï¼ˆCDEï¼‰ï¼ŒåŸºäºæ½œåœ¨ç»“æœæ¨¡å‹åŸç†ï¼Œé€šè¿‡æ¯”è¾ƒå¤šæ¨¡æ€å’Œæ–‡æœ¬åªæœ‰è¾“å…¥çš„è¾“å‡ºæ¥æ¶ˆé™¤è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒçš„æ ·æœ¬ï¼›2ï¼‰æ³¨æ„åŠ›ä¿¡å¿ƒè¯„ä¼°å™¨ï¼ˆACEï¼‰ï¼Œåˆ©ç”¨tokençº§åˆ«çš„è‡ªæˆ‘å…³æ³¨æ¥ä¸¢å¼ƒåœ¨ä¸­é—´æ¨ç†é˜¶æ®µè¢«æ— å…³ç´§è¦çš„tokenä¸»å¯¼çš„æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPæ–¹æ³•ä»…ä½¿ç”¨9.3%çš„è®­ç»ƒæ•°æ®å³å¯å®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬è¶…è¿‡43%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†æ™®éè®¤ä¸ºéœ€è¦å¤§é‡è®­ç»ƒæ•°æ®æ¥æé«˜å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œå¯¼è‡´æ•°æ®å†—ä½™å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æœ¬æ–‡æŒ‘æˆ˜äº†è¿™ä¸€å‡è®¾ï¼Œè®¤ä¸ºæœ‰æ„ä¹‰çš„å¤šæ¨¡æ€æ¨ç†ä»…ç”±è®­ç»ƒæ ·æœ¬ä¸­çš„ç¨€ç–å­é›†ï¼ˆè®¤çŸ¥æ ·æœ¬ï¼‰è§¦å‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©èŒƒå¼â€”â€”æ¨ç†æ¿€æ´»æ½œåŠ›ï¼ˆRAPï¼‰ï¼Œèƒ½å¤Ÿè¯†åˆ«è®¤çŸ¥æ ·æœ¬ï¼Œé€šè¿‡ä¼°è®¡æ¯ä¸ªæ ·æœ¬åˆºæ¿€çœŸå®å¤šæ¨¡æ€æ¨ç†çš„æ½œåŠ›ã€‚</li>
<li>RAPæ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥çš„è¯„ä¼°å™¨ï¼šå› æœå·®å¼‚è¯„ä¼°å™¨ï¼ˆCDEï¼‰å’Œæ³¨æ„åŠ›ä¿¡å¿ƒè¯„ä¼°å™¨ï¼ˆACEï¼‰ï¼Œåˆ†åˆ«ç”¨äºæ¯”è¾ƒå¤šæ¨¡æ€å’Œæ–‡æœ¬åªæœ‰è¾“å…¥çš„è¾“å‡ºæ¥è¯†åˆ«è¿‡åº¦ä¾èµ–è¯­è¨€å…ˆéªŒçš„æ ·æœ¬ï¼Œä»¥åŠåˆ©ç”¨tokençº§åˆ«çš„è‡ªæˆ‘å…³æ³¨æ¥è¯†åˆ«åœ¨ä¸­é—´æ¨ç†é˜¶æ®µè¢«æ— å…³ç´§è¦çš„tokenä¸»å¯¼çš„æ ·æœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥æ›¿æ¢æ¨¡å—ï¼ˆDRMï¼‰ï¼Œç”¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®ä¾‹æ›¿æ¢ç®€å•çš„å®ä¾‹ï¼Œç¡®ä¿å¤æ‚æ€§çš„å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPæ–¹æ³•ä½¿ç”¨ä»…9.3%çš„è®­ç»ƒæ•°æ®å³å¯å®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬è¶…è¿‡43%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3144c37c87389faa648add44ba7a99f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93a7cd062a7c0439a172f0a9931a13b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6737c92057f5813e2fae5df925d425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de3b7b57f62e9de8d8b0fbb46937fa0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-Layer-GRPO-Enhancing-Reasoning-and-Self-Correction-in-Large-Language-Models"><a href="#Multi-Layer-GRPO-Enhancing-Reasoning-and-Self-Correction-in-Large-Language-Models" class="headerlink" title="Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large   Language Models"></a>Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large   Language Models</h2><p><strong>Authors:Fei Ding, Baiqiao Wang, Zijian Zeng, Youwei Wang</strong></p>
<p>The Group Relative Policy Optimization (GRPO) algorithm has demonstrated considerable success in enhancing the reasoning capabilities of large language models (LLMs), as evidenced by DeepSeek-R1. However, the absence of intermediate supervision in GRPO frequently leads to inefficient exploration dynamics. A single error in a complex reasoning chain can invalidate the entire solution, resulting in abrupt reward vanishing and compromising training stability.To address these challenges, we propose MGRPO (Multi-layer GRPO). MGRPO operates in two layers: the first layer employs standard GRPO to generate an initial response. This response, along with the original query, is then fed into a second-layer GRPO process. This second layer is specifically trained to identify and correct errors in the initial response, effectively creating a self-correction loop. This mechanism provides implicit process-level supervision by rewarding successful error correction, without requiring an explicit, densely-annotated reward model. Experimental results on several mathematical reasoning benchmarks demonstrate that MGRPO significantly outperforms standard GRPO, achieving superior performance by fostering both reasoning and self-correction abilities. </p>
<blockquote>
<p>é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼ŒDeepSeek-R1å¯¹æ­¤æä¾›äº†è¯æ®ã€‚ç„¶è€Œï¼ŒGRPOä¸­ç¼ºä¹ä¸­é—´ç›‘ç£é€šå¸¸ä¼šå¯¼è‡´ä½æ•ˆçš„æ¢ç´¢åŠ¨æ€ã€‚å¤æ‚çš„æ¨ç†é“¾ä¸­çš„ä¸€ä¸ªé”™è¯¯å¯èƒ½ä¼šä½¿æ•´ä¸ªè§£å†³æ–¹æ¡ˆæ— æ•ˆï¼Œå¯¼è‡´å¥–åŠ±çªç„¶æ¶ˆå¤±ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå±‚GRPOï¼ˆMGRPOï¼‰ã€‚MGRPOæœ‰ä¸¤å±‚æ“ä½œï¼šç¬¬ä¸€å±‚é‡‡ç”¨æ ‡å‡†GRPOç”Ÿæˆåˆæ­¥å›åº”ã€‚ç„¶åï¼Œè¯¥å“åº”ä¸åŸå§‹æŸ¥è¯¢ä¸€èµ·è¾“å…¥åˆ°ç¬¬äºŒå±‚GRPOè¿‡ç¨‹ä¸­ã€‚ç¬¬äºŒå±‚ä¸“é—¨è®­ç»ƒä»¥è¯†åˆ«å’Œçº æ­£åˆæ­¥å›åº”ä¸­çš„é”™è¯¯ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ›å»ºä¸€ä¸ªè‡ªæˆ‘çº æ­£å¾ªç¯ã€‚è¯¥æœºåˆ¶é€šè¿‡å¥–åŠ±æˆåŠŸçš„é”™è¯¯çº æ­£ï¼Œæä¾›äº†éšå¼çš„æµç¨‹çº§ç›‘ç£ï¼Œè€Œæ— éœ€ä½¿ç”¨æ˜ç¡®ã€å¯†é›†æ³¨é‡Šçš„å¥–åŠ±æ¨¡å‹ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMGRPOæ˜¾è‘—ä¼˜äºæ ‡å‡†GRPOï¼Œé€šè¿‡ä¿ƒè¿›æ¨ç†å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04746v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>GRPOç®—æ³•åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ç¼ºä¹ä¸­é—´ç›‘ç£å¯¼è‡´æ¢ç´¢åŠ¨æ€æ•ˆç‡ä½ä¸‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºå¤šå±‚GRPOï¼ˆMGRPOï¼‰ã€‚MGRPOåˆ†ä¸ºä¸¤å±‚æ“ä½œï¼Œç¬¬ä¸€å±‚ä½¿ç”¨æ ‡å‡†GRPOç”Ÿæˆåˆæ­¥å›åº”ï¼Œç„¶åå°†å…¶ä¸åŸå§‹æŸ¥è¯¢ä¸€èµ·è¾“å…¥åˆ°ç¬¬äºŒå±‚GRPOè¿‡ç¨‹ä¸­è¿›è¡Œé”™è¯¯è¯†åˆ«ä¸ä¿®æ­£ï¼Œå½¢æˆè‡ªæˆ‘ä¿®æ­£å¾ªç¯ã€‚è¿™ç§æœºåˆ¶é€šè¿‡å¥–åŠ±æˆåŠŸçš„é”™è¯¯ä¿®æ­£ï¼Œæ— éœ€æ˜ç¡®çš„å¯†é›†æ³¨é‡Šå¥–åŠ±æ¨¡å‹ï¼Œå³å¯æä¾›éšå¼è¿‡ç¨‹çº§ç›‘ç£ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒMGRPOæ˜¾è‘—ä¼˜äºæ ‡å‡†GRPOï¼Œé€šè¿‡ä¿ƒè¿›æ¨ç†å’Œè‡ªçº èƒ½åŠ›å®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GRPOç®—æ³•å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹ä¸­é—´ç›‘ç£å¯¼è‡´æ¢ç´¢æ•ˆç‡ä¸é«˜ã€‚</li>
<li>MGRPOç®—æ³•é‡‡ç”¨ä¸¤å±‚æ“ä½œï¼Œç¬¬ä¸€å±‚ç”Ÿæˆåˆæ­¥å›åº”ï¼Œç¬¬äºŒå±‚è¿›è¡Œé”™è¯¯è¯†åˆ«ä¸ä¿®æ­£ã€‚</li>
<li>MGRPOé€šè¿‡è‡ªæˆ‘ä¿®æ­£å¾ªç¯ï¼Œæé«˜äº†æ¨¡å‹åº”å¯¹å¤æ‚æ¨ç†é“¾ä¸­é”™è¯¯çš„èƒ½åŠ›ã€‚</li>
<li>MGRPOé‡‡ç”¨éšå¼è¿‡ç¨‹çº§ç›‘ç£æœºåˆ¶ï¼Œé€šè¿‡å¥–åŠ±æˆåŠŸçš„é”™è¯¯ä¿®æ­£ï¼Œæ— éœ€å¯†é›†æ³¨é‡Šçš„å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>MGRPOåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†GRPOã€‚</li>
<li>MGRPOç®—æ³•ä¿ƒè¿›äº†æ¨¡å‹çš„æ¨ç†å’Œè‡ªçº èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7228a8e20a990565074c82eb131fa150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8d270cdb91e99d86bed94c46910e7e8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Empowering-Economic-Simulation-for-Massively-Multiplayer-Online-Games-through-Generative-Agent-Based-Modeling"><a href="#Empowering-Economic-Simulation-for-Massively-Multiplayer-Online-Games-through-Generative-Agent-Based-Modeling" class="headerlink" title="Empowering Economic Simulation for Massively Multiplayer Online Games   through Generative Agent-Based Modeling"></a>Empowering Economic Simulation for Massively Multiplayer Online Games   through Generative Agent-Based Modeling</h2><p><strong>Authors:Bihan Xu, Shiwei Zhao, Runze Wu, Zhenya Huang, Jiawei Wang, Zhipeng Hu, Kai Wang, Haoyu Liu, Tangjie Lv, Le Li, Changjie Fan, Xin Tong, Jiangze Han</strong></p>
<p>Within the domain of Massively Multiplayer Online (MMO) economy research, Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game economics, evolving from rule-based agents to decision-making agents enhanced by reinforcement learning. Nevertheless, existing works encounter significant challenges when attempting to emulate human-like economic activities among agents, particularly regarding agent reliability, sociability, and interpretability. In this study, we take a preliminary step in introducing a novel approach using Large Language Models (LLMs) in MMO economy simulation. Leveraging LLMsâ€™ role-playing proficiency, generative capacity, and reasoning aptitude, we design LLM-driven agents with human-like decision-making and adaptability. These agents are equipped with the abilities of role-playing, perception, memory, and reasoning, addressing the aforementioned challenges effectively. Simulation experiments focusing on in-game economic activities demonstrate that LLM-empowered agents can promote emergent phenomena like role specialization and price fluctuations in line with market rules. </p>
<blockquote>
<p>åœ¨å¤§å‹å¤šäººåœ¨çº¿ï¼ˆMMOï¼‰ç»æµç ”ç©¶é¢†åŸŸï¼ŒåŸºäºä»£ç†çš„å»ºæ¨¡ï¼ˆABMï¼‰å·²ç»æˆä¸ºåˆ†ææ¸¸æˆç»æµå­¦çš„å¼ºå¤§å·¥å…·ï¼Œä»åŸºäºè§„åˆ™çš„ä»£ç†å‘å±•åˆ°é€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„å†³ç­–ä»£ç†ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œåœ¨å°è¯•æ¨¡æ‹Ÿä»£ç†ä¹‹é—´çš„äººç±»ç»æµæ´»åŠ¨æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç†çš„å¯é æ€§ã€ç¤¾äº¤èƒ½åŠ›å’Œè§£é‡Šæ€§æ–¹é¢ã€‚æœ¬ç ”ç©¶åˆæ­¥å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒMMOç»æµæ¨¡æ‹Ÿã€‚å€ŸåŠ©LLMsçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€ç”Ÿæˆèƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…·æœ‰äººç±»å¼å†³ç­–å’Œé€‚åº”èƒ½åŠ›çš„LLMé©±åŠ¨ä»£ç†ã€‚è¿™äº›ä»£ç†å…·å¤‡è§’è‰²æ‰®æ¼”ã€æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†çš„èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¸Šè¿°æŒ‘æˆ˜ã€‚ä»¥æ¸¸æˆå†…ç»æµæ´»åŠ¨ä¸ºé‡ç‚¹çš„æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒLLMèµ‹èƒ½çš„ä»£ç†å¯ä»¥ä¿ƒè¿›è¯¸å¦‚è§’è‰²ä¸“ä¸šåŒ–å’Œä»·æ ¼æ³¢åŠ¨çš„å¸‚åœºè§„åˆ™ç›¸ç¬¦çš„æ–°å…´ç°è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04699v1">PDF</a> KDD2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹å¤šäººåœ¨çº¿ï¼ˆMMOï¼‰ç»æµç ”ç©¶é¢†åŸŸï¼ŒåŸºäºä»£ç†çš„å»ºæ¨¡ï¼ˆABMï¼‰å·²é€æ¸æˆä¸ºåˆ†ææ¸¸æˆç»æµçš„æœ‰åŠ›å·¥å…·ï¼Œä»åŸºäºè§„åˆ™çš„ä»£ç†å‘å±•åˆ°é€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºå†³ç­–èƒ½åŠ›çš„ä»£ç†ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶åœ¨å°è¯•æ¨¡æ‹Ÿä»£ç†ä¹‹é—´çš„äººç±»ç»æµæ´»åŠ¨æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç†çš„å¯é æ€§ã€ç¤¾äº¤èƒ½åŠ›å’Œå¯è§£é‡Šæ€§æ–¹é¢ã€‚æœ¬ç ”ç©¶åˆæ­¥å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºMMOç»æµæ¨¡æ‹Ÿï¼Œå€ŸåŠ©LLMçš„è§’è‰²æ‰®æ¼”èƒ½åŠ›ã€ç”Ÿæˆèƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ï¼Œè®¾è®¡å‡ºå…·æœ‰äººç±»å†³ç­–å’Œé€‚åº”èƒ½åŠ›çš„LLMé©±åŠ¨ä»£ç†ã€‚è¿™äº›ä»£ç†å…·å¤‡è§’è‰²æ‰®æ¼”ã€æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚æ¨¡æ‹Ÿå®éªŒå…³æ³¨æ¸¸æˆå†…ç»æµæ´»åŠ¨ï¼Œè¯æ˜LLMèµ‹èƒ½çš„ä»£ç†èƒ½å¤Ÿä¿ƒè¿›è§’è‰²ä¸“ä¸šåŒ–ã€ä»·æ ¼æ³¢åŠ¨ç­‰ç¬¦åˆå¸‚åœºè§„åˆ™çš„ç°è±¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agent-Based Modeling (ABM)æ˜¯åˆ†ææ¸¸æˆç»æµçš„é‡è¦å·¥å…·ï¼Œä½†å®ƒæ¨¡æ‹Ÿäººç±»ç»æµæ´»åŠ¨çš„å‡†ç¡®æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§’è‰²æ‰®æ¼”ã€ç”Ÿæˆèƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯åº”ç”¨äºMMOç»æµæ¨¡æ‹Ÿã€‚</li>
<li>LLMé©±åŠ¨çš„ä»£ç†å…·å¤‡äººç±»å†³ç­–å’Œé€‚åº”èƒ½åŠ›ï¼Œèƒ½æœ‰æ•ˆè§£å†³ABMé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ä»£ç†çš„å¯é æ€§ã€ç¤¾äº¤èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>LLMèµ‹èƒ½çš„ä»£ç†èƒ½ä¿ƒè¿›è§’è‰²ä¸“ä¸šåŒ–å’Œä»·æ ¼æ³¢åŠ¨ç­‰ç¬¦åˆå¸‚åœºè§„åˆ™çš„ç°è±¡ã€‚</li>
<li>LLMåœ¨MMOç»æµæ¨¡æ‹Ÿä¸­çš„åº”ç”¨æ˜¯åˆ›æ–°æ€§çš„å°è¯•ï¼Œä¸ºæœªæ¥æ¸¸æˆç»æµæ¨¡æ‹Ÿæä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç»“åˆäººå·¥æ™ºèƒ½ä¸æ¸¸æˆç»æµçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºä»£ç†çš„å»ºæ¨¡ç›¸ç»“åˆçš„æ–¹æ³•åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚ç¤¾ä¼šç§‘å­¦ã€ç»æµå­¦ï¼‰çš„æ¨¡æ‹Ÿä¸­ä¹Ÿå¯èƒ½å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5597fceac849f54973a291c81cd2ce9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-345ed8147158bbfcb8e5549f55c94871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4a2980f6fb752bd47f508d606fa674.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1115abba10b0aec7debaa6bb475c23fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c0d607ab82234d0592acc390c1e4b6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="On-the-Mechanism-of-Reasoning-Pattern-Selection-in-Reinforcement-Learning-for-Language-Models"><a href="#On-the-Mechanism-of-Reasoning-Pattern-Selection-in-Reinforcement-Learning-for-Language-Models" class="headerlink" title="On the Mechanism of Reasoning Pattern Selection in Reinforcement   Learning for Language Models"></a>On the Mechanism of Reasoning Pattern Selection in Reinforcement   Learning for Language Models</h2><p><strong>Authors:Xingwu Chen, Tianle Li, Difan Zou</strong></p>
<p>Reinforcement learning (RL) has demonstrated remarkable success in enhancing model capabilities, including instruction-following, preference learning, and reasoning. Yet despite its empirical successes, the mechanisms by which RL improves reasoning abilities remain poorly understood. We present a systematic study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that its primary benefit comes from optimizing the selection of existing reasoning patterns. Through extensive experiments, we demonstrate that RLVR-trained models preferentially adopt high-success-rate reasoning patterns while mostly maintaining stable performance on individual patterns. We further develop theoretical analyses on the convergence and training dynamics of RLVR based on a simplified question-reason-answer model. We study the gradient flow and show that RLVR can indeed find the solution that selects the reason pattern with the highest success rate. Besides, our theoretical results   reveal two distinct regimes regarding the convergence of RLVR training: (1) rapid convergence for models with relatively strong initial reasoning capabilities versus (2) slower optimization dynamics for weaker models. Furthermore, we show that the slower optimization for weaker models can be mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using a feasibly high-quality SFT dataset. We validate the theoretical findings through extensive experiments. This work advances our theoretical understanding of RLâ€™s role in LLM fine-tuning and offers insights for further enhancing reasoning capabilities. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æé«˜æ¨¡å‹èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€åå¥½å­¦ä¹ å’Œæ¨ç†ç­‰ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶åœ¨å®è¯ç ”ç©¶ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†RLæé«˜æ¨ç†èƒ½åŠ›çš„æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘ã€‚æˆ‘ä»¬å¯¹å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œè¡¨æ˜å…¶ä¸»è¦ä¼˜åŠ¿åœ¨äºä¼˜åŒ–ç°æœ‰æ¨ç†æ¨¡å¼çš„é€‰æ‹©ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜RLVRè®­ç»ƒçš„æ¨¡å‹å€¾å‘äºé‡‡ç”¨é«˜æˆåŠŸç‡çš„æ¨ç†æ¨¡å¼ï¼ŒåŒæ—¶åŸºæœ¬ä¸Šä¿æŒäº†å¯¹å•ä¸ªæ¨¡å¼çš„ç¨³å®šæ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åŸºäºç®€åŒ–çš„é—®ç­”æ¨¡å‹ï¼Œå¯¹RLVRçš„æ”¶æ•›æ€§å’Œè®­ç»ƒåŠ¨æ€è¿›è¡Œäº†ç†è®ºåˆ†æã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¢¯åº¦æµï¼Œå¹¶è¯æ˜RLVRç¡®å®å¯ä»¥æ‰¾åˆ°é€‰æ‹©æˆåŠŸç‡æœ€é«˜çš„æ¨ç†æ¨¡å¼çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç†è®ºç»“æœæ­ç¤ºäº†RLVRè®­ç»ƒçš„æ”¶æ•›è¿‡ç¨‹ä¸­çš„ä¸¤ç§ä¸åŒçŠ¶æ€ï¼šï¼ˆ1ï¼‰å¯¹äºå…·æœ‰ç›¸å¯¹è¾ƒå¼ºçš„åˆå§‹æ¨ç†èƒ½åŠ›çš„æ¨¡å‹çš„å¿«é€Ÿæ”¶æ•›ï¼Œä»¥åŠï¼ˆ2ï¼‰å¯¹äºè¾ƒå¼±çš„æ¨¡å‹çš„è¾ƒæ…¢ä¼˜åŒ–åŠ¨æ€ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨ä½¿ç”¨é«˜è´¨é‡çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†è¿›è¡ŒRLVRä¹‹å‰ï¼Œå¯ä»¥é€šè¿‡åº”ç”¨æœ‰ç›‘ç£å¾®è°ƒæ¥ç¼“è§£è¾ƒå¼±æ¨¡å‹çš„è¾ƒæ…¢ä¼˜åŒ–é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºå‘ç°ã€‚è¿™é¡¹å·¥ä½œåŠ æ·±äº†æˆ‘ä»¬å¯¹äºRLåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­çš„ä½œç”¨çš„ç†è®ºç†è§£ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æé«˜æ¨ç†èƒ½åŠ›æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04695v1">PDF</a> 30 pages, 6 figures, 1 table</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡æ¨¡å‹èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€åå¥½å­¦ä¹ å’Œæ¨ç†ç­‰ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶å®è·µåº”ç”¨å–å¾—äº†æˆåŠŸï¼Œä½†RLæå‡æ¨ç†èƒ½åŠ›çš„æœºåˆ¶ä»çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬ç ”ç©¶å¯¹å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œå‘ç°å…¶ä¸»è¦ä¼˜åŠ¿åœ¨äºä¼˜åŒ–ç°æœ‰æ¨ç†æ¨¡å¼çš„é€‰å–ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†RLVRè®­ç»ƒçš„æ¨¡å‹ä¼šä¼˜å…ˆé‡‡ç”¨é«˜æˆåŠŸç‡çš„æ¨ç†æ¨¡å¼ï¼ŒåŒæ—¶å¤§è‡´ä¿æŒå¯¹å•ä¸ªæ¨¡å¼çš„ç¨³å®šæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜åŸºäºç®€åŒ–çš„é—®ç­”æ¨¡å‹å¯¹RLVRçš„æ”¶æ•›æ€§å’Œè®­ç»ƒåŠ¨æ€è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶å‘ç°RLVRç¡®å®èƒ½æ‰¾åˆ°é€‰æ‹©æˆåŠŸç‡æœ€é«˜çš„æ¨ç†æ¨¡å¼çš„è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç†è®ºç»“æœè¿˜æ­ç¤ºäº†RLVRè®­ç»ƒçš„æ”¶æ•›å­˜åœ¨ä¸¤ç§ä¸åŒçš„æƒ…å†µï¼šä¸€æ˜¯å…·æœ‰ç›¸å¯¹è¾ƒå¼ºçš„åˆå§‹æ¨ç†èƒ½åŠ›çš„æ¨¡å‹çš„å¿«é€Ÿæ”¶æ•›ï¼ŒäºŒæ˜¯è¾ƒå¼±æ¨¡å‹çš„è¾ƒæ…¢ä¼˜åŒ–åŠ¨æ€ã€‚åœ¨è¾ƒå¼±çš„æ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨RLVRä¹‹å‰åº”ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç”¨é«˜è´¨é‡çš„SFTæ•°æ®é›†æ˜¯å¯è¡Œçš„ã€‚æœ¬ç ”ç©¶åŠ æ·±äº†æˆ‘ä»¬å¯¹äºRLåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒä¸­ä½œç”¨çš„ç†è®ºç†è§£ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æé«˜æ¨ç†èƒ½åŠ›æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç”¨äºæå‡æ¨¡å‹çš„å¤šé¡¹èƒ½åŠ›ï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€åå¥½å­¦ä¹ å’Œæ¨ç†ã€‚</li>
<li>RLVRçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºä¼˜åŒ–ç°æœ‰æ¨ç†æ¨¡å¼çš„é€‰å–ã€‚</li>
<li>RLVRè®­ç»ƒçš„æ¨¡å‹ä¼šä¼˜å…ˆé‡‡ç”¨é«˜æˆåŠŸç‡çš„æ¨ç†æ¨¡å¼ã€‚</li>
<li>RLVRè®­ç»ƒçš„ç†è®ºåˆ†ææ­ç¤ºäº†å…¶æ”¶æ•›æ€§çš„ä¸¤ä¸ªä¸åŒæƒ…å†µï¼Œä»¥åŠé’ˆå¯¹è¾ƒå¼±æ¨¡å‹çš„ä¼˜åŒ–åŠ¨æ€ã€‚</li>
<li>å¯¹äºåˆå§‹æ¨ç†èƒ½åŠ›è¾ƒå¼±çš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥ç¼“è§£è¾ƒæ…¢çš„ä¼˜åŒ–åŠ¨æ€ã€‚</li>
<li>ä½¿ç”¨é«˜è´¨é‡çš„SFTæ•°æ®é›†åœ¨RLVRè®­ç»ƒå‰æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04695">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c733c3dbf8049d47eed28701716c2e98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40533fd93dde105a2d9019c23cf80adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f7c8b2413febdd96a99e8a1df4b5dbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afaf10b2b1071c20da239ad8ea76e682.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CHANCERY-Evaluating-corporate-governance-reasoning-capabilities-in-language-models"><a href="#CHANCERY-Evaluating-corporate-governance-reasoning-capabilities-in-language-models" class="headerlink" title="CHANCERY: Evaluating corporate governance reasoning capabilities in   language models"></a>CHANCERY: Evaluating corporate governance reasoning capabilities in   language models</h2><p><strong>Authors:Lucas Irwin, Arda Kaz, Peiyao Sheng, Pramod Viswanath</strong></p>
<p>Law has long been a domain that has been popular in natural language processing (NLP) applications. Reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. Nevertheless, while multiple legal datasets exist, none have thus far focused specifically on reasoning tasks. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark (CHANCERY) to test a modelâ€™s ability to reason about whether executive&#x2F;board&#x2F;shareholderâ€™s proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models - modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The modelâ€™s task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents exhibit superior performance, with agents based on the ReAct and CodeAct frameworks scoring 76.1% and 78.1% respectively, further confirming the advanced legal reasoning capabilities required to score highly on the benchmark. We also conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of SOTA models. </p>
<blockquote>
<p>æ³•å¾‹é¢†åŸŸé•¿æœŸä»¥æ¥åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­é¢‡å—æ¬¢è¿ã€‚æ¨ç†ï¼ˆåŒ…æ‹¬é€»è¾‘æ¨æ–­å’Œä¸å…ˆä¾‹å»ºç«‹è”ç³»çš„èƒ½åŠ›ï¼‰æ˜¯ç°å®ä¸–ç•Œä¸­æ³•å¾‹å®è·µçš„æ ¸å¿ƒéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œå°½ç®¡å­˜åœ¨å¤šä¸ªæ³•å¾‹æ•°æ®é›†ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰ä¸“é—¨é’ˆå¯¹æ¨ç†ä»»åŠ¡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å…¬å¸æ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCHANCERYï¼‰ï¼Œå…³æ³¨æ³•å¾‹ç¯å¢ƒçš„ä¸€ä¸ªç‰¹å®šæ–¹é¢ï¼Œä»¥æµ‹è¯•æ¨¡å‹å¯¹é«˜ç®¡&#x2F;è‘£äº‹ä¼š&#x2F;è‚¡ä¸œæå‡ºçš„è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆå…¬å¸æ²»ç†ç« ç¨‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•é¦–åˆ›äº†ä¸€ç§é’ˆå¯¹è¯­è¨€æ¨¡å‹çš„å…¬å¸æ²»ç†æ¨ç†æµ‹è¯•â€”â€”æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å…¬å¸æ²»ç†æ³•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å…¬å¸ç« ç¨‹ï¼ˆä¸€ç»„ç®¡ç†å¥‘çº¦ï¼‰å’Œä¸€é¡¹é«˜ç®¡è¡ŒåŠ¨ææ¡ˆã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯äºŒåˆ†ç±»ä»»åŠ¡ä¹‹ä¸€ï¼šæ¨ç†è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆç« ç¨‹ä¸­çš„è§„åˆ™ã€‚æˆ‘ä»¬éµå¾ªå…¬å¸æ²»ç†çš„æ—¢å®šåŸåˆ™åˆ›å»ºäº†è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬24é¡¹å…·ä½“çš„å…¬å¸æ²»ç†åŸåˆ™å’Œä»åŒ…å«1ä¸‡ä»½çœŸå®å…¬å¸ç« ç¨‹çš„æ€»æ•°æ®é›†ä¸­æŒ‘é€‰å‡ºçš„79ä»½ä»£è¡¨ä¸åŒè¡Œä¸šçš„ç« ç¨‹ã€‚å¯¹æœ€æ–°æ¨ç†æ¨¡å‹çš„è¯„ä¼°è¯å®äº†è¯¥åŸºå‡†æµ‹è¯•çš„éš¾åº¦ï¼Œå¦‚Claude 3.7 Sonnetå’ŒGPT-4oç­‰æ¨¡å‹åˆ†åˆ«å®ç°äº†64.5%å’Œ75.2%çš„å‡†ç¡®ç‡ã€‚æ¨ç†ä»£ç†è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŸºäºReActå’ŒCodeActæ¡†æ¶çš„ä»£ç†åˆ†åˆ«å¾—åˆ†76.1%å’Œ78.1%ï¼Œè¿™è¿›ä¸€æ­¥è¯å®äº†è¦åœ¨åŸºå‡†æµ‹è¯•ä¸­è·å¾—é«˜åˆ†éœ€è¦å…ˆè¿›çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å½“å‰æ¨ç†æ¨¡å‹é¢ä¸´å›°éš¾çš„ç±»å‹çš„é—®é¢˜ï¼Œæ­ç¤ºäº†å°–ç«¯æ¨¡å‹çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04636v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¼ä¸šæ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCHANCERYï¼‰ï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹å¯¹ä¼ä¸šæ²»ç†ç« ç¨‹ä¸­è‘£äº‹ã€è‚¡ä¸œå’Œæ‰§è¡Œå±‚è¡Œä¸ºçš„åˆè§„æ€§åˆ¤æ–­èƒ½åŠ›ã€‚æ­¤åŸºå‡†æµ‹è¯•é‡‡ç”¨ç°å®ä¼ä¸šæ²»ç†æ³•å¾‹åŸåˆ™ï¼ŒåŒ…å«äº†ä¼ä¸šç« ç¨‹å’Œä¸€ä»½é«˜ç®¡è¡ŒåŠ¨ææ¡ˆã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯å¯¹è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆç« ç¨‹è§„å®šè¿›è¡ŒäºŒå…ƒåˆ†ç±»åˆ¤æ–­ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å°šå¾…æå‡ï¼Œè€Œå…·å¤‡ç‰¹å®šæ¡†æ¶çš„æ¨ç†ä»£ç†è¡¨ç°è¾ƒå¥½ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜åˆ†æäº†å½“å‰æ¨ç†æ¨¡å‹é¢ä¸´çš„é—®é¢˜ç±»å‹ï¼Œæ­ç¤ºäº†å…¶å¯¹å…ˆè¿›æ¨¡å‹æ³•å¾‹æ¨ç†èƒ½åŠ›çš„æ·±åˆ»è§è§£ã€‚

**Key Takeaways**

1. æ³•å¾‹æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­çš„çƒ­é—¨é¢†åŸŸï¼Œä½†ç°æœ‰çš„æ³•å¾‹æ•°æ®é›†å°šæœªä¸“æ³¨äºæ¨ç†ä»»åŠ¡ã€‚
2. å¼•å…¥äº†ä¸€ä¸ªå…¨æ–°çš„ä¼ä¸šæ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCHANCERYï¼‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åˆ¤æ–­ä¼ä¸šæ²»ç†ç« ç¨‹ä¸­è¡Œä¸ºåˆè§„æ€§çš„èƒ½åŠ›ã€‚
3. è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä¼ä¸šç« ç¨‹å’Œä¸€ä»½é«˜ç®¡è¡ŒåŠ¨ææ¡ˆï¼Œæ¨¡å‹éœ€åˆ¤æ–­è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆç« ç¨‹è§„å®šã€‚
4. å½“å‰å…ˆè¿›çš„æ¨ç†æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ‰å¾…æé«˜ï¼Œè€Œç‰¹å®šæ¡†æ¶çš„æ¨ç†ä»£ç†è¡¨ç°è¾ƒå¥½ã€‚
5. ä¼ä¸šæ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•éµå¾ªç°å®ä¼ä¸šæ²»ç†æ³•å¾‹åŸåˆ™ï¼Œå…·æœ‰å®é™…åº”ç”¨åœºæ™¯ä»·å€¼ã€‚
6. æ–‡ç« åˆ†æäº†æ¨ç†æ¨¡å‹é¢ä¸´çš„é—®é¢˜ç±»å‹ï¼Œæ­ç¤ºäº†å…¶æ³•å¾‹æ¨ç†èƒ½åŠ›çš„å¼±ç‚¹ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f312da29397be4f3e13017327b90ad5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceeb1e39bfff816768326c1692c37f13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-368924729e8aae91811cf976e1c382ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff5390e160a0d2860af39ec4a08aa638.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Look-Before-You-Leap-A-GUI-Critic-R1-Model-for-Pre-Operative-Error-Diagnosis-in-GUI-Automation"><a href="#Look-Before-You-Leap-A-GUI-Critic-R1-Model-for-Pre-Operative-Error-Diagnosis-in-GUI-Automation" class="headerlink" title="Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error   Diagnosis in GUI Automation"></a>Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error   Diagnosis in GUI Automation</h2><p><strong>Authors:Yuyang Wanyan, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Jiabo Ye, Yutong Kou, Ming Yan, Fei Huang, Xiaoshan Yang, Weiming Dong, Changsheng Xu</strong></p>
<p>In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the modelâ€™s feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–ã€‚ä¸åŒäºä¸€èˆ¬çš„ç¦»çº¿å¤šæ¨¡æ€ä»»åŠ¡ï¼ŒGUIè‡ªåŠ¨åŒ–æ˜¯åœ¨åœ¨çº¿äº’åŠ¨ç¯å¢ƒä¸­æ‰§è¡Œï¼Œéœ€è¦æ ¹æ®ç¯å¢ƒçš„å®æ—¶çŠ¶æ€è¿›è¡Œä¸€æ­¥ä¸€æ­¥çš„å†³ç­–ã€‚æ­¤ä»»åŠ¡å¯¹æ¯ä¸€æ­¥çš„å†³ç­–é”™è¯¯å®¹å¿åº¦è¾ƒä½ï¼Œå› ä¸ºä»»ä½•é”™è¯¯éƒ½å¯èƒ½ç´¯ç§¯å¹¶ç ´åè¿‡ç¨‹ï¼Œå¹¶å¯èƒ½å¯¼è‡´ä¸å¯é€†çš„ç»“æœï¼Œå¦‚åˆ é™¤æˆ–ä»˜æ¬¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ¯å‰æ‰¹è¯„æœºåˆ¶ï¼Œé€šè¿‡æ¨ç†æ½œåœ¨çš„ç»“æœå’Œè¡ŒåŠ¨çš„æ­£ç¡®æ€§ï¼Œåœ¨å®é™…æ‰§è¡Œä¹‹å‰æä¾›æœ‰æ•ˆçš„åé¦ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå»ºè®®æ„ŸçŸ¥æ¢¯åº¦ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ç­–ç•¥æ¥æ„å»ºæˆ‘ä»¬çš„æœ¯å‰æ‰¹è¯„æ¨¡å‹GUI-Critic-R1ï¼Œç»“åˆä¸€ç§æ–°çš„å»ºè®®å¥–åŠ±æ¥æé«˜æ¨¡å‹åé¦ˆçš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºæ¨ç†å¼•å¯¼çš„æ•°æ®æ”¶é›†ç®¡é“æ¥åˆ›å»ºGUI-Critic-Trainå’ŒGUI-Critic-Testï¼Œä»¥å¡«è¡¥ç°æœ‰GUIè¯„è®ºå®¶æ•°æ®ä¸­çš„ç©ºç™½ã€‚åœ¨ç§»åŠ¨å’Œç½‘ç»œé¢†åŸŸçš„GUI-Critic-Testä¸Šçš„é™æ€å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GUI-Critic-R1åœ¨è¯„è®ºå®¶å‡†ç¡®æ€§æ–¹é¢æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä¸å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ã€‚åœ¨GUIè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ä¸Šçš„åŠ¨æ€è¯„ä¼°è¿›ä¸€æ­¥çªå‡ºäº†æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼Œæé«˜äº†æˆåŠŸç‡å’Œæ“ä½œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–ä¸­çš„æœ€æ–°åº”ç”¨ã€‚ä¸ºæé«˜åœ¨çº¿äº’åŠ¨ç¯å¢ƒä¸­æ¯ä¸€æ­¥å†³ç­–çš„æ­£ç¡®æ€§ï¼Œé™ä½ç´¯ç§¯é”™è¯¯çš„é£é™©ï¼Œæå‡ºä¸€ç§æœ¯å‰æ‰¹è¯„æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹æ½œåœ¨ç»“æœå’Œè¡ŒåŠ¨çš„æ­£ç¡®æ€§æ¥æä¾›æœ‰æ•ˆåé¦ˆã€‚é‡‡ç”¨å»ºè®®å¥–åŠ±çš„ç­–ç•¥ä¼˜åŒ–æœ¯å‰æ‰¹è¯„æ¨¡å‹GUI-Critic-R1çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œå»ºç«‹åŸºäºæ¨ç†å¼•å¯¼çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œåˆ›å»ºGUI-Critic-Trainå’ŒGUI-Critic-Testæ•°æ®é›†ï¼Œå¡«è¡¥GUIæ‰¹è¯„æ•°æ®çš„ç©ºç™½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUI-Critic-R1åœ¨æ‰¹è¯„ç²¾åº¦ä¸Šè¾ƒå½“å‰MLLMsæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨GUIè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²å¹¿æ³›åº”ç”¨äºåŒ…æ‹¬GUIè‡ªåŠ¨åŒ–åœ¨å†…çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚</li>
<li>GUIè‡ªåŠ¨åŒ–éœ€åœ¨åœ¨çº¿äº’åŠ¨ç¯å¢ƒä¸­è¿›è¡Œï¼Œè¦æ±‚åŸºäºå®æ—¶ç¯å¢ƒçš„çŠ¶å†µè¿›è¡Œé€æ­¥å†³ç­–ã€‚</li>
<li>æå‡ºä¸€ç§æœ¯å‰æ‰¹è¯„æœºåˆ¶ï¼Œé€šè¿‡é¢„æµ‹æ½œåœ¨ç»“æœå’Œè¡ŒåŠ¨çš„æ­£ç¡®æ€§æä¾›æœ‰æ•ˆåé¦ˆã€‚</li>
<li>é‡‡ç”¨å»ºè®®å¥–åŠ±çš„ç­–ç•¥ä¼˜åŒ–GUI-Critic-R1æ¨¡å‹ã€‚</li>
<li>å»ºç«‹åŸºäºæ¨ç†å¼•å¯¼çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œåˆ›å»ºGUI-Criticæ•°æ®é›†ä»¥å¡«è¡¥ç°æœ‰ç©ºç™½ã€‚</li>
<li>GUI-Critic-R1åœ¨æ‰¹è¯„ç²¾åº¦ä¸Šè¾ƒå½“å‰MLLMsæœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5e00fc533e610c5a8d545f2bbd147bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be09ca89e8b02a4bd05d03767ce414f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e111669a9a39cdf9fa8ff178bad8ad.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Are-LLMs-Reliable-Translators-of-Logical-Reasoning-Across-Lexically-Diversified-Contexts"><a href="#Are-LLMs-Reliable-Translators-of-Logical-Reasoning-Across-Lexically-Diversified-Contexts" class="headerlink" title="Are LLMs Reliable Translators of Logical Reasoning Across Lexically   Diversified Contexts?"></a>Are LLMs Reliable Translators of Logical Reasoning Across Lexically   Diversified Contexts?</h2><p><strong>Authors:Qingchuan Li, Jiatong Li, Zirui Liu, Mingyue Cheng, Yuting Zeng, Qi Liu, Tongxuan Liu</strong></p>
<p>Neuro-symbolic approaches combining large language models (LLMs) with solvers excels in logical reasoning problems need long reasoning chains. In this paradigm, LLMs serve as translators, converting natural language reasoning problems into formal logic formulas. Then reliable symbolic solvers return correct solutions. Despite their success, we find that LLMs, as translators, struggle to handle lexical diversification, a common linguistic phenomenon, indicating that LLMs as logic translators are unreliable in real-world scenarios. Moreover, existing logical reasoning benchmarks lack lexical diversity, failing to challenge LLMsâ€™ ability to translate such text and thus obscuring this issue. In this work, we propose SCALe, a benchmark designed to address this significant gap through <strong>logic-invariant lexical diversification</strong>. By using LLMs to transform original benchmark datasets into lexically diversified but logically equivalent versions, we evaluate LLMsâ€™ ability to consistently map diverse expressions to uniform logical symbols on these new datasets. Experiments using SCALe further confirm that current LLMs exhibit deficiencies in this capability. Building directly on the deficiencies identified through our benchmark, we propose a new method, MenTaL, to address this limitation. This method guides LLMs to first construct a table unifying diverse expressions before performing translation. Applying MenTaL through in-context learning and supervised fine-tuning (SFT) significantly improves the performance of LLM translators on lexically diversified text. Our code is now available at <a target="_blank" rel="noopener" href="https://github.com/wufeiwuwoshihua/LexicalDiver">https://github.com/wufeiwuwoshihua/LexicalDiver</a>. </p>
<blockquote>
<p>ç¥ç»ç¬¦å·æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§£ç®—å™¨ï¼Œåœ¨éœ€è¦é•¿æ¨ç†é“¾çš„é€»è¾‘æ¨ç†é—®é¢˜ä¸­è¡¨ç°å‡ºè‰²ã€‚åœ¨æ­¤èŒƒå¼ä¸­ï¼ŒLLMå……å½“ç¿»è¯‘ï¼Œå°†è‡ªç„¶è¯­è¨€æ¨ç†é—®é¢˜è½¬æ¢ä¸ºæ­£å¼çš„é€»è¾‘å…¬å¼ã€‚ç„¶åå¯é çš„ç¬¦å·è§£ç®—å™¨è¿”å›æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡å®ƒä»¬å–å¾—äº†æˆåŠŸï¼Œä½†æˆ‘ä»¬å‘ç°LLMä½œä¸ºç¿»è¯‘åœ¨å¤„ç†è¯æ±‡å¤šæ ·åŒ–è¿™ä¸€å¸¸è§è¯­è¨€ç°è±¡æ—¶é‡åˆ°äº†å›°éš¾ï¼Œè¿™è¡¨æ˜LLMä½œä¸ºé€»è¾‘ç¿»è¯‘åœ¨çœŸå®åœºæ™¯ä¸­æ˜¯ä¸å¯é çš„ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ç¼ºä¹è¯æ±‡å¤šæ ·æ€§ï¼Œæœªèƒ½æŒ‘æˆ˜LLMç¿»è¯‘æ­¤ç±»æ–‡æœ¬çš„èƒ½åŠ›ï¼Œä»è€Œæ©ç›–äº†è¿™ä¸ªé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SCALEåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡é€»è¾‘ä¸å˜çš„è¯æ±‡å¤šæ ·åŒ–æ¥è§£å†³è¿™ä¸€é‡è¦å·®è·ã€‚é€šè¿‡ä½¿ç”¨LLMå°†åŸå§‹åŸºå‡†æ•°æ®é›†è½¬æ¢ä¸ºè¯æ±‡ä¸°å¯Œä½†é€»è¾‘ç­‰ä»·çš„ç‰ˆæœ¬ï¼Œæˆ‘ä»¬åœ¨è¿™äº›æ–°æ•°æ®é›†ä¸Šè¯„ä¼°äº†LLMå°†å„ç§è¡¨è¾¾ä¸€è‡´åœ°æ˜ å°„åˆ°ç»Ÿä¸€é€»è¾‘ç¬¦å·çš„èƒ½åŠ›ã€‚ä½¿ç”¨SCALEåŸºå‡†æµ‹è¯•çš„å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œå½“å‰LLMåœ¨è¿™æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚é€šè¿‡æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç¡®å®šçš„ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MenTaLæ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚æ­¤æ–¹æ³•å¼•å¯¼LLMåœ¨ç¿»è¯‘ä¹‹å‰é¦–å…ˆæ„å»ºä¸€ä¸ªç»Ÿä¸€å„ç§è¡¨è¾¾çš„è¡¨æ ¼ã€‚é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åº”ç”¨MenTaLï¼Œå¯ä»¥æ˜¾ç€æé«˜LLMç¿»è¯‘åœ¨è¯æ±‡ä¸°å¯Œæ–‡æœ¬ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ä»£ç ç°å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/wufeiwuwoshihua/LexicalDiver%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/wufeiwuwoshihua/LexicalDiverä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04575v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç¥ç»ç¬¦å·æ–¹æ³•ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ±‚è§£å™¨åœ¨å¤„ç†éœ€è¦é•¿æ¨ç†é“¾çš„é€»è¾‘æ¨ç†é—®é¢˜æ—¶çš„ä¼˜åŠ¿ä¸ä¸è¶³ã€‚æ–‡ä¸­æŒ‡å‡ºï¼ŒLLMsä½œä¸ºç¿»è¯‘è€…ï¼Œåœ¨åº”å¯¹è¯æ±‡å¤šæ ·åŒ–è¿™ä¸€å¸¸è§è¯­è¨€ç°è±¡æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSCALeçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡é€»è¾‘ä¸å˜çš„è¯æ±‡å¤šæ ·åŒ–æ¥è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œè¯„ä¼°LLMsåœ¨ä¸åŒè¡¨è¾¾å½¢å¼è½¬æ¢ä¸ºé€»è¾‘ç¬¦å·çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰LLMsåœ¨æ­¤èƒ½åŠ›ä¸Šå­˜åœ¨ç¼ºé™·ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MenTaLæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºç»Ÿä¸€ä¸åŒè¡¨è¾¾çš„è¡¨æ ¼æ¥è¿›è¡Œç¿»è¯‘ï¼Œæ˜¾è‘—æé«˜äº†LLMsåœ¨è¯æ±‡ä¸°å¯Œæ–‡æœ¬ä¸Šçš„ç¿»è¯‘æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç¬¦å·æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ±‚è§£å™¨ï¼Œæ“…é•¿å¤„ç†éœ€è¦é•¿æ¨ç†é“¾çš„é€»è¾‘æ¨ç†é—®é¢˜ã€‚</li>
<li>LLMsåœ¨ä½œä¸ºé€»è¾‘ç¿»è¯‘å™¨æ—¶ï¼Œå¤„ç†è¯æ±‡å¤šæ ·åŒ–å­˜åœ¨å›°éš¾ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­æ˜¯å¸¸è§çš„è¯­è¨€ç°è±¡ã€‚</li>
<li>ç°æœ‰é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ç¼ºä¹è¯æ±‡å¤šæ ·æ€§ï¼Œæ— æ³•å……åˆ†æŒ‘æˆ˜LLMsçš„ç¿»è¯‘èƒ½åŠ›ï¼Œä»è€Œæ©ç›–äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†SCALeåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡é€»è¾‘ä¸å˜çš„è¯æ±‡å¤šæ ·åŒ–æ¥è¯„ä¼°LLMsçš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå½“å‰LLMsåœ¨å°†ä¸åŒè¡¨è¾¾è½¬æ¢ä¸ºé€»è¾‘ç¬¦å·æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ä¸ºè§£å†³LLMsåœ¨è¯æ±‡ä¸°å¯Œæ–‡æœ¬ç¿»è¯‘ä¸Šçš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†MenTaLæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºç»Ÿä¸€è¡¨è¾¾è¡¨æ ¼è¿›è¡Œç¿»è¯‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-908908829676fd2a9b67c0e3c5944acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f714a2141c22fdfbd20a66162d542a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb9f10f46fb948e1601614660a1d8ba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9516d853722093a2d318a9f4206fdc19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b8f1acfe87251b962601dbc24a4239c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32b866d248726a476ef58ea795420284.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-31593c9a93ac5fd04bd000716e4cef24.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  SparseMM Head Sparsity Emerges from Visual Concept Responses in MLLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a42c1726d4ab1f7ccf9aeffb4ab2564a.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  EnergyMoGen Compositional Human Motion Generation with Energy-Based   Diffusion Model in Latent Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
