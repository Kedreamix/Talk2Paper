<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-08  VideoMathQA Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-05d7ca8eb66922610ad3cf28a702c0b5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    87 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-08-更新"><a href="#2025-06-08-更新" class="headerlink" title="2025-06-08 更新"></a>2025-06-08 更新</h1><h2 id="VideoMathQA-Benchmarking-Mathematical-Reasoning-via-Multimodal-Understanding-in-Videos"><a href="#VideoMathQA-Benchmarking-Mathematical-Reasoning-via-Multimodal-Understanding-in-Videos" class="headerlink" title="VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos"></a>VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal   Understanding in Videos</h2><p><strong>Authors:Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, Fahad Khan</strong></p>
<p>Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over $920$ man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: <a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/VideoMathQA">https://mbzuai-oryx.github.io/VideoMathQA</a> </p>
<blockquote>
<p>在真实世界的视频环境中进行数学推理与静态图像或文本中的挑战存在根本性的不同。它要求解释细微的视觉信息，准确阅读手写或数字文本，并整合口语线索，这些线索通常随时间分散并非线性分布。在这种多模态情境中，成功的关键不仅在于感知，还在于从丰富且嘈杂的内容流中有选择地识别和整合正确的上下文细节。为此，我们引入了VideoMathQA基准测试，旨在评估模型在视频上执行这种时间扩展的跨模态推理的能力。该基准测试涵盖了10个多样化的数学领域，涉及的视频时长从10秒到超过1小时不等。它要求模型解释结构化的视觉内容，理解说明性叙述，并在视觉、音频和文本模式之间共同建立概念。我们聘请了研究生水平的专家以确保高质量，总标注时长超过920小时。为了反映真实世界场景，问题围绕三个核心推理挑战进行设计：直接问题解决，答案基于所提出的问题；概念迁移，要求将所学方法应用于新问题；以及深度指令理解，涉及对多步骤解释的长期推理和部分解决方案。每个问题都包含多步骤推理注释，可以精细诊断模型的能力。通过这个基准测试，我们强调了现有方法的局限性，并为必须在时间延长和模式丰富的数学问题环境中进行推理的模型建立了系统的评估框架。我们的基准测试和评估代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/VideoMathQA">https://mbzuai-oryx.github.io/VideoMathQA</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05349v1">PDF</a> VideoMathQA Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文介绍了VideoMathQA基准测试，该测试旨在评估模型在视频上进行时间扩展跨模态推理的能力。该基准测试包含10个数学领域的多样化视频内容，时长从10秒到超过1小时不等。它要求模型解读结构化视觉内容、理解教学叙述，并在视觉、音频和文本模式之间共同建立概念。通过此基准测试，突显了现有方法的局限性，并为需要在时间扩展和模态丰富的数学问题解决环境中进行推理的模型建立了系统的评估框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoMathQA是一个旨在评估模型在视频上进行跨模态推理能力的基准测试。</li>
<li>测试涉及10个数学领域的多样化视频内容，时长不一。</li>
<li>模型需解读结构化视觉内容、理解教学叙述，并在各模式间建立概念。</li>
<li>测试包含三种核心推理挑战：直接问题解决、概念迁移和深度教学理解。</li>
<li>每个问题包含多步骤推理注释，能精细诊断模型能力。</li>
<li>现有方法的局限性在测试中突显。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bf491dc0235f11fd4346ca0e416f280a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b350e0428e0a3e219e41243d2e00837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4e2cffc9746b9eff55e4d003f59bf73.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning"><a href="#MINT-CoT-Enabling-Interleaved-Visual-Tokens-in-Mathematical-Chain-of-Thought-Reasoning" class="headerlink" title="MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning"></a>MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical   Chain-of-Thought Reasoning</h2><p><strong>Authors:Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li</strong></p>
<p>Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a> </p>
<blockquote>
<p>思维链（CoT）已在大型语言模型（LLM）中广泛提高了数学推理能力，但将其扩展到多模态领域仍然具有挑战性。现有工作要么采用类似的文本推理进行图像输入，要么寻求将视觉信号融入数学思维链。然而，它们在解决数学问题方面面临三个主要局限性：依赖粗粒度的盒状图像区域、视觉编码器对数学内容的感知有限，以及依赖外部能力进行视觉修改。在本文中，我们提出MINT-CoT，引入用于思维链视觉推理的数学交叉令牌。MINT-CoT通过自适应地将相关视觉令牌插入文本推理步骤中，实现思维链中的视觉与文本交叉融合。它通过动态选择数学图形内的任何形状视觉区域来实现这一功能。为了支持这一功能，我们构建了MINT-CoT数据集，包含54K个数学问题，每个推理步骤都与令牌级别的视觉区域对齐，并配备严格的数据生成流程。我们还提出了一个三阶段的MINT-CoT训练策略，逐步结合纯文本思维链SFT、交叉思维链SFT和交叉思维链RL，从而得到我们的MINT-CoT-7B模型。大量实验表明，我们的方法在数学领域的有效视觉交叉推理中非常有效，MINT-CoT-7B在MathVista上优于基线模型+34.08%，在GeoQA上优于基线模型+28.78%，在MMStar上优于基线模型+23.2%。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xinyan-cxy/MINT-CoT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05331v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a></p>
<p><strong>Summary</strong><br>在大型语言模型（LLM）中，链式思维（Chain-of-Thought，简称CoT）已经广泛应用于数学推理领域。然而，将其扩展到多模态领域仍然具有挑战性。现有方法要么采用类似的文本推理进行图像输入，要么寻求将视觉信号融入数学CoT。但它们在解决数学问题方面面临三个主要局限性：依赖粗粒度的框状图像区域、视觉编码器对数学内容的感知有限，以及依赖外部能力进行视觉修改。针对这些问题，本文提出MINT-CoT方法，通过引入数学交错令牌（Mathematical INterleaved Tokens）进行链式思维视觉推理。MINT-CoT自适应地将相关视觉令牌插入文本推理步骤中，通过动态选择数学图形内的视觉区域来增强这种能力。此外，构建了MINT-CoT数据集和采用三阶段训练策略，最终推出MINT-CoT-7B模型。实验表明，该模型在MathVista、GeoQA和MMStar上的表现均优于基线模型。详细信息可通过链接查看：<a target="_blank" rel="noopener" href="https://github.com/xinyan-cxy/MINT-CoT">https://github.com/xinyan-cxy/MINT-CoT</a>。简洁理解即为本文主要提出了一种创新的视觉化推理技术方法以及对应的数据集与模型优化训练策略等以优化对数学推理能力的提升与应用落地性并实际取得超越其他现有方案的优秀效果总结摘要内容为提高多模态领域内的数学推理能力研究现状及所面临的困难点挑战据此展开深入研究从而引出所创新性地提出MINT结合实体理论分析说明了创新方案的显著优势总结方法以及其性能特点及应用推广信息明确具体的行业意义应用潜力和市场前景为该领域的持续发展与提升提供了一种创新性和实践性的技术方法关键特点在于将视觉推理与数学推理紧密结合实现了更高效更准确的数学问题解决能力概括简洁准确突出主题及亮点无冗余内容<strong>Key Takeaways</strong>:</p>
<ol>
<li>MINT-CoT解决了将链式思维（CoT）扩展到多模态领域的挑战。现有方法面临依赖于粗粒度图像区域、缺乏深度感知数学内容等限制。</li>
<li>MINT-CoT引入数学交错令牌，实现自适应地将视觉信息与文本推理结合，提升数学问题的感知和解决方案准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9e0e517d4f8b10aa9ca2f54ac60ba249.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ac84bdb90b912b32fbef78fa3d2fff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e516bcb6bcba5b8144ca314a595fcedb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-568fe4ddee8c914dbbeabe58cb026b37.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AV-Reasoner-Improving-and-Benchmarking-Clue-Grounded-Audio-Visual-Counting-for-MLLMs"><a href="#AV-Reasoner-Improving-and-Benchmarking-Clue-Grounded-Audio-Visual-Counting-for-MLLMs" class="headerlink" title="AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual   Counting for MLLMs"></a>AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual   Counting for MLLMs</h2><p><strong>Authors:Lidong Lu, Guo Chen, Zhiqi Li, Yicheng Liu, Tong Lu</strong></p>
<p>Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model’s counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on <a target="_blank" rel="noopener" href="https://av-reasoner.github.io/">https://av-reasoner.github.io</a>. </p>
<blockquote>
<p>尽管视频理解领域已经取得了一定的进展，但当前的多媒体大型语言模型（MLLMs）在计数任务上仍然面临挑战。现有的基准测试受限于短视频、限定查询范围、缺乏线索标注和弱多模式覆盖。在本文中，我们介绍了CG-AV-Counting，这是一个手动标注的基于线索的计数基准测试，包含1027个多模式问题和5845个标注线索，涵盖497个长视频。它支持黑箱和白箱评估，是端到端和基于推理的计数的综合测试平台。为了探索提高模型计数能力的方法，我们提出了AV-Reasoner，这是一个通过GRPO和课程学习进行训练的模型，能够从相关任务中推广计数能力。AV-Reasoner在多个基准测试上取得了最新结果，证明了强化学习的有效性。然而，实验表明，在域外基准测试上，语言空间中的推理无法带来性能提升。相关代码和基准测试已在<a target="_blank" rel="noopener" href="https://av-reasoner.github.io发布./">https://av-reasoner.github.io发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05328v1">PDF</a> 21 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了视频理解领域的挑战，当前的多模态语言模型在处理计数任务时存在困难。为解决此问题，文章提出了CG-AV-Counting基准测试，包含1,027个多模态问题和5,845个针对497个长视频的注释线索。此外，为提升模型的计数能力，文章还提出了AV-Reasoner模型，通过通用区域优先（GRPO）和课程学习进行训练，具备从相关任务中泛化计数的能力。然而，实验表明，在域外基准测试中，语言空间的推理并未带来性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前多模态语言模型在视频理解领域的计数任务上仍有困难。</li>
<li>CG-AV-Counting基准测试被引入，包含大量手动注释的线索，支持黑箱和白箱评估，是端到端和推理计数方法的综合测试平台。</li>
<li>AV-Reasoner模型通过通用区域优先（GRPO）和课程学习进行训练，具有泛化计数能力，并在多个基准测试中达到最佳效果。</li>
<li>强化学习在提升模型计数能力上展现出有效性。</li>
<li>在域外基准测试中，语言空间推理并未带来性能提升。</li>
<li>代码和基准测试已在<a target="_blank" rel="noopener" href="https://av-reasoner.github.io上发布./">https://av-reasoner.github.io上发布。</a></li>
<li>现有视频计数任务面临的挑战包括短视频、限定查询、缺乏线索注释和弱多模态覆盖等问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a4579d79fb55fb8e08662919c927c310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8844e3bbc0e3a812c6dfb8a1fad711ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4df35e8c10b38aca84cc0bf811c8750b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef501bffe39fc643dce8dc89296d9fcd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning"><a href="#Micro-Act-Mitigate-Knowledge-Conflict-in-Question-Answering-via-Actionable-Self-Reasoning" class="headerlink" title="Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning"></a>Micro-Act: Mitigate Knowledge Conflict in Question Answering via   Actionable Self-Reasoning</h2><p><strong>Authors:Nan Huo, Jinyang Li, Bowen Qin, Ge Qu, Xiaolong Li, Xiaodong Li, Chenhao Ma, Reynold Cheng</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems commonly suffer from Knowledge Conflicts, where retrieved external knowledge contradicts the inherent, parametric knowledge of large language models (LLMs). It adversely affects performance on downstream tasks such as question answering (QA). Existing approaches often attempt to mitigate conflicts by directly comparing two knowledge sources in a side-by-side manner, but this can overwhelm LLMs with extraneous or lengthy contexts, ultimately hindering their ability to identify and mitigate inconsistencies. To address this issue, we propose Micro-Act a framework with a hierarchical action space that automatically perceives context complexity and adaptively decomposes each knowledge source into a sequence of fine-grained comparisons. These comparisons are represented as actionable steps, enabling reasoning beyond the superficial context. Through extensive experiments on five benchmark datasets, Micro-Act consistently achieves significant increase in QA accuracy over state-of-the-art baselines across all 5 datasets and 3 conflict types, especially in temporal and semantic types where all baselines fail significantly. More importantly, Micro-Act exhibits robust performance on non-conflict questions simultaneously, highlighting its practical value in real-world RAG applications. </p>
<blockquote>
<p>检索增强生成（RAG）系统通常面临知识冲突问题，其中检索到的外部知识与大语言模型（LLM）的固有参数知识相矛盾。这对问答等下游任务性能产生不利影响。现有方法往往通过并排比较两种知识源来减轻冲突，但这可能使LLM面临过多或冗长的上下文，最终阻碍其识别和缓解不一致的能力。为了解决这一问题，我们提出了Micro-Act框架，它具有分层动作空间，可自动感知上下文复杂性，并自适应地将每个知识源分解为一系列精细的比较。这些比较表现为可操作的步骤，使推理超越表层上下文。通过在五个基准数据集上进行广泛实验，Micro-Act在所有五个数据集和三种冲突类型上始终实现了问答准确度的显著提高，尤其在时间和语义类型方面，所有基线都出现了显著失败。更重要的是，Micro-Act在非冲突问题上也表现出稳健的性能，这突出了其在现实世界RAG应用中的实用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05278v1">PDF</a> Accepted by ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Retrieval-Augmented Generation（RAG）系统中常见的知识冲突问题，即检索的外部知识与大型语言模型（LLM）的固有参数知识之间的矛盾。该问题对问答等下游任务性能产生负面影响。现有方法常常通过并排比较两种知识源来减轻冲突，但这可能使LLM面临过多复杂或冗长的语境，从而难以识别和缓解不一致性。为解决此问题，本文提出Micro-Act框架，该框架具有层次化的动作空间，能够自动感知语境复杂性，并自适应地将每个知识源分解成一系列精细的比较。这些比较表现为可操作的步骤，使推理超越了肤浅语境。在五个基准数据集上的广泛实验表明，Micro-Act在所有数据集和三种冲突类型上均实现了对最新基准线的问答准确度显著提高，特别是在时间和语义类型的冲突上。更重要的是，Micro-Act在非冲突问题上也表现出稳健的性能，凸显其在现实RAG应用中的实用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG系统面临知识冲突问题，即外部检索知识与LLM固有知识之间的矛盾。</li>
<li>知识冲突对下游任务如问答的性能产生负面影响。</li>
<li>现有方法通过并排比较知识源来减轻冲突，但可能使LLM面临复杂或冗长的语境。</li>
<li>Micro-Act框架具有层次化的动作空间，能自动感知语境复杂性并分解知识源。</li>
<li>Micro-Act通过精细的比较和可操作的步骤进行推理，超越肤浅语境。</li>
<li>Micro-Act在多个数据集和多种冲突类型上实现了对最新方法的显著性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-827152fce07163683ee4731b5081d264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c41fbee446036f82e6d35bef820a5cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cb7fa84605926f84e3840f592cd3cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16b53a70df398ac3e002d9d2ed33389e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d870c33b971ca6264a6abf75e66f1b84.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Counterfactual-reasoning-an-analysis-of-in-context-emergence"><a href="#Counterfactual-reasoning-an-analysis-of-in-context-emergence" class="headerlink" title="Counterfactual reasoning: an analysis of in-context emergence"></a>Counterfactual reasoning: an analysis of in-context emergence</h2><p><strong>Authors:Moritz Miller, Bernhard Schölkopf, Siyuan Guo</strong></p>
<p>Large-scale neural language models (LMs) exhibit remarkable performance in in-context learning: the ability to learn and reason the input context on the fly without parameter update. This work studies in-context counterfactual reasoning in language models, that is, to predict the consequences of changes under hypothetical scenarios. We focus on studying a well-defined synthetic setup: a linear regression task that requires noise abduction, where accurate prediction is based on inferring and copying the contextual noise from factual observations. We show that language models are capable of counterfactual reasoning in this controlled setup and provide insights that counterfactual reasoning for a broad class of functions can be reduced to a transformation on in-context observations; we find self-attention, model depth, and data diversity in pre-training drive performance in Transformers. More interestingly, our findings extend beyond regression tasks and show that Transformers can perform noise abduction on sequential data, providing preliminary evidence on the potential for counterfactual story generation. Our code is available under <a target="_blank" rel="noopener" href="https://github.com/moXmiller/counterfactual-reasoning.git">https://github.com/moXmiller/counterfactual-reasoning.git</a> . </p>
<blockquote>
<p>大规模神经网络语言模型（LMs）在上下文学习方面表现出卓越的性能：能够在不更新参数的情况下，即时学习和推理输入上下文。本研究关注语言模型中的上下文反事实推理，即预测假设场景变化后的结果。我们专注于研究一个定义明确的合成设置：需要进行噪声推断的线性回归任务，准确预测基于从实际观察中推断和复制上下文噪声。我们证明语言模型能够在这种受控设置中进行反事实推理，并提供见解，即一类广泛的函数的反事实推理可以归结为对上下文观察的转换；我们发现自注意力、模型深度以及预训练中的数据多样性是驱动Transformer性能的关键因素。更有趣的是，我们的研究结果超越了回归任务，表明Transformer可以对顺序数据进行噪声推断，为潜在的反事实故事生成提供了初步证据。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/moXmiller/counterfactual-reasoning.git%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/moXmiller/counterfactual-reasoning.git找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05188v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模神经网络语言模型展现出强大的上下文学习能力，即无需更新参数就能在语境中即时学习和推理。本研究探讨语言模型中的上下文反事实推理，即预测假设场景变化后的结果。研究重点是一个明确的合成设置：需要进行噪声推断的线性回归任务，准确预测基于从实际观察中推断和复制上下文噪声。研究发现语言模型能够在这种控制设置中进行反事实推理，并提供见解，即反事实推理可以简化为对上下文观察的转换；我们发现自注意力、模型深度和预训练数据多样性是推动变压器性能的关键。更有趣的是，我们的研究结果超出了回归任务的范围，显示变压器可以在序列数据上进行噪声推断，为反事实故事生成的潜力提供了初步证据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模神经网络语言模型展现出强大的上下文学习能力。</li>
<li>语言模型具备反事实推理能力，在控制环境下可进行预测。</li>
<li>反事实推理可简化为对上下文观察的转换。</li>
<li>自注意力、模型深度和预训练数据多样性对变压器性能至关重要。</li>
<li>语言模型不仅能在回归任务中进行噪声推断，还能在序列数据上进行。</li>
<li>研究为反事实故事生成的潜力提供了初步证据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05188">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7e38aa240d9f01787ea9e94cdee7c7c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a36eb44ac8dd3f87a8e846f883366c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c79a71c539518c8d288f4349a32489b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbe6920623c737b38cc262512456c0d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TreeRPO-Tree-Relative-Policy-Optimization"><a href="#TreeRPO-Tree-Relative-Policy-Optimization" class="headerlink" title="TreeRPO: Tree Relative Policy Optimization"></a>TreeRPO: Tree Relative Policy Optimization</h2><p><strong>Authors:Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, Jing Tang</strong></p>
<p>Large Language Models (LLMs) have shown remarkable reasoning capabilities through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However, a key limitation of existing approaches is that rewards defined at the full trajectory level provide insufficient guidance for optimizing the intermediate steps of a reasoning process. To address this, we introduce \textbf{\name}, a novel method that estimates the mathematical expectations of rewards at various reasoning steps using tree sampling. Unlike prior methods that rely on a separate step reward model, \name directly estimates these rewards through this sampling process. Building on the group-relative reward training mechanism of GRPO, \name innovatively computes rewards based on step-level groups generated during tree sampling. This advancement allows \name to produce fine-grained and dense reward signals, significantly enhancing the learning process and overall performance of LLMs. Experimental results demonstrate that our \name algorithm substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test benchmarks, increasing it from 19.0% to 35.5%. Furthermore, \name significantly outperforms GRPO by 2.9% in performance while simultaneously reducing the average response length by 18.1%, showcasing its effectiveness and efficiency. Our code will be available at \href{<a target="_blank" rel="noopener" href="https://github.com/yangzhch6/TreeRPO%7D%7Bhttps://github.com/yangzhch6/TreeRPO%7D">https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）通过强化学习与可验证奖励（RLVR）方法显示出卓越的推理能力。然而，现有方法的关键局限性在于，在完整轨迹层面定义的奖励无法为推理过程中优化中间步骤提供足够的指导。为了解决这一问题，我们引入了一种新方法——树采样期望奖励算法（暂且称为XX算法）。该算法通过树采样估计不同推理步骤的奖励的数学期望。不同于以往依赖于单独步骤奖励模型的方法，XX算法通过采样过程直接估计这些奖励。基于相对奖励训练机制的GRPO算法，XX算法创新地根据树采样过程中产生的步骤级别组计算奖励。这一进展使得XX算法能够产生精细且密集的奖励信号，从而极大地提高了学习过程和LLM的整体性能。实验结果表明，我们的XX算法在测试基准上显著提高了Qwen-2.5-Math的平均Pass@1准确率，从19.0%提高到35.5%。此外，XX算法在性能上比GRPO高出2.9%，同时平均响应长度减少了18.1%，显示了其有效性和效率。我们的代码将发布在：<a target="_blank" rel="noopener" href="https://github.com/yangzhch6/TreeRPO">https://github.com/yangzhch6/TreeRPO</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05183v1">PDF</a> 13pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>基于强化学习与可验证奖励（RLVR）方法，大型语言模型（LLMs）展现出惊人的推理能力。然而，现有方法的一个关键局限是，在完整轨迹层面定义的奖励无法为优化推理过程的中间步骤提供足够指导。为解决这一问题，我们提出一种新方法\name，通过树采样估计推理步骤中不同奖励的数学期望。不同于依赖单独步骤奖励模型的先前方法，\name通过采样过程直接估计这些奖励。建立在相对奖励训练机制的GRPO基础上，\name根据树采样过程中产生的步骤级别组计算奖励。这一进展产生了精细且密集的奖励信号，显著增强了学习过程和LLMs的整体性能。实验结果表明，我们的\name算法在测试基准上大幅提高了Qwen-2.5-Math的平均Pass@1准确率，从19.0%提升至35.5%。此外，\name在性能上较GRPO高出2.9%，同时平均响应长度减少了18.1%，展现出其有效性和高效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）通过强化学习与可验证奖励（RLVR）展现出强大的推理能力。</li>
<li>现有方法奖励定义在完整轨迹层面，缺乏对中间步骤优化的指导。</li>
<li>\name方法通过树采样估计推理步骤中不同奖励的数学期望。</li>
<li>\name直接通过采样过程估计奖励，不同于依赖单独步骤奖励模型的先前方法。</li>
<li>\name建立在相对奖励训练机制的GRPO基础上，根据步骤级别组计算奖励。</li>
<li>\name算法显著提高Qwen-2.5-Math的Pass@1准确率，从19.0%提升至35.5%。</li>
<li>\name在性能上较GRPO有所提升，同时减少平均响应长度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05183">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4fe78c01f7eae517a79dcece73bed4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4acbebde56aa146aafa7f9ceabd5c4a9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiCoRe-Enhancing-Zero-shot-Event-Detection-via-Divergent-Convergent-LLM-Reasoning"><a href="#DiCoRe-Enhancing-Zero-shot-Event-Detection-via-Divergent-Convergent-LLM-Reasoning" class="headerlink" title="DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM   Reasoning"></a>DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM   Reasoning</h2><p><strong>Authors:Tanmay Parekh, Kartik Mehta, Ninareh Mehrabi, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline – establishing DiCoRe as a strong zero-shot ED framework. </p>
<blockquote>
<p>零事件检测（ED）是在没有任何训练数据的情况下，识别自然语言文本中的事件提及的任务，对于特定领域文档理解至关重要。理解复杂的事件本体论，从段落中提取特定领域的触发词，并适当地进行结构化设置，增加了零事件检测对于大型语言模型（LLM）的实用性和局限性。为此，我们提出了DiCoRe，一个发散收敛推理框架，它通过Dreamer和Grounder将事件检测任务解耦。Dreamer通过开放式事件发现鼓励发散推理，有助于提高事件覆盖率。相反，Grounder引入收敛推理，以有限状态机引导的有约束解码来使自由形式的预测与特定任务的指令对齐。此外，LLM-Judge验证最终输出以确保高精确度。通过对五个领域的六个数据集和九个LLM的广泛实验，我们证明了DiCoRe如何始终优于先前的零样本迁移学习和推理基准测试，在最佳基准测试上实现了平均F1得分4-7%的提升——确立了DiCoRe作为强大的零样本事件检测框架的地位。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05128v1">PDF</a> Submitted at ACL ARR May 2025</p>
<p><strong>Summary</strong></p>
<p>基于文本理解任务的需要，研究提出了零样本事件检测（ED）的重要性。然而，大型语言模型（LLM）在处理零样本事件检测时存在事件覆盖不足的问题。为此，本文提出了一种发散收敛推理框架DiCoRe，通过Dreamer进行发散推理实现开放式事件发现，并通过Grounder引入收敛推理将自由形式的预测与任务特定指令对齐。此外，LLM-Judge验证了最终输出结果的精确度。实验表明，DiCoRe框架在各种数据集和多个大型语言模型上均表现出优异的性能，平均F1得分比最佳基线高出4-7%，成为了强大的零样本事件检测框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本事件检测（ED）在特定领域文档理解中具有重要作用。</li>
<li>大型语言模型（LLM）在处理零样本事件检测时存在事件覆盖不足的问题。</li>
<li>DiCoRe框架结合了发散推理和收敛推理来解决这一问题。</li>
<li>Dreamer通过开放式事件发现鼓励发散推理。</li>
<li>Grounder通过有限状态机引导约束解码来实现收敛推理。</li>
<li>LLM-Judge验证最终输出的精确度以确保高精确度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05128">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-45deb2d05f87c810e14938ef5aadbcbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458ea81d0f0ef0f86c4e1ae18ab4898d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc33850d5d56fceba6395036a2baab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd09bf1c6e26884ebe35899d760a7cb9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05aa89c1b637896cfb1d73be07e1683b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05d7ca8eb66922610ad3cf28a702c0b5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Reason-to-Recommend-Using-Interaction-of-Thought-Reasoning-to-Enhance-LLM-Recommendation"><a href="#Reason-to-Recommend-Using-Interaction-of-Thought-Reasoning-to-Enhance-LLM-Recommendation" class="headerlink" title="Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance   LLM Recommendation"></a>Reason-to-Recommend: Using Interaction-of-Thought Reasoning to Enhance   LLM Recommendation</h2><p><strong>Authors:Keyu Zhao, Fengli Xu, Yong Li</strong></p>
<p>Driven by advances in Large Language Models (LLMs), integrating them into recommendation tasks has gained interest due to their strong semantic understanding and prompt flexibility. Prior work encoded user-item interactions or metadata into prompts for recommendations. In parallel, LLM reasoning, boosted by test-time scaling and reinforcement learning, has excelled in fields like mathematics and code, where reasoning traces and correctness signals are clear, enabling high performance and interpretability. However, directly applying these reasoning methods to recommendation is ineffective because user feedback is implicit and lacks reasoning supervision. To address this, we propose $\textbf{R2Rec}$, a reasoning-enhanced recommendation framework that samples interaction chains from the user-item graph and converts them into structured interaction-of-thoughts via a progressive masked prompting strategy, with each thought representing stepwise reasoning grounded in interaction context. This allows LLMs to simulate step-by-step decision-making based on implicit patterns. We design a two-stage training pipeline: supervised fine-tuning teaches basic reasoning from high-quality traces, and reinforcement learning refines reasoning via reward signals, alleviating sparse explicit supervision. Experiments on three real-world datasets show R2Rec outperforms classical and LLM-based baselines with an average $\textbf{10.48%}$ improvement in HitRatio@1 and $\textbf{131.81%}$ gain over the original LLM. Furthermore, the explicit reasoning chains enhance interpretability by revealing the decision process. Our code is available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2Rec-7C5D">https://anonymous.4open.science/r/R2Rec-7C5D</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）的进步，将其集成到推荐任务中已引起关注，因为它们具有强大的语义理解和提示灵活性。早期工作将用户-项目互动或元数据编码为推荐提示。与此同时，借助测试时缩放和强化学习，LLM推理在数学和代码等领域表现出色，这些领域的推理轨迹和正确性信号清晰可见，能够实现高性能和可解释性。然而，直接将这些推理方法应用于推荐是无效的，因为用户反馈是隐式的，缺乏推理监督。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05069v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着大型语言模型（LLMs）的进展，将其整合到推荐任务中已引起关注。过去的工作将用户-项目互动或元数据编码到提示中以为推荐提供支持。同时，LLM推理在测试时通过比例尺调整和强化学习得到加强，在数学和代码等领域表现卓越，这些领域的推理痕迹和正确性信号清晰。然而，直接将这些推理方法应用于推荐是不有效的，因为用户反馈是隐式的，缺乏推理监督。为解决此问题，我们提出一种增强推理的推荐框架R2Rec，从用户-项目图中采样互动链，并通过渐进式遮罩提示策略将其转化为结构化思维。每个思维代表基于互动上下文的逐步推理。这允许LLMs模拟基于隐式模式的逐步决策。我们设计了一个两阶段训练流程：监督微调从高质量轨迹中学习基本推理，强化学习通过奖励信号优化推理，缓解稀疏的显式监督。在三个真实数据集上的实验表明，R2Rec在HitRatio@1指标上平均提高了10.48%，在原始LLM基础上提高了131.81%。此外，明确的推理链通过揭示决策过程增强了可解释性。我们的代码可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2Rec-7C5D">匿名链接</a>找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLMs）在推荐任务中的应用日益受到关注，因其强大的语义理解和提示灵活性。</li>
<li>用户反馈的隐式性和缺乏推理监督是直接将LLM应用于推荐的主要挑战。</li>
<li>R2Rec框架旨在通过采样互动链并将其转化为结构化思维来增强推理，使LLMs能够模拟基于隐式模式的逐步决策过程。</li>
<li>R2Rec采用两阶段训练流程，包括监督微调用于学习基本推理和强化学习以通过奖励信号优化推理。</li>
<li>实验表明R2Rec在多个真实数据集上的性能优于传统和LLM基线方法。</li>
<li>R2Rec提高了推荐的解释性，通过揭示推理链显示决策过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05069">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75d96050ade5d95c15f45b9ccf3de6f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b72bde7e4a204171837b240d008eb8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10fc519bb44d8ceb84f07b7987d6c274.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4408501d434c99080d4589f583aaaaa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mathematical-Reasoning-for-Unmanned-Aerial-Vehicles-A-RAG-Based-Approach-for-Complex-Arithmetic-Reasoning"><a href="#Mathematical-Reasoning-for-Unmanned-Aerial-Vehicles-A-RAG-Based-Approach-for-Complex-Arithmetic-Reasoning" class="headerlink" title="Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based   Approach for Complex Arithmetic Reasoning"></a>Mathematical Reasoning for Unmanned Aerial Vehicles: A RAG-Based   Approach for Complex Arithmetic Reasoning</h2><p><strong>Authors:Mehdi Azarafza, Mojtaba Nayyeri, Faezeh Pasandideh, Steffen Staab, Achim Rettberg</strong></p>
<p>Autonomous UAV operation necessitates reliable mathematical reasoning for tasks such as trajectory planning and power management. While traditional flight control relies on hardcoded equations, recent Large Language Models (LLMs) offer potential for more flexible problem-solving but struggle with reliably selecting and applying correct mathematical formulations and executing precise multi-step arithmetic. We propose RAG-UAV, a retrieval-augmented generation framework designed to improve the mathematical reasoning of several LLMs (including GPT o1&#x2F;Turbo, Llama-3.2&#x2F;3.3, Mistral, and DeepSeek R1) in UAV-specific contexts by providing access to relevant domain literature. To conduct an initial assessment, we introduce the UAV-Math-Bench, a small problem set comprising 20 UAV-centric mathematical problems across four difficulty levels. Our experiments demonstrate that incorporating retrieval substantially increases exact answer accuracy (achieving up to 75% with o1), reduces instances of incorrect formulation selection (from 25% without RAG to 5% with RAG), decreases numerical errors, reducing Mean Squared Error (MSE) by orders of magnitude for the best-performing models. This pilot study indicates that RAG can enable general-purpose LLMs to function as more reliable tools for engineering analysis, although direct real-time flight control requires further investigation and validation on a larger scale. All benchmark data, question and answer are publicly available. </p>
<blockquote>
<p>自主无人机的操作需要进行可靠的数学推理，以完成轨迹规划和电源管理等任务。虽然传统的飞行控制依赖于硬编码的方程，但最近的大型语言模型（LLM）为更灵活的解决问题提供了潜力，但在可靠选择和应用正确的数学公式以及执行精确的多步骤算术方面仍存在困难。我们提出了RAG-UAV，这是一个增强检索生成的框架，旨在通过提供相关的领域文献，提高多种LLM（包括GPT o1&#x2F;Turbo、Llama 3.2&#x2F;3.3、Mistral和DeepSeek R1）在无人机特定上下文中的数学推理能力。为了进行初步评估，我们推出了UAV-Math-Bench，这是一组包含20个以无人机为中心的数学问题的小问题集，分为四个难度级别。我们的实验表明，加入检索功能可以显著提高精确答案的准确性（在o1的情况下达到了75%），减少了错误公式选择的情况（从没有RAG时的25%减少到使用RAG时的5%），减少了数值错误，对于表现最佳的模型，均方误差（MSE）降低了多个数量级。这项初步研究表明，RAG可以使通用LLM更加可靠地用于工程分析，但直接的实时飞行控制需要进一步的大规模研究和验证。所有基准数据、问题和答案都是公开可用的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04998v1">PDF</a> 15 pages, 7 figures, 4 appendix subsections</p>
<p><strong>Summary</strong></p>
<p>本文章探讨了自主无人机操作对数学推理的可靠性需求，并指出了传统飞行控制方法与现代大型语言模型（LLM）之间的差异。文章介绍了一个名为RAG-UAV的框架，该框架通过提供相关领域文献的检索，旨在提高LLM在数学推理方面的能力。为初步评估该框架的效果，文章还设计了一个包含20个无人机中心数学问题的UAV-Math-Bench问题集。实验结果表明，结合检索技术可以显著提高精确答案的准确性，减少错误的公式选择，降低数值误差。尽管还需要在更大规模上进行进一步的研究和验证，但这项初步研究结果表明，RAG可以使通用LLM更可靠地用于工程分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主无人机操作需要可靠的数学推理来完成任务，如轨迹规划和电源管理。</li>
<li>传统飞行控制依赖硬编码方程，而大型语言模型（LLM）提供更为灵活的解决方案。</li>
<li>LLM在选择和应用正确的数学公式以及执行精确的多步算术运算方面存在挑战。</li>
<li>RAG-UAV框架旨在通过提供相关领域文献的检索，提高LLM在数学推理方面的能力。</li>
<li>UAV-Math-Bench问题集用于初步评估RAG-UAV的效果，包含20个难度不同的无人机中心数学问题。</li>
<li>结合检索技术可以显著提高精确答案的准确性，减少错误的公式选择和数值误差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bff509a6297ca62456f6ab69d97c9b7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fce262e74d9aa9cad09dc64c99a5bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35801ea526d023ba78ee2ed6c03531d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dff3de08a62ded728053e424b81c3dd0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Objects-to-Anywhere-A-Holistic-Benchmark-for-Multi-level-Visual-Grounding-in-3D-Scenes"><a href="#From-Objects-to-Anywhere-A-Holistic-Benchmark-for-Multi-level-Visual-Grounding-in-3D-Scenes" class="headerlink" title="From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual   Grounding in 3D Scenes"></a>From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual   Grounding in 3D Scenes</h2><p><strong>Authors:Tianxu Wang, Zhuofan Zhang, Ziyu Zhu, Yue Fan, Jing Xiong, Pengxiang Li, Xiaojian Ma, Qing Li</strong></p>
<p>3D visual grounding has made notable progress in localizing objects within complex 3D scenes. However, grounding referring expressions beyond objects in 3D scenes remains unexplored. In this paper, we introduce Anywhere3D-Bench, a holistic 3D visual grounding benchmark consisting of 2,632 referring expression-3D bounding box pairs spanning four different grounding levels: human-activity areas, unoccupied space beyond objects, objects in the scene, and fine-grained object parts. We assess a range of state-of-the-art 3D visual grounding methods alongside large language models (LLMs) and multimodal LLMs (MLLMs) on Anywhere3D-Bench. Experimental results reveal that space-level and part-level visual grounding pose the greatest challenges: space-level tasks require a more comprehensive spatial reasoning ability, for example, modeling distances and spatial relations within 3D space, while part-level tasks demand fine-grained perception of object composition. Even the best performance model, OpenAI o4-mini, achieves only 23.57% accuracy on space-level tasks and 33.94% on part-level tasks, significantly lower than its performance on area-level and object-level tasks. These findings underscore a critical gap in current models’ capacity to understand and reason about 3D scene beyond object-level semantics. </p>
<blockquote>
<p>在复杂的3D场景中定位物体方面，3D视觉定位已经取得了显著的进展。然而，在3D场景中定位对象之外的参照表达仍然未被探索。本文中，我们介绍了Anywhere3D-Bench，这是一个全面的3D视觉定位基准测试，包含2632个参照表达-3D边界框对，跨越四个不同的定位级别：人类活动区域、对象之外的未占用空间、场景中的对象以及精细粒度的对象部分。我们在Anywhere3D-Bench上评估了一系列最先进的3D视觉定位方法，以及与大型语言模型（LLMs）和多模态LLMs（MLLMs）的结合。实验结果表明，空间级别和部件级别的视觉定位构成了最大的挑战：空间级别的任务需要更全面的空间推理能力，例如建模距离和三维空间内的空间关系，而部件级别的任务则要求对对象组成进行精细的感知。即使是表现最佳的模型OpenAI o4-mini在空间级别任务上的准确率也只有23.57%，在部件级别任务上的准确率为33.94%，远低于其在区域级别和对象级别任务上的表现。这些发现强调了当前模型在理解和推理超出对象级别语义的3D场景方面的能力上存在关键差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04897v1">PDF</a> </p>
<p><strong>Summary</strong><br>在复杂的三维场景中定位物体，三维视觉定位技术已经取得了显著进展。然而，本文介绍了一项全新的挑战——超越物体层面的三维场景参照表达定位。我们推出了Anywhere3D-Bench基准测试平台，包含2632个参照表达与三维边界框配对，覆盖四个不同的定位级别。评估结果指出，空间级别和部件级别的视觉定位最具挑战性。空间级别的任务需要全面的空间推理能力，如建模距离和空间关系；部件级别的任务则需要精细的物体构成感知。即使表现最佳的OpenAI o4-mini模型在空间级别和部件级别的任务上的准确率也只有23.57%和33.94%，明显低于其在区域级别和物体级别的表现。这突显了当前模型在理解和推理超越物体层面的三维场景方面的巨大差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D视觉定位技术在物体定位方面已取得显著进展，但超越物体层面的三维场景参照表达定位仍具挑战性。</li>
<li>Anywhere3D-Bench是一个全新的三维视觉定位基准测试平台，包含多种不同级别的定位任务。</li>
<li>空间级别和部件级别的视觉定位任务最具挑战性，需要高级的空间推理和精细物体感知能力。</li>
<li>现有模型在理解和推理超越物体层面的三维场景方面存在显著差距。</li>
<li>即使是表现最佳的模型，在空间级别和部件级别的任务上的准确率也较低。</li>
<li>这一研究强调了未来模型需要提升的空间推理和精细物体感知能力的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-158f3cdfecadc5cef902a7cb7cac580a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24d762b104491c5082deae1cac1be995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5e019e5dd0bfbdc21beaf68ea8d6ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7d6ef1054f46d7fbbd3299b0010acb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b9d2704ae70c233e1fcc09b2ae1066f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ICPC-Eval-Probing-the-Frontiers-of-LLM-Reasoning-with-Competitive-Programming-Contests"><a href="#ICPC-Eval-Probing-the-Frontiers-of-LLM-Reasoning-with-Competitive-Programming-Contests" class="headerlink" title="ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive   Programming Contests"></a>ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive   Programming Contests</h2><p><strong>Authors:Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a> </p>
<blockquote>
<p>随着大型推理模型在复杂编码和推理任务方面取得显著进展，现有的基准测试（如LiveCodeBench和CodeElo）不足以在真实竞争环境中评估大型语言模型（LLM）的编码能力。此外，目前的评估指标（如Pass@K）无法捕捉到推理模型的反射能力。为了应对这些挑战，我们提出了<strong>ICPC-Eval</strong>，这是一个顶级竞技编码基准测试，旨在探索LLM推理的前沿。ICPC-Eval包含了118个精心挑选的问题，这些问题来自近11年来在世界各地举办的ICPC比赛，提供了三个关键贡献：1）模拟现实的ICPC竞赛场景，其问题类型和难度分布与真实竞赛一致；2）一种稳健的测试案例生成方法和相应的本地评估工具包，可实现高效和准确的本地评估；3）一个有效的测试时间缩放评估指标Refine@K，它允许根据执行反馈进行解决方案的迭代修复。结果强调评估复杂推理能力是一项重大挑战：顶级推理模型（如DeepSeek-R1）在与非推理模型相比时，通常需要多次代码反馈来充分发挥其上下文推理潜力。尽管代码生成方面最近取得了进展，但这些模型仍然落后于顶级人类团队。我们在<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%AD%A4%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E3%80%82">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs上发布了此基准测试。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04894v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型推理模型在复杂编码和推理任务方面取得显著进展，但现有评估基准如LiveCodeBench和CodeElo不足以在真实竞赛环境中评估大型语言模型（LLM）的编码能力。此外，当前的评估指标如Pass@K无法捕捉推理模型的反思能力。为解决这些挑战，提出ICPC-Eval评估基准，包含来自世界各地近期举办的ICPC竞赛中的精心挑选的118个问题，提供三个关键贡献：一是模拟真实的ICPC竞赛场景；二是提供稳健的测试案例生成方法和相应的本地评估工具包；三是推出有效的测试时间缩放评估指标Refine@K，允许基于执行反馈进行解决方案的迭代修复。结果显示，评估复杂推理能力存在重大挑战，顶级推理模型如DeepSeek-R1在多轮代码反馈方面的依赖度高，且在代码生成方面仍需赶超顶尖人类团队。详情请访问：<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在复杂编码和推理任务上的进展显著，但现有评估基准不足以在真实竞赛环境中全面评价其性能。</li>
<li>ICPC-Eval是一个新的评估基准，模拟真实的ICPC竞赛场景，旨在评估LLM的推理能力。</li>
<li>ICPC-Eval包含来自世界各地近期ICPC竞赛的问题，提供稳健的测试案例生成和本地评估工具包。</li>
<li>推出新的评估指标Refine@K，允许基于执行反馈进行解决方案的迭代修复。</li>
<li>顶级推理模型如DeepSeek-R1依赖多轮代码反馈来充分发挥其推理潜力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-57cb1519d1a29a166136659236eb8506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd34d81ea122abf780291ff20bdd7d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c124707f584f9566dc35195070ea8c77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66f41f14ad042f410f5875fd5bcfa65e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee08eb41910023cc948ea2c5efc89d8f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Joint-Evaluation-of-Answer-and-Reasoning-Consistency-for-Hallucination-Detection-in-Large-Reasoning-Models"><a href="#Joint-Evaluation-of-Answer-and-Reasoning-Consistency-for-Hallucination-Detection-in-Large-Reasoning-Models" class="headerlink" title="Joint Evaluation of Answer and Reasoning Consistency for Hallucination   Detection in Large Reasoning Models"></a>Joint Evaluation of Answer and Reasoning Consistency for Hallucination   Detection in Large Reasoning Models</h2><p><strong>Authors:Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu</strong></p>
<p>Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, making them a new source of hallucination that is difficult to detect. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the model’s reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the model’s decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. This joint analysis enables fine-grained hallucination detection even when the final answer appears correct. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/bebr2/RACE">https://github.com/bebr2/RACE</a>. </p>
<blockquote>
<p>大型推理模型（LRMs）通过明确的、多步骤的推理轨迹扩展了大规模语言模型，从而增强了在复杂任务上的透明度和性能。然而，这些推理轨迹可能是冗余或逻辑不一致的，成为难以检测的一种新的幻觉来源。现有的幻觉检测方法主要关注答案级别的不确定性，往往无法检测由模型推理轨迹产生的幻觉或逻辑不一致。这种疏忽对于LRM来说尤为严重，其中明确的思维轨迹不仅是模型决策过程的重要支持，也是潜在幻觉的关键来源。为此，我们提出了针对LRM中幻觉检测的RACE（推理与答案一致性评估）新框架。RACE通过提取关键的推理步骤并计算四个诊断信号：推理轨迹的样本间一致性、基于熵的答案不确定性、推理与答案之间的语义对齐以及推理的内部连贯性。这种联合分析即使在最终答案看似正确的情况下，也能实现精细的幻觉检测。在不同数据集和不同LLM上的实验表明，RACE优于现有的幻觉检测基线，为评估LRM提供了稳健和通用的解决方案。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/bebr2/RACE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bebr2/RACE获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04832v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型推理模型（LRMs）结合了大型语言模型与明确的推理步骤，旨在提高复杂任务的透明度和性能。然而，这些推理轨迹可能冗余或逻辑不一致，成为新的难以检测的幻想来源。本文提出了一种专门针对LRM中幻觉检测的新框架——RACE（推理与答案一致性评估）。RACE通过提取关键推理步骤并计算四种诊断信号（推理轨迹的跨样本一致性、基于熵的答案不确定性、推理与答案之间的语义对齐以及推理的内部连贯性）来进行精细的幻觉检测。实验表明，RACE在数据集和大型语言模型上的表现均优于现有幻觉检测基线，为评估LRM提供了稳健且可推广的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型（LRMs）结合了大型语言模型与明确的推理步骤，旨在增强复杂任务的性能。</li>
<li>LRMs中的推理轨迹可能冗余或逻辑不一致，成为新的幻觉来源。</li>
<li>现有幻觉检测方法主要关注答案级别的不确定性，往往无法检测到由模型推理轨迹产生的幻觉或逻辑不一致。</li>
<li>RACE框架通过提取关键推理步骤和计算四种诊断信号进行精细的幻觉检测。</li>
<li>RACE框架包括跨样本的推理轨迹一致性、答案的不确定性、推理与答案的语义对齐以及推理的内部连贯性。</li>
<li>实验表明，RACE在多个数据集和大型语言模型上的表现优于现有幻觉检测基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0fcd32d5fa2d99918a8a60e82dab7280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5bd5fbc4669756f09163786b0b8cef4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MMSU-A-Massive-Multi-task-Spoken-Language-Understanding-and-Reasoning-Benchmark"><a href="#MMSU-A-Massive-Multi-task-Spoken-Language-Understanding-and-Reasoning-Benchmark" class="headerlink" title="MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark"></a>MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning   Benchmark</h2><p><strong>Authors:Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, Helen Meng</strong></p>
<p>Speech inherently contains rich acoustic information that extends far beyond the textual language. In real-world spoken language understanding, effective interpretation often requires integrating semantic meaning (e.g., content), paralinguistic features (e.g., emotions, speed, pitch) and phonological characteristics (e.g., prosody, intonation, rhythm), which are embedded in speech. While recent multimodal Speech Large Language Models (SpeechLLMs) have demonstrated remarkable capabilities in processing audio information, their ability to perform fine-grained perception and complex reasoning in natural speech remains largely unexplored. To address this gap, we introduce MMSU, a comprehensive benchmark designed specifically for understanding and reasoning in spoken language. MMSU comprises 5,000 meticulously curated audio-question-answer triplets across 47 distinct tasks. To ground our benchmark in linguistic theory, we systematically incorporate a wide range of linguistic phenomena, including phonetics, prosody, rhetoric, syntactics, semantics, and paralinguistics. Through a rigorous evaluation of 14 advanced SpeechLLMs, we identify substantial room for improvement in existing models, highlighting meaningful directions for future optimization. MMSU establishes a new standard for comprehensive assessment of spoken language understanding, providing valuable insights for developing more sophisticated human-AI speech interaction systems. MMSU benchmark is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ddwang2000/MMSU">https://huggingface.co/datasets/ddwang2000/MMSU</a>. Evaluation Code is available at <a target="_blank" rel="noopener" href="https://github.com/dingdongwang/MMSU_Bench">https://github.com/dingdongwang/MMSU_Bench</a>. </p>
<blockquote>
<p>语音本身包含丰富的声音信息，远远超出文本语言的范围。在现实世界的口语理解中，有效的解释通常需要整合语义（例如内容）、副语言特征（例如情感、语速、音调）和语音特征（例如韵律、语调、节奏），这些特征都嵌入在语音中。虽然最近的多媒体语音大语言模型（SpeechLLMs）在处理音频信息方面表现出了显著的能力，但它们在自然语音的精细感知和复杂推理方面的能力仍待探索。为了弥补这一差距，我们引入了MMSU，这是一个专门为口语理解和推理而设计的综合基准测试。MMSU包含5000个精心挑选的音频-问题-答案三元组，涵盖47个不同的任务。为了在我们的基准测试中融入语言学理论，我们系统地融入了一系列的语言现象，包括语音学、韵律、修辞、句法、语义和副语言学。通过对14个先进的SpeechLLM的严格评估，我们发现了现有模型的改进空间很大，并指出了未来优化的有意义方向。MMSU为口语理解的全面评估建立了新标准，为开发更复杂的人类-人工智能语音交互系统提供了宝贵的见解。MMSU基准测试可在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ddwang2000/MMSU">https://huggingface.co/datasets/ddwang2000/MMSU</a>获取。评估代码可在<a target="_blank" rel="noopener" href="https://github.com/dingdongwang/MMSU_Bench">https://github.com/dingdongwang/MMSU_Bench</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04779v1">PDF</a> MMSU benchmark is available at   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ddwang2000/MMSU">https://huggingface.co/datasets/ddwang2000/MMSU</a>. Evaluation Code is available   at <a target="_blank" rel="noopener" href="https://github.com/dingdongwang/MMSU_Bench">https://github.com/dingdongwang/MMSU_Bench</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音包含丰富的声音信息，这些信息远远超出文本语言。在现实世界的口语理解中，有效的解释往往需要整合语义（例如内容）、副语言特征（例如情感、语速、音调）和语音特征（例如韵律、语调、节奏），这些特征都嵌入在语音中。为了理解和推理口语，我们引入了MMSU，这是一个专门为口语理解而设计的综合基准测试。MMSU包含5000个精心策划的音频-问题-答案三元组，跨越47个不同的任务。通过严格的评估，我们发现现有模型仍有很大的改进空间，为未来的优化指明了有意义的方向。MMSU为全面评估口语理解建立了新的标准，为开发更复杂的人机语音交互系统提供了有价值的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音包含丰富的声音信息，超越文本语言。</li>
<li>有效的口语理解需要整合语义、副语言特征和语音特征。</li>
<li>MMSU是一个专门用于口语理解的综合基准测试。</li>
<li>MMSU包含5000个音频-问题-答案三元组，跨越47个任务。</li>
<li>现有语音LLM模型在精细感知和复杂推理方面仍有改进空间。</li>
<li>MMSU为口语理解建立了新的评估标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04779">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-83775bd894dd31699e9bdff1eeec3af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ebe57b946e07556d641ee02d32671a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac749dfa9b08441fdb1d9b3f7aac5c48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4839781e6418b3c55cc01d1cd83a62f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fb57d31a4d5387cc09aed126635fb13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d81217bed9cda7ae4ffa963153163e6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Truth-in-the-Few-High-Value-Data-Selection-for-Efficient-Multi-Modal-Reasoning"><a href="#Truth-in-the-Few-High-Value-Data-Selection-for-Efficient-Multi-Modal-Reasoning" class="headerlink" title="Truth in the Few: High-Value Data Selection for Efficient Multi-Modal   Reasoning"></a>Truth in the Few: High-Value Data Selection for Efficient Multi-Modal   Reasoning</h2><p><strong>Authors:Shenshen Li, Kaiyuan Deng, Lei Wang, Hao Yang, Chong Peng, Peng Yan, Fumin Shen, Heng Tao Shen, Xing Xu</strong></p>
<p>While multi-modal large language models (MLLMs) have made significant progress in complex reasoning tasks via reinforcement learning, it is commonly believed that extensive training data is necessary for improving multi-modal reasoning ability, inevitably leading to data redundancy and substantial computational costs. However, can smaller high-value datasets match or outperform full corpora for multi-modal reasoning in MLLMs? In this work, we challenge this assumption through a key observation: meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, termed cognitive samples, whereas the majority contribute marginally. Building on this insight, we propose a novel data selection paradigm termed Reasoning Activation Potential (RAP), which identifies cognitive samples by estimating each sample’s potential to stimulate genuine multi-modal reasoning by two complementary estimators: 1) Causal Discrepancy Estimator (CDE) based on the potential outcome model principle, eliminates samples that overly rely on language priors by comparing outputs between multi-modal and text-only inputs; 2) Attention Confidence Estimator (ACE), which exploits token-level self-attention to discard samples dominated by irrelevant but over-emphasized tokens in intermediate reasoning stages. Moreover, we introduce a Difficulty-aware Replacement Module (DRM) to substitute trivial instances with cognitively challenging ones, thereby ensuring complexity for robust multi-modal reasoning. Experiments on six datasets show that our RAP method consistently achieves superior performance using only 9.3% of the training data, while reducing computational costs by over 43%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Leo-ssl/RAP">https://github.com/Leo-ssl/RAP</a>. </p>
<blockquote>
<p>虽然多模态大型语言模型（MLLMs）通过强化学习在复杂推理任务方面取得了显著进展，但人们普遍认为，为了提高多模态推理能力，需要大量的训练数据，这不可避免地导致了数据冗余和巨大的计算成本。然而，较小的高价值数据集是否能在多模态推理方面与完整语料库相匹配甚至表现更好？在这项工作中，我们通过一项关键观察结果来挑战这一假设：有意义的多模态推理仅由一小部分训练样本触发，这些样本被称为认知样本，而大多数样本贡献甚微。基于这一见解，我们提出了一种新的数据选择范式，称为“推理激活潜力”（RAP），它通过两个互补的估计器来识别认知样本：1）基于潜在结果模型原理的因果差异估计器（CDE），通过比较多模态和文本输入的输出，消除过度依赖语言先验的样本；2）注意力置信估计器（ACE），利用标记级别的自我注意力来丢弃在推理阶段被无关但过度强调的标记所主导的样本。此外，我们引入了一个难度感知替换模块（DRM），以认知上具有挑战性的实例替换掉简单的实例，从而确保复杂性以实现稳健的多模态推理。在六个数据集上的实验表明，我们的RAP方法仅使用9.3%的训练数据就能持续实现优越的性能，同时降低计算成本超过43%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Leo-ssl/RAP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Leo-ssl/RAP上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文挑战了多模态大型语言模型（MLLMs）在多模态推理中必须依赖大量训练数据的传统观念。通过关键观察发现，有意义的多模态推理仅由训练样本中的稀疏子集（称为认知样本）触发，其余大部分样本贡献甚微。基于此，提出了一种新的数据选择范式——推理激活潜力（RAP），通过两个互补的评估器来识别认知样本：1）因果差异评估器（CDE），基于潜在结果模型原理，通过比较多模态和文本只有输入的输出来消除过度依赖语言先验的样本；2）注意力信心评估器（ACE），利用token级别的自我关注来丢弃在中间推理阶段被无关紧要的token主导的样本。实验结果表明，RAP方法仅使用9.3%的训练数据即可实现卓越性能，同时降低计算成本超过43%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在复杂推理任务中取得了显著进步，但普遍认为需要大量训练数据来提高多模态推理能力，导致数据冗余和计算成本高昂。</li>
<li>本文挑战了这一假设，认为有意义的多模态推理仅由训练样本中的稀疏子集（认知样本）触发。</li>
<li>提出了一种新的数据选择范式——推理激活潜力（RAP），能够识别认知样本，通过估计每个样本刺激真实多模态推理的潜力。</li>
<li>RAP方法包括两个互补的评估器：因果差异评估器（CDE）和注意力信心评估器（ACE），分别用于比较多模态和文本只有输入的输出来识别过度依赖语言先验的样本，以及利用token级别的自我关注来识别在中间推理阶段被无关紧要的token主导的样本。</li>
<li>引入了一个难度感知替换模块（DRM），用具有挑战性的实例替换简单的实例，确保复杂性的多模态推理。</li>
<li>实验结果表明，RAP方法使用仅9.3%的训练数据即可实现卓越性能，同时降低计算成本超过43%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04755">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3144c37c87389faa648add44ba7a99f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93a7cd062a7c0439a172f0a9931a13b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6737c92057f5813e2fae5df925d425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6de3b7b57f62e9de8d8b0fbb46937fa0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-Layer-GRPO-Enhancing-Reasoning-and-Self-Correction-in-Large-Language-Models"><a href="#Multi-Layer-GRPO-Enhancing-Reasoning-and-Self-Correction-in-Large-Language-Models" class="headerlink" title="Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large   Language Models"></a>Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large   Language Models</h2><p><strong>Authors:Fei Ding, Baiqiao Wang, Zijian Zeng, Youwei Wang</strong></p>
<p>The Group Relative Policy Optimization (GRPO) algorithm has demonstrated considerable success in enhancing the reasoning capabilities of large language models (LLMs), as evidenced by DeepSeek-R1. However, the absence of intermediate supervision in GRPO frequently leads to inefficient exploration dynamics. A single error in a complex reasoning chain can invalidate the entire solution, resulting in abrupt reward vanishing and compromising training stability.To address these challenges, we propose MGRPO (Multi-layer GRPO). MGRPO operates in two layers: the first layer employs standard GRPO to generate an initial response. This response, along with the original query, is then fed into a second-layer GRPO process. This second layer is specifically trained to identify and correct errors in the initial response, effectively creating a self-correction loop. This mechanism provides implicit process-level supervision by rewarding successful error correction, without requiring an explicit, densely-annotated reward model. Experimental results on several mathematical reasoning benchmarks demonstrate that MGRPO significantly outperforms standard GRPO, achieving superior performance by fostering both reasoning and self-correction abilities. </p>
<blockquote>
<p>集团相对策略优化（GRPO）算法在提高大型语言模型（LLM）的推理能力方面取得了显著的成功，DeepSeek-R1对此提供了证据。然而，GRPO中缺乏中间监督通常会导致低效的探索动态。复杂的推理链中的一个错误可能会使整个解决方案无效，导致奖励突然消失，影响训练稳定性。为了解决这些挑战，我们提出了多层GRPO（MGRPO）。MGRPO有两层操作：第一层采用标准GRPO生成初步回应。然后，该响应与原始查询一起输入到第二层GRPO过程中。第二层专门训练以识别和纠正初步回应中的错误，从而有效地创建一个自我纠正循环。该机制通过奖励成功的错误纠正，提供了隐式的流程级监督，而无需使用明确、密集注释的奖励模型。在多个数学推理基准测试上的实验结果表明，MGRPO显著优于标准GRPO，通过促进推理和自我纠正能力，实现了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04746v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>GRPO算法在提升大型语言模型的推理能力方面取得了显著成功，但缺乏中间监督导致探索动态效率低下。为解决此问题，提出多层GRPO（MGRPO）。MGRPO分为两层操作，第一层使用标准GRPO生成初步回应，然后将其与原始查询一起输入到第二层GRPO过程中进行错误识别与修正，形成自我修正循环。这种机制通过奖励成功的错误修正，无需明确的密集注释奖励模型，即可提供隐式过程级监督。在多个数学推理基准测试上，MGRPO显著优于标准GRPO，通过促进推理和自纠能力实现卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>GRPO算法增强了大型语言模型的推理能力，但缺乏中间监督导致探索效率不高。</li>
<li>MGRPO算法采用两层操作，第一层生成初步回应，第二层进行错误识别与修正。</li>
<li>MGRPO通过自我修正循环，提高了模型应对复杂推理链中错误的能力。</li>
<li>MGRPO采用隐式过程级监督机制，通过奖励成功的错误修正，无需密集注释的奖励模型。</li>
<li>MGRPO在多个数学推理基准测试上显著优于标准GRPO。</li>
<li>MGRPO算法促进了模型的推理和自纠能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7228a8e20a990565074c82eb131fa150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8d270cdb91e99d86bed94c46910e7e8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Empowering-Economic-Simulation-for-Massively-Multiplayer-Online-Games-through-Generative-Agent-Based-Modeling"><a href="#Empowering-Economic-Simulation-for-Massively-Multiplayer-Online-Games-through-Generative-Agent-Based-Modeling" class="headerlink" title="Empowering Economic Simulation for Massively Multiplayer Online Games   through Generative Agent-Based Modeling"></a>Empowering Economic Simulation for Massively Multiplayer Online Games   through Generative Agent-Based Modeling</h2><p><strong>Authors:Bihan Xu, Shiwei Zhao, Runze Wu, Zhenya Huang, Jiawei Wang, Zhipeng Hu, Kai Wang, Haoyu Liu, Tangjie Lv, Le Li, Changjie Fan, Xin Tong, Jiangze Han</strong></p>
<p>Within the domain of Massively Multiplayer Online (MMO) economy research, Agent-Based Modeling (ABM) has emerged as a robust tool for analyzing game economics, evolving from rule-based agents to decision-making agents enhanced by reinforcement learning. Nevertheless, existing works encounter significant challenges when attempting to emulate human-like economic activities among agents, particularly regarding agent reliability, sociability, and interpretability. In this study, we take a preliminary step in introducing a novel approach using Large Language Models (LLMs) in MMO economy simulation. Leveraging LLMs’ role-playing proficiency, generative capacity, and reasoning aptitude, we design LLM-driven agents with human-like decision-making and adaptability. These agents are equipped with the abilities of role-playing, perception, memory, and reasoning, addressing the aforementioned challenges effectively. Simulation experiments focusing on in-game economic activities demonstrate that LLM-empowered agents can promote emergent phenomena like role specialization and price fluctuations in line with market rules. </p>
<blockquote>
<p>在大型多人在线（MMO）经济研究领域，基于代理的建模（ABM）已经成为分析游戏经济学的强大工具，从基于规则的代理发展到通过强化学习增强的决策代理。然而，现有工作在尝试模拟代理之间的人类经济活动时面临重大挑战，特别是在代理的可靠性、社交能力和解释性方面。本研究初步引入了一种新的方法，利用大型语言模型（LLMs）进行MMO经济模拟。借助LLMs的角色扮演能力、生成能力和推理能力，我们设计了具有人类式决策和适应能力的LLM驱动代理。这些代理具备角色扮演、感知、记忆和推理的能力，有效地解决了上述挑战。以游戏内经济活动为重点的模拟实验表明，LLM赋能的代理可以促进诸如角色专业化和价格波动的市场规则相符的新兴现象。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04699v1">PDF</a> KDD2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>在大型多人在线（MMO）经济研究领域，基于代理的建模（ABM）已逐渐成为分析游戏经济的有力工具，从基于规则的代理发展到通过强化学习增强决策能力的代理。然而，现有研究在尝试模拟代理之间的人类经济活动时面临诸多挑战，特别是在代理的可靠性、社交能力和可解释性方面。本研究初步引入大型语言模型（LLM）用于MMO经济模拟，借助LLM的角色扮演能力、生成能力和推理能力，设计出具有人类决策和适应能力的LLM驱动代理。这些代理具备角色扮演、感知、记忆和推理能力，能有效应对上述挑战。模拟实验关注游戏内经济活动，证明LLM赋能的代理能够促进角色专业化、价格波动等符合市场规则的现象。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agent-Based Modeling (ABM)是分析游戏经济的重要工具，但它模拟人类经济活动的准确性有待提高。</li>
<li>大型语言模型（LLM）在角色扮演、生成能力和推理能力方面表现出色，可应用于MMO经济模拟。</li>
<li>LLM驱动的代理具备人类决策和适应能力，能有效解决ABM面临的挑战，如代理的可靠性、社交能力和可解释性。</li>
<li>LLM赋能的代理能促进角色专业化和价格波动等符合市场规则的现象。</li>
<li>LLM在MMO经济模拟中的应用是创新性的尝试，为未来游戏经济模拟提供了新的方向。</li>
<li>该研究为结合人工智能与游戏经济的研究提供了有价值的参考。</li>
<li>大型语言模型和基于代理的建模相结合的方法在其他领域（如社会科学、经济学）的模拟中也可能具有潜在应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04699">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5597fceac849f54973a291c81cd2ce9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-345ed8147158bbfcb8e5549f55c94871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4a2980f6fb752bd47f508d606fa674.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1115abba10b0aec7debaa6bb475c23fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c0d607ab82234d0592acc390c1e4b6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="On-the-Mechanism-of-Reasoning-Pattern-Selection-in-Reinforcement-Learning-for-Language-Models"><a href="#On-the-Mechanism-of-Reasoning-Pattern-Selection-in-Reinforcement-Learning-for-Language-Models" class="headerlink" title="On the Mechanism of Reasoning Pattern Selection in Reinforcement   Learning for Language Models"></a>On the Mechanism of Reasoning Pattern Selection in Reinforcement   Learning for Language Models</h2><p><strong>Authors:Xingwu Chen, Tianle Li, Difan Zou</strong></p>
<p>Reinforcement learning (RL) has demonstrated remarkable success in enhancing model capabilities, including instruction-following, preference learning, and reasoning. Yet despite its empirical successes, the mechanisms by which RL improves reasoning abilities remain poorly understood. We present a systematic study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that its primary benefit comes from optimizing the selection of existing reasoning patterns. Through extensive experiments, we demonstrate that RLVR-trained models preferentially adopt high-success-rate reasoning patterns while mostly maintaining stable performance on individual patterns. We further develop theoretical analyses on the convergence and training dynamics of RLVR based on a simplified question-reason-answer model. We study the gradient flow and show that RLVR can indeed find the solution that selects the reason pattern with the highest success rate. Besides, our theoretical results   reveal two distinct regimes regarding the convergence of RLVR training: (1) rapid convergence for models with relatively strong initial reasoning capabilities versus (2) slower optimization dynamics for weaker models. Furthermore, we show that the slower optimization for weaker models can be mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using a feasibly high-quality SFT dataset. We validate the theoretical findings through extensive experiments. This work advances our theoretical understanding of RL’s role in LLM fine-tuning and offers insights for further enhancing reasoning capabilities. </p>
<blockquote>
<p>强化学习（RL）在提高模型能力方面取得了显著的成果，包括指令遵循、偏好学习和推理等。然而，尽管其在实证研究中取得了成功，但RL提高推理能力的机制仍知之甚少。我们对带有可验证奖励的强化学习（RLVR）进行了系统研究，表明其主要优势在于优化现有推理模式的选择。通过广泛的实验，我们证明RLVR训练的模型倾向于采用高成功率的推理模式，同时基本上保持了对单个模式的稳定性能。我们进一步基于简化的问答模型，对RLVR的收敛性和训练动态进行了理论分析。我们研究了梯度流，并证明RLVR确实可以找到选择成功率最高的推理模式的解决方案。此外，我们的理论结果揭示了RLVR训练的收敛过程中的两种不同状态：（1）对于具有相对较强的初始推理能力的模型的快速收敛，以及（2）对于较弱的模型的较慢优化动态。而且，我们还发现，在使用高质量的有监督微调（SFT）数据集进行RLVR之前，可以通过应用有监督微调来缓解较弱模型的较慢优化问题。我们通过广泛的实验验证了这些理论发现。这项工作加深了我们对于RL在大型语言模型微调中的作用的理论理解，并为进一步提高推理能力提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04695v1">PDF</a> 30 pages, 6 figures, 1 table</p>
<p><strong>Summary</strong><br>强化学习（RL）在提升模型能力方面取得了显著的成功，包括指令遵循、偏好学习和推理等。然而，尽管其实践应用取得了成功，但RL提升推理能力的机制仍知之甚少。本研究对带有可验证奖励的强化学习（RLVR）进行了系统研究，发现其主要优势在于优化现有推理模式的选取。通过广泛的实验，我们展示了RLVR训练的模型会优先采用高成功率的推理模式，同时大致保持对单个模式的稳定性能。我们还基于简化的问答模型对RLVR的收敛性和训练动态进行了理论分析，并发现RLVR确实能找到选择成功率最高的推理模式的解决方案。此外，我们的理论结果还揭示了RLVR训练的收敛存在两种不同的情况：一是具有相对较强的初始推理能力的模型的快速收敛，二是较弱模型的较慢优化动态。在较弱的模型上，我们可以通过在RLVR之前应用有监督微调（SFT）来缓解这个问题，使用高质量的SFT数据集是可行的。本研究加深了我们对于RL在大型语言模型微调中作用的理论理解，并为进一步提高推理能力提供了见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习（RL）已用于提升模型的多项能力，包括指令遵循、偏好学习和推理。</li>
<li>RLVR的主要优势在于优化现有推理模式的选取。</li>
<li>RLVR训练的模型会优先采用高成功率的推理模式。</li>
<li>RLVR训练的理论分析揭示了其收敛性的两个不同情况，以及针对较弱模型的优化动态。</li>
<li>对于初始推理能力较弱的模型，可以通过有监督微调（SFT）来缓解较慢的优化动态。</li>
<li>使用高质量的SFT数据集在RLVR训练前是有效的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04695">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c733c3dbf8049d47eed28701716c2e98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40533fd93dde105a2d9019c23cf80adf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f7c8b2413febdd96a99e8a1df4b5dbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afaf10b2b1071c20da239ad8ea76e682.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CHANCERY-Evaluating-corporate-governance-reasoning-capabilities-in-language-models"><a href="#CHANCERY-Evaluating-corporate-governance-reasoning-capabilities-in-language-models" class="headerlink" title="CHANCERY: Evaluating corporate governance reasoning capabilities in   language models"></a>CHANCERY: Evaluating corporate governance reasoning capabilities in   language models</h2><p><strong>Authors:Lucas Irwin, Arda Kaz, Peiyao Sheng, Pramod Viswanath</strong></p>
<p>Law has long been a domain that has been popular in natural language processing (NLP) applications. Reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. Nevertheless, while multiple legal datasets exist, none have thus far focused specifically on reasoning tasks. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark (CHANCERY) to test a model’s ability to reason about whether executive&#x2F;board&#x2F;shareholder’s proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models - modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The model’s task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents exhibit superior performance, with agents based on the ReAct and CodeAct frameworks scoring 76.1% and 78.1% respectively, further confirming the advanced legal reasoning capabilities required to score highly on the benchmark. We also conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of SOTA models. </p>
<blockquote>
<p>法律领域长期以来在自然语言处理（NLP）应用中颇受欢迎。推理（包括逻辑推断和与先例建立联系的能力）是现实世界中法律实践的核心部分。然而，尽管存在多个法律数据集，但目前还没有专门针对推理任务的数据集。我们通过引入公司治理推理基准测试（CHANCERY），关注法律环境的一个特定方面，以测试模型对高管&#x2F;董事会&#x2F;股东提出的行动是否符合公司治理章程的推理能力。该基准测试首创了一种针对语言模型的公司治理推理测试——模拟现实世界的公司治理法。该基准测试包括公司章程（一组管理契约）和一项高管行动提案。模型的任务是二分类任务之一：推理行动是否符合章程中的规则。我们遵循公司治理的既定原则创建了这一基准测试，包括24项具体的公司治理原则和从包含1万份真实公司章程的总数据集中挑选出的79份代表不同行业的章程。对最新推理模型的评估证实了该基准测试的难度，如Claude 3.7 Sonnet和GPT-4o等模型分别实现了64.5%和75.2%的准确率。推理代理表现出卓越的性能，基于ReAct和CodeAct框架的代理分别得分76.1%和78.1%，这进一步证实了要在基准测试中获得高分需要先进的法律推理能力。我们还分析了当前推理模型面临困难的类型的问题，揭示了尖端模型的法律推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04636v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>本文介绍了一个企业治理推理基准测试（CHANCERY），旨在测试模型对企业治理章程中董事、股东和执行层行为的合规性判断能力。此基准测试采用现实企业治理法律原则，包含了企业章程和一份高管行动提案。模型的任务是对行动是否符合章程规定进行二元分类判断。评估显示，当前先进的推理模型在该基准测试中表现尚待提升，而具备特定框架的推理代理表现较好。同时，文章还分析了当前推理模型面临的问题类型，揭示了其对先进模型法律推理能力的深刻见解。

**Key Takeaways**

1. 法律是自然语言处理（NLP）应用中的热门领域，但现有的法律数据集尚未专注于推理任务。
2. 引入了一个全新的企业治理推理基准测试（CHANCERY），用于评估模型判断企业治理章程中行为合规性的能力。
3. 该基准测试包含企业章程和一份高管行动提案，模型需判断行动是否符合章程规定。
4. 当前先进的推理模型在该基准测试中表现有待提高，而特定框架的推理代理表现较好。
5. 企业治理推理基准测试遵循现实企业治理法律原则，具有实际应用场景价值。
6. 文章分析了推理模型面临的问题类型，揭示了其法律推理能力的弱点。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04636">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f312da29397be4f3e13017327b90ad5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceeb1e39bfff816768326c1692c37f13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-368924729e8aae91811cf976e1c382ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff5390e160a0d2860af39ec4a08aa638.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Look-Before-You-Leap-A-GUI-Critic-R1-Model-for-Pre-Operative-Error-Diagnosis-in-GUI-Automation"><a href="#Look-Before-You-Leap-A-GUI-Critic-R1-Model-for-Pre-Operative-Error-Diagnosis-in-GUI-Automation" class="headerlink" title="Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error   Diagnosis in GUI Automation"></a>Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error   Diagnosis in GUI Automation</h2><p><strong>Authors:Yuyang Wanyan, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Jiabo Ye, Yutong Kou, Ming Yan, Fei Huang, Xiaoshan Yang, Weiming Dong, Changsheng Xu</strong></p>
<p>In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model’s feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency. </p>
<blockquote>
<p>近年来，多模态大型语言模型（MLLMs）已被广泛应用于多模态推理任务，包括图形用户界面（GUI）自动化。不同于一般的离线多模态任务，GUI自动化是在在线互动环境中执行，需要根据环境的实时状态进行一步一步的决策。此任务对每一步的决策错误容忍度较低，因为任何错误都可能累积并破坏过程，并可能导致不可逆的结果，如删除或付款。为了解决这些问题，我们引入了一种术前批评机制，通过推理潜在的结果和行动的正确性，在实际执行之前提供有效的反馈。具体来说，我们提出了一个建议感知梯度相对策略优化（S-GRPO）策略来构建我们的术前批评模型GUI-Critic-R1，结合一种新的建议奖励来提高模型反馈的可靠性。此外，我们开发了一个基于推理引导的数据收集管道来创建GUI-Critic-Train和GUI-Critic-Test，以填补现有GUI评论家数据中的空白。在移动和网络领域的GUI-Critic-Test上的静态实验表明，我们的GUI-Critic-R1在评论家准确性方面提供了显著的优势，与当前的多模态大型语言模型相比。在GUI自动化基准测试上的动态评估进一步突出了我们模型的有效性和优越性，提高了成功率和操作效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04614v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型（MLLMs）在图形用户界面（GUI）自动化中的最新应用。为提高在线互动环境中每一步决策的正确性，降低累积错误的风险，提出一种术前批评机制，通过预测潜在结果和行动的正确性来提供有效反馈。采用建议奖励的策略优化术前批评模型GUI-Critic-R1的可靠性。此外，建立基于推理引导的数据收集管道，创建GUI-Critic-Train和GUI-Critic-Test数据集，填补GUI批评数据的空白。实验结果表明，GUI-Critic-R1在批评精度上较当前MLLMs有显著优势，并在GUI自动化基准测试中验证了其有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）已广泛应用于包括GUI自动化在内的多模态推理任务。</li>
<li>GUI自动化需在在线互动环境中进行，要求基于实时环境的状况进行逐步决策。</li>
<li>提出一种术前批评机制，通过预测潜在结果和行动的正确性提供有效反馈。</li>
<li>采用建议奖励的策略优化GUI-Critic-R1模型。</li>
<li>建立基于推理引导的数据收集管道，创建GUI-Critic数据集以填补现有空白。</li>
<li>GUI-Critic-R1在批评精度上较当前MLLMs有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04614">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f5e00fc533e610c5a8d545f2bbd147bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be09ca89e8b02a4bd05d03767ce414f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e111669a9a39cdf9fa8ff178bad8ad.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Are-LLMs-Reliable-Translators-of-Logical-Reasoning-Across-Lexically-Diversified-Contexts"><a href="#Are-LLMs-Reliable-Translators-of-Logical-Reasoning-Across-Lexically-Diversified-Contexts" class="headerlink" title="Are LLMs Reliable Translators of Logical Reasoning Across Lexically   Diversified Contexts?"></a>Are LLMs Reliable Translators of Logical Reasoning Across Lexically   Diversified Contexts?</h2><p><strong>Authors:Qingchuan Li, Jiatong Li, Zirui Liu, Mingyue Cheng, Yuting Zeng, Qi Liu, Tongxuan Liu</strong></p>
<p>Neuro-symbolic approaches combining large language models (LLMs) with solvers excels in logical reasoning problems need long reasoning chains. In this paradigm, LLMs serve as translators, converting natural language reasoning problems into formal logic formulas. Then reliable symbolic solvers return correct solutions. Despite their success, we find that LLMs, as translators, struggle to handle lexical diversification, a common linguistic phenomenon, indicating that LLMs as logic translators are unreliable in real-world scenarios. Moreover, existing logical reasoning benchmarks lack lexical diversity, failing to challenge LLMs’ ability to translate such text and thus obscuring this issue. In this work, we propose SCALe, a benchmark designed to address this significant gap through <strong>logic-invariant lexical diversification</strong>. By using LLMs to transform original benchmark datasets into lexically diversified but logically equivalent versions, we evaluate LLMs’ ability to consistently map diverse expressions to uniform logical symbols on these new datasets. Experiments using SCALe further confirm that current LLMs exhibit deficiencies in this capability. Building directly on the deficiencies identified through our benchmark, we propose a new method, MenTaL, to address this limitation. This method guides LLMs to first construct a table unifying diverse expressions before performing translation. Applying MenTaL through in-context learning and supervised fine-tuning (SFT) significantly improves the performance of LLM translators on lexically diversified text. Our code is now available at <a target="_blank" rel="noopener" href="https://github.com/wufeiwuwoshihua/LexicalDiver">https://github.com/wufeiwuwoshihua/LexicalDiver</a>. </p>
<blockquote>
<p>神经符号方法结合了大型语言模型（LLM）和解算器，在需要长推理链的逻辑推理问题中表现出色。在此范式中，LLM充当翻译，将自然语言推理问题转换为正式的逻辑公式。然后可靠的符号解算器返回正确的解决方案。尽管它们取得了成功，但我们发现LLM作为翻译在处理词汇多样化这一常见语言现象时遇到了困难，这表明LLM作为逻辑翻译在真实场景中是不可靠的。此外，现有的逻辑推理基准测试缺乏词汇多样性，未能挑战LLM翻译此类文本的能力，从而掩盖了这个问题。在这项工作中，我们提出了SCALE基准测试，旨在通过逻辑不变的词汇多样化来解决这一重要差距。通过使用LLM将原始基准数据集转换为词汇丰富但逻辑等价的版本，我们在这些新数据集上评估了LLM将各种表达一致地映射到统一逻辑符号的能力。使用SCALE基准测试的实验进一步证实，当前LLM在这方面存在缺陷。通过我们的基准测试确定的不足，我们提出了一种新的方法MenTaL来解决这一限制。此方法引导LLM在翻译之前首先构建一个统一各种表达的表格。通过上下文学习和监督微调（SFT）应用MenTaL，可以显着提高LLM翻译在词汇丰富文本上的表现。我们的代码现已在<a target="_blank" rel="noopener" href="https://github.com/wufeiwuwoshihua/LexicalDiver%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/wufeiwuwoshihua/LexicalDiver上可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04575v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了神经符号方法结合大型语言模型（LLMs）和求解器在处理需要长推理链的逻辑推理问题时的优势与不足。文中指出，LLMs作为翻译者，在应对词汇多样化这一常见语言现象时表现不佳。为此，本文提出了一种名为SCALe的基准测试，旨在通过逻辑不变的词汇多样化来解决这一难题，评估LLMs在不同表达形式转换为逻辑符号的能力。实验结果表明，当前LLMs在此能力上存在缺陷。为解决此问题，本文提出了MenTaL方法，通过构建统一不同表达的表格来进行翻译，显著提高了LLMs在词汇丰富文本上的翻译性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经符号方法结合了大型语言模型（LLMs）和求解器，擅长处理需要长推理链的逻辑推理问题。</li>
<li>LLMs在作为逻辑翻译器时，处理词汇多样化存在困难，这在现实场景中是常见的语言现象。</li>
<li>现有逻辑推理基准测试缺乏词汇多样性，无法充分挑战LLMs的翻译能力，从而掩盖了这一问题。</li>
<li>本文提出了SCALe基准测试，旨在通过逻辑不变的词汇多样化来评估LLMs的能力。</li>
<li>实验结果表明，当前LLMs在将不同表达转换为逻辑符号方面存在缺陷。</li>
<li>为解决LLMs在词汇丰富文本翻译上的不足，本文提出了MenTaL方法，通过构建统一表达表格进行翻译。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04575">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-908908829676fd2a9b67c0e3c5944acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f714a2141c22fdfbd20a66162d542a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb9f10f46fb948e1601614660a1d8ba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9516d853722093a2d318a9f4206fdc19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b8f1acfe87251b962601dbc24a4239c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32b866d248726a476ef58ea795420284.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-31593c9a93ac5fd04bd000716e4cef24.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-08  SparseMM Head Sparsity Emerges from Visual Concept Responses in MLLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a42c1726d4ab1f7ccf9aeffb4ab2564a.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-06-06  EnergyMoGen Compositional Human Motion Generation with Energy-Based   Diffusion Model in Latent Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
