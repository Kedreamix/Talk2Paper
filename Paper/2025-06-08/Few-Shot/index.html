<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-08  Just a Scratch Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0e52c6ee4cd9ee24a5c221b6ed9faf13.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-08-更新"><a href="#2025-06-08-更新" class="headerlink" title="2025-06-08 更新"></a>2025-06-08 更新</h1><h2 id="Just-a-Scratch-Enhancing-LLM-Capabilities-for-Self-harm-Detection-through-Intent-Differentiation-and-Emoji-Interpretation"><a href="#Just-a-Scratch-Enhancing-LLM-Capabilities-for-Self-harm-Detection-through-Intent-Differentiation-and-Emoji-Interpretation" class="headerlink" title="Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation"></a>Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation</h2><p><strong>Authors:Soumitra Ghosh, Gopendra Vikram Singh,  Shambhavi, Sabarna Choudhury, Asif Ekbal</strong></p>
<p>Self-harm detection on social media is critical for early intervention and mental health support, yet remains challenging due to the subtle, context-dependent nature of such expressions. Identifying self-harm intent aids suicide prevention by enabling timely responses, but current large language models (LLMs) struggle to interpret implicit cues in casual language and emojis. This work enhances LLMs’ comprehension of self-harm by distinguishing intent through nuanced language-emoji interplay. We present the Centennial Emoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with contextual self-harm interpretations and the Self-Harm Identification aNd intent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering detailed annotations for self-harm labels, casual mentions (CMs), and serious intents (SIs). Our unified framework: a) enriches inputs using CESM-100; b) fine-tunes LLMs for multi-task learning: self-harm detection (primary) and CM&#x2F;SI span detection (auxiliary); c) generates explainable rationales for self-harm predictions. We evaluate the framework on three state-of-the-art LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and fine-tuned scenarios. By coupling intent differentiation with contextual cues, our approach commendably enhances LLM performance in both detection and explanation tasks, effectively addressing the inherent ambiguity in self-harm signals. The SHINES dataset, CESM-100 and codebase are publicly available at: <a target="_blank" rel="noopener" href="https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES">https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES</a> . </p>
<blockquote>
<p>社交媒体上的自我伤害检测对于早期干预和精神健康支持至关重要，但由于此类表达的细微和语境依赖性，它仍然是一个挑战。识别自我伤害意图有助于通过及时响应预防自杀，但当前的大型语言模型（LLM）在解释日常用语和表情符号中的隐含线索方面存在困难。这项工作通过区分语言与表情符号之间的微妙互动，增强了大型语言模型对自我伤害意图的理解。我们推出了百年表情符号敏感性矩阵（CESM-100），这是一组包含语境自我伤害解释的100个表情符号的精选集，以及自我伤害识别与意图提取（SHINES）数据集，为自我伤害标签、偶然提及（CM）和严重意图（SI）提供了详细的注释。我们的统一框架包括：a）使用CESM-100丰富输入；b）微调LLM进行多任务学习：自我伤害检测（主要任务）和CM&#x2F;SI跨度检测（辅助任务）；c）生成自我伤害预测的可解释理由。我们在三个最先进的大型语言模型上评估了该框架：Llama 3、Mental-Alpaca和MentalLlama，以及在零样本、少样本和微调场景中进行了测试。通过意图区分与上下文线索相结合，我们的方法在提高检测和解释任务的性能方面都取得了可喜的效果，有效地解决了自我伤害信号中的固有模糊性。SHINES数据集、CESM-100和源代码可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES">https://www.iitp.ac.in/~ai-nlp-ml&#x2F;resources.html#SHINES</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05073v1">PDF</a> To be published in the Proceedings of the 63rd Annual Meeting of the   Association for Computational Linguistics (ACL 2025 Main)</p>
<p><strong>Summary</strong></p>
<p>此文本关于通过利用情绪矩阵和数据集增强大型语言模型对社交媒体上自我伤害表达意图的理解。研究团队推出了Centennial Emoji Sensitivity Matrix（CESM-100）和SHINES数据集，以丰富输入和微调大型语言模型。该框架通过区分自我伤害的意图和上下文线索，提高了大型语言模型在自我伤害检测任务中的性能，并生成解释性理由。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体的自我伤害检测对早期干预和精神健康支持至关重要。</li>
<li>当前大型语言模型在解读隐晦线索和情绪表达方面存在挑战。</li>
<li>CESM-100包含100个具有上下文自我伤害解读的emoji。</li>
<li>SHINES数据集提供详细的自我伤害标签、随意提及（CMs）和严重意图（SIs）注释。</li>
<li>统一框架包括：使用CESM-100丰富输入，微调大型语言模型进行多任务学习，生成自我伤害预测的解释性理由。</li>
<li>该框架在三种先进的大型语言模型中表现良好，能够有效提高自我伤害检测和解释任务的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05073">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bfe558e4984976c692a5fe4a15f7648d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4452dd9fa0879ba7a377eb999790838d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3747d7fd6351e8416546f8852382457a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465632bcc02f4ef5ca85da501be64120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82b0638295029e85872b1df0872f9bc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-360276a37f65ee33463f8d4aa23ec8df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b8ac9ac883ee7d84e58bafc5037dbfa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompting-LLMs-Length-Control-for-Isometric-Machine-Translation"><a href="#Prompting-LLMs-Length-Control-for-Isometric-Machine-Translation" class="headerlink" title="Prompting LLMs: Length Control for Isometric Machine Translation"></a>Prompting LLMs: Length Control for Isometric Machine Translation</h2><p><strong>Authors:Dávid Javorský, Ondřej Bojar, François Yvon</strong></p>
<p>In this study, we explore the effectiveness of isometric machine translation across multiple language pairs (En$\to$De, En$\to$Fr, and En$\to$Es) under the conditions of the IWSLT Isometric Shared Task 2022. Using eight open-source large language models (LLMs) of varying sizes, we investigate how different prompting strategies, varying numbers of few-shot examples, and demonstration selection influence translation quality and length control. We discover that the phrasing of instructions, when aligned with the properties of the provided demonstrations, plays a crucial role in controlling the output length. Our experiments show that LLMs tend to produce shorter translations only when presented with extreme examples, while isometric demonstrations often lead to the models disregarding length constraints. While few-shot prompting generally enhances translation quality, further improvements are marginal across 5, 10, and 20-shot settings. Finally, considering multiple outputs allows to notably improve overall tradeoff between the length and quality, yielding state-of-the-art performance for some language pairs. </p>
<blockquote>
<p>在这项研究中，我们在IWSLT等距共享任务2022的条件下，探索了等距机器翻译在多语言对（英语到德语、英语到法语和英语到西班牙语）之间的有效性。我们使用八种开源的大型语言模型（LLM），探讨了不同的提示策略、不同数量的少量示例和演示选择对翻译质量和长度控制的影响。我们发现，当指令的措辞与所提供的演示属性相符时，对输出长度的控制起着至关重要的作用。我们的实验表明，只有当给出极端示例时，LLM才会产生较短的翻译，而等距演示往往导致模型忽略长度约束。虽然少量提示通常可以提高翻译质量，但在5、10和20个示例的设置中，进一步的改进幅度较小。最后，考虑多个输出可以显著改善长度和质量之间的总体权衡，为某些语言对带来最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04855v1">PDF</a> Accepted to IWSLT 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了IWSLT等距共享任务2022条件下，使用八个不同规模的大型语言模型（LLMs）对跨多语言对（如英语至德语、英语至法语和英语至西班牙语）的等距机器翻译的有效性。研究探讨了不同的提示策略、不同数量的少量示例和演示选择如何影响翻译质量和长度控制。实验发现，当指令与提供的演示内容相匹配时，指令措辞对控制输出长度起着至关重要的作用。此外，LLMs往往只在呈现极端示例时才产生较短的翻译，而等距演示往往导致模型忽略长度约束。虽然少量提示通常可以提高翻译质量，但在5、10和20次拍摄设置之间的进一步改进是微不足道的。最后，考虑多个输出可以显著改善长度和质量之间的总体权衡，对某些语言对而言达到了最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在IWSLT等距共享任务下，研究了跨多种语言对的等距机器翻译效果。</li>
<li>使用八个不同规模的大型语言模型进行实验。</li>
<li>研究发现指令措辞与演示内容的匹配对控制输出长度至关重要。</li>
<li>LLMs在呈现极端示例时产生较短的翻译。</li>
<li>等距演示可能导致模型忽略长度约束。</li>
<li>少量提示可以提高翻译质量，但进一步提高有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-02dc132cc0d64b2685782166266019b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ebe5c25aa1c30e85a4dc4fdd9c55ba8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddfec3f9130efe6b77128806a84468ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5500da1eaf7a3e0b22c82a07f0a429aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50e5dc30a8acae2ae6a9f261bc934a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bbb46b1ef7dce415d4e7b657a7f1b06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aff9cbc7a06de0edaadaa4a924d05a87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-157d00cca897e4be07abd8dd67bcea76.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-Few-Shot-Vision-Language-Model-Adaptation"><a href="#Robust-Few-Shot-Vision-Language-Model-Adaptation" class="headerlink" title="Robust Few-Shot Vision-Language Model Adaptation"></a>Robust Few-Shot Vision-Language Model Adaptation</h2><p><strong>Authors:Hanxin Wang, Tian Liu, Shu Kong</strong></p>
<p>Pretrained VLMs achieve strong performance on downstream tasks when adapted with just a few labeled examples. As the adapted models inevitably encounter out-of-distribution (OOD) test data that deviates from the in-distribution (ID) task-specific training data, enhancing OOD generalization in few-shot adaptation is critically important. We study robust few-shot VLM adaptation, aiming to increase both ID and OOD accuracy. By comparing different adaptation methods (e.g., prompt tuning, linear probing, contrastive finetuning, and full finetuning), we uncover three key findings: (1) finetuning with proper hyperparameters significantly outperforms the popular VLM adaptation methods prompt tuning and linear probing; (2) visual encoder-only finetuning achieves better efficiency and accuracy than contrastively finetuning both visual and textual encoders; (3) finetuning the top layers of the visual encoder provides the best balance between ID and OOD accuracy. Building on these findings, we propose partial finetuning of the visual encoder empowered with two simple augmentation techniques: (1) retrieval augmentation which retrieves task-relevant data from the VLM’s pretraining dataset to enhance adaptation, and (2) adversarial perturbation which promotes robustness during finetuning. Results show that the former&#x2F;latter boosts OOD&#x2F;ID accuracy while slightly sacrificing the ID&#x2F;OOD accuracy. Yet, perhaps understandably, naively combining the two does not maintain their best OOD&#x2F;ID accuracy. We address this dilemma with the developed SRAPF, Stage-wise Retrieval Augmentation-based Adversarial Partial Finetuning. SRAPF consists of two stages: (1) partial finetuning the visual encoder using both ID and retrieved data, and (2) adversarial partial finetuning with few-shot ID data. Extensive experiments demonstrate that SRAPF achieves the state-of-the-art ID and OOD accuracy on the ImageNet OOD benchmarks. </p>
<blockquote>
<p>预训练VLM模型仅在少量标记样本的情况下就能适应下游任务并表现出强大的性能。由于适应的模型不可避免地会遇到偏离特定任务训练数据的分布外（OOD）测试数据，因此在有限样本中提高OOD的泛化能力变得至关重要。我们研究了稳健的少样本VLM适应性训练，旨在提高IN和OOD的准确率。通过对不同的适应性训练方法的比较（如提示调整、线性探测、对比微调以及全微调），我们发现了三个关键观点：（1）使用适当的超参数进行微调显著优于流行的VLM适应性训练方法提示调整和线性探测；（2）仅对视觉编码器进行微调，相比于对比式地同时微调视觉和文本编码器，能更有效地提高效率和准确性；（3）微调视觉编码器的顶层提供了IN和OOD准确率之间的最佳平衡。基于这些发现，我们提出了对视觉编码器进行部分微调，辅以两种简单的增强技术：（1）检索增强技术，它从VLM的预训练数据集中检索任务相关数据以增强适应性；（2）对抗扰动技术，它在微调过程中促进稳健性。结果表明，前者提高了OOD准确性，而后者则提高了IN准确性，但两者同时使用时可能无法保持其最佳的OOD或IN准确性。为解决这一困境，我们开发了基于阶段检索增强的对抗性部分微调（SRAPF）。SRAPF分为两个阶段：（1）使用IN和检索数据对视觉编码器进行部分微调；（2）使用少量的IN数据进行对抗性部分微调。大量实验表明，SRAPF在ImageNet OOD基准测试中实现了最先进的IN和OOD准确率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04713v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://hannawang09.github.io/projects/srapf/">https://hannawang09.github.io/projects/srapf/</a></p>
<p><strong>Summary</strong></p>
<p>预训练VLM在少量标注样本的适应下，能在下游任务上实现出色的性能。为了适应不可避免地遇到偏离特定任务训练数据的分布外（OOD）测试数据的问题，提高少样本VLM适应的OOD泛化能力至关重要。本研究旨在提高ID和OOD的准确性。通过比较不同的适应方法，我们发现全参数微调能显著优于流行的VLM适应方法，如提示微调线性探测和对比微调；仅微调视觉编码器比对比微调视觉和文本编码器更有效且准确；微调视觉编码器的顶层在ID和OOD准确性之间提供了最佳的平衡。基于这些发现，我们提出了对视觉编码器的部分微调，并结合两种简单的增强技术：检索增强和对抗扰动。最后为了解决二者结合的困境提出了SRAPF（基于阶段检索增强的对抗性部分微调）。SRAPF包含两个阶段，能在ImageNet OOD基准测试中实现最佳的ID和OOD准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练VLM模型在少样本适应下表现出色，特别是在下游任务上。</li>
<li>增强少样本VLM适应的OOD泛化至关重要。</li>
<li>全参数微调是一种有效的VLM适应方法，优于提示微调、线性探测和对比微调。</li>
<li>仅微调视觉编码器既提高了效率又提高了准确性。</li>
<li>检索增强技术能提高OOD准确性，而对抗扰动则促进模型在微调过程中的稳健性。</li>
<li>合并两种增强技术需要解决的最佳策略是使用SRAPF（基于阶段检索增强的对抗性部分微调）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04713">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bc2740816b43053f844edf6ce8fc66e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f73b06cdf365560c7f80822b169df9a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-360a0e1e2b5cf121bf9b910f2f07314f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfaa80dace65ed81a1fa70c5124d2b6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724cf77aeb8b260db872ae712c808730.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Interpretable-Few-Shot-Image-Classification-via-Prototypical-Concept-Guided-Mixture-of-LoRA-Experts"><a href="#Interpretable-Few-Shot-Image-Classification-via-Prototypical-Concept-Guided-Mixture-of-LoRA-Experts" class="headerlink" title="Interpretable Few-Shot Image Classification via Prototypical   Concept-Guided Mixture of LoRA Experts"></a>Interpretable Few-Shot Image Classification via Prototypical   Concept-Guided Mixture of LoRA Experts</h2><p><strong>Authors:Zhong Ji, Rongshuai Wei, Jingren Liu, Yanwei Pang, Jungong Han</strong></p>
<p>Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance.To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module.Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns.In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability.Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries.Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%-8.7% relative gains in 5-way 5-shot classification.These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems. </p>
<blockquote>
<p>自解释模型（SEMs）依赖于原型概念学习（PCL）来使其视觉识别过程更具可解释性，但在数据稀缺的环境中，由于训练样本不足导致性能不佳，它们经常面临挑战。为了解决这一局限性，我们提出了一个Few-Shot原型概念分类（FSPCC）框架，该框架系统地减轻了低数据环境下的两个关键挑战：参数不平衡和表示不匹配。具体来说，我们的方法利用LoRA专家混合物（MoLE）进行参数有效的适应，确保在主干和PCL模块之间分配可训练参数的平衡。同时，跨模块概念引导强制执行主干特征表示与原型概念激活模式之间的紧密对齐。此外，我们采用了一种多层次特征保留策略，融合了各层中的空间和语义线索，从而丰富了学习的表示并减轻了有限数据可用性所带来的挑战。最后，为了提高可解释性并最小化概念重叠，我们引入了一种几何感知概念判别损失，该损失强制执行概念之间的正交性，鼓励更解耦和透明的决策边界。在六个流行基准测试（CUB-200-2011、mini-ImageNet、CIFAR-FS、Stanford Cars、FGVC-Aircraft和DTD）上的实验结果表明，我们的方法在所有情况下均显著优于现有SEM，在5路5分类射击分类中相对提高了4.2%-8.7%。这些发现表明将概念学习与少镜头适应相结合以实现更高的准确性和更清晰的模型可解释性是有效的，这为更透明的视觉识别系统铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04673v1">PDF</a> 13 pages,5 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个Few-Shot Prototypical Concept Classification（FSPCC）框架，解决了数据稀缺环境下自我解释模型（SEMs）面临的问题。通过利用混合LoRA专家（MoLE）进行参数高效适应，以及跨模块概念引导和多层次特征保留策略，该框架提高了模型的性能。同时，引入几何感知概念判别损失，增强了解释性并减少概念重叠。实验结果表明，该框架在多个数据集上均显著优于现有SEMs，为耦合概念学习与少样本适应以实现更高精度和更清晰模型解释铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了Few-Shot Prototypical Concept Classification (FSPCC) 框架以解决数据稀缺环境中SEMs的问题。</li>
<li>通过混合LoRA专家（MoLE）实现参数高效适应，平衡分配可训练参数。</li>
<li>跨模块概念引导确保特征表示与原型概念激活模式紧密对齐。</li>
<li>采用多层次特征保留策略，融合空间语义线索，丰富学习到的表示并应对数据有限挑战。</li>
<li>引入几何感知概念判别损失，增强模型解释性和概念之间的正交性。</li>
<li>实验结果在多数据集上表明FSPCC框架显著优于现有SEMs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04673">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6afc3d91271a8c4a354deab1a6d6ae64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e52c6ee4cd9ee24a5c221b6ed9faf13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689022c329247048e7e21c1a619303b4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AuthGuard-Generalizable-Deepfake-Detection-via-Language-Guidance"><a href="#AuthGuard-Generalizable-Deepfake-Detection-via-Language-Guidance" class="headerlink" title="AuthGuard: Generalizable Deepfake Detection via Language Guidance"></a>AuthGuard: Generalizable Deepfake Detection via Language Guidance</h2><p><strong>Authors:Guangyu Shen, Zhihua Li, Xiang Xu, Tianchen Zhao, Zheng Zhang, Dongsheng An, Zhuowen Tu, Yifan Xing, Qin Zhang</strong></p>
<p>Existing deepfake detection techniques struggle to keep-up with the ever-evolving novel, unseen forgeries methods. This limitation stems from their reliance on statistical artifacts learned during training, which are often tied to specific generation processes that may not be representative of samples from new, unseen deepfake generation methods encountered at test time. We propose that incorporating language guidance can improve deepfake detection generalization by integrating human-like commonsense reasoning – such as recognizing logical inconsistencies and perceptual anomalies – alongside statistical cues. To achieve this, we train an expert deepfake vision encoder by combining discriminative classification with image-text contrastive learning, where the text is generated by generalist MLLMs using few-shot prompting. This allows the encoder to extract both language-describable, commonsense deepfake artifacts and statistical forgery artifacts from pixel-level distributions. To further enhance robustness, we integrate data uncertainty learning into vision-language contrastive learning, mitigating noise in image-text supervision. Our expert vision encoder seamlessly interfaces with an LLM, further enabling more generalized and interpretable deepfake detection while also boosting accuracy. The resulting framework, AuthGuard, achieves state-of-the-art deepfake detection accuracy in both in-distribution and out-of-distribution settings, achieving AUC gains of 6.15% on the DFDC dataset and 16.68% on the DF40 dataset. Additionally, AuthGuard significantly enhances deepfake reasoning, improving performance by 24.69% on the DDVQA dataset. </p>
<blockquote>
<p>现有深度伪造检测技术难以跟上不断演变的全新和未知伪造方法。这一局限性源于它们依赖于训练过程中学习的统计特征，这些特征通常与特定的生成过程相关联，可能无法代表测试时遇到的新未知深度伪造生成方法的样本。我们提出，通过融入语言指导可以改善深度伪造检测的泛化能力，方法是结合人类常识推理，如识别逻辑不一致和感知异常等，同时辅以统计线索。为此，我们通过结合判别分类和图像文本对比学习来训练专业的深度伪造视觉编码器。其中文本是由通用MLLM通过少量提示生成的。这使得编码器能够从像素级分布中提取语言可描述、常识性的深度伪造特征和统计伪造特征。为了进一步提高稳健性，我们将数据不确定性学习整合到视觉语言对比学习中，减轻图像文本监督中的噪声。我们的专业视觉编码器无缝地接口与大型语言模型，进一步实现了更通用和可解释的深度伪造检测，同时提高了准确性。所得到的框架AuthGuard在内外分布环境中均达到了最先进的深度伪造检测准确率，在DFDC数据集上AUC增益达6.15%，在DF40数据集上达16.68%。此外，AuthGuard显著提高了深度伪造推理能力，在DDVQA数据集上的性能提高24.69%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着深度伪造技术的不断发展，现有检测手段难以应对新型、未见过的伪造方法。为此，本文提出结合语言指导提高深度伪造检测的泛化能力，通过集成人类常识推理，如逻辑不一致和感知异常的认识，以及统计线索。通过训练深度伪造视觉编码器，结合判别分类和图像文本对比学习，利用少量提示生成文本。这允许编码器从像素级分布中提取语言可描述的常识性深度伪造伪迹和统计伪造伪迹。为了进一步增加稳健性，本文还将数据不确定性学习融入视觉语言对比学习，减轻图像文本监督中的噪声。本文的专家视觉编码器与大型语言模型无缝对接，更通用、可解释的深度伪造检测在提高了准确性的同时。所得到的框架AuthGuard在内外分布设置中实现了最先进的深度伪造检测精度，在DFDC数据集上AUC增益达6.15%，在DF40数据集上增益达16.68%。此外，AuthGuard还显著提高了深度伪造推理性能，在DDVQA数据集上的性能提高了24.69%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有深度伪造检测技术面临挑战，难以应对新型未见过的伪造方法。</li>
<li>结合语言指导提高深度伪造检测的泛化能力，通过集成人类常识推理。</li>
<li>通过训练深度伪造视觉编码器，结合判别分类与图像文本对比学习。</li>
<li>编码器可提取语言可描述的常识性深度伪造伪迹和统计伪造伪迹。</li>
<li>集成数据不确定性学习增强稳健性，减轻图像文本监督中的噪声。</li>
<li>专家视觉编码器与大型语言模型无缝对接，提高深度伪造检测的准确性和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cbec10a7a8d1009c9997ed2e7c494f31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0abeb3fa97b3337cdb3def1bd79b0a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b5cc3a0e4f6df8acb79946871915b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79972f7ab4a471d88d1a8423e4103cff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Building-a-Few-Shot-Cross-Domain-Multilingual-NLU-Model-for-Customer-Care"><a href="#Building-a-Few-Shot-Cross-Domain-Multilingual-NLU-Model-for-Customer-Care" class="headerlink" title="Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer   Care"></a>Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer   Care</h2><p><strong>Authors:Saurabh Kumar, Sourav Bansal, Neeraj Agrawal, Priyanka Bhatt</strong></p>
<p>Customer care is an essential pillar of the e-commerce shopping experience with companies spending millions of dollars each year, employing automation and human agents, across geographies (like US, Canada, Mexico, Chile), channels (like Chat, Interactive Voice Response (IVR)), and languages (like English, Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on annotated data have shown good performance in downstream tasks relevant to Customer Care. However, model performance is largely subject to the availability of sufficient annotated domain-specific data. Cross-domain availability of data remains a bottleneck, thus building an intent classifier that generalizes across domains (defined by channel, geography, and language) with only a few annotations, is of great practical value. In this paper, we propose an embedder-cum-classifier model architecture which extends state-of-the-art domain-specific models to other domains with only a few labeled samples. We adopt a supervised fine-tuning approach with isotropic regularizers to train a domain-specific sentence embedder and a multilingual knowledge distillation strategy to generalize this embedder across multiple domains. The trained embedder, further augmented with a simple linear classifier can be deployed for new domains. Experiments on Canada and Mexico e-commerce Customer Care dataset with few-shot intent detection show an increase in accuracy by 20-23% against the existing state-of-the-art pre-trained models. </p>
<blockquote>
<p>客户服务是电子商务购物体验的重要支柱，公司每年花费数百万美元，跨越地理区域（如美国、加拿大、墨西哥、智利）、渠道（如聊天、交互式语音响应（IVR）和语言（如英语、西班牙语）采用自动化和人工代理。使用预训练模型如多语言BERT，并在注释数据上进行微调，在与客户服务相关的下游任务中表现出良好性能。然而，模型性能在很大程度上取决于足够的注释领域特定数据的可用性。跨域数据的可用性仍然是一个瓶颈，因此，建立一个仅使用少量注释即可跨领域（由渠道、地理和语言定义）推广的意图分类器，具有巨大的实用价值。在本文中，我们提出了一种嵌入分类器模型架构，该架构将最先进领域特定的模型推广到少数样本的其它领域。我们采用带有同构正则化的监督微调方法来训练特定领域的句子嵌入器，并采用多语言知识蒸馏策略使该嵌入器跨多个领域进行推广。经过训练的嵌入器进一步与一个简单线性分类器结合，可用于新领域。在加拿大和墨西哥电子商务客户服务数据集上进行的小样本意图检测实验表明，与现有的最先进的预训练模型相比，准确率提高了20%-23%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了客户关怀在电子商务购物体验中的重要性，并指出企业在不同的地理区域、渠道和语言上投入巨资。当前最先进的预训练模型如多语言BERT在客户关怀相关下游任务中表现出良好性能，但模型性能很大程度上取决于充足的注释领域数据的可用性。针对跨领域数据可用性的瓶颈，本文提出了一种嵌入分类器模型架构，该架构仅使用少量标注样本即可将最先进的领域特定模型扩展到其他领域。通过采用带有同构正则化的监督微调方法来训练领域特定的句子嵌入器，以及采用多语言知识蒸馏策略来使嵌入器跨多个领域进行概括。将经过训练的嵌入器进一步配合简单的线性分类器部署到新的领域中。在加拿大和墨西哥电子商务客户关怀数据集上的少量意图检测实验表明，与现有的最先进的预训练模型相比，准确率提高了20-23%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>客户关怀在电子商务中扮演着重要角色，企业为此投入巨大。</li>
<li>先进的预训练模型如多语言BERT在客户关怀相关任务中表现良好，但受限于注释领域数据的可用性。</li>
<li>跨领域数据可用性是瓶颈，需要模型能够在少量标注样本下跨领域进行推广。</li>
<li>提出的嵌入分类器模型架构能够通过监督微调方法训练领域特定的句子嵌入器。</li>
<li>采用多语言知识蒸馏策略使嵌入器跨多个领域进行概括。</li>
<li>训练的嵌入器配合简单的线性分类器可部署到新的领域中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04389">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-311f775004090127677f7cc61b25fc9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aab59962f74941d67bd575248327282c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab018830ce3194d3c76c7f23f733757d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce401acf417d05c7bf8cadf148f8f734.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a7b4f3fb46a75d06c95df8edded0bb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53dc8c9e4e7c5fee235ad6669d63a925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff4ec810c9d67e30c06f86b261fc42e9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can-Masked-Autoencoders-Also-Listen-to-Birds"><a href="#Can-Masked-Autoencoders-Also-Listen-to-Birds" class="headerlink" title="Can Masked Autoencoders Also Listen to Birds?"></a>Can Masked Autoencoders Also Listen to Birds?</h2><p><strong>Authors:Lukas Rauch, René Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz</strong></p>
<p>Masked Autoencoders (MAEs) have shown competitive results in audio classification by learning rich semantic representations through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, thereby revealing the performance limitations of general-domain Audio-MAE models. This work demonstrates that bridging this domain gap requires more than domain-specific pretraining data; adapting the entire training pipeline is crucial. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSet’s multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAE’s prototypical probes outperform linear probing by up to 37%$_\text{p}$ in MAP and narrow the gap to fine-tuning to approximately 3.3%$_\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains. </p>
<blockquote>
<p>基于Masked Autoencoders（MAEs）在音频分类中展现了出色的结果，它通过高效的自监督重建任务学习丰富的语义表示。然而，当通用模型直接应用于细粒度音频域时，其泛化能力较差。具体来说，鸟类声音分类需要区分物种间的细微差异并应对高物种内部的声音变化，从而揭示了通用领域音频MAE模型的性能局限性。这项工作表明，缩小这一领域差距不仅需要特定领域的预训练数据；调整整个训练管道也至关重要。我们系统地回顾并适应了预训练配方、微调方法以及使用BirdSet（一个与AudioSet相当的大规模生物声学数据集）的鸟类声音的冻结特征利用。我们得到的Bird-MAE在BirdSet的多标签分类基准测试中取得了最新成果。此外，我们引入了参数高效的原型探测，增强了冻结MAE表示的实用性，并在低资源环境中接近微调性能。Bird-MAE的原型探针在MAP中的表现比线性探针高出最多37%，并缩小了在BirdSet下游任务上的微调差距，平均约达3.3%。Bird-MAE还在我们新建立的BirdSet少样本基准测试中展示了强大的少样本能力，突显了针对细粒度音频领域量身定制的自监督学习管道的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12880v2">PDF</a> under review @TMLR</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Masked Autoencoders（MAEs）在音频分类中的竞争性能，尤其是在鸟声分类中的应用。研究发现在精细粒度音频领域，通用模型存在局限性。为了克服这一局限性，本研究重新设计并优化了预训练配方、微调方法和冻结特征的利用方式，以适应鸟声数据。同时引入了参数高效的原型探测技术，提高了冻结MAE表示的实用性，并在低资源环境中接近微调性能。最终，Bird-MAE模型在BirdSet多标签分类基准测试中取得了最新先进成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Autoencoders (MAEs) 在音频分类中展现出竞争力，特别是在鸟声分类中的应用。</li>
<li>通用模型在精细粒度音频领域存在局限性，需要针对特定领域进行优化。</li>
<li>研究重新设计了预训练配方、微调方法和冻结特征的利用方式，以适应鸟声数据。</li>
<li>引入了参数高效的原型探测技术，提高了冻结MAE表示的实用性。</li>
<li>Bird-MAE模型在BirdSet多标签分类基准测试中取得了最新成果。</li>
<li>Bird-MAE模型通过原型探测展示了强大的少样本能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c95e35d561e795abc926d5577adde137.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3241ad7918215e514014c801ff5ce4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83fd6ee9d0f1462de6c3ed1077eefd77.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SNaRe-Domain-aware-Data-Generation-for-Low-Resource-Event-Detection"><a href="#SNaRe-Domain-aware-Data-Generation-for-Low-Resource-Event-Detection" class="headerlink" title="SNaRe: Domain-aware Data Generation for Low-Resource Event Detection"></a>SNaRe: Domain-aware Data Generation for Low-Resource Event Detection</h2><p><strong>Authors:Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Event Detection (ED) – the task of identifying event mentions from natural language text – is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot&#x2F;few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaRe’s stronger annotation quality and reduced domain drift. </p>
<blockquote>
<p>事件检测（ED）——从自然语言文本中识别事件提及的任务——对于在生物医学、法律和流行病学等高度专业化领域进行推理至关重要。数据生成已证明在扩大其在更广泛应用中的效用方面非常有效，而无需昂贵的专家注释。然而，当将现有的生成方法应用于专业领域时，它们会面临标签噪声的问题，即注释不正确，以及领域漂移，表现为生成句子与目标领域之间的分布不匹配。为了解决这些问题，我们引入了SNaRe，这是一个领域感知的合成数据生成框架，由三个组件组成：侦察兵（Scout）、叙述者（Narrator）和精炼者（Refiner）。Scout从目标领域的未标记数据中提取触发器，并使用语料库级别的统计信息来整理高质量的专业特定触发器列表，以缓解领域漂移。叙述者根据这些触发器生成高质量且与领域相符的句子，而精炼者则识别其他事件提及，确保高注释质量。在三个不同领域的ED数据集上的实验表明，SNaRe的表现优于最佳基线，在零样本&#x2F;少样本设置中平均F1得分提高了3-7%，在多语言生成中F1得分提高了4-20%。通过分析生成的触发器命中率和人类评估，证实了SNaRe的注释质量更高，领域漂移减少。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17394v2">PDF</a> Under review at ACL ARR May 2025</p>
<p><strong>Summary</strong></p>
<p>事件检测（ED）是从自然语言文本中识别事件提及的任务，对于生物医学、法律和流行病学等高度专业化领域中的推理至关重要。数据生成已证明可以有效扩大其在更广泛应用中的实用性，而无需昂贵的专家注释。然而，当现有生成方法应用于专业领域时，它们会面临标签噪声（注释不正确）和领域漂移（生成句子与目标领域之间的分布不匹配）的问题。为解决这些问题，我们引入了SNaRe，这是一个领域感知的合成数据生成框架，由三个组件组成：侦察兵、叙述者和精炼者。侦察兵从目标领域的未标记数据中提取触发器，并使用语料库级别的统计数据来优化高质量的领域特定触发器列表，以减轻领域漂移的问题。叙述者根据这些触发器生成高质量的领域对齐句子，而精炼者则识别其他事件提及，确保高注释质量。在三个不同的领域ED数据集上的实验表明，SNaRe的表现超过了最佳基线，在零样本&#x2F;少样本设置中平均F1得分提高了3-7%，在多语言生成中提高了4-20%的F1得分。对生成的触发器命中率和人类评估的分析证实了SNaRe更强的注释质量和减少的领域漂移问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>事件检测（ED）在高度专业化的领域中如生物医学、法律和流行病学中起到关键作用。</li>
<li>数据生成扩大了事件检测在更广泛应用中的实用性，但面临标签噪声和领域漂移的挑战。</li>
<li>SNaRe是一个领域感知的合成数据生成框架，包含侦察兵、叙述者和精炼者三个组件。</li>
<li>侦察兵通过提取目标领域的触发器并优化高质量的领域特定触发器列表来减轻领域漂移问题。</li>
<li>叙述者根据触发器生成高质量的领域对齐句子。</li>
<li>精炼者确保高注释质量，通过识别其他事件提及。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17394">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fde820f63794c3e97f86be76f6cf424a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07e1df7a4c7e9ae0b6579835f046ec8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbd0a0ce9c561e145c6eac9c1a070aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ee014b8b748ec27f7643a9aee8df598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d9a82ecb493cd0335bfddff8c300dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d32cbfad380367c8a2b8dbc65a47cd8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unleashing-The-Power-of-Pre-Trained-Language-Models-for-Irregularly-Sampled-Time-Series"><a href="#Unleashing-The-Power-of-Pre-Trained-Language-Models-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Unleashing The Power of Pre-Trained Language Models for Irregularly   Sampled Time Series"></a>Unleashing The Power of Pre-Trained Language Models for Irregularly   Sampled Time Series</h2><p><strong>Authors:Weijia Zhang, Chenlong Yin, Hao Liu, Hui Xiong</strong></p>
<p>Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series analysis, intending to create a unified foundation model that addresses various time series analytical tasks. However, these efforts predominantly focus on Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by Irregularly Sampled Time Series (ISTS), which are characterized by uneven sampling intervals and prevalent missing data. To bridge this gap, this work takes the first step in exploring the potential of PLMs for ISTS analysis. We begin by investigating the effect of various methods for representing ISTS, aiming to maximize the efficacy of PLMs in the analysis. Furthermore, we propose a unified PLM-based framework, named ISTS-PLM, to address diverse ISTS analytical tasks. It integrates novel time-aware and variable-aware PLMs tailored to tackle the intractable intra- and inter-time series modeling in ISTS. Finally, extensive experiments on a comprehensive benchmark demonstrate that the ISTS-PLM, utilizing a structured and effective series-based representation for ISTS, consistently achieves state-of-the-art performance across various analytical tasks, such as classification, interpolation, extrapolation, few-shot and zero-shot learning scenarios, spanning scientific domains like healthcare, biomechanics, and climate science. </p>
<blockquote>
<p>预训练语言模型（如ChatGPT）在自然语言处理领域取得了显著进展。这一进步激发了一系列创新研究，探索将预训练语言模型适应于时间序列分析，旨在创建一个统一的基础模型，以解决各种时间序列分析任务。然而，这些努力主要集中在规则采样时间序列（RSTS）上，忽视了不规则采样时间序列（ISTS）带来的独特挑战，其特点是采样间隔不均匀且普遍存在缺失数据。为了填补这一空白，本研究首次探索了预训练语言模型在ISTS分析中的潜力。我们首先从研究表示ISTS的各种方法的效果开始，旨在最大限度地提高预训练语言模型在分析中的有效性。此外，我们提出了一种基于预训练语言模型的统一框架，名为ISTS-PLM，用于处理多样的ISTS分析任务。它集成了新型的时间感知和变量感知预训练语言模型，专门用于解决ISTS中复杂的单时间序列和多时间序列建模问题。最后，在全面的基准测试上进行的大量实验表明，ISTS-PLM利用结构化和有效的ISTS系列表示方法，在各种分析任务中始终实现了最先进的性能，如分类、插值、外推、少样本和零样本学习场景，涵盖医疗、生物力学和气候科学等科学领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08328v2">PDF</a> Accepted by KDD’25</p>
<p><strong>Summary</strong></p>
<p>预训练语言模型（PLMs）如ChatGPT在自然语言处理领域取得了显著进展，并激发了一系列将其适应时间序列分析的研究。然而，这些研究主要关注规则采样时间序列（RSTS），忽略了不规则采样时间序列（ISTS）带来的独特挑战，如不均匀的采样间隔和普遍存在的缺失数据。本研究首次探索了PLMs在ISTS分析中的潜力，研究了表示ISTS的各种方法，并提出了一个统一的基于PLMs的框架ISTS-PLM，以解决多样的ISTS分析任务。该框架结合了新型的时间感知和变量感知PLMs，以处理ISTS中的复杂的时间序列内和时间序列间的建模问题。在全面的基准测试上的大量实验表明，ISTS-PLM利用结构化和有效的系列表示方法，在各种分析任务上实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练语言模型（PLMs）在自然语言处理领域有重大进展。</li>
<li>PLMs正在被探索以适应时间序列分析。</li>
<li>目前的研究主要关注规则采样时间序列（RSTS），忽略了不规则采样时间序列（ISTS）的挑战。</li>
<li>ISTS具有不均匀的采样间隔和普遍的缺失数据。</li>
<li>本研究探索了PLMs在ISTS分析中的潜力，并研究了表示ISTS的不同方法。</li>
<li>提出了一个名为ISTS-PLM的统一框架，该框架结合时间感知和变量感知PLMs以解决多样的ISTS分析任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08328">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e6d27ccbf37218e745435247ca1e852e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474a385a7704ebe30d7fc158fd21b1dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a4c0516c9c33a8526b6c84c7be3cbac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ae7988dd5365aee4d919999f11d6013.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-024178d03eb18285b87c073de72f8965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e7bcc2d1eb0dc6c43ade8662ed589e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea36764e1af6271fd4d3a7388adad22.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fb31f01aafaa23a009a481c6f091105b.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-08  Deep learning image burst stacking to reconstruct high-resolution   ground-based solar observations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96d12c51275a45c5dadcb236c0b4f035.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-08  Truly Self-Improving Agents Require Intrinsic Metacognitive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
