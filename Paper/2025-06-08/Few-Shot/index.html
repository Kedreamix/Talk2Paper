<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  Just a Scratch Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0e52c6ee4cd9ee24a5c221b6ed9faf13.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    40 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-08-æ›´æ–°"><a href="#2025-06-08-æ›´æ–°" class="headerlink" title="2025-06-08 æ›´æ–°"></a>2025-06-08 æ›´æ–°</h1><h2 id="Just-a-Scratch-Enhancing-LLM-Capabilities-for-Self-harm-Detection-through-Intent-Differentiation-and-Emoji-Interpretation"><a href="#Just-a-Scratch-Enhancing-LLM-Capabilities-for-Self-harm-Detection-through-Intent-Differentiation-and-Emoji-Interpretation" class="headerlink" title="Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation"></a>Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection   through Intent Differentiation and Emoji Interpretation</h2><p><strong>Authors:Soumitra Ghosh, Gopendra Vikram Singh,  Shambhavi, Sabarna Choudhury, Asif Ekbal</strong></p>
<p>Self-harm detection on social media is critical for early intervention and mental health support, yet remains challenging due to the subtle, context-dependent nature of such expressions. Identifying self-harm intent aids suicide prevention by enabling timely responses, but current large language models (LLMs) struggle to interpret implicit cues in casual language and emojis. This work enhances LLMsâ€™ comprehension of self-harm by distinguishing intent through nuanced language-emoji interplay. We present the Centennial Emoji Sensitivity Matrix (CESM-100), a curated set of 100 emojis with contextual self-harm interpretations and the Self-Harm Identification aNd intent Extraction with Supportive emoji sensitivity (SHINES) dataset, offering detailed annotations for self-harm labels, casual mentions (CMs), and serious intents (SIs). Our unified framework: a) enriches inputs using CESM-100; b) fine-tunes LLMs for multi-task learning: self-harm detection (primary) and CM&#x2F;SI span detection (auxiliary); c) generates explainable rationales for self-harm predictions. We evaluate the framework on three state-of-the-art LLMs-Llama 3, Mental-Alpaca, and MentalLlama, across zero-shot, few-shot, and fine-tuned scenarios. By coupling intent differentiation with contextual cues, our approach commendably enhances LLM performance in both detection and explanation tasks, effectively addressing the inherent ambiguity in self-harm signals. The SHINES dataset, CESM-100 and codebase are publicly available at: <a target="_blank" rel="noopener" href="https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES">https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES</a> . </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“ä¸Šçš„è‡ªæˆ‘ä¼¤å®³æ£€æµ‹å¯¹äºæ—©æœŸå¹²é¢„å’Œç²¾ç¥å¥åº·æ”¯æŒè‡³å…³é‡è¦ï¼Œä½†ç”±äºæ­¤ç±»è¡¨è¾¾çš„ç»†å¾®å’Œè¯­å¢ƒä¾èµ–æ€§ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¯†åˆ«è‡ªæˆ‘ä¼¤å®³æ„å›¾æœ‰åŠ©äºé€šè¿‡åŠæ—¶å“åº”é¢„é˜²è‡ªæ€ï¼Œä½†å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£é‡Šæ—¥å¸¸ç”¨è¯­å’Œè¡¨æƒ…ç¬¦å·ä¸­çš„éšå«çº¿ç´¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡åŒºåˆ†è¯­è¨€ä¸è¡¨æƒ…ç¬¦å·ä¹‹é—´çš„å¾®å¦™äº’åŠ¨ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è‡ªæˆ‘ä¼¤å®³æ„å›¾çš„ç†è§£ã€‚æˆ‘ä»¬æ¨å‡ºäº†ç™¾å¹´è¡¨æƒ…ç¬¦å·æ•æ„Ÿæ€§çŸ©é˜µï¼ˆCESM-100ï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«è¯­å¢ƒè‡ªæˆ‘ä¼¤å®³è§£é‡Šçš„100ä¸ªè¡¨æƒ…ç¬¦å·çš„ç²¾é€‰é›†ï¼Œä»¥åŠè‡ªæˆ‘ä¼¤å®³è¯†åˆ«ä¸æ„å›¾æå–ï¼ˆSHINESï¼‰æ•°æ®é›†ï¼Œä¸ºè‡ªæˆ‘ä¼¤å®³æ ‡ç­¾ã€å¶ç„¶æåŠï¼ˆCMï¼‰å’Œä¸¥é‡æ„å›¾ï¼ˆSIï¼‰æä¾›äº†è¯¦ç»†çš„æ³¨é‡Šã€‚æˆ‘ä»¬çš„ç»Ÿä¸€æ¡†æ¶åŒ…æ‹¬ï¼šaï¼‰ä½¿ç”¨CESM-100ä¸°å¯Œè¾“å…¥ï¼›bï¼‰å¾®è°ƒLLMè¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼šè‡ªæˆ‘ä¼¤å®³æ£€æµ‹ï¼ˆä¸»è¦ä»»åŠ¡ï¼‰å’ŒCM&#x2F;SIè·¨åº¦æ£€æµ‹ï¼ˆè¾…åŠ©ä»»åŠ¡ï¼‰ï¼›cï¼‰ç”Ÿæˆè‡ªæˆ‘ä¼¤å®³é¢„æµ‹çš„å¯è§£é‡Šç†ç”±ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°äº†è¯¥æ¡†æ¶ï¼šLlama 3ã€Mental-Alpacaå’ŒMentalLlamaï¼Œä»¥åŠåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚é€šè¿‡æ„å›¾åŒºåˆ†ä¸ä¸Šä¸‹æ–‡çº¿ç´¢ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ£€æµ‹å’Œè§£é‡Šä»»åŠ¡çš„æ€§èƒ½æ–¹é¢éƒ½å–å¾—äº†å¯å–œçš„æ•ˆæœï¼Œæœ‰æ•ˆåœ°è§£å†³äº†è‡ªæˆ‘ä¼¤å®³ä¿¡å·ä¸­çš„å›ºæœ‰æ¨¡ç³Šæ€§ã€‚SHINESæ•°æ®é›†ã€CESM-100å’Œæºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://www.iitp.ac.in/~ai-nlp-ml/resources.html#SHINES">https://www.iitp.ac.in/~ai-nlp-ml&#x2F;resources.html#SHINES</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05073v1">PDF</a> To be published in the Proceedings of the 63rd Annual Meeting of the   Association for Computational Linguistics (ACL 2025 Main)</p>
<p><strong>Summary</strong></p>
<p>æ­¤æ–‡æœ¬å…³äºé€šè¿‡åˆ©ç”¨æƒ…ç»ªçŸ©é˜µå’Œæ•°æ®é›†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹ç¤¾äº¤åª’ä½“ä¸Šè‡ªæˆ‘ä¼¤å®³è¡¨è¾¾æ„å›¾çš„ç†è§£ã€‚ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†Centennial Emoji Sensitivity Matrixï¼ˆCESM-100ï¼‰å’ŒSHINESæ•°æ®é›†ï¼Œä»¥ä¸°å¯Œè¾“å…¥å’Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒºåˆ†è‡ªæˆ‘ä¼¤å®³çš„æ„å›¾å’Œä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªæˆ‘ä¼¤å®³æ£€æµ‹ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶ç”Ÿæˆè§£é‡Šæ€§ç†ç”±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“çš„è‡ªæˆ‘ä¼¤å®³æ£€æµ‹å¯¹æ—©æœŸå¹²é¢„å’Œç²¾ç¥å¥åº·æ”¯æŒè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£è¯»éšæ™¦çº¿ç´¢å’Œæƒ…ç»ªè¡¨è¾¾æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>CESM-100åŒ…å«100ä¸ªå…·æœ‰ä¸Šä¸‹æ–‡è‡ªæˆ‘ä¼¤å®³è§£è¯»çš„emojiã€‚</li>
<li>SHINESæ•°æ®é›†æä¾›è¯¦ç»†çš„è‡ªæˆ‘ä¼¤å®³æ ‡ç­¾ã€éšæ„æåŠï¼ˆCMsï¼‰å’Œä¸¥é‡æ„å›¾ï¼ˆSIsï¼‰æ³¨é‡Šã€‚</li>
<li>ç»Ÿä¸€æ¡†æ¶åŒ…æ‹¬ï¼šä½¿ç”¨CESM-100ä¸°å¯Œè¾“å…¥ï¼Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼Œç”Ÿæˆè‡ªæˆ‘ä¼¤å®³é¢„æµ‹çš„è§£é‡Šæ€§ç†ç”±ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ä¸‰ç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæé«˜è‡ªæˆ‘ä¼¤å®³æ£€æµ‹å’Œè§£é‡Šä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfe558e4984976c692a5fe4a15f7648d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4452dd9fa0879ba7a377eb999790838d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3747d7fd6351e8416546f8852382457a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465632bcc02f4ef5ca85da501be64120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82b0638295029e85872b1df0872f9bc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-360276a37f65ee33463f8d4aa23ec8df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b8ac9ac883ee7d84e58bafc5037dbfa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Prompting-LLMs-Length-Control-for-Isometric-Machine-Translation"><a href="#Prompting-LLMs-Length-Control-for-Isometric-Machine-Translation" class="headerlink" title="Prompting LLMs: Length Control for Isometric Machine Translation"></a>Prompting LLMs: Length Control for Isometric Machine Translation</h2><p><strong>Authors:DÃ¡vid JavorskÃ½, OndÅ™ej Bojar, FranÃ§ois Yvon</strong></p>
<p>In this study, we explore the effectiveness of isometric machine translation across multiple language pairs (En$\to$De, En$\to$Fr, and En$\to$Es) under the conditions of the IWSLT Isometric Shared Task 2022. Using eight open-source large language models (LLMs) of varying sizes, we investigate how different prompting strategies, varying numbers of few-shot examples, and demonstration selection influence translation quality and length control. We discover that the phrasing of instructions, when aligned with the properties of the provided demonstrations, plays a crucial role in controlling the output length. Our experiments show that LLMs tend to produce shorter translations only when presented with extreme examples, while isometric demonstrations often lead to the models disregarding length constraints. While few-shot prompting generally enhances translation quality, further improvements are marginal across 5, 10, and 20-shot settings. Finally, considering multiple outputs allows to notably improve overall tradeoff between the length and quality, yielding state-of-the-art performance for some language pairs. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨IWSLTç­‰è·å…±äº«ä»»åŠ¡2022çš„æ¡ä»¶ä¸‹ï¼Œæ¢ç´¢äº†ç­‰è·æœºå™¨ç¿»è¯‘åœ¨å¤šè¯­è¨€å¯¹ï¼ˆè‹±è¯­åˆ°å¾·è¯­ã€è‹±è¯­åˆ°æ³•è¯­å’Œè‹±è¯­åˆ°è¥¿ç­ç‰™è¯­ï¼‰ä¹‹é—´çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨å…«ç§å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ¢è®¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ã€ä¸åŒæ•°é‡çš„å°‘é‡ç¤ºä¾‹å’Œæ¼”ç¤ºé€‰æ‹©å¯¹ç¿»è¯‘è´¨é‡å’Œé•¿åº¦æ§åˆ¶çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“æŒ‡ä»¤çš„æªè¾ä¸æ‰€æä¾›çš„æ¼”ç¤ºå±æ€§ç›¸ç¬¦æ—¶ï¼Œå¯¹è¾“å‡ºé•¿åº¦çš„æ§åˆ¶èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåªæœ‰å½“ç»™å‡ºæç«¯ç¤ºä¾‹æ—¶ï¼ŒLLMæ‰ä¼šäº§ç”Ÿè¾ƒçŸ­çš„ç¿»è¯‘ï¼Œè€Œç­‰è·æ¼”ç¤ºå¾€å¾€å¯¼è‡´æ¨¡å‹å¿½ç•¥é•¿åº¦çº¦æŸã€‚è™½ç„¶å°‘é‡æç¤ºé€šå¸¸å¯ä»¥æé«˜ç¿»è¯‘è´¨é‡ï¼Œä½†åœ¨5ã€10å’Œ20ä¸ªç¤ºä¾‹çš„è®¾ç½®ä¸­ï¼Œè¿›ä¸€æ­¥çš„æ”¹è¿›å¹…åº¦è¾ƒå°ã€‚æœ€åï¼Œè€ƒè™‘å¤šä¸ªè¾“å‡ºå¯ä»¥æ˜¾è‘—æ”¹å–„é•¿åº¦å’Œè´¨é‡ä¹‹é—´çš„æ€»ä½“æƒè¡¡ï¼Œä¸ºæŸäº›è¯­è¨€å¯¹å¸¦æ¥æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04855v1">PDF</a> Accepted to IWSLT 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†IWSLTç­‰è·å…±äº«ä»»åŠ¡2022æ¡ä»¶ä¸‹ï¼Œä½¿ç”¨å…«ä¸ªä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è·¨å¤šè¯­è¨€å¯¹ï¼ˆå¦‚è‹±è¯­è‡³å¾·è¯­ã€è‹±è¯­è‡³æ³•è¯­å’Œè‹±è¯­è‡³è¥¿ç­ç‰™è¯­ï¼‰çš„ç­‰è·æœºå™¨ç¿»è¯‘çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶æ¢è®¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ã€ä¸åŒæ•°é‡çš„å°‘é‡ç¤ºä¾‹å’Œæ¼”ç¤ºé€‰æ‹©å¦‚ä½•å½±å“ç¿»è¯‘è´¨é‡å’Œé•¿åº¦æ§åˆ¶ã€‚å®éªŒå‘ç°ï¼Œå½“æŒ‡ä»¤ä¸æä¾›çš„æ¼”ç¤ºå†…å®¹ç›¸åŒ¹é…æ—¶ï¼ŒæŒ‡ä»¤æªè¾å¯¹æ§åˆ¶è¾“å‡ºé•¿åº¦èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ­¤å¤–ï¼ŒLLMså¾€å¾€åªåœ¨å‘ˆç°æç«¯ç¤ºä¾‹æ—¶æ‰äº§ç”Ÿè¾ƒçŸ­çš„ç¿»è¯‘ï¼Œè€Œç­‰è·æ¼”ç¤ºå¾€å¾€å¯¼è‡´æ¨¡å‹å¿½ç•¥é•¿åº¦çº¦æŸã€‚è™½ç„¶å°‘é‡æç¤ºé€šå¸¸å¯ä»¥æé«˜ç¿»è¯‘è´¨é‡ï¼Œä½†åœ¨5ã€10å’Œ20æ¬¡æ‹æ‘„è®¾ç½®ä¹‹é—´çš„è¿›ä¸€æ­¥æ”¹è¿›æ˜¯å¾®ä¸è¶³é“çš„ã€‚æœ€åï¼Œè€ƒè™‘å¤šä¸ªè¾“å‡ºå¯ä»¥æ˜¾è‘—æ”¹å–„é•¿åº¦å’Œè´¨é‡ä¹‹é—´çš„æ€»ä½“æƒè¡¡ï¼Œå¯¹æŸäº›è¯­è¨€å¯¹è€Œè¨€è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨IWSLTç­‰è·å…±äº«ä»»åŠ¡ä¸‹ï¼Œç ”ç©¶äº†è·¨å¤šç§è¯­è¨€å¯¹çš„ç­‰è·æœºå™¨ç¿»è¯‘æ•ˆæœã€‚</li>
<li>ä½¿ç”¨å…«ä¸ªä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå®éªŒã€‚</li>
<li>ç ”ç©¶å‘ç°æŒ‡ä»¤æªè¾ä¸æ¼”ç¤ºå†…å®¹çš„åŒ¹é…å¯¹æ§åˆ¶è¾“å‡ºé•¿åº¦è‡³å…³é‡è¦ã€‚</li>
<li>LLMsåœ¨å‘ˆç°æç«¯ç¤ºä¾‹æ—¶äº§ç”Ÿè¾ƒçŸ­çš„ç¿»è¯‘ã€‚</li>
<li>ç­‰è·æ¼”ç¤ºå¯èƒ½å¯¼è‡´æ¨¡å‹å¿½ç•¥é•¿åº¦çº¦æŸã€‚</li>
<li>å°‘é‡æç¤ºå¯ä»¥æé«˜ç¿»è¯‘è´¨é‡ï¼Œä½†è¿›ä¸€æ­¥æé«˜æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-02dc132cc0d64b2685782166266019b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ebe5c25aa1c30e85a4dc4fdd9c55ba8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddfec3f9130efe6b77128806a84468ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5500da1eaf7a3e0b22c82a07f0a429aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50e5dc30a8acae2ae6a9f261bc934a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bbb46b1ef7dce415d4e7b657a7f1b06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aff9cbc7a06de0edaadaa4a924d05a87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-157d00cca897e4be07abd8dd67bcea76.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-Few-Shot-Vision-Language-Model-Adaptation"><a href="#Robust-Few-Shot-Vision-Language-Model-Adaptation" class="headerlink" title="Robust Few-Shot Vision-Language Model Adaptation"></a>Robust Few-Shot Vision-Language Model Adaptation</h2><p><strong>Authors:Hanxin Wang, Tian Liu, Shu Kong</strong></p>
<p>Pretrained VLMs achieve strong performance on downstream tasks when adapted with just a few labeled examples. As the adapted models inevitably encounter out-of-distribution (OOD) test data that deviates from the in-distribution (ID) task-specific training data, enhancing OOD generalization in few-shot adaptation is critically important. We study robust few-shot VLM adaptation, aiming to increase both ID and OOD accuracy. By comparing different adaptation methods (e.g., prompt tuning, linear probing, contrastive finetuning, and full finetuning), we uncover three key findings: (1) finetuning with proper hyperparameters significantly outperforms the popular VLM adaptation methods prompt tuning and linear probing; (2) visual encoder-only finetuning achieves better efficiency and accuracy than contrastively finetuning both visual and textual encoders; (3) finetuning the top layers of the visual encoder provides the best balance between ID and OOD accuracy. Building on these findings, we propose partial finetuning of the visual encoder empowered with two simple augmentation techniques: (1) retrieval augmentation which retrieves task-relevant data from the VLMâ€™s pretraining dataset to enhance adaptation, and (2) adversarial perturbation which promotes robustness during finetuning. Results show that the former&#x2F;latter boosts OOD&#x2F;ID accuracy while slightly sacrificing the ID&#x2F;OOD accuracy. Yet, perhaps understandably, naively combining the two does not maintain their best OOD&#x2F;ID accuracy. We address this dilemma with the developed SRAPF, Stage-wise Retrieval Augmentation-based Adversarial Partial Finetuning. SRAPF consists of two stages: (1) partial finetuning the visual encoder using both ID and retrieved data, and (2) adversarial partial finetuning with few-shot ID data. Extensive experiments demonstrate that SRAPF achieves the state-of-the-art ID and OOD accuracy on the ImageNet OOD benchmarks. </p>
<blockquote>
<p>é¢„è®­ç»ƒVLMæ¨¡å‹ä»…åœ¨å°‘é‡æ ‡è®°æ ·æœ¬çš„æƒ…å†µä¸‹å°±èƒ½é€‚åº”ä¸‹æ¸¸ä»»åŠ¡å¹¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç”±äºé€‚åº”çš„æ¨¡å‹ä¸å¯é¿å…åœ°ä¼šé‡åˆ°åç¦»ç‰¹å®šä»»åŠ¡è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æµ‹è¯•æ•°æ®ï¼Œå› æ­¤åœ¨æœ‰é™æ ·æœ¬ä¸­æé«˜OODçš„æ³›åŒ–èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ç ”ç©¶äº†ç¨³å¥çš„å°‘æ ·æœ¬VLMé€‚åº”æ€§è®­ç»ƒï¼Œæ—¨åœ¨æé«˜INå’ŒOODçš„å‡†ç¡®ç‡ã€‚é€šè¿‡å¯¹ä¸åŒçš„é€‚åº”æ€§è®­ç»ƒæ–¹æ³•çš„æ¯”è¾ƒï¼ˆå¦‚æç¤ºè°ƒæ•´ã€çº¿æ€§æ¢æµ‹ã€å¯¹æ¯”å¾®è°ƒä»¥åŠå…¨å¾®è°ƒï¼‰ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸‰ä¸ªå…³é”®è§‚ç‚¹ï¼šï¼ˆ1ï¼‰ä½¿ç”¨é€‚å½“çš„è¶…å‚æ•°è¿›è¡Œå¾®è°ƒæ˜¾è‘—ä¼˜äºæµè¡Œçš„VLMé€‚åº”æ€§è®­ç»ƒæ–¹æ³•æç¤ºè°ƒæ•´å’Œçº¿æ€§æ¢æµ‹ï¼›ï¼ˆ2ï¼‰ä»…å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œç›¸æ¯”äºå¯¹æ¯”å¼åœ°åŒæ—¶å¾®è°ƒè§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼›ï¼ˆ3ï¼‰å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„é¡¶å±‚æä¾›äº†INå’ŒOODå‡†ç¡®ç‡ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œéƒ¨åˆ†å¾®è°ƒï¼Œè¾…ä»¥ä¸¤ç§ç®€å•çš„å¢å¼ºæŠ€æœ¯ï¼šï¼ˆ1ï¼‰æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œå®ƒä»VLMçš„é¢„è®­ç»ƒæ•°æ®é›†ä¸­æ£€ç´¢ä»»åŠ¡ç›¸å…³æ•°æ®ä»¥å¢å¼ºé€‚åº”æ€§ï¼›ï¼ˆ2ï¼‰å¯¹æŠ—æ‰°åŠ¨æŠ€æœ¯ï¼Œå®ƒåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ä¿ƒè¿›ç¨³å¥æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå‰è€…æé«˜äº†OODå‡†ç¡®æ€§ï¼Œè€Œåè€…åˆ™æé«˜äº†INå‡†ç¡®æ€§ï¼Œä½†ä¸¤è€…åŒæ—¶ä½¿ç”¨æ—¶å¯èƒ½æ— æ³•ä¿æŒå…¶æœ€ä½³çš„OODæˆ–INå‡†ç¡®æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€å›°å¢ƒï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºé˜¶æ®µæ£€ç´¢å¢å¼ºçš„å¯¹æŠ—æ€§éƒ¨åˆ†å¾®è°ƒï¼ˆSRAPFï¼‰ã€‚SRAPFåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ä½¿ç”¨INå’Œæ£€ç´¢æ•°æ®å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œéƒ¨åˆ†å¾®è°ƒï¼›ï¼ˆ2ï¼‰ä½¿ç”¨å°‘é‡çš„INæ•°æ®è¿›è¡Œå¯¹æŠ—æ€§éƒ¨åˆ†å¾®è°ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSRAPFåœ¨ImageNet OODåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„INå’ŒOODå‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04713v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://hannawang09.github.io/projects/srapf/">https://hannawang09.github.io/projects/srapf/</a></p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒVLMåœ¨å°‘é‡æ ‡æ³¨æ ·æœ¬çš„é€‚åº”ä¸‹ï¼Œèƒ½åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚ä¸ºäº†é€‚åº”ä¸å¯é¿å…åœ°é‡åˆ°åç¦»ç‰¹å®šä»»åŠ¡è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æµ‹è¯•æ•°æ®çš„é—®é¢˜ï¼Œæé«˜å°‘æ ·æœ¬VLMé€‚åº”çš„OODæ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æé«˜IDå’ŒOODçš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒçš„é€‚åº”æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°å…¨å‚æ•°å¾®è°ƒèƒ½æ˜¾è‘—ä¼˜äºæµè¡Œçš„VLMé€‚åº”æ–¹æ³•ï¼Œå¦‚æç¤ºå¾®è°ƒçº¿æ€§æ¢æµ‹å’Œå¯¹æ¯”å¾®è°ƒï¼›ä»…å¾®è°ƒè§†è§‰ç¼–ç å™¨æ¯”å¯¹æ¯”å¾®è°ƒè§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨æ›´æœ‰æ•ˆä¸”å‡†ç¡®ï¼›å¾®è°ƒè§†è§‰ç¼–ç å™¨çš„é¡¶å±‚åœ¨IDå’ŒOODå‡†ç¡®æ€§ä¹‹é—´æä¾›äº†æœ€ä½³çš„å¹³è¡¡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹è§†è§‰ç¼–ç å™¨çš„éƒ¨åˆ†å¾®è°ƒï¼Œå¹¶ç»“åˆä¸¤ç§ç®€å•çš„å¢å¼ºæŠ€æœ¯ï¼šæ£€ç´¢å¢å¼ºå’Œå¯¹æŠ—æ‰°åŠ¨ã€‚æœ€åä¸ºäº†è§£å†³äºŒè€…ç»“åˆçš„å›°å¢ƒæå‡ºäº†SRAPFï¼ˆåŸºäºé˜¶æ®µæ£€ç´¢å¢å¼ºçš„å¯¹æŠ—æ€§éƒ¨åˆ†å¾®è°ƒï¼‰ã€‚SRAPFåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œèƒ½åœ¨ImageNet OODåŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€ä½³çš„IDå’ŒOODå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒVLMæ¨¡å‹åœ¨å°‘æ ·æœ¬é€‚åº”ä¸‹è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šã€‚</li>
<li>å¢å¼ºå°‘æ ·æœ¬VLMé€‚åº”çš„OODæ³›åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>å…¨å‚æ•°å¾®è°ƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„VLMé€‚åº”æ–¹æ³•ï¼Œä¼˜äºæç¤ºå¾®è°ƒã€çº¿æ€§æ¢æµ‹å’Œå¯¹æ¯”å¾®è°ƒã€‚</li>
<li>ä»…å¾®è°ƒè§†è§‰ç¼–ç å™¨æ—¢æé«˜äº†æ•ˆç‡åˆæé«˜äº†å‡†ç¡®æ€§ã€‚</li>
<li>æ£€ç´¢å¢å¼ºæŠ€æœ¯èƒ½æé«˜OODå‡†ç¡®æ€§ï¼Œè€Œå¯¹æŠ—æ‰°åŠ¨åˆ™ä¿ƒè¿›æ¨¡å‹åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>åˆå¹¶ä¸¤ç§å¢å¼ºæŠ€æœ¯éœ€è¦è§£å†³çš„æœ€ä½³ç­–ç•¥æ˜¯ä½¿ç”¨SRAPFï¼ˆåŸºäºé˜¶æ®µæ£€ç´¢å¢å¼ºçš„å¯¹æŠ—æ€§éƒ¨åˆ†å¾®è°ƒï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc2740816b43053f844edf6ce8fc66e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f73b06cdf365560c7f80822b169df9a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-360a0e1e2b5cf121bf9b910f2f07314f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfaa80dace65ed81a1fa70c5124d2b6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724cf77aeb8b260db872ae712c808730.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Interpretable-Few-Shot-Image-Classification-via-Prototypical-Concept-Guided-Mixture-of-LoRA-Experts"><a href="#Interpretable-Few-Shot-Image-Classification-via-Prototypical-Concept-Guided-Mixture-of-LoRA-Experts" class="headerlink" title="Interpretable Few-Shot Image Classification via Prototypical   Concept-Guided Mixture of LoRA Experts"></a>Interpretable Few-Shot Image Classification via Prototypical   Concept-Guided Mixture of LoRA Experts</h2><p><strong>Authors:Zhong Ji, Rongshuai Wei, Jingren Liu, Yanwei Pang, Jungong Han</strong></p>
<p>Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance.To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module.Meanwhile, cross-module concept guidance enforces tight alignment between the backboneâ€™s feature representations and the prototypical concept activation patterns.In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability.Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries.Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%-8.7% relative gains in 5-way 5-shot classification.These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems. </p>
<blockquote>
<p>è‡ªè§£é‡Šæ¨¡å‹ï¼ˆSEMsï¼‰ä¾èµ–äºåŸå‹æ¦‚å¿µå­¦ä¹ ï¼ˆPCLï¼‰æ¥ä½¿å…¶è§†è§‰è¯†åˆ«è¿‡ç¨‹æ›´å…·å¯è§£é‡Šæ€§ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­ï¼Œç”±äºè®­ç»ƒæ ·æœ¬ä¸è¶³å¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œå®ƒä»¬ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªFew-ShotåŸå‹æ¦‚å¿µåˆ†ç±»ï¼ˆFSPCCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°å‡è½»äº†ä½æ•°æ®ç¯å¢ƒä¸‹çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå‚æ•°ä¸å¹³è¡¡å’Œè¡¨ç¤ºä¸åŒ¹é…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨LoRAä¸“å®¶æ··åˆç‰©ï¼ˆMoLEï¼‰è¿›è¡Œå‚æ•°æœ‰æ•ˆçš„é€‚åº”ï¼Œç¡®ä¿åœ¨ä¸»å¹²å’ŒPCLæ¨¡å—ä¹‹é—´åˆ†é…å¯è®­ç»ƒå‚æ•°çš„å¹³è¡¡ã€‚åŒæ—¶ï¼Œè·¨æ¨¡å—æ¦‚å¿µå¼•å¯¼å¼ºåˆ¶æ‰§è¡Œä¸»å¹²ç‰¹å¾è¡¨ç¤ºä¸åŸå‹æ¦‚å¿µæ¿€æ´»æ¨¡å¼ä¹‹é—´çš„ç´§å¯†å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¤šå±‚æ¬¡ç‰¹å¾ä¿ç•™ç­–ç•¥ï¼Œèåˆäº†å„å±‚ä¸­çš„ç©ºé—´å’Œè¯­ä¹‰çº¿ç´¢ï¼Œä»è€Œä¸°å¯Œäº†å­¦ä¹ çš„è¡¨ç¤ºå¹¶å‡è½»äº†æœ‰é™æ•°æ®å¯ç”¨æ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œä¸ºäº†æé«˜å¯è§£é‡Šæ€§å¹¶æœ€å°åŒ–æ¦‚å¿µé‡å ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥æ¦‚å¿µåˆ¤åˆ«æŸå¤±ï¼Œè¯¥æŸå¤±å¼ºåˆ¶æ‰§è¡Œæ¦‚å¿µä¹‹é—´çš„æ­£äº¤æ€§ï¼Œé¼“åŠ±æ›´è§£è€¦å’Œé€æ˜çš„å†³ç­–è¾¹ç•Œã€‚åœ¨å…­ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ï¼ˆCUB-200-2011ã€mini-ImageNetã€CIFAR-FSã€Stanford Carsã€FGVC-Aircraftå’ŒDTDï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æƒ…å†µä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰SEMï¼Œåœ¨5è·¯5åˆ†ç±»å°„å‡»åˆ†ç±»ä¸­ç›¸å¯¹æé«˜äº†4.2%-8.7%ã€‚è¿™äº›å‘ç°è¡¨æ˜å°†æ¦‚å¿µå­¦ä¹ ä¸å°‘é•œå¤´é€‚åº”ç›¸ç»“åˆä»¥å®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´æ¸…æ™°çš„æ¨¡å‹å¯è§£é‡Šæ€§æ˜¯æœ‰æ•ˆçš„ï¼Œè¿™ä¸ºæ›´é€æ˜çš„è§†è§‰è¯†åˆ«ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04673v1">PDF</a> 13 pages,5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªFew-Shot Prototypical Concept Classificationï¼ˆFSPCCï¼‰æ¡†æ¶ï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹è‡ªæˆ‘è§£é‡Šæ¨¡å‹ï¼ˆSEMsï¼‰é¢ä¸´çš„é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨æ··åˆLoRAä¸“å®¶ï¼ˆMoLEï¼‰è¿›è¡Œå‚æ•°é«˜æ•ˆé€‚åº”ï¼Œä»¥åŠè·¨æ¨¡å—æ¦‚å¿µå¼•å¯¼å’Œå¤šå±‚æ¬¡ç‰¹å¾ä¿ç•™ç­–ç•¥ï¼Œè¯¥æ¡†æ¶æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå¼•å…¥å‡ ä½•æ„ŸçŸ¥æ¦‚å¿µåˆ¤åˆ«æŸå¤±ï¼Œå¢å¼ºäº†è§£é‡Šæ€§å¹¶å‡å°‘æ¦‚å¿µé‡å ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰SEMsï¼Œä¸ºè€¦åˆæ¦‚å¿µå­¦ä¹ ä¸å°‘æ ·æœ¬é€‚åº”ä»¥å®ç°æ›´é«˜ç²¾åº¦å’Œæ›´æ¸…æ™°æ¨¡å‹è§£é‡Šé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Few-Shot Prototypical Concept Classification (FSPCC) æ¡†æ¶ä»¥è§£å†³æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­SEMsçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ··åˆLoRAä¸“å®¶ï¼ˆMoLEï¼‰å®ç°å‚æ•°é«˜æ•ˆé€‚åº”ï¼Œå¹³è¡¡åˆ†é…å¯è®­ç»ƒå‚æ•°ã€‚</li>
<li>è·¨æ¨¡å—æ¦‚å¿µå¼•å¯¼ç¡®ä¿ç‰¹å¾è¡¨ç¤ºä¸åŸå‹æ¦‚å¿µæ¿€æ´»æ¨¡å¼ç´§å¯†å¯¹é½ã€‚</li>
<li>é‡‡ç”¨å¤šå±‚æ¬¡ç‰¹å¾ä¿ç•™ç­–ç•¥ï¼Œèåˆç©ºé—´è¯­ä¹‰çº¿ç´¢ï¼Œä¸°å¯Œå­¦ä¹ åˆ°çš„è¡¨ç¤ºå¹¶åº”å¯¹æ•°æ®æœ‰é™æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å‡ ä½•æ„ŸçŸ¥æ¦‚å¿µåˆ¤åˆ«æŸå¤±ï¼Œå¢å¼ºæ¨¡å‹è§£é‡Šæ€§å’Œæ¦‚å¿µä¹‹é—´çš„æ­£äº¤æ€§ã€‚</li>
<li>å®éªŒç»“æœåœ¨å¤šæ•°æ®é›†ä¸Šè¡¨æ˜FSPCCæ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰SEMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6afc3d91271a8c4a354deab1a6d6ae64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e52c6ee4cd9ee24a5c221b6ed9faf13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689022c329247048e7e21c1a619303b4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AuthGuard-Generalizable-Deepfake-Detection-via-Language-Guidance"><a href="#AuthGuard-Generalizable-Deepfake-Detection-via-Language-Guidance" class="headerlink" title="AuthGuard: Generalizable Deepfake Detection via Language Guidance"></a>AuthGuard: Generalizable Deepfake Detection via Language Guidance</h2><p><strong>Authors:Guangyu Shen, Zhihua Li, Xiang Xu, Tianchen Zhao, Zheng Zhang, Dongsheng An, Zhuowen Tu, Yifan Xing, Qin Zhang</strong></p>
<p>Existing deepfake detection techniques struggle to keep-up with the ever-evolving novel, unseen forgeries methods. This limitation stems from their reliance on statistical artifacts learned during training, which are often tied to specific generation processes that may not be representative of samples from new, unseen deepfake generation methods encountered at test time. We propose that incorporating language guidance can improve deepfake detection generalization by integrating human-like commonsense reasoning â€“ such as recognizing logical inconsistencies and perceptual anomalies â€“ alongside statistical cues. To achieve this, we train an expert deepfake vision encoder by combining discriminative classification with image-text contrastive learning, where the text is generated by generalist MLLMs using few-shot prompting. This allows the encoder to extract both language-describable, commonsense deepfake artifacts and statistical forgery artifacts from pixel-level distributions. To further enhance robustness, we integrate data uncertainty learning into vision-language contrastive learning, mitigating noise in image-text supervision. Our expert vision encoder seamlessly interfaces with an LLM, further enabling more generalized and interpretable deepfake detection while also boosting accuracy. The resulting framework, AuthGuard, achieves state-of-the-art deepfake detection accuracy in both in-distribution and out-of-distribution settings, achieving AUC gains of 6.15% on the DFDC dataset and 16.68% on the DF40 dataset. Additionally, AuthGuard significantly enhances deepfake reasoning, improving performance by 24.69% on the DDVQA dataset. </p>
<blockquote>
<p>ç°æœ‰æ·±åº¦ä¼ªé€ æ£€æµ‹æŠ€æœ¯éš¾ä»¥è·Ÿä¸Šä¸æ–­æ¼”å˜çš„å…¨æ–°å’ŒæœªçŸ¥ä¼ªé€ æ–¹æ³•ã€‚è¿™ä¸€å±€é™æ€§æºäºå®ƒä»¬ä¾èµ–äºè®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ çš„ç»Ÿè®¡ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾é€šå¸¸ä¸ç‰¹å®šçš„ç”Ÿæˆè¿‡ç¨‹ç›¸å…³è”ï¼Œå¯èƒ½æ— æ³•ä»£è¡¨æµ‹è¯•æ—¶é‡åˆ°çš„æ–°æœªçŸ¥æ·±åº¦ä¼ªé€ ç”Ÿæˆæ–¹æ³•çš„æ ·æœ¬ã€‚æˆ‘ä»¬æå‡ºï¼Œé€šè¿‡èå…¥è¯­è¨€æŒ‡å¯¼å¯ä»¥æ”¹å–„æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ–¹æ³•æ˜¯ç»“åˆäººç±»å¸¸è¯†æ¨ç†ï¼Œå¦‚è¯†åˆ«é€»è¾‘ä¸ä¸€è‡´å’Œæ„ŸçŸ¥å¼‚å¸¸ç­‰ï¼ŒåŒæ—¶è¾…ä»¥ç»Ÿè®¡çº¿ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆåˆ¤åˆ«åˆ†ç±»å’Œå›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒä¸“ä¸šçš„æ·±åº¦ä¼ªé€ è§†è§‰ç¼–ç å™¨ã€‚å…¶ä¸­æ–‡æœ¬æ˜¯ç”±é€šç”¨MLLMé€šè¿‡å°‘é‡æç¤ºç”Ÿæˆçš„ã€‚è¿™ä½¿å¾—ç¼–ç å™¨èƒ½å¤Ÿä»åƒç´ çº§åˆ†å¸ƒä¸­æå–è¯­è¨€å¯æè¿°ã€å¸¸è¯†æ€§çš„æ·±åº¦ä¼ªé€ ç‰¹å¾å’Œç»Ÿè®¡ä¼ªé€ ç‰¹å¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å°†æ•°æ®ä¸ç¡®å®šæ€§å­¦ä¹ æ•´åˆåˆ°è§†è§‰è¯­è¨€å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œå‡è½»å›¾åƒæ–‡æœ¬ç›‘ç£ä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬çš„ä¸“ä¸šè§†è§‰ç¼–ç å™¨æ— ç¼åœ°æ¥å£ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å®ç°äº†æ›´é€šç”¨å’Œå¯è§£é‡Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ŒåŒæ—¶æé«˜äº†å‡†ç¡®æ€§ã€‚æ‰€å¾—åˆ°çš„æ¡†æ¶AuthGuardåœ¨å†…å¤–åˆ†å¸ƒç¯å¢ƒä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ·±åº¦ä¼ªé€ æ£€æµ‹å‡†ç¡®ç‡ï¼Œåœ¨DFDCæ•°æ®é›†ä¸ŠAUCå¢ç›Šè¾¾6.15%ï¼Œåœ¨DF40æ•°æ®é›†ä¸Šè¾¾16.68%ã€‚æ­¤å¤–ï¼ŒAuthGuardæ˜¾è‘—æé«˜äº†æ·±åº¦ä¼ªé€ æ¨ç†èƒ½åŠ›ï¼Œåœ¨DDVQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½æé«˜24.69%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œç°æœ‰æ£€æµ‹æ‰‹æ®µéš¾ä»¥åº”å¯¹æ–°å‹ã€æœªè§è¿‡çš„ä¼ªé€ æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºç»“åˆè¯­è¨€æŒ‡å¯¼æé«˜æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡é›†æˆäººç±»å¸¸è¯†æ¨ç†ï¼Œå¦‚é€»è¾‘ä¸ä¸€è‡´å’Œæ„ŸçŸ¥å¼‚å¸¸çš„è®¤è¯†ï¼Œä»¥åŠç»Ÿè®¡çº¿ç´¢ã€‚é€šè¿‡è®­ç»ƒæ·±åº¦ä¼ªé€ è§†è§‰ç¼–ç å™¨ï¼Œç»“åˆåˆ¤åˆ«åˆ†ç±»å’Œå›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨å°‘é‡æç¤ºç”Ÿæˆæ–‡æœ¬ã€‚è¿™å…è®¸ç¼–ç å™¨ä»åƒç´ çº§åˆ†å¸ƒä¸­æå–è¯­è¨€å¯æè¿°çš„å¸¸è¯†æ€§æ·±åº¦ä¼ªé€ ä¼ªè¿¹å’Œç»Ÿè®¡ä¼ªé€ ä¼ªè¿¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢åŠ ç¨³å¥æ€§ï¼Œæœ¬æ–‡è¿˜å°†æ•°æ®ä¸ç¡®å®šæ€§å­¦ä¹ èå…¥è§†è§‰è¯­è¨€å¯¹æ¯”å­¦ä¹ ï¼Œå‡è½»å›¾åƒæ–‡æœ¬ç›‘ç£ä¸­çš„å™ªå£°ã€‚æœ¬æ–‡çš„ä¸“å®¶è§†è§‰ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼å¯¹æ¥ï¼Œæ›´é€šç”¨ã€å¯è§£é‡Šçš„æ·±åº¦ä¼ªé€ æ£€æµ‹åœ¨æé«˜äº†å‡†ç¡®æ€§çš„åŒæ—¶ã€‚æ‰€å¾—åˆ°çš„æ¡†æ¶AuthGuardåœ¨å†…å¤–åˆ†å¸ƒè®¾ç½®ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ç²¾åº¦ï¼Œåœ¨DFDCæ•°æ®é›†ä¸ŠAUCå¢ç›Šè¾¾6.15%ï¼Œåœ¨DF40æ•°æ®é›†ä¸Šå¢ç›Šè¾¾16.68%ã€‚æ­¤å¤–ï¼ŒAuthGuardè¿˜æ˜¾è‘—æé«˜äº†æ·±åº¦ä¼ªé€ æ¨ç†æ€§èƒ½ï¼Œåœ¨DDVQAæ•°æ®é›†ä¸Šçš„æ€§èƒ½æé«˜äº†24.69%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ·±åº¦ä¼ªé€ æ£€æµ‹æŠ€æœ¯é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥åº”å¯¹æ–°å‹æœªè§è¿‡çš„ä¼ªé€ æ–¹æ³•ã€‚</li>
<li>ç»“åˆè¯­è¨€æŒ‡å¯¼æé«˜æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡é›†æˆäººç±»å¸¸è¯†æ¨ç†ã€‚</li>
<li>é€šè¿‡è®­ç»ƒæ·±åº¦ä¼ªé€ è§†è§‰ç¼–ç å™¨ï¼Œç»“åˆåˆ¤åˆ«åˆ†ç±»ä¸å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>ç¼–ç å™¨å¯æå–è¯­è¨€å¯æè¿°çš„å¸¸è¯†æ€§æ·±åº¦ä¼ªé€ ä¼ªè¿¹å’Œç»Ÿè®¡ä¼ªé€ ä¼ªè¿¹ã€‚</li>
<li>é›†æˆæ•°æ®ä¸ç¡®å®šæ€§å­¦ä¹ å¢å¼ºç¨³å¥æ€§ï¼Œå‡è½»å›¾åƒæ–‡æœ¬ç›‘ç£ä¸­çš„å™ªå£°ã€‚</li>
<li>ä¸“å®¶è§†è§‰ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼å¯¹æ¥ï¼Œæé«˜æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cbec10a7a8d1009c9997ed2e7c494f31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0abeb3fa97b3337cdb3def1bd79b0a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b5cc3a0e4f6df8acb79946871915b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79972f7ab4a471d88d1a8423e4103cff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Building-a-Few-Shot-Cross-Domain-Multilingual-NLU-Model-for-Customer-Care"><a href="#Building-a-Few-Shot-Cross-Domain-Multilingual-NLU-Model-for-Customer-Care" class="headerlink" title="Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer   Care"></a>Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer   Care</h2><p><strong>Authors:Saurabh Kumar, Sourav Bansal, Neeraj Agrawal, Priyanka Bhatt</strong></p>
<p>Customer care is an essential pillar of the e-commerce shopping experience with companies spending millions of dollars each year, employing automation and human agents, across geographies (like US, Canada, Mexico, Chile), channels (like Chat, Interactive Voice Response (IVR)), and languages (like English, Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on annotated data have shown good performance in downstream tasks relevant to Customer Care. However, model performance is largely subject to the availability of sufficient annotated domain-specific data. Cross-domain availability of data remains a bottleneck, thus building an intent classifier that generalizes across domains (defined by channel, geography, and language) with only a few annotations, is of great practical value. In this paper, we propose an embedder-cum-classifier model architecture which extends state-of-the-art domain-specific models to other domains with only a few labeled samples. We adopt a supervised fine-tuning approach with isotropic regularizers to train a domain-specific sentence embedder and a multilingual knowledge distillation strategy to generalize this embedder across multiple domains. The trained embedder, further augmented with a simple linear classifier can be deployed for new domains. Experiments on Canada and Mexico e-commerce Customer Care dataset with few-shot intent detection show an increase in accuracy by 20-23% against the existing state-of-the-art pre-trained models. </p>
<blockquote>
<p>å®¢æˆ·æœåŠ¡æ˜¯ç”µå­å•†åŠ¡è´­ç‰©ä½“éªŒçš„é‡è¦æ”¯æŸ±ï¼Œå…¬å¸æ¯å¹´èŠ±è´¹æ•°ç™¾ä¸‡ç¾å…ƒï¼Œè·¨è¶Šåœ°ç†åŒºåŸŸï¼ˆå¦‚ç¾å›½ã€åŠ æ‹¿å¤§ã€å¢¨è¥¿å“¥ã€æ™ºåˆ©ï¼‰ã€æ¸ é“ï¼ˆå¦‚èŠå¤©ã€äº¤äº’å¼è¯­éŸ³å“åº”ï¼ˆIVRï¼‰å’Œè¯­è¨€ï¼ˆå¦‚è‹±è¯­ã€è¥¿ç­ç‰™è¯­ï¼‰é‡‡ç”¨è‡ªåŠ¨åŒ–å’Œäººå·¥ä»£ç†ã€‚ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¦‚å¤šè¯­è¨€BERTï¼Œå¹¶åœ¨æ³¨é‡Šæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¸å®¢æˆ·æœåŠ¡ç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ¨¡å‹æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¶³å¤Ÿçš„æ³¨é‡Šé¢†åŸŸç‰¹å®šæ•°æ®çš„å¯ç”¨æ€§ã€‚è·¨åŸŸæ•°æ®çš„å¯ç”¨æ€§ä»ç„¶æ˜¯ä¸€ä¸ªç“¶é¢ˆï¼Œå› æ­¤ï¼Œå»ºç«‹ä¸€ä¸ªä»…ä½¿ç”¨å°‘é‡æ³¨é‡Šå³å¯è·¨é¢†åŸŸï¼ˆç”±æ¸ é“ã€åœ°ç†å’Œè¯­è¨€å®šä¹‰ï¼‰æ¨å¹¿çš„æ„å›¾åˆ†ç±»å™¨ï¼Œå…·æœ‰å·¨å¤§çš„å®ç”¨ä»·å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åµŒå…¥åˆ†ç±»å™¨æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„å°†æœ€å…ˆè¿›é¢†åŸŸç‰¹å®šçš„æ¨¡å‹æ¨å¹¿åˆ°å°‘æ•°æ ·æœ¬çš„å…¶å®ƒé¢†åŸŸã€‚æˆ‘ä»¬é‡‡ç”¨å¸¦æœ‰åŒæ„æ­£åˆ™åŒ–çš„ç›‘ç£å¾®è°ƒæ–¹æ³•æ¥è®­ç»ƒç‰¹å®šé¢†åŸŸçš„å¥å­åµŒå…¥å™¨ï¼Œå¹¶é‡‡ç”¨å¤šè¯­è¨€çŸ¥è¯†è’¸é¦ç­–ç•¥ä½¿è¯¥åµŒå…¥å™¨è·¨å¤šä¸ªé¢†åŸŸè¿›è¡Œæ¨å¹¿ã€‚ç»è¿‡è®­ç»ƒçš„åµŒå…¥å™¨è¿›ä¸€æ­¥ä¸ä¸€ä¸ªç®€å•çº¿æ€§åˆ†ç±»å™¨ç»“åˆï¼Œå¯ç”¨äºæ–°é¢†åŸŸã€‚åœ¨åŠ æ‹¿å¤§å’Œå¢¨è¥¿å“¥ç”µå­å•†åŠ¡å®¢æˆ·æœåŠ¡æ•°æ®é›†ä¸Šè¿›è¡Œçš„å°æ ·æœ¬æ„å›¾æ£€æµ‹å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œå‡†ç¡®ç‡æé«˜äº†20%-23%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å®¢æˆ·å…³æ€€åœ¨ç”µå­å•†åŠ¡è´­ç‰©ä½“éªŒä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºä¼ä¸šåœ¨ä¸åŒçš„åœ°ç†åŒºåŸŸã€æ¸ é“å’Œè¯­è¨€ä¸ŠæŠ•å…¥å·¨èµ„ã€‚å½“å‰æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹å¦‚å¤šè¯­è¨€BERTåœ¨å®¢æˆ·å…³æ€€ç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†æ¨¡å‹æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå……è¶³çš„æ³¨é‡Šé¢†åŸŸæ•°æ®çš„å¯ç”¨æ€§ã€‚é’ˆå¯¹è·¨é¢†åŸŸæ•°æ®å¯ç”¨æ€§çš„ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åµŒå…¥åˆ†ç±»å™¨æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬å³å¯å°†æœ€å…ˆè¿›çš„é¢†åŸŸç‰¹å®šæ¨¡å‹æ‰©å±•åˆ°å…¶ä»–é¢†åŸŸã€‚é€šè¿‡é‡‡ç”¨å¸¦æœ‰åŒæ„æ­£åˆ™åŒ–çš„ç›‘ç£å¾®è°ƒæ–¹æ³•æ¥è®­ç»ƒé¢†åŸŸç‰¹å®šçš„å¥å­åµŒå…¥å™¨ï¼Œä»¥åŠé‡‡ç”¨å¤šè¯­è¨€çŸ¥è¯†è’¸é¦ç­–ç•¥æ¥ä½¿åµŒå…¥å™¨è·¨å¤šä¸ªé¢†åŸŸè¿›è¡Œæ¦‚æ‹¬ã€‚å°†ç»è¿‡è®­ç»ƒçš„åµŒå…¥å™¨è¿›ä¸€æ­¥é…åˆç®€å•çš„çº¿æ€§åˆ†ç±»å™¨éƒ¨ç½²åˆ°æ–°çš„é¢†åŸŸä¸­ã€‚åœ¨åŠ æ‹¿å¤§å’Œå¢¨è¥¿å“¥ç”µå­å•†åŠ¡å®¢æˆ·å…³æ€€æ•°æ®é›†ä¸Šçš„å°‘é‡æ„å›¾æ£€æµ‹å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œå‡†ç¡®ç‡æé«˜äº†20-23%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¢æˆ·å…³æ€€åœ¨ç”µå­å•†åŠ¡ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä¼ä¸šä¸ºæ­¤æŠ•å…¥å·¨å¤§ã€‚</li>
<li>å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹å¦‚å¤šè¯­è¨€BERTåœ¨å®¢æˆ·å…³æ€€ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†å—é™äºæ³¨é‡Šé¢†åŸŸæ•°æ®çš„å¯ç”¨æ€§ã€‚</li>
<li>è·¨é¢†åŸŸæ•°æ®å¯ç”¨æ€§æ˜¯ç“¶é¢ˆï¼Œéœ€è¦æ¨¡å‹èƒ½å¤Ÿåœ¨å°‘é‡æ ‡æ³¨æ ·æœ¬ä¸‹è·¨é¢†åŸŸè¿›è¡Œæ¨å¹¿ã€‚</li>
<li>æå‡ºçš„åµŒå…¥åˆ†ç±»å™¨æ¨¡å‹æ¶æ„èƒ½å¤Ÿé€šè¿‡ç›‘ç£å¾®è°ƒæ–¹æ³•è®­ç»ƒé¢†åŸŸç‰¹å®šçš„å¥å­åµŒå…¥å™¨ã€‚</li>
<li>é‡‡ç”¨å¤šè¯­è¨€çŸ¥è¯†è’¸é¦ç­–ç•¥ä½¿åµŒå…¥å™¨è·¨å¤šä¸ªé¢†åŸŸè¿›è¡Œæ¦‚æ‹¬ã€‚</li>
<li>è®­ç»ƒçš„åµŒå…¥å™¨é…åˆç®€å•çš„çº¿æ€§åˆ†ç±»å™¨å¯éƒ¨ç½²åˆ°æ–°çš„é¢†åŸŸä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-311f775004090127677f7cc61b25fc9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aab59962f74941d67bd575248327282c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab018830ce3194d3c76c7f23f733757d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce401acf417d05c7bf8cadf148f8f734.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a7b4f3fb46a75d06c95df8edded0bb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53dc8c9e4e7c5fee235ad6669d63a925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff4ec810c9d67e30c06f86b261fc42e9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can-Masked-Autoencoders-Also-Listen-to-Birds"><a href="#Can-Masked-Autoencoders-Also-Listen-to-Birds" class="headerlink" title="Can Masked Autoencoders Also Listen to Birds?"></a>Can Masked Autoencoders Also Listen to Birds?</h2><p><strong>Authors:Lukas Rauch, RenÃ© Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz</strong></p>
<p>Masked Autoencoders (MAEs) have shown competitive results in audio classification by learning rich semantic representations through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, thereby revealing the performance limitations of general-domain Audio-MAE models. This work demonstrates that bridging this domain gap requires more than domain-specific pretraining data; adapting the entire training pipeline is crucial. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSetâ€™s multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAEâ€™s prototypical probes outperform linear probing by up to 37%$_\text{p}$ in MAP and narrow the gap to fine-tuning to approximately 3.3%$_\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains. </p>
<blockquote>
<p>åŸºäºMasked Autoencodersï¼ˆMAEsï¼‰åœ¨éŸ³é¢‘åˆ†ç±»ä¸­å±•ç°äº†å‡ºè‰²çš„ç»“æœï¼Œå®ƒé€šè¿‡é«˜æ•ˆçš„è‡ªç›‘ç£é‡å»ºä»»åŠ¡å­¦ä¹ ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå½“é€šç”¨æ¨¡å‹ç›´æ¥åº”ç”¨äºç»†ç²’åº¦éŸ³é¢‘åŸŸæ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚å…·ä½“æ¥è¯´ï¼Œé¸Ÿç±»å£°éŸ³åˆ†ç±»éœ€è¦åŒºåˆ†ç‰©ç§é—´çš„ç»†å¾®å·®å¼‚å¹¶åº”å¯¹é«˜ç‰©ç§å†…éƒ¨çš„å£°éŸ³å˜åŒ–ï¼Œä»è€Œæ­ç¤ºäº†é€šç”¨é¢†åŸŸéŸ³é¢‘MAEæ¨¡å‹çš„æ€§èƒ½å±€é™æ€§ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œç¼©å°è¿™ä¸€é¢†åŸŸå·®è·ä¸ä»…éœ€è¦ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®ï¼›è°ƒæ•´æ•´ä¸ªè®­ç»ƒç®¡é“ä¹Ÿè‡³å…³é‡è¦ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾å¹¶é€‚åº”äº†é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•ä»¥åŠä½¿ç”¨BirdSetï¼ˆä¸€ä¸ªä¸AudioSetç›¸å½“çš„å¤§è§„æ¨¡ç”Ÿç‰©å£°å­¦æ•°æ®é›†ï¼‰çš„é¸Ÿç±»å£°éŸ³çš„å†»ç»“ç‰¹å¾åˆ©ç”¨ã€‚æˆ‘ä»¬å¾—åˆ°çš„Bird-MAEåœ¨BirdSetçš„å¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹ï¼Œå¢å¼ºäº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸­æ¥è¿‘å¾®è°ƒæ€§èƒ½ã€‚Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨MAPä¸­çš„è¡¨ç°æ¯”çº¿æ€§æ¢é’ˆé«˜å‡ºæœ€å¤š37%ï¼Œå¹¶ç¼©å°äº†åœ¨BirdSetä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¾®è°ƒå·®è·ï¼Œå¹³å‡çº¦è¾¾3.3%ã€‚Bird-MAEè¿˜åœ¨æˆ‘ä»¬æ–°å»ºç«‹çš„BirdSetå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å¼ºå¤§çš„å°‘æ ·æœ¬èƒ½åŠ›ï¼Œçªæ˜¾äº†é’ˆå¯¹ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸé‡èº«å®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ ç®¡é“çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12880v2">PDF</a> under review @TMLR</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Masked Autoencodersï¼ˆMAEsï¼‰åœ¨éŸ³é¢‘åˆ†ç±»ä¸­çš„ç«äº‰æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é¸Ÿå£°åˆ†ç±»ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°åœ¨ç²¾ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸï¼Œé€šç”¨æ¨¡å‹å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæœ¬ç ”ç©¶é‡æ–°è®¾è®¡å¹¶ä¼˜åŒ–äº†é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•å’Œå†»ç»“ç‰¹å¾çš„åˆ©ç”¨æ–¹å¼ï¼Œä»¥é€‚åº”é¸Ÿå£°æ•°æ®ã€‚åŒæ—¶å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œæé«˜äº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸­æ¥è¿‘å¾®è°ƒæ€§èƒ½ã€‚æœ€ç»ˆï¼ŒBird-MAEæ¨¡å‹åœ¨BirdSetå¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°å…ˆè¿›æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Autoencoders (MAEs) åœ¨éŸ³é¢‘åˆ†ç±»ä¸­å±•ç°å‡ºç«äº‰åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é¸Ÿå£°åˆ†ç±»ä¸­çš„åº”ç”¨ã€‚</li>
<li>é€šç”¨æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>ç ”ç©¶é‡æ–°è®¾è®¡äº†é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•å’Œå†»ç»“ç‰¹å¾çš„åˆ©ç”¨æ–¹å¼ï¼Œä»¥é€‚åº”é¸Ÿå£°æ•°æ®ã€‚</li>
<li>å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œæé«˜äº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ã€‚</li>
<li>Bird-MAEæ¨¡å‹åœ¨BirdSetå¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚</li>
<li>Bird-MAEæ¨¡å‹é€šè¿‡åŸå‹æ¢æµ‹å±•ç¤ºäº†å¼ºå¤§çš„å°‘æ ·æœ¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c95e35d561e795abc926d5577adde137.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3241ad7918215e514014c801ff5ce4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83fd6ee9d0f1462de6c3ed1077eefd77.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SNaRe-Domain-aware-Data-Generation-for-Low-Resource-Event-Detection"><a href="#SNaRe-Domain-aware-Data-Generation-for-Low-Resource-Event-Detection" class="headerlink" title="SNaRe: Domain-aware Data Generation for Low-Resource Event Detection"></a>SNaRe: Domain-aware Data Generation for Low-Resource Event Detection</h2><p><strong>Authors:Tanmay Parekh, Yuxuan Dong, Lucas Bandarkar, Artin Kim, I-Hung Hsu, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Event Detection (ED) â€“ the task of identifying event mentions from natural language text â€“ is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot&#x2F;few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaReâ€™s stronger annotation quality and reduced domain drift. </p>
<blockquote>
<p>äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰â€”â€”ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­è¯†åˆ«äº‹ä»¶æåŠçš„ä»»åŠ¡â€”â€”å¯¹äºåœ¨ç”Ÿç‰©åŒ»å­¦ã€æ³•å¾‹å’Œæµè¡Œç—…å­¦ç­‰é«˜åº¦ä¸“ä¸šåŒ–é¢†åŸŸè¿›è¡Œæ¨ç†è‡³å…³é‡è¦ã€‚æ•°æ®ç”Ÿæˆå·²è¯æ˜åœ¨æ‰©å¤§å…¶åœ¨æ›´å¹¿æ³›åº”ç”¨ä¸­çš„æ•ˆç”¨æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œè€Œæ— éœ€æ˜‚è´µçš„ä¸“å®¶æ³¨é‡Šã€‚ç„¶è€Œï¼Œå½“å°†ç°æœ‰çš„ç”Ÿæˆæ–¹æ³•åº”ç”¨äºä¸“ä¸šé¢†åŸŸæ—¶ï¼Œå®ƒä»¬ä¼šé¢ä¸´æ ‡ç­¾å™ªå£°çš„é—®é¢˜ï¼Œå³æ³¨é‡Šä¸æ­£ç¡®ï¼Œä»¥åŠé¢†åŸŸæ¼‚ç§»ï¼Œè¡¨ç°ä¸ºç”Ÿæˆå¥å­ä¸ç›®æ ‡é¢†åŸŸä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SNaReï¼Œè¿™æ˜¯ä¸€ä¸ªé¢†åŸŸæ„ŸçŸ¥çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼šä¾¦å¯Ÿå…µï¼ˆScoutï¼‰ã€å™è¿°è€…ï¼ˆNarratorï¼‰å’Œç²¾ç‚¼è€…ï¼ˆRefinerï¼‰ã€‚Scoutä»ç›®æ ‡é¢†åŸŸçš„æœªæ ‡è®°æ•°æ®ä¸­æå–è§¦å‘å™¨ï¼Œå¹¶ä½¿ç”¨è¯­æ–™åº“çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯æ¥æ•´ç†é«˜è´¨é‡çš„ä¸“ä¸šç‰¹å®šè§¦å‘å™¨åˆ—è¡¨ï¼Œä»¥ç¼“è§£é¢†åŸŸæ¼‚ç§»ã€‚å™è¿°è€…æ ¹æ®è¿™äº›è§¦å‘å™¨ç”Ÿæˆé«˜è´¨é‡ä¸”ä¸é¢†åŸŸç›¸ç¬¦çš„å¥å­ï¼Œè€Œç²¾ç‚¼è€…åˆ™è¯†åˆ«å…¶ä»–äº‹ä»¶æåŠï¼Œç¡®ä¿é«˜æ³¨é‡Šè´¨é‡ã€‚åœ¨ä¸‰ä¸ªä¸åŒé¢†åŸŸçš„EDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSNaReçš„è¡¨ç°ä¼˜äºæœ€ä½³åŸºçº¿ï¼Œåœ¨é›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬è®¾ç½®ä¸­å¹³å‡F1å¾—åˆ†æé«˜äº†3-7%ï¼Œåœ¨å¤šè¯­è¨€ç”Ÿæˆä¸­F1å¾—åˆ†æé«˜äº†4-20%ã€‚é€šè¿‡åˆ†æç”Ÿæˆçš„è§¦å‘å™¨å‘½ä¸­ç‡å’Œäººç±»è¯„ä¼°ï¼Œè¯å®äº†SNaReçš„æ³¨é‡Šè´¨é‡æ›´é«˜ï¼Œé¢†åŸŸæ¼‚ç§»å‡å°‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17394v2">PDF</a> Under review at ACL ARR May 2025</p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰æ˜¯ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­è¯†åˆ«äº‹ä»¶æåŠçš„ä»»åŠ¡ï¼Œå¯¹äºç”Ÿç‰©åŒ»å­¦ã€æ³•å¾‹å’Œæµè¡Œç—…å­¦ç­‰é«˜åº¦ä¸“ä¸šåŒ–é¢†åŸŸä¸­çš„æ¨ç†è‡³å…³é‡è¦ã€‚æ•°æ®ç”Ÿæˆå·²è¯æ˜å¯ä»¥æœ‰æ•ˆæ‰©å¤§å…¶åœ¨æ›´å¹¿æ³›åº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œè€Œæ— éœ€æ˜‚è´µçš„ä¸“å®¶æ³¨é‡Šã€‚ç„¶è€Œï¼Œå½“ç°æœ‰ç”Ÿæˆæ–¹æ³•åº”ç”¨äºä¸“ä¸šé¢†åŸŸæ—¶ï¼Œå®ƒä»¬ä¼šé¢ä¸´æ ‡ç­¾å™ªå£°ï¼ˆæ³¨é‡Šä¸æ­£ç¡®ï¼‰å’Œé¢†åŸŸæ¼‚ç§»ï¼ˆç”Ÿæˆå¥å­ä¸ç›®æ ‡é¢†åŸŸä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…ï¼‰çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SNaReï¼Œè¿™æ˜¯ä¸€ä¸ªé¢†åŸŸæ„ŸçŸ¥çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼šä¾¦å¯Ÿå…µã€å™è¿°è€…å’Œç²¾ç‚¼è€…ã€‚ä¾¦å¯Ÿå…µä»ç›®æ ‡é¢†åŸŸçš„æœªæ ‡è®°æ•°æ®ä¸­æå–è§¦å‘å™¨ï¼Œå¹¶ä½¿ç”¨è¯­æ–™åº“çº§åˆ«çš„ç»Ÿè®¡æ•°æ®æ¥ä¼˜åŒ–é«˜è´¨é‡çš„é¢†åŸŸç‰¹å®šè§¦å‘å™¨åˆ—è¡¨ï¼Œä»¥å‡è½»é¢†åŸŸæ¼‚ç§»çš„é—®é¢˜ã€‚å™è¿°è€…æ ¹æ®è¿™äº›è§¦å‘å™¨ç”Ÿæˆé«˜è´¨é‡çš„é¢†åŸŸå¯¹é½å¥å­ï¼Œè€Œç²¾ç‚¼è€…åˆ™è¯†åˆ«å…¶ä»–äº‹ä»¶æåŠï¼Œç¡®ä¿é«˜æ³¨é‡Šè´¨é‡ã€‚åœ¨ä¸‰ä¸ªä¸åŒçš„é¢†åŸŸEDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSNaReçš„è¡¨ç°è¶…è¿‡äº†æœ€ä½³åŸºçº¿ï¼Œåœ¨é›¶æ ·æœ¬&#x2F;å°‘æ ·æœ¬è®¾ç½®ä¸­å¹³å‡F1å¾—åˆ†æé«˜äº†3-7%ï¼Œåœ¨å¤šè¯­è¨€ç”Ÿæˆä¸­æé«˜äº†4-20%çš„F1å¾—åˆ†ã€‚å¯¹ç”Ÿæˆçš„è§¦å‘å™¨å‘½ä¸­ç‡å’Œäººç±»è¯„ä¼°çš„åˆ†æè¯å®äº†SNaReæ›´å¼ºçš„æ³¨é‡Šè´¨é‡å’Œå‡å°‘çš„é¢†åŸŸæ¼‚ç§»é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶æ£€æµ‹ï¼ˆEDï¼‰åœ¨é«˜åº¦ä¸“ä¸šåŒ–çš„é¢†åŸŸä¸­å¦‚ç”Ÿç‰©åŒ»å­¦ã€æ³•å¾‹å’Œæµè¡Œç—…å­¦ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>æ•°æ®ç”Ÿæˆæ‰©å¤§äº†äº‹ä»¶æ£€æµ‹åœ¨æ›´å¹¿æ³›åº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œä½†é¢ä¸´æ ‡ç­¾å™ªå£°å’Œé¢†åŸŸæ¼‚ç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>SNaReæ˜¯ä¸€ä¸ªé¢†åŸŸæ„ŸçŸ¥çš„åˆæˆæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…å«ä¾¦å¯Ÿå…µã€å™è¿°è€…å’Œç²¾ç‚¼è€…ä¸‰ä¸ªç»„ä»¶ã€‚</li>
<li>ä¾¦å¯Ÿå…µé€šè¿‡æå–ç›®æ ‡é¢†åŸŸçš„è§¦å‘å™¨å¹¶ä¼˜åŒ–é«˜è´¨é‡çš„é¢†åŸŸç‰¹å®šè§¦å‘å™¨åˆ—è¡¨æ¥å‡è½»é¢†åŸŸæ¼‚ç§»é—®é¢˜ã€‚</li>
<li>å™è¿°è€…æ ¹æ®è§¦å‘å™¨ç”Ÿæˆé«˜è´¨é‡çš„é¢†åŸŸå¯¹é½å¥å­ã€‚</li>
<li>ç²¾ç‚¼è€…ç¡®ä¿é«˜æ³¨é‡Šè´¨é‡ï¼Œé€šè¿‡è¯†åˆ«å…¶ä»–äº‹ä»¶æåŠã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fde820f63794c3e97f86be76f6cf424a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07e1df7a4c7e9ae0b6579835f046ec8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbd0a0ce9c561e145c6eac9c1a070aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ee014b8b748ec27f7643a9aee8df598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d9a82ecb493cd0335bfddff8c300dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d32cbfad380367c8a2b8dbc65a47cd8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unleashing-The-Power-of-Pre-Trained-Language-Models-for-Irregularly-Sampled-Time-Series"><a href="#Unleashing-The-Power-of-Pre-Trained-Language-Models-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Unleashing The Power of Pre-Trained Language Models for Irregularly   Sampled Time Series"></a>Unleashing The Power of Pre-Trained Language Models for Irregularly   Sampled Time Series</h2><p><strong>Authors:Weijia Zhang, Chenlong Yin, Hao Liu, Hui Xiong</strong></p>
<p>Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series analysis, intending to create a unified foundation model that addresses various time series analytical tasks. However, these efforts predominantly focus on Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by Irregularly Sampled Time Series (ISTS), which are characterized by uneven sampling intervals and prevalent missing data. To bridge this gap, this work takes the first step in exploring the potential of PLMs for ISTS analysis. We begin by investigating the effect of various methods for representing ISTS, aiming to maximize the efficacy of PLMs in the analysis. Furthermore, we propose a unified PLM-based framework, named ISTS-PLM, to address diverse ISTS analytical tasks. It integrates novel time-aware and variable-aware PLMs tailored to tackle the intractable intra- and inter-time series modeling in ISTS. Finally, extensive experiments on a comprehensive benchmark demonstrate that the ISTS-PLM, utilizing a structured and effective series-based representation for ISTS, consistently achieves state-of-the-art performance across various analytical tasks, such as classification, interpolation, extrapolation, few-shot and zero-shot learning scenarios, spanning scientific domains like healthcare, biomechanics, and climate science. </p>
<blockquote>
<p>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è¿™ä¸€è¿›æ­¥æ¿€å‘äº†ä¸€ç³»åˆ—åˆ›æ–°ç ”ç©¶ï¼Œæ¢ç´¢å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹é€‚åº”äºæ—¶é—´åºåˆ—åˆ†æï¼Œæ—¨åœ¨åˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ï¼Œä»¥è§£å†³å„ç§æ—¶é—´åºåˆ—åˆ†æä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨è§„åˆ™é‡‡æ ·æ—¶é—´åºåˆ—ï¼ˆRSTSï¼‰ä¸Šï¼Œå¿½è§†äº†ä¸è§„åˆ™é‡‡æ ·æ—¶é—´åºåˆ—ï¼ˆISTSï¼‰å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå…¶ç‰¹ç‚¹æ˜¯é‡‡æ ·é—´éš”ä¸å‡åŒ€ä¸”æ™®éå­˜åœ¨ç¼ºå¤±æ•°æ®ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨ISTSåˆ†æä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬é¦–å…ˆä»ç ”ç©¶è¡¨ç¤ºISTSçš„å„ç§æ–¹æ³•çš„æ•ˆæœå¼€å§‹ï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°æé«˜é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨åˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œåä¸ºISTS-PLMï¼Œç”¨äºå¤„ç†å¤šæ ·çš„ISTSåˆ†æä»»åŠ¡ã€‚å®ƒé›†æˆäº†æ–°å‹çš„æ—¶é—´æ„ŸçŸ¥å’Œå˜é‡æ„ŸçŸ¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè§£å†³ISTSä¸­å¤æ‚çš„å•æ—¶é—´åºåˆ—å’Œå¤šæ—¶é—´åºåˆ—å»ºæ¨¡é—®é¢˜ã€‚æœ€åï¼Œåœ¨å…¨é¢çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒISTS-PLMåˆ©ç”¨ç»“æ„åŒ–å’Œæœ‰æ•ˆçš„ISTSç³»åˆ—è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨å„ç§åˆ†æä»»åŠ¡ä¸­å§‹ç»ˆå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚åˆ†ç±»ã€æ’å€¼ã€å¤–æ¨ã€å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ï¼Œæ¶µç›–åŒ»ç–—ã€ç”Ÿç‰©åŠ›å­¦å’Œæ°”å€™ç§‘å­¦ç­‰ç§‘å­¦é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08328v2">PDF</a> Accepted by KDDâ€™25</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰å¦‚ChatGPTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¹¶æ¿€å‘äº†ä¸€ç³»åˆ—å°†å…¶é€‚åº”æ—¶é—´åºåˆ—åˆ†æçš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦å…³æ³¨è§„åˆ™é‡‡æ ·æ—¶é—´åºåˆ—ï¼ˆRSTSï¼‰ï¼Œå¿½ç•¥äº†ä¸è§„åˆ™é‡‡æ ·æ—¶é—´åºåˆ—ï¼ˆISTSï¼‰å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚ä¸å‡åŒ€çš„é‡‡æ ·é—´éš”å’Œæ™®éå­˜åœ¨çš„ç¼ºå¤±æ•°æ®ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†PLMsåœ¨ISTSåˆ†æä¸­çš„æ½œåŠ›ï¼Œç ”ç©¶äº†è¡¨ç¤ºISTSçš„å„ç§æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºPLMsçš„æ¡†æ¶ISTS-PLMï¼Œä»¥è§£å†³å¤šæ ·çš„ISTSåˆ†æä»»åŠ¡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ–°å‹çš„æ—¶é—´æ„ŸçŸ¥å’Œå˜é‡æ„ŸçŸ¥PLMsï¼Œä»¥å¤„ç†ISTSä¸­çš„å¤æ‚çš„æ—¶é—´åºåˆ—å†…å’Œæ—¶é—´åºåˆ—é—´çš„å»ºæ¨¡é—®é¢˜ã€‚åœ¨å…¨é¢çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒISTS-PLMåˆ©ç”¨ç»“æ„åŒ–å’Œæœ‰æ•ˆçš„ç³»åˆ—è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨å„ç§åˆ†æä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰é‡å¤§è¿›å±•ã€‚</li>
<li>PLMsæ­£åœ¨è¢«æ¢ç´¢ä»¥é€‚åº”æ—¶é—´åºåˆ—åˆ†æã€‚</li>
<li>ç›®å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨è§„åˆ™é‡‡æ ·æ—¶é—´åºåˆ—ï¼ˆRSTSï¼‰ï¼Œå¿½ç•¥äº†ä¸è§„åˆ™é‡‡æ ·æ—¶é—´åºåˆ—ï¼ˆISTSï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>ISTSå…·æœ‰ä¸å‡åŒ€çš„é‡‡æ ·é—´éš”å’Œæ™®éçš„ç¼ºå¤±æ•°æ®ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢ç´¢äº†PLMsåœ¨ISTSåˆ†æä¸­çš„æ½œåŠ›ï¼Œå¹¶ç ”ç©¶äº†è¡¨ç¤ºISTSçš„ä¸åŒæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä¸ºISTS-PLMçš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆæ—¶é—´æ„ŸçŸ¥å’Œå˜é‡æ„ŸçŸ¥PLMsä»¥è§£å†³å¤šæ ·çš„ISTSåˆ†æä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6d27ccbf37218e745435247ca1e852e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474a385a7704ebe30d7fc158fd21b1dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a4c0516c9c33a8526b6c84c7be3cbac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ae7988dd5365aee4d919999f11d6013.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-024178d03eb18285b87c073de72f8965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e7bcc2d1eb0dc6c43ade8662ed589e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea36764e1af6271fd4d3a7388adad22.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fb31f01aafaa23a009a481c6f091105b.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  Deep learning image burst stacking to reconstruct high-resolution   ground-based solar observations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96d12c51275a45c5dadcb236c0b4f035.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  Truly Self-Improving Agents Require Intrinsic Metacognitive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
