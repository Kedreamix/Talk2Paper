<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  Contrastive Flow Matching">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-b07b4027ffc3ee280a3b21e0c6311fea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-08-æ›´æ–°"><a href="#2025-06-08-æ›´æ–°" class="headerlink" title="2025-06-08 æ›´æ–°"></a>2025-06-08 æ›´æ–°</h1><h2 id="Contrastive-Flow-Matching"><a href="#Contrastive-Flow-Matching" class="headerlink" title="Contrastive Flow Matching"></a>Contrastive Flow Matching</h2><p><strong>Authors:George Stoica, Vivek Ramanujan, Xiang Fan, Ali Farhadi, Ranjay Krishna, Judy Hoffman</strong></p>
<p>Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteedâ€“flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: <a target="_blank" rel="noopener" href="https://github.com/gstoica27/DeltaFM.git">https://github.com/gstoica27/DeltaFM.git</a>. </p>
<blockquote>
<p>æ— æ¡ä»¶æµåŒ¹é…è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼ºåˆ¶æ ·æœ¬å¯¹ä¹‹é—´çš„æµæ˜¯å”¯ä¸€çš„ï¼Œä»æºåˆ†å¸ƒä¼ è¾“æ ·æœ¬åˆ°ç›®æ ‡åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨æ¡ä»¶è®¾ç½®ï¼ˆä¾‹å¦‚ï¼Œç±»æ¡ä»¶æ¨¡å‹ï¼‰ä¸­ï¼Œè¿™ç§å”¯ä¸€æ€§ä¸å†è¢«ä¿è¯â€”â€”æ¥è‡ªä¸åŒæ¡ä»¶çš„æµå¯èƒ½ä¼šé‡å ï¼Œå¯¼è‡´æ›´æ¨¡ç³Šçš„ç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†å¯¹æ¯”æµåŒ¹é…ï¼ˆContrastive Flow Matchingï¼‰ï¼Œè¿™æ˜¯å¯¹æµåŒ¹é…ç›®æ ‡çš„æ‰©å±•ï¼Œå®ƒæ˜ç¡®åœ°å¼ºåˆ¶æ‰§è¡Œæ‰€æœ‰æ¡ä»¶æµä¹‹é—´çš„å”¯ä¸€æ€§ï¼Œå¢å¼ºäº†æ¡ä»¶åˆ†ç¦»ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¢åŠ äº†ä¸€ä¸ªå¯¹æ¯”ç›®æ ‡ï¼Œè¯¥ç›®æ ‡æœ€å¤§åŒ–ä»»æ„æ ·æœ¬å¯¹ä¹‹é—´é¢„æµ‹æµçš„å·®å¼‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸Šè¿›è¡Œå¹¿æ³›çš„å®éªŒï¼Œåœ¨ç±»æ¡ä»¶ï¼ˆImageNet-1kï¼‰å’Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆCC3Mï¼‰åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å¯¹æ¯”æµåŒ¹é…çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨å¯¹æ¯”æµåŒ¹é…è®­ç»ƒæ¨¡å‹ï¼ˆ1ï¼‰å°†è®­ç»ƒé€Ÿåº¦æé«˜äº†é«˜è¾¾9å€ï¼Œï¼ˆ2ï¼‰å‡å°‘äº†é«˜è¾¾5å€çš„é™å™ªæ­¥éª¤ï¼Œå¹¶ä¸”ï¼ˆ3ï¼‰ä¸ç”¨æµåŒ¹é…è®­ç»ƒç›¸åŒæ¨¡å‹ç›¸æ¯”ï¼ŒFIDé™ä½äº†é«˜è¾¾8.9ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/gstoica27/DeltaFM.git%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/gstoica27/DeltaFM.gitå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05350v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä¸­çš„å¯¹æ¯”æµåŒ¹é…æ–¹æ³•é€šè¿‡åœ¨é¢„æµ‹æµä¸­æ˜ç¡®å¼ºåˆ¶æ‰§è¡Œå”¯ä¸€æ€§æ¥æ”¹è¿›æ¡ä»¶åˆ†ç¦»ã€‚é€šè¿‡æ·»åŠ æœ€å¤§åŒ–ä»»æ„æ ·æœ¬å¯¹é¢„æµ‹æµä¹‹é—´å·®å¼‚åº¦çš„å¯¹æ¯”ç›®æ ‡æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æé«˜è®­ç»ƒé€Ÿåº¦ã€å‡å°‘å»å™ªæ­¥éª¤æ•°é‡å¹¶é™ä½FIDå¾—åˆ†ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨æŒ‡å®šé“¾æ¥ä¸Šã€‚æ­¤æ–¹æ³•æœ‰åŠ©äºæ”¹å–„æ¡ä»¶è®¾ç½®ä¸‹æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†ç±»æ¡ä»¶ä»»åŠ¡å’Œæ–‡æœ¬è½¬å›¾åƒä»»åŠ¡æ—¶è¡¨ç°æ˜¾è‘—ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹æ¯”æµåŒ¹é…æ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„æ”¹è¿›æ–¹æ³•ï¼Œæ—¨åœ¨åŠ å¼ºæ¡ä»¶åˆ†ç¦»ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»æ¡ä»¶æ¨¡å‹å’Œæ–‡æœ¬è½¬å›¾åƒæ¨¡å‹ä¸­ã€‚å®ƒé€šè¿‡å¼ºåˆ¶æ‰§è¡Œé¢„æµ‹æµä¸­çš„å”¯ä¸€æ€§æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”æµåŒ¹é…å¼•å…¥äº†ä¸€ä¸ªå¯¹æ¯”ç›®æ ‡ï¼Œè¯¥ç›®æ ‡æœ€å¤§åŒ–ä»»æ„æ ·æœ¬å¯¹é¢„æµ‹æµä¹‹é—´çš„ä¸ç›¸ä¼¼åº¦ã€‚è¿™ç§å·®å¼‚åº¦æœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°åŒºåˆ†ä¸åŒæ¡ä»¶ä¸‹çš„æµï¼Œä»è€Œå‡å°‘æ¨¡ç³Šç”Ÿæˆçš„å¯èƒ½æ€§ã€‚</li>
<li>å¯¹æ¯”æµåŒ¹é…èƒ½æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œæœ€å¤šå¯è¾¾9å€ã€‚è¿™æ„å‘³ç€ä½¿ç”¨å¯¹æ¯”æµåŒ¹é…çš„æ¨¡å‹å¯ä»¥åœ¨æ›´çŸ­çš„æ—¶é—´å†…å®Œæˆè®­ç»ƒä»»åŠ¡ã€‚</li>
<li>å¯¹æ¯”æµåŒ¹é…å‡å°‘äº†å»å™ªæ­¥éª¤çš„æ•°é‡ï¼Œæœ€å¤šå¯è¾¾5å€ã€‚è¿™ä½¿å¾—æ‰©æ•£æ¨¡å‹åœ¨è¿›è¡Œå™ªå£°æ•°æ®é¢„å¤„ç†æ—¶æ›´ä¸ºé«˜æ•ˆã€‚è¿™ä¹Ÿæœ‰åˆ©äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒå’Œé™ä½è®¡ç®—æˆæœ¬ã€‚è¿™ç§æ•ˆç‡çš„æé«˜å¾—ç›Šäºæ¨¡å‹å¯¹äºè¾“å…¥æ•°æ®çœŸå®æ€§å’Œå¤šæ ·æ€§çš„æ›´å¥½ç†è§£ã€‚</li>
<li>å¯¹æ¯”æµåŒ¹é…é™ä½äº†FIDå¾—åˆ†ï¼Œæœ€å¤šå¯é™ä½8.9åˆ†ã€‚FIDå¾—åˆ†æ˜¯è¡¡é‡ç”Ÿæˆå›¾åƒè´¨é‡çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ï¼Œå› æ­¤è¿™ä¸€æ”¹è¿›å¯¹äºæé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡è‡³å…³é‡è¦ã€‚è¿™ä¹Ÿè¯æ˜äº†å¯¹æ¯”æµåŒ¹é…åœ¨æ”¹å–„æ‰©æ•£æ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸€æ”¹è¿›æœ‰åŠ©äºç”Ÿæˆæ›´åŠ çœŸå®å’Œå¤šæ ·åŒ–çš„å›¾åƒï¼Œä»è€Œæé«˜æ¨¡å‹çš„å®ç”¨æ€§ã€‚é€šè¿‡å¼ºåŒ–æ¨¡å‹çš„æ„ŸçŸ¥è´¨é‡ï¼Œå¯¹æ¯”æµåŒ¹é…ä½¿å¾—æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ›´å…·ç«äº‰åŠ›ã€‚è¿™ä¸€è¿›æ­¥æœ‰åŠ©äºæ¨åŠ¨æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”æµåŒ¹é…è¿˜èƒ½å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å¤„ç†å¤æ‚çš„å›¾åƒç”Ÿæˆä»»åŠ¡å’Œæ•°æ®é›†å¤šæ ·æ€§é—®é¢˜ã€‚å› æ­¤ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œæ½œåŠ›ä»·å€¼ã€‚éšç€æ›´å¤šç ”ç©¶çš„æ·±å…¥è¿›è¡Œå’ŒæŠ€æœ¯çš„ä¸æ–­è¿­ä»£æ›´æ–°ï¼Œç›¸ä¿¡å¯¹æ¯”æµåŒ¹é…å°†ä¼šåœ¨æ‰©æ•£æ¨¡å‹ä¸­å‘æŒ¥æ›´å¤§çš„ä½œç”¨å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•è¿›æ­¥ã€‚é€šè¿‡ä¼˜åŒ–ç®—æ³•ç»“æ„å’Œæ”¹è¿›è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…æœªæ¥æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æ›´é«˜è´¨é‡å’Œæ€§èƒ½è¡¨ç°ã€‚è¿™ä¹Ÿä¸ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¤„ç†é¢†åŸŸçš„å‘å±•å¼€è¾Ÿäº†æ–°çš„é“è·¯å’Œæ–¹å‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­æ·±å…¥ç ”ç©¶å’Œä¼˜åŒ–å°†æœ‰åŠ©äºå®ç°æ›´å¤šåˆ›æ–°çªç ´å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯é©æ–°è¿›ç¨‹é¡ºåˆ©è¿›è¡Œï¼ç”±äºç®—æ³•çš„å¤šæ ·æ€§å’Œå®é™…åº”ç”¨éœ€æ±‚çš„ä¸æ–­å¢é•¿è¿™ä¸€é¢†åŸŸçš„æœªæ¥å°†å…·æœ‰æ›´å¤šæ½œåŠ›å¹¶ä¸”å°†ç»§ç»­å‘æŒ¥å…³é”®ä½œç”¨ä»¥ç¡®ä¿æ–°æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å’Œæ¨å¹¿æ™®åŠä¸ºç¤¾ä¼šè¿›æ­¥å’Œå‘å±•è´¡çŒ®åŠ›é‡</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e8a7af8dbc08a7c22a81f40611e78bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab29980565f88c7f99bb1fecae82e3a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73aedb034fa92b615492bbe42ab9eee9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7d0ddc3a55f4a9979231cc4233fc483.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3292fcec4ee32c3ad39649ff270b68c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-Diffusion-Transformer-Designs-via-Grafting"><a href="#Exploring-Diffusion-Transformer-Designs-via-Grafting" class="headerlink" title="Exploring Diffusion Transformer Designs via Grafting"></a>Exploring Diffusion Transformer Designs via Grafting</h2><p><strong>Authors:Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei</strong></p>
<p>Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present grafting, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL&#x2F;2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for DiT-XL&#x2F;2) using &lt;2% pretraining compute. We then graft a text-to-image model (PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL&#x2F;2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2x and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: <a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu</a> </p>
<blockquote>
<p>è®¾è®¡æ¨¡å‹æ¶æ„éœ€è¦è¿›è¡Œè¯¸å¦‚é€‰æ‹©æ“ä½œç¬¦ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›ã€å·ç§¯ï¼‰å’Œé…ç½®ï¼ˆä¾‹å¦‚æ·±åº¦ã€å®½åº¦ï¼‰ç­‰å†³ç­–ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›å†³ç­–å¯¹æ¨¡å‹è´¨é‡çš„å½±å“éœ€è¦è¿›è¡Œæ˜‚è´µçš„é¢„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†æ¶æ„çš„æ¢ç©¶ã€‚å—åˆ°å¦‚ä½•åŸºäºç°æœ‰ä»£ç æ„å»ºæ–°è½¯ä»¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªé—®é¢˜ï¼šèƒ½å¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥ç ”ç©¶æ–°çš„æ¶æ„è®¾è®¡ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å«æ¥ï¼ˆä¸€ç§ç®€å•çš„æ–¹æ³•ï¼‰ï¼Œç”¨äºç¼–è¾‘é¢„è®­ç»ƒçš„æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰ä»¥åœ¨è¾ƒå°çš„è®¡ç®—é¢„ç®—ä¸‹å®ç°æ–°çš„æ¶æ„ã€‚åŸºäºæˆ‘ä»¬å¯¹æ¿€æ´»è¡Œä¸ºå’Œæ³¨æ„åŠ›å±€éƒ¨æ€§çš„åˆ†æï¼Œæˆ‘ä»¬ä»¥DiT-XL&#x2F;2è®¾è®¡ä¸ºåŸºç¡€æ„å»ºäº†ä¸€ä¸ªæµ‹è¯•å¹³å°ï¼Œä»¥ç ”ç©¶å«æ¥å¯¹æ¨¡å‹è´¨é‡çš„å½±å“ã€‚ä½¿ç”¨è¿™ä¸ªæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬é€šè¿‡å«æ¥å¼€å‘äº†ä¸€ç³»åˆ—æ··åˆè®¾è®¡ï¼šç”¨é—¨æ§å·ç§¯ã€å±€éƒ¨æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›æ›¿æ¢softmaxæ³¨æ„åŠ›ï¼Œå¹¶ç”¨å¯å˜æ‰©å±•ç‡å’Œå·ç§¯å˜ä½“æ›¿æ¢MLPã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šæ··åˆè®¾è®¡åœ¨é¢„è®­ç»ƒè®¡ç®—ä½¿ç”¨é‡ä¸åˆ°2%çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨FIDï¼ˆ2.38-2.64ä¸DiT-XL&#x2F;2çš„2.27ï¼‰è¾¾åˆ°äº†è‰¯å¥½çš„è´¨é‡ã€‚ç„¶åæˆ‘ä»¬å°†ä¸€ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆPixArt-Sigmaï¼‰è¿›è¡Œå«æ¥ï¼Œå®ç°äº†1.43å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶GenEvalåˆ†æ•°ä¸‹é™ä¸åˆ°2%ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œé€šè¿‡å«æ¥é‡æ–°æ„å»ºDiT-XL&#x2F;2ï¼Œå°†æ¯ä¸€å¯¹è¿ç»­çš„å˜å‹å™¨å—è½¬æ¢ä¸ºå¹¶è¡Œå—ã€‚è¿™å‡å°‘äº†æ¨¡å‹æ·±åº¦çš„ä¸€åŠï¼Œå¹¶äº§ç”Ÿäº†æ›´å¥½çš„è´¨é‡ï¼ˆFIDï¼š2.77ï¼‰ï¼Œä¼˜äºå…¶ä»–å…·æœ‰ç›¸ä¼¼æ·±åº¦çš„æ¨¡å‹ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡å«æ¥é¢„è®­ç»ƒçš„DiTsæ¥æ¢ç´¢æ–°çš„æ‰©æ•£æ¨¡å‹è®¾è®¡ï¼Œç¼–è¾‘èŒƒå›´ä»æ“ä½œç¬¦æ›¿æ¢åˆ°æ¶æ„é‡ç»„ã€‚ä»£ç å’Œå«æ¥æ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05340v1">PDF</a> 22 pages; Project website: <a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡é€šè¿‡å€Ÿé‰´ç°æœ‰è½¯ä»¶çš„æ„å»ºæ–¹å¼ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹æ¶æ„è®¾è®¡æ–¹æ³•â€”â€”å«æ¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•å…è®¸åœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—ä¸‹å®ç°æ–°çš„æ¶æ„ï¼Œæ— éœ€æ˜‚è´µçš„é¢„è®­ç»ƒæˆæœ¬ã€‚ç ”ç©¶ä»¥æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ä¸ºåŸºç¡€ï¼Œé€šè¿‡æ¿€æ´»è¡Œä¸ºå’Œæ³¨æ„åŠ›å±€éƒ¨æ€§çš„åˆ†æï¼Œæ„å»ºäº†ä¸€ä¸ªæµ‹è¯•å¹³å°æ¥ç ”ç©¶å«æ¥å¯¹æ¨¡å‹è´¨é‡çš„å½±å“ã€‚åœ¨æ­¤å¹³å°ä¸Šï¼Œé€šè¿‡æ›¿æ¢softmaxæ³¨æ„åŠ›æœºåˆ¶ã€å±€éƒ¨æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›ä»¥åŠMLPçš„æ‰©å±•æ¯”å’Œå·ç§¯å˜ä½“ç­‰æ–¹å¼ï¼Œå¼€å‘äº†ä¸€ç³»åˆ—æ··åˆè®¾è®¡ã€‚è®¸å¤šæ··åˆè®¾è®¡åœ¨é¢„è®­ç»ƒè®¡ç®—é‡ä¸åˆ°2%çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†è‰¯å¥½çš„æ¨¡å‹è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†å¦‚ä½•å°†æ–‡æœ¬åˆ°å›¾åƒçš„æ¨¡å‹ï¼ˆPixArt-Sigmaï¼‰è¿›è¡Œå«æ¥ï¼Œå®ç°äº†1.43å€çš„åŠ é€Ÿï¼ŒåŒæ—¶GenEvalåˆ†æ•°ä¸‹é™ä¸åˆ°2%ã€‚æœ€åï¼Œé€šè¿‡å«æ¥é¢„è®­ç»ƒçš„DiTsé‡æ–°æ„å»ºDiT-XL&#x2F;2æ¨¡å‹ï¼Œå®ç°äº†æ¨¡å‹çš„å¹¶è¡ŒåŒ–é‡æ„ï¼Œå‡å°‘äº†æ¨¡å‹æ·±åº¦ï¼Œå¹¶æé«˜äº†æ¨¡å‹è´¨é‡ã€‚ç ”ç©¶å±•ç¤ºäº†é€šè¿‡å«æ¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥æ¢ç´¢æ–°çš„æ‰©æ•£æ¨¡å‹è®¾è®¡çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†é€šè¿‡å«æ¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥ç ”ç©¶æ–°æ¶æ„çš„æ–¹æ³•ï¼Œé™ä½äº†æ¨¡å‹è®¾è®¡çš„æˆæœ¬ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŸºäºDiT-XL&#x2F;2è®¾è®¡çš„æµ‹è¯•å¹³å°ï¼Œç”¨äºç ”ç©¶å«æ¥å¯¹æ¨¡å‹è´¨é‡çš„å½±å“ã€‚</li>
<li>é€šè¿‡æ›¿æ¢æ³¨æ„åŠ›æœºåˆ¶å’ŒMLPçš„æ–¹å¼ï¼Œå¼€å‘äº†ä¸€ç³»åˆ—æ··åˆè®¾è®¡ï¼Œå®ç°äº†è‰¯å¥½çš„æ¨¡å‹è´¨é‡ã€‚</li>
<li>å±•ç¤ºäº†å°†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œå«æ¥çš„å®ä¾‹ï¼Œæé«˜äº†æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å«æ¥é‡æ–°æ„å»ºDiT-XL&#x2F;2æ¨¡å‹ï¼Œå®ç°äº†æ¨¡å‹çš„å¹¶è¡ŒåŒ–é‡æ„ï¼Œå‡å°‘äº†æ¨¡å‹æ·±åº¦ï¼Œæé«˜äº†è´¨é‡ã€‚</li>
<li>å±•ç¤ºäº†å«æ¥é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨æ¢ç´¢æ–°çš„æ‰©æ•£æ¨¡å‹è®¾è®¡æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>æä¾›äº†ç›¸å…³ä»£ç å’Œå«æ¥æ¨¡å‹çš„åœ¨çº¿èµ„æºï¼ˆ<a target="_blank" rel="noopener" href="https://grafting.stanford.edu)./">https://grafting.stanford.eduï¼‰ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1287da3969631556863ec370d4f2aa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e74ff4b30f53f8ba7836aa86927dd2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d46bc3806e04a5d5868564af91a233b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3940bf603b77c759e8b1293fa88b839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b8d1b02559903edd1458b51dfa1a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb4c6c4074112e116ac2560fed9daca0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SeedEdit-3-0-Fast-and-High-Quality-Generative-Image-Editing"><a href="#SeedEdit-3-0-Fast-and-High-Quality-Generative-Image-Editing" class="headerlink" title="SeedEdit 3.0: Fast and High-Quality Generative Image Editing"></a>SeedEdit 3.0: Fast and High-Quality Generative Image Editing</h2><p><strong>Authors:Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang</strong></p>
<p>We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0 [22], which significantly improves over our previous version [27] in both aspects of edit instruction following and image content (e.g., ID&#x2F;IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and a reward loss. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%). </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SeedEdit 3.0ï¼Œä»¥åŠä¸ä¹‹é…å¥—çš„æ–‡ç”Ÿå›¾æ¨¡å‹Seedream 3.0 [22]ã€‚ç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬[27]ï¼ŒSeedEdit 3.0åœ¨éµå¾ªç¼–è¾‘æŒ‡ä»¤å’Œä¿ç•™å›¾åƒå†…å®¹ï¼ˆä¾‹å¦‚ID&#x2F;IPï¼‰æ–¹é¢å¯¹çœŸå®å›¾åƒè¾“å…¥æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚é™¤äº†æ¨¡å‹çš„å‡çº§ï¼Œæœ¬æŠ¥å‘Šè¿˜å±•ç¤ºäº†å‡ ä¸ªå…³é”®æ”¹è¿›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¢å¼ºå‹æ•°æ®æ•´ç†ç®¡é“ï¼Œé‡‡ç”¨å…ƒä¿¡æ¯èŒƒå¼å’Œå…ƒä¿¡æ¯åµŒå…¥ç­–ç•¥ï¼Œæœ‰åŠ©äºæ··åˆæ¥è‡ªå¤šä¸ªæ•°æ®æºçš„å›¾ç‰‡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰©å±•ç¼–è¾‘æ•°æ®ï¼Œå…ƒä¿¡æ¯æœ‰åŠ©äºå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹æ›´ç´§å¯†åœ°è”ç³»èµ·æ¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè”åˆå­¦ä¹ ç®¡é“ï¼Œç”¨äºè®¡ç®—æ‰©æ•£æŸå¤±å’Œå¥–åŠ±æŸå¤±ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•åŸºå‡†ä¸Šè¯„ä¼°äº†SeedEdit 3.0åœ¨çœŸå®å›¾åƒç¼–è¾‘æ–¹é¢çš„è¡¨ç°ï¼Œå®ƒåœ¨å¤šä¸ªæ–¹é¢è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œä½¿ç”¨ç‡è¾¾åˆ°56.1%ï¼Œç›¸è¾ƒäºSeedEdit 1.6ï¼ˆ38.4%ï¼‰ã€GPT4oï¼ˆ37.1%ï¼‰å’ŒGemini 2.0ï¼ˆ30.3%ï¼‰æœ‰ç€æ˜¾è‘—æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05083v1">PDF</a> Our website: <a target="_blank" rel="noopener" href="https://seed.bytedance.com/tech/seededit">https://seed.bytedance.com/tech/seededit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SeedEdit 3.0ä¸å…¶é…å¥—çš„T2Iæ¨¡å‹Seedream 3.0çš„å‡çº§ç‰ˆæœ¬ã€‚æ–°ç‰ˆæœ¬åœ¨éµå¾ªç¼–è¾‘æŒ‡ä»¤å’Œä¿ç•™å›¾åƒå†…å®¹ï¼ˆå¦‚ID&#x2F;IPï¼‰æ–¹é¢æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼šé‡‡ç”¨å…ƒä¿¡æ¯èŒƒå¼å’Œå…ƒä¿¡æ¯åµŒå…¥ç­–ç•¥å¢å¼ºæ•°æ®æ•´ç†æµç¨‹ï¼Œä»¥æœ‰æ•ˆæ•´åˆæ¥è‡ªå¤šä¸ªæ•°æ®æºçš„å›¾ç‰‡ï¼›å¼•å…¥è”åˆå­¦ä¹ ç®¡é“è®¡ç®—æ‰©æ•£æŸå¤±å’Œå¥–åŠ±æŸå¤±ï¼›æœ€ååœ¨æµ‹è¯•åŸºå‡†ä¸Šè¯„ä¼°SeedEdit 3.0å¯¹çœŸå®å›¾åƒç¼–è¾‘çš„æ•ˆæœï¼Œå…¶åœ¨å¤šæ–¹é¢è¾¾åˆ°æœ€ä½³å¹³è¡¡ï¼Œä½¿ç”¨ç‡è¾¾åˆ°56.1%ï¼Œç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬å’Œå…¶ä»–æ¨¡å‹æœ‰æ˜æ˜¾æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SeedEdit 3.0ä¸å…¶é…å¥—çš„T2Iæ¨¡å‹Seedream 3.0æ¨å‡ºå‡çº§ç‰ˆæœ¬ï¼Œæ˜¾è‘—æ”¹è¿›äº†ç¼–è¾‘æŒ‡ä»¤éµå¾ªå’Œå›¾åƒå†…å®¹ä¿ç•™çš„æ–¹é¢ã€‚</li>
<li>é‡‡ç”¨å…ƒä¿¡æ¯èŒƒå¼å’ŒåµŒå…¥ç­–ç•¥æ”¹è¿›æ•°æ®æ•´ç†æµç¨‹ï¼Œèƒ½æœ‰æ•ˆæ•´åˆä¸åŒæ¥æºçš„å›¾åƒæ•°æ®ã€‚</li>
<li>å¼•å…¥è”åˆå­¦ä¹ ç®¡é“è®¡ç®—æ‰©æ•£æŸå¤±å’Œå¥–åŠ±æŸå¤±ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SeedEdit 3.0åœ¨çœŸå®å›¾åƒç¼–è¾‘çš„æµ‹è¯•åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†å¤šæ–¹é¢çš„æœ€ä½³å¹³è¡¡ã€‚</li>
<li>æ–°ç‰ˆæœ¬çš„ä½¿ç”¨ç‡è¾¾åˆ°äº†56.1%ï¼Œç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬å’Œå…¶ä»–æ¨¡å‹ï¼Œå¦‚SeedEdit 1.6ã€GPT4oå’ŒGemini 2.0ï¼Œæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>å…ƒä¿¡æ¯å¯¹äºè¿æ¥VLMå’Œæ‰©æ•£æ¨¡å‹æ›´åŠ ç´§å¯†æœ‰å¸®åŠ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b07b4027ffc3ee280a3b21e0c6311fea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fb4600dc35b860b01b8ad838e2f0ae0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2934c8258ac1cc2a447e3edeff776025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ef819710f5a2be0462d56c5b563e6cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1943abbace22e3888206647fb27b7fe.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Invisible-Backdoor-Triggers-in-Image-Editing-Model-via-Deep-Watermarking"><a href="#Invisible-Backdoor-Triggers-in-Image-Editing-Model-via-Deep-Watermarking" class="headerlink" title="Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking"></a>Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking</h2><p><strong>Authors:Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen</strong></p>
<p>Diffusion models have achieved remarkable progress in both image generation and editing. However, recent studies have revealed their vulnerability to backdoor attacks, in which specific patterns embedded in the input can manipulate the modelâ€™s behavior. Most existing research in this area has proposed attack frameworks focused on the image generation pipeline, leaving backdoor attacks in image editing relatively unexplored. Among the few studies targeting image editing, most utilize visible triggers, which are impractical because they introduce noticeable alterations to the input image before editing. In this paper, we propose a novel attack framework that embeds invisible triggers into the image editing process via poisoned training data. We leverage off-the-shelf deep watermarking models to encode imperceptible watermarks as backdoor triggers. Our goal is to make the model produce the predefined backdoor target when it receives watermarked inputs, while editing clean images normally according to the given prompt. With extensive experiments across different watermarking models, the proposed method achieves promising attack success rates. In addition, the analysis results of the watermark characteristics in term of backdoor attack further support the effectiveness of our approach. The code is available at:<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/BackdoorImageEditing">https://github.com/aiiu-lab/BackdoorImageEditing</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†å®ƒä»¬å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„è„†å¼±æ€§ï¼Œå…¶ä¸­è¾“å…¥ä¸­åµŒå…¥çš„ç‰¹å®šæ¨¡å¼å¯ä»¥æ“çºµæ¨¡å‹çš„è¡Œä¸ºã€‚å°½ç®¡ç›®å‰å·²æœ‰è®¸å¤šé’ˆå¯¹å›¾åƒç”Ÿæˆç®¡é“çš„æ”»å‡»æ¡†æ¶ç ”ç©¶ï¼Œä½†é’ˆå¯¹å›¾åƒç¼–è¾‘ä¸­çš„åé—¨æ”»å‡»ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚åœ¨å°‘æ•°é’ˆå¯¹å›¾åƒç¼–è¾‘çš„ç ”ç©¶ä¸­ï¼Œå¤§å¤šæ•°ä½¿ç”¨å¯è§è§¦å‘å™¨ï¼Œå› ä¸ºå®ƒä»¬ä¼šåœ¨ç¼–è¾‘ä¹‹å‰åœ¨è¾“å…¥å›¾åƒä¸­å¼•å…¥æ˜æ˜¾çš„æ”¹åŠ¨ï¼Œå› æ­¤ä¸å¤ªå®ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡ä¸­æ¯’è®­ç»ƒæ•°æ®åœ¨å›¾åƒç¼–è¾‘è¿‡ç¨‹ä¸­åµŒå…¥ä¸å¯è§è§¦å‘å™¨ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æˆçš„æ·±åº¦æ°´å°æ¨¡å‹æ¥ç¼–ç ä¸å¯å¯Ÿè§‰çš„æ°´å°ä½œä¸ºåé—¨è§¦å‘å™¨ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹åœ¨æ¥æ”¶åˆ°æ°´å°è¾“å…¥æ—¶äº§ç”Ÿé¢„è®¾çš„åé—¨ç›®æ ‡ï¼ŒåŒæ—¶æ ¹æ®ç»™å®šçš„æç¤ºæ­£å¸¸ç¼–è¾‘å¹²å‡€å›¾åƒã€‚åœ¨ä¸åŒæ°´å°æ¨¡å‹çš„å¹¿æ³›å®éªŒä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å–å¾—äº†æœ‰å‰æ™¯çš„æ”»å‡»æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œå¯¹æ°´å°ç‰¹å¾åœ¨åé—¨æ”»å‡»æ–¹é¢çš„åˆ†æç»“æœè¿›ä¸€æ­¥æ”¯æŒäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/aiiu-lab/BackdoorImageEditing">https://github.com/aiiu-lab/BackdoorImageEditing</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04879v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æœ€è¿‘çš„ç ”ç©¶å‘ç°å®ƒä»¬å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œè¾“å…¥ä¸­çš„ç‰¹å®šæ¨¡å¼å¯ä»¥æ“çºµæ¨¡å‹çš„è¡Œä¸ºã€‚è™½ç„¶å·²æœ‰ç ”ç©¶å…³æ³¨å›¾åƒç”Ÿæˆç®¡é“çš„æ”»å‡»æ¡†æ¶ï¼Œä½†é’ˆå¯¹å›¾åƒç¼–è¾‘çš„åé—¨æ”»å‡»ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡ä¸­æ¯’è®­ç»ƒæ•°æ®åœ¨å›¾åƒç¼–è¾‘è¿‡ç¨‹ä¸­åµŒå…¥ä¸å¯è§è§¦å‘å› ç´ ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æˆçš„æ·±åº¦æ°´å°æ¨¡å‹ç¼–ç ä¸å¯å¯Ÿè§‰çš„æ°´å°ä½œä¸ºåé—¨è§¦å‘å› ç´ ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹åœ¨æ¥æ”¶å¸¦æ°´å°çš„è¾“å…¥æ—¶äº§ç”Ÿé¢„è®¾çš„åé—¨ç›®æ ‡ï¼ŒåŒæ—¶æ ¹æ®ç»™å®šçš„æç¤ºæ­£å¸¸ç¼–è¾‘å¹²å‡€å›¾åƒã€‚é€šè¿‡åœ¨ä¸åŒæ°´å°æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒï¼Œè¯¥æ–¹æ³•å–å¾—äº†æœ‰å¸Œæœ›çš„æ”»å‡»æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘é¢†åŸŸè¡¨ç°å‡ºæ˜¾è‘—çš„è¿›æ­¥ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶å‘ç°æ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œå…¶ä¸­è¾“å…¥ä¸­çš„ç‰¹å®šæ¨¡å¼å¯ä»¥æ“çºµæ¨¡å‹è¡Œä¸ºã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å›¾åƒç”Ÿæˆç®¡é“çš„æ”»å‡»æ¡†æ¶ä¸Šï¼Œè€Œé’ˆå¯¹å›¾åƒç¼–è¾‘çš„åé—¨æ”»å‡»ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡ä¸­æ¯’è®­ç»ƒæ•°æ®åµŒå…¥ä¸å¯è§è§¦å‘å› ç´ åˆ°å›¾åƒç¼–è¾‘è¿‡ç¨‹ä¸­ã€‚</li>
<li>åˆ©ç”¨ç°æˆçš„æ·±åº¦æ°´å°æ¨¡å‹ç¼–ç ä¸å¯å¯Ÿè§‰çš„æ°´å°ä½œä¸ºåé—¨è§¦å‘å› ç´ ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ç›®æ ‡æ˜¯åœ¨æ¥æ”¶å¸¦æ°´å°çš„è¾“å…¥æ—¶ä½¿æ¨¡å‹äº§ç”Ÿé¢„è®¾çš„åé—¨ç›®æ ‡ï¼ŒåŒæ—¶æ­£å¸¸ç¼–è¾‘å¹²å‡€å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-772546f5a1100cf08fe55b1ceb0be37e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b42369589e8608868da96dd37c60219.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2532968f555e635ef40487b91bf0c8cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d18778481eaeb0cca91ca527e8444549.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd1c0500234e4753d573afe72e59e031.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bf989c13ad9f761a7179ff5a064eacb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoders-Again"><a href="#Sparse-Autoencoders-Again" class="headerlink" title="Sparse Autoencoders, Again?"></a>Sparse Autoencoders, Again?</h2><p><strong>Authors:Yin Lu, Tong He, Xuening Zhu, David Wipf</strong></p>
<p>Is there really much more to say about sparse autoencoders (SAEs)? Autoencoders in general, and SAEs in particular, represent deep architectures that are capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite the wide-ranging applicability, there have been relatively few changes to SAEs beyond the original recipe from decades ago, namely, standard deep encoder&#x2F;decoder layers trained with a classical&#x2F;deterministic sparse regularizer applied within the latent space. One possible exception is the variational autoencoder (VAE), which adopts a stochastic encoder module capable of producing sparse representations when applied to manifold data. In this work we formalize underappreciated weaknesses with both canonical SAEs, as well as analogous VAEs applied to similar tasks, and propose a hybrid alternative model that circumvents these prior limitations. In terms of theoretical support, we prove that global minima of our proposed model recover certain forms of structured data spread across a union of manifolds. Meanwhile, empirical evaluations on synthetic and real-world datasets substantiate the efficacy of our approach in accurately estimating underlying manifold dimensions and producing sparser latent representations without compromising reconstruction error. In general, we are able to exceed the performance of equivalent-capacity SAEs and VAEs, as well as recent diffusion models where applicable, within domains such as images and language model activation patterns. </p>
<blockquote>
<p>å…³äºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰è¿˜æœ‰ä»€ä¹ˆæ›´å¤šå¯è¯´çš„å—ï¼Ÿæ€»çš„æ¥è¯´ï¼Œè‡ªç¼–ç å™¨ï¼Œå°¤å…¶æ˜¯SAEsï¼Œä»£è¡¨äº†èƒ½å¤Ÿå»ºæ¨¡æ•°æ®ä¸­çš„ä½ç»´æ½œåœ¨ç»“æ„çš„æ·±åº¦æ¶æ„ã€‚è¿™æ ·çš„ç»“æ„é™¤äº†åæ˜ å…¶ä»–äº‹ç‰©ä¹‹å¤–ï¼Œè¿˜å¯èƒ½åæ˜ å¤§å‹è¯­è¨€æ¨¡å‹æ¿€æ´»ä¸­çš„å…³è”æ¨¡å¼æˆ–å¤æ‚çš„è‡ªç„¶å›¾åƒæµå½¢ã€‚å°½ç®¡SAEså…·æœ‰å¹¿æ³›çš„åº”ç”¨èŒƒå›´ï¼Œä½†é™¤äº†å‡ åå¹´å‰çš„åŸå§‹é…æ–¹ä¹‹å¤–ï¼Œç›¸å¯¹è¾ƒå°‘çš„æ”¹å˜è¢«åº”ç”¨äºSAEsã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æ½œåœ¨ç©ºé—´å†…ä½¿ç”¨ç»å…¸&#x2F;ç¡®å®šæ€§ç¨€ç–æ­£åˆ™åŒ–è®­ç»ƒçš„æ ‡å‡†æ·±åº¦ç¼–ç å™¨&#x2F;è§£ç å™¨å±‚ã€‚ä¸€ä¸ªå¯èƒ½çš„ä¾‹å¤–æ˜¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§éšæœºç¼–ç å™¨æ¨¡å—ï¼Œå½“åº”ç”¨äºæµå½¢æ•°æ®æ—¶èƒ½å¤Ÿäº§ç”Ÿç¨€ç–è¡¨ç¤ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­£å¼æå‡ºäº†è¢«å¿½è§†çš„å¼±ç‚¹ï¼Œæ— è®ºæ˜¯æ ‡å‡†çš„SAEsè¿˜æ˜¯ç±»ä¼¼ä»»åŠ¡çš„VAEsï¼Œå¹¶æå‡ºäº†ä¸€ç§æ··åˆçš„æ›¿ä»£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè§„é¿è¿™äº›å…ˆå‰çš„é™åˆ¶ã€‚åœ¨ç†è®ºæ”¯æŒæ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºæ¨¡å‹çš„å…¨å±€æœ€å°å€¼èƒ½å¤Ÿæ¢å¤è·¨å¤šä¸ªæµå½¢çš„æŸç§å½¢å¼çš„ç»“æ„åŒ–æ•°æ®ã€‚åŒæ—¶ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¯å®äº†æˆ‘ä»¬æ–¹æ³•åœ¨å‡†ç¡®ä¼°è®¡æ½œåœ¨æµå½¢ç»´åº¦å’Œäº§ç”Ÿç¨€ç–æ½œåœ¨è¡¨ç¤ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šæŸå®³é‡å»ºè¯¯å·®ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å›¾åƒå’Œè¯­è¨€æ¨¡å‹æ¿€æ´»æ¨¡å¼ç­‰é¢†åŸŸè¶…è¿‡äº†åŒç­‰å®¹é‡çš„SAEså’ŒVAEsçš„æ€§èƒ½ï¼Œä»¥åŠåœ¨é€‚ç”¨çš„æœ€æ–°æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04859v1">PDF</a> Accepted to the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰èƒ½å¤Ÿå»ºæ¨¡æ•°æ®ä¸­çš„ä½ç»´æ½œåœ¨ç»“æ„ï¼Œå¯åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹æ¿€æ´»ä¸­çš„ç›¸å…³æ€§æ¨¡å¼æˆ–å¤æ‚çš„è‡ªç„¶å›¾åƒæµå½¢ç­‰é¢†åŸŸã€‚å°½ç®¡å…¶åº”ç”¨å¹¿æ³›ï¼Œä½†SAEçš„æ”¹è¿›ç›¸å¯¹è¾ƒå°‘ã€‚æœ¬æ–‡å½¢å¼åŒ–äº†ç»å…¸SAEå’Œç±»ä¼¼ä»»åŠ¡çš„VAEæ‰€å¿½è§†çš„å¼±ç‚¹ï¼Œå¹¶æå‡ºä¸€ç§è§„é¿è¿™äº›å…ˆå‰å±€é™çš„æ··åˆæ›¿ä»£æ¨¡å‹ã€‚åœ¨ç†è®ºæ”¯æŒæ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹çš„å…¨å±€æœ€å°å€¼èƒ½å¤Ÿæ¢å¤è·¨è”åˆæµå½¢åˆ†å¸ƒçš„ç»“æ„åŒ–æ•°æ®å½¢å¼ã€‚åŒæ—¶ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¯å®äº†æˆ‘ä»¬æ–¹æ³•åœ¨å‡†ç¡®ä¼°è®¡æ½œåœ¨æµå½¢ç»´åº¦å’Œäº§ç”Ÿç¨€ç–æ½œåœ¨è¡¨ç¤ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸”ä¸ä¼šå¢åŠ é‡å»ºè¯¯å·®ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨å›¾åƒå’Œè¯­è¨€æ¨¡å‹æ¿€æ´»æ¨¡å¼ç­‰é¢†åŸŸè¶…è¿‡äº†åŒç­‰å®¹é‡çš„SAEå’ŒVAEä»¥åŠæœ€è¿‘çš„æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰èƒ½å¤Ÿå»ºæ¨¡æ•°æ®çš„ä½ç»´æ½œåœ¨ç»“æ„ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨é¢†åŸŸã€‚</li>
<li>å°½ç®¡SAEæœ‰æ‰€å‘å±•ï¼Œä½†ç›¸å¯¹ç¼ºä¹é‡å¤§æ”¹è¿›ï¼Œé™¤äº†å‡ åå¹´å‰çš„ç»å…¸ç¨€ç–æ­£åˆ™åŒ–æ–¹æ³•å¤–ï¼Œå¾ˆå°‘æœ‰æ˜¾è‘—å˜åŒ–ã€‚</li>
<li>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰é‡‡ç”¨éšæœºç¼–ç å™¨æ¨¡å—ï¼Œå¯ä»¥åº”ç”¨äºæµå½¢æ•°æ®äº§ç”Ÿç¨€ç–è¡¨ç¤ºï¼Œæ˜¯SAEçš„ä¸€ä¸ªå¯èƒ½çš„ä¾‹å¤–ã€‚</li>
<li>æœ¬æ–‡æ­ç¤ºäº†ç»å…¸SAEå’ŒVAEåœ¨å¤„ç†ç±»ä¼¼ä»»åŠ¡æ—¶çš„å¼±ç‚¹ï¼Œå¹¶æå‡ºä¸€ç§æ··åˆæ¨¡å‹æ¥è§„é¿è¿™äº›å±€é™ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ç†è®ºä¸Šæœ‰æ”¯æŒï¼Œèƒ½å¤Ÿæ¢å¤è·¨è”åˆæµå½¢çš„ç»“æ„åŒ–æ•°æ®å½¢å¼ã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºï¼Œæ–°æ¨¡å‹åœ¨ä¼°è®¡æ½œåœ¨æµå½¢ç»´åº¦å’Œäº§ç”Ÿç¨€ç–è¡¨ç¤ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶ä¸å¢åŠ é‡å»ºè¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ce489e7226cbf492910cd6d63918974d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2839c97697e1774bcba9aec0b0918924.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f04b18284bd0a7cfb8ce8dbf7d4b71ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04011231d485016c3e5bbd3ef9a608f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SmartAvatar-Text-and-Image-Guided-Human-Avatar-Generation-with-VLM-AI-Agents"><a href="#SmartAvatar-Text-and-Image-Guided-Human-Avatar-Generation-with-VLM-AI-Agents" class="headerlink" title="SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI   Agents"></a>SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI   Agents</h2><p><strong>Authors:Alexander Huang-Menders, Xinhang Liu, Andy Xu, Yuyao Zhang, Chi-Keung Tang, Yu-Wing Tai</strong></p>
<p>SmartAvatar is a vision-language-agent-driven framework for generating fully rigged, animation-ready 3D human avatars from a single photo or textual prompt. While diffusion-based methods have made progress in general 3D object generation, they continue to struggle with precise control over human identity, body shape, and animation readiness. In contrast, SmartAvatar leverages the commonsense reasoning capabilities of large vision-language models (VLMs) in combination with off-the-shelf parametric human generators to deliver high-quality, customizable avatars. A key innovation is an autonomous verification loop, where the agent renders draft avatars, evaluates facial similarity, anatomical plausibility, and prompt alignment, and iteratively adjusts generation parameters for convergence. This interactive, AI-guided refinement process promotes fine-grained control over both facial and body features, enabling users to iteratively refine their avatars via natural-language conversations. Unlike diffusion models that rely on static pre-trained datasets and offer limited flexibility, SmartAvatar brings users into the modeling loop and ensures continuous improvement through an LLM-driven procedural generation and verification system. The generated avatars are fully rigged and support pose manipulation with consistent identity and appearance, making them suitable for downstream animation and interactive applications. Quantitative benchmarks and user studies demonstrate that SmartAvatar outperforms recent text- and image-driven avatar generation systems in terms of reconstructed mesh quality, identity fidelity, attribute accuracy, and animation readiness, making it a versatile tool for realistic, customizable avatar creation on consumer-grade hardware. </p>
<blockquote>
<p>SmartAvataræ˜¯ä¸€ä¸ªä»¥è§†è§‰è¯­è¨€é©±åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒèƒ½ä»å•ä¸€ç…§ç‰‡æˆ–æ–‡æœ¬æç¤ºç”Ÿæˆå®Œæ•´é…ç½®çš„ä¸‰ç»´åŠ¨æ€äººå½¢è§’è‰²ã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨ä¸€èˆ¬ä¸‰ç»´å¯¹è±¡ç”Ÿæˆæ–¹é¢å–å¾—äº†ä¸€å®šçš„è¿›å±•ï¼Œä½†åœ¨å¯¹äººç‰©èº«ä»½ã€èº«ä½“å½¢æ€å’ŒåŠ¨ç”»å‡†å¤‡æ€§çš„ç²¾ç¡®æ§åˆ¶æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSmartAvataråˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ï¼Œç»“åˆç°æˆçš„å‚æ•°åŒ–äººç‰©ç”Ÿæˆå™¨ï¼Œæä¾›é«˜è´¨é‡çš„å¯å®šåˆ¶ä¸ªæ€§åŒ–è§’è‰²ã€‚ä¸€ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹æ˜¯ä¸€ä¸ªè‡ªä¸»çš„éªŒè¯å¾ªç¯ï¼Œå…¶ä¸­ä»£ç†æ¸²æŸ“è§’è‰²è‰ç¨¿ï¼Œè¯„ä¼°é¢éƒ¨ç›¸ä¼¼æ€§ã€è§£å‰–åˆç†æ€§å’Œæç¤ºå¯¹é½åº¦ï¼Œå¹¶è¿­ä»£è°ƒæ•´ç”Ÿæˆå‚æ•°ä»¥è¾¾åˆ°æ”¶æ•›ã€‚è¿™ç§äº¤äº’çš„AIå¼•å¯¼ç»†åŒ–è¿‡ç¨‹ä¿ƒè¿›äº†å¯¹é¢éƒ¨ç‰¹å¾çš„ç²¾ç»†æ§åˆ¶ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€å¯¹è¯é€æ­¥ç»†åŒ–ä»–ä»¬çš„ä¸ªæ€§åŒ–è§’è‰²ã€‚ä¸åŒäºä¾èµ–é™æ€é¢„è®­ç»ƒæ•°æ®é›†ä¸”çµæ´»æ€§æœ‰é™çš„æ‰©æ•£æ¨¡å‹ï¼ŒSmartAvatarå°†ç”¨æˆ·å¼•å…¥å»ºæ¨¡å¾ªç¯ï¼Œå¹¶é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æµç¨‹ç”Ÿæˆå’ŒéªŒè¯ç³»ç»Ÿç¡®ä¿æŒç»­æ”¹è¿›ã€‚ç”Ÿæˆçš„ä¸ªæ€§åŒ–è§’è‰²å®Œå…¨é…å¤‡éª¨éª¼ç³»ç»Ÿå¹¶æ”¯æŒå§¿æ€æ“çºµï¼ŒåŒæ—¶ä¿æŒä¸€è‡´çš„å¤–è§‚èº«ä»½ç‰¹å¾ï¼Œä½¿å…¶é€‚ç”¨äºä¸‹æ¸¸åŠ¨ç”»å’Œäº¤äº’å¼åº”ç”¨ç¨‹åºã€‚å®šé‡åŸºå‡†æµ‹è¯•å’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œåœ¨é‡å»ºç½‘æ ¼è´¨é‡ã€èº«ä»½ä¿çœŸåº¦ã€å±æ€§å‡†ç¡®æ€§å’ŒåŠ¨ç”»å‡†å¤‡æ€§æ–¹é¢ï¼ŒSmartAvatarä¼˜äºæœ€æ–°çš„æ–‡æœ¬å’Œå›¾åƒé©±åŠ¨çš„ä¸ªæ€§åŒ–è§’è‰²ç”Ÿæˆç³»ç»Ÿï¼Œæˆä¸ºé¢å‘å¤§ä¼—ç¡¬ä»¶çš„é€šç”¨å·¥å…·ï¼Œç”¨äºåˆ›å»ºé€¼çœŸçš„å¯å®šåˆ¶ä¸ªæ€§åŒ–è§’è‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04606v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>SmartAvataræ˜¯ä¸€ä¸ªåŸºäºè§†è§‰è¯­è¨€ä»£ç†é©±åŠ¨çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ ç…§ç‰‡æˆ–æ–‡æœ¬æç¤ºç”Ÿæˆå®Œæ•´çš„ã€é€‚åˆåŠ¨ç”»çš„3Däººä½“å¤´åƒã€‚å®ƒç»“åˆå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ä¸ç°æˆçš„å‚æ•°åŒ–äººä½“ç”Ÿæˆå™¨ï¼Œå®ç°äº†é«˜è´¨é‡ã€å¯å®šåˆ¶åŒ–çš„å¤´åƒã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºè‡ªä¸»éªŒè¯å¾ªç¯ï¼Œä»£ç†å¯ä»¥æ¸²æŸ“è‰ç¨¿å¤´åƒï¼Œè¯„ä¼°é¢éƒ¨ç›¸ä¼¼æ€§ã€è§£å‰–åˆç†æ€§å’Œæç¤ºå¯¹é½æ€§ï¼Œå¹¶è¿­ä»£è°ƒæ•´ç”Ÿæˆå‚æ•°ä»¥è¾¾åˆ°æ”¶æ•›ã€‚è¿™ç§äº¤äº’å¼çš„AIå¼•å¯¼ç»†åŒ–è¿‡ç¨‹ä½¿ç”¨æˆ·èƒ½å¤Ÿç²¾ç»†æ§åˆ¶é¢éƒ¨å’Œèº«ä½“ç‰¹å¾ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å¯¹è¯é€æ­¥ä¼˜åŒ–å¤´åƒã€‚ç›¸è¾ƒäºä¾èµ–é™æ€é¢„è®­ç»ƒæ•°æ®é›†ã€çµæ´»æ€§æœ‰é™çš„æ‰©æ•£æ¨¡å‹ï¼ŒSmartAvatarå°†ç”¨æˆ·çº³å…¥å»ºæ¨¡å¾ªç¯ï¼Œå¹¶é€šè¿‡LLMé©±åŠ¨çš„ç¨‹åºç”Ÿæˆå’ŒéªŒè¯ç³»ç»Ÿç¡®ä¿æŒç»­æ”¹è¿›ã€‚ç”Ÿæˆçš„å¤´åƒå…·æœ‰å®Œå…¨çš„éª¨æ¶åŠ¨ç”»æ”¯æŒï¼Œå¯è¿›è¡Œå§¿åŠ¿æ“ä½œï¼Œä¸”èº«ä»½å’Œå¤–è§‚ä¸€è‡´ï¼Œé€‚ç”¨äºä¸‹æ¸¸åŠ¨ç”»å’Œäº¤äº’åº”ç”¨ã€‚SmartAvataråœ¨é‡å»ºç½‘æ ¼è´¨é‡ã€èº«ä»½ä¿çœŸåº¦ã€å±æ€§å‡†ç¡®æ€§å’ŒåŠ¨ç”»å‡†å¤‡æ–¹é¢è¡¨ç°å‡ºè¶…è¶Šè¿‘æœŸæ–‡æœ¬å’Œå›¾åƒé©±åŠ¨å¤´åƒç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½ï¼Œæˆä¸ºåœ¨æ¶ˆè´¹è€…çº§ç¡¬ä»¶ä¸Šåˆ›å»ºçœŸå®å¯å®šåˆ¶çš„å¤´åƒçš„é€šç”¨å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmartAvataræ˜¯ä¸€ä¸ªä»å•ä¸€ç…§ç‰‡æˆ–æ–‡æœ¬æç¤ºç”Ÿæˆ3Däººä½“å¤´åƒçš„æ¡†æ¶ã€‚</li>
<li>å®ƒåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸å‚æ•°åŒ–äººä½“ç”Ÿæˆå™¨ç»“åˆï¼Œå®ç°é«˜è´¨é‡ã€å¯å®šåˆ¶çš„å¤´åƒç”Ÿæˆã€‚</li>
<li>SmartAvatarå…·å¤‡è‡ªä¸»éªŒè¯å¾ªç¯ï¼Œå¯è¿­ä»£ä¼˜åŒ–ç”Ÿæˆçš„å¤´åƒã€‚</li>
<li>è¯¥æ¡†æ¶å…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€å¯¹è¯ç²¾ç»†æ§åˆ¶å¹¶ä¼˜åŒ–å¤´åƒçš„é¢éƒ¨å’Œèº«ä½“ç‰¹å¾ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ï¼ŒSmartAvataræ›´å…·çµæ´»æ€§ï¼Œå¹¶å°†ç”¨æˆ·çº³å…¥å»ºæ¨¡å¾ªç¯ä¸­ã€‚</li>
<li>SmartAvatarç”Ÿæˆçš„å¤´åƒå…·æœ‰å®Œå…¨çš„éª¨æ¶åŠ¨ç”»æ”¯æŒï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸åº”ç”¨å’ŒåŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e20bc90edb684c380c4b047dedb8f66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb19d30c3e36234caa3be1acbdbcf30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00b06c789a384d477fec83b08e72360f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HMAR-Efficient-Hierarchical-Masked-Auto-Regressive-Image-Generation"><a href="#HMAR-Efficient-Hierarchical-Masked-Auto-Regressive-Image-Generation" class="headerlink" title="HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation"></a>HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation</h2><p><strong>Authors:Hermann Kumbong, Xian Liu, Tsung-Yi Lin, Ming-Yu Liu, Xihui Liu, Ziwei Liu, Daniel Y. Fu, Christopher RÃ©, David W. Romero</strong></p>
<p>Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed and quality gap between autoregressive image models and diffusion models. VAR reformulates autoregressive modeling by decomposing an image into successive resolution scales. During inference, an image is generated by predicting all the tokens in the next (higher-resolution) scale, conditioned on all tokens in all previous (lower-resolution) scales. However, this formulation suffers from reduced image quality due to the parallel generation of all tokens in a resolution scale; has sequence lengths scaling superlinearly in image resolution; and requires retraining to change the sampling schedule.   We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image generation algorithm that alleviates these issues using next-scale prediction and masked prediction to generate high-quality images with fast sampling. HMAR reformulates next-scale prediction as a Markovian process, wherein the prediction of each resolution scale is conditioned only on tokens in its immediate predecessor instead of the tokens in all predecessor resolutions. When predicting a resolution scale, HMAR uses a controllable multi-step masked generation procedure to generate a subset of the tokens in each step. On ImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform parameter-matched VAR, diffusion, and autoregressive baselines. We develop efficient IO-aware block-sparse attention kernels that allow HMAR to achieve faster training and inference times over VAR by over 2.5x and 1.75x respectively, as well as over 3x lower inference memory footprint. Finally, HMAR yields additional flexibility over VAR; its sampling schedule can be changed without further training, and it can be applied to image editing tasks in a zero-shot manner. </p>
<blockquote>
<p>è§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼ˆVARï¼‰åœ¨å¼¥è¡¥è‡ªå›å½’å›¾åƒæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„é€Ÿåº¦å’Œè´¨é‡çš„å·®è·æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚VARé€šè¿‡å°†ä»å›¾åƒåˆ†è§£æˆè¿ç»­çš„åˆ†è¾¨ç‡å°ºåº¦æ¥é‡æ–°åˆ¶å®šè‡ªå›å½’å»ºæ¨¡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡åœ¨æ‰€æœ‰å…ˆå‰çš„è¾ƒä½åˆ†è¾¨ç‡å°ºåº¦ä¸Šçš„æ‰€æœ‰æ ‡è®°è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªï¼ˆè¾ƒé«˜åˆ†è¾¨ç‡ï¼‰å°ºåº¦çš„æ‰€æœ‰æ ‡è®°çš„å›¾åƒã€‚ç„¶è€Œï¼Œç”±äºåœ¨ä¸€ä¸ªåˆ†è¾¨ç‡å°ºåº¦ä¸Šå¹¶è¡Œç”Ÿæˆæ‰€æœ‰æ ‡è®°ï¼Œè¿™ç§è¡¨è¿°å½¢å¼å­˜åœ¨å›¾åƒè´¨é‡ä¸‹é™çš„é—®é¢˜ï¼›åºåˆ—é•¿åº¦éšå›¾åƒåˆ†è¾¨ç‡æŒ‰è¶…çº¿æ€§æ‰©å±•ï¼›å¹¶ä¸”éœ€è¦é‡è®­æ¥æ”¹å˜é‡‡æ ·æ—¶é—´è¡¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04421v1">PDF</a> Accepted to CVPR 2025. Project Page:   <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dir/hmar/">https://research.nvidia.com/labs/dir/hmar/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼ˆVARï¼‰åœ¨å¼¥è‡ªå›å½’å›¾åƒæ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„é€Ÿåº¦å’Œè´¨é‡çš„å·®è·æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚HMARæ˜¯ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆç®—æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹å’Œæ©ç é¢„æµ‹æ¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶å®ç°å¿«é€Ÿé‡‡æ ·ï¼Œè§£å†³äº†VARå­˜åœ¨çš„é—®é¢˜ã€‚HMARå°†ä¸‹ä¸€å°ºåº¦é¢„æµ‹é‡æ–°å®šä¹‰ä¸ºé©¬å°”å¯å¤«è¿‡ç¨‹ï¼Œå…¶ä¸­æ¯ä¸ªåˆ†è¾¨ç‡å°ºåº¦çš„é¢„æµ‹ä»…å–å†³äºå…¶ç›´æ¥å‰é©±çš„æ ‡è®°ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å‰é©±åˆ†è¾¨ç‡çš„æ ‡è®°ã€‚åœ¨ç”Ÿæˆåˆ†è¾¨ç‡å°ºåº¦æ—¶ï¼ŒHMARä½¿ç”¨å¯æ§çš„å¤šæ­¥æ©ç ç”Ÿæˆç¨‹åºï¼Œåœ¨æ¯ä¸€æ­¥ä¸­ç”Ÿæˆæ ‡è®°çš„å­é›†ã€‚åœ¨ImageNet 256x256å’Œ512x512åŸºå‡†æµ‹è¯•ä¸­ï¼ŒHMARæ¨¡å‹ä¸å‚æ•°åŒ¹é…çš„VARã€æ‰©æ•£å’Œè‡ªå›å½’åŸºçº¿ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬å¼€å‘äº†é«˜æ•ˆçš„IOæ„ŸçŸ¥å—ç¨€ç–æ³¨æ„åŠ›å†…æ ¸ï¼Œä½¿HMARåœ¨è®­ç»ƒå’Œæ¨ç†æ—¶é—´æ–¹é¢æ¯”VARåˆ†åˆ«å¿«2.5å€å’Œ1.75å€ï¼Œæ¨ç†å†…å­˜å ç”¨å‡å°‘è¶…è¿‡3å€ã€‚æœ€åï¼ŒHMARè¾ƒVARå…·æœ‰é¢å¤–çš„çµæ´»æ€§ï¼›å®ƒçš„é‡‡æ ·è®¡åˆ’å¯ä»¥åœ¨æ— éœ€è¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œæ›´æ”¹ï¼Œå¹¶ä¸”å®ƒå¯ä»¥ä»¥é›¶æ ·æœ¬æ–¹å¼åº”ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>HMARæ˜¯ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆç®—æ³•ï¼Œé€šè¿‡ä¸‹ä¸€å°ºåº¦é¢„æµ‹å’Œæ©ç é¢„æµ‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œè§£å†³äº†VARæ¨¡å‹å¹¶è¡Œç”Ÿæˆæ‰€æœ‰æ ‡è®°å¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>HMARå°†ä¸‹ä¸€å°ºåº¦é¢„æµ‹é‡æ–°å®šä¹‰ä¸ºé©¬å°”å¯å¤«è¿‡ç¨‹ï¼Œä»…ä¾èµ–äºå…¶ç›´æ¥å‰é©±çš„æ ‡è®°è¿›è¡Œé¢„æµ‹ï¼Œæé«˜äº†æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>HMARä½¿ç”¨å¯æ§çš„å¤šæ­¥æ©ç ç”Ÿæˆç¨‹åºï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥ä¸­ç”Ÿæˆæ ‡è®°çš„å­é›†ï¼Œè¿›ä¸€æ­¥æé«˜å›¾åƒç”Ÿæˆçš„çµæ´»æ€§ã€‚</li>
<li>åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­ï¼ŒHMARæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸å‚æ•°åŒ¹é…çš„VARã€æ‰©æ•£å’Œè‡ªå›å½’åŸºçº¿ç›¸åŒ¹é…æˆ–æ›´å¥½ã€‚</li>
<li>HMARé€šè¿‡é«˜æ•ˆçš„IOæ„ŸçŸ¥å—ç¨€ç–æ³¨æ„åŠ›å†…æ ¸å®ç°äº†å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼Œç›¸æ¯”VARæœ‰æ˜¾è‘—çš„æ•ˆç‡æå‡ã€‚</li>
<li>HMARå…·æœ‰é¢å¤–çš„çµæ´»æ€§ï¼Œå…¶é‡‡æ ·è®¡åˆ’å¯ä»¥åœ¨æ— éœ€è¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œæ›´æ”¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5c93abb34ae5c1bb2742423ae2bb256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85f43a8ea8dc7506617b4bc30793abaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec792fffa3a956b7f4f7eabe018c74d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3db3bc1624eea526e9ea762600ef7261.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-525bab02abae354c9acc85171b269b0b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Is-Perturbation-Based-Image-Protection-Disruptive-to-Image-Editing"><a href="#Is-Perturbation-Based-Image-Protection-Disruptive-to-Image-Editing" class="headerlink" title="Is Perturbation-Based Image Protection Disruptive to Image Editing?"></a>Is Perturbation-Based Image Protection Disruptive to Image Editing?</h2><p><strong>Authors:Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati</strong></p>
<p>The remarkable image generation capabilities of state-of-the-art diffusion models, such as Stable Diffusion, can also be misused to spread misinformation and plagiarize copyrighted materials. To mitigate the potential risks associated with image editing, current image protection methods rely on adding imperceptible perturbations to images to obstruct diffusion-based editing. A fully successful protection for an image implies that the output of editing attempts is an undesirable, noisy image which is completely unrelated to the reference image. In our experiments with various perturbation-based image protection methods across multiple domains (natural scene images and artworks) and editing tasks (image-to-image generation and style editing), we discover that such protection does not achieve this goal completely. In most scenarios, diffusion-based editing of protected images generates a desirable output image which adheres precisely to the guidance prompt. Our findings suggest that adding noise to images may paradoxically increase their association with given text prompts during the generation process, leading to unintended consequences such as better resultant edits. Hence, we argue that perturbation-based methods may not provide a sufficient solution for robust image protection against diffusion-based editing. </p>
<blockquote>
<p>å…ˆè¿›æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„å‡ºè‰²å›¾åƒç”Ÿæˆèƒ½åŠ›ä¹Ÿå¯èƒ½è¢«è¯¯ç”¨ï¼Œä»¥ä¼ æ’­è¯¯å¯¼ä¿¡æ¯å’ŒæŠ„è¢­å—ç‰ˆæƒä¿æŠ¤çš„ææ–™ã€‚ä¸ºäº†å‡å°‘ä¸å›¾åƒç¼–è¾‘ç›¸å…³çš„æ½œåœ¨é£é™©ï¼Œå½“å‰çš„å›¾åƒä¿æŠ¤æ–¹æ³•ä¾èµ–äºåœ¨å›¾åƒä¸Šæ·»åŠ ç»†å¾®çš„æ‰°åŠ¨æ¥é˜»ç¢åŸºäºæ‰©æ•£çš„ç¼–è¾‘ã€‚å¯¹äºå›¾åƒæ¥è¯´ï¼Œå®Œå…¨æˆåŠŸçš„ä¿æŠ¤æ„å‘³ç€ç¼–è¾‘å°è¯•çš„è¾“å‡ºæ˜¯ä¸€ä¸ªä¸å—æ¬¢è¿çš„ã€å˜ˆæ‚çš„å›¾åƒï¼Œä¸å‚è€ƒå›¾åƒå®Œå…¨æ— å…³ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¢†åŸŸï¼ˆè‡ªç„¶åœºæ™¯å›¾åƒå’Œè‰ºæœ¯å“ï¼‰å’Œå„ç§ç¼–è¾‘ä»»åŠ¡ï¼ˆå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆå’Œæ ·å¼ç¼–è¾‘ï¼‰ä¸­ï¼Œå¯¹å„ç§åŸºäºæ‰°åŠ¨çš„å›¾åƒä¿æŠ¤æ–¹æ³•è¿›è¡Œäº†å®éªŒï¼Œå‘ç°è¿™ç§ä¿æŠ¤å¹¶æ²¡æœ‰å®Œå…¨å®ç°è¿™ä¸€ç›®æ ‡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯¹å—ä¿æŠ¤å›¾åƒçš„æ‰©æ•£å¼ç¼–è¾‘ä¼šäº§ç”Ÿä¸€ä¸ªä»¤äººæ»¡æ„çš„è¾“å‡ºå›¾åƒï¼Œè¯¥å›¾åƒç²¾ç¡®åœ°éµå¾ªäº†æŒ‡å¯¼æç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‘å›¾åƒæ·»åŠ å™ªå£°å¯èƒ½ä¼šå¢åŠ å…¶ä¸ç»™å®šæ–‡æœ¬æç¤ºçš„å…³è”åº¦ï¼Œå¯¼è‡´æ„æƒ³ä¸åˆ°çš„åæœï¼Œå¦‚æ›´å¥½çš„ç¼–è¾‘ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¤ä¸ºåŸºäºæ‰°åŠ¨çš„æ–¹æ³•å¯èƒ½æ— æ³•ä¸ºæŠµæŠ—æ‰©æ•£å¼ç¼–è¾‘æä¾›è¶³å¤Ÿçš„å›¾åƒä¿æŠ¤è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04394v1">PDF</a> 6 pages, 8 figures, accepted by ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>å…ˆè¿›æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„å›¾åƒç”Ÿæˆèƒ½åŠ›å¼ºå¤§ï¼Œä½†ä¹Ÿå¯èƒ½è¢«ç”¨äºä¼ æ’­è™šå‡ä¿¡æ¯å’ŒæŠ„è¢­ç‰ˆæƒææ–™ã€‚å½“å‰å›¾åƒä¿æŠ¤æ–¹æ³•é€šè¿‡ç»™å›¾åƒæ·»åŠ å‡ ä¹ä¸å¯å¯Ÿè§‰çš„æ‰°åŠ¨æ¥é˜»æ­¢åŸºäºæ‰©æ•£çš„ç¼–è¾‘ï¼Œä½†å®éªŒè¡¨æ˜è¿™ç§æ–¹æ³•å¹¶ä¸å®Œå…¨æœ‰æ•ˆã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯¹å—ä¿æŠ¤å›¾åƒè¿›è¡ŒåŸºäºæ‰©æ•£çš„ç¼–è¾‘ä»ç„¶å¯ä»¥ç”Ÿæˆç¬¦åˆæŒ‡å¯¼æç¤ºçš„ç†æƒ³å›¾åƒã€‚å› æ­¤ï¼ŒåŸºäºæ‰°åŠ¨çš„æ–¹æ³•å¯èƒ½æ— æ³•ä¸ºå¯¹æŠ—åŸºäºæ‰©æ•£çš„ç¼–è¾‘æä¾›è¶³å¤Ÿçš„å›¾åƒä¿æŠ¤è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œä½†å¯èƒ½è¢«è¯¯ç”¨äºä¼ æ’­è™šå‡ä¿¡æ¯å’Œä¾µçŠ¯ç‰ˆæƒã€‚</li>
<li>å½“å‰å›¾åƒä¿æŠ¤æ–¹æ³•é€šè¿‡æ·»åŠ æ‰°åŠ¨æ¥é˜»æ­¢åŸºäºæ‰©æ•£çš„ç¼–è¾‘ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•å¹¶ä¸æ€»èƒ½æœ‰æ•ˆä¿æŠ¤å›¾åƒå…å—åŸºäºæ‰©æ•£çš„ç¼–è¾‘ã€‚</li>
<li>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯¹å—ä¿æŠ¤å›¾åƒè¿›è¡ŒåŸºäºæ‰©æ•£çš„ç¼–è¾‘å¯ä»¥ç”Ÿæˆç¬¦åˆæŒ‡å¯¼æç¤ºçš„ç†æƒ³å›¾åƒã€‚</li>
<li>æ·»åŠ å™ªå£°å¯èƒ½ä¼šå¢å¼ºå›¾åƒä¸ç»™å®šæ–‡æœ¬æç¤ºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…³è”ã€‚</li>
<li>åŸºäºæ‰°åŠ¨çš„æ–¹æ³•å¯èƒ½æ— æ³•ä¸ºå¯¹æŠ—åŸºäºæ‰©æ•£çš„ç¼–è¾‘æä¾›è¶³å¤Ÿçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80ab0ee3498cb3c2e1040fd88dc8413a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7b422ab92ad88db4eba00bf75b270c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f4e05efa69775f77cddc438402eeb16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ffe1f5b22fb8e9f35925aa199d61c59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f64a1de61d8fd632141d77cf64e9fafa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4334572ee6ba7b7ade5781da0c1947c6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HuGeDiff-3D-Human-Generation-via-Diffusion-with-Gaussian-Splatting"><a href="#HuGeDiff-3D-Human-Generation-via-Diffusion-with-Gaussian-Splatting" class="headerlink" title="HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting"></a>HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</h2><p><strong>Authors:Maksym Ivashechkin, Oscar Mendez, Richard Bowden</strong></p>
<p>3D human generation is an important problem with a wide range of applications in computer vision and graphics. Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge. Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance. The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model. We present a weakly supervised pipeline that tries to address these challenges. In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a state-of-the-art image diffusion model. Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture. Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality. We will make the code and dataset available. </p>
<blockquote>
<p>ä¸‰ç»´äººç‰©ç”Ÿæˆæ˜¯ä¸€ä¸ªåœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨çš„é‡è¦é—®é¢˜ã€‚å°½ç®¡æœ€è¿‘åœ¨ç”Ÿæˆäººå·¥æ™ºèƒ½ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰æˆ–æ¸²æŸ“æ–¹æ³•ï¼ˆå¦‚ç¥ç»è¾å°„åœºæˆ–é«˜æ–¯å–·ç»˜ï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†æ ¹æ®æ–‡æœ¬æç¤ºæ§åˆ¶å‡†ç¡®ä¸‰ç»´äººç‰©çš„ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•åœ¨ç»†èŠ‚ã€æ‰‹éƒ¨å’Œé¢éƒ¨çš„å‡†ç¡®æ¸²æŸ“ã€äººç‰©é€¼çœŸåº¦ä»¥åŠå¤–è§‚æ§åˆ¶ç­‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚äººç±»å›¾åƒæ•°æ®ç¼ºä¹å¤šæ ·æ€§ã€é€¼çœŸæ€§ä»¥åŠæ³¨é‡Šä¹Ÿä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™é˜»ç¢äº†åŸºç¡€ä¸‰ç»´äººç‰©æ¨¡å‹çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼±ç›‘ç£çš„ç®¡é“ï¼Œè¯•å›¾è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç”Ÿæˆä¸€ä¸ªå…·æœ‰å¯æ§å±æ€§ï¼ˆå¦‚å¤–è§‚ã€ç§æ—ã€æ€§åˆ«ç­‰ï¼‰çš„å…‰ç…§çœŸå®æ„Ÿäººç‰©å›¾åƒæ•°æ®é›†ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå˜å‹å™¨æ¶æ„çš„æœ‰æ•ˆä»å›¾åƒç‰¹å¾åˆ°ä¸‰ç»´ç‚¹äº‘çš„æ˜ å°„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸€ä¸ªç‚¹äº‘æ‰©æ•£æ¨¡å‹æ¥å®Œæˆé—­ç¯ï¼Œè¯¥æ¨¡å‹ä¸ç”¨äºç”ŸæˆåŸå§‹æ ·æœ¬çš„ç›¸åŒæ–‡æœ¬æç¤ºæ¡ä»¶ç›¸åŒã€‚æˆ‘ä»¬åœ¨ä¸‰ç»´äººç‰©ç”Ÿæˆæ–¹é¢å®ç°äº†ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”çš„æ•°é‡çº§åŠ é€Ÿï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ–‡æœ¬æç¤ºå¯¹é½ã€é€¼çœŸåº¦å’Œæ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬å°†å…¬å¼€æä¾›ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹é’ˆå¯¹3Däººç‰©ç”Ÿæˆçš„æœ€æ–°ç ”ç©¶ï¼Œè¯¥ç ”ç©¶ä½¿ç”¨å¼±ç›‘ç£ç®¡é“è§£å†³äº†ä¸€äº›æŒ‘æˆ˜æ€§é—®é¢˜ã€‚é¦–å…ˆåˆ©ç”¨å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·æœ‰å¯æ§å±æ€§çš„å†™å®äººç‰©å›¾åƒæ•°æ®é›†ã€‚æ¥ç€ï¼Œæå‡ºä¸€ç§é«˜æ•ˆçš„ä»å›¾åƒç‰¹å¾åˆ°3Dç‚¹äº‘çš„æ˜ å°„æ–¹æ³•ã€‚æœ€åï¼Œé€šè¿‡è®­ç»ƒç‚¹äº‘æ‰©æ•£æ¨¡å‹ï¼Œå®ç°å¯¹ä¸ç”ŸæˆåŸå§‹æ ·æœ¬ç›¸åŒçš„æ–‡æœ¬æç¤ºçš„å“åº”ã€‚æ­¤æ–¹æ³•åœ¨æé«˜3Däººç‰©ç”Ÿæˆé€Ÿåº¦ã€æ–‡æœ¬æç¤ºå¯¹é½ã€çœŸå®æ„Ÿå’Œæ¸²æŸ“è´¨é‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åœ¨ç”Ÿæˆå¼AIé¢†åŸŸä¸­çš„ä¸€é¡¹æ–°ç ”ç©¶ï¼Œä¸“æ³¨äºè§£å†³è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­å…³äºç”ŸæˆçœŸå®æ„Ÿä¸‰ç»´äººç‰©æ¨¡å‹çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¼±ç›‘ç£ç®¡é“ï¼Œé¦–å…ˆåˆ©ç”¨å…ˆè¿›çš„å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå…·æœ‰å¯æ§å±æ€§çš„çœŸå®äººç‰©å›¾åƒæ•°æ®é›†ã€‚è¿™äº›å¯æ§å±æ€§åŒ…æ‹¬å¤–è§‚ã€ç§æ—å’Œæ€§åˆ«ç­‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ä»å›¾åƒç‰¹å¾æ˜ å°„åˆ°ä¸‰ç»´ç‚¹äº‘çš„ç®—æ³•ï¼Œé‡‡ç”¨åŸºäºtransformerçš„æ¶æ„å®ç°æ˜ å°„è¿‡ç¨‹ã€‚</li>
<li>è®­ç»ƒäº†ç‚¹äº‘æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆç›¸åº”çš„ä¸‰ç»´äººç‰©æ¨¡å‹ã€‚è¿™ä¸€æ­¥éª¤å®ç°äº†é—­ç¯å¤„ç†æµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d2d2f3ce38bd474e9adeed7f29d2122f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a611cb6fa25bf87446b9cc1111c3e1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e36d40106b72cd457b912f6076715ce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a9b443b7f9edcee44d72ca847d01df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca7222d37d3231c5544b8640800d1fae.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance"><a href="#Attentive-Eraser-Unleashing-Diffusion-Modelâ€™s-Object-Removal-Potential-via-Self-Attention-Redirection-Guidance" class="headerlink" title="Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance"></a>Attentive Eraser: Unleashing Diffusion Modelâ€™s Object Removal Potential   via Self-Attention Redirection Guidance</h2><p><strong>Authors:Benlei Cui, Wenhao Sun, Xue-Mei Dong, Jingqun Tang, Yi Liu</strong></p>
<p>Recently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after remova1l. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pre-trained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æ–°æ™‹è€…å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å¤§æ”¾å¼‚å½©ã€‚ç„¶è€Œï¼Œå½“ç”¨äºå¯¹è±¡å»é™¤ä»»åŠ¡æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚äº§ç”Ÿéšæœºä¼ªå½±å’Œåœ¨å»é™¤ååœ¨å‰æ™¯å¯¹è±¡åŒºåŸŸæ— æ³•ç”¨é€‚å½“çš„å†…å®¹è¿›è¡Œé‡ç»˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è°ƒæ•´çš„Attentive Eraseræ–¹æ³•ï¼Œä½¿é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç¨³å®šæœ‰æ•ˆçš„å¯¹è±¡å»é™¤ã€‚é¦–å…ˆï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›å›¾å½±å“ç”Ÿæˆå›¾åƒçš„ç»“æ„å’Œå½¢çŠ¶ç»†èŠ‚çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›æ¿€æ´»å’ŒæŠ‘åˆ¶ï¼ˆASSï¼‰ï¼Œå®ƒæ ¹æ®ç»™å®šçš„æ©è†œé‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œåœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼˜å…ˆè€ƒè™‘èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªæ³¨æ„åŠ›é‡å®šå‘å¼•å¯¼ï¼ˆSARGï¼‰ï¼Œåˆ©ç”¨ASSé‡å®šå‘çš„è‡ªæ³¨æ„åŠ›æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°åœ¨æ©è†œå†…å»é™¤å‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶ç”Ÿæˆæ—¢åˆç†åˆè¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨å„ç§é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­çš„å¯¹è±¡å»é™¤è¡¨ç°å‡ºç¨³å®šæ€§å’Œæœ‰æ•ˆæ€§ï¼Œç”šè‡³è¶…è¶Šäº†åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯ä»¥åœ¨å„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ä¸­å®ç°ï¼Œè¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Anonym0u3/AttentiveEraser">https://github.com/Anonym0u3/AttentiveEraser</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12974v7">PDF</a> Accepted by AAAI 2025(Oral)</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸå´­éœ²å¤´è§’ï¼Œå°¤å…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ç„¶è€Œï¼Œåœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬ä»é¢ä¸´ç”Ÿæˆéšæœºä¼ªå½±å’Œæ— æ³•é‡æ–°ç»˜åˆ¶å‰æ™¯å¯¹è±¡åŒºåŸŸçš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è°ƒæ•´çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹æ–¹æ³•â€”â€”Attentive Eraserï¼Œç”¨äºå®ç°ç¨³å®šå’Œæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚é€šè¿‡é‡æ–°è®¾è®¡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å†…çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†èƒŒæ™¯è€Œéå‰æ™¯å¯¹è±¡ï¼Œä»è€Œå®ç°åœ¨åå‘ç”Ÿæˆè¿‡ç¨‹ä¸­ç¨³å®šå»é™¤å¯¹è±¡ã€‚æ­¤å¤–ï¼Œå¼•å…¥è‡ªæ³¨æ„åŠ›é‡å®šå‘å¼•å¯¼ï¼ˆSARGï¼‰ï¼Œåˆ©ç”¨ASSå¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨å»é™¤å‰æ™¯å¯¹è±¡çš„åŒæ—¶ç”Ÿæˆåˆç†ä¸”è¿è´¯çš„å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼ŒAttentive Eraseråœ¨å„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ç¨³å®šæœ‰æ•ˆï¼Œç”šè‡³è¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒAttentive Eraserå¯åº”ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œå…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸè¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚</li>
<li>åœ¨å¯¹è±¡ç§»é™¤ä»»åŠ¡ä¸­ï¼Œæ‰©æ•£æ¨¡å‹é¢ä¸´ç”Ÿæˆéšæœºä¼ªå½±å’Œæ— æ³•é‡æ–°ç»˜åˆ¶å‰æ™¯çš„é—®é¢˜ã€‚</li>
<li>Attentive Eraseræ–¹æ³•è§£å†³äº†è¿™äº›é—®é¢˜ï¼Œè®©é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å®ç°ç¨³å®šå’Œæœ‰æ•ˆçš„å¯¹è±¡ç§»é™¤ã€‚</li>
<li>Attentive Eraseré€šè¿‡é‡æ–°è®¾è®¡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¼˜å…ˆå¤„ç†èƒŒæ™¯åŒºåŸŸã€‚</li>
<li>å¼•å…¥è‡ªæ³¨æ„åŠ›é‡å®šå‘å¼•å¯¼ï¼ˆSARGï¼‰ï¼Œæé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œè¿è´¯æ€§ã€‚</li>
<li>å®éªŒè¯æ˜Attentive Eraseråœ¨å„ç§é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-803178e9e14e0b470c11ba0560302143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22a85791f9ca71a67255ff3526b87793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60d90a9fc622e916ad2be2cbe76154b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d38d1501446a78f9354c70defe4087cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0f7e55399d95265f218e3fdd27c817.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="David-and-Goliath-Small-One-step-Model-Beats-Large-Diffusion-with-Score-Post-training"><a href="#David-and-Goliath-Small-One-step-Model-Beats-Large-Diffusion-with-Score-Post-training" class="headerlink" title="David and Goliath: Small One-step Model Beats Large Diffusion with Score   Post-training"></a>David and Goliath: Small One-step Model Beats Large Diffusion with Score   Post-training</h2><p><strong>Authors:Weijian Luo, Colin Zhang, Debing Zhang, Zhengyang Geng</strong></p>
<p>We propose Diff-Instruct* (DI*), a data-efficient post-training approach for one-step text-to-image generative models to improve its human preferences without requiring image data. Our method frames alignment as online reinforcement learning from human feedback (RLHF), which optimizes the one-step model to maximize human reward functions while being regularized to be kept close to a reference diffusion process. Unlike traditional RLHF approaches, which rely on the Kullback-Leibler divergence as the regularization, we introduce a novel general score-based divergence regularization that substantially improves performance as well as post-training stability. Although the general score-based RLHF objective is intractable to optimize, we derive a strictly equivalent tractable loss function in theory that can efficiently compute its \emph{gradient} for optimizations. We introduce \emph{DI*-SDXL-1step}, which is a 2.6B one-step text-to-image model at a resolution of $1024\times 1024$, post-trained from DMD2 w.r.t SDXL. \textbf{Our 2.6B \emph{DI*-SDXL-1step} model outperforms the 50-step 12B FLUX-dev model} in ImageReward, PickScore, and CLIP score on the Parti prompts benchmark while using only 1.88% of the inference time. This result clearly shows that with proper post-training, the small one-step model is capable of beating huge multi-step diffusion models. Our model is open-sourced at this link: <a target="_blank" rel="noopener" href="https://github.com/pkulwj1994/diff_instruct_star">https://github.com/pkulwj1994/diff_instruct_star</a>. We hope our findings can contribute to human-centric machine learning techniques. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Diff-Instruct<em>ï¼ˆDI</em>ï¼‰è¿™ä¸€æ•°æ®é«˜æ•ˆçš„åè®­ç»ƒæ–¹å¼ï¼Œç”¨äºæ”¹è¿›ä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„äººæ€§åå¥½ï¼Œè€Œæ— éœ€å›¾åƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å¯¹é½ä½œä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„ä»»åŠ¡æ¡†æ¶ï¼Œä¼˜åŒ–ä¸€æ­¥æ¨¡å‹ä»¥æœ€å¤§åŒ–äººç±»å¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘å‚è€ƒæ‰©æ•£è¿‡ç¨‹çš„æ­£åˆ™åŒ–ã€‚ä¸åŒäºä¼ ç»Ÿçš„ä¾èµ–Kullback-Leibleræ•£åº¦ä½œä¸ºæ­£åˆ™åŒ–çš„RLHFæ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹é€šç”¨åŸºäºåˆ†æ•°çš„æ•£åº¦æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¿™æå¤§åœ°æé«˜äº†æ€§èƒ½å’Œåè®­ç»ƒç¨³å®šæ€§ã€‚å°½ç®¡é€šç”¨åŸºäºåˆ†æ•°çš„RLHFç›®æ ‡éš¾ä»¥ä¼˜åŒ–ï¼Œä½†æˆ‘ä»¬ä»ç†è®ºä¸Šæ¨å¯¼å‡ºäº†ä¸€ä¸ªä¸¥æ ¼ç­‰ä»·çš„å¯å¤„ç†æŸå¤±å‡½æ•°ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è®¡ç®—å…¶æ¢¯åº¦è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä»‹ç»äº†DI*-SDXL-1stepï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†è¾¨ç‡ä¸º$1024\times 1024$çš„2.6Bä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œæ˜¯åŸºäºDMD2å…³äºSDXLçš„åè®­ç»ƒç»“æœã€‚æˆ‘ä»¬çš„2.6B DI*-SDXL-1stepæ¨¡å‹åœ¨Partiæç¤ºåŸºå‡†æµ‹è¯•ä¸­ï¼Œåœ¨ImageRewardã€PickScoreå’ŒCLIPåˆ†æ•°æ–¹é¢è¶…è¶Šäº†50æ­¥çš„12B FLUX-devæ¨¡å‹ï¼ŒåŒæ—¶ä»…ä½¿ç”¨1.88%çš„æ¨ç†æ—¶é—´ã€‚è¿™ä¸€ç»“æœæ¸…æ¥šåœ°è¡¨æ˜ï¼Œé€šè¿‡é€‚å½“çš„åè®­ç»ƒï¼Œå°å‹çš„ä¸€æ­¥æ¨¡å‹èƒ½å¤Ÿå‡»è´¥å¤§å‹çš„å¤šæ­¥æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ­¤é“¾æ¥å¤„å¼€æºï¼š<a target="_blank" rel="noopener" href="https://github.com/pkulwj1994/diff_instruct_star%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E8%83%BD%E4%B8%BA%E4%BB%A5%E4%BA%BA%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE%E3%80%82">https://github.com/pkulwj1994/diff_instruct_starã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºä»¥äººä¸ºä¸­å¿ƒçš„æœºå™¨å­¦ä¹ æŠ€æœ¯åšå‡ºè´¡çŒ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20898v3">PDF</a> Revision: paper accepted by the ICML2025 main conference</p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§æ•°æ®é«˜æ•ˆçš„åè®­ç»ƒæ–¹å¼Diff-Instruct<em>ï¼ˆDI</em>ï¼‰ï¼Œç”¨äºä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¯åœ¨æ— éœ€å›¾åƒæ•°æ®çš„æƒ…å†µä¸‹æé«˜å…¶äººç±»åå¥½åº¦ã€‚è¯¥æ–¹æ³•å°†å¯¹é½è§†ä¸ºåœ¨çº¿å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œæ—¨åœ¨ä½¿ä¸€æ­¥æ¨¡å‹æœ€å¤§åŒ–äººç±»å¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶ä¿æŒå¯¹å‚è€ƒæ‰©æ•£è¿‡ç¨‹çš„æ¥è¿‘ã€‚å¼•å…¥äº†ä¸€ç§æ–°å‹é€šç”¨è¯„åˆ†åŸºç¡€ä¸Šçš„åˆ†æ­§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½åŠåè®­ç»ƒç¨³å®šæ€§ã€‚è™½ç„¶é€šç”¨è¯„åˆ†åŸºç¡€ä¸Šçš„RLHFç›®æ ‡éš¾ä»¥ä¼˜åŒ–ï¼Œä½†ç†è®ºä¸Šæ¨å¯¼å‡ºäº†ä¸€ä¸ªç­‰æ•ˆçš„å¯å¤„ç†æŸå¤±å‡½æ•°ï¼Œå¯æœ‰æ•ˆåœ°ä¸ºå…¶ä¼˜åŒ–è®¡ç®—æ¢¯åº¦ã€‚ä»‹ç»äº†åŸºäºDI*çš„SDXL-1æ­¥æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä»DMD2å…³äºSDXLçš„åè®­ç»ƒå‡ºæ¥çš„åˆ†è¾¨ç‡ä¸º$1024\times 1024$çš„2.6Bä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚ç›¸è¾ƒäºFLUX-devæ¨¡å‹çš„50æ­¥12Bæ¨¡å‹ï¼Œåœ¨ImageRewardã€PickScoreå’ŒCLIPåˆ†æ•°ä¸Šè¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶ä»…ä½¿ç”¨1.88%çš„æ¨ç†æ—¶é—´ã€‚ç»“æœè¯æ˜ï¼Œé€šè¿‡é€‚å½“åè®­ç»ƒï¼Œå°å‹ä¸€æ­¥æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šå¤§å‹å¤šæ­¥æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹å·²åœ¨æ­¤é“¾æ¥å¼€æºï¼š<a target="_blank" rel="noopener" href="https://github.com/pkulwj1994/diff_instruct_star%E3%80%82%E6%88%91%E4%BB%AC%E6%9C%9F%E6%9C%9B%E7%A0%94%E7%A9%B6%E6%88%90%E6%9E%9C%E8%83%BD%E4%B8%BA%E4%BA%BA%E7%B1%BB%E4%B8%AD%E5%BF%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE%E3%80%82">https://github.com/pkulwj1994/diff_instruct_starã€‚æˆ‘ä»¬æœŸæœ›ç ”ç©¶æˆæœèƒ½ä¸ºäººç±»ä¸­å¿ƒæœºå™¨å­¦ä¹ æŠ€æœ¯åšå‡ºè´¡çŒ®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†Diff-Instruct<em>ï¼ˆDI</em>ï¼‰æ–¹æ³•ï¼Œä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ•°æ®é«˜æ•ˆåè®­ç»ƒç­–ç•¥ï¼Œèƒ½åœ¨ä¸ä¾èµ–å›¾åƒæ•°æ®çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹å¯¹äººç±»åå¥½çš„é€‚åº”åº¦ã€‚</li>
<li>é‡‡ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œå¯¹é½ï¼Œä½¿æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­èƒ½å¤Ÿæœ€å¤§åŒ–äººç±»å¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶ä¿æŒå¯¹å‚è€ƒæ‰©æ•£è¿‡ç¨‹çš„æ¥è¿‘ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹é€šç”¨è¯„åˆ†åŸºç¡€ä¸Šçš„åˆ†æ­§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½åŠåè®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>é€šè¿‡ç†è®ºæ¨å¯¼ï¼Œæ‰¾åˆ°äº†ä¸€ä¸ªç­‰æ•ˆçš„å¯å¤„ç†æŸå¤±å‡½æ•°ï¼Œä¾¿äºä¼˜åŒ–è®¡ç®—æ¢¯åº¦ã€‚</li>
<li>ä»‹ç»äº†åä¸ºDI*-SDXL-1stepçš„æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶ä¼˜ç§€æ€§èƒ½ï¼šç›¸è¾ƒäºå¤§å‹å¤šæ­¥æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å›¾åƒå¥–åŠ±ã€æŒ‘é€‰åˆ†æ•°å’ŒCLIPåˆ†æ•°æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ï¼ŒåŒæ—¶æ˜¾è‘—ç¼©çŸ­äº†æ¨ç†æ—¶é—´ã€‚</li>
<li>æ¨¡å‹å·²å¼€æºä¾›å…¬ä¼—ä½¿ç”¨ï¼Œé“¾æ¥ä¸º<a target="_blank" rel="noopener" href="https://github.com/pkulwj1994/diff_instruct_star%E3%80%82">https://github.com/pkulwj1994/diff_instruct_starã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88bf866eee39fb4295d839ab68aec18c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afadfe6a3a5638a16df0f730a9a114a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d4e45d22a9f64a9014af7c84d9b8831.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b4ae01e9699b2cf20a94369249f41db9.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  Refer to Anything with Vision-Language Prompts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-2c7003cc9dbc9e4c3491a4b1ab250d70.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-08  ProJo4D Progressive Joint Optimization for Sparse-View Inverse Physics   Estimation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
