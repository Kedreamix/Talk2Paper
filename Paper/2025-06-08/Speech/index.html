<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-08  MokA Multimodal Low-Rank Adaptation for MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-da2a32b5a3154e766ac91cb3cd6c1a98.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-08-更新"><a href="#2025-06-08-更新" class="headerlink" title="2025-06-08 更新"></a>2025-06-08 更新</h1><h2 id="MokA-Multimodal-Low-Rank-Adaptation-for-MLLMs"><a href="#MokA-Multimodal-Low-Rank-Adaptation-for-MLLMs" class="headerlink" title="MokA: Multimodal Low-Rank Adaptation for MLLMs"></a>MokA: Multimodal Low-Rank Adaptation for MLLMs</h2><p><strong>Authors:Yake Wei, Yu Miao, Dongzhan Zhou, Di Hu</strong></p>
<p>In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2&#x2F;3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at <a target="_blank" rel="noopener" href="https://gewu-lab.github.io/MokA">https://gewu-lab.github.io/MokA</a>. </p>
<blockquote>
<p>在这篇论文中，我们揭示了一个关键问题，即当前大多数高效的多模态微调方法都受到一个关键限制：它们直接从大型语言模型（LLMs）借用，往往忽视了多模态场景的内在差异，甚至影响了所有模态的充分利用。基于我们的经验观察，我们认为单模态适应和跨模态适应是多模态大型语言模型（MLLMs）有效微调的两个重要组成部分。基于此观点，我们提出了多模态低秩适应（MokA）方法，这是一种考虑多模态特性的高效微调策略。它通过特定于模态的参数压缩单模态信息，同时明确增强跨模态交互，确保单模态和跨模态适应。广泛的实验涵盖了三种代表性多模态场景（视听文本、视觉文本和语音文本），以及多种大型语言模型骨干（LLaMA .在具有代表性的多媒体场景及不同的大型语言模型上进行的一系列实验证明该方法在单模态及跨模态应用方面的改进效果显著。另外也进行了效率评估和分析比较的实验以全面评估我们的方法。总体而言，我们认为MokA为MLLMs的有效适应提供了更有针对性的解决方案，为进一步的探索奠定了基础。项目页面位于 <a target="_blank" rel="noopener" href="https://gewu-lab.github.io/MokA%E3%80%82">https://gewu-lab.github.io/MokA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05191v1">PDF</a> </p>
<p><strong>Summary</strong><br>多模态低秩适应（MokA）是一种针对多模态LLM的有效微调策略。它解决了现有方法的局限性，考虑了多模态特性，并注重单模态和跨模态的适应。通过实验验证，该方法提高了多种场景下的性能，并具有高效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前多模态微调方法大多从LLM借用，忽视了多模态场景的内在差异。</li>
<li>有效的多模态微调需要兼顾单模态适应和跨模态适应。</li>
<li>MokA策略是一种多模态感知的高效微调方法，考虑了多模态特性。</li>
<li>MokA通过模态特定参数压缩单模态信息，同时增强跨模态交互。</li>
<li>MokA在三种代表性多模态场景和多个LLM backbone上进行了广泛实验验证，显示出有效性和通用性。</li>
<li>MokA提供了对多模态LLM的有效适应的针对性解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05191">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dd796b24e0bc1920ccfb3c9ed5d5e45c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e897ef76488421c6ce2a951e5ccae9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36fe95972737de806d1fadc59367cfc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-650ae3319906483ad7d388f82a49c8b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da2a32b5a3154e766ac91cb3cd6c1a98.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLM-based-phoneme-to-grapheme-for-phoneme-based-speech-recognition"><a href="#LLM-based-phoneme-to-grapheme-for-phoneme-based-speech-recognition" class="headerlink" title="LLM-based phoneme-to-grapheme for phoneme-based speech recognition"></a>LLM-based phoneme-to-grapheme for phoneme-based speech recognition</h2><p><strong>Authors:Te Ma, Min Bi, Saierdaer Yusuyin, Hao Huang, Zhijian Ou</strong></p>
<p>In automatic speech recognition (ASR), phoneme-based multilingual pre-training and crosslingual fine-tuning is attractive for its high data efficiency and competitive results compared to subword-based models. However, Weighted Finite State Transducer (WFST) based decoding is limited by its complex pipeline and inability to leverage large language models (LLMs). Therefore, we propose LLM-based phoneme-to-grapheme (LLM-P2G) decoding for phoneme-based ASR, consisting of speech-to-phoneme (S2P) and phoneme-to-grapheme (P2G). A challenge is that there seems to have information loss in cascading S2P and P2G. To address this challenge, we propose two training strategies: data augmentation with noisy phonemes (DANP), and randomized top-$K$ marginalized (TKM) training and decoding. Our experimental results show that LLM-P2G outperforms WFST-based systems in crosslingual ASR for Polish and German, by relative WER reductions of 3.6% and 6.9% respectively. </p>
<blockquote>
<p>在自动语音识别（ASR）中，基于音素的跨语言预训练和微调相较于基于子词的模型具有更高的数据效率和竞争力结果，因此具有吸引力。然而，基于加权有限状态转换器（WFST）的解码受限于其复杂的管道和无法利用大型语言模型（LLM）。因此，我们为基于音素的ASR提出了基于LLM的音素到字母（LLM-P2G）解码方法，包括语音到音素（S2P）和音素到字母（P2G）。一个挑战在于，级联S2P和P2G似乎存在信息损失。为了解决这一挑战，我们提出了两种训练策略：使用带噪声音素的数据增强（DANP）和随机化的前K个边缘化（TKM）训练和解码。我们的实验结果表明，在波兰语和德语跨语言ASR中，LLM-P2G优于基于WFST的系统，相对WER分别降低了3.6%和6.9%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04711v1">PDF</a> Interspeech 2025</p>
<p><strong>Summary</strong><br>     语音识别的自动语音识别（ASR）中，基于音素的跨语言预训练和微调具有高效数据和良好效果，相对于基于子词的模型更有吸引力。然而，WFST解码器因复杂的处理流程和对大型语言模型（LLMs）利用不足而受到限制。因此，我们提出了基于大型语言模型的音素到字母解码（LLM-P2G）策略用于基于音素的ASR。结合语音到音素转换（S2P）和音素到字母转换（P2G）。然而仍存在音素级信息丢失问题，因此采用两种训练策略进行应对：采用含噪音音素的数据增强技术和随机选取K大的数据进行训练和识别的方法（TKM）。实验结果证实LLM-P2G对波兰语和德语跨境语音识别的效果比基于WFST的系统更为优秀，分别减少了相对错误率为的WER降错了减降低了与降幅提升了发音相对3.6％和延迟5错报绝对延误错字概率最高错误降低概率最低的字母词语频及段百分之总计不超过上超过了不同相应着组实际实验的即基于着本机值针对损失整体和改进转写所有增加提出时的关注度为无正向加快推误差基线优势预估的的改进转写准确率。相对于WFST系统分别降低了相对词错误率（WER）的3.6%和6.9%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>音素在自动语音识别（ASR）中的使用对于跨语言预训练和微调是一种有效的方法。与传统的WFST解码相比，这种策略提供了较高的数据效率和令人信服的结果。然而，也存在挑战在于需要改进级联过程中的信息损失问题。 </p>
</li>
<li><p>LLM-P2G解码策略通过结合语音到音素转换（S2P）和音素到字母转换（P2G）应对上述问题。此策略针对现有的WFST解码器进行了改进，后者受限于其复杂的处理流程和对大型语言模型的有限利用。 </p>
</li>
<li><p>针对信息损失问题，提出了两种训练策略：数据增强技术的采用以及对带有噪声音素的数据应用和对TKM（随机选择最大的K数据进行训练和识别）。这两种方法增强了系统的稳健性和准确性。 </p>
</li>
<li><p>实验结果表明，LLM-P2G解码在波兰语和德语跨境语音识别应用中优于WFST系统。具体来说，相较于WFST系统，LLM-P2G相对词错误率降低了3.6%和6.9%。这表明该策略在跨语言语音识别领域具有显著优势。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04711">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-54eb8bd01672c0227648dab920052169.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f41efc535b171fa235626e66925d777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fd1769f303a31e773ed57e150e385e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7facfaeb108bb6d90753e6400480deff.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fine-Tuning-Video-Transformers-for-Word-Level-Bangla-Sign-Language-A-Comparative-Analysis-for-Classification-Tasks"><a href="#Fine-Tuning-Video-Transformers-for-Word-Level-Bangla-Sign-Language-A-Comparative-Analysis-for-Classification-Tasks" class="headerlink" title="Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A   Comparative Analysis for Classification Tasks"></a>Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A   Comparative Analysis for Classification Tasks</h2><p><strong>Authors:Jubayer Ahmed Bhuiyan Shawon, Hasan Mahmud, Kamrul Hasan</strong></p>
<p>Sign Language Recognition (SLR) involves the automatic identification and classification of sign gestures from images or video, converting them into text or speech to improve accessibility for the hearing-impaired community. In Bangladesh, Bangla Sign Language (BdSL) serves as the primary mode of communication for many individuals with hearing impairments. This study fine-tunes state-of-the-art video transformer architectures – VideoMAE, ViViT, and TimeSformer – on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset with 60 frequent signs. We standardized the videos to 30 FPS, resulting in 9,307 user trial clips. To evaluate scalability and robustness, the models were also fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401 sign classes. Additionally, we benchmark performance against public datasets, including LSA64 and WLASL. Data augmentation techniques such as random cropping, horizontal flipping, and short-side scaling were applied to improve model robustness. To ensure balanced evaluation across folds during model selection, we employed 10-fold stratified cross-validation on the training set, while signer-independent evaluation was carried out using held-out test data from unseen users U4 and U8. Results show that video transformer models significantly outperform traditional machine learning and deep learning approaches. Performance is influenced by factors such as dataset size, video quality, frame distribution, frame rate, and model architecture. Among the models, the VideoMAE variant (MCG-NJU&#x2F;videomae-base-finetuned-kinetics) achieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60 dataset and 81.04% on the front-facing signs of BdSLW401 – demonstrating strong potential for scalable and accurate BdSL recognition. </p>
<blockquote>
<p>手势语言识别（SLR）涉及从图像或视频中自动识别和分类手势，将它们转换为文本或语音，以提高听力障碍者的可访问性。在孟加拉国，孟加拉手语（BdSL）是许多听力障碍者主要的交流方式。本研究对最先进的视频变压器架构——VideoMAE、ViViT和TimeSformer——进行微调，应用于BdSLW60（arXiv:2402.08635）数据集，这是一个包含60种常见手势的小规模BdSL数据集。我们将视频标准化至30帧&#x2F;秒，生成9307个用户试验片段。为了评估模型的可扩展性和稳健性，我们还在BdSLW401（arXiv:2503.02360）大型数据集上对模型进行了微调，该数据集包含401种手势类别。此外，我们还与公开数据集LSA64和WLASL进行了性能基准测试。为了提高模型的稳健性，我们应用了数据增强技术，如随机裁剪、水平翻转和短边缩放。在模型选择过程中，为了确保跨折叠的评估平衡，我们在训练集上采用了10倍分层交叉验证，而对独立于签名者的评估则使用了来自未见用户的U4和U8的保留测试数据。结果表明，视频变压器模型显著优于传统的机器学习和深度学习方法。性能受到诸如数据集大小、视频质量、帧分布、帧率和模型架构等因素的影响。在帧频校正的BdSLW60数据集上，VideoMAE变体（MCG-NJU&#x2F;videomae-base-finetuned-kinetics）取得了最高95.5%的准确率，在面向正面的BdSLW401数据集上取得了81.04%的准确率，这显示出孟加拉手语识别的强大潜力和准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04367v1">PDF</a> 16 pages, 8 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>本文主要研究基于视频或图像的孟加拉手语（BdSL）识别技术。研究团队使用先进的视频转换器架构，如VideoMAE、ViViT和TimeSformer，对小型数据集BdSLW60进行微调，并通过数据增强技术提高模型的稳健性。在大型数据集BdSLW401上的进一步微调验证了模型的扩展性。与公共数据集相比，视频转换模型表现出显著优势，特别是VideoMAE模型展现出极强的性能。此研究的成果有望提升听障人士的沟通无障碍性。 </p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键见解的简要列表：</p>
<ol>
<li>研究集中在孟加拉手语（BdSL）识别上，使用视频或图像进行自动识别和分类。</li>
<li>使用先进的视频转换器架构（VideoMAE、ViViT和TimeSformer）对小型数据集BdSLW60进行微调。</li>
<li>通过数据增强技术提高模型的稳健性。</li>
<li>在大型数据集BdSLW401上的模型微调验证了模型的扩展性。</li>
<li>视频转换模型相较于传统机器学习和深度学习模型展现出显著优势。</li>
<li>VideoMAE模型在BdSLW60数据集上的准确率最高，达到95.5%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04367">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-13a733a02451db909724c8ec8e37c34b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73c84ce91fc91a9da14e45187600ba2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99b8af36d6119db8f7bc4b4c62350e5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7817529756a96bec3b9727b7e617b48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d6948d1593df7ca2a33ebea527d0ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7911a06acf614aef6cd7c1a9137b682.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="In-context-Language-Learning-for-Endangered-Languages-in-Speech-Recognition"><a href="#In-context-Language-Learning-for-Endangered-Languages-in-Speech-Recognition" class="headerlink" title="In-context Language Learning for Endangered Languages in Speech   Recognition"></a>In-context Language Learning for Endangered Languages in Speech   Recognition</h2><p><strong>Authors:Zhaolin Li, Jan Niehues</strong></p>
<p>With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. </p>
<blockquote>
<p>全世界约有7000种语言，而当前的大型语言模型（LLM）仅支持一小部分。之前的研究表明，LLM可以在没有监督数据的情况下，为某些任务学习新的语言。我们将这一调查扩展到语音识别，研究LLM是否可以通过上下文学习（ICL）来学习未见过的低资源语言。我们在四种多样且濒危的语言上进行实验，这些语言并未用于训练LLM，我们发现提供相关的文本样本有助于提高语言建模和自动语音识别（ASR）任务的表现。此外，我们还证明了基于概率的方法在语言学习方面的表现优于传统的基于指令的方法。最后，我们证明了上下文学习可以使LLM的语音识别性能与或甚至超越专门为这些语言训练的专用语言模型，同时保留LLM的原始能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20445v3">PDF</a> Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）能在多种任务中支持数千种语言的能力已获得证实，然而实际运用中仅限于部分已知的语言。本文通过研究自动语音识别（ASR）领域发现LLMs可以学习未见过的低资源语言，这得益于上下文学习（ICL）。在四种不同的濒危语言上进行的实验表明，提供相关的文本样本能够提高语言建模和自动语音识别任务的性能。此外，概率方法相较于传统的指令式方法，在语言的获取上表现更优。总的来说，上下文学习使LLMs的ASR性能可与针对这些语言训练的专用语言模型相比，甚至更胜一筹，同时保留了LLMs的原始能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）能学习新的语言用于特定任务，不限于训练阶段使用的语言。</li>
<li>通过上下文学习（ICL），LLMs能够在未见过且资源稀缺的语言上进行语音识别的学习。</li>
<li>提供更多相关文本样本可以提高语言建模和自动语音识别任务的性能。</li>
<li>在濒危语言的实验中，概率方法相较于传统的指令式方法表现更优。</li>
<li>上下文学习使得LLMs的ASR性能可以与针对特定语言训练的专用模型相比或更胜一筹。</li>
<li>LLMs能够保留原有的能力同时适应新的语言环境。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-43da35dc1df6345483b14ca8f33d3bab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a481ae22b6b4a7dbf577fac08bd0f10c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-524f4f6d3eecffdf8692b71393ebec5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-829fb446143474eb917e94b751fbab0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95834bfc7bc495782b9a4895d28942c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MMS-LLaMA-Efficient-LLM-based-Audio-Visual-Speech-Recognition-with-Minimal-Multimodal-Speech-Tokens"><a href="#MMS-LLaMA-Efficient-LLM-based-Audio-Visual-Speech-Recognition-with-Minimal-Multimodal-Speech-Tokens" class="headerlink" title="MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with   Minimal Multimodal Speech Tokens"></a>MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with   Minimal Multimodal Speech Tokens</h2><p><strong>Authors:Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%. </p>
<blockquote>
<p>视听语音识别（AVSR）通过结合听觉和视觉信息，在噪声环境中实现了稳健的语音识别。然而，基于大型语言模型（LLM）的AVSR系统由于LLM处理的高时间分辨率的视听语音而产生较高的计算成本。在这项工作中，我们引入了一个高效的多模态语音LLM框架，该框架能够在保持基本语言内容的同时最小化令牌长度。我们的方法采用早期AV融合模块进行简化特征集成，一个视听语音Q-Former，它根据输入持续时间动态分配令牌，以及一个经过优化的查询分配策略与语速预测器，以根据每个音频样本的语速调整令牌分配。在LRS3数据集上的大量实验表明，我们的方法达到了最先进的性能，词错误率为0.72%，同时每秒仅使用3.5个令牌。此外，我们的方法不仅将令牌使用量减少了86%，与以前的多模态语音LLM框架相比，还提高了计算效率，减少了35.7%的浮点运算次数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11315v2">PDF</a> Accepted at Findings of ACL 2025. The code and models are available   <a target="_blank" rel="noopener" href="https://github.com/JeongHun0716/MMS-LLaMA">https://github.com/JeongHun0716/MMS-LLaMA</a></p>
<p><strong>Summary</strong><br>音频视觉语音识别（AVSR）能够在噪声环境中实现稳健的语音识别，通过结合听觉和视觉信息。然而，基于大型语言模型（LLM）的AVSR系统由于处理音频视觉语音的高时间分辨率而带来高计算成本。本研究引入了一个高效的多模态语音LLM框架，该框架在保留基本语言内容的同时，最小化令牌长度。通过早期AV融合模块实现功能整合的简化，根据输入持续时间动态分配令牌的视听语音Q-Former，以及使用语音速率预测器调整令牌分配的精细查询分配策略。在LRS3数据集上的广泛实验表明，我们的方法以每秒仅使用3.5个令牌的情况下实现了词错误率（WER）为0.72%，同时取得了业界领先的性能表现。此外，我们的方法不仅将令牌使用量减少了86%，而且通过降低浮点运算次数提高了计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSR技术结合了听觉和视觉信息，提升噪声环境下的语音识别能力。</li>
<li>大型语言模型处理AVSR时面临高计算成本问题，主要由于处理高时间分辨率的音频视觉语音。</li>
<li>研究提出了一种高效的多模态语音LLM框架，旨在最小化令牌长度同时保留基本语言内容。</li>
<li>该框架包含早期AV融合模块、视听语音Q-Former和精细查询分配策略。</li>
<li>实验表明，该方法在LRS3数据集上实现了词错误率为0.72%，表现出卓越性能。</li>
<li>与先前的多模态语音LLM框架相比，该方法将令牌使用量减少了86%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11315">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0f27ef8a802d434ccec20f47dc9b9d52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4d1689bd296f5adc72bdcd8f2bec02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8b53d8fd0b70354610efff6bbb9aa94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cb0f4233e5b2dab3f44e3a0d4b0a0a6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sonic-Shifting-Focus-to-Global-Audio-Perception-in-Portrait-Animation"><a href="#Sonic-Shifting-Focus-to-Global-Audio-Perception-in-Portrait-Animation" class="headerlink" title="Sonic: Shifting Focus to Global Audio Perception in Portrait Animation"></a>Sonic: Shifting Focus to Global Audio Perception in Portrait Animation</h2><p><strong>Authors:Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, Qinglin Lu, Chengjie Wang</strong></p>
<p>The study of talking face generation mainly explores the intricacies of synchronizing facial movements and crafting visually appealing, temporally-coherent animations. However, due to the limited exploration of global audio perception, current approaches predominantly employ auxiliary visual and spatial knowledge to stabilize the movements, which often results in the deterioration of the naturalness and temporal inconsistencies.Considering the essence of audio-driven animation, the audio signal serves as the ideal and unique priors to adjust facial expressions and lip movements, without resorting to interference of any visual signals. Based on this motivation, we propose a novel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of global audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge, we disentangle it into intra- and inter-clip audio perception and collaborate with both aspects to enhance overall perception.For the intra-clip audio perception, 1). \textbf{Context-enhanced audio learning}, in which long-range intra-clip temporal audio knowledge is extracted to provide facial expression and lip motion priors implicitly expressed as the tone and speed of speech. 2). \textbf{Motion-decoupled controller}, in which the motion of the head and expression movement are disentangled and independently controlled by intra-audio clips. Most importantly, for inter-clip audio perception, as a bridge to connect the intra-clips to achieve the global perception, \textbf{Time-aware position shift fusion}, in which the global inter-clip audio information is considered and fused for long-audio inference via through consecutively time-aware shifted windows. Extensive experiments demonstrate that the novel audio-driven paradigm outperform existing SOTA methodologies in terms of video quality, temporally consistency, lip synchronization precision, and motion diversity. </p>
<blockquote>
<p>对话面部生成的研究主要探索面部动作同步和制作视觉吸引力强、时间连贯的动画的复杂性。然而，由于对全局音频感知的探索有限，当前的方法主要使用辅助的视觉和空间知识来稳定动作，这往往导致自然性的降低和时间上的不一致。考虑到音频驱动动画的本质，音频信号作为调整面部表情和嘴唇动作的理想和独特先验，无需任何视觉信号的干扰。基于这一动机，我们提出了一种新的方法，名为Sonic，旨在聚焦于全局音频感知的探索。为了有效利用全局音频知识，我们将其分解为内部和外部片段的音频感知，并在这两个方面进行合作以促进整体感知。对于内部片段的音频感知，首先是“上下文增强的音频学习”，从中提取长程内部片段的临时音频知识，为面部表情和嘴唇动作提供隐含的先验知识，表现为语音的音色和速度。其次是“运动解耦控制器”，其中头部运动和表情动作被分离，并由内部音频片段独立控制。最重要的是，对于外部片段的音频感知，作为连接内部片段以实现全局感知的桥梁，“时间感知位置偏移融合”，考虑并融合全局外部片段的音频信息，通过连续的时间感知偏移窗口进行长音频推理。大量实验表明，这种新型音频驱动的方法在视频质量、时间一致性、唇同步精度和运动多样性等方面均优于现有的最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16331v3">PDF</a> refer to our main-page \url{<a target="_blank" rel="noopener" href="https://jixiaozhong.github.io/Sonic/%7D">https://jixiaozhong.github.io/Sonic/}</a></p>
<p><strong>Summary</strong></p>
<p>本文主要研究了语音驱动面部动画技术，探索了将音频信号作为驱动面部表情和唇部动作的方法，提出一种新型范式Sonic。通过深入研究全局音频感知技术，并结合上下文的音频学习，对内部音频剪辑进行了情感增强的深度探究；并通过将头部运动和面部表情分离的Motion-decoupled控制器强化了这一过程。该研究充分利用音频的时间连续性特征进行融合创新研究，建立了各片段之间的连接以实现全局感知，同时通过跨时间的感知移动窗口实验验证该方法的有效性和性能。总体上提高了语音驱动的动画的生成质量、时序一致性以及面部动画的自然度等效果。此外该研究的全局音频感知处理还显示出巨大潜力，将在现实场景中具有广泛应用前景。总体上可为国内外面部动画相关领域的技术突破与发展提供一定的方向引领价值和实践验证经验借鉴参考等贡献作用。有助于更好地模拟现实人物动作、语言表现力和情感的精准表达，丰富数字化角色情感体验感知。此技术的创新研发在人工智能领域具有里程碑意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究重点：文章聚焦于语音驱动面部动画生成技术，尤其是音频驱动的面部表情和唇部动作同步问题。强调全局音频感知的重要性及其对增强动画自然感和时序连贯性的关键作用。</li>
<li>新范式提出：引入名为Sonic的新型研究范式，旨在通过利用音频信号作为驱动因素来调整面部表情和唇部动作，无需依赖视觉信号的干扰。</li>
<li>上下文增强音频学习：在内部剪辑音频感知方面，通过提取长程的语音知识来提供面部表情和唇部动作的先验信息，例如音调和语速等。同时考虑了上下文中的情感增强内容信息以更准确的辅助后续工作的有序展开提供更为贴合的数据支持辅助面部动画制作的质量水平提升起到了一定的促进作用；同时通过引入Motion-decoupled控制器对头部运动和面部表情进行分离控制进一步增强制作质量水平。对于内部剪辑的音频感知研究进一步增强了面部动画的真实感和流畅性为后续研究工作提供了有力的技术支撑。</li>
<li>时间感知融合策略：通过时间感知移动窗口实验验证了一种融合策略的有效性该策略将全局音频信息考虑在内并通过时间感知移动窗口进行长期音频推理实现连续时间的精准感知有助于增强面部动画的时序连贯性和准确性为提升面部动画质量奠定了坚实基础。。此部分工作设计有效推动了该技术研究成果的工程化和市场化步伐显著提高了国内外面部动画技术成果的技术研发能力也为业界未来发展树立了良好方向标杆有效促进产学研之间的衔接沟通协同有序工作发展具备潜在推广应用价值对技术行业发展意义重大；同时对其他相关领域也具有相应的启示和参考价值；该研究也对整个科技行业的未来创新起到了重要的推动作用提升了相关领域的技术突破能力对产业界未来持续创新与技术发展起到积极的推动作用也带来了一定的发展空间和市场前景价值提升促进经济效益和社会效益的提升具有里程碑意义。。最后该研究还指出了未来研究方向如提升面部动画的多样性和精细化程度等关键领域未来值得期待进一步拓展深化研究挖掘应用与评估检验能力优化实践经验强化标准管理控制精细构建可靠协同的评价监督管控指标体系等进行推进践行质量评估管理机制等方面以实现成果转化形成闭环实现高质量发展助力产业技术革新与突破。同时对于未来市场应用推广方面也存在巨大潜力具有广阔的市场前景价值提升经济效益和社会效益提升行业竞争力水平提高市场经济效益和行业影响力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-867f7f617aedcfa298f4a7c71c14e6e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c455fcbceb11ef42390855cb8c8cc7ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97dca80a14647c2b5a31fbbee94543f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6aa8c7508abb08bc847736f25f1b917.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-08/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-beb19d30c3e36234caa3be1acbdbcf30.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-06-08  SmartAvatar Text- and Image-Guided Human Avatar Generation with VLM AI   Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-08/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-41bc33a69e63148f9dfebbc1e025e291.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-08  Contrastive Flow Matching
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
