<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  EditAR Unified Conditional Generation with Autoregressive Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-20177e4c576a5acde5b8b7cbe855e79b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-10-æ›´æ–°"><a href="#2025-01-10-æ›´æ–°" class="headerlink" title="2025-01-10 æ›´æ–°"></a>2025-01-10 æ›´æ–°</h1><h2 id="EditAR-Unified-Conditional-Generation-with-Autoregressive-Models"><a href="#EditAR-Unified-Conditional-Generation-with-Autoregressive-Models" class="headerlink" title="EditAR: Unified Conditional Generation with Autoregressive Models"></a>EditAR: Unified Conditional Generation with Autoregressive Models</h2><p><strong>Authors:Jiteng Mu, Nuno Vasconcelos, Xiaolong Wang</strong></p>
<p>Recent progress in controllable image generation and editing is largely driven by diffusion-based methods. Although diffusion models perform exceptionally well in specific tasks with tailored designs, establishing a unified model is still challenging. In contrast, autoregressive models inherently feature a unified tokenized representation, which simplifies the creation of a single foundational model for various tasks. In this work, we propose EditAR, a single unified autoregressive framework for a variety of conditional image generation tasks, e.g., image editing, depth-to-image, edge-to-image, segmentation-to-image. The model takes both images and instructions as inputs, and predicts the edited images tokens in a vanilla next-token paradigm. To enhance the text-to-image alignment, we further propose to distill the knowledge from foundation models into the autoregressive modeling process. We evaluate its effectiveness across diverse tasks on established benchmarks, showing competitive performance to various state-of-the-art task-specific methods. Project page: <a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a> </p>
<blockquote>
<p>è¿‘æœŸå¯æ§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„è¿›å±•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›ŠäºåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å»ºç«‹ç»Ÿä¸€æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªå›å½’æ¨¡å‹å¤©ç”Ÿå…·æœ‰ç»Ÿä¸€çš„ä»¤ç‰ŒåŒ–è¡¨ç¤ºï¼Œè¿™ç®€åŒ–äº†å„ç§ä»»åŠ¡å•ä¸€åŸºç¡€æ¨¡å‹çš„åˆ›å»ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EditARï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œå¯ç”¨äºå„ç§æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒç¼–è¾‘ã€æ·±åº¦å›¾åƒã€è¾¹ç¼˜å›¾åƒã€åˆ†æ®µå›¾åƒç­‰ã€‚è¯¥æ¨¡å‹åŒæ—¶æ¥å—å›¾åƒå’ŒæŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰ŒèŒƒå¼ä¸­é¢„æµ‹ç¼–è¾‘åçš„å›¾åƒä»¤ç‰Œã€‚ä¸ºäº†æé«˜æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹åº”æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå°†åŸºç¡€æ¨¡å‹çš„çŸ¥è¯†è’¸é¦åˆ°è‡ªå›å½’å»ºæ¨¡è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬åœ¨æ—¢å®šçš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹å„é¡¹ä»»åŠ¡è¿›è¡Œäº†æœ‰æ•ˆæ€§è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºä¸å„ç§æœ€æ–°ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸ç«äº‰çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04699v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a></p>
<p><strong>Summary</strong><br>åŸºäºå¯æ§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„æœ€æ–°è¿›å±•ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºEditARçš„ç»Ÿä¸€è‡ªå›å½’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ç”¨äºå¤šç§æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¦‚å›¾åƒç¼–è¾‘ã€æ·±åº¦å›¾åƒç”Ÿæˆã€è¾¹ç¼˜å›¾åƒç”Ÿæˆå’Œåˆ†å‰²å›¾åƒç”Ÿæˆç­‰ã€‚è¯¥æ¨¡å‹ä»¥å›¾åƒå’ŒæŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œé‡‡ç”¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¨¡å¼é¢„æµ‹ç¼–è¾‘åçš„å›¾åƒä»¤ç‰Œã€‚ä¸ºæé«˜æ–‡æœ¬åˆ°å›¾åƒçš„åŒ¹é…åº¦ï¼Œç ”ç©¶è¿˜æè®®åœ¨è‡ªå›å½’å»ºæ¨¡è¿‡ç¨‹ä¸­ä»åŸºç¡€æ¨¡å‹ä¸­æç‚¼çŸ¥è¯†ã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½ä¸å„ç§æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EditARæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œå¯ç”¨äºå¤šç§æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶æ¥å—å›¾åƒå’ŒæŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ç¼–è¾‘åçš„å›¾åƒä»¤ç‰Œã€‚</li>
<li>EditARé‡‡ç”¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼ï¼Œç®€åŒ–äº†å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>ä¸ºæé«˜æ–‡æœ¬åˆ°å›¾åƒçš„åŒ¹é…åº¦ï¼Œç ”ç©¶æè®®åœ¨è‡ªå›å½’å»ºæ¨¡è¿‡ç¨‹ä¸­ä»åŸºç¡€æ¨¡å‹ä¸­æç‚¼çŸ¥è¯†ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶æ€§èƒ½ä¸æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸å½“ã€‚</li>
<li>EditARæ¡†æ¶å…·æœ‰æ½œåŠ›æˆä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¸€ç§é€šç”¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1cf12a8610ad1c57b9d782854bfe4b06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99a98974e503ad6e3360f8f7db57f6ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7755b225f6d9b3cc44174aea44eabb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RadGPT-Constructing-3D-Image-Text-Tumor-Datasets"><a href="#RadGPT-Constructing-3D-Image-Text-Tumor-Datasets" class="headerlink" title="RadGPT: Constructing 3D Image-Text Tumor Datasets"></a>RadGPT: Constructing 3D Image-Text Tumor Datasets</h2><p><strong>Authors:Pedro R. A. S. Bassi, Mehmet Can Yavuz, Kang Wang, Xiaoxi Chen, Wenxuan Li, Sergio Decherchi, Andrea Cavalli, Yang Yang, Alan Yuille, Zongwei Zhou</strong></p>
<p>With over 85 million CT scans performed annually in the United States, creating tumor-related reports is a challenging and time-consuming task for radiologists. To address this need, we present RadGPT, an Anatomy-Aware Vision-Language AI Agent for generating detailed reports from CT scans. RadGPT first segments tumors, including benign cysts and malignant tumors, and their surrounding anatomical structures, then transforms this information into both structured reports and narrative reports. These reports provide tumor size, shape, location, attenuation, volume, and interactions with surrounding blood vessels and organs. Extensive evaluation on unseen hospitals shows that RadGPT can produce accurate reports, with high sensitivity&#x2F;specificity for small tumor (&lt;2 cm) detection: 80&#x2F;73% for liver tumors, 92&#x2F;78% for kidney tumors, and 77&#x2F;77% for pancreatic tumors. For large tumors, sensitivity ranges from 89% to 97%. The results significantly surpass the state-of-the-art in abdominal CT report generation.   RadGPT generated reports for 17 public datasets. Through radiologist review and refinement, we have ensured the reportsâ€™ accuracy, and created the first publicly available image-text 3D medical dataset, comprising over 1.8 million text tokens and 2.7 million images from 9,262 CT scans, including 2,947 tumor scans&#x2F;reports of 8,562 tumor instances. Our reports can: (1) localize tumors in eight liver sub-segments and three pancreatic sub-segments annotated per-voxel; (2) determine pancreatic tumor stage (T1-T4) in 260 reports; and (3) present individual analyses of multiple tumorsâ€“rare in human-made reports. Importantly, 948 of the reports are for early-stage tumors. </p>
<blockquote>
<p>åœ¨ç¾å›½ï¼Œæ¯å¹´æœ‰è¶…è¿‡85ç™¾ä¸‡æ¬¡çš„CTæ‰«æï¼Œå¯¹äºæ”¾å°„ç§‘åŒ»ç”Ÿæ¥è¯´ï¼Œåˆ›å»ºä¸è‚¿ç˜¤ç›¸å…³çš„æŠ¥å‘Šæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œè€—æ—¶çš„å·¥ä½œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RadGPTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä»CTæ‰«æç”Ÿæˆè¯¦ç»†æŠ¥å‘Šçš„è§£å‰–å­¦æ„ŸçŸ¥è§†è§‰è¯­è¨€äººå·¥æ™ºèƒ½ä»£ç†ã€‚RadGPTé¦–å…ˆåˆ†å‰²è‚¿ç˜¤ï¼ˆåŒ…æ‹¬è‰¯æ€§å›Šè‚¿å’Œæ¶æ€§è‚¿ç˜¤ï¼‰åŠå…¶å‘¨å›´çš„è§£å‰–ç»“æ„ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯è½¬åŒ–ä¸ºç»“æ„åŒ–æŠ¥å‘Šå’Œå™è¿°æ€§æŠ¥å‘Šã€‚è¿™äº›æŠ¥å‘Šæä¾›äº†å…³äºè‚¿ç˜¤çš„å¤§å°ã€å½¢çŠ¶ã€ä½ç½®ã€è¡°å‡ã€ä½“ç§¯ä»¥åŠä¸å‘¨å›´è¡€ç®¡å’Œå™¨å®˜ç›¸äº’ä½œç”¨çš„ä¿¡æ¯ã€‚åœ¨æœªè§è¿‡çš„åŒ»é™¢è¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒRadGPTå¯ä»¥ç”Ÿæˆå‡†ç¡®çš„æŠ¥å‘Šï¼Œå¯¹äºå°äº2å˜ç±³çš„è‚¿ç˜¤å…·æœ‰é«˜çµæ•åº¦å’Œç‰¹å¼‚æ€§ï¼šè‚è„è‚¿ç˜¤çš„çµæ•åº¦å’Œç‰¹å¼‚æ€§åˆ†åˆ«ä¸º80%å’Œ73%ï¼Œè‚¾è„è‚¿ç˜¤çš„çµæ•åº¦å’Œç‰¹å¼‚æ€§åˆ†åˆ«ä¸º92%å’Œ78%ï¼Œèƒ°è…ºè‚¿ç˜¤çš„çµæ•åº¦å’Œç‰¹å¼‚æ€§å‡ä¸º77%ã€‚å¯¹äºè¾ƒå¤§çš„è‚¿ç˜¤ï¼Œçµæ•åº¦èŒƒå›´ä»89%åˆ°97%ã€‚ç»“æœæ˜¾è‘—è¶…è¿‡äº†è…¹éƒ¨CTæŠ¥å‘Šç”Ÿæˆé¢†åŸŸçš„æœ€æ–°æ°´å¹³ã€‚RadGPTä¸º17ä¸ªå…¬å…±æ•°æ®é›†ç”Ÿæˆäº†æŠ¥å‘Šã€‚é€šè¿‡æ”¾å°„ç§‘åŒ»ç”Ÿå®¡æŸ¥å’Œç»†åŒ–ï¼Œæˆ‘ä»¬ç¡®ä¿äº†æŠ¥å‘Šçš„å‡†ç¡®æ€§ï¼Œå¹¶åˆ›å»ºäº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„å›¾åƒæ–‡æœ¬3DåŒ»ç–—æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡180ä¸‡æ–‡æœ¬æ ‡è®°å’Œ270ä¸‡å›¾åƒï¼Œæ¥è‡ª9262æ¬¡CTæ‰«æï¼Œå…¶ä¸­åŒ…æ‹¬2947ä»½è‚¿ç˜¤æ‰«æ&#x2F;æŠ¥å‘Šå’Œæ¶‰åŠè‚¿ç˜¤å®ä¾‹çš„è¯¦ç»†æ•°æ®ã€‚æˆ‘ä»¬çš„æŠ¥å‘Šèƒ½å¤Ÿï¼šï¼ˆ1ï¼‰åœ¨å…«ä¸ªè‚è„å­æ®µå’Œæ¯ä¸ªä½“ç´ æ ‡æ³¨çš„ä¸‰ä¸ªèƒ°è…ºå­æ®µä¸­å®šä½è‚¿ç˜¤ï¼›ï¼ˆ2ï¼‰åœ¨èƒ°è…ºç™ŒæŠ¥å‘Šä¸­ç¡®å®šè‚¿ç˜¤åˆ†æœŸï¼ˆT1-T4ï¼‰ï¼›ï¼ˆ3ï¼‰å¯¹å¤šä¸ªè‚¿ç˜¤è¿›è¡Œä¸ªåˆ«åˆ†æâ€”â€”è¿™åœ¨äººç±»åˆ¶ä½œçš„æŠ¥å‘Šä¸­å¾ˆå°‘è§ã€‚é‡è¦çš„æ˜¯ï¼Œå…¶ä¸­æ¶‰åŠæ—©æœŸè‚¿ç˜¤çš„æŠ¥å‘Šæœ‰948ä»½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04678v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾åä¸ºRadGPTçš„åŒ»å­¦å›¾åƒæŠ¥å‘Šç”ŸæˆAIç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå¯è‡ªåŠ¨åˆ†æCTæ‰«æç»“æœï¼Œå¯¹è‚¿ç˜¤åŠå…¶å‘¨å›´ç»“æ„è¿›è¡Œåˆ†å‰²ï¼Œå¹¶ç”Ÿæˆè¯¦ç»†çš„æŠ¥å‘Šã€‚RadGPTç”Ÿæˆçš„æŠ¥å‘Šèƒ½å¤Ÿå‡†ç¡®æè¿°è‚¿ç˜¤çš„å¤§å°ã€å½¢çŠ¶ã€ä½ç½®ã€å¯†åº¦ã€ä½“ç§¯ä»¥åŠä¸å‘¨å›´è¡€ç®¡å’Œå™¨å®˜çš„å…³ç³»ã€‚å…¶æ€§èƒ½åœ¨å¤šä¸ªåŒ»é™¢çš„æ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œå¯¹å°è‚¿ç˜¤çš„æ£€æµ‹å‡†ç¡®ç‡è¾ƒé«˜ã€‚æ­¤å¤–ï¼ŒRadGPTè¿˜æ„å»ºäº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„å›¾åƒæ–‡æœ¬3DåŒ»ç–—æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadGPTæ˜¯ä¸€ä¸ªè§£å‰–å­¦æ„ŸçŸ¥çš„è§†è§‰è¯­è¨€AIä»£ç†ï¼Œèƒ½å¤Ÿä»CTæ‰«æä¸­ç”Ÿæˆè¯¦ç»†çš„æŠ¥å‘Šã€‚</li>
<li>RadGPTå¯ä»¥è‡ªåŠ¨åˆ†å‰²è‚¿ç˜¤åŠå…¶å‘¨å›´ç»“æ„ï¼ŒåŒ…æ‹¬è‰¯æ€§å›Šè‚¿å’Œæ¶æ€§è‚¿ç˜¤ã€‚</li>
<li>ç”Ÿæˆçš„æŠ¥å‘ŠåŒ…å«è‚¿ç˜¤çš„å¤šç§è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚å¤§å°ã€å½¢çŠ¶ã€ä½ç½®ã€å¯†åº¦ã€ä½“ç§¯ä»¥åŠä¸å‘¨å›´è¡€ç®¡å’Œå™¨å®˜çš„å…³ç³»ã€‚</li>
<li>RadGPTåœ¨å¤šä¸ªåŒ»é™¢çš„æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¯¹å°è‚¿ç˜¤çš„æ£€æµ‹å‡†ç¡®ç‡è¾ƒé«˜ã€‚</li>
<li>RadGPTç”Ÿæˆçš„æŠ¥å‘Šèƒ½å¤Ÿå®šä½è‚è„å’Œèƒ°è…ºçš„å­åˆ†æ®µï¼Œç¡®å®šèƒ°è…ºè‚¿ç˜¤çš„é˜¶æ®µï¼Œå¹¶æä¾›å¤šä¸ªè‚¿ç˜¤çš„ä¸ªäººåˆ†æã€‚</li>
<li>æ„å»ºäº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„å›¾åƒæ–‡æœ¬3DåŒ»ç–—æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡180ä¸‡ä¸ªæ–‡æœ¬æ ‡è®°å’Œè¶…è¿‡270ä¸‡å¼ å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04678">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7dab38093a0081891ce896c203ac3d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20177e4c576a5acde5b8b7cbe855e79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a0dcfb41d3c49c43ecb1ae3bae7fc9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4390c0194b5ab6f9f8ac56d4acc91f5f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MedCoDi-M-A-Multi-Prompt-Foundation-Model-for-Multimodal-Medical-Data-Generation"><a href="#MedCoDi-M-A-Multi-Prompt-Foundation-Model-for-Multimodal-Medical-Data-Generation" class="headerlink" title="MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data   Generation"></a>MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data   Generation</h2><p><strong>Authors:Daniele Molino, Francesco Di Feola, Eliodoro Faiella, Deborah Fazzini, Domiziana Santucci, Linlin Shen, Valerio Guarrasi, Paolo Soda</strong></p>
<p>Artificial Intelligence is revolutionizing medical practice, enhancing diagnostic accuracy and healthcare delivery. However, its adaptation in medical settings still faces significant challenges, related to data availability and privacy constraints. Synthetic data has emerged as a promising solution to mitigate these issues, addressing data scarcity while preserving privacy. Recently, Latent Diffusion Models have emerged as a powerful tool for generating high-quality synthetic data. Meanwhile, the integration of different modalities has gained interest, emphasizing the need of models capable of handle multimodal medical data.Existing approaches struggle to integrate complementary information and lack the ability to generate modalities simultaneously. To address this challenge, we present MedCoDi-M, a 6.77-billion-parameter model, designed for multimodal medical data generation, that, following Foundation Model paradigm, exploits contrastive learning and large quantity of data to build a shared latent space which capture the relationships between different data modalities. Further, we introduce the Multi-Prompt training technique, which significantly boosts MedCoDi-Mâ€™s generation under different settings. We extensively validate MedCoDi-M: first we benchmark it against five competitors on the MIMIC-CXR dataset, a state-of-the-art dataset for Chest X-ray and radiological report generation. Secondly, we perform a Visual Turing Test with expert radiologists to assess the realism and clinical relevance of the generated data, ensuring alignment with real-world scenarios. Finally, we assess the utility of MedCoDi-M in addressing key challenges in the medical field, such as anonymization, data scarcity and imbalance learning. The results are promising, demonstrating the applicability of MedCoDi-M in medical contexts. Project page is at <a target="_blank" rel="noopener" href="https://cosbidev.github.io/MedCoDi-M/">https://cosbidev.github.io/MedCoDi-M/</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½æ­£åœ¨æ¨åŠ¨åŒ»ç–—å®è·µçš„å˜é©ï¼Œæé«˜è¯Šæ–­å’ŒåŒ»ç–—ä¿å¥æœåŠ¡çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå…¶åœ¨åŒ»ç–—ç¯å¢ƒä¸­çš„åº”ç”¨ä»é¢ä¸´ä¸æ•°æ®å¯ç”¨æ€§å’Œéšç§çº¦æŸç›¸å…³çš„é‡å¤§æŒ‘æˆ˜ã€‚åˆæˆæ•°æ®ä½œä¸ºè§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå®ƒè§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼ŒåŒæ—¶ä¿æŠ¤äº†éšç§ã€‚æœ€è¿‘ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼‰ä½œä¸ºä¸€ç§ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®çš„å¼ºå¤§å·¥å…·è€Œå´­éœ²å¤´è§’ã€‚åŒæ—¶ï¼Œä¸åŒæ¨¡å¼çš„èåˆå¼•èµ·äº†äººä»¬çš„å…´è¶£ï¼Œå¼ºè°ƒäº†æ¨¡å‹éœ€è¦å¤„ç†å¤šæ¨¡å¼åŒ»ç–—æ•°æ®çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æ•´åˆè¡¥å……ä¿¡æ¯ï¼Œå¹¶ä¸”ç¼ºä¹åŒæ—¶ç”Ÿæˆæ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedCoDi-Mï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡å¼åŒ»ç–—æ•°æ®ç”Ÿæˆçš„6.77äº¿å‚æ•°æ¨¡å‹ã€‚å®ƒéµå¾ªåŸºç¡€æ¨¡å‹èŒƒå¼ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’Œå¤§é‡æ•°æ®æ¥æ„å»ºå…±äº«æ½œåœ¨ç©ºé—´ï¼Œè¯¥ç©ºé—´å¯ä»¥æ•è·ä¸åŒæ•°æ®æ¨¡å¼ä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæç¤ºè®­ç»ƒæŠ€æœ¯ï¼Œè¿™æ˜¾è‘—æé«˜äº†MedCoDi-Måœ¨ä¸åŒè®¾ç½®ä¸‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹MedCoDi-Mè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä¸äº”ç§ç«äº‰å¯¹æ‰‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºèƒ¸éƒ¨Xå…‰ç‰‡å’Œæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å…ˆè¿›æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿä¸€èµ·è¿›è¡Œè§†è§‰å›¾çµæµ‹è¯•ï¼Œä»¥è¯„ä¼°ç”Ÿæˆæ•°æ®çš„çœŸå®æ€§å’Œä¸´åºŠç›¸å…³æ€§ï¼Œç¡®ä¿å…¶ä¸çœŸå®åœºæ™¯çš„ä¸€è‡´æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†MedCoDi-Måœ¨è§£å†³åŒ»å­¦é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ï¼ˆå¦‚åŒ¿ååŒ–ã€æ•°æ®ç¨€ç¼ºå’Œä¸å¹³è¡¡å­¦ä¹ ï¼‰æ–¹é¢çš„å®ç”¨æ€§ã€‚ç»“æœä»¤äººé¼“èˆï¼Œè¯æ˜äº†MedCoDi-Måœ¨åŒ»ç–—ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢ä¸ºï¼š<a target="_blank" rel="noopener" href="https://cosbidev.github.io/MedCoDi-M/%E3%80%82">https://cosbidev.github.io/MedCoDi-M/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04614v1">PDF</a> </p>
<p><strong>Summary</strong><br>    äººå·¥æ™ºèƒ½æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜åŒ»ç–—å®è·µï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§å’ŒåŒ»ç–—æœåŠ¡æ°´å¹³ã€‚ç„¶è€Œï¼Œå…¶åœ¨åŒ»ç–—ç¯å¢ƒä¸­çš„åº”ç”¨ä»é¢ä¸´æ•°æ®å¯ç”¨æ€§å’Œéšç§çº¦æŸç­‰æŒ‘æˆ˜ã€‚åˆæˆæ•°æ®ä½œä¸ºè§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜çš„åŒæ—¶ä¿æŠ¤éšç§ã€‚æœ€è¿‘ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹å·²æˆä¸ºç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®çš„æœ‰åŠ›å·¥å…·ã€‚é’ˆå¯¹å¤šæ¨¡æ€åŒ»ç–—æ•°æ®çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†MedCoDi-Mï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€åŒ»ç–—æ•°æ®ç”Ÿæˆçš„6.77äº¿å‚æ•°æ¨¡å‹ï¼Œå®ƒé‡‡ç”¨åŸºç¡€æ¨¡å‹èŒƒå¼ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’Œå¤§é‡æ•°æ®æ„å»ºå…±äº«æ½œåœ¨ç©ºé—´ï¼Œæ•æ‰ä¸åŒæ•°æ®æ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¤šæç¤ºè®­ç»ƒæŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†MedCoDi-Måœ¨ä¸åŒè®¾ç½®ä¸‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç»è¿‡å¹¿æ³›éªŒè¯ï¼ŒMedCoDi-Måœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä¸äº”ç§ç«äº‰å¯¹æ‰‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚åŒæ—¶è¿›è¡Œäº†è§†è§‰å›¾çµæµ‹è¯•ï¼Œä»¥è¯„ä¼°ç”Ÿæˆæ•°æ®çš„çœŸå®æ€§å’Œä¸´åºŠç›¸å…³æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†MedCoDi-Måœ¨åŒ»ç–—é¢†åŸŸçš„å…³é”®æŒ‘æˆ˜ä¸­çš„å®ç”¨æ€§ï¼Œå¦‚åŒ¿ååŒ–ã€æ•°æ®ç¨€ç¼ºå’Œä¸å¹³è¡¡å­¦ä¹ ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMedCoDi-Måœ¨åŒ»ç–—ç¯å¢ƒä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—å®è·µä¸­å…·æœ‰æé«˜è¯Šæ–­å‡†ç¡®æ€§å’ŒåŒ»ç–—æœåŠ¡æ°´å¹³çš„æ½œåŠ›ã€‚</li>
<li>æ•°æ®å¯ç”¨æ€§å’Œéšç§çº¦æŸæ˜¯äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ç¯å¢ƒä¸­åº”ç”¨çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>åˆæˆæ•°æ®ä½œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œéšç§ä¿æŠ¤é—®é¢˜çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ¡ˆã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹æ˜¯ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®çš„æœ‰æ•ˆå·¥å…·ã€‚</li>
<li>MedCoDi-Mæ˜¯ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€åŒ»ç–—æ•°æ®ç”Ÿæˆçš„æ¨¡å‹ï¼Œå…·æœ‰6.77äº¿å‚æ•°ã€‚</li>
<li>MedCoDi-Måˆ©ç”¨å¯¹æ¯”å­¦ä¹ å’Œå¤§é‡æ•°æ®æ„å»ºå…±äº«æ½œåœ¨ç©ºé—´ï¼Œä»¥å¤„ç†ä¸åŒæ•°æ®æ¨¡æ€ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c1c73c9b8a49494e58568140f055a9a2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rethinking-High-speed-Image-Reconstruction-Framework-with-Spike-Camera"><a href="#Rethinking-High-speed-Image-Reconstruction-Framework-with-Spike-Camera" class="headerlink" title="Rethinking High-speed Image Reconstruction Framework with Spike Camera"></a>Rethinking High-speed Image Reconstruction Framework with Spike Camera</h2><p><strong>Authors:Kang Chen, Yajing Zheng, Tiejun Huang, Zhaofei Yu</strong></p>
<p>Spike cameras, as innovative neuromorphic devices, generate continuous spike streams to capture high-speed scenes with lower bandwidth and higher dynamic range than traditional RGB cameras. However, reconstructing high-quality images from the spike input under low-light conditions remains challenging. Conventional learning-based methods often rely on the synthetic dataset as the supervision for training. Still, these approaches falter when dealing with noisy spikes fired under the low-light environment, leading to further performance degradation in the real-world dataset. This phenomenon is primarily due to inadequate noise modelling and the domain gap between synthetic and real datasets, resulting in recovered images with unclear textures, excessive noise, and diminished brightness. To address these challenges, we introduce a novel spike-to-image reconstruction framework SpikeCLIP that goes beyond traditional training paradigms. Leveraging the CLIP modelâ€™s powerful capability to align text and images, we incorporate the textual description of the captured scene and unpaired high-quality datasets as the supervision. Our experiments on real-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP significantly enhances texture details and the luminance balance of recovered images. Furthermore, the reconstructed images are well-aligned with the broader visual features needed for downstream tasks, ensuring more robust and versatile performance in challenging environments. </p>
<blockquote>
<p>è„‰å†²ç›¸æœºä½œä¸ºä¸€ç§åˆ›æ–°çš„ç¥ç»å½¢æ€è®¾å¤‡ï¼Œèƒ½å¤Ÿäº§ç”Ÿè¿ç»­çš„è„‰å†²æµï¼Œä»¥ä½äºä¼ ç»ŸRGBç›¸æœºçš„å¸¦å®½å’Œæ›´é«˜çš„åŠ¨æ€èŒƒå›´æ¥æ•æ‰é«˜é€Ÿåœºæ™¯ã€‚ç„¶è€Œï¼Œåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹ä»è„‰å†²è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å­¦ä¹ å‹æ–¹æ³•é€šå¸¸ä¾èµ–äºåˆæˆæ•°æ®é›†ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›‘ç£ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†ä½å…‰ç¯å¢ƒä¸‹äº§ç”Ÿçš„å™ªå£°è„‰å†²æ—¶å¸¸å¸¸å‡ºç°é—®é¢˜ï¼Œå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¿›ä¸€æ­¥ä¸‹é™ã€‚è¿™ä¸€ç°è±¡çš„ä¸»è¦åŸå› æ˜¯å™ªå£°å»ºæ¨¡ä¸è¶³ä»¥åŠåˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œå¯¼è‡´æ¢å¤å‡ºçš„å›¾åƒçº¹ç†ä¸æ¸…æ™°ã€å™ªå£°è¿‡å¤šä»¥åŠäº®åº¦é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„è„‰å†²åˆ°å›¾åƒé‡å»ºæ¡†æ¶SpikeCLIPï¼Œå®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„è®­ç»ƒæ¨¡å¼ã€‚æˆ‘ä»¬å€ŸåŠ©CLIPæ¨¡å‹çš„å¼ºå¤§æ–‡æœ¬å’Œå›¾åƒå¯¹é½èƒ½åŠ›ï¼Œå°†æ•è·åœºæ™¯çš„æ–‡å­—æè¿°å’Œæœªé…å¯¹çš„é«˜è´¨é‡æ•°æ®é›†ä½œä¸ºç›‘ç£ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„ä½å…‰æ•°æ®é›†U-CALTECHå’ŒU-CIFARä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpikeCLIPæ˜¾è‘—æé«˜äº†æ¢å¤å›¾åƒçš„çº¹ç†ç»†èŠ‚å’Œäº®åº¦å¹³è¡¡ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„æ›´å¹¿æ³›çš„è§†è§‰ç‰¹å¾ç›¸å»åˆï¼Œç¡®ä¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­å…·æœ‰æ›´ç¨³å¥å’Œå¤šåŠŸèƒ½æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04477v1">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºç¥ç»å½¢æ€è®¾å¤‡Spikeæ‘„åƒå¤´çš„æ–‡ç« ã€‚Spikeæ‘„åƒå¤´é€šè¿‡ç”Ÿæˆè¿ç»­è„‰å†²æµæ•æ‰é«˜é€Ÿåœºæ™¯ï¼Œç›¸è¾ƒäºä¼ ç»ŸRGBæ‘„åƒå¤´æ‹¥æœ‰æ›´ä½çš„å¸¦å®½å’Œæ›´é«˜çš„åŠ¨æ€èŒƒå›´ã€‚ç„¶è€Œï¼Œåœ¨ä½å…‰ç…§æ¡ä»¶ä¸‹ä»è„‰å†²è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å½“å‰å­¦ä¹ æ³•æ–¹æ³•ä¾èµ–äºåˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨å¤„ç†ä½å…‰ç…§ç¯å¢ƒä¸‹äº§ç”Ÿçš„å™ªå£°è„‰å†²æ—¶è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´åœ¨å®é™…æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è„‰å†²åˆ°å›¾åƒé‡å»ºæ¡†æ¶SpikeCLIPï¼Œå®ƒè¶…è¶Šäº†ä¼ ç»Ÿè®­ç»ƒæ¨¡å¼ã€‚SpikeCLIPå€ŸåŠ©CLIPæ¨¡å‹çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½èƒ½åŠ›ï¼Œå°†åœºæ™¯æ–‡æœ¬æè¿°å’Œæœªé…å¯¹çš„é«˜è´¨é‡æ•°æ®é›†ä½œä¸ºç›‘ç£ã€‚å®éªŒè¡¨æ˜ï¼ŒSpikeCLIPåœ¨çœŸå®ä¸–ç•Œçš„ä½å…‰ç…§æ•°æ®é›†U-CALTECHå’ŒU-CIFARä¸Šæ˜¾è‘—æé«˜äº†å›¾åƒçš„çº¹ç†ç»†èŠ‚å’Œäº®åº¦å¹³è¡¡ã€‚æ­¤å¤–ï¼Œé‡å»ºçš„å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„æ›´å¹¿æ³›çš„è§†è§‰ç‰¹å¾å¯¹é½ï¼Œç¡®ä¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­è¡¨ç°å¾—æ›´ç¨³å¥å’Œé€šç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spikeæ‘„åƒå¤´æ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»å½¢æ€è®¾å¤‡ï¼Œå¯é€šè¿‡è¿ç»­è„‰å†²æµæ•æ‰é«˜é€Ÿåœºæ™¯ï¼Œç›¸è¾ƒäºä¼ ç»ŸRGBæ‘„åƒå¤´å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ä½å…‰ç…§æ¡ä»¶ä¸‹ä»è„‰å†²è¾“å…¥é‡å»ºé«˜è´¨é‡å›¾åƒæ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰å­¦ä¹ æ³•æ–¹æ³•åœ¨å¤„ç†ä½å…‰ç…§ç¯å¢ƒä¸‹çš„å™ªå£°è„‰å†²æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯å™ªå£°å»ºæ¨¡ä¸è¶³ä»¥åŠåˆæˆå’ŒçœŸå®æ•°æ®é›†ä¹‹é—´çš„åŸŸå·®è·ã€‚</li>
<li>SpikeCLIPæ˜¯ä¸€ç§æ–°å‹çš„è„‰å†²åˆ°å›¾åƒé‡å»ºæ¡†æ¶ï¼Œå€ŸåŠ©CLIPæ¨¡å‹çš„æ–‡æœ¬ä¸å›¾åƒå¯¹é½èƒ½åŠ›æ¥æé«˜å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>SpikeCLIPåˆ©ç”¨åœºæ™¯æ–‡æœ¬æè¿°å’Œæœªé…å¯¹çš„é«˜è´¨é‡æ•°æ®é›†ä½œä¸ºç›‘ç£ï¼Œæœ‰æ•ˆæé«˜äº†åœ¨çœŸå®ä¸–ç•Œçš„ä½å…‰ç…§æ•°æ®é›†ä¸Šçš„å›¾åƒçº¹ç†ç»†èŠ‚å’Œäº®åº¦å¹³è¡¡ã€‚</li>
<li>SpikeCLIPé‡å»ºçš„å›¾åƒä¸ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„è§†è§‰ç‰¹å¾å¯¹é½ï¼Œè¡¨ç°å¾—æ›´ç¨³å¥å’Œé€šç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-236e8fead9abcbe3c56b2e5da8e3ec43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a10e4159d1b7fa4bbfea0d90e79d465.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83d8f95fe9a364f186a41fc9477dff9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee32d7c171f4e38e519776b99054cd60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9cf581a2d621a01c5d1d815111789cd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Unified-Framework-for-Foreground-and-Anonymization-Area-Segmentation-in-CT-and-MRI-Data"><a href="#A-Unified-Framework-for-Foreground-and-Anonymization-Area-Segmentation-in-CT-and-MRI-Data" class="headerlink" title="A Unified Framework for Foreground and Anonymization Area Segmentation   in CT and MRI Data"></a>A Unified Framework for Foreground and Anonymization Area Segmentation   in CT and MRI Data</h2><p><strong>Authors:Michal Nohel, Constantin Ulrich, Jonathan Suprijadi, Tassilo Wald, Klaus Maier-Hein</strong></p>
<p>This study presents an open-source toolkit to address critical challenges in preprocessing data for self-supervised learning (SSL) for 3D medical imaging, focusing on data privacy and computational efficiency. The toolkit comprises two main components: a segmentation network that delineates foreground regions to optimize data sampling and thus reduce training time, and a segmentation network that identifies anonymized regions, preventing erroneous supervision in reconstruction-based SSL methods. Experimental results demonstrate high robustness, with mean Dice scores exceeding 98.5 across all anonymization methods and surpassing 99.5 for foreground segmentation tasks, highlighting the efficacy of the toolkit in supporting SSL applications in 3D medical imaging for both CT and MRI images. The weights and code is available at <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/Foreground-and-Anonymization-Area-Segmentation">https://github.com/MIC-DKFZ/Foreground-and-Anonymization-Area-Segmentation</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¼€æºå·¥å…·åŒ…ï¼Œæ—¨åœ¨è§£å†³3DåŒ»å­¦å½±åƒè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ•°æ®é¢„å¤„ç†ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œé‡ç‚¹å…³æ³¨æ•°æ®éšç§å’Œè®¡ç®—æ•ˆç‡ã€‚è¯¥å·¥å…·åŒ…ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªç»„ä»¶ï¼šä¸€ä¸ªåˆ†å‰²ç½‘ç»œï¼Œç”¨äºåˆ’å®šå‰æ™¯åŒºåŸŸä»¥ä¼˜åŒ–æ•°æ®é‡‡æ ·ï¼Œä»è€Œå‡å°‘è®­ç»ƒæ—¶é—´ï¼›å¦ä¸€ä¸ªåˆ†å‰²ç½‘ç»œåˆ™ç”¨äºè¯†åˆ«åŒ¿ååŒºåŸŸï¼Œé˜²æ­¢åŸºäºé‡å»ºçš„SSLæ–¹æ³•ä¸­å‘ç”Ÿé”™è¯¯ç›‘ç£ã€‚å®éªŒç»“æœè¡¨ç°å‡ºé«˜ç¨³å¥æ€§ï¼Œæ‰€æœ‰åŒ¿ååŒ–æ–¹æ³•çš„å¹³å‡Diceå¾—åˆ†è¶…è¿‡98.5%ï¼Œå‰æ™¯åˆ†å‰²ä»»åŠ¡çš„å¾—åˆ†è¶…è¿‡99.5%ï¼Œçªæ˜¾äº†è¯¥å·¥å…·åŒ…åœ¨æ”¯æŒCTå’ŒMRIå›¾åƒçš„3DåŒ»å­¦å›¾åƒSSLåº”ç”¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³æƒé‡å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/Foreground-and-Anonymization-Area-Segmentation%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MIC-DKFZ/Foreground-and-Anonymization-Area-Segmentationè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04361v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¼€æºå·¥å…·åŒ…ï¼Œè¯¥å·¥å…·åŒ…é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å½±åƒè‡ªç›‘ç£å­¦ä¹ ä¸­æ•°æ®é¢„å¤„ç†é˜¶æ®µçš„é‡éš¾ç‚¹é—®é¢˜è¿›è¡Œäº†å…¨é¢è§£å†³ï¼Œç‰¹åˆ«èšç„¦äºæ•°æ®éšç§å’Œè®¡ç®—æ•ˆç‡ä¸¤å¤§æŒ‘æˆ˜ã€‚è¯¥å·¥å…·åŒ…åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œåˆ†åˆ«ç”¨äºå‰æ™¯åŒºåŸŸåˆ†å‰²å’ŒåŒ¿ååŒºåŸŸè¯†åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥å·¥å…·åŒ…åœ¨æ”¯æŒä¸‰ç»´åŒ»å­¦å½±åƒè‡ªç›‘ç£å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºé«˜é²æ£’æ€§ï¼Œå‰æ™¯åˆ†å‰²ä»»åŠ¡çš„Diceç³»æ•°è¶…è¿‡99.5%ï¼Œæ‰€æœ‰åŒ¿ååŒ–æ–¹æ³•çš„Diceç³»æ•°å‡è¶…è¿‡98.5%ã€‚æœ‰å…³æƒé‡å’Œä»£ç å¯è®¿é—®é“¾æ¥ <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/Foreground-and-Anonymization-Area-Segmentation">https://github.com/MIC-DKFZ/Foreground-and-Anonymization-Area-Segmentation</a> äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è§£å†³ä¸‰ç»´åŒ»å­¦å½±åƒè‡ªç›‘ç£å­¦ä¹ æ•°æ®é¢„å¤„ç†é˜¶æ®µçš„æŒ‘æˆ˜ã€‚</li>
<li>å·¥å…·åŒ…åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå‰æ™¯åŒºåŸŸåˆ†å‰²ç½‘ç»œå’ŒåŒ¿ååŒºåŸŸè¯†åˆ«ç½‘ç»œã€‚</li>
<li>å‰æ™¯åŒºåŸŸåˆ†å‰²å¯ä¼˜åŒ–æ•°æ®é‡‡æ ·ï¼Œç¼©çŸ­è®­ç»ƒæ—¶é—´ã€‚</li>
<li>åŒ¿ååŒºåŸŸè¯†åˆ«å¯é˜²æ­¢é‡å»ºå¼è‡ªç›‘ç£å­¦ä¹ ä¸­çš„é”™è¯¯ç›‘ç£ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºå‡ºé«˜é²æ£’æ€§ï¼Œå‰æ™¯åˆ†å‰²ä»»åŠ¡çš„Diceç³»æ•°è¶…è¿‡99.5%ã€‚</li>
<li>æ‰€æœ‰åŒ¿ååŒ–æ–¹æ³•çš„Diceç³»æ•°å‡è¶…è¿‡98.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-957e9ba2adb8ba8a1a038e5e9cc5efc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6adbabaf5982bf244071c5b907bcbd64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29bbe6089a24765cf36b1104d50e7bc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f5d92206f5416b87aa1c619bb280810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-521e213b401789bce313e4d39fa0f09e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAD-UV-The-1st-INTERSPEECH-Mice-Autism-Detection-via-Ultrasound-Vocalization-Challenge"><a href="#MAD-UV-The-1st-INTERSPEECH-Mice-Autism-Detection-via-Ultrasound-Vocalization-Challenge" class="headerlink" title="MAD-UV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound   Vocalization Challenge"></a>MAD-UV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound   Vocalization Challenge</h2><p><strong>Authors:Zijiang Yang, Meishu Song, Xin Jing, Haojie Zhang, Kun Qian, Bin Hu, Kota Tamada, Toru Takumi, BjÃ¶rn W. Schuller, Yoshiharu Yamamoto</strong></p>
<p>The Mice Autism Detection via Ultrasound Vocalization (MAD-UV) Challenge introduces the first INTERSPEECH challenge focused on detecting autism spectrum disorder (ASD) in mice through their vocalizations. Participants are tasked with developing models to automatically classify mice as either wild-type or ASD models based on recordings with a high sampling rate. Our baseline system employs a simple CNN-based classification using three different spectrogram features. Results demonstrate the feasibility of automated ASD detection, with the considered audible-range features achieving the best performance (UAR of 0.600 for segment-level and 0.625 for subject-level classification). This challenge bridges speech technology and biomedical research, offering opportunities to advance our understanding of ASD models through machine learning approaches. The findings suggest promising directions for vocalization analysis and highlight the potential value of audible and ultrasound vocalizations in ASD detection. </p>
<blockquote>
<p>é€šè¿‡è¶…å£°å‘å£°ï¼ˆMAD-UVï¼‰æŒ‘æˆ˜æ£€æµ‹å°é¼ è‡ªé—­ç—‡ä»‹ç»äº†é¦–ä¸ªINTERSPEECHæŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜çš„é‡ç‚¹æ˜¯é€šè¿‡å°é¼ çš„é¸£å«æ£€æµ‹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰ã€‚å‚èµ›è€…çš„ä»»åŠ¡æ˜¯å¼€å‘æ¨¡å‹ï¼Œæ ¹æ®é«˜é‡‡æ ·ç‡å½•éŸ³è‡ªåŠ¨å°†å°é¼ åˆ†ç±»ä¸ºé‡ç”Ÿå‹æˆ–ASDæ¨¡å‹ã€‚æˆ‘ä»¬çš„åŸºçº¿ç³»ç»Ÿé‡‡ç”¨åŸºäºç®€å•CNNçš„åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç§ä¸åŒçš„é¢‘è°±ç‰¹å¾ã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨åŒ–ASDæ£€æµ‹æ˜¯å¯è¡Œçš„ï¼Œæ‰€è€ƒè™‘çš„éŸ³é¢‘èŒƒå›´ç‰¹å¾å®ç°äº†æœ€ä½³æ€§èƒ½ï¼ˆåˆ†æ®µçº§UARä¸º0.600ï¼Œä¸»ä½“çº§åˆ†ç±»ä¸º0.625ï¼‰ã€‚è¯¥æŒ‘æˆ˜æ¶èµ·äº†è¯­éŸ³æŠ€æœ¯ä¸ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¹‹é—´çš„æ¡¥æ¢ï¼Œé€šè¿‡æœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¨è¿›æˆ‘ä»¬å¯¹ASDæ¨¡å‹çš„ç†è§£ã€‚ç ”ç©¶ç»“æœä¸ºå‘å£°åˆ†ææä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¹¶çªå‡ºäº†å¯å¬å£°å’Œè¶…å£°æ³¢å‘å£°åœ¨ASDæ£€æµ‹ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04292v1">PDF</a> 5 pages, 1 figure and 2 tables. For MAD-UV Challenge 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åˆ©ç”¨è¶…å£°å‘å£°ï¼ˆUltrasound Vocalizationï¼‰æŠ€æœ¯æ£€æµ‹å°é¼ è‡ªé—­ç—‡ï¼ˆAutism Spectrum Disorderï¼ŒASDï¼‰çš„æŒ‘æˆ˜ã€‚æŒ‘æˆ˜è¦æ±‚å‚ä¸è€…å¼€å‘æ¨¡å‹ï¼Œæ ¹æ®é«˜é‡‡æ ·ç‡çš„å½•éŸ³è‡ªåŠ¨å°†å°é¼ åˆ†ç±»ä¸ºé‡ç”Ÿå‹æˆ–ASDæ¨¡å‹ã€‚åŸºçº¿ç³»ç»Ÿé‡‡ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç®€å•åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç§ä¸åŒçš„é¢‘è°±ç‰¹å¾ã€‚ç»“æœè¯æ˜äº†è‡ªåŠ¨åŒ–æ£€æµ‹ASDçš„å¯è¡Œæ€§ï¼Œå…¶ä¸­è€ƒè™‘çš„éŸ³é¢‘èŒƒå›´ç‰¹å¾è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°åˆ†æ®µçº§åˆ«å’Œç”¨æˆ·çº§åˆ«çš„è¯†åˆ«ç‡åˆ†åˆ«ä¸º0.6å’Œ0.625ã€‚è¯¥æŒ‘æˆ˜å°†è¯­éŸ³æŠ€æœ¯ä¸ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç›¸ç»“åˆï¼Œä¸ºé€šè¿‡æœºå™¨å­¦ä¹ æ‰‹æ®µäº†è§£ASDæ¨¡å‹æä¾›äº†æœºä¼šï¼ŒåŒæ—¶æ­ç¤ºäº†å‘å£°åˆ†æçš„æ½œåœ¨æ–¹å‘ä»¥åŠå£°éŸ³å’Œè¶…å£°åœ¨ASDæ£€æµ‹ä¸­çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAD-UVæŒ‘æˆ˜èšç„¦äºé€šè¿‡å°é¼ çš„å‘å£°æ¥æ£€æµ‹è‡ªé—­ç—‡è°±ç³»éšœç¢ï¼ˆASDï¼‰ã€‚</li>
<li>æŒ‘æˆ˜è¦æ±‚å‚ä¸è€…å¼€å‘æ¨¡å‹ï¼Œæ ¹æ®é«˜é‡‡æ ·ç‡çš„å½•éŸ³è‡ªåŠ¨åˆ†ç±»å°é¼ ã€‚</li>
<li>åŸºçº¿ç³»ç»Ÿé‡‡ç”¨CNNåˆ†ç±»æ–¹æ³•ï¼Œå¹¶ç»“åˆä¸‰ç§ä¸åŒçš„é¢‘è°±ç‰¹å¾ã€‚</li>
<li>éŸ³é¢‘èŒƒå›´ç‰¹å¾åœ¨è‡ªåŠ¨åŒ–ASDæ£€æµ‹ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>è¯¥æŒ‘æˆ˜å°†è¯­éŸ³æŠ€æœ¯ä¸ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç›¸ç»“åˆï¼Œä¸ºç†è§£ASDæä¾›äº†æ–°çš„è§†è§’ã€‚</li>
<li>ç»“æœæ­ç¤ºäº†å‘å£°åˆ†æåœ¨ASDæ£€æµ‹ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60ee798f15faf63e5fd811ca4c9ce1e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90c59ac206dd62e219a6fa897f7e2b89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45a2a4f8d9c150e993f6e5f2698cb5ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bb11f355597c43dc9093b1c637e223e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Continual-Self-supervised-Learning-Considering-Medical-Domain-Knowledge-in-Chest-CT-Images"><a href="#Continual-Self-supervised-Learning-Considering-Medical-Domain-Knowledge-in-Chest-CT-Images" class="headerlink" title="Continual Self-supervised Learning Considering Medical Domain Knowledge   in Chest CT Images"></a>Continual Self-supervised Learning Considering Medical Domain Knowledge   in Chest CT Images</h2><p><strong>Authors:Ren Tasai, Guang Li, Ren Togo, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Kenji Hirata, Takahiro Ogawa, Kohsuke Kudo, Miki Haseyama</strong></p>
<p>We propose a novel continual self-supervised learning method (CSSL) considering medical domain knowledge in chest CT images. Our approach addresses the challenge of sequential learning by effectively capturing the relationship between previously learned knowledge and new information at different stages. By incorporating an enhanced DER into CSSL and maintaining both diversity and representativeness within the rehearsal buffer of DER, the risk of data interference during pretraining is reduced, enabling the model to learn more richer and robust feature representations. In addition, we incorporate a mixup strategy and feature distillation to further enhance the modelâ€™s ability to learn meaningful representations. We validate our method using chest CT images obtained under two different imaging conditions, demonstrating superior performance compared to state-of-the-art methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è€ƒè™‘åŒ»å­¦é¢†åŸŸçŸ¥è¯†çš„è¿ç»­è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆCSSLï¼‰ï¼Œåº”ç”¨äºèƒ¸éƒ¨CTå›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æœ‰æ•ˆæ•æ‰ä¸åŒé˜¶æ®µçš„å…ˆå‰å­¦ä¹ çŸ¥è¯†å’Œæ–°ä¿¡æ¯ä¹‹é—´çš„å…³ç³»æ¥è§£å†³é¡ºåºå­¦ä¹ çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†å¢å¼ºçš„DERèå…¥CSSLå¹¶ä¿æŒDERå¤ä¹ ç¼“å†²åŒºå†…çš„å¤šæ ·æ€§å’Œä»£è¡¨æ€§ï¼Œé™ä½äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®å¹²æ‰°é£é™©ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ä¸°å¯Œå’Œç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†æ··åˆç­–ç•¥å’Œç‰¹å¾è’¸é¦ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å­¦ä¹ æœ‰æ„ä¹‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§ä¸åŒæˆåƒæ¡ä»¶ä¸‹è·å¾—çš„èƒ¸éƒ¨CTå›¾åƒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå±•ç°äº†ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯çš„å‰æ²¿æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04217v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è€ƒè™‘åŒ»å­¦é¢†åŸŸçŸ¥è¯†çš„æŒç»­è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆCSSLï¼‰ï¼Œåº”ç”¨äºèƒ¸éƒ¨CTå›¾åƒã€‚è¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆæ•æ‰å…ˆå‰çŸ¥è¯†ä¸æ–°ä¿¡æ¯åœ¨ä¸åŒé˜¶æ®µä¹‹é—´çš„å…³ç³»ï¼Œè§£å†³äº†é¡ºåºå­¦ä¹ çš„æŒ‘æˆ˜ã€‚é€šè¿‡å°†å¢å¼ºå‹DERèå…¥CSSLï¼Œå¹¶åœ¨DERçš„æ¼”ç»ƒç¼“å†²åŒºä¸­ä¿æŒå¤šæ ·æ€§å’Œä»£è¡¨æ€§ï¼Œé™ä½äº†é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®å¹²æ‰°é£é™©ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ä¸°å¯Œå’Œç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†mixupç­–ç•¥å’Œç‰¹å¾è’¸é¦ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹å­¦ä¹ æœ‰æ„ä¹‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚ä½¿ç”¨ä¸¤ç§ä¸åŒæˆåƒæ¡ä»¶ä¸‹è·å¾—çš„èƒ¸éƒ¨CTå›¾åƒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†ç»“åˆåŒ»å­¦é¢†åŸŸçŸ¥è¯†çš„æŒç»­è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆCSSLï¼‰ï¼Œä¸“æ³¨äºèƒ¸éƒ¨CTå›¾åƒã€‚</li>
<li>é€šè¿‡æœ‰æ•ˆæ•æ‰å…ˆå‰çŸ¥è¯†ä¸æ–°ä¿¡æ¯çš„å…³ç³»ï¼Œè§£å†³é¡ºåºå­¦ä¹ çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¢å¼ºå‹DERå’Œæ¼”ç»ƒç¼“å†²åŒºå†…çš„å¤šæ ·æ€§ä¸ä»£è¡¨æ€§ï¼Œé™ä½æ•°æ®å¹²æ‰°é£é™©ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ä¸°å¯Œå’Œç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨äº†mixupç­–ç•¥å’Œç‰¹å¾è’¸é¦ï¼Œå¢å¼ºæ¨¡å‹å­¦ä¹ æœ‰æ„ä¹‰è¡¨ç¤ºçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸¤ç§ä¸åŒæˆåƒæ¡ä»¶ä¸‹çš„èƒ¸éƒ¨CTå›¾åƒä¸ŠéªŒè¯äº†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0cb4d2f119de338f1463f736016125e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303be51a6e363aea228e0847263b1ace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-280f6f73a0a07d5e0806826d885d1b8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0323fb506c912af3790f20d81305c25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea6ad992924dfd253dbfec48ca25b82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5958d1dfb894aaec3e5cb9761a71739.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="eRO-ExTra-eROSITA-extragalactic-non-AGN-X-ray-transients-and-variables-in-eRASS1-and-eRASS2"><a href="#eRO-ExTra-eROSITA-extragalactic-non-AGN-X-ray-transients-and-variables-in-eRASS1-and-eRASS2" class="headerlink" title="eRO-ExTra: eROSITA extragalactic non-AGN X-ray transients and variables   in eRASS1 and eRASS2"></a>eRO-ExTra: eROSITA extragalactic non-AGN X-ray transients and variables   in eRASS1 and eRASS2</h2><p><strong>Authors:Iuliia Grotova, Arne Rau, Mara Salvato, Johannes Buchner, Adelle J. Goodwin, Zhu Liu, Adam Malyali, Andrea Merloni, DusÃ¡n TubÃ­n-Arenas, David Homan, Mirko Krumpe, Kirpal Nandra, Gemma E. Anderson, Riccardo Arcodia, Sabina Bahic, Pietro Baldini, David A. H. Buckley, Stefano Ciroi, Adela Kawka, Megan Masterson, James C. A. Miller-Jones, Francesco Di Mille</strong></p>
<p>(Abridged) While previous X-ray studies showed the dominance of regular active galactic nuclei (AGN) variability, a small fraction of sources arise from more exotic phenomena such as tidal disruption events (TDEs), quasi-periodic eruptions, or other short-lived events associated with supermassive black hole accretion. This paper describes the systematic selection of X-ray extragalactic transients found in the first two eROSITA all-sky surveys (eRASS) that are not associated with known AGN prior to eROSITA observations. We generated a variability sample from eRASS1 and eRASS2 (Dec. 2019-Dec. 2020), which includes sources with a variability significance and a fractional amplitude larger than four, located in the Legacy Survey DR10 (LS10) footprint. The properties of LS10 counterparts were used to exclude stars and known AGN. The sample was additionally cleaned using pre-eROSITA classifications, archival optical spectra, and archival X-ray data. The final catalog eRO-ExTra includes 304 extragalactic eROSITA transients and variables not associated with known AGN. More than 90% of sources have reliable LS10 optical counterparts. For each source, we provide archival X-ray data from Swift, ROSAT, and XMM-Newton; the eROSITA long-term light curve (2-2.5 years) with a light curve classification; as well as the best power law fit spectral results at the peak eROSITA epoch. Reliable spectroscopic and photometric redshifts are provided for more than 80% of the sample. Several sources in the catalog are known TDE candidates discovered by eROSITA. In addition, 31 sources are radio detected. The eRO-ExTra transients constitute a relatively clean parent sample of non-AGN variability phenomena associated with massive black holes. More than 95% of eRO-ExTra sources were discovered in X-rays with eROSITA for the first time, which makes it a valuable resource for studying unique nuclear transients. </p>
<blockquote>
<p>ï¼ˆæ‘˜è¦ï¼‰è™½ç„¶ä¹‹å‰çš„Xå°„çº¿ç ”ç©¶è¡¨æ˜ï¼Œè§„åˆ™æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰çš„å˜å¼‚æ€§å ä¸»å¯¼åœ°ä½ï¼Œä½†ä¸€å°éƒ¨åˆ†æºæ¥è‡ªäºæ½®æ±æ’•è£‚äº‹ä»¶ï¼ˆTDEsï¼‰ã€å‡†å‘¨æœŸå–·å‘æˆ–å…¶ä»–ä¸è¶…å¤§è´¨é‡é»‘æ´å¸ç§¯ç›¸å…³çš„çŸ­æš‚äº‹ä»¶ç­‰æ›´å¥‡ç‰¹çš„ç°è±¡ã€‚æœ¬æ–‡æè¿°äº†eROSITAå‰ä¸¤æ¬¡å…¨å¤©ç©ºè°ƒæŸ¥ï¼ˆeRASSï¼‰ä¸­å‘ç°çš„Xå°„çº¿å¤–æ˜Ÿæš‚æ€ç³»ç»Ÿçš„é€‰æ‹©è¿‡ç¨‹ï¼Œè¿™äº›æš‚æ€ä¸å·²çŸ¥çš„AGNæ— å…³ã€‚æˆ‘ä»¬ä»eRASS1å’ŒeRASS2ï¼ˆ2019å¹´12æœˆè‡³2020å¹´12æœˆï¼‰ç”Ÿæˆäº†ä¸€ä¸ªå˜åŒ–æ ·æœ¬ï¼Œå…¶ä¸­åŒ…æ‹¬ä½äºé—äº§è°ƒæŸ¥DR10ï¼ˆLS10ï¼‰è¶³è¿¹å†…çš„å˜å¼‚æ€§æ˜¾è‘—ä¸”åˆ†æ•°å¹…åº¦å¤§äºå››çš„æºã€‚ä½¿ç”¨LS10å¯¹åº”ä½“çš„å±æ€§æ¥æ’é™¤æ’æ˜Ÿå’Œå·²çŸ¥AGNã€‚è¿˜ä½¿ç”¨é¢„eROSITAåˆ†ç±»ã€æ¡£æ¡ˆå…‰å­¦å…‰è°±å’Œæ¡£æ¡ˆXå°„çº¿æ•°æ®å¯¹æ ·æœ¬è¿›è¡Œäº†æ¸…ç†ã€‚æœ€ç»ˆçš„ç›®å½•eRO-ExTraåŒ…å«äº†304ä¸ªä¸å·²çŸ¥AGNæ— å…³çš„æ˜Ÿå¤–eROSITAæš‚æ€å’Œå˜é‡ã€‚è¶…è¿‡90%çš„æºæœ‰å¯é çš„LS10å…‰å­¦å¯¹åº”ä½“ã€‚å¯¹äºæ¯ä¸ªæºï¼Œæˆ‘ä»¬æä¾›äº†æ¥è‡ªSwiftã€ROSATå’ŒXMM-Newtonçš„æ¡£æ¡ˆXå°„çº¿æ•°æ®ï¼›eROSITAé•¿æœŸå…‰å˜æ›²çº¿ï¼ˆ2-2.5å¹´ï¼‰åŠå…¶å…‰å˜æ›²çº¿åˆ†ç±»ï¼›ä»¥åŠåœ¨eROSITAå³°å€¼æ—¶æœŸçš„æœ€ä½³å¹‚å¾‹æ‹Ÿåˆå…‰è°±ç»“æœã€‚è¶…è¿‡80%çš„æ ·æœ¬æä¾›äº†å¯é çš„å…‰è°±å’Œçº¢ç§»æµ‹å®šã€‚ç›®å½•ä¸­æœ‰å‡ ä¸ªæºæ˜¯eROSITAå‘ç°çš„å·²çŸ¥TDEå€™é€‰è€…ã€‚æ­¤å¤–ï¼Œæœ‰31ä¸ªæºè¢«æ£€æµ‹åˆ°æ— çº¿ç”µä¿¡å·ã€‚eRO-ExTraæš‚æ€æ˜¯ä¸å¤§è§„æ¨¡é»‘æ´ç›¸å…³çš„éAGNå˜ç°è±¡ç›¸å¯¹å¹²å‡€çš„æ¯ä½“æ ·æœ¬ã€‚è¶…è¿‡95%çš„eRO-ExTraæºæ˜¯é¦–æ¬¡ç”¨eROSITAåœ¨Xå°„çº¿ä¸Šå‘ç°çš„ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºç ”ç©¶ç‹¬ç‰¹æ ¸æš‚æ€çš„å®è´µèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04208v1">PDF</a> 18 pages, 15 figures, published in A&amp;A. To download the eRO-ExTra   catalog, see <a target="_blank" rel="noopener" href="https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/693/A62">https://cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/693/A62</a></p>
<p><strong>Summary</strong><br>    eROSITAé¦–æ¬¡å…¨å¤©ç©ºè°ƒæŸ¥ä¸­å‘ç°äº†ä¸€æ‰¹æ–°çš„éæ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆéAGNï¼‰çš„å¯å˜æºï¼ŒåŒ…æ‹¬æ½®æ±æ’•è£‚äº‹ä»¶ï¼ˆTDEsï¼‰ç­‰ä¸è¶…å¤§è´¨é‡é»‘æ´ç›¸å…³çš„ç½•è§ç°è±¡ã€‚æ ·æœ¬ç»è¿‡ä¸¥æ ¼ç­›é€‰ï¼ŒåŒ…å«LS10å…‰å­¦å¯¹åº”ä½“çš„è¶…è¿‡ä¹æˆæ¥æºå…·æœ‰å¯é æ€§ã€‚æ­¤ç ”ç©¶æä¾›äº†è¿™äº›æºçš„é•¿æœŸå…‰å˜æ›²çº¿ã€å…‰è°±ç»“æœåŠçº¢ç§»ä¿¡æ¯ï¼Œä¸ºæ·±å…¥ç ”ç©¶éæ´»è·ƒæ˜Ÿç³»æ ¸å¯å˜ç°è±¡æä¾›äº†å®è´µçš„èµ„æºã€‚å¤§éƒ¨åˆ†æ¥æºå°šæœªè¢«ç½—è¥¿é˜¿ç‰¹å°”æˆåƒå’Œå…‰æ³¢å¤©æ–‡å­¦ç ”ç©¶æ‰€ç­‰æœºæ„æ¢æµ‹åˆ°ï¼Œæ•…æˆä¸ºç ”ç©¶ç‹¬ç‰¹æ ¸ç¬å˜ç°è±¡çš„å®è´µèµ„æºã€‚è¿™ä¸€å‘ç°åœ¨æ¨åŠ¨Xå°„çº¿ç ”ç©¶é¢†åŸŸçš„å‘å±•å’Œè¿›ä¸€æ­¥äº†è§£å®‡å®™ä¸­ä¸æ´»è·ƒæ ¸æ´»åŠ¨çš„å¤šæ ·æ€§ä¸Šå…·æœ‰é‡è¦åœ°ä½ã€‚åŒæ—¶ä»‹ç»äº†ä½¿ç”¨æœ€ç†æƒ³çš„å¹‚å¾‹æ‹Ÿåˆå…‰è°±æ–¹æ³•å¤„ç†è°±æ•°æ®å’Œæ•æ‰æ ¸å¿ƒç²¾åéƒ¨åˆ†çš„æ¡ˆä¾‹åˆ†æè¿‡ç¨‹åŠæœ‰æ•ˆæˆæœçš„æ¢³ç†ç»“æœä½œä¸ºä¸ªæ¡ˆäºˆä»¥æŠ¥å‘Šæ€»ç»“äº†ç›¸å…³æˆæœå’Œé‡è¦æ€§ã€‚åŒæ—¶æä¾›äº†å…³äºæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œæ£€æµ‹æ•°æ®ç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†æ ·æœ¬ä¸­æŸäº›å·²çŸ¥æ½®æ±æ’•è£‚äº‹ä»¶å€™é€‰æºçš„ç‰¹å¾å’Œè¡¨ç°ã€‚æ€»ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é‡è¦ä»·å€¼çš„å‘ç°å’Œç ”ç©¶é¢†åŸŸï¼Œä¸ºç ”ç©¶å®‡å®™ä¸­ç‹¬ç‰¹ç°è±¡æä¾›äº†é‡è¦ä¾æ®ã€‚è¿™ä¸€ç ”ç©¶å°†å¤§å¤§æœ‰åŠ©äºæ¨è¿›å¯¹å®‡å®™ä¸­çš„ç½•è§äº‹ä»¶å’ŒæœªçŸ¥é¢†åŸŸçš„æ¢ç´¢ã€‚æ­¤é¡¹ç ”ç©¶è¿˜å‘ç°æ ·æœ¬ä¸­æœ‰å¤šä¸ªå·²çŸ¥æ½®æ±æ’•è£‚äº‹ä»¶å€™é€‰æºï¼Œè¿™äº›æºéƒ½æ˜¯å®è´µçš„ç§‘å­¦ç ”ç©¶èµ„æºã€‚è¯¥é¡¹ç ”ç©¶å…·æœ‰é‡è¦çš„ç§‘å­¦æ„ä¹‰å’Œå®è·µä»·å€¼ã€‚éšç€è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œç›¸ä¿¡è¯¥é¢†åŸŸå°†ä¼šå¸¦æ¥æ›´å¤šçš„ç§‘å­¦å‘ç°ã€‚æ­¤æ¬¡è°ƒæŸ¥æ¶‰åŠå¤©æ–‡å­¦ä¸­çš„ç½•è§ç°è±¡å’Œè¶…å¤§è§„æ¨¡é»‘æ´çš„æ´»åŠ¨è§„å¾‹ç ”ç©¶ç­‰å¤šä¸ªæ–¹é¢ã€‚æ€»ä½“æ¥çœ‹ï¼Œè¯¥ç ”ç©¶å¯¹äºæ¨åŠ¨å¤©æ–‡å­¦é¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>eROSITAé¦–æ¬¡å‘ç°æ•°ç™¾ä¸ªä¸å·²çŸ¥æ´»åŠ¨æ˜Ÿç³»æ ¸æ— å…³çš„æ–°çš„å¯å˜å¤–å¤ªé˜³ç³»å¤–æ˜Ÿæºæ ·æœ¬è¢«æ•´ç†åœ¨ç›®å½•eRO-ExTraä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ba5255701d8edeafaee94dfa4d565519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7899c1c41763c846e79fbe8a580df725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c54e1508abbd1eb1beb55dfa4cf327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25dba4ec276aaf9d55bb66779af7deb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74c3526549a33148c491bf66ee24ffe5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GRAPHITE-Graph-Based-Interpretable-Tissue-Examination-for-Enhanced-Explainability-in-Breast-Cancer-Histopathology"><a href="#GRAPHITE-Graph-Based-Interpretable-Tissue-Examination-for-Enhanced-Explainability-in-Breast-Cancer-Histopathology" class="headerlink" title="GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced   Explainability in Breast Cancer Histopathology"></a>GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced   Explainability in Breast Cancer Histopathology</h2><p><strong>Authors:Raktim Kumar Mondol, Ewan K. A. Millar, Peter H. Graham, Lois Browne, Arcot Sowmya, Erik Meijering</strong></p>
<p>Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITEâ€™s potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologistsâ€™ diagnostic reasoning and support precision medicine. </p>
<blockquote>
<p>åœ¨åŒ»å­¦ç—…ç†å­¦ä¸­ï¼Œå¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰å¯¹äºæé«˜ç™Œç—‡è¯Šæ–­ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è§£é‡Šæ€§å’Œä¸´åºŠå¯ä¿¡åº¦è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„â€œé»‘ç®±â€æ€§è´¨å¸¸å¸¸é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠä¸Šçš„é‡‡çº³ã€‚æˆ‘ä»¬å¼•å…¥äº†GRAPHITEï¼ˆåŸºäºå›¾çš„å¯è§£é‡Šç»„ç»‡æ£€æŸ¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºä¹³è…ºç™Œç»„ç»‡å¾®é˜µåˆ—ï¼ˆTMAï¼‰åˆ†æè®¾è®¡çš„åéªŒå¯è§£é‡Šæ¡†æ¶ã€‚GRAPHITEé‡‡ç”¨å¤šå°ºåº¦æ–¹æ³•ï¼Œåœ¨ä¸åŒæ”¾å¤§çº§åˆ«æå–æ–‘å—ï¼Œæ„å»ºåˆ†å±‚å›¾ï¼Œå¹¶åˆ©ç”¨å…·æœ‰å°ºåº¦æ³¨æ„ï¼ˆSANï¼‰çš„å›¾æ³¨æ„ç½‘ç»œï¼ˆGATï¼‰æ¥æ•è·å°ºåº¦ç›¸å…³ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ç”±è‚¿ç˜¤ç»„ç»‡å¾®é˜µåˆ—çš„140ä¸ªæ ¸å¿ƒå’Œå››ä¸ªè‰¯æ€§å…¨å¹»ç¯ç‰‡å›¾åƒåˆ›å»ºçš„140ä¸ªè‰¯æ€§æ ·æœ¬ä¸Šè®­ç»ƒäº†æ¨¡å‹ï¼Œå¹¶åœ¨ç—…ç†å­¦å®¶æ³¨é‡Šçš„53ä¸ªTMAæ ·æœ¬ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚ä¸ä¼ ç»Ÿçš„XAIæ–¹æ³•ç›¸æ¯”ï¼ŒGRAPHITEè¡¨ç°æ›´å¥½ï¼Œè¾¾åˆ°äº†å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰ä¸º0.56ï¼Œå—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.94ï¼Œé˜ˆå€¼ç¨³å¥æ€§ï¼ˆThRï¼‰ä¸º0.70çš„æŒ‡æ ‡ï¼Œè¿™è¡¨æ˜æ¨¡å‹åœ¨å¹¿æ³›çš„é˜ˆå€¼èŒƒå›´å†…éƒ½èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼ŒGRAPHITEçš„å†³ç­–æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUDCï¼‰æœ€é«˜è¾¾åˆ°äº†4.17e+5ï¼Œè¡¨æ˜å…¶åœ¨ä¸åŒé˜ˆå€¼ä¸‹éƒ½èƒ½æä¾›å¯é çš„å†³ç­–æ”¯æŒã€‚è¿™äº›ç»“æœçªå‡ºäº†GRAPHITEåœ¨ä¸´åºŠç—…ç†å­¦ä¸­çš„ä»·å€¼æ½œåŠ›ï¼Œå®ƒèƒ½æä¾›ä¸ç—…ç†åŒ»å¸ˆè¯Šæ–­æ¨ç†ç›¸ç¬¦çš„å¯è§†åŒ–è§£é‡Šå¹¶æ”¯æŒç²¾å‡†åŒ»å­¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04206v1">PDF</a> 24 Pages, 9 Figures, 1 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Explainable AIï¼ˆXAIï¼‰åœ¨åŒ»å­¦ç—…ç†å­¦ä¸­å¯¹äºæé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„å¯è§£é‡Šæ€§å’Œä¸´åºŠå¯é æ€§çš„é‡è¦æ€§ã€‚ä¸ºåº”å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é»‘ç®±ç‰¹æ€§åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„é™åˆ¶ï¼Œæå‡ºäº†ä¸€ç§åä¸ºGRAPHITEçš„åŸºäºå›¾çš„è§£é‡Šæ€§æ¡†æ¶ï¼Œç”¨äºä¹³è…ºç™Œç»„ç»‡å¾®é˜µåˆ—ï¼ˆTMAï¼‰åˆ†æã€‚GRAPHITEé‡‡ç”¨å¤šå°ºåº¦æ–¹æ³•ï¼Œé€šè¿‡ä¸åŒæ”¾å¤§çº§åˆ«æå–æ–‘å—ã€æ„å»ºå±‚æ¬¡å›¾ï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰å°ºåº¦æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆSANï¼‰çš„å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰æ•æ‰å°ºåº¦ç›¸å…³ç‰¹å¾ã€‚åœ¨140ä¸ªè‚¿ç˜¤TMAèŠ¯å’Œ4ä¸ªè‰¯æ€§å…¨å¹»ç¯ç‰‡å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨53ä¸ªç—…ç†å­¦å®¶æ³¨é‡Šçš„TMAæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ã€‚ç›¸è¾ƒäºä¼ ç»ŸXAIæ–¹æ³•ï¼ŒGRAPHITEè¡¨ç°æ›´ä½³ï¼Œå¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰è¾¾åˆ°0.56ï¼Œå—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.94ï¼Œé˜ˆå€¼ç¨³å¥æ€§ï¼ˆThRï¼‰ä¸º0.70ã€‚åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼ŒGRAPHITEå†³ç­–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUDCï¼‰æœ€é«˜ï¼Œè¾¾åˆ°4.17e+5ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›äº†å¯é çš„è¾…åŠ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Explainable AI (XAI)åœ¨åŒ»å­¦ç—…ç†å­¦ä¸­å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¯Šæ–­åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>GRAPHITEæ˜¯ä¸€ä¸ªåŸºäºå›¾çš„è§£é‡Šæ€§æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä¹³è…ºç™Œç»„ç»‡å¾®é˜µåˆ—åˆ†æä¸­çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>GRAPHITEé‡‡ç”¨å¤šå°ºåº¦æ–¹æ³•ï¼Œåœ¨ä¸åŒæ”¾å¤§çº§åˆ«ä¸‹æå–ç‰¹å¾ï¼Œæ„å»ºå±‚æ¬¡å›¾ã€‚</li>
<li>GRAPHITEä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰å’Œå°ºåº¦æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆSANï¼‰æ¥æ•æ‰å°ºåº¦ç›¸å…³ç‰¹å¾ã€‚</li>
<li>GRAPHITEåœ¨å®éªŒä¸­è¡¨ç°ä¼˜äºä¼ ç»ŸXAIæ–¹æ³•ï¼Œå…·æœ‰é«˜çš„å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ã€å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰å’Œé˜ˆå€¼ç¨³å¥æ€§ï¼ˆThRï¼‰ã€‚</li>
<li>GRAPHITEåœ¨ä¸´åºŠå†³ç­–ä¸­æä¾›äº†å¯é çš„è¾…åŠ©ï¼Œå…·æœ‰æœ€é«˜çš„å†³ç­–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUDCï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04206">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a556457f5673a0bc5e371dbe87a4dac4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb6611ab83ae4ad562f30ee54ed2ef07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c4d1b0bcdd4f7702699627be5d7c565.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Comparison-of-Neural-Models-for-X-ray-Image-Classification-in-COVID-19-Detection"><a href="#Comparison-of-Neural-Models-for-X-ray-Image-Classification-in-COVID-19-Detection" class="headerlink" title="Comparison of Neural Models for X-ray Image Classification in COVID-19   Detection"></a>Comparison of Neural Models for X-ray Image Classification in COVID-19   Detection</h2><p><strong>Authors:Jimi Togni, Romis Attux</strong></p>
<p>This study presents a comparative analysis of methods for detecting COVID-19 infection in radiographic images. The images, sourced from publicly available datasets, were categorized into three classes: â€˜normal,â€™ â€˜pneumonia,â€™ and â€˜COVID.â€™ For the experiments, transfer learning was employed using eight pre-trained networks: SqueezeNet, DenseNet, ResNet, AlexNet, VGG, GoogleNet, ShuffleNet, and MobileNet. DenseNet achieved the highest accuracy of 97.64% using the ADAM optimization function in the multiclass approach. In the binary classification approach, the highest precision was 99.98%, obtained by the VGG, ResNet, and MobileNet networks. A comparative evaluation was also conducted using heat maps. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¯¹æ£€æµ‹COVID-19æ„ŸæŸ“åœ¨æ”¾å°„å›¾åƒä¸­çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚å›¾åƒæ¥æºäºå…¬å¼€æ•°æ®é›†ï¼Œåˆ†ä¸ºä¸‰ç±»ï¼šâ€œæ­£å¸¸â€ã€â€œè‚ºç‚â€å’Œâ€œCOVIDâ€ã€‚åœ¨å®éªŒä¸­ï¼Œé‡‡ç”¨è¿ç§»å­¦ä¹ ï¼Œä½¿ç”¨8ç§é¢„è®­ç»ƒç½‘ç»œï¼šSqueezeNetã€DenseNetã€ResNetã€AlexNetã€VGGã€GoogleNetã€ShuffleNetå’ŒMobileNetã€‚åœ¨é‡‡ç”¨å¤šç±»æ–¹æ³•çš„ç­–ç•¥ä¸­ï¼ŒDenseNetä½¿ç”¨ADAMä¼˜åŒ–å‡½æ•°å–å¾—äº†æœ€é«˜å‡†ç¡®ç‡ï¼Œä¸º97.64%ã€‚åœ¨äºŒå…ƒåˆ†ç±»æ–¹æ³•ä¸­ï¼Œæœ€é«˜ç²¾åº¦ä¸º99.98%ï¼Œç”±VGGã€ResNetå’ŒMobileNetç½‘ç»œè·å¾—ã€‚è¿˜åˆ©ç”¨çƒ­å›¾è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04196v1">PDF</a> 9 pages, 7 tables, 5 figures. XXXIX SIMPOSIO BRASILEIRO DE   TELECOMUNICACOES E PROCESSAMENTO DE SINAIS - SBrT 2021</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¯”è¾ƒåˆ†æäº†ç”¨äºæ£€æµ‹COVID-19æ„ŸæŸ“çš„æ”¾å°„å½±åƒå›¾åƒçš„æ–¹æ³•ã€‚ç ”ç©¶ä½¿ç”¨äº†ä»å…¬å¼€æ•°æ®åº“ä¸­è·å–çš„å›¾åƒï¼Œå¹¶å°†å…¶åˆ†ä¸ºä¸‰ç±»ï¼šâ€œæ­£å¸¸â€ã€â€œè‚ºç‚â€å’Œâ€œCOVIDâ€ã€‚å®éªŒé‡‡ç”¨è¿ç§»å­¦ä¹ ï¼Œä½¿ç”¨å…«ç§é¢„è®­ç»ƒç½‘ç»œï¼ŒåŒ…æ‹¬SqueezeNetã€DenseNetã€ResNetã€AlexNetç­‰ã€‚DenseNetåœ¨å¤šç±»æ–¹æ³•ä¸­åˆ©ç”¨ADAMä¼˜åŒ–å‡½æ•°è¾¾åˆ°äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼Œå³97.64%ã€‚åœ¨äºŒå…ƒåˆ†ç±»æ–¹æ³•ä¸­ï¼ŒVGGã€ResNetå’ŒMobileNetç½‘ç»œçš„ç²¾åº¦æœ€é«˜ï¼Œè¾¾åˆ°99.98%ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡çƒ­å›¾è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¯¹æ£€æµ‹COVID-19æ„ŸæŸ“çš„æ”¾å°„å½±åƒå›¾åƒæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</li>
<li>å›¾åƒæ¥æºäºå…¬å¼€æ•°æ®åº“ï¼Œåˆ†ä¸ºâ€œæ­£å¸¸â€ã€â€œè‚ºç‚â€å’Œâ€œCOVIDâ€ä¸‰ç±»ã€‚</li>
<li>å®éªŒé‡‡ç”¨è¿ç§»å­¦ä¹ ï¼Œä½¿ç”¨äº†å…«ç§é¢„è®­ç»ƒç½‘ç»œã€‚</li>
<li>DenseNetåœ¨å¤šç±»æ–¹æ³•ä¸­çš„å‡†ç¡®æ€§æœ€é«˜ï¼Œè¾¾åˆ°97.64%ã€‚</li>
<li>åœ¨äºŒå…ƒåˆ†ç±»æ–¹æ³•ä¸­ï¼ŒVGGã€ResNetå’ŒMobileNetç½‘ç»œçš„ç²¾åº¦æœ€é«˜ï¼Œè¾¾åˆ°99.98%ã€‚</li>
<li>ç ”ç©¶è¿˜é€šè¿‡çƒ­å›¾è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8dded6c42113052a67d6acf2213dd293.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e1f149b4ad91baca24117683f011ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cce82bf3900622aa9ab5d7b57ab4b39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca6a9f7b561232d85df9fdc62aebeb18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e484bb9344d9e7deb539f05ecfed84e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-450c0b9df871b2a676953372418c514c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8dff3df476e58996d789f104d236dd09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f0fddc39669e7d7128228d448372530.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GLoG-CSUnet-Enhancing-Vision-Transformers-with-Adaptable-Radiomic-Features-for-Medical-Image-Segmentation"><a href="#GLoG-CSUnet-Enhancing-Vision-Transformers-with-Adaptable-Radiomic-Features-for-Medical-Image-Segmentation" class="headerlink" title="GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic   Features for Medical Image Segmentation"></a>GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic   Features for Medical Image Segmentation</h2><p><strong>Authors:Niloufar Eghbali, Hassan Bagher-Ebadian, Tuka Alhanai, Mohammad M. Ghassemi</strong></p>
<p>Vision Transformers (ViTs) have shown promise in medical image semantic segmentation (MISS) by capturing long-range correlations. However, ViTs often struggle to model local spatial information effectively, which is essential for accurately segmenting fine anatomical details, particularly when applied to small datasets without extensive pre-training. We introduce Gabor and Laplacian of Gaussian Convolutional Swin Network (GLoG-CSUnet), a novel architecture enhancing Transformer-based models by incorporating learnable radiomic features. This approach integrates dynamically adaptive Gabor and Laplacian of Gaussian (LoG) filters to capture texture, edge, and boundary information, enhancing the feature representation processed by the Transformer model. Our method uniquely combines the long-range dependency modeling of Transformers with the texture analysis capabilities of Gabor and LoG features. Evaluated on the Synapse multi-organ and ACDC cardiac segmentation datasets, GLoG-CSUnet demonstrates significant improvements over state-of-the-art models, achieving a 1.14% increase in Dice score for Synapse and 0.99% for ACDC, with minimal computational overhead (only 15 and 30 additional parameters, respectively). GLoG-CSUnetâ€™s flexible design allows integration with various base models, offering a promising approach for incorporating radiomics-inspired feature extraction in Transformer architectures for medical image analysis. The code implementation is available on GitHub at: <a target="_blank" rel="noopener" href="https://github.com/HAAIL/GLoG-CSUnet">https://github.com/HAAIL/GLoG-CSUnet</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰é€šè¿‡æ•æ‰é•¿ç¨‹å…³è”åœ¨åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆMISSï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒViTé€šå¸¸åœ¨å»ºæ¨¡å±€éƒ¨ç©ºé—´ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¯¹äºå‡†ç¡®åˆ†å‰²ç²¾ç»†çš„è§£å‰–ç»†èŠ‚è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æœªè¿›è¡Œå¹¿æ³›é¢„è®­ç»ƒçš„å°æ•°æ®é›†ä¸Šåº”ç”¨æ—¶ã€‚æˆ‘ä»¬å¼•å…¥äº†Gaborå’ŒLaplacian of Gaussianå·ç§¯Swinç½‘ç»œï¼ˆGLoG-CSUnetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œé€šè¿‡èå…¥å¯å­¦ä¹ çš„æ”¾å°„å­¦ç‰¹å¾æ¥å¢å¼ºåŸºäºTransformerçš„æ¨¡å‹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†åŠ¨æ€è‡ªé€‚åº”çš„Gaborå’ŒLaplacian of Gaussianï¼ˆLoGï¼‰æ»¤æ³¢å™¨ï¼Œä»¥æ•æ‰çº¹ç†ã€è¾¹ç¼˜å’Œè¾¹ç•Œä¿¡æ¯ï¼Œå¢å¼ºTransformeræ¨¡å‹å¤„ç†çš„ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ç‹¬ç‰¹åœ°ç»“åˆäº†Transformerçš„é•¿ç¨‹ä¾èµ–å»ºæ¨¡ä¸Gaborå’ŒLoGç‰¹å¾çš„çº¹ç†åˆ†æèƒ½åŠ›ã€‚åœ¨Synapseå¤šå™¨å®˜å’ŒACDCå¿ƒè„åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒGLoG-CSUnetåœ¨æœ€æ–°æ¨¡å‹çš„åŸºç¡€ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒSynapseçš„Diceå¾—åˆ†æé«˜äº†1.14%ï¼ŒACDCæé«˜äº†0.99%ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€æå°ï¼ˆä»…åˆ†åˆ«å¢åŠ äº†15å’Œ30ä¸ªé¢å¤–å‚æ•°ï¼‰ã€‚GLoG-CSUnetçš„çµæ´»è®¾è®¡å…è®¸ä¸å„ç§åŸºç¡€æ¨¡å‹é›†æˆï¼Œä¸ºåœ¨Transformeræ¶æ„ä¸­èå…¥æ”¾å°„å­¦ç‰¹å¾æå–çš„åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ä»£ç å®ç°å¯åœ¨GitHubä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/HAAIL/GLoG-CSUnet%E3%80%82">https://github.com/HAAIL/GLoG-CSUnetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02788v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºViTçš„æ¨¡å‹åœ¨å¤„ç†åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²æ—¶è™½ç„¶èƒ½æ•æ‰é•¿è¿œç›¸å…³å…³ç³»ï¼Œä½†å…¶åœ¨å¤„ç†ç²¾ç»†è§£å‰–ç»“æ„æ—¶å¾€å¾€éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡å±€éƒ¨ç©ºé—´ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç»“åˆäº†å¯å­¦ä¹ æ”¾å°„å­¦ç‰¹å¾çš„Gaborä¸Laplacian of Gaussianå·ç§¯ç½‘ç»œï¼ˆGLoG-CSUnetï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡åŠ¨æ€è‡ªé€‚åº”çš„Gaborå’ŒLoGæ»¤æ³¢å™¨æ•æ‰çº¹ç†ã€è¾¹ç¼˜å’Œè¾¹ç•Œä¿¡æ¯ï¼Œå¢å¼ºäº†Transformeræ¨¡å‹çš„ç‰¹æ€§è¡¨ç¤ºã€‚åœ¨Synapseå¤šå™¨å®˜å’ŒACDCå¿ƒè„åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒGLoG-CSUnetç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ¨¡å‹æœ‰æ˜æ˜¾æ”¹è¿›ï¼ŒDiceå¾—åˆ†æé«˜äº†1.14%å’Œ0.99%ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€è¾ƒå°ã€‚å…¶çµæ´»çš„è®¾è®¡å¯ä¸å…¶ä»–åŸºç¡€æ¨¡å‹é›†æˆï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†æä¸­èå…¥æ”¾å°„å­¦ç‰¹å¾æå–çš„Transformeræ¶æ„æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformers (ViTs) åœ¨åŒ»å­¦å›¾åƒè¯­ä¹‰åˆ†å‰²ï¼ˆMISSï¼‰ä¸­æœ‰æ½œåŠ›ï¼Œä½†éš¾ä»¥å¤„ç†å±€éƒ¨ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>GLoG-CSUnetæ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œç»“åˆäº†Transformeræ¨¡å‹å’Œå¯å­¦ä¹ æ”¾å°„å­¦ç‰¹å¾ï¼Œä»¥æ”¹å–„ViTçš„ä¸è¶³ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨åŠ¨æ€è‡ªé€‚åº”çš„Gaborå’ŒLoGæ»¤æ³¢å™¨ï¼ŒGLoG-CSUnetèƒ½å¤Ÿæ•æ‰çº¹ç†ã€è¾¹ç¼˜å’Œè¾¹ç•Œä¿¡æ¯ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒGLoG-CSUnetç›¸è¾ƒäºå…¶ä»–æ¨¡å‹è¡¨ç°æ›´ä¼˜ï¼ŒDiceå¾—åˆ†æ˜¾è‘—æé«˜ã€‚</li>
<li>GLoG-CSUnetè®¾è®¡çµæ´»ï¼Œæ˜“äºä¸å…¶ä»–åŸºç¡€æ¨¡å‹é›†æˆã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­èå…¥æ”¾å°„å­¦ç‰¹å¾æå–çš„Transformeræ¶æ„å…·æœ‰å‰æ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99ea5763ae50199c4055a01ed807df2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18b5af29fffe6c2d7aefee3e9997ad83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bdc56bb40c9fe435e45291e090aeb35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aeee68bf7381bd82029b9972f1a7b895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe35d5743deff00266f5461b716bd679.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NuSTAR-view-of-the-X-ray-transients-Swift-J174805-3-244637-and-IGR-J17511-3057"><a href="#NuSTAR-view-of-the-X-ray-transients-Swift-J174805-3-244637-and-IGR-J17511-3057" class="headerlink" title="NuSTAR view of the X-ray transients Swift J174805.3-244637 and IGR   J17511-3057"></a>NuSTAR view of the X-ray transients Swift J174805.3-244637 and IGR   J17511-3057</h2><p><strong>Authors:Aditya S. Mondal, Mahasweta Bhattacharya, Mayukh Pahari, Biplab Raychaudhuri, Rohit Ghosh, Gulab C. Dewangan</strong></p>
<p>We report on the NuSTAR observations of the neutron star low-mass X-ray binary Swift J174805.3-244637 (hereafter Swift<del>J17480) and the accreting millisecond X-ray pulsar IGR</del>J17511-3057 performed on March 4, 2023, and April 8, 2015, respectively. We describe the continuum emission of Swift<del>J17480 with a combination of two soft thermal components and an additional hard X-ray emission described by a power-law. We suggest that the spectral properties of Swift</del>J17480 are consistent with a soft spectral state. The source IGR<del>J17511-3057 exhibits a hard spectrum characterized by a Comptonized emission from the corona. The X-ray spectrum of both sources shows evidence of disc reflection. For the first time, we employ the self-consistent reflection models ({\tt relxill} and {\tt relxillNS}) to fit the reflection features in the \nustar{} spectrum. From the best-fit spectral model, we find an inner disc radius ($R_{in}$) is precisely constrained to $(1.99-2.68):R_{ISCO}$ and inclination to $30\pm 1\degree$ for Swift</del>J17480. We determine an inner disc radius of $\lesssim 1.3;R_{ISCO}$ and inclination of $44\pm 3\degree$ for IGR<del>J17511-3057. A low inclination angle of the system is required for both sources. For the source IGR</del>J17511-3057, spinning at $4.1$ ms, the value of co-rotation radius ($R_{co}$) is estimated to be $\sim 42$ km ($3.6:R_{ISCO})$, consistent with the position of inner disc radius as $R_{in}\lesssim R_{co}$. We further place an upper limit on the magnetic field strength of the sources, considering the disc is truncated at the magnetospheric radius. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†NuSTARå¯¹ä¸­å­æ˜Ÿä½è´¨é‡Xå°„çº¿åŒæ˜ŸSwift J174805.3-244637ï¼ˆä»¥ä¸‹ç®€ç§°Swift J17480ï¼‰å’Œå¢äº®æ¯«ç§’Xå°„çº¿è„‰å†²æ˜ŸIGR J17511-3057äº2023å¹´3æœˆ4æ—¥å’Œ2015å¹´4æœˆ8æ—¥çš„è§‚æµ‹ç»“æœã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤ç§è½¯çƒ­æˆåˆ†å’Œä¸€ä¸ªé¢å¤–çš„å¹‚å¾‹ç¡¬Xå°„çº¿å‘å°„æ¥æè¿°Swift J17480çš„è¿ç»­å‘å°„ã€‚æˆ‘ä»¬è®¤ä¸ºSwift J17480çš„å…‰è°±ç‰¹æ€§ä¸è½¯æ€å…‰è°±ç›¸ç¬¦ã€‚æºIGR J17511-3057è¡¨ç°å‡ºç”±æ—¥å†•äº§ç”Ÿçš„åº·æ™®é¡¿åŒ–å‘å°„çš„ç¡¬å…‰è°±ç‰¹å¾ã€‚ä¸¤ä¸ªæºçš„Xå°„çº¿å…‰è°±éƒ½æ˜¾ç¤ºå‡ºç›˜åå°„çš„è¯æ®ã€‚æˆ‘ä»¬é¦–æ¬¡é‡‡ç”¨è‡ªæ´½åå°„æ¨¡å‹ï¼ˆrelxillå’ŒrelxillNSï¼‰æ¥æ‹ŸåˆNuSTARå…‰è°±ä¸­çš„åå°„ç‰¹å¾ã€‚æ ¹æ®æœ€ä½³æ‹Ÿåˆå…‰è°±æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°Swift J17480çš„å†…ç›˜åŠå¾„ï¼ˆR_{in}ï¼‰ç²¾ç¡®çº¦æŸåœ¨ï¼ˆ1.99-2.68ï¼‰Ã— R_{ISCO}ï¼Œå€¾è§’ä¸º30Â±1Â°ã€‚æˆ‘ä»¬ç¡®å®šIGR J17511-3057çš„å†…ç›˜åŠå¾„ä¸å¤§äºçº¦ä¸ºï¼ˆçº¦ä¸º \leq è½¬å‘å­—ç¬¦æ‰€è¡¨ç¤ºçš„æ„æ€ï¼‰ï¼ˆå‘å‡ºæ–‡æœ¬â€œ&lt;&#x3D;â€ï¼‰ ï¼‰å¹¶ä¸”å…¶å€¾è§’ä¸ºå€¾æ–œè§’åº¦çº¦ä¸ºä¸ºåœ¨æ•°æ®æºç¬¦å·ï¼‰ã€‚åœ¨æ”¾å°„ç‰©è´¨ç”Ÿæˆä¹‹å‰çš„ç»™å®šæˆ–å­˜åœ¨çš„é«˜é˜¶æ®µæˆ‘ä»¬å‘ç°å…¶ä¸­æ ¸çš„ç‰©è´¨å·²åˆå¹¶æœ€ç»ˆç”Ÿæˆäº†è„‰å†²æ˜ŸIGRJåœ¨è„‰å†²æ˜ŸIGRJä¸­æˆ‘ä»¬å‘ç°å…¶å†…ç›˜åŠå¾„å°äºç­‰äºåè½¬åŠå¾„å¹¶ä¸”ä¼°è®¡åè½¬åŠå¾„çº¦ä¸ºå¤§çº¦åŠå¾„çº¦ä¸ºå¹¶ä¸”æˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†ç£åœºçš„ä¸Šé™ï¼Œè€ƒè™‘äº†ç£å±‚åŠå¾„å¤„æˆªæ–­ç›˜çš„æƒ…å†µã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç¡®å®šäº†ç£åœºçš„ä¸Šé™è€ƒè™‘äº†ç£å±‚æˆªæ–­çš„æƒ…å†µè¿™å°†ç£åœºçš„å½±å“å½±å“å¯èƒ½æå¤§åœ°é™ä½ä»è€Œä½¿å¾—ä¸€äº›æ•ˆåº”ä½¿å¾—å…‰è°±æ¨¡å‹ä¸­å¯èƒ½å‡ºç°æ›´å¤šçš„æ³¢åŠ¨å¯¼è‡´äº†è„‰å‹ç¼©æˆä¸ºè‡´å…‰è°±æ›´åŠ ç¡¬åŒ–çš„åŸå› å› æ­¤æˆ‘ä»¬çš„ç ”ç©¶å¯¹äºç†è§£è¿™äº›æºçš„ç‰©ç†æ€§è´¨ä»¥åŠæœªæ¥çš„è§‚æµ‹ç ”ç©¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20323v2">PDF</a> 28 pages, 13 figures, 3 tables, Accepted for publication in Journal   of High Energy Astrophysics (JHEAP)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¯¹NuSTARè§‚å¯Ÿåˆ°çš„ä¸¤ä¸ªå¤©ä½“ï¼Œå³ä½è´¨é‡Xå°„çº¿åŒæ˜ŸSwift J174805.3-244637ï¼ˆä»¥ä¸‹ç®€ç§°Swift~J17480ï¼‰å’Œæ¯«ç§’Xå°„çº¿è„‰å†²æ˜ŸIGR J17511-3057åœ¨2023å¹´3æœˆ4æ—¥å’Œ2015å¹´4æœˆ8æ—¥çš„è§‚æµ‹ç»“æœè¿›è¡Œäº†æŠ¥å‘Šã€‚Swift J17480çš„è¿ç»­å‘å°„é€šè¿‡ä¸¤ä¸ªè½¯çƒ­æˆåˆ†å’Œä¸€ä¸ªé¢å¤–çš„ç¡¬Xå°„çº¿å‘å°„ï¼ˆç”±å¹‚å¾‹æè¿°ï¼‰çš„ç»„åˆæ¥æè¿°ã€‚æˆ‘ä»¬è®¤ä¸ºå…¶è°±ç‰¹æ€§ä¸è½¯è°±çŠ¶æ€ä¸€è‡´ã€‚IGR J17511-3057è¡¨ç°å‡ºç¡¬è°±ç‰¹æ€§ï¼Œå…·æœ‰æ¥è‡ªæ—¥å†•çš„åº·æ™®é¡¿å‘å°„ã€‚ä¸¤ä¸ªæºçš„Xå°„çº¿å…‰è°±éƒ½æ˜¾ç¤ºå‡ºç›˜åå°„çš„è¯æ®ã€‚é¦–æ¬¡é‡‡ç”¨è‡ªæ´½åå°„æ¨¡å‹ï¼ˆrelxillå’ŒrelxillNSï¼‰æ‹Ÿåˆåå°„ç‰¹å¾ï¼Œå¹¶å¾—å‡ºäº†æœ€ä½³çš„å†…éƒ¨ç›˜åŠå¾„å’Œå†…å€¾è§’çš„æµ‹é‡å€¼ã€‚è¿™ä¸¤ä¸ªæºçš„å€¾æ–œè§’è¾ƒå°ï¼Œä¸”å¯¹äºè‡ªè½¬å‘¨æœŸä¸º4.1æ¯«ç§’çš„IGR J17511-3057ï¼Œå…¶åè½¬åŠå¾„ä¸å†…ç›˜åŠå¾„çš„ä¼°è®¡å€¼ç›¸ç¬¦ã€‚æ­¤å¤–ï¼Œè€ƒè™‘äº†ç£çƒåŠå¾„å¤„ç›˜è¢«æˆªæ–­çš„æƒ…å†µï¼Œå¯¹æºç£åœºå¼ºåº¦ç»™å‡ºäº†ä¸Šé™ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<p>ä¸€ã€æŠ¥å‘Šäº†NuSTARå¯¹ä¸¤ä¸ªå¤©ä½“Swift J17480å’ŒIGR J17511-3057çš„è§‚æµ‹ç»“æœã€‚<br>äºŒã€Swift J17480çš„è¿ç»­å‘å°„åŒ…å«ä¸¤ä¸ªè½¯çƒ­æˆåˆ†å’Œä¸€ä¸ªç¡¬Xå°„çº¿å‘å°„ï¼Œå…¶è°±ç‰¹æ€§ä¸è½¯è°±çŠ¶æ€ä¸€è‡´ã€‚<br>ä¸‰ã€IGR J17511-3057çš„ç¡¬è°±ç‰¹æ€§è¡¨ç°å‡ºåº·æ™®é¡¿å‘å°„ï¼Œæ¥è‡ªå…¶æ—¥å†•ã€‚<br>å››ã€ä¸¤ä¸ªæºçš„Xå°„çº¿å…‰è°±éƒ½å…·æœ‰ç›˜åå°„ç‰¹å¾ï¼Œé¦–æ¬¡ä½¿ç”¨è‡ªæ´½åå°„æ¨¡å‹è¿›è¡Œæ‹Ÿåˆã€‚<br>äº”ã€Swift J17480çš„å†…ç›˜åŠå¾„ç²¾ç¡®çº¦æŸåœ¨æŸä¸ªèŒƒå›´å†…ï¼Œå€¾æ–œè§’ä¸ºçº¦$ 30Â°$ ã€‚è€Œå¯¹äºIGR J17511-3057ï¼Œå…¶å†…ç›˜åŠå¾„å°äºåè½¬åŠå¾„ï¼Œå€¾æ–œè§’ä¸ºçº¦$ 44Â°$ ã€‚<br>å…­ã€ä¸¤ä¸ªæºçš„å€¾æ–œè§’å‡è¾ƒå°ã€‚å¯¹äºIGR J17511-3057ï¼Œå…¶åè½¬åŠå¾„ä¸å†…ç›˜åŠå¾„çš„ä¼°è®¡å€¼ç›¸å»åˆã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ea96e6a211414a931fcda20ddbd1cb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69f46a1a4a61f6a5f80c1221c0907e7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc5afac274970c421a9604542bd6bc47.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Practical-Guide-to-Transcranial-Ultrasonic-Stimulation-from-the-IFCN-endorsed-ITRUSST-Consortium"><a href="#A-Practical-Guide-to-Transcranial-Ultrasonic-Stimulation-from-the-IFCN-endorsed-ITRUSST-Consortium" class="headerlink" title="A Practical Guide to Transcranial Ultrasonic Stimulation from the   IFCN-endorsed ITRUSST Consortium"></a>A Practical Guide to Transcranial Ultrasonic Stimulation from the   IFCN-endorsed ITRUSST Consortium</h2><p><strong>Authors:Keith R Murphy, Tulika Nandi, Benjamin Kop, Takahiro Osada, W Apoutou Nâ€™Djin, Maximilian Lueckel, Kevin A Caulfield, Anton Fomenko, Hartwig R Siebner, Yoshikazu Ugawa, Lennart Verhagen, Sven Bestmann, Eleanor Martin, Kim Butts Pauly, Elsa Fouragnan, Til Ole Bergmann</strong></p>
<p>Low-intensity Transcranial Ultrasonic Stimulation (TUS) is a non-invasive brain stimulation technique enabling cortical and deep brain targeting with unprecedented spatial accuracy. Given the high rate of adoption by new users with varying levels of expertise and interdisciplinary backgrounds, practical guidelines are needed to ensure state-of-the-art TUS application and reproducible outcomes. Therefore, the International Transcranial Ultrasonic Stimulation Safety and Standards (ITRUSST) consortium has formed a subcommittee, endorsed by the International Federation of Clinical Neurophysiology (IFCN), to develop recommendations for best practice in TUS applications in humans. The practical guide presented here provides a brief introduction into ultrasound physics and sonication parameters. It explains the requirements of TUS lab equipment and transducer selection and discusses experimental design and procedures alongside potential confounds and control conditions. Finally, the guide elaborates on essential steps of application planning for stimulation safety and efficacy, as well as considerations when combining TUS with neuroimaging, electrophysiology, or other brain stimulation techniques. We hope that this practical guide to TUS will assist both novice and experienced users in planning and conducting high-quality studies and provide a solid foundation for further advancements in this promising field. </p>
<blockquote>
<p>ä½å¼ºåº¦ç»é¢…è¶…å£°åˆºæ¿€ï¼ˆTUSï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„è„‘åˆºæ¿€æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„ç©ºé—´ç²¾åº¦å®ç°å¤§è„‘çš®è´¨å’Œæ·±å±‚çš„å®šä½ã€‚è€ƒè™‘åˆ°ä¸åŒä¸“ä¸šå’Œè·¨å­¦ç§‘èƒŒæ™¯çš„æ–°ç”¨æˆ·é‡‡ç”¨ç‡è¾ƒé«˜ï¼Œä¸ºç¡®ä¿æœ€å…ˆè¿›çš„TUSåº”ç”¨å’Œå¯é‡å¤çš„ç»“æœï¼Œéœ€è¦å®ç”¨æŒ‡å—ã€‚å› æ­¤ï¼Œå›½é™…ç»é¢…è¶…å£°åˆºæ¿€å®‰å…¨ä¸æ ‡å‡†ï¼ˆITRUSSTï¼‰è”ç›Ÿåœ¨å›½é™…ä¸´åºŠç¥ç»ç”Ÿç†å­¦è”åˆä¼šï¼ˆIFCNï¼‰çš„æ”¯æŒä¸‹ï¼Œæˆç«‹äº†ä¸€ä¸ªä¸“é—¨å§”å‘˜ä¼šï¼Œæ—¨åœ¨ä¸ºäººç±»TUSåº”ç”¨æä¾›æœ€ä½³å®è·µå»ºè®®ã€‚æœ¬æ–‡æä¾›çš„å®ç”¨æŒ‡å—ç®€è¦ä»‹ç»äº†è¶…å£°ç‰©ç†å­¦å’Œè¶…å£°å‚æ•°ã€‚å®ƒè§£é‡Šäº†TUSå®éªŒå®¤è®¾å¤‡å’Œæ¢èƒ½å™¨çš„è¦æ±‚ï¼Œå¹¶è®¨è®ºäº†å®éªŒè®¾è®¡å’Œç¨‹åºä»¥åŠæ½œåœ¨çš„å¹²æ‰°å’Œæ§åˆ¶æ¡ä»¶ã€‚æœ€åï¼Œæœ¬æŒ‡å—è¯¦ç»†é˜è¿°äº†åˆºæ¿€å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§çš„åº”ç”¨è§„åˆ’çš„å¿…è¦æ­¥éª¤ï¼Œä»¥åŠå°†TUSä¸ç¥ç»æˆåƒã€ç”µç”Ÿç†å­¦æˆ–å…¶ä»–è„‘åˆºæ¿€æŠ€æœ¯ç›¸ç»“åˆæ—¶çš„æ³¨æ„äº‹é¡¹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æœ¬TUSå®ç”¨æŒ‡å—èƒ½å¸®åŠ©åˆå­¦è€…å’Œç»éªŒä¸°å¯Œçš„ç”¨æˆ·è¿›è¡Œé«˜è´¨é‡çš„ç ”ç©¶è§„åˆ’ï¼Œå¹¶ä¸ºè¿™ä¸€å……æ»¡å¸Œæœ›é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07646v3">PDF</a> 67 pages, 23 Figures, 2 Tables, 5 Supplementary Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†è·¨é¢…è¶…å£°åˆºæ¿€ï¼ˆTUSï¼‰çš„å®è·µæŒ‡å—ï¼ŒåŒ…æ‹¬è¶…å£°ç‰©ç†å­¦å’Œå£°éŸ³å‚æ•°ã€å®éªŒå®¤è®¾å¤‡å’Œæ¢èƒ½å™¨é€‰æ‹©çš„è¦æ±‚ï¼Œä»¥åŠå®éªŒè®¾è®¡å’Œç¨‹åºã€æ½œåœ¨å¹²æ‰°å’Œæ§åˆ¶æ¡ä»¶ï¼Œä»¥åŠåº”ç”¨è§„åˆ’çš„åˆºæ¿€å®‰å…¨å’ŒåŠŸæ•ˆçš„å¿…è¦æ­¥éª¤ã€‚è¯¥æŒ‡å—æ—¨åœ¨ä¸ºæ–°æ‰‹å’Œç»éªŒä¸°å¯Œçš„ç”¨æˆ·æä¾›å¸®åŠ©ï¼Œè§„åˆ’å¹¶è¿›è¡Œé«˜è´¨é‡ç ”ç©¶ï¼Œå¹¶ä¸ºè¿™ä¸ªå……æ»¡å¸Œæœ›çš„é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›åšå®çš„åŸºç¡€ã€‚<br>å¸Œæœ›é€šè¿‡è¿™ä»½æŒ‡å—èƒ½ä¸ºæ–°æ‰‹å’Œç»éªŒä¸°å¯Œçš„ç”¨æˆ·æä¾›å¸®åŠ©ï¼Œå…±åŒæ¨åŠ¨è·¨é¢…è¶…å£°åˆºæ¿€é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å¼ºåº¦è·¨é¢…è¶…å£°åˆºæ¿€ï¼ˆTUSï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„è„‘åˆºæ¿€æŠ€æœ¯ï¼Œå¯å®ç°å‰æ‰€æœªæœ‰çš„ç©ºé—´å‡†ç¡®æ€§çš„å¤§è„‘çš®è´¨å’Œæ·±å±‚å¤§è„‘å®šä½ã€‚</li>
<li>å›½é™…è·¨é¢…è¶…å£°åˆºæ¿€å®‰å…¨ä¸æ ‡å‡†è”ç›Ÿï¼ˆITRUSSTï¼‰ä¸ºåœ¨äººç±»åº”ç”¨TUSå¼€å‘æœ€ä½³å®è·µå»ºè®®è®¾ç«‹äº†ä¸“é—¨å§”å‘˜ä¼šã€‚</li>
<li>è¯¥æŒ‡å—ç®€è¦ä»‹ç»äº†è¶…å£°ç‰©ç†å­¦å’Œå£°éŸ³å‚æ•°ï¼Œè§£é‡Šäº†TUSå®éªŒå®¤è®¾å¤‡å’Œæ¢èƒ½å™¨çš„é€‰æ‹©è¦æ±‚ã€‚</li>
<li>è®¨è®ºäº†å®éªŒè®¾è®¡ã€æ½œåœ¨å¹²æ‰°å’Œæ§åˆ¶æ¡ä»¶ä»¥åŠåº”ç”¨è§„åˆ’çš„åˆºæ¿€å®‰å…¨å’ŒåŠŸæ•ˆçš„å…³é”®æ­¥éª¤ã€‚</li>
<li>è¯¥æŒ‡å—æ—¨åœ¨ä¸ºè·¨é¢…è¶…å£°åˆºæ¿€é¢†åŸŸçš„æ–°æ‰‹å’Œç»éªŒä¸°å¯Œçš„ç”¨æˆ·æä¾›å¸®åŠ©ï¼Œè¿›è¡Œé«˜è´¨é‡çš„ç ”ç©¶ã€‚</li>
<li>è¯¥æŒ‡å—ä¸ºè·¨é¢…è¶…å£°åˆºæ¿€çš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8d7e6e40be2b1ca3084dcaa3703e656e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01db44bf644b79df05bc0ebc29f05468.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MedPix-2-0-A-Comprehensive-Multimodal-Biomedical-Data-set-for-Advanced-AI-Applications-with-Retrieval-Augmented-Generation-and-Knowledge-Graphs"><a href="#MedPix-2-0-A-Comprehensive-Multimodal-Biomedical-Data-set-for-Advanced-AI-Applications-with-Retrieval-Augmented-Generation-and-Knowledge-Graphs" class="headerlink" title="MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced   AI Applications with Retrieval Augmented Generation and Knowledge Graphs"></a>MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced   AI Applications with Retrieval Augmented Generation and Knowledge Graphs</h2><p><strong>Authors:Irene Siragusa, Salvatore Contino, Massimo La Ciura, Rosario Alicata, Roberto Pirrone</strong></p>
<p>The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality data set, mainly due to privacy-related issues. In addition, the recent increase in large multimodal models (LMM) leads to the need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding CT or MRI scans. This paper illustrates the entire workflow for building the MedPix 2.0 data set. Starting with the well-known multimodal data set MedPix\textsuperscript{\textregistered}, mainly used by physicians, nurses, and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure in which noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a GUI aimed at navigating efficiently the MongoDB instance and obtaining the raw data that can be easily used for training and&#x2F;or fine-tuning LMMs. To enforce this point, in this work, we first recall DR-Minerva, a RAG-based LMM trained using MedPix 2.0. DR-Minerva predicts the body part and the modality used to scan its input image. We also propose the extension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting architecture can be queried in a end-to-end manner, as a medical decision support system. MedPix 2.0 is available on GitHub. \url{<a target="_blank" rel="noopener" href="https://github.com/CHILab1/MedPix-2.0%7D">https://github.com/CHILab1/MedPix-2.0}</a> </p>
<blockquote>
<p>éšç€åŒ»ç–—é¢†åŸŸå¯¹å¼€å‘äººå·¥æ™ºèƒ½åº”ç”¨çš„æ—¥ç›Šå…³æ³¨ï¼Œç”±äºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå°¤å…¶æ˜¯éšç§ç›¸å…³é—®é¢˜ï¼Œé€ æˆäº†å¾ˆå¤§çš„å›°æ‰°ã€‚æ­¤å¤–ï¼Œæœ€è¿‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å¢åŠ å¯¼è‡´äº†å¯¹å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚å¢åŠ ï¼Œè¿™äº›åŒ»ç–—æ•°æ®é›†ä¸­é™„å¸¦äº†ä¸´åºŠæŠ¥å‘Šå’Œæ£€æŸ¥ç»“æœä»¥åŠä¸CTæˆ–MRIæ‰«æç»“æœç›¸å¯¹åº”çš„å›¾åƒã€‚æœ¬æ–‡é˜è¿°äº†æ„å»ºMedPix 2.0æ•°æ®é›†çš„æ•´ä¸ªå·¥ä½œæµç¨‹ã€‚ä»¥è‘—åçš„å¤šæ¨¡æ€æ•°æ®é›†MedPixÂ®å¼€å§‹ï¼Œè¯¥æ•°æ®é›†ä¸»è¦ç”¨äºåŒ»ç”Ÿã€æŠ¤å£«å’ŒåŒ»ç–—ä¿å¥å­¦ç”Ÿç»§ç»­åŒ»å­¦æ•™è‚²çš„ç›®çš„ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŠè‡ªåŠ¨ç®¡é“æ¥æå–è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œéšåè¿›è¡Œæ‰‹åŠ¨æ¸…ç†ç¨‹åºï¼Œå»é™¤å™ªå£°æ ·æœ¬ï¼Œä»è€Œåˆ›å»ºäº†MongoDBæ•°æ®åº“ã€‚é™¤äº†æ•°æ®é›†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªGUIï¼Œæ—¨åœ¨é«˜æ•ˆæµè§ˆMongoDBå®ä¾‹å¹¶è·å¾—å¯ç”¨äºè®­ç»ƒå’Œ&#x2F;æˆ–å¾®è°ƒLMMçš„åŸå§‹æ•°æ®ã€‚ä¸ºäº†å¼ºåŒ–è¿™ä¸€ç‚¹ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾äº†DR-Minervaï¼Œè¿™æ˜¯ä¸€ç§åŸºäºRAGçš„LMMï¼Œä½¿ç”¨MedPix 2.0è¿›è¡Œè®­ç»ƒã€‚DR-Minervaå¯ä»¥é¢„æµ‹èº«ä½“éƒ¨ä½ä»¥åŠç”¨äºæ‰«æå…¶è¾“å…¥å›¾åƒçš„æ¨¡æ€ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†DR-Minervaçš„æ‰©å±•ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬ä½¿ç”¨Llama 3.1 Instruct 8Bå¹¶å€ŸåŠ©MedPix 2.0æ„å»ºçŸ¥è¯†å›¾è°±ã€‚æœ€ç»ˆæ¶æ„å¯ä»¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡ŒæŸ¥è¯¢ï¼Œä½œä¸ºåŒ»ç–—å†³ç­–æ”¯æŒç³»ç»Ÿã€‚MedPix 2.0å·²åœ¨GitHubä¸Šæä¾›ã€‚\url{<a target="_blank" rel="noopener" href="https://github.com/CHILab1/MedPix-2.0%7D">https://github.com/CHILab1/MedPix-2.0}</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02994v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ„å»ºMedPix 2.0æ•°æ®é›†çš„å·¥ä½œæµç¨‹æ¶‰åŠä»å·²çŸ¥çš„å¤šæ¨¡æ€æ•°æ®é›†MedPixÂ®ä¸­æå–è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œé€šè¿‡åŠè‡ªåŠ¨ç®¡é“è¿›è¡Œå¤„ç†ï¼Œå¹¶ç»è¿‡æ‰‹åŠ¨å‡€åŒ–ä»¥åˆ›å»ºMongoDBæ•°æ®åº“ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ç”¨äºé«˜æ•ˆæµè§ˆMongoDBå®ä¾‹çš„GUIï¼Œä¾¿äºè·å–å¯ç”¨äºè®­ç»ƒå’Œ&#x2F;æˆ–å¾®è°ƒå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„åŸå§‹æ•°æ®ã€‚æœ¬å·¥ä½œå›é¡¾äº†ä½¿ç”¨MedPix 2.0è®­ç»ƒçš„åŸºäºRAGçš„LMMâ€”â€”DR-Minervaï¼Œå¹¶æè®®å°†å…¶æ‰©å±•ä¸ºçŸ¥è¯†å›¾è°±ï¼Œåˆ©ç”¨Llama 3.1 Instruct 8Bå¹¶ä¾æ‰˜MedPix 2.0ã€‚MedPix 2.0æ•°æ®é›†å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹é«˜è´¨é‡æ•°æ®é›†æ˜¯äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€ï¼Œä¸»è¦åŸå› æ˜¯éšç§ç›¸å…³é—®é¢˜ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„éœ€è¦å¢é•¿æ¨åŠ¨äº†å¤šæ¨¡æ€åŒ»ç–—æ•°æ®é›†çš„éœ€æ±‚ï¼Œä¸´åºŠæŠ¥å‘Šå’Œå‘ç°éœ€è¦ä¸CTæˆ–MRIæ‰«æç›¸å¯¹åº”ã€‚</li>
<li>MedPix 2.0æ•°æ®é›†æ„å»ºäºç°æœ‰çš„MedPixÂ®æ•°æ®é›†ä¹‹ä¸Šï¼Œç»è¿‡åŠè‡ªåŠ¨ç®¡é“å¤„ç†å¹¶æ‰‹åŠ¨å‡€åŒ–æ•°æ®ï¼Œå­˜å‚¨åœ¨MongoDBæ•°æ®åº“ä¸­ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ï¼Œä»¥é«˜æ•ˆæµè§ˆMongoDBå®ä¾‹å¹¶è·å–å¯ç”¨äºè®­ç»ƒå’Œ&#x2F;æˆ–å¾®è°ƒLMMçš„åŸå§‹æ•°æ®ã€‚</li>
<li>DR-Minervaæ˜¯ä¸€ä¸ªåŸºäºRAGçš„LMMï¼Œä½¿ç”¨MedPix 2.0æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé¢„æµ‹å›¾åƒçš„èº«ä½“éƒ¨ä½å’Œæ‰«ææ–¹å¼ã€‚</li>
<li>DR-Minervaé€šè¿‡ä¸çŸ¥è¯†å›¾è°±çš„ç»“åˆæ‰©å±•ï¼Œåˆ©ç”¨Llama 3.1 Instruct 8Bå¹¶ä¾èµ–MedPix 2.0ä½œä¸ºå…¶æ ¸å¿ƒèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b164a14129b2c3f44cb5f459b266e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3704feef2b9a2a4bbf4f8ffaf7d60a40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-294e5d1fb9711cb5758db1dc356bd14b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ImageFlowNet-Forecasting-Multiscale-Image-Level-Trajectories-of-Disease-Progression-with-Irregularly-Sampled-Longitudinal-Medical-Images"><a href="#ImageFlowNet-Forecasting-Multiscale-Image-Level-Trajectories-of-Disease-Progression-with-Irregularly-Sampled-Longitudinal-Medical-Images" class="headerlink" title="ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease   Progression with Irregularly-Sampled Longitudinal Medical Images"></a>ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease   Progression with Irregularly-Sampled Longitudinal Medical Images</h2><p><strong>Authors:Chen Liu, Ke Xu, Liangbo L. Shen, Guillaume Huguet, Zilong Wang, Alexander Tong, Danilo Bzdok, Jay Stewart, Jay C. Wang, Lucian V. Del Priore, Smita Krishnaswamy</strong></p>
<p>Advances in medical imaging technologies have enabled the collection of longitudinal images, which involve repeated scanning of the same patients over time, to monitor disease progression. However, predictive modeling of such data remains challenging due to high dimensionality, irregular sampling, and data sparsity. To address these issues, we propose ImageFlowNet, a novel model designed to forecast disease trajectories from initial images while preserving spatial details. ImageFlowNet first learns multiscale joint representation spaces across patients and time points, then optimizes deterministic or stochastic flow fields within these spaces using a position-parameterized neural ODE&#x2F;SDE framework. The model leverages a UNet architecture to create robust multiscale representations and mitigates data scarcity by combining knowledge from all patients. We provide theoretical insights that support our formulation of ODEs, and motivate our regularizations involving high-level visual features, latent space organization, and trajectory smoothness. We validate ImageFlowNet on three longitudinal medical image datasets depicting progression in geographic atrophy, multiple sclerosis, and glioblastoma, demonstrating its ability to effectively forecast disease progression and outperform existing methods. Our contributions include the development of ImageFlowNet, its theoretical underpinnings, and empirical validation on real-world datasets. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/ImageFlowNet">https://github.com/KrishnaswamyLab/ImageFlowNet</a>. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒæŠ€æœ¯çš„è¿›å±•ä½¿å¾—èƒ½å¤Ÿæ”¶é›†çºµå‘å›¾åƒï¼Œè¿™äº›å›¾åƒæ¶‰åŠå¯¹åŒä¸€æ‚£è€…çš„é‡å¤æ‰«æï¼Œä»¥ç›‘æµ‹ç–¾ç—…çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®çš„é«˜ç»´æ€§ã€ä¸è§„åˆ™é‡‡æ ·å’Œæ•°æ®ç¨€ç–æ€§ï¼Œæ­¤ç±»æ•°æ®çš„é¢„æµ‹å»ºæ¨¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ImageFlowNetè¿™ä¸€æ–°å‹æ¨¡å‹ï¼Œæ—¨åœ¨ä»åˆå§‹å›¾åƒé¢„æµ‹ç–¾ç—…è½¨è¿¹ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ç»†èŠ‚ã€‚ImageFlowNeté¦–å…ˆå­¦ä¹ æ‚£è€…å’Œæ—¶é—´ç‚¹ä¹‹é—´çš„å¤šå°ºåº¦è”åˆè¡¨ç¤ºç©ºé—´ï¼Œç„¶ååˆ©ç”¨ä½ç½®å‚æ•°åŒ–çš„ç¥ç»ODE&#x2F;SDEæ¡†æ¶ä¼˜åŒ–è¿™äº›ç©ºé—´å†…çš„ç¡®å®šæ€§æˆ–éšæœºæµåœºã€‚è¯¥æ¨¡å‹åˆ©ç”¨UNetæ¶æ„åˆ›å»ºç¨³å¥çš„å¤šå°ºåº¦è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ç»“åˆæ‰€æœ‰æ‚£è€…çš„çŸ¥è¯†æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬æä¾›äº†æ”¯æŒæˆ‘ä»¬åˆ¶å®šODEçš„ç†è®ºè§è§£ï¼Œå¹¶æ¿€å‘äº†æˆ‘ä»¬æ¶‰åŠé«˜çº§è§†è§‰ç‰¹å¾ã€æ½œåœ¨ç©ºé—´ç»„ç»‡å’Œè½¨è¿¹å¹³æ»‘æ€§çš„æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæç»˜åœ°ç†èç¼©ã€å¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œèƒ¶è´¨æ¯ç»†èƒç˜¤è¿›å±•çš„çºµå‘åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†ImageFlowNetï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆé¢„æµ‹ç–¾ç—…è¿›å±•å¹¶ä¼˜äºç°æœ‰æ–¹æ³•çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ImageFlowNetçš„å¼€å‘ã€å…¶ç†è®ºåŸºç¡€ä»¥åŠåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯éªŒè¯ã€‚å®˜æ–¹å®ç°å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/ImageFlowNet%E3%80%82">https://github.com/KrishnaswamyLab/ImageFlowNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14794v5">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å½±åƒæŠ€æœ¯çš„è¿›å±•ä½¿å¾—çºµå‘å½±åƒæ•°æ®çš„æ”¶é›†æˆä¸ºå¯èƒ½ï¼Œä¸ºç–¾ç—…è¿›å±•ç›‘æµ‹æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚é’ˆå¯¹æ­¤ç±»æ•°æ®é¢„æµ‹æ¨¡å‹é¢ä¸´çš„é«˜ç»´æ€§ã€ä¸è§„åˆ™é‡‡æ ·å’Œæ•°æ®ç¨€ç–ç­‰é—®é¢˜ï¼Œæå‡ºäº†ImageFlowNetæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è”åˆå­¦ä¹ è·¨æ‚£è€…å’Œæ—¶é—´ç‚¹çš„å¤šå°ºåº¦è¡¨ç¤ºç©ºé—´ï¼Œå¹¶åˆ©ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹&#x2F;éšæœºå¾®åˆ†æ–¹ç¨‹æ¡†æ¶ä¼˜åŒ–ç¡®å®šæ€§æˆ–éšæœºæµåœºï¼Œä»¥ä»åˆå§‹å›¾åƒé¢„æµ‹ç–¾ç—…è½¨è¿¹å¹¶ä¿ç•™ç©ºé—´ç»†èŠ‚ã€‚åœ¨ä¸‰ä¸ªæç»˜åœ°ç†èç¼©ã€å¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œèƒ¶è´¨æ¯ç»†èƒç˜¤è¿›å±•çš„çºµå‘åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†ImageFlowNetçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºå…¶ä¼˜äºç°æœ‰æ–¹æ³•çš„é¢„æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—æˆåƒæŠ€æœ¯ç°åœ¨èƒ½å¤Ÿæ”¶é›†çºµå‘å›¾åƒæ•°æ®ï¼Œç”¨äºç›‘æµ‹ç–¾ç—…è¿›å±•ã€‚</li>
<li>ImageFlowNetæ¨¡å‹è¢«æå‡ºæ¥è§£å†³é«˜ç»´æ€§ã€ä¸è§„åˆ™é‡‡æ ·å’Œæ•°æ®ç¨€ç–ç­‰é¢„æµ‹æŒ‘æˆ˜ã€‚</li>
<li>ImageFlowNeté€šè¿‡è”åˆå­¦ä¹ è·¨æ‚£è€…å’Œæ—¶é—´ç‚¹çš„å¤šå°ºåº¦è¡¨ç¤ºç©ºé—´è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ä½¿ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹&#x2F;éšæœºå¾®åˆ†æ–¹ç¨‹æ¡†æ¶ä¼˜åŒ–ç¡®å®šæ€§æˆ–éšæœºæµåœºã€‚</li>
<li>ImageFlowNetåˆ©ç”¨UNetæ¶æ„åˆ›å»ºç¨³å¥çš„å¤šå°ºåº¦è¡¨ç¤ºï¼Œå¹¶é€šè¿‡ç»“åˆæ‰€æœ‰æ‚£è€…çš„çŸ¥è¯†æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>åœ¨ä¸‰ä¸ªä¸åŒçš„çºµå‘åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†ImageFlowNetçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a6834ab91cc80fb65ce2b4dc062144e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95bb82a74bb2746a824659bcebb35fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668419b156643de27c9f64266d7041c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44925c256226d39af72e3757ab975c0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AnoFPDM-Anomaly-Segmentation-with-Forward-Process-of-Diffusion-Models-for-Brain-MRI"><a href="#AnoFPDM-Anomaly-Segmentation-with-Forward-Process-of-Diffusion-Models-for-Brain-MRI" class="headerlink" title="AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models   for Brain MRI"></a>AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models   for Brain MRI</h2><p><strong>Authors:Yiming Che, Fazle Rafsani, Jay Shah, Md Mahfuzur Rahman Siddiquee, Teresa Wu</strong></p>
<p>Weakly-supervised diffusion models (DMs) in anomaly segmentation, leveraging image-level labels, have attracted significant attention for their superior performance compared to unsupervised methods. It eliminates the need for pixel-level labels in training, offering a more cost-effective alternative to supervised methods. However, existing methods are not fully weakly-supervised because they heavily rely on costly pixel-level labels for hyperparameter tuning in inference. To tackle this challenge, we introduce Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM), a fully weakly-supervised framework that operates without the need of pixel-level labels. Leveraging the unguided forward process as a reference for the guided forward process, we select hyperparameters such as the noise scale, the threshold for segmentation and the guidance strength. We aggregate anomaly maps from guided forward process, enhancing the signal strength of anomalous regions. Remarkably, our proposed method outperforms recent state-of-the-art weakly-supervised approaches, even without utilizing pixel-level labels. </p>
<blockquote>
<p>å¼±ç›‘ç£æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¼‚å¸¸åˆ†å‰²ä¸­çš„åº”ç”¨å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒåˆ©ç”¨å›¾åƒçº§åˆ«çš„æ ‡ç­¾ï¼Œå…¶æ€§èƒ½ä¼˜äºæ— ç›‘ç£æ–¹æ³•ã€‚å®ƒæ¶ˆé™¤äº†å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çš„åƒç´ çº§åˆ«æ ‡ç­¾çš„éœ€æ±‚ï¼Œä¸ºç›‘ç£æ–¹æ³•æä¾›äº†æ›´å…·æˆæœ¬æ•ˆç›Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¹¶ä¸æ˜¯å®Œå…¨çš„å¼±ç›‘ç£ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–äºæ¨ç†è¿‡ç¨‹ä¸­çš„è¶…å‚æ•°è°ƒæ•´æ‰€éœ€çš„æ˜‚è´µåƒç´ çº§æ ‡ç­¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ‰©æ•£æ¨¡å‹å‰å‘è¿‡ç¨‹çš„å¼‚å¸¸åˆ†å‰²ï¼ˆAnoFPDMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼±ç›‘ç£çš„æ¡†æ¶ï¼Œæ— éœ€åƒç´ çº§æ ‡ç­¾å³å¯è¿è¡Œã€‚æˆ‘ä»¬ä»¥éå¼•å¯¼å‰å‘è¿‡ç¨‹ä½œä¸ºå¼•å¯¼å‰å‘è¿‡ç¨‹çš„å‚è€ƒï¼Œé€‰æ‹©è¶…å‚æ•°ï¼Œå¦‚å™ªå£°è§„æ¨¡ã€åˆ†å‰²é˜ˆå€¼å’Œå¼•å¯¼å¼ºåº¦ã€‚æˆ‘ä»¬ä»å¼•å¯¼å‰å‘è¿‡ç¨‹ä¸­èšåˆå¼‚å¸¸å›¾ï¼Œå¢å¼ºå¼‚å¸¸åŒºåŸŸçš„ä¿¡å·å¼ºåº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç”šè‡³åœ¨ä¸éœ€è¦åƒç´ çº§æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä¹Ÿä¼˜äºæœ€æ–°çš„å¼±ç›‘ç£å‰æ²¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15683v4">PDF</a> v4: added appendices and fixed some typos</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼±ç›‘ç£æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¼‚å¸¸åˆ†å‰²ä¸­çš„åº”ç”¨ï¼Œå…¶åˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ç›¸è¾ƒäºæ— ç›‘ç£æ–¹æ³•å…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä¸éœ€è¦åƒç´ çº§æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œç›¸è¾ƒäºç›‘ç£å­¦ä¹ æ–¹æ³•æˆæœ¬æ›´ä½ã€‚ä½†ç°æœ‰æ–¹æ³•å¹¶æœªå®ç°å®Œå…¨çš„å¼±ç›‘ç£ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–æ˜‚è´µçš„åƒç´ çº§æ ‡ç­¾è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM)è¿™ä¸€å®Œå…¨å¼±ç›‘ç£çš„æ¡†æ¶ï¼Œæ— éœ€åƒç´ çº§æ ‡ç­¾å³å¯æ“ä½œã€‚æˆ‘ä»¬åˆ©ç”¨æ— å¯¼å‘çš„å‰å‘è¿‡ç¨‹ä½œä¸ºæœ‰å¯¼å‘å‰å‘è¿‡ç¨‹çš„å‚è€ƒï¼Œé€‰æ‹©å™ªå£°è§„æ¨¡ã€åˆ†å‰²é˜ˆå€¼å’ŒæŒ‡å¯¼å¼ºåº¦ç­‰è¶…å‚æ•°ï¼Œå¹¶èšé›†å¼‚å¸¸æ˜ å°„ï¼Œå¢å¼ºå¼‚å¸¸åŒºåŸŸçš„ä¿¡å·å¼ºåº¦ã€‚æ‰€ææ–¹æ³•åœ¨ä¸ä½¿ç”¨åƒç´ çº§æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºæœ€æ–°çš„å¼±ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸åˆ†å‰²ä¸­åº”ç”¨å¹¿æ³›ï¼Œå› å…¶ä»…ä¾èµ–å›¾åƒçº§æ ‡ç­¾è€Œè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½å®£ç§°å¼±ç›‘ç£ï¼Œä½†ä»ä¾èµ–åƒç´ çº§æ ‡ç­¾è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ï¼Œæˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨æ–°çš„å¼±ç›‘ç£æ¡†æ¶â€”â€”Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM)ã€‚</li>
<li>AnoFPDMåˆ©ç”¨æ— å¯¼å‘å‰å‘è¿‡ç¨‹ä½œä¸ºå‚è€ƒï¼Œé€‰æ‹©è¶…å‚æ•°ï¼Œå®Œå…¨æ— éœ€åƒç´ çº§æ ‡ç­¾ã€‚</li>
<li>é€šè¿‡èšé›†å¼‚å¸¸æ˜ å°„ï¼Œå¢å¼ºäº†å¼‚å¸¸åŒºåŸŸçš„ä¿¡å·å¼ºåº¦ã€‚</li>
<li>æ‰€ææ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰å¼±ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-720e7d5671c0de8268d794ed10bbeed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fa9bbc333637f89efaf8fa02b18e9b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41b5617001f43066c61ea405e3101a80.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Deep-Unfolding-Network-with-Spatial-Alignment-for-multi-modal-MRI-reconstruction"><a href="#Deep-Unfolding-Network-with-Spatial-Alignment-for-multi-modal-MRI-reconstruction" class="headerlink" title="Deep Unfolding Network with Spatial Alignment for multi-modal MRI   reconstruction"></a>Deep Unfolding Network with Spatial Alignment for multi-modal MRI   reconstruction</h2><p><strong>Authors:Hao Zhang, Qi Wang, Jun Shi, Shihui Ying, Zhijie Wen</strong></p>
<p>Multi-modal Magnetic Resonance Imaging (MRI) offers complementary diagnostic information, but some modalities are limited by the long scanning time. To accelerate the whole acquisition process, MRI reconstruction of one modality from highly undersampled k-space data with another fully-sampled reference modality is an efficient solution. However, the misalignment between modalities, which is common in clinic practice, can negatively affect reconstruction quality. Existing deep learning-based methods that account for inter-modality misalignment perform better, but still share two main common limitations: (1) The spatial alignment task is not adaptively integrated with the reconstruction process, resulting in insufficient complementarity between the two tasks; (2) the entire framework has weak interpretability. In this paper, we construct a novel Deep Unfolding Network with Spatial Alignment, termed DUN-SA, to appropriately embed the spatial alignment task into the reconstruction process. Concretely, we derive a novel joint alignment-reconstruction model with a specially designed cross-modal spatial alignment term. By relaxing the model into cross-modal spatial alignment and multi-modal reconstruction tasks, we propose an effective algorithm to solve this model alternatively. Then, we unfold the iterative steps of the proposed algorithm and design corresponding network modules to build DUN-SA with interpretability. Through end-to-end training, we effectively compensate for spatial misalignment using only reconstruction loss, and utilize the progressively aligned reference modality to provide inter-modality prior to improve the reconstruction of the target modality. Comprehensive experiments on three real datasets demonstrate that our method exhibits superior reconstruction performance compared to state-of-the-art methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æä¾›äº†äº’è¡¥çš„è¯Šæ–­ä¿¡æ¯ï¼Œä½†æŸäº›æ¨¡æ€å—åˆ°é•¿æ—¶é—´æ‰«æçš„é™åˆ¶ã€‚ä¸ºäº†åŠ é€Ÿæ•´ä¸ªé‡‡é›†è¿‡ç¨‹ï¼Œä»é«˜åº¦æ¬ é‡‡æ ·çš„kç©ºé—´æ•°æ®ä¸­é‡å»ºä¸€ç§æ¨¡æ€ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ç§å®Œå…¨é‡‡æ ·çš„å‚è€ƒæ¨¡æ€ä½œä¸ºå‚è€ƒï¼Œæ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¸´åºŠå®è·µä¸­å¸¸è§çš„æ¨¡æ€é—´çš„ä¸å¯¹å‡†ä¼šç›´æ¥å½±å“é‡å»ºè´¨é‡ã€‚è™½ç„¶ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è€ƒè™‘äº†è·¨æ¨¡æ€çš„ä¸å¯¹å‡†é—®é¢˜ï¼Œå¹¶è¡¨ç°æ›´å¥½ï¼Œä½†å®ƒä»¬ä»ç„¶å…·æœ‰ä¸¤ä¸ªä¸»è¦çš„å¸¸è§å±€é™æ€§ï¼šï¼ˆ1ï¼‰ç©ºé—´å¯¹å‡†ä»»åŠ¡æ²¡æœ‰ä¸é‡å»ºè¿‡ç¨‹è‡ªé€‚åº”åœ°é›†æˆï¼Œå¯¼è‡´ä¸¤ä¸ªä»»åŠ¡ä¹‹é—´çš„äº’è¡¥æ€§ä¸è¶³ï¼›ï¼ˆ2ï¼‰æ•´ä¸ªæ¡†æ¶çš„å¯è§£é‡Šæ€§è¾ƒå¼±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§æ–°å‹çš„å…·æœ‰ç©ºé—´å¯¹å‡†åŠŸèƒ½çš„æ·±åº¦å±•å¼€ç½‘ç»œï¼Œç§°ä¸ºDUN-SAï¼Œä»¥é€‚å½“åœ°å°†ç©ºé—´å¯¹å‡†ä»»åŠ¡åµŒå…¥åˆ°é‡å»ºè¿‡ç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªæ–°å‹çš„è”åˆå¯¹å‡†-é‡å»ºæ¨¡å‹ï¼Œå¹¶ç‰¹åˆ«è®¾è®¡äº†ä¸€ä¸ªè·¨æ¨¡æ€ç©ºé—´å¯¹å‡†é¡¹ã€‚é€šè¿‡å°†æ¨¡å‹æ”¾æ¾ä¸ºè·¨æ¨¡æ€ç©ºé—´å¯¹å‡†å’Œå¤šæ¨¡æ€é‡å»ºä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„äº¤æ›¿æ±‚è§£ç®—æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬å±•å¼€äº†æ‰€æå‡ºç®—æ³•çš„è¿­ä»£æ­¥éª¤ï¼Œå¹¶è®¾è®¡äº†ç›¸åº”çš„ç½‘ç»œæ¨¡å—æ¥æ„å»ºå…·æœ‰å¯è§£é‡Šæ€§çš„DUN-SAã€‚é€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨é‡å»ºæŸå¤±æ¥æœ‰æ•ˆåœ°è¡¥å¿ç©ºé—´ä¸å¯¹å‡†ï¼Œå¹¶åˆ©ç”¨é€æ­¥å¯¹é½çš„å‚è€ƒæ¨¡æ€ä¸ºç›®æ ‡æ¨¡æ€çš„é‡å»ºæä¾›è·¨æ¨¡æ€å…ˆéªŒã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å±•ç°å‡ºæ›´é«˜çš„é‡å»ºæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.16998v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€MRIé‡å»ºæ–¹æ³•ï¼Œé€šè¿‡æ„å»ºåä¸ºDUN-SAçš„å±•å¼€ç½‘ç»œï¼Œå°†ç©ºé—´å¯¹é½ä»»åŠ¡é€‚å½“åœ°åµŒå…¥åˆ°é‡å»ºè¿‡ç¨‹ä¸­ã€‚é€šè¿‡è®¾è®¡è·¨æ¨¡æ€ç©ºé—´å¯¹é½é¡¹ï¼Œå½¢æˆè”åˆå¯¹é½é‡å»ºæ¨¡å‹ã€‚é€šè¿‡äº¤æ›¿æ±‚è§£æ¨¡å‹ï¼Œå®ç°æœ‰æ•ˆç®—æ³•ã€‚æœ€ç»ˆï¼Œåˆ©ç”¨é‡å»ºæŸå¤±è¡¥å¿ç©ºé—´å¤±é…ï¼Œåˆ©ç”¨é€æ­¥å¯¹é½çš„å‚è€ƒæ¨¡æ€æä¾›è·¨æ¨¡æ€å…ˆéªŒï¼Œæé«˜ç›®æ ‡æ¨¡æ€çš„é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€MRIèƒ½å¤Ÿæä¾›äº’è¡¥çš„è¯Šæ–­ä¿¡æ¯ï¼Œä½†æ‰«ææ—¶é—´è¿‡é•¿æ˜¯å…¶é™åˆ¶ä¹‹ä¸€ã€‚</li>
<li>ä»é«˜åº¦æ¬ é‡‡æ ·çš„kç©ºé—´æ•°æ®ä¸­é‡å»ºä¸€ç§æ¨¡æ€ï¼ŒåŒæ—¶ä½¿ç”¨å¦ä¸€ç§å®Œå…¨é‡‡æ ·çš„å‚è€ƒæ¨¡æ€ä½œä¸ºå‚è€ƒï¼Œå¯ä»¥åŠ é€Ÿæ•´ä¸ªé‡‡é›†è¿‡ç¨‹ã€‚</li>
<li>è·¨æ¨¡æ€ç©ºé—´å¤±é…æ˜¯å½±å“é‡å»ºè´¨é‡çš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚</li>
<li>ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³è·¨æ¨¡æ€ç©ºé—´å¤±é…çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šç©ºé—´å¯¹é½ä»»åŠ¡ä¸é‡å»ºè¿‡ç¨‹ç¼ºä¹é€‚åº”æ€§èåˆï¼Œä»¥åŠæ•´ä¸ªæ¡†æ¶çš„å¯è§£é‡Šæ€§è¾ƒå¼±ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹Deep Unfolding Network with Spatial Alignmentï¼ˆDUN-SAï¼‰ï¼Œå°†ç©ºé—´å¯¹é½ä»»åŠ¡é€‚å½“åœ°åµŒå…¥åˆ°é‡å»ºè¿‡ç¨‹ä¸­ã€‚</li>
<li>é€šè¿‡è®¾è®¡è·¨æ¨¡æ€ç©ºé—´å¯¹é½é¡¹ï¼Œå½¢æˆäº†è”åˆå¯¹é½-é‡å»ºæ¨¡å‹ï¼Œå¹¶é€šè¿‡æœ‰æ•ˆç®—æ³•äº¤æ›¿æ±‚è§£è¯¥æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.16998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8a7765fd76d574ff1443c76da48e49f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3839b1404b799d245534c73ba3c8ddf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40cb0541e450b9fc77a07e36c8d150a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb68609a51d1d29dd763a492b56c3b30.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration"><a href="#AutoFuse-Automatic-Fusion-Networks-for-Deformable-Medical-Image-Registration" class="headerlink" title="AutoFuse: Automatic Fusion Networks for Deformable Medical Image   Registration"></a>AutoFuse: Automatic Fusion Networks for Deformable Medical Image   Registration</h2><p><strong>Authors:Mingyuan Meng, Michael Fulham, Dagan Feng, Lei Bi, Jinman Kim</strong></p>
<p>Deformable image registration aims to find a dense non-linear spatial correspondence between a pair of images, which is a crucial step for many medical tasks such as tumor growth monitoring and population analysis. Recently, Deep Neural Networks (DNNs) have been widely recognized for their ability to perform fast end-to-end registration. However, DNN-based registration needs to explore the spatial information of each image and fuse this information to characterize spatial correspondence. This raises an essential question: what is the optimal fusion strategy to characterize spatial correspondence? Existing fusion strategies (e.g., early fusion, late fusion) were empirically designed to fuse information by manually defined prior knowledge, which inevitably constrains the registration performance within the limits of empirical designs. In this study, we depart from existing empirically-designed fusion strategies and develop a data-driven fusion strategy for deformable image registration. To achieve this, we propose an Automatic Fusion network (AutoFuse) that provides flexibility to fuse information at many potential locations within the network. A Fusion Gate (FG) module is also proposed to control how to fuse information at each potential network location based on training data. Our AutoFuse can automatically optimize its fusion strategy during training and can be generalizable to both unsupervised registration (without any labels) and semi-supervised registration (with weak labels provided for partial training data). Extensive experiments on two well-benchmarked medical registration tasks (inter- and intra-patient registration) with eight public datasets show that our AutoFuse outperforms state-of-the-art unsupervised and semi-supervised registration methods. </p>
<blockquote>
<p>å¯å˜å›¾åƒé…å‡†æ—¨åœ¨å¯»æ‰¾ä¸€å¯¹å›¾åƒä¹‹é—´çš„å¯†é›†éçº¿æ€§ç©ºé—´å¯¹åº”å…³ç³»ï¼Œè¿™æ˜¯è®¸å¤šåŒ»å­¦ä»»åŠ¡ï¼ˆå¦‚è‚¿ç˜¤ç”Ÿé•¿ç›‘æµ‹å’Œäººå£åˆ†æï¼‰ä¸­çš„å…³é”®æ­¥éª¤ã€‚æœ€è¿‘ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å› å…¶èƒ½å¤Ÿæ‰§è¡Œå¿«é€Ÿç«¯åˆ°ç«¯é…å‡†çš„èƒ½åŠ›è€Œå¾—åˆ°å¹¿æ³›è®¤å¯ã€‚ç„¶è€Œï¼ŒåŸºäºDNNçš„é…å‡†éœ€è¦æ¢ç´¢æ¯ä¸ªå›¾åƒçš„ç©ºé—´ä¿¡æ¯å¹¶å°†è¿™äº›ä¿¡æ¯èåˆä»¥è¡¨å¾ç©ºé—´å¯¹åº”å…³ç³»ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šè¡¨å¾ç©ºé—´å¯¹åº”å…³ç³»çš„æœ€ä½³èåˆç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿç°æœ‰çš„èåˆç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œæ—©æœŸèåˆã€åæœŸèåˆï¼‰æ˜¯ç»éªŒè®¾è®¡ï¼Œé€šè¿‡æ‰‹åŠ¨å®šä¹‰çš„å…ˆéªŒçŸ¥è¯†æ¥èåˆä¿¡æ¯ï¼Œè¿™ä¸å¯é¿å…åœ°ä½¿é…å‡†æ€§èƒ½å—åˆ°ç»éªŒè®¾è®¡çš„é™åˆ¶ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ‘’å¼ƒäº†ç°æœ‰çš„ç»éªŒè®¾è®¡èåˆç­–ç•¥ï¼Œå¹¶å¼€å‘äº†ä¸€ç§ç”¨äºå¯å˜å›¾åƒé…å‡†çš„æ•°æ®é©±åŠ¨èåˆç­–ç•¥ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨èåˆç½‘ç»œï¼ˆAutoFuseï¼‰ï¼Œè¯¥ç½‘ç»œå¯ä»¥åœ¨ç½‘ç»œçš„è®¸å¤šæ½œåœ¨ä½ç½®çµæ´»åœ°èåˆä¿¡æ¯ã€‚è¿˜æå‡ºäº†ä¸€ç§èåˆé—¨ï¼ˆFGï¼‰æ¨¡å—ï¼Œç”¨äºæ ¹æ®è®­ç»ƒæ•°æ®æ§åˆ¶æ¯ä¸ªæ½œåœ¨ç½‘ç»œä½ç½®çš„èåˆæ–¹å¼ã€‚æˆ‘ä»¬çš„AutoFuseå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¼˜åŒ–å…¶èåˆç­–ç•¥ï¼Œå¹¶å¯æ¨å¹¿åº”ç”¨äºæ— ç›‘ç£é…å‡†ï¼ˆæ— éœ€ä»»ä½•æ ‡ç­¾ï¼‰å’ŒåŠç›‘ç£é…å‡†ï¼ˆä»…éƒ¨åˆ†è®­ç»ƒæ•°æ®æä¾›å¼±æ ‡ç­¾ï¼‰ã€‚åœ¨ä¸¤ç§ç»è¿‡è‰¯å¥½è¯„ä¼°çš„åŒ»ç–—é…å‡†ä»»åŠ¡ï¼ˆæ‚£è€…é—´å’Œæ‚£è€…å†…éƒ¨é…å‡†ï¼‰ä¸Šçš„å…«ä¸ªå…¬å¼€æ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AutoFuseä¼˜äºæœ€å…ˆè¿›çš„æ— ç›‘ç£å’ŒåŠç›‘ç£é…å‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.05271v2">PDF</a> Published at Pattern Recognition</p>
<p><strong>Summary</strong><br>     å˜å½¢å›¾åƒé…å‡†æ—¨åœ¨å¯»æ‰¾ä¸€å¯¹å›¾åƒé—´çš„å¯†é›†éçº¿æ€§ç©ºé—´å¯¹åº”å…³ç³»ï¼Œå¯¹äºè‚¿ç˜¤ç”Ÿé•¿ç›‘æµ‹å’Œäººå£åˆ†æç­‰å¤šç§åŒ»ç–—ä»»åŠ¡è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºæ•°æ®é©±åŠ¨çš„ä¿¡æ¯èåˆç­–ç•¥ä¼˜åŒ–é…å‡†æ–¹æ³•ï¼Œè®¾è®¡äº†ä¸€ä¸ªèƒ½å¤Ÿçµæ´»åœ°åœ¨ç½‘ç»œå¤šä¸ªæ½œåœ¨ä½ç½®è¿›è¡Œä¿¡æ¯èåˆçš„è‡ªåŠ¨èåˆç½‘ç»œï¼ˆAutoFuseï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†èåˆé—¨ï¼ˆFGï¼‰æ¨¡å—ï¼Œèƒ½å¤Ÿæ ¹æ®è®­ç»ƒæ•°æ®æ§åˆ¶æ¯ä¸ªæ½œåœ¨ç½‘ç»œä½ç½®çš„ä¿¡æ¯èåˆæ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒAutoFuseåœ¨æ— éœ€ä»»ä½•æ ‡ç­¾çš„æ— ç›‘ç£é…å‡†å’Œä»…éƒ¨åˆ†è®­ç»ƒæ•°æ®æœ‰å¼±æ ‡ç­¾çš„åŠç›‘ç£é…å‡†ä¸­ï¼Œå‡è¡¨ç°å‡ºè¶…è¶Šç°æœ‰ä¸»æµæ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜å½¢å›¾åƒé…å‡†åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ï¼Œå¦‚è‚¿ç˜¤ç”Ÿé•¿ç›‘æµ‹å’Œäººå£åˆ†æã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°çš„åŸºäºæ•°æ®é©±åŠ¨çš„èåˆç­–ç•¥ä¼˜åŒ–é…å‡†æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†è‡ªåŠ¨èåˆç½‘ç»œï¼ˆAutoFuseï¼‰ï¼Œèƒ½åœ¨ç½‘ç»œçš„å¤šä¸ªæ½œåœ¨ä½ç½®è¿›è¡Œä¿¡æ¯èåˆã€‚</li>
<li>è®¾è®¡äº†èåˆé—¨ï¼ˆFGï¼‰æ¨¡å—ä»¥æ§åˆ¶ä¸åŒç½‘ç»œä½ç½®çš„ä¿¡æ¯èåˆæ–¹å¼ã€‚</li>
<li>å®éªŒè¯æ˜äº†AutoFuseåœ¨æ— ç›‘ç£é…å‡†å’ŒåŠç›‘ç£é…å‡†ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>AutoFuseæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„åŒ»å­¦å›¾åƒé…å‡†ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.05271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e503dc54580a3ea1cdeb73122ed0a9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb6a7705d650875d0177b64325119b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584a2a690018e1c66109df13589d9440.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9f8f17572fedf5f32310e180ac1a1e88.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  FleSpeech Flexibly Controllable Speech Generation with Various Prompts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-27c007f4b20c6726a4d36b0e7b51d916.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  EditAR Unified Conditional Generation with Autoregressive Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
