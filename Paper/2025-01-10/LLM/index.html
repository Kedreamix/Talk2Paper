<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  EpiCoder Encompassing Diversity and Complexity in Code Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-856339fb27e67cc4b71e7ca33d61753a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-10-æ›´æ–°"><a href="#2025-01-10-æ›´æ–°" class="headerlink" title="2025-01-10 æ›´æ–°"></a>2025-01-10 æ›´æ–°</h1><h2 id="EpiCoder-Encompassing-Diversity-and-Complexity-in-Code-Generation"><a href="#EpiCoder-Encompassing-Diversity-and-Complexity-in-Code-Generation" class="headerlink" title="EpiCoder: Encompassing Diversity and Complexity in Code Generation"></a>EpiCoder: Encompassing Diversity and Complexity in Code Generation</h2><p><strong>Authors:Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, Jinsong Su, Qi Chen, Scarlett Li</strong></p>
<p>Effective instruction tuning is indispensable for optimizing code LLMs, aligning model behavior with user expectations and enhancing model performance in real-world applications. However, most existing methods focus on code snippets, which are limited to specific functionalities and rigid structures, restricting the complexity and diversity of the synthesized data. To address these limitations, we introduce a novel feature tree-based synthesis framework inspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic structure of code, our framework models semantic relationships between code elements, enabling the generation of more nuanced and diverse data. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features. This process enables the identification of more complex patterns and relationships within the code. By sampling subtrees with controlled depth and breadth, our framework allows precise adjustments to the complexity of the generated code, supporting a wide range of tasks from simple function-level operations to intricate multi-file scenarios. We fine-tuned widely-used base models to create the EpiCoder series, achieving state-of-the-art performance at both the function and file levels across multiple benchmarks. Notably, empirical evidence indicates that our approach shows significant potential in synthesizing highly complex repository-level code data. Further analysis elucidates the merits of this approach by rigorously assessing data complexity and diversity through software engineering principles and LLM-as-a-judge method. </p>
<blockquote>
<p>å¯¹ä»£ç LLMè¿›è¡Œä¼˜åŒ–è°ƒæ•´æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œè¿™èƒ½ä½¿æ¨¡å‹è¡Œä¸ºä¸ç”¨æˆ·æœŸæœ›ç›¸ç¬¦ï¼Œæé«˜æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»£ç ç‰‡æ®µï¼Œè¿™å±€é™äºç‰¹å®šåŠŸèƒ½å’Œå›ºå®šç»“æ„ï¼Œä»è€Œé™åˆ¶äº†åˆæˆæ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å—åˆ°æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„å¯å‘ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºç‰¹å¾æ ‘çš„åˆæˆæ¡†æ¶ã€‚ä¸åŒäºASTæ•æ‰ä»£ç çš„è¯­æ³•ç»“æ„ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯¹ä»£ç å…ƒç´ ä¹‹é—´çš„è¯­ä¹‰å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆæ›´å¾®å¦™å’Œå¤šæ ·åŒ–çš„æ•°æ®ã€‚ç‰¹å¾æ ‘æ˜¯æ ¹æ®åŸå§‹æ•°æ®æ„å»ºçš„ï¼Œå¹¶ç»è¿‡è¿­ä»£ä¼˜åŒ–æ¥å¢åŠ æå–ç‰¹å¾çš„æ•°é‡å’Œå¤šæ ·æ€§ã€‚è¿™ä¸ªè¿‡ç¨‹èƒ½å¤Ÿè¯†åˆ«ä»£ç å†…éƒ¨æ›´å¤æ‚çš„æ¨¡å¼å’Œå…³ç³»ã€‚é€šè¿‡æ§åˆ¶æ·±åº¦å’Œå¹¿åº¦æ¥é‡‡æ ·å­æ ‘ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥å¯¹ç”Ÿæˆçš„ä»£ç çš„å¤æ‚æ€§è¿›è¡Œç²¾ç¡®è°ƒæ•´ï¼Œæ”¯æŒä»ç®€å•çš„å‡½æ•°çº§æ“ä½œåˆ°å¤æ‚çš„å¤šæ–‡ä»¶åœºæ™¯çš„å„ç§ä»»åŠ¡ã€‚æˆ‘ä»¬å¾®è°ƒäº†å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ï¼Œåˆ›å»ºäº†EpiCoderç³»åˆ—ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å‡½æ•°å’Œæ–‡ä»¶çº§åˆ«çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»éªŒè¯æ®è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆé«˜åº¦å¤æ‚çš„ä»“åº“çº§ä»£ç æ•°æ®æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚è¿›ä¸€æ­¥çš„åˆ†æé€šè¿‡è½¯ä»¶å·¥ç¨‹åŸç†å’ŒLLMè¯„ä¼°æ–¹æ³•ä¸¥æ ¼åœ°è¯„ä¼°æ•°æ®çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œé˜æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04694v1">PDF</a> 40 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹ä»£ç LLMçš„ä¼˜åŒ–æŒ‡ä»¤è°ƒæ•´çš„é‡è¦æ€§ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡æ–°å‹ç‰¹å¾æ ‘åˆæˆæ¡†æ¶è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å»ºæ¨¡ä»£ç å…ƒç´ é—´çš„è¯­ä¹‰å…³ç³»ï¼Œç”Ÿæˆæ›´ä¸ºç»†è‡´å’Œå¤šæ ·çš„æ•°æ®ã€‚é€šè¿‡æ„å»ºç‰¹å¾æ ‘å¹¶è¿­ä»£ä¼˜åŒ–ï¼Œèƒ½è¯†åˆ«æ›´å¤æ‚çš„ä»£ç æ¨¡å¼å’Œå…³ç³»ã€‚é€šè¿‡æ§åˆ¶å­æ ‘çš„æ·±åº¦å’Œå¹¿åº¦é‡‡æ ·ï¼Œè¯¥æ¡†æ¶æ”¯æŒä»ç®€å•å‡½æ•°çº§æ“ä½œåˆ°å¤æ‚å¤šæ–‡ä»¶åœºæ™¯çš„å„ç§ä»»åŠ¡ã€‚ç»éªŒè¯æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆé«˜åº¦å¤æ‚çš„ä»“åº“çº§ä»£ç æ•°æ®æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç LLMçš„ä¼˜åŒ–æŒ‡ä»¤è°ƒæ•´å¯¹äºæ¨¡å‹æ€§èƒ½ä¼˜åŒ–å’Œç”¨æˆ·æœŸæœ›å¯¹é½è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»£ç ç‰‡æ®µï¼Œå­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥å¤„ç†å¤æ‚å’Œå¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚</li>
<li>æ–°å‹ç‰¹å¾æ ‘åˆæˆæ¡†æ¶é€šè¿‡å»ºæ¨¡ä»£ç å…ƒç´ é—´çš„è¯­ä¹‰å…³ç³»ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆæ›´ç»†è‡´å’Œå¤šæ ·çš„æ•°æ®ã€‚</li>
<li>ç‰¹å¾æ ‘çš„æ„å»ºå’Œè¿­ä»£ä¼˜åŒ–æœ‰åŠ©äºè¯†åˆ«æ›´å¤æ‚çš„ä»£ç æ¨¡å¼å’Œå…³ç³»ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒé€šè¿‡æ§åˆ¶å­æ ‘çš„æ·±åº¦å’Œå¹¿åº¦é‡‡æ ·ï¼Œé€‚åº”ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ã€‚</li>
<li>EpiCoderç³»åˆ—æ¨¡å‹é€šè¿‡å¾®è°ƒå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ï¼Œå®ç°äº†å‡½æ•°å’Œæ–‡ä»¶çº§åˆ«å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e7cc753f24eedf75b8992bc24ca64a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aadc7888e69f644514b73aff41649c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1285a189705479bb9d5b9e2e0def6ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c51f93b882fe5ae29e62ebf0032fc7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20a9126d4c5f9863455a732d1cb17e2f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="URSA-Understanding-and-Verifying-Chain-of-thought-Reasoning-in-Multimodal-Mathematics"><a href="#URSA-Understanding-and-Verifying-Chain-of-thought-Reasoning-in-Multimodal-Mathematics" class="headerlink" title="URSA: Understanding and Verifying Chain-of-thought Reasoning in   Multimodal Mathematics"></a>URSA: Understanding and Verifying Chain-of-thought Reasoning in   Multimodal Mathematics</h2><p><strong>Authors:Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang</strong></p>
<p>Chain-of-thought (CoT) reasoning has been widely applied in the mathematical reasoning of Large Language Models (LLMs). Recently, the introduction of derivative process supervision on CoT trajectories has sparked discussions on enhancing scaling capabilities during test time, thereby boosting the potential of these models. However, in multimodal mathematical reasoning, the scarcity of high-quality CoT training data has hindered existing models from achieving high-precision CoT reasoning and has limited the realization of reasoning potential during test time. In this work, we propose a three-module synthesis strategy that integrates CoT distillation, trajectory-format rewriting, and format unification. It results in a high-quality CoT reasoning instruction fine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively validate the state-of-the-art (SOTA) performance of the trained URSA-7B model on multiple multimodal mathematical benchmarks. For test-time scaling, we introduce a data synthesis strategy that automatically generates process annotation datasets, known as DualMath-1.1M, focusing on both interpretation and logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT reasoning capabilities to robust supervision abilities. The trained URSA-RM-7B acts as a verifier, effectively enhancing the performance of URSA-7B at test time. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD) verifying capabilities, showcasing its generalization. Model weights, training data and code will be open-sourced. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å·²åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚æœ€è¿‘ï¼Œå¯¹æ€ç»´é“¾è½¨è¿¹çš„æ´¾ç”Ÿè¿‡ç¨‹ç›‘ç£çš„è®¨è®ºå¼•å‘äº†å…³äºæé«˜æµ‹è¯•æ—¶çš„æ‰©å±•èƒ½åŠ›çš„å…´è¶£ï¼Œä»è€Œæé«˜äº†è¿™äº›æ¨¡å‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­ï¼Œé«˜è´¨é‡æ€ç»´é“¾è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé˜»ç¢äº†ç°æœ‰æ¨¡å‹å®ç°é«˜ç²¾åº¦æ€ç»´é“¾æ¨ç†ï¼Œå¹¶é™åˆ¶äº†æµ‹è¯•æ—¶æ¨ç†æ½œåŠ›çš„å®ç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰æ¨¡å—åˆæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥é›†æˆäº†æ€ç»´é“¾è’¸é¦ã€è½¨è¿¹æ ¼å¼é‡å†™å’Œæ ¼å¼ç»Ÿä¸€ã€‚è¿™å¯¼è‡´åœ¨å¤šæ¨¡æ€æ•°å­¦ä¸­äº§ç”Ÿäº†é«˜è´¨é‡çš„æ€ç»´é“¾æ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†MMathCoT-1Mã€‚æˆ‘ä»¬å…¨é¢éªŒè¯äº†è®­ç»ƒåçš„URSA-7Bæ¨¡å‹åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°æ€§èƒ½ã€‚ä¸ºäº†æµ‹è¯•æ—¶çš„æ‰©å±•èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ•°æ®åˆæˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è‡ªåŠ¨ç”Ÿæˆåä¸ºDualMath-1.1Mçš„è¿‡ç¨‹æ³¨é‡Šæ•°æ®é›†ï¼Œä¾§é‡äºè§£é‡Šå’Œé€»è¾‘ã€‚é€šè¿‡å¯¹URSA-7Båœ¨DualMath-1.1Mä¸Šè¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œæˆ‘ä»¬ä»æ€ç»´é“¾æ¨ç†èƒ½åŠ›è½¬å˜ä¸ºå¼ºå¤§çš„ç›‘ç£èƒ½åŠ›ã€‚ç»è¿‡è®­ç»ƒçš„URSA-RM-7Bå……å½“éªŒè¯å™¨ï¼Œæœ‰æ•ˆæé«˜URSA-7Båœ¨æµ‹è¯•æ—¶çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒURSA-RM-7Bè¿˜å±•ç¤ºäº†å‡ºè‰²çš„ç¦»ç¾¤éªŒè¯èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹çš„æƒé‡ã€è®­ç»ƒæ•°æ®å’Œä»£ç å°†å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04686v1">PDF</a> 27 pages, 10 tables, 17 figures. The training data has been released.   The code and model are currently undergoing internal review. They will be   made available soon. Project url: <a target="_blank" rel="noopener" href="https://ursa-math.github.io/">https://ursa-math.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­ï¼Œé€šè¿‡å¼•å…¥é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å’Œè¡ç”Ÿè¿‡ç¨‹ç›‘ç£ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›çš„æ–¹æ³•ã€‚é’ˆå¯¹é«˜è´¨é‡CoTè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªä¸‰æ¨¡å—åˆæˆç­–ç•¥ï¼ŒåŒ…æ‹¬CoTè’¸é¦ã€è½¨è¿¹æ ¼å¼é‡å†™å’Œæ ¼å¼ç»Ÿä¸€ï¼Œå¹¶åˆ›å»ºäº†MMathCoT-1Mé«˜è´¨é‡CoTæ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸€ç§æ•°æ®åˆæˆç­–ç•¥ï¼Œè‡ªåŠ¨ç”Ÿæˆè¿‡ç¨‹æ³¨è§£æ•°æ®é›†DualMath-1.1Mï¼Œå¹¶ç”¨äºè¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„ç›‘ç£èƒ½åŠ›ã€‚ç»è¿‡åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„éªŒè¯ï¼ŒURSA-RM-7Bæ¨¡å‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›ã€éªŒè¯èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¢«å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†ä¸­ã€‚</li>
<li>è¡ç”Ÿè¿‡ç¨‹ç›‘ç£æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›ã€‚</li>
<li>é«˜è´¨é‡CoTè®­ç»ƒæ•°æ®çš„ç¨€ç¼ºé™åˆ¶äº†å¤šæ¨¡æ€æ•°å­¦æ¨ç†æ¨¡å‹çš„ç²¾åº¦å’Œæ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªä¸‰æ¨¡å—åˆæˆç­–ç•¥æ¥åˆ›å»ºé«˜è´¨é‡çš„CoTæ¨ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†MMathCoT-1Mã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ•°æ®åˆæˆç­–ç•¥ï¼Œè‡ªåŠ¨ç”Ÿæˆè¿‡ç¨‹æ³¨è§£æ•°æ®é›†DualMath-1.1Mï¼Œä»¥æé«˜æ¨¡å‹çš„ç›‘ç£èƒ½åŠ›ã€‚</li>
<li>URSA-RM-7Bæ¨¡å‹åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cda7ea07772c3ab96a9d26d46cfe3f50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52d9cef69f497bebd40fc2c6c35e8019.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fcf8e692b330bba3c4090259bb6cc50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8f099fa585d717af370f3d96e6c9d0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8590efd0df5753896e3448fc96700fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d676bfc744ee1459df0c59dbcdd8013.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c64753becfa2c0bb74796fc9127c6c4a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-System-2-Reasoning-in-LLMs-Learning-How-to-Think-With-Meta-Chain-of-Though"><a href="#Towards-System-2-Reasoning-in-LLMs-Learning-How-to-Think-With-Meta-Chain-of-Though" class="headerlink" title="Towards System 2 Reasoning in LLMs: Learning How to Think With Meta   Chain-of-Though"></a>Towards System 2 Reasoning in LLMs: Learning How to Think With Meta   Chain-of-Though</h2><p><strong>Authors:Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, Chelsea Finn</strong></p>
<p>We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³å…ƒæ€ç»´é“¾ï¼ˆMeta-CoTï¼‰ï¼Œå®ƒé€šè¿‡å¯¹åˆ°è¾¾ç‰¹å®šæ€ç»´é“¾ï¼ˆCoTï¼‰æ‰€éœ€çš„åŸºæœ¬æ¨ç†è¿›è¡Œæ˜¾å¼å»ºæ¨¡ï¼Œä»è€Œæ‰©å±•äº†ä¼ ç»Ÿçš„æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚æˆ‘ä»¬æä¾›äº†æ¥è‡ªè¡¨ç°å‡ºä¸ä¸Šä¸‹æ–‡æœç´¢ä¸€è‡´çš„è¡Œä¸ºçš„å…ˆè¿›æ¨¡å‹çš„å®è¯è¯æ®ï¼Œå¹¶æ¢ç´¢äº†é€šè¿‡è¿‡ç¨‹ç›‘ç£ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œæœç´¢ç®—æ³•äº§ç”Ÿå…ƒæ€ç»´é“¾çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è®­ç»ƒæ¨¡å‹äº§ç”Ÿå…ƒæ€ç»´é“¾çš„å…·ä½“æµç¨‹ï¼ŒåŒ…æ‹¬åˆ©ç”¨çº¿æ€§åŒ–æœç´¢è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ä»¥åŠå¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å¼€æ”¾çš„ç ”ç©¶é—®é¢˜ï¼ŒåŒ…æ‹¬è§„æ¨¡å®šå¾‹ã€éªŒè¯å™¨è§’è‰²å’Œå‘ç°æ–°å‹æ¨ç†ç®—æ³•çš„å¯èƒ½æ€§ã€‚è¿™é¡¹å·¥ä½œåœ¨ç†è®ºä¸å®è·µä¸Šä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å®ç°å…ƒæ€ç»´é“¾é“ºå¹³äº†é“è·¯ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸­æ›´å¼ºå¤§ã€æ›´äººæ€§åŒ–çš„æ¨ç†èƒ½åŠ›å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04682v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–æ¡†æ¶â€”â€”Meta Chain-of-Thoughtï¼ˆMeta-CoTï¼‰ï¼Œå®ƒé€šè¿‡æ˜ç¡®å»ºæ¨¡åˆ°è¾¾ç‰¹å®šChain-of-Thoughtï¼ˆCoTï¼‰æ‰€éœ€çš„åº•å±‚æ¨ç†ï¼Œæ‰©å±•äº†ä¼ ç»ŸCoTã€‚æœ¬æ–‡é€šè¿‡å®è¯è¯æ®å±•ç¤ºäº†å…ˆè¿›æ¨¡å‹ä¸ä¸Šä¸‹æ–‡æœç´¢ä¸€è‡´çš„è¡Œä¸ºï¼Œå¹¶æ¢ç´¢äº†é€šè¿‡è¿‡ç¨‹ç›‘ç£ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œæœç´¢ç®—æ³•äº§ç”ŸMeta-CoTçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–‡ç« æ¦‚è¿°äº†è®­ç»ƒæ¨¡å‹ä»¥äº§ç”ŸMeta-CoTsçš„å…·ä½“æµç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨çº¿æ€§åŒ–æœç´¢è½¨è¿¹çš„æŒ‡ä»¤å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€‚æœ€åï¼Œæœ¬æ–‡è®¨è®ºäº†å¼€æ”¾ç ”ç©¶é—®é¢˜ï¼ŒåŒ…æ‹¬è§„æ¨¡æ³•åˆ™ã€éªŒè¯å™¨è§’è‰²ä»¥åŠå‘ç°æ–°å‹æ¨ç†ç®—æ³•çš„å¯èƒ½æ€§ã€‚æœ¬ç ”ç©¶ä¸ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å®ç°Meta-CoTæä¾›äº†ç†è®ºå’Œå®è·µè·¯çº¿å›¾ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸­æ›´å¼ºå¤§ã€æ›´äººæ€§åŒ–çš„æ¨ç†é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Meta Chain-of-Thoughtï¼ˆMeta-CoTï¼‰æ¡†æ¶ï¼Œæ‰©å±•äº†ä¼ ç»ŸCoTï¼Œé€šè¿‡æ˜ç¡®å»ºæ¨¡åº•å±‚æ¨ç†æ¥æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å®è¯è¯æ®å±•ç¤ºäº†å…ˆè¿›æ¨¡å‹ä¸ä¸Šä¸‹æ–‡æœç´¢ä¸€è‡´çš„è¡Œä¸ºã€‚</li>
<li>æ¢è®¨äº†äº§ç”ŸMeta-CoTçš„ä¸‰ç§æ–¹æ³•ï¼šè¿‡ç¨‹ç›‘ç£ã€åˆæˆæ•°æ®ç”Ÿæˆå’Œæœç´¢ç®—æ³•ã€‚</li>
<li>æä¾›äº†è®­ç»ƒæ¨¡å‹ä»¥äº§ç”ŸMeta-CoTsçš„å…·ä½“æµç¨‹ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒã€çº¿æ€§åŒ–æœç´¢è½¨è¿¹å’Œå¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€‚</li>
<li>å¼ºè°ƒäº†è§„æ¨¡æ³•åˆ™ã€éªŒè¯å™¨è§’è‰²ä»¥åŠå‘ç°æ–°å‹æ¨ç†ç®—æ³•ç­‰å¼€æ”¾ç ”ç©¶é—®é¢˜çš„é‡è¦æ€§ã€‚</li>
<li>Meta-CoTæ¡†æ¶æœ‰åŠ©äºå®ç°æ›´å¼ºå¤§ã€æ›´äººæ€§åŒ–çš„AIæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2246487d3bf20c0ca5fdc351e3a3ea97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3df13c82cc8eee988c173d56b70c33cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0d5cb38c7453123f0b5c57090d68cdd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-They-the-Same-Exploring-Visual-Correspondence-Shortcomings-of-Multimodal-LLMs"><a href="#Are-They-the-Same-Exploring-Visual-Correspondence-Shortcomings-of-Multimodal-LLMs" class="headerlink" title="Are They the Same? Exploring Visual Correspondence Shortcomings of   Multimodal LLMs"></a>Are They the Same? Exploring Visual Correspondence Shortcomings of   Multimodal LLMs</h2><p><strong>Authors:Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, Lu Qi</strong></p>
<p>Recent advancements in multimodal models have shown a strong ability in visual perception, reasoning abilities, and vision-language understanding. However, studies on visual matching ability are missing, where finding the visual correspondence of objects is essential in vision research. Our research reveals that the matching capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM) benchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15 open-source datasets and Internet videos with manual annotation. We categorize the data samples of MMVM benchmark into eight aspects based on the required cues and capabilities to more comprehensively evaluate and analyze current MLLMs. In addition, we have designed an automatic annotation pipeline to generate the MMVM SFT dataset, including 220K visual matching data with reasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with two novel technical designs: fine-grained vision expert with object-level contrastive learning and instruction augmentation strategy. CoLVA achieves 51.06% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and baseline by 8.41% and 23.58% OA, respectively. The results show the effectiveness of our MMVM SFT dataset and our novel technical designs. Code, benchmark, dataset, and models are available at <a target="_blank" rel="noopener" href="https://github.com/zhouyiks/CoLVA">https://github.com/zhouyiks/CoLVA</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€æ¨¡å‹çš„è¿›å±•æ˜¾ç¤ºå‡ºå¼ºå¤§çš„è§†è§‰æ„ŸçŸ¥ã€æ¨ç†èƒ½åŠ›å’Œè§†è§‰è¯­è¨€ç†è§£ã€‚ç„¶è€Œï¼Œå…³äºè§†è§‰åŒ¹é…èƒ½åŠ›çš„ç ”ç©¶ä»æœ‰æ‰€ç¼ºå¤±ï¼Œè§†è§‰ç ”ç©¶ä¸­æ‰¾åˆ°ç‰©ä½“çš„è§†è§‰å¯¹åº”ç‰©è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨ç°æœ‰çš„å¼ºå¤§LLMæ¨¡å‹GPT-4oä¸­ï¼Œæœ€è¿‘çš„å¤šæ¨¡æ€LLMï¼ˆMLLMsï¼‰çš„åŒ¹é…èƒ½åŠ›ä»ç„¶è¡¨ç°å‡ºç³»ç»Ÿæ€§çš„ä¸è¶³ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¤šæ¨¡æ€è§†è§‰åŒ¹é…ï¼ˆMMVMï¼‰åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥å¯¹è¶…è¿‡30ç§ä¸åŒçš„MLLMè¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚MMVMåŸºå‡†æµ‹è¯•ç”±åŒ…å«æ‰‹åŠ¨æ³¨é‡Šçš„15ä¸ªå¼€æºæ•°æ®é›†å’Œäº’è”ç½‘è§†é¢‘æ„å»ºè€Œæˆã€‚æˆ‘ä»¬æ ¹æ®æ‰€éœ€çº¿ç´¢å’Œèƒ½åŠ›å°†MMVMåŸºå‡†æµ‹è¯•çš„æ•°æ®æ ·æœ¬åˆ†ä¸ºå…«ä¸ªæ–¹é¢ï¼Œä»¥æ›´å…¨é¢åœ°å¯¹å½“å‰LLMè¿›è¡Œè¯„ä¼°å’Œåˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ³¨é‡Šç®¡é“æ¥ç”ŸæˆMMVM SFTæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬å¸¦æœ‰æ¨ç†æ³¨é‡Šçš„22ä¸‡è§†è§‰åŒ¹é…æ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†CoLVAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¯¹æ¯”MLLMï¼Œå…·æœ‰ä¸¤é¡¹æ–°é¢–çš„æŠ€æœ¯è®¾è®¡ï¼šå…·æœ‰å¯¹è±¡çº§å¯¹æ¯”å­¦ä¹ çš„ç²¾ç»†è§†è§‰ä¸“å®¶å’ŒæŒ‡ä»¤å¢å¼ºç­–ç•¥ã€‚CoLVAåœ¨MMVMåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ•´ä½“å‡†ç¡®åº¦ä¸ºç™¾åˆ†ä¹‹äº”åä¸€ï¼Œè¶…è¿‡GPT-4oå’ŒåŸºçº¿æ¨¡å‹åˆ†åˆ«ä¸ºç™¾åˆ†ä¹‹å…«ç‚¹å››ä¸€å’Œç™¾åˆ†ä¹‹äºŒåä¸‰ç‚¹äº”å…«ã€‚ç»“æœè¡¨æ˜æˆ‘ä»¬çš„MMVM SFTæ•°æ®é›†å’Œæ–°é¢–çš„æŠ€æœ¯è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚ä»£ç ã€åŸºå‡†æµ‹è¯•ã€æ•°æ®é›†å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhouyiks/CoLVA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhouyiks/CoLVAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04670v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://zhouyiks.github.io/projects/CoLVA/">https://zhouyiks.github.io/projects/CoLVA/</a></p>
<p><strong>Summary</strong><br>     æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥ã€æ¨ç†èƒ½åŠ›å’Œè§†è§‰è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§èƒ½åŠ›ï¼Œä½†åœ¨è§†è§‰åŒ¹é…èƒ½åŠ›æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§çŸ­æ¿ã€‚ç ”ç©¶æ„å»ºäº†å¤šæ¨¡æ€è§†è§‰åŒ¹é…ï¼ˆMMVMï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡30ç§ä¸åŒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚MMVMåŸºå‡†æµ‹è¯•åŒ…å«æ‰‹åŠ¨æ ‡æ³¨çš„å¼€æ”¾æºæ•°æ®é›†å’Œäº’è”ç½‘è§†é¢‘æ•°æ®æ ·æœ¬ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†è‡ªåŠ¨æ ‡æ³¨ç®¡é“ç”ŸæˆMMVM SFTæ•°æ®é›†ï¼ŒåŒ…å«å¸¦æœ‰æ¨ç†æ ‡æ³¨çš„22ä¸‡è§†è§‰åŒ¹é…æ•°æ®ã€‚æœ€åï¼Œæå‡ºä¸€ç§æ–°å‹å¯¹æ¯”å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹CoLVAï¼Œå…·æœ‰ç²¾ç»†è§†è§‰ä¸“å®¶å’Œå¯¹è±¡çº§å¯¹æ¯”å­¦ä¹ ä»¥åŠæŒ‡ä»¤å¢å¼ºç­–ç•¥ç­‰æŠ€æœ¯ç‰¹ç‚¹ï¼Œåœ¨MMVMåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥ã€æ¨ç†å’Œè§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è§†è§‰åŒ¹é…èƒ½åŠ›çš„ç ”ç©¶ä»ç„¶ç¼ºå¤±ã€‚</li>
<li>è§†è§‰åŒ¹é…åœ¨è§†è§‰ç ”ç©¶ä¸­è‡³å…³é‡è¦ï¼Œæ¶‰åŠæ‰¾åˆ°å¯¹è±¡çš„è§†è§‰å¯¹åº”å…³ç³»ã€‚</li>
<li>ç°æœ‰å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ¹é…èƒ½åŠ›ä¸Šä»æœ‰ç³»ç»Ÿæ€§ä¸è¶³ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€è§†è§‰åŒ¹é…ï¼ˆMMVMï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è¶…è¿‡30ç§ä¸åŒçš„LLMsã€‚</li>
<li>MMVMåŸºå‡†æµ‹è¯•åŒ…å«æ‰‹åŠ¨æ ‡æ³¨çš„å¼€æ”¾æºæ•°æ®é›†å’Œäº’è”ç½‘è§†é¢‘æ•°æ®æ ·æœ¬ï¼Œåˆ†ä¸ºå…«ä¸ªæ–¹é¢è¿›è¡Œå…¨é¢è¯„ä¼°å’Œåˆ†æã€‚</li>
<li>è®¾è®¡äº†è‡ªåŠ¨æ ‡æ³¨ç®¡é“ç”ŸæˆMMVM SFTæ•°æ®é›†ï¼ŒåŒ…å«å¸¦æœ‰æ¨ç†æ ‡æ³¨çš„22ä¸‡è§†è§‰åŒ¹é…æ•°æ®æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08d367cd71ccdcd4f3af7ed0ddc39a3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34bdd213d91dd115fd275bad7292a2f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e25af84565edb80743729eb48718ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6999699b34d0cc74d1f9b52eb39e291e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FlairGPT-Repurposing-LLMs-for-Interior-Designs"><a href="#FlairGPT-Repurposing-LLMs-for-Interior-Designs" class="headerlink" title="FlairGPT: Repurposing LLMs for Interior Designs"></a>FlairGPT: Repurposing LLMs for Interior Designs</h2><p><strong>Authors:Gabrielle Littlefair, Niladri Shekhar Dutt, Niloy J. Mitra</strong></p>
<p>Interior design involves the careful selection and arrangement of objects to create an aesthetically pleasing, functional, and harmonized space that aligns with the clientâ€™s design brief. This task is particularly challenging, as a successful design must not only incorporate all the necessary objects in a cohesive style, but also ensure they are arranged in a way that maximizes accessibility, while adhering to a variety of affordability and usage considerations. Data-driven solutions have been proposed, but these are typically room- or domain-specific and lack explainability in their design design considerations used in producing the final layout. In this paper, we investigate if large language models (LLMs) can be directly utilized for interior design. While we find that LLMs are not yet capable of generating complete layouts, they can be effectively leveraged in a structured manner, inspired by the workflow of interior designers. By systematically probing LLMs, we can reliably generate a list of objects along with relevant constraints that guide their placement. We translate this information into a design layout graph, which is then solved using an off-the-shelf constrained optimization setup to generate the final layouts. We benchmark our algorithm in various design configurations against existing LLM-based methods and human designs, and evaluate the results using a variety of quantitative and qualitative metrics along with user studies. In summary, we demonstrate that LLMs, when used in a structured manner, can effectively generate diverse high-quality layouts, making them a viable solution for creating large-scale virtual scenes. Project webpage at <a target="_blank" rel="noopener" href="https://flairgpt.github.io/">https://flairgpt.github.io/</a> </p>
<blockquote>
<p>å®¤å†…è®¾è®¡æ¶‰åŠå¯¹è±¡çš„ç²¾å¿ƒé€‰æ‹©å’Œå¸ƒç½®ï¼Œä»¥åˆ›é€ ä¸€ä¸ªç¾è§‚ã€å®ç”¨ã€å’Œè°çš„ç©ºé—´ï¼Œè¿™ä¸ªç©ºé—´è¦ç¬¦åˆå®¢æˆ·çš„è®¾è®¡ç®€æŠ¥ã€‚è¿™ä¸€ä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¸€ä¸ªæˆåŠŸçš„è®¾è®¡ä¸ä»…è¦ä»¥åè°ƒçš„é£æ ¼èå…¥æ‰€æœ‰å¿…éœ€çš„å¯¹è±¡ï¼Œè¿˜è¦ç¡®ä¿å®ƒä»¬çš„å¸ƒç½®æ–¹å¼èƒ½æœ€å¤§é™åº¦åœ°æé«˜å¯è®¿é—®æ€§ï¼ŒåŒæ—¶è€ƒè™‘åˆ°å„ç§ç»æµæ€§å’Œä½¿ç”¨æ–¹é¢çš„å› ç´ ã€‚æ•°æ®é©±åŠ¨è§£å†³æ–¹æ¡ˆå·²ç»è¢«æå‡ºï¼Œä½†è¿™äº›è§£å†³æ–¹æ¡ˆé€šå¸¸æ˜¯é’ˆå¯¹ç‰¹å®šæˆ¿é—´æˆ–é¢†åŸŸçš„ï¼Œå¹¶ä¸”åœ¨è®¾è®¡è€ƒè™‘å› ç´ ä¸­ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œæ— æ³•ç”¨äºç”Ÿæˆæœ€ç»ˆå¸ƒå±€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ˜¯å¦å¯ä»¥ç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå®¤å†…è®¾è®¡ã€‚è™½ç„¶æˆ‘ä»¬å‘ç°LLMç›®å‰è¿˜ä¸èƒ½ç”Ÿæˆå®Œæ•´çš„å¸ƒå±€ï¼Œä½†å¯ä»¥é€šè¿‡å—å®¤å†…è®¾è®¡å¸ˆå·¥ä½œæµç¨‹å¯å‘çš„ç»“æ„åŒ–æ–¹å¼æœ‰æ•ˆåœ°åˆ©ç”¨å®ƒä»¬ã€‚é€šè¿‡ç³»ç»Ÿåœ°æ¢æµ‹LLMï¼Œæˆ‘ä»¬å¯ä»¥å¯é åœ°ç”Ÿæˆå¯¹è±¡åˆ—è¡¨ä»¥åŠæŒ‡å¯¼å…¶æ”¾ç½®çš„ç›¸å…³çº¦æŸã€‚æˆ‘ä»¬å°†è¿™äº›ä¿¡æ¯è½¬åŒ–ä¸ºè®¾è®¡å¸ƒå±€å›¾ï¼Œç„¶åä½¿ç”¨ç°æˆçš„çº¦æŸä¼˜åŒ–è®¾ç½®æ¥è§£å†³ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„å¸ƒå±€ã€‚æˆ‘ä»¬åœ¨å„ç§è®¾è®¡é…ç½®ä¸­å¯¹æˆ‘ä»¬çš„ç®—æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¯¹è±¡åŒ…æ‹¬ç°æœ‰çš„LLMæ–¹æ³•å’Œäººå·¥è®¾è®¡ï¼Œå¹¶ä½¿ç”¨å„ç§å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä»¥åŠç”¨æˆ·ç ”ç©¶æ¥è¯„ä¼°ç»“æœã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“LLMä»¥ç»“æ„åŒ–çš„æ–¹å¼ä½¿ç”¨æ—¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å¸ƒå±€ï¼Œä½¿å…¶æˆä¸ºåˆ›å»ºå¤§è§„æ¨¡è™šæ‹Ÿåœºæ™¯çš„ä¸€ç§å¯è¡Œè§£å†³æ–¹æ¡ˆã€‚é¡¹ç›®ç½‘é¡µåœ°å€æ˜¯<a target="_blank" rel="noopener" href="https://flairgpt.github.io/">https://flairgpt.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04648v1">PDF</a> Accepted at EUROGRAPHICS 2025</p>
<p><strong>Summary</strong>ï¼š<br>å®¤å†…è®¾è®¡æ¶‰åŠç²¾å¿ƒé€‰æ‹©å’Œå®‰æ’ç‰©å“ï¼Œä»¥åˆ›é€ ä¸€ä¸ªç¾è§‚ã€å®ç”¨ã€å’Œè°çš„ç©ºé—´ï¼Œç¬¦åˆå®¢æˆ·çš„è®¾è®¡éœ€æ±‚ã€‚æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®¤å†…è®¾è®¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚è™½ç„¶LLMå°šä¸èƒ½ç”Ÿæˆå®Œæ•´çš„å¸ƒå±€ï¼Œä½†å¯ä»¥é€šè¿‡ç³»ç»ŸæŸ¥è¯¢ç”Ÿæˆç‰©å“åˆ—è¡¨å’Œç›¸å…³çº¦æŸï¼ŒæŒ‡å¯¼ç‰©å“çš„æ‘†æ”¾ã€‚è¿™äº›ä¿¡æ¯è¢«è½¬åŒ–ä¸ºè®¾è®¡å¸ƒå±€å›¾ï¼Œç„¶åä½¿ç”¨ç°æˆçš„çº¦æŸä¼˜åŒ–è®¾ç½®ç”Ÿæˆæœ€ç»ˆå¸ƒå±€ã€‚æœ¬æ–‡è¯„ä¼°äº†ç®—æ³•åœ¨å„ç§è®¾è®¡é…ç½®ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸ç°æœ‰LLMæ–¹æ³•å’Œäººç±»è®¾è®¡è¿›è¡Œäº†æ¯”è¾ƒã€‚æ€»ä½“è€Œè¨€ï¼Œå½“ä»¥ç»“æ„åŒ–çš„æ–¹å¼ä½¿ç”¨æ—¶ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å¸ƒå±€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å®¤å†…è®¾è®¡æ¶‰åŠé€‰æ‹©ç‰©å“å¹¶å®‰æ’å…¶å¸ƒå±€ä»¥åˆ›é€ ç¾è§‚ã€å®ç”¨ä¸”ç¬¦åˆå®¢æˆ·è®¾è®¡éœ€æ±‚çš„å’Œè°ç©ºé—´ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®¤å†…è®¾è®¡ä¸­æœ‰åº”ç”¨æ½œåŠ›ã€‚</li>
<li>LLMå°šä¸èƒ½ç”Ÿæˆå®Œæ•´çš„å¸ƒå±€è®¾è®¡ï¼Œä½†å¯é€šè¿‡ç³»ç»ŸæŸ¥è¯¢ç”Ÿæˆç‰©å“åˆ—è¡¨å’Œç›¸å…³çº¦æŸã€‚</li>
<li>LLMç”Ÿæˆçš„ä¿¡æ¯è¢«è½¬åŒ–ä¸ºè®¾è®¡å¸ƒå±€å›¾ï¼Œç„¶åä½¿ç”¨çº¦æŸä¼˜åŒ–è®¾ç½®ç”Ÿæˆæœ€ç»ˆå¸ƒå±€ã€‚</li>
<li>ç®—æ³•åœ¨å„ç§è®¾è®¡é…ç½®ä¸­çš„è¡¨ç°å¾—åˆ°äº†è¯„ä¼°ï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•å’Œäººç±»è®¾è®¡è¿›è¡Œäº†æ¯”è¾ƒã€‚</li>
<li>LLMå¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆå¤šæ ·åŒ–çš„é«˜è´¨é‡å¸ƒå±€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf8fbdf17b67290d394cf6788de0cf64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76c4c57dec77f051ac68d9f7b93b23e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9857dbcf39dcfd3e0abff1060a54278d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d477a567b9c2b808964cde1448f3c6c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5a59b29bd790e65069a755b96bded14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2992ac651d97b95cbfeabb27f5c3f15f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="InfiGUIAgent-A-Multimodal-Generalist-GUI-Agent-with-Native-Reasoning-and-Reflection"><a href="#InfiGUIAgent-A-Multimodal-Generalist-GUI-Agent-with-Native-Reasoning-and-Reflection" class="headerlink" title="InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning   and Reflection"></a>InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning   and Reflection</h2><p><strong>Authors:Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, Fei Wu</strong></p>
<p>Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce \textit{InfiGUIAgent}, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. \textit{InfiGUIAgent} achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at \url{<a target="_blank" rel="noopener" href="https://github.com/Reallm-Labs/InfiGUIAgent%7D">https://github.com/Reallm-Labs/InfiGUIAgent}</a>. </p>
<blockquote>
<p>å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œåœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é©±åŠ¨ä¸‹ï¼Œå·²æ˜¾ç¤ºå‡ºåœ¨è®¡ç®—æœºå’Œç§»åŠ¨ç”µè¯ç­‰è®¡ç®—è®¾å¤‡ä¸Šå®ç°ä»»åŠ¡è‡ªåŠ¨åŒ–çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä»£ç†é¢ä¸´å¤šæ­¥æ¨ç†å’Œä¾èµ–æ–‡æœ¬æ³¨é‡Šçš„æŒ‘æˆ˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†åŸºäºMLLMçš„GUIä»£ç†\text{InfiGUIAgent}ï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µç›‘ç£å¾®è°ƒç®¡é“è¿›è¡Œè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µå¢å¼ºåŸºæœ¬èƒ½åŠ›ï¼Œå¦‚GUIç†è§£å’Œå®šä½èƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µåˆ©ç”¨åˆæˆæ•°æ®é›†æˆåˆ†å±‚æ¨ç†å’ŒæœŸæœ›åæ˜ æ¨ç†èƒ½åŠ›ï¼Œä»¥å®ç°ä»£ç†çš„å›ºæœ‰æ¨ç†èƒ½åŠ›ã€‚\text{InfiGUIAgent}åœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œçªæ˜¾äº†å›ºæœ‰æ¨ç†èƒ½åŠ›åœ¨æé«˜GUIäº¤äº’ä»¥å®Œæˆè‡ªåŠ¨åŒ–ä»»åŠ¡æ–¹é¢çš„ä½œç”¨ã€‚ç›¸å…³èµ„æºå¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š\url{<a target="_blank" rel="noopener" href="https://github.com/Reallm-Labs/InfiGUIAgent%7D%E3%80%82">https://github.com/Reallm-Labs/InfiGUIAgent}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04575v1">PDF</a> 14 pages, 7 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>GUIç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰Agentç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨ï¼Œåœ¨è‡ªåŠ¨å®Œæˆè®¡ç®—æœºå’Œç§»åŠ¨ç”µè¯ç­‰è®¡ç®—è®¾å¤‡çš„ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰ä»£ç†é¢ä¸´å¤šæ­¥æ¨ç†å’Œä¾èµ–æ–‡æœ¬æ³¨é‡Šçš„æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æ¨å‡ºåä¸ºInfiGUIAgentçš„æ–°å‹GUI Agentï¼Œè¯¥ä»£ç†é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œç›‘ç£ç²¾ç»†è°ƒæ•´ç®¡é“è¿›è¡Œè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µæå‡ç†è§£GUIå’ŒåŸºæœ¬æ¥åœ°æŠ€èƒ½ï¼Œç¬¬äºŒé˜¶æ®µé›†æˆå±‚æ¬¡æ¨ç†å’ŒæœŸæœ›åå°„æ¨ç†æŠ€èƒ½ï¼Œå¹¶åˆ©ç”¨åˆæˆæ•°æ®å®ç°ä»£ç†çš„æœ¬åœ°æ¨ç†èƒ½åŠ›ã€‚InfiGUIAgentåœ¨å¤šä¸ªGUIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œçªæ˜¾äº†æœ¬åœ°æ¨ç†æŠ€èƒ½åœ¨æé«˜è‡ªåŠ¨åŒ–ä»»åŠ¡çš„GUIäº¤äº’èƒ½åŠ›ä¸­çš„é‡è¦ä½œç”¨ã€‚GitHubèµ„æºåº“åœ°å€ä¸ºï¼š[é“¾æ¥åœ°å€]ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Reallm-Labs/InfiGUIAgent%EF%BC%89%E3%80%82">https://github.com/Reallm-Labs/InfiGUIAgentï¼‰ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUIç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰Agentåœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨è¿™äº›ä»£ç†çš„èƒ½åŠ›æ˜¾è‘—ã€‚</li>
<li>å½“å‰GUI Agenté¢ä¸´å¤šæ­¥æ¨ç†å’Œä¾èµ–æ–‡æœ¬æ³¨é‡Šçš„æŒ‘æˆ˜ã€‚</li>
<li>InfiGUIAgentæ˜¯ä¸€ç§æ–°å‹çš„GUI Agentï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µç›‘ç£ç²¾ç»†è°ƒæ•´ç®¡é“è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µå…³æ³¨åŸºæœ¬æŠ€èƒ½çš„å¢å¼ºï¼Œå¦‚ç†è§£GUIå’ŒåŸºæœ¬æ¥åœ°æŠ€èƒ½ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé›†æˆå±‚æ¬¡æ¨ç†å’ŒæœŸæœ›åå°„æ¨ç†æŠ€èƒ½ï¼Œå¹¶åˆ©ç”¨åˆæˆæ•°æ®å®ç°æœ¬åœ°æ¨ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f08bf9906531152b2939e94c60076855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8757b61c0f04fb9dd18906440d0db596.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ac6a9cb7c68283d1b32312a1cf3ab3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1386b5447a0d00323cf5f914f14ed3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="OpenOmni-Large-Language-Models-Pivot-Zero-shot-Omnimodal-Alignment-across-Language-with-Real-time-Self-Aware-Emotional-Speech-Synthesis"><a href="#OpenOmni-Large-Language-Models-Pivot-Zero-shot-Omnimodal-Alignment-across-Language-with-Real-time-Self-Aware-Emotional-Speech-Synthesis" class="headerlink" title="OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment   across Language with Real-time Self-Aware Emotional Speech Synthesis"></a>OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment   across Language with Real-time Self-Aware Emotional Speech Synthesis</h2><p><strong>Authors:Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Yangyi Chen, Hamid Alinejad-Rokny, Fei Huang</strong></p>
<p>Recent advancements in omnimodal learning have been achieved in understanding and generation across images, text, and speech, though mainly within proprietary models. Limited omnimodal datasets and the inherent challenges associated with real-time emotional speech generation have hindered open-source progress. To address these issues, we propose openomni, a two-stage training method combining omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pre-trained speech model is further trained on text-image tasks to generalize from vision to speech in a (near) zero-shot manner, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder facilitates real-time emotional speech through training on speech tasks and preference learning. Experiments demonstrate that openomni consistently improves across omnimodal, vision-language, and speech-language evaluations, enabling natural, emotion-rich dialogues and real-time emotional speech generation. </p>
<blockquote>
<p>å°½ç®¡ä¸»è¦åœ¨ä¸“æœ‰æ¨¡å‹å†…ï¼Œå¤šæ¨¡æ€å­¦ä¹ åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³çš„ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œæœ‰é™çš„å¤šæ¨¡æ€æ•°æ®é›†å’Œå®æ—¶æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆæ‰€å›ºæœ‰çš„æŒ‘æˆ˜é˜»ç¢äº†å¼€æºè¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†openomniï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¤šæ¨¡æ€å¯¹é½å’Œè¯­éŸ³ç”Ÿæˆçš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œä»¥å¼€å‘å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åœ¨å¯¹é½é˜¶æ®µï¼Œé¢„è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹è¿›ä¸€æ­¥åœ¨æ–‡æœ¬å›¾åƒä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ï¼ˆæ¥è¿‘ï¼‰é›¶æ ·æœ¬çš„æ–¹å¼ä»è§†è§‰æ¨å¹¿åˆ°è¯­éŸ³ï¼Œä¼˜äºåœ¨ä¸‰å…ƒæ¨¡æ€æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨è¯­éŸ³ç”Ÿæˆé˜¶æ®µï¼Œä¸€ä¸ªè½»é‡çº§çš„è§£ç å™¨é€šè¿‡è¯­éŸ³ä»»åŠ¡å’Œåå¥½å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä¾¿äºå®æ—¶æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼Œopenomniåœ¨å¤šæ¨¡æ€ã€è§†è§‰è¯­è¨€å’Œè¯­éŸ³è¯­è¨€è¯„ä¼°ä¸­å‡æœ‰æ‰€æé«˜ï¼Œå¯å®ç°è‡ªç„¶ã€æƒ…æ„Ÿä¸°å¯Œçš„å¯¹è¯å’Œå®æ—¶æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04561v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è¿‘æœŸå¤šæ¨¡æ€å­¦ä¹ åœ¨å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³æ–¹é¢çš„ç†è§£ä¸å‘å±•å–å¾—è¿›æ­¥ï¼Œä¸»è¦é›†ä¸­åœ¨ä¸“æœ‰æ¨¡å‹ä¸Šã€‚é’ˆå¯¹å¤šæ¨¡æ€æ•°æ®é›†ç¨€ç¼ºä»¥åŠå®æ—¶æƒ…ç»ªè¯­éŸ³ç”Ÿæˆé¢ä¸´çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œopenomniâ€çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€å¯¹é½å’Œè¯­éŸ³ç”Ÿæˆï¼Œä»¥å¼€å‘å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é¦–å…ˆè¿›è¡Œå¯¹é½è®­ç»ƒï¼Œåœ¨æ–‡æœ¬å›¾åƒä»»åŠ¡ä¸Šè¿›ä¸€æ­¥è®­ç»ƒé¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹ï¼Œå®ç°é›¶æ ·æœ¬æˆ–æ¥è¿‘é›¶æ ·æœ¬çš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šä¸‰æ¨¡æ€æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚å…¶æ¬¡è¿›è¡Œè¯­éŸ³ç”Ÿæˆè®­ç»ƒï¼Œé€šè¿‡è¯­éŸ³ä»»åŠ¡å’Œåå¥½å­¦ä¹ è®­ç»ƒè½»é‡çº§è§£ç å™¨ï¼Œå®ç°å®æ—¶æƒ…ç»ªè¯­éŸ³ç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œopenomniåœ¨å¤šæ¨¡æ€ã€è§†è§‰è¯­è¨€å’Œè¯­éŸ³è¯­è¨€è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯å®ç°è‡ªç„¶ã€æƒ…æ„Ÿä¸°å¯Œçš„å¯¹è¯å’Œå®æ—¶æƒ…ç»ªè¯­éŸ³ç”Ÿæˆã€‚ </p>
<p><strong>Key Takeaways</strong> </p>
<p>åˆ—å‡ºå¦‚ä¸‹è¦ç‚¹ä½œä¸ºå…³é”®æ”¶è·ç‚¹ï¼š </p>
<ol>
<li>Openomniæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ–¹æ³•ï¼Œç”¨äºå®ç°å…ˆè¿›çš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´çš„ç†è§£å’Œç”Ÿæˆã€‚ </li>
<li>å¯¹é½é˜¶æ®µåˆ©ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹è¿›è¡Œæ–‡æœ¬å›¾åƒä»»åŠ¡è®­ç»ƒï¼Œå®ç°äº†ä»è§†è§‰åˆ°è¯­éŸ³çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ </li>
<li>å¯¹æ¯”å…¶ä»–æ¨¡å‹åœ¨ä¸‰æ¨¡æ€æ•°æ®é›†ä¸Šçš„è®­ç»ƒç»“æœï¼Œæ­¤æ–¹æ³•è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚ </li>
<li>åœ¨è¯­éŸ³ç”Ÿæˆé˜¶æ®µï¼Œé€šè¿‡è½»é‡çº§è§£ç å™¨è¿›è¡Œå®æ—¶æƒ…ç»ªè¯­éŸ³ç”Ÿæˆè®­ç»ƒã€‚ </li>
<li>è¯¥æ–¹æ³•é€šè¿‡åå¥½å­¦ä¹ ä¼˜åŒ–å®æ—¶æƒ…ç»ªè¯­éŸ³ç”Ÿæˆæ•ˆæœã€‚ </li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œopenomniåœ¨å¤šæ¨¡æ€ã€è§†è§‰è¯­è¨€å’Œè¯­éŸ³è¯­è¨€è¯„ä¼°ä¸­æœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04561">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92a5d62043c3833727c19f0bd01c29d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-856339fb27e67cc4b71e7ca33d61753a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-305abf61800caf355fc1caae410cc70e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d31f7a3bcd73d4ef46049763ff3ce337.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="rStar-Math-Small-LLMs-Can-Master-Math-Reasoning-with-Self-Evolved-Deep-Thinking"><a href="#rStar-Math-Small-LLMs-Can-Master-Math-Reasoning-with-Self-Evolved-Deep-Thinking" class="headerlink" title="rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep   Thinking"></a>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep   Thinking</h2><p><strong>Authors:Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang</strong></p>
<p>We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising â€œdeep thinkingâ€ through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na&quot;ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMsâ€™ math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8&#x2F;15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/rStar">https://github.com/microsoft/rStar</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºrStar-Mathï¼Œä»¥è¯æ˜å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å¯ä»¥åœ¨æ— éœ€é«˜çº§æ¨¡å‹è’¸é¦çš„æƒ…å†µä¸‹ï¼Œä¸OpenAI o1çš„æ•°å­¦æ¨ç†èƒ½åŠ›ç›¸æŠ—è¡¡ï¼Œç”šè‡³å®ç°è¶…è¶Šã€‚rStar-Mathé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œâ€œæ·±åº¦æ€è€ƒâ€æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­æ•°å­¦ç­–ç•¥SLMåœ¨æµ‹è¯•æ—¶è¿›è¡Œæœç´¢ï¼Œç”±åŸºäºSLMçš„æµç¨‹å¥–åŠ±æ¨¡å‹è¿›è¡Œå¼•å¯¼ã€‚rStar-Mathé’ˆå¯¹è®­ç»ƒä¸¤ä¸ªSLMçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸‰é¡¹åˆ›æ–°ï¼šï¼ˆ1ï¼‰ä¸€ç§æ–°å‹ä»£ç å¢å¼ºCoTæ•°æ®åˆæˆæ–¹æ³•ï¼Œå®ƒæ‰§è¡Œå¤§é‡çš„MCTSå›åˆåˆ¶æ¨¡æ‹Ÿï¼Œä»¥ç”Ÿæˆç”¨äºè®­ç»ƒç­–ç•¥SLMçš„ç»è¿‡é€æ­¥éªŒè¯çš„æ¨ç†è½¨è¿¹ï¼›ï¼ˆ2ï¼‰ä¸€ç§æ–°å‹æµç¨‹å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œé¿å…äº†ç®€å•çš„æ­¥éª¤çº§è¯„åˆ†æ³¨é‡Šï¼Œä»è€Œäº§ç”Ÿäº†æ›´æœ‰æ•ˆçš„æµç¨‹åå¥½æ¨¡å‹ï¼ˆPPMï¼‰ï¼›ï¼ˆ3ï¼‰ä¸€ç§è‡ªæˆ‘è¿›åŒ–æ–¹æ³•ï¼Œå…¶ä¸­ç­–ç•¥SLMå’ŒPPMä»é›¶å¼€å§‹æ„å»ºï¼Œå¹¶é€šè¿‡è¿­ä»£è¿›åŒ–æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä¸º747kæ•°å­¦é—®é¢˜åˆæˆæ•°ç™¾ä¸‡è§£å†³æ–¹æ¡ˆæ¥è¿›è¡Œå››è½®è‡ªæˆ‘è¿›åŒ–ï¼ŒrStar-Mathæ¨åŠ¨äº†SLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒå°†Qwen2.5-Math-7Bä»58.8%æé«˜åˆ°90.0%ï¼Œå°†Phi3-mini-3.8Bä»41.4%æé«˜åˆ°86.4%ï¼Œè¶…è¿‡äº†o1-previewçš„+4.5%å’Œ+0.9%ã€‚åœ¨ç¾å›½æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆAIMEï¼‰ä¸­ï¼ŒrStar-Mathå¹³å‡è§£å†³äº†8&#x2F;15çš„é—®é¢˜ï¼Œä½åˆ—å‰20%ï¼Œç›¸å½“äºé¡¶å°–é«˜ä¸­ç”Ÿæ•°å­¦æ°´å¹³ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/rStar%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/microsoft/rStarä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04519v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>rStar-Mathå±•ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å¯ä»¥é€šè¿‡Monte Carloæ ‘æœç´¢ï¼ˆMCTSï¼‰çš„â€œæ·±åº¦æ€è€ƒâ€æ–¹å¼ï¼Œåœ¨ä¸éœ€è¦é«˜çº§æ¨¡å‹è’¸é¦çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ç”šè‡³è¶…è¶ŠOpenAIåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚rStar-Mathé€šè¿‡ä¸‰é¡¹åˆ›æ–°è§£å†³è®­ç»ƒä¸¤ä¸ªSLMæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼šä¸€ç§æ–°å‹çš„ä»£ç å¢å¼ºCoTæ•°æ®åˆæˆæ–¹æ³•ã€ä¸€ç§æ–°å‹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•ä»¥åŠä¸€ç§è‡ªæˆ‘è¿›åŒ–ç­–ç•¥ã€‚ç»è¿‡å¤šè½®è‡ªæˆ‘è¿›åŒ–ï¼Œä»¥åŠä¸º747kæ•°å­¦é—®é¢˜åˆæˆæ•°ç™¾ä¸‡è§£å†³æ–¹æ¡ˆï¼ŒrStar-Mathå°†SLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›æå‡è‡³æœ€æ–°æ°´å¹³ã€‚åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å°†Qwen2.5-Math-7Bä¸Phi3-mini-3.8Bçš„è¡¨ç°ä»ä¹‹å‰çš„ç™¾åˆ†æ¯”æå‡åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨USA Math Olympiadï¼ˆAIMEï¼‰ä¸Šå¹³å‡è§£å†³äº†8ä¸ªé—®é¢˜ä¸­çš„å¤§çº¦ä¸€åŠï¼Œè¡¨ç°ä¼˜äºå¤§å¤šæ•°é«˜ä¸­ç”Ÿã€‚æ›´å¤šä¿¡æ¯å’Œèµ„æºå¯åœ¨GitHubçš„rStaré¡¹ç›®ä¸­è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>rStar-Mathå±•ç¤ºäº†å°å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„å¼ºå¤§èƒ½åŠ›ï¼Œå¯åŒ¹æ•Œç”šè‡³è¶…è¶ŠOpenAI o1æ¨¡å‹ã€‚</li>
<li>rStar-Mathé€šè¿‡Monte Carloæ ‘æœç´¢å®ç°æ·±åº¦æ€è€ƒï¼Œæ— éœ€é«˜çº§æ¨¡å‹è’¸é¦ã€‚</li>
<li>rStar-Mathå¼•å…¥ä¸‰é¡¹åˆ›æ–°æ¥è§£å†³è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®åˆæˆã€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè‡ªæˆ‘è¿›åŒ–ç­–ç•¥ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘è¿›åŒ–ä¸å¤§é‡åˆæˆè§£å†³æ–¹æ¡ˆï¼ŒrStar-Mathæå‡äº†è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›è‡³æœ€æ–°æ°´å¹³ã€‚</li>
<li>åœ¨MATHåŸºå‡†æµ‹è¯•ä¸­ï¼ŒrStar-Mathæ˜¾è‘—æé«˜äº†ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>rStar-Mathåœ¨MATH Olympiadä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡è§£å†³åŠæ•°ä»¥ä¸Šé—®é¢˜ï¼Œå±•ç°å‡ºé«˜æ°´å‡†çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b493fcdc79ae693de913bef5c616df72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc0e347cdba3648d54da49af0e69d2b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d525dec5839a75c60ca8ed70086672b1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CGP-Tuning-Structure-Aware-Soft-Prompt-Tuning-for-Code-Vulnerability-Detection"><a href="#CGP-Tuning-Structure-Aware-Soft-Prompt-Tuning-for-Code-Vulnerability-Detection" class="headerlink" title="CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability   Detection"></a>CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability   Detection</h2><p><strong>Authors:Ruijun Feng, Hammond Pearce, Pietro Liguori, Yulei Sui</strong></p>
<p>Large language models (LLMs) have been proposed as powerful tools for detecting software vulnerabilities, where task-specific fine-tuning is typically employed to provide vulnerability-specific knowledge to the LLMs for this purpose. However, traditional full-parameter fine-tuning is inefficient for modern, complex LLMs, which contain billions of parameters.   Soft prompt tuning has been suggested as a more efficient alternative for fine-tuning LLMs in general cases. However, pure soft prompt tuning treats source code as plain text, losing structural information inherent in source code. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to address this issue, are unable to preserve the rich semantic information within code graphs, as they are primarily designed for general graph-related tasks and focus more on adjacency information. They also fail to ensure computational efficiency while accounting for graph-text interactions.   This paper, therefore, introduces a new code graph-enhanced, structure-aware soft prompt tuning method for vulnerability detection, referred to as CGP-Tuning. It employs innovative type-aware embeddings to capture the rich semantic information within code graphs, along with a novel and efficient cross-modal alignment module that achieves linear computational cost while incorporating graph-text interactions. The proposed CGP-Tuning is evaluated on the latest DiverseVul dataset and the most recent open-source code LLMs, CodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning outperforms the best state-of-the-art method by an average of 3.5 percentage points in accuracy, without compromising its vulnerability detection capabilities for long source code. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«æè®®ä½œä¸ºæ£€æµ‹è½¯ä»¶æ¼æ´çš„å¼ºå¤§å·¥å…·ã€‚é€šå¸¸ï¼Œä¼šé‡‡ç”¨ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼ˆfine-tuningï¼‰æ¥ä¸ºLLMæä¾›é’ˆå¯¹æ¼æ´çš„ç‰¹å®šçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¯¹äºåŒ…å«æ•°åäº¿å‚æ•°çš„ç°ä»£å¤æ‚LLMï¼Œä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒæ˜¯ä¸é«˜æ•ˆçš„ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè½¯æç¤ºè°ƒæ•´ï¼ˆsoft prompt tuningï¼‰è¢«å»ºè®®ä¸ºä¸€ç§æ›´é«˜æ•ˆçš„LLMå¾®è°ƒæ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œçº¯è½¯æç¤ºè°ƒæ•´å°†æºä»£ç è§†ä¸ºçº¯æ–‡æœ¬ï¼Œä»è€Œå¤±å»äº†æºä»£ç ä¸­å›ºæœ‰çš„ç»“æ„ä¿¡æ¯ã€‚åŒæ—¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜çš„å›¾å¢å¼ºè½¯æç¤ºè°ƒæ•´æ–¹æ³•æ— æ³•ä¿ç•™ä»£ç å›¾ä¸­çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦é’ˆå¯¹ä¸€èˆ¬å›¾ç›¸å…³ä»»åŠ¡è€Œè®¾è®¡ï¼Œæ›´ä¾§é‡äºé‚»æ¥ä¿¡æ¯ã€‚å®ƒä»¬ä¹Ÿæœªèƒ½ç¡®ä¿è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶è€ƒè™‘å›¾æ–‡äº¤äº’ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç”¨äºæ¼æ´æ£€æµ‹çš„ä»£ç å›¾å¢å¼ºã€ç»“æ„æ„ŸçŸ¥è½¯æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œç§°ä¸ºCGP-Tuningã€‚å®ƒé‡‡ç”¨åˆ›æ–°çš„ç±»å‹æ„ŸçŸ¥åµŒå…¥æ¥æ•è·ä»£ç å›¾ä¸­çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»¥åŠä¸€ä¸ªé«˜æ•ˆçš„æ–°å‹è·¨æ¨¡æ€å¯¹é½æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è®¡ç®—æˆæœ¬ä¸ºçº¿æ€§çš„åŒæ—¶å®ç°äº†å›¾æ–‡äº¤äº’ã€‚æå‡ºçš„CGP-Tuningåœ¨æœ€æ–°çš„DiverseVulæ•°æ®é›†å’Œæœ€æ–°çš„å¼€æºä»£ç LLMï¼ˆCodeLlamaå’ŒCodeGemmaï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCGP-Tuningåœ¨å‡†ç¡®ç‡ä¸Šå¹³å‡ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•3.5ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶å¯¹äºé•¿æºä»£ç çš„æ¼æ´æ£€æµ‹èƒ½åŠ›ä¸å¦¥åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04510v1">PDF</a> 14 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹è½¯ä»¶æ¼æ´æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œé€šå¸¸é€šè¿‡ç‰¹å®šä»»åŠ¡å¾®è°ƒæ¥æä¾›æ¼æ´ç‰¹å®šçŸ¥è¯†ã€‚ç„¶è€Œï¼Œå¯¹äºç°ä»£å¤æ‚çš„LLMï¼Œä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒæ•ˆç‡ä½ä¸‹ã€‚è½¯æç¤ºè°ƒä½œä¸ºä¸€ç§æ›´é«˜æ•ˆçš„LLMå¾®è°ƒæ–¹æ³•è¢«æå‡ºï¼Œä½†çº¯è½¯æç¤ºè°ƒå°†æºä»£ç è§†ä¸ºçº¯æ–‡æœ¬ï¼Œä¸¢å¤±äº†æºä»£ç çš„å†…åœ¨ç»“æ„ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç å›¾å¢å¼ºç»“æ„æ„ŸçŸ¥è½¯æç¤ºè°ƒæ–¹æ³•ï¼Œç§°ä¸ºCGP-Tuningï¼Œç”¨äºæ¼æ´æ£€æµ‹ã€‚å®ƒé‡‡ç”¨ç±»å‹æ„ŸçŸ¥åµŒå…¥æ•æ‰ä»£ç å›¾ä¸­çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»¥åŠé«˜æ•ˆè·¨æ¨¡æ€å¯¹é½æ¨¡å—ï¼Œå®ç°çº¿æ€§è®¡ç®—æˆæœ¬å¹¶èå…¥å›¾æ–‡æœ¬äº¤äº’ã€‚åœ¨æœ€æ–°çš„DiverseVulæ•°æ®é›†å’Œå¼€æºä»£ç LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCGP-Tuningåœ¨å‡†ç¡®ç‡ä¸Šå¹³å‡è¶…å‡ºæœ€ä½³ç°æœ‰æ–¹æ³•3.5ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”å¯¹é•¿æºä»£ç çš„æ¼æ´æ£€æµ‹èƒ½åŠ›ä¸å—å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡æ£€æµ‹è½¯ä»¶æ¼æ´çš„æ½œåŠ›ï¼Œé€šè¿‡ç‰¹å®šä»»åŠ¡å¾®è°ƒæä¾›æ¼æ´ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>ä¼ ç»Ÿå…¨å‚æ•°å¾®è°ƒå¯¹äºç°ä»£å¤æ‚çš„LLMæ•ˆç‡ä½ä¸‹ã€‚</li>
<li>è½¯æç¤ºè°ƒæ–¹æ³•è¢«è§†ä¸ºæ›´é«˜æ•ˆçš„LLMå¾®è°ƒæ›¿ä»£æ–¹æ¡ˆï¼Œä½†å­˜åœ¨å¤„ç†æºä»£ç æ—¶ä¸¢å¤±ç»“æ„ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>CGP-Tuningæ˜¯ä¸€ç§æ–°çš„ä»£ç å›¾å¢å¼ºç»“æ„æ„ŸçŸ¥è½¯æç¤ºè°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>CGP-Tuningé‡‡ç”¨ç±»å‹æ„ŸçŸ¥åµŒå…¥å’Œé«˜æ•ˆè·¨æ¨¡æ€å¯¹é½æ¨¡å—ã€‚</li>
<li>CGP-Tuningåœ¨æœ€æ–°çš„æ•°æ®é›†å’ŒLLMä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e9e07b8e395e4c60576ac120e6805ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab08051f6a7c51a1242afdcbe80e805e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475198b408dc0a5a3830da56f6ed9ce7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4acea4b810adf9938ffd5371b229372.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-178f8e3831f3c66c3705587c79dd5b95.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Integrating-remote-sensing-data-assimilation-deep-learning-and-large-language-model-for-interactive-wheat-breeding-yield-prediction"><a href="#Integrating-remote-sensing-data-assimilation-deep-learning-and-large-language-model-for-interactive-wheat-breeding-yield-prediction" class="headerlink" title="Integrating remote sensing data assimilation, deep learning and large   language model for interactive wheat breeding yield prediction"></a>Integrating remote sensing data assimilation, deep learning and large   language model for interactive wheat breeding yield prediction</h2><p><strong>Authors:Guofeng Yang, Nanfei Jin, Wenjie Ai, Zhonghua Zheng, Yuhong He, Yong He</strong></p>
<p>Yield is one of the core goals of crop breeding. By predicting the potential yield of different breeding materials, breeders can screen these materials at various growth stages to select the best performing. Based on unmanned aerial vehicle remote sensing technology, high-throughput crop phenotyping data in breeding areas is collected to provide data support for the breeding decisions of breeders. However, the accuracy of current yield predictions still requires improvement, and the usability and user-friendliness of yield forecasting tools remain suboptimal. To address these challenges, this study introduces a hybrid method and tool for crop yield prediction, designed to allow breeders to interactively and accurately predict wheat yield by chatting with a large language model (LLM). First, the newly designed data assimilation algorithm is used to assimilate the leaf area index into the WOFOST model. Then, selected outputs from the assimilation process, along with remote sensing inversion results, are used to drive the time-series temporal fusion transformer model for wheat yield prediction. Finally, based on this hybrid method and leveraging an LLM with retrieval augmented generation technology, we developed an interactive yield prediction Web tool that is user-friendly and supports sustainable data updates. This tool integrates multi-source data to assist breeding decision-making. This study aims to accelerate the identification of high-yield materials in the breeding process, enhance breeding efficiency, and enable more scientific and smart breeding decisions. </p>
<blockquote>
<p>äº§é‡æ˜¯ä½œç‰©è‚²ç§çš„æ ¸å¿ƒç›®æ ‡ä¹‹ä¸€ã€‚é€šè¿‡é¢„æµ‹ä¸åŒè‚²ç§ææ–™çš„æ½œåœ¨äº§é‡ï¼Œè‚²ç§å®¶å¯ä»¥åœ¨å„ä¸ªç”Ÿé•¿é˜¶æ®µå¯¹è¿™äº›ææ–™è¿›è¡Œç­›é€‰ï¼Œä»¥é€‰æ‹©è¡¨ç°æœ€ä½³çš„å“ç§ã€‚åŸºäºæ— äººæœºé¥æ„ŸæŠ€æœ¯ï¼Œæ”¶é›†è‚²ç§åŒºçš„é«˜é€šé‡ä½œç‰©è¡¨å‹æ•°æ®ï¼Œä¸ºè‚²ç§å®¶çš„è‚²ç§å†³ç­–æä¾›äº†æ•°æ®æ”¯æŒã€‚ç„¶è€Œï¼Œå½“å‰äº§é‡é¢„æµ‹çš„å‡†ç¡®åº¦ä»æœ‰å¾…æé«˜ï¼Œäº§é‡é¢„æµ‹å·¥å…·çš„ä½¿ç”¨æ€§å’Œå‹å¥½æ€§ä¹Ÿä¸å°½å¦‚äººæ„ã€‚æœ¬ç ”ç©¶ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥äº†ä¸€ç§ç”¨äºä½œç‰©äº§é‡é¢„æµ‹çš„æ··åˆæ–¹æ³•å’Œå·¥å…·ï¼Œè®¾è®¡è¯¥å·¥å…·çš„ç›®çš„æ˜¯è®©è‚²ç§å®¶é€šè¿‡ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èŠå¤©ï¼Œä»¥äº¤äº’æ–¹å¼å‡†ç¡®é¢„æµ‹å°éº¦äº§é‡ã€‚é¦–å…ˆï¼Œä½¿ç”¨æ–°è®¾è®¡çš„æ•°æ®åŒåŒ–ç®—æ³•å°†å¶é¢ç§¯æŒ‡æ•°åŒåŒ–åˆ°WOFOSTæ¨¡å‹ä¸­ã€‚ç„¶åï¼Œå°†åŒåŒ–è¿‡ç¨‹é€‰æ‹©çš„è¾“å‡ºä¸é¥æ„Ÿåæ¼”ç»“æœç›¸ç»“åˆï¼Œç”¨äºé©±åŠ¨æ—¶é—´åºåˆ—æ—¶é—´èåˆå˜å‹å™¨æ¨¡å‹è¿›è¡Œå°éº¦äº§é‡é¢„æµ‹ã€‚æœ€åï¼ŒåŸºäºè¿™ç§æ··åˆæ–¹æ³•ï¼Œå¹¶åˆ©ç”¨å…·æœ‰æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäº¤äº’å¼çš„äº§é‡é¢„æµ‹ç½‘ç»œå·¥å…·ï¼Œè¯¥å·¥å…·å‹å¥½ä¸”æ”¯æŒå¯æŒç»­æ•°æ®æ›´æ–°ã€‚è¯¥å·¥å…·æ•´åˆäº†å¤šæºæ•°æ®ï¼Œè¾…åŠ©è‚²ç§å†³ç­–ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åŠ å¿«è‚²ç§è¿‡ç¨‹ä¸­é«˜äº§é‡ææ–™çš„è¯†åˆ«ï¼Œæé«˜è‚²ç§æ•ˆç‡ï¼Œå¹¶åšå‡ºæ›´ç§‘å­¦å’Œæ™ºèƒ½çš„è‚²ç§å†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04487v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºæ— äººæœºé¥æ„ŸæŠ€æœ¯çš„é«˜é€šé‡ä½œç‰©è¡¨å‹æ•°æ®ä¸ºè‚²ç§å†³ç­–æä¾›äº†æ•°æ®æ”¯æŒã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ··åˆæ–¹æ³•å’Œå·¥å…·ï¼Œæ—¨åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‚²ç§è€…è¿›è¡Œäº¤äº’ï¼Œå‡†ç¡®é¢„æµ‹å°éº¦äº§é‡ã€‚è¯¥å·¥å…·é‡‡ç”¨æ–°çš„æ•°æ®åŒåŒ–ç®—æ³•å°†å¶ç‰‡æŒ‡æ•°èå…¥WOFOSTæ¨¡å‹ä¸­ï¼Œåˆ©ç”¨åŒåŒ–è¿‡ç¨‹å’Œé¥æ„Ÿåæ¼”ç»“æœé©±åŠ¨æ—¶é—´åºåˆ—æ—¶åºèåˆå™¨æ¨¡å‹é¢„æµ‹å°éº¦äº§é‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªäº¤äº’å¼åœ¨çº¿äº§é‡é¢„æµ‹å·¥å…·ï¼Œæ•´åˆå¤šæºæ•°æ®è¾…åŠ©è‚²ç§å†³ç­–ï¼Œæ—¨åœ¨åŠ é€Ÿè‚²ç§è¿‡ç¨‹ä¸­é«˜äº§ææ–™çš„è¯†åˆ«ï¼Œæé«˜è‚²ç§æ•ˆç‡ï¼Œä½¿è‚²ç§å†³ç­–æ›´åŠ ç§‘å­¦å’Œæ™ºèƒ½åŒ–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨æ— äººæœºé¥æ„ŸæŠ€æœ¯æ”¶é›†è‚²ç§åŒºåŸŸçš„é«˜é€šé‡ä½œç‰©è¡¨å‹æ•°æ®ï¼Œä¸ºè‚²ç§å†³ç­–æä¾›æ”¯æŒã€‚</li>
<li>å½“å‰äº§é‡é¢„æµ‹çš„å‡†ç¡®æ€§å’Œç”¨æˆ·å‹å¥½æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>å¼•å…¥æ··åˆæ–¹æ³•ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°ä¸è‚²ç§è€…çš„äº¤äº’å¼äº§é‡é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨æ–°çš„æ•°æ®åŒåŒ–ç®—æ³•å°†å¶ç‰‡æŒ‡æ•°èå…¥WOFOSTæ¨¡å‹ä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆåŒåŒ–è¿‡ç¨‹å’Œé¥æ„Ÿåæ¼”ç»“æœï¼Œåˆ©ç”¨æ—¶é—´åºåˆ—èåˆå™¨æ¨¡å‹è¿›è¡Œå°éº¦äº§é‡é¢„æµ‹ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„åœ¨çº¿äº§é‡é¢„æµ‹å·¥å…·ï¼Œæ”¯æŒå¯æŒç»­æ•°æ®æ›´æ–°ï¼Œæ•´åˆå¤šæºæ•°æ®è¾…åŠ©è‚²ç§å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2add94364bf31c8aa47fa8f44d712dcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cc87636c0da101f962155441ff70696.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d2b70b317c5630495c36a95f7d7831d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2c55b3c6a8e8a2d82b70e8bca3d257e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hidden-Entity-Detection-from-GitHub-Leveraging-Large-Language-Models"><a href="#Hidden-Entity-Detection-from-GitHub-Leveraging-Large-Language-Models" class="headerlink" title="Hidden Entity Detection from GitHub Leveraging Large Language Models"></a>Hidden Entity Detection from GitHub Leveraging Large Language Models</h2><p><strong>Authors:Lu Gan, Martin Blum, Danilo Dessi, Brigitte Mathiak, Ralf Schenkel, Stefan Dietze</strong></p>
<p>Named entity recognition is an important task when constructing knowledge bases from unstructured data sources. Whereas entity detection methods mostly rely on extensive training data, Large Language Models (LLMs) have paved the way towards approaches that rely on zero-shot learning (ZSL) or few-shot learning (FSL) by taking advantage of the capabilities LLMs acquired during pretraining. Specifically, in very specialized scenarios where large-scale training data is not available, ZSL &#x2F; FSL opens new opportunities. This paper follows this recent trend and investigates the potential of leveraging Large Language Models (LLMs) in such scenarios to automatically detect datasets and software within textual content from GitHub repositories. While existing methods focused solely on named entities, this study aims to broaden the scope by incorporating resources such as repositories and online hubs where entities are also represented by URLs. The study explores different FSL prompt learning approaches to enhance the LLMsâ€™ ability to identify dataset and software mentions within repository texts. Through analyses of LLM effectiveness and learning strategies, this paper offers insights into the potential of advanced language models for automated entity detection. </p>
<blockquote>
<p>ä»éç»“æ„åŒ–æ•°æ®æºæ„å»ºçŸ¥è¯†åº“æ—¶ï¼Œå®ä½“è¯†åˆ«æ˜¯ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚è™½ç„¶å®ä½“æ£€æµ‹æ–¹æ³•å¤§å¤šä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€è¾Ÿäº†ä¾èµ–é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZSLï¼‰æˆ–å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„æ–¹æ³•çš„é“è·¯ï¼Œåˆ©ç”¨LLMåœ¨é¢„è®­ç»ƒæœŸé—´è·å¾—çš„èƒ½åŠ›ã€‚ç‰¹åˆ«åœ°ï¼Œåœ¨æ— æ³•ä½¿ç”¨å¤§è§„æ¨¡è®­ç»ƒæ•°æ®çš„ä¸“ä¸šåœºæ™¯ä¸­ï¼ŒZSL&#x2F;FSLå¸¦æ¥äº†æ–°çš„æœºä¼šã€‚æœ¬æ–‡éµå¾ªè¿™ä¸€æœ€æ–°è¶‹åŠ¿ï¼Œæ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨GitHubä»“åº“çš„æ–‡æœ¬å†…å®¹ä¸­è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†å’Œè½¯ä»¶å·¥å…·çš„æ½œåŠ›ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•åªå…³æ³¨å‘½åå®ä½“ï¼Œä½†æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼•å…¥ä»“åº“å’Œåœ¨çº¿ä¸­å¿ƒç­‰èµ„æºï¼ˆå…¶ä¸­å®ä½“ä¹Ÿä»¥URLè¡¨ç¤ºï¼‰æ¥æ‰©å¤§èŒƒå›´ã€‚è¯¥ç ”ç©¶æ¢ç´¢äº†ä¸åŒçš„FSLæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜LLMåœ¨ä»“åº“æ–‡æœ¬ä¸­è¯†åˆ«æ•°æ®é›†å’Œè½¯ä»¶æåŠçš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹LLMçš„æœ‰æ•ˆæ€§å’Œå­¦ä¹ ç­–ç•¥çš„åˆ†æï¼Œæœ¬æ–‡æä¾›äº†å¯¹é«˜çº§è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨å®ä½“æ£€æµ‹æ–¹é¢çš„æ½œåŠ›çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04455v1">PDF</a> accepted by KDD2024 workshop DL4KG</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å‘½åå®ä½“è¯†åˆ«æ˜¯ä»éç»“æ„åŒ–æ•°æ®æºæ„å»ºçŸ¥è¯†åº“æ—¶çš„é‡è¦ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„å®ä½“æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ä¸ºé‚£äº›ä¾é é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZSLï¼‰æˆ–å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„æ–¹æ³•é“ºå¹³äº†é“è·¯ã€‚ç‰¹åˆ«æ˜¯å½“å¤§è§„æ¨¡è®­ç»ƒæ•°æ®æ— æ³•è·å–æ—¶ï¼ŒZSL&#x2F;FSLæ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡éµå¾ªè¿™ä¸€æœ€æ–°è¶‹åŠ¿ï¼Œæ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨GitHubå­˜å‚¨åº“ä¸­è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†å’Œè½¯ä»¶çš„æŠ€æœ¯æ½œåŠ›ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å‘½åå®ä½“ä¸Šï¼Œè€Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡å¼•å…¥èµ„æºï¼ˆå¦‚å­˜å‚¨åº“å’Œåœ¨çº¿ä¸­å¿ƒï¼‰ï¼Œå…¶ä¸­å®ä½“ä¹Ÿç”±URLè¡¨ç¤ºï¼Œä»è€Œæ‰©å¤§èŒƒå›´ã€‚è¯¥ç ”ç©¶æ¢ç´¢äº†ä¸åŒçš„FSLæç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜LLMåœ¨å­˜å‚¨åº“æ–‡æœ¬ä¸­è¯†åˆ«æ•°æ®é›†å’Œè½¯ä»¶æåŠçš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹LLMçš„æ•ˆæœå’Œå­¦ä¹ ç­–ç•¥çš„åˆ†æï¼Œæœ¬æ–‡æä¾›äº†é«˜çº§è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨å®ä½“æ£€æµ‹æ–¹é¢çš„æ½œåŠ›çš„è§è§£ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†åº“æ„å»ºä¸­èµ·åˆ°é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZSLï¼‰å’Œå°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ç¯å¢ƒä¸‹ã€‚</li>
<li>å½“å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ä¸å¯è·å–æ—¶ï¼ŒLLMsçš„æ½œåŠ›åœ¨ç‰¹æ®Šåœºæ™¯ä¸­å°¤ä¸ºçªå‡ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å‘½åå®ä½“çš„è¯†åˆ«ï¼Œè€Œæœ¬ç ”ç©¶æ‰©å±•äº†è¿™ä¸€èŒƒå›´ï¼Œè€ƒè™‘äº†åŒ…æ‹¬URLåœ¨å†…çš„å…¶ä»–èµ„æºè¡¨ç¤ºå®ä½“ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†ä¸åŒçš„FSLæç¤ºå­¦ä¹ æ–¹æ³•ä»¥å¢å¼ºLLMè¯†åˆ«æ•°æ®é›†å’Œè½¯ä»¶æåŠçš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡GitHubå­˜å‚¨åº“æ–‡æœ¬è¿›è¡Œå®ä½“æ£€æµ‹æ˜¯ä¸€ä¸ªæ–°é¢–ä¸”å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>LLMsåœ¨è‡ªåŠ¨å®ä½“æ£€æµ‹æ–¹é¢çš„æ½œåŠ›å·¨å¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„æ–‡æœ¬æ•°æ®æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a41a168e56612e8792b77d690cdb1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c89543b75437b3d3f9c82ddbb0b5b10b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f18f7fff3c7cd82076faed0eb309aa.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Zero-Shot-Open-Vocabulary-Pipeline-for-Dialogue-Understanding"><a href="#A-Zero-Shot-Open-Vocabulary-Pipeline-for-Dialogue-Understanding" class="headerlink" title="A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding"></a>A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding</h2><p><strong>Authors:Abdulfattah Safa, GÃ¶zde GÃ¼l Åahin</strong></p>
<p>Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20% better Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1, with up to 90% fewer requests to the LLM API. </p>
<blockquote>
<p>å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰å¯¹äºç†è§£ç”¨æˆ·éœ€æ±‚å’Œåœ¨æ‰§è¡Œä»»åŠ¡å¯¼å‘å‹å¯¹è¯ä¸­æ‰§è¡Œé€‚å½“çš„ç³»ç»Ÿæ“ä½œè‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¤§å¤šæ•°DSTæ–¹æ³•éƒ½æ˜¯ä¸ºé¢„å®šä¹‰çš„ç±»è®¾è®¡å·¥ä½œçš„ï¼Œå¹¶å‡å®šæœ‰é»„é‡‘é¢†åŸŸæ ‡ç­¾å¯ç”¨ï¼Œéš¾ä»¥é€‚åº”æ–°çš„æ’æ§½å€¼ã€‚è™½ç„¶åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç³»ç»Ÿæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„é›¶å°„å‡»DSTæ€§èƒ½ï¼Œä½†å®ƒä»¬è¦ä¹ˆéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¦ä¹ˆå®ƒä»¬çš„è¡¨ç°ä½äºç°æœ‰çš„å®Œå…¨è®­ç»ƒçš„ç³»ç»Ÿï¼Œä»è€Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶å°„ã€å¼€æ”¾è¯æ±‡çš„ç³»ç»Ÿï¼Œå®ƒå°†é¢†åŸŸåˆ†ç±»å’ŒDSTé›†æˆåœ¨ä¸€ä¸ªå•ä¸€çš„ç®¡é“ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å°†DSTé‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªé—®ç­”ä»»åŠ¡ï¼Œä»¥ä¾›èƒ½åŠ›è¾ƒå·®çš„æ¨¡å‹ä½¿ç”¨ï¼Œå¹¶ä¸ºé€‚åº”æ€§æ›´å¼ºçš„æ¨¡å‹ä½¿ç”¨è‡ªæˆ‘å®Œå–„æç¤ºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä¸ä¾èµ–äºæœ¬ä½“è®ºä¸­å®šä¹‰çš„å›ºå®šæ’æ§½å€¼ï¼Œå…è®¸ç³»ç»ŸåŠ¨æ€é€‚åº”ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰çš„æœ€ä½³æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯æ˜å®ƒåœ¨Multi-WOZ 2.1ç­‰æ•°æ®é›†ä¸Šæä¾›äº†é«˜è¾¾20%çš„è”åˆç›®æ ‡å‡†ç¡®ç‡ï¼ˆJGAï¼‰ï¼Œè€Œä¸”å¯¹LLM APIçš„è¯·æ±‚å‡å°‘äº†é«˜è¾¾90%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15861v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ä¸­ç†è§£ç”¨æˆ·éœ€æ±‚å¹¶æ‰§è¡Œé€‚å½“ç³»ç»Ÿæ“ä½œè‡³å…³é‡è¦ã€‚ç°æœ‰çš„DSTæ–¹æ³•å¤§å¤šåœ¨é¢„è®¾çš„æœ¬ä½“è®ºå†…è®¾è®¡ï¼Œå¹¶å‡å®šé»„é‡‘é¢†åŸŸæ ‡ç­¾çš„å¯ç”¨æ€§ï¼Œéš¾ä»¥é€‚åº”æ–°æ§½ä½å€¼ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„ç³»ç»Ÿå±•ç°å‡ºæœ‰å‰æ™¯çš„é›¶æ ·æœ¬DSTæ€§èƒ½ï¼Œä½†å®ƒä»¬éœ€è¦åºå¤§çš„è®¡ç®—èµ„æºæˆ–æ€§èƒ½ä½äºç°æœ‰çš„å®Œå…¨è®­ç»ƒç³»ç»Ÿï¼Œé™åˆ¶äº†å®ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé›¶æ ·æœ¬ã€å¼€æ”¾è¯æ±‡çš„ç³»ç»Ÿï¼Œå°†é¢†åŸŸåˆ†ç±»å’ŒDSTæ•´åˆåœ¨ä¸€ä¸ªå•ä¸€ç®¡é“ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å°†DSTé‡æ–°æ„å»ºä¸ºå¯¹è¾ƒå¼±æ¨¡å‹çš„é—®é¢˜å›ç­”ä»»åŠ¡ï¼Œå¹¶ä¸ºæ›´çµæ´»çš„æ¨¡å‹é‡‡ç”¨è‡ªæˆ‘ä¿®æ­£æç¤ºã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä¸ä¾èµ–äºæœ¬ä½“è®ºä¸­å®šä¹‰çš„å›ºå®šæ§½å€¼ï¼Œä½¿ç³»ç»Ÿèƒ½å¤ŸåŠ¨æ€é€‚åº”ã€‚æˆ‘ä»¬å°†æ–¹æ³•ä¸ç°æœ‰æœ€ä½³æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯æ˜åœ¨Multi-WOZ 2.1ç­‰æ•°æ®é›†ä¸Šï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè”åˆç›®æ ‡å‡†ç¡®ç‡ï¼ˆJGAï¼‰æé«˜äº†é«˜è¾¾20%ï¼Œå¯¹LLM APIçš„è¯·æ±‚å‡å°‘äº†é«˜è¾¾90%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å¯¹è¯ä¸­å¾ˆé‡è¦ï¼Œéœ€è¦ç†è§£ç”¨æˆ·éœ€æ±‚å¹¶å®æ–½ç›¸åº”ç³»ç»Ÿæ“ä½œã€‚</li>
<li>ç°æœ‰DSTæ–¹æ³•å¤§å¤šåŸºäºé¢„è®¾æœ¬ä½“è®ºè®¾è®¡ï¼Œéš¾ä»¥é€‚åº”æ–°æ§½ä½å€¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é›¶æ ·æœ¬DSTæ€§èƒ½ä¸Šè¡¨ç°æœ‰å‰æ™¯ï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºæˆ–æ€§èƒ½å¯èƒ½ä½äºå®Œå…¨è®­ç»ƒçš„ç³»ç»Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬ã€å¼€æ”¾è¯æ±‡çš„ç³»ç»Ÿï¼Œé›†æˆé¢†åŸŸåˆ†ç±»å’ŒDSTåœ¨ä¸€ä¸ªå•ä¸€ç®¡é“ä¸­ã€‚</li>
<li>å°†DSTé‡æ–°æ„å»ºä¸ºé—®é¢˜å›ç­”ä»»åŠ¡ï¼Œé€‚ç”¨äºè¾ƒå¼±æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨è‡ªæˆ‘ä¿®æ­£æç¤ºï¼Œé€‚åº”æ›´çµæ´»çš„æ¨¡å‹ã€‚</li>
<li>ç³»ç»Ÿä¸ä¾èµ–å›ºå®šçš„æ§½ä½å€¼ï¼Œèƒ½å¤ŸåŠ¨æ€é€‚åº”ã€‚åœ¨Multi-WOZ 2.1æ•°æ®é›†ä¸Šï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè”åˆç›®æ ‡å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾20%ï¼Œå¯¹LLM APIçš„è¯·æ±‚å‡å°‘äº†é«˜è¾¾90%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09234ad3f35c6699c299fb28252c9cbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-834e57677efa9be4bb631b4357786491.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df961a3badd4e37160aa520cb6ab7b1e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HAF-RM-A-Hybrid-Alignment-Framework-for-Reward-Model-Training"><a href="#HAF-RM-A-Hybrid-Alignment-Framework-for-Reward-Model-Training" class="headerlink" title="HAF-RM: A Hybrid Alignment Framework for Reward Model Training"></a>HAF-RM: A Hybrid Alignment Framework for Reward Model Training</h2><p><strong>Authors:Shujun Liu, Xiaoyu Shen, Yuhang Lai, Siyuan Wang, Shengbin Yue, Zengfeng Huang, Xuanjing Huang, Zhongyu Wei</strong></p>
<p>The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards. In this paper, we propose a hybrid alignment framework HaF-RM for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score. It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level. Experiment results on five datasets sufficiently show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model. By decoupling the reward modeling procedure and incorporating hybrid supervision, our HaF-RM framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at <a target="_blank" rel="noopener" href="https://haf-rm.github.io/">https://haf-rm.github.io</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½ã€è¯„ä¼°å’Œæ•°æ®æ„å»ºä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶äººå‘˜ä¸»è¦é€šè¿‡æ•°æ®æ”¹è¿›æ¥ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ï¼Œéµå¾ªä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œç›´æ¥ä¼˜åŒ–é¢„æµ‹çš„å¥–åŠ±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¥–åŠ±æ¨¡å‹è®­ç»ƒçš„æ··åˆå¯¹é½æ¡†æ¶HaF-RMï¼Œé™¤äº†å¥–åŠ±åˆ†æ•°ä¹‹å¤–ï¼Œè¿˜å¼•å…¥äº†tokençº§ç­–ç•¥æ¦‚ç‡çš„é¢å¤–çº¦æŸã€‚å®ƒå¯ä»¥åŒæ—¶ç›‘ç£tokençº§åˆ«çš„å†…éƒ¨åå¥½æ¨¡å‹ï¼Œå¹¶ä¼˜åŒ–åºåˆ—çº§åˆ«çš„å¥–åŠ±æ¨¡å‹çš„æ˜ å°„å±‚ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ··åˆæ¡†æ¶ç”¨äºè®­ç»ƒé«˜è´¨é‡å¥–åŠ±æ¨¡å‹æ˜¯æœ‰æ•ˆå’Œé«˜æ•ˆçš„ã€‚é€šè¿‡è§£è€¦å¥–åŠ±å»ºæ¨¡è¿‡ç¨‹å¹¶èå…¥æ··åˆç›‘ç£ï¼Œæˆ‘ä»¬çš„HaF-RMæ¡†æ¶æä¾›äº†ä¸€ç§æœ‰åŸåˆ™ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½å’Œå¯¹é½æ€§ï¼Œè¿™æ˜¯å¼ºå¤§è¯­è¨€æ¨¡å‹è´Ÿè´£ä»»å‘å±•çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://haf-rm.github.ioå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ./">https://haf-rm.github.ioå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04185v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHaF-RMçš„æ··åˆå¯¹é½æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ¡†æ¶çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†é¢å¤–çš„æ ‡è®°çº§ç­–ç•¥æ¦‚ç‡çº¦æŸï¼Œå¯ä»¥åŒæ—¶ç›‘ç£å†…éƒ¨åå¥½æ¨¡å‹çš„æ ‡è®°çº§ä¼˜åŒ–å’Œå¥–åŠ±æ¨¡å‹çš„åºåˆ—çº§æ˜ å°„å±‚ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ··åˆæ¡†æ¶èƒ½æœ‰æ•ˆæé«˜å¥–åŠ±æ¨¡å‹çš„è´¨é‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å¯¹é½ã€è¯„ä¼°å’Œæ•°æ®å¤„ç†æ–¹é¢å˜å¾—æ—¥ç›Šé‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡æ•°æ®æ”¹è¿›æ¥ä¼˜åŒ–å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHaF-RMçš„æ··åˆå¯¹é½æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>HaF-RMæ¡†æ¶å¼•å…¥é¢å¤–çš„æ ‡è®°çº§ç­–ç•¥æ¦‚ç‡çº¦æŸã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½åŒæ—¶ç›‘ç£å†…éƒ¨åå¥½æ¨¡å‹çš„æ ‡è®°çº§ä¼˜åŒ–å’Œå¥–åŠ±æ¨¡å‹çš„åºåˆ—çº§æ˜ å°„å±‚ä¼˜åŒ–ã€‚</li>
<li>åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†HaF-RMæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f1144cb2add4ac82d75e50b46794cf15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caa7279ee050b3d6c0c361b02207b026.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2d83973735be28ccdd7b9b7a26588b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77991e956c8bd899880aae66bd8ef868.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6161ba6ccb62202c449e31f2236f1435.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b63545dbe762b9abfd14ec75bc5f0468.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Lemur-Log-Parsing-with-Entropy-Sampling-and-Chain-of-Thought-Merging"><a href="#Lemur-Log-Parsing-with-Entropy-Sampling-and-Chain-of-Thought-Merging" class="headerlink" title="Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"></a>Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging</h2><p><strong>Authors:Wei Zhang, Hongcheng Guo, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li</strong></p>
<p>Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method for large language models (LLMs). LLMs exhibit exceptional semantic comprehension, deftly distinguishing between parameters and invariant tokens. We have conducted experiments on large-scale public datasets. Extensive evaluation demonstrates that Lemur achieves the state-of-the-art performance and impressive efficiency. The Code is available at <a target="_blank" rel="noopener" href="https://github.com/zwpride/lemur">https://github.com/zwpride/lemur</a>. </p>
<blockquote>
<p>æ—¥å¿—è®°å½•ç”±å¹¿æ³›çš„è½¯ä»¶ç³»ç»Ÿç”Ÿæˆï¼Œå¯¹äºç›‘æ§ç³»ç»Ÿè¡Œä¸ºè‡³å…³é‡è¦ã€‚å…ˆè¿›çš„æ—¥å¿—åˆ†ææœ‰åŠ©äºæ£€æµ‹ã€é¢„è­¦å’Œè¯Šæ–­ç³»ç»Ÿæ•…éšœã€‚æ—¥å¿—è§£ææ˜¯å°†åŸå§‹æ—¥å¿—æ¶ˆæ¯è½¬æ¢ä¸ºç»“æ„åŒ–æ¨¡æ¿çš„è¿‡ç¨‹ï¼Œæ˜¯æ—¥å¿—åˆ†æè‡ªåŠ¨åŒ–çš„å…³é”®é˜¶æ®µã€‚ç°æœ‰çš„æ—¥å¿—è§£æå™¨ç”±äºä¾èµ–äººå·¥è§„åˆ™è€Œæ— æ³•è¯†åˆ«æ­£ç¡®çš„æ¨¡æ¿ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¾§é‡äºç»Ÿè®¡ç‰¹å¾ï¼Œè€Œå¿½ç•¥äº†æ—¥å¿—æ¶ˆæ¯ä¸­çš„è¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…ˆè¿›çš„æ—¥å¿—è§£ææ¡†æ¶ï¼Œé‡‡ç”¨ç†µé‡‡æ ·å’Œæ€ç»´é“¾åˆå¹¶ï¼ˆLemurï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æ‘’å¼ƒç¹ççš„äººå·¥è§„åˆ™ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—ä¿¡æ¯ç†µå¯å‘çš„æ–°å‹é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¯¹å…¸å‹æ—¥å¿—è¿›è¡Œèšç±»ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ—¥å¿—æ¨¡æ¿çš„åˆå¹¶æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡äº†ä¸€ç§æ€ç»´é“¾æ–¹æ³•ã€‚LLMè¡¨ç°å‡ºå“è¶Šçš„ç†è§£èƒ½åŠ›ï¼Œèƒ½å¤ŸåŒºåˆ†å‚æ•°å’Œä¸å˜ä»¤ç‰Œã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒLemurè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ•ˆç‡ä»¤äººå°è±¡æ·±åˆ»ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zwpride/lemur%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zwpride/lemurè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.18205v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ—¥å¿—åˆ†ææ˜¯ç›‘æµ‹è½¯ä»¶ç³»ç»Ÿè¡Œä¸ºçš„é‡è¦æ‰‹æ®µï¼Œå…¶ä¸­æ—¥å¿—è§£ææ˜¯è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æçš„å…³é”®ç¯èŠ‚ã€‚ç°æœ‰æ—¥å¿—è§£ææ–¹æ³•ä¾èµ–äººå·¥è§„åˆ™ï¼Œå¿½è§†è¯­ä¹‰ä¿¡æ¯ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«æ—¥å¿—æ¨¡æ¿ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä¿¡æ¯ç†µé‡‡æ ·å’Œé“¾å¼åˆå¹¶æ€æƒ³çš„å…ˆè¿›æ—¥å¿—è§£ææ¡†æ¶ï¼ˆLemurï¼‰ï¼Œé€šè¿‡ä¿¡æ¯ç†µé‡‡æ ·æ–¹æ³•é«˜æ•ˆèšç±»å…¸å‹æ—¥å¿—ï¼Œé‡‡ç”¨é“¾å¼åˆå¹¶ç­–ç•¥æå‡æ—¥å¿—æ¨¡æ¿åˆå¹¶æ•ˆæœï¼Œåœ¨å¤§è§„æ¨¡å…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¥å¿—åˆ†æåœ¨ç³»ç»Ÿæ•…éšœæ£€æµ‹ã€é¢„è­¦å’Œè¯Šæ–­ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚</li>
<li>æ—¥å¿—è§£ææ˜¯è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æçš„æ ¸å¿ƒç¯èŠ‚ï¼Œéœ€è¦å°†åŸå§‹æ—¥å¿—æ¶ˆæ¯è½¬æ¢ä¸ºç»“æ„åŒ–æ¨¡æ¿ã€‚</li>
<li>ç°æœ‰æ—¥å¿—è§£ææ–¹æ³•å­˜åœ¨ä¾èµ–äººå·¥è§„åˆ™ã€éš¾ä»¥å‡†ç¡®è¯†åˆ«æ—¥å¿—æ¨¡æ¿çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Lemuræ¡†æ¶é‡‡ç”¨ä¿¡æ¯ç†µé‡‡æ ·æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆèšç±»å…¸å‹æ—¥å¿—ã€‚</li>
<li>Lemuråˆ©ç”¨é“¾å¼åˆå¹¶ç­–ç•¥ï¼Œæå‡äº†æ—¥å¿—æ¨¡æ¿çš„åˆå¹¶æ•ˆæœã€‚</li>
<li>Lemuråœ¨å¤§è§„æ¨¡å…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.18205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba8badd094594d96811abc370e687f55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-554cdda8c630884962e14943a7855ef8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d895f03ca8caaf3552a467023ca33aa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da19dc9d8ec9fa700421f45e7e24f1c6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Tool-Use-with-Chain-of-Abstraction-Reasoning"><a href="#Efficient-Tool-Use-with-Chain-of-Abstraction-Reasoning" class="headerlink" title="Efficient Tool Use with Chain-of-Abstraction Reasoning"></a>Efficient Tool Use with Chain-of-Abstraction Reasoning</h2><p><strong>Authors:Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut, Tianlu Wang</strong></p>
<p>To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs. </p>
<blockquote>
<p>ä¸ºäº†å®ç°ä¸äººç±»æœŸæœ›ç›¸ç¬¦çš„å¿ å®æ¨ç†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦å°†ä»–ä»¬çš„æ¨ç†åŸºäºç°å®ä¸–ç•Œçš„çŸ¥è¯†ï¼ˆå¦‚ç½‘ç»œäº‹å®ã€æ•°å­¦å’Œç‰©ç†è§„åˆ™ï¼‰ã€‚å·¥å…·å¸®åŠ©LLMè®¿é—®å¤–éƒ¨çŸ¥è¯†ï¼Œä½†åœ¨å¾®è°ƒLLMä»£ç†ï¼ˆå¦‚Toolformerï¼‰ä»¥åœ¨éœ€è¦æ•´ä½“å’Œé«˜æ•ˆå·¥å…·ä½¿ç”¨è§„åˆ’çš„è·¨æ­¥éª¤æ¨ç†é—®é¢˜ä¸­è°ƒç”¨å·¥å…·æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°åœ¨è·¨æ­¥éª¤æ¨ç†ä¸­åˆ©ç”¨å·¥å…·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºæŠ½è±¡é“¾æ³•ï¼ˆChain-of-Abstractionï¼ŒCoAï¼‰ï¼Œå®ƒè®­ç»ƒLLMé¦–å…ˆé€šè¿‡æŠ½è±¡å ä½ç¬¦è§£ç æ¨ç†é“¾ï¼Œç„¶åé€šè¿‡è°ƒç”¨é¢†åŸŸå·¥å…·æ¥å…·ä½“åŒ–æ¯ä¸ªæ¨ç†é“¾ï¼Œå¡«å……ç‰¹å®šçŸ¥è¯†ã€‚è¿™ç§æŠ½è±¡é“¾çš„è§„åˆ’ä½¿LLMèƒ½å¤Ÿå­¦ä¹ æ›´é€šç”¨çš„æ¨ç†ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯¹ä¸åŒæ¨ç†é—®é¢˜çš„é¢†åŸŸçŸ¥è¯†å˜åŒ–å…·æœ‰ç¨³å¥æ€§ï¼ˆå¦‚æ•°å­¦ç»“æœçš„å˜åŒ–ï¼‰ã€‚å®ƒè¿˜å…è®¸LLMå¹¶è¡Œæ‰§è¡Œè§£ç å’Œè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œä»è€Œé¿å…äº†ç­‰å¾…å·¥å…·å“åº”è€Œå¯¼è‡´çš„æ¨ç†å»¶è¿Ÿã€‚åœ¨æ•°å­¦æ¨ç†å’ŒWiki QAé¢†åŸŸï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„æµ‹è¯•é›†ä¸Šå‡ä¼˜äºä¹‹å‰çš„æ€ç»´é“¾å’Œå·¥å…·å¢å¼ºåŸºçº¿æ–¹æ³•ï¼Œå¹³å‡é—®ç­”å‡†ç¡®æ€§æé«˜äº†çº¦6%ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒçš„LLMä»£ç†è¿˜æ˜¾ç¤ºå‡ºæ›´æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨ï¼Œæ¨ç†é€Ÿåº¦å¹³å‡æ¯”åŸºçº¿å·¥å…·å¢å¼ºLLMå¿«çº¦1.4å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.17464v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¦å®ç°ç¬¦åˆäººç±»æœŸæœ›çš„å¿ å®æ¨ç†ï¼Œéœ€è¦å°†å…¶æ¨ç†åŸºç¡€å»ºç«‹åœ¨ç°å®ä¸–ç•ŒçŸ¥è¯†ä¸Šï¼Œå¦‚ç½‘ç»œäº‹å®ã€æ•°å­¦å’Œç‰©ç†è§„åˆ™ç­‰ã€‚å°½ç®¡æœ‰å·¥å…·å¸®åŠ©LLMè®¿é—®è¿™äº›å¤–éƒ¨çŸ¥è¯†ï¼Œä½†åœ¨å¾®è°ƒLLMä»£ç†ä»¥åœ¨å…·æœ‰äº’è”å·¥å…·è°ƒç”¨çš„å¤šæ­¥éª¤æ¨ç†é—®é¢˜ä¸­è°ƒç”¨å·¥å…·æ—¶ï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä½¿LLMæ›´å¥½åœ°åœ¨å¤šæ­¥éª¤æ¨ç†ä¸­åˆ©ç”¨å·¥å…·ã€‚æˆ‘ä»¬çš„æ–¹æ³•â€”â€”æŠ½è±¡é“¾ï¼ˆChain-of-Abstractionï¼ŒCoAï¼‰â€”â€”è®­ç»ƒLLMé¦–å…ˆè§£ç å¸¦æœ‰æŠ½è±¡å ä½ç¬¦çš„æ¨ç†é“¾ï¼Œç„¶åè°ƒç”¨é¢†åŸŸå·¥å…·æ¥å¡«å……å…·ä½“çŸ¥è¯†ä»¥å…·ä½“åŒ–æ¯ä¸ªæ¨ç†é“¾ã€‚è¿™ç§æŠ½è±¡é“¾çš„è§„åˆ’ä½¿LLMèƒ½å¤Ÿå­¦ä¹ æ›´é€šç”¨çš„æ¨ç†ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯¹ä¸åŒæ¨ç†é—®é¢˜çš„é¢†åŸŸçŸ¥è¯†å˜åŒ–å…·æœ‰ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸LLMå¹¶è¡Œè¿›è¡Œè§£ç å’Œè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œä»è€Œé¿å…äº†å› ç­‰å¾…å·¥å…·å“åº”è€Œé€ æˆçš„æ¨ç†å»¶è¿Ÿã€‚åœ¨æ•°å­¦æ¨ç†å’ŒWiki QAé¢†åŸŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æµ‹è¯•é›†ä¸Šå‡ä¼˜äºä¹‹å‰çš„æ€è€ƒé“¾å’Œå·¥å…·å¢å¼ºåŸºçº¿ï¼Œå¹³å‡é—®ç­”å‡†ç¡®æ€§æé«˜çº¦6%ã€‚ä½¿ç”¨æˆ‘ä»¬æ–¹æ³•è®­ç»ƒçš„LLMä»£ç†è¿˜è¡¨ç°å‡ºæ›´æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨ï¼Œæ¨ç†é€Ÿåº¦å¹³å‡æ¯”åŸºçº¿å·¥å…·å¢å¼ºLLMå¿«çº¦1.4å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦åŸºäºç°å®ä¸–ç•ŒçŸ¥è¯†ï¼ˆå¦‚ç½‘ç»œäº‹å®ã€æ•°å­¦å’Œç‰©ç†è§„åˆ™ï¼‰è¿›è¡Œæ¨ç†ï¼Œä»¥ç¬¦åˆäººç±»æœŸæœ›ã€‚</li>
<li>åœ¨å¤šæ­¥éª¤æ¨ç†ä¸­è°ƒç”¨äº’è”å·¥å…·å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦å¾®è°ƒLLMä»£ç†ä»¥æ›´å¥½åœ°åˆ©ç”¨å·¥å…·ã€‚</li>
<li>æå‡ºçš„æŠ½è±¡é“¾ï¼ˆCoAï¼‰æ–¹æ³•å…è®¸LLMè§£ç å¸¦æœ‰æŠ½è±¡å ä½ç¬¦çš„æ¨ç†é“¾ï¼Œç„¶åè°ƒç”¨é¢†åŸŸå·¥å…·å¡«å……å…·ä½“çŸ¥è¯†ã€‚</li>
<li>æŠ½è±¡é“¾è§„åˆ’ä½¿LLMèƒ½å¤Ÿå­¦ä¹ æ›´é€šç”¨çš„æ¨ç†ç­–ç•¥ï¼Œå¯¹é¢†åŸŸçŸ¥è¯†å˜åŒ–å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>CoAæ–¹æ³•æé«˜äº†LLMåœ¨æ•°å­¦æ¨ç†å’ŒWiki QAé¢†åŸŸçš„æ€§èƒ½ï¼Œå¹³å‡é—®ç­”å‡†ç¡®æ€§æé«˜çº¦6%ã€‚</li>
<li>ä½¿ç”¨CoAæ–¹æ³•è®­ç»ƒçš„LLMä»£ç†è¡¨ç°å‡ºæ›´æœ‰æ•ˆçš„å·¥å…·ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.17464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-498db010257209aea26591903bd3354c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32884e12c45e007a86b3f4319e4db68e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1817f8664eb616856f414cfe35e610f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eca1fc03329fee862e62c90d0b4bcc44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e37a82d5a394f1129c3a493f0d0e0e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition"><a href="#Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition" class="headerlink" title="Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model   in End-to-End Speech Recognition"></a>Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model   in End-to-End Speech Recognition</h2><p><strong>Authors:Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi</strong></p>
<p>We propose to utilize an instruction-tuned large language model (LLM) for guiding the text generation process in automatic speech recognition (ASR). Modern large language models (LLMs) are adept at performing various text generation tasks through zero-shot learning, prompted with instructions designed for specific objectives. This paper explores the potential of LLMs to derive linguistic information that can facilitate text generation in end-to-end ASR models. Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further. The proposed model is built on the joint CTC and attention architecture, with the LLM serving as a front-end feature extractor for the decoder. The ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding and fed into the LLM along with a specific instruction. The decoder subsequently takes as input the LLM output to perform token predictions, combining acoustic information from the encoder and the powerful linguistic information provided by the LLM. Experimental results show that the proposed LLM-guided model achieves a relative gain of approximately 13% in word error rates across major benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬æè®®åˆ©ç”¨ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æŒ‡å¯¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿é€šè¿‡é›¶æ ·æœ¬å­¦ä¹ å®Œæˆå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡é€šè¿‡é’ˆå¯¹ç‰¹å®šç›®æ ‡è®¾è®¡çš„æŒ‡ä»¤æ¥æç¤ºã€‚æœ¬æ–‡æ¢è®¨äº†LLMåœ¨ç«¯åˆ°ç«¯ASRæ¨¡å‹ä¸­ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æŒ‡ä»¤LLMæ¥çº æ­£ASRå‡è®¾ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆçš„è¡¨ç¤ºæ¥è¿›ä¸€æ­¥æ”¹è¿›è¾“å‡ºã€‚æ‰€æå‡ºçš„æ¨¡å‹åŸºäºCTCå’Œæ³¨æ„åŠ›æ¶æ„çš„è”åˆæ„å»ºï¼ŒLLMä½œä¸ºè§£ç å™¨å‰ç«¯ç‰¹å¾æå–å™¨ã€‚éœ€è¦çº æ­£çš„ASRå‡è®¾æ˜¯é€šè¿‡CTCè§£ç ä»ç¼–ç å™¨è·å¾—çš„ï¼Œå¹¶ä¸ç‰¹å®šæŒ‡ä»¤ä¸€èµ·è¾“å…¥åˆ°LLMä¸­ã€‚éšåï¼Œè§£ç å™¨ä»¥LLMè¾“å‡ºä½œä¸ºè¾“å…¥æ¥è¿›è¡Œä»¤ç‰Œé¢„æµ‹ï¼Œç»“åˆæ¥è‡ªç¼–ç å™¨çš„å£°éŸ³ä¿¡æ¯å’ŒLLMæä¾›çš„å¼ºå¤§è¯­è¨€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LLMå¼•å¯¼æ¨¡å‹åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†çº¦13%çš„ç›¸å¯¹è¯é”™è¯¯ç‡æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10524v3">PDF</a> Accepted to ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡å¯¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚ç°ä»£LLMé€šè¿‡é›¶æ ·æœ¬å­¦ä¹ å®Œæˆå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œé€šè¿‡ä¸ºç‰¹å®šç›®æ ‡è®¾è®¡æŒ‡ä»¤æ¥æ‰§è¡Œã€‚æœ¬æ–‡æ¢ç´¢äº†LLMåœ¨ç«¯åˆ°ç«¯ASRæ¨¡å‹ä¸­ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„æ½œåŠ›ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬æŒ‡ç¤ºLLMçº æ­£ASRå‡è®¾ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶ä½¿ç”¨LLMç”Ÿæˆçš„è¡¨ç¤ºæ¥è¿›ä¸€æ­¥å®Œå–„è¾“å‡ºã€‚æå‡ºçš„æ¨¡å‹å»ºç«‹åœ¨è”åˆCTCå’Œæ³¨æ„åŠ›æ¶æ„ä¸Šï¼ŒLLMä½œä¸ºè§£ç å™¨çš„å‰ç«¯ç‰¹å¾æå–å™¨ã€‚ASRå‡è®¾ç»è¿‡ä¿®æ­£åï¼Œé€šè¿‡CTCè§£ç ä»ç¼–ç å™¨è·å¾—ï¼Œå¹¶ä¸ç‰¹å®šæŒ‡ä»¤ä¸€èµ·è¾“å…¥LLMã€‚è§£ç å™¨éšåä»¥LLMè¾“å‡ºä½œä¸ºè¾“å…¥è¿›è¡Œä»¤ç‰Œé¢„æµ‹ï¼Œç»“åˆç¼–ç å™¨çš„å£°éŸ³ä¿¡æ¯å’ŒLLMæä¾›çš„å¼ºå¤§è¯­è¨€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LLMå¼•å¯¼æ¨¡å‹åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¯é”™è¯¯ç‡ç›¸å¯¹æé«˜äº†çº¦13%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥æŒ‡å¯¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>LLMå…·å¤‡é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡ç‰¹å®šæŒ‡ä»¤å®Œæˆå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>LLMåœ¨ç«¯åˆ°ç«¯ASRæ¨¡å‹ä¸­çš„æ½œåŠ›åœ¨äºå…¶èƒ½çº æ­£ASRå‡è®¾ä¸­çš„è¯­æ³•é”™è¯¯å¹¶æ”¹å–„è¾“å‡ºã€‚</li>
<li>æå‡ºçš„æ¨¡å‹ç»“åˆCTCè§£ç å’Œæ³¨æ„åŠ›æ¶æ„ï¼Œå…¶ä¸­LLMä½œä¸ºè§£ç å™¨çš„å‰ç«¯ç‰¹å¾æå–å™¨ã€‚</li>
<li>ASRå‡è®¾ç»è¿‡ä¿®æ­£åï¼Œé€šè¿‡ç‰¹å®šæŒ‡ä»¤è¾“å…¥LLMï¼Œå¹¶ç»“åˆå£°å­¦ä¿¡æ¯è¿›è¡Œä»¤ç‰Œé¢„æµ‹ã€‚</li>
<li>LLMå¼•å¯¼çš„æ¨¡å‹åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ç›¸å¯¹è¾ƒé«˜çš„æ€§èƒ½æå‡ï¼Œè¯é”™è¯¯ç‡é™ä½äº†çº¦13%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.10524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be53b67817602b3e3c2c7795746ef361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6e915597c776147881ec29999ece777.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2a67cbc5de1e0b44da8a1f55d0ae8ff.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b25984dfd7391457dc2b62d68543cb6d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  InfiGUIAgent A Multimodal Generalist GUI Agent with Native Reasoning   and Reflection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-09/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b3850fb6af61123d78ead3663846461a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-09  VideoAnydoor High-fidelity Video Object Insertion with Precise Motion   Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
