<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-10  EditAR Unified Conditional Generation with Autoregressive Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-27c007f4b20c6726a4d36b0e7b51d916.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-10-更新"><a href="#2025-01-10-更新" class="headerlink" title="2025-01-10 更新"></a>2025-01-10 更新</h1><h2 id="EditAR-Unified-Conditional-Generation-with-Autoregressive-Models"><a href="#EditAR-Unified-Conditional-Generation-with-Autoregressive-Models" class="headerlink" title="EditAR: Unified Conditional Generation with Autoregressive Models"></a>EditAR: Unified Conditional Generation with Autoregressive Models</h2><p><strong>Authors:Jiteng Mu, Nuno Vasconcelos, Xiaolong Wang</strong></p>
<p>Recent progress in controllable image generation and editing is largely driven by diffusion-based methods. Although diffusion models perform exceptionally well in specific tasks with tailored designs, establishing a unified model is still challenging. In contrast, autoregressive models inherently feature a unified tokenized representation, which simplifies the creation of a single foundational model for various tasks. In this work, we propose EditAR, a single unified autoregressive framework for a variety of conditional image generation tasks, e.g., image editing, depth-to-image, edge-to-image, segmentation-to-image. The model takes both images and instructions as inputs, and predicts the edited images tokens in a vanilla next-token paradigm. To enhance the text-to-image alignment, we further propose to distill the knowledge from foundation models into the autoregressive modeling process. We evaluate its effectiveness across diverse tasks on established benchmarks, showing competitive performance to various state-of-the-art task-specific methods. Project page: <a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a> </p>
<blockquote>
<p>近期可控图像生成和编辑的进展主要得益于基于扩散的方法。尽管扩散模型在特定任务中表现非常出色，但建立统一模型仍然具有挑战性。相比之下，自回归模型天生具有统一的令牌化表示，这简化了为各种任务创建单一基础模型的过程。在这项工作中，我们提出了EditAR，这是一个统一的自回归框架，可用于各种条件图像生成任务，例如图像编辑、深度到图像、边缘到图像、分割到图像。该模型同时接受图像和指令作为输入，并在标准的下一个令牌范式中预测编辑后的图像令牌。为了增强文本到图像的对应性，我们进一步提出将来自基础模型的知识蒸馏到自回归建模过程中。我们在既定的基准测试上对多种任务进行了有效性评估，显示出与各种最先进的任务特定方法相竞争的性能。项目页面：<a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04699v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a></p>
<p><strong>Summary</strong></p>
<p>扩散模型在可控图像生成和编辑方面取得了最新进展，表现出强大的能力。尽管扩散模型在特定任务上的表现卓越，但建立统一模型仍然具有挑战。与此相反，自回归模型具有内在的统一标记表示，简化了各种任务的基础模型的创建。在此工作中，我们提出了EditAR，这是一个统一的自回归框架，可用于多种条件图像生成任务，如图像编辑、深度图像、边缘图像、分割图像等。该模型接受图像和指令作为输入，并在标准的下一个令牌模式中预测编辑后的图像令牌。为了增强文本到图像的对应，我们进一步提出将知识从基础模型蒸馏到自回归建模过程中。我们在多个任务上的评估结果表明，其性能与各种最先进的任务特定方法相竞争。想了解更多关于此项目的信息，请访问：<a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">项目页面链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在可控图像生成和编辑方面取得显著进展。</li>
<li>自回归模型具有内在的统一标记表示，便于创建统一模型应对多种任务。</li>
<li>EditAR框架是一个统一的自回归模型，适用于多种条件图像生成任务。</li>
<li>EditAR接受图像和指令作为输入，预测编辑后的图像令牌。</li>
<li>通过蒸馏技术，增强了文本到图像的对应准确性。</li>
<li>在多个任务上的评估结果表明，EditAR性能与最先进的任务特定方法相竞争。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04699">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cf12a8610ad1c57b9d782854bfe4b06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a98974e503ad6e3360f8f7db57f6ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e7755b225f6d9b3cc44174aea44eabb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPAR3D-Stable-Point-Aware-Reconstruction-of-3D-Objects-from-Single-Images"><a href="#SPAR3D-Stable-Point-Aware-Reconstruction-of-3D-Objects-from-Single-Images" class="headerlink" title="SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single   Images"></a>SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single   Images</h2><p><strong>Authors:Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, Varun Jampani</strong></p>
<p>We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based modeling and generative modeling. Regression methods efficiently infer visible surfaces, but struggle with occluded regions. Generative methods handle uncertain regions better by modeling distributions, but are computationally expensive and the generation is often misaligned with visible surfaces. In this paper, we present SPAR3D, a novel two-stage approach aiming to take the best of both directions. The first stage of SPAR3D generates sparse 3D point clouds using a lightweight point diffusion model, which has a fast sampling speed. The second stage uses both the sampled point cloud and the input image to create highly detailed meshes. Our two-stage design enables probabilistic modeling of the ill-posed single-image 3D task while maintaining high computational efficiency and great output fidelity. Using point clouds as an intermediate representation further allows for interactive user edits. Evaluated on diverse datasets, SPAR3D demonstrates superior performance over previous state-of-the-art methods, at an inference speed of 0.7 seconds. Project page with code and model: <a target="_blank" rel="noopener" href="https://spar3d.github.io/">https://spar3d.github.io</a> </p>
<blockquote>
<p>我们研究了单图像3D对象重建的问题。近期的研究工作主要分为两个方向：基于回归的建模和生成式建模。回归方法能够有效地推断出可见表面，但在处理遮挡区域时遇到困难。生成式方法通过建模分布来处理不确定区域，但计算成本较高，且生成的结果往往与可见表面不匹配。在本文中，我们提出了SPAR3D，这是一种新颖的两阶段方法，旨在融合两个方向的优点。SPAR3D的第一阶段使用轻量级的点扩散模型生成稀疏的3D点云，具有快速的采样速度。第二阶段则使用采样得到的点云和输入图像来创建高度详细的网格。我们的两阶段设计能够在保持高计算效率和输出质量的同时，对不适定的单图像3D任务进行概率建模。以点云作为中间表示形式还可以进一步实现交互式用户编辑。在多种数据集上的评估表明，SPAR3D在推理速度为0.7秒的情况下，其性能优于以前的最先进方法。项目页面包含代码和模型：<a target="_blank" rel="noopener" href="https://spar3d.github.io/">https://spar3d.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04689v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了单图像三维物体重建的问题。当前研究主要分两大方向：回归建模和生成建模。回归方法能高效推断可见表面，但难以处理遮挡区域。生成方法通过建模分布来处理不确定区域，但计算量大且生成结果常与可见表面不匹配。本研究提出SPAR3D，一种结合两者优点的新型两阶段方法。第一阶段利用轻量级点扩散模型生成稀疏三维点云，采样速度快；第二阶段结合采样点云和输入图像创建高精度网格。SPAR3D的两阶段设计实现了单图像三维任务的概率建模，同时保持高计算效率和出色的输出保真度。利用点云作为中间表示形式，还可实现用户交互编辑。在多个数据集上的评估显示，SPAR3D在推理速度达到0.7秒的情况下，性能优于之前的最先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究背景是关于单图像三维物体重建问题，该领域近期主要分为回归建模和生成建模两大方向。</li>
<li>回归方法能够高效推断可见表面，但在处理遮挡区域时遇到困难。</li>
<li>生成方法通过建模分布来处理不确定区域，但存在计算量大和生成结果与可见表面不匹配的问题。</li>
<li>SPAR3D是一种新型的两阶段方法，旨在结合两者的优点。</li>
<li>第一阶段利用轻量级点扩散模型快速生成稀疏三维点云。</li>
<li>第二阶段结合采样点云和输入图像创建高精度网格，实现良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04689">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2fee052660da0754b0aa18a9e714ddff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f99fc7cd8c3614b79b727950db5283ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11ff5df348ebc5845db78b0eceea446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbcfd35c54a069d8c0fcf1b494db840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b3de74654fe28ad8dba627c8c114f34.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models"><a href="#DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models" class="headerlink" title="DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion   Models"></a>DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion   Models</h2><p><strong>Authors:Hyogon Ryu, NaHyeon Park, Hyunjung Shim</strong></p>
<p>Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. To mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights and activations into lower-bit formats. However, existing methods often struggle to preserve both image quality and text-image alignment, particularly in lower-bit($&lt;$ 8bits) quantization. In this paper, we analyze the challenges associated with quantizing text-to-image diffusion models from a distributional perspective. Our analysis reveals that activation outliers play a crucial role in determining image quality. Additionally, we identify distinctive patterns in cross-attention scores, which significantly affects text-image alignment. To address these challenges, we propose Distribution-aware Group Quantization (DGQ), a method that identifies and adaptively handles pixel-wise and channel-wise outliers to preserve image quality. Furthermore, DGQ applies prompt-specific logarithmic quantization scales to maintain text-image alignment. Our method demonstrates remarkable performance on datasets such as MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit quantization of text-to-image diffusion models without requiring additional fine-tuning of weight quantization parameters. </p>
<blockquote>
<p>尽管文本到图像扩散模型在各种任务中得到了广泛应用，但其计算和内存需求限制了实际应用的范围。为了缓解这个问题，已经探索了扩散模型的量化。通过压缩权重和激活值到低位格式，它可以减少内存使用量和计算成本。然而，现有方法往往难以在图像质量和文本-图像对齐方面取得平衡，特别是在低位（&lt;8位）量化中。在本文中，我们从分布的角度分析了量化文本到图像扩散模型所面临的挑战。我们的分析表明，激活异常值在决定图像质量方面起着至关重要的作用。此外，我们还发现了跨注意力分数的独特模式，这显著影响了文本-图像对齐。为了应对这些挑战，我们提出了分布感知组量化（DGQ）方法，该方法能够识别和自适应处理像素级和通道级的异常值，以保留图像质量。此外，DGQ应用提示特定的对数量化尺度以维持文本-图像对齐。我们的方法在MS-COCO和PartiPrompts等数据集上表现出了卓越的性能。我们是首批成功实现文本到图像扩散模型的低位量化的研究者，且无需对权重量化参数进行额外的微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04304v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ugonfor.kr/DGQ">https://ugonfor.kr/DGQ</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了文本到图像扩散模型的量化问题。由于扩散模型在各项任务中的广泛应用，其计算与内存需求限制了实际应用。为解决这一问题，研究者尝试对扩散模型进行量化，以降低内存使用和计算成本。然而，现有方法难以在保持图像质量和文本-图像对齐方面达到平衡，特别是在低位（&lt;8位）量化中尤为明显。本文从分布角度出发，分析了量化文本到图像扩散模型的挑战，并提出一种名为分布感知组量化（DGQ）的方法，该方法能够自适应处理像素级和通道级的异常值，以保留图像质量，并应用提示特定对数量化尺度来维持文本-图像对齐。DGQ方法在MS-COCO和PartiPrompts等数据集上表现出卓越性能，并且是首个成功实现低位量化的文本到图像扩散模型方法，无需对权重量化参数进行额外微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像扩散模型在计算和内存需求方面存在挑战，限制了其实际应用。</li>
<li>量化是降低扩散模型内存使用和计算成本的一种方法。</li>
<li>现有量化方法在保持图像质量和文本-图像对齐方面存在困难，特别是在低位量化中。</li>
<li>本文从分布角度分析了文本到图像扩散模型的量化挑战。</li>
<li>激活异常值对图像质量至关重要。</li>
<li>交叉注意力得分对文本-图像对齐有重要影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04304">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d71dc06bf1edf6861c2eec03bf3e258c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8306f331673ad6125396d214d3f428b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57468beda073fc5fa40399b03496ba65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a9b2c900bffb7795c2efc4a6ee10534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06cce2ce693aa97239b6b69905703f23.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adapting-Image-to-Video-Diffusion-Models-for-Large-Motion-Frame-Interpolation"><a href="#Adapting-Image-to-Video-Diffusion-Models-for-Large-Motion-Frame-Interpolation" class="headerlink" title="Adapting Image-to-Video Diffusion Models for Large-Motion Frame   Interpolation"></a>Adapting Image-to-Video Diffusion Models for Large-Motion Frame   Interpolation</h2><p><strong>Authors:Luoxu Jin, Hiroshi Watanabe</strong></p>
<p>With the development of video generation models has advanced significantly in recent years, we adopt large-scale image-to-video diffusion models for video frame interpolation. We present a conditional encoder designed to adapt an image-to-video model for large-motion frame interpolation. To enhance performance, we integrate a dual-branch feature extractor and propose a cross-frame attention mechanism that effectively captures both spatial and temporal information, enabling accurate interpolations of intermediate frames. Our approach demonstrates superior performance on the Fr&#39;echet Video Distance (FVD) metric when evaluated against other state-of-the-art approaches, particularly in handling large motion scenarios, highlighting advancements in generative-based methodologies. </p>
<blockquote>
<p>随着视频生成模型近年来显著发展，我们采用大规模图像到视频的扩散模型进行视频帧插值。我们提出了一种条件编码器，旨在适应图像到视频模型进行大运动帧插值。为了提高性能，我们集成了双分支特征提取器，并提出了一种跨帧注意力机制，该机制可以有效地捕获空间和时间信息，从而实现中间帧的准确插值。与其他最先进的方法相比，我们的方法在Fréchet视频距离（FVD）指标上表现出卓越的性能，特别是在处理大运动场景时，凸显了基于生成的方法的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17042v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于大规模图像到视频的扩散模型的视频帧插值技术。采用条件编码器设计，以适应图像到视频模型的插值处理。为提高性能，结合了双分支特征提取器，并提出跨帧注意力机制，能有效捕捉空间和时间信息，实现了准确的中间帧插值。相较于其他前沿方法，特别是在处理大运动场景时，本文方法在Fréchet视频距离（FVD）指标上表现出卓越性能，突显了生成式方法的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>采用大规模图像到视频的扩散模型进行视频帧插值。</li>
<li>设计了条件编码器以适应图像到视频模型的插值处理。</li>
<li>结合双分支特征提取器提高性能。</li>
<li>提出跨帧注意力机制，捕捉空间和时间信息。</li>
<li>实现了准确的中间帧插值。</li>
<li>在Fréchet视频距离（FVD）指标上表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7f3e169baccfefe48a49a79ba9cb130e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30a95588000c6480ffcf1adaaab61f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7ef32c88224c5b7c8e1ef388868036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4701af9c4a0021500deaa15d8d74ed81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239a8abf3459480bfc13d5ec875a1a72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f541d618bd6d22adae1294430a0ef491.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy"><a href="#Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy" class="headerlink" title="Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy"></a>Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy</h2><p><strong>Authors:Shaoyan Pan, Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</strong></p>
<p>The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation. </p>
<blockquote>
<p>在心脏介入手术的荧光透视视频中，对导线进行准确的分割对于计算机辅助导航任务至关重要。虽然深度学习的方法在导线分割方面已经表现出了高准确性和稳健性，但它们需要大量的标注数据集来实现泛化，这突显了对丰富标注数据的需要，以提高模型的性能。为了解决这一挑战，我们提出了分割引导的帧一致性视频扩散模型（SF-VD），以生成大量的标注荧光透视视频，增强导线分割网络的训练数据。SF-VD通过独立建模场景分布和运动分布来利用标注较少的视频。它通过根据指定的输入掩码生成带有导线的二维荧光透视图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，通过帧一致性策略确保帧到帧的一致性。分割引导机制进一步通过调整导线对比度来完善这一过程，确保合成图像的可见性范围广泛。在荧光透视数据集上的评估证实了所生成视频的高质量，并显示出导线分割的显著改善。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16050v3">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>在心脏介入荧光视频中的导线准确分割对于计算机辅助导航任务至关重要。针对现有深度学习模型需要大量标注数据以提高性能的问题，我们提出了基于分割引导的帧一致性视频扩散模型（SF-VD）。该模型通过独立建模场景分布和运动分布，生成大量标注的荧光视频，增强导线分割网络的训练数据。SF-VD首先根据指定的输入掩膜生成二维荧光图像来采样场景分布，然后通过逐步生成后续帧来采样运动分布，确保帧间一致性。分割引导机制进一步调整导线的对比度，确保合成图像的可见性多样。在荧光数据集上的评估证实了生成视频的高质量，并在导线分割方面取得了显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>准确分割心脏介入荧光视频中的导线对计算机辅助导航至关重要。</li>
<li>深度学习模型在导线分割中表现出高准确性和稳健性，但需要大量标注数据以提高性能。</li>
<li>提出了一种新的模型SF-VD来解决标注数据不足的问题。</li>
<li>SF-VD通过独立建模场景分布和运动分布来生成大量标注的荧光视频。</li>
<li>SF-VD利用帧一致性策略确保生成的视频帧间连贯性。</li>
<li>分割引导机制用于调整导线的对比度，增加合成图像的可见性多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-27c007f4b20c6726a4d36b0e7b51d916.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9a2f0b88c0190b7511a9244cdd8f011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6dfead2b062aa6a8f15edfc59eb3429.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d47112dbe22818737ff4b9362916e1d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Open-Source-Acceleration-of-Stable-Diffusion-cpp-Deployable-on-All-Devices"><a href="#Open-Source-Acceleration-of-Stable-Diffusion-cpp-Deployable-on-All-Devices" class="headerlink" title="Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All   Devices"></a>Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All   Devices</h2><p><strong>Authors:Jingxu Ng, Cheng Lv, Pu Zhao, Wei Niu, Juyi Lin, Minzhou Pan, Yun Liang, Yanzhi Wang</strong></p>
<p>Stable diffusion plays a crucial role in generating high-quality images. However, image generation is time-consuming and memory-intensive. To address this, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate the diffusion models. Although it is lightweight, the current implementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference latency and massive memory usage. To address this, in this work, we present an optimized version of Sdcpp leveraging the Winograd algorithm to accelerate 2D convolution operations, which is the primary bottleneck in the pipeline. By analyzing both dependent and independent computation graphs, we exploit the device’s locality and parallelism to achieve substantial performance improvements. Our framework delivers correct end-to-end results across various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and SDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for individual convolutional layers and an inference speedup up to 4.79x for the overall image generation process, compared with the original Sdcpp on M1 pro. Homepage: <a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a> </p>
<blockquote>
<p>稳定扩散在生成高质量图像中起着至关重要的作用。然而，图像生成是耗时的且需要大量内存。为了解决这一问题，stable-diffusion.cpp（Sdcpp）作为一个高效的推理框架应运而生，以加速扩散模型的运行。虽然它很轻便，但Sdcpp中ggml_conv_2d算子的当前实现并不理想，存在推理延迟高和内存使用量大的问题。针对这一问题，我们在本工作中推出了一个优化版的Sdcpp，利用Winograd算法加速2D卷积操作，这是管道中的主要瓶颈。通过分析有依赖和无依赖的计算图，我们利用设备的局部性和并行性实现了显著的性能提升。我们的框架在各种稳定的扩散模型中都能提供正确的端到端结果，包括SDv1.4、v1.5、v2.1、SDXL和SDXL-Turbo。我们的评估结果表明，与原始Sdcpp在M1 pro上的表现相比，单个卷积层的速度提高了2.76倍，整个图像生成过程的推理速度提高了4.79倍。更多信息请访问：<a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05781v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>稳定扩散在生成高质量图像中起到关键作用，但图像生成耗时且占用过多的内存资源。为了解决这个问题，stable-diffusion.cpp（Sdcpp）作为一个高效的推理框架应运而生，以加速扩散模型的运行。然而，Sdcpp中ggml_conv_2d算子的当前实现并不理想，存在推理延迟高和内存使用量大等问题。为解决这些问题，本研究提出了一种利用Winograd算法优化Sdcpp的版本，以加速2D卷积操作，这是管道中的主要瓶颈。通过分析有依赖和无依赖的计算图，我们充分利用设备的局部性和并行性，实现了显著的性能提升。我们的框架适用于各种稳定扩散模型，包括SDv1.4、v1.5、v2.1、SDXL和SDXL-Turbo。评估结果显示，相对于原始Sdcpp在M1 pro上的表现，我们的框架对单个卷积层的加速最高可达2.76倍，对整个图像生成过程的推理速度提高最高可达4.79倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稳定扩散在生成高质量图像中扮演重要角色，但存在时间消耗和内存占用的问题。</li>
<li>stable-diffusion.cpp（Sdcpp）作为推理框架旨在优化扩散模型的运行效率。</li>
<li>Sdcpp中的ggml_conv_2d算子当前实现存在缺陷，导致高推理延迟和大量内存使用。</li>
<li>利用Winograd算法优化Sdcpp，加速2D卷积操作，解决主要瓶颈问题。</li>
<li>通过分析计算图，充分利用设备局部性和并行性，实现显著性能提升。</li>
<li>优化的框架适用于多种稳定扩散模型，包括SDv1.4、v1.5、v2.1、SDXL和SDXL-Turbo。</li>
<li>评估结果显示，相对于原始Sdcpp，优化后的框架在多个层面实现了显著的性能加速。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05781">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7b7675966091bc2de4f8e4d01ce77bc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d5835bb8fa6be35fd93dee6dd71e9be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13fdcbe21c9a16b15051d0562161b8a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2308dcd61b76d787ee9744ef3a7da592.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c258fbd8dd02404fc5bfaa4f332fe259.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PointDreamer-Zero-shot-3D-Textured-Mesh-Reconstruction-from-Colored-Point-Cloud"><a href="#PointDreamer-Zero-shot-3D-Textured-Mesh-Reconstruction-from-Colored-Point-Cloud" class="headerlink" title="PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored   Point Cloud"></a>PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored   Point Cloud</h2><p><strong>Authors:Qiao Yu, Xianzhi Li, Yuan Tang, Xu Han, Jinfeng Xu, Long Hu, Min Chen</strong></p>
<p>Reconstructing textured meshes from colored point clouds is an important but challenging task. Most existing methods yield blurry-looking textures or rely on 3D training data that are hard to acquire. Regarding this, we propose PointDreamer, a novel framework for textured mesh reconstruction from colored point cloud via diffusion-based 2D inpainting. Specifically, we first reconstruct an untextured mesh. Next, we project the input point cloud into 2D space to generate sparse multi-view images, and then inpaint empty pixels utilizing a pre-trained 2D diffusion model. After that, we unproject the colors of the inpainted dense images onto the untextured mesh, thus obtaining the final textured mesh. This project-inpaint-unproject pipeline bridges the gap between 3D point clouds and 2D diffusion models for the first time. Thanks to the powerful 2D diffusion model pre-trained on extensive 2D data, PointDreamer reconstructs clear, high-quality textures with high robustness to sparse or noisy input. Also, it’s zero-shot requiring no extra training. In addition, we design Non-Border-First unprojection strategy to address the border-area inconsistency issue, which is less explored but commonly-occurred in methods that generate 3D textures from multiview images. Extensive qualitative and quantitative experiments on various synthetic and real-scanned datasets show the SoTA performance of PointDreamer, by significantly outperforming baseline methods with 30% improvement in LPIPS score (from 0.118 to 0.068). Code at: <a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/PointDreamer">https://github.com/YuQiao0303/PointDreamer</a>. </p>
<blockquote>
<p>从彩色点云重建纹理网格是一项重要且具有挑战性的任务。大多数现有方法产生的纹理模糊，或者依赖于难以获取的3D训练数据。针对这一问题，我们提出了PointDreamer，这是一种新的从彩色点云重建纹理网格的方法，基于扩散的2D图像修复技术。具体来说，我们首先重建一个无纹理的网格。然后，我们将输入的点云投影到2D空间，生成稀疏的多视角图像，并利用预训练的2D扩散模型填充空白像素。之后，我们将填充后的密集图像的颜色反投影到无纹理的网格上，从而获得最终的纹理网格。这种投影-修复-反投影的管道首次填补了3D点云和2D扩散模型之间的空白。由于预训练的强大的2D扩散模型在大量的2D数据上进行了训练，PointDreamer能够重建清晰、高质量的纹理，对稀疏或嘈杂的输入具有高度的鲁棒性。此外，它是零次射击，不需要额外的训练。另外，我们设计了非边界优先的反投影策略，以解决从多视角图像生成3D纹理时较少探索但常发生的边界区域不一致问题。在各种合成和真实扫描数据集上的大量定性和定量实验表明，PointDreamer的性能处于领先水平，相较于基线方法在LPIPS得分上有30%的改进（从0.118提高到0.068）。代码地址：<a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/PointDreamer%E3%80%82">https://github.com/YuQiao0303/PointDreamer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15811v2">PDF</a> </p>
<p><strong>摘要</strong><br>    PointDreamer是一个基于扩散模型的框架，可从彩色点云中重建纹理网格。它采用投影-修复-反投影的流程，利用预训练的二维扩散模型填充稀疏像素，从而得到清晰的、高质量的纹理。该框架无需额外训练，对稀疏或噪声输入具有鲁棒性。此外，还设计了解决边界区域不一致问题的非边界优先反投影策略。在多个合成和真实扫描数据集上的实验表明，PointDreamer性能优于基线方法，在LPIPS得分上有30%的改进。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>PointDreamer是一个用于从彩色点云中重建纹理网格的新框架。</li>
<li>该框架采用投影-修复-反投影流程，结合三维点云和二维扩散模型。</li>
<li>利用预训练的二维扩散模型，PointDreamer能够生成清晰、高质量的纹理。</li>
<li>该框架对稀疏或噪声输入具有鲁棒性，且无需额外训练。</li>
<li>引入非边界优先反投影策略，解决从多视角图像生成三维纹理时常见的边界区域不一致问题。</li>
<li>在多个数据集上的实验表明，PointDreamer在性能上优于基线方法。</li>
<li>PointDreamer的代码已公开，可供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15811">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4ca16acfe286a6226cba7e4739f57265.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dc8168dfdac9e719ffd155e677c572b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872b36aa0f7a8995c1a5207d19b7cd2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a727b77d1556a49e7e4a16e41f370e3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12afbf0768c18f510537d16dcbb9e583.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Stylebreeder-Exploring-and-Democratizing-Artistic-Styles-through-Text-to-Image-Models"><a href="#Stylebreeder-Exploring-and-Democratizing-Artistic-Styles-through-Text-to-Image-Models" class="headerlink" title="Stylebreeder: Exploring and Democratizing Artistic Styles through   Text-to-Image Models"></a>Stylebreeder: Exploring and Democratizing Artistic Styles through   Text-to-Image Models</h2><p><strong>Authors:Matthew Zheng, Enis Simsar, Hidir Yesiltepe, Federico Tombari, Joel Simon, Pinar Yanardag</strong></p>
<p>Text-to-image models are becoming increasingly popular, revolutionizing the landscape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce \texttt{STYLEBREEDER}, a comprehensive dataset of 6.8M images and 1.8M prompts generated by 95K users on Artbreeder, a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, user-generated styles that transcend conventional categories like ‘cyberpunk’ or ‘Picasso,’ we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code and models are available at <a target="_blank" rel="noopener" href="https://stylebreeder.github.io/">https://stylebreeder.github.io</a> under a Public Domain (CC0) license. </p>
<blockquote>
<p>文本转图像模型正日益受到欢迎，它通过生成高度详细和创造性的视觉内容，彻底改变了数字艺术创作的格局。这些模型已被广泛应用于各个领域，特别是在艺术生成领域，它们促进了广泛的创意表达，并使艺术创作变得民主化。在本文中，我们介绍了<code>STYLEBREEDER</code>数据集，该数据集包含由Artbreeder平台上9.5万名用户生成的680万张图像和180万个提示，Artbreeder平台已成为拥有超过13百万用户的创意探索重要中心。我们利用这个数据集进行了一系列任务，旨在识别不同的艺术风格、生成个性化内容以及根据用户兴趣推荐风格。通过记录超越传统类别（如“赛博朋克”或“毕加索”）的独特用户生成风格，我们探索了独特的大众源风格的潜力，这些风格可能深入洞察全球用户的集体创意心理。我们还评估了不同的个性化方法来增强艺术表现力，并引入了一个风格图谱，以LoRA格式提供这些模型供公众使用。我们的研究表明，文本转图像扩散模型具有揭示和促进独特艺术表达方式的潜力，进一步使人工智能在艺术创作中民主化，并促进一个更加多样化和包容性的艺术社区。数据集、代码和模型可在<a target="_blank" rel="noopener" href="https://stylebreeder.github.io/">https://stylebreeder.github.io</a>上以公共领域（CC0）许可证使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14599v2">PDF</a> Accepted at NeurIPS 2024 D&amp;B Track, Project page:   <a target="_blank" rel="noopener" href="https://stylebreeder.github.io/">https://stylebreeder.github.io</a> HuggingFace DB Page:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stylebreeder/stylebreeder">https://huggingface.co/datasets/stylebreeder/stylebreeder</a></p>
<p><strong>Summary</strong>：</p>
<p>文本到图像模型正日益普及，它们通过促进高度详细和创造性的视觉内容生成，革命性地改变了数字艺术创作的格局。本文介绍了与Artbreeder平台相关的数据集“STYLEBREEDER”，该平台包含超过1.3亿用户生成的图像和提示，已成为创意探索的重要中心。研究通过一系列任务探索了独特、用户生成的艺术风格，并评估了个性化方法以增强艺术表现力。此外，该研究还推出了风格图谱，以LoRA格式提供这些模型供公众使用。研究表明文本到图像扩散模型具有促进独特艺术表达和创新艺术社区的潜力。数据集、代码和模型均可在<a target="_blank" rel="noopener" href="https://stylebreeder.github.io网站上免费访问./">https://stylebreeder.github.io网站上免费访问。</a> </p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>文本到图像模型在数字艺术领域越来越受欢迎，可生成高度详细和创造性的视觉内容。</li>
<li>“STYLEBREEDER”数据集由超过680万张图像和数百万个提示组成，反映了用户的集体创造力。</li>
<li>该研究探索了用户生成的艺术风格，超越了传统的艺术风格分类，如科幻朋克或毕加索风格等。</li>
<li>研究评估了个性化方法来增强艺术表现力，并引入了风格图谱来展示这些模型的应用。</li>
<li>这些模型和工具旨在进一步推动艺术的民主化，促进更多多样化的艺术社区的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14599">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6509be896176aa2a76399bc4fa9ffc66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e807d9b1fcab7488c37c4a72b4d28645.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53651f2dce8d084d5c4329de8d8db8ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f17470272146fed20391edf498a160e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9cb009800080ea37bed899406964a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb8932cae5c6b8906a43d42b421de4e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AnoFPDM-Anomaly-Segmentation-with-Forward-Process-of-Diffusion-Models-for-Brain-MRI"><a href="#AnoFPDM-Anomaly-Segmentation-with-Forward-Process-of-Diffusion-Models-for-Brain-MRI" class="headerlink" title="AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models   for Brain MRI"></a>AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models   for Brain MRI</h2><p><strong>Authors:Yiming Che, Fazle Rafsani, Jay Shah, Md Mahfuzur Rahman Siddiquee, Teresa Wu</strong></p>
<p>Weakly-supervised diffusion models (DMs) in anomaly segmentation, leveraging image-level labels, have attracted significant attention for their superior performance compared to unsupervised methods. It eliminates the need for pixel-level labels in training, offering a more cost-effective alternative to supervised methods. However, existing methods are not fully weakly-supervised because they heavily rely on costly pixel-level labels for hyperparameter tuning in inference. To tackle this challenge, we introduce Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM), a fully weakly-supervised framework that operates without the need of pixel-level labels. Leveraging the unguided forward process as a reference for the guided forward process, we select hyperparameters such as the noise scale, the threshold for segmentation and the guidance strength. We aggregate anomaly maps from guided forward process, enhancing the signal strength of anomalous regions. Remarkably, our proposed method outperforms recent state-of-the-art weakly-supervised approaches, even without utilizing pixel-level labels. </p>
<blockquote>
<p>弱监督扩散模型（DMs）在异常分割中的应用已经引起了广泛关注，它利用图像级标签，相较于无监督方法表现出卓越的性能。它消除了对训练过程中像素级标签的需求，为监督方法提供了更具成本效益的替代方案。然而，现有方法并非完全弱监督，因为它们严重依赖于推理过程中的超参数调整所需的昂贵像素级标签。为了应对这一挑战，我们引入了基于扩散模型正向过程的异常分割（AnoFPDM），这是一个完全弱监督的框架，无需像素级标签即可运行。我们以非引导正向过程作为引导正向过程的参考，选择超参数，如噪声规模、分割阈值和引导强度。我们从引导的正向过程中聚合异常图，增强异常区域的信号强度。值得注意的是，我们提出的方法即使在不需要像素级标签的情况下，也优于最新的弱监督方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15683v4">PDF</a> v4: added appendices and fixed some typos</p>
<p><strong>Summary</strong></p>
<p>基于图像级标签的弱监督扩散模型在异常分割中受到广泛关注，其性能优于无监督方法。它消除了对像素级标签训练的需求，提供了更经济替代监督方法。然而，现有方法并非完全弱监督，因为它们依赖于昂贵的像素级标签进行推理阶段的超参数调整。为了解决这个问题，我们提出了基于扩散模型的异常分割正向过程（AnomFPDM），这是一个完全弱监督的框架，无需像素级标签即可运行。利用未引导的正向过程作为引导正向过程的参考，我们选择超参数，如噪声规模、分割阈值和引导强度。我们从引导正向过程中聚合异常图，增强异常区域的信号强度。值得注意的是，我们提出的方法在无像素级标签的情况下，甚至超越了最新的弱监督方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>弱监督扩散模型在异常分割中受到关注，其性能优于无监督方法。</li>
<li>现有方法仍依赖像素级标签进行超参数调整，成本较高。</li>
<li>提出了一种全新的弱监督框架AnomFPDM，无需像素级标签。</li>
<li>AnomFPDM利用未引导的正向过程作为引导正向过程的参考，选择关键超参数。</li>
<li>通过聚合来自引导正向过程的异常图，增强了异常区域的信号强度。</li>
<li>AnomFPDM超越了最新的弱监督方法，即使在没有像素级标签的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15683">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-720e7d5671c0de8268d794ed10bbeed0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fa9bbc333637f89efaf8fa02b18e9b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41b5617001f43066c61ea405e3101a80.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NeuralDiffuser-Neuroscience-inspired-Diffusion-Guidance-for-fMRI-Visual-Reconstruction"><a href="#NeuralDiffuser-Neuroscience-inspired-Diffusion-Guidance-for-fMRI-Visual-Reconstruction" class="headerlink" title="NeuralDiffuser: Neuroscience-inspired Diffusion Guidance for fMRI Visual   Reconstruction"></a>NeuralDiffuser: Neuroscience-inspired Diffusion Guidance for fMRI Visual   Reconstruction</h2><p><strong>Authors:Haoyu Li, Hao Wu, Badong Chen</strong></p>
<p>Reconstructing visual stimuli from functional Magnetic Resonance Imaging fMRI enables fine-grained retrieval of brain activity. However, the accurate reconstruction of diverse details, including structure, background, texture, color, and more, remains challenging. The stable diffusion models inevitably result in the variability of reconstructed images, even under identical conditions. To address this challenge, we first uncover the neuroscientific perspective of diffusion methods, which primarily involve top-down creation using pre-trained knowledge from extensive image datasets, but tend to lack detail-driven bottom-up perception, leading to a loss of faithful details. In this paper, we propose NeuralDiffuser, which incorporates primary visual feature guidance to provide detailed cues in the form of gradients. This extension of the bottom-up process for diffusion models achieves both semantic coherence and detail fidelity when reconstructing visual stimuli. Furthermore, we have developed a novel guidance strategy for reconstruction tasks that ensures the consistency of repeated outputs with original images rather than with various outputs. Extensive experimental results on the Natural Senses Dataset (NSD) qualitatively and quantitatively demonstrate the advancement of NeuralDiffuser by comparing it against baseline and state-of-the-art methods horizontally, as well as conducting longitudinal ablation studies. </p>
<blockquote>
<p>从功能性磁共振成像（fMRI）重建视觉刺激能够实现精细的大脑活动检索。然而，准确重建包括结构、背景、纹理、颜色等在内的各种细节仍然具有挑战性。稳定的扩散模型即使在相同条件下也不可避免地导致重建图像的变化。为了解决这一挑战，我们首先揭示了扩散方法在神经科学方面的视角，其主要涉及使用来自大量图像数据集的预训练知识进行自上而下的创建，但往往缺乏细节驱动的自下而上的感知，导致丢失忠实细节。在本文中，我们提出了NeuralDiffuser，它结合了主要视觉特征指导，以梯度形式提供详细线索。这种扩散模型的自下而上过程的扩展在重建视觉刺激时实现了语义连贯性和细节保真度。此外，我们还为重建任务开发了一种新的指导策略，确保重复输出与原始图像的一致性，而不是与各种输出的一致。在自然人感数据集（NSD）上的大量实验结果定性和定量地证明了NeuralDiffuser的先进性，通过与基准和最新技术方法进行横向比较，并进行纵向消融研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13809v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于功能磁共振成像（fMRI）的视觉刺激重建问题，指出重建过程中结构、背景、纹理、色彩等细节准确重建的挑战。针对扩散模型在重建过程中的固有变异性问题，文章从神经科学角度分析了扩散方法的特点，并提出了NeuralDiffuser方案。该方案通过引入主要视觉特征引导，以梯度形式提供细节线索，实现了扩散模型的自下而上过程扩展，在重建视觉刺激时既保证了语义连贯性又保持了细节真实性。此外，还开发了一种新的重建任务引导策略，确保重复输出的一致性，与原始图像相比，避免了输出多样性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉刺激从功能磁共振成像（fMRI）的重建对于精细的大脑活动恢复具有重要意义，但准确重建多样细节仍具有挑战性。</li>
<li>扩散模型在视觉刺激重建中存在稳定性和输出变异性问题。</li>
<li>扩散模型缺乏细节驱动的自下而上感知，导致丢失忠实细节。</li>
<li>NeuralDiffuser方案通过引入视觉特征引导，以梯度形式提供细节线索，实现语义连贯性和细节真实性的平衡。</li>
<li>NeuralDiffuser采用新的引导策略，确保重复输出的一致性，避免输出多样性。</li>
<li>广泛实验结果表明，NeuralDiffuser在横向和纵向评估中均表现出优越性。</li>
<li>研究结果对理解大脑活动和推动视觉刺激重建技术的发展具有重要影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5cc864b359efffa98fe4c171564e0ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b21d74e29b282a6e34e87ed4b0160e9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb440f7024d6d1358c9af9da3efe84e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c5b371ba03822c199bd415211a4294.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31c2ce6d454e057332fda6eb228f0aae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-20177e4c576a5acde5b8b7cbe855e79b.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-10  EditAR Unified Conditional Generation with Autoregressive Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4ef1999029e199a062374e07a8757d9d.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-01-10  FatesGS Fast and Accurate Sparse-View Surface Reconstruction using   Gaussian Splatting with Depth-Feature Consistency
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26024.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
