<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  EditAR Unified Conditional Generation with Autoregressive Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-27c007f4b20c6726a4d36b0e7b51d916.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    39 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-10-æ›´æ–°"><a href="#2025-01-10-æ›´æ–°" class="headerlink" title="2025-01-10 æ›´æ–°"></a>2025-01-10 æ›´æ–°</h1><h2 id="EditAR-Unified-Conditional-Generation-with-Autoregressive-Models"><a href="#EditAR-Unified-Conditional-Generation-with-Autoregressive-Models" class="headerlink" title="EditAR: Unified Conditional Generation with Autoregressive Models"></a>EditAR: Unified Conditional Generation with Autoregressive Models</h2><p><strong>Authors:Jiteng Mu, Nuno Vasconcelos, Xiaolong Wang</strong></p>
<p>Recent progress in controllable image generation and editing is largely driven by diffusion-based methods. Although diffusion models perform exceptionally well in specific tasks with tailored designs, establishing a unified model is still challenging. In contrast, autoregressive models inherently feature a unified tokenized representation, which simplifies the creation of a single foundational model for various tasks. In this work, we propose EditAR, a single unified autoregressive framework for a variety of conditional image generation tasks, e.g., image editing, depth-to-image, edge-to-image, segmentation-to-image. The model takes both images and instructions as inputs, and predicts the edited images tokens in a vanilla next-token paradigm. To enhance the text-to-image alignment, we further propose to distill the knowledge from foundation models into the autoregressive modeling process. We evaluate its effectiveness across diverse tasks on established benchmarks, showing competitive performance to various state-of-the-art task-specific methods. Project page: <a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a> </p>
<blockquote>
<p>è¿‘æœŸå¯æ§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„è¿›å±•ä¸»è¦å¾—ç›ŠäºåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°éå¸¸å‡ºè‰²ï¼Œä½†å»ºç«‹ç»Ÿä¸€æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‡ªå›å½’æ¨¡å‹å¤©ç”Ÿå…·æœ‰ç»Ÿä¸€çš„ä»¤ç‰ŒåŒ–è¡¨ç¤ºï¼Œè¿™ç®€åŒ–äº†ä¸ºå„ç§ä»»åŠ¡åˆ›å»ºå•ä¸€åŸºç¡€æ¨¡å‹çš„è¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EditARï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œå¯ç”¨äºå„ç§æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒç¼–è¾‘ã€æ·±åº¦åˆ°å›¾åƒã€è¾¹ç¼˜åˆ°å›¾åƒã€åˆ†å‰²åˆ°å›¾åƒã€‚è¯¥æ¨¡å‹åŒæ—¶æ¥å—å›¾åƒå’ŒæŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰ŒèŒƒå¼ä¸­é¢„æµ‹ç¼–è¾‘åçš„å›¾åƒä»¤ç‰Œã€‚ä¸ºäº†å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒçš„å¯¹åº”æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå°†æ¥è‡ªåŸºç¡€æ¨¡å‹çš„çŸ¥è¯†è’¸é¦åˆ°è‡ªå›å½’å»ºæ¨¡è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬åœ¨æ—¢å®šçš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹å¤šç§ä»»åŠ¡è¿›è¡Œäº†æœ‰æ•ˆæ€§è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºä¸å„ç§æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸ç«äº‰çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04699v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">https://jitengmu.github.io/EditAR/</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å¯æ§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°å“è¶Šï¼Œä½†å»ºç«‹ç»Ÿä¸€æ¨¡å‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜ã€‚ä¸æ­¤ç›¸åï¼Œè‡ªå›å½’æ¨¡å‹å…·æœ‰å†…åœ¨çš„ç»Ÿä¸€æ ‡è®°è¡¨ç¤ºï¼Œç®€åŒ–äº†å„ç§ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹çš„åˆ›å»ºã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EditARï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œå¯ç”¨äºå¤šç§æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¦‚å›¾åƒç¼–è¾‘ã€æ·±åº¦å›¾åƒã€è¾¹ç¼˜å›¾åƒã€åˆ†å‰²å›¾åƒç­‰ã€‚è¯¥æ¨¡å‹æ¥å—å›¾åƒå’ŒæŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œå¹¶åœ¨æ ‡å‡†çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¨¡å¼ä¸­é¢„æµ‹ç¼–è¾‘åçš„å›¾åƒä»¤ç‰Œã€‚ä¸ºäº†å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒçš„å¯¹åº”ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå°†çŸ¥è¯†ä»åŸºç¡€æ¨¡å‹è’¸é¦åˆ°è‡ªå›å½’å»ºæ¨¡è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¸å„ç§æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸ç«äº‰ã€‚æƒ³äº†è§£æ›´å¤šå…³äºæ­¤é¡¹ç›®çš„ä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://jitengmu.github.io/EditAR/">é¡¹ç›®é¡µé¢é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¯æ§å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>è‡ªå›å½’æ¨¡å‹å…·æœ‰å†…åœ¨çš„ç»Ÿä¸€æ ‡è®°è¡¨ç¤ºï¼Œä¾¿äºåˆ›å»ºç»Ÿä¸€æ¨¡å‹åº”å¯¹å¤šç§ä»»åŠ¡ã€‚</li>
<li>EditARæ¡†æ¶æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¨¡å‹ï¼Œé€‚ç”¨äºå¤šç§æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>EditARæ¥å—å›¾åƒå’ŒæŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ç¼–è¾‘åçš„å›¾åƒä»¤ç‰Œã€‚</li>
<li>é€šè¿‡è’¸é¦æŠ€æœ¯ï¼Œå¢å¼ºäº†æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹åº”å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒEditARæ€§èƒ½ä¸æœ€å…ˆè¿›çš„ä»»åŠ¡ç‰¹å®šæ–¹æ³•ç›¸ç«äº‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cf12a8610ad1c57b9d782854bfe4b06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99a98974e503ad6e3360f8f7db57f6ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e7755b225f6d9b3cc44174aea44eabb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPAR3D-Stable-Point-Aware-Reconstruction-of-3D-Objects-from-Single-Images"><a href="#SPAR3D-Stable-Point-Aware-Reconstruction-of-3D-Objects-from-Single-Images" class="headerlink" title="SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single   Images"></a>SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single   Images</h2><p><strong>Authors:Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, Varun Jampani</strong></p>
<p>We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based modeling and generative modeling. Regression methods efficiently infer visible surfaces, but struggle with occluded regions. Generative methods handle uncertain regions better by modeling distributions, but are computationally expensive and the generation is often misaligned with visible surfaces. In this paper, we present SPAR3D, a novel two-stage approach aiming to take the best of both directions. The first stage of SPAR3D generates sparse 3D point clouds using a lightweight point diffusion model, which has a fast sampling speed. The second stage uses both the sampled point cloud and the input image to create highly detailed meshes. Our two-stage design enables probabilistic modeling of the ill-posed single-image 3D task while maintaining high computational efficiency and great output fidelity. Using point clouds as an intermediate representation further allows for interactive user edits. Evaluated on diverse datasets, SPAR3D demonstrates superior performance over previous state-of-the-art methods, at an inference speed of 0.7 seconds. Project page with code and model: <a target="_blank" rel="noopener" href="https://spar3d.github.io/">https://spar3d.github.io</a> </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å•å›¾åƒ3Då¯¹è±¡é‡å»ºçš„é—®é¢˜ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œä¸»è¦åˆ†ä¸ºä¸¤ä¸ªæ–¹å‘ï¼šåŸºäºå›å½’çš„å»ºæ¨¡å’Œç”Ÿæˆå¼å»ºæ¨¡ã€‚å›å½’æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨æ–­å‡ºå¯è§è¡¨é¢ï¼Œä½†åœ¨å¤„ç†é®æŒ¡åŒºåŸŸæ—¶é‡åˆ°å›°éš¾ã€‚ç”Ÿæˆå¼æ–¹æ³•é€šè¿‡å»ºæ¨¡åˆ†å¸ƒæ¥å¤„ç†ä¸ç¡®å®šåŒºåŸŸï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä¸”ç”Ÿæˆçš„ç»“æœå¾€å¾€ä¸å¯è§è¡¨é¢ä¸åŒ¹é…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SPAR3Dï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œæ—¨åœ¨èåˆä¸¤ä¸ªæ–¹å‘çš„ä¼˜ç‚¹ã€‚SPAR3Dçš„ç¬¬ä¸€é˜¶æ®µä½¿ç”¨è½»é‡çº§çš„ç‚¹æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¨€ç–çš„3Dç‚¹äº‘ï¼Œå…·æœ‰å¿«é€Ÿçš„é‡‡æ ·é€Ÿåº¦ã€‚ç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨é‡‡æ ·å¾—åˆ°çš„ç‚¹äº‘å’Œè¾“å…¥å›¾åƒæ¥åˆ›å»ºé«˜åº¦è¯¦ç»†çš„ç½‘æ ¼ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè®¾è®¡èƒ½å¤Ÿåœ¨ä¿æŒé«˜è®¡ç®—æ•ˆç‡å’Œè¾“å‡ºè´¨é‡çš„åŒæ—¶ï¼Œå¯¹ä¸é€‚å®šçš„å•å›¾åƒ3Dä»»åŠ¡è¿›è¡Œæ¦‚ç‡å»ºæ¨¡ã€‚ä»¥ç‚¹äº‘ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼è¿˜å¯ä»¥è¿›ä¸€æ­¥å®ç°äº¤äº’å¼ç”¨æˆ·ç¼–è¾‘ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSPAR3Dåœ¨æ¨ç†é€Ÿåº¦ä¸º0.7ç§’çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢åŒ…å«ä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://spar3d.github.io/">https://spar3d.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04689v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å•å›¾åƒä¸‰ç»´ç‰©ä½“é‡å»ºçš„é—®é¢˜ã€‚å½“å‰ç ”ç©¶ä¸»è¦åˆ†ä¸¤å¤§æ–¹å‘ï¼šå›å½’å»ºæ¨¡å’Œç”Ÿæˆå»ºæ¨¡ã€‚å›å½’æ–¹æ³•èƒ½é«˜æ•ˆæ¨æ–­å¯è§è¡¨é¢ï¼Œä½†éš¾ä»¥å¤„ç†é®æŒ¡åŒºåŸŸã€‚ç”Ÿæˆæ–¹æ³•é€šè¿‡å»ºæ¨¡åˆ†å¸ƒæ¥å¤„ç†ä¸ç¡®å®šåŒºåŸŸï¼Œä½†è®¡ç®—é‡å¤§ä¸”ç”Ÿæˆç»“æœå¸¸ä¸å¯è§è¡¨é¢ä¸åŒ¹é…ã€‚æœ¬ç ”ç©¶æå‡ºSPAR3Dï¼Œä¸€ç§ç»“åˆä¸¤è€…ä¼˜ç‚¹çš„æ–°å‹ä¸¤é˜¶æ®µæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è½»é‡çº§ç‚¹æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¨€ç–ä¸‰ç»´ç‚¹äº‘ï¼Œé‡‡æ ·é€Ÿåº¦å¿«ï¼›ç¬¬äºŒé˜¶æ®µç»“åˆé‡‡æ ·ç‚¹äº‘å’Œè¾“å…¥å›¾åƒåˆ›å»ºé«˜ç²¾åº¦ç½‘æ ¼ã€‚SPAR3Dçš„ä¸¤é˜¶æ®µè®¾è®¡å®ç°äº†å•å›¾åƒä¸‰ç»´ä»»åŠ¡çš„æ¦‚ç‡å»ºæ¨¡ï¼ŒåŒæ—¶ä¿æŒé«˜è®¡ç®—æ•ˆç‡å’Œå‡ºè‰²çš„è¾“å‡ºä¿çœŸåº¦ã€‚åˆ©ç”¨ç‚¹äº‘ä½œä¸ºä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œè¿˜å¯å®ç°ç”¨æˆ·äº¤äº’ç¼–è¾‘ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSPAR3Dåœ¨æ¨ç†é€Ÿåº¦è¾¾åˆ°0.7ç§’çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯æ˜¯å…³äºå•å›¾åƒä¸‰ç»´ç‰©ä½“é‡å»ºé—®é¢˜ï¼Œè¯¥é¢†åŸŸè¿‘æœŸä¸»è¦åˆ†ä¸ºå›å½’å»ºæ¨¡å’Œç”Ÿæˆå»ºæ¨¡ä¸¤å¤§æ–¹å‘ã€‚</li>
<li>å›å½’æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆæ¨æ–­å¯è§è¡¨é¢ï¼Œä½†åœ¨å¤„ç†é®æŒ¡åŒºåŸŸæ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>ç”Ÿæˆæ–¹æ³•é€šè¿‡å»ºæ¨¡åˆ†å¸ƒæ¥å¤„ç†ä¸ç¡®å®šåŒºåŸŸï¼Œä½†å­˜åœ¨è®¡ç®—é‡å¤§å’Œç”Ÿæˆç»“æœä¸å¯è§è¡¨é¢ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li>
<li>SPAR3Dæ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œæ—¨åœ¨ç»“åˆä¸¤è€…çš„ä¼˜ç‚¹ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è½»é‡çº§ç‚¹æ‰©æ•£æ¨¡å‹å¿«é€Ÿç”Ÿæˆç¨€ç–ä¸‰ç»´ç‚¹äº‘ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µç»“åˆé‡‡æ ·ç‚¹äº‘å’Œè¾“å…¥å›¾åƒåˆ›å»ºé«˜ç²¾åº¦ç½‘æ ¼ï¼Œå®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2fee052660da0754b0aa18a9e714ddff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f99fc7cd8c3614b79b727950db5283ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11ff5df348ebc5845db78b0eceea446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbcfd35c54a069d8c0fcf1b494db840.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b3de74654fe28ad8dba627c8c114f34.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models"><a href="#DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models" class="headerlink" title="DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion   Models"></a>DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion   Models</h2><p><strong>Authors:Hyogon Ryu, NaHyeon Park, Hyunjung Shim</strong></p>
<p>Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. To mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights and activations into lower-bit formats. However, existing methods often struggle to preserve both image quality and text-image alignment, particularly in lower-bit($&lt;$ 8bits) quantization. In this paper, we analyze the challenges associated with quantizing text-to-image diffusion models from a distributional perspective. Our analysis reveals that activation outliers play a crucial role in determining image quality. Additionally, we identify distinctive patterns in cross-attention scores, which significantly affects text-image alignment. To address these challenges, we propose Distribution-aware Group Quantization (DGQ), a method that identifies and adaptively handles pixel-wise and channel-wise outliers to preserve image quality. Furthermore, DGQ applies prompt-specific logarithmic quantization scales to maintain text-image alignment. Our method demonstrates remarkable performance on datasets such as MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit quantization of text-to-image diffusion models without requiring additional fine-tuning of weight quantization parameters. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶è®¡ç®—å’Œå†…å­˜éœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨çš„èŒƒå›´ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„é‡åŒ–ã€‚é€šè¿‡å‹ç¼©æƒé‡å’Œæ¿€æ´»å€¼åˆ°ä½ä½æ ¼å¼ï¼Œå®ƒå¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨é‡å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨å›¾åƒè´¨é‡å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢å–å¾—å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ä½ï¼ˆ&lt;8ä½ï¼‰é‡åŒ–ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»åˆ†å¸ƒçš„è§’åº¦åˆ†æäº†é‡åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¿€æ´»å¼‚å¸¸å€¼åœ¨å†³å®šå›¾åƒè´¨é‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†è·¨æ³¨æ„åŠ›åˆ†æ•°çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œè¿™æ˜¾è‘—å½±å“äº†æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å¸ƒæ„ŸçŸ¥ç»„é‡åŒ–ï¼ˆDGQï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å’Œè‡ªé€‚åº”å¤„ç†åƒç´ çº§å’Œé€šé“çº§çš„å¼‚å¸¸å€¼ï¼Œä»¥ä¿ç•™å›¾åƒè´¨é‡ã€‚æ­¤å¤–ï¼ŒDGQåº”ç”¨æç¤ºç‰¹å®šçš„å¯¹æ•°é‡åŒ–å°ºåº¦ä»¥ç»´æŒæ–‡æœ¬-å›¾åƒå¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MS-COCOå’ŒPartiPromptsç­‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ˜¯é¦–æ‰¹æˆåŠŸå®ç°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä½ä½é‡åŒ–çš„ç ”ç©¶è€…ï¼Œä¸”æ— éœ€å¯¹æƒé‡é‡åŒ–å‚æ•°è¿›è¡Œé¢å¤–çš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04304v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ugonfor.kr/DGQ">https://ugonfor.kr/DGQ</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é‡åŒ–é—®é¢˜ã€‚ç”±äºæ‰©æ•£æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå…¶è®¡ç®—ä¸å†…å­˜éœ€æ±‚é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…å°è¯•å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä»¥é™ä½å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢è¾¾åˆ°å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ä½ï¼ˆ&lt;8ä½ï¼‰é‡åŒ–ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚æœ¬æ–‡ä»åˆ†å¸ƒè§’åº¦å‡ºå‘ï¼Œåˆ†æäº†é‡åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºä¸€ç§åä¸ºåˆ†å¸ƒæ„ŸçŸ¥ç»„é‡åŒ–ï¼ˆDGQï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªé€‚åº”å¤„ç†åƒç´ çº§å’Œé€šé“çº§çš„å¼‚å¸¸å€¼ï¼Œä»¥ä¿ç•™å›¾åƒè´¨é‡ï¼Œå¹¶åº”ç”¨æç¤ºç‰¹å®šå¯¹æ•°é‡åŒ–å°ºåº¦æ¥ç»´æŒæ–‡æœ¬-å›¾åƒå¯¹é½ã€‚DGQæ–¹æ³•åœ¨MS-COCOå’ŒPartiPromptsç­‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªæˆåŠŸå®ç°ä½ä½é‡åŒ–çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œæ— éœ€å¯¹æƒé‡é‡åŒ–å‚æ•°è¿›è¡Œé¢å¤–å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜éœ€æ±‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>é‡åŒ–æ˜¯é™ä½æ‰©æ•£æ¨¡å‹å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æˆæœ¬çš„ä¸€ç§æ–¹æ³•ã€‚</li>
<li>ç°æœ‰é‡åŒ–æ–¹æ³•åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ä½é‡åŒ–ä¸­ã€‚</li>
<li>æœ¬æ–‡ä»åˆ†å¸ƒè§’åº¦åˆ†æäº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é‡åŒ–æŒ‘æˆ˜ã€‚</li>
<li>æ¿€æ´»å¼‚å¸¸å€¼å¯¹å›¾åƒè´¨é‡è‡³å…³é‡è¦ã€‚</li>
<li>äº¤å‰æ³¨æ„åŠ›å¾—åˆ†å¯¹æ–‡æœ¬-å›¾åƒå¯¹é½æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d71dc06bf1edf6861c2eec03bf3e258c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8306f331673ad6125396d214d3f428b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57468beda073fc5fa40399b03496ba65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a9b2c900bffb7795c2efc4a6ee10534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06cce2ce693aa97239b6b69905703f23.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adapting-Image-to-Video-Diffusion-Models-for-Large-Motion-Frame-Interpolation"><a href="#Adapting-Image-to-Video-Diffusion-Models-for-Large-Motion-Frame-Interpolation" class="headerlink" title="Adapting Image-to-Video Diffusion Models for Large-Motion Frame   Interpolation"></a>Adapting Image-to-Video Diffusion Models for Large-Motion Frame   Interpolation</h2><p><strong>Authors:Luoxu Jin, Hiroshi Watanabe</strong></p>
<p>With the development of video generation models has advanced significantly in recent years, we adopt large-scale image-to-video diffusion models for video frame interpolation. We present a conditional encoder designed to adapt an image-to-video model for large-motion frame interpolation. To enhance performance, we integrate a dual-branch feature extractor and propose a cross-frame attention mechanism that effectively captures both spatial and temporal information, enabling accurate interpolations of intermediate frames. Our approach demonstrates superior performance on the Fr&#39;echet Video Distance (FVD) metric when evaluated against other state-of-the-art approaches, particularly in handling large motion scenarios, highlighting advancements in generative-based methodologies. </p>
<blockquote>
<p>éšç€è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥æ˜¾è‘—å‘å±•ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¸§æ’å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡ä»¶ç¼–ç å™¨ï¼Œæ—¨åœ¨é€‚åº”å›¾åƒåˆ°è§†é¢‘æ¨¡å‹è¿›è¡Œå¤§è¿åŠ¨å¸§æ’å€¼ã€‚ä¸ºäº†æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬é›†æˆäº†åŒåˆ†æ”¯ç‰¹å¾æå–å™¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§è·¨å¸§æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥æœ‰æ•ˆåœ°æ•è·ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œä»è€Œå®ç°ä¸­é—´å¸§çš„å‡†ç¡®æ’å€¼ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨FrÃ©chetè§†é¢‘è·ç¦»ï¼ˆFVDï¼‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è¿åŠ¨åœºæ™¯æ—¶ï¼Œå‡¸æ˜¾äº†åŸºäºç”Ÿæˆçš„æ–¹æ³•çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17042v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§è§„æ¨¡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹çš„è§†é¢‘å¸§æ’å€¼æŠ€æœ¯ã€‚é‡‡ç”¨æ¡ä»¶ç¼–ç å™¨è®¾è®¡ï¼Œä»¥é€‚åº”å›¾åƒåˆ°è§†é¢‘æ¨¡å‹çš„æ’å€¼å¤„ç†ã€‚ä¸ºæé«˜æ€§èƒ½ï¼Œç»“åˆäº†åŒåˆ†æ”¯ç‰¹å¾æå–å™¨ï¼Œå¹¶æå‡ºè·¨å¸§æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆæ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œå®ç°äº†å‡†ç¡®çš„ä¸­é—´å¸§æ’å€¼ã€‚ç›¸è¾ƒäºå…¶ä»–å‰æ²¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è¿åŠ¨åœºæ™¯æ—¶ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨FrÃ©chetè§†é¢‘è·ç¦»ï¼ˆFVDï¼‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œçªæ˜¾äº†ç”Ÿæˆå¼æ–¹æ³•çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‡ç”¨å¤§è§„æ¨¡å›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œè§†é¢‘å¸§æ’å€¼ã€‚</li>
<li>è®¾è®¡äº†æ¡ä»¶ç¼–ç å™¨ä»¥é€‚åº”å›¾åƒåˆ°è§†é¢‘æ¨¡å‹çš„æ’å€¼å¤„ç†ã€‚</li>
<li>ç»“åˆåŒåˆ†æ”¯ç‰¹å¾æå–å™¨æé«˜æ€§èƒ½ã€‚</li>
<li>æå‡ºè·¨å¸§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ã€‚</li>
<li>å®ç°äº†å‡†ç¡®çš„ä¸­é—´å¸§æ’å€¼ã€‚</li>
<li>åœ¨FrÃ©chetè§†é¢‘è·ç¦»ï¼ˆFVDï¼‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f3e169baccfefe48a49a79ba9cb130e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30a95588000c6480ffcf1adaaab61f8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7ef32c88224c5b7c8e1ef388868036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4701af9c4a0021500deaa15d8d74ed81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239a8abf3459480bfc13d5ec875a1a72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f541d618bd6d22adae1294430a0ef491.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy"><a href="#Label-Efficient-Data-Augmentation-with-Video-Diffusion-Models-for-Guidewire-Segmentation-in-Cardiac-Fluoroscopy" class="headerlink" title="Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy"></a>Label-Efficient Data Augmentation with Video Diffusion Models for   Guidewire Segmentation in Cardiac Fluoroscopy</h2><p><strong>Authors:Shaoyan Pan, Yikang Liu, Lin Zhao, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun</strong></p>
<p>The accurate segmentation of guidewires in interventional cardiac fluoroscopy videos is crucial for computer-aided navigation tasks. Although deep learning methods have demonstrated high accuracy and robustness in wire segmentation, they require substantial annotated datasets for generalizability, underscoring the need for extensive labeled data to enhance model performance. To address this challenge, we propose the Segmentation-guided Frame-consistency Video Diffusion Model (SF-VD) to generate large collections of labeled fluoroscopy videos, augmenting the training data for wire segmentation networks. SF-VD leverages videos with limited annotations by independently modeling scene distribution and motion distribution. It first samples the scene distribution by generating 2D fluoroscopy images with wires positioned according to a specified input mask, and then samples the motion distribution by progressively generating subsequent frames, ensuring frame-to-frame coherence through a frame-consistency strategy. A segmentation-guided mechanism further refines the process by adjusting wire contrast, ensuring a diverse range of visibility in the synthesized image. Evaluation on a fluoroscopy dataset confirms the superior quality of the generated videos and shows significant improvements in guidewire segmentation. </p>
<blockquote>
<p>åœ¨å¿ƒè„ä»‹å…¥æ‰‹æœ¯çš„è§å…‰é€è§†è§†é¢‘ä¸­ï¼Œå¯¹å¯¼çº¿è¿›è¡Œå‡†ç¡®çš„åˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©å¯¼èˆªä»»åŠ¡è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨å¯¼çº¿åˆ†å‰²æ–¹é¢å·²ç»è¡¨ç°å‡ºäº†é«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®é›†æ¥å®ç°æ³›åŒ–ï¼Œè¿™çªæ˜¾äº†å¯¹ä¸°å¯Œæ ‡æ³¨æ•°æ®çš„éœ€è¦ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å‰²å¼•å¯¼çš„å¸§ä¸€è‡´æ€§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆSF-VDï¼‰ï¼Œä»¥ç”Ÿæˆå¤§é‡çš„æ ‡æ³¨è§å…‰é€è§†è§†é¢‘ï¼Œå¢å¼ºå¯¼çº¿åˆ†å‰²ç½‘ç»œçš„è®­ç»ƒæ•°æ®ã€‚SF-VDé€šè¿‡ç‹¬ç«‹å»ºæ¨¡åœºæ™¯åˆ†å¸ƒå’Œè¿åŠ¨åˆ†å¸ƒæ¥åˆ©ç”¨æ ‡æ³¨è¾ƒå°‘çš„è§†é¢‘ã€‚å®ƒé€šè¿‡æ ¹æ®æŒ‡å®šçš„è¾“å…¥æ©ç ç”Ÿæˆå¸¦æœ‰å¯¼çº¿çš„äºŒç»´è§å…‰é€è§†å›¾åƒæ¥é‡‡æ ·åœºæ™¯åˆ†å¸ƒï¼Œç„¶åé€šè¿‡é€æ­¥ç”Ÿæˆåç»­å¸§æ¥é‡‡æ ·è¿åŠ¨åˆ†å¸ƒï¼Œé€šè¿‡å¸§ä¸€è‡´æ€§ç­–ç•¥ç¡®ä¿å¸§åˆ°å¸§çš„ä¸€è‡´æ€§ã€‚åˆ†å‰²å¼•å¯¼æœºåˆ¶è¿›ä¸€æ­¥é€šè¿‡è°ƒæ•´å¯¼çº¿å¯¹æ¯”åº¦æ¥å®Œå–„è¿™ä¸€è¿‡ç¨‹ï¼Œç¡®ä¿åˆæˆå›¾åƒçš„å¯è§æ€§èŒƒå›´å¹¿æ³›ã€‚åœ¨è§å…‰é€è§†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯å®äº†æ‰€ç”Ÿæˆè§†é¢‘çš„é«˜è´¨é‡ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¯¼çº¿åˆ†å‰²çš„æ˜¾è‘—æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16050v3">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¿ƒè„ä»‹å…¥è§å…‰è§†é¢‘ä¸­çš„å¯¼çº¿å‡†ç¡®åˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©å¯¼èˆªä»»åŠ¡è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ä»¥æé«˜æ€§èƒ½çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆ†å‰²å¼•å¯¼çš„å¸§ä¸€è‡´æ€§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆSF-VDï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ç‹¬ç«‹å»ºæ¨¡åœºæ™¯åˆ†å¸ƒå’Œè¿åŠ¨åˆ†å¸ƒï¼Œç”Ÿæˆå¤§é‡æ ‡æ³¨çš„è§å…‰è§†é¢‘ï¼Œå¢å¼ºå¯¼çº¿åˆ†å‰²ç½‘ç»œçš„è®­ç»ƒæ•°æ®ã€‚SF-VDé¦–å…ˆæ ¹æ®æŒ‡å®šçš„è¾“å…¥æ©è†œç”ŸæˆäºŒç»´è§å…‰å›¾åƒæ¥é‡‡æ ·åœºæ™¯åˆ†å¸ƒï¼Œç„¶åé€šè¿‡é€æ­¥ç”Ÿæˆåç»­å¸§æ¥é‡‡æ ·è¿åŠ¨åˆ†å¸ƒï¼Œç¡®ä¿å¸§é—´ä¸€è‡´æ€§ã€‚åˆ†å‰²å¼•å¯¼æœºåˆ¶è¿›ä¸€æ­¥è°ƒæ•´å¯¼çº¿çš„å¯¹æ¯”åº¦ï¼Œç¡®ä¿åˆæˆå›¾åƒçš„å¯è§æ€§å¤šæ ·ã€‚åœ¨è§å…‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯å®äº†ç”Ÿæˆè§†é¢‘çš„é«˜è´¨é‡ï¼Œå¹¶åœ¨å¯¼çº¿åˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®åˆ†å‰²å¿ƒè„ä»‹å…¥è§å…‰è§†é¢‘ä¸­çš„å¯¼çº¿å¯¹è®¡ç®—æœºè¾…åŠ©å¯¼èˆªè‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¯¼çº¿åˆ†å‰²ä¸­è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œä½†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹SF-VDæ¥è§£å†³æ ‡æ³¨æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>SF-VDé€šè¿‡ç‹¬ç«‹å»ºæ¨¡åœºæ™¯åˆ†å¸ƒå’Œè¿åŠ¨åˆ†å¸ƒæ¥ç”Ÿæˆå¤§é‡æ ‡æ³¨çš„è§å…‰è§†é¢‘ã€‚</li>
<li>SF-VDåˆ©ç”¨å¸§ä¸€è‡´æ€§ç­–ç•¥ç¡®ä¿ç”Ÿæˆçš„è§†é¢‘å¸§é—´è¿è´¯æ€§ã€‚</li>
<li>åˆ†å‰²å¼•å¯¼æœºåˆ¶ç”¨äºè°ƒæ•´å¯¼çº¿çš„å¯¹æ¯”åº¦ï¼Œå¢åŠ åˆæˆå›¾åƒçš„å¯è§æ€§å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27c007f4b20c6726a4d36b0e7b51d916.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9a2f0b88c0190b7511a9244cdd8f011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6dfead2b062aa6a8f15edfc59eb3429.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d47112dbe22818737ff4b9362916e1d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Open-Source-Acceleration-of-Stable-Diffusion-cpp-Deployable-on-All-Devices"><a href="#Open-Source-Acceleration-of-Stable-Diffusion-cpp-Deployable-on-All-Devices" class="headerlink" title="Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All   Devices"></a>Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All   Devices</h2><p><strong>Authors:Jingxu Ng, Cheng Lv, Pu Zhao, Wei Niu, Juyi Lin, Minzhou Pan, Yun Liang, Yanzhi Wang</strong></p>
<p>Stable diffusion plays a crucial role in generating high-quality images. However, image generation is time-consuming and memory-intensive. To address this, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate the diffusion models. Although it is lightweight, the current implementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference latency and massive memory usage. To address this, in this work, we present an optimized version of Sdcpp leveraging the Winograd algorithm to accelerate 2D convolution operations, which is the primary bottleneck in the pipeline. By analyzing both dependent and independent computation graphs, we exploit the deviceâ€™s locality and parallelism to achieve substantial performance improvements. Our framework delivers correct end-to-end results across various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and SDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for individual convolutional layers and an inference speedup up to 4.79x for the overall image generation process, compared with the original Sdcpp on M1 pro. Homepage: <a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a> </p>
<blockquote>
<p>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå›¾åƒç”Ÿæˆæ˜¯è€—æ—¶çš„ä¸”éœ€è¦å¤§é‡å†…å­˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œstable-diffusion.cppï¼ˆSdcppï¼‰ä½œä¸ºä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†æ¡†æ¶åº”è¿è€Œç”Ÿï¼Œä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„è¿è¡Œã€‚è™½ç„¶å®ƒå¾ˆè½»ä¾¿ï¼Œä½†Sdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å¹¶ä¸ç†æƒ³ï¼Œå­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜å’Œå†…å­˜ä½¿ç”¨é‡å¤§çš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬å·¥ä½œä¸­æ¨å‡ºäº†ä¸€ä¸ªä¼˜åŒ–ç‰ˆçš„Sdcppï¼Œåˆ©ç”¨Winogradç®—æ³•åŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè¿™æ˜¯ç®¡é“ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚é€šè¿‡åˆ†ææœ‰ä¾èµ–å’Œæ— ä¾èµ–çš„è®¡ç®—å›¾ï¼Œæˆ‘ä»¬åˆ©ç”¨è®¾å¤‡çš„å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å„ç§ç¨³å®šçš„æ‰©æ•£æ¨¡å‹ä¸­éƒ½èƒ½æä¾›æ­£ç¡®çš„ç«¯åˆ°ç«¯ç»“æœï¼ŒåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åŸå§‹Sdcppåœ¨M1 proä¸Šçš„è¡¨ç°ç›¸æ¯”ï¼Œå•ä¸ªå·ç§¯å±‚çš„é€Ÿåº¦æé«˜äº†2.76å€ï¼Œæ•´ä¸ªå›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ¨ç†é€Ÿåº¦æé«˜äº†4.79å€ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/SealAILab/stable-diffusion-cpp">https://github.com/SealAILab/stable-diffusion-cpp</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05781v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½†å›¾åƒç”Ÿæˆè€—æ—¶ä¸”å ç”¨è¿‡å¤šçš„å†…å­˜èµ„æºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œstable-diffusion.cppï¼ˆSdcppï¼‰ä½œä¸ºä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†æ¡†æ¶åº”è¿è€Œç”Ÿï¼Œä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„è¿è¡Œã€‚ç„¶è€Œï¼ŒSdcppä¸­ggml_conv_2dç®—å­çš„å½“å‰å®ç°å¹¶ä¸ç†æƒ³ï¼Œå­˜åœ¨æ¨ç†å»¶è¿Ÿé«˜å’Œå†…å­˜ä½¿ç”¨é‡å¤§ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨Winogradç®—æ³•ä¼˜åŒ–Sdcppçš„ç‰ˆæœ¬ï¼Œä»¥åŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè¿™æ˜¯ç®¡é“ä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚é€šè¿‡åˆ†ææœ‰ä¾èµ–å’Œæ— ä¾èµ–çš„è®¡ç®—å›¾ï¼Œæˆ‘ä»¬å……åˆ†åˆ©ç”¨è®¾å¤‡çš„å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€‚ç”¨äºå„ç§ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›¸å¯¹äºåŸå§‹Sdcppåœ¨M1 proä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯¹å•ä¸ªå·ç§¯å±‚çš„åŠ é€Ÿæœ€é«˜å¯è¾¾2.76å€ï¼Œå¯¹æ•´ä¸ªå›¾åƒç”Ÿæˆè¿‡ç¨‹çš„æ¨ç†é€Ÿåº¦æé«˜æœ€é«˜å¯è¾¾4.79å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨³å®šæ‰©æ•£åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å­˜åœ¨æ—¶é—´æ¶ˆè€—å’Œå†…å­˜å ç”¨çš„é—®é¢˜ã€‚</li>
<li>stable-diffusion.cppï¼ˆSdcppï¼‰ä½œä¸ºæ¨ç†æ¡†æ¶æ—¨åœ¨ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚</li>
<li>Sdcppä¸­çš„ggml_conv_2dç®—å­å½“å‰å®ç°å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´é«˜æ¨ç†å»¶è¿Ÿå’Œå¤§é‡å†…å­˜ä½¿ç”¨ã€‚</li>
<li>åˆ©ç”¨Winogradç®—æ³•ä¼˜åŒ–Sdcppï¼ŒåŠ é€Ÿ2Då·ç§¯æ“ä½œï¼Œè§£å†³ä¸»è¦ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ†æè®¡ç®—å›¾ï¼Œå……åˆ†åˆ©ç”¨è®¾å¤‡å±€éƒ¨æ€§å’Œå¹¶è¡Œæ€§ï¼Œå®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>ä¼˜åŒ–çš„æ¡†æ¶é€‚ç”¨äºå¤šç§ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬SDv1.4ã€v1.5ã€v2.1ã€SDXLå’ŒSDXL-Turboã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç›¸å¯¹äºåŸå§‹Sdcppï¼Œä¼˜åŒ–åçš„æ¡†æ¶åœ¨å¤šä¸ªå±‚é¢å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b7675966091bc2de4f8e4d01ce77bc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d5835bb8fa6be35fd93dee6dd71e9be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13fdcbe21c9a16b15051d0562161b8a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2308dcd61b76d787ee9744ef3a7da592.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c258fbd8dd02404fc5bfaa4f332fe259.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PointDreamer-Zero-shot-3D-Textured-Mesh-Reconstruction-from-Colored-Point-Cloud"><a href="#PointDreamer-Zero-shot-3D-Textured-Mesh-Reconstruction-from-Colored-Point-Cloud" class="headerlink" title="PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored   Point Cloud"></a>PointDreamer: Zero-shot 3D Textured Mesh Reconstruction from Colored   Point Cloud</h2><p><strong>Authors:Qiao Yu, Xianzhi Li, Yuan Tang, Xu Han, Jinfeng Xu, Long Hu, Min Chen</strong></p>
<p>Reconstructing textured meshes from colored point clouds is an important but challenging task. Most existing methods yield blurry-looking textures or rely on 3D training data that are hard to acquire. Regarding this, we propose PointDreamer, a novel framework for textured mesh reconstruction from colored point cloud via diffusion-based 2D inpainting. Specifically, we first reconstruct an untextured mesh. Next, we project the input point cloud into 2D space to generate sparse multi-view images, and then inpaint empty pixels utilizing a pre-trained 2D diffusion model. After that, we unproject the colors of the inpainted dense images onto the untextured mesh, thus obtaining the final textured mesh. This project-inpaint-unproject pipeline bridges the gap between 3D point clouds and 2D diffusion models for the first time. Thanks to the powerful 2D diffusion model pre-trained on extensive 2D data, PointDreamer reconstructs clear, high-quality textures with high robustness to sparse or noisy input. Also, itâ€™s zero-shot requiring no extra training. In addition, we design Non-Border-First unprojection strategy to address the border-area inconsistency issue, which is less explored but commonly-occurred in methods that generate 3D textures from multiview images. Extensive qualitative and quantitative experiments on various synthetic and real-scanned datasets show the SoTA performance of PointDreamer, by significantly outperforming baseline methods with 30% improvement in LPIPS score (from 0.118 to 0.068). Code at: <a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/PointDreamer">https://github.com/YuQiao0303/PointDreamer</a>. </p>
<blockquote>
<p>ä»å½©è‰²ç‚¹äº‘é‡å»ºçº¹ç†ç½‘æ ¼æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•äº§ç”Ÿçš„çº¹ç†æ¨¡ç³Šï¼Œæˆ–è€…ä¾èµ–äºéš¾ä»¥è·å–çš„3Dè®­ç»ƒæ•°æ®ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PointDreamerï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä»å½©è‰²ç‚¹äº‘é‡å»ºçº¹ç†ç½‘æ ¼çš„æ–¹æ³•ï¼ŒåŸºäºæ‰©æ•£çš„2Då›¾åƒä¿®å¤æŠ€æœ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡å»ºä¸€ä¸ªæ— çº¹ç†çš„ç½‘æ ¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¾“å…¥çš„ç‚¹äº‘æŠ•å½±åˆ°2Dç©ºé—´ï¼Œç”Ÿæˆç¨€ç–çš„å¤šè§†è§’å›¾åƒï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹å¡«å……ç©ºç™½åƒç´ ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†å¡«å……åçš„å¯†é›†å›¾åƒçš„é¢œè‰²åæŠ•å½±åˆ°æ— çº¹ç†çš„ç½‘æ ¼ä¸Šï¼Œä»è€Œè·å¾—æœ€ç»ˆçš„çº¹ç†ç½‘æ ¼ã€‚è¿™ç§æŠ•å½±-ä¿®å¤-åæŠ•å½±çš„ç®¡é“é¦–æ¬¡å¡«è¡¥äº†3Dç‚¹äº‘å’Œ2Dæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„ç©ºç™½ã€‚ç”±äºé¢„è®­ç»ƒçš„å¼ºå¤§çš„2Dæ‰©æ•£æ¨¡å‹åœ¨å¤§é‡çš„2Dæ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼ŒPointDreamerèƒ½å¤Ÿé‡å»ºæ¸…æ™°ã€é«˜è´¨é‡çš„çº¹ç†ï¼Œå¯¹ç¨€ç–æˆ–å˜ˆæ‚çš„è¾“å…¥å…·æœ‰é«˜åº¦çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œå®ƒæ˜¯é›¶æ¬¡å°„å‡»ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚å¦å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†éè¾¹ç•Œä¼˜å…ˆçš„åæŠ•å½±ç­–ç•¥ï¼Œä»¥è§£å†³ä»å¤šè§†è§’å›¾åƒç”Ÿæˆ3Dçº¹ç†æ—¶è¾ƒå°‘æ¢ç´¢ä½†å¸¸å‘ç”Ÿçš„è¾¹ç•ŒåŒºåŸŸä¸ä¸€è‡´é—®é¢˜ã€‚åœ¨å„ç§åˆæˆå’ŒçœŸå®æ‰«ææ•°æ®é›†ä¸Šçš„å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒPointDreamerçš„æ€§èƒ½å¤„äºé¢†å…ˆæ°´å¹³ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•åœ¨LPIPSå¾—åˆ†ä¸Šæœ‰30%çš„æ”¹è¿›ï¼ˆä»0.118æé«˜åˆ°0.068ï¼‰ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/YuQiao0303/PointDreamer%E3%80%82">https://github.com/YuQiao0303/PointDreamerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15811v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    PointDreameræ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œå¯ä»å½©è‰²ç‚¹äº‘ä¸­é‡å»ºçº¹ç†ç½‘æ ¼ã€‚å®ƒé‡‡ç”¨æŠ•å½±-ä¿®å¤-åæŠ•å½±çš„æµç¨‹ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹å¡«å……ç¨€ç–åƒç´ ï¼Œä»è€Œå¾—åˆ°æ¸…æ™°çš„ã€é«˜è´¨é‡çš„çº¹ç†ã€‚è¯¥æ¡†æ¶æ— éœ€é¢å¤–è®­ç»ƒï¼Œå¯¹ç¨€ç–æˆ–å™ªå£°è¾“å…¥å…·æœ‰é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†è§£å†³è¾¹ç•ŒåŒºåŸŸä¸ä¸€è‡´é—®é¢˜çš„éè¾¹ç•Œä¼˜å…ˆåæŠ•å½±ç­–ç•¥ã€‚åœ¨å¤šä¸ªåˆæˆå’ŒçœŸå®æ‰«ææ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPointDreameræ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨LPIPSå¾—åˆ†ä¸Šæœ‰30%çš„æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PointDreameræ˜¯ä¸€ä¸ªç”¨äºä»å½©è‰²ç‚¹äº‘ä¸­é‡å»ºçº¹ç†ç½‘æ ¼çš„æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨æŠ•å½±-ä¿®å¤-åæŠ•å½±æµç¨‹ï¼Œç»“åˆä¸‰ç»´ç‚¹äº‘å’ŒäºŒç»´æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹ï¼ŒPointDreamerèƒ½å¤Ÿç”Ÿæˆæ¸…æ™°ã€é«˜è´¨é‡çš„çº¹ç†ã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹ç¨€ç–æˆ–å™ªå£°è¾“å…¥å…·æœ‰é²æ£’æ€§ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>å¼•å…¥éè¾¹ç•Œä¼˜å…ˆåæŠ•å½±ç­–ç•¥ï¼Œè§£å†³ä»å¤šè§†è§’å›¾åƒç”Ÿæˆä¸‰ç»´çº¹ç†æ—¶å¸¸è§çš„è¾¹ç•ŒåŒºåŸŸä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPointDreameråœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>PointDreamerçš„ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4ca16acfe286a6226cba7e4739f57265.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dc8168dfdac9e719ffd155e677c572b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-872b36aa0f7a8995c1a5207d19b7cd2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a727b77d1556a49e7e4a16e41f370e3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12afbf0768c18f510537d16dcbb9e583.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Stylebreeder-Exploring-and-Democratizing-Artistic-Styles-through-Text-to-Image-Models"><a href="#Stylebreeder-Exploring-and-Democratizing-Artistic-Styles-through-Text-to-Image-Models" class="headerlink" title="Stylebreeder: Exploring and Democratizing Artistic Styles through   Text-to-Image Models"></a>Stylebreeder: Exploring and Democratizing Artistic Styles through   Text-to-Image Models</h2><p><strong>Authors:Matthew Zheng, Enis Simsar, Hidir Yesiltepe, Federico Tombari, Joel Simon, Pinar Yanardag</strong></p>
<p>Text-to-image models are becoming increasingly popular, revolutionizing the landscape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce \texttt{STYLEBREEDER}, a comprehensive dataset of 6.8M images and 1.8M prompts generated by 95K users on Artbreeder, a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, user-generated styles that transcend conventional categories like â€˜cyberpunkâ€™ or â€˜Picasso,â€™ we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code and models are available at <a target="_blank" rel="noopener" href="https://stylebreeder.github.io/">https://stylebreeder.github.io</a> under a Public Domain (CC0) license. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹æ­£æ—¥ç›Šå—åˆ°æ¬¢è¿ï¼Œå®ƒé€šè¿‡ç”Ÿæˆé«˜åº¦è¯¦ç»†å’Œåˆ›é€ æ€§çš„è§†è§‰å†…å®¹ï¼Œå½»åº•æ”¹å˜äº†æ•°å­—è‰ºæœ¯åˆ›ä½œçš„æ ¼å±€ã€‚è¿™äº›æ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨è‰ºæœ¯ç”Ÿæˆé¢†åŸŸï¼Œå®ƒä»¬ä¿ƒè¿›äº†å¹¿æ³›çš„åˆ›æ„è¡¨è¾¾ï¼Œå¹¶ä½¿è‰ºæœ¯åˆ›ä½œå˜å¾—æ°‘ä¸»åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†<code>STYLEBREEDER</code>æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±Artbreederå¹³å°ä¸Š9.5ä¸‡åç”¨æˆ·ç”Ÿæˆçš„680ä¸‡å¼ å›¾åƒå’Œ180ä¸‡ä¸ªæç¤ºï¼ŒArtbreederå¹³å°å·²æˆä¸ºæ‹¥æœ‰è¶…è¿‡13ç™¾ä¸‡ç”¨æˆ·çš„åˆ›æ„æ¢ç´¢é‡è¦ä¸­å¿ƒã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªæ•°æ®é›†è¿›è¡Œäº†ä¸€ç³»åˆ—ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«ä¸åŒçš„è‰ºæœ¯é£æ ¼ã€ç”Ÿæˆä¸ªæ€§åŒ–å†…å®¹ä»¥åŠæ ¹æ®ç”¨æˆ·å…´è¶£æ¨èé£æ ¼ã€‚é€šè¿‡è®°å½•è¶…è¶Šä¼ ç»Ÿç±»åˆ«ï¼ˆå¦‚â€œèµ›åšæœ‹å…‹â€æˆ–â€œæ¯•åŠ ç´¢â€ï¼‰çš„ç‹¬ç‰¹ç”¨æˆ·ç”Ÿæˆé£æ ¼ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç‹¬ç‰¹çš„å¤§ä¼—æºé£æ ¼çš„æ½œåŠ›ï¼Œè¿™äº›é£æ ¼å¯èƒ½æ·±å…¥æ´å¯Ÿå…¨çƒç”¨æˆ·çš„é›†ä½“åˆ›æ„å¿ƒç†ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸åŒçš„ä¸ªæ€§åŒ–æ–¹æ³•æ¥å¢å¼ºè‰ºæœ¯è¡¨ç°åŠ›ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªé£æ ¼å›¾è°±ï¼Œä»¥LoRAæ ¼å¼æä¾›è¿™äº›æ¨¡å‹ä¾›å…¬ä¼—ä½¿ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹å…·æœ‰æ­ç¤ºå’Œä¿ƒè¿›ç‹¬ç‰¹è‰ºæœ¯è¡¨è¾¾æ–¹å¼çš„æ½œåŠ›ï¼Œè¿›ä¸€æ­¥ä½¿äººå·¥æ™ºèƒ½åœ¨è‰ºæœ¯åˆ›ä½œä¸­æ°‘ä¸»åŒ–ï¼Œå¹¶ä¿ƒè¿›ä¸€ä¸ªæ›´åŠ å¤šæ ·åŒ–å’ŒåŒ…å®¹æ€§çš„è‰ºæœ¯ç¤¾åŒºã€‚æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://stylebreeder.github.io/">https://stylebreeder.github.io</a>ä¸Šä»¥å…¬å…±é¢†åŸŸï¼ˆCC0ï¼‰è®¸å¯è¯ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14599v2">PDF</a> Accepted at NeurIPS 2024 D&amp;B Track, Project page:   <a target="_blank" rel="noopener" href="https://stylebreeder.github.io/">https://stylebreeder.github.io</a> HuggingFace DB Page:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stylebreeder/stylebreeder">https://huggingface.co/datasets/stylebreeder/stylebreeder</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ­£æ—¥ç›Šæ™®åŠï¼Œå®ƒä»¬é€šè¿‡ä¿ƒè¿›é«˜åº¦è¯¦ç»†å’Œåˆ›é€ æ€§çš„è§†è§‰å†…å®¹ç”Ÿæˆï¼Œé©å‘½æ€§åœ°æ”¹å˜äº†æ•°å­—è‰ºæœ¯åˆ›ä½œçš„æ ¼å±€ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸Artbreederå¹³å°ç›¸å…³çš„æ•°æ®é›†â€œSTYLEBREEDERâ€ï¼Œè¯¥å¹³å°åŒ…å«è¶…è¿‡1.3äº¿ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒå’Œæç¤ºï¼Œå·²æˆä¸ºåˆ›æ„æ¢ç´¢çš„é‡è¦ä¸­å¿ƒã€‚ç ”ç©¶é€šè¿‡ä¸€ç³»åˆ—ä»»åŠ¡æ¢ç´¢äº†ç‹¬ç‰¹ã€ç”¨æˆ·ç”Ÿæˆçš„è‰ºæœ¯é£æ ¼ï¼Œå¹¶è¯„ä¼°äº†ä¸ªæ€§åŒ–æ–¹æ³•ä»¥å¢å¼ºè‰ºæœ¯è¡¨ç°åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¨å‡ºäº†é£æ ¼å›¾è°±ï¼Œä»¥LoRAæ ¼å¼æä¾›è¿™äº›æ¨¡å‹ä¾›å…¬ä¼—ä½¿ç”¨ã€‚ç ”ç©¶è¡¨æ˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å…·æœ‰ä¿ƒè¿›ç‹¬ç‰¹è‰ºæœ¯è¡¨è¾¾å’Œåˆ›æ–°è‰ºæœ¯ç¤¾åŒºçš„æ½œåŠ›ã€‚æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://stylebreeder.github.ioç½‘ç«™ä¸Šå…è´¹è®¿é—®./">https://stylebreeder.github.ioç½‘ç«™ä¸Šå…è´¹è®¿é—®ã€‚</a> </p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨æ•°å­—è‰ºæœ¯é¢†åŸŸè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå¯ç”Ÿæˆé«˜åº¦è¯¦ç»†å’Œåˆ›é€ æ€§çš„è§†è§‰å†…å®¹ã€‚</li>
<li>â€œSTYLEBREEDERâ€æ•°æ®é›†ç”±è¶…è¿‡680ä¸‡å¼ å›¾åƒå’Œæ•°ç™¾ä¸‡ä¸ªæç¤ºç»„æˆï¼Œåæ˜ äº†ç”¨æˆ·çš„é›†ä½“åˆ›é€ åŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶æ¢ç´¢äº†ç”¨æˆ·ç”Ÿæˆçš„è‰ºæœ¯é£æ ¼ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è‰ºæœ¯é£æ ¼åˆ†ç±»ï¼Œå¦‚ç§‘å¹»æœ‹å…‹æˆ–æ¯•åŠ ç´¢é£æ ¼ç­‰ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸ªæ€§åŒ–æ–¹æ³•æ¥å¢å¼ºè‰ºæœ¯è¡¨ç°åŠ›ï¼Œå¹¶å¼•å…¥äº†é£æ ¼å›¾è°±æ¥å±•ç¤ºè¿™äº›æ¨¡å‹çš„åº”ç”¨ã€‚</li>
<li>è¿™äº›æ¨¡å‹å’Œå·¥å…·æ—¨åœ¨è¿›ä¸€æ­¥æ¨åŠ¨è‰ºæœ¯çš„æ°‘ä¸»åŒ–ï¼Œä¿ƒè¿›æ›´å¤šå¤šæ ·åŒ–çš„è‰ºæœ¯ç¤¾åŒºçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14599">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6509be896176aa2a76399bc4fa9ffc66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e807d9b1fcab7488c37c4a72b4d28645.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53651f2dce8d084d5c4329de8d8db8ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f17470272146fed20391edf498a160e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9cb009800080ea37bed899406964a27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb8932cae5c6b8906a43d42b421de4e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AnoFPDM-Anomaly-Segmentation-with-Forward-Process-of-Diffusion-Models-for-Brain-MRI"><a href="#AnoFPDM-Anomaly-Segmentation-with-Forward-Process-of-Diffusion-Models-for-Brain-MRI" class="headerlink" title="AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models   for Brain MRI"></a>AnoFPDM: Anomaly Segmentation with Forward Process of Diffusion Models   for Brain MRI</h2><p><strong>Authors:Yiming Che, Fazle Rafsani, Jay Shah, Md Mahfuzur Rahman Siddiquee, Teresa Wu</strong></p>
<p>Weakly-supervised diffusion models (DMs) in anomaly segmentation, leveraging image-level labels, have attracted significant attention for their superior performance compared to unsupervised methods. It eliminates the need for pixel-level labels in training, offering a more cost-effective alternative to supervised methods. However, existing methods are not fully weakly-supervised because they heavily rely on costly pixel-level labels for hyperparameter tuning in inference. To tackle this challenge, we introduce Anomaly Segmentation with Forward Process of Diffusion Models (AnoFPDM), a fully weakly-supervised framework that operates without the need of pixel-level labels. Leveraging the unguided forward process as a reference for the guided forward process, we select hyperparameters such as the noise scale, the threshold for segmentation and the guidance strength. We aggregate anomaly maps from guided forward process, enhancing the signal strength of anomalous regions. Remarkably, our proposed method outperforms recent state-of-the-art weakly-supervised approaches, even without utilizing pixel-level labels. </p>
<blockquote>
<p>å¼±ç›‘ç£æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å¼‚å¸¸åˆ†å‰²ä¸­çš„åº”ç”¨å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå®ƒåˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ï¼Œç›¸è¾ƒäºæ— ç›‘ç£æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å®ƒæ¶ˆé™¤äº†å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­åƒç´ çº§æ ‡ç­¾çš„éœ€æ±‚ï¼Œä¸ºç›‘ç£æ–¹æ³•æä¾›äº†æ›´å…·æˆæœ¬æ•ˆç›Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¹¶éå®Œå…¨å¼±ç›‘ç£ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–äºæ¨ç†è¿‡ç¨‹ä¸­çš„è¶…å‚æ•°è°ƒæ•´æ‰€éœ€çš„æ˜‚è´µåƒç´ çº§æ ‡ç­¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ‰©æ•£æ¨¡å‹æ­£å‘è¿‡ç¨‹çš„å¼‚å¸¸åˆ†å‰²ï¼ˆAnoFPDMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼±ç›‘ç£çš„æ¡†æ¶ï¼Œæ— éœ€åƒç´ çº§æ ‡ç­¾å³å¯è¿è¡Œã€‚æˆ‘ä»¬ä»¥éå¼•å¯¼æ­£å‘è¿‡ç¨‹ä½œä¸ºå¼•å¯¼æ­£å‘è¿‡ç¨‹çš„å‚è€ƒï¼Œé€‰æ‹©è¶…å‚æ•°ï¼Œå¦‚å™ªå£°è§„æ¨¡ã€åˆ†å‰²é˜ˆå€¼å’Œå¼•å¯¼å¼ºåº¦ã€‚æˆ‘ä»¬ä»å¼•å¯¼çš„æ­£å‘è¿‡ç¨‹ä¸­èšåˆå¼‚å¸¸å›¾ï¼Œå¢å¼ºå¼‚å¸¸åŒºåŸŸçš„ä¿¡å·å¼ºåº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å³ä½¿åœ¨ä¸éœ€è¦åƒç´ çº§æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä¹Ÿä¼˜äºæœ€æ–°çš„å¼±ç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15683v4">PDF</a> v4: added appendices and fixed some typos</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸åˆ†å‰²ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…¶æ€§èƒ½ä¼˜äºæ— ç›‘ç£æ–¹æ³•ã€‚å®ƒæ¶ˆé™¤äº†å¯¹åƒç´ çº§æ ‡ç­¾è®­ç»ƒçš„éœ€æ±‚ï¼Œæä¾›äº†æ›´ç»æµæ›¿ä»£ç›‘ç£æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¹¶éå®Œå…¨å¼±ç›‘ç£ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºæ˜‚è´µçš„åƒç´ çº§æ ‡ç­¾è¿›è¡Œæ¨ç†é˜¶æ®µçš„è¶…å‚æ•°è°ƒæ•´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å¼‚å¸¸åˆ†å‰²æ­£å‘è¿‡ç¨‹ï¼ˆAnomFPDMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼±ç›‘ç£çš„æ¡†æ¶ï¼Œæ— éœ€åƒç´ çº§æ ‡ç­¾å³å¯è¿è¡Œã€‚åˆ©ç”¨æœªå¼•å¯¼çš„æ­£å‘è¿‡ç¨‹ä½œä¸ºå¼•å¯¼æ­£å‘è¿‡ç¨‹çš„å‚è€ƒï¼Œæˆ‘ä»¬é€‰æ‹©è¶…å‚æ•°ï¼Œå¦‚å™ªå£°è§„æ¨¡ã€åˆ†å‰²é˜ˆå€¼å’Œå¼•å¯¼å¼ºåº¦ã€‚æˆ‘ä»¬ä»å¼•å¯¼æ­£å‘è¿‡ç¨‹ä¸­èšåˆå¼‚å¸¸å›¾ï¼Œå¢å¼ºå¼‚å¸¸åŒºåŸŸçš„ä¿¡å·å¼ºåº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ— åƒç´ çº§æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œç”šè‡³è¶…è¶Šäº†æœ€æ–°çš„å¼±ç›‘ç£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸åˆ†å‰²ä¸­å—åˆ°å…³æ³¨ï¼Œå…¶æ€§èƒ½ä¼˜äºæ— ç›‘ç£æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»ä¾èµ–åƒç´ çº§æ ‡ç­¾è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ï¼Œæˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨æ–°çš„å¼±ç›‘ç£æ¡†æ¶AnomFPDMï¼Œæ— éœ€åƒç´ çº§æ ‡ç­¾ã€‚</li>
<li>AnomFPDMåˆ©ç”¨æœªå¼•å¯¼çš„æ­£å‘è¿‡ç¨‹ä½œä¸ºå¼•å¯¼æ­£å‘è¿‡ç¨‹çš„å‚è€ƒï¼Œé€‰æ‹©å…³é”®è¶…å‚æ•°ã€‚</li>
<li>é€šè¿‡èšåˆæ¥è‡ªå¼•å¯¼æ­£å‘è¿‡ç¨‹çš„å¼‚å¸¸å›¾ï¼Œå¢å¼ºäº†å¼‚å¸¸åŒºåŸŸçš„ä¿¡å·å¼ºåº¦ã€‚</li>
<li>AnomFPDMè¶…è¶Šäº†æœ€æ–°çš„å¼±ç›‘ç£æ–¹æ³•ï¼Œå³ä½¿åœ¨æ²¡æœ‰åƒç´ çº§æ ‡ç­¾çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-720e7d5671c0de8268d794ed10bbeed0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fa9bbc333637f89efaf8fa02b18e9b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41b5617001f43066c61ea405e3101a80.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NeuralDiffuser-Neuroscience-inspired-Diffusion-Guidance-for-fMRI-Visual-Reconstruction"><a href="#NeuralDiffuser-Neuroscience-inspired-Diffusion-Guidance-for-fMRI-Visual-Reconstruction" class="headerlink" title="NeuralDiffuser: Neuroscience-inspired Diffusion Guidance for fMRI Visual   Reconstruction"></a>NeuralDiffuser: Neuroscience-inspired Diffusion Guidance for fMRI Visual   Reconstruction</h2><p><strong>Authors:Haoyu Li, Hao Wu, Badong Chen</strong></p>
<p>Reconstructing visual stimuli from functional Magnetic Resonance Imaging fMRI enables fine-grained retrieval of brain activity. However, the accurate reconstruction of diverse details, including structure, background, texture, color, and more, remains challenging. The stable diffusion models inevitably result in the variability of reconstructed images, even under identical conditions. To address this challenge, we first uncover the neuroscientific perspective of diffusion methods, which primarily involve top-down creation using pre-trained knowledge from extensive image datasets, but tend to lack detail-driven bottom-up perception, leading to a loss of faithful details. In this paper, we propose NeuralDiffuser, which incorporates primary visual feature guidance to provide detailed cues in the form of gradients. This extension of the bottom-up process for diffusion models achieves both semantic coherence and detail fidelity when reconstructing visual stimuli. Furthermore, we have developed a novel guidance strategy for reconstruction tasks that ensures the consistency of repeated outputs with original images rather than with various outputs. Extensive experimental results on the Natural Senses Dataset (NSD) qualitatively and quantitatively demonstrate the advancement of NeuralDiffuser by comparing it against baseline and state-of-the-art methods horizontally, as well as conducting longitudinal ablation studies. </p>
<blockquote>
<p>ä»åŠŸèƒ½æ€§ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰é‡å»ºè§†è§‰åˆºæ¿€èƒ½å¤Ÿå®ç°ç²¾ç»†çš„å¤§è„‘æ´»åŠ¨æ£€ç´¢ã€‚ç„¶è€Œï¼Œå‡†ç¡®é‡å»ºåŒ…æ‹¬ç»“æ„ã€èƒŒæ™¯ã€çº¹ç†ã€é¢œè‰²ç­‰åœ¨å†…çš„å„ç§ç»†èŠ‚ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç¨³å®šçš„æ‰©æ•£æ¨¡å‹å³ä½¿åœ¨ç›¸åŒæ¡ä»¶ä¸‹ä¹Ÿä¸å¯é¿å…åœ°å¯¼è‡´é‡å»ºå›¾åƒçš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆæ­ç¤ºäº†æ‰©æ•£æ–¹æ³•åœ¨ç¥ç»ç§‘å­¦æ–¹é¢çš„è§†è§’ï¼Œå…¶ä¸»è¦æ¶‰åŠä½¿ç”¨æ¥è‡ªå¤§é‡å›¾åƒæ•°æ®é›†çš„é¢„è®­ç»ƒçŸ¥è¯†è¿›è¡Œè‡ªä¸Šè€Œä¸‹çš„åˆ›å»ºï¼Œä½†å¾€å¾€ç¼ºä¹ç»†èŠ‚é©±åŠ¨çš„è‡ªä¸‹è€Œä¸Šçš„æ„ŸçŸ¥ï¼Œå¯¼è‡´ä¸¢å¤±å¿ å®ç»†èŠ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NeuralDiffuserï¼Œå®ƒç»“åˆäº†ä¸»è¦è§†è§‰ç‰¹å¾æŒ‡å¯¼ï¼Œä»¥æ¢¯åº¦å½¢å¼æä¾›è¯¦ç»†çº¿ç´¢ã€‚è¿™ç§æ‰©æ•£æ¨¡å‹çš„è‡ªä¸‹è€Œä¸Šè¿‡ç¨‹çš„æ‰©å±•åœ¨é‡å»ºè§†è§‰åˆºæ¿€æ—¶å®ç°äº†è¯­ä¹‰è¿è´¯æ€§å’Œç»†èŠ‚ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºé‡å»ºä»»åŠ¡å¼€å‘äº†ä¸€ç§æ–°çš„æŒ‡å¯¼ç­–ç•¥ï¼Œç¡®ä¿é‡å¤è¾“å‡ºä¸åŸå§‹å›¾åƒçš„ä¸€è‡´æ€§ï¼Œè€Œä¸æ˜¯ä¸å„ç§è¾“å‡ºçš„ä¸€è‡´ã€‚åœ¨è‡ªç„¶äººæ„Ÿæ•°æ®é›†ï¼ˆNSDï¼‰ä¸Šçš„å¤§é‡å®éªŒç»“æœå®šæ€§å’Œå®šé‡åœ°è¯æ˜äº†NeuralDiffuserçš„å…ˆè¿›æ€§ï¼Œé€šè¿‡ä¸åŸºå‡†å’Œæœ€æ–°æŠ€æœ¯æ–¹æ³•è¿›è¡Œæ¨ªå‘æ¯”è¾ƒï¼Œå¹¶è¿›è¡Œçºµå‘æ¶ˆèç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.13809v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºåŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰çš„è§†è§‰åˆºæ¿€é‡å»ºé—®é¢˜ï¼ŒæŒ‡å‡ºé‡å»ºè¿‡ç¨‹ä¸­ç»“æ„ã€èƒŒæ™¯ã€çº¹ç†ã€è‰²å½©ç­‰ç»†èŠ‚å‡†ç¡®é‡å»ºçš„æŒ‘æˆ˜ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨é‡å»ºè¿‡ç¨‹ä¸­çš„å›ºæœ‰å˜å¼‚æ€§é—®é¢˜ï¼Œæ–‡ç« ä»ç¥ç»ç§‘å­¦è§’åº¦åˆ†æäº†æ‰©æ•£æ–¹æ³•çš„ç‰¹ç‚¹ï¼Œå¹¶æå‡ºäº†NeuralDiffuseræ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å¼•å…¥ä¸»è¦è§†è§‰ç‰¹å¾å¼•å¯¼ï¼Œä»¥æ¢¯åº¦å½¢å¼æä¾›ç»†èŠ‚çº¿ç´¢ï¼Œå®ç°äº†æ‰©æ•£æ¨¡å‹çš„è‡ªä¸‹è€Œä¸Šè¿‡ç¨‹æ‰©å±•ï¼Œåœ¨é‡å»ºè§†è§‰åˆºæ¿€æ—¶æ—¢ä¿è¯äº†è¯­ä¹‰è¿è´¯æ€§åˆä¿æŒäº†ç»†èŠ‚çœŸå®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ç§æ–°çš„é‡å»ºä»»åŠ¡å¼•å¯¼ç­–ç•¥ï¼Œç¡®ä¿é‡å¤è¾“å‡ºçš„ä¸€è‡´æ€§ï¼Œä¸åŸå§‹å›¾åƒç›¸æ¯”ï¼Œé¿å…äº†è¾“å‡ºå¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰åˆºæ¿€ä»åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰çš„é‡å»ºå¯¹äºç²¾ç»†çš„å¤§è„‘æ´»åŠ¨æ¢å¤å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å‡†ç¡®é‡å»ºå¤šæ ·ç»†èŠ‚ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰åˆºæ¿€é‡å»ºä¸­å­˜åœ¨ç¨³å®šæ€§å’Œè¾“å‡ºå˜å¼‚æ€§é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç¼ºä¹ç»†èŠ‚é©±åŠ¨çš„è‡ªä¸‹è€Œä¸Šæ„ŸçŸ¥ï¼Œå¯¼è‡´ä¸¢å¤±å¿ å®ç»†èŠ‚ã€‚</li>
<li>NeuralDiffuseræ–¹æ¡ˆé€šè¿‡å¼•å…¥è§†è§‰ç‰¹å¾å¼•å¯¼ï¼Œä»¥æ¢¯åº¦å½¢å¼æä¾›ç»†èŠ‚çº¿ç´¢ï¼Œå®ç°è¯­ä¹‰è¿è´¯æ€§å’Œç»†èŠ‚çœŸå®æ€§çš„å¹³è¡¡ã€‚</li>
<li>NeuralDiffuseré‡‡ç”¨æ–°çš„å¼•å¯¼ç­–ç•¥ï¼Œç¡®ä¿é‡å¤è¾“å‡ºçš„ä¸€è‡´æ€§ï¼Œé¿å…è¾“å‡ºå¤šæ ·æ€§ã€‚</li>
<li>å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuralDiffuseråœ¨æ¨ªå‘å’Œçºµå‘è¯„ä¼°ä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹ç†è§£å¤§è„‘æ´»åŠ¨å’Œæ¨åŠ¨è§†è§‰åˆºæ¿€é‡å»ºæŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.13809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5cc864b359efffa98fe4c171564e0ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b21d74e29b282a6e34e87ed4b0160e9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb440f7024d6d1358c9af9da3efe84e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c5b371ba03822c199bd415211a4294.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31c2ce6d454e057332fda6eb228f0aae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-20177e4c576a5acde5b8b7cbe855e79b.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  EditAR Unified Conditional Generation with Autoregressive Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4ef1999029e199a062374e07a8757d9d.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  FatesGS Fast and Accurate Sparse-View Surface Reconstruction using   Gaussian Splatting with Depth-Feature Consistency
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26024.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
