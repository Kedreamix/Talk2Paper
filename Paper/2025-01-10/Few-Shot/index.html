<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-10  Hidden Entity Detection from GitHub Leveraging Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fab9ca2636ad6eac80ec4eabed11f4e0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-10-更新"><a href="#2025-01-10-更新" class="headerlink" title="2025-01-10 更新"></a>2025-01-10 更新</h1><h2 id="Hidden-Entity-Detection-from-GitHub-Leveraging-Large-Language-Models"><a href="#Hidden-Entity-Detection-from-GitHub-Leveraging-Large-Language-Models" class="headerlink" title="Hidden Entity Detection from GitHub Leveraging Large Language Models"></a>Hidden Entity Detection from GitHub Leveraging Large Language Models</h2><p><strong>Authors:Lu Gan, Martin Blum, Danilo Dessi, Brigitte Mathiak, Ralf Schenkel, Stefan Dietze</strong></p>
<p>Named entity recognition is an important task when constructing knowledge bases from unstructured data sources. Whereas entity detection methods mostly rely on extensive training data, Large Language Models (LLMs) have paved the way towards approaches that rely on zero-shot learning (ZSL) or few-shot learning (FSL) by taking advantage of the capabilities LLMs acquired during pretraining. Specifically, in very specialized scenarios where large-scale training data is not available, ZSL &#x2F; FSL opens new opportunities. This paper follows this recent trend and investigates the potential of leveraging Large Language Models (LLMs) in such scenarios to automatically detect datasets and software within textual content from GitHub repositories. While existing methods focused solely on named entities, this study aims to broaden the scope by incorporating resources such as repositories and online hubs where entities are also represented by URLs. The study explores different FSL prompt learning approaches to enhance the LLMs’ ability to identify dataset and software mentions within repository texts. Through analyses of LLM effectiveness and learning strategies, this paper offers insights into the potential of advanced language models for automated entity detection. </p>
<blockquote>
<p>从非结构化数据源构建知识库时，命名实体识别是一项重要任务。实体检测方法大多依赖于大量的训练数据，而大型语言模型（LLM）的出现为依赖于零样本学习（ZSL）或少样本学习（FSL）的方法铺平了道路。它们通过利用大型语言模型在预训练期间获得的能力来实现这一点。特别地，在无法使用大规模训练数据的高度专业化场景中，ZSL&#x2F;FSL带来了新的机会。本文遵循这一最新趋势，探讨了在这种场景中利用大型语言模型（LLM）自动检测GitHub存储库中的数据集和软件资源的潜力。现有的方法只专注于命名实体，而本研究旨在通过整合存储库和在线中心等资源（其中实体也以URL形式表示）来扩大范围。该研究探索了不同的FSL提示学习方法来提高LLM在存储库文本中识别数据集和软件提及的能力。通过对LLM的有效性和学习策略的分析，本文提供了对高级语言模型在自动实体检测方面的潜力的深刻见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04455v1">PDF</a> accepted by KDD2024 workshop DL4KG</p>
<p><strong>Summary</strong>：</p>
<p>大规模语言模型（LLM）可在缺乏大规模训练数据的情况下，利用零样本学习（ZSL）或少样本学习（FSL）技术在构建知识库时自动检测GitHub存储库中的数据集和软件。本文主要探索使用LLM来自动检测文本内容中的数据集和软件，不仅关注命名实体识别，还融入资源如存储库和在线中心，其中实体通过URL表示。通过探索不同的FSL提示学习方法，增强LLM在存储库文本中识别数据集和软件的能力。通过对LLM的效果和学习策略的分析，为自动化实体检测提供深入见解。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>大型语言模型（LLM）能够在缺乏大规模训练数据的情况下，利用零样本学习（ZSL）或少样本学习（FSL）技术识别GitHub存储库中的实体。</li>
<li>现有方法主要关注命名实体识别，而本文研究扩大了范围，纳入如存储库和在线中心等资源的考虑。</li>
<li>研究探索了不同的FSL提示学习方法来增强LLM在存储库文本中识别数据集和软件的能力。</li>
<li>该研究通过对LLM的有效性和学习策略的分析，为自动化实体检测提供了见解。</li>
<li>借助LLM的技术可以在一定程度上简化并改进数据集合和软件包的检测与识别。</li>
<li>URL的引入增强了实体识别的准确性，因为许多数据集和软件的引用都包含URL链接。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04455">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6a41a168e56612e8792b77d690cdb1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c89543b75437b3d3f9c82ddbb0b5b10b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f18f7fff3c7cd82076faed0eb309aa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DispFormer-Pretrained-Transformer-for-Flexible-Dispersion-Curve-Inversion-from-Global-Synthesis-to-Regional-Applications"><a href="#DispFormer-Pretrained-Transformer-for-Flexible-Dispersion-Curve-Inversion-from-Global-Synthesis-to-Regional-Applications" class="headerlink" title="DispFormer: Pretrained Transformer for Flexible Dispersion Curve   Inversion from Global Synthesis to Regional Applications"></a>DispFormer: Pretrained Transformer for Flexible Dispersion Curve   Inversion from Global Synthesis to Regional Applications</h2><p><strong>Authors:Feng Liu, Bao Deng, Rui Su, Lei Bai, Wanli Ouyang</strong></p>
<p>Surface wave dispersion curve inversion is essential for estimating subsurface Shear-wave velocity ($v_s$), yet traditional methods often struggle to balance computational efficiency with inversion accuracy. While deep learning approaches show promise, previous studies typically require large amounts of labeled data and struggle with real-world datasets that have varying period ranges, missing data, and low signal-to-noise ratios. This study proposes DispFormer, a transformer-based neural network for inverting the $v_s$ profile from Rayleigh-wave phase and group dispersion curves. DispFormer processes dispersion data at each period independently, thereby allowing it to handle data of varying lengths without requiring network modifications or alignment between training and testing data. The performance is demonstrated by pre-training it on a global synthetic dataset and testing it on two regional synthetic datasets using zero-shot and few-shot strategies. Results indicate that zero-shot DispFormer, even without any labeled data, produces inversion profiles that match well with the ground truth, providing a deployable initial model generator to assist traditional methods. When labeled data is available, few-shot DispFormer outperforms traditional methods with only a small number of labels. Furthermore, real-world tests indicate that DispFormer effectively handles varying length data, and yields lower data residuals than reference models. These findings demonstrate that DispFormer provides a robust foundation model for dispersion curve inversion and is a promising approach for broader applications. </p>
<blockquote>
<p>表面波弥散曲线反演对于估计地下剪切波速度（v_s）至关重要，但传统方法往往在计算效率和反演精度之间难以取得平衡。虽然深度学习方法显示出潜力，但之前的研究通常需要大量标记数据，并且在处理具有不同周期范围、缺失数据和低信噪比的真实世界数据集时面临困难。本研究提出了DispFormer，这是一个基于转换器的神经网络，用于从瑞利波相位和群弥散曲线反演v_s剖面。DispFormer独立处理每个周期的弥散数据，因此能够处理不同长度的数据，而无需对网络和训练和测试数据进行修改或对齐。通过在全球合成数据集上进行预训练，并在两个区域合成数据集上使用零样本和少样本策略进行测试，验证了其性能。结果表明，零样本DispFormer即使没有标记数据也能产生与真实情况匹配良好的反演剖面，为传统方法提供了一个可部署的初始模型生成器。当有标记数据时，少样本DispFormer仅需少量标签就能超越传统方法。此外，真实世界测试表明，DispFormer有效处理不同长度的数据，并且产生的数据残差低于参考模型。这些发现表明，DispFormer为弥散曲线反演提供了稳健的基础模型，是更广泛应用的有前途的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04366v1">PDF</a> 11 pages, 11 figures, related codes and data are available at   <a target="_blank" rel="noopener" href="https://github.com/liufeng2317/DispFormer">https://github.com/liufeng2317/DispFormer</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于变压器的神经网络DispFormer，用于从Rayleigh波相位和群速度色散曲线反演地下剪切波速度（$v_s$）分布。DispFormer能够处理不同长度的数据，且无需修改网络结构或训练与测试数据对齐。通过全球合成数据集进行预训练，并在两个区域合成数据集上采用零样本和少样本策略进行测试，结果显示零样本DispFormer即使没有标记数据也能生成与真实情况匹配的反演剖面，为传统方法提供了可部署的初始模型生成器。当使用少量标记数据时，少样本DispFormer表现优于传统方法。此外，实际测试表明，DispFormer能有效处理不同长度的数据，并产生较低的数据残差。总之，DispFormer为色散曲线反演提供了稳健的基础模型，在更广领域具有广阔的应用前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DispFormer是一种基于神经网络的变压器架构，用于反演地下剪切波速度（$v_s$）。</li>
<li>DispFormer能够处理不同长度的数据，无需调整网络结构或数据对齐。</li>
<li>通过全球合成数据集预训练的DispFormer，在零样本状态下表现出良好的反演性能。</li>
<li>当使用少量标记数据时，少样本DispFormer优于传统方法。</li>
<li>DispFormer在实际测试中表现出色，能有效处理不同长度的数据并产生较低的数据残差。</li>
<li>DispFormer为色散曲线反演提供了稳健的基础模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04366">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81d112b711c101e6023573b9369ab351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1330185bd5a9bb3da2f82d6a3fe9933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-408147874b81da5e369f1dbf4545e743.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b598e0bea196a2ab18a7d1fba6e9d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-521e3099d1d6715728b0393d0124c0fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4054ffd55fdd710bf9abf18189de8dff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0833568912e078a34d4634f3236334fc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Online-Gaussian-Test-Time-Adaptation-of-Vision-Language-Models"><a href="#Online-Gaussian-Test-Time-Adaptation-of-Vision-Language-Models" class="headerlink" title="Online Gaussian Test-Time Adaptation of Vision-Language Models"></a>Online Gaussian Test-Time Adaptation of Vision-Language Models</h2><p><strong>Authors:Clément Fuchs, Maxime Zanella, Christophe De Vleeschouwer</strong></p>
<p>Online test-time adaptation (OTTA) of vision-language models (VLMs) has recently garnered increased attention to take advantage of data observed along a stream to improve future predictions. Unfortunately, existing methods rely on dataset-specific hyperparameters, significantly limiting their adaptability to unseen tasks. In response, we propose Online Gaussian Adaptation (OGA), a novel method that models the likelihoods of visual features using Gaussian distributions and incorporates zero-shot priors into an interpretable Maximum A Posteriori (MAP) estimation framework with fixed hyper-parameters across all datasets. We demonstrate that OGA outperforms state-of-the-art methods on most datasets and runs. Additionally, we show that combining OTTA with popular few-shot techniques (a practical yet overlooked setting in prior research) is highly beneficial. Furthermore, our experimental study reveals that common OTTA evaluation protocols, which average performance over at most three runs per dataset, are inadequate due to the substantial variability observed across runs for all OTTA methods. Therefore, we advocate for more rigorous evaluation practices, including increasing the number of runs and considering additional quantitative metrics, such as our proposed Expected Tail Accuracy (ETA), calculated as the average accuracy in the worst 10% of runs. We hope these contributions will encourage more rigorous and diverse evaluation practices in the OTTA community. Code is available at <a target="_blank" rel="noopener" href="https://github.com/cfuchs2023/OGA">https://github.com/cfuchs2023/OGA</a> . </p>
<blockquote>
<p>在线测试时间适应（OTTA）视觉语言模型（VLMs）最近引起了人们的广泛关注，利用观察到的数据流来改善未来的预测。然而，现有方法依赖于特定数据集的超参数，极大地限制了它们对未见任务的适应能力。作为回应，我们提出了在线高斯适应（OGA），这是一种新型方法，利用高斯分布对视觉特征的概率进行建模，并将零样本先验融入可解释的最大后验（MAP）估计框架中，该框架在所有数据集上都具有固定的超参数。我们证明OGA在大多数数据集和运行中的表现都优于最新技术。此外，我们还展示了将OTTA与流行的少量镜头技术相结合（这在先前的研究中是一个实用但被忽视的设置）是非常有益的。此外，我们的实验研究表明，常见的OTTA评估协议（在每个数据集上平均运行最多三次的性能）是不充分的，因为所有OTTA方法在运行之间的观察值都有很大差异。因此，我们提倡更严格的评估实践，包括增加运行次数并考虑额外的定量指标，如我们提出的预期尾部精度（ETA），计算为在最差的10%运行中平均精度的值。我们希望这些贡献将鼓励OTTA社区采用更严格和多样化的评估实践。代码可在<a target="_blank" rel="noopener" href="https://github.com/cfuchs2023/OGA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cfuchs2023/OGA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04352v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了在线高斯适应（OGA）方法，该方法利用高斯分布对视觉特征进行建模，并将零样本先验纳入可解释的最大后验估计框架中。此方法在所有数据集上都使用固定的超参数，提高了对不同任务的适应性。实验表明，OGA在大多数数据集和运行中的表现优于现有方法。此外，结合在线测试时间适应（OTTA）和流行的少样本技术，效果更佳。作者对现有的OTTA评估协议提出质疑，认为现有的评估协议不足够全面和可靠，建议采用更严格的评估方法和更多定量指标来评价模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OGA方法利用高斯分布建模视觉特征，提高了模型的适应性和预测性能。</li>
<li>OGA在所有数据集上使用固定超参数，增强了其泛化能力。</li>
<li>OGA在大多数数据集上的表现优于现有方法。</li>
<li>结合OTTA和少样本技术能进一步提升模型性能。</li>
<li>现有的OTTA评估协议存在不足，需要更严格的评估方法和更多定量指标来全面评价模型性能。</li>
<li>作者提出了一种新的评估指标——预期尾部准确率（ETA），用于更准确地衡量模型在不同运行中的性能稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a49980565a258988916ade760b1c55aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fab9ca2636ad6eac80ec4eabed11f4e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79ebc13543f85ba004230bbe041d0ca8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives"><a href="#More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives" class="headerlink" title="More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives"></a>More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives</h2><p><strong>Authors:Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan</strong></p>
<p>Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as the number of ICL demonstrations increases from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce DR-ICL, a novel optimization method that enhances model performance through Differentiated Learning and advantage-based Reweighting objectives. Globally, DR-ICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby improving generalization. This approach allows the model to handle varying numbers of shots effectively, mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the Many-Shot ICL Benchmark (MICLB)-a large-scale benchmark covering shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes. MICLB facilitates the evaluation of many-shot ICL strategies across seven prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate that LLMs enhanced with DR-ICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and benchmark dataset hoping to facilitate further research in many-shot ICL. </p>
<blockquote>
<p>大型语言模型（LLM）在不需更新参数的情况下，擅长进行少量上下文学习（ICL）。然而，随着ICL演示的数量从几个增加到许多，性能往往达到峰值并最终下降。我们确定了这一趋势的两个主要原因：次优的负对数似然（NLL）优化目标和增量数据噪声。为了解决这些问题，我们引入了DR-ICL这一新型优化方法，它通过差异化学习和基于优势的重加权目标来提高模型性能。从全局角度看，DR-ICL通过优化NLL目标进行差异化学习，确保多镜头性能超越零镜头水平。从局部角度看，它利用强化学习得到的累积优势动态调整多镜头演示的权重，从而提高泛化能力。这种方法使模型能够有效地处理不同数量的镜头，减轻噪声数据的影响。由于缺少具有多种多镜头分布的多任务数据集，我们开发了多镜头ICL基准测试（MICLB）——一个大规模基准测试，涵盖从1到350的射击次数，序列中的令牌数高达8000个——用于微调目的。MICLB便于在七个主要NLP任务和五十个不同数据集上评估多镜头ICL策略的效果。实验结果表明，采用DR-ICL增强的大型语言模型在各种任务的多镜头设置中取得了显著改进，包括域内和域外场景。我们发布代码和基准数据集，希望能进一步推动多镜头ICL的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04070v1">PDF</a> 13 pages, 8 figures, 11 tables</p>
<p><strong>摘要</strong></p>
<p>大语言模型在不需要参数更新的少量上下文学习场景中表现出色，但随着上下文学习示例从几个增加到多个，性能往往会达到峰值并最终下降。研究团队针对这一问题提出了DR-ICL这一新型优化方法，通过差异化学习和基于优势的加权优化目标来提升模型性能。该方法通过差异化学习优化负对数似然目标，确保多示例性能超越零示例水平。同时，它还会根据强化学习的累积优势动态调整多示例演示的权重，从而提高模型的泛化能力。这种方法使模型能够更有效地处理不同数量的示例，减轻噪声数据的影响。此外，为了解决多任务数据集缺乏多样多变的多示例分布问题，研究团队开发了一个大规模基准测试平台——多示例上下文学习基准（MICLB），涵盖了从1到350个示例，序列长度高达8000个标记的多种场景，用于精细调整目的。MICLB能够在七个主要NLP任务和五十个不同数据集上评估多示例上下文学习策略。实验结果表明，采用DR-ICL增强的大语言模型在各种任务的多示例设置中都取得了显著改进，包括域内和域外场景。我们公开了代码和基准数据集，希望进一步推动多示例上下文学习领域的研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大语言模型在不需要参数更新的少量上下文学习中表现出色，但随着示例数量增加，性能会下降。</li>
<li>性能下降的主要原因包括次优的负对数似然优化目标和增加的数据噪声。</li>
<li>DR-ICL是一种新型优化方法，通过差异化学习和基于优势的加权来优化模型性能。</li>
<li>DR-ICL能够在全球范围内优化负对数似然目标，并在局部通过借鉴强化学习的累积优势动态调整多示例演示的权重。</li>
<li>DR-ICL允许模型有效处理不同数量的示例，并减轻噪声数据的影响。</li>
<li>为了促进多示例上下文学习研究，研究团队开发了一个大规模基准测试平台——多示例上下文学习基准（MICLB），涵盖多种NLP任务和数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-36b0009946a6ceda17564a2d63590a78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02a20821473a9e1da132110f06e999ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-121b4f1da0318451f08f703376339193.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebb477671929c42bd2861cd8a4b49ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-166d78d39b98ac5c931d706a5390b2db.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Soft-Sensor-Method-with-Uncertainty-Awareness-and-Self-Explanation-Based-on-Large-Language-Models-Enhanced-by-Domain-Knowledge-Retrieval"><a href="#A-Soft-Sensor-Method-with-Uncertainty-Awareness-and-Self-Explanation-Based-on-Large-Language-Models-Enhanced-by-Domain-Knowledge-Retrieval" class="headerlink" title="A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation   Based on Large Language Models Enhanced by Domain Knowledge Retrieval"></a>A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation   Based on Large Language Models Enhanced by Domain Knowledge Retrieval</h2><p><strong>Authors:Shuo Tong, Han Liu, Runyuan Guo, Wenqing Wang, Xueqiong Tian, Lingyun Wei, Lin Zhang, Huayong Wu, Ding Liu, Youmin Zhang</strong></p>
<p>Data-driven soft sensors are crucial in predicting key performance indicators in industrial systems. However, current methods predominantly rely on the supervised learning paradigms of parameter updating, which inherently faces challenges such as high development costs, poor robustness, training instability, and lack of interpretability. Recently, large language models (LLMs) have demonstrated significant potential across various domains, notably through In-Context Learning (ICL), which enables high-performance task execution with minimal input-label demonstrations and no prior training. This paper aims to replace supervised learning with the emerging ICL paradigm for soft sensor modeling to address existing challenges and explore new avenues for advancement. To achieve this, we propose a novel framework called the Few-shot Uncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes the Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware Few-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial Knowledge Vector Storage to enhance LLMs’ domain-specific knowledge, enabling zero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based context demonstrations of structured data to prompt LLMs to execute ICL for predicting and propose a context sample retrieval augmentation strategy to improve performance. Additionally, we explored LLMs’ AIGC and probabilistic characteristics to propose self-explanation and uncertainty quantification methods for constructing a trustworthy soft sensor. Extensive experiments demonstrate that our method achieved state-of-the-art predictive performance, strong robustness, and flexibility, effectively mitigates training instability found in traditional methods. To the best of our knowledge, this is the first work to establish soft sensor utilizing LLMs. </p>
<blockquote>
<p>数据驱动软传感器在工业系统中预测关键性能指标方面发挥着重要作用。然而，当前的方法主要依赖于参数更新的监督学习模式，这固有地面临着高开发成本、鲁棒性差、训练不稳定和缺乏可解释性等挑战。最近，大型语言模型（LLM）在各个领域表现出了巨大的潜力，尤其是通过上下文学习（ICL），它能够在少量输入标签演示的情况下实现高性能的任务执行，无需预先训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03295v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>数据驱动型软传感器对预测工业系统关键性能指标至关重要。然而，当前方法主要依赖参数更新的监督学习模式，这固有地面临高开发成本、鲁棒性差、训练不稳定和缺乏可解释性等挑战。近期，大型语言模型（LLM）在各个领域展现出巨大潜力，尤其是在无需事先训练的情境学习（ICL）方面。本文旨在将新兴的ICL范式应用于软传感器建模，以应对现有挑战并探索新的进步途径。为此，我们提出了一种名为Few-shot Uncertainty-aware and self-Explaining Soft Sensor（LLM-FUESS）的新框架，包括Zero-shot Auxiliary Variable Selector（LLM-ZAVS）和Uncertainty-aware Few-shot Soft Sensor（LLM-UFSS）。LLM-ZAVS从工业知识向量存储中检索以增强LLM的领域特定知识，实现零辅助变量选择。在LLM-UFSS中，我们利用结构化数据的文本上下文演示来提示LLM执行ICL进行预测，并提出一种上下文样本检索增强策略来提高性能。此外，我们还探索了LLM的自解释和不确定性量化方法，以构建可信赖的软传感器。大量实验表明，我们的方法达到了先进的预测性能、强大的鲁棒性和灵活性，有效地解决了传统方法中的训练不稳定问题。据我们所知，这是首次利用LLM建立软传感器的工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据驱动型软传感器在预测工业系统关键性能指标方面扮演重要角色。</li>
<li>当前软传感器方法主要基于监督学习，存在高成本、缺乏鲁棒性、训练不稳定和缺乏可解释性问题。</li>
<li>大型语言模型（LLM）和情境学习（ICL）在软传感器建模中具有巨大潜力。</li>
<li>论文提出了Few-shot Uncertainty-aware and self-Explaining Soft Sensor（LLM-FUESS）框架，结合LLM-ZAVS和LLM-UFSS，以改进软传感器性能。</li>
<li>LLM-ZAVS利用工业知识向量存储增强LLM的领域特定知识，实现零辅助变量选择。</li>
<li>LLM-UFSS利用文本上下文演示和上下文样本检索增强策略，提高预测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1515b84ef3499546e99e8bfc563a0f9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5292ad4f5db3e3c5216dbb908b21d3c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0ad17a519770ec26387bd518940eb60.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CaT-BENCH-Benchmarking-Language-Model-Understanding-of-Causal-and-Temporal-Dependencies-in-Plans"><a href="#CaT-BENCH-Benchmarking-Language-Model-Understanding-of-Causal-and-Temporal-Dependencies-in-Plans" class="headerlink" title="CaT-BENCH: Benchmarking Language Model Understanding of Causal and   Temporal Dependencies in Plans"></a>CaT-BENCH: Benchmarking Language Model Understanding of Causal and   Temporal Dependencies in Plans</h2><p><strong>Authors:Yash Kumar Lal, Vanya Cohen, Nathanael Chambers, Niranjan Balasubramanian, Raymond Mooney</strong></p>
<p>Understanding the abilities of LLMs to reason about natural language plans, such as instructional text and recipes, is critical to reliably using them in decision-making systems. A fundamental aspect of plans is the temporal order in which their steps needs to be executed, which reflects the underlying causal dependencies between them. We introduce CaT-Bench, a benchmark of Step Order Prediction questions, which test whether a step must necessarily occur before or after another in cooking recipe plans. We use this to evaluate how well frontier LLMs understand causal and temporal dependencies. We find that SOTA LLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased towards predicting dependence more often, perhaps relying on temporal order of steps as a heuristic. While prompting for explanations and using few-shot examples improve performance, the best F1 result is only 0.73. Further, human evaluation of explanations along with answer correctness show that, on average, humans do not agree with model reasoning. Surprisingly, we also find that explaining after answering leads to better performance than normal chain-of-thought prompting, and LLM answers are not consistent across questions about the same step pairs. Overall, results show that LLMs’ ability to detect dependence between steps has significant room for improvement. </p>
<blockquote>
<p>理解大型语言模型（LLM）在决策系统推理中针对自然语言计划（如说明性文本和食谱）的能力对于可靠地使用它们至关重要。计划的一个基本方面是按照其步骤需要执行的时序顺序，这反映了它们之间的潜在因果依赖关系。我们引入了CaT-Bench，一个步骤顺序预测问题的基准测试，测试烹饪食谱计划中某个步骤是否必须在另一个步骤之前或之后发生。我们使用它来评估前沿LLM对因果和时间依赖关系的理解程度。我们发现，最先进的LLM表现令人失望（最佳零样本的F1分数仅为0.59），并且偏向于更频繁地预测依赖性，或许依赖步骤的时间顺序作为启发式方法。虽然提示解释和使用少量示例可以提高性能，但最佳F1结果仅为0.73。此外，通过对解释和答案正确性的人类评估显示，平均而言，人类并不认同模型的推理。令人惊讶的是，我们还发现，在回答问题后解释比正常思维链提示导致更好的性能，并且LLM对同一步骤对的答案并不一致。总体而言，结果表明LLM检测步骤之间依赖性的能力仍有很大提升空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.15823v3">PDF</a> Accepted to EMNLP 2024 Main Conference</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）理解和处理自然语言计划（如指令文本和食谱）的能力对于在决策系统中可靠使用它们至关重要。计划的一个基本方面是步骤执行的时序顺序，这反映了它们之间的潜在因果依赖关系。本文介绍了CaT-Bench，一个步骤顺序预测问题的基准测试，用于测试LLMs在烹饪食谱计划中理解因果和时间依赖关系的能力。研究发现，现有最前沿的LLMs表现令人失望（最佳零样本F1分数仅为0.59），且倾向于过度预测依赖性，可能依赖步骤的时序顺序作为启发式。虽然提示解释和使用少量样本可以提高性能，但最佳F1结果仅为0.73。此外，对解释和答案正确性的人类评估显示，LLMs的推理平均而言并不符合人类的理解。令人惊讶的是，我们还发现先答题再解释的模式比传统的思考链提示更有效，并且对于同一步骤对的各种问题，LLM的答案并不一致。总体而言，LLMs在检测步骤间依赖关系的能力方面仍有很大的提升空间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在处理自然语言计划方面的能力对于在决策系统中的使用至关重要。</li>
<li>CaT-Bench基准测试用于评估LLMs在理解烹饪食谱计划中的因果和时间依赖关系。</li>
<li>当前LLMs在步骤顺序预测方面的表现不佳，最佳F1分数仅为0.73。</li>
<li>LLMs倾向于过度预测步骤之间的依赖性，可能依赖时序启发式。</li>
<li>提示解释和使用少量样本可以提高LLMs的性能，但仍存在改进空间。</li>
<li>人类评估显示LLMs的推理并不符合人类的理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.15823">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f03311eb9726ef6474c0bd5aa5059da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35e91675950f334a2021ff36da4e96ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e6044741ccbf4f5ea1a9cb8e5fa09a0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Rho-1-Not-All-Tokens-Are-What-You-Need"><a href="#Rho-1-Not-All-Tokens-Are-What-You-Need" class="headerlink" title="Rho-1: Not All Tokens Are What You Need"></a>Rho-1: Not All Tokens Are What You Need</h2><p><strong>Authors:Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen</strong></p>
<p>Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that “9l training”. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training. </p>
<blockquote>
<p>之前的语言模型预训练方法都是将所有训练令牌统一应用下一个令牌预测损失。我们挑战这一常规，提出“9l训练”方法。我们的初步分析研究了语言模型的令牌级训练动态，揭示了不同令牌的不同损失模式。基于这些见解，我们引入了一种名为Rho-1的新语言模型。不同于传统LMs学习预测语料库中的每个下一个令牌，Rho-1采用选择性语言建模（Selective Language Modeling，SLM），只针对与所需分布对齐的有用令牌进行选择性训练。这种方法涉及使用参考模型为预训练令牌打分，然后针对得分较高的令牌使用有针对性的损失来训练语言模型。在连续预训练15B OpenWebMath语料库时，Rho-1在9个数学任务中的小样本精度提高了高达30%。经过微调后，Rho-1-1B和7B在MATH数据集上达到了最新的最好成绩，分别为40.6%和51.8%，分别匹配了DeepSeekMath只有其预训练令牌的3%。此外，在对80B通用令牌进行连续预训练时，Rho-1在15个不同任务上平均提高了6.8%的性能，提高了语言模型预训练的效率与性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.07965v4">PDF</a> First two authors equal contribution</p>
<p><strong>Summary</strong></p>
<p>该文挑战了传统语言模型预训练的方式，提出一种名为Rho-1的新型语言模型。该模型采用选择性语言建模（SLM）方法，只针对有用且符合期望分布的令牌进行训练。通过评分预训练令牌并使用有针对性的损失函数对高分令牌进行训练，在持续预训练时提高了模型的效率和性能。在多个任务中，Rho-1的预训练令牌数量相较于其他模型大大减少，但仍取得了显著的准确性提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>传统语言模型预训练采用对所有令牌应用下一个令牌预测损失的方法，而本文提出挑战并引入选择性语言建模（SLM）。</li>
<li>Rho-1模型使用参考模型对预训练令牌进行评分，只针对高分令牌进行有针对性的损失训练。</li>
<li>Rho-1模型在持续预训练时，相较于其他模型使用更少的预训练令牌数量。</li>
<li>Rho-1在多个数学任务中实现了高达30%的少样本准确性提升。</li>
<li>Rho-1在MATH数据集上的表现达到或超越了现有技术，即使只使用少量预训练令牌。</li>
<li>Rho-1在多种任务上平均提高了模型的效率和性能，平均提升幅度达到6.8%。</li>
<li>Rho-1模型的引入为语言模型的预训练提供了新的视角和方法论基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.07965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-905554d8f282d5b54c986cb69db390c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62cbd37a55128d00a6c0c4d4d6b3db36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e67d84554c46160b55ad3faaf302fd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8715239cda5adcb6370b69c2d9b3e8a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6251b7c2d0a642fd2c586c2ba121eb29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2952de91398a6a427d8c70821e213c0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FILP-3D-Enhancing-3D-Few-shot-Class-incremental-Learning-with-Pre-trained-Vision-Language-Models"><a href="#FILP-3D-Enhancing-3D-Few-shot-Class-incremental-Learning-with-Pre-trained-Vision-Language-Models" class="headerlink" title="FILP-3D: Enhancing 3D Few-shot Class-incremental Learning with   Pre-trained Vision-Language Models"></a>FILP-3D: Enhancing 3D Few-shot Class-incremental Learning with   Pre-trained Vision-Language Models</h2><p><strong>Authors:Wan Xu, Tianyu Huang, Tianyu Qu, Guanglei Yang, Yiwen Guo, Wangmeng Zuo</strong></p>
<p>Few-shot class-incremental learning (FSCIL) aims to mitigate the catastrophic forgetting issue when a model is incrementally trained on limited data. However, many of these works lack effective exploration of prior knowledge, rendering them unable to effectively address the domain gap issue in the context of 3D FSCIL, thereby leading to catastrophic forgetting. The Contrastive Vision-Language Pre-Training (CLIP) model serves as a highly suitable backbone for addressing the challenges of 3D FSCIL due to its abundant shape-related prior knowledge. Unfortunately, its direct application to 3D FSCIL still faces the incompatibility between 3D data representation and the 2D features, primarily manifested as feature space misalignment and significant noise. To address the above challenges, we introduce the FILP-3D framework with two novel components: the Redundant Feature Eliminator (RFE) for feature space misalignment and the Spatial Noise Compensator (SNC) for significant noise. RFE aligns the feature spaces of input point clouds and their embeddings by performing a unique dimensionality reduction on the feature space of pre-trained models (PTMs), effectively eliminating redundant information without compromising semantic integrity. On the other hand, SNC is a graph-based 3D model designed to capture robust geometric information within point clouds, thereby augmenting the knowledge lost due to projection, particularly when processing real-world scanned data. Moreover, traditional accuracy metrics are proven to be biased due to the imbalance in existing 3D datasets. Therefore we propose 3D FSCIL benchmark FSCIL3D-XL and novel evaluation metrics that offer a more nuanced assessment of a 3D FSCIL model. Experimental results on both established and our proposed benchmarks demonstrate that our approach significantly outperforms existing state-of-the-art methods. </p>
<blockquote>
<p>少量类别增量学习（FSCIL）旨在解决模型在有限数据上逐步训练时出现的灾难性遗忘问题。然而，这些工作中的许多缺乏对先验知识的有效探索，导致它们无法有效解决3D FSCIL中的领域差距问题，从而导致灾难性遗忘。对比视觉语言预训练（CLIP）模型由于其丰富的形状相关先验知识，成为应对3D FSCIL挑战的理想骨干网。然而，其直接应用于3D FSCIL仍然面临3D数据表示与2D特征之间的不兼容问题，主要表现为特征空间不对齐和显著噪声。为了解决上述挑战，我们引入了FILP-3D框架，其中包括两个新组件：用于特征空间不对齐的冗余特征消除器（RFE）和用于显著噪声的空间噪声补偿器（SNC）。RFE通过对预训练模型（PTM）的特征空间进行独特的降维操作，对齐输入点云及其嵌入的特征空间，有效地消除了冗余信息，同时不损害语义完整性。另一方面，SNC是一个基于图的3D模型，旨在捕获点云中的稳健几何信息，从而弥补因投影而丢失的知识，特别是在处理现实世界扫描数据时。此外，由于现有3D数据集的不平衡，传统准确性指标被证明是有偏见的。因此，我们提出了3D FSCIL基准测试FSCIL3D-XL和新的评估指标，为3D FSCIL模型提供更微妙的评估。在既定基准测试和我们提出的基准测试上的实验结果都表明，我们的方法显著优于现有最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.17051v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对few-shot类增量学习（FSCIL）中的灾难性遗忘问题，文章提出了使用对比视觉语言预训练（CLIP）模型的FILP-3D框架。该框架引入了两个新组件：用于特征空间不对齐的冗余特征消除器（RFE）和用于显著噪声的空间噪声补偿器（SNC）。同时，为解决传统准确率度量在3D数据集上的偏见问题，文章提出了FSCIL3D-XL基准测试和新型评估指标。实验结果表明，该方法在现有和传统基准测试上的表现均显著优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot类增量学习（FSCIL）面临灾难性遗忘问题，需要探索先前知识来解决。</li>
<li>对比视觉语言预训练（CLIP）模型因其丰富的形状相关先验知识，适合解决3D FSCIL的挑战。</li>
<li>FILP-3D框架通过引入冗余特征消除器（RFE）和空间噪声补偿器（SNC）来解决3D数据表示与2D特征之间的不兼容问题。</li>
<li>RFE通过对预训练模型的特征空间进行独特的降维操作，消除了冗余信息，同时保持语义完整性。</li>
<li>SNC是一个基于图的3D模型，旨在捕捉点云中的稳健几何信息，从而弥补因投影而丢失的知识。</li>
<li>传统准确率度量在3D数据集上存在偏见，因此提出了FSCIL3D-XL基准测试和新型评估指标，以提供更细致的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.17051">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bbc141e2deb1da69f532de88383d6f9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afc19195904a063af8bd12e5a52b86e4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-20bae26bab7d77089dc1955b48b4174f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-01-10  MultiMax Sparse and Multi-Modal Attention Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6f397f7d2e8cb3133ad298a905f61c9b.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-01-10  MultiMax Sparse and Multi-Modal Attention Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">13597.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
