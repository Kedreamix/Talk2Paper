<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  FleSpeech Flexibly Controllable Speech Generation with Various Prompts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d06a3ceadc3bc6f9e5ad1a1e2b2c5b77.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-10-æ›´æ–°"><a href="#2025-01-10-æ›´æ–°" class="headerlink" title="2025-01-10 æ›´æ–°"></a>2025-01-10 æ›´æ–°</h1><h2 id="FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts"><a href="#FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts" class="headerlink" title="FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"></a>FleSpeech: Flexibly Controllable Speech Generation with Various Prompts</h2><p><strong>Authors:Hanzhao Li, Yuke Li, Xinsheng Wang, Jingbin Hu, Qicong Xie, Shan Yang, Lei Xie</strong></p>
<p>Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speakerâ€™s timbre, or choosing a style and generating a voice that matches a characterâ€™s visual appearance. To overcome these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at <a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech/">https://kkksuper.github.io/FleSpeech/</a> </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å¯æ§è¯­éŸ³ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€æˆ–å›ºå®šçš„æç¤ºï¼Œè¿™é™åˆ¶äº†åˆ›é€ æ€§å’Œçµæ´»æ€§ã€‚è¿™äº›å±€é™ä½¿å¾—åœ¨æŸäº›åœºæ™¯ä¸‹éš¾ä»¥æ»¡è¶³ç‰¹å®šç”¨æˆ·çš„éœ€æ±‚ï¼Œä¾‹å¦‚åœ¨ä¿ç•™é€‰å®šæ¼”è®²è€…éŸ³è´¨çš„åŒæ—¶è°ƒæ•´é£æ ¼ï¼Œæˆ–è€…é€‰æ‹©é£æ ¼å¹¶ç”Ÿæˆä¸è§’è‰²è§†è§‰å¤–è§‚ç›¸åŒ¹é…çš„è¯­éŸ³ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FleSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šé˜¶æ®µè¯­éŸ³ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå„ç§æ§åˆ¶å½¢å¼ï¼Œå…è®¸æ›´çµæ´»åœ°æ“ä½œè¯­éŸ³å±æ€§ã€‚FleSpeeché‡‡ç”¨å¤šæ¨¡æ€æç¤ºç¼–ç å™¨ï¼Œå¤„ç†å’Œç»Ÿä¸€ä¸åŒçš„æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æç¤ºï¼Œå½¢æˆè¿è´¯çš„è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¢å¼ºäº†è¯­éŸ³åˆæˆçš„é€‚åº”æ€§ï¼Œæ”¯æŒå¯¹ç”Ÿæˆçš„è¯­éŸ³è¿›è¡Œåˆ›é€ æ€§å’Œç²¾ç¡®çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºå¼€å‘å¤šæ¨¡æ€æ•°æ®é›†çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚ç»¼åˆçš„ä¸»è§‚å’Œå®¢è§‚å®éªŒè¯æ˜äº†FleSpeechçš„æœ‰æ•ˆæ€§ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech/%E6%89%BE%E5%88%B0%E3%80%82">https://kkksuper.github.io/FleSpeech/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04644v1">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µè¯­éŸ³ç”Ÿæˆæ¡†æ¶â€”â€”FleSpeechï¼Œè¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿå¯æ§è¯­éŸ³ç”Ÿæˆæ–¹æ³•ä¸­çš„é™åˆ¶ï¼Œå¦‚å•ä¸€æˆ–å›ºå®šçš„æç¤ºæ–¹å¼ã€‚FleSpeeché€šè¿‡æ•´åˆå¤šç§å½¢å¼çš„æ§åˆ¶ï¼Œå®ç°äº†è¯­éŸ³å±æ€§çš„çµæ´»æ“æ§ï¼Œèƒ½å¤Ÿå¤„ç†å¹¶ç»Ÿä¸€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æç¤ºï¼Œä»è€Œå¢å¼ºè¯­éŸ³åˆæˆçš„é€‚åº”æ€§å¹¶æ”¯æŒå¯¹ç”Ÿæˆè¯­éŸ³çš„åˆ›é€ æ€§ä¸ç²¾ç¡®æ§åˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†FleSpeechçš„å¤šæ¨¡æ€æ•°æ®é›†æ”¶é›†æµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FleSpeechè§£å†³äº†ä¼ ç»Ÿå¯æ§è¯­éŸ³ç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚å•ä¸€æˆ–å›ºå®šçš„æç¤ºæ–¹å¼ã€‚</li>
<li>FleSpeeché‡‡ç”¨å¤šé˜¶æ®µæ¡†æ¶ï¼Œå®ç°æ›´çµæ´»çš„è¯­éŸ³å±æ€§æ“æ§ã€‚</li>
<li>FleSpeechèƒ½å¤Ÿå¤„ç†å¹¶ç»Ÿä¸€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æç¤ºï¼Œå¢å¼ºè¯­éŸ³åˆæˆçš„é€‚åº”æ€§ã€‚</li>
<li>FleSpeechæ”¯æŒåˆ›é€ æ€§ä¸ç²¾ç¡®æ§åˆ¶ç”Ÿæˆçš„è¯­éŸ³ã€‚</li>
<li>FleSpeeché‡‡ç”¨å¤šæ¨¡æ€æ•°æ®é›†æ”¶é›†æµç¨‹ï¼Œæœ‰åŠ©äºè¯¥é¢†åŸŸçš„ç ”ç©¶ä¸åº”ç”¨ã€‚</li>
<li>FleSpeechçš„æœ‰æ•ˆæ€§é€šè¿‡ä¸»è§‚å’Œå®¢è§‚å®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbedcac4125baeefcfaf324eec1523ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682fe78391efffab7b91b971a10ef258.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Right-Label-Context-in-End-to-End-Training-of-Time-Synchronous-ASR-Models"><a href="#Right-Label-Context-in-End-to-End-Training-of-Time-Synchronous-ASR-Models" class="headerlink" title="Right Label Context in End-to-End Training of Time-Synchronous ASR   Models"></a>Right Label Context in End-to-End Training of Time-Synchronous ASR   Models</h2><p><strong>Authors:Tina Raissi, Ralf Schlter, Hermann Ney</strong></p>
<p>Current time-synchronous sequence-to-sequence automatic speech recognition (ASR) models are trained by using sequence level cross-entropy that sums over all alignments. Due to the discriminative formulation, incorporating the right label context into the training criterionâ€™s gradient causes normalization problems and is not mathematically well-defined. The classic hybrid neural network hidden Markov model (NN-HMM) with its inherent generative formulation enables conditioning on the right label context. However, due to the HMM state-tying the identity of the right label context is never modeled explicitly. In this work, we propose a factored loss with auxiliary left and right label contexts that sums over all alignments. We show that the inclusion of the right label context is particularly beneficial when training data resources are limited. Moreover, we also show that it is possible to build a factored hybrid HMM system by relying exclusively on the full-sum criterion. Experiments were conducted on Switchboard 300h and LibriSpeech 960h. </p>
<blockquote>
<p>å½“å‰çš„æ—¶é—´åŒæ­¥åºåˆ—åˆ°åºåˆ—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ˜¯é€šè¿‡ä½¿ç”¨åºåˆ—çº§åˆ«çš„äº¤å‰ç†µï¼ˆå¯¹æ‰€æœ‰å¯¹é½è¿›è¡Œæ±‚å’Œï¼‰æ¥è¿›è¡Œè®­ç»ƒçš„ã€‚ç”±äºé‡‡ç”¨åˆ¤åˆ«å¼å…¬å¼ï¼Œå°†æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çº³å…¥è®­ç»ƒæ ‡å‡†çš„æ¢¯åº¦ä¸­ä¼šå¯¼è‡´å½’ä¸€åŒ–é—®é¢˜ï¼Œå¹¶ä¸”åœ¨æ•°å­¦ä¸Šå®šä¹‰ä¸æ˜ç¡®ã€‚ç»å…¸çš„æ··åˆç¥ç»ç½‘ç»œéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆNN-HMMï¼‰å‡­å€Ÿå…¶å†…åœ¨çš„ç”Ÿæˆå¼å…¬å¼ï¼Œèƒ½å¤Ÿå®ç°åŸºäºæ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚ç„¶è€Œï¼Œç”±äºHMMçš„çŠ¶æ€ç»‘å®šï¼Œæ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çš„èº«ä»½ä»æœªè¢«æ˜¾å¼å»ºæ¨¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰è¾…åŠ©çš„å·¦å’Œå³æ ‡ç­¾ä¸Šä¸‹æ–‡çš„åˆ†è§£æŸå¤±å‡½æ•°ï¼Œå¯¹æ‰€æœ‰å¯¹é½è¿›è¡Œæ±‚å’Œã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨è®­ç»ƒæ•°æ®èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¼•å…¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡ç‰¹åˆ«æœ‰ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä»…ä¾èµ–å…¨å’Œå‡†åˆ™æ¥æ„å»ºåˆ†è§£æ··åˆHMMç³»ç»Ÿä¹Ÿæ˜¯å¯èƒ½çš„ã€‚å®éªŒåœ¨Switchboard 300å°æ—¶å’ŒLibriSpeech 960å°æ—¶æ•°æ®é›†ä¸Šè¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04521v1">PDF</a> Accepted for presentation at ICASSP 2025</p>
<p><strong>Summary</strong><br>     å½“å‰æ—¶åºåŒæ­¥çš„åºåˆ—åˆ°åºåˆ—è¯­éŸ³è¯†åˆ«æ¨¡å‹ä½¿ç”¨åºåˆ—çº§åˆ«çš„äº¤å‰ç†µè¿›è¡Œè®­ç»ƒï¼Œè¯¥äº¤å‰ç†µå¯¹æ‰€æœ‰å¯¹é½è¿›è¡Œæ±‚å’Œã€‚ç”±äºåˆ¤åˆ«å¼å…¬å¼çš„é—®é¢˜ï¼Œå°†æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çº³å…¥è®­ç»ƒæ ‡å‡†çš„æ¢¯åº¦ä¸­ä¼šå¯¼è‡´å½’ä¸€åŒ–é—®é¢˜ï¼Œä¸”åœ¨æ•°å­¦ä¸Šæœªæ˜ç¡®å®šä¹‰ã€‚ç»å…¸çš„æ··åˆç¥ç»ç½‘ç»œéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆNN-HMMï¼‰å…·æœ‰å…¶å›ºæœ‰çš„ç”Ÿæˆå¼è¡¨è¿°ï¼Œå¯ä»¥åŸºäºæ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚ä½†ç”±äºHMMçš„çŠ¶æ€ç»‘å®šï¼Œå¹¶æœªæ˜ç¡®å»ºæ¨¡æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çš„èº«ä»½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰è¾…åŠ©å·¦å³æ ‡ç­¾ä¸Šä¸‹æ–‡çš„åˆ†è§£æŸå¤±ï¼Œè¯¥æŸå¤±å¯¹æ‰€æœ‰å¯¹é½è¿›è¡Œæ±‚å’Œã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨è®­ç»ƒæ•°æ®èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¼•å…¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡ç‰¹åˆ«æœ‰ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä»…ä¾é å…¨å’Œå‡†åˆ™æ¥æ„å»ºåˆ†è§£æ··åˆHMMç³»ç»Ÿçš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ASRæ¨¡å‹ä½¿ç”¨åºåˆ—çº§åˆ«äº¤å‰ç†µè¿›è¡Œè®­ç»ƒï¼Œå¯¹æ‰€æœ‰å¯¹é½æ±‚å’Œã€‚</li>
<li>åˆ¤åˆ«å¼å…¬å¼å°†æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çº³å…¥è®­ç»ƒæ ‡å‡†çš„æ¢¯åº¦ä¼šå¼•å‘é—®é¢˜ã€‚</li>
<li>ç»å…¸çš„NN-HMMæ¨¡å‹å¯ä»¥åŸºäºæ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä½†æœªæ˜ç¡®å»ºæ¨¡å…¶èº«ä»½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¸¦æœ‰è¾…åŠ©å·¦å³æ ‡ç­¾ä¸Šä¸‹æ–‡çš„åˆ†è§£æŸå¤±ã€‚</li>
<li>å¼•å…¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ç›Šã€‚</li>
<li>ä»…ä¾é å…¨å’Œå‡†åˆ™å¯ä»¥æ„å»ºåˆ†è§£æ··åˆHMMç³»ç»Ÿã€‚</li>
<li>å®éªŒåœ¨Switchboard 300hå’ŒLibriSpeech 960hæ•°æ®é›†ä¸Šè¿›è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4032f82988921cd10a90fdbe243064bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d200467c6c9ab9497e1faffd6671107.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ZSVC-Zero-shot-Style-Voice-Conversion-with-Disentangled-Latent-Diffusion-Models-and-Adversarial-Training"><a href="#ZSVC-Zero-shot-Style-Voice-Conversion-with-Disentangled-Latent-Diffusion-Models-and-Adversarial-Training" class="headerlink" title="ZSVC: Zero-shot Style Voice Conversion with Disentangled Latent   Diffusion Models and Adversarial Training"></a>ZSVC: Zero-shot Style Voice Conversion with Disentangled Latent   Diffusion Models and Adversarial Training</h2><p><strong>Authors:Xinfa Zhu, Lei He, Yujia Xiao, Xi Wang, Xu Tan, Sheng Zhao, Lei Xie</strong></p>
<p>Style voice conversion aims to transform the speaking style of source speech into a desired style while keeping the original speakerâ€™s identity. However, previous style voice conversion approaches primarily focus on well-defined domains such as emotional aspects, limiting their practical applications. In this study, we present ZSVC, a novel Zero-shot Style Voice Conversion approach that utilizes a speech codec and a latent diffusion model with speech prompting mechanism to facilitate in-context learning for speaking style conversion. To disentangle speaking style and speaker timbre, we introduce information bottleneck to filter speaking style in the source speech and employ Uncertainty Modeling Adaptive Instance Normalization (UMAdaIN) to perturb the speaker timbre in the style prompt. Moreover, we propose a novel adversarial training strategy to enhance in-context learning and improve style similarity. Experiments conducted on 44,000 hours of speech data demonstrate the superior performance of ZSVC in generating speech with diverse speaking styles in zero-shot scenarios. </p>
<blockquote>
<p>é£æ ¼è¯­éŸ³è½¬æ¢æ—¨åœ¨å°†æºè¯­éŸ³çš„è®²è¯é£æ ¼è½¬æ¢ä¸ºæ‰€éœ€çš„é£æ ¼ï¼ŒåŒæ—¶ä¿æŒåŸå§‹è¯´è¯è€…çš„èº«ä»½ã€‚ç„¶è€Œï¼Œä»¥å‰çš„é£æ ¼è¯­éŸ³è½¬æ¢æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æƒ…ç»ªç­‰æ–¹é¢ç­‰å®šä¹‰æ˜ç¡®çš„é¢†åŸŸï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ZSVCï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è¯­éŸ³ç¼–è§£ç å™¨å’Œå¸¦æœ‰è¯­éŸ³æç¤ºæœºåˆ¶çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹é›¶é£æ ¼è¯­éŸ³è½¬æ¢æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›è¯´è¯é£æ ¼è½¬æ¢çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ä¸ºäº†è§£å¼€è¯´è¯é£æ ¼å’Œè¯´è¯è€…éŸ³è‰²ï¼Œæˆ‘ä»¬å¼•å…¥ä¿¡æ¯ç“¶é¢ˆæ¥è¿‡æ»¤æºè¯­éŸ³ä¸­çš„è¯´è¯é£æ ¼ï¼Œå¹¶é‡‡ç”¨ä¸ç¡®å®šæ€§å»ºæ¨¡è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼ˆUMAdaINï¼‰æ¥æ‰°åŠ¨é£æ ¼æç¤ºä¸­çš„è¯´è¯è€…éŸ³è‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å¯¹æŠ—è®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºä¸Šä¸‹æ–‡å­¦ä¹ å¹¶æé«˜é£æ ¼ç›¸ä¼¼æ€§ã€‚åœ¨44000å°æ—¶è¯­éŸ³æ•°æ®è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒZSVCåœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ç”Ÿæˆå…·æœ‰å¤šç§è¯´è¯é£æ ¼çš„è¯­éŸ³æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04416v1">PDF</a> 5 pages, 3 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é›¶æ ·æœ¬é£æ ¼è¯­éŸ³è½¬æ¢æ–¹æ³•ï¼ˆZSVCï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è¯­éŸ³ç¼–è§£ç å™¨å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆè¯´è¯é£æ ¼æç¤ºï¼Œè¿›è¡Œé£æ ¼è¯­éŸ³è½¬æ¢ã€‚ç ”ç©¶æå‡ºäº†ä¿¡æ¯ç“¶é¢ˆå’Œä¸ç¡®å®šæ€§å»ºæ¨¡è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼ˆUMAdaINï¼‰ç­‰æŠ€æœ¯æ¥å¤„ç†è¯´è¯é£æ ¼å’Œå‘éŸ³ç‰¹æ€§çš„è½¬æ¢é—®é¢˜ã€‚é€šè¿‡æå‡ºçš„å¯¹æŠ—è®­ç»ƒç­–ç•¥ï¼Œæé«˜åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„é£æ ¼ç›¸ä¼¼æ€§å’Œå­¦ä¹ è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ZSVCæ˜¯ä¸€ç§é›¶æ ·æœ¬é£æ ¼è¯­éŸ³è½¬æ¢æ–¹æ³•ï¼Œä¸“æ³¨äºè½¬æ¢æºè¯­éŸ³çš„è¯´è¯é£æ ¼ä¸ºæœŸæœ›çš„é£æ ¼ï¼ŒåŒæ—¶ä¿æŒåŸå§‹è¯´è¯è€…çš„èº«ä»½ã€‚</li>
<li>åˆ©ç”¨è¯­éŸ³ç¼–è§£ç å™¨å’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¯´è¯é£æ ¼è½¬æ¢ã€‚</li>
<li>é‡‡ç”¨ä¿¡æ¯ç“¶é¢ˆæŠ€æœ¯æ¥åˆ†ç¦»è¯´è¯é£æ ¼å’Œå‘éŸ³ç‰¹æ€§ã€‚</li>
<li>ä½¿ç”¨UMAdaINæŠ€æœ¯æ¥å¹²æ‰°è¯´è¯é£æ ¼æç¤ºä¸­çš„å‘éŸ³ç‰¹æ€§ã€‚</li>
<li>æå‡ºå¯¹æŠ—è®­ç»ƒç­–ç•¥ä»¥æé«˜é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„é£æ ¼ç›¸ä¼¼æ€§å’Œå­¦ä¹ è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bc43c0640e871caa449c189f72daa4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28fdfd640522705d8c8342003539461b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04ab1b46fa02c3265974d9558a9ab974.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-521d2a5cc5b94d15486590e8c1258235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee53f6cc307320acbf4158bb979f794.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57d2b7be1706f5012f5d8a06c7833270.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Phone-purity-Guided-Discrete-Tokens-for-Dysarthric-Speech-Recognition"><a href="#Phone-purity-Guided-Discrete-Tokens-for-Dysarthric-Speech-Recognition" class="headerlink" title="Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition"></a>Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition</h2><p><strong>Authors:Huimeng Wang, Xurong Xie, Mengzhe Geng, Shujie Hu, Haoning Xu, Youjun Chen, Zhaoqing Li, Jiajun Deng, Xunying Liu</strong></p>
<p>Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99% and 1.77% absolute (3.21% and 4.82% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means&#x2F;VAE-VQ clusters after introducing phone-purity guidance. </p>
<blockquote>
<p>æå–çš„ç¦»æ•£ä»¤ç‰Œæä¾›é«˜æ•ˆä¸”é€‚åº”äºç‰¹å®šé¢†åŸŸçš„è¯­éŸ³ç‰¹å¾ã€‚å®ƒä»¬åœ¨é’ˆå¯¹å‘éŸ³ä¸ç²¾ç¡®ä»¥åŠä¸æ­£å¸¸å£°éŸ³å­˜åœ¨è¾ƒå¤§å·®å¼‚çš„æ··ä¹±è¯­éŸ³ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°æ¢ç´¢ã€‚ä¸ºäº†æé«˜åœ¨è¿ç»­ç‰¹å¾çš„æ— ç›‘ç£K-meansæˆ–å‘é‡é‡åŒ–è¿‡ç¨‹ä¸­è¢«å‰Šå¼±çš„è¯­éŸ³è¾¨åˆ«èƒ½åŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ç”¨äºå£é½¿ä¸æ¸…çš„è¯­éŸ³è¯†åˆ«çš„æ–°å‹ç”µè¯çº¯å‡€åº¦å¼•å¯¼ï¼ˆPPGï¼‰ç¦»æ•£ä»¤ç‰Œã€‚è¯­éŸ³æ ‡ç­¾ç›‘ç£ç”¨äºè§„èŒƒæœ€å¤§ä¼¼ç„¶å’Œé‡å»ºè¯¯å·®æˆæœ¬ï¼Œè¿™äº›æˆæœ¬ç”¨äºåŸºäºæ ‡å‡†K-meanså’ŒVAE-VQçš„ç¦»æ•£ä»¤ç‰Œæå–ã€‚åœ¨UASpeechè¯­æ–™åº“ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä»HuBERTä¸­æå–çš„PPGç¦»æ•£ä»¤ç‰Œç‰¹å¾å§‹ç»ˆä¼˜äºä½¿ç”¨éPPGçš„K-meansæˆ–VAE-VQä»¤ç‰Œçš„æ··åˆTDNNå’Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰Conformerç³»ç»Ÿã€‚åœ¨UASpeechçš„16ä½å£é½¿ä¸æ¸…çš„è¯´è¯è€…çš„æµ‹è¯•é›†ä¸Šï¼Œé€šè¿‡ç»Ÿè®¡æ˜¾è‘—çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰å‡å°‘é«˜è¾¾ç»å¯¹å€¼çš„0.99ï¼…å’Œ1.77ï¼…ï¼ˆç›¸å¯¹å€¼åˆ†åˆ«ä¸º3.21ï¼…å’Œ4.82ï¼…ï¼‰ã€‚é€šè¿‡å°†ä½¿ç”¨ä¸åŒä»¤ç‰Œç‰¹å¾çš„ç³»ç»Ÿè¿›è¡Œç»„åˆï¼Œè·å¾—äº†æœ€ä½çš„WERä¸º23.25ï¼…ã€‚åœ¨ç”µè¯çº¯å‡€åº¦æŒ‡æ ‡æ–¹é¢ä¹Ÿå–å¾—äº†æŒç»­çš„æ”¹è¿›ã€‚T-SNEå¯è§†åŒ–è¿›ä¸€æ­¥è¯æ˜äº†å¼•å…¥ç”µè¯çº¯å‡€åº¦æŒ‡å¯¼åï¼ŒK-means&#x2F;VAE-VQé›†ç¾¤ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ›´åŠ æ¸…æ™°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04379v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong><br>ç¦»æ•£è¯­éŸ³ç‰¹å¾æå–æŠ€æœ¯å¯¹äºè¯­éŸ³é¢†åŸŸå…·æœ‰é«˜æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚é’ˆå¯¹å‘éŸ³ä¸ç²¾ç¡®ã€ä¸æ­£å¸¸è¯­éŸ³å­˜åœ¨è¾ƒå¤§å·®å¼‚çš„éšœç¢æ€§è¯­éŸ³ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„ç”µè¯çº¯å‡€åº¦å¼•å¯¼ï¼ˆPPGï¼‰ç¦»æ•£ä»¤ç‰ŒæŠ€æœ¯ç”¨äºè¯†åˆ«ã€‚é€šè¿‡åˆ©ç”¨è¯­éŸ³æ ‡ç­¾ç›‘ç£æ¥è§„èŒƒæœ€å¤§ä¼¼ç„¶å’Œé‡å»ºè¯¯å·®æˆæœ¬ï¼Œæ”¹å–„ä¼ ç»Ÿçš„Kå‡å€¼å’ŒåŸºäºVAE-VQçš„ç¦»æ•£ä»¤ç‰Œæå–ä¸­çš„è¯­éŸ³è¾¨è¯†é—®é¢˜ã€‚åœ¨UASpeechè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨PPGç¦»æ•£ä»¤ç‰Œç‰¹å¾çš„HuBERTç³»ç»Ÿä¼˜äºæ··åˆTDNNå’Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰Conformerç³»ç»Ÿï¼Œåœ¨ä»£ç æœ¬å¤§å°ä¸åŒçš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»å¯¹é™ä½è¾¾1.77%ï¼Œç›¸å¯¹é™ä½è¾¾4.82%ã€‚ç»“åˆä¸åŒä»¤ç‰Œç‰¹å¾çš„ç³»ç»Ÿå¯è·å¾—æœ€ä½WERä¸º23.25%ã€‚åŒæ—¶ï¼Œç”µè¯çº¯å‡€åº¦æŒ‡æ ‡ä¹Ÿæœ‰æ˜¾è‘—æé«˜ã€‚T-SNEå¯è§†åŒ–è¿›ä¸€æ­¥è¯æ˜å¼•å…¥ç”µè¯çº¯å‡€åº¦æŒ‡å¯¼åï¼ŒKå‡å€¼&#x2F; VAE-VQé›†ç¾¤ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ›´åŠ æ¸…æ™°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£è¯­éŸ³ç‰¹å¾æå–æŠ€æœ¯å¯¹äºè¯­éŸ³é¢†åŸŸå…·æœ‰é«˜æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>é’ˆå¯¹éšœç¢æ€§è¯­éŸ³ï¼ˆå¦‚å‘éŸ³ä¸ç²¾ç¡®ç­‰ï¼‰ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”µè¯çº¯å‡€åº¦å¼•å¯¼ï¼ˆPPGï¼‰ç¦»æ•£ä»¤ç‰ŒæŠ€æœ¯ç”¨äºè¯†åˆ«ã€‚</li>
<li>é€šè¿‡è¯­éŸ³æ ‡ç­¾ç›‘ç£æ”¹å–„ä¼ ç»Ÿçš„Kå‡å€¼å’ŒåŸºäºVAE-VQçš„ç¦»æ•£ä»¤ç‰Œæå–ä¸­çš„è¯­éŸ³è¾¨è¯†é—®é¢˜ã€‚</li>
<li>åœ¨UASpeechè¯­æ–™åº“çš„å®éªŒä¸­ï¼Œé‡‡ç”¨PPGç¦»æ•£ä»¤ç‰Œç‰¹å¾çš„HuBERTç³»ç»Ÿè¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹äºå…¶ä»–ç³»ç»Ÿæœ‰æ˜æ˜¾é™ä½çš„å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>ç»“åˆä¸åŒä»¤ç‰Œç‰¹å¾çš„ç³»ç»Ÿå¯ä»¥è·å¾—æ›´ä½çš„WERã€‚</li>
<li>ç”µè¯çº¯å‡€åº¦æŒ‡æ ‡æ˜¾è‘—æé«˜ï¼Œè¯æ˜PPGç¦»æ•£ä»¤ç‰ŒæŠ€æœ¯å¯¹äºè¯­éŸ³è¯†åˆ«çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c478a236d32b4221878017f8ad95f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-540d839d57a235589d279f477cf0a3e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95d0a83e2ac9e401214805042779b478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51a849d39581e6f58775db5f9fd54192.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-168b0be0b2d932a705519783e505e34c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LipGen-Viseme-Guided-Lip-Video-Generation-for-Enhancing-Visual-Speech-Recognition"><a href="#LipGen-Viseme-Guided-Lip-Video-Generation-for-Enhancing-Visual-Speech-Recognition" class="headerlink" title="LipGen: Viseme-Guided Lip Video Generation for Enhancing Visual Speech   Recognition"></a>LipGen: Viseme-Guided Lip Video Generation for Enhancing Visual Speech   Recognition</h2><p><strong>Authors:Bowen Hao, Dongliang Zhou, Xiaojie Li, Xingyu Zhang, Liang Xie, Jianlong Wu, Erwei Yin</strong></p>
<p>Visual speech recognition (VSR), commonly known as lip reading, has garnered significant attention due to its wide-ranging practical applications. The advent of deep learning techniques and advancements in hardware capabilities have significantly enhanced the performance of lip reading models. Despite these advancements, existing datasets predominantly feature stable video recordings with limited variability in lip movements. This limitation results in models that are highly sensitive to variations encountered in real-world scenarios. To address this issue, we propose a novel framework, LipGen, which aims to improve model robustness by leveraging speech-driven synthetic visual data, thereby mitigating the constraints of current datasets. Additionally, we introduce an auxiliary task that incorporates viseme classification alongside attention mechanisms. This approach facilitates the efficient integration of temporal information, directing the modelâ€™s focus toward the relevant segments of speech, thereby enhancing discriminative capabilities. Our method demonstrates superior performance compared to the current state-of-the-art on the lip reading in the wild (LRW) dataset and exhibits even more pronounced advantages under challenging conditions. </p>
<blockquote>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰ï¼Œä¹Ÿç§°ä¸ºå”‡è¯»ï¼Œç”±äºå…¶å¹¿æ³›çš„å®é™…åº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‡ºç°å’Œç¡¬ä»¶èƒ½åŠ›çš„è¿›æ­¥æå¤§åœ°æé«˜äº†å”‡è¯»æ¨¡å‹çš„æ€§èƒ½ã€‚å°½ç®¡æœ‰è¿™äº›è¿›æ­¥ï¼Œç°æœ‰æ•°æ®é›†ä¸»è¦ä»¥ç¨³å®šçš„è§†é¢‘å½•åˆ¶ä¸ºä¸»ï¼Œå”‡åŠ¨å˜åŒ–æœ‰é™ã€‚è¿™ä¸€å±€é™æ€§å¯¼è‡´æ¨¡å‹å¯¹ç°å®åœºæ™¯ä¸­é‡åˆ°çš„å˜åŠ¨éå¸¸æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹æ¡†æ¶LipGenï¼Œæ—¨åœ¨åˆ©ç”¨è¯­éŸ³é©±åŠ¨åˆæˆè§†è§‰æ•°æ®ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä»è€Œç¼“è§£å½“å‰æ•°æ®é›†çš„çº¦æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼Œç»“åˆvisemeåˆ†ç±»å’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæœ‰æ•ˆåœ°æ•´åˆæ—¶é—´ä¿¡æ¯ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨è¯­éŸ³çš„ç›¸å…³ç‰‡æ®µï¼Œä»è€Œæé«˜è¾¨åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å”‡è¯»é¢†åŸŸçš„LRWæ•°æ®é›†ä¸Šå±•ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´ä¸ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04204v1">PDF</a> This paper has been accepted for presentation at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰ï¼Œä¿—ç§°å”‡è¯»æŠ€æœ¯ï¼Œå› å…¶å¹¿æ³›çš„å®è·µåº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•å’Œç¡¬ä»¶èƒ½åŠ›çš„è¿›æ­¥ï¼Œå”‡è¯»æ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†ä¸»è¦ä»¥ç¨³å®šçš„è§†é¢‘å½•åˆ¶ä¸ºä¸»ï¼Œå”‡åŠ¨å˜åŒ–æœ‰é™ï¼Œå¯¼è‡´æ¨¡å‹å¯¹çœŸå®åœºæ™¯ä¸­çš„å˜åŒ–é«˜åº¦æ•æ„Ÿã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LipGenè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨è¯­éŸ³é©±åŠ¨åˆæˆè§†è§‰æ•°æ®ï¼Œæå‡æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œä»¥ç¼“è§£å½“å‰æ•°æ®é›†çš„çº¦æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€é¡¹è¾…åŠ©ä»»åŠ¡ï¼Œç»“åˆvisemeåˆ†ç±»å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥é«˜æ•ˆåœ°æ•´åˆæ—¶åºä¿¡æ¯ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨äºè¯­éŸ³çš„å…³é”®ç‰‡æ®µï¼Œä»è€Œæå‡å…¶è¾¨åˆ«èƒ½åŠ›ã€‚ç›¸è¾ƒäºç°æœ‰çš„å”‡è¯»æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å”‡è¯»é‡ï¼ˆLRWï¼‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸‹ä¼˜åŠ¿æ›´ä¸ºæ˜æ˜¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æŠ€æœ¯å› å…¶å®ç”¨æ€§å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†ä¸»è¦ä¾èµ–ç¨³å®šè§†é¢‘å½•åˆ¶ï¼Œé™åˆ¶äº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>LipGenæ¡†æ¶æ—¨åœ¨æé«˜æ¨¡å‹ç¨³å¥æ€§ï¼Œé€šè¿‡åˆ©ç”¨è¯­éŸ³é©±åŠ¨çš„åˆæˆè§†è§‰æ•°æ®ç¼“è§£æ•°æ®çº¦æŸã€‚</li>
<li>å¼•å…¥visemeåˆ†ç±»å’Œæ³¨æ„åŠ›æœºåˆ¶çš„è¾…åŠ©ä»»åŠ¡ï¼Œä»¥æé«˜æ¨¡å‹çš„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•åœ¨å”‡è¯»é‡ï¼ˆLRWï¼‰æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸‹ï¼Œè¯¥æ–¹æ³•è¾ƒç°æœ‰æŠ€æœ¯æ›´å…·ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7bf530835b274ffa416d3a7c78b88e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a31488feaafd63b636b5de76af0c5f22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d91a4cbfbe72f01bac16f09373078e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d3639caf65bc2dfce3ef688e994e8e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Listening-and-Seeing-Again-Generative-Error-Correction-for-Audio-Visual-Speech-Recognition"><a href="#Listening-and-Seeing-Again-Generative-Error-Correction-for-Audio-Visual-Speech-Recognition" class="headerlink" title="Listening and Seeing Again: Generative Error Correction for Audio-Visual   Speech Recognition"></a>Listening and Seeing Again: Generative Error Correction for Audio-Visual   Speech Recognition</h2><p><strong>Authors:Rui Liu, Hongyu Yuan, Haizhou Li</strong></p>
<p>Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech Recognition (AVSR) takes audio and visual signals simultaneously to infer the transcription. Recent studies have shown that Large Language Models (LLMs) can be effectively used for Generative Error Correction (GER) in ASR by predicting the best transcription from ASR-generated N-best hypotheses. However, these LLMs lack the ability to simultaneously understand audio and visual, making the GER approach challenging to apply in AVSR. In this work, we propose a novel GER paradigm for AVSR, termed AVGER, that follows the concept of &#96;&#96;listening and seeing againâ€™â€™. Specifically, we first use the powerful AVSR system to read the audio and visual signals to get the N-Best hypotheses, and then use the Q-former-based Multimodal Synchronous Encoder to read the audio and visual information again and convert them into an audio and video compression representation respectively that can be understood by LLM. Afterward, the audio-visual compression representation and the N-Best hypothesis together constitute a Cross-modal Prompt to guide the LLM in producing the best transcription. In addition, we also proposed a Multi-Level Consistency Constraint training criterion, including logits-level, utterance-level and representations-level, to improve the correction accuracy while enhancing the interpretability of audio and visual compression representations. The experimental results on the LRS3 dataset show that our method outperforms current mainstream AVSR systems. The proposed AVGER can reduce the Word Error Rate (WER) by 24% compared to them. Code and models can be found at: <a target="_blank" rel="noopener" href="https://github.com/CircleRedRain/AVGER">https://github.com/CircleRedRain/AVGER</a>. </p>
<blockquote>
<p>ä¸ä¼ ç»Ÿçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸åŒï¼Œè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åŒæ—¶å¤„ç†éŸ³é¢‘å’Œè§†è§‰ä¿¡å·æ¥è¿›è¡Œè½¬å½•ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡é¢„æµ‹ASRç”Ÿæˆçš„N-bestå‡è®¾ä¸­çš„æœ€ä½³è½¬å½•ï¼Œæœ‰æ•ˆåœ°ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„ç”Ÿæˆé”™è¯¯æ ¡æ­£ï¼ˆGERï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›LLMæ— æ³•åŒæ—¶ç†è§£éŸ³é¢‘å’Œè§†è§‰ä¿¡å·ï¼Œä½¿å¾—åœ¨AVSRä¸­åº”ç”¨GERæ–¹æ³•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„AVSRçš„GERæ–¹æ³•ï¼Œç§°ä¸ºAVGERï¼Œå®ƒéµå¾ªâ€œå†æ¬¡è†å¬å’Œè§‚çœ‹â€çš„æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¼ºå¤§çš„AVSRç³»ç»Ÿè¯»å–éŸ³é¢‘å’Œè§†è§‰ä¿¡å·ï¼Œä»¥è·å¾—N-bestå‡è®¾ï¼Œç„¶åä½¿ç”¨åŸºäºQ-formerçš„å¤šæ¨¡æ€åŒæ­¥ç¼–ç å™¨å†æ¬¡è¯»å–éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºLLMå¯ä»¥ç†è§£çš„éŸ³é¢‘å’Œè§†é¢‘å‹ç¼©è¡¨ç¤ºå½¢å¼ã€‚ä¹‹åï¼Œè§†å¬å‹ç¼©è¡¨ç¤ºå’ŒN-bestå‡è®¾å…±åŒæ„æˆä¸€ä¸ªè·¨æ¨¡æ€æç¤ºï¼Œå¼•å¯¼LLMç”Ÿæˆæœ€ä½³è½¬å½•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å¤šçº§åˆ«ä¸€è‡´æ€§çº¦æŸè®­ç»ƒå‡†åˆ™ï¼ŒåŒ…æ‹¬logitsçº§åˆ«ã€è¯è¯­çº§åˆ«å’Œè¡¨ç¤ºçº§åˆ«ï¼Œä»¥æé«˜æ ¡æ­£å‡†ç¡®æ€§ï¼ŒåŒæ—¶å¢å¼ºè§†å¬å‹ç¼©è¡¨ç¤ºçš„å¯è§£é‡Šæ€§ã€‚åœ¨LRS3æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå½“å‰ä¸»æµçš„AVSRç³»ç»Ÿã€‚ä¸å®ƒä»¬ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„AVGERå¯ä»¥å°†å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½24%ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CircleRedRain/AVGER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CircleRedRain/AVGERæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04038v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰çš„ç”Ÿæˆå¼è¯¯å·®æ ¡æ­£ï¼ˆAVGERï¼‰æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆéŸ³é¢‘å’Œè§†è§‰ä¿¡å·ï¼Œåˆ©ç”¨å¼ºå¤§çš„AVSRç³»ç»Ÿè·å–N-bestå‡è®¾ï¼Œå†é€šè¿‡Q-formeråŸºå¤šæ¨¡æ€åŒæ­¥ç¼–ç å™¨å¯¹éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯è¿›è¡ŒäºŒæ¬¡å¤„ç†ï¼Œè½¬åŒ–ä¸ºå¯è¢«å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç†è§£çš„éŸ³é¢‘è§†è§‰å‹ç¼©è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¤šçº§åˆ«ä¸€è‡´æ€§çº¦æŸè®­ç»ƒå‡†åˆ™ï¼Œä»¥æé«˜æ ¡æ­£ç²¾åº¦å¹¶å¢å¼ºéŸ³é¢‘å’Œè§†è§‰å‹ç¼©è¡¨ç¤ºçš„å¯è§£é‡Šæ€§ã€‚åœ¨LRS3æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¸»æµAVSRç³»ç»Ÿï¼Œå¯é™ä½24%çš„å•è¯é”™è¯¯ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AVGERæ–¹æ³•ç»“åˆäº†éŸ³é¢‘å’Œè§†è§‰ä¿¡å·è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œæé«˜äº†è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨å¼ºå¤§çš„AVSRç³»ç»Ÿè·å–N-bestå‡è®¾ï¼Œä¸ºç”Ÿæˆå¼è¯¯å·®æ ¡æ­£ï¼ˆGERï¼‰æä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ã€‚</li>
<li>Q-formeråŸºå¤šæ¨¡æ€åŒæ­¥ç¼–ç å™¨èƒ½å¤Ÿå°†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºå¯ç†è§£çš„å‹ç¼©è¡¨ç¤ºï¼Œä¾¿äºLLMè¿›è¡Œå¤„ç†ã€‚</li>
<li>æå‡ºå¤šçº§åˆ«ä¸€è‡´æ€§çº¦æŸè®­ç»ƒå‡†åˆ™ï¼Œæ—¨åœ¨æé«˜æ ¡æ­£ç²¾åº¦å¹¶å¢å¼ºéŸ³é¢‘å’Œè§†è§‰è¡¨ç¤ºçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>AVGERæ–¹æ³•åœ¨LRS3æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¸»æµAVSRç³»ç»Ÿã€‚</li>
<li>AVGERæ–¹æ³•å¯é™ä½å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰24%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CircleRedRain/AVGER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CircleRedRain/AVGERæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09d69c4484a4853336ab0fabd18070cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6d4b7418f9149bd5d1b302f5c8a0dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a4302191f716822abaaa245f35c2977.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40abe0392690b40b4bac30378b6442fb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models"><a href="#Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models" class="headerlink" title="Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models"></a>Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models</h2><p><strong>Authors:Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</strong></p>
<p>We propose Samba ASR,the first state of the art Automatic Speech Recognition(ASR)model leveraging the novel Mamba architecture as both encoder and decoder,built on the foundation of state space models(SSMs).Unlike transformerbased ASR models,which rely on self-attention mechanisms to capture dependencies,Samba ASR effectively models both local and global temporal dependencies using efficient statespace dynamics,achieving remarkable performance gains.By addressing the limitations of transformers,such as quadratic scaling with input length and difficulty in handling longrange dependencies,Samba ASR achieves superior accuracy and efficiency.Experimental results demonstrate that Samba ASR surpasses existing opensource transformerbased ASR models across various standard benchmarks,establishing it as the new state of theart in ASR.Extensive evaluations on the benchmark dataset show significant improvements in Word Error Rate(WER),with competitive performance even in lowresource scenarios.Furthermore,the inherent computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.Our contributions include the development of a new Samba ASR architecture for automatic speech recognition(ASR),demonstrating the superiority of structured statespace models(SSMs)over transformer based models for speech sequence processing.We provide a comprehensive evaluation on public benchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth analysis of computational efficiency,robustness to noise,and sequence generalization.This work highlights the viability of Mamba SSMs as a transformerfree alternative for efficient and accurate ASR.By leveraging the advancements of statespace modeling,Samba ASR redefines ASR performance standards and sets a new benchmark for future research in this field. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Samba ASRï¼Œè¿™æ˜¯é¦–æ¬¾ç»“åˆæ–°å‹Mambaæ¶æ„ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨çš„å…ˆè¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå®ƒå»ºç«‹åœ¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åŸºç¡€ä¸Šã€‚ä¸åŒäºä¾èµ–è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•è·ä¾èµ–å…³ç³»çš„åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼ŒSamba ASRä½¿ç”¨é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´åŠ¨åŠ›å­¦æœ‰æ•ˆåœ°å¯¹å±€éƒ¨å’Œå…¨å±€æ—¶é—´ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡è§£å†³å˜å‹å™¨æ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚è¾“å…¥é•¿åº¦çš„äºŒæ¬¡æ‰©å±•å’Œå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»çš„å›°éš¾ï¼ŒSamba ASRåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šæ›´èƒœä¸€ç­¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSamba ASRåœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œæˆä¸ºASRé¢†åŸŸçš„æ–°æŠ€æœ¯å‰æ²¿ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå³ä½¿åœ¨èµ„æºåŒ®ä¹çš„åœºæ™¯ä¸­ä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒMambaæ¶æ„çš„å›ºæœ‰è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿Samba ASRæˆä¸ºé€‚ç”¨äºå„ç§ASRä»»åŠ¡çš„å¯æ‰©å±•å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¼€å‘æ–°çš„Samba ASRæ¶æ„ï¼Œè¯æ˜ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨è¯­éŸ³åºåˆ—å¤„ç†æ–¹é¢ä¼˜äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶å¤„äºè¡Œä¸šé¡¶å°–çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶å¯¹è®¡ç®—æ•ˆç‡ã€æŠ—å™ªå£°å¹²æ‰°çš„é²æ£’æ€§å’Œåºåˆ—æ³›åŒ–èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†Mamba SSMsä½œä¸ºæ— å˜å‹å™¨çš„é«˜æ•ˆå‡†ç¡®ASRçš„æ›¿ä»£æ–¹æ¡ˆçš„å¯è¡Œæ€§ã€‚é€šè¿‡åˆ©ç”¨çŠ¶æ€ç©ºé—´å»ºæ¨¡çš„è¿›å±•ï¼ŒSamba ASRé‡æ–°å®šä¹‰äº†ASRçš„æ€§èƒ½æ ‡å‡†ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02832v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Samba ASRæ˜¯ä¸€æ¬¾åˆ©ç”¨Mambaæ¶æ„çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå®ƒåœ¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åŸºç¡€ä¸Šå®ç°äº†æœ¬åœ°å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§çš„æœ‰æ•ˆå»ºæ¨¡ã€‚ä¸ä¼ ç»ŸåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ç›¸æ¯”ï¼ŒSamba ASRé€šè¿‡è§£å†³è¾“å…¥é•¿åº¦å’Œé•¿è·ç¦»ä¾èµ–æ€§é—®é¢˜ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSamba ASRåœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ï¼Œä¸”åœ¨ä½èµ„æºåœºæ™¯ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒMambaæ¶æ„çš„è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ä½¿Samba ASRæˆä¸ºå¤šæ ·åŒ–çš„ASRä»»åŠ¡çš„å¯æ‰©å±•å’Œç¨³å¥è§£å†³æ–¹æ¡ˆã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸçš„ä¼˜è¶Šæ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Samba ASRæ˜¯ä¸€ä¸ªåŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚</li>
<li>å®ƒé‡‡ç”¨Mambaæ¶æ„ï¼Œæ—¢ä½œä¸ºç¼–ç å™¨ä¹Ÿä½œä¸ºè§£ç å™¨ã€‚</li>
<li>ä¸åŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ç›¸æ¯”ï¼ŒSamba ASRé€šè¿‡å»ºæ¨¡æœ¬åœ°å’Œå…¨å±€æ—¶é—´ä¾èµ–æ€§å®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>Samba ASRè§£å†³äº†åŸºäºå˜å‹å™¨çš„æ¨¡å‹çš„å±€é™æ€§ï¼Œå¦‚è¾“å…¥é•¿åº¦çš„äºŒæ¬¡æ‰©å±•å’Œé•¿è·ç¦»ä¾èµ–æ€§çš„å¤„ç†å›°éš¾ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSamba ASRåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„å¼€æºåŸºäºå˜å‹å™¨çš„ASRæ¨¡å‹ã€‚</li>
<li>Samba ASRåœ¨ä½èµ„æºåœºæ™¯ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¸”å…·æœ‰è‰¯å¥½çš„è®¡ç®—æ•ˆç‡å’Œå‚æ•°ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ca006037fc2db0b7fffe8971c5d1af2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation"><a href="#A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation" class="headerlink" title="A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation"></a>A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation</h2><p><strong>Authors:Jingyuan Wang, Jie Zhang, Shihao Chen, Miao Sun</strong></p>
<p>Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under fixed-speaker conditions, but with a much lower computational cost and a certain degree of SCP capability. The reproducible code and audio examples are available at <a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN">https://github.com/jywanng/LBCCN</a>. </p>
<blockquote>
<p>åŒè€³è¯­éŸ³å¢å¼ºï¼ˆBSEï¼‰æ—¨åœ¨å…±åŒæé«˜å¬åŠ›è®¾å¤‡æ¥æ”¶åˆ°çš„å™ªå£°ä¿¡å·çš„è´¨é‡å’Œè¯­éŸ³æ¸…æ™°åº¦ï¼Œå¹¶ä¿ç•™ç›®æ ‡çš„ç©ºé—´çº¿ç´¢ä»¥å®ç°è‡ªç„¶è†å¬ã€‚ç°æœ‰æ–¹æ³•ç»å¸¸åœ¨å™ªå£°é™ä½ï¼ˆNRï¼‰èƒ½åŠ›å’Œç©ºé—´çº¿ç´¢ä¿ç•™ï¼ˆSCPï¼‰å‡†ç¡®æ€§ä¹‹é—´åšå‡ºå¦¥åï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„å£°å­¦åœºæ™¯ä¸­è®¡ç®—éœ€æ±‚è¾ƒé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„è½»é‡çº§åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰ï¼Œå®ƒé€šè¿‡è¿‡æ»¤ä½é¢‘å¸¦å®ç°å‡ºè‰²çš„NRæ•ˆæœå¹¶ä¿æŒå…¶ä½™éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜ç¡®åœ°ç»“åˆäº†é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ é€’å‡½æ•°çš„ä¼°è®¡ï¼Œä»¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸåº¦å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å›ºå®šè¯´è¯äººæ¡ä»¶ä¸‹ï¼Œæ‰€æå‡ºçš„LBCCNçš„NRæ€§èƒ½å¯ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸åª²ç¾ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ï¼Œå¹¶ä¸”å…·æœ‰ä¸€å®šçš„SCPèƒ½åŠ›ã€‚å¯å¤ç”¨çš„ä»£ç å’ŒéŸ³é¢‘ç¤ºä¾‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jywanng/LBCCNæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12444v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå­¦ä¹ çš„è½»é‡åŒ–åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³å¢å¼ºæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯æ—¨åœ¨æé«˜è¯­éŸ³è´¨é‡å’Œå¯æ‡‚åº¦ï¼ŒåŒæ—¶ä¿ç•™ç›®æ ‡çš„ç©ºé—´çº¿ç´¢ä»¥å®ç°è‡ªç„¶è†å¬ä½“éªŒã€‚é€šè¿‡è¿‡æ»¤ä½é¢‘å¸¦å¹¶ä¿æŒå…¶ä½™éƒ¨åˆ†å®ç°é™å™ªï¼ŒåŒæ—¶ä¼°è®¡é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°ä»¥ç¡®ä¿ç©ºé—´çº¿ç´¢ä¿çœŸå’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒLBCCNåœ¨å›ºå®šè¯´è¯äººæ¡ä»¶ä¸‹å…·æœ‰ç›¸å½“çš„é™å™ªæ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬æ›´ä½ï¼Œå¹¶å…·æœ‰ä¸€å®šçš„ç©ºé—´çº¿ç´¢ä¿ç•™èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Binaural speech enhancement (BSE)æ—¨åœ¨æé«˜å™ªå£°ä¿¡å·çš„è¯­éŸ³è´¨é‡å’Œå¯æ‡‚åº¦ï¼ŒåŒæ—¶ä¿ç•™è‡ªç„¶è†å¬æ—¶çš„ç©ºé—´çº¿ç´¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸åœ¨å™ªå£°å‡å°‘ï¼ˆNRï¼‰èƒ½åŠ›ã€ç©ºé—´çº¿ç´¢ä¿ç•™ï¼ˆSCPï¼‰ç²¾åº¦ä»¥åŠå¤æ‚å£°å­¦åœºæ™¯ä¸­çš„è®¡ç®—éœ€æ±‚ä¹‹é—´é¢ä¸´æƒè¡¡ã€‚</li>
<li>æå‡ºçš„åŸºäºå­¦ä¹ çš„è½»é‡åŒ–åŒè€³å¤æ‚å·ç§¯ç½‘ç»œï¼ˆLBCCNï¼‰é€šè¿‡è¿‡æ»¤ä½é¢‘å¸¦è¿›è¡Œé™å™ªï¼Œå¹¶ä¼°è®¡é€šé“é—´ç›¸å¯¹å£°å­¦ä¼ è¾“å‡½æ•°ä»¥ç¡®ä¿ç©ºé—´çº¿ç´¢çš„ä¿çœŸå’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>LBCCNå…·æœ‰ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„é™å™ªæ€§èƒ½ï¼Œä½†åœ¨å›ºå®šè¯´è¯äººæ¡ä»¶ä¸‹è®¡ç®—æˆæœ¬æ›´ä½ã€‚</li>
<li>LBCCNè¿˜å…·æœ‰ä¸€å®šçš„ç©ºé—´çº¿ç´¢ä¿ç•™èƒ½åŠ›ã€‚</li>
<li>å¯å¤ç”¨çš„ä»£ç å’ŒéŸ³é¢‘ç¤ºä¾‹å¯åœ¨æŒ‡å®šé“¾æ¥æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-066b92f13243890d65ee73efe45e55ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21ce27fbc4661dce67f3f7d85fdc007c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6583098319cc6828f446a99587a33183.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Channel-Aware-Domain-Adaptive-Generative-Adversarial-Network-for-Robust-Speech-Recognition"><a href="#Channel-Aware-Domain-Adaptive-Generative-Adversarial-Network-for-Robust-Speech-Recognition" class="headerlink" title="Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust   Speech Recognition"></a>Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust   Speech Recognition</h2><p><strong>Authors:Chien-Chun Wang, Li-Wei Chen, Cheng-Kang Chou, Hung-Shin Lee, Berlin Chen, Hsin-Min Wang</strong></p>
<p>While pre-trained automatic speech recognition (ASR) systems demonstrate impressive performance on matched domains, their performance often degrades when confronted with channel mismatch stemming from unseen recording environments and conditions. To mitigate this issue, we propose a novel channel-aware data simulation method for robust ASR training. Our method harnesses the synergistic power of channel-extractive techniques and generative adversarial networks (GANs). We first train a channel encoder capable of extracting embeddings from arbitrary audio. On top of this, channel embeddings are extracted using a minimal amount of target-domain data and used to guide a GAN-based speech synthesizer. This synthesizer generates speech that faithfully preserves the phonetic content of the input while mimicking the channel characteristics of the target domain. We evaluate our method on the challenging Hakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving relative character error rate (CER) reductions of 20.02% and 9.64%, respectively, compared to the baselines. These results highlight the efficacy of our channel-aware data simulation method for bridging the gap between source- and target-domain acoustics. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨åŒ¹é…é¢†åŸŸä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å½“é¢ä¸´ç”±æœªè§è¿‡çš„å½•éŸ³ç¯å¢ƒå’Œæ¡ä»¶å¯¼è‡´çš„é€šé“ä¸åŒ¹é…é—®é¢˜æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¨³å¥ASRè®­ç»ƒçš„æ–°å‹é€šé“æ„ŸçŸ¥æ•°æ®ä»¿çœŸæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†é€šé“æå–æŠ€æœ¯å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ååŒåŠ›é‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªèƒ½å¤Ÿä»ä»»æ„éŸ³é¢‘ä¸­æå–åµŒå…¥çš„é€šé“ç¼–ç å™¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½¿ç”¨å°‘é‡ç›®æ ‡åŸŸæ•°æ®æå–é€šé“åµŒå…¥ï¼Œå¹¶ç”¨äºæŒ‡å¯¼åŸºäºGANçš„è¯­éŸ³åˆæˆå™¨ã€‚è¯¥åˆæˆå™¨ç”Ÿæˆçš„è¯­éŸ³å¿ å®ä¿ç•™äº†è¾“å…¥è¯­éŸ³çš„è¯­éŸ³å†…å®¹ï¼ŒåŒæ—¶æ¨¡ä»¿ç›®æ ‡åŸŸçš„é€šé“ç‰¹æ€§ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¢å®¶è¯ï¼ˆHATï¼‰å’Œå°æ¹¾è¯ï¼ˆTATï¼‰è¯­æ–™åº“ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œç›¸å¯¹å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰åˆ†åˆ«é™ä½äº†20.02%å’Œ9.64%ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„é€šé“æ„ŸçŸ¥æ•°æ®ä»¿çœŸæ–¹æ³•åœ¨å¼¥åˆæºåŸŸå’Œç›®æ ‡åŸŸå£°å­¦ä¹‹é—´çš„å·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12386v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„é€šé“æ„ŸçŸ¥æ•°æ®ä»¿çœŸæ–¹æ³•ï¼Œç”¨äºå¢å¼ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„é²æ£’æ€§è®­ç»ƒã€‚è¯¥æ–¹æ³•ç»“åˆäº†é€šé“æå–æŠ€æœ¯å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜åŠ¿ï¼Œé€šè¿‡è®­ç»ƒé€šé“ç¼–ç å™¨æå–ä»»æ„éŸ³é¢‘çš„åµŒå…¥ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨å°‘é‡ç›®æ ‡åŸŸæ•°æ®æå–é€šé“åµŒå…¥æ¥å¼•å¯¼åŸºäºGANçš„è¯­éŸ³åˆæˆå™¨ã€‚åˆæˆå™¨ç”Ÿæˆçš„è¯­éŸ³æ—¢ä¿ç•™äº†è¾“å…¥è¯­éŸ³çš„è¯­éŸ³å†…å®¹ï¼Œåˆæ¨¡ä»¿äº†ç›®æ ‡åŸŸçš„é€šé“ç‰¹æ€§ã€‚åœ¨æŒ‘æˆ˜æ€§çš„å®¢å®¶è¯è·¨å°æ¹¾ï¼ˆHATï¼‰å’Œå°æ¹¾è·¨å°æ¹¾ï¼ˆTATï¼‰è¯­æ–™åº“ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰åˆ†åˆ«é™ä½äº†20.02%å’Œ9.64%ï¼Œå‡¸æ˜¾äº†è¯¥é€šé“æ„ŸçŸ¥æ•°æ®ä»¿çœŸæ–¹æ³•åœ¨å¼¥åˆæºåŸŸå’Œç›®æ ‡åŸŸå£°å­¦å·®å¼‚æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šé“æ„ŸçŸ¥æ•°æ®ä»¿çœŸæ–¹æ³•ç”¨äºå¢å¼ºASRç³»ç»Ÿçš„é²æ£’æ€§è®­ç»ƒã€‚</li>
<li>ç»“åˆäº†é€šé“æå–æŠ€æœ¯å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚</li>
<li>é€šè¿‡è®­ç»ƒé€šé“ç¼–ç å™¨æå–ä»»æ„éŸ³é¢‘çš„åµŒå…¥ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨å°‘é‡ç›®æ ‡åŸŸæ•°æ®æå–é€šé“åµŒå…¥æ¥å¼•å¯¼GAN-basedè¯­éŸ³åˆæˆå™¨ã€‚</li>
<li>åˆæˆå™¨ç”Ÿæˆçš„è¯­éŸ³æ—¢ä¿ç•™è¾“å…¥è¯­éŸ³å†…å®¹ï¼Œåˆæ¨¡ä»¿ç›®æ ‡åŸŸçš„é€šé“ç‰¹æ€§ã€‚</li>
<li>åœ¨å®¢å®¶è¯è·¨å°æ¹¾ï¼ˆHATï¼‰å’Œå°æ¹¾è·¨å°æ¹¾ï¼ˆTATï¼‰è¯­æ–™åº“ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-932cd87ee1e6df6ddce3db880ea655ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbf4c5f6ee2dc20f4942b5ea3619e42c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec36549d0b86aae3ffe5bd4d8228a0a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31fdfa2996590542127034399b38423e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module"><a href="#DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module" class="headerlink" title="DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module"></a>DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module</h2><p><strong>Authors:Xinyu Wang, Haotian Jiang, Haolin Huang, Yu Fang, Mengjie Xu, Qian Wang</strong></p>
<p>Speech recognition is the technology that enables machines to interpret and process human speech, converting spoken language into text or commands. This technology is essential for applications such as virtual assistants, transcription services, and communication tools. The Audio-Visual Speech Recognition (AVSR) model enhances traditional speech recognition, particularly in noisy environments, by incorporating visual modalities like lip movements and facial expressions. While traditional AVSR models trained on large-scale datasets with numerous parameters can achieve remarkable accuracy, often surpassing human performance, they also come with high training costs and deployment challenges. To address these issues, we introduce an efficient AVSR model that reduces the number of parameters through the integration of a Dual Conformer Interaction Module (DCIM). In addition, we propose a pre-training method that further optimizes model performance by selectively updating parameters, leading to significant improvements in efficiency. Unlike conventional models that require the system to independently learn the hierarchical relationship between audio and visual modalities, our approach incorporates this distinction directly into the model architecture. This design enhances both efficiency and performance, resulting in a more practical and effective solution for AVSR tasks. </p>
<blockquote>
<p>è¯­éŸ³è¯†åˆ«æ˜¯ä¸€é¡¹èƒ½å¤Ÿä½¿æœºå™¨è§£é‡Šå’Œå¤„ç†äººç±»è¯­éŸ³çš„æŠ€æœ¯ï¼Œå®ƒå°†å£è¯­è½¬åŒ–ä¸ºæ–‡æœ¬æˆ–å‘½ä»¤ã€‚å¯¹äºè™šæ‹ŸåŠ©ç†ã€è½¬å½•æœåŠ¡å’Œé€šä¿¡å·¥å…·ç­‰åº”ç”¨ç¨‹åºè€Œè¨€ï¼Œè¿™é¡¹æŠ€æœ¯è‡³å…³é‡è¦ã€‚è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹é€šè¿‡èå…¥è¯¸å¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ç­‰è§†è§‰æ¨¡å¼ï¼Œå¢å¼ºäº†ä¼ ç»Ÿè¯­éŸ³è¯†åˆ«çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ã€‚è™½ç„¶ç»è¿‡å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„ä¼ ç»ŸAVSRæ¨¡å‹å…·æœ‰è®¸å¤šå‚æ•°ï¼Œå¯ä»¥è·å¾—æƒŠäººçš„å‡†ç¡®æ€§ï¼Œç”šè‡³è¶…è¶Šäººç±»è¡¨ç°ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†é«˜æ˜‚çš„è®­ç»ƒæˆæœ¬å’Œéƒ¨ç½²æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé«˜æ•ˆçš„AVSRæ¨¡å‹ï¼Œé€šè¿‡é›†æˆåŒå·ç§¯äº¤äº’æ¨¡å—ï¼ˆDCIMï¼‰å‡å°‘äº†å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§æ›´æ–°å‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä»è€Œåœ¨æ•ˆç‡ä¸Šå–å¾—äº†æ˜¾ç€æé«˜ã€‚ä¸åŒäºä¼ ç»Ÿæ¨¡å‹éœ€è¦ç³»ç»Ÿç‹¬ç«‹å­¦ä¹ éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å°†è¿™ç§åŒºåˆ«çº³å…¥æ¨¡å‹æ¶æ„ä¸­ã€‚è¿™ç§è®¾è®¡æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ï¼Œä¸ºAVSRä»»åŠ¡æä¾›äº†æ›´å®ç”¨ã€æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00481v5">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong><br>    è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„è§†å¬è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥åŒè½¬æ¢å™¨äº¤äº’æ¨¡å—å’Œé¢„è®­ç»ƒæ–¹æ³•æ¥ä¼˜åŒ–æ€§èƒ½å’Œæ•ˆç‡ï¼Œä½¿å…¶åœ¨å˜ˆæ‚ç¯å¢ƒä¸­å®ç°é«˜æ•ˆã€å‡†ç¡®çš„è¯­éŸ³è¯†åˆ«ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥åŒè½¬æ¢å™¨äº¤äº’æ¨¡å—æ¥å‡å°‘å‚æ•°æ•°é‡ï¼Œæé«˜äº†è§†å¬è¯­éŸ³è¯†åˆ«çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§æ›´æ–°å‚æ•°æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹å°†è§†å¬æ¨¡æ€çš„å±‚æ¬¡å…³ç³»ç›´æ¥èå…¥æ¨¡å‹æ¶æ„ä¸­ï¼Œå¢å¼ºäº†æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>æ”¹è¿›åçš„æ¨¡å‹åœ¨å˜ˆæ‚ç¯å¢ƒä¸­å®ç°é«˜æ•ˆã€å‡†ç¡®çš„è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>è¯¥æ¨¡å‹å‡å°‘äº†è®­ç»ƒæˆæœ¬ï¼Œæé«˜äº†éƒ¨ç½²æ•ˆç‡ã€‚</li>
<li>è¯¥æ¨¡å‹æé«˜äº†è§†å¬è¯­éŸ³è¯†åˆ«çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abdbc4de40727de0d4409fb9fcb83a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c61bd461280aaa599e4e0945408115aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b83c3536ec993295c4b5cf18813237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f51746486c27928c98b9d2520d32fd6b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MEDSAGE-Enhancing-Robustness-of-Medical-Dialogue-Summarization-to-ASR-Errors-with-LLM-generated-Synthetic-Dialogues"><a href="#MEDSAGE-Enhancing-Robustness-of-Medical-Dialogue-Summarization-to-ASR-Errors-with-LLM-generated-Synthetic-Dialogues" class="headerlink" title="MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR   Errors with LLM-generated Synthetic Dialogues"></a>MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR   Errors with LLM-generated Synthetic Dialogues</h2><p><strong>Authors:Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler</strong></p>
<p>Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech into text, yet the errors they introduce can significantly degrade the performance of downstream tasks like summarization. This issue is particularly pronounced in clinical dialogue summarization, a low-resource domain where supervised data for fine-tuning is scarce, necessitating the use of ASR models as black-box solutions. Employing conventional data augmentation for enhancing the noise robustness of summarization models is not feasible either due to the unavailability of sufficient medical dialogue audio recordings and corresponding ASR transcripts. To address this challenge, we propose MEDSAGE, an approach for generating synthetic samples for data augmentation using Large Language Models (LLMs). Specifically, we leverage the in-context learning capabilities of LLMs and instruct them to generate ASR-like errors based on a few available medical dialogue examples with audio recordings. Experimental results show that LLMs can effectively model ASR noise, and incorporating this noisy data into the training process significantly improves the robustness and accuracy of medical dialogue summarization systems. This approach addresses the challenges of noisy ASR outputs in critical applications, offering a robust solution to enhance the reliability of clinical dialogue summarization. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å®ƒä»¬å¼•å…¥çš„é”™è¯¯ä¼šæ˜¾è‘—å½±å“ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦ï¼‰çš„æ€§èƒ½ã€‚è¿™ä¸€é—®é¢˜åœ¨ä¸´åºŠå¯¹è¯æ‘˜è¦ä¸­å°¤ä¸ºçªå‡ºï¼Œè¿™æ˜¯ä¸€ä¸ªèµ„æºåŒ®ä¹çš„é¢†åŸŸï¼Œç¼ºä¹å¾®è°ƒæ‰€éœ€çš„å¤§é‡ç›‘ç£æ•°æ®ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ASRæ¨¡å‹ä½œä¸ºé»‘ç®±è§£å†³æ–¹æ¡ˆã€‚ç”±äºç¼ºå°‘è¶³å¤Ÿçš„åŒ»å­¦å¯¹è¯éŸ³é¢‘å½•éŸ³å’Œç›¸åº”çš„ASRè½¬å½•ï¼Œå› æ­¤é‡‡ç”¨ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•æ¥æé«˜æ‘˜è¦æ¨¡å‹çš„æŠ—å™ªæ€§èƒ½å¹¶ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MEDSAGEæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆåˆæˆæ ·æœ¬è¿›è¡Œæ•°æ®å¢å¼ºçš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä»¥å°‘é‡å¯ç”¨çš„åŒ»å­¦å¯¹è¯ä¾‹å­ï¼ˆå¸¦æœ‰éŸ³é¢‘å½•éŸ³ï¼‰ä¸ºåŸºç¡€ï¼ŒæŒ‡å¯¼å®ƒä»¬ç”Ÿæˆç±»ä¼¼ASRçš„é”™è¯¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹ŸASRå™ªå£°ï¼Œå°†è¿™ç§å˜ˆæ‚çš„æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥æ˜¾è‘—æé«˜åŒ»å­¦å¯¹è¯æ‘˜è¦ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†å…³é”®åº”ç”¨ä¸­ASRè¾“å‡ºå™ªå£°çš„é—®é¢˜ï¼Œä¸ºæé«˜ä¸´åºŠå¯¹è¯æ‘˜è¦çš„å¯é æ€§æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14418v3">PDF</a> Accepted by the Thirty-Ninth AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¯¹å°†è¯­éŸ³è½¬åŒ–ä¸ºæ–‡æœ¬å…·æœ‰å…³é”®ä½œç”¨ï¼Œä½†å…¶äº§ç”Ÿçš„é”™è¯¯ä¼šå¯¹ä¸‹æ¸¸ä»»åŠ¡å¦‚æ‘˜è¦çš„ç”Ÿæˆæ€§èƒ½äº§ç”Ÿä¸¥é‡å½±å“ã€‚ç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠå¯¹è¯æ‘˜è¦è¿™ä¸€èµ„æºåŒ®ä¹çš„é¢†åŸŸï¼Œç”±äºç¼ºå°‘ç²¾ç»†è°ƒæ•´æ‰€éœ€çš„ç›‘ç£æ•°æ®ï¼Œå¿…é¡»ä¾èµ–ASRæ¨¡å‹ä½œä¸ºé»‘ç®±è§£å†³æ–¹æ¡ˆã€‚ç”±äºç¼ºå°‘è¶³å¤Ÿçš„åŒ»ç–—å¯¹è¯éŸ³é¢‘å½•éŸ³å’Œç›¸åº”çš„ASRè½¬å½•ï¼Œä½¿ç”¨ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•æé«˜æ‘˜è¦æ¨¡å‹çš„æŠ—å™ªå£°èƒ½åŠ›å¹¶ä¸å¯è¡Œã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºMEDSAGEæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆåˆæˆæ ·æœ¬è¿›è¡Œæ•°æ®å¢å¼ºã€‚æˆ‘ä»¬åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæ ¹æ®å°‘é‡å¯ç”¨çš„å¸¦éŸ³é¢‘å½•éŸ³çš„åŒ»ç–—å¯¹è¯ç¤ºä¾‹ï¼ŒæŒ‡å¯¼å®ƒä»¬ç”Ÿæˆç±»ä¼¼ASRçš„é”™è¯¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆæ¨¡æ‹ŸASRå™ªå£°ï¼Œå°†è¿™ç±»å™ªå£°æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹ï¼Œèƒ½æ˜¾è‘—æé«˜åŒ»ç–—å¯¹è¯æ‘˜è¦ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†å™ªå£°ASRè¾“å‡ºåœ¨å…³é”®åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œä¸ºæé«˜ä¸´åºŠå¯¹è¯æ‘˜è¦çš„å¯é æ€§æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿåœ¨è½¬å½•è¯­éŸ³ä¸ºæ–‡æœ¬æ—¶çš„é‡è¦æ€§åŠå…¶å¯èƒ½äº§ç”Ÿçš„é”™è¯¯å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ã€‚</li>
<li>ä¸´åºŠå¯¹è¯æ‘˜è¦æ˜¯ä¸€ä¸ªèµ„æºåŒ®ä¹çš„é¢†åŸŸï¼Œéœ€è¦è§£å†³ASRæ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•å› ç¼ºä¹åŒ»ç–—å¯¹è¯éŸ³é¢‘å½•éŸ³å’Œç›¸åº”çš„ASRè½¬å½•è€Œä¸å¯è¡Œã€‚</li>
<li>MEDSAGEæ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆæ ·æœ¬è¿›è¡Œæ•°æ®å¢å¼ºã€‚</li>
<li>LLMså¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹ŸASRå™ªå£°ã€‚</li>
<li>å°†å™ªå£°æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹å¯ä»¥æé«˜åŒ»ç–—å¯¹è¯æ‘˜è¦ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.14418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d62b5b4fca929b8e4ace0fdd4dcf3b5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3828a3fcc22da0994a726b03755e92a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d06a3ceadc3bc6f9e5ad1a1e2b2c5b77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81343766147d9871335e110270d5bfad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdcecd4922015328507b0f67e052583e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975eb3ef9e2e682bb195734411d10f1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79746d1b63469ea30f56a90286148c6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d920b4aab26d6002fb3f7a1c0b6522d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Prosody-Analysis-of-Audiobooks"><a href="#Prosody-Analysis-of-Audiobooks" class="headerlink" title="Prosody Analysis of Audiobooks"></a>Prosody Analysis of Audiobooks</h2><p><strong>Authors:Charuta Pethe, Bach Pham, Felix D Childress, Yunting Yin, Steven Skiena</strong></p>
<p>Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—ä»æ–‡æœ¬ç”Ÿæˆè‡ªç„¶å£°éŸ³çš„éŸ³é¢‘æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œæœ‰å£°ä¹¦çš„å™è¿°æ¶‰åŠè¯»è€…çš„å¤§èƒ†å‘å£°å’Œè¯­è°ƒå˜åŒ–ï¼Œæ›´åŠ ä¾èµ–äºå™è¿°ä¸­çš„æƒ…æ„Ÿã€å¯¹è¯å’Œæè¿°ã€‚ä½¿ç”¨æˆ‘ä»¬93æœ¬å·²å¯¹é½çš„ä¹¦ç±ä¸æœ‰å£°ä¹¦çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†é‡‡ç”¨è¯­è¨€å»ºæ¨¡æ¥é¢„æµ‹å™è¿°æ–‡æœ¬ä¸­çš„éŸµå¾‹å±æ€§ï¼ˆéŸ³è°ƒã€éŸ³é‡å’Œè¯­é€Ÿï¼‰çš„æ”¹è¿›æ¨¡å‹ã€‚æˆ‘ä»¬çš„é¢„æµ‹éŸµå¾‹å±æ€§ä¸äººç±»æœ‰å£°ä¹¦é˜…è¯»ä¹‹é—´çš„ç›¸å…³æ€§è¦å¥½äºæœ€å…ˆè¿›å•†ä¸šTTSç³»ç»Ÿçš„ç»“æœï¼šåœ¨æˆ‘ä»¬é¢„æµ‹çš„éŸ³è°ƒæ–¹é¢ï¼Œæœ‰22æœ¬å›¾ä¹¦ä¸äººçš„é˜…è¯»é«˜åº¦ç›¸å…³ï¼›é¢„æµ‹éŸ³é‡å±æ€§æ–¹é¢ï¼Œæœ‰23æœ¬å›¾ä¹¦ä¸äººçš„é˜…è¯»é«˜åº¦ç›¸ä¼¼ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹äººç±»è¯„ä¼°ç ”ç©¶ï¼Œé‡åŒ–äººä»¬å¯¹äºå¸¦éŸµå¾‹å¢å¼ºåŠŸèƒ½çš„æœ‰å£°ä¹¦é˜…è¯»ä¸å•†ä¸šæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿåå¥½ç¨‹åº¦å¦‚ä½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06930v3">PDF</a> Accepted to IEEE ICSC 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œä»æ–‡æœ¬ç”Ÿæˆè‡ªç„¶éŸ³é¢‘å·²æˆä¸ºå¯èƒ½ã€‚é’ˆå¯¹æœ‰å£°ä¹¦æœ—è¯»ä¸­éœ€è¦æ›´å¤šæƒ…æ„Ÿå’Œå™è¿°æ€§çš„ç‰¹ç‚¹ï¼Œæœ¬æ–‡ä½¿ç”¨åŒ…å«93ä¸ªå¯¹é½çš„ä¹¦ç±-æœ‰å£°ä¹¦å¯¹æ•°æ®é›†ï¼Œæå‡ºäº†æ”¹è¿›çš„è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹è¯­è°ƒå±æ€§ï¼ˆéŸ³è°ƒã€éŸ³é‡å’Œè¯­é€Ÿï¼‰ã€‚é¢„æµ‹ç»“æœä¸çœŸäººæœ—è¯»é«˜åº¦ä¸€è‡´ï¼Œç›¸è¾ƒäºå…ˆè¿›çš„å•†ä¸šæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿæœ‰æ›´å¥½çš„è¡¨ç°ã€‚å¹¶è¿›è¡Œäº†äººç±»è¯„ä¼°å®éªŒï¼Œå®šé‡ç ”ç©¶äº†äººä»¬å¯¹äºæœ‰è¯­è°ƒå¢å¼ºåŠŸèƒ½çš„æœ‰å£°ä¹¦é˜…è¯»çš„åå¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯ç°åœ¨å¯ä»¥ç”Ÿæˆè‡ªç„¶çš„éŸ³é¢‘ã€‚</li>
<li>æœ‰å£°ä¹¦æœ—è¯»æ¶‰åŠæ›´å¤šçš„æƒ…æ„Ÿå’Œå™è¿°æ€§å…ƒç´ ã€‚</li>
<li>ä½¿ç”¨è¯­è¨€æ¨¡å‹å¯¹è¯­è°ƒå±æ€§ï¼ˆéŸ³è°ƒã€éŸ³é‡å’Œè¯­é€Ÿï¼‰è¿›è¡Œé¢„æµ‹æ”¹è¿›ã€‚</li>
<li>å¯¹æ¯”å•†ä¸šæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿï¼Œæ”¹è¿›æ¨¡å‹çš„é¢„æµ‹ç»“æœæ›´æ¥è¿‘çœŸäººæœ—è¯»ã€‚</li>
<li>åœ¨é¢„æµ‹éŸ³è°ƒæ–¹é¢ï¼Œæ”¹è¿›æ¨¡å‹åœ¨22æœ¬ä¹¦ä¸­çš„è¡¨ç°ä¼˜äºçœŸäººæœ—è¯»ã€‚</li>
<li>åœ¨é¢„æµ‹éŸ³é‡æ–¹é¢ï¼Œæ”¹è¿›æ¨¡å‹åœ¨å¤§å¤šæ•°ä¹¦ç±ä¸­çš„è¡¨ç°æ¥è¿‘çœŸäººæœ—è¯»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.06930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f8f17572fedf5f32310e180ac1a1e88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3596c3c6ad7af43de6496044233d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a2c3a8ae2f91eeb29c249e6f63faf7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef0acaa420db7f2760bb9d8cfea97aa4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab4d8f8b25b14eee3cec0f768c2d098a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df777e96e2b2050984a94830cb4f46e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-937f86509cb704fce76a372eea7deed9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ab72640571f530ee23df85187d0322f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-10eefef6b7dcbf914cf52bb1f2afbec4.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  iFADIT Invertible Face Anonymization via Disentangled Identity   Transform
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-94858bbfe4437e939e6ab782219e63b7.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  Boosting Salient Object Detection with Knowledge Distillated from Large   Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
