<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-10  FleSpeech Flexibly Controllable Speech Generation with Various Prompts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d06a3ceadc3bc6f9e5ad1a1e2b2c5b77.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-10-更新"><a href="#2025-01-10-更新" class="headerlink" title="2025-01-10 更新"></a>2025-01-10 更新</h1><h2 id="FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts"><a href="#FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts" class="headerlink" title="FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"></a>FleSpeech: Flexibly Controllable Speech Generation with Various Prompts</h2><p><strong>Authors:Hanzhao Li, Yuke Li, Xinsheng Wang, Jingbin Hu, Qicong Xie, Shan Yang, Lei Xie</strong></p>
<p>Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker’s timbre, or choosing a style and generating a voice that matches a character’s visual appearance. To overcome these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at <a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech/">https://kkksuper.github.io/FleSpeech/</a> </p>
<blockquote>
<p>传统的可控语音生成方法通常依赖于单一或固定的提示，这限制了创造性和灵活性。这些局限使得在某些场景下难以满足特定用户的需求，例如在保留选定演讲者音质的同时调整风格，或者选择风格并生成与角色视觉外观相匹配的语音。为了克服这些挑战，我们提出了FleSpeech，这是一个新型的多阶段语音生成框架，通过整合各种控制形式，允许更灵活地操作语音属性。FleSpeech采用多模态提示编码器，处理和统一不同的文本、音频和视觉提示，形成连贯的表示。这种方法增强了语音合成的适应性，支持对生成的语音进行创造性和精确的控制。此外，我们还为开发多模态数据集的数据收集管道，以促进该领域的进一步研究和应用。综合的主观和客观实验证明了FleSpeech的有效性。音频样本可在<a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech/%E6%89%BE%E5%88%B0%E3%80%82">https://kkksuper.github.io/FleSpeech/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04644v1">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong><br>文本主要提出了一种新型的多阶段语音生成框架——FleSpeech，该框架解决了传统可控语音生成方法中的限制，如单一或固定的提示方式。FleSpeech通过整合多种形式的控制，实现了语音属性的灵活操控，能够处理并统一文本、音频和视觉提示，从而增强语音合成的适应性并支持对生成语音的创造性与精确控制。此外，还介绍了FleSpeech的多模态数据集收集流程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FleSpeech解决了传统可控语音生成方法的局限性，如单一或固定的提示方式。</li>
<li>FleSpeech采用多阶段框架，实现更灵活的语音属性操控。</li>
<li>FleSpeech能够处理并统一文本、音频和视觉提示，增强语音合成的适应性。</li>
<li>FleSpeech支持创造性与精确控制生成的语音。</li>
<li>FleSpeech采用多模态数据集收集流程，有助于该领域的研究与应用。</li>
<li>FleSpeech的有效性通过主观和客观实验得到了验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dbedcac4125baeefcfaf324eec1523ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682fe78391efffab7b91b971a10ef258.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Right-Label-Context-in-End-to-End-Training-of-Time-Synchronous-ASR-Models"><a href="#Right-Label-Context-in-End-to-End-Training-of-Time-Synchronous-ASR-Models" class="headerlink" title="Right Label Context in End-to-End Training of Time-Synchronous ASR   Models"></a>Right Label Context in End-to-End Training of Time-Synchronous ASR   Models</h2><p><strong>Authors:Tina Raissi, Ralf Schlter, Hermann Ney</strong></p>
<p>Current time-synchronous sequence-to-sequence automatic speech recognition (ASR) models are trained by using sequence level cross-entropy that sums over all alignments. Due to the discriminative formulation, incorporating the right label context into the training criterion’s gradient causes normalization problems and is not mathematically well-defined. The classic hybrid neural network hidden Markov model (NN-HMM) with its inherent generative formulation enables conditioning on the right label context. However, due to the HMM state-tying the identity of the right label context is never modeled explicitly. In this work, we propose a factored loss with auxiliary left and right label contexts that sums over all alignments. We show that the inclusion of the right label context is particularly beneficial when training data resources are limited. Moreover, we also show that it is possible to build a factored hybrid HMM system by relying exclusively on the full-sum criterion. Experiments were conducted on Switchboard 300h and LibriSpeech 960h. </p>
<blockquote>
<p>当前的时间同步序列到序列自动语音识别（ASR）模型是通过使用序列级别的交叉熵（对所有对齐进行求和）来进行训练的。由于采用判别式公式，将正确的标签上下文纳入训练标准的梯度中会导致归一化问题，并且在数学上定义不明确。经典的混合神经网络隐马尔可夫模型（NN-HMM）凭借其内在的生成式公式，能够实现基于正确的标签上下文进行条件设置。然而，由于HMM的状态绑定，正确的标签上下文的身份从未被显式建模。在这项工作中，我们提出了一种带有辅助的左和右标签上下文的分解损失函数，对所有对齐进行求和。我们表明，在训练数据资源有限的情况下，引入正确的标签上下文特别有益。此外，我们还表明，仅依赖全和准则来构建分解混合HMM系统也是可能的。实验在Switchboard 300小时和LibriSpeech 960小时数据集上进行。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04521v1">PDF</a> Accepted for presentation at ICASSP 2025</p>
<p><strong>Summary</strong><br>     当前时序同步的序列到序列语音识别模型使用序列级别的交叉熵进行训练，该交叉熵对所有对齐进行求和。由于判别式公式的问题，将正确的标签上下文纳入训练标准的梯度中会导致归一化问题，且在数学上未明确定义。经典的混合神经网络隐马尔可夫模型（NN-HMM）具有其固有的生成式表述，可以基于正确的标签上下文进行条件处理。但由于HMM的状态绑定，并未明确建模正确的标签上下文的身份。在这项工作中，我们提出了一种带有辅助左右标签上下文的分解损失，该损失对所有对齐进行求和。我们表明，在训练数据资源有限的情况下，引入正确的标签上下文特别有益。此外，我们还展示了仅依靠全和准则来构建分解混合HMM系统的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前ASR模型使用序列级别交叉熵进行训练，对所有对齐求和。</li>
<li>判别式公式将正确的标签上下文纳入训练标准的梯度会引发问题。</li>
<li>经典的NN-HMM模型可以基于正确的标签上下文进行条件处理，但未明确建模其身份。</li>
<li>提出了一种带有辅助左右标签上下文的分解损失。</li>
<li>引入正确的标签上下文在训练数据有限的情况下特别有益。</li>
<li>仅依靠全和准则可以构建分解混合HMM系统。</li>
<li>实验在Switchboard 300h和LibriSpeech 960h数据集上进行。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04521">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4032f82988921cd10a90fdbe243064bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d200467c6c9ab9497e1faffd6671107.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ZSVC-Zero-shot-Style-Voice-Conversion-with-Disentangled-Latent-Diffusion-Models-and-Adversarial-Training"><a href="#ZSVC-Zero-shot-Style-Voice-Conversion-with-Disentangled-Latent-Diffusion-Models-and-Adversarial-Training" class="headerlink" title="ZSVC: Zero-shot Style Voice Conversion with Disentangled Latent   Diffusion Models and Adversarial Training"></a>ZSVC: Zero-shot Style Voice Conversion with Disentangled Latent   Diffusion Models and Adversarial Training</h2><p><strong>Authors:Xinfa Zhu, Lei He, Yujia Xiao, Xi Wang, Xu Tan, Sheng Zhao, Lei Xie</strong></p>
<p>Style voice conversion aims to transform the speaking style of source speech into a desired style while keeping the original speaker’s identity. However, previous style voice conversion approaches primarily focus on well-defined domains such as emotional aspects, limiting their practical applications. In this study, we present ZSVC, a novel Zero-shot Style Voice Conversion approach that utilizes a speech codec and a latent diffusion model with speech prompting mechanism to facilitate in-context learning for speaking style conversion. To disentangle speaking style and speaker timbre, we introduce information bottleneck to filter speaking style in the source speech and employ Uncertainty Modeling Adaptive Instance Normalization (UMAdaIN) to perturb the speaker timbre in the style prompt. Moreover, we propose a novel adversarial training strategy to enhance in-context learning and improve style similarity. Experiments conducted on 44,000 hours of speech data demonstrate the superior performance of ZSVC in generating speech with diverse speaking styles in zero-shot scenarios. </p>
<blockquote>
<p>风格语音转换旨在将源语音的讲话风格转换为所需的风格，同时保持原始说话者的身份。然而，以前的风格语音转换方法主要集中在情绪等方面等定义明确的领域，限制了其实际应用。本研究提出了ZSVC，这是一种利用语音编解码器和带有语音提示机制的潜在扩散模型的新型零风格语音转换方法，以促进说话风格转换的上下文学习。为了解开说话风格和说话者音色，我们引入信息瓶颈来过滤源语音中的说话风格，并采用不确定性建模自适应实例归一化（UMAdaIN）来扰动风格提示中的说话者音色。此外，我们提出了一种新型对抗训练策略，以增强上下文学习并提高风格相似性。在44000小时语音数据进行的实验表明，ZSVC在零样本场景下生成具有多种说话风格的语音方面表现出卓越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04416v1">PDF</a> 5 pages, 3 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>文本主要介绍了零样本风格语音转换方法（ZSVC）。该方法利用语音编解码器和潜在扩散模型，结合说话风格提示，进行风格语音转换。研究提出了信息瓶颈和不确定性建模自适应实例归一化（UMAdaIN）等技术来处理说话风格和发音特性的转换问题。通过提出的对抗训练策略，提高在零样本场景下的风格相似性和学习表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ZSVC是一种零样本风格语音转换方法，专注于转换源语音的说话风格为期望的风格，同时保持原始说话者的身份。</li>
<li>利用语音编解码器和潜在扩散模型进行说话风格转换。</li>
<li>采用信息瓶颈技术来分离说话风格和发音特性。</li>
<li>使用UMAdaIN技术来干扰说话风格提示中的发音特性。</li>
<li>提出对抗训练策略以提高零样本场景下的风格相似性和学习表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04416">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4bc43c0640e871caa449c189f72daa4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28fdfd640522705d8c8342003539461b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04ab1b46fa02c3265974d9558a9ab974.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-521d2a5cc5b94d15486590e8c1258235.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee53f6cc307320acbf4158bb979f794.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57d2b7be1706f5012f5d8a06c7833270.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Phone-purity-Guided-Discrete-Tokens-for-Dysarthric-Speech-Recognition"><a href="#Phone-purity-Guided-Discrete-Tokens-for-Dysarthric-Speech-Recognition" class="headerlink" title="Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition"></a>Phone-purity Guided Discrete Tokens for Dysarthric Speech Recognition</h2><p><strong>Authors:Huimeng Wang, Xurong Xie, Mengzhe Geng, Shujie Hu, Haoning Xu, Youjun Chen, Zhaoqing Li, Jiajun Deng, Xunying Liu</strong></p>
<p>Discrete tokens extracted provide efficient and domain adaptable speech features. Their application to disordered speech that exhibits articulation imprecision and large mismatch against normal voice remains unexplored. To improve their phonetic discrimination that is weakened during unsupervised K-means or vector quantization of continuous features, this paper proposes novel phone-purity guided (PPG) discrete tokens for dysarthric speech recognition. Phonetic label supervision is used to regularize maximum likelihood and reconstruction error costs used in standard K-means and VAE-VQ based discrete token extraction. Experiments conducted on the UASpeech corpus suggest that the proposed PPG discrete token features extracted from HuBERT consistently outperform hybrid TDNN and End-to-End (E2E) Conformer systems using non-PPG based K-means or VAE-VQ tokens across varying codebook sizes by statistically significant word error rate (WER) reductions up to 0.99% and 1.77% absolute (3.21% and 4.82% relative) respectively on the UASpeech test set of 16 dysarthric speakers. The lowest WER of 23.25% was obtained by combining systems using different token features. Consistent improvements on the phone purity metric were also achieved. T-SNE visualization further demonstrates sharper decision boundaries were produced between K-means&#x2F;VAE-VQ clusters after introducing phone-purity guidance. </p>
<blockquote>
<p>提取的离散令牌提供高效且适应于特定领域的语音特征。它们在针对发音不精确以及与正常声音存在较大差异的混乱语音中的应用尚未得到探索。为了提高在连续特征的无监督K-means或向量量化过程中被削弱的语音辨别能力，本文提出了用于口齿不清的语音识别的新型电话纯净度引导（PPG）离散令牌。语音标签监督用于规范最大似然和重建误差成本，这些成本用于基于标准K-means和VAE-VQ的离散令牌提取。在UASpeech语料库上进行的实验表明，从HuBERT中提取的PPG离散令牌特征始终优于使用非PPG的K-means或VAE-VQ令牌的混合TDNN和端到端（E2E）Conformer系统。在UASpeech的16位口齿不清的说话者的测试集上，通过统计显著的字错误率（WER）减少高达绝对值的0.99％和1.77％（相对值分别为3.21％和4.82％）。通过将使用不同令牌特征的系统进行组合，获得了最低的WER为23.25％。在电话纯净度指标方面也取得了持续的改进。T-SNE可视化进一步证明了引入电话纯净度指导后，K-means&#x2F;VAE-VQ集群之间的决策边界更加清晰。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04379v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong><br>离散语音特征提取技术对于语音领域具有高效性和适应性。针对发音不精确、与正常语音存在较大差异的障碍性语音，本文提出一种新型的电话纯净度引导（PPG）离散令牌技术用于识别。通过利用语音标签监督来规范最大似然和重建误差成本，改善传统的K均值和基于VAE-VQ的离散令牌提取中的语音辨识问题。在UASpeech语料库上的实验表明，采用PPG离散令牌特征的HuBERT系统优于混合TDNN和端到端（E2E）Conformer系统，在代码本大小不同的情况下，相对字词错误率（WER）绝对降低达1.77%，相对降低达4.82%。结合不同令牌特征的系统可获得最低WER为23.25%。同时，电话纯净度指标也有显著提高。T-SNE可视化进一步证明引入电话纯净度指导后，K均值&#x2F; VAE-VQ集群之间的决策边界更加清晰。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>离散语音特征提取技术对于语音领域具有高效性和适应性。</li>
<li>针对障碍性语音（如发音不精确等），本文提出了一种电话纯净度引导（PPG）离散令牌技术用于识别。</li>
<li>通过语音标签监督改善传统的K均值和基于VAE-VQ的离散令牌提取中的语音辨识问题。</li>
<li>在UASpeech语料库的实验中，采用PPG离散令牌特征的HuBERT系统表现优异，相对于其他系统有明显降低的字词错误率（WER）。</li>
<li>结合不同令牌特征的系统可以获得更低的WER。</li>
<li>电话纯净度指标显著提高，证明PPG离散令牌技术对于语音识别的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c478a236d32b4221878017f8ad95f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-540d839d57a235589d279f477cf0a3e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95d0a83e2ac9e401214805042779b478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51a849d39581e6f58775db5f9fd54192.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-168b0be0b2d932a705519783e505e34c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LipGen-Viseme-Guided-Lip-Video-Generation-for-Enhancing-Visual-Speech-Recognition"><a href="#LipGen-Viseme-Guided-Lip-Video-Generation-for-Enhancing-Visual-Speech-Recognition" class="headerlink" title="LipGen: Viseme-Guided Lip Video Generation for Enhancing Visual Speech   Recognition"></a>LipGen: Viseme-Guided Lip Video Generation for Enhancing Visual Speech   Recognition</h2><p><strong>Authors:Bowen Hao, Dongliang Zhou, Xiaojie Li, Xingyu Zhang, Liang Xie, Jianlong Wu, Erwei Yin</strong></p>
<p>Visual speech recognition (VSR), commonly known as lip reading, has garnered significant attention due to its wide-ranging practical applications. The advent of deep learning techniques and advancements in hardware capabilities have significantly enhanced the performance of lip reading models. Despite these advancements, existing datasets predominantly feature stable video recordings with limited variability in lip movements. This limitation results in models that are highly sensitive to variations encountered in real-world scenarios. To address this issue, we propose a novel framework, LipGen, which aims to improve model robustness by leveraging speech-driven synthetic visual data, thereby mitigating the constraints of current datasets. Additionally, we introduce an auxiliary task that incorporates viseme classification alongside attention mechanisms. This approach facilitates the efficient integration of temporal information, directing the model’s focus toward the relevant segments of speech, thereby enhancing discriminative capabilities. Our method demonstrates superior performance compared to the current state-of-the-art on the lip reading in the wild (LRW) dataset and exhibits even more pronounced advantages under challenging conditions. </p>
<blockquote>
<p>视觉语音识别（VSR），也称为唇读，由于其广泛的实际应用而备受关注。深度学习技术的出现和硬件能力的进步极大地提高了唇读模型的性能。尽管有这些进步，现有数据集主要以稳定的视频录制为主，唇动变化有限。这一局限性导致模型对现实场景中遇到的变动非常敏感。为了解决这一问题，我们提出了一个新型框架LipGen，旨在利用语音驱动合成视觉数据，提高模型的稳健性，从而缓解当前数据集的约束。此外，我们引入了一个辅助任务，结合viseme分类和注意力机制。这种方法有助于有效地整合时间信息，引导模型关注语音的相关片段，从而提高辨别能力。我们的方法在唇读领域的LRW数据集上展现了卓越的性能，并在具有挑战性的条件下表现出更为明显的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04204v1">PDF</a> This paper has been accepted for presentation at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>视觉语音识别（VSR），俗称唇读技术，因其广泛的实践应用而备受关注。随着深度学习技术的发展和硬件能力的进步，唇读模型的性能得到了显著提升。然而，现有数据集主要以稳定的视频录制为主，唇动变化有限，导致模型对真实场景中的变化高度敏感。为解决这一问题，我们提出了LipGen这一新型框架，旨在利用语音驱动合成视觉数据，提升模型的稳健性，以缓解当前数据集的约束。此外，我们还引入了一项辅助任务，结合viseme分类和注意力机制，以高效地整合时序信息，引导模型关注于语音的关键片段，从而提升其辨别能力。相较于现有的唇读技术，我们的方法在唇读野（LRW）数据集上表现出卓越的性能，并在具有挑战性的环境下优势更为明显。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语音识别（VSR）技术因其实用性受到广泛关注。</li>
<li>现有数据集主要依赖稳定视频录制，限制了模型的适应性。</li>
<li>LipGen框架旨在提高模型稳健性，通过利用语音驱动的合成视觉数据缓解数据约束。</li>
<li>引入viseme分类和注意力机制的辅助任务，以提高模型的辨别能力。</li>
<li>方法在唇读野（LRW）数据集上表现优越。</li>
<li>在具有挑战性的环境下，该方法较现有技术更具优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f7bf530835b274ffa416d3a7c78b88e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a31488feaafd63b636b5de76af0c5f22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d91a4cbfbe72f01bac16f09373078e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d3639caf65bc2dfce3ef688e994e8e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Listening-and-Seeing-Again-Generative-Error-Correction-for-Audio-Visual-Speech-Recognition"><a href="#Listening-and-Seeing-Again-Generative-Error-Correction-for-Audio-Visual-Speech-Recognition" class="headerlink" title="Listening and Seeing Again: Generative Error Correction for Audio-Visual   Speech Recognition"></a>Listening and Seeing Again: Generative Error Correction for Audio-Visual   Speech Recognition</h2><p><strong>Authors:Rui Liu, Hongyu Yuan, Haizhou Li</strong></p>
<p>Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech Recognition (AVSR) takes audio and visual signals simultaneously to infer the transcription. Recent studies have shown that Large Language Models (LLMs) can be effectively used for Generative Error Correction (GER) in ASR by predicting the best transcription from ASR-generated N-best hypotheses. However, these LLMs lack the ability to simultaneously understand audio and visual, making the GER approach challenging to apply in AVSR. In this work, we propose a novel GER paradigm for AVSR, termed AVGER, that follows the concept of &#96;&#96;listening and seeing again’’. Specifically, we first use the powerful AVSR system to read the audio and visual signals to get the N-Best hypotheses, and then use the Q-former-based Multimodal Synchronous Encoder to read the audio and visual information again and convert them into an audio and video compression representation respectively that can be understood by LLM. Afterward, the audio-visual compression representation and the N-Best hypothesis together constitute a Cross-modal Prompt to guide the LLM in producing the best transcription. In addition, we also proposed a Multi-Level Consistency Constraint training criterion, including logits-level, utterance-level and representations-level, to improve the correction accuracy while enhancing the interpretability of audio and visual compression representations. The experimental results on the LRS3 dataset show that our method outperforms current mainstream AVSR systems. The proposed AVGER can reduce the Word Error Rate (WER) by 24% compared to them. Code and models can be found at: <a target="_blank" rel="noopener" href="https://github.com/CircleRedRain/AVGER">https://github.com/CircleRedRain/AVGER</a>. </p>
<blockquote>
<p>与传统的自动语音识别（ASR）不同，视听语音识别（AVSR）同时处理音频和视觉信号来进行转录。最近的研究表明，大型语言模型（LLM）可以通过预测ASR生成的N-best假设中的最佳转录，有效地用于自动语音识别中的生成错误校正（GER）。然而，这些LLM无法同时理解音频和视觉信号，使得在AVSR中应用GER方法具有挑战性。在这项工作中，我们提出了一种新型的AVSR的GER方法，称为AVGER，它遵循“再次聆听和观看”的概念。具体来说，我们首先使用强大的AVSR系统读取音频和视觉信号，以获得N-best假设，然后使用基于Q-former的多模态同步编码器再次读取音频和视觉信息，并将它们转换为LLM可以理解的音频和视频压缩表示形式。之后，视听压缩表示和N-best假设共同构成一个跨模态提示，引导LLM生成最佳转录。此外，我们还提出了多级别一致性约束训练准则，包括logits级别、话语级别和表示级别，以提高校正准确性，同时增强视听压缩表示的可解释性。在LRS3数据集上的实验结果表明，我们的方法优于当前主流的AVSR系统。与它们相比，所提出的AVGER可以将单词错误率（WER）降低24%。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/CircleRedRain/AVGER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CircleRedRain/AVGER找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04038v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出了一种针对音频视觉语音识别（AVSR）的生成式误差校正（AVGER）新方法。该方法结合音频和视觉信号，利用强大的AVSR系统获取N-best假设，再通过Q-former基多模态同步编码器对音频和视觉信息进行二次处理，转化为可被大型语言模型（LLM）理解的音频视觉压缩表示。此外，还提出了多级别一致性约束训练准则，以提高校正精度并增强音频和视觉压缩表示的可解释性。在LRS3数据集上的实验结果表明，该方法优于主流AVSR系统，可降低24%的单词错误率。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>AVGER方法结合了音频和视觉信号进行语音识别，提高了识别的准确性。</li>
<li>利用强大的AVSR系统获取N-best假设，为生成式误差校正（GER）提供了更丰富的信息。</li>
<li>Q-former基多模态同步编码器能够将音频和视觉信息转化为可理解的压缩表示，便于LLM进行处理。</li>
<li>提出多级别一致性约束训练准则，旨在提高校正精度并增强音频和视觉表示的可解释性。</li>
<li>AVGER方法在LRS3数据集上的表现优于主流AVSR系统。</li>
<li>AVGER方法可降低单词错误率（WER）24%，显示出其在实际应用中的有效性。</li>
<li>研究的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/CircleRedRain/AVGER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CircleRedRain/AVGER找到。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-09d69c4484a4853336ab0fabd18070cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6d4b7418f9149bd5d1b302f5c8a0dce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a4302191f716822abaaa245f35c2977.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40abe0392690b40b4bac30378b6442fb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models"><a href="#Samba-ASR-State-Of-The-Art-Speech-Recognition-Leveraging-Structured-State-Space-Models" class="headerlink" title="Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models"></a>Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured   State-Space Models</h2><p><strong>Authors:Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi</strong></p>
<p>We propose Samba ASR,the first state of the art Automatic Speech Recognition(ASR)model leveraging the novel Mamba architecture as both encoder and decoder,built on the foundation of state space models(SSMs).Unlike transformerbased ASR models,which rely on self-attention mechanisms to capture dependencies,Samba ASR effectively models both local and global temporal dependencies using efficient statespace dynamics,achieving remarkable performance gains.By addressing the limitations of transformers,such as quadratic scaling with input length and difficulty in handling longrange dependencies,Samba ASR achieves superior accuracy and efficiency.Experimental results demonstrate that Samba ASR surpasses existing opensource transformerbased ASR models across various standard benchmarks,establishing it as the new state of theart in ASR.Extensive evaluations on the benchmark dataset show significant improvements in Word Error Rate(WER),with competitive performance even in lowresource scenarios.Furthermore,the inherent computational efficiency and parameter optimization of the Mamba architecture make Samba ASR a scalable and robust solution for diverse ASR tasks.Our contributions include the development of a new Samba ASR architecture for automatic speech recognition(ASR),demonstrating the superiority of structured statespace models(SSMs)over transformer based models for speech sequence processing.We provide a comprehensive evaluation on public benchmarks,showcasing stateoftheart(SOTA)performance,and present an indepth analysis of computational efficiency,robustness to noise,and sequence generalization.This work highlights the viability of Mamba SSMs as a transformerfree alternative for efficient and accurate ASR.By leveraging the advancements of statespace modeling,Samba ASR redefines ASR performance standards and sets a new benchmark for future research in this field. </p>
<blockquote>
<p>我们提出了Samba ASR，这是首款结合新型Mamba架构作为编码器和解码器的先进自动语音识别（ASR）模型，它建立在状态空间模型（SSMs）的基础上。不同于依赖自注意力机制捕获依赖关系的基于变压器的ASR模型，Samba ASR使用高效的状态空间动力学有效地对局部和全局时间依赖关系进行建模，实现了显著的性能提升。通过解决变压器模型的局限性，如输入长度的二次扩展和处理长距离依赖关系的困难，Samba ASR在准确性和效率上更胜一筹。实验结果表明，Samba ASR在各种标准基准测试中超越了现有的开源基于变压器的ASR模型，成为ASR领域的新技术前沿。在基准数据集上的广泛评估显示，其在词错误率（WER）方面取得了显著改进，即使在资源匮乏的场景中也具有竞争力。此外，Mamba架构的固有计算效率和参数优化使Samba ASR成为适用于各种ASR任务的可扩展和稳健的解决方案。我们的贡献包括为自动语音识别（ASR）开发新的Samba ASR架构，证明结构化状态空间模型（SSMs）在语音序列处理方面优于基于变压器的模型。我们在公共基准测试上进行了全面评估，展示了其处于行业顶尖的性能表现，并对计算效率、抗噪声干扰的鲁棒性和序列泛化能力进行了深入分析。这项工作突出了Mamba SSMs作为无变压器的高效准确ASR的替代方案的可行性。通过利用状态空间建模的进展，Samba ASR重新定义了ASR的性能标准，并为该领域的未来研究设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02832v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Samba ASR是一款利用Mamba架构的自动语音识别（ASR）模型，它在状态空间模型（SSMs）的基础上实现了本地和全局时间依赖性的有效建模。与传统基于变压器的ASR模型相比，Samba ASR通过解决输入长度和长距离依赖性问题，实现了更高的准确性和效率。实验结果表明，Samba ASR在各种标准基准测试中超过了现有的开源基于变压器的ASR模型，且在低资源场景中表现出竞争力。此外，Mamba架构的计算效率和参数优化使Samba ASR成为多样化的ASR任务的可扩展和稳健解决方案。此研究展示了状态空间模型在语音识别领域的优越性，为未来的研究设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Samba ASR是一个基于状态空间模型的自动语音识别（ASR）模型。</li>
<li>它采用Mamba架构，既作为编码器也作为解码器。</li>
<li>与基于变压器的ASR模型相比，Samba ASR通过建模本地和全局时间依赖性实现了更高的性能。</li>
<li>Samba ASR解决了基于变压器的模型的局限性，如输入长度的二次扩展和长距离依赖性的处理困难。</li>
<li>实验结果显示，Samba ASR在多种基准测试中超越了现有的开源基于变压器的ASR模型。</li>
<li>Samba ASR在低资源场景中表现出竞争力，且具有良好的计算效率和参数优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ca006037fc2db0b7fffe8971c5d1af2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation"><a href="#A-Lightweight-and-Real-Time-Binaural-Speech-Enhancement-Model-with-Spatial-Cues-Preservation" class="headerlink" title="A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation"></a>A Lightweight and Real-Time Binaural Speech Enhancement Model with   Spatial Cues Preservation</h2><p><strong>Authors:Jingyuan Wang, Jie Zhang, Shihao Chen, Miao Sun</strong></p>
<p>Binaural speech enhancement (BSE) aims to jointly improve the speech quality and intelligibility of noisy signals received by hearing devices and preserve the spatial cues of the target for natural listening. Existing methods often suffer from the compromise between noise reduction (NR) capacity and spatial cues preservation (SCP) accuracy and a high computational demand in complex acoustic scenes. In this work, we present a learning-based lightweight binaural complex convolutional network (LBCCN), which excels in NR by filtering low-frequency bands and keeping the rest. Additionally, our approach explicitly incorporates the estimation of interchannel relative acoustic transfer function to ensure the spatial cues fidelity and speech clarity. Results show that the proposed LBCCN can achieve a comparable NR performance to state-of-the-art methods under fixed-speaker conditions, but with a much lower computational cost and a certain degree of SCP capability. The reproducible code and audio examples are available at <a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN">https://github.com/jywanng/LBCCN</a>. </p>
<blockquote>
<p>双耳语音增强（BSE）旨在共同提高听力设备接收到的噪声信号的质量和语音清晰度，并保留目标的空间线索以实现自然聆听。现有方法经常在噪声降低（NR）能力和空间线索保留（SCP）准确性之间做出妥协，并且在复杂的声学场景中计算需求较高。在这项工作中，我们提出了一种基于学习的轻量级双耳复杂卷积网络（LBCCN），它通过过滤低频带实现出色的NR效果并保持其余部分。此外，我们的方法明确地结合了通道间相对声学传递函数的估计，以确保空间线索的保真度和语音清晰度。结果表明，在固定说话人条件下，所提出的LBCCN的NR性能可与最先进的方法相媲美，但计算成本更低，并且具有一定的SCP能力。可复用的代码和音频示例可在<a target="_blank" rel="noopener" href="https://github.com/jywanng/LBCCN%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jywanng/LBCCN找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12444v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于学习的轻量化双耳复杂卷积网络（LBCCN）在噪声环境下的语音增强技术。该技术旨在提高语音质量和可懂度，同时保留目标的空间线索以实现自然聆听体验。通过过滤低频带并保持其余部分实现降噪，同时估计通道间相对声学传输函数以确保空间线索保真和语音清晰度。与现有方法相比，LBCCN在固定说话人条件下具有相当的降噪性能，但计算成本更低，并具有一定的空间线索保留能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Binaural speech enhancement (BSE)旨在提高噪声信号的语音质量和可懂度，同时保留自然聆听时的空间线索。</li>
<li>现有方法常常在噪声减少（NR）能力、空间线索保留（SCP）精度以及复杂声学场景中的计算需求之间面临权衡。</li>
<li>提出的基于学习的轻量化双耳复杂卷积网络（LBCCN）通过过滤低频带进行降噪，并估计通道间相对声学传输函数以确保空间线索的保真和语音清晰度。</li>
<li>LBCCN具有与最新技术相当的降噪性能，但在固定说话人条件下计算成本更低。</li>
<li>LBCCN还具有一定的空间线索保留能力。</li>
<li>可复用的代码和音频示例可在指定链接找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12444">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-066b92f13243890d65ee73efe45e55ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21ce27fbc4661dce67f3f7d85fdc007c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6583098319cc6828f446a99587a33183.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Channel-Aware-Domain-Adaptive-Generative-Adversarial-Network-for-Robust-Speech-Recognition"><a href="#Channel-Aware-Domain-Adaptive-Generative-Adversarial-Network-for-Robust-Speech-Recognition" class="headerlink" title="Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust   Speech Recognition"></a>Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust   Speech Recognition</h2><p><strong>Authors:Chien-Chun Wang, Li-Wei Chen, Cheng-Kang Chou, Hung-Shin Lee, Berlin Chen, Hsin-Min Wang</strong></p>
<p>While pre-trained automatic speech recognition (ASR) systems demonstrate impressive performance on matched domains, their performance often degrades when confronted with channel mismatch stemming from unseen recording environments and conditions. To mitigate this issue, we propose a novel channel-aware data simulation method for robust ASR training. Our method harnesses the synergistic power of channel-extractive techniques and generative adversarial networks (GANs). We first train a channel encoder capable of extracting embeddings from arbitrary audio. On top of this, channel embeddings are extracted using a minimal amount of target-domain data and used to guide a GAN-based speech synthesizer. This synthesizer generates speech that faithfully preserves the phonetic content of the input while mimicking the channel characteristics of the target domain. We evaluate our method on the challenging Hakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving relative character error rate (CER) reductions of 20.02% and 9.64%, respectively, compared to the baselines. These results highlight the efficacy of our channel-aware data simulation method for bridging the gap between source- and target-domain acoustics. </p>
<blockquote>
<p>预训练的自动语音识别（ASR）系统在匹配领域上表现出令人印象深刻的性能，但当面临由未见过的录音环境和条件导致的通道不匹配问题时，它们的性能往往会下降。为了缓解这个问题，我们提出了一种用于稳健ASR训练的新型通道感知数据仿真方法。我们的方法结合了通道提取技术和生成对抗网络（GANs）的协同力量。首先，我们训练一个能够从任意音频中提取嵌入的通道编码器。在此基础上，使用少量目标域数据提取通道嵌入，并用于指导基于GAN的语音合成器。该合成器生成的语音忠实保留了输入语音的语音内容，同时模仿目标域的通道特性。我们在具有挑战性的客家话（HAT）和台湾话（TAT）语料库上评估了我们的方法，与基线相比，相对字符错误率（CER）分别降低了20.02%和9.64%。这些结果凸显了我们的通道感知数据仿真方法在弥合源域和目标域声学之间的差距方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12386v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新颖的通道感知数据仿真方法，用于增强自动语音识别（ASR）系统的鲁棒性训练。该方法结合了通道提取技术和生成对抗网络（GANs）的优势，通过训练通道编码器提取任意音频的嵌入信息，并使用少量目标域数据提取通道嵌入来引导基于GAN的语音合成器。合成器生成的语音既保留了输入语音的语音内容，又模仿了目标域的通道特性。在挑战性的客家话跨台湾（HAT）和台湾跨台湾（TAT）语料库上的评估结果表明，与基线相比，字符错误率（CER）分别降低了20.02%和9.64%，凸显了该通道感知数据仿真方法在弥合源域和目标域声学差异方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通道感知数据仿真方法用于增强ASR系统的鲁棒性训练。</li>
<li>结合了通道提取技术和生成对抗网络（GANs）。</li>
<li>通过训练通道编码器提取任意音频的嵌入信息。</li>
<li>使用少量目标域数据提取通道嵌入来引导GAN-based语音合成器。</li>
<li>合成器生成的语音既保留输入语音内容，又模仿目标域的通道特性。</li>
<li>在客家话跨台湾（HAT）和台湾跨台湾（TAT）语料库上进行了评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12386">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-932cd87ee1e6df6ddce3db880ea655ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbf4c5f6ee2dc20f4942b5ea3619e42c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec36549d0b86aae3ffe5bd4d8228a0a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31fdfa2996590542127034399b38423e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module"><a href="#DCIM-AVSR-Efficient-Audio-Visual-Speech-Recognition-via-Dual-Conformer-Interaction-Module" class="headerlink" title="DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module"></a>DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer   Interaction Module</h2><p><strong>Authors:Xinyu Wang, Haotian Jiang, Haolin Huang, Yu Fang, Mengjie Xu, Qian Wang</strong></p>
<p>Speech recognition is the technology that enables machines to interpret and process human speech, converting spoken language into text or commands. This technology is essential for applications such as virtual assistants, transcription services, and communication tools. The Audio-Visual Speech Recognition (AVSR) model enhances traditional speech recognition, particularly in noisy environments, by incorporating visual modalities like lip movements and facial expressions. While traditional AVSR models trained on large-scale datasets with numerous parameters can achieve remarkable accuracy, often surpassing human performance, they also come with high training costs and deployment challenges. To address these issues, we introduce an efficient AVSR model that reduces the number of parameters through the integration of a Dual Conformer Interaction Module (DCIM). In addition, we propose a pre-training method that further optimizes model performance by selectively updating parameters, leading to significant improvements in efficiency. Unlike conventional models that require the system to independently learn the hierarchical relationship between audio and visual modalities, our approach incorporates this distinction directly into the model architecture. This design enhances both efficiency and performance, resulting in a more practical and effective solution for AVSR tasks. </p>
<blockquote>
<p>语音识别是一项能够使机器解释和处理人类语音的技术，它将口语转化为文本或命令。对于虚拟助理、转录服务和通信工具等应用程序而言，这项技术至关重要。视听语音识别（AVSR）模型通过融入诸如嘴唇动作和面部表情等视觉模式，增强了传统语音识别的性能，特别是在嘈杂的环境中。虽然经过大规模数据集训练的传统AVSR模型具有许多参数，可以获得惊人的准确性，甚至超越人类表现，但它们也带来了高昂的训练成本和部署挑战。为了解决这些问题，我们引入了一个高效的AVSR模型，通过集成双卷积交互模块（DCIM）减少了参数数量。此外，我们还提出了一种预训练方法，通过选择性更新参数进一步优化模型性能，从而在效率上取得了显着提高。不同于传统模型需要系统独立学习音频和视觉模态之间的层次关系，我们的方法直接将这种区别纳入模型架构中。这种设计提高了效率和性能，为AVSR任务提供了更实用、更有效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00481v5">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>摘要</strong><br>    该文本介绍了一种改进的视听语音识别模型，通过引入双转换器交互模块和预训练方法来优化性能和效率，使其在嘈杂环境中实现高效、准确的语音识别。</p>
<p><strong>关键要点</strong></p>
<ol>
<li>该模型通过引入双转换器交互模块来减少参数数量，提高了视听语音识别的效率。</li>
<li>提出了一种预训练方法，通过选择性更新参数来优化模型性能。</li>
<li>该模型将视听模态的层次关系直接融入模型架构中，增强了效率和性能。</li>
<li>改进后的模型在嘈杂环境中实现高效、准确的语音识别。</li>
<li>该模型减少了训练成本，提高了部署效率。</li>
<li>该模型提高了视听语音识别的实用性和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abdbc4de40727de0d4409fb9fcb83a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c61bd461280aaa599e4e0945408115aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b83c3536ec993295c4b5cf18813237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f51746486c27928c98b9d2520d32fd6b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MEDSAGE-Enhancing-Robustness-of-Medical-Dialogue-Summarization-to-ASR-Errors-with-LLM-generated-Synthetic-Dialogues"><a href="#MEDSAGE-Enhancing-Robustness-of-Medical-Dialogue-Summarization-to-ASR-Errors-with-LLM-generated-Synthetic-Dialogues" class="headerlink" title="MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR   Errors with LLM-generated Synthetic Dialogues"></a>MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR   Errors with LLM-generated Synthetic Dialogues</h2><p><strong>Authors:Kuluhan Binici, Abhinav Ramesh Kashyap, Viktor Schlegel, Andy T. Liu, Vijay Prakash Dwivedi, Thanh-Tung Nguyen, Xiaoxue Gao, Nancy F. Chen, Stefan Winkler</strong></p>
<p>Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech into text, yet the errors they introduce can significantly degrade the performance of downstream tasks like summarization. This issue is particularly pronounced in clinical dialogue summarization, a low-resource domain where supervised data for fine-tuning is scarce, necessitating the use of ASR models as black-box solutions. Employing conventional data augmentation for enhancing the noise robustness of summarization models is not feasible either due to the unavailability of sufficient medical dialogue audio recordings and corresponding ASR transcripts. To address this challenge, we propose MEDSAGE, an approach for generating synthetic samples for data augmentation using Large Language Models (LLMs). Specifically, we leverage the in-context learning capabilities of LLMs and instruct them to generate ASR-like errors based on a few available medical dialogue examples with audio recordings. Experimental results show that LLMs can effectively model ASR noise, and incorporating this noisy data into the training process significantly improves the robustness and accuracy of medical dialogue summarization systems. This approach addresses the challenges of noisy ASR outputs in critical applications, offering a robust solution to enhance the reliability of clinical dialogue summarization. </p>
<blockquote>
<p>自动语音识别（ASR）系统在将语音转录为文本方面起着至关重要的作用，但它们引入的错误会显著影响下游任务（如摘要）的性能。这一问题在临床对话摘要中尤为突出，这是一个资源匮乏的领域，缺乏微调所需的大量监督数据，因此需要使用ASR模型作为黑箱解决方案。由于缺少足够的医学对话音频录音和相应的ASR转录，因此采用传统数据增强方法来提高摘要模型的抗噪性能并不可行。为了解决这一挑战，我们提出了MEDSAGE方法，这是一种利用大型语言模型（LLM）生成合成样本进行数据增强的方法。具体来说，我们利用LLM的上下文学习能力，以少量可用的医学对话例子（带有音频录音）为基础，指导它们生成类似ASR的错误。实验结果表明，LLM可以有效地模拟ASR噪声，将这种嘈杂的数据纳入训练过程，可以显著提高医学对话摘要系统的稳健性和准确性。该方法解决了关键应用中ASR输出噪声的问题，为提高临床对话摘要的可靠性提供了稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14418v3">PDF</a> Accepted by the Thirty-Ninth AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>自动语音识别（ASR）系统对将语音转化为文本具有关键作用，但其产生的错误会对下游任务如摘要的生成性能产生严重影响。特别是在临床对话摘要这一资源匮乏的领域，由于缺少精细调整所需的监督数据，必须依赖ASR模型作为黑箱解决方案。由于缺少足够的医疗对话音频录音和相应的ASR转录，使用传统数据增强方法提高摘要模型的抗噪声能力并不可行。为解决这一挑战，我们提出MEDSAGE方法，利用大型语言模型（LLM）生成合成样本进行数据增强。我们利用LLM的上下文学习能力，根据少量可用的带音频录音的医疗对话示例，指导它们生成类似ASR的错误。实验结果表明，LLM可以有效模拟ASR噪声，将这类噪声数据纳入训练过程，能显著提高医疗对话摘要系统的稳健性和准确性。该方法解决了噪声ASR输出在关键应用中的挑战，为提高临床对话摘要的可靠性提供了稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASR系统在转录语音为文本时的重要性及其可能产生的错误对下游任务的影响。</li>
<li>临床对话摘要是一个资源匮乏的领域，需要解决ASR模型的挑战。</li>
<li>传统数据增强方法因缺乏医疗对话音频录音和相应的ASR转录而不可行。</li>
<li>MEDSAGE方法利用大型语言模型生成合成样本进行数据增强。</li>
<li>LLMs可以有效地模拟ASR噪声。</li>
<li>将噪声数据纳入训练过程可以提高医疗对话摘要系统的稳健性和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.14418">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d62b5b4fca929b8e4ace0fdd4dcf3b5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3828a3fcc22da0994a726b03755e92a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d06a3ceadc3bc6f9e5ad1a1e2b2c5b77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81343766147d9871335e110270d5bfad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdcecd4922015328507b0f67e052583e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-975eb3ef9e2e682bb195734411d10f1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79746d1b63469ea30f56a90286148c6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d920b4aab26d6002fb3f7a1c0b6522d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Prosody-Analysis-of-Audiobooks"><a href="#Prosody-Analysis-of-Audiobooks" class="headerlink" title="Prosody Analysis of Audiobooks"></a>Prosody Analysis of Audiobooks</h2><p><strong>Authors:Charuta Pethe, Bach Pham, Felix D Childress, Yunting Yin, Steven Skiena</strong></p>
<p>Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems. </p>
<blockquote>
<p>近期文本转语音（TTS）技术的进步使得从文本生成自然声音的音频成为可能。然而，有声书的叙述涉及读者的大胆发声和语调变化，更加依赖于叙述中的情感、对话和描述。使用我们93本已对齐的书籍与有声书的数据集，我们提出了采用语言建模来预测叙述文本中的韵律属性（音调、音量和语速）的改进模型。我们的预测韵律属性与人类有声书阅读之间的相关性要好于最先进商业TTS系统的结果：在我们预测的音调方面，有22本图书与人的阅读高度相关；预测音量属性方面，有23本图书与人的阅读高度相似。最后，我们进行了一项人类评估研究，量化人们对于带韵律增强功能的有声书阅读与商业文本转语音系统偏好程度如何。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06930v3">PDF</a> Accepted to IEEE ICSC 2025</p>
<p><strong>Summary</strong></p>
<p>随着文本转语音技术的最新进展，从文本生成自然音频已成为可能。针对有声书朗读中需要更多情感和叙述性的特点，本文使用包含93个对齐的书籍-有声书对数据集，提出了改进的语言模型来预测语调属性（音调、音量和语速）。预测结果与真人朗读高度一致，相较于先进的商业文本转语音系统有更好的表现。并进行了人类评估实验，定量研究了人们对于有语调增强功能的有声书阅读的偏好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转语音技术现在可以生成自然的音频。</li>
<li>有声书朗读涉及更多的情感和叙述性元素。</li>
<li>使用语言模型对语调属性（音调、音量和语速）进行预测改进。</li>
<li>对比商业文本转语音系统，改进模型的预测结果更接近真人朗读。</li>
<li>在预测音调方面，改进模型在22本书中的表现优于真人朗读。</li>
<li>在预测音量方面，改进模型在大多数书籍中的表现接近真人朗读。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.06930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f8f17572fedf5f32310e180ac1a1e88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3596c3c6ad7af43de6496044233d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a2c3a8ae2f91eeb29c249e6f63faf7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef0acaa420db7f2760bb9d8cfea97aa4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab4d8f8b25b14eee3cec0f768c2d098a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df777e96e2b2050984a94830cb4f46e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-937f86509cb704fce76a372eea7deed9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ab72640571f530ee23df85187d0322f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-10eefef6b7dcbf914cf52bb1f2afbec4.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-10  iFADIT Invertible Face Anonymization via Disentangled Identity   Transform
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-94858bbfe4437e939e6ab782219e63b7.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-01-10  Boosting Salient Object Detection with Knowledge Distillated from Large   Foundation Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
