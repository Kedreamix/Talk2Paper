<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  LeakyCLIP Extracting Training Data from CLIP">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fbb890dd60799f5e4d90b79ecab16db4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-05-æ›´æ–°"><a href="#2025-08-05-æ›´æ–°" class="headerlink" title="2025-08-05 æ›´æ–°"></a>2025-08-05 æ›´æ–°</h1><h2 id="LeakyCLIP-Extracting-Training-Data-from-CLIP"><a href="#LeakyCLIP-Extracting-Training-Data-from-CLIP" class="headerlink" title="LeakyCLIP: Extracting Training Data from CLIP"></a>LeakyCLIP: Extracting Training Data from CLIP</h2><p><strong>Authors:Yunhao Chen, Shujie Wang, Xin Wang, Xingjun Ma</strong></p>
<p>Understanding the memorization and privacy leakage risks in Contrastive Languageâ€“Image Pretraining (CLIP) is critical for ensuring the security of multimodal models. Recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. In this work, we investigate data memorization and extraction risks in CLIP through the lens of CLIP inversion, a process that aims to reconstruct training images from text prompts. To this end, we introduce \textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from CLIP embeddings. We identify three key challenges in CLIP inversion: 1) non-robust features, 2) limited visual semantics in text embeddings, and 3) low reconstruction fidelity. To address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to enhance optimization smoothness, 2) linear transformation-based embedding alignment, and 3) Stable Diffusion-based refinement to improve fidelity. Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358% improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. Our work introduces a practical method for CLIP inversion while offering novel insights into the nature and scope of privacy risks in multimodal models. </p>
<blockquote>
<p>ç†è§£å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ä¸­çš„è®°å¿†å’Œéšç§æ³„éœ²é£é™©å¯¹äºç¡®ä¿å¤šæ¨¡æ€æ¨¡å‹çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†ä»æ‰©æ•£æ¨¡å‹ä¸­æå–æ•æ„Ÿè®­ç»ƒæ ·æœ¬çš„å¯è¡Œæ€§ï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„è®°å¿†å’Œæ³„éœ²ä¿¡æ¯çš„è¶‹åŠ¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡CLIPåè½¬çš„è§†è§’æ¥ç ”ç©¶CLIPä¸­çš„æ•°æ®è®°å¿†å’Œæå–é£é™©ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬æç¤ºé‡å»ºè®­ç»ƒå›¾åƒçš„è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textbf{LeakyCLIP}ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨ä»CLIPåµŒå…¥ä¸­å®ç°é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬å‘ç°CLIPåè½¬é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç‰¹å¾ä¸ç¨³å¥ï¼Œ2ï¼‰æ–‡æœ¬åµŒå…¥ä¸­çš„è§†è§‰è¯­ä¹‰æœ‰é™ï¼Œä»¥åŠ3ï¼‰é‡å»ºä¿çœŸåº¦ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒLeakyCLIPé‡‡ç”¨1ï¼‰å¯¹æŠ—å¾®è°ƒå¢å¼ºä¼˜åŒ–å¹³æ»‘åº¦ï¼Œ2ï¼‰åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½ï¼Œä»¥åŠ3ï¼‰åŸºäºç¨³å®šæ‰©æ•£çš„ç»†åŒ–ä»¥æé«˜ä¿çœŸåº¦ã€‚ç»éªŒç»“æœè¡¨æ˜LeakyCLIPçš„ä¼˜è¶Šæ€§ï¼Œåœ¨LAION-2Bå­é›†ä¸Šä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒViT-B-16çš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°æµ‹é‡ï¼ˆSSIMï¼‰æé«˜äº†358%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†æ™®éçš„æ³„éœ²é£é™©ï¼Œè¡¨æ˜ç”šè‡³å¯ä»¥ä»ä½ä¿çœŸé‡å»ºçš„æŒ‡æ ‡ä¸­æˆåŠŸæ¨æ–­å‡ºè®­ç»ƒæ•°æ®æˆå‘˜ã€‚æˆ‘ä»¬çš„å·¥ä½œä»‹ç»äº†ä¸€ç§å®ç”¨çš„CLIPåè½¬æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†å¯¹å¤šæ¨¡æ€æ¨¡å‹ä¸­éšç§é£é™©æ€§è´¨å’ŒèŒƒå›´çš„æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00756v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡æ¢è®¨äº†Contrastive Language--Image Pretrainingï¼ˆCLIPï¼‰æ¨¡å‹ä¸­çš„è®°å¿†ä¸éšç§æ³„éœ²é£é™©ã€‚ç ”ç©¶è€…é€šè¿‡CLIPåæ¼”çš„è§’åº¦ç ”ç©¶æ•°æ®è®°å¿†ä¸æå–é£é™©ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åä¸ºLeakyCLIPçš„æ–°å‹æ”»å‡»æ¡†æ¶ï¼Œæ—¨åœ¨ä»CLIPåµŒå…¥ä¸­å®ç°é«˜è´¨é‡ã€è¯­ä¹‰å‡†ç¡®çš„å›¾åƒé‡å»ºã€‚ç ”ç©¶è¯†åˆ«äº†ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜å¹¶æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œæœ€ç»ˆå®ç°äº†é«˜æ•ˆçš„å›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°å³ä½¿ä½è´¨é‡çš„å›¾åƒé‡å»ºä¹Ÿå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚æœ¬æ–‡æä¾›äº†å®ç”¨çš„CLIPåæ¼”æ–¹æ³•ï¼Œå¹¶å¯¹å¤šæ¨¡æ€æ¨¡å‹çš„éšç§é£é™©æä¾›äº†æ–°çš„è§è§£ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹ä¸­å­˜åœ¨è®°å¿†å’Œéšç§æ³„éœ²é£é™©ï¼Œå°¤å…¶æ˜¯æ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>LeakyCLIPæ”»å‡»æ¡†æ¶ç”¨äºä»CLIPåµŒå…¥ä¸­å®ç°é«˜è´¨é‡å›¾åƒé‡å»ºã€‚</li>
<li>CLIPåæ¼”é¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šéç¨³å¥ç‰¹å¾ã€æ–‡æœ¬åµŒå…¥ä¸­æœ‰é™çš„è§†è§‰è¯­ä¹‰å’Œä½é‡å»ºä¿çœŸåº¦ã€‚</li>
<li>LeakyCLIPé€šè¿‡å¯¹æŠ—æ€§å¾®è°ƒã€åŸºäºçº¿æ€§å˜æ¢çš„åµŒå…¥å¯¹é½å’ŒåŸºäºStable Diffusionçš„ç»†åŒ–æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>LeakyCLIPåœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ä¸Šå®ç°äº†è¶…è¿‡ViT-B-16åŸºçº¿æ–¹æ³•358%çš„æ”¹è¿›ã€‚</li>
<li>å³ä½¿æ˜¯ä½è´¨é‡çš„å›¾åƒé‡å»ºä¹Ÿå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96a2b224c4fec8217555faa576acc1d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64a0eefcf7f259bd66e4c30339e3e1ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbb890dd60799f5e4d90b79ecab16db4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a56def90e3a3794c4e19cc07cad4730.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Minimum-Data-Maximum-Impact-20-annotated-samples-for-explainable-lung-nodule-classification"><a href="#Minimum-Data-Maximum-Impact-20-annotated-samples-for-explainable-lung-nodule-classification" class="headerlink" title="Minimum Data, Maximum Impact: 20 annotated samples for explainable lung   nodule classification"></a>Minimum Data, Maximum Impact: 20 annotated samples for explainable lung   nodule classification</h2><p><strong>Authors:Luisa GallÃ©e, Catharina Silvia Lisson, Christoph Gerhard Lisson, Daniela Drees, Felix Weig, Daniel Vogele, Meinrad Beer, Michael GÃ¶tz</strong></p>
<p>Classification models that provide human-interpretable explanations enhance cliniciansâ€™ trust and usability in medical image diagnosis. One research focus is the integration and prediction of pathology-related visual attributes used by radiologists alongside the diagnosis, aligning AI decision-making with clinical reasoning. Radiologists use attributes like shape and texture as established diagnostic criteria and mirroring these in AI decision-making both enhances transparency and enables explicit validation of model outputs. However, the adoption of such models is limited by the scarcity of large-scale medical image datasets annotated with these attributes. To address this challenge, we propose synthesizing attribute-annotated data using a generative model. We enhance the Diffusion Model with attribute conditioning and train it using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset. Incorporating its generated images into the training of an explainable model boosts performance, increasing attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with only the small real attribute-annotated dataset. This work highlights the potential of synthetic data to overcome dataset limitations, enhancing the applicability of explainable models in medical image analysis. </p>
<blockquote>
<p>æä¾›äººç±»å¯è§£é‡Šçš„è§£é‡Šçš„åˆ†ç±»æ¨¡å‹å¢å¼ºäº†ä¸´åºŠåŒ»ç”Ÿå¯¹åŒ»å­¦å›¾åƒè¯Šæ–­çš„ä¿¡ä»»å’Œå¯ç”¨æ€§ã€‚ä¸€ä¸ªç ”ç©¶é‡ç‚¹æ˜¯æ•´åˆå’Œé¢„æµ‹ç—…ç†ç›¸å…³çš„è§†è§‰å±æ€§ï¼Œè¿™äº›å±æ€§è¢«æ”¾å°„ç§‘åŒ»ç”Ÿç”¨äºè¯Šæ–­ï¼Œä½¿AIå†³ç­–ä¸ä¸´åºŠæ¨ç†ç›¸ä¸€è‡´ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿä½¿ç”¨å½¢çŠ¶å’Œçº¹ç†ç­‰å±æ€§ä½œä¸ºæ—¢å®šçš„è¯Šæ–­ä¾æ®ï¼Œå¹¶åœ¨AIå†³ç­–ä¸­åæ˜ è¿™äº›ä¾æ®ï¼Œæ—¢æé«˜äº†é€æ˜åº¦ï¼Œåˆèƒ½å¤Ÿæ˜ç¡®éªŒè¯æ¨¡å‹è¾“å‡ºã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼Œè¿™äº›å±æ€§è¢«æ ‡æ³¨çš„æ•°æ®é›†ç¨€ç¼ºï¼Œå› æ­¤è¿™ç§æ¨¡å‹çš„é‡‡ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç”Ÿæˆæ¨¡å‹åˆæˆå±æ€§æ ‡æ³¨æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå±æ€§æ¡ä»¶å¢å¼ºï¼Œä»…ä½¿ç”¨LIDC-IDRIæ•°æ®é›†çš„20ä¸ªå±æ€§æ ‡æ³¨çš„è‚ºç»“èŠ‚æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚å°†å…¶ç”Ÿæˆçš„å›¾åƒçº³å…¥å¯è§£é‡Šæ¨¡å‹çš„è®­ç»ƒï¼Œæé«˜äº†æ€§èƒ½ï¼Œä¸ä»…ä½¿ç”¨å°å‹çœŸå®å±æ€§æ ‡æ³¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œå±æ€§é¢„æµ‹ç²¾åº¦æé«˜äº†13.4%ï¼Œç›®æ ‡é¢„æµ‹ç²¾åº¦æé«˜äº†1.8%ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åˆæˆæ•°æ®åœ¨å…‹æœæ•°æ®é›†é™åˆ¶æ–¹é¢çš„æ½œåŠ›ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00639v1">PDF</a> Accepted at iMIMIC - Interpretability of Machine Intelligence in   Medical Image Computing workshop MICCAI 2025 Medical Image Computing and   Computer Assisted Intervention</p>
<p><strong>Summary</strong><br>     åˆ†ç±»æ¨¡å‹æä¾›å¯è§£é‡Šçš„è§£é‡Šï¼Œå¢å¼ºä¸´åºŠåŒ»ç”Ÿå¯¹åŒ»å­¦å½±åƒè¯Šæ–­çš„ä¿¡ä»»å’Œä½¿ç”¨æ€§ã€‚ç ”ç©¶é‡ç‚¹æ˜¯æ•´åˆå¹¶é¢„æµ‹æ”¾å°„ç§‘åŒ»ç”Ÿä½¿ç”¨çš„ç—…ç†ç›¸å…³è§†è§‰å±æ€§ï¼Œä½¿AIå†³ç­–ä¸ä¸´åºŠæ¨ç†ç›¸ä¸€è‡´ã€‚ä½¿ç”¨å½¢çŠ¶å’Œçº¹ç†ç­‰å±æ€§ä½œä¸ºè¯Šæ–­æ ‡å‡†ï¼Œåœ¨AIå†³ç­–ä¸­ä½“ç°è¿™äº›æ ‡å‡†æé«˜äº†é€æ˜åº¦å’Œæ¨¡å‹è¾“å‡ºçš„æ˜ç¡®éªŒè¯æ€§ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨æœ‰è¿™äº›å±æ€§çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼Œè¿™äº›æ¨¡å‹çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç”Ÿæˆæ¨¡å‹åˆæˆå±æ€§æ ‡æ³¨æ•°æ®ã€‚æˆ‘ä»¬å¢å¼ºäº†æ‰©æ•£æ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œå±æ€§æ¡ä»¶è®­ç»ƒï¼Œä»…ä½¿ç”¨LIDC-IDRIæ•°æ®é›†çš„20ä¸ªå±æ€§æ ‡æ³¨çš„è‚ºç»“èŠ‚æ ·æœ¬ã€‚å°†å…¶ç”Ÿæˆçš„å›¾åƒçº³å…¥å¯è§£é‡Šæ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œæé«˜äº†æ€§èƒ½ï¼Œä¸ä»…ä½¿ç”¨å°å‹çœŸå®å±æ€§æ ‡æ³¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œå±æ€§é¢„æµ‹å‡†ç¡®ç‡æé«˜13.4%ï¼Œç›®æ ‡é¢„æµ‹å‡†ç¡®ç‡æé«˜1.8%ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†åˆæˆæ•°æ®åœ¨å…‹æœæ•°æ®é›†é™åˆ¶æ–¹é¢çš„æ½œåŠ›ï¼Œæé«˜äº†å¯è§£é‡Šæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†ç±»æ¨¡å‹æä¾›è§£é‡Šæ€§å¯ä»¥å¢å¼ºä¸´åºŠåŒ»ç”Ÿå¯¹åŒ»å­¦å½±åƒè¯Šæ–­çš„ä¿¡ä»»å’Œä½¿ç”¨æ€§ã€‚</li>
<li>AIå†³ç­–ä¸ä¸´åºŠæ¨ç†çš„æ•´åˆæ˜¯ç ”ç©¶çš„é‡ç‚¹ï¼ŒåŒ…æ‹¬é¢„æµ‹ç—…ç†ç›¸å…³çš„è§†è§‰å±æ€§ã€‚</li>
<li>æ”¾å°„ç§‘åŒ»ç”Ÿä½¿ç”¨çš„å±æ€§å¦‚å½¢çŠ¶å’Œçº¹ç†è¢«çº³å…¥AIå†³ç­–ä¸­ï¼Œä»¥æé«˜é€æ˜åº¦å’Œæ¨¡å‹éªŒè¯æ€§ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡æ ‡æ³¨åŒ»å­¦å›¾åƒæ•°æ®é›†æ˜¯åº”ç”¨è¿™äº›æ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆå±æ€§æ ‡æ³¨æ•°æ®ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>åˆæˆæ•°æ®åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä½¿ç”¨æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-833719a8e8b6963d92a663d80d5b8acf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b96a3fbad894322271d3110c7db37668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc449bcbef936fabec35aca03f6146a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-024df8b8103f227d218eb8f3b6068837.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Wukong-Framework-for-Not-Safe-For-Work-Detection-in-Text-to-Image-systems"><a href="#Wukong-Framework-for-Not-Safe-For-Work-Detection-in-Text-to-Image-systems" class="headerlink" title="Wukong Framework for Not Safe For Work Detection in Text-to-Image   systems"></a>Wukong Framework for Not Safe For Work Detection in Text-to-Image   systems</h2><p><strong>Authors:Mingrui Liu, Sixiao Zhang, Cheng Long</strong></p>
<p>Text-to-Image (T2I) generation is a popular AI-generated content (AIGC) technology enabling diverse and creative image synthesis. However, some outputs may contain Not Safe For Work (NSFW) content (e.g., violence), violating community guidelines. Detecting NSFW content efficiently and accurately, known as external safeguarding, is essential. Existing external safeguards fall into two types: text filters, which analyze user prompts but overlook T2I model-specific variations and are prone to adversarial attacks; and image filters, which analyze final generated images but are computationally costly and introduce latency. Diffusion models, the foundation of modern T2I systems like Stable Diffusion, generate images through iterative denoising using a U-Net architecture with ResNet and Transformer blocks. We observe that: (1) early denoising steps define the semantic layout of the image, and (2) cross-attention layers in U-Net are crucial for aligning text and image regions. Based on these insights, we propose Wukong, a transformer-based NSFW detection framework that leverages intermediate outputs from early denoising steps and reuses U-Netâ€™s pre-trained cross-attention parameters. Wukong operates within the diffusion process, enabling early detection without waiting for full image generation. We also introduce a new dataset containing prompts, seeds, and image-specific NSFW labels, and evaluate Wukong on this and two public benchmarks. Results show that Wukong significantly outperforms text-based safeguards and achieves comparable accuracy of image filters, while offering much greater efficiency. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ˜¯ä¸€ç§æµè¡Œçš„AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°å¤šæ ·åŒ–å’Œåˆ›æ„æ€§çš„å›¾åƒåˆæˆã€‚ç„¶è€Œï¼Œä¸€äº›è¾“å‡ºå¯èƒ½åŒ…å«ä¸å®œåœ¨å·¥ä½œåœºåˆå±•ç¤ºï¼ˆNSFWï¼‰çš„å†…å®¹ï¼ˆä¾‹å¦‚æš´åŠ›ï¼‰ï¼Œè¿™è¿åäº†ç¤¾åŒºå‡†åˆ™ã€‚é«˜æ•ˆä¸”å‡†ç¡®åœ°æ£€æµ‹NSFWå†…å®¹ï¼Œè¢«ç§°ä¸ºå¤–éƒ¨å®‰å…¨ä¿éšœï¼Œæ˜¯è‡³å…³é‡è¦çš„ã€‚ç°æœ‰çš„å¤–éƒ¨å®‰å…¨ä¿éšœæªæ–½åˆ†ä¸ºä¸¤ç§ï¼šæ–‡æœ¬è¿‡æ»¤å™¨ï¼Œå®ƒåˆ†æç”¨æˆ·æç¤ºï¼Œä½†å¿½ç•¥äº†T2Iæ¨¡å‹ç‰¹å®šçš„å˜åŒ–ï¼Œå¹¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼›å›¾åƒè¿‡æ»¤å™¨ï¼Œå®ƒåˆ†ææœ€ç»ˆç”Ÿæˆçš„å›¾åƒï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚å¹¶å¼•å…¥å»¶è¿Ÿã€‚æ‰©æ•£æ¨¡å‹æ˜¯ç°ä»£T2Iç³»ç»Ÿï¼ˆå¦‚Stable Diffusionï¼‰çš„åŸºç¡€ï¼Œé€šè¿‡è¿­ä»£å»å™ªç”Ÿæˆå›¾åƒï¼Œä½¿ç”¨å¸¦æœ‰ResNetå’ŒTransformerå—çš„U-Netæ¶æ„ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼šï¼ˆ1ï¼‰æ—©æœŸçš„å»å™ªæ­¥éª¤å®šä¹‰äº†å›¾åƒè¯­ä¹‰å¸ƒå±€ï¼Œï¼ˆ2ï¼‰U-Netä¸­çš„äº¤å‰æ³¨æ„å±‚å¯¹äºå¯¹é½æ–‡æœ¬å’Œå›¾åƒåŒºåŸŸè‡³å…³é‡è¦ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå˜å‹å™¨çš„Wukong NSFWæ£€æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ—©æœŸå»å™ªæ­¥éª¤çš„ä¸­é—´è¾“å‡ºå¹¶é‡æ–°ä½¿ç”¨U-Netçš„é¢„è®­ç»ƒäº¤å‰æ³¨æ„å‚æ•°ã€‚Wukongåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­è¿è¡Œï¼Œèƒ½å¤Ÿå®ç°æ— éœ€ç­‰å¾…å®Œæ•´å›¾åƒç”Ÿæˆçš„æ—©æœŸæ£€æµ‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å«æç¤ºã€ç§å­å’Œå›¾åƒç‰¹å®šNSFWæ ‡ç­¾çš„æ–°æ•°æ®é›†ï¼Œå¹¶åœ¨è¯¥æ•°æ®é›†å’Œä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå¯¹Wukongè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒWukongåœ¨æ–‡æœ¬ä¿éšœæªæ–½æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶å®ç°äº†ä¸å›¾åƒè¿‡æ»¤å™¨ç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æä¾›äº†æ›´é«˜çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00591v1">PDF</a> Under review</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åä¸ºâ€œæ‚Ÿç©ºâ€çš„NSFWæ£€æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ—©æœŸå»å™ªæ­¥éª¤ä¸­çš„ä¸­é—´è¾“å‡ºæ¥æ£€æµ‹å›¾åƒå†…å®¹çš„å®‰å…¨æ€§ã€‚æ‚Ÿç©ºèƒ½åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­è¿è¡Œï¼Œæ— éœ€ç­‰å¾…å®Œæ•´çš„å›¾åƒç”Ÿæˆå°±èƒ½å®ç°æ—©æœŸæ£€æµ‹ï¼Œæ—¢å‡†ç¡®åˆé«˜æ•ˆã€‚åŒæ—¶å¼•å…¥æ–°æ•°æ®é›†ç”¨äºè¯„ä¼°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iç”ŸæˆæŠ€æœ¯æ˜¯ä¸€ç§æµè¡Œçš„AIGCæŠ€æœ¯ï¼Œå¯ä»¥ç”Ÿæˆå¤šæ ·ä¸”æœ‰åˆ›æ„çš„å›¾åƒï¼Œä½†å¯èƒ½åŒ…å«NSFWå†…å®¹ã€‚</li>
<li>æ£€æµ‹NSFWå†…å®¹å¯¹äºéµå®ˆç¤¾åŒºå‡†åˆ™è‡³å…³é‡è¦ï¼Œç°æœ‰å¤–éƒ¨ä¿éšœæªæ–½å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ˜¯T2Iç³»ç»Ÿçš„åŸºçŸ³ï¼Œé€šè¿‡è¿­ä»£å»å™ªç”Ÿæˆå›¾åƒã€‚</li>
<li>æ—©æœŸå»å™ªæ­¥éª¤å®šä¹‰äº†å›¾åƒè¯­ä¹‰å¸ƒå±€ï¼ŒU-Netä¸­çš„è·¨æ³¨æ„åŠ›å±‚å¯¹äºæ–‡æœ¬å’Œå›¾åƒåŒºåŸŸçš„å¯¹é½è‡³å…³é‡è¦ã€‚</li>
<li>æ‚Ÿç©ºæ£€æµ‹æ¡†æ¶åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨æ—©æœŸå»å™ªæ­¥éª¤çš„ä¸­é—´è¾“å‡ºæ¥æ£€æµ‹NSFWå†…å®¹ã€‚</li>
<li>æ‚Ÿç©ºèƒ½åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­è¿›è¡Œæ—©æœŸæ£€æµ‹ï¼Œæ— éœ€ç­‰å¾…å®Œæ•´å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-072cce07a11888f633931211c5deb5ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cbccf0a907a7527d99abbd5dce1acf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bec27d9791f99f5cc2f849754b8856e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a371e75408fe3f3433bbd397dfc2970b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdfd44472c4a3c7e10fd34f0d3602edc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer"><a href="#LAMIC-Layout-Aware-Multi-Image-Composition-via-Scalability-of-Multimodal-Diffusion-Transformer" class="headerlink" title="LAMIC: Layout-Aware Multi-Image Composition via Scalability of   Multimodal Diffusion Transformer"></a>LAMIC: Layout-Aware Multi-Image Composition via Scalability of   Multimodal Diffusion Transformer</h2><p><strong>Authors:Yuzhuo Chen, Zehua Ma, Jianhua Wang, Kai Kang, Shunyu Yao, Weiming Zhang</strong></p>
<p>In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMICâ€™s superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMICâ€™s performance is expected to scale accordingly. Our implementation is available at: <a target="_blank" rel="noopener" href="https://github.com/Suchenl/LAMIC">https://github.com/Suchenl/LAMIC</a>. </p>
<blockquote>
<p>åœ¨å¯æ§å›¾åƒåˆæˆä¸­ï¼Œä»å¤šä¸ªå‚è€ƒå›¾åƒç”Ÿæˆå…·æœ‰ç©ºé—´å¸ƒå±€æ„è¯†çš„è¿è´¯å’Œä¸€è‡´å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†LAMICï¼Œä¸€ä¸ªåŸºäºå¸ƒå±€æ„ŸçŸ¥çš„å¤šå›¾åƒç»„åˆæ¡†æ¶ï¼Œå®ƒé¦–æ¬¡ä»¥æ— è®­ç»ƒçš„æ–¹å¼å°†å•å‚è€ƒæ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°å¤šå‚è€ƒåœºæ™¯ã€‚åŸºäºMMDiTæ¨¡å‹ï¼ŒLAMICå¼•å…¥äº†ä¸¤ç§å³æ’å³ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼š1ï¼‰ç¾¤ç»„éš”ç¦»æ³¨æ„åŠ›ï¼ˆGIAï¼‰ä»¥å¢å¼ºå®ä½“è§£çº ç¼ ï¼›å’Œ2ï¼‰åŒºåŸŸè°ƒåˆ¶æ³¨æ„åŠ›ï¼ˆRMAï¼‰ä»¥å®ç°å¸ƒå±€æ„ŸçŸ¥ç”Ÿæˆã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸‰é¡¹æŒ‡æ ‡ï¼š1ï¼‰åŒ…å«ç‡ï¼ˆIN-Rï¼‰å’Œå¡«å……ç‡ï¼ˆFI-Rï¼‰ä»¥è¯„ä¼°å¸ƒå±€æ§åˆ¶ï¼›å’Œ2ï¼‰èƒŒæ™¯ç›¸ä¼¼æ€§ï¼ˆBG-Sï¼‰ä»¥è¡¡é‡èƒŒæ™¯ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLAMICåœ¨å¤§å¤šæ•°ä¸»è¦æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼šå®ƒåœ¨æ‰€æœ‰è®¾ç½®ä¸‹çš„ID-Sã€BG-Sã€IN-Rå’ŒAVGåˆ†æ•°æ–¹é¢ä¸€ç›´ä¼˜äºç°æœ‰çš„å¤šå‚è€ƒåŸºçº¿ï¼Œå¹¶åœ¨å¤æ‚ç»„åˆä»»åŠ¡ä¸­è·å¾—äº†æœ€ä½³DPGã€‚è¿™äº›ç»“æœè¯æ˜äº†LAMICåœ¨èº«ä»½ä¿ç•™ã€èƒŒæ™¯ä¿ç•™ã€å¸ƒå±€æ§åˆ¶å’Œæç¤ºéµå¾ªæ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œè¿™äº›éƒ½æ— éœ€ä»»ä½•è®­ç»ƒæˆ–å¾®è°ƒï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç»§æ‰¿å…ˆè¿›å•å‚è€ƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå¹¶èƒ½å¤Ÿå®ç°æ— ç¼æ‰©å±•åˆ°å¤šå›¾åƒåœºæ™¯ï¼ŒLAMICä¸ºå¯æ§å¤šå›¾åƒç»„åˆå»ºç«‹äº†æ–°çš„æ— è®­ç»ƒèŒƒå¼ã€‚éšç€åŸºç¡€æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼ŒLAMICçš„æ€§èƒ½é¢„è®¡ä¹Ÿä¼šç›¸åº”æå‡ã€‚æˆ‘ä»¬çš„å®ç°å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Suchenl/LAMIC%E3%80%82">https://github.com/Suchenl/LAMICã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00477v1">PDF</a> 8 pages, 5 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šå‚è€ƒæ‰©æ•£æ¨¡å‹æ¡†æ¶LAMICï¼Œå¯åˆæˆå…·æœ‰ç©ºé—´å¸ƒå±€æ„è¯†çš„è¿è´¯ä¸”ä¸€è‡´çš„å¤šå¼ å›¾åƒã€‚å®ƒé€šè¿‡å¼•å…¥é›†å›¢éš”ç¦»æ³¨æ„åŠ›å’ŒåŒºåŸŸè°ƒåˆ¶æ³¨æ„åŠ›ä¸¤ç§æ’ä»¶å¼æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†åœ¨å¤šä¸ªå›¾åƒä¸Šçš„å¤šå‚è€ƒå›¾åƒåˆæˆã€‚æ­¤å¤–ï¼Œä¸ºäº†å…¨é¢è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸‰é¡¹æ–°æŒ‡æ ‡ç”¨äºè¯„ä¼°å¸ƒå±€æ§åˆ¶ã€èƒŒæ™¯ä¸€è‡´æ€§ç­‰èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒLAMICåœ¨å¤§å¤šæ•°ä¸»è¦æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LAMICæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šå‚è€ƒæ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºåˆæˆå…·æœ‰ç©ºé—´å¸ƒå±€æ„è¯†çš„è¿è´¯ä¸”ä¸€è‡´çš„å¤šå¼ å›¾åƒã€‚</li>
<li>å®ƒé€šè¿‡å¼•å…¥Group Isolation Attention (GIA)å’ŒRegion-Modulated Attention (RMA)æ¥å¤„ç†å¤šå‚è€ƒå›¾åƒã€‚</li>
<li>ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¼•å…¥äº†ä¸‰é¡¹æ–°æŒ‡æ ‡ï¼šInclusion Ratio (IN-R)ã€Fill Ratio (FI-R)å’ŒBackground Similarity (BG-S)ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºLAMICåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨èº«ä»½ä¿æŒã€èƒŒæ™¯ä¿ç•™ã€å¸ƒå±€æ§åˆ¶å’Œæç¤ºéµå¾ªæ–¹é¢ã€‚</li>
<li>LAMICå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºäº†å…ˆè¿›å•å‚è€ƒæ¨¡å‹çš„ä¼˜ç‚¹ä»¥åŠæ— ç¼æ‰©å±•åˆ°å¤šå›¾åƒåœºæ™¯çš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00477">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7bd58791e0f5228d3df23a54dd0fb2b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bfb3e336fe99dae6bebebaee3df632a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a28313ad8762516b5adc233044192fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87b73744f8b7c8a431a0c1262046c6f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0e126440ec9fa4de86833387c19e129.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SDMatte-Grafting-Diffusion-Models-for-Interactive-Matting"><a href="#SDMatte-Grafting-Diffusion-Models-for-Interactive-Matting" class="headerlink" title="SDMatte: Grafting Diffusion Models for Interactive Matting"></a>SDMatte: Grafting Diffusion Models for Interactive Matting</h2><p><strong>Authors:Longfei Huang, Yu Liang, Hao Zhang, Jinwei Chen, Wei Dong, Lunde Chen, Wanyu Liu, Bo Li, Pengtao Jiang</strong></p>
<p>Recent interactive matting methods have shown satisfactory performance in capturing the primary regions of objects, but they fall short in extracting fine-grained details in edge regions. Diffusion models trained on billions of image-text pairs, demonstrate exceptional capability in modeling highly complex data distributions and synthesizing realistic texture details, while exhibiting robust text-driven interaction capabilities, making them an attractive solution for interactive matting. To this end, we propose SDMatte, a diffusion-driven interactive matting model, with three key contributions. First, we exploit the powerful priors of diffusion models and transform the text-driven interaction capability into visual prompt-driven interaction capability to enable interactive matting. Second, we integrate coordinate embeddings of visual prompts and opacity embeddings of target objects into U-Net, enhancing SDMatteâ€™s sensitivity to spatial position information and opacity information. Third, we propose a masked self-attention mechanism that enables the model to focus on areas specified by visual prompts, leading to better performance. Extensive experiments on multiple datasets demonstrate the superior performance of our method, validating its effectiveness in interactive matting. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/vivoCameraResearch/SDMatte">https://github.com/vivoCameraResearch/SDMatte</a>. </p>
<blockquote>
<p>è¿‘æœŸäº¤äº’å¼æ‹”æ¯›ï¼ˆmattingï¼‰æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºæ•æ‰ç‰©ä½“ä¸»è¦åŒºåŸŸçš„æ»¡æ„æ€§èƒ½ï¼Œä½†åœ¨è¾¹ç¼˜åŒºåŸŸæå–ç²¾ç»†ç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚åœ¨æ•°åäº¿å›¾åƒæ–‡æœ¬å¯¹ä¸Šè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨æ¨¡æ‹Ÿé«˜åº¦å¤æ‚çš„æ•°æ®åˆ†å¸ƒå’Œåˆæˆé€¼çœŸçš„çº¹ç†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒæ—¶å±•ç°å‡ºç¨³å¥çš„æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºäº¤äº’å¼æ‹”æ¯›çš„ç†æƒ³è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SDMatteï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„äº¤äº’å¼æ‹”æ¯›æ¨¡å‹ï¼Œæœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒï¼Œå°†æ–‡æœ¬é©±åŠ¨çš„äº¤äº’èƒ½åŠ›è½¬åŒ–ä¸ºè§†è§‰æç¤ºé©±åŠ¨çš„äº¤äº’èƒ½åŠ›ï¼Œä»¥å®ç°äº¤äº’å¼æ‹”æ¯›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†è§†è§‰æç¤ºçš„åæ ‡åµŒå…¥å’Œç›®æ ‡å¯¹è±¡çš„é€æ˜åº¦åµŒå…¥æ•´åˆåˆ°U-Netä¸­ï¼Œæé«˜äº†SDMatteå¯¹ç©ºé—´ä½ç½®ä¿¡æ¯å’Œé€æ˜åº¦ä¿¡æ¯çš„æ•æ„Ÿåº¦ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ©è†œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è§†è§‰æç¤ºæŒ‡å®šçš„åŒºåŸŸï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼ŒéªŒè¯äº†å…¶åœ¨äº¤äº’å¼æ‹”æ¯›ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vivocameraresearch/sdmatte%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vivoCameraResearch/SDMatteä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00443v1">PDF</a> Accepted at ICCV 2025, 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„äº¤äº’å¼æŠ å›¾æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒä¿¡æ¯ï¼Œå°†æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›è½¬åŒ–ä¸ºè§†è§‰æç¤ºé©±åŠ¨äº¤äº’èƒ½åŠ›ï¼Œå®ç°äº†äº¤äº’å¼æŠ å›¾ã€‚é€šè¿‡æ•´åˆè§†è§‰æç¤ºçš„åæ ‡åµŒå…¥å’Œç›®æ ‡å¯¹è±¡çš„é€æ˜åº¦åµŒå…¥åˆ°U-Netä¸­ï¼Œå¹¶å¼•å…¥æ©è†œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ¨¡å‹å¯¹ç©ºé—´ä½ç½®ä¿¡æ¯å’Œé€æ˜åº¦ä¿¡æ¯çš„æ•æ„Ÿåº¦ï¼Œå®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•è·å¯¹è±¡ä¸»è¦åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨è¾¹ç¼˜åŒºåŸŸæå–ç²¾ç»†ç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰å¯¹é«˜åº¦å¤æ‚æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡å’Œåˆæˆé€¼çœŸçº¹ç†ç»†èŠ‚çš„èƒ½åŠ›ï¼ŒåŒæ—¶å±•ç°å‡ºå¼ºå¤§çš„æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„äº¤äº’å¼æŠ å›¾æŠ€æœ¯SDMatteï¼Œè¯¥æŠ€æœ¯å°†æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›è½¬åŒ–ä¸ºè§†è§‰æç¤ºé©±åŠ¨äº¤äº’èƒ½åŠ›ã€‚</li>
<li>SDMatteé€šè¿‡æ•´åˆè§†è§‰æç¤ºçš„åæ ‡åµŒå…¥å’Œç›®æ ‡å¯¹è±¡çš„é€æ˜åº¦åµŒå…¥åˆ°U-Netä¸­ï¼Œæé«˜äº†å¯¹ç©ºé—´ä½ç½®ä¿¡æ¯å’Œé€æ˜åº¦ä¿¡æ¯çš„æ•æ„Ÿåº¦ã€‚</li>
<li>SDMatteå¼•å…¥äº†ä¸€ç§æ©è†œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è§†è§‰æç¤ºæŒ‡å®šçš„åŒºåŸŸï¼Œä»è€Œå®ç°æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSDMatteæ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨äº¤äº’å¼æŠ å›¾ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a917487e6dc50f9aa2e47f3bf447adff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd6f9f4704e803c5d52b6bbbf3772fac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88feef2fd2525d079f43b631a707aabb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfeaf2427b1a6fbff11e3f4d65328765.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Accurate-Latent-Inversion-for-Generative-Image-Steganography-via-Rectified-Flow"><a href="#Accurate-Latent-Inversion-for-Generative-Image-Steganography-via-Rectified-Flow" class="headerlink" title="Accurate Latent Inversion for Generative Image Steganography via   Rectified Flow"></a>Accurate Latent Inversion for Generative Image Steganography via   Rectified Flow</h2><p><strong>Authors:Yuqi Qian, Yun Cao, Meiyang Lv, Haocheng Fu</strong></p>
<p>Steganography based on diffusion models has attracted increasing attention due to its ability to generate high-quality images and exhibit strong robustness. In such approaches, the secret message is first embedded into the initial latent variable, and then the stego image is generated through the forward process. To extract the message, an inversion process is required to reconstruct the latent variables from the received image. However, inaccurate latent inversion leads to significant discrepancies between the reconstructed and original latent variables, rendering message extraction infeasible. To address this issue, we propose \textbf{RF-Stego}, a novel generative image steganography method that enables accurate latent inversion and significantly improves extraction accuracy. First, we develop the \textbf{P}ath \textbf{C}onsistency \textbf{L}inear \textbf{I}nversion (\textbf{PCLI}), which imposes formal constraints on the inversion process. By explicitly aligning it with the forward generation path and modeling both directions along a shared linear path, PCLI eliminates path mismatch and ensures path consistency throughout the steganographic process. Second, through rigorous theoretical proof, we demonstrate that \textbf{R}ectified \textbf{F}low \textbf{(RF)} offers both theoretical reversibility and numerical stability in the inversion process. Based on this, we replace traditional unstable samplers with RF sampler which effectively improves the numerical precision of the inversion process. Experimental results show RF-Stego outperforms state-of-the-art methods in terms of extraction accuracy, image quality, robustness, security and generation efficiency. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„éšå†™æœ¯å› å…¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§è€Œè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œé¦–å…ˆå°†ç§˜å¯†ä¿¡æ¯åµŒå…¥åˆ°åˆå§‹æ½œåœ¨å˜é‡ä¸­ï¼Œç„¶åé€šè¿‡å‰å‘è¿‡ç¨‹ç”Ÿæˆéšå†™å›¾åƒã€‚ä¸ºäº†æå–ä¿¡æ¯ï¼Œéœ€è¦è¿›è¡Œåå‘è¿‡ç¨‹æ¥ä»æ¥æ”¶åˆ°çš„å›¾åƒä¸­é‡å»ºæ½œåœ¨å˜é‡ã€‚ç„¶è€Œï¼Œä¸å‡†ç¡®çš„æ½œåœ¨å˜é‡åå‘ä¼šå¯¼è‡´é‡å»ºçš„æ½œåœ¨å˜é‡ä¸åŸå§‹å˜é‡ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä»è€Œä½¿ä¿¡æ¯æå–å˜å¾—ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç”Ÿæˆå›¾åƒéšå†™æœ¯æ–¹æ³•ï¼Œåä¸ºRF-Stegoï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°å‡†ç¡®çš„æ½œåœ¨å˜é‡åå‘ï¼Œå¹¶æ˜¾è‘—æé«˜æå–ç²¾åº¦ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†è·¯å¾„ä¸€è‡´æ€§çº¿æ€§åå‘ï¼ˆPCLIï¼‰ï¼Œå¯¹åå‘è¿‡ç¨‹æ–½åŠ æ­£è§„çº¦æŸã€‚PCLIé€šè¿‡æ˜ç¡®å°†å…¶ä¸å‰å‘ç”Ÿæˆè·¯å¾„å¯¹é½ï¼Œå¹¶å¯¹ä¸¤ä¸ªæ–¹å‘è¿›è¡Œå…±äº«çº¿æ€§è·¯å¾„å»ºæ¨¡ï¼Œæ¶ˆé™¤äº†è·¯å¾„ä¸åŒ¹é…ï¼Œç¡®ä¿äº†éšå†™æœ¯è¿‡ç¨‹ä¸­çš„è·¯å¾„ä¸€è‡´æ€§ã€‚å…¶æ¬¡ï¼Œé€šè¿‡ä¸¥æ ¼çš„ç†è®ºè¯æ˜ï¼Œæˆ‘ä»¬è¯æ˜äº†æ ¡æ­£æµï¼ˆRFï¼‰åœ¨åå‘è¿‡ç¨‹ä¸­æä¾›äº†ç†è®ºä¸Šçš„å¯é€†æ€§å’Œæ•°å€¼ç¨³å®šæ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬ç”¨RFé‡‡æ ·å™¨æ›¿æ¢äº†ä¼ ç»Ÿçš„ä¸ç¨³å®šé‡‡æ ·å™¨ï¼Œæœ‰æ•ˆæé«˜åå‘è¿‡ç¨‹çš„æ•°å€¼ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRF-Stegoåœ¨æå–ç²¾åº¦ã€å›¾åƒè´¨é‡ã€ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œç”Ÿæˆæ•ˆç‡ç­‰æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„éšå†™æœ¯å› å…¶èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶å±•ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§è€Œå¤‡å—å…³æ³¨ã€‚é€šè¿‡å°†ç§˜å¯†ä¿¡æ¯åµŒå…¥åˆå§‹æ½œåœ¨å˜é‡ï¼Œç”Ÿæˆéšå†™å›¾åƒï¼Œå†é€šè¿‡æ­£å‘è¿‡ç¨‹è¿›è¡Œä¼ è¾“ã€‚æå–ä¿¡æ¯æ—¶ï¼Œéœ€è¦é€†å‘è¿‡ç¨‹é‡å»ºæ½œåœ¨å˜é‡ã€‚ç„¶è€Œï¼Œä¸å‡†ç¡®çš„æ½œåœ¨å˜é‡é€†å‘ä¼šå¯¼è‡´é‡å»ºä¸åŸå§‹æ½œåœ¨å˜é‡ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œä½¿å¾—ä¿¡æ¯æå–å˜å¾—ä¸å¯èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹ç”Ÿæˆå›¾åƒéšå†™æœ¯æ–¹æ³•â€”â€”RF-Stegoï¼Œèƒ½å®ç°å‡†ç¡®çš„æ½œåœ¨å˜é‡é€†å‘å¹¶æ˜¾è‘—æé«˜æå–ç²¾åº¦ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘å‡ºè·¯å¾„ä¸€è‡´æ€§çº¿æ€§é€†å‘ï¼ˆPCLIï¼‰ï¼Œå¯¹é€†å‘è¿‡ç¨‹æ–½åŠ æ­£å¼çº¦æŸã€‚å…¶æ¬¡ï¼Œé€šè¿‡ä¸¥æ ¼çš„ç†è®ºè¯æ˜ï¼Œæˆ‘ä»¬å‘ç°æ•´æµæµï¼ˆRFï¼‰åœ¨ç†è®ºä¸Šå…·æœ‰å¯é€†æ€§å’Œæ•°å€¼ç¨³å®šæ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨RFé‡‡æ ·å™¨æ›¿ä»£ä¼ ç»Ÿçš„ä¸ç¨³å®šé‡‡æ ·å™¨ï¼Œæœ‰æ•ˆæé«˜é€†å‘è¿‡ç¨‹çš„æ•°å€¼ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRF-Stegoåœ¨æå–ç²¾åº¦ã€å›¾åƒè´¨é‡ã€ç¨³å¥æ€§ã€å®‰å…¨æ€§å’Œç”Ÿæˆæ•ˆç‡æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨éšå†™æœ¯é¢†åŸŸå—åˆ°å…³æ³¨ï¼Œå› å…¶èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒå¹¶å±•ç°ç¨³å¥æ€§ã€‚</li>
<li>ç°æœ‰éšå†™æœ¯åœ¨æ½œåœ¨å˜é‡é€†å‘æ—¶å­˜åœ¨ä¸å‡†ç¡®é—®é¢˜ï¼Œå¯¼è‡´ä¿¡æ¯æå–å›°éš¾ã€‚</li>
<li>RF-Stegoé€šè¿‡PCLIæŠ€æœ¯å®ç°è·¯å¾„ä¸€è‡´æ€§ï¼Œç¡®ä¿éšå†™è¿‡ç¨‹ä¸­çš„è·¯å¾„åŒ¹é…ã€‚</li>
<li>RFæµå…·æœ‰ç†è®ºä¸Šçš„å¯é€†æ€§å’Œæ•°å€¼ç¨³å®šæ€§ï¼Œèƒ½æé«˜é€†å‘è¿‡ç¨‹çš„ç²¾åº¦ã€‚</li>
<li>é‡‡ç”¨RFé‡‡æ ·å™¨æ›¿ä»£ä¼ ç»Ÿä¸ç¨³å®šé‡‡æ ·å™¨ï¼Œæé«˜æ•°å€¼ç²¾åº¦ã€‚</li>
<li>RF-Stegoåœ¨æå–ç²¾åº¦ã€å›¾åƒè´¨é‡ã€ç¨³å¥æ€§ã€å®‰å…¨æ€§åŠç”Ÿæˆæ•ˆç‡æ–¹é¢è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b8753ca27200c39ceaeba93535ac938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc67c659f15f1459625332b3f3621f5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36fc7f6e2c88831250bd226f0a971e01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd5840f2c5f9c49c4b9019fe0275a6da.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DC-AE-1-5-Accelerating-Diffusion-Model-Convergence-with-Structured-Latent-Space"><a href="#DC-AE-1-5-Accelerating-Diffusion-Model-Convergence-with-Structured-Latent-Space" class="headerlink" title="DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured   Latent Space"></a>DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured   Latent Space</h2><p><strong>Authors:Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, Han Cai</strong></p>
<p>We present DC-AE 1.5, a new family of deep compression autoencoders for high-resolution diffusion models. Increasing the autoencoderâ€™s latent channel number is a highly effective approach for improving its reconstruction quality. However, it results in slow convergence for diffusion models, leading to poorer generation quality despite better reconstruction quality. This issue limits the quality upper bound of latent diffusion models and hinders the employment of autoencoders with higher spatial compression ratios. We introduce two key innovations to address this challenge: i) Structured Latent Space, a training-based approach to impose a desired channel-wise structure on the latent space with front latent channels capturing object structures and latter latent channels capturing image details; ii) Augmented Diffusion Training, an augmented diffusion training strategy with additional diffusion training objectives on object latent channels to accelerate convergence. With these techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better image generation quality than DC-AE-f32c32 while being 4x faster. Code: <a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen">https://github.com/dc-ai-projects/DC-Gen</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DC-AE 1.5ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨å®¶æ—ã€‚å¢åŠ è‡ªç¼–ç å™¨çš„æ½œåœ¨é€šé“æ•°é‡æ˜¯æé«˜å…¶é‡å»ºè´¨é‡çš„ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ä¼šå¯¼è‡´æ‰©æ•£æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å˜æ…¢ï¼Œç»“æœæ˜¯åœ¨é‡å»ºè´¨é‡æé«˜çš„æƒ…å†µä¸‹ç”Ÿæˆè´¨é‡è¾ƒå·®ã€‚è¿™ä¸ªé—®é¢˜é™åˆ¶äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„è´¨é‡ä¸Šé™ï¼Œå¹¶é˜»ç¢äº†ä½¿ç”¨æ›´é«˜ç©ºé—´å‹ç¼©æ¯”çš„è‡ªç¼–ç å™¨ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ši)ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œå¯¹æ½œåœ¨ç©ºé—´æ–½åŠ æ‰€éœ€çš„é€šé“ç»“æ„ï¼Œå‰é¢çš„æ½œåœ¨é€šé“æ•è·å¯¹è±¡ç»“æ„ï¼Œåé¢çš„æ½œåœ¨é€šé“æ•è·å›¾åƒç»†èŠ‚ï¼›ii)å¢å¼ºæ‰©æ•£è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºæ‰©æ•£è®­ç»ƒç­–ç•¥ï¼Œåœ¨å¯¹è±¡æ½œåœ¨é€šé“ä¸Šå¢åŠ æ‰©æ•£è®­ç»ƒç›®æ ‡ä»¥åŠ é€Ÿæ”¶æ•›ã€‚é€šè¿‡è¿™äº›æŠ€æœ¯ï¼ŒDC-AE 1.5åœ¨æ”¶æ•›é€Ÿåº¦å’Œæ‰©æ•£è§„æ¨¡ç»“æœæ–¹é¢ä¼˜äºDC-AEã€‚åœ¨ImageNet 512x512ä¸Šï¼ŒDC-AE-1.5-f64c128åœ¨å›¾åƒç”Ÿæˆè´¨é‡ä¸Šä¼˜äºDC-AE-f32c32ï¼ŒåŒæ—¶é€Ÿåº¦æ›´å¿«ï¼Œä¸ºåŸæ¥çš„å››å€ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-Gen%E3%80%82">https://github.com/dc-ai-projects/DC-Genã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00413v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨DC-AE 1.5åœ¨é«˜åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ä»‹ç»ã€‚é€šè¿‡å¢åŠ è‡ªç¼–ç å™¨çš„æ½œåœ¨é€šé“æ•°é‡å¯æé«˜é‡å»ºè´¨é‡ï¼Œä½†ä¼šå¯¼è‡´æ‰©æ•£æ¨¡å‹æ”¶æ•›é€Ÿåº¦å‡æ…¢ï¼Œå½±å“ç”Ÿæˆè´¨é‡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥ä¸¤é¡¹å…³é”®æŠ€æœ¯ï¼šç»“æ„åŒ–æ½œåœ¨ç©ºé—´å’Œå¢å¼ºæ‰©æ•£è®­ç»ƒã€‚DC-AE 1.5é€šè¿‡è¿™ä¸¤é¡¹æŠ€æœ¯å®ç°äº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ‰©æ•£å°ºåº¦ç»“æœã€‚åœ¨ImageNet 512x512ä¸Šï¼ŒDC-AE 1.5çš„f64c128ç‰ˆæœ¬åœ¨ä¿æŒæ›´å¥½å›¾åƒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œé€Ÿåº¦æ¯”DC-AEçš„f32c32ç‰ˆæœ¬å¿«å››å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DC-AE 1.5æ˜¯æ·±åº¦å‹ç¼©è‡ªç¼–ç å™¨çš„æ–°ç‰ˆæœ¬ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å¢åŠ è‡ªç¼–ç å™¨çš„æ½œåœ¨é€šé“æ•°é‡å¯ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œä½†ä¼šå¯¼è‡´æ‰©æ•£æ¨¡å‹æ”¶æ•›é€Ÿåº¦å‡æ…¢ã€‚</li>
<li>ç»“æ„åŒ–æ½œåœ¨ç©ºé—´æ˜¯ä¸€ç§åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ½œåœ¨ç©ºé—´ä¸Šæ–½åŠ æ‰€éœ€çš„é€šé“ç»“æ„ã€‚</li>
<li>å¢å¼ºæ‰©æ•£è®­ç»ƒæ˜¯ä¸€ç§åŠ é€Ÿæ”¶æ•›çš„ç­–ç•¥ï¼Œé€šè¿‡é¢å¤–çš„æ‰©æ•£è®­ç»ƒç›®æ ‡åœ¨å¯¹è±¡æ½œåœ¨é€šé“ä¸Šå®ç°ã€‚</li>
<li>DC-AE 1.5é€šè¿‡åº”ç”¨ä¸Šè¿°ä¸¤é¡¹æŠ€æœ¯ï¼Œå®ç°äº†æ¯”DC-AEæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´å¥½çš„æ‰©æ•£å°ºåº¦ç»“æœã€‚</li>
<li>åœ¨ImageNet 512x512ä¸Šï¼ŒDC-AE 1.5çš„f64c128ç‰ˆæœ¬åœ¨å›¾åƒç”Ÿæˆè´¨é‡å’Œé€Ÿåº¦ä¸Šå‡è¡¨ç°æ›´ä¼˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d0ec8f96ee785759933264c1cafd56da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a9eb7798c987f66d5ef21e477170277.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14f266b3652c3e6fe52120ddca1e0c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b707d1ed5b899e326d8923f00d200428.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f9b6faaa9aae6d748139a0526af5b4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d11c31301c2dcbce987222803045fd14.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="BOOD-Boundary-based-Out-Of-Distribution-Data-Generation"><a href="#BOOD-Boundary-based-Out-Of-Distribution-Data-Generation" class="headerlink" title="BOOD: Boundary-based Out-Of-Distribution Data Generation"></a>BOOD: Boundary-based Out-Of-Distribution Data Generation</h2><p><strong>Authors:Qilin Liao, Shuo Yang, Bo Zhao, Ping Luo, Hengshuang Zhao</strong></p>
<p>Harnessing the power of diffusion models to synthesize auxiliary training data based on latent space features has proven effective in enhancing out-of-distribution (OOD) detection performance. However, extracting effective features outside the in-distribution (ID) boundary in latent space remains challenging due to the difficulty of identifying decision boundaries between classes. This paper proposes a novel framework called Boundary-based Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD features and generates human-compatible outlier images using diffusion models. BOOD first learns a text-conditioned latent feature space from the ID dataset, selects ID features closest to the decision boundary, and perturbs them to cross the decision boundary to form OOD features. These synthetic OOD features are then decoded into images in pixel space by a diffusion model. Compared to previous works, BOOD provides a more training efficient strategy for synthesizing informative OOD features, facilitating clearer distinctions between ID and OOD data. Extensive experimental results on common benchmarks demonstrate that BOOD surpasses the state-of-the-art method significantly, achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27% improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset. </p>
<blockquote>
<p>åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆè¾…åŠ©è®­ç»ƒæ•°æ®ï¼ŒåŸºäºæ½œåœ¨ç©ºé—´ç‰¹å¾æå‡ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æ€§èƒ½å·²ç»å¾—åˆ°äº†è¯å®ã€‚ç„¶è€Œï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­ï¼Œç”±äºéš¾ä»¥ç¡®å®šç±»ä¹‹é—´çš„å†³ç­–è¾¹ç•Œï¼Œåœ¨åˆ†å¸ƒå†…ï¼ˆIDï¼‰è¾¹ç•Œä¹‹å¤–æå–æœ‰æ•ˆç‰¹å¾ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¾¹ç•Œçš„ç¦»ç¾¤æ•°æ®ç”Ÿæˆï¼ˆBOODï¼‰æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆé«˜è´¨é‡OODç‰¹å¾å¹¶ç”Ÿæˆäººç±»å…¼å®¹çš„å¼‚å¸¸å›¾åƒã€‚BOODé¦–å…ˆå­¦ä¹ æ¥è‡ªIDæ•°æ®é›†çš„æ–‡æœ¬æ¡ä»¶æ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œé€‰æ‹©æœ€æ¥è¿‘å†³ç­–è¾¹ç•Œçš„IDç‰¹å¾ï¼Œå¹¶é€šè¿‡æ‰°åŠ¨å®ƒä»¬æ¥è·¨è¶Šå†³ç­–è¾¹ç•Œä»¥å½¢æˆOODç‰¹å¾ã€‚è¿™äº›åˆæˆçš„OODç‰¹å¾éšåé€šè¿‡æ‰©æ•£æ¨¡å‹åœ¨åƒç´ ç©ºé—´ä¸­è§£ç ä¸ºå›¾åƒã€‚ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒBOODä¸ºåˆæˆä¿¡æ¯ä¸°å¯Œçš„OODç‰¹å¾æä¾›äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œæœ‰åŠ©äºæ›´æ¸…æ¥šåœ°åŒºåˆ†IDå’ŒOODæ•°æ®ã€‚åœ¨å¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒBOODæ˜¾è‘—è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•ï¼Œåœ¨CIFAR-10Oæ•°æ®é›†ä¸Šå®ç°äº†å¹³å‡FPR95é™ä½29.64%ï¼ˆä»40.31%é™è‡³åˆ°ä½è‡³ä½è‡³ä¸ºç‰¹ä½åˆ°ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³ä»…ä½è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™è‡³è‡³é™ä½è‡³çš„é™è‡³è‡³10.67%ï¼‰ï¼Œå¹³å‡AUROCæé«˜äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡äº†æå‡åˆ°äº†äº†äº†äº†äº†äº†äº†äº†äº†äº†äº†äº†äº†æå‡åˆ°é«˜7.27%ï¼ˆä»æé«˜äº†æå‡åˆ°åˆ°æå‡åˆ°æå‡åˆ°é«˜è¾¾è¾¾åˆ°æé«˜æé«˜æé«˜äº†ä»åˆ°æ€»è®¡çš„æåˆ°äº†è¾¾åˆ°æå‡è‡³æ€»å…±æ˜¯æ€»æé«˜åˆ°æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æ˜¯æ€»è®¡æé«˜äº†æ€»è®¡ä¸ºä»é«˜è¾¾è¾¾åˆ°ä»90.15%æé«˜åˆ°è¾¾åˆ°97.42%ï¼‰ã€‚æ•´ä½“è¡¨ç°æ€§èƒ½å¤§å¹…åº¦æå‡è¡¨æ˜äº†BOODæ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿åŠæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00350v1">PDF</a> 14 pages, 8 figures, To be published in the Proceedings of the   International Conference on Machine Learning (ICML) 2025</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆè¾…åŠ©è®­ç»ƒæ•°æ®ï¼ŒåŸºäºæ½œåœ¨ç©ºé—´ç‰¹å¾æé«˜æ£€æµ‹æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¾¹ç•Œçš„å¤–éƒ¨æ•°æ®ç”Ÿæˆï¼ˆBOODï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åˆæˆé«˜è´¨é‡å¤–éƒ¨ç‰¹å¾å¹¶ç”Ÿæˆäººç±»å…¼å®¹çš„å¼‚å¸¸å›¾åƒã€‚BOODå­¦ä¹ æ–‡æœ¬æ¡ä»¶ä¸‹çš„æ½œåœ¨ç‰¹å¾ç©ºé—´ï¼Œé€‰æ‹©æœ€æ¥è¿‘å†³ç­–è¾¹ç•Œçš„å†…éƒ¨ç‰¹å¾ï¼Œé€šè¿‡æ‰°åŠ¨å½¢æˆå¤–éƒ¨ç‰¹å¾ã€‚åˆæˆå¤–éƒ¨ç‰¹å¾åœ¨åƒç´ ç©ºé—´è§£ç ä¸ºå›¾åƒã€‚ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒBOODä¸ºåˆæˆä¿¡æ¯æ€§å¤–éƒ¨ç‰¹å¾æä¾›äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œæ›´æ¸…æ™°åœ°åŒºåˆ†å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBOODæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºåˆæˆè¾…åŠ©è®­ç»ƒæ•°æ®ï¼Œèƒ½æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºBOODçš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å¤–éƒ¨åˆ†å¸ƒç‰¹å¾ã€‚</li>
<li>BOODåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†åˆæˆçš„å¤–éƒ¨ç‰¹å¾è§£ç ä¸ºå›¾åƒã€‚</li>
<li>BOODå­¦ä¹ æ–‡æœ¬æ¡ä»¶ä¸‹çš„æ½œåœ¨ç‰¹å¾ç©ºé—´å¹¶é€‰æ‹©æ¥è¿‘å†³ç­–è¾¹ç•Œçš„å†…éƒ¨ç‰¹å¾ã€‚</li>
<li>é€šè¿‡æ‰°åŠ¨å†…éƒ¨ç‰¹å¾æ¥å½¢æˆå¤–éƒ¨ç‰¹å¾å¹¶è·¨è¶Šå†³ç­–è¾¹ç•Œã€‚</li>
<li>BOODç­–ç•¥èƒ½æ›´æ¸…æ™°åœ°åŒºåˆ†å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®ï¼Œæé«˜åŒºåˆ†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a27f8a5833d151eaa01761cc8eb9aa56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3632922ff54fd9ef43e9a84bf36ae8d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adffebe85d369b45f7e826d502b065c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f46bab455c31aba8278942e9682c2c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-533bb747effbdd80c8a25bc7cbeeb24b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d98f6db8fefd58ac1461bc99e9c7f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fea121cf837e9550564b82c1c7f38d86.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Steering-Guidance-for-Personalized-Text-to-Image-Diffusion-Models"><a href="#Steering-Guidance-for-Personalized-Text-to-Image-Diffusion-Models" class="headerlink" title="Steering Guidance for Personalized Text-to-Image Diffusion Models"></a>Steering Guidance for Personalized Text-to-Image Diffusion Models</h2><p><strong>Authors:Sunghyun Park, Seokeon Choi, Hyoungwoo Park, Sungrack Yun</strong></p>
<p>Personalizing text-to-image diffusion models is crucial for adapting the pre-trained models to specific target concepts, enabling diverse image generation. However, fine-tuning with few images introduces an inherent trade-off between aligning with the target distribution (e.g., subject fidelity) and preserving the broad knowledge of the original model (e.g., text editability). Existing sampling guidance methods, such as classifier-free guidance (CFG) and autoguidance (AG), fail to effectively guide the output toward well-balanced space: CFG restricts the adaptation to the target distribution, while AG compromises text alignment. To address these limitations, we propose personalization guidance, a simple yet effective method leveraging an unlearned weak model conditioned on a null text prompt. Moreover, our method dynamically controls the extent of unlearning in a weak model through weight interpolation between pre-trained and fine-tuned models during inference. Unlike existing guidance methods, which depend solely on guidance scales, our method explicitly steers the outputs toward a balanced latent space without additional computational overhead. Experimental results demonstrate that our proposed guidance can improve text alignment and target distribution fidelity, integrating seamlessly with various fine-tuning strategies. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯¹äºå°†é¢„è®­ç»ƒæ¨¡å‹é€‚åº”äºç‰¹å®šç›®æ ‡æ¦‚å¿µè‡³å…³é‡è¦ï¼Œè¿™èƒ½å¤Ÿå®ç°å¤šæ ·åŒ–çš„å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œä½¿ç”¨å°‘é‡å›¾åƒè¿›è¡Œå¾®è°ƒä¼šåœ¨é€‚åº”ç›®æ ‡åˆ†å¸ƒï¼ˆä¾‹å¦‚ä¸»é¢˜ä¿çœŸåº¦ï¼‰å’Œä¿ç•™åŸå§‹æ¨¡å‹çš„å¹¿æ³›çŸ¥è¯†ï¼ˆä¾‹å¦‚æ–‡æœ¬å¯ç¼–è¾‘æ€§ï¼‰ä¹‹é—´å¼•å…¥å›ºæœ‰çš„æƒè¡¡ã€‚ç°æœ‰çš„é‡‡æ ·æŒ‡å¯¼æ–¹æ³•ï¼Œå¦‚æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆCFGï¼‰å’Œè‡ªåŠ¨æŒ‡å¯¼ï¼ˆAGï¼‰ï¼Œæ— æ³•æœ‰æ•ˆåœ°å°†è¾“å‡ºå¯¼å‘å¹³è¡¡çš„ç©ºé—´ï¼šCFGé™åˆ¶äº†ç›®æ ‡åˆ†å¸ƒçš„é€‚åº”ï¼Œè€ŒAGåˆ™å¦¥åäº†æ–‡æœ¬å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–æŒ‡å¯¼æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨æœªå­¦ä¹ çš„å¼±æ¨¡å‹ä»¥ç©ºæ–‡æœ¬æç¤ºä¸ºæ¡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ¨æ–­æœŸé—´åœ¨é¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ä¹‹é—´è¿›è¡Œæƒé‡æ’å€¼ï¼ŒåŠ¨æ€æ§åˆ¶å¼±æ¨¡å‹çš„æœªå­¦ä¹ ç¨‹åº¦ã€‚ä¸ç°æœ‰ä¾èµ–æŒ‡å¯¼å°ºåº¦çš„æŒ‡å¯¼æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ˜ç¡®åœ°å°†è¾“å‡ºå¯¼å‘å¹³è¡¡æ½œåœ¨ç©ºé—´ï¼Œè€Œæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æŒ‡å¯¼æ–¹æ³•èƒ½å¤Ÿæ”¹è¿›æ–‡æœ¬å¯¹é½å’Œç›®æ ‡åˆ†å¸ƒä¿çœŸåº¦ï¼Œæ— ç¼é›†æˆå„ç§å¾®è°ƒç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00319v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼ŒæŒ‡å‡ºåœ¨é€‚åº”é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šç›®æ ‡æ¦‚å¿µæ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºä¸€ç§æ–°å‹çš„ä¸ªäººåŒ–æŒ‡å¯¼æ–¹æ³•ï¼Œåˆ©ç”¨æœªå­¦ä¹ çš„å¼±æ¨¡å‹å¹¶å¯¹å…¶è¿›è¡ŒåŠ¨æ€æ§åˆ¶ï¼Œä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°å¹³è¡¡è¾“å‡ºç©ºé—´ã€‚æ­¤æ–¹æ³•é€šè¿‡æƒé‡æ’å€¼å®ç°é¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ä¹‹é—´çš„å¹³è¡¡ï¼Œä»è€Œæé«˜æ–‡æœ¬å¯¹é½å’Œç›®æ ‡åˆ†å¸ƒä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–æ˜¯å…³é”®ï¼Œå°¤å…¶åœ¨é€‚åº”é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šç›®æ ‡æ¦‚å¿µæ—¶ã€‚</li>
<li>ç°å­˜çš„é‡‡æ ·æŒ‡å¯¼æ–¹æ³•å¦‚æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆCFGï¼‰å’Œè‡ªåŠ¨æŒ‡å¯¼ï¼ˆAGï¼‰éš¾ä»¥æœ‰æ•ˆå®ç°ç›®æ ‡åˆ†å¸ƒå¯¹é½ä¸ä¿æŒåŸå§‹çŸ¥è¯†ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„ä¸ªäººåŒ–æŒ‡å¯¼æ–¹æ³•åˆ©ç”¨æœªå­¦ä¹ çš„å¼±æ¨¡å‹ï¼Œé€šè¿‡æƒé‡æ’å€¼åœ¨é¢„è®­ç»ƒæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹é—´åŠ¨æ€æ§åˆ¶è¾“å‡ºç©ºé—´ã€‚</li>
<li>ä¸ç°æœ‰æŒ‡å¯¼æ–¹æ³•ä¸åŒï¼Œæ–°æ–¹æ³•å¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ˜ç¡®å¼•å¯¼è¾“å‡ºè¾¾åˆ°å¹³è¡¡æ½œåœ¨ç©ºé—´ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ–°æå‡ºçš„æŒ‡å¯¼æ–¹æ³•å¯ä»¥æé«˜æ–‡æœ¬å¯¹é½å’Œç›®æ ‡åˆ†å¸ƒä¿çœŸåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°å„ç§å¾®è°ƒç­–ç•¥ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be4d8d42ce76c59aab5eab8ce79e6c6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93d4322bb676557be9cea8f3eda24cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a3da7980f8fb13a57d95d240d181fd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bc0e3af2c0215a77f7b35d736b07909.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-711c78ebe1b5aa3e080963329785c1e8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Jet-Image-Generation-in-High-Energy-Physics-Using-Diffusion-Models"><a href="#Jet-Image-Generation-in-High-Energy-Physics-Using-Diffusion-Models" class="headerlink" title="Jet Image Generation in High Energy Physics Using Diffusion Models"></a>Jet Image Generation in High Energy Physics Using Diffusion Models</h2><p><strong>Authors:Victor D. Martinez, Vidya Manian, Sudhir Malik</strong></p>
<p>This article presents, for the first time, the application of diffusion models for generating jet images corresponding to proton-proton collision events at the Large Hadron Collider (LHC). The kinematic variables of quark, gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset are mapped to two-dimensional image representations. Diffusion models are trained on these images to learn the spatial distribution of jet constituents. We compare the performance of score-based diffusion models and consistency models in accurately generating class-conditional jet images. Unlike approaches based on latent distributions, our method operates directly in image space. The fidelity of the generated images is evaluated using several metrics, including the Fr&#39;echet Inception Distance (FID), which demonstrates that consistency models achieve higher fidelity and generation stability compared to score-based diffusion models. These advancements offer significant improvements in computational efficiency and generation accuracy, providing valuable tools for High Energy Physics (HEP) research. </p>
<blockquote>
<p>æœ¬æ–‡é¦–æ¬¡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¤§å‹å¼ºå­å¯¹æ’æœºï¼ˆLHCï¼‰è´¨å­-è´¨å­ç¢°æ’äº‹ä»¶å¯¹åº”çš„å–·å°„å›¾åƒä¸­çš„åº”ç”¨ã€‚JetNetæ¨¡æ‹Ÿæ•°æ®é›†ä¸­çš„å¤¸å…‹ã€èƒ¶å­ã€Wç»è‰²å­ã€Zç»è‰²å­å’Œé¡¶çº§å¤¸å…‹å–·å°„çš„è¿åŠ¨å­¦å˜é‡è¢«æ˜ å°„åˆ°äºŒç»´å›¾åƒè¡¨ç¤ºã€‚æ‰©æ•£æ¨¡å‹åœ¨è¿™äº›å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å–·å°„æˆåˆ†çš„ç©ºé—´åˆ†å¸ƒã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å’Œä¸€è‡´æ€§æ¨¡å‹åœ¨å‡†ç¡®ç”Ÿæˆç±»æ¡ä»¶å–·å°„å›¾åƒæ–¹é¢çš„æ€§èƒ½ã€‚ä¸åŒäºåŸºäºæ½œåœ¨åˆ†å¸ƒçš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥åœ¨å›¾åƒç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚ç”Ÿæˆçš„å›¾åƒçš„ä¿çœŸåº¦é€šè¿‡åŒ…æ‹¬FrÃ©chet Inception Distanceï¼ˆFIDï¼‰åœ¨å†…çš„å¤šä¸ªæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜ä¸€è‡´æ€§æ¨¡å‹åœ¨ä¿çœŸåº¦å’Œç”Ÿæˆç¨³å®šæ€§æ–¹é¢ä¼˜äºåŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹ã€‚è¿™äº›è¿›æ­¥åœ¨è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆå‡†ç¡®æ€§æ–¹é¢æä¾›äº†é‡å¤§æ”¹è¿›ï¼Œä¸ºé«˜èƒ½ç‰©ç†å­¦ï¼ˆHEPï¼‰ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00250v1">PDF</a> The paper is under review at IEEE Transactions in Nuclear Science</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨å¤§å‹å¼ºå­å¯¹æ’æœºï¼ˆLHCï¼‰è´¨å­-è´¨å­ç¢°æ’äº‹ä»¶ä¸­çš„å–·å°„å›¾åƒç”Ÿæˆåº”ç”¨ã€‚æ–‡ç« åˆ©ç”¨JetNetæ¨¡æ‹Ÿæ•°æ®é›†å°†å¤¸å…‹ã€èƒ¶å­ã€Wç»è‰²å­ã€Zç»è‰²å­å’Œé¡¶çº§å–·å°„ç‰©çš„è¿åŠ¨å˜é‡æ˜ å°„æˆäºŒç»´å›¾åƒè¡¨ç¤ºã€‚æ‰©æ•£æ¨¡å‹åœ¨è¿™äº›å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å–·å°„æˆåˆ†çš„ç©ºé—´åˆ†å¸ƒã€‚æ–‡ç« æ¯”è¾ƒäº†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å’Œä¸€è‡´æ€§æ¨¡å‹åœ¨ç”Ÿæˆç±»æ¡ä»¶å–·å°„å›¾åƒæ–¹é¢çš„å‡†ç¡®æ€§ã€‚ä¸åŸºäºæ½œåœ¨åˆ†å¸ƒçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨å›¾åƒç©ºé—´ä¸­è¿›è¡Œæ“ä½œã€‚ç”Ÿæˆçš„å›¾åƒçš„ä¿çœŸåº¦é€šè¿‡åŒ…æ‹¬FrÃ©chet Inception Distanceï¼ˆFIDï¼‰åœ¨å†…çš„å¤šä¸ªæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜ä¸€è‡´æ€§æ¨¡å‹ç›¸è¾ƒäºåŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦å’Œç”Ÿæˆç¨³å®šæ€§ã€‚è¿™äº›è¿›å±•åœ¨è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆå‡†ç¡®æ€§æ–¹é¢æä¾›äº†é‡å¤§æ”¹è¿›ï¼Œä¸ºé«˜èƒ½ç‰©ç†å­¦ï¼ˆHEPï¼‰ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºå¤§å‹å¼ºå­å¯¹æ’æœºçš„è´¨å­-è´¨å­ç¢°æ’äº‹ä»¶çš„å–·å°„å›¾åƒç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨JetNetæ¨¡æ‹Ÿæ•°æ®é›†å°†è¿åŠ¨å˜é‡æ˜ å°„æˆäºŒç»´å›¾åƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç©ºé—´ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å–·å°„æˆåˆ†çš„ç©ºé—´åˆ†å¸ƒã€‚</li>
<li>æ¯”è¾ƒäº†åŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å’Œä¸€è‡´æ€§æ¨¡å‹åœ¨ç”Ÿæˆç±»æ¡ä»¶å–·å°„å›¾åƒæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ä¸€è‡´æ€§æ¨¡å‹ç›¸è¾ƒäºåŸºäºåˆ†æ•°çš„æ‰©æ•£æ¨¡å‹å…·æœ‰æ›´é«˜çš„å›¾åƒç”Ÿæˆä¿çœŸåº¦å’Œç¨³å®šæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºé«˜èƒ½ç‰©ç†å­¦ç ”ç©¶æä¾›äº†é‡è¦çš„å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—æ•ˆç‡å’Œç”Ÿæˆå‡†ç¡®æ€§æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2a274906585d9a8aef653dc6e90c312e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a06de0bd420de706d70b9b1e3d6a12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47e995d14bf61700b3df7ac5fe28c28f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22e5ff5f34a7446a518ce06ade17ab8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3265668719b22589a3bff0e1c7083066.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DCT-Shield-A-Robust-Frequency-Domain-Defense-against-Malicious-Image-Editing"><a href="#DCT-Shield-A-Robust-Frequency-Domain-Defense-against-Malicious-Image-Editing" class="headerlink" title="DCT-Shield: A Robust Frequency Domain Defense against Malicious Image   Editing"></a>DCT-Shield: A Robust Frequency Domain Defense against Malicious Image   Editing</h2><p><strong>Authors:Aniruddha Bala, Rohit Chowdhury, Rohan Jaiswal, Siddharth Roheda</strong></p>
<p>Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ä½¿å¾—é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œç®€å•çš„å›¾åƒç¼–è¾‘æˆä¸ºå¯èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å‘äº†äººä»¬å¯¹å›¾åƒå®‰å…¨çš„æ‹…å¿§ã€‚èƒ½å¤Ÿè®¿é—®ç”¨æˆ·å›¾åƒçš„æ”»å‡»è€…å¯èƒ½ä¼šåˆ©ç”¨è¿™äº›å·¥å…·è¿›è¡Œæ¶æ„ç¼–è¾‘ã€‚æœ€è¿‘çš„é˜²å¾¡æ–¹æ³•è¯•å›¾é€šè¿‡åœ¨åƒç´ ç©ºé—´æ·»åŠ æœ‰é™å™ªå£°æ¥ä¿æŠ¤å›¾åƒï¼Œä»¥ç ´ååŸºäºæ‰©æ•£çš„ç¼–è¾‘æ¨¡å‹çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œä»¥å‰æ–¹æ³•æ·»åŠ çš„å¯¹æŠ—æ€§å™ªå£°å¾ˆå®¹æ˜“è¢«äººçœ¼å¯Ÿè§‰ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¸­çš„å¤§å¤šæ•°åœ¨å¯è¡Œçš„åƒç´ é¢„ç®—ä¸‹å¯¹JPEGå‹ç¼©ç­‰å‡€åŒ–æŠ€æœ¯å¹¶ä¸ç¨³å¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¾“å…¥å›¾åƒçš„ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç³»æ•°ä¸­å¼•å…¥å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œç›´æ¥åœ¨é¢‘åŸŸè¿›è¡Œæ‰°åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨JPEGç®¡é“ç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒï¼Œæœ‰æ•ˆé˜²æ­¢æ¶æ„å›¾åƒç¼–è¾‘ã€‚åœ¨å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼•å…¥è¾ƒå°‘è§†è§‰ä¼ªå½±çš„åŒæ—¶ï¼Œä¿æŒäº†ç±»ä¼¼çš„ç¼–è¾‘ä¿æŠ¤æ°´å¹³å’Œå¯¹æŠ—å™ªå£°å‡€åŒ–æŠ€æœ¯çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17894v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹æŠ€æœ¯çš„è¿›æ­¥ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œå›¾åƒç¼–è¾‘å˜å¾—æ›´åŠ è½»æ¾ï¼Œè¿™ä¹Ÿå¼•å‘äº†äººä»¬å¯¹å›¾åƒå®‰å…¨çš„æ‹…å¿§ã€‚æ”»å‡»è€…å¯èƒ½ä¼šåˆ©ç”¨è¿™äº›å·¥å…·å¯¹ç”¨æˆ·å›¾åƒè¿›è¡Œæ¶æ„ç¼–è¾‘ã€‚æœ€è¿‘çš„é˜²å¾¡æ–¹æ³•è¯•å›¾é€šè¿‡æ·»åŠ åƒç´ ç©ºé—´ä¸­çš„æœ‰é™å™ªå£°æ¥ä¿æŠ¤å›¾åƒï¼Œä»è€Œç ´ååŸºäºæ‰©æ•£çš„ç¼–è¾‘æ¨¡å‹çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œä»¥å‰æ–¹æ³•æ·»åŠ çš„å¯¹æŠ—æ€§å™ªå£°å¾ˆå®¹æ˜“è¢«äººçœ¼å¯Ÿè§‰ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•ä¸­çš„å¤§å¤šæ•°åœ¨å¯è¡Œçš„åƒç´ é¢„ç®—ä¸‹å¹¶ä¸é€‚ç”¨äºå‡€åŒ–æŠ€æœ¯ï¼Œå¦‚JPEGå‹ç¼©ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡åœ¨é¢‘ç‡åŸŸä¸­ç›´æ¥ä¿®æ”¹è¾“å…¥å›¾åƒçš„ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç³»æ•°æ¥å¼•å…¥å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚é€šè¿‡åˆ©ç”¨JPEGç®¡é“ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆå¯¹æŠ—æ€§å›¾åƒï¼Œæœ‰æ•ˆé˜²æ­¢æ¶æ„å›¾åƒç¼–è¾‘ã€‚åœ¨å¤šç§ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥çš„è§†è§‰ä¼ªå½±æ›´å°‘ï¼ŒåŒæ—¶ä¿æŒäº†ç±»ä¼¼çš„ç¼–è¾‘ä¿æŠ¤æ°´å¹³å’Œå¯¹æŠ—å™ªå£°å‡€åŒ–æŠ€æœ¯çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¿›æ­¥ä½¿å¾—é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œå›¾åƒç¼–è¾‘å˜å¾—è½»æ¾ï¼Œå¼•å‘å¯¹å›¾åƒå®‰å…¨çš„æ‹…å¿§ã€‚</li>
<li>æ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™äº›å·¥å…·è¿›è¡Œæ¶æ„å›¾åƒç¼–è¾‘ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æ–¹æ³•é€šè¿‡æ·»åŠ åƒç´ ç©ºé—´ä¸­çš„å™ªå£°æ¥ä¿æŠ¤å›¾åƒï¼Œä½†è¿™ç§æ–¹æ³•å®¹æ˜“è¢«å¯Ÿè§‰å¹¶ä¸å¤Ÿç¨³å¥ã€‚</li>
<li>æå‡ºçš„ä¼˜åŒ–æ–¹æ³•åœ¨é¢‘ç‡åŸŸå¼•å…¥å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œé€šè¿‡ä¿®æ”¹è¾“å…¥å›¾åƒçš„ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰ç³»æ•°æ¥é˜²å¾¡æ¶æ„ç¼–è¾‘ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆçš„å¯¹æŠ—æ€§å›¾åƒèƒ½æœ‰æ•ˆé˜²æ­¢æ¶æ„å›¾åƒç¼–è¾‘ã€‚</li>
<li>æ–¹æ³•åœ¨å¹¿æ³›å®éªŒä¸‹è¡¨ç°å‡ºè¾ƒå°‘çš„è§†è§‰ä¼ªå½±å’Œè¾ƒé«˜çš„ç¼–è¾‘ä¿æŠ¤æ°´å¹³åŠç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-059af042d155f5269abab5f4db920650.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5640c7db3af90df708be1057e0ee747f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d41a61df492ac705cebf63bb9a76757.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Generating-Novel-Brain-Morphology-by-Deforming-Learned-Templates"><a href="#Generating-Novel-Brain-Morphology-by-Deforming-Learned-Templates" class="headerlink" title="Generating Novel Brain Morphology by Deforming Learned Templates"></a>Generating Novel Brain Morphology by Deforming Learned Templates</h2><p><strong>Authors:Alan Q. Wang, Fangrui Huang, Bailey Trang, Wei Peng, Mohammad Abbasi, Kilian Pohl, Mert Sabuncu, Ehsan Adeli</strong></p>
<p>Designing generative models for 3D structural brain MRI that synthesize morphologically-plausible and attribute-specific (e.g., age, sex, disease state) samples is an active area of research. Existing approaches based on frameworks like GANs or diffusion models synthesize the image directly, which may limit their ability to capture intricate morphological details. In this work, we propose a 3D brain MRI generation method based on state-of-the-art latent diffusion models (LDMs), called MorphLDM, that generates novel images by applying synthesized deformation fields to a learned template. Instead of using a reconstruction-based autoencoder (as in a typical LDM), our encoder outputs a latent embedding derived from both an image and a learned template that is itself the output of a template decoder; this latent is passed to a deformation field decoder, whose output is applied to the learned template. A registration loss is minimized between the original image and the deformed template with respect to the encoder and both decoders. Empirically, our approach outperforms generative baselines on metrics spanning image diversity, adherence with respect to input conditions, and voxel-based morphometry. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm">https://github.com/alanqrwang/morphldm</a>. </p>
<blockquote>
<p>è®¾è®¡é’ˆå¯¹3Dç»“æ„è„‘MRIçš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åˆæˆå½¢æ€ä¸Šåˆç†ä¸”å…·æœ‰ç‰¹å®šå±æ€§ï¼ˆä¾‹å¦‚å¹´é¾„ã€æ€§åˆ«ã€ç–¾ç—…çŠ¶æ€ï¼‰çš„æ ·æœ¬æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚ç°æœ‰åŸºäºGANæˆ–æ‰©æ•£æ¨¡å‹ç­‰æ¡†æ¶çš„æ–¹æ³•ç›´æ¥åˆæˆå›¾åƒï¼Œè¿™å¯èƒ½é™åˆ¶äº†å®ƒä»¬æ•æ‰å¤æ‚å½¢æ€ç»†èŠ‚çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„3Dè„‘MRIç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºMorphLDMã€‚å®ƒé€šè¿‡åº”ç”¨åˆæˆå˜å½¢åœºåˆ°ä¸€ä¸ªå­¦ä¹ åˆ°çš„æ¨¡æ¿æ¥ç”Ÿæˆæ–°çš„å›¾åƒã€‚æˆ‘ä»¬çš„ç¼–ç å™¨è¾“å‡ºçš„æ½œåœ¨åµŒå…¥æ¥æºäºå›¾åƒå’Œå­¦ä¹ åˆ°çš„æ¨¡æ¿æœ¬èº«ï¼Œè¿™ä¸ªæ¨¡æ¿æ˜¯æ¨¡æ¿è§£ç å™¨çš„è¾“å‡ºï¼›è¿™ä¸ªæ½œåœ¨åµŒå…¥è¢«ä¼ é€’ç»™å˜å½¢åœºè§£ç å™¨ï¼Œå…¶è¾“å‡ºåº”ç”¨äºå­¦ä¹ åˆ°çš„æ¨¡æ¿ã€‚é€šè¿‡æœ€å°åŒ–åŸå§‹å›¾åƒå’Œå˜å½¢æ¨¡æ¿ä¹‹é—´çš„æ³¨å†ŒæŸå¤±ï¼Œå…³äºç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨çš„æŸå¤±è¢«æœ€å°åŒ–ã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå¤šæ ·æ€§ã€å¯¹è¾“å…¥æ¡ä»¶çš„éµå¾ªæ€§ä»¥åŠåŸºäºä½“ç´ çš„å½¢æ€æµ‹é‡ç­‰æŒ‡æ ‡ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†åŸºå‡†ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alanqrwang/morphldmä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03778v3">PDF</a> Provisional Acceptance at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„3Dè„‘MRIç”Ÿæˆæ–¹æ³•MorphLDMï¼Œé€šè¿‡åº”ç”¨åˆæˆå˜å½¢åœºäºå­¦ä¹ æ¨¡æ¿ï¼Œç”Ÿæˆæ–°çš„å›¾åƒã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸åŒäºå…¸å‹LDMçš„é‡å»ºå¼è‡ªç¼–ç å™¨ï¼Œå…¶ç¼–ç å™¨è¾“å‡ºçš„æ˜¯æ¥è‡ªå›¾åƒå’Œå­¦ä¹ æ¨¡æ¿çš„æ½œåœ¨åµŒå…¥ï¼Œè¯¥æ½œåœ¨åµŒå…¥ä¼ é€’ç»™å˜å½¢åœºè§£ç å™¨ï¼Œå…¶è¾“å‡ºåº”ç”¨äºå­¦ä¹ æ¨¡æ¿ã€‚é€šè¿‡æœ€å°åŒ–åŸå§‹å›¾åƒä¸å˜å½¢æ¨¡æ¿ä¹‹é—´çš„æ³¨å†ŒæŸå¤±ï¼Œå…³äºç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå¤šæ ·æ€§ã€ç¬¦åˆè¾“å…¥æ¡ä»¶ä»¥åŠä½“ç´ å½¢æ€æµ‹é‡æ–¹é¢ä¼˜äºåŸºå‡†ç”Ÿæˆæ¨¡å‹ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è®¾è®¡ç”Ÿæˆæ¨¡å‹ä»¥åˆæˆå…·æœ‰å½¢æ€åˆç†æ€§å’Œç‰¹å®šå±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ç–¾ç—…çŠ¶æ€ï¼‰çš„3Dç»“æ„è„‘MRIæ ·æœ¬ã€‚</li>
<li>å½“å‰åŸºäºGANæˆ–æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•å¯èƒ½æ— æ³•æ•æ‰å¤æ‚çš„å½¢æ€ç»†èŠ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„3Dè„‘MRIç”Ÿæˆæ–¹æ³•ï¼Œåä¸ºMorphLDMã€‚</li>
<li>MorphLDMé€šè¿‡åº”ç”¨åˆæˆå˜å½¢åœºåˆ°å­¦ä¹ æ¨¡æ¿ç”Ÿæˆæ–°å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨ä¸åŒäºå…¸å‹çš„LDMçš„ç¼–ç å™¨ç»“æ„ï¼Œç»“åˆäº†å›¾åƒå’Œå­¦ä¹ æ¨¡æ¿çš„æ½œåœ¨åµŒå…¥ã€‚</li>
<li>é€šè¿‡æœ€å°åŒ–åŸå§‹å›¾åƒå’Œå˜å½¢æ¨¡æ¿ä¹‹é—´çš„æ³¨å†ŒæŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å›¾åƒå¤šæ ·æ€§ã€éµå¾ªè¾“å…¥æ¡ä»¶ä»¥åŠåŸºäºä½“ç´ çš„å½¢æ€æµ‹é‡æ–¹é¢è¶…è¿‡äº†åŸºå‡†ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-635c7c20058627ea115fbf7b3a325670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5592fa3279ad2dff6283f3ba0f8e793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f1fb47401ef9a0efd5831d0c1822cc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c30907be9e88147d3a507b7df4033fca.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FaceLift-Learning-Generalizable-Single-Image-3D-Face-Reconstruction-from-Synthetic-Heads"><a href="#FaceLift-Learning-Generalizable-Single-Image-3D-Face-Reconstruction-from-Synthetic-Heads" class="headerlink" title="FaceLift: Learning Generalizable Single Image 3D Face Reconstruction   from Synthetic Heads"></a>FaceLift: Learning Generalizable Single Image 3D Face Reconstruction   from Synthetic Heads</h2><p><strong>Authors:Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu</strong></p>
<p>We present FaceLift, a novel feed-forward approach for generalizable high-quality 360-degree 3D head reconstruction from a single image. Our pipeline first employs a multi-view latent diffusion model to generate consistent side and back views from a single facial input, which then feeds into a transformer-based reconstructor that produces a comprehensive 3D Gaussian splats representation. Previous methods for monocular 3D face reconstruction often lack full view coverage or view consistency due to insufficient multi-view supervision. We address this by creating a high-quality synthetic head dataset that enables consistent supervision across viewpoints. To bridge the domain gap between synthetic training data and real-world images, we propose a simple yet effective technique that ensures the view generation process maintains fidelity to the input by learning to reconstruct the input image alongside the view generation. Despite being trained exclusively on synthetic data, our method demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art 3D face reconstruction methods on identity preservation, detail recovery, and rendering quality. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FaceLiftï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å‰é¦ˆæ–¹æ³•ï¼Œç”¨äºä»å•ä¸€å›¾åƒè¿›è¡Œå¯æ³›åŒ–çš„é«˜è´¨é‡360åº¦3Då¤´éƒ¨é‡å»ºã€‚æˆ‘ä»¬çš„ç®¡é“é¦–å…ˆé‡‡ç”¨å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»å•ä¸ªé¢éƒ¨è¾“å…¥ç”Ÿæˆä¸€è‡´çš„ä¾§é¢å’ŒèƒŒé¢è§†å›¾ï¼Œç„¶åå°†å…¶è¾“å…¥åŸºäºå˜å‹å™¨çš„é‡å»ºå™¨ï¼Œç”Ÿæˆå…¨é¢çš„3Dé«˜æ–¯splatè¡¨ç¤ºã€‚ä»¥å‰çš„æ–¹æ³•åœ¨å•ç›®3Dé¢éƒ¨é‡å»ºä¸­å¸¸å¸¸ç¼ºä¹å…¨è§†è§’è¦†ç›–æˆ–è§†è§’ä¸€è‡´æ€§ï¼Œå› ä¸ºç¼ºä¹è¶³å¤Ÿçš„å¤šè§†è§’ç›‘ç£ã€‚æˆ‘ä»¬é€šè¿‡åˆ›å»ºé«˜è´¨é‡åˆæˆå¤´éƒ¨æ•°æ®é›†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ•°æ®é›†å¯ä»¥åœ¨ä¸åŒè§†è§’è¿›è¡Œä¸€è‡´ç›‘ç£ã€‚ä¸ºäº†ç¼©å°åˆæˆè®­ç»ƒæ•°æ®å’ŒçœŸå®ä¸–ç•Œå›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç¡®ä¿è§†å›¾ç”Ÿæˆè¿‡ç¨‹é€šè¿‡å­¦ä¹ ä¸è§†å›¾ç”Ÿæˆä¸€èµ·é‡å»ºè¾“å…¥å›¾åƒæ¥ä¿æŒå¯¹è¾“å…¥çš„ä¿çœŸåº¦ã€‚å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨çœŸå®ä¸–ç•Œå›¾åƒä¸Šå´è¡¨ç°å‡ºäº†æƒŠäººçš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†FaceLiftåœ¨èº«ä»½ä¿ç•™ã€ç»†èŠ‚æ¢å¤å’Œæ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºæœ€æ–°çš„3Dé¢éƒ¨é‡å»ºæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17812v2">PDF</a> ICCV 2025 Camera-Ready Version. Project Page:   <a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift">https://weijielyu.github.io/FaceLift</a></p>
<p><strong>Summary</strong></p>
<p>FaceLiftæ˜¯ä¸€ç§æ–°å‹å‰é¦ˆæ–¹æ³•ï¼Œç”¨äºä»å•ä¸€å›¾åƒå®ç°å¯æ³›åŒ–çš„é«˜è´¨é‡360åº¦3Då¤´éƒ¨é‡å»ºã€‚å®ƒé€šè¿‡å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´çš„ä¾§é¢å’ŒèƒŒé¢è§†è§’ï¼Œç„¶ååˆ©ç”¨åŸºäºå˜å‹å™¨çš„é‡å»ºå™¨ç”Ÿæˆå…¨é¢çš„3Dé«˜æ–¯splatè¡¨ç¤ºã€‚è¯¥æ–¹æ³•è§£å†³äº†å› ç¼ºä¹å¤šè§†è§’ç›‘ç£è€Œå¯¼è‡´çš„å‰è§†å›¾è¦†ç›–ä¸è¶³æˆ–è§†è§’ä¸ä¸€è‡´çš„é—®é¢˜ã€‚é€šè¿‡åˆ›å»ºé«˜è´¨é‡åˆæˆå¤´éƒ¨æ•°æ®é›†å®ç°è·¨è§†è§’çš„ä¸€è‡´ç›‘ç£ã€‚åŒæ—¶æå‡ºç®€å•æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç¡®ä¿è§†å›¾ç”Ÿæˆè¿‡ç¨‹ä¿æŒå¯¹è¾“å…¥çš„å¿ å®åº¦ï¼Œå¹¶åœ¨è§†å›¾ç”Ÿæˆçš„åŒæ—¶è¿›è¡Œé‡å»ºè¾“å…¥å›¾åƒçš„å­¦ä¹ ã€‚å°½ç®¡ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è¯¥æ–¹æ³•åœ¨çœŸå®å›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨èº«ä»½ä¿ç•™ã€ç»†èŠ‚æ¢å¤å’Œæ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FaceLiftæ˜¯ä¸€ç§ç”¨äºä»å•ä¸€å›¾åƒè¿›è¡Œ3Då¤´éƒ¨é‡å»ºçš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹å’ŒåŸºäºå˜å‹å™¨çš„é‡å»ºå™¨ç”Ÿæˆå…¨é¢ä¸”é«˜è´¨é‡çš„3Då¤´éƒ¨è¡¨ç¤ºã€‚</li>
<li>è§£å†³äº†å› ç¼ºä¹å¤šè§†è§’ç›‘ç£è€Œå¯¼è‡´çš„å‰è§†å›¾è¦†ç›–ä¸è¶³æˆ–è§†è§’ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨åˆæˆå¤´éƒ¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†è·¨è§†è§’çš„ä¸€è‡´ç›‘ç£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç¡®ä¿è§†å›¾ç”Ÿæˆè¿‡ç¨‹å¿ å®äºè¾“å…¥ã€‚</li>
<li>æ–¹æ³•åœ¨çœŸå®å›¾åƒä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-129fb9c008f14623efb54ddd18e56e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d792376bd62220383925d54d80accb99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd6a3554f78441437c635b68d0221071.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a39094fc068d33f638092dd98a5656f6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-96a2b224c4fec8217555faa576acc1d1.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  Numerical Uncertainty in Linear Registration An Experimental Study
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5e48a641b7ca8867e72c919cd104a6e8.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  A Conditional GAN for Tabular Data Generation with Probabilistic   Sampling of Latent Subspaces
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
