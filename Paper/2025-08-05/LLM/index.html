<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  Adacc Adaptive Compression and Activation Checkpointing for LLM Memory   Management">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-69f1e105d34c0d7871615cd69ff366d0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-05-æ›´æ–°"><a href="#2025-08-05-æ›´æ–°" class="headerlink" title="2025-08-05 æ›´æ–°"></a>2025-08-05 æ›´æ–°</h1><h2 id="Adacc-Adaptive-Compression-and-Activation-Checkpointing-for-LLM-Memory-Management"><a href="#Adacc-Adaptive-Compression-and-Activation-Checkpointing-for-LLM-Memory-Management" class="headerlink" title="Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory   Management"></a>Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory   Management</h2><p><strong>Authors:Ping Chen, Zhuohong Deng, Ping Li, Shuibing He, Hongzi Zhu, Yi Zheng, Zhefeng Wang, Baoxing Huai, Minyi Guo</strong></p>
<p>Training large language models often employs recomputation to alleviate memory pressure, which can introduce up to 30% overhead in real-world scenarios. In this paper, we propose Adacc, a novel memory management framework that combines adaptive compression and activation checkpointing to reduce the GPU memory footprint. It comprises three modules: (1) We design layer-specific compression algorithms that account for outliers in LLM tensors, instead of directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We propose an optimal scheduling policy that employs MILP to determine the best memory optimization for each tensor. (3) To accommodate changes in training tensors, we introduce an adaptive policy evolution mechanism that adjusts the policy during training to enhance throughput. Experimental results show that Adacc can accelerate the LLM training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy to the Baseline. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œé€šå¸¸é‡‡ç”¨é‡æ–°è®¡ç®—æ¥ç¼“è§£å†…å­˜å‹åŠ›ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­å¯èƒ½ä¼šå¼•å…¥é«˜è¾¾30%çš„å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Adaccï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å†…å­˜ç®¡ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡è‡ªé€‚åº”å‹ç¼©å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯æ¥å‡å°‘GPUå†…å­˜å ç”¨ã€‚å®ƒåŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬è®¾è®¡äº†é’ˆå¯¹LLMå¼ é‡å¼‚å¸¸å€¼çš„ç‰¹å®šå±‚å‹ç¼©ç®—æ³•ï¼Œè€Œä¸æ˜¯ç›´æ¥é‡åŒ–FP16åˆ°INT4çš„æµ®ç‚¹æ•°ï¼Œä»¥ç¡®ä¿æ¨¡å‹ç²¾åº¦ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ€ä½³è°ƒåº¦ç­–ç•¥ï¼Œé‡‡ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’æ¥ç¡®å®šæ¯ä¸ªå¼ é‡çš„æœ€ä½³å†…å­˜ä¼˜åŒ–ã€‚ï¼ˆ3ï¼‰ä¸ºäº†åº”å¯¹è®­ç»ƒå¼ é‡çš„å˜åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”ç­–ç•¥æ¼”åŒ–æœºåˆ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´ç­–ç•¥ä»¥æé«˜ååé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ¡†æ¶ç›¸æ¯”ï¼ŒAdaccå¯ä»¥åŠ é€ŸLLMçš„è®­ç»ƒé€Ÿåº¦ï¼Œè¾¾åˆ°1.01xè‡³1.37xï¼ŒåŒæ—¶ä¿æŒä¸åŸºçº¿ç›¸å½“çš„æ¨¡å‹ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00806v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAdaccçš„æ–°å‹å†…å­˜ç®¡ç†æ¡†æ¶ï¼Œç”¨äºå‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ—¶çš„GPUå†…å­˜å ç”¨ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è‡ªé€‚åº”å‹ç¼©å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œé€šè¿‡è®¾è®¡å±‚ç‰¹å®šçš„å‹ç¼©ç®—æ³•ã€é‡‡ç”¨æœ€ä¼˜è°ƒåº¦ç­–ç•¥ä»¥åŠè‡ªé€‚åº”ç­–ç•¥è¿›åŒ–æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿è¯æ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼ŒåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Adaccæ¡†æ¶é€šè¿‡ç»“åˆè‡ªé€‚åº”å‹ç¼©å’Œæ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ—¶çš„GPUå†…å­˜å ç”¨ã€‚</li>
<li>æå‡ºäº†å±‚ç‰¹å®šçš„å‹ç¼©ç®—æ³•ï¼Œè€ƒè™‘åˆ°äº†å¤§å‹è¯­è¨€æ¨¡å‹å¼ é‡ä¸­çš„å¼‚å¸¸å€¼ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆMILPï¼‰æ¥ç¡®å®šæ¯ä¸ªå¼ é‡çš„æœ€ä½³å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”ç­–ç•¥è¿›åŒ–æœºåˆ¶ï¼Œä»¥é€‚åº”è®­ç»ƒå¼ é‡çš„å˜åŒ–ï¼Œæé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„ååé‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAdaccæ¡†æ¶èƒ½å¤ŸåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒï¼Œæœ€é«˜è¾¾åˆ°1.37å€ï¼ŒåŒæ—¶ä¿æŒä¸åŸºçº¿æ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ã€‚</li>
<li>Adaccæ¡†æ¶é€šè¿‡ä¼˜åŒ–å†…å­˜ç®¡ç†ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæä¾›äº†æ›´é«˜çš„æ•ˆç‡å’Œæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1f54c0ba7f1c70023fccfea0a7138547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-807c1dfeba6a2b65a9a8c87a450eff8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f90a78b6f062860c7c5127a470ad10b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e431dc87c70cc12c60434f3c6cdf88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8da379385136346fe645bbcb7bdc3a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8ce0b41c45b600efa02a610faaca60cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-938549a8927b6f3be40324b45a9d3cea.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ITUNLP-at-SemEval-2025-Task-8-Question-Answering-over-Tabular-Data-A-Zero-Shot-Approach-using-LLM-Driven-Code-Generation"><a href="#ITUNLP-at-SemEval-2025-Task-8-Question-Answering-over-Tabular-Data-A-Zero-Shot-Approach-using-LLM-Driven-Code-Generation" class="headerlink" title="ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A   Zero-Shot Approach using LLM-Driven Code Generation"></a>ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A   Zero-Shot Approach using LLM-Driven Code Generation</h2><p><strong>Authors:Atakan Site, Emre Hakan Erdemir, GÃ¼lÅŸen EryiÄŸit</strong></p>
<p>This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paperâ€™s submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬ä¸ºSemEval-2025 Task 8ï¼šDataBenchï¼Œè¡¨æ ¼æ•°æ®é—®ç­”ç³»ç»Ÿæ‰€å¼€å‘çš„æŠ€æœ¯ã€‚æ­¤ä»»åŠ¡çš„ä¸»è¦ç›®æ ‡æ˜¯åœ¨ä¸¤ä¸ªå­ä»»åŠ¡ï¼ˆDataBench QAï¼ˆå­ä»»åŠ¡Iï¼‰å’ŒDataBench Lite QAï¼ˆå­ä»»åŠ¡IIï¼‰ï¼‰ä¸Šï¼Œå¯¹ç»™å®šçš„è¡¨æ ¼æ•°æ®é›†è¿›è¡Œè·¨åŸŸé—®ç­”ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«å¼ºè°ƒåˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨æœ€æ–°å¼€æºLLMçš„Pythonä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–æç¤ºç­–ç•¥ç”Ÿæˆå¯æ‰§è¡Œçš„Pandasä»£ç ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸åŒçš„LLMåœ¨Pythonä»£ç ç”Ÿæˆæ–¹é¢çš„æ•ˆæœä¸å°½ç›¸åŒã€‚æ­¤å¤–ï¼Œç»“æœè¡¨æ˜ï¼Œä¸æ›¿ä»£æ–¹æ³•ç›¸æ¯”ï¼ŒPythonä»£ç ç”Ÿæˆåœ¨è¡¨æ ¼é—®ç­”æ–¹é¢å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚è™½ç„¶åœ¨æœ¬è®ºæ–‡æäº¤æ—¶æˆ‘ä»¬å°šä¸æ¸…æ¥šé›¶æ ·æœ¬ç³»ç»Ÿä¸­çš„æ’åï¼Œä½†æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å­ä»»åŠ¡Iä¸­æ’åç¬¬8ï¼Œåœ¨å­ä»»åŠ¡IIä¸­æ’åç¬¬6ï¼Œåœ¨å¼€æºæ¨¡å‹ç±»åˆ«ä¸­ä½åˆ—å‰30åç³»ç»Ÿè¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00762v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹SemEval-2025 Task 8ï¼šDataBenchè¡¨æ ¼æ•°æ®é—®ç­”ä»»åŠ¡çš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚ç³»ç»Ÿä¸»è¦å…³æ³¨äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³DataBench QAï¼ˆSubtask Iï¼‰å’ŒDataBench Lite QAï¼ˆSubtask IIï¼‰ä¸¤ä¸ªå­ä»»åŠ¡ã€‚é€šè¿‡ä¼˜åŒ–çš„æç¤ºç­–ç•¥ï¼Œåˆ©ç”¨å…ˆè¿›çš„å¼€æºLLMç”Ÿæˆå¯æ‰§è¡ŒPythonä»£ç ã€‚å®éªŒè¡¨æ˜ä¸åŒLLMåœ¨Pythonä»£ç ç”Ÿæˆæ–¹é¢çš„æ•ˆæœä¸åŒï¼Œä¸”Pythonä»£ç ç”Ÿæˆåœ¨è¡¨æ ¼é—®ç­”æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æäº¤æ—¶æœªçŸ¥é›¶æ ·æœ¬ç³»ç»Ÿæ’åï¼Œä½†åœ¨å­ä»»åŠ¡Iå’Œå­ä»»åŠ¡IIä¸­ï¼Œè¯¥ç³»ç»Ÿåœ¨å¼€æºæ¨¡å‹ç±»åˆ«ä¸­åˆ†åˆ«æ’åç¬¬å…«å’Œç¬¬å…­ï¼Œä¸”åœ¨30ä¸ªç³»ç»Ÿä¸­è¡¨ç°ä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿé’ˆå¯¹SemEval-2025 Task 8çš„DataBenchè¡¨æ ¼æ•°æ®é—®ç­”ä»»åŠ¡ï¼Œæå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é‡‡ç”¨Pythonä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å¼€æºLLMç”Ÿæˆå¯æ‰§è¡ŒPandasä»£ç ã€‚</li>
<li>ä¸åŒLLMåœ¨Pythonä»£ç ç”Ÿæˆæ–¹é¢çš„æ•ˆæœä¸åŒã€‚</li>
<li>Pythonä»£ç ç”Ÿæˆåœ¨è¡¨æ ¼é—®ç­”æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>ç³»ç»Ÿåœ¨DataBench QAï¼ˆSubtask Iï¼‰å’ŒDataBench Lite QAï¼ˆSubtask IIï¼‰ä¸¤ä¸ªå­ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œåˆ†åˆ«æ’åç¬¬å…«å’Œç¬¬å…­ã€‚</li>
<li>ç³»ç»Ÿåœ¨30ä¸ªç³»ç»Ÿä¸­è¡¨ç°ä¼˜äºåŸºçº¿ï¼Œä½“ç°äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c9268214047ce14e1ae9b2f24127a4a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6089cbf89182a9586a8418c85a433c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec6797cfe65f2f7d4e7753610827a756.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d46549929412803539c8f3f870e2eab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-168dc65f3b68e55e9d75f85394270246.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GLiDRE-Generalist-Lightweight-model-for-Document-level-Relation-Extraction"><a href="#GLiDRE-Generalist-Lightweight-model-for-Document-level-Relation-Extraction" class="headerlink" title="GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction"></a>GLiDRE: Generalist Lightweight model for Document-level Relation   Extraction</h2><p><strong>Authors:Robin Armingaud, Romaric BesanÃ§on</strong></p>
<p>Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to the need to model complex interactions between entities across sentences. Current approaches, largely based on the ATLOP architecture, are commonly evaluated on benchmarks like DocRED and Re-DocRED. However, their performance in zero-shot or few-shot settings remains largely underexplored due to the taskâ€™s complexity. Recently, the GLiNER model has shown that a compact NER model can outperform much larger Large Language Models. With a similar motivation, we introduce GLiDRE, a new model for document-level relation extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against state-of-the-art models across various data settings on the Re-DocRED dataset. Our results demonstrate that GLiDRE achieves state-of-the-art performance in few-shot scenarios. Our code is publicly available. </p>
<blockquote>
<p>å…³ç³»æŠ½å–ï¼ˆREï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œå…¶æ–‡æ¡£çº§å˜ä½“å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦å»ºæ¨¡è·¨å¥å­å®ä½“ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚å½“å‰çš„æ–¹æ³•å¤§å¤šåŸºäºATLOPæ¶æ„ï¼Œé€šå¸¸åœ¨DocREDå’ŒRe-DocREDç­‰åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œç”±äºä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå®ƒä»¬åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æƒ…å†µä¸‹çš„æ€§èƒ½ä»é²œæœ‰ç ”ç©¶ã€‚æœ€è¿‘ï¼ŒGLiNERæ¨¡å‹è¡¨æ˜ï¼Œç´§å‡‘çš„å‘½åå®ä½“è¯†åˆ«æ¨¡å‹å¯ä»¥è¶…è¶Šæ›´å¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åŸºäºç±»ä¼¼çš„åŠ¨æœºï¼Œæˆ‘ä»¬æ¨å‡ºäº†GLiDREï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–‡æ¡£çº§å…³ç³»æŠ½å–æ¨¡å‹ï¼Œå»ºç«‹åœ¨GliNERçš„å…³é”®æ€æƒ³ä¹‹ä¸Šã€‚æˆ‘ä»¬åœ¨Re-DocREDæ•°æ®é›†çš„å„ç§æ•°æ®è®¾ç½®ä¸Šï¼Œå°†GLiDREä¸æœ€æ–°æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸­ï¼ŒGLiDREè¾¾åˆ°äº†æœ€æ–°çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00757v1">PDF</a> Submitted to ARR July</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æ¡£çº§åˆ«çš„å…³ç³»æŠ½å–ï¼ˆREï¼‰æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠå½“å‰çš„è§£å†³ç­–ç•¥ã€‚åŸºäºGLiNERæ¨¡å‹çš„çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–‡æ¡£çº§å…³ç³»æŠ½å–æ¨¡å‹GLiDREã€‚åœ¨Re-DocREDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGLiDREåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æ¡£çº§å…³ç³»æŠ½å–ï¼ˆREï¼‰éœ€è¦å»ºæ¨¡è·¨å¥å­çš„å®ä½“ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œå› æ­¤å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å¤§å¤šåŸºäºATLOPæ¶æ„ï¼Œå¹¶åœ¨DocREDå’ŒRe-DocREDç­‰åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä»·ã€‚</li>
<li>é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æƒ…å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°å°šå¾…æ¢ç´¢ã€‚</li>
<li>GLiNERæ¨¡å‹æ˜¾ç¤ºç´§å‡‘çš„NERæ¨¡å‹å¯ä»¥è¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æå‡ºçš„GLiDREæ¨¡å‹å€Ÿé‰´äº†GLiNERçš„å…³é”®æ€æƒ³ï¼Œä¸ºæ–‡æ¡£çº§å…³ç³»æŠ½å–æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨Re-DocREDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLiDREåœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94675a1d0d65b30e27f7a57dda8d14b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98da164be67a9a30ec0bd493f8e7908b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1dd48fb63c782524e8d8d35bdf3ac78.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MIHBench-Benchmarking-and-Mitigating-Multi-Image-Hallucinations-in-Multimodal-Large-Language-Models"><a href="#MIHBench-Benchmarking-and-Mitigating-Multi-Image-Hallucinations-in-Multimodal-Large-Language-Models" class="headerlink" title="MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in   Multimodal Large Language Models"></a>MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in   Multimodal Large Language Models</h2><p><strong>Authors:Jiale Li, Mingrui Wu, Zixiang Jin, Hao Chen, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</strong></p>
<p>Despite growing interest in hallucination in Multimodal Large Language Models, existing studies primarily focus on single-image settings, leaving hallucination in multi-image scenarios largely unexplored. To address this gap, we conduct the first systematic study of hallucinations in multi-image MLLMs and propose MIHBench, a benchmark specifically tailored for evaluating object-related hallucinations across multiple images. MIHBench comprises three core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object Count Hallucination, and Object Identity Consistency Hallucination, targeting semantic understanding across object existence, quantity reasoning, and cross-view identity consistency. Through extensive evaluation, we identify key factors associated with the occurrence of multi-image hallucinations, including: a progressive relationship between the number of image inputs and the likelihood of hallucination occurrences; a strong correlation between single-image hallucination tendencies and those observed in multi-image contexts; and the influence of same-object image ratios and the positional placement of negative samples within image sequences on the occurrence of object identity consistency hallucination. To address these challenges, we propose a Dynamic Attention Balancing mechanism that adjusts inter-image attention distributions while preserving the overall visual attention proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that our method effectively reduces hallucination occurrences and enhances semantic integration and reasoning stability in multi-image scenarios. </p>
<blockquote>
<p>å°½ç®¡å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰ï¼ˆhallucinationï¼‰çš„å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•å›¾åƒè®¾ç½®ä¸Šï¼Œè€Œå¯¹å¤šå›¾åƒåœºæ™¯ä¸­çš„å¹»è§‰åˆ™é²œæœ‰æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹å¤šå›¾åƒMLLMä¸­çš„å¹»è§‰è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ï¼Œå¹¶æå‡ºäº†MIHBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è·¨å¤šå›¾åƒå¯¹è±¡ç›¸å…³å¹»è§‰çš„åŸºå‡†æµ‹è¯•ã€‚MIHBenchåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šå¤šå›¾åƒå¯¹è±¡å­˜åœ¨å¹»è§‰ã€å¤šå›¾åƒå¯¹è±¡è®¡æ•°å¹»è§‰å’Œå¯¹è±¡èº«ä»½ä¸€è‡´æ€§å¹»è§‰ï¼Œæ—¨åœ¨é’ˆå¯¹å¯¹è±¡å­˜åœ¨çš„è¯­ä¹‰ç†è§£ã€æ•°é‡æ¨ç†å’Œè·¨è§†å›¾èº«ä»½ä¸€è‡´æ€§ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸å¤šå›¾åƒå¹»è§‰å‘ç”Ÿç›¸å…³çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬ï¼šå›¾åƒè¾“å…¥æ•°é‡ä¸å¹»è§‰å‘ç”Ÿå¯èƒ½æ€§ä¹‹é—´çš„æ¸è¿›å…³ç³»ï¼›å•å›¾åƒå¹»è§‰å€¾å‘ä¸å¤šå›¾åƒä¸Šä¸‹æ–‡ä¸­çš„å¹»è§‰å€¾å‘ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ï¼›ä»¥åŠç›¸åŒå¯¹è±¡å›¾åƒæ¯”ä¾‹å’Œè´Ÿæ ·æœ¬åœ¨å›¾åƒåºåˆ—ä¸­çš„ä½ç½®å¯¹å¯¹è±¡èº«ä»½ä¸€è‡´æ€§å¹»è§‰å‘ç”Ÿçš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€æ³¨æ„åŠ›å¹³è¡¡æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥è°ƒæ•´å›¾åƒé—´çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼ŒåŒæ—¶ä¿æŒæ•´ä½“çš„è§†è§‰æ³¨æ„åŠ›æ¯”ä¾‹ã€‚åœ¨å¤šä¸ªæœ€å…ˆè¿›çš„MLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†å¹»è§‰çš„å‘ç”Ÿï¼Œå¹¶å¢å¼ºäº†å¤šå›¾åƒåœºæ™¯ä¸­çš„è¯­ä¹‰é›†æˆå’Œæ¨ç†ç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00726v1">PDF</a> ACM MM25 has accepted this paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¤šå›¾åƒåœºæ™¯ä¸‹çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ã€‚ä¸ºè§£å†³è¯„ä¼°å¯¹è±¡ç›¸å…³å¹»è§‰çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸“é—¨ç”¨äºè¯„ä¼°å¤šå›¾åƒä¸­å¯¹è±¡ç›¸å…³å¹»è§‰çš„MIHBenchåŸºå‡†æµ‹è¯•ã€‚MIHBenchåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼Œé’ˆå¯¹è·¨å¤šå›¾åƒçš„å¯¹è±¡å­˜åœ¨ã€æ•°é‡æ¨ç†å’Œè·¨è§†å›¾èº«ä»½ä¸€è‡´æ€§çš„è¯­ä¹‰ç†è§£ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œç¡®å®šäº†ä¸å¤šå›¾åƒå¹»è§‰å‘ç”Ÿç›¸å…³çš„å…³é”®å› ç´ ï¼Œå¹¶æå‡ºäº†åŠ¨æ€æ³¨æ„åŠ›å¹³è¡¡æœºåˆ¶ï¼Œä»¥è°ƒæ•´å›¾åƒé—´çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œå‡å°‘å¹»è§‰å‘ç”Ÿï¼Œæé«˜å¤šå›¾åƒåœºæ™¯ä¸­çš„è¯­ä¹‰é›†æˆå’Œæ¨ç†ç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•å›¾åƒè®¾ç½®ä¸Šï¼Œå¯¹äºå¤šå›¾åƒåœºæ™¯ä¸­çš„å¹»è§‰ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>æå‡ºMIHBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šå›¾åƒä¸­å¯¹è±¡ç›¸å…³çš„å¹»è§‰ã€‚</li>
<li>MIHBenchåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒä»»åŠ¡ï¼šå¤šå›¾åƒå¯¹è±¡å­˜åœ¨å¹»è§‰ã€å¤šå›¾åƒå¯¹è±¡è®¡æ•°å¹»è§‰å’Œå¯¹è±¡èº«ä»½ä¸€è‡´æ€§å¹»è§‰ï¼Œåˆ†åˆ«é’ˆå¯¹è¯­ä¹‰ç†è§£ã€æ•°é‡æ¨ç†å’Œè·¨è§†å›¾èº«ä»½ä¸€è‡´æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°å¤šå›¾åƒå¹»è§‰ä¸ä¸€äº›å…³é”®å› ç´ ç›¸å…³ï¼ŒåŒ…æ‹¬å›¾åƒè¾“å…¥æ•°é‡ä¸å¹»è§‰å‘ç”Ÿæ¦‚ç‡çš„å…³ç³»ã€å•å›¾åƒå¹»è§‰ä¸å¤šå›¾åƒå¹»è§‰ä¹‹é—´çš„å¼ºç›¸å…³æ€§ä»¥åŠåŒä¸€å¯¹è±¡å›¾åƒæ¯”ä¾‹å’Œè´Ÿæ ·æœ¬ä½ç½®å¯¹å¯¹è±¡èº«ä»½ä¸€è‡´æ€§å¹»è§‰çš„å½±å“ã€‚</li>
<li>æå‡ºçš„åŠ¨æ€æ³¨æ„åŠ›å¹³è¡¡æœºåˆ¶èƒ½æœ‰æ•ˆè°ƒæ•´å›¾åƒé—´çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œå‡å°‘å¹»è§‰å‘ç”Ÿã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æœºåˆ¶èƒ½æé«˜å¤šå›¾åƒåœºæ™¯ä¸­çš„è¯­ä¹‰é›†æˆå’Œæ¨ç†ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-788ce8c703cb5d8f19fa91aa17c8af3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c14802707ee538d90c9bd43706a94444.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce8e473921ef828bbb6f6a835fdd3f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2c25b6fcaa7b9c730f282e23ded3264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6433f4c7eae662c61f686b5c26f809c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dynamically-Adaptive-Reasoning-via-LLM-Guided-MCTS-for-Efficient-and-Context-Aware-KGQA"><a href="#Dynamically-Adaptive-Reasoning-via-LLM-Guided-MCTS-for-Efficient-and-Context-Aware-KGQA" class="headerlink" title="Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA"></a>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA</h2><p><strong>Authors:Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu</strong></p>
<p>Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ—¨åœ¨è§£é‡Šè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„å…³ç³»å’Œè¯­ä¹‰ç»“æ„è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œä»¥è·å–å‡†ç¡®çš„ç­”æ¡ˆã€‚æœ€è¿‘çš„KGQAæ–¹æ³•ä¸»è¦éµå¾ªâ€œæ£€ç´¢-æ¨ç†â€èŒƒå¼ï¼Œä¾èµ–äºå›¾ç¥ç»ç½‘ç»œæˆ–å¯å‘å¼è§„åˆ™è¿›è¡Œé™æ€è·¯å¾„æå–ï¼Œæˆ–è€…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æç¤ºæ¥è”åˆæ‰§è¡Œæ£€ç´¢å’Œæ¨ç†çš„åŠ¨æ€è·¯å¾„ç”Ÿæˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå‰è€…å—é™äºé™æ€è·¯å¾„æå–å’Œç¼ºä¹ä¸Šä¸‹æ–‡ç»†åŒ–ï¼Œè€Œåè€…åˆ™ç”±äºä¾èµ–äºå›ºå®šçš„è¯„åˆ†å‡½æ•°å’Œå¤§é‡çš„LLMè°ƒç”¨è€Œè®¡ç®—æˆæœ¬é«˜ï¼Œå¹¶ä¸”åœ¨è·¯å¾„è¯„ä¼°æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºåŠ¨æ€è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ¨ç†ï¼ˆDAMRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ç¬¦å·æœç´¢ä¸è‡ªé€‚åº”è·¯å¾„è¯„ä¼°ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œå¯å®ç°é«˜æ•ˆã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚DAMRé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºéª¨å¹²ï¼Œä»¥åŸºäºLLMçš„è§„åˆ’å™¨ä¸ºæŒ‡å¯¼ï¼Œåœ¨æ¯ä¸ªæ­¥éª¤ä¸­é€‰æ‹©å‰kä¸ªç›¸å…³å…³ç³»ä»¥å‡å°‘æœç´¢ç©ºé—´ã€‚ä¸ºäº†æé«˜è·¯å¾„è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºTransformerçš„è¯„åˆ†å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ï¼Œè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯è¡Œæ€§ä¼°è®¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šè·³æ¨ç†è¿‡ç¨‹ä¸­æ•æ‰ç»†å¾®çš„è¯­ä¹‰å˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£é«˜è´¨é‡ç›‘ç£çš„ç¨€ç¼ºæ€§ï¼ŒDAMRèå…¥äº†ä¸€ç§åŠ¨æ€ä¼ªè·¯å¾„ç»†åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¼šå®šæœŸä»æœç´¢è¿‡ç¨‹ä¸­æ¢ç´¢çš„éƒ¨åˆ†è·¯å¾„ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼Œä½¿è¯„åˆ†å™¨èƒ½å¤Ÿä¸æ–­é€‚åº”ä¸æ–­å˜åŒ–çš„æ¨ç†è½¨è¿¹åˆ†å¸ƒã€‚åœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDAMRæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00719v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æå‡ºä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŠ¨æ€è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¨ç†ï¼ˆDAMRï¼‰ã€‚DAMRç»“åˆäº†ç¬¦å·æœç´¢å’Œè‡ªé€‚åº”è·¯å¾„è¯„ä¼°ï¼Œå®ç°äº†é«˜æ•ˆä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚å®ƒé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºéª¨æ¶ï¼Œå¹¶ä½¿ç”¨åŸºäºLLMçš„è§„åˆ’å™¨æ¥æŒ‡å¯¼æœç´¢ï¼Œå‡å°‘æœç´¢ç©ºé—´ã€‚åŒæ—¶ï¼ŒDAMRå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºTransformerçš„è¯„åˆ†å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ï¼Œæé«˜è·¯å¾„è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒDAMRè¿˜èå…¥äº†åŠ¨æ€ä¼ªè·¯å¾„ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥ç¼“è§£é«˜è´¨é‡ç›‘ç£æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒDAMRåœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KGQAæ—¨åœ¨åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„å…³ç³»å’Œè¯­ä¹‰ç»“æ„æ¥è§£ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚</li>
<li>ç°æœ‰KGQAæ–¹æ³•ä¸»è¦éµå¾ªæ£€ç´¢ä¸æ¨ç†çš„èŒƒå¼ï¼Œä½†å­˜åœ¨é™æ€è·¯å¾„æå–çš„å±€é™æ€§ä»¥åŠé«˜è®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚</li>
<li>DAMRæ¡†æ¶ç»“åˆäº†ç¬¦å·æœç´¢å’Œè‡ªé€‚åº”è·¯å¾„è¯„ä¼°ï¼Œå®ç°é«˜æ•ˆä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚</li>
<li>DAMRä½¿ç”¨LLMè§„åˆ’å™¨æŒ‡å¯¼çš„MCTSæ¥å‡å°‘æœç´¢ç©ºé—´ï¼Œå¹¶å¼•å…¥åŸºäºTransformerçš„è¯„åˆ†å™¨æé«˜è·¯å¾„è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>DAMRé€šè¿‡åŠ¨æ€ä¼ªè·¯å¾„ä¼˜åŒ–æœºåˆ¶é€‚åº”ç›‘ç£æ•°æ®çš„ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDAMRåœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ebfc33de37a08936bbe724e2890914b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c62f7894d735a00a6dea0d5634122089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf505b9dc6c487552431b50ab86f1112.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e52da940666d043ab1ac88bb37555e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-082a9cf3864a93bde0621052557ae1d0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Better-Call-Claude-Can-LLMs-Detect-Changes-of-Writing-Style"><a href="#Better-Call-Claude-Can-LLMs-Detect-Changes-of-Writing-Style" class="headerlink" title="Better Call Claude: Can LLMs Detect Changes of Writing Style?"></a>Better Call Claude: Can LLMs Detect Changes of Writing Style?</h2><p><strong>Authors:Johannes RÃ¶misch, Svetlana Gorovaia, Mariia Halchynska, Gleb Schmidt, Ivan P. Yamshchikov</strong></p>
<p>This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 â€œMulti-Author Writing Style Analysisâ€ datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œè€…é£æ ¼åˆ†æä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€ï¼šå¥å­çº§é£æ ¼å˜åŒ–æ£€æµ‹ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å®˜æ–¹PAN 2024å¹´å’Œ2025å¹´çš„â€œå¤šä½œè€…å†™ä½œé£æ ¼åˆ†æâ€æ•°æ®é›†ä¸Šè¯„ä¼°äº†å››ä¸ªLLMçš„è¡¨ç°ï¼Œå¹¶å¾—å‡ºäº†ä¸€äº›è§‚å¯Ÿç»“æœã€‚é¦–å…ˆï¼Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¯¹å†™ä½œé£æ ¼çš„å˜åŒ–éå¸¸æ•æ„Ÿâ€”â€”ç”šè‡³åœ¨å•ä¸ªå¥å­çš„ç²’åº¦çº§åˆ«ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬çš„å‡†ç¡®æ€§ä¸ºè¯¥ä»»åŠ¡å»ºç«‹äº†å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†çº¿ï¼Œè¶…è¶Šäº†PANç«èµ›æå‡ºçš„åŸºå‡†çº¿ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†è¯­ä¹‰å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜æœ€æ–°ä¸€ä»£çš„LLMå¯èƒ½æ¯”å…ˆå‰æŠ¥é“çš„æ›´åŠ æ•æ„Ÿäºç‹¬ç«‹äºå†…å®¹çš„çº¯ç²¹é£æ ¼ä¿¡å·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00680v1">PDF</a> </p>
<p><strong>Summary</strong>: æ–‡ç« è¯„ä¼°äº†æœ€æ–°å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¹‹ä¸€â€”â€”å¥å­çº§é£æ ¼å˜åŒ–æ£€æµ‹ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æ–‡ç« åœ¨PANå®˜æ–¹æ•°æ®é›†ä¸Šå¯¹å››ç§LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè§‚å¯Ÿåˆ°æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¯¹ç»†å¾®çš„å†™ä½œé£æ ¼å˜åŒ–æ•æ„Ÿï¼Œå‡†ç¡®ç‡ä¸ºè¯¥ä»»åŠ¡è®¾å®šäº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†çº¿ï¼Œå¹¶è¶…è¶Šäº†PANç«èµ›çš„åŸºçº¿å»ºè®®ã€‚æ­¤å¤–ï¼Œæ–‡ç« æ¢è®¨äº†è¯­ä¹‰å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“ï¼Œå¹¶æä¾›äº†è¯æ®è¡¨æ˜æœ€æ–°ä¸€ä»£çš„LLMå¯èƒ½å¯¹å†…å®¹ç‹¬ç«‹å’Œçº¯ç²¹çš„é£æ ¼ä¿¡å·æ›´ä¸ºæ•æ„Ÿã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¥å­çº§çš„å†™ä½œé£æ ¼å˜åŒ–æ•æ„Ÿã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨PANæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡è®¾å®šäº†æŒ‘æˆ˜æ€§çš„åŸºå‡†çº¿ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨é£æ ¼åˆ†æä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†PANç«èµ›çš„åŸºçº¿å»ºè®®ã€‚</li>
<li>æœ€æ–°ä¸€ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å†…å®¹ç‹¬ç«‹å’Œçº¯ç²¹çš„é£æ ¼ä¿¡å·æ›´ä¸ºæ•æ„Ÿã€‚</li>
<li>è¯­ä¹‰å¯¹æ¨¡å‹é¢„æµ‹æœ‰å½±å“ã€‚</li>
<li>æ–‡ç« è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7bd51ae6bdf409bcb7a618d1723d4f6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505cceba670a49755f00fafa46623594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69df2e7f87564cfa66db727d13c00598.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MCeT-Behavioral-Model-Correctness-Evaluation-using-Large-Language-Models"><a href="#MCeT-Behavioral-Model-Correctness-Evaluation-using-Large-Language-Models" class="headerlink" title="MCeT: Behavioral Model Correctness Evaluation using Large Language   Models"></a>MCeT: Behavioral Model Correctness Evaluation using Large Language   Models</h2><p><strong>Authors:Khaled Ahmed, Jialing Song, Boqi Chen, Ou Wei, Bingzhou Zheng</strong></p>
<p>Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models.   In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram. </p>
<blockquote>
<p>è¡Œä¸ºæ¨¡å‹å›¾ï¼ˆä¾‹å¦‚åºåˆ—å›¾ï¼‰æ˜¯ä¸€ç§é‡è¦çš„æ–‡æ¡£å½¢å¼ï¼Œé€šå¸¸ç”±ç³»ç»Ÿå·¥ç¨‹å¸ˆæ ¹æ®éœ€æ±‚æ–‡æ¡£è¿›è¡Œè®¾è®¡ï¼Œå¯ä»¥å…¨æ‰‹åŠ¨å®Œæˆï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è®¾è®¡å·¥å…·è¾…åŠ©å®Œæˆã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºAIå»ºæ¨¡åŠ©æ‰‹çš„ä½¿ç”¨è¶Šæ¥è¶Šå¤šï¼Œè‡ªåŠ¨ç”Ÿæˆè¿™äº›å›¾çš„è‡ªåŠ¨åŒ–ç¨‹åº¦ä¼šè¶Šæ¥è¶Šé«˜ã€‚è¿™è¦æ±‚å¼€å‘æ›´å…ˆè¿›çš„è‡ªåŠ¨æ¨¡å‹æ­£ç¡®æ€§è¯„ä¼°å·¥å…·ã€‚è¿™ç§å·¥å…·å¯ç”¨äºè¯„ä¼°æ‰‹åŠ¨å’ŒAIè‡ªåŠ¨ç”Ÿæˆçš„æ¨¡å‹ï¼Œä¸ºç³»ç»Ÿå·¥ç¨‹å¸ˆæä¾›åé¦ˆæ„è§ï¼Œå¹¶å…è®¸AIåŠ©æ‰‹è¿›è¡Œè‡ªæˆ‘è¯„ä¼°å’Œå¢å¼ºä»–ä»¬ç”Ÿæˆçš„æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MCeTï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„å·¥å…·ï¼Œç”¨äºæ ¹æ®ç›¸åº”çš„éœ€æ±‚æ–‡æœ¬è¯„ä¼°è¡Œä¸ºæ¨¡å‹çš„æ­£ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åºåˆ—å›¾çš„æ­£ç¡®æ€§ï¼Œå¹¶ç”Ÿæˆæ¨¡å‹å­˜åœ¨çš„é—®é¢˜åˆ—è¡¨ã€‚æˆ‘ä»¬ä½¿ç”¨LLMè¿›è¡Œæ­£ç¡®æ€§è¯„ä¼°ä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬å·²æ˜¾ç¤ºå‡ºå‡ºè‰²çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œç›´æ¥è¦æ±‚LLMå°†å›¾è¡¨ä¸éœ€æ±‚è¿›è¡Œæ¯”è¾ƒï¼Œåªèƒ½å‘ç°ä¸åˆ°35%çš„é—®é¢˜ï¼Œè€Œç»éªŒä¸°å¯Œçš„å·¥ç¨‹å¸ˆåˆ™å¯ä»¥å‘ç°æ›´å¤šçš„é—®é¢˜ã€‚æˆ‘ä»¬æè®®é‡‡ç”¨ä¸€ç§ç²¾ç»†çš„å¤šè§’åº¦æ–¹æ³•æ¥è¡¥å……ç›´æ¥æ£€æŸ¥ï¼›æˆ‘ä»¬å°†å›¾è¡¨åˆ†å‰²æˆåŸå­ä¸å¯åˆ†çš„äº’åŠ¨ï¼Œå¹¶å°†éœ€æ±‚æ–‡æœ¬åˆ†å‰²æˆåŸå­ã€ç‹¬ç«‹çš„é¡¹ã€‚æˆ‘ä»¬å°†å›¾è¡¨ä¸åŸå­éœ€æ±‚è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å°†æ¯ä¸ªå›¾è¡¨åŸå­ä¸éœ€æ±‚è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå„ç§è§’åº¦ï¼Œæ¥å‡è½»LLMäº§ç”Ÿçš„è™šæ„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç»¼åˆæ–¹æ³•æé«˜äº†ç›´æ¥æ–¹æ³•çš„ç²¾åº¦ï¼Œä»æ•°æ®é›†ä¸­çš„0.58æé«˜åˆ°0.81ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å‘ç°äº†ç»éªŒä¸°å¯Œçš„å·¥ç¨‹å¸ˆå‘ç°çš„90%ä»¥ä¸Šçš„é—®é¢˜ï¼Œå¹¶å¹³å‡æ¯ä¸ªå›¾è¡¨æŠ¥å‘Šäº†6ä¸ªæ–°é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00630v1">PDF</a> MODELS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºAIå»ºæ¨¡åŠ©æ‰‹çš„ä½¿ç”¨æ—¥ç›Šæ™®åŠï¼Œè¡Œä¸ºæ¨¡å‹å›¾ï¼ˆå¦‚åºåˆ—å›¾ï¼‰çš„è‡ªåŠ¨ç”Ÿæˆéœ€è¦æ›´å…ˆè¿›çš„æ¨¡å‹æ­£ç¡®æ€§è¯„ä¼°å·¥å…·ã€‚æœ¬æ–‡æå‡ºäº†MCeTï¼Œç¬¬ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨çš„å·¥å…·ï¼Œç”¨äºè¯„ä¼°è¡Œä¸ºæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åºåˆ—å›¾ï¼‰ç›¸å¯¹äºå…¶å¯¹åº”éœ€æ±‚æ–‡æœ¬çš„æ­£ç¡®æ€§ï¼Œå¹¶ç”Ÿæˆé—®é¢˜åˆ—è¡¨ã€‚è™½ç„¶LLMåœ¨è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç›´æ¥ä½¿ç”¨LLMæ¯”è¾ƒå›¾è¡¨å’Œéœ€æ±‚å‘ç°çš„é—®é¢˜ä¸åˆ°ç»éªŒä¸°å¯Œçš„å·¥ç¨‹å¸ˆæ‰€èƒ½å‘ç°çš„35%ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾ç»†çš„ã€å¤šè§†è§’çš„æ–¹æ³•ä½œä¸ºè¡¥å……ï¼Œå°†å›¾è¡¨åˆ†å‰²æˆä¸å¯å†åˆ†çš„åŸå­äº¤äº’ï¼Œå°†éœ€æ±‚æ–‡æœ¬åˆ†å‰²æˆè‡ªæˆ‘åŒ…å«çš„åŸå­é¡¹ã€‚é€šè¿‡æ¯”è¾ƒåŸå­å›¾è¡¨å’ŒåŸå­éœ€æ±‚ä»¥åŠç»“åˆè‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ç²¾åº¦ï¼Œå¹¶å‡è½»äº†LLMè™šæ„çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¯”ç›´æ¥æ–¹æ³•æ›´ç²¾ç¡®åœ°å‘ç°äº†ç»éªŒä¸°å¯Œçš„å·¥ç¨‹å¸ˆæ‰€å¿½è§†çš„é—®é¢˜ï¼Œå¹¶ä¸”å¹³å‡æ¯ä¸ªå›¾è¡¨æŠ¥å‘Šäº†6ä¸ªæ–°é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¡Œä¸ºæ¨¡å‹å›¾ï¼ˆå¦‚åºåˆ—å›¾ï¼‰åœ¨ç³»ç»Ÿè®¾è®¡ä¸­çš„é‡è¦æ€§åŠå…¶ä¸éœ€æ±‚æ–‡æ¡£çš„å…³ç³»ã€‚</li>
<li>éšç€LLMçš„å‘å±•ï¼Œè¶Šæ¥è¶Šå¤šçš„è‡ªåŠ¨åŒ–å·¥å…·è¢«ç”¨äºç”Ÿæˆè¡Œä¸ºæ¨¡å‹å›¾ã€‚</li>
<li>æå‡ºäº†MCeTå·¥å…·ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„å·¥å…·ï¼Œç”¨äºè¯„ä¼°è¡Œä¸ºæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯åºåˆ—å›¾ï¼‰çš„æ­£ç¡®æ€§ã€‚</li>
<li>LLMè™½ç„¶å…·æœ‰å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä½†ç›´æ¥ç”¨äºæ¨¡å‹æ­£ç¡®æ€§è¯„ä¼°çš„æ•ˆæœæœ‰é™ã€‚</li>
<li>ä¸ºäº†æé«˜è¯„ä¼°ç²¾åº¦ï¼Œæå‡ºäº†ä¸€ç§ç²¾ç»†çš„ã€å¤šè§†è§’çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬å°†å›¾è¡¨å’Œéœ€æ±‚æ–‡æœ¬åˆ†è§£ä¸ºæœ€å°çš„ã€è‡ªæˆ‘åŒ…å«çš„å•å…ƒè¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>æå‡ºçš„è‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥æ–¹æ³•æœ‰åŠ©äºå‡è½»LLMè™šæ„çš„é—®é¢˜ã€‚</li>
<li>ä¸ç›´æ¥æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨å‘ç°æ¨¡å‹é—®é¢˜æ–¹é¢æ›´åŠ ç²¾ç¡®å’Œæœ‰æ•ˆï¼Œå¹³å‡æ¯ä¸ªå›¾è¡¨æŠ¥å‘Šäº†6ä¸ªæ–°é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b761929dfb8e7c8fd1721371305eff33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b57e722433214aa0e8ab1c6d77a5a308.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce3acf633474318350494f8baa367847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a3f30e87597ab090fbff9bea5fb007f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-857304af46a1e8286a03176f367e4d66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89c797a76b2b4f02c646cc23d697f11b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LeakSealer-A-Semisupervised-Defense-for-LLMs-Against-Prompt-Injection-and-Leakage-Attacks"><a href="#LeakSealer-A-Semisupervised-Defense-for-LLMs-Against-Prompt-Injection-and-Leakage-Attacks" class="headerlink" title="LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection   and Leakage Attacks"></a>LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection   and Leakage Attacks</h2><p><strong>Authors:Francesco Panebianco, Stefano Bonfanti, Francesco TrovÃ², Michele Carminati</strong></p>
<p>The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ³›åŒ–èƒ½åŠ›ä½¿å…¶åœ¨å„ç§åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›éƒ¨ç½²ã€‚ç„¶è€Œï¼Œè¿™ç§å¢åŠ çš„é‡‡ç”¨å·²ç»å¸¦æ¥äº†å¤šç§å®‰å…¨å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨è¶Šç‹±å’Œæ•°æ®æ³„éœ²æ”»å‡»çš„å½¢å¼ä¸­ã€‚æ­¤å¤–ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è™½ç„¶åœ¨æé«˜LLMå“åº”çš„ä¸Šä¸‹æ–‡æ„è¯†æ–¹é¢å‘æŒ¥äº†ä½œç”¨ï¼Œä½†æ— æ„ä¸­å¼•å…¥äº†å¯èƒ½å¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²çš„æ¼æ´ã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤æ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†æLLMç³»ç»Ÿå†å²äº¤äº’æ•°æ®çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”ŸæˆæŒ‰ä¸»é¢˜åˆ†ç±»çš„ä½¿ç”¨å›¾ï¼ˆåŒ…æ‹¬å¯¹æŠ—äº¤äº’ï¼‰ã€‚è¿™ç§æ–¹æ³•è¿˜æä¾›äº†è¿½è¸ªè¶Šç‹±æ”»å‡»æ¨¡å¼æ¼”å˜çš„æ³•åŒ»æ´å¯Ÿã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†LeakSealerï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†é™æ€åˆ†æä»¥æä¾›æ³•åŒ»æ´å¯Ÿå’ŒåŠ¨æ€é˜²å¾¡åœ¨äººæœºå¾ªç¯ï¼ˆHITLï¼‰ç®¡é“ä¸­ã€‚è¯¥æŠ€æœ¯å¯ä»¥è¯†åˆ«ä¸»é¢˜ç»„å¹¶æ£€æµ‹å¼‚å¸¸æ¨¡å¼ï¼Œä»è€Œå®ç°ä¸»åŠ¨é˜²å¾¡æœºåˆ¶ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§æƒ…æ™¯å¯¹LeakSealerè¿›è¡Œäº†å®è¯è¯„ä¼°ï¼šï¼ˆ1ï¼‰è¶Šç‹±å°è¯•ï¼Œé‡‡ç”¨å…¬å…±åŸºå‡†æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰ä¸ªäººä¿¡æ¯æ³„éœ²ï¼Œè¾…ä»¥æ ‡è®°çš„LLMäº¤äº’æ•°æ®é›†ã€‚åœ¨é™æ€è®¾ç½®ä¸­ï¼ŒLeakSealeråœ¨è¯†åˆ«æç¤ºæ³¨å…¥æ—¶åœ¨ToxicChatæ•°æ®é›†ä¸Šå®ç°äº†æœ€é«˜çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚åœ¨åŠ¨æ€è®¾ç½®ä¸­ï¼Œä¸ªäººä¿¡æ¯æ³„éœ²æ£€æµ‹è¾¾åˆ°äº†0.97çš„AUPRCï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿å¦‚Llama Guardã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00602v1">PDF</a> 22 pages, preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠåº”ç”¨å¸¦æ¥äº†è®¸å¤šå®‰å…¨å¨èƒï¼Œå¦‚è¶Šç‹±å’Œæ•°æ®æ³„éœ²æ”»å‡»ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ†æLLMç³»ç»Ÿå†å²äº¤äº’æ•°æ®çš„æ–¹æ³•ï¼Œç”ŸæˆæŒ‰ä¸»é¢˜åˆ†ç±»çš„ä½¿ç”¨åœ°å›¾ï¼Œæä¾›è¿½è¸ªè¶Šç‹±æ”»å‡»æ¨¡å¼æ¼”å˜çš„æ³•åŒ»è§è§£ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºLeakSealeræ¡†æ¶ï¼Œç»“åˆé™æ€åˆ†æå’ŒåŠ¨æ€é˜²å¾¡ï¼Œåœ¨äººæœºäº¤äº’ç®¡é“ä¸­è¯†åˆ«ä¸»é¢˜ç»„å’Œæ£€æµ‹å¼‚å¸¸æ¨¡å¼ï¼Œå®ç°ä¸»åŠ¨é˜²å¾¡æœºåˆ¶ã€‚ç»éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒLeakSealeråœ¨è¯†åˆ«æç¤ºæ³¨å…¥å’Œæ£€æµ‹ä¸ªäººä¿¡æ¯æ³„éœ²æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¹¿æ³›åº”ç”¨å¸¦æ¥äº†å®‰å…¨å¨èƒï¼Œå¦‚è¶Šç‹±å’Œæ•°æ®æ³„éœ²æ”»å‡»ã€‚</li>
<li>æå‡ºä¸€ç§åˆ†æLLMå†å²äº¤äº’æ•°æ®çš„æ–¹æ³•ï¼Œç”Ÿæˆä½¿ç”¨åœ°å›¾ä»¥è¿½è¸ªæ”»å‡»æ¨¡å¼æ¼”å˜ã€‚</li>
<li>LeakSealeræ¡†æ¶ç»“åˆé™æ€åˆ†æå’ŒåŠ¨æ€é˜²å¾¡ï¼Œå®ç°ä¸»åŠ¨é˜²å¾¡æœºåˆ¶ã€‚</li>
<li>LeakSealerèƒ½å¤Ÿè¯†åˆ«ä¸»é¢˜ç»„å¹¶æ£€æµ‹å¼‚å¸¸æ¨¡å¼ã€‚</li>
<li>åœ¨é™æ€åˆ†æä¸­ï¼ŒLeakSealeråœ¨è¯†åˆ«æç¤ºæ³¨å…¥æ–¹é¢è¡¨ç°å‡ºé«˜ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚</li>
<li>åœ¨åŠ¨æ€åœºæ™¯ä¸‹ï¼Œä¸ªäººä¿¡æ¯æ³„éœ²æ£€æµ‹æ€§èƒ½ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09177a7689e9390d40298851b13a42ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dde82b0a2c33e042239ea58f699b6688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d6233e9326b7f038cd6ab51bcdae481.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Calibrated-Language-Models-and-How-to-Find-Them-with-Label-Smoothing"><a href="#Calibrated-Language-Models-and-How-to-Find-Them-with-Label-Smoothing" class="headerlink" title="Calibrated Language Models and How to Find Them with Label Smoothing"></a>Calibrated Language Models and How to Find Them with Label Smoothing</h2><p><strong>Authors:Jerry Huang, Peng Lu, Qiuhao Zeng</strong></p>
<p>Recent advances in natural language processing (NLP) have opened up greater opportunities to enable fine-tuned large language models (LLMs) to behave as more powerful interactive agents through improved instruction-following ability. However, understanding how this impacts confidence calibration for reliable model output has not been researched in full. In this work, we examine various open-sourced LLMs, identifying significant calibration degradation after instruction tuning in each. Seeking a practical solution, we look towards label smoothing, which has been shown as an effective method to regularize for overconfident predictions but has yet to be widely adopted in the supervised fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing is sufficient to maintain calibration throughout the SFT process. However, settings remain where the effectiveness of smoothing is severely diminished, in particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to stem from the ability to become over-confident, which has a direct relationship with the hidden size and vocabulary size, and justify this theoretically and experimentally. Finally, we address an outstanding issue regarding the memory footprint of the cross-entropy loss computation in the label smoothed loss setting, designing a customized kernel to dramatically reduce memory consumption without sacrificing speed or performance in comparison to existing solutions for non-smoothed losses. </p>
<blockquote>
<p>è¿‘æœŸè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è¿›å±•ä¸ºé€šè¿‡æ”¹è¿›æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä½¿ç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°ä¸ºæ›´å¼ºå¤§çš„äº¤äº’å¼ä»£ç†æä¾›äº†æ›´å¤§æœºä¼šã€‚ç„¶è€Œï¼Œäº†è§£è¿™å¯¹å¯é æ¨¡å‹è¾“å‡ºçš„ä¿¡å¿ƒæ ¡å‡†å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å„ç§å¼€æºLLMï¼Œå‘ç°æ¯ä¸ªåœ¨æŒ‡ä»¤è°ƒæ•´åçš„æ ¡å‡†éƒ½æœ‰æ˜æ˜¾é€€åŒ–ã€‚ä¸ºäº†å¯»æ‰¾å®é™…è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬è½¬å‘æ ‡ç­¾å¹³æ»‘ï¼Œå®ƒå·²è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•æ¥é˜²æ­¢è¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ï¼Œä½†å°šæœªåœ¨LLMçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­å¹¿æ³›é‡‡ç”¨ã€‚æˆ‘ä»¬é¦–å…ˆæ´å¯Ÿä¸ºä½•æ ‡ç­¾å¹³æ»‘è¶³ä»¥åœ¨æ•´ä¸ªSFTè¿‡ç¨‹ä¸­ç»´æŒæ ¡å‡†ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€äº›è®¾ç½®ä¸­å¹³æ»‘çš„æœ‰æ•ˆæ€§å¤§å¤§å‡å¼±ï¼Œå°¤å…¶æ˜¯å¤§å‹è¯æ±‡è¡¨LLMï¼ˆLV-LLMï¼‰çš„æƒ…å†µã€‚æˆ‘ä»¬è®¤ä¸ºåŸå› æºäºæˆä¸ºè¿‡åº¦è‡ªä¿¡çš„èƒ½åŠ›ï¼Œå®ƒä¸éšè—å¤§å°å’Œè¯æ±‡å¤§å°æœ‰ç›´æ¥å…³ç³»ï¼Œå¹¶ä»ç†è®ºå’Œå®è·µè§’åº¦å¯¹æ­¤è¿›è¡Œäº†è®ºè¯ã€‚æœ€åï¼Œæˆ‘ä»¬è§£å†³äº†æ ‡ç­¾å¹³æ»‘æŸå¤±è®¾ç½®ä¸­äº¤å‰ç†µæŸå¤±è®¡ç®—çš„å†…å­˜å ç”¨é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªè‡ªå®šä¹‰å†…æ ¸ï¼Œä»¥æ˜¾è‘—å‡å°‘å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¸ç‰ºç‰²é€Ÿåº¦æˆ–ä¸éå¹³æ»‘æŸå¤±çš„ç°æœ‰è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00264v1">PDF</a> Accepted to the Forty-second International Conference on Machine   Learning (ICML) 2025. First two authors contributed equally</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è¿›å±•ä¸ºé€šè¿‡æ”¹è¿›æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°ä¸ºæ›´å¼ºå¤§çš„äº¤äº’å¼ä»£ç†æä¾›äº†æ›´å¤šæœºä¼šã€‚ç„¶è€Œï¼Œå°šæœªå®Œå…¨ç ”ç©¶è¿™å¦‚ä½•å½±å“å¯é æ¨¡å‹è¾“å‡ºçš„ä¿¡å¿ƒæ ¡å‡†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å„ç§å¼€æºLLMï¼Œå‘ç°å®ƒä»¬åœ¨æŒ‡ä»¤è°ƒæ•´åé¢ä¸´æ ¡å‡†é€€åŒ–é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å…³æ³¨æ ‡ç­¾å¹³æ»‘ï¼Œå®ƒå·²è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³è¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ï¼Œä½†åœ¨LLMçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­å°šæœªå¹¿æ³›ä½¿ç”¨ã€‚æˆ‘ä»¬æ·±å…¥æ¢è®¨äº†æ ‡ç­¾å¹³æ»‘åœ¨ç»´æŒSFTè¿‡ç¨‹ä¸­æ ¡å‡†çš„åŸå› ï¼Œå¹¶æ­ç¤ºäº†å¤§å‹è¯æ±‡è¡¨LLMï¼ˆLV-LLMï¼‰ä¸­å¹³æ»‘æ•ˆæœå‡å¼±çš„æƒ…å†µã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯å› ä¸ºå®ƒä»¬å®¹æ˜“å˜å¾—è¿‡äºè‡ªä¿¡ï¼Œä¸éšè—å¤§å°å’Œè¯æ±‡é‡æœ‰ç›´æ¥è”ç³»ï¼Œå¹¶ä»ç†è®ºå’Œå®è·µä¸Šè¯æ˜äº†è¿™ä¸€ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬è§£å†³äº†æ ‡ç­¾å¹³æ»‘æŸå¤±è®¾ç½®ä¸­çš„äº¤å‰ç†µæŸå¤±è®¡ç®—å†…å­˜å ç”¨é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªå®šä¹‰å†…æ ¸ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²é€Ÿåº¦æˆ–æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½å†…å­˜æ¶ˆè€—ï¼Œä¸éå¹³æ»‘æŸå¤±ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„è¿›å±•ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿè¡¨ç°å¾—åƒæ›´å¼ºå¤§çš„äº¤äº’å¼ä»£ç†ã€‚</li>
<li>æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æé«˜å¸¦æ¥äº†ä¿¡å¿ƒæ ¡å‡†çš„é—®é¢˜ï¼Œéœ€è¦æ·±å…¥ç ”ç©¶ã€‚</li>
<li>æ ‡ç­¾å¹³æ»‘æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³è¿‡åº¦è‡ªä¿¡çš„é¢„æµ‹ã€‚</li>
<li>åœ¨LLMçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸­ï¼Œæ ‡ç­¾å¹³æ»‘å¯ä»¥å¸®åŠ©ç»´æŒæ ¡å‡†ã€‚</li>
<li>å¤§å‹è¯æ±‡è¡¨LLMï¼ˆLV-LLMï¼‰å¯èƒ½ä¼šå‡ºç°è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œè¿™ä¸å…¶éšè—å¤§å°å’Œè¯æ±‡é‡æœ‰å…³ã€‚</li>
<li>äº¤å‰ç†µæŸå¤±è®¡ç®—ä¸­çš„å†…å­˜å ç”¨æ˜¯æ ‡ç­¾å¹³æ»‘æŸå¤±è®¾ç½®ä¸­çš„ä¸€ä¸ªé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e5865a5dc9e6857bb6c71d6e9a8b4a70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-068e943b3bd429ff9edb5e56c52d23af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68ce63542061a6bc9afa0791a7f834cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38dc1d0023a779868a39b28ea89cc499.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8260917ae52ccd48b2f057bee347cea2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Watch-the-Weights-Unsupervised-monitoring-and-control-of-fine-tuned-LLMs"><a href="#Watch-the-Weights-Unsupervised-monitoring-and-control-of-fine-tuned-LLMs" class="headerlink" title="Watch the Weights: Unsupervised monitoring and control of fine-tuned   LLMs"></a>Watch the Weights: Unsupervised monitoring and control of fine-tuned   LLMs</h2><p><strong>Authors:Ziqian Zhong, Aditi Raghunathan</strong></p>
<p>The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution.   In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision.   For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42% and can even steer the model to recover â€œunlearnedâ€ information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation.   Our implementation can be found at <a target="_blank" rel="noopener" href="https://github.com/fjzzq2002/WeightWatch">https://github.com/fjzzq2002/WeightWatch</a>. </p>
<blockquote>
<p>å¼ºå¤§çš„å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å¸ƒé€šå¸¸ä¸ä¼šé™„å¸¦å…¶å®Œæ•´çš„è®­ç»ƒæ•°æ®ã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é‚£äº›åŸºäºæ¿€æ´»çš„æ–¹æ³•ï¼Œé€šå¸¸éœ€è¦æˆ–å‡è®¾åˆ†å¸ƒç›¸ä¼¼çš„æ•°æ®ã€‚å½“æ£€æµ‹å’Œé˜²èŒƒåƒåé—¨è¿™æ ·çš„æ–°å‹æ½œåœ¨å¨èƒæ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„é™åˆ¶ï¼Œå› ä¸ºåé—¨åœ¨å®šä¹‰ä¸Šå±äºç¦»ç¾¤å€¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥ç†è§£ã€ç›‘æ§å’Œæ§åˆ¶ç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ–¹æ³•è§£é‡Šæƒé‡è€Œä¸æ˜¯æ¿€æ´»ï¼Œä»è€Œé¿å…äº†éœ€è¦ä½¿ç”¨ç±»ä¼¼äºæœªçŸ¥è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒæ•°æ®ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¾®è°ƒæ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ä¹‹é—´çš„æƒé‡å·®å¼‚çš„ä¸Šéƒ¨å¥‡å¼‚å‘é‡å¯¹åº”äºæ–°è·å¾—çš„è¡Œä¸ºã€‚é€šè¿‡ç›‘æµ‹è¿™äº›æ–¹å‘ä¸Šæ¿€æ´»çš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬å¯ä»¥é«˜ç²¾åº¦åœ°æ£€æµ‹å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥çš„æ˜¾è‘—è¡Œä¸ºã€‚å¯¹äºå¸¦æœ‰ç§˜å¯†è§¦å‘æœºåˆ¶æ—¶ç»•è¿‡å®‰å…¨æœºåˆ¶çš„åé—¨æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é˜»æ­¢é«˜è¾¾100%çš„æ”»å‡»ï¼Œè¯¯æŠ¥ç‡ä½äº1.2%ã€‚å¯¹äºå·²ç»è¿›è¡Œå»å­¦ä¹ çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®æ£€æµ‹åˆ°è¢«åˆ é™¤çš„ä¸»é¢˜çš„æ¨ç†ï¼Œå‡†ç¡®ç‡é«˜è¾¾95.42%ï¼Œç”šè‡³å¯ä»¥å°†æ¨¡å‹å¼•å¯¼æ¢å¤â€œæœªå­¦ä¹ â€çš„ä¿¡æ¯ã€‚é™¤äº†ç›‘æ§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜æ˜¾ç¤ºå‡ºåœ¨é¢„éƒ¨ç½²æ¨¡å‹å®¡è®¡ä¸­çš„æ½œåŠ›ï¼šé€šè¿‡åˆ†æå•†ä¸šæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼ˆOLMoã€Llamaã€Qwenï¼‰ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå‘ç°ç‰¹å®šæ¨¡å‹çš„å¾®è°ƒé‡ç‚¹ï¼ŒåŒ…æ‹¬è¥é”€ç­–ç•¥å’ŒMidjourneyæç¤ºç”Ÿæˆã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨æ­¤æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/fjzzq2002/WeightWatch">https://github.com/fjzzq2002/WeightWatch</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00161v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç†è§£ã€ç›‘æ§å’Œæ§åˆ¶å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è§£è¯»æƒé‡è€Œéæ¿€æ´»å€¼ï¼Œä»è€Œæ— éœ€ä¸æœªçŸ¥è®­ç»ƒæ•°æ®åˆ†å¸ƒç›¸ä¼¼çš„æ•°æ®ã€‚æ–°æ–¹æ³•èƒ½å¤Ÿç²¾å‡†æ£€æµ‹å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥çš„å…³é”®è¡Œä¸ºï¼Œå¹¶å¯¹å¸¦æœ‰åé—¨æ”»å‡»å’Œå®‰å…¨æœºåˆ¶çš„æ¨¡å‹è¿›è¡Œç›‘æ§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½æ£€æµ‹æ¨¡å‹æ˜¯å¦å·²åˆ é™¤æŸäº›ä¸»é¢˜ï¼Œå¹¶å¯èƒ½ç”¨äºæ¨¡å‹é¢„éƒ¨ç½²å®¡è®¡ï¼Œæ­ç¤ºå•†ä¸šæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„ç‰¹å®šç„¦ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°æ–¹æ³•é€šè¿‡è§£è¯»LLMæ¨¡å‹çš„æƒé‡ï¼Œçªç ´äº†ä¼ ç»Ÿè§£é‡Šæ€§æ–¹æ³•å¯¹æ•°æ®åˆ†å¸ƒç›¸ä¼¼çš„é™åˆ¶ã€‚</li>
<li>æ–¹æ³•å¯ä»¥ç²¾å‡†æ£€æµ‹å¾®è°ƒæ¨¡å‹ä¸­å¼•å…¥çš„æ–°è¡Œä¸ºã€‚</li>
<li>å¯¹äºå¸¦æœ‰åé—¨çš„æ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé˜»æ­¢æ”»å‡»å¹¶å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿæ£€æµ‹æ¨¡å‹æ˜¯å¦å·²åˆ é™¤æŸäº›ä¸»é¢˜ï¼Œå¹¶å…·æœ‰ä¸€å®šçš„æ¢å¤â€œæœªå­¦ä¹ â€ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰æ½œåŠ›ç”¨äºæ¨¡å‹é¢„éƒ¨ç½²å®¡è®¡ï¼Œæ­ç¤ºå•†ä¸šæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„ç‰¹å®šç„¦ç‚¹ã€‚</li>
<li>æ–°æ–¹æ³•çš„å®æ–½ç»†èŠ‚å¯åœ¨æŒ‡å®šçš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e86e6ed5db08a3d2260cafe781da196.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc2d330b4f8821e810ed902356cd816b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62afadb9181cc97b6fbf9e3bba71bce9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Private-GPTs-for-LLM-driven-testing-in-software-development-and-machine-learning"><a href="#Private-GPTs-for-LLM-driven-testing-in-software-development-and-machine-learning" class="headerlink" title="Private GPTs for LLM-driven testing in software development and machine   learning"></a>Private GPTs for LLM-driven testing in software development and machine   learning</h2><p><strong>Authors:Jakub Jagielski, Consuelo Rojas, Markus Abel</strong></p>
<p>In this contribution, we examine the capability of private GPTs to automatically generate executable test code based on requirements. More specifically, we use acceptance criteria as input, formulated as part of epics, or stories, which are typically used in modern development processes. This gives product owners, or business intelligence, respectively, a way to directly produce testable criteria through the use of LLMs. We explore the quality of the so-produced tests in two ways: i) directly by letting the LLM generate code from requirements, ii) through an intermediate step using Gherkin syntax. As a result, it turns out that the two-step procedure yields better results -where we define better in terms of human readability and best coding practices, i.e. lines of code and use of additional libraries typically used in testing. Concretely, we evaluate prompt effectiveness across two scenarios: a simple â€œHello Worldâ€ program and a digit classification model, showing that structured prompts lead to higher-quality test outputs. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç§äººGPTæ ¹æ®éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•ä»£ç çš„èƒ½åŠ›ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä½œä¸ºè¾“å…¥é¡¹çš„éªŒæ”¶æ ‡å‡†ï¼Œè¿™äº›æ ‡å‡†è¢«åˆ¶å®šä¸ºå²è¯—æˆ–æ•…äº‹çš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸ç”¨äºç°ä»£å¼€å‘è¿‡ç¨‹ä¸­ã€‚è¿™ä¸ºäº§å“æ‰€æœ‰è€…æˆ–å•†ä¸šæ™ºèƒ½æä¾›äº†ä¸€ç§é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥ç”Ÿæˆå¯æµ‹è¯•æ ‡å‡†çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ç§æ–¹å¼æ¢ç´¢äº†æ‰€ç”Ÿæˆæµ‹è¯•çš„è´¨é‡ï¼šiï¼‰ç›´æ¥è®©å¤§å‹è¯­è¨€æ¨¡å‹ä»éœ€æ±‚ç”Ÿæˆä»£ç ï¼Œiiï¼‰é€šè¿‡ä¸€ä¸ªä½¿ç”¨Gherkinè¯­æ³•çš„ä¸­é—´æ­¥éª¤ã€‚ç»“æœè¡¨æ˜ï¼Œä¸¤æ­¥ç¨‹åºäº§ç”Ÿæ›´å¥½çš„ç»“æœâ€”â€”æˆ‘ä»¬å°†â€œæ›´å¥½â€å®šä¹‰ä¸ºäººç±»å¯è¯»æ€§å’Œæœ€ä½³ç¼–ç å®è·µï¼Œå³ä»£ç è¡Œæ•°ä»¥åŠæµ‹è¯•è¿‡ç¨‹ä¸­é€šå¸¸ä½¿ç”¨çš„é¢å¤–åº“çš„ä½¿ç”¨æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ä¸¤ç§æƒ…å†µä¸‹è¯„ä¼°æç¤ºçš„æœ‰æ•ˆæ€§ï¼šâ€œHello Worldâ€ç¨‹åºå’Œæ•°å­—åˆ†ç±»æ¨¡å‹ã€‚è¿™æ˜¾ç¤ºç»“æ„åŒ–çš„æç¤ºå¯ä»¥äº§ç”Ÿæ›´é«˜è´¨é‡çš„æµ‹è¯•è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06509v2">PDF</a> 5 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç§äººGPTæ ¹æ®éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•ä»£ç çš„èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨åœ¨ç°ä»£åŒ–å¼€å‘æµç¨‹ä¸­å¸¸è§çš„å²è¯—æˆ–æ•…äº‹ä¸­çš„éªŒæ”¶æ ‡å‡†ä½œä¸ºè¾“å…¥ï¼Œè®©äº§å“è´Ÿè´£äººæˆ–å•†ä¸šæ™ºèƒ½äººå‘˜é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥äº§ç”Ÿå¯æµ‹è¯•çš„æ ‡å‡†ã€‚é€šè¿‡è®©å¤§å‹è¯­è¨€æ¨¡å‹ç›´æ¥ä»éœ€æ±‚ç”Ÿæˆä»£ç å’Œä½¿ç”¨Gherkinè¯­æ³•è¿›è¡Œä¸­é—´æ­¥éª¤çš„ä¸¤ç§æ–¹å¼æ¥æ¢ç©¶æ‰€ç”Ÿæˆæµ‹è¯•çš„è´¨é‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸¤æ­¥ç¨‹åºèƒ½äº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œè¿™é‡Œæ‰€è¯´çš„æ›´å¥½æ˜¯æŒ‡äººç±»å¯è¯»æ€§å’Œéµå¾ªæœ€ä½³ç¼–ç å®è·µï¼Œä¾‹å¦‚å‡å°‘ä»£ç è¡Œæ•°å’Œä½¿ç”¨æµ‹è¯•è¿‡ç¨‹ä¸­å¸¸è§çš„é¢å¤–åº“ã€‚é€šè¿‡å¯¹ä¸¤ç§åœºæ™¯ï¼ˆâ€œHello Worldâ€ç¨‹åºå’Œæ•°å­—åˆ†ç±»æ¨¡å‹ï¼‰çš„æç¤ºæœ‰æ•ˆæ€§è¯„ä¼°ï¼Œæ˜¾ç¤ºç»“æ„åŒ–æç¤ºèƒ½äº§ç”Ÿæ›´é«˜è´¨é‡çš„æµ‹è¯•è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§äººGPTå…·å¤‡æ ¹æ®éœ€æ±‚è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œæµ‹è¯•ä»£ç çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥å°†äº§å“éªŒæ”¶æ ‡å‡†è½¬åŒ–ä¸ºå¯æµ‹è¯•çš„æ ‡å‡†ã€‚</li>
<li>é€šè¿‡ç›´æ¥ç”Ÿæˆä»£ç å’Œä½¿ç”¨Gherkinè¯­æ³•çš„ä¸¤ç§æ–¹å¼å¯¹æ¯”ï¼Œå‘ç°ä¸¤æ­¥ç¨‹åºï¼ˆå®šä¹‰éœ€æ±‚å’Œç”Ÿæˆä»£ç ï¼‰äº§ç”Ÿçš„ç»“æœæ›´ä½³ã€‚</li>
<li>ä¸¤æ­¥ç¨‹åºäº§ç”Ÿçš„ç»“æœè¢«è®¤ä¸ºæ˜¯æ›´å¥½çš„ï¼Œå› ä¸ºå®ƒä»¬æ›´æ˜“äºäººç±»é˜…è¯»å¹¶éµå¾ªæœ€ä½³ç¼–ç å®è·µã€‚</li>
<li>æµ‹è¯•è´¨é‡è¯„ä¼°åŒ…æ‹¬å‡å°‘ä»£ç è¡Œæ•°å’Œä½¿ç”¨æµ‹è¯•ä¸­å¸¸è§çš„é¢å¤–åº“ã€‚</li>
<li>åœ¨ä¸åŒåœºæ™¯ä¸‹ï¼ˆå¦‚â€œHello Worldâ€ç¨‹åºå’Œæ•°å­—åˆ†ç±»æ¨¡å‹ï¼‰ï¼Œç»“æ„åŒ–æç¤ºèƒ½æé«˜æµ‹è¯•è¾“å‡ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-30a86d192f721feba6ed4f9f2bb08010.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10d788dce417fe066e6c3886205e379e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-290904d0e0aeee1d0d336bbf3b742e48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-373b30ec5563674feb2f4cc686a11d2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6fd0a15401520b3fb1fc006e0f0dacc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-503cb50a3eecb6692847c424d09bde4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-defc9e7180a19433671dffa3f0a5754d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7e17596addb6d139b47658ec8d67eb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cbc4ed7a730b73a3cd2d902317fa331.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Mitigating-Gender-Bias-via-Fostering-Exploratory-Thinking-in-LLMs"><a href="#Mitigating-Gender-Bias-via-Fostering-Exploratory-Thinking-in-LLMs" class="headerlink" title="Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs"></a>Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs</h2><p><strong>Authors:Kangda Wei, Hasnat Md Abdullah, Ruihong Huang</strong></p>
<p>Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data. We release the code and generated data at: <a target="_blank" rel="noopener" href="https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main">https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸è¡¨ç°å‡ºæ€§åˆ«åè§ï¼Œå¯¼è‡´ä¸åŒæƒ…å¢ƒä¸‹å¯¹ç”·å¥³ä¸»ä½“çš„ä¸å…¬å¹³å¯¹å¾…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œä»¥æ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¢ç´¢æ€§æ€ç»´ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡åœ¨ç»“æ„ç›¸åŒã€é“å¾·æ¨¡ç³Šçš„æƒ…å¢ƒä¸‹å±•ç¤ºç”·æ€§å’Œå¥³æ€§ä¸»äººå…¬çš„æ•…äº‹å¯¹ï¼Œä¿ƒä½¿æ¨¡å‹äº§ç”Ÿé“å¾·åˆ¤æ–­å¹¶å¯¹æ¯”ä»–ä»¬çš„å·®å¼‚æ¥å¼•å‡ºè¿™ç§ç”Ÿæˆæ¨¡å‹çš„ä½¿ç”¨ç›®çš„ï¼Œåˆºæ¿€å®ƒä»¬æ¢ç´¢åŒä¸€äº‹ä»¶çš„é“å¾·è®¤çŸ¥å¯èƒ½å­˜åœ¨çš„å·®å¼‚æ€§å¹¶æ®æ­¤åšå‡ºåˆ¤æ–­ã€‚å½“å­˜åœ¨ä¸ä¸€è‡´æ—¶ï¼Œè¯¥æ¨¡å‹è¢«å¼•å¯¼äº§ç”Ÿå¹³è¡¡ã€æ— æ€§åˆ«çš„åˆ¤æ–­ã€‚è¿™äº›æ•…äº‹åˆ¤æ–­å¯¹ç”¨äºé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘æ€§åˆ«åè§çš„åŒæ—¶ï¼Œè¿˜ä¿æŒç”šè‡³æé«˜äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BB%A3%E7%A0%81%E5%92%8C%E7%94%9F%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/mainä¸Šå‘å¸ƒä»£ç å’Œç”Ÿæˆçš„æ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17217v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨æ€§åˆ«åè§é—®é¢˜ï¼Œå¯¼è‡´åœ¨ä¸åŒæƒ…å¢ƒä¸‹å¯¹ç”·æ€§å’Œå¥³æ€§ä¸»ä½“çš„å¾…é‡ä¸å¹³ç­‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°å‹æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œä¿ƒè¿›LLMsçš„æ¢ç©¶å¼æ€è€ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ç”ŸæˆåŒ…å«ç”·å¥³ä¸»è§’çš„ç»“æ„ç›¸åŒã€é“å¾·æ¨¡ç³Šçš„æ•…äº‹å¯¹ï¼Œæ¿€å‘æ¨¡å‹å¯¹é“å¾·åˆ¤æ–­çš„æ¯”è¾ƒä¸å¼•å¯¼ï¼Œé’ˆå¯¹ä¸ä¸€è‡´ä¹‹å¤„ä½¿æ¨¡å‹äº§ç”Ÿå¹³è¡¡ã€æ— æ€§åˆ«çš„åˆ¤æ–­ã€‚ä½¿ç”¨æ•…äº‹åˆ¤æ–­å¯¹é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘æ€§åˆ«åè§çš„åŒæ—¶ï¼Œä¿æŒç”šè‡³æå‡äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å…¬å¼€ä»£ç å’Œç”Ÿæˆçš„æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså­˜åœ¨æ€§åˆ«åè§é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆæ•…äº‹å¯¹æ¿€å‘æ¨¡å‹å¯¹æ€§åˆ«å¹³ç­‰çš„æ€è€ƒã€‚</li>
<li>æ•…äº‹å¯¹åŒ…å«ç”·å¥³ä¸»è§’åœ¨ç»“æ„ç›¸åŒã€é“å¾·æ¨¡ç³Šçš„åœºæ™¯ä¸­çš„å¯¹æ¯”ã€‚</li>
<li>é€šè¿‡æ¯”è¾ƒæ¨¡å‹çš„é“å¾·åˆ¤æ–­ï¼Œå¼•å¯¼æ¨¡å‹äº§ç”Ÿå¹³è¡¡ã€æ— æ€§åˆ«çš„åˆ¤æ–­ã€‚</li>
<li>ä½¿ç”¨æ•…äº‹åˆ¤æ–­å¯¹é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œæ¨¡å‹çš„å¾®è°ƒæˆ–ä¼˜åŒ–ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•æœ‰æ•ˆå‡å°‘æ€§åˆ«åè§ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ac70b2f39df7af0fab3443e531745e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d81fd3c2a78879e5666e78c92410cc43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2f5cc01b0eb8e3906a5c1fbc8b6eb7e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MV-MATH-Evaluating-Multimodal-Math-Reasoning-in-Multi-Visual-Contexts"><a href="#MV-MATH-Evaluating-Multimodal-Math-Reasoning-in-Multi-Visual-Contexts" class="headerlink" title="MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts"></a>MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts</h2><p><strong>Authors:Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, Cheng-Lin Liu</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown promising capabilities in mathematical reasoning within visual contexts across various datasets. However, most existing multimodal math benchmarks are limited to single-visual contexts, which diverges from the multi-visual scenarios commonly encountered in real-world mathematical applications. To address this gap, we introduce MV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical problems. Each problem integrates multiple images interleaved with text, derived from authentic K-12 scenarios, and enriched with detailed annotations. MV-MATH includes multiple-choice, free-form, and multi-step questions, covering 11 subject areas across 3 difficulty levels, and serves as a comprehensive and rigorous benchmark for assessing MLLMsâ€™ mathematical reasoning in multi-visual contexts. Through extensive experimentation, we observe that MLLMs encounter substantial challenges in multi-visual math tasks, with a considerable performance gap relative to human capabilities on MV-MATH. Furthermore, we analyze the performance and error patterns of various models, providing insights into MLLMsâ€™ mathematical reasoning capabilities within multi-visual settings. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§æ•°æ®é›†çš„å¯è§†åŒ–ä¸Šä¸‹æ–‡ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ä»…é™äºå•ä¸€çš„å¯è§†åŒ–ä¸Šä¸‹æ–‡ï¼Œè¿™ä¸ç°å®ä¸–ç•Œä¸­æ•°å­¦åº”ç”¨æ‰€é‡åˆ°çš„å¤šè§†è§‰åœºæ™¯å­˜åœ¨åå·®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†MV-MATHï¼šä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„åŒ…å«2009ä¸ªé«˜è´¨é‡æ•°å­¦é—®é¢˜çš„æ•°æ®é›†ã€‚æ¯ä¸ªé—®é¢˜èåˆäº†å¤šä¸ªå›¾åƒå’Œæ–‡æœ¬ï¼Œæ¥æºäºçœŸå®çš„K-12åœºæ™¯ï¼Œå¹¶è¾…ä»¥è¯¦ç»†çš„æ³¨é‡Šã€‚MV-MATHåŒ…å«é€‰æ‹©é¢˜ã€è‡ªç”±å½¢å¼å’Œåˆ†æ­¥é—®é¢˜ï¼Œæ¶µç›–3ä¸ªéš¾åº¦çº§åˆ«çš„11ä¸ªä¸»é¢˜é¢†åŸŸï¼Œæ˜¯è¯„ä¼°MLLMsåœ¨å¤šè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›çš„å…¨é¢å’Œä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°MLLMsåœ¨å¤šè§†è§‰æ•°å­¦ä»»åŠ¡ä¸­é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¸MV-MATHä¸Šçš„äººç±»èƒ½åŠ›ç›¸æ¯”ï¼Œå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†å„ç§æ¨¡å‹çš„æ€§èƒ½å’Œé”™è¯¯æ¨¡å¼ï¼Œæ·±å…¥äº†è§£äº†MLLMsåœ¨å¤šè§†è§‰ç¯å¢ƒä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20808v6">PDF</a> 45 pages, accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>MLLMså±•ç°å‡ºè·¨å¤šç§æ•°æ®é›†åœ¨è§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰å¤§å¤šæ•°å¤šæ¨¡å¼æ•°å­¦åŸºå‡†æµ‹è¯•ä»…é™äºå•ä¸€è§†è§‰åœºæ™¯ï¼Œä¸ç°å®ä¸–ç•Œä¸­çš„å¤šè§†è§‰åœºæ™¯æ•°å­¦åº”ç”¨å­˜åœ¨å·®è·ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥MV-MATHæ•°æ®é›†ï¼ŒåŒ…å«2009ä¸ªé«˜è´¨é‡æ•°å­¦é—®é¢˜ï¼Œé›†æˆå¤šä¸ªå›¾åƒå’Œæ–‡æœ¬ï¼ŒæºäºçœŸå®çš„K-12åœºæ™¯ï¼Œå¹¶é™„è¯¦ç»†æ³¨é‡Šã€‚MV-MATHåŒ…æ‹¬é€‰æ‹©é¢˜ã€è‡ªç”±å½¢å¼å’Œè·¨æ­¥éª¤é—®é¢˜ï¼Œæ¶µç›–3ä¸ªéš¾åº¦çº§åˆ«çš„11ä¸ªä¸»é¢˜é¢†åŸŸï¼Œæ˜¯è¯„ä¼°MLLMåœ¨å¤šè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›çš„å…¨é¢ä¸¥æ ¼åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒMLLMåœ¨å¤šè§†è§‰æ•°å­¦ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œä¸MV-MATHä¸Šçš„äººç±»èƒ½åŠ›å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMså±•ç°å‡ºæ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†éœ€é¢å¯¹å¤šè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é™äºå•ä¸€è§†è§‰åœºæ™¯ï¼Œä¸ç°å®åº”ç”¨ä¸ç¬¦ã€‚</li>
<li>MV-MATHæ•°æ®é›†ç”¨äºè¯„ä¼°MLLMåœ¨å¤šè§†è§‰ä¸Šä¸‹æ–‡ä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MV-MATHåŒ…å«çœŸå®K-12åœºæ™¯çš„æ•°å­¦é—®é¢˜ï¼Œå¹¶é™„è¯¦ç»†æ³¨é‡Šã€‚</li>
<li>MV-MATHæ¶µç›–å¤šç§é¢˜å‹å’Œéš¾åº¦çº§åˆ«ï¼Œå…·æœ‰å…¨é¢æ€§å’Œä¸¥æ ¼æ€§ã€‚</li>
<li>MLLMsåœ¨å¤„ç†å¤šè§†è§‰æ•°å­¦ä»»åŠ¡æ—¶å­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a466bff1fddc1e3136553076c8f256ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69f1e105d34c0d7871615cd69ff366d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dec3bdef9d973d64022f86af6cbfcd56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d41853a63df46ad31999b88e3a67445f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8feff1894bb4f3c7106f1f5280d7a3d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d07f01a17e6521e2a86b9cdee1ade9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-072abd12976f5676b9a61c063ee9d021.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="IssueBench-Millions-of-Realistic-Prompts-for-Measuring-Issue-Bias-in-LLM-Writing-Assistance"><a href="#IssueBench-Millions-of-Realistic-Prompts-for-Measuring-Issue-Bias-in-LLM-Writing-Assistance" class="headerlink" title="IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in   LLM Writing Assistance"></a>IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in   LLM Writing Assistance</h2><p><strong>Authors:Paul RÃ¶ttger, Musashi Hinck, Valentin Hofmann, Kobi Hackenburg, Valentina Pyatkin, Faeze Brahman, Dirk Hovy</strong></p>
<p>Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. â€œwrite a blog aboutâ€) and 212 political issues (e.g. â€œAI regulationâ€) from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨å¸®åŠ©æ•°ç™¾ä¸‡ç”¨æˆ·æ’°å†™å…³äºå„ç§é—®é¢˜çš„æ–‡æœ¬ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·ä¼šæ¥è§¦åˆ°ä¸åŒçš„è§‚ç‚¹ã€‚è¿™å¼•å‘äº†å…³äºé—®é¢˜åè§çš„é—®é¢˜ï¼Œå³LLMå¾€å¾€å¯¹ç»™å®šé—®é¢˜åªå‘ˆç°ä¸€ç§è§‚ç‚¹ï¼Œè¿™å¯èƒ½ä¼šåè¿‡æ¥å½±å“ç”¨æˆ·å¯¹è¿™ä¸€é—®é¢˜çš„çœ‹æ³•ã€‚ç„¶è€Œï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿˜æ— æ³•è¡¡é‡åœ¨çœŸå®çš„ç”¨æˆ·äº’åŠ¨ä¸­ï¼ŒLLMå®é™…è¡¨ç°å‡ºå“ªäº›é—®é¢˜åè§ï¼Œè¿™ä½¿å¾—è§£å†³æ¥è‡ªæœ‰åè§çš„LLMçš„é£é™©å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†IssueBenchï¼šä¸€å¥—ç”¨äºè¡¡é‡LLMå†™ä½œè¾…åŠ©ä¸­çš„é—®é¢˜åè§çš„249ä¸‡ä¸ªç°å®æç¤ºã€‚æˆ‘ä»¬æ ¹æ®3900ä¸ªæ¨¡æ¿ï¼ˆä¾‹å¦‚ï¼Œâ€œå†™ä¸€ç¯‡åšå®¢æ–‡ç« â€ï¼‰å’Œæ¥è‡ªçœŸå®ç”¨æˆ·äº’åŠ¨çš„212ä¸ªæ”¿æ²»é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œâ€œäººå·¥æ™ºèƒ½ç›‘ç®¡â€ï¼‰æ„å»ºäº†å®ƒã€‚ä½¿ç”¨IssueBenchï¼Œæˆ‘ä»¬è¯æ˜äº†é—®é¢˜åè§åœ¨æœ€æ–°LLMä¸­æ™®éå­˜åœ¨ä¸”æŒç»­å­˜åœ¨ã€‚æˆ‘ä»¬è¿˜å‘ç°ä¸åŒæ¨¡å‹ä¹‹é—´çš„åè§éå¸¸ç›¸ä¼¼ï¼Œå¹¶ä¸”åœ¨æŸäº›é—®é¢˜ä¸Šï¼Œæ‰€æœ‰æ¨¡å‹çš„è§‚ç‚¹ä¸ç¾å›½æ°‘ä¸»å…šé€‰æ°‘è€Œéå…±å’Œå…šé€‰æ°‘çš„è§‚ç‚¹æ›´ä¸ºä¸€è‡´ã€‚IssueBenchå¯ä»¥è½»æ¾é€‚åº”ä»¥åŒ…å«å…¶ä»–é—®é¢˜ã€æ¨¡æ¿æˆ–ä»»åŠ¡ã€‚é€šè¿‡å®ç°ç¨³å¥å’Œç°å®çš„æµ‹é‡ï¼Œæˆ‘ä»¬å¸Œæœ›IssueBenchèƒ½ä¸ºå…³äºLLMåè§ä»¥åŠå¦‚ä½•è§£å†³å®ƒä»¬çš„è®¨è®ºæä¾›æ–°çš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08395v2">PDF</a> under review</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¸®åŠ©ç”¨æˆ·æ’°å†™å…³äºå„ç§é—®é¢˜çš„æ–‡æœ¬æ—¶ï¼Œä¼šå‘ç”¨æˆ·æš´éœ²ä¸åŒè§‚ç‚¹ï¼Œå¼•å‘å¯¹é—®é¢˜åè§çš„é—®é¢˜ã€‚ç›®å‰å°šæ— æ³•è¡¡é‡LLMåœ¨å®é™…ç”¨æˆ·äº¤äº’ä¸­è¡¨ç°å‡ºçš„å…·ä½“é—®é¢˜åè§ï¼Œéš¾ä»¥è§£å†³LLMåè§é£é™©ã€‚å› æ­¤ï¼Œåˆ›å»ºäº†IssueBenchï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«3.9kæ¨¡æ¿å’ŒçœŸå®ç”¨æˆ·äº¤äº’ä¸­æ”¶é›†çš„212ä¸ªæ”¿æ²»é—®é¢˜çš„è¿‘äº¿æ¡å®é™…å†™ä½œæç¤ºé›†åˆï¼Œç”¨äºæµ‹é‡LLMå†™ä½œè¾…åŠ©ä¸­çš„é—®é¢˜åè§ã€‚ä½¿ç”¨IssueBenchï¼Œæˆ‘ä»¬å‘ç°é—®é¢˜åè§åœ¨æœ€æ–°LLMä¸­æ™®éå­˜åœ¨ä¸”æŒç»­å­˜åœ¨ã€‚æ­¤å¤–ï¼Œä¸åŒæ¨¡å‹ä¹‹é—´çš„åè§æƒŠäººåœ°ç›¸ä¼¼ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨éƒ¨åˆ†é—®é¢˜ä¸Šéƒ½ä¸ç¾å›½æ°‘ä¸»å…šé€‰æ°‘æ„è§æ›´ä¸ºä¸€è‡´ã€‚IssueBenchå¯ä»¥è½»æ¾é€‚åº”åŒ…å«å…¶ä»–è®®é¢˜ã€æ¨¡æ¿æˆ–ä»»åŠ¡çš„åœºæ™¯ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æä¾›ç¨³å¥è€Œç°å®çš„æµ‹é‡æ‰‹æ®µï¼Œä¸ºå…³äºLLMåè§çš„è®¨è®ºä»¥åŠå¦‚ä½•è§£å†³å®ƒä»¬æä¾›æ–°çš„è¯æ®è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMé€šè¿‡å±•ç¤ºä¸åŒè§‚ç‚¹ä¸ºç”¨æˆ·å†™ä½œæä¾›ä¾¿åˆ©ï¼Œä½†ä¹Ÿå¯èƒ½å¼•å‘é—®é¢˜åè§çš„é—®é¢˜ã€‚</li>
<li>ç›®å‰æ— æ³•å‡†ç¡®æµ‹é‡LLMåœ¨ç”¨æˆ·äº¤äº’ä¸­çš„å…·ä½“é—®é¢˜åè§ï¼Œä½¿å¾—è§£å†³åè§é£é™©å˜å¾—å›°éš¾ã€‚</li>
<li>IssueBenchæ—¨åœ¨é€šè¿‡åŒ…å«çœŸå®ç”¨æˆ·äº¤äº’æ•°æ®çš„è¿‘äº¿æ¡å†™ä½œæç¤ºæ¥æµ‹é‡LLMå†™ä½œè¾…åŠ©ä¸­çš„é—®é¢˜åè§ã€‚</li>
<li>IssueBenchå‘ç°æœ€æ–°LLMæ™®éä¸”æŒç»­å­˜åœ¨é—®é¢˜åè§ã€‚</li>
<li>ä¸åŒLLMé—´çš„åè§è¡¨ç°æƒŠäººåœ°ç›¸ä¼¼ï¼Œä¸”åœ¨éƒ¨åˆ†é—®é¢˜ä¸Šä¸ç¾å›½æ°‘ä¸»å…šé€‰æ°‘æ„è§æ›´ä¸€è‡´ã€‚</li>
<li>IssueBenchå¯ä»¥çµæ´»é€‚åº”åŒ…æ‹¬å…¶ä»–è®®é¢˜ã€æ¨¡æ¿æˆ–ä»»åŠ¡çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc47c883a617ba1b99ac3d0f6a7c066f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e21e0d1e69dc04fa37359c8b171a671a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80a21af076fe9b6311fb06e643af79c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6937e6cd521b11e9f75a7c0e542a8c9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="PanoLlama-Generating-Endless-and-Coherent-Panoramas-with-Next-Token-Prediction-LLMs"><a href="#PanoLlama-Generating-Endless-and-Coherent-Panoramas-with-Next-Token-Prediction-LLMs" class="headerlink" title="PanoLlama: Generating Endless and Coherent Panoramas with   Next-Token-Prediction LLMs"></a>PanoLlama: Generating Endless and Coherent Panoramas with   Next-Token-Prediction LLMs</h2><p><strong>Authors:Teng Zhou, Xiaoyu Zhang, Yongchuan Tang</strong></p>
<p>Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research. The code is available at <a target="_blank" rel="noopener" href="https://github.com/0606zt/PanoLlama">https://github.com/0606zt/PanoLlama</a>. </p>
<blockquote>
<p>å…¨æ™¯å›¾åƒç”Ÿæˆï¼ˆPIGï¼‰æ—¨åœ¨åˆ›å»ºä»»æ„é•¿åº¦çš„è¿è´¯å›¾åƒã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½å±äºè”åˆæ‰©æ•£èŒƒå¼ï¼Œä½†å®ƒä»¬å¤æ‚çš„å¯å‘å¼è£å‰ªè¿æ¥è®¾è®¡ç»å¸¸é™åˆ¶å…¶å®ç°å¤šçº§è¿è´¯æ€§çš„èƒ½åŠ›ã€‚é€šè¿‡å°†æ­¤æŒ‘æˆ˜åˆ†è§£ä¸ºå…¶æ ¸å¿ƒç»„ä»¶ï¼Œæˆ‘ä»¬å‘ç°å®ƒä¸ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è‡ªç„¶å¯¹é½ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬é‡‡ç”¨è‡ªå›å½’ï¼ˆARï¼‰èŒƒå¼è¿›è¡ŒPIGå»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰ARï¼ˆVARï¼‰æ¨¡å‹ä»…é™äºå›ºå®šå¤§å°çš„ç”Ÿæˆï¼Œç¼ºä¹ç”Ÿæˆå…¨æ™¯å›¾åƒçš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PanoLlamaï¼Œè¿™æ˜¯ä¸€ä¸ªå®ç°æ— é™è¿è´¯å…¨æ™¯ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼Œé‡‡ç”¨è‡ªå›å½’èŒƒå¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼€å‘äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç­–ç•¥ï¼Œåˆ©ç”¨ä»¤ç‰Œé‡å®šå‘å…‹æœç°æœ‰VARæ¨¡å‹çš„å¤§å°é™åˆ¶ï¼Œå®ç°æ°´å¹³å’Œå‚ç›´æ–¹å‘ä¸Šçš„ä¸‹ä¸€ä¸ªè£å‰ªé¢„æµ‹ã€‚è¿™æ›´æ–°äº†PIGç®¡é“ï¼ŒåŒæ—¶åœ¨è¿è´¯æ€§ï¼ˆ47.50%ï¼‰ã€ä¿çœŸåº¦ï¼ˆ28.16%ï¼‰å’Œç¾å­¦ï¼ˆ15%ï¼‰æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒPanoLlamaæ”¯æŒå…¶ä»–PIGæ–¹æ³•æ— æ³•å®ç°çš„åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬æ— é®ç½©å¸ƒå±€æ§åˆ¶ã€å¤šå°ºåº¦å’Œå¤šæŒ‡å¯¼åˆæˆã€‚ä¸ºäº†ä¿ƒè¿›æ ‡å‡†åŒ–è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†ä¸€ä¸ªåŒ…å«1000ä¸ªæç¤ºå’Œ100å¤šä¸ªä¸»é¢˜çš„æ•°æ®é›†ï¼Œä¸ºPIGç ”ç©¶æä¾›äº†æ–°çš„æµ‹è¯•åŸºå‡†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/0606zt/PanoLlama%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/0606zt/PanoLlamaä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15867v3">PDF</a> </p>
<p><strong>Summary</strong><br>å…¨æ™¯å›¾åƒç”Ÿæˆï¼ˆPIGï¼‰æ—¨åœ¨åˆ›å»ºè¿è´¯çš„ä»»æ„é•¿åº¦å›¾åƒã€‚ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨è”åˆæ‰©æ•£èŒƒå¼ï¼Œä½†å…¶å¤æ‚çš„å¯å‘å¼è£å‰ªè¿æ¥è®¾è®¡é™åˆ¶äº†å…¶å¤šçº§è¿è´¯æ€§èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡è§£æ„è¿™ä¸€æŒ‘æˆ˜çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œå‘ç°å®ƒä¸ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹è‡ªç„¶å¯¹é½ï¼Œäºæ˜¯é‡‡ç”¨è‡ªåŠ¨å›å½’ï¼ˆARï¼‰èŒƒå¼è¿›è¡ŒPIGå»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰ARï¼ˆVARï¼‰æ¨¡å‹ä»…é™äºå›ºå®šå¤§å°çš„ç”Ÿæˆï¼Œæ— æ³•ç”Ÿæˆå…¨æ™¯å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PanoLlamaï¼Œä¸€ä¸ªå®ç°æ— é™è¿è´¯å…¨æ™¯ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼€å‘äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç­–ç•¥ï¼Œé‡‡ç”¨ä»¤ç‰Œé‡å®šå‘å…‹æœç°æœ‰VARæ¨¡å‹çš„å¤§å°é™åˆ¶ï¼Œå®ç°æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„ä¸‹ä¸€ä¸ªè£å‰ªé¢„æµ‹ã€‚è¿™é‡æ–°è®¾è®¡äº†PIGç®¡é“ï¼ŒåŒæ—¶åœ¨è¿è´¯æ€§ï¼ˆ47.50%ï¼‰ã€ä¿çœŸåº¦ï¼ˆ28.16%ï¼‰å’Œç¾å­¦ï¼ˆ15%ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒPanoLlamaè¿˜æ”¯æŒå…¶ä»–PIGæ–¹æ³•æ— æ³•å®ç°çš„åº”ç”¨ï¼ŒåŒ…æ‹¬æ— é®ç½©å¸ƒå±€æ§åˆ¶ã€å¤šå°ºåº¦å¤šæŒ‡å¯¼åˆæˆç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Panoramic Image Generation (PIG) æ—¨åœ¨ç”Ÿæˆè¿è´¯çš„ä»»æ„é•¿åº¦å›¾åƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨è”åˆæ‰©æ•£èŒƒå¼ï¼Œå­˜åœ¨å¤šçº§è¿è´¯æ€§é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡é‡‡ç”¨è‡ªåŠ¨å›å½’ï¼ˆARï¼‰èŒƒå¼è¿›è¡ŒPIGå»ºæ¨¡ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>PanoLlamaæ¡†æ¶å®ç°æ— é™è¿è´¯å…¨æ™¯ç”Ÿæˆï¼Œé‡‡ç”¨æ— éœ€è®­ç»ƒçš„ç­–ç•¥å’Œä»¤ç‰Œé‡å®šå‘ã€‚</li>
<li>PanoLlamaåœ¨è¿è´¯æ€§ã€ä¿çœŸåº¦å’Œç¾å­¦æ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</li>
<li>PanoLlamaæ”¯æŒæ— é®ç½©å¸ƒå±€æ§åˆ¶ã€å¤šå°ºåº¦å¤šæŒ‡å¯¼åˆæˆç­‰ç‹¬ç‰¹åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b514f59709f74565fbf20827dbec084b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16975f6fb37574d9ac06120141ba37b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17828cdc89cf90edbd78ad00da393f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1efee21fdccdce9f5fe866648e87db8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed1fb733d7df32ffa8d7910edd2fda54.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LLaVA-Video-Video-Instruction-Tuning-With-Synthetic-Data"><a href="#LLaVA-Video-Video-Instruction-Tuning-With-Synthetic-Data" class="headerlink" title="LLaVA-Video: Video Instruction Tuning With Synthetic Data"></a>LLaVA-Video: Video Instruction Tuning With Synthetic Data</h2><p><strong>Authors:Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, Chunyuan Li</strong></p>
<p>The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints. </p>
<blockquote>
<p>è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å‘å±•å—åˆ°äº†ä»ç½‘ç»œæ”¶é›†å¤§é‡é«˜è´¨é‡åŸå§‹æ•°æ®å›°éš¾çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œå³åˆ›å»ºä¸“é—¨ä¸ºè§†é¢‘æŒ‡ä»¤è·Ÿéšè®¾è®¡çš„é«˜è´¨é‡åˆæˆæ•°æ®é›†LLaVA-Video-178Kã€‚è¯¥æ•°æ®é›†åŒ…å«å…³é”®ä»»åŠ¡ï¼Œå¦‚è¯¦ç»†æè¿°ã€å¼€æ”¾å¼é—®ç­”ï¼ˆQAï¼‰å’Œå¤šé¡¹é€‰æ‹©QAã€‚é€šè¿‡åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå¹¶ç»“åˆç°æœ‰çš„è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æ–°çš„è§†é¢‘LMMâ€”â€”LLaVA-Videoã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLLaVA-Videoåœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­éƒ½å–å¾—äº†å¼ºåŠ²çš„è¡¨ç°ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è®¡åˆ’å‘å¸ƒæ•°æ®é›†ã€å…¶ç”Ÿæˆç®¡é“å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02713v3">PDF</a> Project page:   <a target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-09-30-llava-video/">https://llava-vl.github.io/blog/2024-09-30-llava-video/</a>; Accepted at TMLR</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹è§†é¢‘å¤§æ¨¡æ€æ¨¡å‹ï¼ˆLLaVA-Videoï¼‰è¢«å¼€å‘å‡ºæ¥ï¼Œè§£å†³äº†ç½‘ç»œå¤§æ•°æ®é‡‡é›†å›°éš¾çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ›å»ºé«˜è´¨é‡åˆæˆæ•°æ®é›†LLaVA-Video-178Kè¿›è¡Œè®­ç»ƒï¼Œæ”¯æŒè¯¦ç»†æ ‡æ³¨ã€å¼€æ”¾é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ç­‰ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜å…¶æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚è®¡åˆ’å‘å¸ƒæ•°æ®é›†ã€ç”Ÿæˆç®¡é“å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€å‘äº†æ–°å‹è§†é¢‘å¤§æ¨¡æ€æ¨¡å‹LLaVA-Videoï¼Œè§£å†³äº†ç½‘ç»œå¤§æ•°æ®é‡‡é›†å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é«˜è´¨é‡åˆæˆæ•°æ®é›†LLaVA-Video-178Kç”¨äºè®­ç»ƒè§†é¢‘æŒ‡ä»¤è·Ÿéšæ¨¡å‹ã€‚</li>
<li>LLaVA-Videoæ”¯æŒè¯¦ç»†æ ‡æ³¨ã€å¼€æ”¾é—®ç­”å’Œå¤šé¡¹é€‰æ‹©é¢˜ç­‰ä»»åŠ¡ã€‚</li>
<li>å®éªŒè¯æ˜LLaVA-Videoåœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LLaVA-Videoæ•°æ®é›†çš„æœ‰æ•ˆæ€§å¾—åˆ°éªŒè¯ã€‚</li>
<li>è®¡åˆ’å…¬å¼€æ•°æ®é›†ã€ç”Ÿæˆç®¡é“å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13517edb27db775ef38e78d7d05e9631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e560a3c73479296df46cf7c2159c076f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f3ba50a22701d12a776b0c8d974bba2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3f727b4e0d7798955c08ca0bc850617.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Loss-Landscape-Degeneracy-and-Stagewise-Development-in-Transformers"><a href="#Loss-Landscape-Degeneracy-and-Stagewise-Development-in-Transformers" class="headerlink" title="Loss Landscape Degeneracy and Stagewise Development in Transformers"></a>Loss Landscape Degeneracy and Stagewise Development in Transformers</h2><p><strong>Authors:Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, Daniel Murfet</strong></p>
<p>Deep learning involves navigating a high-dimensional loss landscape over the neural network parameter space. Over the course of training, complex computational structures form and re-form inside the neural network, leading to shifts in input&#x2F;output behavior. It is a priority for the science of deep learning to uncover principles governing the development of neural network structure and behavior. Drawing on the framework of singular learning theory, we propose that model development is deeply linked to degeneracy in the local geometry of the loss landscape. We investigate this link by monitoring loss landscape degeneracy throughout training, as quantified by the local learning coefficient, for a transformer language model and an in-context linear regression transformer. We show that training can be divided into distinct periods of change in loss landscape degeneracy, and that these changes in degeneracy coincide with significant changes in the internal computational structure and the input&#x2F;output behavior of the transformers. This finding provides suggestive evidence that degeneracy and development are linked in transformers, underscoring the potential of a degeneracy-based perspective for understanding modern deep learning. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¶‰åŠåœ¨é«˜ç»´æŸå¤±æ™¯è§‚ä¸­å¯¼èˆªç¥ç»ç½‘ç»œå‚æ•°ç©ºé—´ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œå†…éƒ¨ä¼šå½¢æˆå’Œé‡æ–°å½¢æˆå¤æ‚çš„è®¡ç®—ç»“æ„ï¼Œå¯¼è‡´è¾“å…¥&#x2F;è¾“å‡ºè¡Œä¸ºå‘ç”Ÿå˜åŒ–ã€‚æ­ç¤ºç¥ç»ç½‘ç»œç»“æ„å’Œè¡Œä¸ºå‘å±•çš„åŸç†æ˜¯æ·±åº¦å­¦ä¹ çš„ç§‘å­¦ç ”ç©¶çš„ä¼˜å…ˆä»»åŠ¡ã€‚æˆ‘ä»¬å€ŸåŠ©å¥‡å¼‚å­¦ä¹ ç†è®ºçš„æ¡†æ¶ï¼Œæå‡ºæ¨¡å‹å‘å±•ä¸æŸå¤±æ™¯è§‚å±€éƒ¨å‡ ä½•çš„é€€åŒ–æ€§æœ‰å¯†åˆ‡è”ç³»ã€‚æˆ‘ä»¬é€šè¿‡ç›‘æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ™¯è§‚é€€åŒ–æ€§ï¼Œä»¥å±€éƒ¨å­¦ä¹ ç³»æ•°è¿›è¡Œé‡åŒ–ï¼Œæ¥ç ”ç©¶è¿™ä¸€è”ç³»ï¼Œç ”ç©¶å¯¹è±¡ä¸ºå˜å‹å™¨è¯­è¨€æ¨¡å‹å’Œä¸Šä¸‹æ–‡çº¿æ€§å›å½’å˜å‹å™¨ã€‚æˆ‘ä»¬å±•ç¤ºäº†è®­ç»ƒè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºå‡ ä¸ªæŸå¤±æ™¯è§‚é€€åŒ–æ€§å‘ç”Ÿæ˜¾è‘—å˜åŒ–çš„æ—¶æœŸï¼Œè¿™äº›é€€åŒ–æ€§çš„å˜åŒ–ä¸å˜å‹å™¨çš„å†…éƒ¨è®¡ç®—ç»“æ„å’Œè¾“å…¥&#x2F;è¾“å‡ºè¡Œä¸ºçš„æ˜¾è‘—å˜åŒ–ç›¸å»åˆã€‚è¿™ä¸€å‘ç°æä¾›äº†é€€åŒ–æ€§ä¸å‘å±•åœ¨å˜å‹å™¨ä¸­ç›¸äº’å…³è”çš„æš—ç¤ºæ€§è¯æ®ï¼Œå¼ºè°ƒäº†ä»é€€åŒ–æ€§è§’åº¦ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.02364v3">PDF</a> To appear, TMLR. Material on essential dynamics from v1 of this   preprint has been removed and developed in arXiv:2501.17745</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æ¶‰åŠåœ¨é«˜ç»´æŸå¤±æ™¯è§‚ä¸­å¯¼èˆªç¥ç»ç½‘ç»œå‚æ•°ç©ºé—´ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œå†…éƒ¨å½¢æˆå’Œé‡æ–°å½¢æˆå¤æ‚çš„è®¡ç®—ç»“æ„ï¼Œå¯¼è‡´è¾“å…¥&#x2F;è¾“å‡ºè¡Œä¸ºå‘ç”Ÿå˜åŒ–ã€‚æˆ‘ä»¬æå‡ºæ¨¡å‹å‘å±•ä¸æŸå¤±æ™¯è§‚å±€éƒ¨å‡ ä½•çš„é€€åŒ–æ€§æœ‰å¯†åˆ‡è”ç³»ï¼Œå¹¶é€šè¿‡ç›‘æµ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ™¯è§‚é€€åŒ–æ€§è¿›è¡Œç ”ç©¶ï¼Œä»¥transformerè¯­è¨€æ¨¡å‹å’Œä¸Šä¸‹æ–‡çº¿æ€§å›å½’transformerä¸ºä¾‹ï¼Œå±•ç¤ºäº†æŸå¤±æ™¯è§‚é€€åŒ–æ€§çš„å˜åŒ–ä¸å†…éƒ¨è®¡ç®—ç»“æ„ä»¥åŠè¾“å…¥&#x2F;è¾“å‡ºè¡Œä¸ºçš„æ˜¾è‘—å˜åŒ–ç›¸ä¸€è‡´ï¼Œè¿™æä¾›äº†é€€åŒ–æ€§ä¸å‘å±•åœ¨transformerä¸­ç›¸è”ç³»çš„è¯æ®ï¼Œçªæ˜¾äº†ä»é€€åŒ–æ€§è§’åº¦ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¶‰åŠå¯¼èˆªé«˜ç»´æŸå¤±æ™¯è§‚å’Œç¥ç»ç½‘ç»œå‚æ•°ç©ºé—´ã€‚</li>
<li>ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå½¢æˆå’Œé‡æ–°å½¢æˆå¤æ‚çš„è®¡ç®—ç»“æ„ã€‚</li>
<li>æ¨¡å‹å‘å±•ä¸æŸå¤±æ™¯è§‚å±€éƒ¨å‡ ä½•çš„é€€åŒ–æ€§æœ‰å¯†åˆ‡è”ç³»ã€‚</li>
<li>é€šè¿‡ç›‘æµ‹æŸå¤±æ™¯è§‚é€€åŒ–æ€§ï¼Œå‘ç°è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ˜æ˜¾çš„é€€åŒ–æ€§å˜åŒ–æœŸã€‚</li>
<li>æŸå¤±æ™¯è§‚é€€åŒ–æ€§çš„å˜åŒ–ä¸ç¥ç»ç½‘ç»œçš„å†…éƒ¨è®¡ç®—ç»“æ„å’Œè¾“å…¥&#x2F;è¾“å‡ºè¡Œä¸ºçš„æ˜¾è‘—å˜åŒ–ç›¸ä¸€è‡´ã€‚</li>
<li>æä¾›äº†é€€åŒ–æ€§ä¸å‘å±•åœ¨transformeræ¨¡å‹ä¸­ç›¸è”ç³»çš„è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.02364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d39677e38a521cf1ec3f031fd3570b57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95ba1a2d03543dd3bdab48372bd81231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb5a924cd824c4253aa6fe65706eeda1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d391bd3e038bb4a273be2ebe3b09e30a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9b52d6cf0dd744e4ee5028cbdcbf0318.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5a6df828ed4375e7e133da154195e59e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
