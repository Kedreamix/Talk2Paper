<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5a6df828ed4375e7e133da154195e59e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-05-æ›´æ–°"><a href="#2025-08-05-æ›´æ–°" class="headerlink" title="2025-08-05 æ›´æ–°"></a>2025-08-05 æ›´æ–°</h1><h2 id="Dynamically-Adaptive-Reasoning-via-LLM-Guided-MCTS-for-Efficient-and-Context-Aware-KGQA"><a href="#Dynamically-Adaptive-Reasoning-via-LLM-Guided-MCTS-for-Efficient-and-Context-Aware-KGQA" class="headerlink" title="Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA"></a>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA</h2><p><strong>Authors:Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu</strong></p>
<p>Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ—¨åœ¨è§£é‡Šè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„å…³ç³»å’Œè¯­ä¹‰ç»“æ„è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œä»¥è·å–å‡†ç¡®ç­”æ¡ˆã€‚æœ€è¿‘çš„KGQAæ–¹æ³•ä¸»è¦éµå¾ªâ€œæ£€ç´¢åæ¨ç†â€çš„æ¨¡å¼ï¼Œä¾èµ–äºå›¾ç¥ç»ç½‘ç»œ(GNNs)æˆ–å¯å‘å¼è§„åˆ™è¿›è¡Œé™æ€è·¯å¾„æå–ï¼Œæˆ–è€…ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æç¤ºæ¥è”åˆæ‰§è¡Œæ£€ç´¢å’Œæ¨ç†çš„åŠ¨æ€è·¯å¾„ç”Ÿæˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå‰è€…ç”±äºé™æ€è·¯å¾„æå–å’Œç¼ºä¹ä¸Šä¸‹æ–‡ç»†åŒ–è€Œé€‚åº”æ€§æœ‰é™ï¼Œåè€…åˆ™ç”±äºä¾èµ–äºå›ºå®šçš„è¯„åˆ†å‡½æ•°å’Œå¤§é‡çš„LLMè°ƒç”¨è€Œè®¡ç®—æˆæœ¬é«˜ï¼Œå¹¶ä¸”åœ¨è·¯å¾„è¯„ä¼°æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºåŠ¨æ€è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ¨ç†ï¼ˆDAMRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†ç¬¦å·æœç´¢ä¸è‡ªé€‚åº”è·¯å¾„è¯„ä¼°ç›¸ç»“åˆçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚DAMRé‡‡ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä½œä¸ºéª¨å¹²ï¼Œä»¥åŸºäºLLMçš„è§„åˆ’å™¨ä¸ºæŒ‡å¯¼ï¼Œåœ¨æ¯ä¸ªæ­¥éª¤é€‰æ‹©å‰kä¸ªç›¸å…³å…³ç³»æ¥å‡å°‘æœç´¢ç©ºé—´ã€‚ä¸ºäº†æé«˜è·¯å¾„è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºTransformerçš„è¯„åˆ†è€…ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ï¼Œè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯è¡Œæ€§ä¼°è®¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šè·³æ¨ç†è¿‡ç¨‹ä¸­æ•æ‰ç»†å¾®çš„è¯­ä¹‰å˜åŒ–ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼“è§£é«˜è´¨é‡ç›‘ç£çš„ç¨€ç¼ºæ€§ï¼ŒDAMRç»“åˆäº†åŠ¨æ€ä¼ªè·¯å¾„ç»†åŒ–æœºåˆ¶ï¼Œå®šæœŸä»æœç´¢è¿‡ç¨‹ä¸­æ¢ç´¢çš„éƒ¨åˆ†è·¯å¾„ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼Œä½¿è¯„åˆ†è€…èƒ½å¤Ÿä¸æ–­é€‚åº”ä¸æ–­å˜åŒ–çš„æ¨ç†è½¨è¿¹åˆ†å¸ƒã€‚åœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDAMRæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00719v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDAMRï¼ˆåŠ¨æ€è‡ªé€‚åº”è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¨ç†ï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ä¸­çš„æ ¸å¿ƒé—®é¢˜ã€‚DAMRç»“åˆäº†ç¬¦å·æœç´¢å’Œè‡ªé€‚åº”è·¯å¾„è¯„ä¼°ï¼Œå®ç°äº†é«˜æ•ˆä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚å®ƒé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰éª¨å¹²å’ŒåŸºäºLLMçš„è§„åˆ’å™¨ï¼Œé€‰æ‹©æ¯ä¸€æ­¥çš„å‰kä¸ªç›¸å…³å…³ç³»æ¥ç¼©å°æœç´¢ç©ºé—´ã€‚ä¸ºæé«˜è·¯å¾„è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŸºäºTransformerçš„è¯„åˆ†å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ï¼Œè¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆç†æ€§ä¼°è®¡ã€‚æ­¤å¤–ï¼Œä¸ºç¼“è§£é«˜è´¨é‡ç›‘ç£æ•°æ®çš„ç¨€ç¼ºæ€§ï¼ŒDAMRé‡‡ç”¨åŠ¨æ€ä¼ªè·¯å¾„ä¼˜åŒ–æœºåˆ¶ï¼Œä»æœç´¢è¿‡ç¨‹ä¸­æ¢ç´¢çš„éƒ¨åˆ†è·¯å¾„ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼Œä½¿è¯„åˆ†å™¨èƒ½å¤Ÿä¸æ–­é€‚åº”ä¸æ–­å˜åŒ–çš„æ¨ç†è½¨è¿¹åˆ†å¸ƒã€‚åœ¨å¤šä¸ªKGQAåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDAMRæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>KGQAæ—¨åœ¨åˆ©ç”¨çŸ¥è¯†å›¾è°±çš„å…³ç³»å’Œè¯­ä¹‰ç»“æ„æ¥è§£ç­”è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œè¿‘æœŸæ–¹æ³•ä¸»è¦éµå¾ªæ£€ç´¢-æ¨ç†èŒƒå¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨é€‚åº”æ€§æœ‰é™å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œç¼ºä¹ä¸Šä¸‹æ–‡ç²¾ä¿®æˆ–å›ºå®šè¯„åˆ†å‡½æ•°ä¸å‡†ç¡®ã€‚</li>
<li>DAMRæ¡†æ¶ç»“åˆäº†ç¬¦å·æœç´¢å’Œè‡ªé€‚åº”è·¯å¾„è¯„ä¼°ï¼Œå®ç°é«˜æ•ˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„KGQAã€‚</li>
<li>DAMRä½¿ç”¨Monte Carloæ ‘æœç´¢ï¼ˆMCTSï¼‰å’ŒLLMè§„åˆ’å™¨æ¥ç¼©å°æœç´¢ç©ºé—´ï¼Œå¹¶é€‰å‡ºæœ€ç›¸å…³çš„å…³ç³»ã€‚</li>
<li>å¼•å…¥åŸºäºTransformerçš„è¯„åˆ†å™¨è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆç†æ€§ä¼°è®¡ï¼Œé€šè¿‡è”åˆç¼–ç é—®é¢˜å’Œå…³ç³»åºåˆ—ã€‚</li>
<li>DAMRé‡‡ç”¨åŠ¨æ€ä¼ªè·¯å¾„ä¼˜åŒ–æœºåˆ¶ï¼Œä»æœç´¢è¿‡ç¨‹ä¸­ç”Ÿæˆè®­ç»ƒä¿¡å·ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”å˜åŒ–çš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDAMRåœ¨KGQAä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ebfc33de37a08936bbe724e2890914b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c62f7894d735a00a6dea0d5634122089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf505b9dc6c487552431b50ab86f1112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e52da940666d043ab1ac88bb37555e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-082a9cf3864a93bde0621052557ae1d0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Analysing-Temporal-Reasoning-in-Description-Logics-Using-Formal-Grammars"><a href="#Analysing-Temporal-Reasoning-in-Description-Logics-Using-Formal-Grammars" class="headerlink" title="Analysing Temporal Reasoning in Description Logics Using Formal Grammars"></a>Analysing Temporal Reasoning in Description Logics Using Formal Grammars</h2><p><strong>Authors:Camille Bourgaux, Anton Gnatenko, MichaÃ«l Thomazo</strong></p>
<p>We establish a correspondence between (fragments of) $\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$ description logic with the LTL operator $\bigcirc^k$, and some specific kinds of formal grammars, in particular, conjunctive grammars (context-free grammars equipped with the operation of intersection). This connection implies that $\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicity of models, and further leads to undecidability of query answering in $\mathcal{TEL}^\bigcirc$, closing a question left open since the introduction of $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidability of query answering for some new interesting fragments of $\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools and algorithms for conjunctive grammars. </p>
<blockquote>
<p>æˆ‘ä»¬å»ºç«‹äº†$\mathcal{TEL}^\bigcirc$ï¼ˆä¸€ç§å¸¦æœ‰LTLè¿ç®—ç¬¦$\bigcirc^k$çš„$\mathcal{EL}$æè¿°é€»è¾‘çš„æ—¶æ€æ‰©å±•ï¼‰çš„ç‰‡æ®µä¸æŸäº›ç‰¹å®šç±»å‹çš„æ­£å¼è¯­æ³•ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œç‰¹åˆ«æ˜¯ä¸ç»“åˆè¯­æ³•ï¼ˆé…å¤‡äº¤é›†æ“ä½œçš„ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼‰ã€‚è¿™ç§è”ç³»æ„å‘³ç€$\mathcal{TEL}^\bigcirc$ä¸å…·æœ‰æ¨¡å‹çš„æœ€ç»ˆå‘¨æœŸæ€§å±æ€§ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥å¯¼è‡´$\mathcal{TEL}^\bigcirc$ä¸­çš„æŸ¥è¯¢å›ç­”ä¸å¯åˆ¤å®šï¼Œè‡ª$\mathcal{TEL}^\bigcirc$å¼•å…¥ä»¥æ¥è¿™ä¸€é—®é¢˜ä¸€ç›´æ‚¬è€Œæœªå†³ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸ä¸º$\mathcal{TEL}^\bigcirc$çš„ä¸€äº›æœ‰è¶£æ–°ç‰‡æ®µå»ºç«‹æŸ¥è¯¢å›ç­”çš„åˆ¤å®šæ€§ï¼Œå¹¶ä¸ºæ­¤ç›®çš„é‡ç”¨ç»“åˆè¯­æ³•çš„ç°æœ‰å·¥å…·å’Œç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00575v1">PDF</a> This is an extended version of a paper appearing at the 28th European   Conference on Artificial Intelligence (ECAI 2025). 20 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å»ºç«‹äº†æè¿°é€»è¾‘$\mathcal{TEL}^\bigcirc$ï¼ˆä¸€ç§å¸¦æœ‰LTLæ“ä½œç¬¦$\bigcirc^k$çš„$\mathcal{EL}$æè¿°é€»è¾‘çš„æ—¶ç©ºæ‰©å±•ï¼‰ä¸æŸäº›ç‰¹å®šç±»å‹çš„æ­£å¼è¯­æ³•ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œç‰¹åˆ«æ˜¯ä¸ç»“åˆè¯­æ³•ï¼ˆå¸¦æœ‰äº¤é›†æ“ä½œçš„ä¸Šä¸‹æ–‡æ— å…³è¯­æ³•ï¼‰çš„å¯¹åº”ã€‚è¿™ä¸€è”ç³»è¡¨æ˜$\mathcal{TEL}^\bigcirc$ä¸å…·æœ‰æ¨¡å‹çš„æœ€ç»ˆå‘¨æœŸæ€§å±æ€§ï¼Œå¹¶è¿›ä¸€æ­¥å¯¼è‡´$\mathcal{TEL}^\bigcirc$ä¸­çš„æŸ¥è¯¢å›ç­”å˜å¾—ä¸å¯åˆ¤å®šï¼Œä»è€Œè§£å†³äº†ä¸€ä¸ªè‡ª$\mathcal{TEL}^\bigcirc$å¼•å…¥ä»¥æ¥å°±æ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸ä¸º$\mathcal{TEL}^\bigcirc$çš„ä¸€äº›æ–°æœ‰è¶£ç‰‡æ®µå»ºç«‹æŸ¥è¯¢å›ç­”çš„åˆ¤å®šæ€§ï¼Œå¹¶ä¸ºæ­¤ç›®çš„é‡ç”¨ç»“åˆè¯­æ³•çš„ç°æœ‰å·¥å…·å’Œç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»ºç«‹äº†$\mathcal{TEL}^\bigcirc$ä¸ç‰¹å®šç±»å‹çš„æ­£å¼è¯­æ³•ï¼ˆç‰¹åˆ«æ˜¯ç»“åˆè¯­æ³•ï¼‰ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>$\mathcal{TEL}^\bigcirc$ä¸å…·æœ‰æ¨¡å‹çš„æœ€ç»ˆå‘¨æœŸæ€§å±æ€§ã€‚</li>
<li>$\mathcal{TEL}^\bigcirc$ä¸­çš„æŸ¥è¯¢å›ç­”å˜å¾—ä¸å¯åˆ¤å®šï¼Œè§£å†³äº†ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„é—®é¢˜ã€‚</li>
<li>å¯¹äº$\mathcal{TEL}^\bigcirc$çš„ä¸€äº›æ–°ç‰‡æ®µï¼Œå¯ä»¥å»ºç«‹æŸ¥è¯¢å›ç­”çš„åˆ¤å®šæ€§ã€‚</li>
<li>å¯ä»¥åˆ©ç”¨ç°æœ‰å·¥å…·å’Œç®—æ³•æ¥è§£å†³ç»“åˆè¯­æ³•çš„é—®é¢˜ã€‚</li>
<li>è¿™ç§è”ç³»å¯¹äºç†è§£å’Œåˆ†æ$\mathcal{TEL}^\bigcirc$çš„æ€§è´¨å’Œé™åˆ¶æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ff7fe877084587872932d250a36d54d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-114a4a4547df7f7591d4901874518511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a61b080579108da310c93042196574a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6388b5bb4117f557fa1140741764fac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b18d41c9a3e33d68110eca64c82cf418.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SynAdapt-Learning-Adaptive-Reasoning-in-Large-Language-Models-via-Synthetic-Continuous-Chain-of-Thought"><a href="#SynAdapt-Learning-Adaptive-Reasoning-in-Large-Language-Models-via-Synthetic-Continuous-Chain-of-Thought" class="headerlink" title="SynAdapt: Learning Adaptive Reasoning in Large Language Models via   Synthetic Continuous Chain-of-Thought"></a>SynAdapt: Learning Adaptive Reasoning in Large Language Models via   Synthetic Continuous Chain-of-Thought</h2><p><strong>Authors:Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, Ziqian Zeng</strong></p>
<p>While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off. </p>
<blockquote>
<p>è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†ç”±äºç”Ÿæˆç¦»æ•£CoTæ ‡è®°ï¼ˆDCoTï¼‰è€Œäº§ç”Ÿäº†æ˜¾è‘—çš„æ—¶é—´æˆæœ¬ã€‚è¿ç»­CoTï¼ˆCCoTï¼‰æä¾›äº†æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç°æœ‰çš„CCoTæ–¹æ³•å—åˆ°é—´æ¥å¾®è°ƒã€å¯¹é½æœ‰é™æˆ–ç›®æ ‡ä¸ä¸€è‡´çš„é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ›æ–°çš„é«˜æ•ˆæ¨ç†æ¡†æ¶SynAdaptã€‚å…·ä½“æ¥è¯´ï¼ŒSynAdaptç”ŸæˆåˆæˆCCoTï¼Œä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ç²¾ç¡®æœ‰æ•ˆçš„å¯¹é½ç›®æ ‡ã€‚è¿™ç§åˆæˆCCoTæ˜ç¡®æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ CCoTå¹¶ç›´æ¥æ¨å¯¼å‡ºå‡†ç¡®ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œä»…ä¾èµ–CCoTè§£å†³éš¾é¢˜æ˜¯ä¸è¶³å¤Ÿçš„ã€‚ä¸ºæ­¤ï¼ŒSynAdapté›†æˆéš¾åº¦åˆ†ç±»å™¨ï¼Œåˆ©ç”¨é—®é¢˜ä¸Šä¸‹æ–‡å’ŒCCoTæ¥è¯†åˆ«éš¾é¢˜ã€‚CCoTåœ¨çŸ­æš‚æ¨ç†åèƒ½æœ‰æ•ˆå¸®åŠ©è¯†åˆ«éš¾é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°æ€è€ƒè¿™äº›éš¾é¢˜ä»¥æé«˜æ€§èƒ½ã€‚åœ¨ä¸åŒéš¾åº¦çº§åˆ«çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒç»“æœå……åˆ†è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†æœ€ä½³çš„å‡†ç¡®æ€§-æ•ˆç‡æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00574v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€é¡¹å…³äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æŠ¥å‘Šã€‚æ–‡ç« æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç»“åˆè¿ç»­è®¤çŸ¥é“¾ï¼ˆCCoTï¼‰å’Œåˆæˆè®¤çŸ¥é“¾ï¼ˆSynthetic CCoTï¼‰æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æŠ¥å‘Šæå‡ºäº†ä¸€ç§åä¸ºSynAdaptçš„åˆ›æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç”ŸæˆåˆæˆCCoTä½œä¸ºç²¾ç¡®æœ‰æ•ˆçš„å¯¹é½ç›®æ ‡æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜ä»‹ç»äº†é›†æˆéš¾åº¦åˆ†ç±»å™¨çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«éš¾ä»¥å›ç­”çš„é—®é¢˜å¹¶ä¿ƒä½¿æ¨¡å‹é‡æ–°æ€è€ƒè¿™äº›é—®é¢˜ä»¥æé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„å®éªŒç»“æœè¡¨ç°å‡ºæœ€ä½³çš„æ€§èƒ½æ•ˆç‡æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Chain-of-Thought (CoT) æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œä½†ç”±äºç¦»æ•£CoTæ ‡è®°ï¼ˆDCoTï¼‰çš„äº§ç”Ÿäº§ç”Ÿäº†æ˜¾è‘—çš„æ—¶é—´æˆæœ¬ã€‚</li>
<li>è¿ç»­CoTï¼ˆCCoTï¼‰ä½œä¸ºä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ³•å—åˆ°äº†å…³æ³¨ï¼Œä½†ç°æœ‰CCoTæ–¹æ³•å­˜åœ¨é—´æ¥å¾®è°ƒã€å¯¹é½æœ‰é™æˆ–ç›®æ ‡ä¸ä¸€è‡´ç­‰å±€é™æ€§ã€‚</li>
<li>SynAdaptæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç”ŸæˆåˆæˆCCoTä½œä¸ºç²¾ç¡®æœ‰æ•ˆçš„å¯¹é½ç›®æ ‡æ¥å…‹æœè¿™äº›é™åˆ¶ï¼Œå¹¶æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­¦ä¹ CCoTå¹¶å¾—å‡ºå‡†ç¡®ç­”æ¡ˆã€‚</li>
<li>ä»…ä¾èµ–CCoTæ— æ³•è§£å†³å¤æ‚é—®é¢˜ã€‚SynAdaptç»“åˆäº†éš¾åº¦åˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨åˆ©ç”¨é—®é¢˜ä¸Šä¸‹æ–‡å’ŒCCoTæ¥è¯†åˆ«éš¾ä»¥å›ç­”çš„é—®é¢˜ï¼Œå¹¶ä¿ƒä½¿æ¨¡å‹é‡æ–°æ€è€ƒè¿™äº›é—®é¢˜ä»¥æé«˜æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edced01797ff3b96a0b58450c0d8ee30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e672ff69b4a673696d0176b500bea8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-584429837d2a11ef818c66dfe4cb1d4d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Thinking-Machines-Mathematical-Reasoning-in-the-Age-of-LLMs"><a href="#Thinking-Machines-Mathematical-Reasoning-in-the-Age-of-LLMs" class="headerlink" title="Thinking Machines: Mathematical Reasoning in the Age of LLMs"></a>Thinking Machines: Mathematical Reasoning in the Age of LLMs</h2><p><strong>Authors:Andrea Asperti, Alberto Naibo, Claudio Sacerdoti Coen</strong></p>
<p>Large Language Models (LLMs) have shown remarkable abilities in structured reasoning and symbolic tasks, with coding emerging as a particular area of strength. This success has sparked growing interest in applying LLMs to mathematics, both in informal problem-solving and formal theorem proving. However, progress in formal mathematics has proven to be significantly more difficult, despite surface-level similarities between programming and proof construction. This discrepancy raises important questions about how LLMs &#96;&#96;reasonâ€™â€™, how they are supervised, and whether they internally track a notion of computational or deductive state. In this article, we address the state-of-the-art of the discipline, focusing on recent models and benchmarks, and explore three central issues at the intersection of machine learning and mathematical cognition: (i) the trade-offs between formal and informal mathematics as training domains; (ii) the deeper reasons why proof generation remains more brittle than code synthesis; (iii) and the question of whether LLMs represent, or merely mimic, a notion of evolving logical state. Our goal is not to draw hard boundaries, but to identify where the current limits lie, and how they might be extended. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»“æ„æ¨ç†å’Œç¬¦å·ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå…¶ä¸­ç¼–ç¨‹æˆä¸ºäº†ä¸€ä¸ªç‰¹åˆ«æ“…é•¿çš„é¢†åŸŸã€‚è¿™ä¸€æˆåŠŸå¼•å‘äº†å°†LLMåº”ç”¨äºæ•°å­¦ï¼ˆæ— è®ºæ˜¯éæ­£å¼çš„é—®é¢˜è§£å†³è¿˜æ˜¯æ­£å¼å®šç†è¯æ˜ï¼‰çš„å…´è¶£ä¸æ–­å¢é•¿ã€‚ç„¶è€Œï¼Œå°½ç®¡ç¼–ç¨‹å’Œè¯æ˜æ„å»ºä¹‹é—´å­˜åœ¨è¡¨é¢ç›¸ä¼¼æ€§ï¼Œä½†åœ¨æ­£å¼æ•°å­¦æ–¹é¢çš„è¿›å±•å´è¯æ˜è¦å›°éš¾å¾—å¤šã€‚è¿™ç§å·®å¼‚å¼•å‘äº†å…³äºLLMå¦‚ä½•â€œæ¨ç†â€ã€å¦‚ä½•ç›‘ç£ä»¥åŠå®ƒä»¬æ˜¯å¦å†…éƒ¨è·Ÿè¸ªè®¡ç®—æˆ–æ¼”ç»çŠ¶æ€ç­‰é‡è¦é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è¯¥å­¦ç§‘çš„æœ€æ–°å‘å±•ï¼Œé‡ç‚¹å…³æ³¨æœ€è¿‘çš„æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ¢è®¨äº†æœºå™¨å­¦ä¹ ä¸æ•°å­¦è®¤çŸ¥äº¤å‰çš„ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šï¼ˆiï¼‰æ­£å¼å’Œéæ­£å¼æ•°å­¦ä½œä¸ºè®­ç»ƒé¢†åŸŸçš„æƒè¡¡ï¼›ï¼ˆiiï¼‰è¯æ˜ç”Ÿæˆä»ç„¶æ¯”ä»£ç åˆæˆæ›´è„†å¼±èƒŒåçš„æ·±å±‚åŸå› ï¼›ï¼ˆiiiï¼‰ä»¥åŠLLMæ˜¯å¦ä»£è¡¨æˆ–ä»…ä»…æ˜¯æ¨¡ä»¿ä¸æ–­å˜åŒ–çš„é€»è¾‘çŠ¶æ€çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯åˆ’å®šæ˜ç¡®çš„ç•Œé™ï¼Œè€Œæ˜¯ç¡®å®šå½“å‰å­˜åœ¨çš„é™åˆ¶ä»¥åŠå¦‚ä½•æ‰©å±•è¿™äº›é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00459v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–æ¨ç†å’Œç¬¦å·ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œç¼–ç¨‹æˆä¸ºå…¶ä¼˜åŠ¿é¢†åŸŸä¹‹ä¸€ã€‚è¿™å¼•å‘äº†å°†LLMsåº”ç”¨äºæ•°å­¦ï¼ˆåŒ…æ‹¬éæ­£å¼é—®é¢˜è§£å†³å’Œå½¢å¼åŒ–å®šç†è¯æ˜ï¼‰çš„å…´è¶£ã€‚å°½ç®¡ç¼–ç¨‹å’Œè¯æ˜æ„å»ºä¹‹é—´å­˜åœ¨è¡¨é¢ç›¸ä¼¼æ€§ï¼Œä½†åœ¨æ•°å­¦å½¢å¼åŒ–æ–¹é¢çš„è¿›å±•è¦å›°éš¾å¾—å¤šã€‚æœ¬æ–‡å…³æ³¨è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæ¢è®¨æœºå™¨å­¦ä¹ ä¸æ•°å­¦è®¤çŸ¥äº¤å‰çš„ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šæ­£å¼ä¸éæ­£å¼æ•°å­¦çš„è®­ç»ƒé¢†åŸŸä¹‹é—´çš„æƒè¡¡ã€è¯æ˜ç”Ÿæˆç›¸è¾ƒäºä»£ç åˆæˆæ›´ä¸ºè„†å¼±çš„åŸå› ï¼Œä»¥åŠLLMsæ˜¯çœŸæ­£ä»£è¡¨è¿˜æ˜¯ä»…æ¨¡ä»¿é€»è¾‘çŠ¶æ€çš„æ¼”å˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯åˆ’å®šç•Œé™ï¼Œè€Œæ˜¯ç¡®å®šå½“å‰å­˜åœ¨çš„é™åˆ¶ä»¥åŠå¦‚ä½•æ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç»“æ„åŒ–æ¨ç†å’Œç¬¦å·ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå°¤å…¶åœ¨ç¼–ç¨‹é¢†åŸŸã€‚</li>
<li>LLMsè¢«åº”ç”¨äºæ•°å­¦é—®é¢˜è§£å†³å’Œå®šç†è¯æ˜ï¼Œä½†å½¢å¼åŒ–æ•°å­¦çš„è¿›å±•è¾ƒä¸ºå›°éš¾ã€‚</li>
<li>LLMsåœ¨è¯æ˜ç”Ÿæˆæ–¹é¢ç›¸è¾ƒäºä»£ç åˆæˆæ›´ä¸ºè„†å¼±ã€‚</li>
<li>ç›®å‰å­˜åœ¨ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šæ­£å¼ä¸éæ­£å¼æ•°å­¦çš„è®­ç»ƒé¢†åŸŸæƒè¡¡ã€è¯æ˜ç”Ÿæˆè„†å¼±æ€§çš„æ·±å±‚åŸå› ã€LLMsæ˜¯å¦çœŸæ­£ä»£è¡¨é€»è¾‘çŠ¶æ€çš„æ¼”å˜ã€‚</li>
<li>æœºå™¨å­¦ä¹ ä¸æ•°å­¦è®¤çŸ¥çš„äº¤å‰æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>ç°æœ‰çš„ç ”ç©¶æ—¨åœ¨äº†è§£LLMsçš„ç•Œé™ï¼Œå¹¶å¯»æ‰¾æ‰©å±•å…¶èƒ½åŠ›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abaddbfc4dccb9f2132f21a647e281d9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Loop-Invariant-Generation-A-Hybrid-Framework-of-Reasoning-optimised-LLMs-and-SMT-Solvers"><a href="#Loop-Invariant-Generation-A-Hybrid-Framework-of-Reasoning-optimised-LLMs-and-SMT-Solvers" class="headerlink" title="Loop Invariant Generation: A Hybrid Framework of Reasoning optimised   LLMs and SMT Solvers"></a>Loop Invariant Generation: A Hybrid Framework of Reasoning optimised   LLMs and SMT Solvers</h2><p><strong>Authors:Varun Bharti, Shashwat Jha, Dhruv Kumar, Pankaj Jalote</strong></p>
<p>Loop invariants are essential for proving the correctness of programs with loops. Developing loop invariants is challenging, and fully automatic synthesis cannot be guaranteed for arbitrary programs. Some approaches have been proposed to synthesize loop invariants using symbolic techniques and more recently using neural approaches. These approaches are able to correctly synthesize loop invariants only for subsets of standard benchmarks. In this work, we investigate whether modern, reasoning-optimized large language models can do better. We integrate OpenAIâ€™s O1, O1-mini, and O3-mini into a tightly coupled generate-and-check pipeline with the Z3 SMT solver, using solver counterexamples to iteratively guide invariant refinement. We use Code2Inv benchmark, which provides C programs along with their formal preconditions and postconditions. On this benchmark of 133 tasks, our framework achieves 100% coverage (133 out of 133), outperforming the previous best of 107 out of 133, while requiring only 1-2 model proposals per instance and 14-55 seconds of wall-clock time. These results demonstrate that LLMs possess latent logical reasoning capabilities which can help automate loop invariant synthesis. While our experiments target C-specific programs, this approach should be generalizable to other imperative languages. </p>
<blockquote>
<p>å¾ªç¯ä¸å˜å¼å¯¹äºè¯æ˜å¸¦æœ‰å¾ªç¯çš„ç¨‹åºæ­£ç¡®æ€§è‡³å…³é‡è¦ã€‚å¼€å‘å¾ªç¯ä¸å˜å¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸”æ— æ³•ä¸ºä»»æ„ç¨‹åºä¿è¯å®Œå…¨è‡ªåŠ¨ç»¼åˆã€‚å·²ç»æå‡ºä¸€äº›ä½¿ç”¨ç¬¦å·æŠ€æœ¯å’Œæœ€è¿‘ä½¿ç”¨ç¥ç»ç½‘ç»œæ–¹æ³•ç»¼åˆå¾ªç¯ä¸å˜å¼çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ä»…èƒ½ä¸ºæ ‡å‡†åŸºå‡†æµ‹è¯•é›†çš„å­é›†æ­£ç¡®ç»¼åˆå¾ªç¯ä¸å˜å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥ç°ä»£ã€ç»è¿‡ä¼˜åŒ–æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½åšå¾—æ›´å¥½ã€‚æˆ‘ä»¬å°†OpenAIçš„O1ã€O1-miniå’ŒO3-miniä¸Z3 SMTæ±‚è§£å™¨ç´§å¯†ç»“åˆç”Ÿæˆå’Œæ£€æŸ¥ç®¡é“ï¼Œä½¿ç”¨æ±‚è§£å™¨åä¾‹æ¥è¿­ä»£æŒ‡å¯¼ä¸å˜å¼ç»†åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨Code2InvåŸºå‡†æµ‹è¯•é›†ï¼Œè¯¥æµ‹è¯•é›†æä¾›Cç¨‹åºåŠå…¶æ­£å¼çš„å‰ç½®æ¡ä»¶å’Œåç½®æ¡ä»¶ã€‚åœ¨è¿™133é¡¹ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†100%è¦†ç›–ç‡ï¼ˆ133é¡¹ä¸­çš„133é¡¹ï¼‰ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æˆç»©ï¼ˆ133é¡¹ä¸­çš„107é¡¹ï¼‰ï¼ŒåŒæ—¶æ¯ä¸ªå®ä¾‹ä»…éœ€1-2ä¸ªæ¨¡å‹ææ¡ˆï¼Œå¢™é’Ÿæ—¶é—´14-55ç§’ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡æ½œåœ¨çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ©äºè‡ªåŠ¨åŒ–ç»¼åˆå¾ªç¯ä¸å˜å¼ã€‚è™½ç„¶æˆ‘ä»¬çš„å®éªŒé’ˆå¯¹Cç‰¹å®šç¨‹åºï¼Œä½†æ­¤æ–¹æ³•åº”å¯æ¨å¹¿åˆ°å…¶ä»–å‘½ä»¤å¼è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00419v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¾ªç¯ä¸å˜å¼åœ¨è¯æ˜å¸¦æœ‰å¾ªç¯çš„ç¨‹åºæ­£ç¡®æ€§æ–¹é¢çš„é‡è¦æ€§ã€‚ç”±äºå¼€å‘å¾ªç¯ä¸å˜å¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸”æ— æ³•ä¸ºä»»æ„ç¨‹åºå®ç°å…¨è‡ªåŠ¨ç»¼åˆï¼Œå› æ­¤å·²æå‡ºä¸€äº›ä½¿ç”¨ç¬¦å·æŠ€æœ¯å’Œç¥ç»ç½‘ç»œæ–¹æ³•è¿›è¡Œåˆæˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…èƒ½ä¸ºæ ‡å‡†åŸºå‡†æµ‹è¯•é›†çš„å­é›†æ­£ç¡®åˆæˆå¾ªç¯ä¸å˜å¼ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½æ›´å¥½åœ°è¿›è¡Œæ¨ç†ã€‚ç ”ç©¶æ•´åˆäº†OpenAIçš„O1ã€O1-miniå’ŒO3-miniæ¨¡å‹ï¼Œä¸Z3 SMTæ±‚è§£å™¨æ„å»ºç´§å¯†è€¦åˆçš„ç”Ÿæˆå’Œæ£€æŸ¥ç®¡é“ï¼Œåˆ©ç”¨æ±‚è§£å™¨åä¾‹è¿­ä»£åœ°å¼•å¯¼ä¸å˜å¼ä¼˜åŒ–ã€‚åœ¨Code2InvåŸºå‡†æµ‹è¯•é›†ä¸Šï¼Œç ”ç©¶æ¡†æ¶å®ç°äº†100%çš„è¦†ç›–ç‡ï¼Œä¼˜äºä¹‹å‰çš„æœ€ä½³æˆç»©ï¼Œä¸”æ¯ä¸ªå®ä¾‹ä»…éœ€1-2ä¸ªæ¨¡å‹ææ¡ˆï¼Œå¢™é’Ÿæ—¶é—´14-55ç§’ã€‚ç»“æœè¯æ˜å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡æ½œåœ¨çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ©äºè‡ªåŠ¨åŒ–åˆæˆå¾ªç¯ä¸å˜å¼ã€‚å°½ç®¡æœ¬ç ”ç©¶é’ˆå¯¹Cè¯­è¨€ç¨‹åºï¼Œä½†æ­¤æ–¹æ³•åº”å¯æ¨å¹¿åˆ°å…¶ä»–å‘½ä»¤å¼è¯­è¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾ªç¯ä¸å˜å¼å¯¹è¯æ˜å¸¦æœ‰å¾ªç¯çš„ç¨‹åºæ­£ç¡®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å¼€å‘å¾ªç¯ä¸å˜å¼å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå…¨è‡ªåŠ¨ç»¼åˆæ— æ³•å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»…èƒ½æ­£ç¡®åˆæˆå¾ªç¯ä¸å˜å¼çš„æ ‡å‡†åŸºå‡†æµ‹è¯•é›†çš„å­é›†ã€‚</li>
<li>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡æ½œåœ¨é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ©äºè‡ªåŠ¨åŒ–åˆæˆå¾ªç¯ä¸å˜å¼ã€‚</li>
<li>ç ”ç©¶æ•´åˆäº†OpenAIçš„å¤šä¸ªæ¨¡å‹ä¸Z3 SMTæ±‚è§£å™¨ï¼Œæ„å»ºç”Ÿæˆå’Œæ£€æŸ¥ç®¡é“ã€‚</li>
<li>åœ¨Code2InvåŸºå‡†æµ‹è¯•é›†ä¸Šï¼Œç ”ç©¶æ¡†æ¶å®ç°äº†100%çš„è¦†ç›–ç‡ï¼Œä¼˜äºä¹‹å‰çš„æœ€ä½³æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a902614b4652ded25017cea99e1980cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-502d49a3474331f72becbb63c69d53a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2496f37a9977dc2dd84117aa433103ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e3860303a324dbd40526f15fa714d99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc222e44dd6e9e0d6b69c6450eb65ba3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Co-Reward-Self-supervised-Reinforcement-Learning-for-Large-Language-Model-Reasoning-via-Contrastive-Agreement"><a href="#Co-Reward-Self-supervised-Reinforcement-Learning-for-Large-Language-Model-Reasoning-via-Contrastive-Agreement" class="headerlink" title="Co-Reward: Self-supervised Reinforcement Learning for Large Language   Model Reasoning via Contrastive Agreement"></a>Co-Reward: Self-supervised Reinforcement Learning for Large Language   Model Reasoning via Contrastive Agreement</h2><p><strong>Authors:Zizhuo Zhang, Jianing Zhu, Xinmu Ge, Zihua Zhao, Zhanke Zhou, Xuan Li, Xiao Feng, Jiangchao Yao, Bo Han</strong></p>
<p>Although reinforcement learning with verifiable rewards (RLVR) shows promise in improving the reasoning ability of large language models (LLMs), the scaling up dilemma remains due to the reliance on human annotated labels especially for complex tasks. Recent alternatives that explore various self-reward signals exhibit the eliciting potential of LLM reasoning, but suffer from the non-negligible collapse issue. Inspired by the success of self-supervised learning, we propose \textit{Co-Reward}, a novel RL framework that leverages contrastive agreement across semantically analogical questions as a reward basis. Specifically, we construct a similar question for each training sample (without labels) and synthesize their individual surrogate labels through a simple rollout voting, and then the reward is constructed by cross-referring the labels of each question pair to enforce the internal reasoning consistency across analogical inputs. Intuitively, such a self-supervised reward-shaping mechanism increases the difficulty of learning collapse into a trivial solution, and promotes stable reasoning elicitation and improvement through expanding the input sample variants. Empirically, Co-Reward achieves superior performance compared to other self-reward baselines on multiple reasoning benchmarks and LLM series, and reaches or even surpasses ground-truth (GT) labeled reward, with improvements of up to $+6.8%$ on MATH500 over GT reward on Llama-3.2-3B-Instruct. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/Co-Reward">https://github.com/tmlr-group/Co-Reward</a>. </p>
<blockquote>
<p>å°½ç®¡é€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºå…¶ç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸Šå¯¹äººç±»æ ‡æ³¨æ ‡ç­¾çš„ä¾èµ–ï¼Œè§„æ¨¡åŒ–æ‰©å±•éš¾é¢˜ä»ç„¶å­˜åœ¨ã€‚æœ€è¿‘æ¢ç´¢å„ç§è‡ªæˆ‘å¥–åŠ±ä¿¡å·çš„æ›¿ä»£æ–¹æ¡ˆå±•ç¤ºäº†LLMæ¨ç†çš„æ¿€å‘æ½œåŠ›ï¼Œä½†å­˜åœ¨ä¸å¯å¿½è§†çš„å´©æºƒé—®é¢˜ã€‚å—è‡ªç›‘ç£å­¦ä¹ æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œCo-Rewardâ€çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¯­ä¹‰ç±»ä¼¼é—®é¢˜ä¹‹é—´çš„å¯¹æ¯”ä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±åŸºç¡€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æ„å»ºä¸€ä¸ªç±»ä¼¼çš„é—®é¢˜ï¼ˆæ— æ ‡ç­¾ï¼‰ï¼Œå¹¶é€šè¿‡ç®€å•çš„æ»šåŠ¨æŠ•ç¥¨åˆæˆå„è‡ªçš„æ›¿ä»£æ ‡ç­¾ï¼Œç„¶åé€šè¿‡äº¤å‰å¼•ç”¨æ¯å¯¹é—®é¢˜çš„æ ‡ç­¾æ¥æ„å»ºå¥–åŠ±ï¼Œä»¥åŠ å¼ºç±»ä¼¼è¾“å…¥ä¹‹é—´çš„å†…éƒ¨æ¨ç†ä¸€è‡´æ€§ã€‚ç›´è§‚åœ°çœ‹ï¼Œè¿™ç§è‡ªç›‘ç£çš„å¥–åŠ±å¡‘é€ æœºåˆ¶å¢åŠ äº†å­¦ä¹ å´©æºƒåˆ°ç®€å•è§£å†³æ–¹æ¡ˆçš„éš¾åº¦ï¼Œå¹¶é€šè¿‡æ‰©å¤§è¾“å…¥æ ·æœ¬å˜ä½“ä¿ƒè¿›äº†ç¨³å®šçš„æ¨ç†æ¿€å‘å’Œæå‡ã€‚ä»ç»éªŒä¸Šçœ‹ï¼ŒCo-Rewardåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’ŒLLMç³»åˆ—ä¸Šç›¸å¯¹äºå…¶ä»–è‡ªæˆ‘å¥–åŠ±åŸºçº¿å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³è¾¾åˆ°æˆ–è¶…è¶Šäº†åŸºäºçœŸå®æ ‡ç­¾çš„å¥–åŠ±ï¼Œåœ¨Llama-3.2-3B-Instructçš„MATH500ä»»åŠ¡ä¸Šç›¸å¯¹äºçœŸå®å¥–åŠ±æå‡äº†+6.8%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/tmlr-group/Co-Reward%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/tmlr-group/Co-Rewardä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00410v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´æ‰©å±•éš¾é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸Šä¾èµ–äººå·¥æ ‡æ³¨æ ‡ç­¾çš„é—®é¢˜ã€‚å½“å‰ä¸€äº›å°è¯•é‡‡ç”¨è‡ªæˆ‘å¥–åŠ±ä¿¡å·çš„æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºæ¿€å‘LLMæ¨ç†çš„æ½œåŠ›ï¼Œä½†åˆé¢ä¸´ä¸å¯å¿½è§†çš„å´©æºƒé—®é¢˜ã€‚å—è‡ªç›‘ç£å­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºâ€œCo-Rewardâ€çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¯­ä¹‰ç±»ä¼¼é—®é¢˜ä¹‹é—´çš„å¯¹æ¯”ä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±åŸºç¡€ã€‚é€šè¿‡æ„å»ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„ç›¸ä¼¼é—®é¢˜å¹¶åˆæˆå„è‡ªçš„ä»£ç†æ ‡ç­¾ï¼Œä»¥åŠé€šè¿‡äº¤å‰å¼•ç”¨æ¯å¯¹é—®é¢˜çš„æ ‡ç­¾æ¥æ„å»ºå¥–åŠ±ï¼Œå¼ºåŒ–å†…éƒ¨æ¨ç†çš„ä¸€è‡´æ€§ã€‚è¿™ç§è‡ªç›‘ç£çš„å¥–åŠ±æœºåˆ¶æé«˜äº†å­¦ä¹ å´©æºƒåˆ°ç®€å•è§£å†³æ–¹æ¡ˆçš„éš¾åº¦ï¼Œä¿ƒè¿›äº†ç¨³å®šçš„æ¨ç†æ¿€å‘å’Œæ”¹è¿›ï¼Œå¹¶é€šè¿‡æ‰©å±•è¾“å…¥æ ·æœ¬å˜ä½“æ¥å®ç°ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’ŒLLMç³»åˆ—ä¸Šï¼ŒCo-Rewardç›¸è¾ƒäºå…¶ä»–è‡ªæˆ‘å¥–åŠ±åŸºçº¿è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³è¾¾åˆ°æˆ–è¶…è¶Šäº†åŸºäºçœŸå®æ ‡ç­¾çš„å¥–åŠ±æ°´å¹³ï¼Œåœ¨Llama-3.2-3B-Instructä¸Šçš„MATH500ä»»åŠ¡ä¸Šç›¸è¾ƒäºçœŸå®å¥–åŠ±æå‡äº†+6.8%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/tmlr-group/Co-Reward%E3%80%82">https://github.com/tmlr-group/Co-Rewardã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æ‰©å±•éš¾é¢˜ã€‚</li>
<li>å½“å‰æ›¿ä»£æ–¹æ¡ˆå°è¯•é‡‡ç”¨è‡ªæˆ‘å¥–åŠ±ä¿¡å·æ¥æ¿€å‘LLMæ¨ç†æ½œåŠ›ï¼Œä½†é¢ä¸´æ¨¡å‹å´©æºƒé—®é¢˜ã€‚</li>
<li>æå‡ºåä¸ºâ€œCo-Rewardâ€çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æ¯”ä¸€è‡´æ€§ä½œä¸ºå¥–åŠ±åŸºç¡€ã€‚</li>
<li>é€šè¿‡æ„å»ºç›¸ä¼¼é—®é¢˜å¹¶åˆæˆä»£ç†æ ‡ç­¾ï¼Œä»¥åŠäº¤å‰å¼•ç”¨é—®é¢˜å¯¹æ¥æ„å»ºå¥–åŠ±ï¼Œå¼ºåŒ–å†…éƒ¨æ¨ç†ä¸€è‡´æ€§ã€‚</li>
<li>Co-Rewardé‡‡ç”¨è‡ªç›‘ç£å¥–åŠ±æœºåˆ¶ï¼Œæé«˜äº†å­¦ä¹ ç¨³å®šæ€§å¹¶å‡å°‘äº†æ¨¡å‹å´©æºƒé£é™©ã€‚</li>
<li>Co-Rewardåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒLLMç³»åˆ—ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç”šè‡³è¶…è¶ŠåŸºäºçœŸå®æ ‡ç­¾çš„å¥–åŠ±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4216ed72fd0c572bcd297bd7608065a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa608644a1a0f91b8838faa3948797ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1413f7a42fbc7ad408b3ac9eb14ecd40.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CoRGI-Verified-Chain-of-Thought-Reasoning-with-Visual-Grounding"><a href="#CoRGI-Verified-Chain-of-Thought-Reasoning-with-Visual-Grounding" class="headerlink" title="CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding"></a>CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding</h2><p><strong>Authors:Shixin Yi, Lin Shang</strong></p>
<p>Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in vision-language models (VLMs), but it often produces explanations that are linguistically fluent yet lack grounding in visual content. We observe that such hallucinations arise in part from the absence of an explicit verification mechanism during multi-step reasoning. To address this, we propose \textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with \textbf{G}rounded \textbf{I}nsights), a modular framework that introduces visual verification into the reasoning process. CoRGI follows a three-stage pipeline: it first generates a textual reasoning chain, then extracts supporting visual evidence for each reasoning step via a dedicated module (VEVM), and finally synthesizes the textual rationale with visual evidence to generate a grounded, verified answer. The framework can be integrated with existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR benchmark and find that it improves reasoning performance on two representative open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm the contribution of each step in the verification module, and human evaluations suggest that CoRGI leads to more factual and helpful explanations. We also examine alternative designs for the visual verification step and discuss potential limitations of post-hoc verification frameworks. These findings highlight the importance of grounding intermediate reasoning steps in visual evidence to enhance the robustness of multimodal reasoning. </p>
<blockquote>
<p>Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºåœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒäº§ç”Ÿçš„è§£é‡Šåœ¨è¯­è¨€å­¦ä¸Šå¾ˆæµç•…ï¼Œå´åœ¨è§†è§‰å†…å®¹ä¸Šç¼ºä¹ä¾æ®ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™ç§å¹»è§‰éƒ¨åˆ†æºäºå¤šæ­¥éª¤æ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹æ˜ç¡®çš„éªŒè¯æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>CoRGI</strong>ï¼ˆ<strong>C</strong>hain <strong>o</strong>f <strong>R</strong>easoning with <strong>G</strong>rounded <strong>I</strong>nsightsï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼•å…¥è§†è§‰éªŒè¯åˆ°æ¨ç†è¿‡ç¨‹ä¸­çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚CoRGIéµå¾ªä¸€ä¸ªä¸‰é˜¶æ®µçš„æµç¨‹ï¼šé¦–å…ˆç”Ÿæˆæ–‡æœ¬æ¨ç†é“¾ï¼Œç„¶åé€šè¿‡ä¸“ç”¨æ¨¡å—ï¼ˆVEVMï¼‰ä¸ºæ¯ä¸ªæ¨ç†æ­¥éª¤æå–æ”¯æŒæ€§çš„è§†è§‰è¯æ®ï¼Œæœ€åç»“åˆæ–‡æœ¬ç†æ€§å’Œè§†è§‰è¯æ®æ¥ç”Ÿæˆæœ‰ä¾æ®çš„ã€ç»è¿‡éªŒè¯çš„ç­”æ¡ˆã€‚è¯¥æ¡†æ¶å¯ä»¥ä¸ç°æœ‰çš„VLMsé›†æˆï¼Œè€Œæ— éœ€è¿›è¡Œç«¯åˆ°ç«¯çš„å†è®­ç»ƒã€‚æˆ‘ä»¬åœ¨VCRåŸºå‡†æµ‹è¯•ä¸Šå¯¹CoRGIè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å®ƒåœ¨ä¸¤ä¸ªä»£è¡¨æ€§çš„å¼€æºVLMä¸»å¹²ï¼ˆQwen-2.5VLå’ŒLLaVA-1.0ï¼‰ä¸Šæé«˜äº†æ¨ç†æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¯å®äº†éªŒè¯æ¨¡å—ä¸­æ¯ä¸€æ­¥çš„è´¡çŒ®ï¼Œäººç±»è¯„ä¼°è¡¨æ˜CoRGIäº§ç”Ÿçš„è§£é‡Šæ›´åŠ çœŸå®å’Œæœ‰å¸®åŠ©ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è§†è§‰éªŒè¯æ­¥éª¤çš„æ›¿ä»£è®¾è®¡ï¼Œå¹¶è®¨è®ºäº†äº‹åéªŒè¯æ¡†æ¶çš„æ½œåœ¨å±€é™æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å°†ä¸­é—´æ¨ç†æ­¥éª¤å»ºç«‹åœ¨è§†è§‰è¯æ®ä¸Šçš„é‡è¦æ€§ï¼Œä»¥æé«˜å¤šæ¨¡å¼æ¨ç†çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00378v1">PDF</a> Preparing for AAAI 2026, Multimodal Reasoning</p>
<p><strong>Summary</strong>ï¼šé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰æç¤ºåœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¾€å¾€äº§ç”Ÿçš„è§£é‡Šè¯­è¨€æµç•…å´ç¼ºä¹è§†è§‰å†…å®¹çš„æ”¯æ’‘ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºCoRGIæ¡†æ¶ï¼Œå¼•å…¥è§†è§‰éªŒè¯è¿›è¡Œæ¨ç†ã€‚CoRGIéµå¾ªä¸‰é˜¶æ®µæµç¨‹ï¼šç”Ÿæˆæ–‡æœ¬æ¨ç†é“¾ï¼Œé€šè¿‡ä¸“ç”¨æ¨¡å—æå–æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ”¯æŒè§†è§‰è¯æ®ï¼ˆVEVMï¼‰ï¼Œæœ€ååˆæˆæ–‡æœ¬ç†ç”±ä¸è§†è§‰è¯æ®ï¼Œç”Ÿæˆæœ‰æ”¯æ’‘çš„éªŒè¯ç­”æ¡ˆã€‚è¯¥æ¡†æ¶å¯æ•´åˆè‡³ç°æœ‰VLMsè€Œæ— éœ€ç«¯åˆ°ç«¯é‡è®­ã€‚åœ¨VCRåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCoRGIåœ¨ä¸¤ç§ä»£è¡¨æ€§å¼€æºVLMéª¨æ¶ä¸Šæå‡äº†æ¨ç†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºåœ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†ä¸­å±•ç°æ½œåŠ›ï¼Œä½†è§£é‡Šç¼ºä¹è§†è§‰å†…å®¹çš„æ”¯æ’‘ã€‚</li>
<li>å¼•å…¥è§†è§‰éªŒè¯æœºåˆ¶çš„CoRGIæ¡†æ¶æ—¨åœ¨è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>CoRGIéµå¾ªä¸‰é˜¶æ®µæµç¨‹ï¼šç”Ÿæˆæ–‡æœ¬æ¨ç†é“¾ï¼Œæå–è§†è§‰è¯æ®ï¼ŒåˆæˆéªŒè¯ç­”æ¡ˆã€‚</li>
<li>CoRGIå¯æ•´åˆè‡³ç°æœ‰VLMsï¼Œæ— éœ€ç«¯åˆ°ç«¯é‡è®­ã€‚</li>
<li>åœ¨VCRåŸºå‡†æµ‹è¯•ä¸Šï¼ŒCoRGIæé«˜äº†æ¨ç†æ€§èƒ½ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†éªŒè¯æ¨¡å—ä¸­æ¯ä¸€æ­¥çš„è´¡çŒ®ã€‚</li>
<li>äººç±»è¯„ä¼°è¡¨æ˜ï¼ŒCoRGIäº§ç”Ÿçš„è§£é‡Šæ›´çœŸå®ã€æ›´æœ‰å¸®åŠ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5b8129ead941032201a7ec65dac2d85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96a0f1ca5e05538e681e84e2c8cbd3c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57cab7fd004c9b3fb4e8e465445c9da3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45001917badddba4c23a386f52e51f4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-828895ec4792654edd1f4c90884c6032.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PilotRL-Training-Language-Model-Agents-via-Global-Planning-Guided-Progressive-Reinforcement-Learning"><a href="#PilotRL-Training-Language-Model-Agents-via-Global-Planning-Guided-Progressive-Reinforcement-Learning" class="headerlink" title="PilotRL: Training Language Model Agents via Global Planning-Guided   Progressive Reinforcement Learning"></a>PilotRL: Training Language Model Agents via Global Planning-Guided   Progressive Reinforcement Learning</h2><p><strong>Authors:Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang</strong></p>
<p>Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the modelâ€™s ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the modelâ€™s planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å‘ä»£ç†çš„ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨åŸºäºä»£ç†çš„ç¯å¢ƒä¸­éƒ¨ç½²LLMsæ—¶ï¼Œç°æœ‰å·¥ä½œé¢ä¸´æŒ‘æˆ˜ã€‚å¹¿æ³›é‡‡ç”¨çš„ReActä»£ç†èŒƒå¼ä¾§é‡äºå°†å•æ­¥æ¨ç†ä¸å³æ—¶è¡ŒåŠ¨æ‰§è¡Œç›¸ç»“åˆï¼Œè¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦é•¿æœŸæˆ˜ç•¥è§„åˆ’çš„å¤æ‚ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œåœ¨é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ï¼Œè§„åˆ’è€…å’Œæ‰§è¡Œè€…ä¹‹é—´çš„åè°ƒä¹Ÿæ˜¯ä»£ç†è®¾è®¡éœ€è¦è€ƒè™‘çš„å…³é”®å› ç´ ã€‚å¦å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒï¼Œè¿™å¾€å¾€å¯¼è‡´æ¨¡å‹è®°å¿†å·²å»ºç«‹çš„ä»»åŠ¡å®Œæˆè½¨è¿¹ï¼Œä»è€Œåœ¨é¢å¯¹æ–°çš„é—®é¢˜ä¸Šä¸‹æ–‡æ—¶é™åˆ¶å…¶æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè‡ªé€‚åº”å…¨å±€è®¡åˆ’çš„ä»£ç†èŒƒå¼AdaPlanï¼Œæ—¨åœ¨å°†é«˜çº§æ˜¾å¼æŒ‡å¯¼ä¸æ‰§è¡Œç›¸ç»“åˆï¼Œä»¥æ”¯æŒæœ‰æ•ˆçš„é•¿æœŸå†³ç­–ã€‚åŸºäºè¿™ä¸€èŒƒå¼ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†PilotRLï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é€æ­¥å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„LLMä»£ç†å…¨å±€è§„åˆ’å¼•å¯¼è®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆå‘å±•æ¨¡å‹åœ¨è§£å†³ä»£ç†ä»»åŠ¡æ—¶éµå¾ªå…¨å±€è®¡åˆ’ä¸­çš„æ˜¾å¼æŒ‡å¯¼çš„èƒ½åŠ›ã€‚ç„¶ååœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¼˜åŒ–ç”Ÿæˆçš„è®¡åˆ’çš„è´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„è§„åˆ’å’Œæ‰§è¡Œåè°ƒè¿›è¡Œäº†è”åˆä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒPilotRLå¯ä»¥è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒLLaMA3.1-8B-Instruct + PilotRLåœ¨å°é—­æºç GPT-4oä¸Šè¶…è¶Šäº†3.60%ï¼Œåœ¨å‚æ•°è§„æ¨¡ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºGPT-4o-miniæ˜¾ç¤ºå‡ºæ›´å¤§çš„æå‡ï¼Œè¾¾åˆ°äº†55.78%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00344v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é¢å‘ä»£ç†çš„ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä»£ç†ç¯å¢ƒä¸­éƒ¨ç½²æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ•´åˆå³æ—¶è¡ŒåŠ¨æ‰§è¡Œçš„å•æ­¥æ¨ç†ä¸Šï¼Œè¿™é™åˆ¶äº†å…¶åœ¨éœ€è¦é•¿æœŸæˆ˜ç•¥è§„åˆ’çš„å¤æ‚ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä»£ç†è®¾è®¡ä¸­è§£å†³é—®é¢˜çš„è§„åˆ’è€…å’Œæ‰§è¡Œè€…ä¹‹é—´çš„åè°ƒä¹Ÿæ˜¯å…³é”®è€ƒé‡å› ç´ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†è‡ªé€‚åº”å…¨å±€è®¡åˆ’ä»£ç†èŒƒå¼AdaPlanï¼Œæ—¨åœ¨ååŒé«˜çº§æ˜¾å¼æŒ‡å¯¼ä¸æ‰§è¡Œï¼Œæ”¯æŒæœ‰æ•ˆçš„é•¿æœŸå†³ç­–ã€‚åŸºäºè¯¥èŒƒå¼ï¼Œè¿›ä¸€æ­¥æå‡ºäº†PilotRLï¼Œä¸€ä¸ªç”±æ¸è¿›å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„LLMä»£ç†å…¨å±€è§„åˆ’å¼•å¯¼è®­ç»ƒæ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒPilotRLèƒ½å¤Ÿè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒLLaMA3.1-8B-Instruct + PilotRLè¶…è¶Šé—­æºGPT-4o 3.60%ï¼Œåœ¨å‚æ•°è§„æ¨¡ç›¸è¿‘çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºGPT-4o-miniæœ‰æ›´å¤§çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é¢å‘ä»£ç†çš„ä»»åŠ¡æ–¹é¢å·²å±•ç°å‡ºæ˜¾è‘—è¿›å±•ã€‚</li>
<li>åœ¨å¤æ‚çš„éœ€è¦é•¿æœŸæˆ˜ç•¥è§„åˆ’çš„ä»»åŠ¡ä¸­ï¼Œç°æœ‰çš„ä»£ç†æ¨¡å‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>AdaPlanä»£ç†èŒƒå¼æ—¨åœ¨ååŒé«˜çº§æ˜¾å¼æŒ‡å¯¼ä¸æ‰§è¡Œï¼Œä»¥æ”¯æŒé•¿æœŸå†³ç­–ã€‚</li>
<li>PilotRLæ˜¯ä¸€ä¸ªåŸºäºå…¨å±€è§„åˆ’çš„LLMè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ¸è¿›å¼ºåŒ–å­¦ä¹ é©±åŠ¨ã€‚</li>
<li>PilotRLé€šè¿‡ç»“åˆæ˜¾å¼æŒ‡å¯¼å’Œè®¡åˆ’ä¼˜åŒ–æ¥æå‡ä»£ç†çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜PilotRLèƒ½å¤Ÿè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œè¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4eb7734bb824648fa5f88b4a386ba011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50203b192eb29e5275877d3043a67cdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-925faf6ae0f74b8279f9208cd3d13dd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2804ac475cb2f0776b65ee4350c764.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f581113b533344f2160bfd230b49e1dd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Oedipus-and-the-Sphinx-Benchmarking-and-Improving-Visual-Language-Models-for-Complex-Graphic-Reasoning"><a href="#Oedipus-and-the-Sphinx-Benchmarking-and-Improving-Visual-Language-Models-for-Complex-Graphic-Reasoning" class="headerlink" title="Oedipus and the Sphinx: Benchmarking and Improving Visual Language   Models for Complex Graphic Reasoning"></a>Oedipus and the Sphinx: Benchmarking and Improving Visual Language   Models for Complex Graphic Reasoning</h2><p><strong>Authors:Jianyi Zhang, Xu Ji, Ziyin Zhou, Yuchen Zhou, Shubo Shi, Haoyu Wu, Zhen Li, Shizhao Liu</strong></p>
<p>Evaluating the performance of visual language models (VLMs) in graphic reasoning tasks has become an important research topic. However, VLMs still show obvious deficiencies in simulating human-level graphic reasoning capabilities, especially in complex graphic reasoning and abstract problem solving, which are less studied and existing studies only focus on simple graphics. To evaluate the performance of VLMs in complex graphic reasoning, we propose ReasonBench, the first evaluation benchmark focused on structured graphic reasoning tasks, which includes 1,613 questions from real-world intelligence tests. ReasonBench covers reasoning dimensions related to location, attribute, quantity, and multi-element tasks, providing a comprehensive evaluation of the performance of VLMs in spatial, relational, and abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including closed-source and open-source models) and reveal significant limitations of current models. Based on these findings, we propose a dual optimization strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training, all of which improves VLM performance by 33.5%. All experimental data and code are in the repository: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/cistine/ReasonBench">https://huggingface.co/datasets/cistine/ReasonBench</a>. </p>
<blockquote>
<p>è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾å½¢æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ã€‚ç„¶è€Œï¼ŒVLMsåœ¨æ¨¡æ‹Ÿäººç±»æ°´å¹³çš„å›¾å½¢æ¨ç†èƒ½åŠ›æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å›¾å½¢æ¨ç†å’ŒæŠ½è±¡é—®é¢˜è§£å†³æ–¹é¢ã€‚è¿™äº›æ–¹é¢çš„ç›¸å…³ç ”ç©¶è¾ƒå°‘ï¼Œç°æœ‰ç ”ç©¶åªå…³æ³¨ç®€å•çš„å›¾å½¢ã€‚ä¸ºäº†è¯„ä¼°VLMsåœ¨å¤æ‚å›¾å½¢æ¨ç†ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ReasonBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºç»“æ„åŒ–å›¾å½¢æ¨ç†ä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬1613ä¸ªæ¥è‡ªç°å®ä¸–ç•Œæ™ºåŠ›æµ‹è¯•çš„é—®é¢˜ã€‚ReasonBenchæ¶µç›–äº†ä¸ä½ç½®ã€å±æ€§ã€æ•°é‡å’Œå¤šå…ƒä»»åŠ¡ç›¸å…³çš„æ¨ç†ç»´åº¦ï¼Œå…¨é¢è¯„ä¼°äº†VLMsåœ¨ç©ºé—´ã€å…³ç³»å’ŒæŠ½è±¡æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹11ç§ä¸»æµçš„VLMsï¼ˆåŒ…æ‹¬å°é—­æºä»£ç å’Œå¼€æºæ¨¡å‹ï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ­ç¤ºäº†å½“å‰æ¨¡å‹çš„é‡è¦å±€é™æ€§ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†åŒé‡ä¼˜åŒ–ç­–ç•¥ï¼šDiagrammatic Reasoning Chainï¼ˆDiaCoTï¼‰é€šè¿‡åˆ†è§£å±‚æ¬¡æé«˜æ¨ç†çš„å¯è§£é‡Šæ€§ï¼ŒReasonTuneé€šè¿‡è®­ç»ƒæé«˜æ¨¡å‹æ¨ç†çš„ä»»åŠ¡é€‚åº”æ€§ï¼Œè¿™ä¸¤è€…å…±åŒæé«˜äº†VLMçš„æ€§èƒ½ï¼Œè¾¾åˆ°33.5%çš„æå‡ã€‚æ‰€æœ‰å®éªŒæ•°æ®å’Œä»£ç å¯åœ¨ä»¥ä¸‹ä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/cistine/ReasonBench">https://huggingface.co/datasets/cistine/ReasonBench</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00323v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾å½¢æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½è¯„ä¼°å·²æˆä¸ºé‡è¦ç ”ç©¶è¯é¢˜ã€‚ç„¶è€Œï¼ŒVLMsåœ¨æ¨¡æ‹Ÿäººç±»çº§åˆ«çš„å›¾å½¢æ¨ç†èƒ½åŠ›æ–¹é¢ä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å›¾å½¢æ¨ç†å’ŒæŠ½è±¡é—®é¢˜è§£å†³æ–¹é¢ç ”ç©¶è¾ƒå°‘ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ReasonBenchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“æ³¨äºç»“æ„åŒ–å›¾å½¢æ¨ç†ä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬1613ä¸ªæ¥è‡ªç°å®ä¸–ç•Œæ™ºåŠ›æµ‹è¯•çš„é—®é¢˜ã€‚ReasonBenchæ¶µç›–äº†ä¸ä½ç½®ã€å±æ€§ã€æ•°é‡å’Œå¤šå…ƒä»»åŠ¡ç›¸å…³çš„æ¨ç†ç»´åº¦ï¼Œå…¨é¢è¯„ä¼°äº†VLMsåœ¨ç©ºé—´ã€å…³ç³»å’ŒæŠ½è±¡æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚ä½œè€…å¯¹11ç§ä¸»æµVLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æå‡ºäº†åŒé‡ä¼˜åŒ–ç­–ç•¥ï¼šDiagrammatic Reasoning Chainï¼ˆDiaCoTï¼‰é€šè¿‡åˆ†è§£å±‚æ¬¡å¢å¼ºæ¨ç†çš„è§£è¯»æ€§ï¼ŒReasonTuneé€šè¿‡è®­ç»ƒå¢å¼ºæ¨¡å‹æ¨ç†çš„ä»»åŠ¡é€‚åº”æ€§ï¼Œæ•´ä½“æé«˜äº†VLMsçš„æ€§èƒ½33.5%ã€‚ç›¸å…³å®éªŒæ•°æ®å’Œä»£ç å·²å…¬å¼€åœ¨huggingface.co&#x2F;datasets&#x2F;cistine&#x2F;ReasonBenchã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>VLMsåœ¨æ¨¡æ‹Ÿäººç±»çº§åˆ«çš„å›¾å½¢æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œå°¤å…¶åœ¨å¤æ‚å›¾å½¢æ¨ç†å’ŒæŠ½è±¡é—®é¢˜è§£å†³æ–¹é¢ç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>æå‡ºäº†ReasonBenchï¼Œå®ƒæ˜¯é¦–ä¸ªä¸“æ³¨äºç»“æ„åŒ–å›¾å½¢æ¨ç†ä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°VLMsçš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>ReasonBenchè¦†ç›–äº†å¤šç§å›¾å½¢æ¨ç†ç»´åº¦ï¼Œå¦‚ä½ç½®ã€å±æ€§ã€æ•°é‡å’Œå¤šå…ƒä»»åŠ¡ã€‚</li>
<li>å¯¹11ç§ä¸»æµVLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°äº†ç°æœ‰æ¨¡å‹çš„é‡è¦å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†åŒé‡ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬Diagrammatic Reasoning Chainï¼ˆDiaCoTï¼‰å’ŒReasonTuneï¼Œæé«˜äº†VLMsçš„æ€§èƒ½ã€‚</li>
<li>DiaCoTé€šè¿‡åˆ†è§£å±‚æ¬¡å¢å¼ºæ¨ç†çš„è§£è¯»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7634108bf82d6f6e65d792423876b8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f40647a988f952f8e8d9c56c7fc78132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-748c56c906c3968022f1cd5586337e5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cca7abca730e9fca0d1bee275f5a38f0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization"><a href="#RL-PLUS-Countering-Capability-Boundary-Collapse-of-LLMs-in-Reinforcement-Learning-with-Hybrid-policy-Optimization" class="headerlink" title="RL-PLUS: Countering Capability Boundary Collapse of LLMs in   Reinforcement Learning with Hybrid-policy Optimization"></a>RL-PLUS: Countering Capability Boundary Collapse of LLMs in   Reinforcement Learning with Hybrid-policy Optimization</h2><p><strong>Authors:Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, Ge Li</strong></p>
<p>Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its inherently on-policy strategy with LLMâ€™s immense action space and sparse reward. Further, RLVR can lead to the capability boundary collapse, narrowing the LLMâ€™s problem-solving scope. To address this problem, we propose RL-PLUS, a novel approach that synergizes internal exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components: Multiple Importance Sampling to address for distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. The results show that RL-PLUS achieves state-of-the-art performance compared with existing RLVR methods on six math reasoning benchmarks and exhibits superior performance on six out-of-distribution reasoning tasks. It also achieves consistent and significant gains across diverse model families, with average relative improvements ranging from 21.1% to 69.2%. Moreover, Pass@k curves across multiple benchmarks indicate that RL-PLUS effectively resolves the capability boundary collapse problem. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºå…¶å›ºæœ‰çš„åœ¨çº¿ç­–ç•¥ã€LLMçš„å·¨å¤§åŠ¨ä½œç©ºé—´å’Œç¨€ç–å¥–åŠ±ï¼ŒRLVRåœ¨çªç ´åŸºç¡€LLMçš„å†…åœ¨èƒ½åŠ›è¾¹ç•Œæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æ­¤å¤–ï¼ŒRLVRå¯èƒ½å¯¼è‡´èƒ½åŠ›è¾¹ç•Œå´©æºƒï¼Œç¼©å°LLMçš„è§£å†³é—®é¢˜èŒƒå›´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RL-PLUSï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒå°†å†…éƒ¨å‰¥å‰Šï¼ˆå³æ€è€ƒï¼‰ä¸å¤–éƒ¨æ•°æ®ï¼ˆå³å­¦ä¹ ï¼‰ååŒèµ·æ¥ï¼Œä»¥å®ç°æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å¹¶è¶…è¶ŠåŸºç¡€æ¨¡å‹çš„è¾¹ç•Œã€‚RL-PLUSé›†æˆäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šé‡é‡è¦æ€§é‡‡æ ·ï¼Œä»¥è§£å†³å¤–éƒ¨æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ï¼›åŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå¼•å¯¼æ¨¡å‹èµ°å‘é«˜ä»·å€¼ã€æœªæ¢ç´¢çš„æ¨ç†è·¯å¾„ã€‚æˆ‘ä»¬æä¾›ç†è®ºåˆ†æå’Œå¤§é‡å®éªŒæ¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒRL-PLUSåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸ç°æœ‰RLVRæ–¹æ³•ç›¸æ¯”çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å…­ä¸ªç¦»åˆ†å¸ƒæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä¸åŒçš„æ¨¡å‹å®¶æ—ä¸­å®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ”¶ç›Šï¼Œå¹³å‡ç›¸å¯¹æ”¹è¿›èŒƒå›´ä»21.1%åˆ°69.2%ã€‚è€Œä¸”ï¼Œå¤šä¸ªåŸºå‡†æµ‹è¯•çš„Pass@kæ›²çº¿è¡¨æ˜ï¼ŒRL-PLUSæœ‰æ•ˆåœ°è§£å†³äº†èƒ½åŠ›è¾¹ç•Œå´©æºƒé—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00222v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLVRéš¾ä»¥çªç ´åŸºç¡€LLMçš„å†…åœ¨èƒ½åŠ›è¾¹ç•Œï¼Œä¸”å¯èƒ½å¯¼è‡´èƒ½åŠ›è¾¹ç•Œå´©æºƒã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RL-PLUSè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡å†…éƒ¨åˆ©ç”¨ï¼ˆå³æ€è€ƒï¼‰ä¸å¤–éƒ¨æ•°æ®ï¼ˆå³å­¦ä¹ ï¼‰çš„ååŒä½œç”¨ï¼Œå¢å¼ºäº†æ¨ç†èƒ½åŠ›å¹¶è¶…è¶Šäº†åŸºç¡€æ¨¡å‹çš„è¾¹ç•Œã€‚RL-PLUSåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šé‡é‡è¦æ€§é‡‡æ ·ä»¥è§£å†³å¤–éƒ¨æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ï¼Œä»¥åŠåŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°æ¥å¼•å¯¼æ¨¡å‹èµ°å‘é«˜ä»·å€¼ã€æœªè¢«æ¢ç´¢çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRL-PLUSåœ¨å…­ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å…­ä¸ªè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®ƒåœ¨ä¸åŒçš„æ¨¡å‹å®¶æ—ä¸­å®ç°äº†æŒç»­ä¸”æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹³å‡ç›¸å¯¹æ”¹è¿›èŒƒå›´ä»21.1%åˆ°69.2%ã€‚æ­¤å¤–ï¼ŒPass@kæ›²çº¿è¡¨æ˜RL-PLUSæœ‰æ•ˆåœ°è§£å†³äº†èƒ½åŠ›è¾¹ç•Œå´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæé«˜äº†LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨çªç ´èƒ½åŠ›è¾¹ç•Œå’Œå¯èƒ½å¯¼è‡´èƒ½åŠ›è¾¹ç•Œå´©æºƒçš„é—®é¢˜ã€‚</li>
<li>RL-PLUSé€šè¿‡å†…éƒ¨åˆ©ç”¨ä¸å¤–éƒ¨æ•°æ®çš„ååŒä½œç”¨ï¼Œå¢å¼ºäº†æ¨ç†èƒ½åŠ›å¹¶è¶…è¶Šäº†åŸºç¡€æ¨¡å‹çš„è¾¹ç•Œã€‚</li>
<li>RL-PLUSåŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šé‡é‡è¦æ€§é‡‡æ ·å’ŒåŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ã€‚</li>
<li>RL-PLUSåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>RL-PLUSåœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹³å‡ç›¸å¯¹æ”¹è¿›èŒƒå›´è¾ƒå¤§ã€‚</li>
<li>RL-PLUSé€šè¿‡è§£å†³åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜å’Œå¼•å¯¼æ¨¡å‹æ¢ç´¢é«˜ä»·å€¼è·¯å¾„ï¼Œå®ç°äº†å¯¹RLVRæ–¹æ³•çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4672d73db73965f97ab347175b05f2e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd23685a6f250e8e1fdc0fbe6a577635.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving"><a href="#Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving" class="headerlink" title="Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"></a>Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</h2><p><strong>Authors:Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu</strong></p>
<p>LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œå®ƒä»¬é€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œé•¿é“¾æ€ç»´æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œå½“ä»…ä½¿ç”¨è‡ªç„¶è¯­è¨€æ—¶ï¼Œç”±äºç¼ºä¹æ˜ç¡®çš„ç›‘ç£ä¿¡å·ï¼Œå®ƒä»¬åœ¨å®šç†è¯æ˜æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚ä¸“ç”¨é¢†åŸŸè¯­è¨€ï¼ˆå¦‚Leanï¼‰é€šè¿‡è¯æ˜çš„å½¢å¼éªŒè¯æä¾›æ˜ç¡®ç›‘ç£ï¼Œä»è€Œèƒ½å¤Ÿé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{Seed-Prover}ï¼Œä¸€ä¸ªä»¥å¼•ç†é£æ ¼è¿›è¡Œçš„å…¨ç¨‹è¯æ˜æ¨ç†æ¨¡å‹ã€‚Seed-Proverå¯ä»¥åŸºäºLeanåé¦ˆã€å·²è¯æ˜çš„å¼•ç†å’Œè‡ªæˆ‘æ€»ç»“æ¥è¿­ä»£åœ°å®Œå–„å…¶è¯æ˜ã€‚ä¸ºè§£å†³IMOçº§åˆ«çš„ç«èµ›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ï¼Œä»¥å®ç°æ·±åº¦å’Œå¹¿åº¦æ¨ç†ã€‚Seed-Proverè¯æ˜äº†78.1%çš„æ­£å¼åŒ–è¿‡å»IMOé—®é¢˜ï¼Œåœ¨MiniF2Fä¸­è¾¾åˆ°é¥±å’ŒçŠ¶æ€ï¼Œå¹¶åœ¨PutnamBenchä¸Šè¾¾åˆ°50%ä»¥ä¸Šï¼Œå¤§å¤§è¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚ä¸ºäº†è§£å†³Leanä¸­ç¼ºä¹å‡ ä½•æ”¯æŒçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•æ¨ç†å¼•æ“\textbf{Seed-Geometry}ï¼Œå®ƒçš„æ€§èƒ½è¶…è¿‡äº†ä¹‹å‰çš„æ­£å¼å‡ ä½•å¼•æ“ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸¤ä¸ªç³»ç»Ÿå‚åŠ äº†2025å¹´IMOï¼Œå¹¶å®Œå…¨è¯æ˜äº†6ä¸ªé—®é¢˜çš„5ä¸ªã€‚è¿™é¡¹å·¥ä½œåœ¨è‡ªåŠ¨åŒ–æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå±•ç¤ºäº†å½¢å¼éªŒè¯ä¸é•¿é“¾æ€ç»´æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23726v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸é•¿é“¾æ€ç»´å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å®šç†è¯æ˜æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºSeed-Proverçš„åŸºäºè±æ©ï¼ˆLeanï¼‰çš„æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡åé¦ˆã€å·²è¯æ˜çš„å¼•ç†å’Œè‡ªæˆ‘æ€»ç»“æ¥è¿­ä»£å®Œå–„è¯æ˜ã€‚ä¸ºè§£å†³IMOçº§åˆ«çš„ç«èµ›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ï¼Œå®ç°äº†æ·±åº¦ä¸å¹¿åº¦æ¨ç†ã€‚Seed-Proveræ¨¡å‹èƒ½å¤Ÿè¯æ˜å½¢å¼åŒ–IMOé—®é¢˜çš„78.1%ï¼Œé¥±å’ŒMiniF2Fï¼Œå¹¶åœ¨PutnamBenchä¸Šå–å¾—äº†è¶…è¿‡ä¸€åŠçš„å‡†ç¡®ç‡ï¼Œå¤§å¤§è¶…è¿‡äº†ä¹‹å‰çš„æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³è±æ©ä¸­ç¼ºä¹å‡ ä½•æ”¯æŒçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Seed-Geometryå‡ ä½•æ¨ç†å¼•æ“ï¼Œå…¶æ€§èƒ½ä¼˜äºä¹‹å‰çš„æ­£å¼å‡ ä½•å¼•æ“ã€‚ä¸¤ä¸ªç³»ç»Ÿå…±åŒå‚ä¸äº†IMO 2025çš„ç«èµ›ï¼ŒæˆåŠŸè¯æ˜äº†äº”ä¸ªé—®é¢˜ä¸­çš„å…­ä¸ªã€‚è¿™é¡¹å·¥ä½œæ ‡å¿—ç€è‡ªåŠ¨åŒ–æ•°å­¦æ¨ç†çš„é‡å¤§è¿›å±•ï¼Œè¯æ˜äº†å½¢å¼éªŒè¯ä¸é•¿é“¾æ€ç»´æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´å®šç†è¯æ˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹æ¸…æ™°çš„ç›‘ç£ä¿¡å·æ˜¯LLMsåœ¨å®šç†è¯æ˜ä¸­é‡åˆ°å›°éš¾çš„åŸå› ä¹‹ä¸€ã€‚</li>
<li>Seed-Proveræ¨¡å‹é€šè¿‡è¿­ä»£å®Œå–„è¯æ˜ï¼Œç»“åˆäº†åé¦ˆã€å·²è¯æ˜çš„å¼•ç†å’Œè‡ªæˆ‘æ€»ç»“ã€‚</li>
<li>Seed-Proveræ¨¡å‹åœ¨IMOçº§åˆ«é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜ç‡è¶…è¿‡78%ã€‚</li>
<li>Seed-Geometryå‡ ä½•æ¨ç†å¼•æ“è§£å†³äº†è±æ©ä¸­çš„å‡ ä½•æ”¯æŒç¼ºå¤±é—®é¢˜ã€‚</li>
<li>ä¸¤ä¸ªç³»ç»Ÿå…±åŒå‚ä¸äº†IMO 2025ç«èµ›ï¼ŒæˆåŠŸè¯æ˜å¤šæ•°é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6effd03b81fb372233537390b625c79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e8463acf5157b2cd2ef3d412d356fc8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-298533a0aaf0b998c716de78d8c29509.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0199f1b029fa0eb1dc8e8aff3dcddea.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ControlMed-Adding-Reasoning-Control-to-Medical-Language-Model"><a href="#ControlMed-Adding-Reasoning-Control-to-Medical-Language-Model" class="headerlink" title="ControlMed: Adding Reasoning Control to Medical Language Model"></a>ControlMed: Adding Reasoning Control to Medical Language Model</h2><p><strong>Authors:Sung-Min Lee, Siyoon Lee, Juyeon Kim, Kyoungmin Roh</strong></p>
<p>Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis. </p>
<blockquote>
<p>åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¯¹å¢å¼ºå‡†ç¡®åº¦å’Œè§£é‡Šæ€§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†çš„éœ€æ±‚è¶Šæ¥è¶Šå¤§ï¼Œå› ä¸ºä¸´åºŠå†³ç­–çš„ç”Ÿå‘½å…³é”®æ€§è´¨éœ€è¦å¯é çš„æ”¯æŒã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œç°æœ‰çš„æ¨ç†LLMsç»å¸¸äº§ç”Ÿä¸å¿…è¦çš„å†—é•¿æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…å’Œå“åº”å»¶è¿Ÿã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­çš„å®é™…éƒ¨ç½²ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>ControlMed</strong>ï¼Œè¿™æ˜¯ä¸€ç§åŒ»å­¦è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç²¾ç»†çš„æ§åˆ¶æ ‡è®°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æ¨ç†æ—¶ä¸»åŠ¨æ§åˆ¶æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚ControlMedé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼š1ï¼‰åœ¨æ¶µç›–ç›´æ¥å’Œæ¨ç†å“åº”çš„å¤§è§„æ¨¡åˆæˆåŒ»ç–—æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›2ï¼‰ä½¿ç”¨å¤šé•¿åº¦æ¨ç†æ•°æ®å’Œæ˜ç¡®çš„é•¿åº¦æ§åˆ¶æ ‡è®°è¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼›3ï¼‰ä½¿ç”¨æ¨¡å‹åŸºç¡€çš„å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚åœ¨å¤šç§è‹±è¯­å’ŒéŸ©è¯­åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦çµæ´»å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œé€šè¿‡æ§åˆ¶æ¨ç†é•¿åº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒControlMedæ˜¯ä¸´åºŠé—®ç­”å’ŒåŒ»ç–—ä¿¡æ¯åˆ†æçš„å®é™…å¯è¡Œå’Œé€‚åº”æ€§å¼ºçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22545v2">PDF</a> 13 pages</p>
<p><strong>Summary</strong><br>åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ—¥ç›Šå—åˆ°é‡è§†ï¼Œå¹¶å‡ºç°äº†å…·æœ‰æ›´é«˜å‡†ç¡®æ€§å’Œè§£é‡Šæ€§çš„å¢å¼ºæ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¾€å¾€äº§ç”Ÿå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å’Œå“åº”å»¶è¿Ÿè¾ƒå¤§ï¼Œé™åˆ¶äº†åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºControlMedåŒ»å­¦è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç²¾ç»†çš„æ§åˆ¶æ ‡è®°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æ¨ç†æ—¶æ§åˆ¶æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚ControlMedé€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“å®ç°ï¼š1ï¼‰åœ¨æ¶µç›–ç›´æ¥å’Œæ¨ç†å“åº”çš„å¤§è§„æ¨¡åˆæˆåŒ»ç–—æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼›2ï¼‰ä½¿ç”¨å¤šé•¿åº¦æ¨ç†æ•°æ®å’Œæ˜ç¡®çš„é•¿åº¦æ§åˆ¶æ ‡è®°è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼›3ï¼‰ä½¿ç”¨æ¨¡å‹åŸºç¡€å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚åœ¨å¤šç§è‹±è¯­å’ŒéŸ©è¯­åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦çµæ´»åœ°å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™è¡¨æ˜ControlMedæ˜¯ä¸´åºŠé—®ç­”å’ŒåŒ»ç–—ä¿¡æ¯åˆ†æçš„å®ç”¨ä¸”å¯é€‚åº”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å¾—åˆ°é‡è§†ï¼Œå¢å¼ºæ¨¡å‹å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œè§£é‡Šæ€§ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å­˜åœ¨å†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—å¼€é”€å’Œå“åº”å»¶è¿Ÿå¤§ï¼Œå½±å“å®é™…éƒ¨ç½²ã€‚</li>
<li>ControlMedåŒ»å­¦è¯­è¨€æ¨¡å‹é€šè¿‡ç²¾ç»†æ§åˆ¶æ ‡è®°å®ç°æ¨ç†è¿‡ç¨‹é•¿åº¦çš„çµæ´»æ§åˆ¶ã€‚</li>
<li>ControlMedé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒControlMedæ¨¡å‹æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¶Šæœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
<li>ç”¨æˆ·å¯çµæ´»å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä»¥æ»¡è¶³ä¸åŒéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa98a6eb06cbd524dc95ac981bcb347a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5e9c5ce514c320e99de9d6c49f2d7e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1812f1cd2be8cc272305a69f116e35e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Time-RA-Towards-Time-Series-Reasoning-for-Anomaly-with-LLM-Feedback"><a href="#Time-RA-Towards-Time-Series-Reasoning-for-Anomaly-with-LLM-Feedback" class="headerlink" title="Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback"></a>Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback</h2><p><strong>Authors:Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen</strong></p>
<p>Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning. The code (<a target="_blank" rel="noopener" href="https://github.com/yyysjz1997/Time-RA">https://github.com/yyysjz1997/Time-RA</a>) and dataset (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Time-RA/RATs40K">https://huggingface.co/datasets/Time-RA/RATs40K</a>) have been fully open-sourced to support and accelerate future research in this area. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹åœ¨å„ä¸ªé¢†åŸŸéƒ½è‡³å…³é‡è¦ï¼Œç„¶è€Œå½“å‰çš„æ–¹æ³•å¾€å¾€ä»…é™äºç®€å•çš„äºŒå…ƒå¼‚å¸¸åˆ†ç±»ï¼Œç¼ºä¹è¯¦ç»†çš„åˆ†ç±»å’Œè¿›ä¸€æ­¥çš„è§£é‡Šæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œå³æ—¶é—´åºåˆ—å¼‚å¸¸æ¨ç†ï¼ˆTime-RAï¼‰ï¼Œå®ƒå°†ç»å…¸çš„æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä»åˆ¤åˆ«å¼ä»»åŠ¡è½¬å˜ä¸ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼ã€æ¨ç†å¯†é›†å‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†é¦–ä¸ªé¢å‘å¼‚å¸¸æ¨ç†çš„ç°å®ä¸–ç•Œå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†RATs40Kï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§çº¦40,000ä¸ªè·¨10ä¸ªç°å®é¢†åŸŸçš„æ ·æœ¬ï¼Œæ˜ç¡®æ ‡æ³¨äº†å¼‚å¸¸æ¨ç†ã€‚æ¯ä¸ªæ ·æœ¬åŒ…æ‹¬æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®ã€ä¸Šä¸‹æ–‡æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰è¡¨ç¤ºï¼Œæ¯ç§æ•°æ®éƒ½æ ‡æ³¨äº†ç²¾ç»†çš„ç±»åˆ«ï¼ˆå•å˜é‡å¼‚å¸¸14ç±»ï¼Œå¤šå…ƒå¼‚å¸¸6ç±»ï¼‰å’Œç»“æ„åŒ–è§£é‡Šæ¨ç†ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤æ‚çš„æ³¨é‡Šæ¡†æ¶ï¼Œåˆ©ç”¨ç¾¤ä½“ç”Ÿæˆçš„æ ‡ç­¾é€šè¿‡GPT-4é©±åŠ¨çš„åé¦ˆè¿›è¡Œä¿®æ­£ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚å¯¹LLMå’Œå¤šæ¨¡æ€LLMçš„å¹¿æ³›åŸºå‡†æµ‹è¯•å±•ç¤ºäº†å½“å‰æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ï¼Œçªæ˜¾äº†ç›‘ç£å¾®è°ƒçš„é‡è¦ä½œç”¨ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸ºå¯è§£é‡Šæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†çš„æ˜¾è‘—è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚ä»£ç ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/yyysjz1997/Time-RA%EF%BC%89%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88https://huggingface.co/datasets/Time-RA/RATs40K%EF%BC%89%E5%B7%B2%E5%AE%8C%E5%85%A8%E5%BC%80%E6%BA%90%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E5%92%8C%E5%8A%A0%E9%80%9F%E8%AF%A5%E9%A2%86%E5%9F%9F%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/yyysjz1997/Time-RAï¼‰å’Œæ•°æ®é›†ï¼ˆhttps://huggingface.co/datasets/Time-RA/RATs40Kï¼‰å·²å®Œå…¨å¼€æºï¼Œä»¥æ”¯æŒå’ŒåŠ é€Ÿè¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15066v2">PDF</a> Under review. 19 pages, 8 figures, 12 tables. Code and dataset are   publicly available</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°çš„ä»»åŠ¡â€”â€”æ—¶é—´åºåˆ—å¼‚å¸¸æ¨ç†ï¼ˆTime-RAï¼‰ï¼Œæ—¨åœ¨å°†ç»å…¸çš„æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä»åˆ¤åˆ«å¼ä»»åŠ¡è½¬å˜ä¸ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆå¼ã€æ¨ç†å¯†é›†å‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†é¦–ä¸ªç”¨äºå¼‚å¸¸æ¨ç†çš„ç°å®ä¸–ç•Œå¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†RATs40Kï¼ŒåŒ…å«å¤§çº¦40,000ä¸ªè·¨10ä¸ªç°å®é¢†åŸŸçš„æ ·æœ¬ã€‚æ¯ä¸ªæ ·æœ¬éƒ½åŒ…æ‹¬æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®ã€ä¸Šä¸‹æ–‡æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰è¡¨ç¤ºï¼Œå¹¶éƒ½è¿›è¡Œäº†ç²¾ç»†çš„ç±»åˆ«æ ‡æ³¨å’Œç»“æ„åŒ–è§£é‡Šæ¨ç†ã€‚å¼€å‘äº†ä¸€ä¸ªå¤æ‚çš„æ ‡æ³¨æ¡†æ¶ï¼Œåˆ©ç”¨GPT-4é©±åŠ¨çš„åé¦ˆå¯¹é›†ä½“ç”Ÿæˆçš„æ ‡ç­¾è¿›è¡Œç²¾ç‚¼ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚å¯¹LLMså’Œå¤šæ¨¡æ€LLMsçš„å¹¿æ³›åŸºå‡†æµ‹è¯•å±•ç¤ºäº†å½“å‰æ¨¡å‹çš„èƒ½åŠ›å’Œå±€é™æ€§ï¼Œå¼ºè°ƒäº†ç›‘ç£å¾®è°ƒçš„é‡è¦ä½œç”¨ã€‚æœ¬æ–‡çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸ºå¯è§£é‡Šçš„æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†çš„æ˜¾è‘—è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡â€”â€”æ—¶é—´åºåˆ—å¼‚å¸¸æ¨ç†ï¼ˆTime-RAï¼‰ï¼Œå°†ä¼ ç»Ÿçš„æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹è½¬å˜ä¸ºç”Ÿæˆå¼ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªç”¨äºå¼‚å¸¸æ¨ç†çš„å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†RATs40Kï¼ŒåŒ…å«å¤§é‡ç²¾ç»†æ ‡æ³¨çš„æ ·æœ¬ï¼Œæ¶µç›–æ•°å€¼æ—¶é—´åºåˆ—æ•°æ®ã€ä¸Šä¸‹æ–‡æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰è¡¨ç¤ºã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªå¤æ‚çš„æ ‡æ³¨æ¡†æ¶ï¼Œåˆ©ç”¨GPT-4é©±åŠ¨çš„åé¦ˆæé«˜æ ‡æ³¨å‡†ç¡®æ€§ã€‚</li>
<li>å¯¹å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å…¶åœ¨æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸­çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚</li>
<li>æ•°æ®é›†å’Œä»»åŠ¡çš„å‘å¸ƒä¸ºæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†ç›‘ç£å¾®è°ƒåœ¨æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f465422b966415efba1ae6ba1a579523.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b475ae8458d1389a00a692ca95e86aca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27954fea085f5b694f176ad669976507.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e3236dddcf5eb661e83f44dca523f78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee94dc098515856d7c91c0cb040426b8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MiroMind-M1-An-Open-Source-Advancement-in-Mathematical-Reasoning-via-Context-Aware-Multi-Stage-Policy-Optimization"><a href="#MiroMind-M1-An-Open-Source-Advancement-in-Mathematical-Reasoning-via-Context-Aware-Multi-Stage-Policy-Optimization" class="headerlink" title="MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via   Context-Aware Multi-Stage Policy Optimization"></a>MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via   Context-Aware Multi-Stage Policy Optimization</h2><p><strong>Authors:Xingxuan Li, Yao Xiao, Dianwen Ng, Hai Ye, Yue Deng, Xiang Lin, Bin Wang, Zhanfeng Mo, Chong Zhang, Yueyi Zhang, Zonglin Yang, Ruilin Li, Lei Lei, Shihao Xu, Han Zhao, Weiling Chen, Feng Ji, Lidong Bing</strong></p>
<p>Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å·²ç»ä»æµç•…çš„æ–‡æœ¬ç”Ÿæˆå‘å±•åˆ°è·¨å¤šä¸ªé¢†åŸŸçš„å…ˆè¿›æ¨ç†ï¼Œä»è€Œäº§ç”Ÿäº†æ¨ç†è¯­è¨€æ¨¡å‹ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œæ•°å­¦æ¨ç†ä½œä¸ºä¸€ä¸ªä»£è¡¨æ€§åŸºå‡†ï¼Œå®ƒè¦æ±‚ç²¾ç¡®çš„å¤šæ­¥éª¤é€»è¾‘å’ŒæŠ½è±¡æ¨ç†ï¼Œå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–ä»»åŠ¡ã€‚è™½ç„¶é—­æºRLMï¼ˆå¦‚GPT-o3ï¼‰å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶ä¸“æœ‰æ€§è´¨é™åˆ¶äº†é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚å°½ç®¡è®¸å¤šå¼€æºé¡¹ç›®æ—¨åœ¨ç¼©å°è¿™ä¸€å·®è·ï¼Œä½†å…¶ä¸­å¤§å¤šæ•°ç”±äºç¼ºä¹å…³é”®èµ„æºï¼ˆå¦‚æ•°æ®é›†å’Œè¯¦ç»†çš„è®­ç»ƒé…ç½®ï¼‰è€Œç¼ºä¹è¶³å¤Ÿçš„å¼€æ”¾æ€§ï¼Œä»è€Œé˜»ç¢äº†å¯é‡å¤æ€§ã€‚ä¸ºäº†ä¿ƒè¿›RLMå¼€å‘ä¸­çš„æ›´å¤§é€æ˜åº¦ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MiroMind-M1ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŸºäºQwen-2.5æ¶æ„çš„å®Œå…¨å¼€æºRLMï¼Œå…¶æ€§èƒ½å¯ä¸ç°æœ‰å¼€æºRLMç›¸ï¿½ï¿½ åª²ç¾ç”šè‡³æ›´èƒœä¸€ç­¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆåœ¨ç²¾å¿ƒæŒ‘é€‰çš„71.9ä¸‡é“æ•°å­¦æ¨ç†é—®é¢˜æ•°æ®é›†ä¸Šè¿›è¡ŒSFTè®­ç»ƒï¼Œè¿™äº›é—®é¢˜éƒ½æœ‰ç»è¿‡éªŒè¯çš„è§£é¢˜æ­¥éª¤ï¼ˆCoTè½¨è¿¹ï¼‰ï¼Œç„¶ååœ¨6.2ä¸‡é“å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¯éªŒè¯æ€§çš„é—®é¢˜ä¸Šè¿›è¡ŒRLVRè®­ç»ƒã€‚ä¸ºäº†æé«˜RLVRè¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†Context-Aware Multi-Stage Policy Optimizationç®—æ³•ï¼Œè¯¥ç®—æ³•å°†é•¿åº¦æ¸è¿›è®­ç»ƒä¸è‡ªé€‚åº”é‡å¤æƒ©ç½šç›¸ç»“åˆï¼Œä»¥é¼“åŠ±ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„RLè®­ç»ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨AIME24ã€AIME25å’ŒMATHåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆ–ç«äº‰æ€§çš„æ€§èƒ½å’Œå‡ºè‰²çš„ä»¤ç‰Œæ•ˆç‡ï¼Œåœ¨åŸºäºQwen-2.5çš„å¼€æº7Bå’Œ32Bæ¨¡å‹ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚ä¸ºäº†ä¾¿äºå¤åˆ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å®Œæ•´çš„å †æ ˆï¼šæ¨¡å‹ï¼ˆMiroMind-M1-SFT-7Bã€MiroMind-M1-RL-7Bã€MiroMind-M1-RL-32Bï¼‰ã€æ•°æ®é›†ï¼ˆMiroMind-M1-SFT-719Kã€MiroMind-M1-RL-62Kï¼‰ä»¥åŠæ‰€æœ‰çš„è®­ç»ƒå’Œè¯„ä¼°é…ç½®ã€‚æˆ‘ä»¬å¸Œæœ›è¿™äº›èµ„æºèƒ½å¤Ÿæ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶å¹¶æ¨åŠ¨ç¤¾åŒºçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14683v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²ä»æµç•…æ–‡æœ¬ç”Ÿæˆå‘å±•åˆ°è·¨åŸŸé«˜çº§æ¨ç†ï¼Œäº§ç”Ÿäº†æ¨ç†è¯­è¨€æ¨¡å‹ã€‚æ•°å­¦æ¨ç†ä½œä¸ºç²¾ç¡®å¤šæ­¥éª¤é€»è¾‘å’ŒæŠ½è±¡æ¨ç†çš„ä»£è¡¨é¢†åŸŸï¼Œåœ¨æ¨¡å‹ä¸­æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚å¼€æºæ¨ç†è¯­è¨€æ¨¡å‹MiroMind-M1ç³»åˆ—çš„å‡ºç°ï¼Œæ—¨åœ¨ä¿ƒè¿›é€æ˜åº¦ï¼Œå¹¶ä½¿ç”¨Qwen-2.5ä¸ºæ¡†æ¶ï¼Œå…¶æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰å¼€æºæ¨ç†è¯­è¨€æ¨¡å‹çš„æ°´å¹³ã€‚æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼Œç¬¬ä¸€é˜¶æ®µæ˜¯åœ¨ç²¾å¿ƒæŒ‘é€‰çš„71ä¸‡æ•°å­¦æ¨ç†é—®é¢˜è¯­æ–™åº“ä¸Šè¿›è¡ŒSFTè®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µåˆ™æ˜¯åœ¨6.2ä¸‡å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯éªŒè¯é—®é¢˜ä¸Šå®æ–½RLVRè®­ç»ƒã€‚ä¸ºå¢å¼ºRLVRè¿‡ç¨‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ï¼Œå¼•å…¥äº†Context-Aware Multi-Stage Policy Optimizationç®—æ³•ã€‚æ¨¡å‹åœ¨AIME24ã€AIME25å’ŒMATHåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å“è¶Šæˆ–ç«äº‰æ€§çš„è¡¨ç°ã€‚ä¸ºä¿ƒè¿›å¯å¤åˆ¶æ€§ï¼Œå›¢é˜Ÿå…¬å¼€äº†å®Œæ•´çš„æ¨¡å‹ã€æ•°æ®é›†ä»¥åŠæ‰€æœ‰è®­ç»ƒå’Œè¯„ä¼°é…ç½®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²è¿›åŒ–è‡³è·¨åŸŸé«˜çº§æ¨ç†ï¼Œæ•°å­¦æ¨ç†æ˜¯å…¶ä¸­çš„é‡è¦é¢†åŸŸã€‚</li>
<li>MiroMind-M1ç³»åˆ—æ˜¯ä¸€ç§å¼€æºæ¨ç†è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºQwen-2.5æ¡†æ¶æ„å»ºã€‚</li>
<li>æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆåœ¨71ä¸‡æ•°å­¦æ¨ç†é—®é¢˜ä¸Šè¿›è¡ŒSFTè®­ç»ƒï¼Œç„¶ååœ¨6.2ä¸‡æŒ‘æˆ˜æ€§å¯éªŒè¯é—®é¢˜ä¸Šå®æ–½RLVRè®­ç»ƒã€‚</li>
<li>å¼•å…¥Context-Aware Multi-Stage Policy Optimizationç®—æ³•ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</li>
<li>MiroMind-M1æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œè¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ä¸ºä¿ƒè¿›ç ”ç©¶å’Œç¤¾åŒºå‘å±•ï¼Œå›¢é˜Ÿå…¬å¼€äº†å®Œæ•´çš„æ¨¡å‹ã€æ•°æ®é›†åŠè®­ç»ƒå’Œè¯„ä¼°é…ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-72298ef1677de5f657bef63306c70601.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="BusterX-Towards-Unified-Cross-Modal-AI-Generated-Content-Detection-and-Explanation-with-MLLM"><a href="#BusterX-Towards-Unified-Cross-Modal-AI-Generated-Content-Detection-and-Explanation-with-MLLM" class="headerlink" title="BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection   and Explanation with MLLM"></a>BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection   and Explanation with MLLM</h2><p><strong>Authors:Haiquan Wen, Tianxiao Li, Zhenglin Huang, Yiwei He, Guangliang Cheng</strong></p>
<p>Recent advances in generative AI have dramatically improved image and video synthesis capabilities, significantly increasing the risk of misinformation through sophisticated fake content. In response, detection methods have evolved from traditional approaches to multimodal large language models (MLLMs), offering enhanced transparency and interpretability in identifying synthetic media. However, current detection systems remain fundamentally limited by their single-modality design. These approaches analyze images or videos separately, making them ineffective against synthetic content that combines multiple media formats. To address these challenges, we introduce \textbf{BusterX++}, a novel framework designed specifically for cross-modal detection and explanation of synthetic media. Our approach incorporates an advanced reinforcement learning (RL) post-training strategy that eliminates cold-start. Through Multi-stage Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and substantial performance improvements. To enable comprehensive evaluation, we also present \textbf{GenBuster++}, a cross-modal benchmark leveraging state-of-the-art image and video generation techniques. This benchmark comprises 4,000 images and video clips, meticulously curated by human experts using a novel filtering methodology to ensure high quality, diversity, and real-world applicability. Extensive experiments demonstrate the effectiveness and generalizability of our approach. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†å›¾åƒå’Œè§†é¢‘åˆæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿæ˜¾è‘—å¢åŠ äº†é€šè¿‡å¤æ‚è™šå‡å†…å®¹ä¼ æ’­è¯¯ä¿¡æ¯çš„é£é™©ã€‚å¯¹æ­¤ï¼Œæ£€æµ‹æ–¹æ³•å·²ç»ä»ä¼ ç»Ÿæ–¹æ³•è¿›åŒ–åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œåœ¨è¯†åˆ«åˆæˆåª’ä½“æ—¶æä¾›äº†å¢å¼ºé€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå½“å‰æ£€æµ‹ç³»ç»Ÿçš„åŸºæœ¬å±€é™æ€§åœ¨äºå…¶å•æ¨¡æ€è®¾è®¡ã€‚è¿™äº›æ–¹æ³•åˆ†åˆ«åˆ†æå›¾åƒæˆ–è§†é¢‘ï¼Œå¯¹äºç»“åˆå¤šç§åª’ä½“æ ¼å¼çš„åˆæˆå†…å®¹ï¼Œå®ƒä»¬çš„æ•ˆæœå¤§æ‰“æŠ˜æ‰£ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†**BusterX++<strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè·¨æ¨¡æ€æ£€æµ‹å’Œè®¾è®¡åˆæˆåª’ä½“è§£é‡Šè€Œè®¾è®¡çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒç­–ç•¥ï¼Œæ¶ˆé™¤äº†å†·å¯åŠ¨ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒã€æ€è€ƒå¥–åŠ±å’Œæ··åˆæ¨ç†ï¼ŒBusterX++å®ç°äº†ç¨³å®šä¸”æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†</strong>GenBuster++**ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æœ€æ–°å›¾åƒå’Œè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«4000å¼ å›¾åƒå’Œè§†é¢‘ç‰‡æ®µï¼Œç”±äººç±»ä¸“å®¶ä½¿ç”¨æ–°å‹è¿‡æ»¤æ–¹æ³•ç²¾å¿ƒæŒ‘é€‰ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡ã€å¤šæ ·æ€§å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14632v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç”Ÿæˆå¼AIæŠ€æœ¯çš„å¿«é€Ÿå‘å±•æ˜¾è‘—æå‡äº†å›¾åƒå’Œè§†é¢‘åˆæˆèƒ½åŠ›ï¼Œå¢åŠ äº†é€šè¿‡å¤æ‚è™šå‡å†…å®¹ä¼ æ’­è¯¯å¯¼ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæ£€æµ‹æ‰‹æ®µå·²ä»ä¼ ç»Ÿæ–¹æ³•è¿›åŒ–åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæé«˜äº†åœ¨è¯†åˆ«åˆæˆåª’ä½“æ—¶çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå½“å‰æ£€æµ‹ç³»ç»Ÿçš„åŸºæœ¬å±€é™åœ¨äºå…¶å•æ¨¡æ€è®¾è®¡ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹èåˆå¤šç§åª’ä½“æ ¼å¼çš„ç»¼åˆåˆæˆå†…å®¹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºå…¨æ–°æ¡†æ¶**BusterX++<strong>ï¼Œä¸“é—¨ç”¨äºè·¨æ¨¡æ€æ£€æµ‹å¹¶è§£é‡Šåˆæˆåª’ä½“ã€‚é€šè¿‡å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒç­–ç•¥ï¼Œç»“åˆå¤šé˜¶æ®µè®­ç»ƒã€æ€è€ƒå¥–åŠ±å’Œæ··åˆæ¨ç†ï¼ŒBusterX++å®ç°äº†ç¨³å®šä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¸ºè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†</strong>GenBuster++**è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨æœ€å…ˆè¿›çš„å›¾åƒå’Œè§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼ŒåŒ…å«4000å¼ å›¾åƒå’Œè§†é¢‘ç‰‡æ®µï¼Œç”±äººç±»ä¸“å®¶é€šè¿‡æ–°é¢–è¿‡æ»¤æ–¹æ³•ç²¾å¿ƒæŒ‘é€‰ï¼Œç¡®ä¿é«˜è´¨é‡ã€å¤šæ ·æ€§å’Œç°å®åº”ç”¨å¯è¡Œæ€§ã€‚å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIæŠ€æœ¯åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå¢åŠ äº†è™šå‡å†…å®¹ä¼ æ’­çš„é£é™©ã€‚</li>
<li>ä¼ ç»Ÿæ£€æµ‹æ‰‹æ®µå­˜åœ¨å±€é™æ€§ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æä¾›æ›´é«˜çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å½“å‰æ£€æµ‹ç³»ç»Ÿä¸»è¦æ˜¯å•æ¨¡æ€çš„ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹è·¨å¤šç§åª’ä½“æ ¼å¼çš„ç»¼åˆåˆæˆå†…å®¹ã€‚</li>
<li>å¼•å…¥æ–°å‹æ¡†æ¶BusterX++ï¼Œä¸“é—¨ç”¨äºè·¨æ¨¡æ€æ£€æµ‹å¹¶è§£é‡Šåˆæˆåª’ä½“ï¼Œå…·æœ‰å¼ºåŒ–å­¦ä¹ åè®­ç»ƒç­–ç•¥ã€å¤šé˜¶æ®µè®­ç»ƒç­‰ç‰¹ç‚¹ã€‚</li>
<li>BusterX++é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ ã€å¤šé˜¶æ®µè®­ç»ƒç­‰å®ç°æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>æå‡ºGenBuster++è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œç”¨äºå…¨é¢è¯„ä¼°å›¾åƒå’Œè§†é¢‘åˆæˆæ£€æµ‹æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a35e1585063296c66ede85d5eb42690.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-537823fd853e59c354e2ea9821c8d678.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f776d600ef6557b74e058dffb27d1564.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c9650e74d0b9039dbe227f56db2f94d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-297e226886156c7b81ff2c1a58e8c89e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-644ec17f21c7e9aa4d72ba41489a6d23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-076a57feba886961fce9eca44de2d224.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MetaLint-Generalizable-Idiomatic-Code-Quality-Analysis-through-Instruction-Following-and-Easy-to-Hard-Generalization"><a href="#MetaLint-Generalizable-Idiomatic-Code-Quality-Analysis-through-Instruction-Following-and-Easy-to-Hard-Generalization" class="headerlink" title="MetaLint: Generalizable Idiomatic Code Quality Analysis through   Instruction-Following and Easy-to-Hard Generalization"></a>MetaLint: Generalizable Idiomatic Code Quality Analysis through   Instruction-Following and Easy-to-Hard Generalization</h2><p><strong>Authors:Atharva Naik, Lawanya Baghel, Dhakshin Govindarajan, Darsh Agrawal, Daniel Fried, Carolyn Rose</strong></p>
<p>Large Language Models, though successful in code generation, struggle with code quality analysis because they are limited by static training data and canâ€™t easily adapt to evolving best practices. We introduce MetaLint, a new instruction-following framework that formulates code quality analysis as the task of detecting and fixing problematic semantic code fragments or code idioms based on high-level specifications. Unlike conventional approaches that train models on static, rule-based data, MetaLint employs instruction tuning on synthetic linter-generated data to support easy-to-hard generalization, enabling models to adapt to novel or complex code patterns without retraining. To evaluate this, we construct a benchmark of challenging idioms inspired by real-world coding standards such as Python Enhancement Proposals (PEPs) and assess whether MetaLint-trained models reason adaptively or simply memorize. Our results show that MetaLint improves generalization to unseen PEP idioms, achieving a 70.37% F-score on idiom detection with the highest recall (70.43%) among all evaluated models. It also achieves 26.73% on localization, competitive for its 4B parameter size and comparable to larger state-of-the-art models like o3-mini, highlighting its potential for future-proof code quality analysis. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨ä»£ç è´¨é‡åˆ†ææ–¹é¢å´é‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬å—é™äºé™æ€è®­ç»ƒæ•°æ®ï¼Œæ— æ³•è½»æ¾é€‚åº”ä¸æ–­å˜åŒ–çš„æœ€ä½³å®è·µã€‚æˆ‘ä»¬å¼•å…¥äº†MetaLintï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æŒ‡ä»¤éµå¾ªæ¡†æ¶ï¼Œå®ƒå°†ä»£ç è´¨é‡åˆ†æåˆ¶å®šä¸ºåŸºäºé«˜çº§è§„èŒƒæ£€æµ‹å¹¶ä¿®å¤é—®é¢˜è¯­ä¹‰ä»£ç ç‰‡æ®µæˆ–ä»£ç æƒ¯ç”¨æ³•çš„ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„åœ¨é™æ€ã€åŸºäºè§„åˆ™çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒMetaLinté‡‡ç”¨åˆæˆlinterç”Ÿæˆæ•°æ®çš„æŒ‡ä»¤å¾®è°ƒï¼Œæ”¯æŒä»æ˜“åˆ°éš¾çš„æ³›åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°çš„æˆ–å¤æ‚çš„ä»£ç æ¨¡å¼ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä»¥ç°å®ä¸–ç•Œç¼–ç æ ‡å‡†ï¼ˆå¦‚Pythonå¢å¼ºææ¡ˆï¼ˆPEPsï¼‰ï¼‰ä¸ºçµæ„Ÿçš„æŒ‘æˆ˜idiomçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯„ä¼°MetaLintè®­ç»ƒçš„æ¨¡å‹æ˜¯é€‚åº”æ€§æ¨ç†è¿˜æ˜¯ç®€å•è®°å¿†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMetaLintåœ¨æœªè§è¿‡çš„PEPæƒ¯ç”¨æ³•ä¸­çš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†æé«˜ï¼Œåœ¨idiomæ£€æµ‹æ–¹é¢è¾¾åˆ°äº†70.37%çš„Fåˆ†æ•°ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æ¨¡å‹ä¸­å¬å›ç‡æœ€é«˜ï¼ˆ70.43%ï¼‰ã€‚åœ¨å®šä½æ–¹é¢ï¼Œå®ƒè¾¾åˆ°äº†26.73%ï¼Œå¯¹äºå…¶4Bçš„å‚æ•°å¤§å°æ¥è¯´å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸è¾ƒå¤§çš„æœ€æ–°æ¨¡å‹ï¼ˆå¦‚o3-miniï¼‰ç›¸å½“ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨æœªæ¥ä»£ç è´¨é‡åˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11687v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä»£ç è´¨é‡åˆ†ææ–¹é¢å´å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬å—é™äºé™æ€è®­ç»ƒæ•°æ®ï¼Œæ— æ³•è½»æ¾é€‚åº”ä¸æ–­å˜åŒ–çš„æœ€ä½³å®è·µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MetaLintï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æŒ‡ä»¤éµå¾ªæ¡†æ¶ï¼Œå®ƒå°†ä»£ç è´¨é‡åˆ†æåˆ¶å®šä¸ºæ£€æµ‹å¹¶ä¿®å¤åŸºäºé«˜çº§è§„èŒƒçš„é—®é¢˜è¯­ä¹‰ä»£ç ç‰‡æ®µæˆ–ä»£ç æƒ¯ç”¨æ³•çš„ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„åœ¨é™æ€è§„åˆ™åŸºç¡€ä¸Šè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒMetaLinté‡‡ç”¨åˆæˆlinterç”Ÿæˆæ•°æ®çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼Œæ”¯æŒä»æ˜“åˆ°éš¾çš„æ³›åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”äºæ–°å‹æˆ–å¤æ‚çš„ä»£ç æ¨¡å¼ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMetaLintåœ¨æœªè§è¿‡çš„PEPæƒ¯ç”¨æ³•ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†æé«˜ï¼Œåœ¨idiomæ£€æµ‹æ–¹é¢è¾¾åˆ°äº†70.37%çš„F-scoreï¼Œå…¶ä¸­å¬å›ç‡æœ€é«˜ï¼ˆ70.43%ï¼‰ï¼Œåœ¨å®šä½æ–¹é¢ä¹Ÿå–å¾—äº†26.73%çš„ç«äº‰åŠ›æˆç»©ã€‚è¿™çªæ˜¾äº†å…¶åœ¨æœªæ¥ä»£ç è´¨é‡åˆ†æä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä»£ç è´¨é‡åˆ†ææ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MetaLintæ˜¯ä¸€ç§æ–°çš„æŒ‡ä»¤éµå¾ªæ¡†æ¶ï¼Œç”¨äºè§£å†³ä»£ç è´¨é‡åˆ†æé—®é¢˜ã€‚</li>
<li>MetaLinté€šè¿‡åˆæˆlinterç”Ÿæˆæ•°æ®çš„æŒ‡ä»¤å¾®è°ƒæ–¹æ³•æ”¯æŒæ¨¡å‹ä»æ˜“åˆ°éš¾çš„æ³›åŒ–ã€‚</li>
<li>MetaLintèƒ½å¤Ÿåœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä½¿æ¨¡å‹é€‚åº”æ–°å‹æˆ–å¤æ‚çš„ä»£ç æ¨¡å¼ã€‚</li>
<li>MetaLinté€šè¿‡æ£€æµ‹å¹¶ä¿®å¤é—®é¢˜è¯­ä¹‰ä»£ç ç‰‡æ®µæˆ–ä»£ç æƒ¯ç”¨æ³•æ¥å®Œæˆä»»åŠ¡ã€‚</li>
<li>åœ¨æœªè§è¿‡çš„PEPæƒ¯ç”¨æ³•è¯„ä¼°ä¸­ï¼ŒMetaLintæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a6df828ed4375e7e133da154195e59e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c3d3f14462813890eb207112e645a2c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1"><a href="#A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1" class="headerlink" title="A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1"></a>A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1</h2><p><strong>Authors:Marcin PietroÅ„, RafaÅ‚ Olszowski, Jakub GomuÅ‚ka, Filip Gampel, Andrzej Tomski</strong></p>
<p>Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLMâ€™s, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings. </p>
<blockquote>
<p>è®ºè¯æŒ–æ˜ï¼ˆAMï¼‰æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼Œèåˆäº†é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ã€ä¿®è¾å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰é¢†åŸŸçš„è§è§£ã€‚å®ƒæ¶‰åŠè‡ªåŠ¨è¯†åˆ«å’Œæå–è®ºè¯æ€§æˆåˆ†ï¼Œå¦‚å‰æå’Œä¸»å¼ ï¼Œä»¥åŠæ£€æµ‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚æ”¯æŒã€æ”»å‡»æˆ–ä¸­ç«‹ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•å’Œå…¶ä»–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†ææå–è®ºè¯è¯­ä¹‰æ–¹é¢çš„æ•ˆç‡å¤§å¤§æé«˜ã€‚å°½ç®¡æœ‰å¾ˆå¤šåŸºå‡†æµ‹è¯•ç”¨äºæ£€éªŒå’ŒéªŒè¯LLMçš„è´¨é‡ï¼Œä½†åœ¨å…¬å¼€å¯ç”¨çš„è®ºè¯åˆ†ç±»æ•°æ®åº“ä¸­å…³äºè¿™äº›æ¨¡å‹æ“ä½œçš„ç ”ç©¶å’Œç»“æœä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡ä½¿ç”¨Args.meå’ŒUKPç­‰ä¸åŒçš„æ•°æ®é›†å¯¹ä¸€ç³»åˆ—LLMè¿›è¡Œäº†ç ”ç©¶ã€‚æµ‹è¯•çš„æ¨¡å‹åŒ…æ‹¬GPTã€Llamaå’ŒDeepSeekçš„ç‰ˆæœ¬ï¼Œä»¥åŠé‡‡ç”¨Chain-of-Thoughtsç®—æ³•çš„æ¨ç†å¢å¼ºå˜ä½“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒChatGPT-4oçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚åœ¨èå…¥æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¸­ï¼ŒDeepseek-R1è¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡GPT-4oå’ŒDeepseek-R1å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬ä»ç„¶ä¼šå‡ºç°é”™è¯¯ã€‚æœ¬æ–‡è®¨è®ºäº†æ‰€æœ‰æ¨¡å‹æœ€å¸¸è§çš„é”™è¯¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ‰€å‘ˆç°çš„å·¥ä½œæ˜¯ä½¿ç”¨LLMå’Œæç¤ºç®—æ³•å¯¹æ‰€è¿°æ•°æ®é›†è¿›è¡Œçš„é¦–æ¬¡æ›´å¹¿æ³›çš„åˆ†æã€‚è¯¥å·¥ä½œè¿˜æ˜¾ç¤ºäº†å·²çŸ¥æç¤ºç®—æ³•åœ¨è®ºè¯åˆ†æä¸­çš„ä¸€äº›å¼±ç‚¹ï¼Œå¹¶æŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘ã€‚è¯¥å·¥ä½œçš„é™„åŠ å€¼æ˜¯å¯¹ç°æœ‰è®ºè¯æ•°æ®é›†è¿›è¡Œæ·±å…¥åˆ†æå’Œå±•ç¤ºå…¶ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08621v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†è®ºè¯æŒ–æ˜ï¼ˆAMï¼‰è¿™ä¸€è·¨å­¦ç§‘ç ”ç©¶é¢†åŸŸï¼Œå®ƒç»“åˆäº†é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ã€ä¿®è¾å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰é¢†åŸŸçš„è§è§£ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®ºè¯è¯­ä¹‰åˆ†ææå–æ–¹é¢çš„è¿›å±•ï¼Œå¹¶ä½¿ç”¨Args.meå’ŒUKPç­‰æ•°æ®é›†å¯¹GPTã€Llamaå’ŒDeepSeekç­‰æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼ŒChatGPT-4oåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒDeepseek-R1åœ¨ç»“åˆæ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ã€‚ä½†å³ä¾¿æ˜¯æœ€ä¼˜æ¨¡å‹ä¹Ÿå­˜åœ¨é”™è¯¯ï¼Œæ–‡ç« ä¹Ÿè®¨è®ºäº†æœ€å¸¸è§çš„é”™è¯¯ã€‚è¿™ç¯‡æ–‡ç« æ˜¯å¯¹ä½¿ç”¨LLMå’Œæç¤ºç®—æ³•è¿›è¡Œæ•°æ®é›†å¹¿æ³›åˆ†æçš„é¦–æ¬¡å°è¯•ï¼Œå¹¶æŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºè¯æŒ–æ˜ï¼ˆAMï¼‰æ˜¯ä¸€ä¸ªæ¶‰åŠå¤šä¸ªå­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ç­‰ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å’Œæå–è®ºè¯æˆåˆ†ä»¥åŠå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®ºè¯è¯­ä¹‰åˆ†æå’Œæå–æ–¹é¢æœ‰ç€æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå…¶æ•ˆç‡æ›´é«˜ã€‚</li>
<li>ä½¿ç”¨Args.meå’ŒUKPç­‰æ•°æ®é›†çš„æµ‹è¯•è¡¨æ˜ï¼ŒChatGPT-4oåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç»“åˆæ¨ç†èƒ½åŠ›çš„Deepseek-R1æ¨¡å‹åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹å±•ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>å°½ç®¡GPT-4oå’ŒDeepseek-R1è¡¨ç°ä¼˜ç§€ï¼Œä½†å®ƒä»¬ä»ä¼šçŠ¯é”™è¯¯ï¼Œæ–‡ç« è®¨è®ºäº†è¿™äº›é”™è¯¯ä¸­æœ€å¸¸è§çš„ä¸€äº›ã€‚</li>
<li>æœ¬æ–‡æ˜¯å¯¹ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæç¤ºç®—æ³•è¿›è¡Œæ•°æ®é›†å¹¿æ³›åˆ†æçš„é¦–æ¬¡å°è¯•ï¼Œå±•ç¤ºäº†æ•°æ®é›†åœ¨åˆ†æä¸­çš„çŸ­æ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fdd5c028a173a052700b45926a8b7474.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38837f646b5db2769cef3d1544aebc1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-554497861f6c0d11059799e2e0db3fed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5c3d9bcbd47fc0f0f79df92b3663d52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa7415114b13d001e419215dcce9c0c7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SpatialViz-Bench-Automatically-Generated-Spatial-Visualization-Reasoning-Tasks-for-MLLMs"><a href="#SpatialViz-Bench-Automatically-Generated-Spatial-Visualization-Reasoning-Tasks-for-MLLMs" class="headerlink" title="SpatialViz-Bench: Automatically Generated Spatial Visualization   Reasoning Tasks for MLLMs"></a>SpatialViz-Bench: Automatically Generated Spatial Visualization   Reasoning Tasks for MLLMs</h2><p><strong>Authors:Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, Jun Wang</strong></p>
<p>Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmarkâ€™s strong discriminative power, but also uncovers counter-intuitive findings: models show difficulty perception misaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs, default to formulaic derivation over visualization, and paradoxically suffer performance degradation from Chain-of-Thought prompting in open-source models. Through statistical and qualitative analysis of error types, SpatialViz-Bench demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark data and evaluation code are publicly available. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿåœ¨å¤§è„‘ä¸­ç›´æ¥æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒï¼Œè¿™ç§èƒ½åŠ›è¢«ç§°ä¸ºç©ºé—´å¯è§†åŒ–ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†å¯¹ç©ºé—´å¯è§†åŒ–çš„è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œé€šå¸¸åµŒå…¥åœ¨æ›´å¹¿æ³›çš„æ•°å­¦å’Œé€»è¾‘è¯„ä¼°ä¸­ã€‚ç°æœ‰çš„è¯„ä¼°é€šå¸¸ä¾èµ–äºå¯èƒ½ä¸è®­ç»ƒæ•°æ®é‡å çš„æ™ºå•†æµ‹è¯•æˆ–æ•°å­¦ç«èµ›ï¼Œè¿™å¯èƒ½ä¼šå½±å“è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialViz-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«4ä¸ªå­èƒ½åŠ›é¢†åŸŸçš„12é¡¹ä»»åŠ¡ï¼Œå…±è®¡1180ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ã€‚æˆ‘ä»¬å¯¹33ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ä»…æ­ç¤ºäº†æ€§èƒ½ä¸Šçš„å·¨å¤§å·®å¼‚ï¼Œè¯æ˜äº†åŸºå‡†æµ‹è¯•çš„å¼ºå¤§é‰´åˆ«åŠ›ï¼Œè¿˜å‘ç°äº†å…·æœ‰æ‚–å¸¸ç†çš„ç»“æœï¼šæ¨¡å‹å­˜åœ¨ä¸äººç±»ç›´è§‰ç›¸æ‚–çš„æ„ŸçŸ¥å›°éš¾ã€æ˜æ˜¾çš„äºŒç»´åˆ°ä¸‰ç»´æ€§èƒ½ä¸‹é™ã€å€¾å‘äºå…¬å¼æ¨å¯¼è€Œéå¯è§†åŒ–ã€ä»¥åŠåœ¨å¼€æºæ¨¡å‹ä¸­é‡åˆ°â€œæ€ç»´é“¾â€æç¤ºæ—¶çš„è¡¨ç°åè€Œä¸‹é™ç­‰é—®é¢˜ã€‚é€šè¿‡å¯¹é”™è¯¯ç±»å‹çš„ç»Ÿè®¡å’Œå®šæ€§åˆ†æï¼ŒSpatialViz-Benchæ˜¾ç¤ºï¼Œç›®å‰æœ€å…ˆè¿›çš„MLLMåœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨ç¼ºé™·ï¼Œä»è€Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚åŸºå‡†æµ‹è¯•æ•°æ®å’Œè¯„ä¼°ä»£ç å‡å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07610v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´å¯è§†åŒ–æ˜¯äººç±»èƒ½å¤Ÿåœ¨å¤§è„‘ä¸­ç›´æ¥æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒçš„èƒ½åŠ›ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†ç©ºé—´å¯è§†åŒ–å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ï¼Œé€šå¸¸åµŒå…¥åœ¨æ›´å¹¿æ³›çš„æ•°å­¦å’Œé€»è¾‘è¯„ä¼°ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialViz-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«12é¡¹ä»»åŠ¡ï¼Œæ¶µç›–4ç§å­èƒ½åŠ›ï¼Œå…±1180ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ã€‚æˆ‘ä»¬å¯¹33ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ä»…æ­ç¤ºäº†å¹¿æ³›çš„æ€§èƒ½å·®å¼‚å¹¶è¯æ˜äº†åŸºå‡†æµ‹è¯•çš„å¼ºé‰´åˆ«åŠ›ï¼Œè¿˜å‘ç°äº†ä»¤äººå›°æƒ‘çš„ç»“æœï¼šæ¨¡å‹åœ¨æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸äººç±»ç›´è§‰çš„åå·®ã€äºŒç»´åˆ°ä¸‰ç»´æ€§èƒ½æ‚¬å´–æ˜æ˜¾ã€å€¾å‘äºå…¬å¼æ¨å¯¼è€Œéå¯è§†åŒ–ä»¥åŠå¼€æ”¾å¼æ¨¡å‹ä¸­çš„æ€ç»´é“¾æç¤ºåè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚SpatialViz-Benchå±•ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šçš„ä¸è¶³ï¼Œä¸ºè§£å†³è¯¥é¢†åŸŸçš„ä¸€ä¸ªé‡è¦ç©ºç™½æä¾›äº†é€”å¾„ã€‚åŸºå‡†æ•°æ®å’Œè¯„ä¼°ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»æ‹¥æœ‰ç©ºé—´å¯è§†åŒ–èƒ½åŠ›ï¼Œå¯ä»¥ç›´æ¥åœ¨å¤§è„‘ä¸­æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†ç©ºé—´å¯è§†åŒ–èƒ½åŠ›çš„è¯„ä¼°å°šä¸å……åˆ†ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•å¸¸ä¾èµ–äºå¯èƒ½ä¸è®­ç»ƒæ•°æ®é‡å çš„æ™ºå•†æµ‹è¯•æˆ–æ•°å­¦ç«èµ›ï¼Œå½±å“è¯„ä¼°å¯é æ€§ã€‚</li>
<li>SpatialViz-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«12é¡¹ä»»åŠ¡ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„ç©ºé—´å¯è§†åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œè¡¨æ˜åŸºå‡†æµ‹è¯•å…·æœ‰å¾ˆå¼ºçš„é‰´åˆ«åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸äººç±»ç›´è§‰çš„åå·®ï¼Œä¸”åœ¨äºŒç»´åˆ°ä¸‰ç»´è½¬æ¢çš„ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75e00cd960ea0e679dce75c7ddf7a25c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-302fc4627e6041109085ec6688f91c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6b68e7d5a4cc236c3b6cec2db50ef3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d00047e0570f5a34eaf5561f4c003a3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5891d4ce6f62a9897b0e06c90478b25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30d0cdc6ac8bd919d8671d636bdd44b2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning"><a href="#Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning" class="headerlink" title="Perception-Aware Policy Optimization for Multimodal Reasoning"></a>Perception-Aware Policy Optimization for Multimodal Reasoning</h2><p><strong>Authors:Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, a novel policy gradient algorithm that encourages the model to learn to perceive while learning to reason. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term, which can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely on additional data curation, reward models, or stronger teacher models. To further enhance the training stability of PAPO, we introduce the Double Entropy Loss, which effectively regularizes the new KL objective without compromising performance. Despite its simplicity, PAPO yields significant overall improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%-19.1%, on tasks with high vision dependency. We also observe a substantial reduction of 30.5% in perception errors, indicating improved perceptual capabilities with PAPO. Overall, our work introduces a deeper integration of perception-aware supervision into core learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Code and data will be made publicly available for research purposes. Project page: <a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">https://mikewangwzhl.github.io/PAPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²è¢«è¯æ˜æ˜¯èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›çš„ä¸€ç§é«˜åº¦æœ‰æ•ˆçš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡å’Œä¼˜åŒ–ä»ç„¶é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯¯å·®ä¸»è¦æ¥æºäºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†PAPOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­å­¦ä¼šæ„ŸçŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»¥KLæ•£åº¦é¡¹çš„å½¢å¼å¼•å…¥äº†éšæ„ŸçŸ¥æŸå¤±ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°æ’å…¥åˆ°ä¸»æµçš„RLVRç®—æ³•ä¸­ï¼Œå¦‚GRPOå’ŒDAPOã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPAPOä¸ä¾èµ–äºé¢å¤–çš„æ•°æ®æ•´ç†ã€å¥–åŠ±æ¨¡å‹æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜PAPOçš„è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡ç†µæŸå¤±ï¼Œå®ƒæœ‰æ•ˆåœ°æ­£åˆ™åŒ–äº†æ–°çš„KLç›®æ ‡ï¼Œè€Œä¸ä¼šå½±å“æ€§èƒ½ã€‚å°½ç®¡PAPOå¾ˆç®€å•ï¼Œä½†åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒä½¿æ€»ä½“æ€§èƒ½æé«˜äº†4.4%~17.5%ã€‚åœ¨é«˜åº¦ä¾èµ–è§†è§‰çš„ä»»åŠ¡ä¸­ï¼Œæ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œè¾¾åˆ°äº†8.0%~19.1%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯å‡å°‘äº†30.5%ï¼Œè¿™è¡¨æ˜PAPOæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°é›†æˆåˆ°æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰åŸºç¡€æ¨ç†çš„æ–°RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚ä¸ºäº†ç ”ç©¶ç›®çš„ï¼Œä»£ç å’Œæ•°æ®å°†å…¬å¼€æä¾›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06448v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç­–ç•¥æˆåŠŸä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹äºˆäº†å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒRLVRçš„è®¾è®¡å’Œä¼˜åŒ–ä»ç„¶ä»…é™äºçº¯æ–‡æœ¬é¢†åŸŸï¼Œå¯¼è‡´åœ¨åº”å¯¹å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶çš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ”¹å–„å¯¹è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥è¯¯å·®è¿™ä¸€å…³é”®é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºPAPOçš„æ–°å‹ç­–ç•¥æ¢¯åº¦ç®—æ³•ã€‚è¯¥ç®—æ³•å¼•å…¥äº†éšå¼æ„ŸçŸ¥æŸå¤±ï¼Œä»¥KLæ•£åº¦é¡¹çš„å½¢å¼å‡ºç°ï¼Œå¯æ— ç¼é›†æˆåˆ°ä¸»æµçš„RLVRç®—æ³•ä¸­ï¼Œå¦‚GRPOå’ŒDAPOã€‚PAPOä¸ä¾èµ–é¢å¤–çš„æ•°æ®æ•´ç†ã€å¥–åŠ±æ¨¡å‹æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚ä¸ºæé«˜PAPOçš„è®­ç»ƒç¨³å®šæ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†åŒé‡ç†µæŸå¤±ï¼Œæœ‰æ•ˆæ­£åˆ™åŒ–äº†æ–°çš„KLç›®æ ‡ï¼Œä¸”ä¸æŸå®³æ€§èƒ½ã€‚åœ¨å¤šæ ·åŒ–çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œå°½ç®¡æ–¹æ³•ç®€å•ï¼Œä½†PAPOå®ç°äº†æ˜¾è‘—çš„æ€»ä½“æ”¹è¿›ï¼Œæ”¹è¿›å¹…åº¦ä¸º4.4%~17.5%ï¼Œåœ¨é«˜åº¦ä¾èµ–è§†è§‰çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¸ºçªå‡ºï¼Œè¾¾åˆ°8%~19.1%ã€‚åŒæ—¶è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯å‡å°‘äº†30.5%ï¼Œè¿™è¡¨æ˜PAPOå¢å¼ºäº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬ç ”ç©¶å°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±åœ°æ•´åˆåˆ°æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰æ¨ç†çš„æ–°å‹RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRç­–ç•¥è™½æœ‰æ•ˆï¼Œä½†ä»å±€é™äºæ–‡æœ¬é¢†åŸŸï¼Œå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡è¡¨ç°ä¸ä½³ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€æ¨ç†çš„ä¸»è¦è¯¯å·®æ¥æºåœ¨äºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥ã€‚</li>
<li>PAPOæ˜¯ä¸€ç§æ–°å‹ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„æ„ŸçŸ¥ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>PAPOå¼•å…¥éšå¼æ„ŸçŸ¥æŸå¤±å’ŒåŒé‡ç†µæŸå¤±æ¥æé«˜æ€§èƒ½å’Œç¨³å®šæ€§ã€‚</li>
<li>PAPOåœ¨ä¸ä¾èµ–é¢å¤–èµ„æºçš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>åœ¨é«˜åº¦ä¾èµ–è§†è§‰çš„ä»»åŠ¡ä¸Šï¼ŒPAPOçš„æ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ„ŸçŸ¥é”™è¯¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fbfe26e7812ec9e6b648e06af4dff0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf44c02baa83bd65ec827384ea00ac9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc805aaee1789b774c13bebe5c62db41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ae77696b96e7c175701b51476deca66.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RAG-R1-Incentivize-the-Search-and-Reasoning-Capabilities-of-LLMs-through-Multi-query-Parallelism"><a href="#RAG-R1-Incentivize-the-Search-and-Reasoning-Capabilities-of-LLMs-through-Multi-query-Parallelism" class="headerlink" title="RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism"></a>RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism</h2><p><strong>Authors:Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while LLMs remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have aimed to enhance modelsâ€™ search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to reliance on single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, with the aim of reducing inference time and enhancing the modelâ€™s capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶å†…éƒ¨çŸ¥è¯†çš„é™æ€æ€§ï¼ŒLLMä»å®¹æ˜“ç”Ÿæˆè™šæ„æˆ–è¿‡æ—¶çš„å›åº”ã€‚æœ€è¿‘ï¼Œå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•çš„è¿›æ­¥æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´ç€è®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶é‡åˆ°äº†ç”±äºä¾èµ–å•æŸ¥è¯¢æ¨¡å¼è€Œå¯¼è‡´çš„é—®é¢˜ï¼Œå¦‚æ¨ç†æ—¶é—´å»¶é•¿å’Œèƒ½åŠ›å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAG-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨æ¡†æ¶å†…å°†ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ä»å•æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œå¤„ç†ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶æé«˜æ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º13.2%ï¼Œå¹¶å°†æ¨ç†æ—¶é—´å‡å°‘äº†11.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02962v4">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶ç”Ÿæˆå›åº”æ—¶æ˜“äº§ç”Ÿè™šæ„æˆ–è¿‡æ—¶ä¿¡æ¯ã€‚ä¸ºæé«˜æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ï¼Œæœ€è¿‘å‡ºç°äº†å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚å°½ç®¡è¿™äº›æ–¹æ³•å‰æ™¯å¹¿é˜”ï¼Œä½†ä»é¢ä¸´è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜ï¼Œä¸”å­˜åœ¨æ¨ç†æ—¶é—´é•¿ã€èƒ½åŠ›å—é™ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºRAG-R1ï¼Œä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åˆ©ç”¨å†…å¤–éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ‰©å±•äº†æ¡†æ¶å†…çš„ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ï¼Œä»å•æŸ¥è¯¢æ¨¡å¼è½¬å˜ä¸ºå¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ï¼Œæ—¨åœ¨ç¼©çŸ­æ¨ç†æ—¶é—´å¹¶å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º13.2%ï¼Œå¹¶å°†æ¨ç†æ—¶é—´ç¼©çŸ­äº†11.1%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†æ˜“ç”Ÿæˆè™šæ„æˆ–è¿‡æ—¶å›åº”ï¼Œéœ€æ”¹è¿›ã€‚</li>
<li>è¿‘æœŸå‡ºç°çš„å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>RAGæ–¹æ³•é¢ä¸´è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜ï¼Œæ¨ç†æ—¶é—´é•¿ï¼Œèƒ½åŠ›å—é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºRAG-R1è®­ç»ƒæ¡†æ¶ï¼Œä½¿LLMèƒ½è‡ªé€‚åº”åˆ©ç”¨å†…å¤–éƒ¨çŸ¥è¯†ã€‚</li>
<li>RAG-R1æ¡†æ¶æ‰©å±•äº†ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ï¼Œä»å•æŸ¥è¯¢æ¨¡å¼è½¬å˜ä¸ºå¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ã€‚</li>
<li>RAG-R1åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡æœ€å¼ºåŸºçº¿13.2%ï¼Œå¹¶ç¼©çŸ­æ¨ç†æ—¶é—´11.1%ã€‚</li>
<li>RAG-R1çš„æå‡ºä¸ºLLMçš„è¿›æ­¥æä¾›äº†æ–°çš„æ–¹å‘å’Œå¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9b03cd75bd4236512f375709e400284.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb36da1ac9a6f7eb88cdeb128952e69f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9337a9a38b641d5c82a6e77e0cb60a3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-69f1e105d34c0d7871615cd69ff366d0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  Adacc Adaptive Compression and Activation Checkpointing for LLM Memory   Management
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-89c4cfb4cd796460584be61d70764f53.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-05  SpA2V Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware   Video Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
