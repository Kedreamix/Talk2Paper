<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-08-05  Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9b52d6cf0dd744e4ee5028cbdcbf0318.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    44 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-05-更新"><a href="#2025-08-05-更新" class="headerlink" title="2025-08-05 更新"></a>2025-08-05 更新</h1><h2 id="Multi-Agent-Game-Generation-and-Evaluation-via-Audio-Visual-Recordings"><a href="#Multi-Agent-Game-Generation-and-Evaluation-via-Audio-Visual-Recordings" class="headerlink" title="Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings"></a>Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings</h2><p><strong>Authors:Alexia Jolicoeur-Martineau</strong></p>
<p>While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches. </p>
<blockquote>
<p>虽然人工智能在生成文本、音频、图像和视频方面表现出色，但在创建交互式视听内容（如电子游戏）方面仍然面临挑战。当前的LLM（大型语言模型）可以生成JavaScript游戏和动画，但缺乏自动评估指标，并且在需要由人类团队工作数月的复杂内容（多镜头、多代理）方面遇到困境，这些内容的资产通常由艺术家制作。为了解决这些问题，我们开发了一种新的指标和多代理系统。</p>
</blockquote>
<p>我们提出AVR-Eval，这是一种使用音频视觉录制（AVR）的多媒体内容质量相对指标。一种多模式模型（处理文本、视频和音频）比较两个内容的AVR，并使用文本模型审查评估来确定优劣。我们证明AVR-Eval能够正确识别优质内容与损坏或不匹配的内容。</p>
<p>我们构建了AVR-Agent，这是一个多代理系统，可以从多媒体资产库（音频、图像、3D模型）生成JavaScript代码。编码代理选择相关资产，生成多个初始代码，使用AVR-Eval来识别最佳版本，并通过AVR的跨模式代理反馈进行迭代改进。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00632v1">PDF</a> </p>
<p><strong>Summary</strong><br>     当前人工智能在生成文本、音频、图像和视频方面表现出色，但在创建如电子游戏等交互式音视频内容方面仍具挑战。为解决此问题，我们构建了新的度量标准——AVR-Eval，用于评估多媒体内容质量的相对指标，并利用音视频录制（AVRs）进行对比。我们还建立了AVR-Agent多智能体系统，可从多媒体资产库中生成JavaScript代码，通过AVR-Eval选择最佳资产版本并不断改进。实验表明，AVR-Agent生成的内容较一次生成的内容具有更高的胜率，但模型在利用自定义资产和AVR反馈方面存在困难，表明人类与机器的内容创作方式存在根本差异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前人工智能在生成交互式音视频内容如电子游戏方面存在挑战。</li>
<li>AVR-Eval是一种新的相对度量标准，用于评估多媒体内容质量。</li>
<li>AVR-Eval通过对比音视频录制（AVRs）来判断内容优劣。</li>
<li>AVR-Agent是一个多智能体系统，可以从多媒体资产库中生成JavaScript代码。</li>
<li>实验显示AVR-Agent生成的内容较一次生成的内容有更高的胜率。</li>
<li>模型在利用自定义资产和AVR反馈方面存在困难。</li>
<li>人工智能与人类在内容创作方面存在根本差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00632">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-931b845cb1405c20da1950aabcee0418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0db024a7bcc5e8ecc8e4d8f41953af7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9457fd8fe64ae1510bfb1055f9c80aab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc82eb8e265aa6d81396fc1da831cf3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ContestTrade-A-Multi-Agent-Trading-System-Based-on-Internal-Contest-Mechanism"><a href="#ContestTrade-A-Multi-Agent-Trading-System-Based-on-Internal-Contest-Mechanism" class="headerlink" title="ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism"></a>ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism</h2><p><strong>Authors:Li Zhao, Rui Sun, Zuoyou Jiang, Bo Yang, Yuxiao Bai, Mengting Chen, Xinyang Wang, Jing Li, Zuo Bai</strong></p>
<p>In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model’s constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent’s performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics. </p>
<blockquote>
<p>在金融交易领域，基于大型语言模型（LLM）的代理展现出巨大的潜力。然而，对市场噪声的高度敏感性削弱了LLM交易系统的性能。为了解决这一局限性，我们提出了一种新型的多代理系统，该系统具有受现代企业管理结构启发的内部竞争机制。该系统由两个专业团队组成：（1）数据团队——负责处理和压缩大量市场数据，将其转化为多样化的文本因素，确保它们符合模型的受限语境。（2）研究团队——负责基于深度研究方法制定并行多路径交易决策。核心创新在于在每个团队内部实施实时评估和排名机制，以真实的市场反馈为驱动。每个代理的表现会进行持续打分和排名，只有表现最佳的代理的输出才会被采用。这一设计使系统能够自适应地调整动态环境，增强对市场噪声的稳健性，并最终实现卓越的交易性能。实验结果表明，我们提出的系统在多种评估指标上显著优于现有的多代理系统和传统的量化投资方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在金融交易领域展现潜力，但对市场噪声的高度敏感性限制了其性能。为解决这一问题，提出了一种新型多智能体系统，采用内部竞争机制，灵感来源于现代企业管理制度。系统包括数据处理和研究团队，分别负责处理市场数据和做出交易决策。核心创新在于实施实时评估和排名机制，根据市场反馈动态调整系统，增强对噪声的稳健性，提高交易性能。实验表明，该系统优于其他多智能体系统和传统量化投资方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在金融交易领域具有显著潜力。</li>
<li>市场噪声对大型语言模型性能产生负面影响。</li>
<li>新型多智能体系统采用内部竞争机制，类似现代企业管理制度。</li>
<li>系统包括数据处理和研究团队，分别负责不同任务。</li>
<li>实时评估和排名机制根据市场反馈动态调整系统。</li>
<li>该系统增强了系统的适应性和稳健性，提高了交易性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-afbb0750bea927d5fb1da8722da8b3d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-266868a091c9e99deb2f01f35589ca21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ab372cf23bfc749e297c10b2bf1a614.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8281a829e28eaa4cea0e4f596804ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1788b12df0c71c9bb2eaa9c392f8e07c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Cognitive-Kernel-Pro-A-Framework-for-Deep-Research-Agents-and-Agent-Foundation-Models-Training"><a href="#Cognitive-Kernel-Pro-A-Framework-for-Deep-Research-Agents-and-Agent-Foundation-Models-Training" class="headerlink" title="Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent   Foundation Models Training"></a>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent   Foundation Models Training</h2><p><strong>Authors:Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu</strong></p>
<p>General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Tencent/CognitiveKernel-Pro">https://github.com/Tencent/CognitiveKernel-Pro</a> </p>
<blockquote>
<p>通用人工智能代理（AI Agents）越来越多地被认可作为下一代人工智能的基础框架，可实现复杂的推理能力、网页交互能力、编码能力和自主研究能力。然而，当前的大部分代理系统要么是闭源的，要么严重依赖于各种付费的API和专有工具，这给学术界带来了可访问性和再现性的局限性。在此研究中，我们推出了<strong>认知核心Pro（Cognitive Kernel-Pro）</strong>，这是一个完全开源的（在最大程度上）免费的多模块代理框架，旨在推动先进的人工智能代理的开发与评估走向大众化。在认知核心Pro中，我们系统地研究了高质量训练数据的整理，为代理基础模型提供重点支持，特别是在构建查询、轨迹和四大关键领域的可验证答案方面：网络、文件、代码和一般推理。此外，我们还探索了增强代理稳健性和性能的新型策略，包括代理测试时的反思和投票机制。我们在GAIA上对认知核心Pro进行了评估，在开源且免费的代理中取得了最先进的成果。值得注意的是，我们的参数为8B的开源模型超越了之前领先的WebDancer和WebSailor系统，为可访问的高能力人工智能代理建立了新的性能标准。代码可在<a target="_blank" rel="noopener" href="https://github.com/Tencent/CognitiveKernel-Pro">https://github.com/Tencent/CognitiveKernel-Pro</a> 中获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00414v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong></p>
<p>通用人工智能代理框架被视为下一代人工智能的基础框架，具有复杂的推理、网络交互、编码和自主研究能力。然而，当前代理系统多为闭源或依赖各种付费API和专有工具，限制了研究社区的可达性和可重复性。本文介绍了一款开源的代理框架Cognitive Kernel-Pro，旨在使高级人工智能代理的开发和评估民主化。框架重点探究高质量训练数据的筛选与采集，实现跨四域的问题提出、轨迹记录和可验证答案。通过测试和反射以及投票技术，提升代理的稳健性和性能。在GAIA上的评估显示，Cognitive Kernel-Pro表现卓越，其开源模型性能超越WebDancer和WebSailor等领先系统。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>通用AI代理框架成为下一代AI的基础，具备复杂推理、网络交互等能力。</li>
<li>当前代理系统存在闭源和依赖付费工具的问题，限制研究社区的发展。</li>
<li>Cognitive Kernel-Pro是一个开源、多模块的代理框架，旨在推进AI代理的民主化开发。</li>
<li>该框架注重高质量训练数据的筛选与采集，涉及四域的问题提出和答案构建。</li>
<li>通过测试和反射及投票技术提升代理的稳健性和性能。</li>
<li>Cognitive Kernel-Pro在GAIA上的表现卓越，超越其他领先系统。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-27ab2e589bf7a4588cfb72a6d153a9cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-553244b943abba4bb15bdc022e599592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab31d22030e4475d9d49231284b46bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-049c201f7cad2e52e712fbbc299c8d28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8622d94a41b029daadaae975357adb32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32eec58c75c68b392128d3df39165abd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Sari-Sandbox-A-Virtual-Retail-Store-Environment-for-Embodied-AI-Agents"><a href="#Sari-Sandbox-A-Virtual-Retail-Store-Environment-for-Embodied-AI-Agents" class="headerlink" title="Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents"></a>Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents</h2><p><strong>Authors:Janika Deborah Gajo, Gerarld Paul Merales, Jerome Escarcha, Brenden Ashley Molina, Gian Nartea, Emmanuel G. Maminta, Juan Carlos Roldan, Rowel O. Atienza</strong></p>
<p>We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks. Addressing a gap in retail-specific sim environments for embodied agent training, Sari Sandbox features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. We also introduce SariBench, a dataset of annotated human demonstrations across varied task difficulties. Our sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance. We conclude with benchmarks, performance analysis, and recommendations for enhancing realism and scalability. The source code can be accessed via <a target="_blank" rel="noopener" href="https://github.com/upeee/sari-sandbox-env">https://github.com/upeee/sari-sandbox-env</a>. </p>
<blockquote>
<p>我们推出Sari Sandbox，这是一个高保真、逼真的3D零售店模拟环境，用于评估实体代理在购物任务中的人类表现基准。为解决针对实体代理训练的零售特定模拟环境中的差距，Sari Sandbox具有超过250种交互式杂货商品，跨越三种店面配置，通过API进行控制。它支持虚拟现实（VR）的人类互动和一个由视觉语言模型（VLM）驱动的实体代理。我们还推出了SariBench数据集，其中包含各种难度任务的注释人类演示。我们的沙盘让实体代理能够浏览、检查并操作零售商品，提供与人类表现基准对比的基准。我们通过基准测试、性能分析和增强现实性和可扩展性的建议来得出结论。源代码可通过<a target="_blank" rel="noopener" href="https://github.com/upeee/sari-sandbox-env%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/upeee/sari-sandbox-env获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00400v1">PDF</a> 14 pages, accepted in ICCV 2025 Workshop on RetailVision</p>
<p><strong>Summary</strong></p>
<p>Sari Sandbox是一款高保真、逼真的三维零售店模拟环境，用于评估实体代理在购物任务中的表现与人类表现的基准测试。它弥补了零售特定模拟环境在实体代理训练方面的空白，拥有超过250种交互式杂货商品和三种店面配置，通过API进行控制。它支持虚拟现实（VR）进行人类互动以及通过视觉语言模型（VLM）驱动的实体代理。此外，还推出了SariBench数据集，其中包含各种难度任务的标注人类演示。此模拟环境使实体代理能够导航、检查和操作零售商品，并提供与人类表现的基准测试。有关代码可通过<a target="_blank" rel="noopener" href="https://github.com/upeee/sari-sandbox-env%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/upeee/sari-sandbox-env访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sari Sandbox是一个用于评估实体代理在零售环境中的购物任务性能的高保真模拟环境。</li>
<li>它提供了超过250种交互式杂货商品和三种不同的店面配置。</li>
<li>Sari Sandbox支持虚拟现实交互和通过视觉语言模型的实体代理。</li>
<li>SariBench数据集包含了各种难度级别的购物任务标注人类演示。</li>
<li>此模拟环境允许实体代理进行导航、检查商品和操作商品。</li>
<li>提供了与人类表现的基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00400">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-002ba962650ff7b84e64b183c2754d3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc1ae6cf70176c4cdbd403d6e055815e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f063eea36ccf5f61247ced5e6b0e0c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8efc18aaf77347ad53659af51ebecb47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e9773d5068026c0d8cf69b50df0a1b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2bba453c7e912296219f583bd67a513.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b2a942fbcb652d42cf3b1c100165101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b4c4324706a96a2bae1b713dc5406bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce0c74a19882c6db2bd1617f26f6d0f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Cued-Agent-A-Collaborative-Multi-Agent-System-for-Automatic-Cued-Speech-Recognition"><a href="#Cued-Agent-A-Collaborative-Multi-Agent-System-for-Automatic-Cued-Speech-Recognition" class="headerlink" title="Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech   Recognition"></a>Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech   Recognition</h2><p><strong>Authors:Guanjie Huang, Danny H. K. Tsang, Shan Yang, Guangzhi Lei, Li Liu</strong></p>
<p>Cued Speech (CS) is a visual communication system that combines lip-reading with hand coding to facilitate communication for individuals with hearing impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures and lip movements into text via AI-driven methods. Traditionally, the temporal asynchrony between hand and lip movements requires the design of complex modules to facilitate effective multimodal fusion. However, constrained by limited data availability, current methods demonstrate insufficient capacity for adequately training these fusion mechanisms, resulting in suboptimal performance. Recently, multi-agent systems have shown promising capabilities in handling complex tasks with limited data availability. To this end, we propose the first collaborative multi-agent system for ACSR, named Cued-Agent. It integrates four specialized sub-agents: a Multimodal Large Language Model-based Hand Recognition agent that employs keyframe screening and CS expert prompt strategies to decode hand movements, a pretrained Transformer-based Lip Recognition agent that extracts lip features from the input video, a Hand Prompt Decoding agent that dynamically integrates hand prompts with lip features during inference in a training-free manner, and a Self-Correction Phoneme-to-Word agent that enables post-process and end-to-end conversion from phoneme sequences to natural language sentences for the first time through semantic refinement. To support this study, we expand the existing Mandarin CS dataset by collecting data from eight hearing-impaired cuers, establishing a mixed dataset of fourteen subjects. Extensive experiments demonstrate that our Cued-Agent performs superbly in both normal and hearing-impaired scenarios compared with state-of-the-art methods. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/DennisHgj/Cued-Agent">https://github.com/DennisHgj/Cued-Agent</a>. </p>
<blockquote>
<p>提示语（CS）是一种视觉通信系统，它将唇读与手编码结合起来，为听力障碍者提供交流帮助。自动提示语识别（ACSR）旨在通过人工智能驱动的方法将CS手势和唇部动作转化为文字。传统上，手部和唇部动作之间的时间异步性需要设计复杂的模块以实现有效的多模态融合。然而，受限于数据的可用性，当前的方法在训练这些融合机制方面表现出不足的能力，导致性能不佳。最近，多智能体系统在处理有限数据可用性的复杂任务方面表现出了有前景的能力。为此，我们提出了第一个用于ACSR的协作多智能体系统，名为Cued-Agent。它集成了四个专业子智能体：一个基于多模态大型语言模型的Hand Recognition智能体，它采用关键帧筛选和CS专家提示策略来解码手部动作；一个预训练的基于Transformer的Lip Recognition智能体，它从输入视频中提取唇部特征；一个Hand Prompt Decoding智能体，它以无训练的方式在推理过程中动态整合手部提示和唇部特征；以及一个Self-Correction Phoneme-to-Word智能体，它通过语义细化首次实现从音素序列到自然语言句子的后处理和端到端转换。为了支持这项研究，我们从八名听力受损的cuers中收集数据，建立了一个包含十四名参与者的混合数据集。大量实验表明，与最先进的方法相比，我们的Cued-Agent在正常和听力受损场景中表现卓越。实现细节可在<a target="_blank" rel="noopener" href="https://github.com/DennisHgj/Cued-Agent%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DennisHgj/Cued-Agent找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00391v1">PDF</a> 9 pages</p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种名为Cued-Agent的自动Cued Speech（CS）识别系统。该系统运用多模态语言模型和Transformer模型等技术，结合唇读和手语编码，为听力受损人士提供沟通便利。Cued-Agent包含四个子代理，分别负责手语识别、唇特征提取、手语提示解码和语音到文字的自我修正。研究通过扩展现有的普通话CS数据集，建立了包含十四位主体的混合数据集。实验表明，与现有技术相比，Cued-Agent在正常和听力受损场景下的表现均表现优异。</p>
<p><strong>要点分析</strong></p>
<ul>
<li>Cued Speech是一种视觉沟通系统，通过结合唇读和手语编码来促进听力受损人士的沟通。</li>
<li>自动CS识别（ACSR）旨在将CS手势和唇动转化为文字，借助AI驱动的方法实现。</li>
<li>之前的方法受限于手部和唇部动作的时间异步性，需要设计复杂的模块进行多模态融合，但受限于数据有限，效果并不理想。</li>
<li>多代理系统在处理有限数据的复杂任务时表现出潜力。</li>
<li>Cued-Agent是首个针对ACSR的协作多代理系统，包含四个专门子代理，分别负责手语识别、唇特征提取、手语提示的动态集成和语音到文字的转换。</li>
<li>研究通过扩展普通话CS数据集建立了混合数据集，包括来自八名听力受损人士的数据。</li>
<li>实验表明，Cued-Agent的表现优于现有技术，无论是在正常还是听力受损的环境下。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00391">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-91401c64ed56d015332bb5704b98553b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-302d84cc63a58206662a58333363753a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aca5d580a4ee7817e57efd8fbf8ca933.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b15b425f3efb96d3794c16acd35737b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-190bf4f0a1cf0dd46821963981b9af45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c06663f20f47ddeb95835dda366ce09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b52d6cf0dd744e4ee5028cbdcbf0318.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Lucy-edgerunning-agentic-web-search-on-mobile-with-machine-generated-task-vectors"><a href="#Lucy-edgerunning-agentic-web-search-on-mobile-with-machine-generated-task-vectors" class="headerlink" title="Lucy: edgerunning agentic web search on mobile with machine generated   task vectors"></a>Lucy: edgerunning agentic web search on mobile with machine generated   task vectors</h2><p><strong>Authors:Alan Dao, Dinh Bach Vu, Alex Nguyen, Norapat Buppodom</strong></p>
<p>Small language models (SLMs) are inherently limited in knowledge-intensive tasks due to their constrained capacity. While test-time computation offers a path to enhanced performance, most approaches treat reasoning as a fixed or heuristic process. In this work, we propose a new paradigm: viewing the model’s internal reasoning, delimited by <think> and </think> tags, as a dynamic task vector machine. Rather than treating the content inside these tags as a mere trace of thought, we interpret the generation process itself as a mechanism through which the model \textbf{constructs and refines its own task vectors} on the fly. We developed a method to optimize this dynamic task vector machine through RLVR and successfully trained an agentic web-search model. We present Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models such as DeepSeek-V3. This demonstrates that small models can rival large ones when equipped with structured, self-constructed task reasoning. </p>
<blockquote>
<p>小型语言模型（SLM）由于其容量有限，在知识密集型任务中存在固有的局限性。虽然测试时的计算提供了提高性能的途径，但大多数方法都将推理视为一个固定或启发式的过程。在这项工作中，我们提出了一个新的范式：将模型内部的推理（由<think>和</think>标签限定）视为动态任务向量机。我们不再将这些标签内的内容仅仅视为思考的痕迹，而是将生成过程本身解释为一种机制，通过该机制，模型可以即时地<strong>构建和细化自己的任务向量</strong>。我们开发了一种通过RLVR优化这种动态任务向量机的方法，并成功训练了一个代理网页搜索模型。我们推出了Lucy，这是一个拥有1.7B参数的SLM，它利用这种动态推理机制和MCP集成，在SimpleQA基准测试上达到了78.3%的准确率，与诸如DeepSeek-V3等大型模型的性能相当。这证明当配备结构化的自我构建任务推理时，小型模型可以与大型模型相抗衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00360v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新观点，即将小型语言模型（SLM）的内部推理过程视为动态任务向量机。通过利用<think>和</think>标签限定的推理过程，该模型能在运行时构建和细化自己的任务向量。作者开发了一种优化方法RLVR并成功训练了agentic网络搜索模型Lucy，其利用这种动态推理机制在SimpleQA基准测试中实现了78.3%的准确率，与大型模型如DeepSeek-V3表现相当。这表明小型模型在配备结构化自我构建的任务推理机制后，可以与大型模型竞争。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>小型语言模型（SLM）在知识密集型任务上存在局限性。</li>
<li>现有方法通常将推理视为固定或启发式过程。</li>
<li>本文提出了一种新的观点，即将SLM的内部推理过程视为动态任务向量机。</li>
<li>作者利用了<think>和</think>标签来定义模型的动态任务向量。</li>
<li>作者开发了一种优化方法RLVR，并成功训练了一个名为Lucy的agentic网络搜索模型。</li>
<li>Lucy在SimpleQA基准测试中实现了78.3%的准确率，与大型模型表现相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cec52bbe36ac64b5f886ae5872367bdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82844a546069a6405a2b14a7af122a5a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PilotRL-Training-Language-Model-Agents-via-Global-Planning-Guided-Progressive-Reinforcement-Learning"><a href="#PilotRL-Training-Language-Model-Agents-via-Global-Planning-Guided-Progressive-Reinforcement-Learning" class="headerlink" title="PilotRL: Training Language Model Agents via Global Planning-Guided   Progressive Reinforcement Learning"></a>PilotRL: Training Language Model Agents via Global Planning-Guided   Progressive Reinforcement Learning</h2><p><strong>Authors:Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang</strong></p>
<p>Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model’s ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model’s planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale. </p>
<blockquote>
<p>面向代理任务的大型语言模型（LLMs）已经取得了显著的进步。尽管它们具有潜力，但在基于代理的环境中部署LLMs时，现有工作仍面临挑战。广泛采用的ReAct代理范式侧重于将单步推理与即时行动执行相结合，这限制了其在需要长期战略规划的复杂任务中的有效性。此外，在问题解决过程中，规划者和执行者之间的协调也是代理设计中的一个关键因素。另外，当前的方法主要依赖于监督微调，这往往导致模型记忆已建立的任务完成轨迹，从而在面对新的问题上下文时限制其泛化能力。为了应对这些挑战，我们引入了基于自适应全局规划的代理范式AdaPlan，旨在将高级显式指导与执行相结合，以支持有效的长期决策。基于这一范式，我们进一步提出了PilotRL，这是一个由渐进强化学习驱动的LLM代理全局规划引导训练框架。我们首先开发模型在解决代理任务时遵循全局计划显式指导的能力。然后在此基础上，我们专注于优化生成的计划的质量。最后，我们对模型的规划和执行协调进行联合优化。实验表明，PilotRL可以达到最新的性能水平，其中LLaMA3.1-8B-Instruct + PilotRL超越封闭源代码的GPT-4o达3.60%，而在相似的参数规模下，与GPT-4o-mini相比则显示出更大的提升，达到55.78%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00344v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>大型语言模型（LLM）在处理面向代理的任务方面取得了显著进展，但现有工作在将LLM部署在代理环境中时面临挑战。主流代理范式ReAct专注于将即时行动执行与单步推理相结合，这在处理复杂的长远战略计划任务方面有其局限性。此外，代理设计还需要考虑解决问题时规划者和执行者之间的协调问题。当前的方法主要依赖监督微调，这导致模型经常记忆既定的任务完成轨迹，限制了其在面对新情境时的泛化能力。为解决这些挑战，我们提出了自适应全局计划代理范式AdaPlan，旨在实现高级明确指导与执行之间的协同，以支持有效的长期决策。基于这一范式，我们进一步推出了PilotRL，这是一个由全局规划引导的大型语言模型代理训练框架，该框架通过渐进强化学习驱动。实验表明，PilotRL能够实现最先进的性能表现。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型在处理面向代理的任务方面取得了进展，但在部署于代理环境中时面临挑战。</li>
<li>当前代理范式如ReAct在处理复杂、需要长远规划的任务时存在局限性。</li>
<li>AdaPlan范式旨在通过实现高级明确指导与执行之间的协同，克服现有挑战，支持有效的长期决策。</li>
<li>PilotRL是一种基于AdaPlan的新型训练框架，通过渐进强化学习驱动全局规划引导的大型语言模型代理的训练。</li>
<li>PilotRL模型首先发展遵循全局计划明确指导的能力，然后在此基础上优化生成计划的质量。</li>
<li>实验结果显示PilotRL实现了最先进的性能表现，尤其是LLaMA3.1-8B-Instruct与PilotRL的结合表现超越了闭源的GPT-4o。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00344">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4eb7734bb824648fa5f88b4a386ba011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50203b192eb29e5275877d3043a67cdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-925faf6ae0f74b8279f9208cd3d13dd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b2804ac475cb2f0776b65ee4350c764.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f581113b533344f2160bfd230b49e1dd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UAV-ON-A-Benchmark-for-Open-World-Object-Goal-Navigation-with-Aerial-Agents"><a href="#UAV-ON-A-Benchmark-for-Open-World-Object-Goal-Navigation-with-Aerial-Agents" class="headerlink" title="UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial   Agents"></a>UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial   Agents</h2><p><strong>Authors:Jianqiang Xiao, Yuexuan Sun, Yixin Shao, Boxi Gan, Rongqiang Liu, Yanjing Wu, Weili Gua, Xiang Deng</strong></p>
<p>Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments. </p>
<blockquote>
<p>无人机导航是智能实体中一项基础却尚未被充分探索的能力，它使得代理能够在大规模、非结构化环境中运行，传统的导航模式在这些环境中表现不足。然而，现有的大多数研究都遵循视觉和语言导航（VLN）的模式，该模式严重依赖于连续的语言指令，限制了其可扩展性和自主性。为了解决这个问题，我们引入了UAV-ON，这是一个开放世界环境中无人机对象目标导航（ObjectNav）的基准测试，代理可以根据高级语义目标进行操作，而不必依赖VLN中的详细指令指导。UAV-ON包含14个高保真度的Unreal Engine环境，具有多样的语义区域和复杂的空间布局，涵盖城市、自然和混合用途场景。它定义了1270个注释目标对象，每个对象都可以通过实例级别的指令进行表征，这些指令包含类别、物理足迹和视觉描述符，允许基于现实情况的推理。这些指令作为语义目标，为无人机引入了现实模糊性和复杂的推理挑战。为了评估这个基准测试，我们实施了几种基准方法，包括Aerial ObjectNav Agent（AOA），这是一个模块化策略，能够整合指令语义与以自我为中心的观察结果，以实现长期、目标导向的探索。实验结果表明，所有基线在此设置中都面临困难，突显了无人机导航和语义目标定位的挑战性。UAV-ON旨在推动复杂现实环境中基于语义目标描述的无人机自主性研究的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00288v1">PDF</a> Accepted to ACM MM Dataset Track 2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了空中导航在智能体领域中的基本且未被充分探索的能力。传统的导航模式在大型非结构化环境中难以实现空中导航。因此，提出了一种新的大型目标导航任务，称为无人机目标对象导航（UAV-ON）。在此任务中，智能体不再依赖于详细的指令性指导，而是根据高级语义目标进行操作。该任务覆盖了城市、自然和混合用途等不同环境设置。现有方法在此任务上表现不佳，凸显了空中导航和语义目标接地的挑战。UAV-ON的目标是推进在复杂现实环境中实现无人机自主性的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>空中导航是智能体领域中的基本能力，但在大型非结构化环境中存在挑战。</li>
<li>传统导航模式在大型非结构化环境中难以实现空中导航。</li>
<li>UAV-ON是一种新型的大型目标导航任务，智能体基于高级语义目标进行操作。</li>
<li>UAV-ON任务涵盖了城市、自然和混合用途等不同环境设置。</li>
<li>UAV-ON引入了目标对象的实例级指令，包括类别、物理足迹和视觉描述符等信息。</li>
<li>现有方法在UAV-ON任务上表现不佳，凸显了空中导航和语义目标接地的挑战。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00288">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5e3c699a612ce7f277526dcc01abe2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6588344e385635bff4a85a1502857549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-665cf57b6e6180cafb3f5c8310ac5b2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f306caeb85df17078d97caa642e71172.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MetaAgent-Toward-Self-Evolving-Agent-via-Tool-Meta-Learning"><a href="#MetaAgent-Toward-Self-Evolving-Agent-via-Tool-Meta-Learning" class="headerlink" title="MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning"></a>MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</h2><p><strong>Authors:Hongjin Qian, Zheng Liu</strong></p>
<p>In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in <a target="_blank" rel="noopener" href="https://github.com/qhjqhj00/MetaAgent">https://github.com/qhjqhj00/MetaAgent</a>. </p>
<blockquote>
<p>在此工作中，我们提出了MetaAgent，这是一种受实践学习原则启发的代理范式，通过动手实践和持续的自我改进来发展专业知识。MetaAgent从一个最小的工作流程开始，仅具备基本的推理和自适应求助能力。当遇到知识空白时，MetaAgent会生成自然语言求助请求，由专用工具路由器将其路由到最合适的外部工具。随着MetaAgent完成任务，它会持续进行自我反思和答案验证，将可操作的经历提炼成简短的文本，并动态地融入未来的任务上下文。此外，MetaAgent会自主构建内部工具和持久的知识库，通过组织其工具使用历史来进一步提高其检索和整合相关信息的能力。我们将这种持续的数据驱动过程称为“元工具学习”，通过这个过程，MetaAgent能够逐步优化其推理和工具使用策略，无需更改模型参数或需要进行进一步的后续训练。在包括GAIA、WebWalkerQA和BrowseCamp等具有挑战性的知识发现基准测试上，MetaAgent始终优于基于工作流程的基线，并达到或超过了端到端训练的代理水平，证明了自我进化的代理系统在稳健、通用的知识发现方面的潜力。我们在<a target="_blank" rel="noopener" href="https://github.com/qhjqhj00/MetaAgent">https://github.com/qhjqhj00/MetaAgent</a>提供源代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00271v1">PDF</a> Technical Report, 14 pages</p>
<p><strong>Summary</strong><br>基于学习实践的元代理理念框架设计MetaAgent。具有最小的工作流程，基础推理能力和适应性求助机制，在实践中实现知识的增长和自反更新机制，且能够通过动态精炼的任务上下文自我总结发展出一套通用的实用指南和技巧知识库，同时通过求助生成的场景指导应用自动化检索精准所需知识进而自行进化改善使用工具和知识管理的通用模型方法。我们称之为元工具学习。在多个知识发现基准测试集上的表现证明了其优越性。相关代码已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MetaAgent遵循学习实践的元代理理念框架设计。</li>
<li>MetaAgent具有最小的工作流程，并具备基础推理和适应性求助机制。</li>
<li>MetaAgent在实践中通过自我反思和答案验证实现知识的增长和自反更新机制。</li>
<li>MetaAgent通过精炼的任务上下文总结实用指南和技巧知识库。</li>
<li>MetaAgent通过元工具学习改善使用工具和知识管理的通用模型方法。</li>
<li>MetaAgent能够在多个知识发现基准测试集上实现优越表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00271">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-adf6462c179b976d6c5ece6149f9703e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-079420601befd0fc648a4dd93cd4000c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8ea8ab07d0606fc62ff49d5b85cec0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dynamic-Knowledge-Exchange-and-Dual-diversity-Review-Concisely-Unleashing-the-Potential-of-a-Multi-Agent-Research-Team"><a href="#Dynamic-Knowledge-Exchange-and-Dual-diversity-Review-Concisely-Unleashing-the-Potential-of-a-Multi-Agent-Research-Team" class="headerlink" title="Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team"></a>Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team</h2><p><strong>Authors:Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang</strong></p>
<p>Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research. </p>
<blockquote>
<p>科技进步越来越依赖于研究者之间的有效协作，这一动态是大规模语言模型（LLM）才开始模仿的。虽然基于LLM的科学家代理在自主科学发现方面显示出前景，但它们通常缺乏真实世界研究中必不可少的交互式推理和评估机制。我们提出了IDVSCI（内部讨论与投票科学家），这是一个基于LLM的多代理框架，包含两项关键创新：动态知识交换机制，使代理之间能够进行迭代反馈；以及模拟异质专家评估的双重多样性审查模式。这些组件共同促进了更深入的推理和更具创造力和影响力的科学思想的产生。为了评估我们方法的有效性和通用性，我们在两个数据集上进行了实验：一个在计算机科学中广泛使用的基准测试，以及我们在健康科学领域引入的一个新数据集。结果表明，IDVSCI在两个数据集上均表现最佳，优于现有的AI科学家和VIRSCI系统。这些发现突显了在基于LLM的自主研究中建立互动和同行评审动态的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18348v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的科学家代理在自主科学发现方面展现出潜力，但仍缺乏交互式推理和评估机制。本文提出IDVSCI（内部讨论与投票科学家）多代理框架，通过动态知识交换机制和双差异审查范式，模拟专家评估并促进深度推理和创造性科学思想。实验结果表明，IDVSCI在跨学科的基准测试和健康科学领域的新数据集上均表现最佳，优于现有的AI科学家和VIRSCI系统。这凸显了模拟互动和同行评审动态在基于LLM的自主研究中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在模拟科学家间的协作方面显示出潜力，但缺乏真实科研中的交互式推理和评估机制。</li>
<li>IDVSCI框架包含两个关键创新点：动态知识交换机制和双差异审查范式。</li>
<li>动态知识交换机制促进代理间的迭代反馈。</li>
<li>双差异审查范式模拟不同专家的评估，提升深度推理和创造性科学想法的产生。</li>
<li>IDVSCI框架在跨学科基准测试及健康科学新数据集上的实验表现最佳。</li>
<li>IDVSCI相较于现有系统如AI科学家和VIRSCI具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18348">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-95522360003b08b68e4812f358e3ddc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-833a9a216ea7fc467c04b43bb1b71689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c16e80b0ab3f5086de3a9100c8a94c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee39cdbc40963e87f0e1a7c3efabe66.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MemInsight-Autonomous-Memory-Augmentation-for-LLM-Agents"><a href="#MemInsight-Autonomous-Memory-Augmentation-for-LLM-Agents" class="headerlink" title="MemInsight: Autonomous Memory Augmentation for LLM Agents"></a>MemInsight: Autonomous Memory Augmentation for LLM Agents</h2><p><strong>Authors:Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba</strong></p>
<p>Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks. </p>
<blockquote>
<p>大型语言模型（LLM）代理已经进化到可以智能地处理信息、做出决策和与用户或工具进行交互。一个关键的功能是融合了长期记忆能力，使得这些代理能够利用历史交互和知识。然而，不断增长的内存大小和语义结构的需求构成了重大挑战。在这项工作中，我们提出了一种自主记忆增强方法MemInsight，以增强语义数据表示和检索机制。通过利用对历史交互的自主增强，LLM代理能够提供更准确和情境化的响应。我们通过三种任务场景：对话推荐、问答和事件摘要，实证验证了我们的方法的有效性。在LLM-REDIAL数据集上，MemInsight将推荐的说服力提高了1,有助于提高对话的自然度和连贯性；在LoCoMo检索的召回率方面，它比RAG基线高出34%。我们的实验结果表明，MemInsight在多个任务中增强LLM代理的上下文性能方面具有潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21760v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）代理通过集成长期记忆能力，实现了信息的智能处理、决策以及与用户或工具的交互。然而，随着记忆规模的扩大和语义结构的需求增加，挑战也随之增加。本研究提出了一种自主记忆增强方法MemInsight，以提高语义数据表示和检索机制。通过利用自主增强对历史交互的增强，LLM代理可以给出更准确和情境化的响应。我们在三个任务场景中实证验证了所提出方法的有效性，包括对话推荐、问答和事件摘要。在LLM-REDIAL数据集上，MemInsight提高了推荐的说服性达14%。此外，它在LoCoMo检索的召回率方面优于RAG基线34%。结果表明，MemInsight具有增强LLM代理在多个任务中的上下文性能的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）代理通过集成长期记忆能力，实现了智能化处理信息和交互。</li>
<li>自主记忆增强方法MemInsight可以提高语义数据表示和检索机制。</li>
<li>MemInsight利用自主增强技术提高历史交互数据的使用效率，使得LLM代理给出更准确和情境化的响应。</li>
<li>在三个任务场景中验证了MemInsight的有效性，包括对话推荐、问答和事件摘要。</li>
<li>在LLM-REDIAL数据集上，MemInsight提高了推荐的说服性达14%。</li>
<li>MemInsight在LoCoMo检索的召回率方面优于RAG基线34%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d0f6331111e46f0e04a71c438e836ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a52ca5b93b500a2dce8b72bf934380e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5befc8f93ca6dd6bc634678c59a63b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-692e26cec2a17a7aec8ecbae17e06637.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-05/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5c6ec378f0573e36f37f75a0fe5d0198.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-05  Unraveling Hidden Representations A Multi-Modal Layer Analysis for   Better Synthetic Content Forensics
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-69f1e105d34c0d7871615cd69ff366d0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-05  Adacc Adaptive Compression and Activation Checkpointing for LLM Memory   Management
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
