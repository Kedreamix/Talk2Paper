<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-21  MGFI-Net A Multi-Grained Feature Integration Network for Enhanced   Medical Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-dc7f5a85a37c6456a0323059b9394121.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    46 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-21-更新"><a href="#2025-02-21-更新" class="headerlink" title="2025-02-21 更新"></a>2025-02-21 更新</h1><h2 id="MGFI-Net-A-Multi-Grained-Feature-Integration-Network-for-Enhanced-Medical-Image-Segmentation"><a href="#MGFI-Net-A-Multi-Grained-Feature-Integration-Network-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="MGFI-Net: A Multi-Grained Feature Integration Network for Enhanced   Medical Image Segmentation"></a>MGFI-Net: A Multi-Grained Feature Integration Network for Enhanced   Medical Image Segmentation</h2><p><strong>Authors:Yucheng Zeng</strong></p>
<p>Medical image segmentation plays a crucial role in various clinical applications. A major challenge in medical image segmentation is achieving accurate delineation of regions of interest in the presence of noise, low contrast, or complex anatomical structures. Existing segmentation models often neglect the integration of multi-grained information and fail to preserve edge details, which are critical for precise segmentation. To address these challenges, we propose a novel image semantic segmentation model called the Multi-Grained Feature Integration Network (MGFI-Net). Our MGFI-Net is designed with two dedicated modules to tackle these issues. First, to enhance segmentation accuracy, we introduce a Multi-Grained Feature Extraction Module, which leverages hierarchical relationships between different feature scales to selectively focus on the most relevant information. Second, to preserve edge details, we incorporate an Edge Enhancement Module that effectively retains and integrates boundary information to refine segmentation results. Extensive experiments demonstrate that MGFI-Net not only outperforms state-of-the-art methods in terms of segmentation accuracy but also achieves superior time efficiency, establishing it as a leading solution for real-time medical image segmentation. </p>
<blockquote>
<p>医学图像分割在各种临床应用中都扮演着至关重要的角色。医学图像分割面临的一个主要挑战是在噪声、低对比度或复杂解剖结构存在的情况下，实现对感兴趣区域的准确轮廓描绘。现有的分割模型往往忽视了多粒度信息的融合，且无法保留对精确分割至关重要的边缘细节。为了解决这些挑战，我们提出了一种新型的图像语义分割模型，称为多粒度特征融合网络（MGFI-Net）。我们的MGFI-Net设计了两个专用模块来解决这些问题。首先，为了提高分割精度，我们引入了多粒度特征提取模块，该模块利用不同特征尺度之间的层次关系，有选择地关注最相关的信息。其次，为了保留边缘细节，我们融入了一个边缘增强模块，该模块能够有效地保留和融合边界信息，以优化分割结果。大量实验表明，MGFI-Net不仅在分割精度上超越了最先进的方法，而且在时间效率上也表现出卓越的性能，使其成为实时医学图像分割的领先解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13808v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>医疗图像分割在临床应用中扮演着重要角色。现有分割模型在面临噪声、低对比度或复杂解剖结构等问题时，难以实现兴趣区域的准确分割。为解决此挑战，提出了一种名为Multi-Grained Feature Integration Network（MGFI-Net）的新型图像语义分割模型。MGFI-Net通过两个专用模块来解决这些问题：一是引入Multi-Grained特征提取模块，利用不同特征尺度间的层次关系，选择性关注最相关信息，提高分割精度；二是融入边缘增强模块，有效保留和整合边界信息，优化分割结果。实验证明，MGFI-Net不仅在分割精度上超越了现有先进方法，而且在时间效率上也表现出优势，成为实时医疗图像分割的领先解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医疗图像分割在临床应用中的重要性及其所面临的挑战。</li>
<li>现有分割模型在面临噪声、低对比度或复杂解剖结构时的局限性。</li>
<li>提出的Multi-Grained Feature Integration Network（MGFI-Net）模型通过两个专用模块解决这些挑战。</li>
<li>Multi-Grained特征提取模块利用不同特征尺度间的层次关系提高分割精度。</li>
<li>边缘增强模块保留和整合边界信息，优化分割结果。</li>
<li>MGFI-Net在分割精度和时间效率上的优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13808">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b98be27e100a07bb875cc004ac47bf94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-067f896faa5e3949c85ca826ab948488.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27f3b4a19c9c4948b9a6a31883763223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401bf3ab75319e67e63def3a694211ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92cb151be844913daca9208bb3e849f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c978ab5872dcd76c02a726aa97ef86.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention"><a href="#Medical-Image-Classification-with-KAN-Integrated-Transformers-and-Dilated-Neighborhood-Attention" class="headerlink" title="Medical Image Classification with KAN-Integrated Transformers and   Dilated Neighborhood Attention"></a>Medical Image Classification with KAN-Integrated Transformers and   Dilated Neighborhood Attention</h2><p><strong>Authors:Omid Nejati Manzari, Hojat Asgariandehkordi, Taha Koleilat, Yiming Xiao, Hassan Rivaz</strong></p>
<p>Convolutional networks, transformers, hybrid models, and Mamba-based architectures have demonstrated strong performance across various medical image classification tasks. However, these methods were primarily designed to classify clean images using labeled data. In contrast, real-world clinical data often involve image corruptions that are unique to multi-center studies and stem from variations in imaging equipment across manufacturers. In this paper, we introduce the Medical Vision Transformer (MedViTV2), a novel architecture incorporating Kolmogorov-Arnold Network (KAN) layers into the transformer architecture for the first time, aiming for generalized medical image classification. We have developed an efficient KAN block to reduce computational load while enhancing the accuracy of the original MedViT. Additionally, to counteract the fragility of our MedViT when scaled up, we propose an enhanced Dilated Neighborhood Attention (DiNA), an adaptation of the efficient fused dot-product attention kernel capable of capturing global context and expanding receptive fields to scale the model effectively and addressing feature collapse issues. Moreover, a hierarchical hybrid strategy is introduced to stack our Local Feature Perception and Global Feature Perception blocks in an efficient manner, which balances local and global feature perceptions to boost performance. Extensive experiments on 17 medical image classification datasets and 12 corrupted medical image datasets demonstrate that MedViTV2 achieved state-of-the-art results in 27 out of 29 experiments with reduced computational complexity. MedViTV2 is 44% more computationally efficient than the previous version and significantly enhances accuracy, achieving improvements of 4.6% on MedMNIST, 5.8% on NonMNIST, and 13.4% on the MedMNIST-C benchmark. </p>
<blockquote>
<p>卷积网络、变压器、混合模型和基于Mamba的架构已在各种医学图像分类任务中表现出强大的性能。然而，这些方法主要是为使用带标签数据对干净图像进行分类而设计的。相比之下，现实世界的临床数据通常涉及多中心研究独有的图像损坏问题，以及来自不同制造商的成像设备所产生的差异。在本文中，我们介绍了医疗视觉转换器（MedViTV2），这是一种新型架构，首次将Kolmogorov-Arnold网络（KAN）层融入变压器架构中，旨在实现通用医学图像分类。我们开发了一个高效的KAN块，以减少计算负载并提高原始MedViT的准确性。此外，为了抵消我们MedViT在扩大规模时的脆弱性，我们提出了一种增强的膨胀邻域注意力（DiNA），这是对高效融合点积注意力核的适应，能够捕获全局上下文并扩展接收场，以有效地扩展模型并解决特征崩溃问题。此外，还引入了一种分层混合策略，以有效的方式堆叠我们的局部特征感知和全局特征感知块，这可以平衡局部和全局特征感知以提高性能。在17个医学图像分类数据集和12个损坏医学图像数据集上的大量实验表明，MedViTV2在29次实验中的27次获得了最先进的成果，且计算复杂度有所降低。MedViTV2的计算效率比前一个版本提高了44%，并且在MedMNIST上提高了4.6%的准确率，在NonMNIST上提高了5.8%，在MedMNIST-C基准测试上提高了13.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13693v1">PDF</a> </p>
<p><strong>摘要</strong><br>    本文介绍了Medical Vision Transformer V2（MedViTV2）架构，该架构首次将Kolmogorov-Arnold网络（KAN）层融入transformer架构中，旨在实现通用的医学图像分类。通过开发高效的KAN块，减少了计算负载，提高了原始MedViT的准确性。为应对大型MedViT的脆弱性，提出了增强的膨胀邻域注意力（DiNA），能捕捉全局上下文并扩展感受野，有效扩展模型并解决特征崩溃问题。此外，还介绍了分层混合策略，以平衡局部和全局特征感知块，提高性能。在17个医学图像分类数据集和12个腐蚀医学图像数据集上的大量实验表明，MedViTV2在27次实验中获得最佳结果，计算复杂度降低。与前一版本相比，MedViTV2计算效率提高44%，在MedMNIST、NonMNIST和MedMNIST-C基准测试上的准确率分别提高4.6%、5.8%和13.4%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>MedViTV2结合了Kolmogorov-Arnold网络（KAN）层与transformer架构，为医学图像分类提供了新颖的解决方案。</li>
<li>通过引入高效的KAN块，提高了原始MedViT的准确性并降低了计算负载。</li>
<li>提出的增强Dilated Neighborhood Attention（DiNA）能够捕捉全局上下文并扩展感受野，从而有效扩展模型规模并解决特征崩溃问题。</li>
<li>采用了分层混合策略来平衡局部和全局特征感知块，进一步提升了性能。</li>
<li>在多个医学图像分类数据集上的实验表明，MedViTV2在计算效率提高的同时，实现了显著的性能提升。</li>
<li>MedViTV2在多个基准测试上达到了业界最佳性能，包括在MedMNIST、NonMNIST和MedMNIST-C上的准确率显著提高。</li>
<li>MedViTV2架构展现出在应对真实世界临床数据中独特的多中心研究图像腐蚀问题的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81896e600f040ab82142fa4fdc162a05.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87ef7c213bc8251ae28558a2f9ed90f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a125441db26833802fa272ba56ec384d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a2eeb32eb8cac77b34308440cd56739.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4725b8d1770df0430664a9a673701871.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Steven Wang, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mamba’’ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba’s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>在医疗保健领域，对三维（3D）医学图像的有效评估对于诊断和治疗实践至关重要。近年来，深度学习和计算机视觉在医学图像分析和解释方面的应用显著增加。传统方法，如卷积神经网络（CNNs）和视觉转换器（ViTs），面临重大的计算挑战，这促使了架构发展的需求。最近的努力导致了“Mamba”模型等新兴架构的出现，作为传统CNN或ViTs的替代解决方案。Mamba模型在处理一维数据的线性处理方面表现出色，计算需求较低。然而，Mamba在3D医学图像分析方面的潜力尚未得到充分探索，随着维度的增加，可能会面临重大的计算挑战。本手稿提出了MobileViM，这是一个用于高效分割3D医学图像的流线化架构。在MobileViM网络中，我们发明了一种新的维度独立机制和一种双向遍历方法，将其融入基于视觉Mamba的框架中。MobileViM还采用跨尺度桥梁技术，以提高不同医学成像模式的效率和准确性。通过这些增强功能，MobileViM在单个图形处理单元（即NVIDIA RTX 4090）上实现了超过每秒90帧（FPS）的分割速度。此性能比使用相同计算资源的处理3D图像的最先进深度学习模型的性能快24 FPS以上。此外，实验评估表明，MobileViM的性能卓越，在PENGWIN、BraTS2024、ATLAS和Toothfairy2数据集上的Dice相似度得分分别为92.72%、86.69%、80.46%和77.43%，显著超越了现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v1">PDF</a> The code is accessible through:   <a target="_blank" rel="noopener" href="https://github.com/anthonyweidai/MobileViM_3D/">https://github.com/anthonyweidai/MobileViM_3D/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的针对三维医学图像分析的模型MobileViM，该模型采用维度独立机制、双向遍历方法和跨尺度桥接技术，旨在提高计算效率和准确性。在多个数据集上的实验评估表明，MobileViM的性能优于现有模型，实现了高效的医学图像分割。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MobileViM是一个针对三维医学图像分析的模型，旨在提高计算效率和准确性。</li>
<li>MobileViM引入了维度独立机制和双向遍历方法来处理医学图像。</li>
<li>跨尺度桥接技术被用于改善不同成像模态的效率与准确性。</li>
<li>MobileViM实现了超过90帧每秒的分割速度，比现有模型快24帧以上。</li>
<li>在多个数据集上的实验评估显示，MobileViM的性能显著超过现有模型，Dice相似度得分高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-478c4a069cb045964697c54dfba709cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce4cf8702947ea6ba2c02dc49aa37123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e42796ec18c7321bee4b45d36a1c3825.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cd6b470a040419562bc91eea89ac5f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5f77b8f330ea05bdd1df377c514ad9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Enhancing-Chest-X-ray-Classification-through-Knowledge-Injection-in-Cross-Modality-Learning"><a href="#Enhancing-Chest-X-ray-Classification-through-Knowledge-Injection-in-Cross-Modality-Learning" class="headerlink" title="Enhancing Chest X-ray Classification through Knowledge Injection in   Cross-Modality Learning"></a>Enhancing Chest X-ray Classification through Knowledge Injection in   Cross-Modality Learning</h2><p><strong>Authors:Yang Yan, Bingqing Yue, Qiaxuan Li, Man Huang, Jingyu Chen, Zhenzhong Lan</strong></p>
<p>The integration of artificial intelligence in medical imaging has shown tremendous potential, yet the relationship between pre-trained knowledge and performance in cross-modality learning remains unclear. This study investigates how explicitly injecting medical knowledge into the learning process affects the performance of cross-modality classification, focusing on Chest X-ray (CXR) images. We introduce a novel Set Theory-based knowledge injection framework that generates captions for CXR images with controllable knowledge granularity. Using this framework, we fine-tune CLIP model on captions with varying levels of medical information. We evaluate the model’s performance through zero-shot classification on the CheXpert dataset, a benchmark for CXR classification. Our results demonstrate that injecting fine-grained medical knowledge substantially improves classification accuracy, achieving 72.5% compared to 49.9% when using human-generated captions. This highlights the crucial role of domain-specific knowledge in medical cross-modality learning. Furthermore, we explore the influence of knowledge density and the use of domain-specific Large Language Models (LLMs) for caption generation, finding that denser knowledge and specialized LLMs contribute to enhanced performance. This research advances medical image analysis by demonstrating the effectiveness of knowledge injection for improving automated CXR classification, paving the way for more accurate and reliable diagnostic tools. </p>
<blockquote>
<p>人工智能在医学成像方面的应用具有巨大的潜力，然而预训练知识与跨模态学习性能之间的关系仍不明确。本研究调查了将医学知识明确注入学习过程如何影响跨模态分类的性能，重点关注胸部X射线（CXR）图像。我们引入了一种基于集合理论的新型知识注入框架，该框架可控知识粒度，为CXR图像生成描述。利用此框架，我们在不同水平的医疗信息描述上对CLIP模型进行微调。我们在CheXpert数据集上通过零样本分类评估模型性能，这是CXR分类的基准测试。我们的结果表明，注入精细医学知识能显著提高分类准确率，达到72.5%，而使用人工生成的描述时仅为49.9%。这突显了领域特定知识在医学跨模态学习中的关键作用。此外，我们还探讨了知识密度以及专用领域大型语言模型（LLM）在描述生成中的影响，发现更密集的知识和专用的LLM有助于提升性能。本研究通过展示知识注入在提高自动CXR分类方面的有效性，推动了医学图像分析的发展，为更准确、更可靠的诊断工具铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13447v1">PDF</a> Accepted by ICASSP’25</p>
<p><strong>Summary</strong><br>     人工智能在医学成像中的融合展现出巨大潜力，但预训练知识与跨模态学习性能之间的关系尚不清楚。本研究调查了将医学知识明确注入学习过程如何影响跨模态分类的性能，重点关注胸部X光（CXR）图像。引入基于集合理论的知识注入框架，为CXR图像生成可控知识粒度的字幕。使用此框架对CLIP模型进行微调，并使用不同水平的医疗信息进行评估。在CXR分类的基准数据集CheXpert上进行零样本分类，结果显示注入精细医学知识可显著提高分类精度，达到72.5%，而使用人类生成字幕时为49.9%。这表明特定领域的专业知识在医学跨模态学习中起着至关重要的作用。此外，本研究还探讨了知识密度和用于字幕生成的特定领域大型语言模型（LLM）的影响，发现更密集的知识和专门的LLM有助于提升性能。本研究通过展示知识注入在提高自动化CXR分类中的有效性，为医学图像分析的发展铺平了道路，为更准确可靠的诊断工具打下基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在医学成像中具有巨大潜力。</li>
<li>预训练知识与跨模态学习性能之间的关系尚不清楚。</li>
<li>将医学知识注入学习过程可影响跨模态分类性能。</li>
<li>基于集合理论的知识注入框架生成CXR图像字幕。</li>
<li>注入精细医学知识显著提高CXR图像分类精度。</li>
<li>知识密度和特定领域的大型语言模型（LLM）对性能有积极影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13447">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2ccbc9e51f2b6fa927b64d8b7ff716a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-669f7b681c892270c797db543bd28ce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-143f4e64cbccda3015e5c5911ca4c37a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-291fe24541a5b4389ef55e4532a5a7a5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Timing-characterization-of-MALTA-and-MALTA2-pixel-detectors-using-Micro-X-ray-source"><a href="#Timing-characterization-of-MALTA-and-MALTA2-pixel-detectors-using-Micro-X-ray-source" class="headerlink" title="Timing characterization of MALTA and MALTA2 pixel detectors using Micro   X-ray source"></a>Timing characterization of MALTA and MALTA2 pixel detectors using Micro   X-ray source</h2><p><strong>Authors:G. Dash, P. Allport, I. Asensi Tortajada, P. Behera, D. V. Berlea, D. Bortoletto, C. Buttar, V. Dao, L. Fasselt, L. Flores Sanz de Acedo, M. Gazi, L. Gonella, V. Gonzalez, G. Gustavino, S. Haberl, T. Inada, P. Jana, L. Li, H. Pernegger, P. Riedler, W. Snoeys, C. A Solans Sanchez, M. van Rijnbach, M. Vazquez Nunez, A. Vijay, J. Weick, S. Worm</strong></p>
<p>The MALTA monolithic active pixel detector is developed to address the challenges anticipated in future high-energy physics detectors. As part of its characterization, we conducted fast-timing studies necessary to provide a figure of merit for this family of monolithic pixel detectors. MALTA has a metal layer in front-end electronics, and the conventional laser technique is not suitable for fast timing studies due to the reflection of the laser from the metallic surface. X-rays have been employed as a more effective alternative for penetration through these layers. The triggered X-ray set-up is designed to study timing measurements of monolithic detectors. The timing response of the X-ray set-up is characterized using an LGAD. The timing response of the MALTA and MALTA2 pixel detectors is studied, and the best response time of MALTA2 pixel detectors is measured at about 2 ns. </p>
<blockquote>
<p>MALTA单片有源像素探测器是为了应对未来高能物理探测器预期面临的挑战而开发的。作为表征的一部分，我们进行了快速时间研究，为这类单片像素探测器提供性能指标。MALTA前端电子器件中有一层金属，传统的激光技术由于激光在金属表面的反射而不适用于快速时间研究。X射线已被用作穿透这些层更有效的替代方法。触发式X射线装置旨在研究单片探测器的定时测量。X射线装置的定时响应使用LGAD进行表征。研究了MALTA和MALTA2像素探测器的定时响应，并测得MALTA2像素探测器的最佳响应时间为约2纳秒。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13330v1">PDF</a> 13 pages, 11 figures, To be submitted to JINST</p>
<p><strong>Summary</strong><br>     马耳他单片主动像素探测器专为应对未来高能物理探测器面临的挑战而开发。为评估此类单片像素探测器的性能，我们进行了快速时间测定研究。由于该探测器前端电子器件中存在金属层，传统激光技术不适用于快速时间测定。因此，采用X射线作为更有效的替代方案，以穿透这些金属层。触发式X射线装置用于研究单片探测器的定时测量。利用LGAD对X射线装置的定时响应进行了表征，并对马耳他和马耳他2号像素探测器的响应时间进行了研究，其中马耳他2号像素探测器的最佳响应时间约为2纳秒。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MALTA单片主动像素探测器是为了应对未来高能物理探测器面临的挑战而开发的。</li>
<li>快速时间测定研究是为了评估此类单片像素探测器的性能。</li>
<li>由于探测器中的金属层，传统激光技术不适用于快速时间测定。</li>
<li>X射线被用作更有效的替代方案，以穿透金属层。</li>
<li>触发式X射线装置用于研究单片探测器的定时测量。</li>
<li>LGAD被用来表征X射线装置的定时响应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13330">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-59999f15c0f67f9fb2a847eef4e83c65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d36f55489d548fcbc7b75c3064f4efa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-937464f96675c4a3f848e5f4749b8081.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed4d642adf982282f84046363e8321a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44d27bcb144f56410d02d6300c18fdb4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Limited-Angle-CT-Using-Deep-Priors-and-Regularization"><a href="#Data-Efficient-Limited-Angle-CT-Using-Deep-Priors-and-Regularization" class="headerlink" title="Data-Efficient Limited-Angle CT Using Deep Priors and Regularization"></a>Data-Efficient Limited-Angle CT Using Deep Priors and Regularization</h2><p><strong>Authors:Ilmari Vahteristo, Zhi-Song Liu, Andreas Rupp</strong></p>
<p>Reconstructing an image from its Radon transform is a fundamental computed tomography (CT) task arising in applications such as X-ray scans. In many practical scenarios, a full 180-degree scan is not feasible, or there is a desire to reduce radiation exposure. In these limited-angle settings, the problem becomes ill-posed, and methods designed for full-view data often leave significant artifacts. We propose a very low-data approach to reconstruct the original image from its Radon transform under severe angle limitations. Because the inverse problem is ill-posed, we combine multiple regularization methods, including Total Variation, a sinogram filter, Deep Image Prior, and a patch-level autoencoder. We use a differentiable implementation of the Radon transform, which allows us to use gradient-based techniques to solve the inverse problem. Our method is evaluated on a dataset from the Helsinki Tomography Challenge 2022, where the goal is to reconstruct a binary disk from its limited-angle sinogram. We only use a total of 12 data points–eight for learning a prior and four for hyperparameter selection–and achieve results comparable to the best synthetic data-driven approaches. </p>
<blockquote>
<p>从Radon变换重建图像是计算机断层扫描（CT）中的一个基本任务，出现在X射线扫描等应用中。在许多实际场景中，完整的180度扫描并不可行，或者希望减少辐射暴露。在这些有限角度设置中，问题变得不适定，专为全视图数据设计的方法通常会产生明显的伪影。我们提出了一种在严重角度限制下从Radon变换重建原始图像的低数据方法。由于反问题是不适定的，我们将多种正则化方法结合起来，包括总变差、辛诺图滤波器、深度图像先验和补丁级别的自动编码器。我们使用Radon变换的可微实现，这使我们能够使用基于梯度的方法来解决反问题。我们的方法在赫尔辛基断层扫描挑战赛2022的数据集上进行了评估，该比赛的目标是从有限的角度辛诺图中重建二进制磁盘。我们仅使用总共12个数据点（8个用于学习先验知识，4个用于超参数选择），并取得了与最佳合成数据驱动方法相当的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12293v2">PDF</a> 12 pages, 2 reference pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>文章讨论了CT中的图像重建问题，特别是在有限角度扫描的场景下。由于逆问题的不适定性，结合多种正则化方法（如全变分、辛格拉姆滤波器、深度图像先验和斑块级自编码器）进行图像重建。采用可微分的Radon变换实现方式，并利用梯度技术解决逆问题。在赫尔辛基断层扫描挑战赛2022的数据集上进行评估，使用少量数据点即可实现与最佳合成数据驱动方法相当的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章讨论了计算机断层扫描（CT）中的图像重建问题，特别是在有限角度扫描的场景下。</li>
<li>在有限角度设置下，问题变得不适定，专为全视角数据设计的方法通常会产生显著伪影。</li>
<li>提出了一种在严重角度限制下从Radon变换重建原始图像的低数据方法。</li>
<li>由于逆问题的不适定性，结合了多种正则化方法，包括全变分、辛格拉姆滤波器、深度图像先验和斑块级自编码器。</li>
<li>采用可微分的Radon变换实现方式，以便使用梯度技术解决逆问题。</li>
<li>在赫尔辛基断层扫描挑战赛2022的数据集上进行了评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7d2957f15548bbd1a95421990d34445d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5610d74ba60455e0339750a7a8e1e81e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd78d7f9622c23b9e99204ebecdb88c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates"><a href="#HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates" class="headerlink" title="HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates"></a>HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates</h2><p><strong>Authors:Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang</strong></p>
<p>Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complimentary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving indices map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates. </p>
<blockquote>
<p>在超低比特率下，图像压缩对于传统的图像压缩学习（LIC）和生成向量量化（VQ）建模仍然具有挑战性。传统LIC由于重度量化而产生严重伪影，而生成VQ建模由于学习到的生成先验与特定输入之间的不匹配而导致保真度较低。在这项工作中，我们提出了混合扩散图像压缩（HDCompression），这是一个双流传输框架，它结合了生成VQ建模和扩散模型以及传统的LIC，以实现高保真和高感知质量。与之前直接使用预训练的LIC模型从重度量化的潜在信息中产生低质量保真保持信息的混合方法不同，我们使用扩散模型从原始真实输入中提取高质量补充保真信息，这可以在多个方面提高系统性能：改进索引图预测，提高LIC流的保真保持输出，并通过对VQ潜在校正进行条件图像重建。此外，我们的扩散模型基于密集代表性向量（DRV），它轻便且具有非常简单的采样调度器。大量实验表明，我们的HDCompression在定量指标和定性可视化方面均优于以前的传统LIC、生成VQ建模和混合框架，在超低比特率下提供了平衡的稳健压缩性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07160v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>这篇论文提出了一种名为Hybrid-Diffusion Image Compression（HDCompression）的新的图像压缩方法。该方法结合了生成式VQ建模、传统的学习图像压缩和扩散模型，旨在实现高保真和高感知质量。与其他混合方法不同，它使用扩散模型从原始图像中提取高质量信息，以改进系统性能。实验结果证明，该方法在超低比特率下在定量指标和定性可视化方面均优于传统的LIC、生成式VQ建模和混合框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HDCompression是一种新的图像压缩方法，结合了生成式VQ建模、传统的学习图像压缩和扩散模型。</li>
<li>与其他混合方法不同，HDCompression使用扩散模型从原始图像中提取高质量信息，以提高系统性能。</li>
<li>HDCompression能提高索引图预测的准确性。</li>
<li>它能增强学习图像压缩流的保真度保留输出。</li>
<li>HDCompression能细化有条件的图像重建，并进行VQ潜伏校正。</li>
<li>该方法使用基于密集代表向量（DRV）的扩散模型，具有轻量级和非常简单的采样调度器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db750e019fc28828643d3500bf9dca90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a46c4a4b63e6126703b406339e85cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-839ebbafb28640c5cd2d041eb04adec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88b7851930ad7fc7f390c7fc29ef87d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4f26c7faad21302a27a4ae71a9f54c2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-New-Logic-For-Pediatric-Brain-Tumor-Segmentation"><a href="#A-New-Logic-For-Pediatric-Brain-Tumor-Segmentation" class="headerlink" title="A New Logic For Pediatric Brain Tumor Segmentation"></a>A New Logic For Pediatric Brain Tumor Segmentation</h2><p><strong>Authors:Max Bengtsson, Elif Keles, Gorkem Durak, Syed Anwar, Yuri S. Velichko, Marius G. Linguraru, Angela J. Waanders, Ulas Bagci</strong></p>
<p>In this paper, we present a novel approach for segmenting pediatric brain tumors using a deep learning architecture, inspired by expert radiologists’ segmentation strategies. Our model delineates four distinct tumor labels and is benchmarked on a held-out PED BraTS 2024 test set (i.e., pediatric brain tumor datasets introduced by BraTS). Furthermore, we evaluate our model’s performance against the state-of-the-art (SOTA) model using a new external dataset of 30 patients from CBTN (Children’s Brain Tumor Network), labeled in accordance with the PED BraTS 2024 guidelines and 2023 BraTS Adult Glioma dataset. We compare segmentation outcomes with the winning algorithm from the PED BraTS 2023 challenge as the SOTA model. Our proposed algorithm achieved an average Dice score of 0.642 and an HD95 of 73.0 mm on the CBTN test data, outperforming the SOTA model, which achieved a Dice score of 0.626 and an HD95 of 84.0 mm. Moreover, our model exhibits strong generalizability, attaining a 0.877 Dice score in whole tumor segmentation on the BraTS 2023 Adult Glioma dataset, surpassing existing SOTA. Our results indicate that the proposed model is a step towards providing more accurate segmentation for pediatric brain tumors, which is essential for evaluating therapy response and monitoring patient progress. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model">https://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model</a>. </p>
<blockquote>
<p>本文介绍了一种受专家放射科医生分割策略启发的新型深度学习架构，用于分割儿童脑肿瘤。我们的模型能够区分四种不同的肿瘤标签，并在PED BraTS 2024测试集（即BraTS引入的儿童脑肿瘤数据集）上进行基准测试。此外，我们使用符合PED BraTS 2024指南和2023年BraTS成人胶质瘤数据集的CBTN（儿童脑肿瘤网络）新外部数据集30名患者的数据来评估我们模型的表现。我们将分割结果与PED BraTS 2023挑战的获胜算法作为最先进的模型进行比较。我们提出的算法在CBTN测试数据上实现了平均Dice系数为0.642和HD95为73.0毫米，优于最先进的模型（其Dice系数为0.626，HD95为84.0毫米）。此外，我们的模型具有很强的泛化能力，在BraTS 2023成人胶质瘤数据集的全肿瘤分割上获得了0.877的Dice系数，超越了现有的最先进模型。我们的结果表明，所提出的模型是朝着为儿童脑肿瘤提供更精确分割的方向迈出的一步，这对于评估治疗反应和监测患者进展至关重要。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model">https://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model</a>处获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01390v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于深度学习的新方法，用于分割儿童脑肿瘤。该方法受到专家放射科医生分割策略的启发，能明确区分四种不同的肿瘤标签。实验结果显示，该方法在儿童脑肿瘤数据集上的性能优于现有最先进的模型，具备更强的泛化能力。此外，该模型源代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种用于分割儿童脑肿瘤的新方法，基于深度学习架构。</li>
<li>该方法受到专家放射科医生分割策略的启发，可区分四种不同的肿瘤标签。</li>
<li>在PED BraTS 2024测试集上的性能评估显示，该方法优于现有最先进的模型。</li>
<li>该模型在CBTN测试数据上的平均Dice得分为0.642，HD95为73.0mm，而最先进的模型的Dice得分为0.626，HD95为84.0mm。</li>
<li>该模型具有良好的泛化能力，在BraTS 2023成人胶质瘤数据集上的整体肿瘤分割Dice得分达到0.877，超过了现有最先进的模型。</li>
<li>该模型为更准确地分割儿童脑肿瘤提供了可能，这对评估治疗效果和监测患者进展至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.01390">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fb4f878d83b1b9e6aa694ee51c66c186.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bfd45c3337d2d055d98833864277115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5af341952c17f1aa07ed17c6653830.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-21\./crop_医学图像/2411.01390v3/page_2_1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5e764afa3bab82c2c05f9aff9ebfb21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8599962d65e5a97846d830e0089be95a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MedIAnomaly-A-comparative-study-of-anomaly-detection-in-medical-images"><a href="#MedIAnomaly-A-comparative-study-of-anomaly-detection-in-medical-images" class="headerlink" title="MedIAnomaly: A comparative study of anomaly detection in medical images"></a>MedIAnomaly: A comparative study of anomaly detection in medical images</h2><p><strong>Authors:Yu Cai, Weiwen Zhang, Hao Chen, Kwang-Ting Cheng</strong></p>
<p>Anomaly detection (AD) aims at detecting abnormal samples that deviate from the expected normal patterns. Generally, it can be trained merely on normal data, without a requirement for abnormal samples, and thereby plays an important role in rare disease recognition and health screening in the medical domain. Despite the emergence of numerous methods for medical AD, the lack of a fair and comprehensive evaluation causes ambiguous conclusions and hinders the development of this field. To address this problem, this paper builds a benchmark with unified comparison. Seven medical datasets with five image modalities, including chest X-rays, brain MRIs, retinal fundus images, dermatoscopic images, and histopathology images, are curated for extensive evaluation. Thirty typical AD methods, including reconstruction and self-supervised learning-based methods, are involved in comparison of image-level anomaly classification and pixel-level anomaly segmentation. Furthermore, for the first time, we systematically investigate the effect of key components in existing methods, revealing unresolved challenges and potential future directions. The datasets and code are available at <a target="_blank" rel="noopener" href="https://github.com/caiyu6666/MedIAnomaly">https://github.com/caiyu6666/MedIAnomaly</a>. </p>
<blockquote>
<p>异常检测（AD）旨在检测偏离预期正常模式的异常样本。通常，它仅能在正常数据上进行训练，无需异常样本，因此在医学领域的罕见疾病识别和健康筛查中发挥着重要作用。尽管出现了许多医学异常检测方法，但由于缺乏公平和全面的评估，导致结论模糊并阻碍了该领域的发展。针对这一问题，本文建立了一个统一的比较基准。为进行广泛评估，整理了七个医学数据集，包含五种图像模态，包括胸部X射线、脑部MRI、眼底视网膜图像、皮肤镜图像和病理图像。三十种典型的异常检测方法，包括重建和基于自监督学习的方法，都参与了图像级异常分类和像素级异常分割的比较。此外，我们首次系统地研究了现有方法中的关键组件的影响，揭示了未解决的挑战和潜在的未来方向。数据集和代码可通过以下链接获取：&lt;<a target="_blank" rel="noopener" href="https://github.com/caiyu666">https://github.com/caiyu666</a> 您的文本翻译完毕。请继续提问以获取更多帮助。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.04518v4">PDF</a> Accepted to Medical Image Analysis, 2025</p>
<p><strong>Summary</strong><br>医学异常检测（AD）对于检测偏离预期正常模式的异常样本至关重要。本文建立了一个统一的基准测试，包含七个医学数据集和五种图像模态，用于全面评估异常检测方法。涉及图像级别的异常分类和像素级别的异常分割的三十种典型AD方法被进行比较。此外，本文首次系统地探讨了现有方法中的关键组成部分，揭示了未解决的挑战和潜在的未来方向。数据集和代码可在GitHub上获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>异常检测（AD）在医学领域对于识别罕见疾病和健康筛查具有重要意义，能够检测偏离预期正常模式的异常样本。</li>
<li>文章建立了一个统一的基准测试，包含七个医学数据集和五种图像模态，为解决医学AD领域缺乏公平、全面的评估提供了方案。</li>
<li>涉及图像级别的异常分类和像素级别的异常分割的三十种典型AD方法被比较和评估。</li>
<li>文章首次系统地探讨了现有AD方法中的关键组成部分，分析各方法的优势和劣势。</li>
<li>文章揭示了医学AD领域存在的未解决挑战和潜在未来方向，为研究者提供了方向指引。</li>
<li>数据集和代码已公开，便于其他研究者使用和改进。</li>
<li>该研究为医学异常检测的进一步发展奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.04518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-86b7035ec75457573e210d051731f85f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a67476137f268c4786ae862f8b48361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fe445d8de1f208b1d0f28fbf997279e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-deaccb322f740aef11c46f4bc08f54f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c32cf16e8fe551268bf4785007c7b89.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="E2ENet-Dynamic-Sparse-Feature-Fusion-for-Accurate-and-Efficient-3D-Medical-Image-Segmentation"><a href="#E2ENet-Dynamic-Sparse-Feature-Fusion-for-Accurate-and-Efficient-3D-Medical-Image-Segmentation" class="headerlink" title="E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D   Medical Image Segmentation"></a>E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D   Medical Image Segmentation</h2><p><strong>Authors:Boqian Wu, Qiao Xiao, Shiwei Liu, Lu Yin, Mykola Pechenizkiy, Decebal Constantin Mocanu, Maurice Van Keulen, Elena Mocanu</strong></p>
<p>Deep neural networks have evolved as the leading approach in 3D medical image segmentation due to their outstanding performance. However, the ever-increasing model size and computation cost of deep neural networks have become the primary barrier to deploying them on real-world resource-limited hardware. In pursuit of improving performance and efficiency, we propose a 3D medical image segmentation model, named Efficient to Efficient Network (E2ENet), incorporating two parametrically and computationally efficient designs. i. Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse informative multi-scale features while reducing redundancy. ii. Restricted depth-shift in 3D convolution: it leverages the 3D spatial information while keeping the model and computational complexity as 2D-based methods. We conduct extensive experiments on BTCV, AMOS-CT and Brain Tumor Segmentation Challenge, demonstrating that E2ENet consistently achieves a superior trade-off between accuracy and efficiency than prior arts across various resource constraints. E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while saving over 68% parameter count and 29% FLOPs in the inference phase, compared with the previous best-performing method. Our code has been made available at: <a target="_blank" rel="noopener" href="https://github.com/boqian333/E2ENet-Medical">https://github.com/boqian333/E2ENet-Medical</a>. </p>
<blockquote>
<p>深度神经网络因其卓越性能而逐渐演变为3D医学图像分割领域的主要方法。然而，深度神经网络不断增长的模型大小和计算成本已成为将其部署在现实世界资源受限的硬件上的主要障碍。为了提升性能和效率，我们提出了一种名为Efficient to Efficient Network (E2ENet)的3D医学图像分割模型，它融合了两种参数和计算效率高的设计。一是动态稀疏特征融合（DSFF）机制：它自适应地学习融合信息丰富的多尺度特征，同时减少冗余。二是3D卷积中的受限深度移位：它在利用3D空间信息的同时，保持模型与计算复杂度类似于基于二维的方法。我们在BTCV、AMOS-CT和脑肿瘤分割挑战等数据集上进行了大量实验，证明E2ENet在各种资源限制下始终在精度和效率之间达到优于先前技术的权衡。在大型挑战AMOS-CT上，E2ENet的准确度与之前的最佳方法相当，同时在推理阶段节省了超过68%的参数计数和29%的浮点运算次数。我们的代码已在以下网址公开：<a target="_blank" rel="noopener" href="https://github.com/boqian333/E2ENet-Medical%E3%80%82">https://github.com/boqian333/E2ENet-Medical。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.04727v2">PDF</a> Accepted at NeurIPS 2024</p>
<p><strong>Summary</strong><br>     高效神经网络（Efficient to Efficient Network，E2ENet）在医学图像三维分割领域表现出卓越性能，采用动态稀疏特征融合和受限深度位移设计以提高效率和准确性。该模型在多个数据集上实现优异性能，减少参数数量和计算量，相较于先前技术具有更佳的准确性和效率权衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络在医学图像三维分割领域占据领先地位，但模型体积和计算成本日益增加，成为资源受限硬件上部署的障碍。</li>
<li>提出名为Efficient to Efficient Network (E2ENet)的医学图像三维分割模型，旨在提高性能和效率。</li>
<li>E2ENet采用动态稀疏特征融合（DSFF）机制，可自适应地融合信息多尺度特征并减少冗余。</li>
<li>E2ENet引入受限深度位移的3D卷积，利用3D空间信息同时保持模型及计算复杂度类似于2D方法。</li>
<li>广泛实验表明，E2ENet在各种资源约束下较先前技术实现了更高的准确性和效率之间的平衡。</li>
<li>在大规模挑战AMOS-CT上，E2ENet实现了与最佳表现方法相当的准确度，同时在推理阶段节省了超过68%的参数计数和29%的FLOPs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.04727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d999c5e6af3f87ab67b8a808fccb1c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-567b9787ed58c386b16789a35384473b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c08653bb4134ec3360d46487843f958e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72fb9a32c0b3dd7dcc13d5128acbda0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc7f5a85a37c6456a0323059b9394121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bf08072c5e593c34766c9810dcdd1f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3702b3c0f5935ec0f8b6ba1577348917.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9f5911f7ff3f63bb61d925eaf4d274d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-undesired-emergent-behaviors-in-compound-prostate-cancer-detection-systems"><a href="#On-undesired-emergent-behaviors-in-compound-prostate-cancer-detection-systems" class="headerlink" title="On undesired emergent behaviors in compound prostate cancer detection   systems"></a>On undesired emergent behaviors in compound prostate cancer detection   systems</h2><p><strong>Authors:Erlend Sortland Rolfsnes, Philip Thangngat, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</strong></p>
<p>Artificial intelligence systems show promise to aid in the di- agnostic pathway of prostate cancer (PC), by supporting radiologists in interpreting magnetic resonance images (MRI) of the prostate. Most MRI-based systems are designed to detect clinically significant PC le- sions, with the main objective of preventing over-diagnosis. Typically, these systems involve an automatic prostate segmentation component and a clinically significant PC lesion detection component. In spite of the compound nature of the systems, evaluations are presented assum- ing a standalone clinically significant PC detection component. That is, they are evaluated in an idealized scenario and under the assumption that a highly accurate prostate segmentation is available at test time. In this work, we aim to evaluate a clinically significant PC lesion de- tection system accounting for its compound nature. For that purpose, we simulate a realistic deployment scenario and evaluate the effect of two non-ideal and previously validated prostate segmentation modules on the PC detection ability of the compound system. Following, we com- pare them with an idealized setting, where prostate segmentations are assumed to have no faults. We observe significant differences in the de- tection ability of the compound system in a realistic scenario and in the presence of the highest-performing prostate segmentation module (DSC: 90.07+-0.74), when compared to the idealized one (AUC: 77.93 +- 3.06 and 84.30+- 4.07, P&lt;.001). Our results depict the relevance of holistic evalu- ations for PC detection compound systems, where interactions between system components can lead to decreased performance and degradation at deployment time. </p>
<blockquote>
<p>人工智能系统在前列腺癌（PC）诊断路径中显示出巨大潜力，支持放射科医生解释前列腺的磁共振图像（MRI）。大多数基于MRI的系统旨在检测临床重要的PC病变，主要目的是防止过度诊断。这些系统通常包括一个自动前列腺分割组件和一个临床重要的PC病变检测组件。尽管这些系统的复合性质，但评估是基于一个独立的临床重要PC检测组件进行的。也就是说，它们在理想化的场景中进行了评估，并假设测试时存在高度准确的前列腺分割。在这项工作中，我们旨在评估临床重要的PC病变检测系统，并考虑到其复合性质。为此，我们模拟了一个现实部署场景，并评估了两个经过验证的非理想前列腺分割模块对复合系统检测PC能力的影响。之后，我们将它们与理想化设置进行比较，其中假设前列腺分段没有故障。我们观察到，在现实场景中以及在最高性能的前列腺分割模块（DSC：90.07±0.74）的存在下，复合系统的检测能力与理想情况下的检测能力相比存在显著差异（AUC：77.93±3.06和84.30±4.07，P&lt;.001）。我们的结果描绘了全面评估PC检测复合系统的重要性，系统组件之间的相互作用可能导致性能下降并在部署时发生退化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.08381v2">PDF</a> Accepted in MICCAI 2025, CapTiON</p>
<p><strong>摘要</strong><br>     人工智能系统有望辅助前列腺癌的诊断过程，通过支持放射科医生解读前列腺磁共振成像。多数基于MRI的系统旨在检测具有临床意义的前列腺病灶，主要目的是防止过度诊断。这些系统通常包括自动前列腺分割组件和检测具有临床意义的前列腺病灶组件。尽管这些系统的复合性质，但评估是在假设一个独立的前列腺癌检测组件的情况下进行的。也就是说，它们在理想化的场景和假设高度准确的前列腺分割在测试时可用的情况下进行评估。在这项工作中，我们旨在评估一个考虑到其复合性质的前列腺癌检测系统。为此，我们模拟了一个真实的部署场景，并评估了两个先前验证的非理想前列腺分割模块对复合系统检测前列腺癌能力的影响。之后，我们将它们与理想化的设置进行比较，其中假设前列腺分段没有故障。我们观察到复合系统在现实场景中的检测能力与表现最佳的前列腺分割模块存在显著差异（DSC：90.07±0.74），与理想化情况相比（AUC：77.93±3.06和84.30±4.07，P&lt;.001）。我们的结果表明，对前列腺癌检测复合系统进行整体评估具有重要意义，系统组件之间的相互作用可能导致性能下降和部署时的退化。 </p>
<p> <strong>关键见解</strong></p>
<ol>
<li>人工智能系统在解读前列腺磁共振成像方面表现出辅助诊断前列腺癌的潜力。</li>
<li>基于MRI的系统主要目标是检测具有临床意义的前列腺病灶，旨在避免过度诊断。</li>
<li>这些系统包括自动前列腺分割和检测组件，但之前的评估大多假设了独立的前列腺癌检测组件。</li>
<li>在模拟的实际部署场景中评估了复合系统的前列腺癌检测能力，考虑了非理想的前列腺分割模块的影响。</li>
<li>与理想化设置相比，现实场景中的系统检测性能存在显著差异。</li>
<li>最好的前列腺分割模块对复合系统的检测能力有显著影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.08381">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-684acd64eae009a0135f5fdf2cbdbc02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-155e1a1a2184c91b1c1d3ad8c159d84f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb9f1d2738fcdc32e71b6db19148331c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-02-21  A Survey on Bridging EEG Signals and Generative AI From Image and Text   to Beyond
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a79161c4df0531e7c72cec0b3bfecb1f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-21  Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11676k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
