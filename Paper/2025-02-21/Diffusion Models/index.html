<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-21  Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a79161c4df0531e7c72cec0b3bfecb1f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-21-更新"><a href="#2025-02-21-更新" class="headerlink" title="2025-02-21 更新"></a>2025-02-21 更新</h1><h2 id="Secure-and-Efficient-Watermarking-for-Latent-Diffusion-Models-in-Model-Distribution-Scenarios"><a href="#Secure-and-Efficient-Watermarking-for-Latent-Diffusion-Models-in-Model-Distribution-Scenarios" class="headerlink" title="Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios"></a>Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios</h2><p><strong>Authors:Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu</strong></p>
<p>Latent diffusion models have exhibited considerable potential in generative tasks. Watermarking is considered to be an alternative to safeguard the copyright of generative models and prevent their misuse. However, in the context of model distribution scenarios, the accessibility of models to large scale of model users brings new challenges to the security, efficiency and robustness of existing watermark solutions. To address these issues, we propose a secure and efficient watermarking solution. A new security mechanism is designed to prevent watermark leakage and watermark escape, which considers watermark randomness and watermark-model association as two constraints for mandatory watermark injection. To reduce the time cost of training the security module, watermark injection and the security mechanism are decoupled, ensuring that fine-tuning VAE only accomplishes the security mechanism without the burden of learning watermark patterns. A watermark distribution-based verification strategy is proposed to enhance the robustness against diverse attacks in the model distribution scenarios. Experimental results prove that our watermarking consistently outperforms existing six baselines on effectiveness and robustness against ten image processing attacks and adversarial attacks, while enhancing security in the distribution scenarios. </p>
<blockquote>
<p>潜在扩散模型在生成任务中展现出了巨大的潜力。水印被视为保护生成模型版权并防止其被滥用的替代方案。然而，在模型分发场景中，模型对大量用户的可访问性给现有水印解决方案的安全性、效率和稳健性带来了新的挑战。为了解决这些问题，我们提出了一种安全高效的水印解决方案。设计了一种新的安全机制，以防止水印泄露和水印逃逸，该机制将水印随机性和水印与模型的关联作为强制水印注入的两个约束条件。为了减少训练安全模块的时间成本，我们将水印注入与安全机制解耦，确保微调VAE只完成安全机制，而无需学习水印模式的负担。提出了一种基于水印分布的策略，以提高模型分发场景中各种攻击的稳健性。实验结果表明，我们的水印在有效性和对十种图像处理攻击和对抗性攻击的稳健性方面始终优于现有的六种基线方法，同时提高了分发场景中的安全性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13345v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>潜在扩散模型在生成任务中展现出巨大潜力，而水印技术用于保护生成模型的版权并防止滥用。但在模型分发场景下，模型的大规模用户访问给现有水印解决方案的安全性、效率和稳健性带来新挑战。为此，我们提出了一种安全高效的水印解决方案，设计新的安全机制防止水印泄露和逃逸，以水印随机性和水印与模型的关联为强制注入的两个约束。为提高安全模块训练的时间成本，将水印注入与安全机制解耦，确保微调VAE只实现安全机制，无需学习水印模式。提出基于水印分布的验证策略，提高模型分发场景中对抗各种攻击的稳健性。实验证明，我们的水印技术在有效性、对抗十种图像处理攻击和对抗攻击的稳健性方面优于现有六种基线技术，同时在分发场景中提高安全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>潜在扩散模型在生成任务中有巨大潜力，但版权保护面临挑战。</li>
<li>水印技术用于保护生成模型的版权并防止滥用。</li>
<li>模型分发场景下，大规模用户访问给水印解决方案带来新挑战。</li>
<li>提出一种安全高效的水印解决方案，包括设计新的安全机制防止水印泄露和逃逸。</li>
<li>水印随机性和水印与模型的关联是强制水印注入的两个重要约束。</li>
<li>将水印注入与安全机制解耦，以提高训练效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13345">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-83c7279df1b7bd8d6890c28badcf7f38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5148117dc9b5eacd02445e92a4a383e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87c10cd5ab276f45b647ffaf1bd831bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88f995544b688ba8892fa327f4f40e4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-affbefa61a2fee11a71ec9e862376c06.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond"><a href="#A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond" class="headerlink" title="A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond"></a>A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond</h2><p><strong>Authors:Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury</strong></p>
<p>Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction. </p>
<blockquote>
<p>脑机接口（BCI）与生成式人工智能（GenAI）的融合为脑信号解码开辟了新的领域，实现了辅助通信、神经表征学习和多模态融合。特别是利用脑电图（EEG）的脑机接口，提供了一种将神经活动转化为有意义输出的非侵入性手段。深度学习领域的最新进展，包括生成对抗网络（GANs）和基于Transformer的大型语言模型（LLMs），显著提高了基于EEG的图像、文本和语音生成能力。本文综述了基于EEG的多模态生成的最新进展，重点关注（i）通过GANs、变分自动编码器（VAEs）和扩散模型实现EEG到图像生成，以及（ii）利用基于Transformer的语言模型和对比学习方法实现EEG到文本生成。此外，我们还讨论了新兴的EEG到语音合成领域，这是一个不断发展的多模态前沿领域。本文重点介绍了关键数据集、用例、挑战和支撑生成方法的EEG特征编码方法。通过对基于EEG的生成式人工智能进行结构化概述，本综述旨在为研究人员和实践者提供洞察，以促进神经解码的发展，提高辅助技术的性能，并拓展脑机交互的边界。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12048v2">PDF</a> </p>
<p><strong>Summary</strong><br>     脑机接口（BCI）与生成式人工智能（GenAI）的融合为脑信号解码开创了新领域，推动了辅助通信、神经表征学习和多模式融合的发展。脑电图（EEG）为基础的BCI为非侵入式地转化神经活动为有意义输出提供了手段。深度学习领域的最新进展，包括生成对抗网络（GANs）、基于Transformer的大型语言模型（LLMs）等，已显著改善基于EEG的图像、文本和语音生成。本文综述了EEG基多模式生成的最新进展，包括EEG转图像生成和EEG转文本生成，并探讨了新兴的EEG语音合成领域。通过提供EEG基生成式AI的结构性概览，旨在为研究者和实践者提供洞察，以推动神经解码、辅助技术发展和拓展脑机交互的边界。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BCI与GenAI的融合为脑信号解码带来新突破，促进辅助通信和神经表征学习的发展。</li>
<li>EEG为非侵入式地转化神经活动提供手段，结合最新深度学习技术实现图像、文本和语音的生成。</li>
<li>GANs、LLMs等深度学习技术在EEG基生成领域有显著改善。</li>
<li>EEG转图像生成领域包括GANs、VAEs和Diffusion Models等技术。</li>
<li>EEG转文本生成结合了基于Transformer的语言模型和对比学习方法。</li>
<li>EEG语音合成是一个新兴且快速发展的多模式领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12048">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-18b1202b9d9b1b6953902dc1cf5c1af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbcbf7ea3a54bed121bbda3d979cbab5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e2fd71ea6427f2dca809537461c5c98.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers"><a href="#MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers" class="headerlink" title="MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers"></a>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers</h2><p><strong>Authors:Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu</strong></p>
<p>In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. </p>
<blockquote>
<p>在扩散模型的应用中，可控生成具有实际意义，但也具有挑战性。当前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归（MR）扩散则直接修改随机微分方程（SDE）的结构，使得融入图像条件更加简单自然。然而，现有的无训练快速采样器并不直接适用于MR扩散。因此，MR扩散需要数百个功能评估（NFE）来获得高质量样本。在本文中，我们提出了一种名为MRS（MR采样器）的新算法，以减少MR扩散的采样NFE。我们解决了反向时间SDE和与MR扩散相关的概率流常微分方程（PF-ODE），并得出半解析解。这些解决方案由一个分析函数和一个由神经网络参数化的积分组成。基于这个解决方案，我们可以以更少的步骤生成高质量的样本。我们的方法不需要训练，支持包括噪声预测、数据预测和速度预测在内的所有主流参数化方法。大量实验表明，MR采样器在10个不同的图像恢复任务中保持了高采样质量，速度提升了10到20倍。我们的算法加速了MR扩散的采样过程，使其在可控生成中更加实用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07856v3">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong>：本论文提出一种新的算法MRS来解决MR扩散模型中遇到的训练问题。通过对逆向时间随机微分方程和概率流常微分方程进行求解，得到半解析解，包括一个由神经网络参数化的积分。基于这个解，我们能够在较少的步骤内生成高质量样本，从而加速MR扩散模型的采样过程，使其在实际可控生成中更具实用性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>MR扩散模型通过直接修改随机微分方程的结构，简化了图像条件的引入。</li>
<li>当前的无训练快速采样器无法直接应用于MR扩散模型。</li>
<li>MR采样器（MRS）被提出来解决MR扩散模型的采样问题，通过减少所需的函数评估次数（NFEs）来提高效率。</li>
<li>MRS通过对逆向时间SDE和PF-ODE的解决，得到半解析解，该解包括一个解析函数和一个由神经网络参数化的积分。</li>
<li>MRS算法无需训练，支持主流的参数化方法，包括噪声预测、数据预测和速度预测。</li>
<li>实验表明，MR采样器在10个不同的图像恢复任务中，保持高采样质量的同时实现了10到20倍的速度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07856">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-544e27021ce72f2aa43a7668607f01d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-715f6f4c7a6a31bc86cffcf0024918fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10991a13c40b06e24c616fa3a30f9b1d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates"><a href="#HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates" class="headerlink" title="HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates"></a>HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates</h2><p><strong>Authors:Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang</strong></p>
<p>Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complimentary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving indices map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates. </p>
<blockquote>
<p>在超低比特率下，图像压缩对于传统的图像压缩（LIC）和生成向量量化（VQ）建模仍然具有挑战性。传统LIC由于重度量化而产生严重伪影，而生成VQ建模由于生成的先验知识和特定输入之间的不匹配而导致保真度低。在这项工作中，我们提出了混合扩散图像压缩（HDCompression），这是一个双流框架，利用生成VQ建模和扩散模型以及传统LIC，以实现高保真和高感知质量。不同于之前直接使用预训练的LIC模型从重度量化的潜在数据中生成低质量保真度保持信息的混合方法，我们使用扩散模型从原始真实输入中提取高质量补充保真信息，可以在以下几个方面增强系统性能：改善索引映射预测，提高LIC流的保真度保持输出，以及通过VQ潜在校正进行条件图像重建。此外，我们的扩散模型基于密集代表性向量（DRV），具有轻量级和非常简单的采样调度器。大量实验表明，我们的HDCompression在定量指标和定性可视化方面均优于以前的传统LIC、生成VQ建模和混合框架，在超低比特率下提供了平衡的稳健压缩性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07160v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Hybrid-Diffusion Image Compression（HDCompression）的压缩方法，结合了传统图像压缩（LIC）、生成向量量化（VQ）建模和扩散模型。相较于以往直接使用预训练的LIC模型生成低质量保真度信息的方法，HDCompression利用扩散模型从原始图像中提取高质量互补的保真信息，从而提高了系统性能，包括改进索引映射预测、增强LIC流的保真度保留输出以及细化条件图像重建的VQ潜在校正。此外，该方法的扩散模型基于轻量级的密集代表性向量（DRV），采样调度器非常简单。实验表明，HDCompression在定量指标和定性可视化方面都优于传统的LIC、生成VQ建模和混合框架，在超低比特率下实现了平衡的稳健压缩性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面临超低比特率下图像压缩的挑战，传统LIC和生成VQ建模都存在局限性。</li>
<li>HDCompression结合了多种技术，包括生成VQ建模、扩散模型和传统LIC。</li>
<li>HDCompression利用扩散模型从原始图像中提取高质量互补的保真信息。</li>
<li>扩散模型能提高系统性能，改进索引映射预测、增强LIC流的保真度保留输出以及细化条件图像重建。</li>
<li>HDCompression基于轻量级的密集代表性向量（DRV）的扩散模型设计。</li>
<li>实验结果显示HDCompression在定量和定性评估上都优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db750e019fc28828643d3500bf9dca90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a46c4a4b63e6126703b406339e85cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-839ebbafb28640c5cd2d041eb04adec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88b7851930ad7fc7f390c7fc29ef87d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4f26c7faad21302a27a4ae71a9f54c2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise"><a href="#FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise" class="headerlink" title="FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise"></a>FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise</h2><p><strong>Authors:Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang</strong></p>
<p>Text-driven video generation has advanced significantly due to developments in diffusion models. Beyond the training and sampling phases, recent studies have investigated noise priors of diffusion models, as improved noise priors yield better generation results. One recent approach employs the Fourier transform to manipulate noise, marking the initial exploration of frequency operations in this context. However, it often generates videos that lack motion dynamics and imaging details. In this work, we provide a comprehensive theoretical analysis of the variance decay issue present in existing methods, contributing to the loss of details and motion dynamics. Recognizing the critical impact of noise distribution on generation quality, we introduce FreqPrior, a novel noise initialization strategy that refines noise in the frequency domain. Our method features a novel filtering technique designed to address different frequency signals while maintaining the noise prior distribution that closely approximates a standard Gaussian distribution. Additionally, we propose a partial sampling process by perturbing the latent at an intermediate timestep during finding the noise prior, significantly reducing inference time without compromising quality. Extensive experiments on VBench demonstrate that our method achieves the highest scores in both quality and semantic assessments, resulting in the best overall total score. These results highlight the superiority of our proposed noise prior. </p>
<blockquote>
<p>基于文本的视频生成由于扩散模型的发展而取得了显著的进步。除了训练和采样阶段，最近的研究还探索了扩散模型的噪声先验，因为改进的噪声先验会产生更好的生成结果。一种最近的方法使用傅里叶变换来操作噪声，标志着在此背景下对频率操作的初步探索。然而，它通常生成的视频缺乏运动动力和成像细节。在这项工作中，我们对现有方法中存在的方差衰减问题进行了全面的理论分析，这一问题导致了细节和运动动力的丧失。我们认识到噪声分布对生成质量的关键影响，因此引入了FreqPrior，这是一种新的噪声初始化策略，它在频率域中优化噪声。我们的方法采用了一种新型滤波技术，旨在处理不同的频率信号，同时保持噪声先验分布，紧密逼近标准高斯分布。此外，我们通过在中途寻找噪声先验时在潜在空间进行部分采样过程，扰动潜在空间，显著减少了推理时间，同时不妥协质量。在VBench上的广泛实验表明，我们的方法在质量和语义评估中都获得了最高分，总得分最高。这些结果凸显了我们所提出的噪声先验的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03496v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     基于扩散模型的文本驱动视频生成技术取得显著进展。最新研究开始探索扩散模型的噪声先验，改进噪声先验可提升生成效果。本文提供了对现有的方差衰减问题的全面理论分析，影响了生成视频的细节和运动动力丢失问题。我们引入了FreqPrior，一种新颖的噪声初始化策略，在频率域对噪声进行精炼。此外，通过在寻找噪声先验过程中中间时刻扰动潜在变量，提出了部分采样过程，显著缩短了推理时间而不损失质量。实验证明，我们的方法在质量和语义评估上获得最高分，总体表现最佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本驱动视频生成方面的显著进展得益于其噪声先验的研究和改进。</li>
<li>近期研究发现改进噪声先验有助于提高视频生成的品质。</li>
<li>现有方法存在方差衰减问题，导致生成视频缺乏运动动力学和成像细节。</li>
<li>本文全面分析了方差衰减问题，并指出噪声分布对生成质量的关键影响。</li>
<li>引入FreqPrior，一种新颖的噪声初始化策略，针对频率域进行噪声优化。</li>
<li>提出一种部分采样过程，通过中间时刻扰动潜在变量来缩短推理时间，同时保持高质量生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03496">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d66d8aa1b95481f76dcf7974e3d47529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37c94ac0a1a4dc06f70af4af0b66791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c4a1fab50665f60a48c1458a9b8d618.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-277fa8826fe7064737db1aadf7b7b567.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Dataset-Distillation-via-Diffusion-Driven-Patch-Selection-for-Improved-Generalization"><a href="#Efficient-Dataset-Distillation-via-Diffusion-Driven-Patch-Selection-for-Improved-Generalization" class="headerlink" title="Efficient Dataset Distillation via Diffusion-Driven Patch Selection for   Improved Generalization"></a>Efficient Dataset Distillation via Diffusion-Driven Patch Selection for   Improved Generalization</h2><p><strong>Authors:Xinhao Zhong, Shuoyang Sun, Xulin Gu, Zhaoyang Xu, Yaowei Wang, Jianlong Wu, Bin Chen</strong></p>
<p>Dataset distillation offers an efficient way to reduce memory and computational costs by optimizing a smaller dataset with performance comparable to the full-scale original. However, for large datasets and complex deep networks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space limits performance, reducing its practicality. Recent approaches employ pre-trained diffusion models to generate informative images directly, avoiding pixel-level optimization and achieving notable results. However, these methods often face challenges due to distribution shifts between pre-trained models and target datasets, along with the need for multiple distillation steps across varying settings. To address these issues, we propose a novel framework orthogonal to existing diffusion-based distillation methods, leveraging diffusion models for selection rather than generation. Our method starts by predicting noise generated by the diffusion model based on input images and text prompts (with or without label text), then calculates the corresponding loss for each pair. With the loss differences, we identify distinctive regions of the original images. Additionally, we perform intra-class clustering and ranking on selected patches to maintain diversity constraints. This streamlined framework enables a single-step distillation process, and extensive experiments demonstrate that our approach outperforms state-of-the-art methods across various metrics. </p>
<blockquote>
<p>数据集蒸馏提供了一种通过优化小型数据集来减少内存和计算成本的有效方法，其性能可与全尺寸原始数据集相当。然而，对于大型数据集和复杂的深度网络（例如，带有ResNet-101的ImageNet-1K），广泛的优化空间限制了性能，降低了其实用性。最近的方法采用预训练的扩散模型直接生成信息图像，避免了像素级的优化，并取得了显著的结果。然而，这些方法常常面临预训练模型和目标数据集之间分布转移的挑战，以及在不同设置下需要多次蒸馏步骤的问题。为了解决这些问题，我们提出了一种与现有基于扩散的蒸馏方法正交的新型框架，利用扩散模型进行选择而不是生成。我们的方法首先基于输入图像和文本提示（带有或不带标签文本）预测由扩散模型产生的噪声，然后计算每对相应的损失。通过损失差异，我们识别出原始图像的独特区域。此外，我们对选定的补丁进行类内聚类和排名，以保持多样性约束。这种简化的框架使单步蒸馏过程成为可能，大量实验表明，我们的方法在多种指标上超过了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09959v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用扩散模型进行选择而非生成的新框架，以解决现有蒸馏方法在处理大型数据集和复杂深度网络时的性能降低问题。该方法通过预测扩散模型生成的噪声来识别原始图像中的独特区域，并采用类内聚类与排名来保持多样性约束，实现单步蒸馏过程，并在各种指标上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在数据集蒸馏中的应用提供了新的优化思路。</li>
<li>传统蒸馏方法在处理大型数据集和复杂深度网络时存在性能限制。</li>
<li>本文提出的新框架利用扩散模型进行选择，而不是生成图像。</li>
<li>通过预测扩散模型生成的噪声和计算损失差异，识别原始图像中的独特区域。</li>
<li>框架采用类内聚类与排名来保持多样性约束。</li>
<li>实现了一种单步蒸馏过程，简化了流程并提高了效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ef313b351e32625b5bc7666e5b03a2fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc4c96ad788fd7e73452b326ccb4ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca2860f93f47efd33301a053ad22e258.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2e3b569d9e9dec367d35ac3d55caf4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-137d154fa4dae3f25d35c1f318898f4d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiffGuard-Text-Based-Safety-Checker-for-Diffusion-Models"><a href="#DiffGuard-Text-Based-Safety-Checker-for-Diffusion-Models" class="headerlink" title="DiffGuard: Text-Based Safety Checker for Diffusion Models"></a>DiffGuard: Text-Based Safety Checker for Diffusion Models</h2><p><strong>Authors:Massine El Khader, Elias Al Bouzidi, Abdellah Oumida, Mohammed Sbaihi, Eliott Binard, Jean-Philippe Poli, Wassila Ouerdane, Boussad Addad, Katarzyna Kapusta</strong></p>
<p>Recent advances in Diffusion Models have enabled the generation of images from text, with powerful closed-source models like DALL-E and Midjourney leading the way. However, open-source alternatives, such as StabilityAI’s Stable Diffusion, offer comparable capabilities. These open-source models, hosted on Hugging Face, come equipped with ethical filter protections designed to prevent the generation of explicit images. This paper reveals first their limitations and then presents a novel text-based safety filter that outperforms existing solutions. Our research is driven by the critical need to address the misuse of AI-generated content, especially in the context of information warfare. DiffGuard enhances filtering efficacy, achieving a performance that surpasses the best existing filters by over 14%. </p>
<blockquote>
<p>扩散模型的最新进展使得从文本生成图像成为可能，强大的封闭源代码模型，如DALL-E和Midjourney，引领着这一趋势。然而，开源的替代品，如StabilityAI的稳定扩散，也提供了相当的能力。这些开源模型托管在Hugging Face上，配备了旨在防止生成明确图像的伦理过滤器保护。本文首先揭示了它们的局限性，然后提出了一种新型的基于文本的安全过滤器，它超越了现有的解决方案。我们的研究是由解决人工智能生成内容滥用问题的迫切需求所驱动的，特别是在信息战的大背景下。DiffGuard提高了过滤效率，性能超过了现有最佳过滤器超过14%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00064v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本生成的新进展使得Diffusion Models能够实现文本驱动生成图像。闭源模型如DALL-E和Midjourney表现卓越，而开源模型如StabilityAI的Stable Diffusion也具备相当能力，并且加入了防止生成不当图像的伦理滤镜保护。本研究分析了这些模型的局限并提出一种新型的文本安全滤镜DiffGuard，其性能超越现有解决方案超过14%，旨在解决AI生成内容的滥用问题，尤其在信息战环境下。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Models能够实现文本驱动的图像生成。</li>
<li>闭源模型如DALL-E和Midjourney在图像生成方面表现出色。</li>
<li>开源模型如Stable Diffusion具备相似能力，并配备了防止生成不当图像的伦理滤镜。</li>
<li>现有模型存在局限性，需要改进。</li>
<li>提出了一种新型的文本安全滤镜DiffGuard。</li>
<li>DiffGuard的性能超越了现有解决方案至少14%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2c398ee918e3722147d8b395a5328e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b3ff13bed62f0a4e231b48b90ae13b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29aea790179731f0c7c91e75e110516b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1511576b238adf97401d4e6f330dbd9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f67ac79449ce86e4eaa4bbccb6537f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a79161c4df0531e7c72cec0b3bfecb1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6eca8a79dc59d5583ee72c033c77183.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8fa4b98d15dba36153a9bcf62421057.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa33df6edac868ef6ef5f8b27fa98412.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SMITE-Segment-Me-In-TimE"><a href="#SMITE-Segment-Me-In-TimE" class="headerlink" title="SMITE: Segment Me In TimE"></a>SMITE: Segment Me In TimE</h2><p><strong>Authors:Amirhossein Alimohammadi, Sauradip Nag, Saeid Asgari Taghanaki, Andrea Tagliasacchi, Ghassan Hamarneh, Ali Mahdavi Amiri</strong></p>
<p>Segmenting an object in a video presents significant challenges. Each pixel must be accurately labelled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks are defined based on only one or a few sample images. In this paper, we address this issue by employing a pre-trained text to image diffusion model supplemented with an additional tracking mechanism. We demonstrate that our approach can effectively manage various segmentation scenarios and outperforms state-of-the-art alternatives. </p>
<blockquote>
<p>在视频中分割对象存在重大挑战。每个像素都必须被精确标记，这些标签必须在各帧之间保持一致。当分割具有任意粒度时，难度会增加，意味着段落的数量可以任意变化，掩码仅基于一个或几个样本图像来定义。本文中，我们通过采用预训练的文本到图像扩散模型并辅以额外的跟踪机制来解决这个问题。我们证明我们的方法可以有效地管理各种分割场景并优于最先进的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18538v2">PDF</a> ICLR 2025; Project page is at <a target="_blank" rel="noopener" href="https://segment-me-in-time.github.io/">https://segment-me-in-time.github.io/</a></p>
<p><strong>Summary</strong>：<br>本文解决了视频对象分割的问题，采用预训练的文本到图像扩散模型并辅以额外的跟踪机制，能有效处理各种分割场景，表现优于现有技术。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>视频对象分割面临重大挑战，需要准确标记每个像素，并且在各帧之间保持标签一致性。</li>
<li>当分割具有任意粒度时，挑战会加大，意味着片段数量可以任意变化，而掩模仅基于一个或少数样本图像定义。</li>
<li>本文采用预训练的文本到图像扩散模型来处理这个问题。</li>
<li>通过附加的跟踪机制，该方法能有效管理各种分割场景。</li>
<li>该方法表现优于现有技术。</li>
<li>本文提出的方法对于视频对象分割具有广泛的应用前景和实用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18538">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-70e9dcbbdfbdd918b8a376538fb65d7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b17772af44a5b00abe364b1507bb297.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e48ecb51ec524eaadfd9341eb5abd6d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d684073abdc7cfe4990aed844ba4b17.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Denoising-as-Adaptation-Noise-Space-Domain-Adaptation-for-Image-Restoration"><a href="#Denoising-as-Adaptation-Noise-Space-Domain-Adaptation-for-Image-Restoration" class="headerlink" title="Denoising as Adaptation: Noise-Space Domain Adaptation for Image   Restoration"></a>Denoising as Adaptation: Noise-Space Domain Adaptation for Image   Restoration</h2><p><strong>Authors:Kang Liao, Zongsheng Yue, Zhouxia Wang, Chen Change Loy</strong></p>
<p>Although learning-based image restoration methods have made significant progress, they still struggle with limited generalization to real-world scenarios due to the substantial domain gap caused by training on synthetic data. Existing methods address this issue by improving data synthesis pipelines, estimating degradation kernels, employing deep internal learning, and performing domain adaptation and regularization. Previous domain adaptation methods have sought to bridge the domain gap by learning domain-invariant knowledge in either feature or pixel space. However, these techniques often struggle to extend to low-level vision tasks within a stable and compact framework. In this paper, we show that it is possible to perform domain adaptation via the noise space using diffusion models. In particular, by leveraging the unique property of how auxiliary conditional inputs influence the multi-step denoising process, we derive a meaningful diffusion loss that guides the restoration model in progressively aligning both restored synthetic and real-world outputs with a target clean distribution. We refer to this method as denoising as adaptation. To prevent shortcuts during joint training, we present crucial strategies such as channel-shuffling layer and residual-swapping contrastive learning in the diffusion model. They implicitly blur the boundaries between conditioned synthetic and real data and prevent the reliance of the model on easily distinguishable features. Experimental results on three classical image restoration tasks, namely denoising, deblurring, and deraining, demonstrate the effectiveness of the proposed method. </p>
<blockquote>
<p>虽然基于学习的图像恢复方法已经取得了显著进展，但由于在合成数据上训练导致的域差距较大，它们仍然难以推广到现实场景。现有方法通过改进数据合成管道、估计退化核、采用深度内部学习以及执行域适应和正则化来解决这个问题。以前的域适应方法试图通过在学习特征或像素空间中的域不变知识来缩小域差距。然而，这些技术在稳定且紧凑的框架内往往难以扩展到低级视觉任务。在本文中，我们展示了可以通过使用扩散模型在噪声空间中进行域适应。特别是，通过利用辅助条件输入如何影响多步去噪过程的独特属性，我们推导出了一个有意义的扩散损失，该损失指导恢复模型逐步对齐恢复合成和真实世界输出与目标清洁分布。我们将这种方法称为去噪适应。为了防止联合训练过程中的捷径，我们在扩散模型中提出了关键策略，如通道混洗层和残差交换对比学习。它们隐含地模糊了受条件约束的合成数据和真实数据之间的边界，并防止模型依赖于容易区分的特征。在三个经典图像恢复任务——去噪、去模糊和去雨——上的实验结果证明了所提方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18516v3">PDF</a> Accepted by ICLR2025. Project Page:   <a target="_blank" rel="noopener" href="https://kangliao929.github.io/projects/noise-da/">https://kangliao929.github.io/projects/noise-da/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的新型域自适应方法，通过在噪声空间利用辅助条件输入影响多步去噪过程，实现合成数据和真实世界输出与目标清洁分布的逐步对齐。通过通道混洗层和剩余交换对比学习等策略，防止联合训练中的捷径问题，模糊合成数据和真实数据之间的界限。实验结果表明，该方法在图像去噪、去模糊和去雨等三个经典任务中效果显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型被用于域自适应，通过在噪声空间利用辅助条件输入影响去噪过程，实现合成和真实世界数据之间的对齐。</li>
<li>提出了一种新的扩散损失函数，用于指导恢复模型逐步对齐恢复后的合成和真实世界输出与目标清洁分布。</li>
<li>通过通道混洗层和剩余交换对比学习等策略，防止模型在联合训练中出现捷径问题。</li>
<li>提出的方法在图像去噪、去模糊和去雨三个经典任务中取得了实验性的成功。</li>
<li>该方法能够在稳定且紧凑的框架内扩展至低层次视觉任务。</li>
<li>这种方法有助于缩小合成数据和真实世界场景之间的域差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.18516">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f14807528e9cd07a5364c27f2a7fc4d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf4f8c0318fe2d2c3b5ceb1a32ebffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d2967c826e8f022942855d441948047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b28552c6261d625650f1114cafcd765.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dc7f5a85a37c6456a0323059b9394121.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-21  MGFI-Net A Multi-Grained Feature Integration Network for Enhanced   Medical Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f96cd34a36943bfc7e3b0180519f71e6.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-02-21  Geometry-Aware Diffusion Models for Multiview Scene Inpainting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
