<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-21  Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a79161c4df0531e7c72cec0b3bfecb1f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-21-æ›´æ–°"><a href="#2025-02-21-æ›´æ–°" class="headerlink" title="2025-02-21 æ›´æ–°"></a>2025-02-21 æ›´æ–°</h1><h2 id="Secure-and-Efficient-Watermarking-for-Latent-Diffusion-Models-in-Model-Distribution-Scenarios"><a href="#Secure-and-Efficient-Watermarking-for-Latent-Diffusion-Models-in-Model-Distribution-Scenarios" class="headerlink" title="Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios"></a>Secure and Efficient Watermarking for Latent Diffusion Models in Model   Distribution Scenarios</h2><p><strong>Authors:Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu</strong></p>
<p>Latent diffusion models have exhibited considerable potential in generative tasks. Watermarking is considered to be an alternative to safeguard the copyright of generative models and prevent their misuse. However, in the context of model distribution scenarios, the accessibility of models to large scale of model users brings new challenges to the security, efficiency and robustness of existing watermark solutions. To address these issues, we propose a secure and efficient watermarking solution. A new security mechanism is designed to prevent watermark leakage and watermark escape, which considers watermark randomness and watermark-model association as two constraints for mandatory watermark injection. To reduce the time cost of training the security module, watermark injection and the security mechanism are decoupled, ensuring that fine-tuning VAE only accomplishes the security mechanism without the burden of learning watermark patterns. A watermark distribution-based verification strategy is proposed to enhance the robustness against diverse attacks in the model distribution scenarios. Experimental results prove that our watermarking consistently outperforms existing six baselines on effectiveness and robustness against ten image processing attacks and adversarial attacks, while enhancing security in the distribution scenarios. </p>
<blockquote>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚æ°´å°è¢«è§†ä¸ºä¿æŠ¤ç”Ÿæˆæ¨¡å‹ç‰ˆæƒå¹¶é˜²æ­¢å…¶è¢«æ»¥ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨æ¨¡å‹åˆ†å‘åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¯¹å¤§é‡ç”¨æˆ·çš„å¯è®¿é—®æ€§ç»™ç°æœ‰æ°´å°è§£å†³æ–¹æ¡ˆçš„å®‰å…¨æ€§ã€æ•ˆç‡å’Œç¨³å¥æ€§å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®‰å…¨é«˜æ•ˆçš„æ°´å°è§£å†³æ–¹æ¡ˆã€‚è®¾è®¡äº†ä¸€ç§æ–°çš„å®‰å…¨æœºåˆ¶ï¼Œä»¥é˜²æ­¢æ°´å°æ³„éœ²å’Œæ°´å°é€ƒé€¸ï¼Œè¯¥æœºåˆ¶å°†æ°´å°éšæœºæ€§å’Œæ°´å°ä¸æ¨¡å‹çš„å…³è”ä½œä¸ºå¼ºåˆ¶æ°´å°æ³¨å…¥çš„ä¸¤ä¸ªçº¦æŸæ¡ä»¶ã€‚ä¸ºäº†å‡å°‘è®­ç»ƒå®‰å…¨æ¨¡å—çš„æ—¶é—´æˆæœ¬ï¼Œæˆ‘ä»¬å°†æ°´å°æ³¨å…¥ä¸å®‰å…¨æœºåˆ¶è§£è€¦ï¼Œç¡®ä¿å¾®è°ƒVAEåªå®Œæˆå®‰å…¨æœºåˆ¶ï¼Œè€Œæ— éœ€å­¦ä¹ æ°´å°æ¨¡å¼çš„è´Ÿæ‹…ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ°´å°åˆ†å¸ƒçš„ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹åˆ†å‘åœºæ™¯ä¸­å„ç§æ”»å‡»çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ°´å°åœ¨æœ‰æ•ˆæ€§å’Œå¯¹åç§å›¾åƒå¤„ç†æ”»å‡»å’Œå¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰çš„å…­ç§åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æé«˜äº†åˆ†å‘åœºæ™¯ä¸­çš„å®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13345v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œè€Œæ°´å°æŠ€æœ¯ç”¨äºä¿æŠ¤ç”Ÿæˆæ¨¡å‹çš„ç‰ˆæƒå¹¶é˜²æ­¢æ»¥ç”¨ã€‚ä½†åœ¨æ¨¡å‹åˆ†å‘åœºæ™¯ä¸‹ï¼Œæ¨¡å‹çš„å¤§è§„æ¨¡ç”¨æˆ·è®¿é—®ç»™ç°æœ‰æ°´å°è§£å†³æ–¹æ¡ˆçš„å®‰å…¨æ€§ã€æ•ˆç‡å’Œç¨³å¥æ€§å¸¦æ¥æ–°æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®‰å…¨é«˜æ•ˆçš„æ°´å°è§£å†³æ–¹æ¡ˆï¼Œè®¾è®¡æ–°çš„å®‰å…¨æœºåˆ¶é˜²æ­¢æ°´å°æ³„éœ²å’Œé€ƒé€¸ï¼Œä»¥æ°´å°éšæœºæ€§å’Œæ°´å°ä¸æ¨¡å‹çš„å…³è”ä¸ºå¼ºåˆ¶æ³¨å…¥çš„ä¸¤ä¸ªçº¦æŸã€‚ä¸ºæé«˜å®‰å…¨æ¨¡å—è®­ç»ƒçš„æ—¶é—´æˆæœ¬ï¼Œå°†æ°´å°æ³¨å…¥ä¸å®‰å…¨æœºåˆ¶è§£è€¦ï¼Œç¡®ä¿å¾®è°ƒVAEåªå®ç°å®‰å…¨æœºåˆ¶ï¼Œæ— éœ€å­¦ä¹ æ°´å°æ¨¡å¼ã€‚æå‡ºåŸºäºæ°´å°åˆ†å¸ƒçš„éªŒè¯ç­–ç•¥ï¼Œæé«˜æ¨¡å‹åˆ†å‘åœºæ™¯ä¸­å¯¹æŠ—å„ç§æ”»å‡»çš„ç¨³å¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ°´å°æŠ€æœ¯åœ¨æœ‰æ•ˆæ€§ã€å¯¹æŠ—åç§å›¾åƒå¤„ç†æ”»å‡»å’Œå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§æ–¹é¢ä¼˜äºç°æœ‰å…­ç§åŸºçº¿æŠ€æœ¯ï¼ŒåŒæ—¶åœ¨åˆ†å‘åœºæ™¯ä¸­æé«˜å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç‰ˆæƒä¿æŠ¤é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ°´å°æŠ€æœ¯ç”¨äºä¿æŠ¤ç”Ÿæˆæ¨¡å‹çš„ç‰ˆæƒå¹¶é˜²æ­¢æ»¥ç”¨ã€‚</li>
<li>æ¨¡å‹åˆ†å‘åœºæ™¯ä¸‹ï¼Œå¤§è§„æ¨¡ç”¨æˆ·è®¿é—®ç»™æ°´å°è§£å†³æ–¹æ¡ˆå¸¦æ¥æ–°æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§å®‰å…¨é«˜æ•ˆçš„æ°´å°è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬è®¾è®¡æ–°çš„å®‰å…¨æœºåˆ¶é˜²æ­¢æ°´å°æ³„éœ²å’Œé€ƒé€¸ã€‚</li>
<li>æ°´å°éšæœºæ€§å’Œæ°´å°ä¸æ¨¡å‹çš„å…³è”æ˜¯å¼ºåˆ¶æ°´å°æ³¨å…¥çš„ä¸¤ä¸ªé‡è¦çº¦æŸã€‚</li>
<li>å°†æ°´å°æ³¨å…¥ä¸å®‰å…¨æœºåˆ¶è§£è€¦ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13345">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83c7279df1b7bd8d6890c28badcf7f38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5148117dc9b5eacd02445e92a4a383e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87c10cd5ab276f45b647ffaf1bd831bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88f995544b688ba8892fa327f4f40e4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-affbefa61a2fee11a71ec9e862376c06.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond"><a href="#A-Survey-on-Bridging-EEG-Signals-and-Generative-AI-From-Image-and-Text-to-Beyond" class="headerlink" title="A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond"></a>A Survey on Bridging EEG Signals and Generative AI: From Image and Text   to Beyond</h2><p><strong>Authors:Shreya Shukla, Jose Torres, Abhijit Mishra, Jacek Gwizdka, Shounak Roychowdhury</strong></p>
<p>Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¼€è¾Ÿäº†æ–°çš„é¢†åŸŸï¼Œå®ç°äº†è¾…åŠ©é€šä¿¡ã€ç¥ç»è¡¨å¾å­¦ä¹ å’Œå¤šæ¨¡æ€èåˆã€‚ç‰¹åˆ«æ˜¯åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰çš„è„‘æœºæ¥å£ï¼Œæä¾›äº†ä¸€ç§å°†ç¥ç»æ´»åŠ¨è½¬åŒ–ä¸ºæœ‰æ„ä¹‰è¾“å‡ºçš„éä¾µå…¥æ€§æ‰‹æ®µã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’ŒåŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ˜¾è‘—æé«˜äº†åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡ç»¼è¿°äº†åŸºäºEEGçš„å¤šæ¨¡æ€ç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼Œé‡ç‚¹å…³æ³¨ï¼ˆiï¼‰é€šè¿‡GANsã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œæ‰©æ•£æ¨¡å‹å®ç°EEGåˆ°å›¾åƒç”Ÿæˆï¼Œä»¥åŠï¼ˆiiï¼‰åˆ©ç”¨åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•å®ç°EEGåˆ°æ–‡æœ¬ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ–°å…´çš„EEGåˆ°è¯­éŸ³åˆæˆé¢†åŸŸï¼Œè¿™æ˜¯ä¸€ä¸ªä¸æ–­å‘å±•çš„å¤šæ¨¡æ€å‰æ²¿é¢†åŸŸã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†å…³é”®æ•°æ®é›†ã€ç”¨ä¾‹ã€æŒ‘æˆ˜å’Œæ”¯æ’‘ç”Ÿæˆæ–¹æ³•çš„EEGç‰¹å¾ç¼–ç æ–¹æ³•ã€‚é€šè¿‡å¯¹åŸºäºEEGçš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½è¿›è¡Œç»“æ„åŒ–æ¦‚è¿°ï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æ´å¯Ÿï¼Œä»¥ä¿ƒè¿›ç¥ç»è§£ç çš„å‘å±•ï¼Œæé«˜è¾…åŠ©æŠ€æœ¯çš„æ€§èƒ½ï¼Œå¹¶æ‹“å±•è„‘æœºäº¤äº’çš„è¾¹ç•Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12048v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è„‘æœºæ¥å£ï¼ˆBCIï¼‰ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¼€åˆ›äº†æ–°é¢†åŸŸï¼Œæ¨åŠ¨äº†è¾…åŠ©é€šä¿¡ã€ç¥ç»è¡¨å¾å­¦ä¹ å’Œå¤šæ¨¡å¼èåˆçš„å‘å±•ã€‚è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¸ºåŸºç¡€çš„BCIä¸ºéä¾µå…¥å¼åœ°è½¬åŒ–ç¥ç»æ´»åŠ¨ä¸ºæœ‰æ„ä¹‰è¾“å‡ºæä¾›äº†æ‰‹æ®µã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç­‰ï¼Œå·²æ˜¾è‘—æ”¹å–„åŸºäºEEGçš„å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚æœ¬æ–‡ç»¼è¿°äº†EEGåŸºå¤šæ¨¡å¼ç”Ÿæˆçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬EEGè½¬å›¾åƒç”Ÿæˆå’ŒEEGè½¬æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶æ¢è®¨äº†æ–°å…´çš„EEGè¯­éŸ³åˆæˆé¢†åŸŸã€‚é€šè¿‡æä¾›EEGåŸºç”Ÿæˆå¼AIçš„ç»“æ„æ€§æ¦‚è§ˆï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›æ´å¯Ÿï¼Œä»¥æ¨åŠ¨ç¥ç»è§£ç ã€è¾…åŠ©æŠ€æœ¯å‘å±•å’Œæ‹“å±•è„‘æœºäº¤äº’çš„è¾¹ç•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BCIä¸GenAIçš„èåˆä¸ºè„‘ä¿¡å·è§£ç å¸¦æ¥æ–°çªç ´ï¼Œä¿ƒè¿›è¾…åŠ©é€šä¿¡å’Œç¥ç»è¡¨å¾å­¦ä¹ çš„å‘å±•ã€‚</li>
<li>EEGä¸ºéä¾µå…¥å¼åœ°è½¬åŒ–ç¥ç»æ´»åŠ¨æä¾›æ‰‹æ®µï¼Œç»“åˆæœ€æ–°æ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°å›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³çš„ç”Ÿæˆã€‚</li>
<li>GANsã€LLMsç­‰æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨EEGåŸºç”Ÿæˆé¢†åŸŸæœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>EEGè½¬å›¾åƒç”Ÿæˆé¢†åŸŸåŒ…æ‹¬GANsã€VAEså’ŒDiffusion Modelsç­‰æŠ€æœ¯ã€‚</li>
<li>EEGè½¬æ–‡æœ¬ç”Ÿæˆç»“åˆäº†åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹å’Œå¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>EEGè¯­éŸ³åˆæˆæ˜¯ä¸€ä¸ªæ–°å…´ä¸”å¿«é€Ÿå‘å±•çš„å¤šæ¨¡å¼é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12048">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18b1202b9d9b1b6953902dc1cf5c1af7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55c7fcdd87bbca830d14424bc5a6faf9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51ac9d309d1f16be825a0b4c92719973.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbcbf7ea3a54bed121bbda3d979cbab5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e2fd71ea6427f2dca809537461c5c98.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers"><a href="#MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers" class="headerlink" title="MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers"></a>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers</h2><p><strong>Authors:Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu</strong></p>
<p>In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. </p>
<blockquote>
<p>åœ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¸­ï¼Œå¯æ§ç”Ÿæˆå…·æœ‰å®é™…æ„ä¹‰ï¼Œä½†ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰çš„å¯æ§ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„è¯„åˆ†å‡½æ•°ï¼Œè€Œå‡å€¼å›å½’ï¼ˆMRï¼‰æ‰©æ•£åˆ™ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„ç»“æ„ï¼Œä½¿å¾—èå…¥å›¾åƒæ¡ä»¶æ›´åŠ ç®€å•è‡ªç„¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨å¹¶ä¸ç›´æ¥é€‚ç”¨äºMRæ‰©æ•£ã€‚å› æ­¤ï¼ŒMRæ‰©æ•£éœ€è¦æ•°ç™¾ä¸ªåŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰æ¥è·å¾—é«˜è´¨é‡æ ·æœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMRSï¼ˆMRé‡‡æ ·å™¨ï¼‰çš„æ–°ç®—æ³•ï¼Œä»¥å‡å°‘MRæ‰©æ•£çš„é‡‡æ ·NFEã€‚æˆ‘ä»¬è§£å†³äº†åå‘æ—¶é—´SDEå’Œä¸MRæ‰©æ•£ç›¸å…³çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰ï¼Œå¹¶å¾—å‡ºåŠè§£æè§£ã€‚è¿™äº›è§£å†³æ–¹æ¡ˆç”±ä¸€ä¸ªåˆ†æå‡½æ•°å’Œä¸€ä¸ªç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ç§¯åˆ†ç»„æˆã€‚åŸºäºè¿™ä¸ªè§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ›´å°‘çš„æ­¥éª¤ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦è®­ç»ƒï¼Œæ”¯æŒåŒ…æ‹¬å™ªå£°é¢„æµ‹ã€æ•°æ®é¢„æµ‹å’Œé€Ÿåº¦é¢„æµ‹åœ¨å†…çš„æ‰€æœ‰ä¸»æµå‚æ•°åŒ–æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMRé‡‡æ ·å™¨åœ¨10ä¸ªä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸­ä¿æŒäº†é«˜é‡‡æ ·è´¨é‡ï¼Œé€Ÿåº¦æå‡äº†10åˆ°20å€ã€‚æˆ‘ä»¬çš„ç®—æ³•åŠ é€Ÿäº†MRæ‰©æ•£çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å…¶åœ¨å¯æ§ç”Ÿæˆä¸­æ›´åŠ å®ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07856v3">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬è®ºæ–‡æå‡ºä¸€ç§æ–°çš„ç®—æ³•MRSæ¥è§£å†³MRæ‰©æ•£æ¨¡å‹ä¸­é‡åˆ°çš„è®­ç»ƒé—®é¢˜ã€‚é€šè¿‡å¯¹é€†å‘æ—¶é—´éšæœºå¾®åˆ†æ–¹ç¨‹å’Œæ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹è¿›è¡Œæ±‚è§£ï¼Œå¾—åˆ°åŠè§£æè§£ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ç§¯åˆ†ã€‚åŸºäºè¿™ä¸ªè§£ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°‘çš„æ­¥éª¤å†…ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œä»è€ŒåŠ é€ŸMRæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å…¶åœ¨å®é™…å¯æ§ç”Ÿæˆä¸­æ›´å…·å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MRæ‰©æ•£æ¨¡å‹é€šè¿‡ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹çš„ç»“æ„ï¼Œç®€åŒ–äº†å›¾åƒæ¡ä»¶çš„å¼•å…¥ã€‚</li>
<li>å½“å‰çš„æ— è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨æ— æ³•ç›´æ¥åº”ç”¨äºMRæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>MRé‡‡æ ·å™¨ï¼ˆMRSï¼‰è¢«æå‡ºæ¥è§£å†³MRæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·é—®é¢˜ï¼Œé€šè¿‡å‡å°‘æ‰€éœ€çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰æ¥æé«˜æ•ˆç‡ã€‚</li>
<li>MRSé€šè¿‡å¯¹é€†å‘æ—¶é—´SDEå’ŒPF-ODEçš„è§£å†³ï¼Œå¾—åˆ°åŠè§£æè§£ï¼Œè¯¥è§£åŒ…æ‹¬ä¸€ä¸ªè§£æå‡½æ•°å’Œä¸€ä¸ªç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ç§¯åˆ†ã€‚</li>
<li>MRSç®—æ³•æ— éœ€è®­ç»ƒï¼Œæ”¯æŒä¸»æµçš„å‚æ•°åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬å™ªå£°é¢„æµ‹ã€æ•°æ®é¢„æµ‹å’Œé€Ÿåº¦é¢„æµ‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMRé‡‡æ ·å™¨åœ¨10ä¸ªä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸­ï¼Œä¿æŒé«˜é‡‡æ ·è´¨é‡çš„åŒæ—¶å®ç°äº†10åˆ°20å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-544e27021ce72f2aa43a7668607f01d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-715f6f4c7a6a31bc86cffcf0024918fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10991a13c40b06e24c616fa3a30f9b1d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates"><a href="#HDCompression-Hybrid-Diffusion-Image-Compression-for-Ultra-Low-Bitrates" class="headerlink" title="HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates"></a>HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates</h2><p><strong>Authors:Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang</strong></p>
<p>Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-modeling and diffusion models, as well as conventional LIC, to achieve both high fidelity and high perceptual quality. Different from previous hybrid methods that directly use pre-trained LIC models to generate low-quality fidelity-preserving information from heavily quantized latent, we use diffusion models to extract high-quality complimentary fidelity information from the ground-truth input, which can enhance the system performance in several aspects: improving indices map prediction, enhancing the fidelity-preserving output of the LIC stream, and refining conditioned image reconstruction with VQ-latent correction. In addition, our diffusion model is based on a dense representative vector (DRV), which is lightweight with very simple sampling schedulers. Extensive experiments demonstrate that our HDCompression outperforms the previous conventional LIC, generative VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative visualization, providing balanced robust compression performance at ultra-low bitrates. </p>
<blockquote>
<p>åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹ï¼Œå›¾åƒå‹ç¼©å¯¹äºä¼ ç»Ÿçš„å›¾åƒå‹ç¼©ï¼ˆLICï¼‰å’Œç”Ÿæˆå‘é‡é‡åŒ–ï¼ˆVQï¼‰å»ºæ¨¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»ŸLICç”±äºé‡åº¦é‡åŒ–è€Œäº§ç”Ÿä¸¥é‡ä¼ªå½±ï¼Œè€Œç”ŸæˆVQå»ºæ¨¡ç”±äºç”Ÿæˆçš„å…ˆéªŒçŸ¥è¯†å’Œç‰¹å®šè¾“å…¥ä¹‹é—´çš„ä¸åŒ¹é…è€Œå¯¼è‡´ä¿çœŸåº¦ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆæ‰©æ•£å›¾åƒå‹ç¼©ï¼ˆHDCompressionï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒæµæ¡†æ¶ï¼Œåˆ©ç”¨ç”ŸæˆVQå»ºæ¨¡å’Œæ‰©æ•£æ¨¡å‹ä»¥åŠä¼ ç»ŸLICï¼Œä»¥å®ç°é«˜ä¿çœŸå’Œé«˜æ„ŸçŸ¥è´¨é‡ã€‚ä¸åŒäºä¹‹å‰ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„LICæ¨¡å‹ä»é‡åº¦é‡åŒ–çš„æ½œåœ¨æ•°æ®ä¸­ç”Ÿæˆä½è´¨é‡ä¿çœŸåº¦ä¿æŒä¿¡æ¯çš„æ··åˆæ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä»åŸå§‹çœŸå®è¾“å…¥ä¸­æå–é«˜è´¨é‡è¡¥å……ä¿çœŸä¿¡æ¯ï¼Œå¯ä»¥åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢å¢å¼ºç³»ç»Ÿæ€§èƒ½ï¼šæ”¹å–„ç´¢å¼•æ˜ å°„é¢„æµ‹ï¼Œæé«˜LICæµçš„ä¿çœŸåº¦ä¿æŒè¾“å‡ºï¼Œä»¥åŠé€šè¿‡VQæ½œåœ¨æ ¡æ­£è¿›è¡Œæ¡ä»¶å›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹åŸºäºå¯†é›†ä»£è¡¨æ€§å‘é‡ï¼ˆDRVï¼‰ï¼Œå…·æœ‰è½»é‡çº§å’Œéå¸¸ç®€å•çš„é‡‡æ ·è°ƒåº¦å™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„HDCompressionåœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢å‡ä¼˜äºä»¥å‰çš„ä¼ ç»ŸLICã€ç”ŸæˆVQå»ºæ¨¡å’Œæ··åˆæ¡†æ¶ï¼Œåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹æä¾›äº†å¹³è¡¡çš„ç¨³å¥å‹ç¼©æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07160v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHybrid-Diffusion Image Compressionï¼ˆHDCompressionï¼‰çš„å‹ç¼©æ–¹æ³•ï¼Œç»“åˆäº†ä¼ ç»Ÿå›¾åƒå‹ç¼©ï¼ˆLICï¼‰ã€ç”Ÿæˆå‘é‡é‡åŒ–ï¼ˆVQï¼‰å»ºæ¨¡å’Œæ‰©æ•£æ¨¡å‹ã€‚ç›¸è¾ƒäºä»¥å¾€ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„LICæ¨¡å‹ç”Ÿæˆä½è´¨é‡ä¿çœŸåº¦ä¿¡æ¯çš„æ–¹æ³•ï¼ŒHDCompressionåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»åŸå§‹å›¾åƒä¸­æå–é«˜è´¨é‡äº’è¡¥çš„ä¿çœŸä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ç³»ç»Ÿæ€§èƒ½ï¼ŒåŒ…æ‹¬æ”¹è¿›ç´¢å¼•æ˜ å°„é¢„æµ‹ã€å¢å¼ºLICæµçš„ä¿çœŸåº¦ä¿ç•™è¾“å‡ºä»¥åŠç»†åŒ–æ¡ä»¶å›¾åƒé‡å»ºçš„VQæ½œåœ¨æ ¡æ­£ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„æ‰©æ•£æ¨¡å‹åŸºäºè½»é‡çº§çš„å¯†é›†ä»£è¡¨æ€§å‘é‡ï¼ˆDRVï¼‰ï¼Œé‡‡æ ·è°ƒåº¦å™¨éå¸¸ç®€å•ã€‚å®éªŒè¡¨æ˜ï¼ŒHDCompressionåœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§å¯è§†åŒ–æ–¹é¢éƒ½ä¼˜äºä¼ ç»Ÿçš„LICã€ç”ŸæˆVQå»ºæ¨¡å’Œæ··åˆæ¡†æ¶ï¼Œåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å®ç°äº†å¹³è¡¡çš„ç¨³å¥å‹ç¼©æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢ä¸´è¶…ä½æ¯”ç‰¹ç‡ä¸‹å›¾åƒå‹ç¼©çš„æŒ‘æˆ˜ï¼Œä¼ ç»ŸLICå’Œç”ŸæˆVQå»ºæ¨¡éƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>HDCompressionç»“åˆäº†å¤šç§æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç”ŸæˆVQå»ºæ¨¡ã€æ‰©æ•£æ¨¡å‹å’Œä¼ ç»ŸLICã€‚</li>
<li>HDCompressionåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä»åŸå§‹å›¾åƒä¸­æå–é«˜è´¨é‡äº’è¡¥çš„ä¿çœŸä¿¡æ¯ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½æé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œæ”¹è¿›ç´¢å¼•æ˜ å°„é¢„æµ‹ã€å¢å¼ºLICæµçš„ä¿çœŸåº¦ä¿ç•™è¾“å‡ºä»¥åŠç»†åŒ–æ¡ä»¶å›¾åƒé‡å»ºã€‚</li>
<li>HDCompressionåŸºäºè½»é‡çº§çš„å¯†é›†ä»£è¡¨æ€§å‘é‡ï¼ˆDRVï¼‰çš„æ‰©æ•£æ¨¡å‹è®¾è®¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºHDCompressionåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šéƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db750e019fc28828643d3500bf9dca90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a46c4a4b63e6126703b406339e85cb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-839ebbafb28640c5cd2d041eb04adec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88b7851930ad7fc7f390c7fc29ef87d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4f26c7faad21302a27a4ae71a9f54c2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise"><a href="#FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise" class="headerlink" title="FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise"></a>FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise</h2><p><strong>Authors:Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang</strong></p>
<p>Text-driven video generation has advanced significantly due to developments in diffusion models. Beyond the training and sampling phases, recent studies have investigated noise priors of diffusion models, as improved noise priors yield better generation results. One recent approach employs the Fourier transform to manipulate noise, marking the initial exploration of frequency operations in this context. However, it often generates videos that lack motion dynamics and imaging details. In this work, we provide a comprehensive theoretical analysis of the variance decay issue present in existing methods, contributing to the loss of details and motion dynamics. Recognizing the critical impact of noise distribution on generation quality, we introduce FreqPrior, a novel noise initialization strategy that refines noise in the frequency domain. Our method features a novel filtering technique designed to address different frequency signals while maintaining the noise prior distribution that closely approximates a standard Gaussian distribution. Additionally, we propose a partial sampling process by perturbing the latent at an intermediate timestep during finding the noise prior, significantly reducing inference time without compromising quality. Extensive experiments on VBench demonstrate that our method achieves the highest scores in both quality and semantic assessments, resulting in the best overall total score. These results highlight the superiority of our proposed noise prior. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬çš„è§†é¢‘ç”Ÿæˆç”±äºæ‰©æ•£æ¨¡å‹çš„å‘å±•è€Œå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚é™¤äº†è®­ç»ƒå’Œé‡‡æ ·é˜¶æ®µï¼Œæœ€è¿‘çš„ç ”ç©¶è¿˜æ¢ç´¢äº†æ‰©æ•£æ¨¡å‹çš„å™ªå£°å…ˆéªŒï¼Œå› ä¸ºæ”¹è¿›çš„å™ªå£°å…ˆéªŒä¼šäº§ç”Ÿæ›´å¥½çš„ç”Ÿæˆç»“æœã€‚ä¸€ç§æœ€è¿‘çš„æ–¹æ³•ä½¿ç”¨å‚…é‡Œå¶å˜æ¢æ¥æ“ä½œå™ªå£°ï¼Œæ ‡å¿—ç€åœ¨æ­¤èƒŒæ™¯ä¸‹å¯¹é¢‘ç‡æ“ä½œçš„åˆæ­¥æ¢ç´¢ã€‚ç„¶è€Œï¼Œå®ƒé€šå¸¸ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹è¿åŠ¨åŠ¨åŠ›å’Œæˆåƒç»†èŠ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„æ–¹å·®è¡°å‡é—®é¢˜è¿›è¡Œäº†å…¨é¢çš„ç†è®ºåˆ†æï¼Œè¿™ä¸€é—®é¢˜å¯¼è‡´äº†ç»†èŠ‚å’Œè¿åŠ¨åŠ¨åŠ›çš„ä¸§å¤±ã€‚æˆ‘ä»¬è®¤è¯†åˆ°å™ªå£°åˆ†å¸ƒå¯¹ç”Ÿæˆè´¨é‡çš„å…³é”®å½±å“ï¼Œå› æ­¤å¼•å…¥äº†FreqPriorï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å™ªå£°åˆå§‹åŒ–ç­–ç•¥ï¼Œå®ƒåœ¨é¢‘ç‡åŸŸä¸­ä¼˜åŒ–å™ªå£°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ»¤æ³¢æŠ€æœ¯ï¼Œæ—¨åœ¨å¤„ç†ä¸åŒçš„é¢‘ç‡ä¿¡å·ï¼ŒåŒæ—¶ä¿æŒå™ªå£°å…ˆéªŒåˆ†å¸ƒï¼Œç´§å¯†é€¼è¿‘æ ‡å‡†é«˜æ–¯åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ä¸­é€”å¯»æ‰¾å™ªå£°å…ˆéªŒæ—¶åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œéƒ¨åˆ†é‡‡æ ·è¿‡ç¨‹ï¼Œæ‰°åŠ¨æ½œåœ¨ç©ºé—´ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¸å¦¥åè´¨é‡ã€‚åœ¨VBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œè¯­ä¹‰è¯„ä¼°ä¸­éƒ½è·å¾—äº†æœ€é«˜åˆ†ï¼Œæ€»å¾—åˆ†æœ€é«˜ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬æ‰€æå‡ºçš„å™ªå£°å…ˆéªŒçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03496v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬é©±åŠ¨è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—æ˜¾è‘—è¿›å±•ã€‚æœ€æ–°ç ”ç©¶å¼€å§‹æ¢ç´¢æ‰©æ•£æ¨¡å‹çš„å™ªå£°å…ˆéªŒï¼Œæ”¹è¿›å™ªå£°å…ˆéªŒå¯æå‡ç”Ÿæˆæ•ˆæœã€‚æœ¬æ–‡æä¾›äº†å¯¹ç°æœ‰çš„æ–¹å·®è¡°å‡é—®é¢˜çš„å…¨é¢ç†è®ºåˆ†æï¼Œå½±å“äº†ç”Ÿæˆè§†é¢‘çš„ç»†èŠ‚å’Œè¿åŠ¨åŠ¨åŠ›ä¸¢å¤±é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†FreqPriorï¼Œä¸€ç§æ–°é¢–çš„å™ªå£°åˆå§‹åŒ–ç­–ç•¥ï¼Œåœ¨é¢‘ç‡åŸŸå¯¹å™ªå£°è¿›è¡Œç²¾ç‚¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨å¯»æ‰¾å™ªå£°å…ˆéªŒè¿‡ç¨‹ä¸­ä¸­é—´æ—¶åˆ»æ‰°åŠ¨æ½œåœ¨å˜é‡ï¼Œæå‡ºäº†éƒ¨åˆ†é‡‡æ ·è¿‡ç¨‹ï¼Œæ˜¾è‘—ç¼©çŸ­äº†æ¨ç†æ—¶é—´è€Œä¸æŸå¤±è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œè¯­ä¹‰è¯„ä¼°ä¸Šè·å¾—æœ€é«˜åˆ†ï¼Œæ€»ä½“è¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•å¾—ç›Šäºå…¶å™ªå£°å…ˆéªŒçš„ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
<li>è¿‘æœŸç ”ç©¶å‘ç°æ”¹è¿›å™ªå£°å…ˆéªŒæœ‰åŠ©äºæé«˜è§†é¢‘ç”Ÿæˆçš„å“è´¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ–¹å·®è¡°å‡é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆè§†é¢‘ç¼ºä¹è¿åŠ¨åŠ¨åŠ›å­¦å’Œæˆåƒç»†èŠ‚ã€‚</li>
<li>æœ¬æ–‡å…¨é¢åˆ†æäº†æ–¹å·®è¡°å‡é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºå™ªå£°åˆ†å¸ƒå¯¹ç”Ÿæˆè´¨é‡çš„å…³é”®å½±å“ã€‚</li>
<li>å¼•å…¥FreqPriorï¼Œä¸€ç§æ–°é¢–çš„å™ªå£°åˆå§‹åŒ–ç­–ç•¥ï¼Œé’ˆå¯¹é¢‘ç‡åŸŸè¿›è¡Œå™ªå£°ä¼˜åŒ–ã€‚</li>
<li>æå‡ºä¸€ç§éƒ¨åˆ†é‡‡æ ·è¿‡ç¨‹ï¼Œé€šè¿‡ä¸­é—´æ—¶åˆ»æ‰°åŠ¨æ½œåœ¨å˜é‡æ¥ç¼©çŸ­æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d66d8aa1b95481f76dcf7974e3d47529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37c94ac0a1a4dc06f70af4af0b66791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c4a1fab50665f60a48c1458a9b8d618.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-277fa8826fe7064737db1aadf7b7b567.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Dataset-Distillation-via-Diffusion-Driven-Patch-Selection-for-Improved-Generalization"><a href="#Efficient-Dataset-Distillation-via-Diffusion-Driven-Patch-Selection-for-Improved-Generalization" class="headerlink" title="Efficient Dataset Distillation via Diffusion-Driven Patch Selection for   Improved Generalization"></a>Efficient Dataset Distillation via Diffusion-Driven Patch Selection for   Improved Generalization</h2><p><strong>Authors:Xinhao Zhong, Shuoyang Sun, Xulin Gu, Zhaoyang Xu, Yaowei Wang, Jianlong Wu, Bin Chen</strong></p>
<p>Dataset distillation offers an efficient way to reduce memory and computational costs by optimizing a smaller dataset with performance comparable to the full-scale original. However, for large datasets and complex deep networks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space limits performance, reducing its practicality. Recent approaches employ pre-trained diffusion models to generate informative images directly, avoiding pixel-level optimization and achieving notable results. However, these methods often face challenges due to distribution shifts between pre-trained models and target datasets, along with the need for multiple distillation steps across varying settings. To address these issues, we propose a novel framework orthogonal to existing diffusion-based distillation methods, leveraging diffusion models for selection rather than generation. Our method starts by predicting noise generated by the diffusion model based on input images and text prompts (with or without label text), then calculates the corresponding loss for each pair. With the loss differences, we identify distinctive regions of the original images. Additionally, we perform intra-class clustering and ranking on selected patches to maintain diversity constraints. This streamlined framework enables a single-step distillation process, and extensive experiments demonstrate that our approach outperforms state-of-the-art methods across various metrics. </p>
<blockquote>
<p>æ•°æ®é›†è’¸é¦æä¾›äº†ä¸€ç§é€šè¿‡ä¼˜åŒ–å°å‹æ•°æ®é›†æ¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå…¶æ€§èƒ½å¯ä¸å…¨å°ºå¯¸åŸå§‹æ•°æ®é›†ç›¸å½“ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å‹æ•°æ®é›†å’Œå¤æ‚çš„æ·±åº¦ç½‘ç»œï¼ˆä¾‹å¦‚ï¼Œå¸¦æœ‰ResNet-101çš„ImageNet-1Kï¼‰ï¼Œå¹¿æ³›çš„ä¼˜åŒ–ç©ºé—´é™åˆ¶äº†æ€§èƒ½ï¼Œé™ä½äº†å…¶å®ç”¨æ€§ã€‚æœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç›´æ¥ç”Ÿæˆä¿¡æ¯å›¾åƒï¼Œé¿å…äº†åƒç´ çº§çš„ä¼˜åŒ–ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é¢ä¸´é¢„è®­ç»ƒæ¨¡å‹å’Œç›®æ ‡æ•°æ®é›†ä¹‹é—´åˆ†å¸ƒè½¬ç§»çš„æŒ‘æˆ˜ï¼Œä»¥åŠåœ¨ä¸åŒè®¾ç½®ä¸‹éœ€è¦å¤šæ¬¡è’¸é¦æ­¥éª¤çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ç°æœ‰åŸºäºæ‰©æ•£çš„è’¸é¦æ–¹æ³•æ­£äº¤çš„æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€‰æ‹©è€Œä¸æ˜¯ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåŸºäºè¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºï¼ˆå¸¦æœ‰æˆ–ä¸å¸¦æ ‡ç­¾æ–‡æœ¬ï¼‰é¢„æµ‹ç”±æ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„å™ªå£°ï¼Œç„¶åè®¡ç®—æ¯å¯¹ç›¸åº”çš„æŸå¤±ã€‚é€šè¿‡æŸå¤±å·®å¼‚ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºåŸå§‹å›¾åƒçš„ç‹¬ç‰¹åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹é€‰å®šçš„è¡¥ä¸è¿›è¡Œç±»å†…èšç±»å’Œæ’åï¼Œä»¥ä¿æŒå¤šæ ·æ€§çº¦æŸã€‚è¿™ç§ç®€åŒ–çš„æ¡†æ¶ä½¿å•æ­¥è’¸é¦è¿‡ç¨‹æˆä¸ºå¯èƒ½ï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æŒ‡æ ‡ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09959v2">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€‰æ‹©è€Œéç”Ÿæˆçš„æ–°æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰è’¸é¦æ–¹æ³•åœ¨å¤„ç†å¤§å‹æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œæ—¶çš„æ€§èƒ½é™ä½é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å™ªå£°æ¥è¯†åˆ«åŸå§‹å›¾åƒä¸­çš„ç‹¬ç‰¹åŒºåŸŸï¼Œå¹¶é‡‡ç”¨ç±»å†…èšç±»ä¸æ’åæ¥ä¿æŒå¤šæ ·æ€§çº¦æŸï¼Œå®ç°å•æ­¥è’¸é¦è¿‡ç¨‹ï¼Œå¹¶åœ¨å„ç§æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®é›†è’¸é¦ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„ä¼˜åŒ–æ€è·¯ã€‚</li>
<li>ä¼ ç»Ÿè’¸é¦æ–¹æ³•åœ¨å¤„ç†å¤§å‹æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œæ—¶å­˜åœ¨æ€§èƒ½é™åˆ¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ–°æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€‰æ‹©ï¼Œè€Œä¸æ˜¯ç”Ÿæˆå›¾åƒã€‚</li>
<li>é€šè¿‡é¢„æµ‹æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å™ªå£°å’Œè®¡ç®—æŸå¤±å·®å¼‚ï¼Œè¯†åˆ«åŸå§‹å›¾åƒä¸­çš„ç‹¬ç‰¹åŒºåŸŸã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨ç±»å†…èšç±»ä¸æ’åæ¥ä¿æŒå¤šæ ·æ€§çº¦æŸã€‚</li>
<li>å®ç°äº†ä¸€ç§å•æ­¥è’¸é¦è¿‡ç¨‹ï¼Œç®€åŒ–äº†æµç¨‹å¹¶æé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ef313b351e32625b5bc7666e5b03a2fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1cc4c96ad788fd7e73452b326ccb4ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca2860f93f47efd33301a053ad22e258.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2e3b569d9e9dec367d35ac3d55caf4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-137d154fa4dae3f25d35c1f318898f4d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DiffGuard-Text-Based-Safety-Checker-for-Diffusion-Models"><a href="#DiffGuard-Text-Based-Safety-Checker-for-Diffusion-Models" class="headerlink" title="DiffGuard: Text-Based Safety Checker for Diffusion Models"></a>DiffGuard: Text-Based Safety Checker for Diffusion Models</h2><p><strong>Authors:Massine El Khader, Elias Al Bouzidi, Abdellah Oumida, Mohammed Sbaihi, Eliott Binard, Jean-Philippe Poli, Wassila Ouerdane, Boussad Addad, Katarzyna Kapusta</strong></p>
<p>Recent advances in Diffusion Models have enabled the generation of images from text, with powerful closed-source models like DALL-E and Midjourney leading the way. However, open-source alternatives, such as StabilityAIâ€™s Stable Diffusion, offer comparable capabilities. These open-source models, hosted on Hugging Face, come equipped with ethical filter protections designed to prevent the generation of explicit images. This paper reveals first their limitations and then presents a novel text-based safety filter that outperforms existing solutions. Our research is driven by the critical need to address the misuse of AI-generated content, especially in the context of information warfare. DiffGuard enhances filtering efficacy, achieving a performance that surpasses the best existing filters by over 14%. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä½¿å¾—ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒæˆä¸ºå¯èƒ½ï¼Œå¼ºå¤§çš„å°é—­æºä»£ç æ¨¡å‹ï¼Œå¦‚DALL-Eå’ŒMidjourneyï¼Œå¼•é¢†ç€è¿™ä¸€è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œå¼€æºçš„æ›¿ä»£å“ï¼Œå¦‚StabilityAIçš„ç¨³å®šæ‰©æ•£ï¼Œä¹Ÿæä¾›äº†ç›¸å½“çš„èƒ½åŠ›ã€‚è¿™äº›å¼€æºæ¨¡å‹æ‰˜ç®¡åœ¨Hugging Faceä¸Šï¼Œé…å¤‡äº†æ—¨åœ¨é˜²æ­¢ç”Ÿæˆæ˜ç¡®å›¾åƒçš„ä¼¦ç†è¿‡æ»¤å™¨ä¿æŠ¤ã€‚æœ¬æ–‡é¦–å…ˆæ­ç¤ºäº†å®ƒä»¬çš„å±€é™æ€§ï¼Œç„¶åæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ–‡æœ¬çš„å®‰å…¨è¿‡æ»¤å™¨ï¼Œå®ƒè¶…è¶Šäº†ç°æœ‰çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¯ç”±è§£å†³äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹æ»¥ç”¨é—®é¢˜çš„è¿«åˆ‡éœ€æ±‚æ‰€é©±åŠ¨çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æˆ˜çš„å¤§èƒŒæ™¯ä¸‹ã€‚DiffGuardæé«˜äº†è¿‡æ»¤æ•ˆç‡ï¼Œæ€§èƒ½è¶…è¿‡äº†ç°æœ‰æœ€ä½³è¿‡æ»¤å™¨è¶…è¿‡14%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00064v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ç”Ÿæˆçš„æ–°è¿›å±•ä½¿å¾—Diffusion Modelsèƒ½å¤Ÿå®ç°æ–‡æœ¬é©±åŠ¨ç”Ÿæˆå›¾åƒã€‚é—­æºæ¨¡å‹å¦‚DALL-Eå’ŒMidjourneyè¡¨ç°å“è¶Šï¼Œè€Œå¼€æºæ¨¡å‹å¦‚StabilityAIçš„Stable Diffusionä¹Ÿå…·å¤‡ç›¸å½“èƒ½åŠ›ï¼Œå¹¶ä¸”åŠ å…¥äº†é˜²æ­¢ç”Ÿæˆä¸å½“å›¾åƒçš„ä¼¦ç†æ»¤é•œä¿æŠ¤ã€‚æœ¬ç ”ç©¶åˆ†æäº†è¿™äº›æ¨¡å‹çš„å±€é™å¹¶æå‡ºä¸€ç§æ–°å‹çš„æ–‡æœ¬å®‰å…¨æ»¤é•œDiffGuardï¼Œå…¶æ€§èƒ½è¶…è¶Šç°æœ‰è§£å†³æ–¹æ¡ˆè¶…è¿‡14%ï¼Œæ—¨åœ¨è§£å†³AIç”Ÿæˆå†…å®¹çš„æ»¥ç”¨é—®é¢˜ï¼Œå°¤å…¶åœ¨ä¿¡æ¯æˆ˜ç¯å¢ƒä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Modelsèƒ½å¤Ÿå®ç°æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>é—­æºæ¨¡å‹å¦‚DALL-Eå’ŒMidjourneyåœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å¼€æºæ¨¡å‹å¦‚Stable Diffusionå…·å¤‡ç›¸ä¼¼èƒ½åŠ›ï¼Œå¹¶é…å¤‡äº†é˜²æ­¢ç”Ÿæˆä¸å½“å›¾åƒçš„ä¼¦ç†æ»¤é•œã€‚</li>
<li>ç°æœ‰æ¨¡å‹å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬å®‰å…¨æ»¤é•œDiffGuardã€‚</li>
<li>DiffGuardçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰è§£å†³æ–¹æ¡ˆè‡³å°‘14%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2c398ee918e3722147d8b395a5328e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b3ff13bed62f0a4e231b48b90ae13b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-29aea790179731f0c7c91e75e110516b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1511576b238adf97401d4e6f330dbd9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f67ac79449ce86e4eaa4bbccb6537f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a79161c4df0531e7c72cec0b3bfecb1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6eca8a79dc59d5583ee72c033c77183.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8fa4b98d15dba36153a9bcf62421057.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa33df6edac868ef6ef5f8b27fa98412.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SMITE-Segment-Me-In-TimE"><a href="#SMITE-Segment-Me-In-TimE" class="headerlink" title="SMITE: Segment Me In TimE"></a>SMITE: Segment Me In TimE</h2><p><strong>Authors:Amirhossein Alimohammadi, Sauradip Nag, Saeid Asgari Taghanaki, Andrea Tagliasacchi, Ghassan Hamarneh, Ali Mahdavi Amiri</strong></p>
<p>Segmenting an object in a video presents significant challenges. Each pixel must be accurately labelled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks are defined based on only one or a few sample images. In this paper, we address this issue by employing a pre-trained text to image diffusion model supplemented with an additional tracking mechanism. We demonstrate that our approach can effectively manage various segmentation scenarios and outperforms state-of-the-art alternatives. </p>
<blockquote>
<p>åœ¨è§†é¢‘ä¸­åˆ†å‰²å¯¹è±¡å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚æ¯ä¸ªåƒç´ éƒ½å¿…é¡»è¢«ç²¾ç¡®æ ‡è®°ï¼Œè¿™äº›æ ‡ç­¾å¿…é¡»åœ¨å„å¸§ä¹‹é—´ä¿æŒä¸€è‡´ã€‚å½“åˆ†å‰²å…·æœ‰ä»»æ„ç²’åº¦æ—¶ï¼Œéš¾åº¦ä¼šå¢åŠ ï¼Œæ„å‘³ç€æ®µè½çš„æ•°é‡å¯ä»¥ä»»æ„å˜åŒ–ï¼Œæ©ç ä»…åŸºäºä¸€ä¸ªæˆ–å‡ ä¸ªæ ·æœ¬å›¾åƒæ¥å®šä¹‰ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¹¶è¾…ä»¥é¢å¤–çš„è·Ÿè¸ªæœºåˆ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ç®¡ç†å„ç§åˆ†å‰²åœºæ™¯å¹¶ä¼˜äºæœ€å…ˆè¿›çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18538v2">PDF</a> ICLR 2025; Project page is at <a target="_blank" rel="noopener" href="https://segment-me-in-time.github.io/">https://segment-me-in-time.github.io/</a></p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡è§£å†³äº†è§†é¢‘å¯¹è±¡åˆ†å‰²çš„é—®é¢˜ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¹¶è¾…ä»¥é¢å¤–çš„è·Ÿè¸ªæœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆå¤„ç†å„ç§åˆ†å‰²åœºæ™¯ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§†é¢‘å¯¹è±¡åˆ†å‰²é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦å‡†ç¡®æ ‡è®°æ¯ä¸ªåƒç´ ï¼Œå¹¶ä¸”åœ¨å„å¸§ä¹‹é—´ä¿æŒæ ‡ç­¾ä¸€è‡´æ€§ã€‚</li>
<li>å½“åˆ†å‰²å…·æœ‰ä»»æ„ç²’åº¦æ—¶ï¼ŒæŒ‘æˆ˜ä¼šåŠ å¤§ï¼Œæ„å‘³ç€ç‰‡æ®µæ•°é‡å¯ä»¥ä»»æ„å˜åŒ–ï¼Œè€Œæ©æ¨¡ä»…åŸºäºä¸€ä¸ªæˆ–å°‘æ•°æ ·æœ¬å›¾åƒå®šä¹‰ã€‚</li>
<li>æœ¬æ–‡é‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>é€šè¿‡é™„åŠ çš„è·Ÿè¸ªæœºåˆ¶ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç®¡ç†å„ç§åˆ†å‰²åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ–¹æ³•å¯¹äºè§†é¢‘å¯¹è±¡åˆ†å‰²å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-70e9dcbbdfbdd918b8a376538fb65d7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b17772af44a5b00abe364b1507bb297.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e48ecb51ec524eaadfd9341eb5abd6d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d684073abdc7cfe4990aed844ba4b17.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Denoising-as-Adaptation-Noise-Space-Domain-Adaptation-for-Image-Restoration"><a href="#Denoising-as-Adaptation-Noise-Space-Domain-Adaptation-for-Image-Restoration" class="headerlink" title="Denoising as Adaptation: Noise-Space Domain Adaptation for Image   Restoration"></a>Denoising as Adaptation: Noise-Space Domain Adaptation for Image   Restoration</h2><p><strong>Authors:Kang Liao, Zongsheng Yue, Zhouxia Wang, Chen Change Loy</strong></p>
<p>Although learning-based image restoration methods have made significant progress, they still struggle with limited generalization to real-world scenarios due to the substantial domain gap caused by training on synthetic data. Existing methods address this issue by improving data synthesis pipelines, estimating degradation kernels, employing deep internal learning, and performing domain adaptation and regularization. Previous domain adaptation methods have sought to bridge the domain gap by learning domain-invariant knowledge in either feature or pixel space. However, these techniques often struggle to extend to low-level vision tasks within a stable and compact framework. In this paper, we show that it is possible to perform domain adaptation via the noise space using diffusion models. In particular, by leveraging the unique property of how auxiliary conditional inputs influence the multi-step denoising process, we derive a meaningful diffusion loss that guides the restoration model in progressively aligning both restored synthetic and real-world outputs with a target clean distribution. We refer to this method as denoising as adaptation. To prevent shortcuts during joint training, we present crucial strategies such as channel-shuffling layer and residual-swapping contrastive learning in the diffusion model. They implicitly blur the boundaries between conditioned synthetic and real data and prevent the reliance of the model on easily distinguishable features. Experimental results on three classical image restoration tasks, namely denoising, deblurring, and deraining, demonstrate the effectiveness of the proposed method. </p>
<blockquote>
<p>è™½ç„¶åŸºäºå­¦ä¹ çš„å›¾åƒæ¢å¤æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒå¯¼è‡´çš„åŸŸå·®è·è¾ƒå¤§ï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥æ¨å¹¿åˆ°ç°å®åœºæ™¯ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ”¹è¿›æ•°æ®åˆæˆç®¡é“ã€ä¼°è®¡é€€åŒ–æ ¸ã€é‡‡ç”¨æ·±åº¦å†…éƒ¨å­¦ä¹ ä»¥åŠæ‰§è¡ŒåŸŸé€‚åº”å’Œæ­£åˆ™åŒ–æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä»¥å‰çš„åŸŸé€‚åº”æ–¹æ³•è¯•å›¾é€šè¿‡åœ¨å­¦ä¹ ç‰¹å¾æˆ–åƒç´ ç©ºé—´ä¸­çš„åŸŸä¸å˜çŸ¥è¯†æ¥ç¼©å°åŸŸå·®è·ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯åœ¨ç¨³å®šä¸”ç´§å‡‘çš„æ¡†æ¶å†…å¾€å¾€éš¾ä»¥æ‰©å±•åˆ°ä½çº§è§†è§‰ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¯ä»¥é€šè¿‡ä½¿ç”¨æ‰©æ•£æ¨¡å‹åœ¨å™ªå£°ç©ºé—´ä¸­è¿›è¡ŒåŸŸé€‚åº”ã€‚ç‰¹åˆ«æ˜¯ï¼Œé€šè¿‡åˆ©ç”¨è¾…åŠ©æ¡ä»¶è¾“å…¥å¦‚ä½•å½±å“å¤šæ­¥å»å™ªè¿‡ç¨‹çš„ç‹¬ç‰¹å±æ€§ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªæœ‰æ„ä¹‰çš„æ‰©æ•£æŸå¤±ï¼Œè¯¥æŸå¤±æŒ‡å¯¼æ¢å¤æ¨¡å‹é€æ­¥å¯¹é½æ¢å¤åˆæˆå’ŒçœŸå®ä¸–ç•Œè¾“å‡ºä¸ç›®æ ‡æ¸…æ´åˆ†å¸ƒã€‚æˆ‘ä»¬å°†è¿™ç§æ–¹æ³•ç§°ä¸ºå»å™ªé€‚åº”ã€‚ä¸ºäº†é˜²æ­¢è”åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ·å¾„ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£æ¨¡å‹ä¸­æå‡ºäº†å…³é”®ç­–ç•¥ï¼Œå¦‚é€šé“æ··æ´—å±‚å’Œæ®‹å·®äº¤æ¢å¯¹æ¯”å­¦ä¹ ã€‚å®ƒä»¬éšå«åœ°æ¨¡ç³Šäº†å—æ¡ä»¶çº¦æŸçš„åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„è¾¹ç•Œï¼Œå¹¶é˜²æ­¢æ¨¡å‹ä¾èµ–äºå®¹æ˜“åŒºåˆ†çš„ç‰¹å¾ã€‚åœ¨ä¸‰ä¸ªç»å…¸å›¾åƒæ¢å¤ä»»åŠ¡â€”â€”å»å™ªã€å»æ¨¡ç³Šå’Œå»é›¨â€”â€”ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18516v3">PDF</a> Accepted by ICLR2025. Project Page:   <a target="_blank" rel="noopener" href="https://kangliao929.github.io/projects/noise-da/">https://kangliao929.github.io/projects/noise-da/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡åœ¨å™ªå£°ç©ºé—´åˆ©ç”¨è¾…åŠ©æ¡ä»¶è¾“å…¥å½±å“å¤šæ­¥å»å™ªè¿‡ç¨‹ï¼Œå®ç°åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œè¾“å‡ºä¸ç›®æ ‡æ¸…æ´åˆ†å¸ƒçš„é€æ­¥å¯¹é½ã€‚é€šè¿‡é€šé“æ··æ´—å±‚å’Œå‰©ä½™äº¤æ¢å¯¹æ¯”å­¦ä¹ ç­‰ç­–ç•¥ï¼Œé˜²æ­¢è”åˆè®­ç»ƒä¸­çš„æ·å¾„é—®é¢˜ï¼Œæ¨¡ç³Šåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„ç•Œé™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå»å™ªã€å»æ¨¡ç³Šå’Œå»é›¨ç­‰ä¸‰ä¸ªç»å…¸ä»»åŠ¡ä¸­æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºåŸŸè‡ªé€‚åº”ï¼Œé€šè¿‡åœ¨å™ªå£°ç©ºé—´åˆ©ç”¨è¾…åŠ©æ¡ä»¶è¾“å…¥å½±å“å»å™ªè¿‡ç¨‹ï¼Œå®ç°åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æŸå¤±å‡½æ•°ï¼Œç”¨äºæŒ‡å¯¼æ¢å¤æ¨¡å‹é€æ­¥å¯¹é½æ¢å¤åçš„åˆæˆå’ŒçœŸå®ä¸–ç•Œè¾“å‡ºä¸ç›®æ ‡æ¸…æ´åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡é€šé“æ··æ´—å±‚å’Œå‰©ä½™äº¤æ¢å¯¹æ¯”å­¦ä¹ ç­‰ç­–ç•¥ï¼Œé˜²æ­¢æ¨¡å‹åœ¨è”åˆè®­ç»ƒä¸­å‡ºç°æ·å¾„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒå»å™ªã€å»æ¨¡ç³Šå’Œå»é›¨ä¸‰ä¸ªç»å…¸ä»»åŠ¡ä¸­å–å¾—äº†å®éªŒæ€§çš„æˆåŠŸã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ç¨³å®šä¸”ç´§å‡‘çš„æ¡†æ¶å†…æ‰©å±•è‡³ä½å±‚æ¬¡è§†è§‰ä»»åŠ¡ã€‚</li>
<li>è¿™ç§æ–¹æ³•æœ‰åŠ©äºç¼©å°åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œåœºæ™¯ä¹‹é—´çš„åŸŸå·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.18516">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f14807528e9cd07a5364c27f2a7fc4d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf4f8c0318fe2d2c3b5ceb1a32ebffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d2967c826e8f022942855d441948047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b28552c6261d625650f1114cafcd765.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dc7f5a85a37c6456a0323059b9394121.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-21  MGFI-Net A Multi-Grained Feature Integration Network for Enhanced   Medical Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f96cd34a36943bfc7e3b0180519f71e6.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-21  Geometry-Aware Diffusion Models for Multiview Scene Inpainting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
