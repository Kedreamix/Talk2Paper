<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-21  Where&#39;s the Bug? Attention Probing for Scalable Fault Localization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-27d2c2cfdc1b6b372071f675d6641038.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    74 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-21-更新"><a href="#2025-02-21-更新" class="headerlink" title="2025-02-21 更新"></a>2025-02-21 更新</h1><h2 id="Where’s-the-Bug-Attention-Probing-for-Scalable-Fault-Localization"><a href="#Where’s-the-Bug-Attention-Probing-for-Scalable-Fault-Localization" class="headerlink" title="Where’s the Bug? Attention Probing for Scalable Fault Localization"></a>Where’s the Bug? Attention Probing for Scalable Fault Localization</h2><p><strong>Authors:Adam Stein, Arthur Wayne, Aaditya Naik, Mayur Naik, Eric Wong</strong></p>
<p>Ensuring code correctness remains a challenging problem even as large language models (LLMs) become increasingly capable at code-related tasks. While LLM-based program repair systems can propose bug fixes using only a user’s bug report, their effectiveness is fundamentally limited by their ability to perform fault localization (FL), a challenging problem for both humans and LLMs. Existing FL approaches rely on executable test cases, require training on costly and often noisy line-level annotations, or demand resource-intensive LLMs. In this paper, we present Bug Attention Probe (BAP), a method which learns state-of-the-art fault localization without any direct localization labels, outperforming traditional FL baselines and prompting of large-scale LLMs. We evaluate our approach across a variety of code settings, including real-world Java bugs from the standard Defects4J dataset as well as seven other datasets which span a diverse set of bug types and languages. Averaged across all eight datasets, BAP improves by 34.6% top-1 accuracy compared to the strongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost. </p>
<blockquote>
<p>确保代码正确性仍然是一个具有挑战性的问题，即使大型语言模型（LLM）在代码相关任务上越来越强大。虽然基于LLM的程序修复系统仅使用用户的错误报告就可以提出错误修复建议，但它们的有效性从根本上受到故障定位（FL）能力的限制，这对于人类和LLM来说都是一项具有挑战性的任务。现有的故障定位方法依赖于可执行的测试用例，需要基于昂贵的、通常带有噪声的行级注释进行训练，或者需要资源密集型的LLM。在本文中，我们介绍了Bug Attention Probe（BAP）方法，该方法可以在没有任何直接定位标签的情况下实现最先进的故障定位，并且优于传统的故障定位基准线和大型LLM的提示。我们在多种代码设置中对我们的方法进行了评估，包括使用标准Defects4J数据集的真实世界Java错误以及其他涵盖各种错误类型和语言的七个数据集。在八个数据集上平均，BAP在最高准确率方面比最强基准线提高了34.6%，比零样本提示GPT-4o提高了93.4%。此外，BAP还显著优于提示方法，在计算成本方面大大优于大型开放式模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13966v1">PDF</a> 14 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在代码相关任务上的能力日益增强，但保证代码正确性仍然是一个挑战性问题。LLM为基础的程序修复系统可以通过用户的错误报告提出修复建议，但其有效性受限于故障定位（FL）的能力。本文提出了一种新的方法——Bug Attention Probe（BAP），该方法无需直接定位标签即可学习最先进的故障定位，且在多种代码设置上优于传统FL基准测试和大型LLM提示。在包含Defects4J数据集在内的八个数据集上进行的评估显示，BAP在平均准确率上较最强基准测试提高了34.6%，较GPT-4o零样本提示提高了93.4%。此外，BAP在计算成本方面也具有更高的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）在代码相关任务上的能力增强，但代码正确性仍是挑战。</li>
<li>LLM为基础的修复系统受限于故障定位（FL）的能力。</li>
<li>Bug Attention Probe（BAP）方法无需直接定位标签即可学习先进的故障定位。</li>
<li>BAP在多种代码设置上优于传统FL方法和大型LLM提示。</li>
<li>在多个数据集上的评估显示，BAP较最强基准测试和GPT-4o有显著提高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c0d864b6420c9d030ac44a5d9d841fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a683254e6f6d92b6a2303c811fb6b5e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc1304120d3f3920b579c3724ebac90f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a9023d55f2927df29c2df883a33cd1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-21\./crop_LLM/2502.13966v1/page_4_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f034e44463d8eed92904c558994909.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c96b5ccb55f30674f861f0e070e671.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs"><a href="#Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs" class="headerlink" title="Autellix: An Efficient Serving Engine for LLM Agents as General Programs"></a>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</h2><p><strong>Authors:Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica</strong></p>
<p>Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs’ previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM. </p>
<blockquote>
<p>大型语言模型（LLM）应用正逐渐从简单的聊天机器人演变为动态、通用的代理程序，它们扩展了LLM的调用和输出令牌，帮助AI代理进行推理、探索和解决复杂任务。然而，现有的LLM服务系统忽略了程序和调用之间的依赖关系，失去了优化的重要机会。我们的分析表明，提交给LLM服务引擎的程序会经历长时间的累计等待时间，主要是由于个别LLM请求和程序的头线阻塞。为了解决这一问题，我们引入了Autellix，一个将程序视为首要公民的LLM服务系统，以最小化其端到端延迟。Autellix拦截程序提交的LLM调用，丰富调度器的程序级上下文。我们为单线程和分布式程序提出了两种调度算法，这些算法会根据程序之前完成的调用预先判断和优先处理LLM调用。我们的评估表明，在多种LLM和代理工作负载中，与最新系统（如vLLM）相比，Autellix在相同延迟下提高了程序的吞吐量4-15倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13965v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）应用正由简单的聊天机器人演变为动态、通用的智能代理程序。现有LLM服务系统忽视了程序和调用间的依赖关系，导致AI代理在处理复杂任务时受限。为此，我们提出了Autellix系统，它通过丰富调度器的程序级上下文来处理程序作为首要任务，以减少端到端的延迟。评估显示，Autellix在多样化的LLM和智能代理工作负载上，提高了程序的吞吐量，同时与现有系统相比，延迟保持不变。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM应用正演变为更动态、通用的智能代理程序，涉及复杂的推理和问题解决。</li>
<li>现有LLM服务系统忽视了程序和调用间的依赖，导致优化机会丧失。</li>
<li>Autellix系统处理程序作为首要任务，通过丰富调度器的程序级上下文来减少端到端的延迟。</li>
<li>Autellix提出了针对单线程和分布式程序的两种调度算法。</li>
<li>调度算法基于程序之前完成的调用，预先判断和优先处理LLM调用。</li>
<li>评估显示，Autellix在多种LLM和智能代理工作负载上，提高了程序的吞吐量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe6e516ea5fa6e30cc96400b0d184f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86c3539620d46ac89df892574201f42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94e9cf67ae67fc03b242f66ba640e91a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aa7a7d3e85e67f5c7a052d4308cc4fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1ff1ea4dd180efb1ece3ddbd6c69554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac7d3dee34d87a065c3290ed7c22d37f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Neurosymbolic-artificial-intelligence-via-large-language-models-and-coherence-driven-inference"><a href="#Neurosymbolic-artificial-intelligence-via-large-language-models-and-coherence-driven-inference" class="headerlink" title="Neurosymbolic artificial intelligence via large language models and   coherence-driven inference"></a>Neurosymbolic artificial intelligence via large language models and   coherence-driven inference</h2><p><strong>Authors:Steve Huntsman, Jewell Thomas</strong></p>
<p>We devise an algorithm to generate sets of propositions that objectively instantiate graphs that support coherence-driven inference. We then benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a straightforward transformation of) propositions expressed in natural language, with promising results from a single prompt to models optimized for reasoning. Combining coherence-driven inference with consistency evaluations by neural models may advance the state of the art in machine cognition. </p>
<blockquote>
<p>我们设计了一种算法，用于生成客观实例化图集，这些图集支持连贯推理。然后，我们以命题为基准，评估大型语言模型（LLM）从自然语言表达的命题（进行简单转换后）重建连贯图的能力，优化推理的模型从单一提示中取得了令人鼓舞的结果。将连贯推理与神经网络模型的一致性评估相结合，可能会推动机器认知领域的最新进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13953v1">PDF</a> </p>
<p><strong>Summary</strong>：我们设计了一种算法，可以根据命题生成支持连贯推理的图。然后，我们评估大型语言模型（LLM）从自然语言表达的命题重建连贯图的能力，并从单一提示中对优化推理的模型进行基准测试，取得了令人鼓舞的结果。结合连贯推理和神经网络模型的一致性评估可能会推动机器认知的最新进展。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>提出了一种算法，能够根据命题生成支持连贯推理的图。</li>
<li>大型语言模型（LLM）在重建连贯图方面表现出了潜力。</li>
<li>通过单一提示对优化推理的模型进行基准测试取得了令人鼓舞的结果。</li>
<li>结合连贯推理和神经网络模型的一致性评估有助于推动机器认知的进步。</li>
<li>该算法有助于增强语言模型的推理能力。</li>
<li>这种方法在自然语言处理和人工智能领域具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13953">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4f41c4bf883da083ba57b93b839d78ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21c14cb2ed3d7c6ce84715247113a5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38ab58be5d2ceebb3994bded09ac9c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb8d35093314793e531b80161ffa418c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LongPO-Long-Context-Self-Evolution-of-Large-Language-Models-through-Short-to-Long-Preference-Optimization"><a href="#LongPO-Long-Context-Self-Evolution-of-Large-Language-Models-through-Short-to-Long-Preference-Optimization" class="headerlink" title="LongPO: Long Context Self-Evolution of Large Language Models through   Short-to-Long Preference Optimization"></a>LongPO: Long Context Self-Evolution of Large Language Models through   Short-to-Long Preference Optimization</h2><p><strong>Authors:Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales. </p>
<blockquote>
<p>大型语言模型（LLM）通过预训练和对齐过程展现了惊人的能力。然而，在具有充足对齐的长语境场景下，优秀的短语境LLM可能会表现不佳。由于长语境对齐不足以及人类为扩展语境进行标注的不切实际性和平衡短语境和长语境性能的困难，这一对齐过程仍然具有挑战性。为了应对这些挑战，我们引入了LongPO方法，它通过内部转移短语境能力，使短语境LLM能够在长语境任务上自我进化并表现出色。LongPO利用LLM从自我生成的短到长的偏好数据中学习，这些数据由为相同指令生成的配对响应组成，包括长语境输入及其压缩的短语境对应物。这种偏好揭示了短语境对齐过程中培养的LLM的能力和潜力，这些能力在长语境对齐不足的情况下可能会减弱。此外，LongPO还融入了一个短到长的KL约束，以缓解长语境对齐过程中短语境性能的下降。当应用于Mistral-7B-Instruct-v0.2模型，从128K到512K的语境长度时，LongPO充分保留了短语境性能，并在长短语境任务中大大优于简单的SFT和DPO。具体来说，经过我们方法训练的模型可以在长语境基准测试上取得与或超越高级LLM（例如涉及广泛长语境标注和更大参数规模的GPT-4-128K）的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13922v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过预训练和对齐展现出惊人的能力。然而，在长篇上下文场景中，优秀的短篇上下文LLM可能会因缺乏长篇上下文对齐而表现不佳。为解决这一挑战，我们提出了LongPO方法，它能够让短篇上下文LLM通过内部转移短语境能力自我进化，从而在长篇上下文任务上表现出色。LongPO利用LLM从自我生成的短到长的偏好数据中学习，这些数据由为相同指令生成的长篇上下文输入和相应的短篇上下文压缩版本配对响应组成。通过这种方式，LongPO揭示了LLM在短篇上下文对齐过程中培养的能力和潜力，这些能力可能在长篇上下文场景中因对齐不足而减弱。此外，LongPO还引入了一个短到长的KL约束，以缓解长篇上下文对齐过程中短篇上下文性能下降的问题。当应用于Mistral-7B-Instruct-v0.2模型时，从128K到512K的语境长度范围内，LongPO可以完全保留短篇上下文的性能，并在长短语境任务中大大优于简单的SFT和DPO。特别是，经过LongPO训练的模型在长篇上下文基准测试上的表现可与或超越顶级LLM（如GPT-4-128K）相当，这些LLM涉及大量的长篇上下文标注和更大的参数规模。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs通过预训练和对齐展现出强大的能力，但在长篇上下文场景中可能因缺乏足够的长篇上下文对齐而表现不佳。</li>
<li>LongPO方法能够让短篇上下文的LLM自我进化，通过内部转移短语境能力，在长篇上下文任务上实现卓越表现。</li>
<li>LongPO利用LLM学习自我生成的短到长的偏好数据，揭示LLM在短篇上下文对齐过程中的能力和潜力。</li>
<li>LongPO引入短到长的KL约束，以缓解长篇上下文对齐过程中的短篇上下文性能下降问题。</li>
<li>LongPO在保持短篇上下文性能的同时，提升了长篇上下文任务的表现。</li>
<li>LongPO训练的模型在长篇上下文基准测试上的表现可超越一些需要更多长篇上下文标注和更大参数规模的顶级LLM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13922">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a9ab7be1c30c4a97b26e77f18fba7648.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04f70589ed2f63183d95a36e76e000f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-928089c3d6230cd36df83f0509d07f6c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TESS-2-A-Large-Scale-Generalist-Diffusion-Language-Model"><a href="#TESS-2-A-Large-Scale-Generalist-Diffusion-Language-Model" class="headerlink" title="TESS 2: A Large-Scale Generalist Diffusion Language Model"></a>TESS 2: A Large-Scale Generalist Diffusion Language Model</h2><p><strong>Authors:Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan</strong></p>
<p>We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2">https://github.com/hamishivi/tess-2</a>. </p>
<blockquote>
<p>我们介绍了TESS 2，这是一个通用指令遵循扩散语言模型，它超越了当代指令优化扩散模型，与强大的自回归（AR）模型相匹配，有时甚至超过它们。我们通过首先使用强大的AR模型进行持续预训练，以常规的交叉熵作为扩散损失，然后进一步进行指令调整来训练TESS 2。我们发现适应训练以及基础模型的选择对于训练良好的指令遵循扩散模型至关重要。我们进一步提出奖励指导，这是一种新型模块化的推理时间指导程序，无需训练底层模型即可对齐模型输出。最后，我们展示了随着推理时间计算量的增加，TESS 2的性能会进一步提高，这突出了扩散LM在推理时间使用计算量的精细控制方面的实用性。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hamishivi/tess-2获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13917v1">PDF</a> preprint</p>
<p><strong>摘要</strong></p>
<p>本文介绍了TESS 2，这是一种通用指令遵循扩散语言模型，它超越了当代指令调整扩散模型的表现，并且与强大的自回归（AR）模型相匹配甚至在某些方面超过它们。我们通过首先使用常规交叉熵作为扩散损失对强大的AR模型进行持续预训练来训练TESS 2，然后进行进一步的指令调整。我们发现适应训练以及基础模型的选择对于训练良好的指令遵循扩散模型至关重要。我们还提出了奖励指导，这是一种新型模块化的推理时间指导程序，可以在不需要训练底层模型的情况下对齐模型输出。最后，我们表明，随着推理时间计算量的增加，TESS 2的性能可以进一步提高，这突出了扩散语言模型在推理时间使用的计算量方面具有精细可控性的实用性。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>TESS 2是一种通用指令遵循扩散语言模型，表现优于当代指令调整扩散模型和强大的自回归模型。</li>
<li>TESS 2的训练包括使用交叉熵作为扩散损失的持续预训练，以及进一步的指令调整。</li>
<li>适应训练和基础模型的选择对于训练良好的指令遵循扩散模型至关重要。</li>
<li>提出了奖励指导，这是一种新型的推理时间指导程序，可以在不需要训练底层模型的情况下对齐模型输出。</li>
<li>TESS 2的性能随着推理时间计算量的增加而提高。</li>
<li>扩散语言模型具有在推理时间使用的计算量方面的精细可控性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13917">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-49a2986a21913a916752c76c54995e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0306a4da6cef91012aed6eac48cc0e31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ac1d9e2084d7b36cb18481b5fe2fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35b02f49e8d9097cfdd170eef5bcbfa9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation"><a href="#Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation" class="headerlink" title="Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?"></a>Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?</h2><p><strong>Authors:Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park</strong></p>
<p>Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in users’ item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMs’ ability to understand users’ item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec">https://github.com/Sein-Kim/LLM-SRec</a>. </p>
<blockquote>
<p>大型语言模型（LLM）由于其先进的文本理解能力和上下文意识，最近被公认为推荐工具中的有前途的工具。尽管当前基于LLM的推荐（LLM4Rec）模型是在顺序推荐场景下进行训练和评估的，但我们发现这些模型是否理解用户项目交互序列中固有的顺序信息却被大大忽视了。在本文中，我们首先通过一系列实验证明，现有的LLM4Rec模型在训练和推理过程中并没有完全捕获顺序信息。然后，我们提出了一种简单有效的基于LLM的顺序推荐器，称为LLM-SRec。这是一种通过蒸馏从预训练的CF-SRec模型中提取的用户表示，将其融入LLMs中，从而增强LLMs对顺序信息的整合的方法。我们的大量实验表明，LLM-SRec增强了LLMs理解用户项目交互序列的能力，最终提高了推荐性能。此外，与现有的需要微调LLMs的LLM4Rec模型不同，LLM-SRec仅通过训练一些轻量级的MLPs就实现了最先进的性能，这凸显了其在现实世界应用中的实用性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Sein-Kim/LLM-SRec找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13909v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>大语言模型（LLM）在推荐系统中有广泛应用前景，因其强大的文本理解能力和上下文意识。但现有LLM在推荐（LLM4Rec）模型中往往忽略用户交互序列中的时序信息。本文通过实验验证现有LLM4Rec模型在训练和推理过程中未能充分捕捉时序信息，并提出一种简单有效的基于LLM的时序推荐方法LLM-SRec。该方法通过蒸馏预训练CF-SRec模型中的用户表示到LLM中，增强了LLM理解用户交互序列的能力，提高了推荐性能。并且相较于现有LLM4Rec模型，LLM-SRec仅需训练少量轻量级多层感知机（MLPs）即可达到业界领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs展现出在推荐系统中的潜力，归功于其强大的文本理解能力和上下文意识。</li>
<li>现有LLM4Rec模型忽略了用户交互序列中的时序信息。</li>
<li>通过实验验证，现有LLM4Rec模型在训练和推理过程中对时序信息的捕捉不足。</li>
<li>提出一种基于LLM的时序推荐方法LLM-SRec，能有效增强LLM理解用户交互序列的能力。</li>
<li>LLM-SRec通过蒸馏预训练CF-SRec模型中的用户表示到LLM中，提高了推荐性能。</li>
<li>LLM-SRec实现了业界领先的推荐性能，且相较于其他LLM4Rec模型更为实用，仅需训练少量轻量级MLPs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f641e4928de220f35ca90358881b80b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a650a40c6af80db5555293e00112d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa1307b97ddd357ce63767b177741ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac00f70619628c5d1e3f8b7c93cccaf6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Judging-the-Judges-A-Collection-of-LLM-Generated-Relevance-Judgements"><a href="#Judging-the-Judges-A-Collection-of-LLM-Generated-Relevance-Judgements" class="headerlink" title="Judging the Judges: A Collection of LLM-Generated Relevance Judgements"></a>Judging the Judges: A Collection of LLM-Generated Relevance Judgements</h2><p><strong>Authors:Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz</strong></p>
<p>Using Large Language Models (LLMs) for relevance assessments offers promising opportunities to improve Information Retrieval (IR), Natural Language Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing IR experimenters to build evaluation collections with a fraction of the manual human labor currently required. This could help with fresh topics on which there is still limited knowledge and could mitigate the challenges of evaluating ranking systems in low-resource scenarios, where it is challenging to find human annotators. Given the fast-paced recent developments in the domain, many questions concerning LLMs as assessors are yet to be answered. Among the aspects that require further investigation, we can list the impact of various components in a relevance judgment generation pipeline, such as the prompt used or the LLM chosen.   This paper benchmarks and reports on the results of a large-scale automatic relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where different relevance assessment approaches were proposed. In detail, we release and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track relevance judgments produced by eight international teams who participated in the challenge. Given their diverse nature, these automatically generated relevance judgments can help the community not only investigate systematic biases caused by LLMs but also explore the effectiveness of ensemble models, analyze the trade-offs between different models and human assessors, and advance methodologies for improving automated evaluation techniques. The released resource is available at the following link: <a target="_blank" rel="noopener" href="https://llm4eval.github.io/LLMJudge-benchmark/">https://llm4eval.github.io/LLMJudge-benchmark/</a> </p>
<blockquote>
<p>使用大型语言模型（LLM）进行相关性评估为信息检索（IR）、自然语言处理（NLP）和相关领域提供了改进的有前途的机会。实际上，LLM有望让IR实验者使用当前所需人工劳动的一小部分来构建评估集合。这有助于处理有限知识的新主题，并可能减轻在低资源场景中评估排名系统的挑战，在这些场景中，很难找到人类注释者。考虑到该领域的快速发展，关于LLM作为评估者的许多问题还有待回答。在需要进一步调查的方面，我们可以列出相关性判断生成管道中各种组件的影响，例如使用的提示或选择的LLM。本文介绍了大规模自动相关性评估基准测试LLMJudge挑战的结果，该挑战于2024年SIGIR会议上举办，其中提出了不同的相关性评估方法。具体来说，我们发布并基准测试了TREC 2023深度学习轨道相关性判断的42个LLM生成标签，这些标签由参与挑战的八个国际团队产生。这些自动生成的相关性判断具有多样性，不仅有助于社区研究LLM引起的系统偏见，还可以探索集成模型的有效性，分析不同模型与人类评估者之间的权衡，并推进改进自动化评估技术的方法。发布的资源可在以下链接找到：<a target="_blank" rel="noopener" href="https://llm4eval.github.io/LLMJudge-benchmark/">https://llm4eval.github.io/LLMJudge-benchmark/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13908v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）在相关性评估方面的应用为信息检索（IR）、自然语言处理（NLP）等相关领域带来了改进的希望。LLM可以减轻IR实验者构建评估集合时所需的大量手动劳动力，有助于应对缺乏人类注释者评估低资源场景中排名系统的挑战。针对LLM作为评估者的快速发展领域，还有许多问题有待回答，如生成管道中不同组件的影响等。本文介绍了在SIGIR 2024年举办的LLMJudge挑战赛的结果，该挑战赛旨在评估不同的相关性评估方法。通过对比八个国际团队提出的自动生成的TREC 2023深度学习轨迹相关性判断标签，该挑战赛有助于社区研究LLM引起的系统性偏见问题，并探讨集成模型的有效性等关键领域进步研究方法和自动评估技术的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在相关性评估领域展现出改进IR和NLP等行业的潜力。</li>
<li>LLM能够大幅减少构建评估集合所需的人工参与程度。</li>
<li>LLMJudge挑战赛的结果展示了不同相关性评估方法的对比结果。</li>
<li>自动生成的评估数据能够帮助社区探究LLM带来的潜在偏见。</li>
<li>对集成模型有效性的探索是一个关键领域。</li>
<li>通过对比不同模型和人类评估者的结果，可以分析出其中的权衡和取舍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13908">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ab733214a540913731e31010ccd2dd4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-367e64897a24a79d8120fa2368b0eb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae411243971b2e420bc858d70ba9fa24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dacd0f90dbee86cdd150fdecfe410de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddf72ee9b6410db86a66364a13552a22.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DataSciBench-An-LLM-Agent-Benchmark-for-Data-Science"><a href="#DataSciBench-An-LLM-Agent-Benchmark-for-Data-Science" class="headerlink" title="DataSciBench: An LLM Agent Benchmark for Data Science"></a>DataSciBench: An LLM Agent Benchmark for Data Science</h2><p><strong>Authors:Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue</strong></p>
<p>This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at <a target="_blank" rel="noopener" href="https://github.com/THUDM/DataSciBench">https://github.com/THUDM/DataSciBench</a>. </p>
<blockquote>
<p>本文介绍了DataSciBench，这是一个用于评估数据科学中大型语言模型（LLM）能力的全面基准测试。最近的相关基准测试主要侧重于单一任务、容易获得的真实数据和简单的评估指标，这限制了可以评估的任务范围。相比之下，DataSciBench的构建基础是更全面、更精心挑选的自然和具有挑战性的提示，以及不确定的真实数据和评估指标。我们开发了一个半自动化管道来生成真实数据（GT）并验证评估指标。该管道利用并实现了一种基于LLM的自洽和人类验证策略，通过利用收集的提示、预定义的任务类型和聚合功能（指标）来产生准确的GT。此外，我们提出了创新的Task-Function-Code（TFC）框架，根据精确定义的指标和程序规则来评估每个代码执行结果。我们的实验框架包括使用我们收集的多样提示来测试6个API模型、8个开源通用模型和9个开源代码生成模型。该方法旨在提供数据科学中LLM的更全面和严格评估，揭示其优缺点。实验结果表明，API模型在所有指标上均优于开源模型，Deepseek-Coder-33B-Instruct在开源模型中得分最高。我们在<a target="_blank" rel="noopener" href="https://github.com/THUDM/DataSciBench%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/THUDM/DataSciBench上发布了所有代码和数据。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13897v1">PDF</a> 40 pages, 7 figures, 6 tables</p>
<p><strong>Summary</strong><br>DataSciBench是一个全面的评估大型语言模型（LLM）在数据科学方面的能力的基准测试。与现有的主要侧重于单一任务、易于获得真实标记和直观评估指标的基准测试相比，DataSciBench基于更全面和精选的自然和具有挑战性的提示构建，用于不确定的真实标记和评估指标。该研究开发了半自动化管道来生成真实标记和验证评估指标，并利用LLM的自洽性和人工验证策略来产生准确的真实标记。此外，提出了创新的Task-Function-Code（TFC）框架，根据精确定义的指标和程序规则来评估代码执行的结果。实验框架对各种模型进行了测试，包括API模型、开源通用模型和开源代码生成模型。旨在提供更全面、严格的LLM在数据科学方面的评估，揭示其优缺点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DataSciBench是一个用于评估LLM在数据科学方面能力的全面基准测试。</li>
<li>现有基准测试主要关注单一任务，而DataSciBench包含自然和具有挑战性的提示。</li>
<li>研究开发了一个半自动化管道来生成真实标记并验证评估指标。</li>
<li>利用LLM的自洽性和人工验证策略来产生准确的真实标记。</li>
<li>提出了Task-Function-Code（TFC）框架来评估代码执行的结果。</li>
<li>实验框架测试了多种类型的模型，包括API模型、开源通用模型和开源代码生成模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d388a207167df75441b77a0fad6ffb0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ef9a58799020e42abce8c266f36c980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5533d0e8b9a973b416c045ccba6557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec17468b0e0624442aa694262dd403ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dcf0a1ee457cb1d69cf940846598ab0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPEX-Scaling-Feature-Interaction-Explanations-for-LLMs"><a href="#SPEX-Scaling-Feature-Interaction-Explanations-for-LLMs" class="headerlink" title="SPEX: Scaling Feature Interaction Explanations for LLMs"></a>SPEX: Scaling Feature Interaction Explanations for LLMs</h2><p><strong>Authors:Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu</strong></p>
<p>Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths ($\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$. SPEX exploits underlying natural sparsity among interactions – common in real-world data – and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-4o mini) and compositional reasoning in vision-language models. </p>
<blockquote>
<p>大型语言模型（LLM）由于其捕捉输入特征之间复杂交互的能力，为机器学习带来了革命性的变革。像SHAP这样的流行事后解释方法提供边际特征归属，而它们对交互重要性的扩展只适用于较短的输入长度（约20个）。我们提出了Spectral Explainer（SPEX），这是一种模型无关的交互归属算法，可有效地扩展到较大的输入长度（约1000个）。SPEX利用现实世界中常见的交互之间的自然稀疏性，并应用一种稀疏傅里叶变换和通道解码算法来有效地识别重要的交互。我们在三个需要LLM利用输入间交互来完成任务的长文本数据集上进行了实验。对于大输入，SPEX在忠实重建LLM输出方面优于边际归属方法，准确率提高了高达20%。此外，SPEX成功地识别了强烈影响模型输出的关键特征和交互。在我们的数据集之一HotpotQA中，SPEX提供的交互与人类注释相符。最后，我们使用模型无关的方法生成解释，以展示封闭源代码LLM（GPT-4o mini）中的抽象推理和视觉语言模型中的组合推理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13870v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）能够捕捉输入特征之间的复杂交互，从而引领机器学习革命。针对现有解释方法在处理长输入时的局限性，本文提出了Spectral Explainer（SPEX）方法。SPEX是一种模型通用的交互归因算法，可有效地扩展到长输入长度（约1000）。它通过利用现实世界中常见的交互自然稀疏性，采用稀疏傅里叶变换和通道解码算法来高效识别重要的交互。实验表明，SPEX在三个需要利用输入间交互来完成任务的长上下文数据集上的表现优于边际归因方法，忠实地重构了LLM输出。此外，SPEX还能成功识别影响模型输出的关键特征和交互。对于HotpotQA数据集，SPEX提供的交互与人类注释相符。最后，我们利用模型通用的方法为封闭源LLM（GPT-4o mini）和视觉语言模型的组合推理提供了解释。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs能够捕捉输入特征间的复杂交互，推动机器学习进步。</li>
<li>现有解释方法在处理长输入时存在局限性。</li>
<li>Spectral Explainer (SPEX)是一种模型通用的交互归因算法，可扩展到长输入长度。</li>
<li>SPEX利用交互的自然稀疏性，采用稀疏傅里叶变换和通道解码算法。</li>
<li>SPEX在三个长上下文数据集上的表现优于边际归因方法。</li>
<li>SPEX能识别影响模型输出的关键特征和交互。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13870">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e7be0f95d3696bdaba06a349219ebf04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36bae4214017aaf07f9f8badc47ab0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-229a1efe32ef911e1f5174f91df2e949.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-375ceb9efb280cea35cb3c882425440f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1e6d01e490512d7a361285f31c7f7fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-317e52a85ec1be84f0cc294c1561088c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MagicGeo-Training-Free-Text-Guided-Geometric-Diagram-Generation"><a href="#MagicGeo-Training-Free-Text-Guided-Geometric-Diagram-Generation" class="headerlink" title="MagicGeo: Training-Free Text-Guided Geometric Diagram Generation"></a>MagicGeo: Training-Free Text-Guided Geometric Diagram Generation</h2><p><strong>Authors:Junxiao Wang, Ting Zhang, Heng Yu, Jingdong Wang, Hua Huang</strong></p>
<p>Geometric diagrams are critical in conveying mathematical and scientific concepts, yet traditional diagram generation methods are often manual and resource-intensive. While text-to-image generation has made strides in photorealistic imagery, creating accurate geometric diagrams remains a challenge due to the need for precise spatial relationships and the scarcity of geometry-specific datasets. This paper presents MagicGeo, a training-free framework for generating geometric diagrams from textual descriptions. MagicGeo formulates the diagram generation process as a coordinate optimization problem, ensuring geometric correctness through a formal language solver, and then employs coordinate-aware generation. The framework leverages the strong language translation capability of large language models, while formal mathematical solving ensures geometric correctness. We further introduce MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and demonstrate that MagicGeo outperforms current methods in both qualitative and quantitative evaluations. This work provides a scalable, accurate solution for automated diagram generation, with significant implications for educational and academic applications. </p>
<blockquote>
<p>几何图表在传达数学和科学概念方面起着关键作用，然而传统的图表生成方法往往是手动且资源密集型的。尽管文本到图像的生成在照片级图像方面取得了进展，但由于需要精确的空间关系和几何特定数据集的稀缺，创建准确的几何图表仍然是一个挑战。本文针对这一问题，提出了一种无训练框架MagicGeo，用于根据文本描述生成几何图表。MagicGeo将图表生成过程表述为坐标优化问题，通过形式化语言求解器确保几何正确性，然后采用坐标感知生成。该框架利用大型语言模型的强大语言翻译能力，而形式化数学求解则确保了几何正确性。我们还进一步引入了MagicGeoBench数据集，包含220个几何图表描述，并证明MagicGeo在定性和定量评估中都优于当前方法。这项工作为自动化图表生成提供了可扩展且准确的解决方案，对教育应用和学术应用具有重要意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13855v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>几何图形在传达数学和科学概念方面起着关键作用，但传统的图形生成方法往往是手动且资源密集型的。文本到图像生成技术在照片真实感图像方面取得了进展，但创建精确的几何图形仍然存在挑战，这主要是由于需要精确的空间关系以及几何数据集的稀缺性。本文提出了MagicGeo框架，这是一个无需训练的几何图形生成方法，可从文本描述中生成几何图形。MagicGeo将图形生成过程公式化为坐标优化问题，通过形式语言求解器确保几何正确性，然后采用坐标感知生成。该框架利用大型语言模型的强大语言翻译能力，同时形式数学求解确保几何正确性。我们还介绍了MagicGeoBench数据集，包含220个几何图形描述，并证明MagicGeo在定性和定量评估中均优于当前方法。这项研究为自动化图形生成提供了可扩展且准确的解决方案，对教育和学术应用具有重大影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>几何图形在传达数学和科学概念时至关重要，但传统方法生成图形的过程通常是手动且耗资源的。</li>
<li>文本到图像生成技术在照片真实感图像方面有所进展，但生成精确几何图形仍然具有挑战性。</li>
<li>MagicGeo框架能从文本描述中生成几何图形，且无需训练。</li>
<li>MagicGeo将图形生成过程看作坐标优化问题，确保几何正确性。</li>
<li>MagicGeo利用大型语言模型的翻译能力，并结合形式数学求解确保准确性。</li>
<li>引入了MagicGeoBench数据集，包含220个几何图形描述，用于评估生成方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b82140d35d1a07052321dc7ba269e815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9adc3f38d5aff9e7d83d156857f5c528.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7464694961def7d818914a3420f392.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9744213a9b555adfb8e41001b354225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf265197fd1fcd0a6fc2cb2cc60a10a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Based-Recommendations-Through-Personalized-Reasoning"><a href="#Enhancing-LLM-Based-Recommendations-Through-Personalized-Reasoning" class="headerlink" title="Enhancing LLM-Based Recommendations Through Personalized Reasoning"></a>Enhancing LLM-Based Recommendations Through Personalized Reasoning</h2><p><strong>Authors:Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu</strong></p>
<p>Current recommendation systems powered by large language models (LLMs) often underutilize their reasoning capabilities due to a lack of explicit logical structuring. To address this limitation, we introduce CoT-Rec, a framework that integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by incorporating two crucial processes: user preference analysis and item perception evaluation. CoT-Rec operates in two key phases: (1) personalized data extraction, where user preferences and item perceptions are identified, and (2) personalized data application, where this information is leveraged to refine recommendations. Our experimental analysis demonstrates that CoT-Rec improves recommendation accuracy by making better use of LLMs’ reasoning potential. The implementation is publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CoT-Rec">https://anonymous.4open.science/r/CoT-Rec</a>. </p>
<blockquote>
<p>当前由大型语言模型（LLM）驱动的建议系统由于缺少明确的逻辑结构而往往未能充分利用其推理能力。为了解决这一局限性，我们引入了CoT-Rec框架，它通过融入两个关键过程——用户偏好分析和项目感知评估，将思维链（CoT）推理融入LLM驱动的建议中。CoT-Rec有两个关键阶段：1）个性化数据提取，识别用户偏好和项目感知；2) 个性化数据应用，利用这些信息来优化建议。我们的实验分析表明，CoT-Rec通过更好地利用LLM的推理潜力，提高了推荐准确性。实现代码公开在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CoT-Rec%E3%80%82">https://anonymous.4open.science/r/CoT-Rec。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13845v1">PDF</a> 7 pages, under review</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的推荐系统常常由于缺少明确的逻辑结构而无法充分利用其推理能力。为解决这一问题，我们提出CoT-Rec框架，通过引入Chain-of-Thought（CoT）推理，结合用户偏好分析和物品感知评估两个关键过程，提升LLM驱动的推荐系统性能。CoT-Rec包括两个主要阶段：个性化数据提取和个性化数据应用。实验分析显示，CoT-Rec能够更好地利用LLM的推理潜力，提高推荐准确性。其实现可公开访问。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）驱动的推荐系统存在推理能力利用不足的问题。</li>
<li>CoT-Rec框架通过引入Chain-of-Thought（CoT）推理来解决这一问题。</li>
<li>CoT-Rec结合用户偏好分析和物品感知评估两个关键过程。</li>
<li>CoT-Rec包括个性化数据提取和个性化数据应用两个主要阶段。</li>
<li>CoT-Rec能提高推荐系统的准确性。</li>
<li>CoT-Rec的实现可公开访问。</li>
<li>CoT-Rec为LLM在推荐系统中的应用提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13845">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-872c1b6a1ed8ccb08bf520de2d0bc6f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-525fdefff7904fd94d1cda467e9bb16c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e339657951c0c15b1df79f6be4a057cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48cdc45a9c2148d89ecc4e5ab269a24d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67713f8770af29cf888c5c68c6424d77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a20f514314153fc0352fbc7a3c4e7d4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Cross-Domain-Recommendations-with-Memory-Optimized-LLM-Based-User-Agents"><a href="#Enhancing-Cross-Domain-Recommendations-with-Memory-Optimized-LLM-Based-User-Agents" class="headerlink" title="Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based   User Agents"></a>Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based   User Agents</h2><p><strong>Authors:Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu</strong></p>
<p>Large Language Model (LLM)-based user agents have emerged as a powerful tool for improving recommender systems by simulating user interactions. However, existing methods struggle with cross-domain scenarios due to inefficient memory structures, leading to irrelevant information retention and failure to account for social influence factors such as popularity. To address these limitations, we introduce AgentCF++, a novel framework featuring a dual-layer memory architecture and a two-step fusion mechanism to filter domain-specific preferences effectively. Additionally, we propose interest groups with shared memory, allowing the model to capture the impact of popularity trends on users with similar interests. Through extensive experiments on multiple cross-domain datasets, AgentCF++ demonstrates superior performance over baseline models, highlighting its effectiveness in refining user behavior simulation for recommender systems. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AgentCF-plus">https://anonymous.4open.science/r/AgentCF-plus</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的用户代理已成为推荐系统改进的强大工具，通过模拟用户交互来实现。然而，现有方法在处理跨域场景时存在困难，由于内存结构不够高效，导致保留不相关信息以及未能考虑社会影响因素，如人气。为解决这些局限性，我们推出了AgentCF++，这是一个新型框架，具有双层内存架构和两步融合机制，可有效过滤特定域偏好。此外，我们提出了具有共享内存的兴趣小组，使模型能够捕捉流行趋势对具有相似兴趣用户的影响。通过多个跨域数据集的大量实验，AgentCF++在基准模型上表现出卓越性能，凸显其在改进推荐系统的用户行为模拟方面的有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AgentCF-plus%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/AgentCF-plus上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13843v1">PDF</a> 6 pages, under review</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）用户代理通过模拟用户交互增强了推荐系统的效能。然而，现有方法在处理跨域场景时存在内存结构低效的问题，导致保留不相关信息和忽略社会影响因素如流行度。为解决这些问题，我们推出AgentCF++框架，采用双层内存架构和两步融合机制，有效过滤特定域偏好。同时，我们提出兴趣小组共享内存，让模型能捕捉流行趋势对具有相似兴趣用户的影响。透过多次跨域数据集的实验，AgentCF++表现优于基准模型，展现其在推荐系统中优化用户行为模拟的效能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM用户代理模拟用户交互提升了推荐系统的效能。</li>
<li>现有方法在处理跨域场景时存在内存结构低效的问题。</li>
<li>AgentCF++采用双层内存架构和两步融合机制过滤特定域偏好。</li>
<li>AgentCF++通过兴趣小组共享内存捕捉流行趋势对相似兴趣用户的影响。</li>
<li>AgentCF++在跨域数据集上的实验表现优于基准模型。</li>
<li>AgentCF++能有效优化推荐系统中的用户行为模拟。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13843">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0af8176f3a377b86aa9ddeb7566817bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad764e55d31309e372056a1f0acb4281.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5ded8c2d9418d932293afab4dbd02a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-424479d98b0af78a31cec366a657630d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-660c36b05d2acbbc834330d18d0935e3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking"><a href="#Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking" class="headerlink" title="Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking"></a>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking</h2><p><strong>Authors:Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</strong></p>
<p>Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2%, and outperforms Transformer&#x2F;Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways. </p>
<blockquote>
<p>大型语言模型（LLM）在参数约束下存在固有的性能瓶颈，特别是在处理需要复杂推理的关键令牌时。经验分析表明，挑战令牌会在各层之间引发突然的梯度跃升，暴露了标准Transformer中的架构应力点。基于这一见解，我们提出了内思考Transformer（ITT），它重新构想层计算为隐式思考步骤。ITT通过自适应令牌路由动态分配计算，通过剩余思考连接迭代优化表示，并使用思考步骤编码来区分推理阶段。ITT能够在不扩大参数的情况下对关键令牌进行更深的处理。在1.6亿至近4亿参数模型的评估中，ITT使用仅近一半的参数（近4亿参数模型中的仅一半），实现了对标准模型的近九成半性能（即近百分之九十六点五），减少了百分之四十三点二的训练数据，并在十二个基准测试中优于Transformer或循环变体。通过启用推理过程中的弹性计算分配，ITT通过架构优化的隐性思考路径在性能和效率之间取得平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13842v1">PDF</a> 15 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>内蕴思考转换器（ITT）解决了大型语言模型（LLM）在处理关键令牌时的性能瓶颈问题。通过重新设计转换器架构中的层计算为隐性思考步骤，ITT能够动态分配计算资源以处理复杂推理需求的令牌。在关键令牌处理上，ITT不需要额外的参数扩展即可实现深度处理。实验评估显示，ITT能够实现仅使用较少参数的高效性能表现，且可降低训练数据量，同时满足多样化的基准测试要求。在推理过程中，ITT可弹性分配计算资源，实现性能与效率的平衡优化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在处理关键令牌时面临性能瓶颈问题。</li>
<li>关键令牌处理需要复杂推理，导致梯度波动和架构压力。</li>
<li>内蕴思考转换器（ITT）通过隐性思考步骤重新设计转换器架构中的层计算来解决上述问题。</li>
<li>ITT实现了在不需要额外参数扩展的情况下对关键令牌的深度处理。</li>
<li>ITT在减少参数使用和训练数据量方面表现出卓越性能。</li>
<li>ITT能够在推理过程中动态分配计算资源以实现性能与效率的平衡优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e8b72493deee3458d27fe6e91ac1062d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d2c2cfdc1b6b372071f675d6641038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f19bf34fab4abb0135ebb0a4eaa167a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88edeff5ebe306337adc457af1ae6f13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af39aa68e012cd7cd9eb9633692f4275.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Quantifying-Memorization-and-Retriever-Performance-in-Retrieval-Augmented-Vision-Language-Models"><a href="#Quantifying-Memorization-and-Retriever-Performance-in-Retrieval-Augmented-Vision-Language-Models" class="headerlink" title="Quantifying Memorization and Retriever Performance in   Retrieval-Augmented Vision-Language Models"></a>Quantifying Memorization and Retriever Performance in   Retrieval-Augmented Vision-Language Models</h2><p><strong>Authors:Peter Carragher, Abhinand Jha, R Raghav, Kathleen M. Carley</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks. </p>
<blockquote>
<p>大型语言模型（LLM）在问答（QA）方面表现出卓越的能力，但在评估它们对记忆与检索的依赖程度方面的度量标准仍然不够成熟。虽然针对封闭领域的任务微调模型是最新技术，但像GPT-4o这样的通用模型表现出强大的零样本性能。这引发了关于记忆、泛化和检索之间的权衡问题。在这项工作中，我们分析了多模态检索增强型VLMs与基准VLMs在记忆训练数据方面的程度。我们使用WebQA基准测试，对比微调模型与基准VLM在多跳检索和问答方面的表现，研究微调对数据记忆的影响。为了量化端到端检索和问答系统中记忆的程度，我们通过研究问答成功而检索失败的情况，提出了几个代理度量标准。我们的结果表明微调模型在多大程度上依赖于记忆。相比之下，检索增强型VLM的记忆得分较低，但代价是准确性下降（在WebQA测试集上为72%对52%）。因此，我们的度量措施为未来工作带来了挑战，需要在开放域问答和联合检索-问答任务中协调记忆和泛化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13836v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）在问答（QA）方面的出色表现，本研究探讨了评估模型对记忆与检索依赖性的度量方法。研究对比了微调模型与通用模型如GPT-4o在封闭领域任务与零样本任务上的表现，提出了关于记忆、泛化与检索之间权衡的问题。本研究分析了多模态检索增强型VLM模型与基准VLM模型在记忆训练数据方面的差异。通过WebQA基准测试，本研究对比了多跳检索与问答中微调模型的表现，并考察了微调对数据记忆的影响。为量化端到端检索和问答系统中记忆的存储程度，我们提出通过探究问答成功但检索失败的情况来评估代理指标。研究结果显示，微调模型高度依赖记忆。相比之下，检索增强型VLM模型的记忆得分较低，但代价是准确率下降（WebQA测试集上为72%对比52%）。因此，我们的度量措施为未来工作带来了挑战，需要在开放域问答和联合检索问答任务中协调记忆与泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在问答任务中表现出强大的能力，但在评估模型对记忆与检索依赖性的度量方法上仍有待发展。</li>
<li>对比分析微调模型与通用模型在封闭领域任务与零样本任务上的表现，引发对记忆、泛化与检索之间权衡的思考。</li>
<li>多模态检索增强型VLM模型相较于基准VLM模型在记忆训练数据上的差异被分析。</li>
<li>使用WebQA基准测试评估了多跳检索与问答中微调模型的表现。</li>
<li>提出了通过探究问答成功但检索失败的情况来量化模型中记忆的存储程度的新方法。</li>
<li>研究发现微调模型高度依赖记忆来完成任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20c1b64e67bff94771159172eda7abd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3384cd6cf4055aac6b223a0c685c296e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a89ac98f787d50bf4361396970f196ae.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Proving-Olympiad-Inequalities-by-Synergizing-LLMs-and-Symbolic-Reasoning"><a href="#Proving-Olympiad-Inequalities-by-Synergizing-LLMs-and-Symbolic-Reasoning" class="headerlink" title="Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning"></a>Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning</h2><p><strong>Authors:Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma</strong></p>
<p>Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~1). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data. </p>
<blockquote>
<p>大型语言模型（LLM）可以通过在证明系统内生成证明步骤（即策略）来形式化地证明数学定理。然而，可能的策略空间是庞大而复杂的，而可用于形式化证明的训练数据是有限的，这给基于LLM的策略生成带来了重大挑战。为了解决这一问题，我们引入了一种神经符号策略生成器，它协同LLM习得的数学直觉和符号方法编码的领域特定见解。这种集成的关键方面在于确定哪些部分的数学推理最适合LLM，哪些适合符号方法。虽然神经符号集成的高级理念广泛适用于各种数学问题，但在本文中，我们专门针对奥林匹克不等式（图1）进行研究。我们分析了人类如何解决这些问题，并将技术提炼为两种类型的策略：1）缩放，由符号方法处理；2)重写，由LLM处理。此外，我们将符号工具与LLM相结合，以修剪和排列证明目标，实现高效的证明搜索。我们在多个数学竞赛的161个具有挑战的不等式中评估了我们的框架，实现了最先进的性能，并且显著优于现有的LLM和符号方法，而无需额外的训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13834v1">PDF</a> Published as a conference paper at ICLR 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/Lizn-zn/NeqLIPS/">https://github.com/Lizn-zn/NeqLIPS/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）可以通过在证明系统内生成证明步骤（即策略）来证明数学定理。然而，可能的策略空间庞大且复杂，而可用于正式证明的训练数据有限，给LLM的策略生成带来了重大挑战。为解决这一问题，我们提出了一种神经符号策略生成器，它将LLM学习的数学直觉与符号方法编码的领域特定见解相结合。此集成的关键方面在于确定哪些部分的数学推理最适合LLM，哪些适合符号方法。虽然神经符号集成的高级理念广泛适用于各种数学问题，但本文重点关注奥林匹克不等式（图1）。我们分析了人类如何解决这些问题，并将技术提炼为两种策略：一是通过符号方法处理的缩放，二是通过LLM处理的重写。此外，我们将符号工具与LLM相结合，对证明目标进行修剪和排名，以进行有效证明搜索。我们在多个数学竞赛的161个具有挑战的不等式中评估了我们的框架，实现了卓越的性能，显著优于现有的LLM和符号方法，而无需额外的训练数据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM能够通过生成证明步骤（即策略）在数学证明中发挥重要作用。</li>
<li>在有限的训练数据下，策略生成面临挑战，因为策略空间庞大且复杂。</li>
<li>提出了一种神经符号策略生成器来解决这一挑战，结合了LLM的数学直觉和符号方法的领域特定知识。</li>
<li>确定哪些数学推理部分适合LLM和哪些适合符号方法是集成的关键。</li>
<li>通过分析人类解决奥林匹克不等式的方法，将策略分为缩放和重写两种类型。</li>
<li>结合符号工具和LLM对证明目标进行修剪和排名，提高了证明搜索的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86533d54149e9647b84b4af9c25b14bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ed53c1b0e3c1cb02178a38bf59746c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c330f19eecde394cd251d2be74bbdd83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e28c392c79e9d2a66fe796ccd7d8601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49650499fce71635c9135517b5456be5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ArtMentor-AI-Assisted-Evaluation-of-Artworks-to-Explore-Multimodal-Large-Language-Models-Capabilities"><a href="#ArtMentor-AI-Assisted-Evaluation-of-Artworks-to-Explore-Multimodal-Large-Language-Models-Capabilities" class="headerlink" title="ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal   Large Language Models Capabilities"></a>ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal   Large Language Models Capabilities</h2><p><strong>Authors:Chanjin Zheng, Zengyi Yu, Yilin Jiang, Mingzi Zhang, Xunuo Lu, Jing Jin, Liteng Gao</strong></p>
<p>Can Multimodal Large Language Models (MLLMs), with capabilities in perception, recognition, understanding, and reasoning, function as independent assistants in art evaluation dialogues? Current MLLM evaluation methods, which rely on subjective human scoring or costly interviews, lack comprehensive coverage of various scenarios. This paper proposes a process-oriented Human-Computer Interaction (HCI) space design to facilitate more accurate MLLM assessment and development. This approach aids teachers in efficient art evaluation while also recording interactions for MLLM capability assessment. We introduce ArtMentor, a comprehensive space that integrates a dataset and three systems to optimize MLLM evaluation. The dataset consists of 380 sessions conducted by five art teachers across nine critical dimensions. The modular system includes agents for entity recognition, review generation, and suggestion generation, enabling iterative upgrades. Machine learning and natural language processing techniques ensure the reliability of evaluations. The results confirm GPT-4o’s effectiveness in assisting teachers in art evaluation dialogues. Our contributions are available at <a target="_blank" rel="noopener" href="https://artmentor.github.io/">https://artmentor.github.io/</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）具备感知、识别、理解和推理能力，能否在艺术评价对话中充当独立助手？目前的MLLM评估方法依赖于主观的人为打分或昂贵的面试，无法全面覆盖各种场景。本文提出了一种面向过程的计算机人机交互（HCI）空间设计，以促进更准确的多模态语言模型评估和发展。这种方法不仅有助于教师高效地进行艺术评价，同时记录交互情况以评估MLLM的能力。我们介绍了ArtMentor，这是一个综合空间，集成了数据集和三个系统以优化MLLM评估。数据集包含五位艺术教师进行的380个会话，涵盖九个关键维度。模块化系统包括实体识别代理、评论生成代理和建议生成代理，可实现迭代升级。机器学习和自然语言处理技术确保了评价的可靠性。结果证实了GPT-4o在艺术评价对话中辅助教师的有效性。我们的成果可通过<a target="_blank" rel="noopener" href="https://artmentor.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://artmentor.github.io/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13832v1">PDF</a> 18 pages, 12 figures. Accepted by CHI 2025</p>
<p><strong>Summary</strong></p>
<p>MLLM在感知、识别、理解和推理方面的能力使其能够成为艺术评价对话中的独立助手。当前的评价方法缺乏对各种场景的全面覆盖，本文提出了一种面向过程的计算机与人类交互空间设计，以促进更准确的MLLM评估和发展。我们介绍了ArtMentor，这是一个集成了数据集和三重系统的综合空间，用于优化MLLM评价。通过机器学习与自然语言处理技术确保评价的可靠性。最终确认GPT-4o可有效辅助教师进行艺术评价对话。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLM具备多种能力，可成为艺术评价对话的独立助手。</li>
<li>当前MLLM评价方法的局限性，需要更全面的场景覆盖。</li>
<li>面向过程的计算机与人类交互空间设计用于更准确评估和发展MLLM。</li>
<li>ArtMentor是一个综合空间，集成了数据集和多重系统以优化MLLM评价。</li>
<li>ArtMentor数据集包含380个由五位艺术教师进行的会话，涉及九个关键维度。</li>
<li>ArtMentor的模块化系统包括实体识别、评论生成和建议生成代理，可实现迭代升级。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-69d41b3b9b216d2321c3f64fdd79e155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c01e1ac5cbc74efcc4bbdfd86091b9c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="One-Size-doesn’t-Fit-All-A-Personalized-Conversational-Tutoring-Agent-for-Mathematics-Instruction"><a href="#One-Size-doesn’t-Fit-All-A-Personalized-Conversational-Tutoring-Agent-for-Mathematics-Instruction" class="headerlink" title="One Size doesn’t Fit All: A Personalized Conversational Tutoring Agent   for Mathematics Instruction"></a>One Size doesn’t Fit All: A Personalized Conversational Tutoring Agent   for Mathematics Instruction</h2><p><strong>Authors:Ben Liu, Jihan Zhang, Fangquan Lin, Xu Jia, Min Peng</strong></p>
<p>Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \textbf{P}erson\textbf{A}lized \textbf{C}onversational tutoring ag\textbf{E}nt (PACE) for mathematics instruction. PACE simulates students’ learning styles based on the Felder and Silverman learning style model, aligning with each student’s persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students’ comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods. </p>
<blockquote>
<p>大型语言模型（LLM）在各种智能教育系统中得到了越来越广泛的应用，模拟人类导师，促进有效的人机交互。然而，之前的研究往往忽视了识别和适应个别学习者特性的重要性。这种适应对于提高学生参与度和学习效率至关重要，特别是在数学教学方面，不同的学习风格需要个性化策略来促进理解和热情。在本文中，我们提出了一种用于数学教学的个性化会话辅导实体（PACE）。PACE基于Felder和Silverman的学习风格模型模拟学生的学习风格，与每个学生的个性相匹配。通过这种方式，我们的PACE可以有效地评估学生的个性，从而制定与他们独特学习风格相符的个性化教学策略。为了进一步提高学生的理解力，PACE采用苏格拉底教学方法，提供即时反馈，鼓励深入思考。通过构建个性化的教学数据和训练模型，PACE展示了识别和适应每个学生独特需求的能力，显著提高了整体学习体验和效果。此外，我们建立了多方面的评价标准，并进行了广泛的分析，以评估个性化教学的表现。实验结果表明，与现有方法相比，我们的模型在个性化教育体验和激发学生动力方面表现出卓越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12633v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的教育系统模拟人类导师以促进人机有效交互。然而，以往的研究往往忽视了识别和适应个别学习者特性的重要性。特别是在数学教学方面，多样化的学习风格需要个性化策略来促进理解和热情。本文提出了一种个性化对话辅导实体（PACE）用于数学教学。PACE基于Felder和Silverman的学习风格模型模拟学生的学习风格，与学生的个性对齐。通过这种方式，PACE可以有效地评估学生的性格，制定个性化的教学策略，与学生的独特学习风格产生共鸣。为了进一步增强学生的理解，PACE采用苏格拉底教学法提供即时反馈和鼓励深入思考。通过构建个性化的教学数据和训练模型，PACE展现了识别和适应每个学生的独特需求的能力，显著提高了整体学习体验和效果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs在智能教育系统中模拟人类导师以增强人机交互效果。</li>
<li>学习者特性在提升学习体验和效率中至关重要，特别是在数学教学领域。</li>
<li>PACE能根据Felder和Silverman的学习风格模型模拟学生个体的学习风格。</li>
<li>PACE能够评估学生的个性并根据其学习风格定制教学策略。</li>
<li>PACE采用苏格拉底教学法以提供即时反馈和鼓励深度思考，增强学生的学习理解。</li>
<li>通过构建个性化的教学数据和训练模型，PACE展现了适应每个学生的独特需求的能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-23c35e65657f8aa4681797184b55a440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9678a600cab624f2cb06353c352dcccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a15f0a6d0d3ef74598abbdca956e59e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11b1cf6c9736c610fd35f7a197534a73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd1c8b5227a54b608de67aa39fd2d126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab906a856dccdbcef7ccd5d39c8913a2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reasoning-Augmented-Conversation-for-Multi-Turn-Jailbreak-Attacks-on-Large-Language-Models"><a href="#Reasoning-Augmented-Conversation-for-Multi-Turn-Jailbreak-Attacks-on-Large-Language-Models" class="headerlink" title="Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on   Large Language Models"></a>Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on   Large Language Models</h2><p><strong>Authors:Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao</strong></p>
<p>Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMs’ strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at <a target="_blank" rel="noopener" href="https://github.com/NY1024/RACE">https://github.com/NY1024/RACE</a> to facilitate further research in this critical domain. </p>
<blockquote>
<p>多轮越狱攻击通过与大型语言模型（LLM）进行迭代对话来模拟真实世界的人类互动，从而暴露关键的安全漏洞。然而，现有方法往往难以在语义连贯性和攻击效果之间取得平衡，导致良性语义漂移或检测逃避无效。为了解决这一挑战，我们提出了“增强推理对话”（Reasoning-Augmented Conversation）这一新型多轮越狱框架，它将有害查询重新制定为良性推理任务，并利用LLM的强大推理能力来损害安全对齐。具体来说，我们引入了一个攻击状态机框架，系统地模拟问题翻译和迭代推理，确保跨多轮的查询生成连贯性。基于这一框架，我们设计了增益引导探索、自我博弈和拒绝反馈模块，以保留攻击语义、增强效果并维持推理驱动的攻击进展。在多个LLM上的广泛实验表明，RACE在复杂对话场景中实现了最先进的攻击效果，攻击成功率（ASR）提高了高达96%。值得注意的是，我们的方法在针对领先的商业模型OpenAI o1和DeepSeek R1时，ASR分别达到了82%和92%，凸显了其威力。我们在<a target="_blank" rel="noopener" href="https://github.com/NY1024/RACE%E5%8F%91%E5%B8%83%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%AF%A5%E5%85%B3%E9%94%AE%E9%A2%86%E5%9F%9F%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/NY1024/RACE发布代码，以促进该关键领域的研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11054v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型语言模型（LLM）的多轮越狱攻击的新框架——Reasoning-Augmented Conversation（RAC）。该框架通过把有害查询重新构建为良性推理任务，并利用LLM的推理能力来危害安全对齐。实验证明，RACE在复杂对话场景中实现了最先进的攻击效果，攻击成功率（ASR）提高高达96%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多轮越狱攻击模拟真实人类互动，通过迭代对话与大型语言模型（LLM）进行交互，暴露关键安全漏洞。</li>
<li>现有方法难以在语义连贯和攻击效果之间取得平衡，可能导致良性语义漂移或检测逃避失效。</li>
<li>Reasoning-Augmented Conversation（RAC）框架通过把有害查询转化为良性推理任务，利用LLM的推理能力来危及安全对齐。</li>
<li>RAC框架引入攻击状态机框架，系统地建模问题翻译和迭代推理，确保多轮查询的连贯生成。</li>
<li>RAC设计包括收益引导探索、自我游戏和拒绝反馈模块，以保留攻击语义、增强效果和维持推理驱动的攻击进度。</li>
<li>实验证明，RACE在复杂对话场景中实现了最先进的攻击效果，攻击成功率（ASR）提升高达96%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11054">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3bc1b4e9f78646f011196a970bea3326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a630a1498c08e4b8d89679fcd898c095.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-491191897da63e0a5db4b42c030b5673.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Improving-Acoustic-Side-Channel-Attacks-on-Keyboards-Using-Transformers-and-Large-Language-Models"><a href="#Improving-Acoustic-Side-Channel-Attacks-on-Keyboards-Using-Transformers-and-Large-Language-Models" class="headerlink" title="Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers   and Large Language Models"></a>Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers   and Large Language Models</h2><p><strong>Authors:Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai</strong></p>
<p>The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNet’s performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios. </p>
<blockquote>
<p>日常设备中的麦克风越来越普及，对在线服务的依赖程度也越来越高，这加大了针对键盘的声侧信道攻击（ASCAs）的风险。本研究探讨了深度学习技术，特别是视觉变压器（VTs）和大型语言模型（LLMs），以提高此类攻击的有效性和适用性。我们在前人研究的基础上取得了重大改进，CoAtNet模型达到了最先进的性能。我们的CoAtNet在通过智能手机（Phone）记录的键击上提高了5.0%的性能，而在通过Zoom记录的键击上提高了5.9%的性能，超过了之前的基准测试。我们还评估了变压器架构和语言模型，其中表现最佳的VT模型与CoAtNet的性能相匹配。一个关键的进步是引入了用于现实世界的噪声缓解方法。我们通过使用LLMs进行上下文理解，检测和纠正噪声环境中的错误键击，提高ASCA的性能。此外，使用低秩适应（LoRA）进行微调的小型语言模型可实现与具有67倍以上参数的大型模型相当的性能。VTs和LLMs的集成提高了ASCA缓解的实际适用性，标志着这些技术在解决现实场景中的ASCA和错误校正方面的首次应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09782v3">PDF</a> We would like to withdraw our paper due to a significant error in the   experimental methodology, which impacts the validity of our results. The   error specifically affects the analysis presented in Section 4, where an   incorrect dataset preprocessing step led to misleading conclusions</p>
<p><strong>摘要</strong><br>     随着日常生活中设备内置麦克风使用频率增加以及对在线服务的依赖程度不断提升，针对键盘的声学侧信道攻击（ASCAs）风险加剧。本研究探讨了深度学习技术，特别是视觉变压器（VTs）和大型语言模型（LLMs）在提高此类攻击的有效性和适用性方面的作用。相较于先前的研究，我们取得了重大进展，CoAtNet模型的表现尤为突出，达到业界领先水平。在智能手机和Zoom上记录键击时，相较于之前基准测试，CoAtNet的准确率分别提高了5.0%和5.9%。我们还评估了变压器架构和语言模型，其中表现最佳的VT模型与CoAtNet表现相近。一大进步在于引入了一种用于现实世界的噪声缓解方法。通过利用大型语言模型进行上下文理解，我们能够在嘈杂环境中检测和纠正错误的键击，提高ASCA性能。此外，采用Low-Rank Adaptation（LoRA）进行微调的小型语言模型表现良好，性能堪比拥有更多参数的大型模型，比例为67倍。视觉变压器和语言模型的结合提高了ASCA缓解的实际适用性，标志着这些技术首次被用于解决现实场景中的ASCAs和错误校正问题。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>麦克风在日常设备中的普及以及在线服务的依赖增加加剧了针对键盘的声学侧信道攻击（ASCAs）风险。</li>
<li>研究展示了深度学习技术，特别是视觉变压器（VTs）和大型语言模型（LLMs）在增强ASCAs有效性方面的潜力。</li>
<li>CoAtNet模型实现了显著性能提升并领先行业标准。</li>
<li>引入一种用于现实世界的噪声缓解方法以增强声学侧信道攻击性能。</li>
<li>通过大型语言模型的上下文理解能力检测和纠正噪声环境中的错误键击。</li>
<li>结合视觉变压器和语言模型提高ASCA缓解的实际适用性。这标志着首次运用这些技术解决现实场景中的ASCAs和错误校正问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e5265c9c8a7219913d1e01a89746d432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7e0789cfcf7a3534a02cd33bdb6d7fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6270588840f86de4b3007016d05efed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95841e816f35d0005d7d3dcf6fc7b494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-121a024fbd0291d120525b176d1e92a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d777c2c1c996729ab6ad726912d93ab9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df9744b47b68b12857ca7c62aa417381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e43243b366021a4492deab8d780a2037.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebde3e029be267ee99597e4de0aab12b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-40982bd7658f1fcab229dbfb2d2a5dec.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-21  Autellix An Efficient Serving Engine for LLM Agents as General Programs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-85cf51f3b681081b64e1b1d4e62c8145.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-02-20  AV-Flow Transforming Text to Audio-Visual Human-like Interactions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11666.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
