<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-21  Where&#39;s the Bug? Attention Probing for Scalable Fault Localization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-27d2c2cfdc1b6b372071f675d6641038.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-21-æ›´æ–°"><a href="#2025-02-21-æ›´æ–°" class="headerlink" title="2025-02-21 æ›´æ–°"></a>2025-02-21 æ›´æ–°</h1><h2 id="Whereâ€™s-the-Bug-Attention-Probing-for-Scalable-Fault-Localization"><a href="#Whereâ€™s-the-Bug-Attention-Probing-for-Scalable-Fault-Localization" class="headerlink" title="Whereâ€™s the Bug? Attention Probing for Scalable Fault Localization"></a>Whereâ€™s the Bug? Attention Probing for Scalable Fault Localization</h2><p><strong>Authors:Adam Stein, Arthur Wayne, Aaditya Naik, Mayur Naik, Eric Wong</strong></p>
<p>Ensuring code correctness remains a challenging problem even as large language models (LLMs) become increasingly capable at code-related tasks. While LLM-based program repair systems can propose bug fixes using only a userâ€™s bug report, their effectiveness is fundamentally limited by their ability to perform fault localization (FL), a challenging problem for both humans and LLMs. Existing FL approaches rely on executable test cases, require training on costly and often noisy line-level annotations, or demand resource-intensive LLMs. In this paper, we present Bug Attention Probe (BAP), a method which learns state-of-the-art fault localization without any direct localization labels, outperforming traditional FL baselines and prompting of large-scale LLMs. We evaluate our approach across a variety of code settings, including real-world Java bugs from the standard Defects4J dataset as well as seven other datasets which span a diverse set of bug types and languages. Averaged across all eight datasets, BAP improves by 34.6% top-1 accuracy compared to the strongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also significantly more efficient than prompting, outperforming large open-weight models at a small fraction of the computational cost. </p>
<blockquote>
<p>ç¡®ä¿ä»£ç æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå³ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šè¶Šæ¥è¶Šå¼ºå¤§ã€‚è™½ç„¶åŸºäºLLMçš„ç¨‹åºä¿®å¤ç³»ç»Ÿä»…ä½¿ç”¨ç”¨æˆ·çš„é”™è¯¯æŠ¥å‘Šå°±å¯ä»¥æå‡ºé”™è¯¯ä¿®å¤å»ºè®®ï¼Œä½†å®ƒä»¬çš„æœ‰æ•ˆæ€§ä»æ ¹æœ¬ä¸Šå—åˆ°æ•…éšœå®šä½ï¼ˆFLï¼‰èƒ½åŠ›çš„é™åˆ¶ï¼Œè¿™å¯¹äºäººç±»å’ŒLLMæ¥è¯´éƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æ•…éšœå®šä½æ–¹æ³•ä¾èµ–äºå¯æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹ï¼Œéœ€è¦åŸºäºæ˜‚è´µçš„ã€é€šå¸¸å¸¦æœ‰å™ªå£°çš„è¡Œçº§æ³¨é‡Šè¿›è¡Œè®­ç»ƒï¼Œæˆ–è€…éœ€è¦èµ„æºå¯†é›†å‹çš„LLMã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Bug Attention Probeï¼ˆBAPï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•ç›´æ¥å®šä½æ ‡ç­¾çš„æƒ…å†µä¸‹å®ç°æœ€å…ˆè¿›çš„æ•…éšœå®šä½ï¼Œå¹¶ä¸”ä¼˜äºä¼ ç»Ÿçš„æ•…éšœå®šä½åŸºå‡†çº¿å’Œå¤§å‹LLMçš„æç¤ºã€‚æˆ‘ä»¬åœ¨å¤šç§ä»£ç è®¾ç½®ä¸­å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ä½¿ç”¨æ ‡å‡†Defects4Jæ•°æ®é›†çš„çœŸå®ä¸–ç•ŒJavaé”™è¯¯ä»¥åŠå…¶ä»–æ¶µç›–å„ç§é”™è¯¯ç±»å‹å’Œè¯­è¨€çš„ä¸ƒä¸ªæ•°æ®é›†ã€‚åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šå¹³å‡ï¼ŒBAPåœ¨æœ€é«˜å‡†ç¡®ç‡æ–¹é¢æ¯”æœ€å¼ºåŸºå‡†çº¿æé«˜äº†34.6%ï¼Œæ¯”é›¶æ ·æœ¬æç¤ºGPT-4oæé«˜äº†93.4%ã€‚æ­¤å¤–ï¼ŒBAPè¿˜æ˜¾è‘—ä¼˜äºæç¤ºæ–¹æ³•ï¼Œåœ¨è®¡ç®—æˆæœ¬æ–¹é¢å¤§å¤§ä¼˜äºå¤§å‹å¼€æ”¾å¼æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13966v1">PDF</a> 14 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šçš„èƒ½åŠ›æ—¥ç›Šå¢å¼ºï¼Œä½†ä¿è¯ä»£ç æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚LLMä¸ºåŸºç¡€çš„ç¨‹åºä¿®å¤ç³»ç»Ÿå¯ä»¥é€šè¿‡ç”¨æˆ·çš„é”™è¯¯æŠ¥å‘Šæå‡ºä¿®å¤å»ºè®®ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—é™äºæ•…éšœå®šä½ï¼ˆFLï¼‰çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Bug Attention Probeï¼ˆBAPï¼‰ï¼Œè¯¥æ–¹æ³•æ— éœ€ç›´æ¥å®šä½æ ‡ç­¾å³å¯å­¦ä¹ æœ€å…ˆè¿›çš„æ•…éšœå®šä½ï¼Œä¸”åœ¨å¤šç§ä»£ç è®¾ç½®ä¸Šä¼˜äºä¼ ç»ŸFLåŸºå‡†æµ‹è¯•å’Œå¤§å‹LLMæç¤ºã€‚åœ¨åŒ…å«Defects4Jæ•°æ®é›†åœ¨å†…çš„å…«ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒBAPåœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šè¾ƒæœ€å¼ºåŸºå‡†æµ‹è¯•æé«˜äº†34.6%ï¼Œè¾ƒGPT-4oé›¶æ ·æœ¬æç¤ºæé«˜äº†93.4%ã€‚æ­¤å¤–ï¼ŒBAPåœ¨è®¡ç®—æˆæœ¬æ–¹é¢ä¹Ÿå…·æœ‰æ›´é«˜çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šçš„èƒ½åŠ›å¢å¼ºï¼Œä½†ä»£ç æ­£ç¡®æ€§ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>LLMä¸ºåŸºç¡€çš„ä¿®å¤ç³»ç»Ÿå—é™äºæ•…éšœå®šä½ï¼ˆFLï¼‰çš„èƒ½åŠ›ã€‚</li>
<li>Bug Attention Probeï¼ˆBAPï¼‰æ–¹æ³•æ— éœ€ç›´æ¥å®šä½æ ‡ç­¾å³å¯å­¦ä¹ å…ˆè¿›çš„æ•…éšœå®šä½ã€‚</li>
<li>BAPåœ¨å¤šç§ä»£ç è®¾ç½®ä¸Šä¼˜äºä¼ ç»ŸFLæ–¹æ³•å’Œå¤§å‹LLMæç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒBAPè¾ƒæœ€å¼ºåŸºå‡†æµ‹è¯•å’ŒGPT-4oæœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c0d864b6420c9d030ac44a5d9d841fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a683254e6f6d92b6a2303c811fb6b5e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc1304120d3f3920b579c3724ebac90f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a9023d55f2927df29c2df883a33cd1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-21\./crop_LLM/2502.13966v1/page_4_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f034e44463d8eed92904c558994909.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40c96b5ccb55f30674f861f0e070e671.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs"><a href="#Autellix-An-Efficient-Serving-Engine-for-LLM-Agents-as-General-Programs" class="headerlink" title="Autellix: An Efficient Serving Engine for LLM Agents as General Programs"></a>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</h2><p><strong>Authors:Michael Luo, Xiaoxiang Shi, Colin Cai, Tianjun Zhang, Justin Wong, Yichuan Wang, Chi Wang, Yanping Huang, Zhifeng Chen, Joseph E. Gonzalez, Ion Stoica</strong></p>
<p>Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programsâ€™ previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨æ­£é€æ¸ä»ç®€å•çš„èŠå¤©æœºå™¨äººæ¼”å˜ä¸ºåŠ¨æ€ã€é€šç”¨çš„ä»£ç†ç¨‹åºï¼Œå®ƒä»¬æ‰©å±•äº†LLMçš„è°ƒç”¨å’Œè¾“å‡ºä»¤ç‰Œï¼Œå¸®åŠ©AIä»£ç†è¿›è¡Œæ¨ç†ã€æ¢ç´¢å’Œè§£å†³å¤æ‚ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæœåŠ¡ç³»ç»Ÿå¿½ç•¥äº†ç¨‹åºå’Œè°ƒç”¨ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¤±å»äº†ä¼˜åŒ–çš„é‡è¦æœºä¼šã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæäº¤ç»™LLMæœåŠ¡å¼•æ“çš„ç¨‹åºä¼šç»å†é•¿æ—¶é—´çš„ç´¯è®¡ç­‰å¾…æ—¶é—´ï¼Œä¸»è¦æ˜¯ç”±äºä¸ªåˆ«LLMè¯·æ±‚å’Œç¨‹åºçš„å¤´çº¿é˜»å¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Autellixï¼Œä¸€ä¸ªå°†ç¨‹åºè§†ä¸ºé¦–è¦å…¬æ°‘çš„LLMæœåŠ¡ç³»ç»Ÿï¼Œä»¥æœ€å°åŒ–å…¶ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚Autellixæ‹¦æˆªç¨‹åºæäº¤çš„LLMè°ƒç”¨ï¼Œä¸°å¯Œè°ƒåº¦å™¨çš„ç¨‹åºçº§ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬ä¸ºå•çº¿ç¨‹å’Œåˆ†å¸ƒå¼ç¨‹åºæå‡ºäº†ä¸¤ç§è°ƒåº¦ç®—æ³•ï¼Œè¿™äº›ç®—æ³•ä¼šæ ¹æ®ç¨‹åºä¹‹å‰å®Œæˆçš„è°ƒç”¨é¢„å…ˆåˆ¤æ–­å’Œä¼˜å…ˆå¤„ç†LLMè°ƒç”¨ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¤šç§LLMå’Œä»£ç†å·¥ä½œè´Ÿè½½ä¸­ï¼Œä¸æœ€æ–°ç³»ç»Ÿï¼ˆå¦‚vLLMï¼‰ç›¸æ¯”ï¼ŒAutellixåœ¨ç›¸åŒå»¶è¿Ÿä¸‹æé«˜äº†ç¨‹åºçš„ååé‡4-15å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13965v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨æ­£ç”±ç®€å•çš„èŠå¤©æœºå™¨äººæ¼”å˜ä¸ºåŠ¨æ€ã€é€šç”¨çš„æ™ºèƒ½ä»£ç†ç¨‹åºã€‚ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿå¿½è§†äº†ç¨‹åºå’Œè°ƒç”¨é—´çš„ä¾èµ–å…³ç³»ï¼Œå¯¼è‡´AIä»£ç†åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å—é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Autellixç³»ç»Ÿï¼Œå®ƒé€šè¿‡ä¸°å¯Œè°ƒåº¦å™¨çš„ç¨‹åºçº§ä¸Šä¸‹æ–‡æ¥å¤„ç†ç¨‹åºä½œä¸ºé¦–è¦ä»»åŠ¡ï¼Œä»¥å‡å°‘ç«¯åˆ°ç«¯çš„å»¶è¿Ÿã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒAutellixåœ¨å¤šæ ·åŒ–çš„LLMå’Œæ™ºèƒ½ä»£ç†å·¥ä½œè´Ÿè½½ä¸Šï¼Œæé«˜äº†ç¨‹åºçš„ååé‡ï¼ŒåŒæ—¶ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼Œå»¶è¿Ÿä¿æŒä¸å˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåº”ç”¨æ­£æ¼”å˜ä¸ºæ›´åŠ¨æ€ã€é€šç”¨çš„æ™ºèƒ½ä»£ç†ç¨‹åºï¼Œæ¶‰åŠå¤æ‚çš„æ¨ç†å’Œé—®é¢˜è§£å†³ã€‚</li>
<li>ç°æœ‰LLMæœåŠ¡ç³»ç»Ÿå¿½è§†äº†ç¨‹åºå’Œè°ƒç”¨é—´çš„ä¾èµ–ï¼Œå¯¼è‡´ä¼˜åŒ–æœºä¼šä¸§å¤±ã€‚</li>
<li>Autellixç³»ç»Ÿå¤„ç†ç¨‹åºä½œä¸ºé¦–è¦ä»»åŠ¡ï¼Œé€šè¿‡ä¸°å¯Œè°ƒåº¦å™¨çš„ç¨‹åºçº§ä¸Šä¸‹æ–‡æ¥å‡å°‘ç«¯åˆ°ç«¯çš„å»¶è¿Ÿã€‚</li>
<li>Autellixæå‡ºäº†é’ˆå¯¹å•çº¿ç¨‹å’Œåˆ†å¸ƒå¼ç¨‹åºçš„ä¸¤ç§è°ƒåº¦ç®—æ³•ã€‚</li>
<li>è°ƒåº¦ç®—æ³•åŸºäºç¨‹åºä¹‹å‰å®Œæˆçš„è°ƒç”¨ï¼Œé¢„å…ˆåˆ¤æ–­å’Œä¼˜å…ˆå¤„ç†LLMè°ƒç”¨ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼ŒAutellixåœ¨å¤šç§LLMå’Œæ™ºèƒ½ä»£ç†å·¥ä½œè´Ÿè½½ä¸Šï¼Œæé«˜äº†ç¨‹åºçš„ååé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe6e516ea5fa6e30cc96400b0d184f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86c3539620d46ac89df892574201f42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94e9cf67ae67fc03b242f66ba640e91a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aa7a7d3e85e67f5c7a052d4308cc4fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1ff1ea4dd180efb1ece3ddbd6c69554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac7d3dee34d87a065c3290ed7c22d37f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Neurosymbolic-artificial-intelligence-via-large-language-models-and-coherence-driven-inference"><a href="#Neurosymbolic-artificial-intelligence-via-large-language-models-and-coherence-driven-inference" class="headerlink" title="Neurosymbolic artificial intelligence via large language models and   coherence-driven inference"></a>Neurosymbolic artificial intelligence via large language models and   coherence-driven inference</h2><p><strong>Authors:Steve Huntsman, Jewell Thomas</strong></p>
<p>We devise an algorithm to generate sets of propositions that objectively instantiate graphs that support coherence-driven inference. We then benchmark the ability of large language models (LLMs) to reconstruct coherence graphs from (a straightforward transformation of) propositions expressed in natural language, with promising results from a single prompt to models optimized for reasoning. Combining coherence-driven inference with consistency evaluations by neural models may advance the state of the art in machine cognition. </p>
<blockquote>
<p>æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œç”¨äºç”Ÿæˆå®¢è§‚å®ä¾‹åŒ–å›¾é›†ï¼Œè¿™äº›å›¾é›†æ”¯æŒè¿è´¯æ¨ç†ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»¥å‘½é¢˜ä¸ºåŸºå‡†ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„å‘½é¢˜ï¼ˆè¿›è¡Œç®€å•è½¬æ¢åï¼‰é‡å»ºè¿è´¯å›¾çš„èƒ½åŠ›ï¼Œä¼˜åŒ–æ¨ç†çš„æ¨¡å‹ä»å•ä¸€æç¤ºä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚å°†è¿è´¯æ¨ç†ä¸ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€è‡´æ€§è¯„ä¼°ç›¸ç»“åˆï¼Œå¯èƒ½ä¼šæ¨åŠ¨æœºå™¨è®¤çŸ¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13953v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®—æ³•ï¼Œå¯ä»¥æ ¹æ®å‘½é¢˜ç”Ÿæˆæ”¯æŒè¿è´¯æ¨ç†çš„å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„å‘½é¢˜é‡å»ºè¿è´¯å›¾çš„èƒ½åŠ›ï¼Œå¹¶ä»å•ä¸€æç¤ºä¸­å¯¹ä¼˜åŒ–æ¨ç†çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç»“åˆè¿è´¯æ¨ç†å’Œç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€è‡´æ€§è¯„ä¼°å¯èƒ½ä¼šæ¨åŠ¨æœºå™¨è®¤çŸ¥çš„æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œèƒ½å¤Ÿæ ¹æ®å‘½é¢˜ç”Ÿæˆæ”¯æŒè¿è´¯æ¨ç†çš„å›¾ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é‡å»ºè¿è´¯å›¾æ–¹é¢è¡¨ç°å‡ºäº†æ½œåŠ›ã€‚</li>
<li>é€šè¿‡å•ä¸€æç¤ºå¯¹ä¼˜åŒ–æ¨ç†çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚</li>
<li>ç»“åˆè¿è´¯æ¨ç†å’Œç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€è‡´æ€§è¯„ä¼°æœ‰åŠ©äºæ¨åŠ¨æœºå™¨è®¤çŸ¥çš„è¿›æ­¥ã€‚</li>
<li>è¯¥ç®—æ³•æœ‰åŠ©äºå¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿™ç§æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f41c4bf883da083ba57b93b839d78ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21c14cb2ed3d7c6ce84715247113a5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38ab58be5d2ceebb3994bded09ac9c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb8d35093314793e531b80161ffa418c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LongPO-Long-Context-Self-Evolution-of-Large-Language-Models-through-Short-to-Long-Preference-Optimization"><a href="#LongPO-Long-Context-Self-Evolution-of-Large-Language-Models-through-Short-to-Long-Preference-Optimization" class="headerlink" title="LongPO: Long Context Self-Evolution of Large Language Models through   Short-to-Long Preference Optimization"></a>LongPO: Long Context Self-Evolution of Large Language Models through   Short-to-Long Preference Optimization</h2><p><strong>Authors:Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transferring short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully retains short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é¢„è®­ç»ƒå’Œå¯¹é½è¿‡ç¨‹å±•ç°äº†æƒŠäººçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰å……è¶³å¯¹é½çš„é•¿è¯­å¢ƒåœºæ™¯ä¸‹ï¼Œä¼˜ç§€çš„çŸ­è¯­å¢ƒLLMå¯èƒ½ä¼šè¡¨ç°ä¸ä½³ã€‚ç”±äºé•¿è¯­å¢ƒå¯¹é½ä¸è¶³ä»¥åŠäººç±»ä¸ºæ‰©å±•è¯­å¢ƒè¿›è¡Œæ ‡æ³¨çš„ä¸åˆ‡å®é™…æ€§å’Œå¹³è¡¡çŸ­è¯­å¢ƒå’Œé•¿è¯­å¢ƒæ€§èƒ½çš„å›°éš¾ï¼Œè¿™ä¸€å¯¹é½è¿‡ç¨‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LongPOæ–¹æ³•ï¼Œå®ƒé€šè¿‡å†…éƒ¨è½¬ç§»çŸ­è¯­å¢ƒèƒ½åŠ›ï¼Œä½¿çŸ­è¯­å¢ƒLLMèƒ½å¤Ÿåœ¨é•¿è¯­å¢ƒä»»åŠ¡ä¸Šè‡ªæˆ‘è¿›åŒ–å¹¶è¡¨ç°å‡ºè‰²ã€‚LongPOåˆ©ç”¨LLMä»è‡ªæˆ‘ç”Ÿæˆçš„çŸ­åˆ°é•¿çš„åå¥½æ•°æ®ä¸­å­¦ä¹ ï¼Œè¿™äº›æ•°æ®ç”±ä¸ºç›¸åŒæŒ‡ä»¤ç”Ÿæˆçš„é…å¯¹å“åº”ç»„æˆï¼ŒåŒ…æ‹¬é•¿è¯­å¢ƒè¾“å…¥åŠå…¶å‹ç¼©çš„çŸ­è¯­å¢ƒå¯¹åº”ç‰©ã€‚è¿™ç§åå¥½æ­ç¤ºäº†çŸ­è¯­å¢ƒå¯¹é½è¿‡ç¨‹ä¸­åŸ¹å…»çš„LLMçš„èƒ½åŠ›å’Œæ½œåŠ›ï¼Œè¿™äº›èƒ½åŠ›åœ¨é•¿è¯­å¢ƒå¯¹é½ä¸è¶³çš„æƒ…å†µä¸‹å¯èƒ½ä¼šå‡å¼±ã€‚æ­¤å¤–ï¼ŒLongPOè¿˜èå…¥äº†ä¸€ä¸ªçŸ­åˆ°é•¿çš„KLçº¦æŸï¼Œä»¥ç¼“è§£é•¿è¯­å¢ƒå¯¹é½è¿‡ç¨‹ä¸­çŸ­è¯­å¢ƒæ€§èƒ½çš„ä¸‹é™ã€‚å½“åº”ç”¨äºMistral-7B-Instruct-v0.2æ¨¡å‹ï¼Œä»128Kåˆ°512Kçš„è¯­å¢ƒé•¿åº¦æ—¶ï¼ŒLongPOå……åˆ†ä¿ç•™äº†çŸ­è¯­å¢ƒæ€§èƒ½ï¼Œå¹¶åœ¨é•¿çŸ­è¯­å¢ƒä»»åŠ¡ä¸­å¤§å¤§ä¼˜äºç®€å•çš„SFTå’ŒDPOã€‚å…·ä½“æ¥è¯´ï¼Œç»è¿‡æˆ‘ä»¬æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹å¯ä»¥åœ¨é•¿è¯­å¢ƒåŸºå‡†æµ‹è¯•ä¸Šå–å¾—ä¸æˆ–è¶…è¶Šé«˜çº§LLMï¼ˆä¾‹å¦‚æ¶‰åŠå¹¿æ³›é•¿è¯­å¢ƒæ ‡æ³¨å’Œæ›´å¤§å‚æ•°è§„æ¨¡çš„GPT-4-128Kï¼‰çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13922v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é¢„è®­ç»ƒå’Œå¯¹é½å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡åœºæ™¯ä¸­ï¼Œä¼˜ç§€çš„çŸ­ç¯‡ä¸Šä¸‹æ–‡LLMå¯èƒ½ä¼šå› ç¼ºä¹é•¿ç¯‡ä¸Šä¸‹æ–‡å¯¹é½è€Œè¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LongPOæ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿè®©çŸ­ç¯‡ä¸Šä¸‹æ–‡LLMé€šè¿‡å†…éƒ¨è½¬ç§»çŸ­è¯­å¢ƒèƒ½åŠ›è‡ªæˆ‘è¿›åŒ–ï¼Œä»è€Œåœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚LongPOåˆ©ç”¨LLMä»è‡ªæˆ‘ç”Ÿæˆçš„çŸ­åˆ°é•¿çš„åå¥½æ•°æ®ä¸­å­¦ä¹ ï¼Œè¿™äº›æ•°æ®ç”±ä¸ºç›¸åŒæŒ‡ä»¤ç”Ÿæˆçš„é•¿ç¯‡ä¸Šä¸‹æ–‡è¾“å…¥å’Œç›¸åº”çš„çŸ­ç¯‡ä¸Šä¸‹æ–‡å‹ç¼©ç‰ˆæœ¬é…å¯¹å“åº”ç»„æˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒLongPOæ­ç¤ºäº†LLMåœ¨çŸ­ç¯‡ä¸Šä¸‹æ–‡å¯¹é½è¿‡ç¨‹ä¸­åŸ¹å…»çš„èƒ½åŠ›å’Œæ½œåŠ›ï¼Œè¿™äº›èƒ½åŠ›å¯èƒ½åœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡åœºæ™¯ä¸­å› å¯¹é½ä¸è¶³è€Œå‡å¼±ã€‚æ­¤å¤–ï¼ŒLongPOè¿˜å¼•å…¥äº†ä¸€ä¸ªçŸ­åˆ°é•¿çš„KLçº¦æŸï¼Œä»¥ç¼“è§£é•¿ç¯‡ä¸Šä¸‹æ–‡å¯¹é½è¿‡ç¨‹ä¸­çŸ­ç¯‡ä¸Šä¸‹æ–‡æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚å½“åº”ç”¨äºMistral-7B-Instruct-v0.2æ¨¡å‹æ—¶ï¼Œä»128Kåˆ°512Kçš„è¯­å¢ƒé•¿åº¦èŒƒå›´å†…ï¼ŒLongPOå¯ä»¥å®Œå…¨ä¿ç•™çŸ­ç¯‡ä¸Šä¸‹æ–‡çš„æ€§èƒ½ï¼Œå¹¶åœ¨é•¿çŸ­è¯­å¢ƒä»»åŠ¡ä¸­å¤§å¤§ä¼˜äºç®€å•çš„SFTå’ŒDPOã€‚ç‰¹åˆ«æ˜¯ï¼Œç»è¿‡LongPOè®­ç»ƒçš„æ¨¡å‹åœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¯ä¸æˆ–è¶…è¶Šé¡¶çº§LLMï¼ˆå¦‚GPT-4-128Kï¼‰ç›¸å½“ï¼Œè¿™äº›LLMæ¶‰åŠå¤§é‡çš„é•¿ç¯‡ä¸Šä¸‹æ–‡æ ‡æ³¨å’Œæ›´å¤§çš„å‚æ•°è§„æ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡é¢„è®­ç»ƒå’Œå¯¹é½å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡åœºæ™¯ä¸­å¯èƒ½å› ç¼ºä¹è¶³å¤Ÿçš„é•¿ç¯‡ä¸Šä¸‹æ–‡å¯¹é½è€Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>LongPOæ–¹æ³•èƒ½å¤Ÿè®©çŸ­ç¯‡ä¸Šä¸‹æ–‡çš„LLMè‡ªæˆ‘è¿›åŒ–ï¼Œé€šè¿‡å†…éƒ¨è½¬ç§»çŸ­è¯­å¢ƒèƒ½åŠ›ï¼Œåœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šå®ç°å“è¶Šè¡¨ç°ã€‚</li>
<li>LongPOåˆ©ç”¨LLMå­¦ä¹ è‡ªæˆ‘ç”Ÿæˆçš„çŸ­åˆ°é•¿çš„åå¥½æ•°æ®ï¼Œæ­ç¤ºLLMåœ¨çŸ­ç¯‡ä¸Šä¸‹æ–‡å¯¹é½è¿‡ç¨‹ä¸­çš„èƒ½åŠ›å’Œæ½œåŠ›ã€‚</li>
<li>LongPOå¼•å…¥çŸ­åˆ°é•¿çš„KLçº¦æŸï¼Œä»¥ç¼“è§£é•¿ç¯‡ä¸Šä¸‹æ–‡å¯¹é½è¿‡ç¨‹ä¸­çš„çŸ­ç¯‡ä¸Šä¸‹æ–‡æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>LongPOåœ¨ä¿æŒçŸ­ç¯‡ä¸Šä¸‹æ–‡æ€§èƒ½çš„åŒæ—¶ï¼Œæå‡äº†é•¿ç¯‡ä¸Šä¸‹æ–‡ä»»åŠ¡çš„è¡¨ç°ã€‚</li>
<li>LongPOè®­ç»ƒçš„æ¨¡å‹åœ¨é•¿ç¯‡ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¯è¶…è¶Šä¸€äº›éœ€è¦æ›´å¤šé•¿ç¯‡ä¸Šä¸‹æ–‡æ ‡æ³¨å’Œæ›´å¤§å‚æ•°è§„æ¨¡çš„é¡¶çº§LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9ab7be1c30c4a97b26e77f18fba7648.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04f70589ed2f63183d95a36e76e000f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-928089c3d6230cd36df83f0509d07f6c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TESS-2-A-Large-Scale-Generalist-Diffusion-Language-Model"><a href="#TESS-2-A-Large-Scale-Generalist-Diffusion-Language-Model" class="headerlink" title="TESS 2: A Large-Scale Generalist Diffusion Language Model"></a>TESS 2: A Large-Scale Generalist Diffusion Language Model</h2><p><strong>Authors:Jaesung Tae, Hamish Ivison, Sachin Kumar, Arman Cohan</strong></p>
<p>We introduce TESS 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive (AR) models. We train TESS 2 by first adapting a strong AR model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that TESS 2 further improves with increased inference-time compute, highlighting the utility of diffusion LMs in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2">https://github.com/hamishivi/tess-2</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†TESS 2ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æŒ‡ä»¤éµå¾ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å½“ä»£æŒ‡ä»¤ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ï¼Œä¸å¼ºå¤§çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç›¸åŒ¹é…ï¼Œæœ‰æ—¶ç”šè‡³è¶…è¿‡å®ƒä»¬ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆä½¿ç”¨å¼ºå¤§çš„ARæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼Œä»¥å¸¸è§„çš„äº¤å‰ç†µä½œä¸ºæ‰©æ•£æŸå¤±ï¼Œç„¶åè¿›ä¸€æ­¥è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´æ¥è®­ç»ƒTESS 2ã€‚æˆ‘ä»¬å‘ç°é€‚åº”è®­ç»ƒä»¥åŠåŸºç¡€æ¨¡å‹çš„é€‰æ‹©å¯¹äºè®­ç»ƒè‰¯å¥½çš„æŒ‡ä»¤éµå¾ªæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå¥–åŠ±æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¨¡å—åŒ–çš„æ¨ç†æ—¶é—´æŒ‡å¯¼ç¨‹åºï¼Œæ— éœ€è®­ç»ƒåº•å±‚æ¨¡å‹å³å¯å¯¹é½æ¨¡å‹è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†éšç€æ¨ç†æ—¶é—´è®¡ç®—é‡çš„å¢åŠ ï¼ŒTESS 2çš„æ€§èƒ½ä¼šè¿›ä¸€æ­¥æé«˜ï¼Œè¿™çªå‡ºäº†æ‰©æ•£LMåœ¨æ¨ç†æ—¶é—´ä½¿ç”¨è®¡ç®—é‡çš„ç²¾ç»†æ§åˆ¶æ–¹é¢çš„å®ç”¨æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hamishivi/tess-2%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hamishivi/tess-2è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13917v1">PDF</a> preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TESS 2ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨æŒ‡ä»¤éµå¾ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†å½“ä»£æŒ‡ä»¤è°ƒæ•´æ‰©æ•£æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶ä¸”ä¸å¼ºå¤§çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³åœ¨æŸäº›æ–¹é¢è¶…è¿‡å®ƒä»¬ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆä½¿ç”¨å¸¸è§„äº¤å‰ç†µä½œä¸ºæ‰©æ•£æŸå¤±å¯¹å¼ºå¤§çš„ARæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒæ¥è®­ç»ƒTESS 2ï¼Œç„¶åè¿›è¡Œè¿›ä¸€æ­¥çš„æŒ‡ä»¤è°ƒæ•´ã€‚æˆ‘ä»¬å‘ç°é€‚åº”è®­ç»ƒä»¥åŠåŸºç¡€æ¨¡å‹çš„é€‰æ‹©å¯¹äºè®­ç»ƒè‰¯å¥½çš„æŒ‡ä»¤éµå¾ªæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å¥–åŠ±æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¨¡å—åŒ–çš„æ¨ç†æ—¶é—´æŒ‡å¯¼ç¨‹åºï¼Œå¯ä»¥åœ¨ä¸éœ€è¦è®­ç»ƒåº•å±‚æ¨¡å‹çš„æƒ…å†µä¸‹å¯¹é½æ¨¡å‹è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œéšç€æ¨ç†æ—¶é—´è®¡ç®—é‡çš„å¢åŠ ï¼ŒTESS 2çš„æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œè¿™çªå‡ºäº†æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶é—´ä½¿ç”¨çš„è®¡ç®—é‡æ–¹é¢å…·æœ‰ç²¾ç»†å¯æ§æ€§çš„å®ç”¨æ€§ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>TESS 2æ˜¯ä¸€ç§é€šç”¨æŒ‡ä»¤éµå¾ªæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œè¡¨ç°ä¼˜äºå½“ä»£æŒ‡ä»¤è°ƒæ•´æ‰©æ•£æ¨¡å‹å’Œå¼ºå¤§çš„è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>TESS 2çš„è®­ç»ƒåŒ…æ‹¬ä½¿ç”¨äº¤å‰ç†µä½œä¸ºæ‰©æ•£æŸå¤±çš„æŒç»­é¢„è®­ç»ƒï¼Œä»¥åŠè¿›ä¸€æ­¥çš„æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>é€‚åº”è®­ç»ƒå’ŒåŸºç¡€æ¨¡å‹çš„é€‰æ‹©å¯¹äºè®­ç»ƒè‰¯å¥½çš„æŒ‡ä»¤éµå¾ªæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†å¥–åŠ±æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¨ç†æ—¶é—´æŒ‡å¯¼ç¨‹åºï¼Œå¯ä»¥åœ¨ä¸éœ€è¦è®­ç»ƒåº•å±‚æ¨¡å‹çš„æƒ…å†µä¸‹å¯¹é½æ¨¡å‹è¾“å‡ºã€‚</li>
<li>TESS 2çš„æ€§èƒ½éšç€æ¨ç†æ—¶é—´è®¡ç®—é‡çš„å¢åŠ è€Œæé«˜ã€‚</li>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹å…·æœ‰åœ¨æ¨ç†æ—¶é—´ä½¿ç”¨çš„è®¡ç®—é‡æ–¹é¢çš„ç²¾ç»†å¯æ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49a2986a21913a916752c76c54995e47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0306a4da6cef91012aed6eac48cc0e31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ac1d9e2084d7b36cb18481b5fe2fde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35b02f49e8d9097cfdd170eef5bcbfa9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation"><a href="#Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation" class="headerlink" title="Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?"></a>Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?</h2><p><strong>Authors:Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park</strong></p>
<p>Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in usersâ€™ item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMsâ€™ ability to understand usersâ€™ item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec">https://github.com/Sein-Kim/LLM-SRec</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶å…ˆè¿›çš„æ–‡æœ¬ç†è§£èƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ„è¯†ï¼Œæœ€è¿‘è¢«å…¬è®¤ä¸ºæ¨èå·¥å…·ä¸­çš„æœ‰å‰é€”çš„å·¥å…·ã€‚å°½ç®¡å½“å‰åŸºäºLLMçš„æ¨èï¼ˆLLM4Recï¼‰æ¨¡å‹æ˜¯åœ¨é¡ºåºæ¨èåœºæ™¯ä¸‹è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹æ˜¯å¦ç†è§£ç”¨æˆ·é¡¹ç›®äº¤äº’åºåˆ—ä¸­å›ºæœ‰çš„é¡ºåºä¿¡æ¯å´è¢«å¤§å¤§å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯æ˜ï¼Œç°æœ‰çš„LLM4Recæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¹¶æ²¡æœ‰å®Œå…¨æ•è·é¡ºåºä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºLLMçš„é¡ºåºæ¨èå™¨ï¼Œç§°ä¸ºLLM-SRecã€‚è¿™æ˜¯ä¸€ç§é€šè¿‡è’¸é¦ä»é¢„è®­ç»ƒçš„CF-SRecæ¨¡å‹ä¸­æå–çš„ç”¨æˆ·è¡¨ç¤ºï¼Œå°†å…¶èå…¥LLMsä¸­ï¼Œä»è€Œå¢å¼ºLLMså¯¹é¡ºåºä¿¡æ¯çš„æ•´åˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLLM-SRecå¢å¼ºäº†LLMsç†è§£ç”¨æˆ·é¡¹ç›®äº¤äº’åºåˆ—çš„èƒ½åŠ›ï¼Œæœ€ç»ˆæé«˜äº†æ¨èæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„éœ€è¦å¾®è°ƒLLMsçš„LLM4Recæ¨¡å‹ä¸åŒï¼ŒLLM-SRecä»…é€šè¿‡è®­ç»ƒä¸€äº›è½»é‡çº§çš„MLPså°±å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Sein-Kim/LLM-SRecæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13909v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨èç³»ç»Ÿä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ï¼Œå› å…¶å¼ºå¤§çš„æ–‡æœ¬ç†è§£èƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚ä½†ç°æœ‰LLMåœ¨æ¨èï¼ˆLLM4Recï¼‰æ¨¡å‹ä¸­å¾€å¾€å¿½ç•¥ç”¨æˆ·äº¤äº’åºåˆ—ä¸­çš„æ—¶åºä¿¡æ¯ã€‚æœ¬æ–‡é€šè¿‡å®éªŒéªŒè¯ç°æœ‰LLM4Recæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†æ•æ‰æ—¶åºä¿¡æ¯ï¼Œå¹¶æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºLLMçš„æ—¶åºæ¨èæ–¹æ³•LLM-SRecã€‚è¯¥æ–¹æ³•é€šè¿‡è’¸é¦é¢„è®­ç»ƒCF-SRecæ¨¡å‹ä¸­çš„ç”¨æˆ·è¡¨ç¤ºåˆ°LLMä¸­ï¼Œå¢å¼ºäº†LLMç†è§£ç”¨æˆ·äº¤äº’åºåˆ—çš„èƒ½åŠ›ï¼Œæé«˜äº†æ¨èæ€§èƒ½ã€‚å¹¶ä¸”ç›¸è¾ƒäºç°æœ‰LLM4Recæ¨¡å‹ï¼ŒLLM-SRecä»…éœ€è®­ç»ƒå°‘é‡è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰å³å¯è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºåœ¨æ¨èç³»ç»Ÿä¸­çš„æ½œåŠ›ï¼Œå½’åŠŸäºå…¶å¼ºå¤§çš„æ–‡æœ¬ç†è§£èƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚</li>
<li>ç°æœ‰LLM4Recæ¨¡å‹å¿½ç•¥äº†ç”¨æˆ·äº¤äº’åºåˆ—ä¸­çš„æ—¶åºä¿¡æ¯ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œç°æœ‰LLM4Recæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¯¹æ—¶åºä¿¡æ¯çš„æ•æ‰ä¸è¶³ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºLLMçš„æ—¶åºæ¨èæ–¹æ³•LLM-SRecï¼Œèƒ½æœ‰æ•ˆå¢å¼ºLLMç†è§£ç”¨æˆ·äº¤äº’åºåˆ—çš„èƒ½åŠ›ã€‚</li>
<li>LLM-SRecé€šè¿‡è’¸é¦é¢„è®­ç»ƒCF-SRecæ¨¡å‹ä¸­çš„ç”¨æˆ·è¡¨ç¤ºåˆ°LLMä¸­ï¼Œæé«˜äº†æ¨èæ€§èƒ½ã€‚</li>
<li>LLM-SRecå®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ¨èæ€§èƒ½ï¼Œä¸”ç›¸è¾ƒäºå…¶ä»–LLM4Recæ¨¡å‹æ›´ä¸ºå®ç”¨ï¼Œä»…éœ€è®­ç»ƒå°‘é‡è½»é‡çº§MLPsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f641e4928de220f35ca90358881b80b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a650a40c6af80db5555293e00112d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa1307b97ddd357ce63767b177741ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac00f70619628c5d1e3f8b7c93cccaf6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Judging-the-Judges-A-Collection-of-LLM-Generated-Relevance-Judgements"><a href="#Judging-the-Judges-A-Collection-of-LLM-Generated-Relevance-Judgements" class="headerlink" title="Judging the Judges: A Collection of LLM-Generated Relevance Judgements"></a>Judging the Judges: A Collection of LLM-Generated Relevance Judgements</h2><p><strong>Authors:Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz</strong></p>
<p>Using Large Language Models (LLMs) for relevance assessments offers promising opportunities to improve Information Retrieval (IR), Natural Language Processing (NLP), and related fields. Indeed, LLMs hold the promise of allowing IR experimenters to build evaluation collections with a fraction of the manual human labor currently required. This could help with fresh topics on which there is still limited knowledge and could mitigate the challenges of evaluating ranking systems in low-resource scenarios, where it is challenging to find human annotators. Given the fast-paced recent developments in the domain, many questions concerning LLMs as assessors are yet to be answered. Among the aspects that require further investigation, we can list the impact of various components in a relevance judgment generation pipeline, such as the prompt used or the LLM chosen.   This paper benchmarks and reports on the results of a large-scale automatic relevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where different relevance assessment approaches were proposed. In detail, we release and benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track relevance judgments produced by eight international teams who participated in the challenge. Given their diverse nature, these automatically generated relevance judgments can help the community not only investigate systematic biases caused by LLMs but also explore the effectiveness of ensemble models, analyze the trade-offs between different models and human assessors, and advance methodologies for improving automated evaluation techniques. The released resource is available at the following link: <a target="_blank" rel="noopener" href="https://llm4eval.github.io/LLMJudge-benchmark/">https://llm4eval.github.io/LLMJudge-benchmark/</a> </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç›¸å…³æ€§è¯„ä¼°ä¸ºä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œç›¸å…³é¢†åŸŸæä¾›äº†æ”¹è¿›çš„æœ‰å‰é€”çš„æœºä¼šã€‚å®é™…ä¸Šï¼ŒLLMæœ‰æœ›è®©IRå®éªŒè€…ä½¿ç”¨å½“å‰æ‰€éœ€äººå·¥åŠ³åŠ¨çš„ä¸€å°éƒ¨åˆ†æ¥æ„å»ºè¯„ä¼°é›†åˆã€‚è¿™æœ‰åŠ©äºå¤„ç†æœ‰é™çŸ¥è¯†çš„æ–°ä¸»é¢˜ï¼Œå¹¶å¯èƒ½å‡è½»åœ¨ä½èµ„æºåœºæ™¯ä¸­è¯„ä¼°æ’åç³»ç»Ÿçš„æŒ‘æˆ˜ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå¾ˆéš¾æ‰¾åˆ°äººç±»æ³¨é‡Šè€…ã€‚è€ƒè™‘åˆ°è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå…³äºLLMä½œä¸ºè¯„ä¼°è€…çš„è®¸å¤šé—®é¢˜è¿˜æœ‰å¾…å›ç­”ã€‚åœ¨éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥çš„æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥åˆ—å‡ºç›¸å…³æ€§åˆ¤æ–­ç”Ÿæˆç®¡é“ä¸­å„ç§ç»„ä»¶çš„å½±å“ï¼Œä¾‹å¦‚ä½¿ç”¨çš„æç¤ºæˆ–é€‰æ‹©çš„LLMã€‚æœ¬æ–‡ä»‹ç»äº†å¤§è§„æ¨¡è‡ªåŠ¨ç›¸å…³æ€§è¯„ä¼°åŸºå‡†æµ‹è¯•LLMJudgeæŒ‘æˆ˜çš„ç»“æœï¼Œè¯¥æŒ‘æˆ˜äº2024å¹´SIGIRä¼šè®®ä¸Šä¸¾åŠï¼Œå…¶ä¸­æå‡ºäº†ä¸åŒçš„ç›¸å…³æ€§è¯„ä¼°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘å¸ƒå¹¶åŸºå‡†æµ‹è¯•äº†TREC 2023æ·±åº¦å­¦ä¹ è½¨é“ç›¸å…³æ€§åˆ¤æ–­çš„42ä¸ªLLMç”Ÿæˆæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾ç”±å‚ä¸æŒ‘æˆ˜çš„å…«ä¸ªå›½é™…å›¢é˜Ÿäº§ç”Ÿã€‚è¿™äº›è‡ªåŠ¨ç”Ÿæˆçš„ç›¸å…³æ€§åˆ¤æ–­å…·æœ‰å¤šæ ·æ€§ï¼Œä¸ä»…æœ‰åŠ©äºç¤¾åŒºç ”ç©¶LLMå¼•èµ·çš„ç³»ç»Ÿåè§ï¼Œè¿˜å¯ä»¥æ¢ç´¢é›†æˆæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œåˆ†æä¸åŒæ¨¡å‹ä¸äººç±»è¯„ä¼°è€…ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æ¨è¿›æ”¹è¿›è‡ªåŠ¨åŒ–è¯„ä¼°æŠ€æœ¯çš„æ–¹æ³•ã€‚å‘å¸ƒçš„èµ„æºå¯åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://llm4eval.github.io/LLMJudge-benchmark/">https://llm4eval.github.io/LLMJudge-benchmark/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13908v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç›¸å…³æ€§è¯„ä¼°æ–¹é¢çš„åº”ç”¨ä¸ºä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç­‰ç›¸å…³é¢†åŸŸå¸¦æ¥äº†æ”¹è¿›çš„å¸Œæœ›ã€‚LLMå¯ä»¥å‡è½»IRå®éªŒè€…æ„å»ºè¯„ä¼°é›†åˆæ—¶æ‰€éœ€çš„å¤§é‡æ‰‹åŠ¨åŠ³åŠ¨åŠ›ï¼Œæœ‰åŠ©äºåº”å¯¹ç¼ºä¹äººç±»æ³¨é‡Šè€…è¯„ä¼°ä½èµ„æºåœºæ™¯ä¸­æ’åç³»ç»Ÿçš„æŒ‘æˆ˜ã€‚é’ˆå¯¹LLMä½œä¸ºè¯„ä¼°è€…çš„å¿«é€Ÿå‘å±•é¢†åŸŸï¼Œè¿˜æœ‰è®¸å¤šé—®é¢˜æœ‰å¾…å›ç­”ï¼Œå¦‚ç”Ÿæˆç®¡é“ä¸­ä¸åŒç»„ä»¶çš„å½±å“ç­‰ã€‚æœ¬æ–‡ä»‹ç»äº†åœ¨SIGIR 2024å¹´ä¸¾åŠçš„LLMJudgeæŒ‘æˆ˜èµ›çš„ç»“æœï¼Œè¯¥æŒ‘æˆ˜èµ›æ—¨åœ¨è¯„ä¼°ä¸åŒçš„ç›¸å…³æ€§è¯„ä¼°æ–¹æ³•ã€‚é€šè¿‡å¯¹æ¯”å…«ä¸ªå›½é™…å›¢é˜Ÿæå‡ºçš„è‡ªåŠ¨ç”Ÿæˆçš„TREC 2023æ·±åº¦å­¦ä¹ è½¨è¿¹ç›¸å…³æ€§åˆ¤æ–­æ ‡ç­¾ï¼Œè¯¥æŒ‘æˆ˜èµ›æœ‰åŠ©äºç¤¾åŒºç ”ç©¶LLMå¼•èµ·çš„ç³»ç»Ÿæ€§åè§é—®é¢˜ï¼Œå¹¶æ¢è®¨é›†æˆæ¨¡å‹çš„æœ‰æ•ˆæ€§ç­‰å…³é”®é¢†åŸŸè¿›æ­¥ç ”ç©¶æ–¹æ³•å’Œè‡ªåŠ¨è¯„ä¼°æŠ€æœ¯çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ç›¸å…³æ€§è¯„ä¼°é¢†åŸŸå±•ç°å‡ºæ”¹è¿›IRå’ŒNLPç­‰è¡Œä¸šçš„æ½œåŠ›ã€‚</li>
<li>LLMèƒ½å¤Ÿå¤§å¹…å‡å°‘æ„å»ºè¯„ä¼°é›†åˆæ‰€éœ€çš„äººå·¥å‚ä¸ç¨‹åº¦ã€‚</li>
<li>LLMJudgeæŒ‘æˆ˜èµ›çš„ç»“æœå±•ç¤ºäº†ä¸åŒç›¸å…³æ€§è¯„ä¼°æ–¹æ³•çš„å¯¹æ¯”ç»“æœã€‚</li>
<li>è‡ªåŠ¨ç”Ÿæˆçš„è¯„ä¼°æ•°æ®èƒ½å¤Ÿå¸®åŠ©ç¤¾åŒºæ¢ç©¶LLMå¸¦æ¥çš„æ½œåœ¨åè§ã€‚</li>
<li>å¯¹é›†æˆæ¨¡å‹æœ‰æ•ˆæ€§çš„æ¢ç´¢æ˜¯ä¸€ä¸ªå…³é”®é¢†åŸŸã€‚</li>
<li>é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹å’Œäººç±»è¯„ä¼°è€…çš„ç»“æœï¼Œå¯ä»¥åˆ†æå‡ºå…¶ä¸­çš„æƒè¡¡å’Œå–èˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab733214a540913731e31010ccd2dd4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-367e64897a24a79d8120fa2368b0eb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae411243971b2e420bc858d70ba9fa24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dacd0f90dbee86cdd150fdecfe410de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddf72ee9b6410db86a66364a13552a22.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DataSciBench-An-LLM-Agent-Benchmark-for-Data-Science"><a href="#DataSciBench-An-LLM-Agent-Benchmark-for-Data-Science" class="headerlink" title="DataSciBench: An LLM Agent Benchmark for Data Science"></a>DataSciBench: An LLM Agent Benchmark for Data Science</h2><p><strong>Authors:Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue</strong></p>
<p>This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at <a target="_blank" rel="noopener" href="https://github.com/THUDM/DataSciBench">https://github.com/THUDM/DataSciBench</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DataSciBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ•°æ®ç§‘å­¦ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚æœ€è¿‘çš„ç›¸å…³åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºå•ä¸€ä»»åŠ¡ã€å®¹æ˜“è·å¾—çš„çœŸå®æ•°æ®å’Œç®€å•çš„è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™é™åˆ¶äº†å¯ä»¥è¯„ä¼°çš„ä»»åŠ¡èŒƒå›´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDataSciBenchçš„æ„å»ºåŸºç¡€æ˜¯æ›´å…¨é¢ã€æ›´ç²¾å¿ƒæŒ‘é€‰çš„è‡ªç„¶å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºï¼Œä»¥åŠä¸ç¡®å®šçš„çœŸå®æ•°æ®å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–ç®¡é“æ¥ç”ŸæˆçœŸå®æ•°æ®ï¼ˆGTï¼‰å¹¶éªŒè¯è¯„ä¼°æŒ‡æ ‡ã€‚è¯¥ç®¡é“åˆ©ç”¨å¹¶å®ç°äº†ä¸€ç§åŸºäºLLMçš„è‡ªæ´½å’Œäººç±»éªŒè¯ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨æ”¶é›†çš„æç¤ºã€é¢„å®šä¹‰çš„ä»»åŠ¡ç±»å‹å’ŒèšåˆåŠŸèƒ½ï¼ˆæŒ‡æ ‡ï¼‰æ¥äº§ç”Ÿå‡†ç¡®çš„GTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ›æ–°çš„Task-Function-Codeï¼ˆTFCï¼‰æ¡†æ¶ï¼Œæ ¹æ®ç²¾ç¡®å®šä¹‰çš„æŒ‡æ ‡å’Œç¨‹åºè§„åˆ™æ¥è¯„ä¼°æ¯ä¸ªä»£ç æ‰§è¡Œç»“æœã€‚æˆ‘ä»¬çš„å®éªŒæ¡†æ¶åŒ…æ‹¬ä½¿ç”¨æˆ‘ä»¬æ”¶é›†çš„å¤šæ ·æç¤ºæ¥æµ‹è¯•6ä¸ªAPIæ¨¡å‹ã€8ä¸ªå¼€æºé€šç”¨æ¨¡å‹å’Œ9ä¸ªå¼€æºä»£ç ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æä¾›æ•°æ®ç§‘å­¦ä¸­LLMçš„æ›´å…¨é¢å’Œä¸¥æ ¼è¯„ä¼°ï¼Œæ­ç¤ºå…¶ä¼˜ç¼ºç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPIæ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¼€æºæ¨¡å‹ï¼ŒDeepseek-Coder-33B-Instructåœ¨å¼€æºæ¨¡å‹ä¸­å¾—åˆ†æœ€é«˜ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/DataSciBench%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/THUDM/DataSciBenchä¸Šå‘å¸ƒäº†æ‰€æœ‰ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13897v1">PDF</a> 40 pages, 7 figures, 6 tables</p>
<p><strong>Summary</strong><br>DataSciBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°æ®ç§‘å­¦æ–¹é¢çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ç°æœ‰çš„ä¸»è¦ä¾§é‡äºå•ä¸€ä»»åŠ¡ã€æ˜“äºè·å¾—çœŸå®æ ‡è®°å’Œç›´è§‚è¯„ä¼°æŒ‡æ ‡çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒDataSciBenchåŸºäºæ›´å…¨é¢å’Œç²¾é€‰çš„è‡ªç„¶å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºæ„å»ºï¼Œç”¨äºä¸ç¡®å®šçš„çœŸå®æ ‡è®°å’Œè¯„ä¼°æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶å¼€å‘äº†åŠè‡ªåŠ¨åŒ–ç®¡é“æ¥ç”ŸæˆçœŸå®æ ‡è®°å’ŒéªŒè¯è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶åˆ©ç”¨LLMçš„è‡ªæ´½æ€§å’Œäººå·¥éªŒè¯ç­–ç•¥æ¥äº§ç”Ÿå‡†ç¡®çš„çœŸå®æ ‡è®°ã€‚æ­¤å¤–ï¼Œæå‡ºäº†åˆ›æ–°çš„Task-Function-Codeï¼ˆTFCï¼‰æ¡†æ¶ï¼Œæ ¹æ®ç²¾ç¡®å®šä¹‰çš„æŒ‡æ ‡å’Œç¨‹åºè§„åˆ™æ¥è¯„ä¼°ä»£ç æ‰§è¡Œçš„ç»“æœã€‚å®éªŒæ¡†æ¶å¯¹å„ç§æ¨¡å‹è¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬APIæ¨¡å‹ã€å¼€æºé€šç”¨æ¨¡å‹å’Œå¼€æºä»£ç ç”Ÿæˆæ¨¡å‹ã€‚æ—¨åœ¨æä¾›æ›´å…¨é¢ã€ä¸¥æ ¼çš„LLMåœ¨æ•°æ®ç§‘å­¦æ–¹é¢çš„è¯„ä¼°ï¼Œæ­ç¤ºå…¶ä¼˜ç¼ºç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DataSciBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨æ•°æ®ç§‘å­¦æ–¹é¢èƒ½åŠ›çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å•ä¸€ä»»åŠ¡ï¼Œè€ŒDataSciBenchåŒ…å«è‡ªç„¶å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºã€‚</li>
<li>ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŠè‡ªåŠ¨åŒ–ç®¡é“æ¥ç”ŸæˆçœŸå®æ ‡è®°å¹¶éªŒè¯è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>åˆ©ç”¨LLMçš„è‡ªæ´½æ€§å’Œäººå·¥éªŒè¯ç­–ç•¥æ¥äº§ç”Ÿå‡†ç¡®çš„çœŸå®æ ‡è®°ã€‚</li>
<li>æå‡ºäº†Task-Function-Codeï¼ˆTFCï¼‰æ¡†æ¶æ¥è¯„ä¼°ä»£ç æ‰§è¡Œçš„ç»“æœã€‚</li>
<li>å®éªŒæ¡†æ¶æµ‹è¯•äº†å¤šç§ç±»å‹çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬APIæ¨¡å‹ã€å¼€æºé€šç”¨æ¨¡å‹å’Œå¼€æºä»£ç ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d388a207167df75441b77a0fad6ffb0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ef9a58799020e42abce8c266f36c980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5533d0e8b9a973b416c045ccba6557.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec17468b0e0624442aa694262dd403ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dcf0a1ee457cb1d69cf940846598ab0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SPEX-Scaling-Feature-Interaction-Explanations-for-LLMs"><a href="#SPEX-Scaling-Feature-Interaction-Explanations-for-LLMs" class="headerlink" title="SPEX: Scaling Feature Interaction Explanations for LLMs"></a>SPEX: Scaling Feature Interaction Explanations for LLMs</h2><p><strong>Authors:Justin Singh Kang, Landon Butler, Abhineet Agarwal, Yigit Efe Erginbas, Ramtin Pedarsani, Kannan Ramchandran, Bin Yu</strong></p>
<p>Large language models (LLMs) have revolutionized machine learning due to their ability to capture complex interactions between input features. Popular post-hoc explanation methods like SHAP provide marginal feature attributions, while their extensions to interaction importances only scale to small input lengths ($\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic interaction attribution algorithm that efficiently scales to large input lengths ($\approx 1000)$. SPEX exploits underlying natural sparsity among interactions â€“ common in real-world data â€“ and applies a sparse Fourier transform using a channel decoding algorithm to efficiently identify important interactions. We perform experiments across three difficult long-context datasets that require LLMs to utilize interactions between inputs to complete the task. For large inputs, SPEX outperforms marginal attribution methods by up to 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX successfully identifies key features and interactions that strongly influence model output. For one of our datasets, HotpotQA, SPEX provides interactions that align with human annotations. Finally, we use our model-agnostic approach to generate explanations to demonstrate abstract reasoning in closed-source LLMs (GPT-4o mini) and compositional reasoning in vision-language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶æ•æ‰è¾“å…¥ç‰¹å¾ä¹‹é—´å¤æ‚äº¤äº’çš„èƒ½åŠ›ï¼Œä¸ºæœºå™¨å­¦ä¹ å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ã€‚åƒSHAPè¿™æ ·çš„æµè¡Œäº‹åè§£é‡Šæ–¹æ³•æä¾›è¾¹é™…ç‰¹å¾å½’å±ï¼Œè€Œå®ƒä»¬å¯¹äº¤äº’é‡è¦æ€§çš„æ‰©å±•åªé€‚ç”¨äºè¾ƒçŸ­çš„è¾“å…¥é•¿åº¦ï¼ˆçº¦20ä¸ªï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†Spectral Explainerï¼ˆSPEXï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å‹æ— å…³çš„äº¤äº’å½’å±ç®—æ³•ï¼Œå¯æœ‰æ•ˆåœ°æ‰©å±•åˆ°è¾ƒå¤§çš„è¾“å…¥é•¿åº¦ï¼ˆçº¦1000ä¸ªï¼‰ã€‚SPEXåˆ©ç”¨ç°å®ä¸–ç•Œä¸­å¸¸è§çš„äº¤äº’ä¹‹é—´çš„è‡ªç„¶ç¨€ç–æ€§ï¼Œå¹¶åº”ç”¨ä¸€ç§ç¨€ç–å‚…é‡Œå¶å˜æ¢å’Œé€šé“è§£ç ç®—æ³•æ¥æœ‰æ•ˆåœ°è¯†åˆ«é‡è¦çš„äº¤äº’ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªéœ€è¦LLMåˆ©ç”¨è¾“å…¥é—´äº¤äº’æ¥å®Œæˆä»»åŠ¡çš„é•¿æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚å¯¹äºå¤§è¾“å…¥ï¼ŒSPEXåœ¨å¿ å®é‡å»ºLLMè¾“å‡ºæ–¹é¢ä¼˜äºè¾¹é™…å½’å±æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾20%ã€‚æ­¤å¤–ï¼ŒSPEXæˆåŠŸåœ°è¯†åˆ«äº†å¼ºçƒˆå½±å“æ¨¡å‹è¾“å‡ºçš„å…³é”®ç‰¹å¾å’Œäº¤äº’ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¹‹ä¸€HotpotQAä¸­ï¼ŒSPEXæä¾›çš„äº¤äº’ä¸äººç±»æ³¨é‡Šç›¸ç¬¦ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹æ— å…³çš„æ–¹æ³•ç”Ÿæˆè§£é‡Šï¼Œä»¥å±•ç¤ºå°é—­æºä»£ç LLMï¼ˆGPT-4o miniï¼‰ä¸­çš„æŠ½è±¡æ¨ç†å’Œè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„ç»„åˆæ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13870v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ•æ‰è¾“å…¥ç‰¹å¾ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œä»è€Œå¼•é¢†æœºå™¨å­¦ä¹ é©å‘½ã€‚é’ˆå¯¹ç°æœ‰è§£é‡Šæ–¹æ³•åœ¨å¤„ç†é•¿è¾“å…¥æ—¶çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†Spectral Explainerï¼ˆSPEXï¼‰æ–¹æ³•ã€‚SPEXæ˜¯ä¸€ç§æ¨¡å‹é€šç”¨çš„äº¤äº’å½’å› ç®—æ³•ï¼Œå¯æœ‰æ•ˆåœ°æ‰©å±•åˆ°é•¿è¾“å…¥é•¿åº¦ï¼ˆçº¦1000ï¼‰ã€‚å®ƒé€šè¿‡åˆ©ç”¨ç°å®ä¸–ç•Œä¸­å¸¸è§çš„äº¤äº’è‡ªç„¶ç¨€ç–æ€§ï¼Œé‡‡ç”¨ç¨€ç–å‚…é‡Œå¶å˜æ¢å’Œé€šé“è§£ç ç®—æ³•æ¥é«˜æ•ˆè¯†åˆ«é‡è¦çš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼ŒSPEXåœ¨ä¸‰ä¸ªéœ€è¦åˆ©ç”¨è¾“å…¥é—´äº¤äº’æ¥å®Œæˆä»»åŠ¡çš„é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºè¾¹é™…å½’å› æ–¹æ³•ï¼Œå¿ å®åœ°é‡æ„äº†LLMè¾“å‡ºã€‚æ­¤å¤–ï¼ŒSPEXè¿˜èƒ½æˆåŠŸè¯†åˆ«å½±å“æ¨¡å‹è¾“å‡ºçš„å…³é”®ç‰¹å¾å’Œäº¤äº’ã€‚å¯¹äºHotpotQAæ•°æ®é›†ï¼ŒSPEXæä¾›çš„äº¤äº’ä¸äººç±»æ³¨é‡Šç›¸ç¬¦ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨æ¨¡å‹é€šç”¨çš„æ–¹æ³•ä¸ºå°é—­æºLLMï¼ˆGPT-4o miniï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„ç»„åˆæ¨ç†æä¾›äº†è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsèƒ½å¤Ÿæ•æ‰è¾“å…¥ç‰¹å¾é—´çš„å¤æ‚äº¤äº’ï¼Œæ¨åŠ¨æœºå™¨å­¦ä¹ è¿›æ­¥ã€‚</li>
<li>ç°æœ‰è§£é‡Šæ–¹æ³•åœ¨å¤„ç†é•¿è¾“å…¥æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Spectral Explainer (SPEX)æ˜¯ä¸€ç§æ¨¡å‹é€šç”¨çš„äº¤äº’å½’å› ç®—æ³•ï¼Œå¯æ‰©å±•åˆ°é•¿è¾“å…¥é•¿åº¦ã€‚</li>
<li>SPEXåˆ©ç”¨äº¤äº’çš„è‡ªç„¶ç¨€ç–æ€§ï¼Œé‡‡ç”¨ç¨€ç–å‚…é‡Œå¶å˜æ¢å’Œé€šé“è§£ç ç®—æ³•ã€‚</li>
<li>SPEXåœ¨ä¸‰ä¸ªé•¿ä¸Šä¸‹æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºè¾¹é™…å½’å› æ–¹æ³•ã€‚</li>
<li>SPEXèƒ½è¯†åˆ«å½±å“æ¨¡å‹è¾“å‡ºçš„å…³é”®ç‰¹å¾å’Œäº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7be0f95d3696bdaba06a349219ebf04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36bae4214017aaf07f9f8badc47ab0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-229a1efe32ef911e1f5174f91df2e949.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-375ceb9efb280cea35cb3c882425440f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1e6d01e490512d7a361285f31c7f7fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-317e52a85ec1be84f0cc294c1561088c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MagicGeo-Training-Free-Text-Guided-Geometric-Diagram-Generation"><a href="#MagicGeo-Training-Free-Text-Guided-Geometric-Diagram-Generation" class="headerlink" title="MagicGeo: Training-Free Text-Guided Geometric Diagram Generation"></a>MagicGeo: Training-Free Text-Guided Geometric Diagram Generation</h2><p><strong>Authors:Junxiao Wang, Ting Zhang, Heng Yu, Jingdong Wang, Hua Huang</strong></p>
<p>Geometric diagrams are critical in conveying mathematical and scientific concepts, yet traditional diagram generation methods are often manual and resource-intensive. While text-to-image generation has made strides in photorealistic imagery, creating accurate geometric diagrams remains a challenge due to the need for precise spatial relationships and the scarcity of geometry-specific datasets. This paper presents MagicGeo, a training-free framework for generating geometric diagrams from textual descriptions. MagicGeo formulates the diagram generation process as a coordinate optimization problem, ensuring geometric correctness through a formal language solver, and then employs coordinate-aware generation. The framework leverages the strong language translation capability of large language models, while formal mathematical solving ensures geometric correctness. We further introduce MagicGeoBench, a benchmark dataset of 220 geometric diagram descriptions, and demonstrate that MagicGeo outperforms current methods in both qualitative and quantitative evaluations. This work provides a scalable, accurate solution for automated diagram generation, with significant implications for educational and academic applications. </p>
<blockquote>
<p>å‡ ä½•å›¾è¡¨åœ¨ä¼ è¾¾æ•°å­¦å’Œç§‘å­¦æ¦‚å¿µæ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼Œç„¶è€Œä¼ ç»Ÿçš„å›¾è¡¨ç”Ÿæˆæ–¹æ³•å¾€å¾€æ˜¯æ‰‹åŠ¨ä¸”èµ„æºå¯†é›†å‹çš„ã€‚å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆåœ¨ç…§ç‰‡çº§å›¾åƒæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºéœ€è¦ç²¾ç¡®çš„ç©ºé—´å…³ç³»å’Œå‡ ä½•ç‰¹å®šæ•°æ®é›†çš„ç¨€ç¼ºï¼Œåˆ›å»ºå‡†ç¡®çš„å‡ ä½•å›¾è¡¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— è®­ç»ƒæ¡†æ¶MagicGeoï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå‡ ä½•å›¾è¡¨ã€‚MagicGeoå°†å›¾è¡¨ç”Ÿæˆè¿‡ç¨‹è¡¨è¿°ä¸ºåæ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡å½¢å¼åŒ–è¯­è¨€æ±‚è§£å™¨ç¡®ä¿å‡ ä½•æ­£ç¡®æ€§ï¼Œç„¶åé‡‡ç”¨åæ ‡æ„ŸçŸ¥ç”Ÿæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¯­è¨€ç¿»è¯‘èƒ½åŠ›ï¼Œè€Œå½¢å¼åŒ–æ•°å­¦æ±‚è§£åˆ™ç¡®ä¿äº†å‡ ä½•æ­£ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å¼•å…¥äº†MagicGeoBenchæ•°æ®é›†ï¼ŒåŒ…å«220ä¸ªå‡ ä½•å›¾è¡¨æè¿°ï¼Œå¹¶è¯æ˜MagicGeoåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºå½“å‰æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–å›¾è¡¨ç”Ÿæˆæä¾›äº†å¯æ‰©å±•ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æ•™è‚²åº”ç”¨å’Œå­¦æœ¯åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13855v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å‡ ä½•å›¾å½¢åœ¨ä¼ è¾¾æ•°å­¦å’Œç§‘å­¦æ¦‚å¿µæ–¹é¢èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†ä¼ ç»Ÿçš„å›¾å½¢ç”Ÿæˆæ–¹æ³•å¾€å¾€æ˜¯æ‰‹åŠ¨ä¸”èµ„æºå¯†é›†å‹çš„ã€‚æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯åœ¨ç…§ç‰‡çœŸå®æ„Ÿå›¾åƒæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åˆ›å»ºç²¾ç¡®çš„å‡ ä½•å›¾å½¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºéœ€è¦ç²¾ç¡®çš„ç©ºé—´å…³ç³»ä»¥åŠå‡ ä½•æ•°æ®é›†çš„ç¨€ç¼ºæ€§ã€‚æœ¬æ–‡æå‡ºäº†MagicGeoæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å‡ ä½•å›¾å½¢ç”Ÿæˆæ–¹æ³•ï¼Œå¯ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆå‡ ä½•å›¾å½¢ã€‚MagicGeoå°†å›¾å½¢ç”Ÿæˆè¿‡ç¨‹å…¬å¼åŒ–ä¸ºåæ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡å½¢å¼è¯­è¨€æ±‚è§£å™¨ç¡®ä¿å‡ ä½•æ­£ç¡®æ€§ï¼Œç„¶åé‡‡ç”¨åæ ‡æ„ŸçŸ¥ç”Ÿæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è¯­è¨€ç¿»è¯‘èƒ½åŠ›ï¼ŒåŒæ—¶å½¢å¼æ•°å­¦æ±‚è§£ç¡®ä¿å‡ ä½•æ­£ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†MagicGeoBenchæ•°æ®é›†ï¼ŒåŒ…å«220ä¸ªå‡ ä½•å›¾å½¢æè¿°ï¼Œå¹¶è¯æ˜MagicGeoåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡ä¼˜äºå½“å‰æ–¹æ³•ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–å›¾å½¢ç”Ÿæˆæä¾›äº†å¯æ‰©å±•ä¸”å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹æ•™è‚²å’Œå­¦æœ¯åº”ç”¨å…·æœ‰é‡å¤§å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡ ä½•å›¾å½¢åœ¨ä¼ è¾¾æ•°å­¦å’Œç§‘å­¦æ¦‚å¿µæ—¶è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•ç”Ÿæˆå›¾å½¢çš„è¿‡ç¨‹é€šå¸¸æ˜¯æ‰‹åŠ¨ä¸”è€—èµ„æºçš„ã€‚</li>
<li>æ–‡æœ¬åˆ°å›¾åƒç”ŸæˆæŠ€æœ¯åœ¨ç…§ç‰‡çœŸå®æ„Ÿå›¾åƒæ–¹é¢æœ‰æ‰€è¿›å±•ï¼Œä½†ç”Ÿæˆç²¾ç¡®å‡ ä½•å›¾å½¢ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>MagicGeoæ¡†æ¶èƒ½ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆå‡ ä½•å›¾å½¢ï¼Œä¸”æ— éœ€è®­ç»ƒã€‚</li>
<li>MagicGeoå°†å›¾å½¢ç”Ÿæˆè¿‡ç¨‹çœ‹ä½œåæ ‡ä¼˜åŒ–é—®é¢˜ï¼Œç¡®ä¿å‡ ä½•æ­£ç¡®æ€§ã€‚</li>
<li>MagicGeoåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ï¼Œå¹¶ç»“åˆå½¢å¼æ•°å­¦æ±‚è§£ç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥äº†MagicGeoBenchæ•°æ®é›†ï¼ŒåŒ…å«220ä¸ªå‡ ä½•å›¾å½¢æè¿°ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b82140d35d1a07052321dc7ba269e815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9adc3f38d5aff9e7d83d156857f5c528.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7464694961def7d818914a3420f392.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9744213a9b555adfb8e41001b354225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf265197fd1fcd0a6fc2cb2cc60a10a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Based-Recommendations-Through-Personalized-Reasoning"><a href="#Enhancing-LLM-Based-Recommendations-Through-Personalized-Reasoning" class="headerlink" title="Enhancing LLM-Based Recommendations Through Personalized Reasoning"></a>Enhancing LLM-Based Recommendations Through Personalized Reasoning</h2><p><strong>Authors:Jiahao Liu, Xueshuo Yan, Dongsheng Li, Guangping Zhang, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu</strong></p>
<p>Current recommendation systems powered by large language models (LLMs) often underutilize their reasoning capabilities due to a lack of explicit logical structuring. To address this limitation, we introduce CoT-Rec, a framework that integrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by incorporating two crucial processes: user preference analysis and item perception evaluation. CoT-Rec operates in two key phases: (1) personalized data extraction, where user preferences and item perceptions are identified, and (2) personalized data application, where this information is leveraged to refine recommendations. Our experimental analysis demonstrates that CoT-Rec improves recommendation accuracy by making better use of LLMsâ€™ reasoning potential. The implementation is publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CoT-Rec">https://anonymous.4open.science/r/CoT-Rec</a>. </p>
<blockquote>
<p>å½“å‰ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å»ºè®®ç³»ç»Ÿç”±äºç¼ºå°‘æ˜ç¡®çš„é€»è¾‘ç»“æ„è€Œå¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoT-Recæ¡†æ¶ï¼Œå®ƒé€šè¿‡èå…¥ä¸¤ä¸ªå…³é”®è¿‡ç¨‹â€”â€”ç”¨æˆ·åå¥½åˆ†æå’Œé¡¹ç›®æ„ŸçŸ¥è¯„ä¼°ï¼Œå°†æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†èå…¥LLMé©±åŠ¨çš„å»ºè®®ä¸­ã€‚CoT-Recæœ‰ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼š1ï¼‰ä¸ªæ€§åŒ–æ•°æ®æå–ï¼Œè¯†åˆ«ç”¨æˆ·åå¥½å’Œé¡¹ç›®æ„ŸçŸ¥ï¼›2) ä¸ªæ€§åŒ–æ•°æ®åº”ç”¨ï¼Œåˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥ä¼˜åŒ–å»ºè®®ã€‚æˆ‘ä»¬çš„å®éªŒåˆ†æè¡¨æ˜ï¼ŒCoT-Recé€šè¿‡æ›´å¥½åœ°åˆ©ç”¨LLMçš„æ¨ç†æ½œåŠ›ï¼Œæé«˜äº†æ¨èå‡†ç¡®æ€§ã€‚å®ç°ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CoT-Rec%E3%80%82">https://anonymous.4open.science/r/CoT-Recã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13845v1">PDF</a> 7 pages, under review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨èç³»ç»Ÿå¸¸å¸¸ç”±äºç¼ºå°‘æ˜ç¡®çš„é€»è¾‘ç»“æ„è€Œæ— æ³•å……åˆ†åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºCoT-Recæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†ï¼Œç»“åˆç”¨æˆ·åå¥½åˆ†æå’Œç‰©å“æ„ŸçŸ¥è¯„ä¼°ä¸¤ä¸ªå…³é”®è¿‡ç¨‹ï¼Œæå‡LLMé©±åŠ¨çš„æ¨èç³»ç»Ÿæ€§èƒ½ã€‚CoT-RecåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šä¸ªæ€§åŒ–æ•°æ®æå–å’Œä¸ªæ€§åŒ–æ•°æ®åº”ç”¨ã€‚å®éªŒåˆ†ææ˜¾ç¤ºï¼ŒCoT-Recèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨LLMçš„æ¨ç†æ½œåŠ›ï¼Œæé«˜æ¨èå‡†ç¡®æ€§ã€‚å…¶å®ç°å¯å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ¨èç³»ç»Ÿå­˜åœ¨æ¨ç†èƒ½åŠ›åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>CoT-Recæ¡†æ¶é€šè¿‡å¼•å…¥Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>CoT-Recç»“åˆç”¨æˆ·åå¥½åˆ†æå’Œç‰©å“æ„ŸçŸ¥è¯„ä¼°ä¸¤ä¸ªå…³é”®è¿‡ç¨‹ã€‚</li>
<li>CoT-RecåŒ…æ‹¬ä¸ªæ€§åŒ–æ•°æ®æå–å’Œä¸ªæ€§åŒ–æ•°æ®åº”ç”¨ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚</li>
<li>CoT-Recèƒ½æé«˜æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>CoT-Recçš„å®ç°å¯å…¬å¼€è®¿é—®ã€‚</li>
<li>CoT-Recä¸ºLLMåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-872c1b6a1ed8ccb08bf520de2d0bc6f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-525fdefff7904fd94d1cda467e9bb16c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e339657951c0c15b1df79f6be4a057cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48cdc45a9c2148d89ecc4e5ab269a24d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67713f8770af29cf888c5c68c6424d77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a20f514314153fc0352fbc7a3c4e7d4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Cross-Domain-Recommendations-with-Memory-Optimized-LLM-Based-User-Agents"><a href="#Enhancing-Cross-Domain-Recommendations-with-Memory-Optimized-LLM-Based-User-Agents" class="headerlink" title="Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based   User Agents"></a>Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based   User Agents</h2><p><strong>Authors:Jiahao Liu, Shengkang Gu, Dongsheng Li, Guangping Zhang, Mingzhe Han, Hansu Gu, Peng Zhang, Tun Lu, Li Shang, Ning Gu</strong></p>
<p>Large Language Model (LLM)-based user agents have emerged as a powerful tool for improving recommender systems by simulating user interactions. However, existing methods struggle with cross-domain scenarios due to inefficient memory structures, leading to irrelevant information retention and failure to account for social influence factors such as popularity. To address these limitations, we introduce AgentCF++, a novel framework featuring a dual-layer memory architecture and a two-step fusion mechanism to filter domain-specific preferences effectively. Additionally, we propose interest groups with shared memory, allowing the model to capture the impact of popularity trends on users with similar interests. Through extensive experiments on multiple cross-domain datasets, AgentCF++ demonstrates superior performance over baseline models, highlighting its effectiveness in refining user behavior simulation for recommender systems. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AgentCF-plus">https://anonymous.4open.science/r/AgentCF-plus</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”¨æˆ·ä»£ç†å·²æˆä¸ºæ¨èç³»ç»Ÿæ”¹è¿›çš„å¼ºå¤§å·¥å…·ï¼Œé€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’æ¥å®ç°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨åŸŸåœºæ™¯æ—¶å­˜åœ¨å›°éš¾ï¼Œç”±äºå†…å­˜ç»“æ„ä¸å¤Ÿé«˜æ•ˆï¼Œå¯¼è‡´ä¿ç•™ä¸ç›¸å…³ä¿¡æ¯ä»¥åŠæœªèƒ½è€ƒè™‘ç¤¾ä¼šå½±å“å› ç´ ï¼Œå¦‚äººæ°”ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AgentCF++ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå…·æœ‰åŒå±‚å†…å­˜æ¶æ„å’Œä¸¤æ­¥èåˆæœºåˆ¶ï¼Œå¯æœ‰æ•ˆè¿‡æ»¤ç‰¹å®šåŸŸåå¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰å…±äº«å†…å­˜çš„å…´è¶£å°ç»„ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰æµè¡Œè¶‹åŠ¿å¯¹å…·æœ‰ç›¸ä¼¼å…´è¶£ç”¨æˆ·çš„å½±å“ã€‚é€šè¿‡å¤šä¸ªè·¨åŸŸæ•°æ®é›†çš„å¤§é‡å®éªŒï¼ŒAgentCF++åœ¨åŸºå‡†æ¨¡å‹ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡¸æ˜¾å…¶åœ¨æ”¹è¿›æ¨èç³»ç»Ÿçš„ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AgentCF-plus%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/AgentCF-plusä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13843v1">PDF</a> 6 pages, under review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨æˆ·ä»£ç†é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’å¢å¼ºäº†æ¨èç³»ç»Ÿçš„æ•ˆèƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨åŸŸåœºæ™¯æ—¶å­˜åœ¨å†…å­˜ç»“æ„ä½æ•ˆçš„é—®é¢˜ï¼Œå¯¼è‡´ä¿ç•™ä¸ç›¸å…³ä¿¡æ¯å’Œå¿½ç•¥ç¤¾ä¼šå½±å“å› ç´ å¦‚æµè¡Œåº¦ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºAgentCF++æ¡†æ¶ï¼Œé‡‡ç”¨åŒå±‚å†…å­˜æ¶æ„å’Œä¸¤æ­¥èåˆæœºåˆ¶ï¼Œæœ‰æ•ˆè¿‡æ»¤ç‰¹å®šåŸŸåå¥½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºå…´è¶£å°ç»„å…±äº«å†…å­˜ï¼Œè®©æ¨¡å‹èƒ½æ•æ‰æµè¡Œè¶‹åŠ¿å¯¹å…·æœ‰ç›¸ä¼¼å…´è¶£ç”¨æˆ·çš„å½±å“ã€‚é€è¿‡å¤šæ¬¡è·¨åŸŸæ•°æ®é›†çš„å®éªŒï¼ŒAgentCF++è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œå±•ç°å…¶åœ¨æ¨èç³»ç»Ÿä¸­ä¼˜åŒ–ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿçš„æ•ˆèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMç”¨æˆ·ä»£ç†æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’æå‡äº†æ¨èç³»ç»Ÿçš„æ•ˆèƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨åŸŸåœºæ™¯æ—¶å­˜åœ¨å†…å­˜ç»“æ„ä½æ•ˆçš„é—®é¢˜ã€‚</li>
<li>AgentCF++é‡‡ç”¨åŒå±‚å†…å­˜æ¶æ„å’Œä¸¤æ­¥èåˆæœºåˆ¶è¿‡æ»¤ç‰¹å®šåŸŸåå¥½ã€‚</li>
<li>AgentCF++é€šè¿‡å…´è¶£å°ç»„å…±äº«å†…å­˜æ•æ‰æµè¡Œè¶‹åŠ¿å¯¹ç›¸ä¼¼å…´è¶£ç”¨æˆ·çš„å½±å“ã€‚</li>
<li>AgentCF++åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
<li>AgentCF++èƒ½æœ‰æ•ˆä¼˜åŒ–æ¨èç³»ç»Ÿä¸­çš„ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0af8176f3a377b86aa9ddeb7566817bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad764e55d31309e372056a1f0acb4281.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5ded8c2d9418d932293afab4dbd02a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-424479d98b0af78a31cec366a657630d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-660c36b05d2acbbc834330d18d0935e3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking"><a href="#Inner-Thinking-Transformer-Leveraging-Dynamic-Depth-Scaling-to-Foster-Adaptive-Internal-Thinking" class="headerlink" title="Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking"></a>Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster   Adaptive Internal Thinking</h2><p><strong>Authors:Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</strong></p>
<p>Large language models (LLMs) face inherent performance bottlenecks under parameter constraints, particularly in processing critical tokens that demand complex reasoning. Empirical analysis reveals challenging tokens induce abrupt gradient spikes across layers, exposing architectural stress points in standard Transformers. Building on this insight, we propose Inner Thinking Transformer (ITT), which reimagines layer computations as implicit thinking steps. ITT dynamically allocates computation through Adaptive Token Routing, iteratively refines representations via Residual Thinking Connections, and distinguishes reasoning phases using Thinking Step Encoding. ITT enables deeper processing of critical tokens without parameter expansion. Evaluations across 162M-466M parameter models show ITT achieves 96.5% performance of a 466M Transformer using only 162M parameters, reduces training data by 43.2%, and outperforms Transformer&#x2F;Loop variants in 11 benchmarks. By enabling elastic computation allocation during inference, ITT balances performance and efficiency through architecture-aware optimization of implicit thinking pathways. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å‚æ•°çº¦æŸä¸‹å­˜åœ¨å›ºæœ‰çš„æ€§èƒ½ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„å…³é”®ä»¤ç‰Œæ—¶ã€‚ç»éªŒåˆ†æè¡¨æ˜ï¼ŒæŒ‘æˆ˜ä»¤ç‰Œä¼šåœ¨å„å±‚ä¹‹é—´å¼•å‘çªç„¶çš„æ¢¯åº¦è·ƒå‡ï¼Œæš´éœ²äº†æ ‡å‡†Transformerä¸­çš„æ¶æ„åº”åŠ›ç‚¹ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†å†…æ€è€ƒTransformerï¼ˆITTï¼‰ï¼Œå®ƒé‡æ–°æ„æƒ³å±‚è®¡ç®—ä¸ºéšå¼æ€è€ƒæ­¥éª¤ã€‚ITTé€šè¿‡è‡ªé€‚åº”ä»¤ç‰Œè·¯ç”±åŠ¨æ€åˆ†é…è®¡ç®—ï¼Œé€šè¿‡å‰©ä½™æ€è€ƒè¿æ¥è¿­ä»£ä¼˜åŒ–è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨æ€è€ƒæ­¥éª¤ç¼–ç æ¥åŒºåˆ†æ¨ç†é˜¶æ®µã€‚ITTèƒ½å¤Ÿåœ¨ä¸æ‰©å¤§å‚æ•°çš„æƒ…å†µä¸‹å¯¹å…³é”®ä»¤ç‰Œè¿›è¡Œæ›´æ·±çš„å¤„ç†ã€‚åœ¨1.6äº¿è‡³è¿‘4äº¿å‚æ•°æ¨¡å‹çš„è¯„ä¼°ä¸­ï¼ŒITTä½¿ç”¨ä»…è¿‘ä¸€åŠçš„å‚æ•°ï¼ˆè¿‘4äº¿å‚æ•°æ¨¡å‹ä¸­çš„ä»…ä¸€åŠï¼‰ï¼Œå®ç°äº†å¯¹æ ‡å‡†æ¨¡å‹çš„è¿‘ä¹æˆåŠæ€§èƒ½ï¼ˆå³è¿‘ç™¾åˆ†ä¹‹ä¹åå…­ç‚¹äº”ï¼‰ï¼Œå‡å°‘äº†ç™¾åˆ†ä¹‹å››åä¸‰ç‚¹äºŒçš„è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨åäºŒä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºTransformeræˆ–å¾ªç¯å˜ä½“ã€‚é€šè¿‡å¯ç”¨æ¨ç†è¿‡ç¨‹ä¸­çš„å¼¹æ€§è®¡ç®—åˆ†é…ï¼ŒITTé€šè¿‡æ¶æ„ä¼˜åŒ–çš„éšæ€§æ€è€ƒè·¯å¾„åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13842v1">PDF</a> 15 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>å†…è•´æ€è€ƒè½¬æ¢å™¨ï¼ˆITTï¼‰è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å…³é”®ä»¤ç‰Œæ—¶çš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚é€šè¿‡é‡æ–°è®¾è®¡è½¬æ¢å™¨æ¶æ„ä¸­çš„å±‚è®¡ç®—ä¸ºéšæ€§æ€è€ƒæ­¥éª¤ï¼ŒITTèƒ½å¤ŸåŠ¨æ€åˆ†é…è®¡ç®—èµ„æºä»¥å¤„ç†å¤æ‚æ¨ç†éœ€æ±‚çš„ä»¤ç‰Œã€‚åœ¨å…³é”®ä»¤ç‰Œå¤„ç†ä¸Šï¼ŒITTä¸éœ€è¦é¢å¤–çš„å‚æ•°æ‰©å±•å³å¯å®ç°æ·±åº¦å¤„ç†ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒITTèƒ½å¤Ÿå®ç°ä»…ä½¿ç”¨è¾ƒå°‘å‚æ•°çš„é«˜æ•ˆæ€§èƒ½è¡¨ç°ï¼Œä¸”å¯é™ä½è®­ç»ƒæ•°æ®é‡ï¼ŒåŒæ—¶æ»¡è¶³å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•è¦æ±‚ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒITTå¯å¼¹æ€§åˆ†é…è®¡ç®—èµ„æºï¼Œå®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å…³é”®ä»¤ç‰Œæ—¶é¢ä¸´æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>å…³é”®ä»¤ç‰Œå¤„ç†éœ€è¦å¤æ‚æ¨ç†ï¼Œå¯¼è‡´æ¢¯åº¦æ³¢åŠ¨å’Œæ¶æ„å‹åŠ›ã€‚</li>
<li>å†…è•´æ€è€ƒè½¬æ¢å™¨ï¼ˆITTï¼‰é€šè¿‡éšæ€§æ€è€ƒæ­¥éª¤é‡æ–°è®¾è®¡è½¬æ¢å™¨æ¶æ„ä¸­çš„å±‚è®¡ç®—æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ITTå®ç°äº†åœ¨ä¸éœ€è¦é¢å¤–å‚æ•°æ‰©å±•çš„æƒ…å†µä¸‹å¯¹å…³é”®ä»¤ç‰Œçš„æ·±åº¦å¤„ç†ã€‚</li>
<li>ITTåœ¨å‡å°‘å‚æ•°ä½¿ç”¨å’Œè®­ç»ƒæ•°æ®é‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ITTèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºä»¥å®ç°æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e8b72493deee3458d27fe6e91ac1062d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d2c2cfdc1b6b372071f675d6641038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f19bf34fab4abb0135ebb0a4eaa167a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88edeff5ebe306337adc457af1ae6f13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af39aa68e012cd7cd9eb9633692f4275.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Quantifying-Memorization-and-Retriever-Performance-in-Retrieval-Augmented-Vision-Language-Models"><a href="#Quantifying-Memorization-and-Retriever-Performance-in-Retrieval-Augmented-Vision-Language-Models" class="headerlink" title="Quantifying Memorization and Retriever Performance in   Retrieval-Augmented Vision-Language Models"></a>Quantifying Memorization and Retriever Performance in   Retrieval-Augmented Vision-Language Models</h2><p><strong>Authors:Peter Carragher, Abhinand Jha, R Raghav, Kathleen M. Carley</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ï¼ˆQAï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨è¯„ä¼°å®ƒä»¬å¯¹è®°å¿†ä¸æ£€ç´¢çš„ä¾èµ–ç¨‹åº¦æ–¹é¢çš„åº¦é‡æ ‡å‡†ä»ç„¶ä¸å¤Ÿæˆç†Ÿã€‚è™½ç„¶é’ˆå¯¹å°é—­é¢†åŸŸçš„ä»»åŠ¡å¾®è°ƒæ¨¡å‹æ˜¯æœ€æ–°æŠ€æœ¯ï¼Œä½†åƒGPT-4oè¿™æ ·çš„é€šç”¨æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚è¿™å¼•å‘äº†å…³äºè®°å¿†ã€æ³›åŒ–å’Œæ£€ç´¢ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºå‹VLMsä¸åŸºå‡†VLMsåœ¨è®°å¿†è®­ç»ƒæ•°æ®æ–¹é¢çš„ç¨‹åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨WebQAåŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯”å¾®è°ƒæ¨¡å‹ä¸åŸºå‡†VLMåœ¨å¤šè·³æ£€ç´¢å’Œé—®ç­”æ–¹é¢çš„è¡¨ç°ï¼Œç ”ç©¶å¾®è°ƒå¯¹æ•°æ®è®°å¿†çš„å½±å“ã€‚ä¸ºäº†é‡åŒ–ç«¯åˆ°ç«¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿä¸­è®°å¿†çš„ç¨‹åº¦ï¼Œæˆ‘ä»¬é€šè¿‡ç ”ç©¶é—®ç­”æˆåŠŸè€Œæ£€ç´¢å¤±è´¥çš„æƒ…å†µï¼Œæå‡ºäº†å‡ ä¸ªä»£ç†åº¦é‡æ ‡å‡†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜å¾®è°ƒæ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè®°å¿†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ£€ç´¢å¢å¼ºå‹VLMçš„è®°å¿†å¾—åˆ†è¾ƒä½ï¼Œä½†ä»£ä»·æ˜¯å‡†ç¡®æ€§ä¸‹é™ï¼ˆåœ¨WebQAæµ‹è¯•é›†ä¸Šä¸º72%å¯¹52%ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„åº¦é‡æªæ–½ä¸ºæœªæ¥å·¥ä½œå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å¼€æ”¾åŸŸé—®ç­”å’Œè”åˆæ£€ç´¢-é—®ç­”ä»»åŠ¡ä¸­åè°ƒè®°å¿†å’Œæ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13836v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ï¼ˆQAï¼‰æ–¹é¢çš„å‡ºè‰²è¡¨ç°ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†è¯„ä¼°æ¨¡å‹å¯¹è®°å¿†ä¸æ£€ç´¢ä¾èµ–æ€§çš„åº¦é‡æ–¹æ³•ã€‚ç ”ç©¶å¯¹æ¯”äº†å¾®è°ƒæ¨¡å‹ä¸é€šç”¨æ¨¡å‹å¦‚GPT-4oåœ¨å°é—­é¢†åŸŸä»»åŠ¡ä¸é›¶æ ·æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæå‡ºäº†å…³äºè®°å¿†ã€æ³›åŒ–ä¸æ£€ç´¢ä¹‹é—´æƒè¡¡çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶åˆ†æäº†å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºå‹VLMæ¨¡å‹ä¸åŸºå‡†VLMæ¨¡å‹åœ¨è®°å¿†è®­ç»ƒæ•°æ®æ–¹é¢çš„å·®å¼‚ã€‚é€šè¿‡WebQAåŸºå‡†æµ‹è¯•ï¼Œæœ¬ç ”ç©¶å¯¹æ¯”äº†å¤šè·³æ£€ç´¢ä¸é—®ç­”ä¸­å¾®è°ƒæ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶è€ƒå¯Ÿäº†å¾®è°ƒå¯¹æ•°æ®è®°å¿†çš„å½±å“ã€‚ä¸ºé‡åŒ–ç«¯åˆ°ç«¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿä¸­è®°å¿†çš„å­˜å‚¨ç¨‹åº¦ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡æ¢ç©¶é—®ç­”æˆåŠŸä½†æ£€ç´¢å¤±è´¥çš„æƒ…å†µæ¥è¯„ä¼°ä»£ç†æŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¾®è°ƒæ¨¡å‹é«˜åº¦ä¾èµ–è®°å¿†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ£€ç´¢å¢å¼ºå‹VLMæ¨¡å‹çš„è®°å¿†å¾—åˆ†è¾ƒä½ï¼Œä½†ä»£ä»·æ˜¯å‡†ç¡®ç‡ä¸‹é™ï¼ˆWebQAæµ‹è¯•é›†ä¸Šä¸º72%å¯¹æ¯”52%ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„åº¦é‡æªæ–½ä¸ºæœªæ¥å·¥ä½œå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œéœ€è¦åœ¨å¼€æ”¾åŸŸé—®ç­”å’Œè”åˆæ£€ç´¢é—®ç­”ä»»åŠ¡ä¸­åè°ƒè®°å¿†ä¸æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨è¯„ä¼°æ¨¡å‹å¯¹è®°å¿†ä¸æ£€ç´¢ä¾èµ–æ€§çš„åº¦é‡æ–¹æ³•ä¸Šä»æœ‰å¾…å‘å±•ã€‚</li>
<li>å¯¹æ¯”åˆ†æå¾®è°ƒæ¨¡å‹ä¸é€šç”¨æ¨¡å‹åœ¨å°é—­é¢†åŸŸä»»åŠ¡ä¸é›¶æ ·æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¼•å‘å¯¹è®°å¿†ã€æ³›åŒ–ä¸æ£€ç´¢ä¹‹é—´æƒè¡¡çš„æ€è€ƒã€‚</li>
<li>å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºå‹VLMæ¨¡å‹ç›¸è¾ƒäºåŸºå‡†VLMæ¨¡å‹åœ¨è®°å¿†è®­ç»ƒæ•°æ®ä¸Šçš„å·®å¼‚è¢«åˆ†æã€‚</li>
<li>ä½¿ç”¨WebQAåŸºå‡†æµ‹è¯•è¯„ä¼°äº†å¤šè·³æ£€ç´¢ä¸é—®ç­”ä¸­å¾®è°ƒæ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†é€šè¿‡æ¢ç©¶é—®ç­”æˆåŠŸä½†æ£€ç´¢å¤±è´¥çš„æƒ…å†µæ¥é‡åŒ–æ¨¡å‹ä¸­è®°å¿†çš„å­˜å‚¨ç¨‹åº¦çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å‘ç°å¾®è°ƒæ¨¡å‹é«˜åº¦ä¾èµ–è®°å¿†æ¥å®Œæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20c1b64e67bff94771159172eda7abd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3384cd6cf4055aac6b223a0c685c296e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a89ac98f787d50bf4361396970f196ae.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Proving-Olympiad-Inequalities-by-Synergizing-LLMs-and-Symbolic-Reasoning"><a href="#Proving-Olympiad-Inequalities-by-Synergizing-LLMs-and-Symbolic-Reasoning" class="headerlink" title="Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning"></a>Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning</h2><p><strong>Authors:Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma</strong></p>
<p>Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~1). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡åœ¨è¯æ˜ç³»ç»Ÿå†…ç”Ÿæˆè¯æ˜æ­¥éª¤ï¼ˆå³ç­–ç•¥ï¼‰æ¥å½¢å¼åŒ–åœ°è¯æ˜æ•°å­¦å®šç†ã€‚ç„¶è€Œï¼Œå¯èƒ½çš„ç­–ç•¥ç©ºé—´æ˜¯åºå¤§è€Œå¤æ‚çš„ï¼Œè€Œå¯ç”¨äºå½¢å¼åŒ–è¯æ˜çš„è®­ç»ƒæ•°æ®æ˜¯æœ‰é™çš„ï¼Œè¿™ç»™åŸºäºLLMçš„ç­–ç•¥ç”Ÿæˆå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¥ç»ç¬¦å·ç­–ç•¥ç”Ÿæˆå™¨ï¼Œå®ƒååŒLLMä¹ å¾—çš„æ•°å­¦ç›´è§‰å’Œç¬¦å·æ–¹æ³•ç¼–ç çš„é¢†åŸŸç‰¹å®šè§è§£ã€‚è¿™ç§é›†æˆçš„å…³é”®æ–¹é¢åœ¨äºç¡®å®šå“ªäº›éƒ¨åˆ†çš„æ•°å­¦æ¨ç†æœ€é€‚åˆLLMï¼Œå“ªäº›é€‚åˆç¬¦å·æ–¹æ³•ã€‚è™½ç„¶ç¥ç»ç¬¦å·é›†æˆçš„é«˜çº§ç†å¿µå¹¿æ³›é€‚ç”¨äºå„ç§æ•°å­¦é—®é¢˜ï¼Œä½†åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“é—¨é’ˆå¯¹å¥¥æ—åŒ¹å…‹ä¸ç­‰å¼ï¼ˆå›¾1ï¼‰è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬åˆ†æäº†äººç±»å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶å°†æŠ€æœ¯æç‚¼ä¸ºä¸¤ç§ç±»å‹çš„ç­–ç•¥ï¼š1ï¼‰ç¼©æ”¾ï¼Œç”±ç¬¦å·æ–¹æ³•å¤„ç†ï¼›2)é‡å†™ï¼Œç”±LLMå¤„ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç¬¦å·å·¥å…·ä¸LLMç›¸ç»“åˆï¼Œä»¥ä¿®å‰ªå’Œæ’åˆ—è¯æ˜ç›®æ ‡ï¼Œå®ç°é«˜æ•ˆçš„è¯æ˜æœç´¢ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦ç«èµ›çš„161ä¸ªå…·æœ‰æŒ‘æˆ˜çš„ä¸ç­‰å¼ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMå’Œç¬¦å·æ–¹æ³•ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13834v1">PDF</a> Published as a conference paper at ICLR 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/Lizn-zn/NeqLIPS/">https://github.com/Lizn-zn/NeqLIPS/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡åœ¨è¯æ˜ç³»ç»Ÿå†…ç”Ÿæˆè¯æ˜æ­¥éª¤ï¼ˆå³ç­–ç•¥ï¼‰æ¥è¯æ˜æ•°å­¦å®šç†ã€‚ç„¶è€Œï¼Œå¯èƒ½çš„ç­–ç•¥ç©ºé—´åºå¤§ä¸”å¤æ‚ï¼Œè€Œå¯ç”¨äºæ­£å¼è¯æ˜çš„è®­ç»ƒæ•°æ®æœ‰é™ï¼Œç»™LLMçš„ç­–ç•¥ç”Ÿæˆå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·ç­–ç•¥ç”Ÿæˆå™¨ï¼Œå®ƒå°†LLMå­¦ä¹ çš„æ•°å­¦ç›´è§‰ä¸ç¬¦å·æ–¹æ³•ç¼–ç çš„é¢†åŸŸç‰¹å®šè§è§£ç›¸ç»“åˆã€‚æ­¤é›†æˆçš„å…³é”®æ–¹é¢åœ¨äºç¡®å®šå“ªäº›éƒ¨åˆ†çš„æ•°å­¦æ¨ç†æœ€é€‚åˆLLMï¼Œå“ªäº›é€‚åˆç¬¦å·æ–¹æ³•ã€‚è™½ç„¶ç¥ç»ç¬¦å·é›†æˆçš„é«˜çº§ç†å¿µå¹¿æ³›é€‚ç”¨äºå„ç§æ•°å­¦é—®é¢˜ï¼Œä½†æœ¬æ–‡é‡ç‚¹å…³æ³¨å¥¥æ—åŒ¹å…‹ä¸ç­‰å¼ï¼ˆå›¾1ï¼‰ã€‚æˆ‘ä»¬åˆ†æäº†äººç±»å¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶å°†æŠ€æœ¯æç‚¼ä¸ºä¸¤ç§ç­–ç•¥ï¼šä¸€æ˜¯é€šè¿‡ç¬¦å·æ–¹æ³•å¤„ç†çš„ç¼©æ”¾ï¼ŒäºŒæ˜¯é€šè¿‡LLMå¤„ç†çš„é‡å†™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç¬¦å·å·¥å…·ä¸LLMç›¸ç»“åˆï¼Œå¯¹è¯æ˜ç›®æ ‡è¿›è¡Œä¿®å‰ªå’Œæ’åï¼Œä»¥è¿›è¡Œæœ‰æ•ˆè¯æ˜æœç´¢ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°å­¦ç«èµ›çš„161ä¸ªå…·æœ‰æŒ‘æˆ˜çš„ä¸ç­‰å¼ä¸­è¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMå’Œç¬¦å·æ–¹æ³•ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMèƒ½å¤Ÿé€šè¿‡ç”Ÿæˆè¯æ˜æ­¥éª¤ï¼ˆå³ç­–ç•¥ï¼‰åœ¨æ•°å­¦è¯æ˜ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹ï¼Œç­–ç•¥ç”Ÿæˆé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºç­–ç•¥ç©ºé—´åºå¤§ä¸”å¤æ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç¥ç»ç¬¦å·ç­–ç•¥ç”Ÿæˆå™¨æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œç»“åˆäº†LLMçš„æ•°å­¦ç›´è§‰å’Œç¬¦å·æ–¹æ³•çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>ç¡®å®šå“ªäº›æ•°å­¦æ¨ç†éƒ¨åˆ†é€‚åˆLLMå’Œå“ªäº›é€‚åˆç¬¦å·æ–¹æ³•æ˜¯é›†æˆçš„å…³é”®ã€‚</li>
<li>é€šè¿‡åˆ†æäººç±»è§£å†³å¥¥æ—åŒ¹å…‹ä¸ç­‰å¼çš„æ–¹æ³•ï¼Œå°†ç­–ç•¥åˆ†ä¸ºç¼©æ”¾å’Œé‡å†™ä¸¤ç§ç±»å‹ã€‚</li>
<li>ç»“åˆç¬¦å·å·¥å…·å’ŒLLMå¯¹è¯æ˜ç›®æ ‡è¿›è¡Œä¿®å‰ªå’Œæ’åï¼Œæé«˜äº†è¯æ˜æœç´¢çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86533d54149e9647b84b4af9c25b14bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ed53c1b0e3c1cb02178a38bf59746c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c330f19eecde394cd251d2be74bbdd83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e28c392c79e9d2a66fe796ccd7d8601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49650499fce71635c9135517b5456be5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ArtMentor-AI-Assisted-Evaluation-of-Artworks-to-Explore-Multimodal-Large-Language-Models-Capabilities"><a href="#ArtMentor-AI-Assisted-Evaluation-of-Artworks-to-Explore-Multimodal-Large-Language-Models-Capabilities" class="headerlink" title="ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal   Large Language Models Capabilities"></a>ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal   Large Language Models Capabilities</h2><p><strong>Authors:Chanjin Zheng, Zengyi Yu, Yilin Jiang, Mingzi Zhang, Xunuo Lu, Jing Jin, Liteng Gao</strong></p>
<p>Can Multimodal Large Language Models (MLLMs), with capabilities in perception, recognition, understanding, and reasoning, function as independent assistants in art evaluation dialogues? Current MLLM evaluation methods, which rely on subjective human scoring or costly interviews, lack comprehensive coverage of various scenarios. This paper proposes a process-oriented Human-Computer Interaction (HCI) space design to facilitate more accurate MLLM assessment and development. This approach aids teachers in efficient art evaluation while also recording interactions for MLLM capability assessment. We introduce ArtMentor, a comprehensive space that integrates a dataset and three systems to optimize MLLM evaluation. The dataset consists of 380 sessions conducted by five art teachers across nine critical dimensions. The modular system includes agents for entity recognition, review generation, and suggestion generation, enabling iterative upgrades. Machine learning and natural language processing techniques ensure the reliability of evaluations. The results confirm GPT-4oâ€™s effectiveness in assisting teachers in art evaluation dialogues. Our contributions are available at <a target="_blank" rel="noopener" href="https://artmentor.github.io/">https://artmentor.github.io/</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·å¤‡æ„ŸçŸ¥ã€è¯†åˆ«ã€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œèƒ½å¦åœ¨è‰ºæœ¯è¯„ä»·å¯¹è¯ä¸­å……å½“ç‹¬ç«‹åŠ©æ‰‹ï¼Ÿç›®å‰çš„MLLMè¯„ä¼°æ–¹æ³•ä¾èµ–äºä¸»è§‚çš„äººä¸ºæ‰“åˆ†æˆ–æ˜‚è´µçš„é¢è¯•ï¼Œæ— æ³•å…¨é¢è¦†ç›–å„ç§åœºæ™¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘è¿‡ç¨‹çš„è®¡ç®—æœºäººæœºäº¤äº’ï¼ˆHCIï¼‰ç©ºé—´è®¾è®¡ï¼Œä»¥ä¿ƒè¿›æ›´å‡†ç¡®çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹è¯„ä¼°å’Œå‘å±•ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºæ•™å¸ˆé«˜æ•ˆåœ°è¿›è¡Œè‰ºæœ¯è¯„ä»·ï¼ŒåŒæ—¶è®°å½•äº¤äº’æƒ…å†µä»¥è¯„ä¼°MLLMçš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†ArtMentorï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆç©ºé—´ï¼Œé›†æˆäº†æ•°æ®é›†å’Œä¸‰ä¸ªç³»ç»Ÿä»¥ä¼˜åŒ–MLLMè¯„ä¼°ã€‚æ•°æ®é›†åŒ…å«äº”ä½è‰ºæœ¯æ•™å¸ˆè¿›è¡Œçš„380ä¸ªä¼šè¯ï¼Œæ¶µç›–ä¹ä¸ªå…³é”®ç»´åº¦ã€‚æ¨¡å—åŒ–ç³»ç»ŸåŒ…æ‹¬å®ä½“è¯†åˆ«ä»£ç†ã€è¯„è®ºç”Ÿæˆä»£ç†å’Œå»ºè®®ç”Ÿæˆä»£ç†ï¼Œå¯å®ç°è¿­ä»£å‡çº§ã€‚æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç¡®ä¿äº†è¯„ä»·çš„å¯é æ€§ã€‚ç»“æœè¯å®äº†GPT-4oåœ¨è‰ºæœ¯è¯„ä»·å¯¹è¯ä¸­è¾…åŠ©æ•™å¸ˆçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æˆæœå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://artmentor.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://artmentor.github.io/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13832v1">PDF</a> 18 pages, 12 figures. Accepted by CHI 2025</p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨æ„ŸçŸ¥ã€è¯†åˆ«ã€ç†è§£å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿæˆä¸ºè‰ºæœ¯è¯„ä»·å¯¹è¯ä¸­çš„ç‹¬ç«‹åŠ©æ‰‹ã€‚å½“å‰çš„è¯„ä»·æ–¹æ³•ç¼ºä¹å¯¹å„ç§åœºæ™¯çš„å…¨é¢è¦†ç›–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘è¿‡ç¨‹çš„è®¡ç®—æœºä¸äººç±»äº¤äº’ç©ºé—´è®¾è®¡ï¼Œä»¥ä¿ƒè¿›æ›´å‡†ç¡®çš„MLLMè¯„ä¼°å’Œå‘å±•ã€‚æˆ‘ä»¬ä»‹ç»äº†ArtMentorï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†æ•°æ®é›†å’Œä¸‰é‡ç³»ç»Ÿçš„ç»¼åˆç©ºé—´ï¼Œç”¨äºä¼˜åŒ–MLLMè¯„ä»·ã€‚é€šè¿‡æœºå™¨å­¦ä¹ ä¸è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ç¡®ä¿è¯„ä»·çš„å¯é æ€§ã€‚æœ€ç»ˆç¡®è®¤GPT-4oå¯æœ‰æ•ˆè¾…åŠ©æ•™å¸ˆè¿›è¡Œè‰ºæœ¯è¯„ä»·å¯¹è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMå…·å¤‡å¤šç§èƒ½åŠ›ï¼Œå¯æˆä¸ºè‰ºæœ¯è¯„ä»·å¯¹è¯çš„ç‹¬ç«‹åŠ©æ‰‹ã€‚</li>
<li>å½“å‰MLLMè¯„ä»·æ–¹æ³•çš„å±€é™æ€§ï¼Œéœ€è¦æ›´å…¨é¢çš„åœºæ™¯è¦†ç›–ã€‚</li>
<li>é¢å‘è¿‡ç¨‹çš„è®¡ç®—æœºä¸äººç±»äº¤äº’ç©ºé—´è®¾è®¡ç”¨äºæ›´å‡†ç¡®è¯„ä¼°å’Œå‘å±•MLLMã€‚</li>
<li>ArtMentoræ˜¯ä¸€ä¸ªç»¼åˆç©ºé—´ï¼Œé›†æˆäº†æ•°æ®é›†å’Œå¤šé‡ç³»ç»Ÿä»¥ä¼˜åŒ–MLLMè¯„ä»·ã€‚</li>
<li>ArtMentoræ•°æ®é›†åŒ…å«380ä¸ªç”±äº”ä½è‰ºæœ¯æ•™å¸ˆè¿›è¡Œçš„ä¼šè¯ï¼Œæ¶‰åŠä¹ä¸ªå…³é”®ç»´åº¦ã€‚</li>
<li>ArtMentorçš„æ¨¡å—åŒ–ç³»ç»ŸåŒ…æ‹¬å®ä½“è¯†åˆ«ã€è¯„è®ºç”Ÿæˆå’Œå»ºè®®ç”Ÿæˆä»£ç†ï¼Œå¯å®ç°è¿­ä»£å‡çº§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69d41b3b9b216d2321c3f64fdd79e155.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c01e1ac5cbc74efcc4bbdfd86091b9c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="One-Size-doesnâ€™t-Fit-All-A-Personalized-Conversational-Tutoring-Agent-for-Mathematics-Instruction"><a href="#One-Size-doesnâ€™t-Fit-All-A-Personalized-Conversational-Tutoring-Agent-for-Mathematics-Instruction" class="headerlink" title="One Size doesnâ€™t Fit All: A Personalized Conversational Tutoring Agent   for Mathematics Instruction"></a>One Size doesnâ€™t Fit All: A Personalized Conversational Tutoring Agent   for Mathematics Instruction</h2><p><strong>Authors:Ben Liu, Jihan Zhang, Fangquan Lin, Xu Jia, Min Peng</strong></p>
<p>Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \textbf{P}erson\textbf{A}lized \textbf{C}onversational tutoring ag\textbf{E}nt (PACE) for mathematics instruction. PACE simulates studentsâ€™ learning styles based on the Felder and Silverman learning style model, aligning with each studentâ€™s persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance studentsâ€™ comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§æ™ºèƒ½æ•™è‚²ç³»ç»Ÿä¸­å¾—åˆ°äº†è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ï¼Œæ¨¡æ‹Ÿäººç±»å¯¼å¸ˆï¼Œä¿ƒè¿›æœ‰æ•ˆçš„äººæœºäº¤äº’ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶å¾€å¾€å¿½è§†äº†è¯†åˆ«å’Œé€‚åº”ä¸ªåˆ«å­¦ä¹ è€…ç‰¹æ€§çš„é‡è¦æ€§ã€‚è¿™ç§é€‚åº”å¯¹äºæé«˜å­¦ç”Ÿå‚ä¸åº¦å’Œå­¦ä¹ æ•ˆç‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ•™å­¦æ–¹é¢ï¼Œä¸åŒçš„å­¦ä¹ é£æ ¼éœ€è¦ä¸ªæ€§åŒ–ç­–ç•¥æ¥ä¿ƒè¿›ç†è§£å’Œçƒ­æƒ…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ•°å­¦æ•™å­¦çš„ä¸ªæ€§åŒ–ä¼šè¯è¾…å¯¼å®ä½“ï¼ˆPACEï¼‰ã€‚PACEåŸºäºFelderå’ŒSilvermançš„å­¦ä¹ é£æ ¼æ¨¡å‹æ¨¡æ‹Ÿå­¦ç”Ÿçš„å­¦ä¹ é£æ ¼ï¼Œä¸æ¯ä¸ªå­¦ç”Ÿçš„ä¸ªæ€§ç›¸åŒ¹é…ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„PACEå¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°å­¦ç”Ÿçš„ä¸ªæ€§ï¼Œä»è€Œåˆ¶å®šä¸ä»–ä»¬ç‹¬ç‰¹å­¦ä¹ é£æ ¼ç›¸ç¬¦çš„ä¸ªæ€§åŒ–æ•™å­¦ç­–ç•¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å­¦ç”Ÿçš„ç†è§£åŠ›ï¼ŒPACEé‡‡ç”¨è‹æ ¼æ‹‰åº•æ•™å­¦æ–¹æ³•ï¼Œæä¾›å³æ—¶åé¦ˆï¼Œé¼“åŠ±æ·±å…¥æ€è€ƒã€‚é€šè¿‡æ„å»ºä¸ªæ€§åŒ–çš„æ•™å­¦æ•°æ®å’Œè®­ç»ƒæ¨¡å‹ï¼ŒPACEå±•ç¤ºäº†è¯†åˆ«å’Œé€‚åº”æ¯ä¸ªå­¦ç”Ÿç‹¬ç‰¹éœ€æ±‚çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ•´ä½“å­¦ä¹ ä½“éªŒå’Œæ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†å¤šæ–¹é¢çš„è¯„ä»·æ ‡å‡†ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æï¼Œä»¥è¯„ä¼°ä¸ªæ€§åŒ–æ•™å­¦çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ•™è‚²ä½“éªŒå’Œæ¿€å‘å­¦ç”ŸåŠ¨åŠ›æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12633v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•™è‚²ç³»ç»Ÿæ¨¡æ‹Ÿäººç±»å¯¼å¸ˆä»¥ä¿ƒè¿›äººæœºæœ‰æ•ˆäº¤äº’ã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ç ”ç©¶å¾€å¾€å¿½è§†äº†è¯†åˆ«å’Œé€‚åº”ä¸ªåˆ«å­¦ä¹ è€…ç‰¹æ€§çš„é‡è¦æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ•™å­¦æ–¹é¢ï¼Œå¤šæ ·åŒ–çš„å­¦ä¹ é£æ ¼éœ€è¦ä¸ªæ€§åŒ–ç­–ç•¥æ¥ä¿ƒè¿›ç†è§£å’Œçƒ­æƒ…ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–å¯¹è¯è¾…å¯¼å®ä½“ï¼ˆPACEï¼‰ç”¨äºæ•°å­¦æ•™å­¦ã€‚PACEåŸºäºFelderå’ŒSilvermançš„å­¦ä¹ é£æ ¼æ¨¡å‹æ¨¡æ‹Ÿå­¦ç”Ÿçš„å­¦ä¹ é£æ ¼ï¼Œä¸å­¦ç”Ÿçš„ä¸ªæ€§å¯¹é½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒPACEå¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°å­¦ç”Ÿçš„æ€§æ ¼ï¼Œåˆ¶å®šä¸ªæ€§åŒ–çš„æ•™å­¦ç­–ç•¥ï¼Œä¸å­¦ç”Ÿçš„ç‹¬ç‰¹å­¦ä¹ é£æ ¼äº§ç”Ÿå…±é¸£ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå­¦ç”Ÿçš„ç†è§£ï¼ŒPACEé‡‡ç”¨è‹æ ¼æ‹‰åº•æ•™å­¦æ³•æä¾›å³æ—¶åé¦ˆå’Œé¼“åŠ±æ·±å…¥æ€è€ƒã€‚é€šè¿‡æ„å»ºä¸ªæ€§åŒ–çš„æ•™å­¦æ•°æ®å’Œè®­ç»ƒæ¨¡å‹ï¼ŒPACEå±•ç°äº†è¯†åˆ«å’Œé€‚åº”æ¯ä¸ªå­¦ç”Ÿçš„ç‹¬ç‰¹éœ€æ±‚çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ•´ä½“å­¦ä¹ ä½“éªŒå’Œæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨æ™ºèƒ½æ•™è‚²ç³»ç»Ÿä¸­æ¨¡æ‹Ÿäººç±»å¯¼å¸ˆä»¥å¢å¼ºäººæœºäº¤äº’æ•ˆæœã€‚</li>
<li>å­¦ä¹ è€…ç‰¹æ€§åœ¨æå‡å­¦ä¹ ä½“éªŒå’Œæ•ˆç‡ä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ•™å­¦é¢†åŸŸã€‚</li>
<li>PACEèƒ½æ ¹æ®Felderå’ŒSilvermançš„å­¦ä¹ é£æ ¼æ¨¡å‹æ¨¡æ‹Ÿå­¦ç”Ÿä¸ªä½“çš„å­¦ä¹ é£æ ¼ã€‚</li>
<li>PACEèƒ½å¤Ÿè¯„ä¼°å­¦ç”Ÿçš„ä¸ªæ€§å¹¶æ ¹æ®å…¶å­¦ä¹ é£æ ¼å®šåˆ¶æ•™å­¦ç­–ç•¥ã€‚</li>
<li>PACEé‡‡ç”¨è‹æ ¼æ‹‰åº•æ•™å­¦æ³•ä»¥æä¾›å³æ—¶åé¦ˆå’Œé¼“åŠ±æ·±åº¦æ€è€ƒï¼Œå¢å¼ºå­¦ç”Ÿçš„å­¦ä¹ ç†è§£ã€‚</li>
<li>é€šè¿‡æ„å»ºä¸ªæ€§åŒ–çš„æ•™å­¦æ•°æ®å’Œè®­ç»ƒæ¨¡å‹ï¼ŒPACEå±•ç°äº†é€‚åº”æ¯ä¸ªå­¦ç”Ÿçš„ç‹¬ç‰¹éœ€æ±‚çš„èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23c35e65657f8aa4681797184b55a440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9678a600cab624f2cb06353c352dcccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a15f0a6d0d3ef74598abbdca956e59e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11b1cf6c9736c610fd35f7a197534a73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd1c8b5227a54b608de67aa39fd2d126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab906a856dccdbcef7ccd5d39c8913a2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reasoning-Augmented-Conversation-for-Multi-Turn-Jailbreak-Attacks-on-Large-Language-Models"><a href="#Reasoning-Augmented-Conversation-for-Multi-Turn-Jailbreak-Attacks-on-Large-Language-Models" class="headerlink" title="Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on   Large Language Models"></a>Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on   Large Language Models</h2><p><strong>Authors:Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao</strong></p>
<p>Multi-turn jailbreak attacks simulate real-world human interactions by engaging large language models (LLMs) in iterative dialogues, exposing critical safety vulnerabilities. However, existing methods often struggle to balance semantic coherence with attack effectiveness, resulting in either benign semantic drift or ineffective detection evasion. To address this challenge, we propose Reasoning-Augmented Conversation, a novel multi-turn jailbreak framework that reformulates harmful queries into benign reasoning tasks and leverages LLMsâ€™ strong reasoning capabilities to compromise safety alignment. Specifically, we introduce an attack state machine framework to systematically model problem translation and iterative reasoning, ensuring coherent query generation across multiple turns. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Extensive experiments on multiple LLMs demonstrate that RACE achieves state-of-the-art attack effectiveness in complex conversational scenarios, with attack success rates (ASRs) increasing by up to 96%. Notably, our approach achieves ASRs of 82% and 92% against leading commercial models, OpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at <a target="_blank" rel="noopener" href="https://github.com/NY1024/RACE">https://github.com/NY1024/RACE</a> to facilitate further research in this critical domain. </p>
<blockquote>
<p>å¤šè½®è¶Šç‹±æ”»å‡»é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¿­ä»£å¯¹è¯æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„äººç±»äº’åŠ¨ï¼Œä»è€Œæš´éœ²å…³é”®çš„å®‰å…¨æ¼æ´ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨è¯­ä¹‰è¿è´¯æ€§å’Œæ”»å‡»æ•ˆæœä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¯¼è‡´è‰¯æ€§è¯­ä¹‰æ¼‚ç§»æˆ–æ£€æµ‹é€ƒé¿æ— æ•ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå¢å¼ºæ¨ç†å¯¹è¯â€ï¼ˆReasoning-Augmented Conversationï¼‰è¿™ä¸€æ–°å‹å¤šè½®è¶Šç‹±æ¡†æ¶ï¼Œå®ƒå°†æœ‰å®³æŸ¥è¯¢é‡æ–°åˆ¶å®šä¸ºè‰¯æ€§æ¨ç†ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨LLMçš„å¼ºå¤§æ¨ç†èƒ½åŠ›æ¥æŸå®³å®‰å…¨å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ”»å‡»çŠ¶æ€æœºæ¡†æ¶ï¼Œç³»ç»Ÿåœ°æ¨¡æ‹Ÿé—®é¢˜ç¿»è¯‘å’Œè¿­ä»£æ¨ç†ï¼Œç¡®ä¿è·¨å¤šè½®çš„æŸ¥è¯¢ç”Ÿæˆè¿è´¯æ€§ã€‚åŸºäºè¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¢ç›Šå¼•å¯¼æ¢ç´¢ã€è‡ªæˆ‘åšå¼ˆå’Œæ‹’ç»åé¦ˆæ¨¡å—ï¼Œä»¥ä¿ç•™æ”»å‡»è¯­ä¹‰ã€å¢å¼ºæ•ˆæœå¹¶ç»´æŒæ¨ç†é©±åŠ¨çš„æ”»å‡»è¿›å±•ã€‚åœ¨å¤šä¸ªLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒRACEåœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ”»å‡»æ•ˆæœï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰æé«˜äº†é«˜è¾¾96%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é’ˆå¯¹é¢†å…ˆçš„å•†ä¸šæ¨¡å‹OpenAI o1å’ŒDeepSeek R1æ—¶ï¼ŒASRåˆ†åˆ«è¾¾åˆ°äº†82%å’Œ92%ï¼Œå‡¸æ˜¾äº†å…¶å¨åŠ›ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/NY1024/RACE%E5%8F%91%E5%B8%83%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%AF%A5%E5%85%B3%E9%94%AE%E9%A2%86%E5%9F%9F%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/NY1024/RACEå‘å¸ƒä»£ç ï¼Œä»¥ä¿ƒè¿›è¯¥å…³é”®é¢†åŸŸçš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11054v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šè½®è¶Šç‹±æ”»å‡»çš„æ–°æ¡†æ¶â€”â€”Reasoning-Augmented Conversationï¼ˆRACï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡æŠŠæœ‰å®³æŸ¥è¯¢é‡æ–°æ„å»ºä¸ºè‰¯æ€§æ¨ç†ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥å±å®³å®‰å…¨å¯¹é½ã€‚å®éªŒè¯æ˜ï¼ŒRACEåœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ”»å‡»æ•ˆæœï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰æé«˜é«˜è¾¾96%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè½®è¶Šç‹±æ”»å‡»æ¨¡æ‹ŸçœŸå®äººç±»äº’åŠ¨ï¼Œé€šè¿‡è¿­ä»£å¯¹è¯ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº¤äº’ï¼Œæš´éœ²å…³é”®å®‰å…¨æ¼æ´ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨è¯­ä¹‰è¿è´¯å’Œæ”»å‡»æ•ˆæœä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¯èƒ½å¯¼è‡´è‰¯æ€§è¯­ä¹‰æ¼‚ç§»æˆ–æ£€æµ‹é€ƒé¿å¤±æ•ˆã€‚</li>
<li>Reasoning-Augmented Conversationï¼ˆRACï¼‰æ¡†æ¶é€šè¿‡æŠŠæœ‰å®³æŸ¥è¯¢è½¬åŒ–ä¸ºè‰¯æ€§æ¨ç†ä»»åŠ¡ï¼Œåˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›æ¥å±åŠå®‰å…¨å¯¹é½ã€‚</li>
<li>RACæ¡†æ¶å¼•å…¥æ”»å‡»çŠ¶æ€æœºæ¡†æ¶ï¼Œç³»ç»Ÿåœ°å»ºæ¨¡é—®é¢˜ç¿»è¯‘å’Œè¿­ä»£æ¨ç†ï¼Œç¡®ä¿å¤šè½®æŸ¥è¯¢çš„è¿è´¯ç”Ÿæˆã€‚</li>
<li>RACè®¾è®¡åŒ…æ‹¬æ”¶ç›Šå¼•å¯¼æ¢ç´¢ã€è‡ªæˆ‘æ¸¸æˆå’Œæ‹’ç»åé¦ˆæ¨¡å—ï¼Œä»¥ä¿ç•™æ”»å‡»è¯­ä¹‰ã€å¢å¼ºæ•ˆæœå’Œç»´æŒæ¨ç†é©±åŠ¨çš„æ”»å‡»è¿›åº¦ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRACEåœ¨å¤æ‚å¯¹è¯åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ”»å‡»æ•ˆæœï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰æå‡é«˜è¾¾96%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3bc1b4e9f78646f011196a970bea3326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a630a1498c08e4b8d89679fcd898c095.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-491191897da63e0a5db4b42c030b5673.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Improving-Acoustic-Side-Channel-Attacks-on-Keyboards-Using-Transformers-and-Large-Language-Models"><a href="#Improving-Acoustic-Side-Channel-Attacks-on-Keyboards-Using-Transformers-and-Large-Language-Models" class="headerlink" title="Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers   and Large Language Models"></a>Improving Acoustic Side-Channel Attacks on Keyboards Using Transformers   and Large Language Models</h2><p><strong>Authors:Jin Hyun Park, Seyyed Ali Ayati, Yichen Cai</strong></p>
<p>The increasing prevalence of microphones in everyday devices and the growing reliance on online services have amplified the risk of acoustic side-channel attacks (ASCAs) targeting keyboards. This study explores deep learning techniques, specifically vision transformers (VTs) and large language models (LLMs), to enhance the effectiveness and applicability of such attacks. We present substantial improvements over prior research, with the CoAtNet model achieving state-of-the-art performance. Our CoAtNet shows a 5.0% improvement for keystrokes recorded via smartphone (Phone) and 5.9% for those recorded via Zoom compared to previous benchmarks. We also evaluate transformer architectures and language models, with the best VT model matching CoAtNetâ€™s performance. A key advancement is the introduction of a noise mitigation method for real-world scenarios. By using LLMs for contextual understanding, we detect and correct erroneous keystrokes in noisy environments, enhancing ASCA performance. Additionally, fine-tuned lightweight language models with Low-Rank Adaptation (LoRA) deliver comparable performance to heavyweight models with 67X more parameters. This integration of VTs and LLMs improves the practical applicability of ASCA mitigation, marking the first use of these technologies to address ASCAs and error correction in real-world scenarios. </p>
<blockquote>
<p>æ—¥å¸¸è®¾å¤‡ä¸­çš„éº¦å…‹é£è¶Šæ¥è¶Šæ™®åŠï¼Œå¯¹åœ¨çº¿æœåŠ¡çš„ä¾èµ–ç¨‹åº¦ä¹Ÿè¶Šæ¥è¶Šé«˜ï¼Œè¿™åŠ å¤§äº†é’ˆå¯¹é”®ç›˜çš„å£°ä¾§ä¿¡é“æ”»å‡»ï¼ˆASCAsï¼‰çš„é£é™©ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å˜å‹å™¨ï¼ˆVTsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥æé«˜æ­¤ç±»æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨å‰äººç ”ç©¶çš„åŸºç¡€ä¸Šå–å¾—äº†é‡å¤§æ”¹è¿›ï¼ŒCoAtNetæ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„CoAtNetåœ¨é€šè¿‡æ™ºèƒ½æ‰‹æœºï¼ˆPhoneï¼‰è®°å½•çš„é”®å‡»ä¸Šæé«˜äº†5.0%çš„æ€§èƒ½ï¼Œè€Œåœ¨é€šè¿‡Zoomè®°å½•çš„é”®å‡»ä¸Šæé«˜äº†5.9%çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å˜å‹å™¨æ¶æ„å’Œè¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­è¡¨ç°æœ€ä½³çš„VTæ¨¡å‹ä¸CoAtNetçš„æ€§èƒ½ç›¸åŒ¹é…ã€‚ä¸€ä¸ªå…³é”®çš„è¿›æ­¥æ˜¯å¼•å…¥äº†ç”¨äºç°å®ä¸–ç•Œçš„å™ªå£°ç¼“è§£æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨LLMsè¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œæ£€æµ‹å’Œçº æ­£å™ªå£°ç¯å¢ƒä¸­çš„é”™è¯¯é”®å‡»ï¼Œæé«˜ASCAçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹å¯å®ç°ä¸å…·æœ‰67å€ä»¥ä¸Šå‚æ•°çš„å¤§å‹æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚VTså’ŒLLMsçš„é›†æˆæé«˜äº†ASCAç¼“è§£çš„å®é™…é€‚ç”¨æ€§ï¼Œæ ‡å¿—ç€è¿™äº›æŠ€æœ¯åœ¨è§£å†³ç°å®åœºæ™¯ä¸­çš„ASCAå’Œé”™è¯¯æ ¡æ­£æ–¹é¢çš„é¦–æ¬¡åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09782v3">PDF</a> We would like to withdraw our paper due to a significant error in the   experimental methodology, which impacts the validity of our results. The   error specifically affects the analysis presented in Section 4, where an   incorrect dataset preprocessing step led to misleading conclusions</p>
<p><strong>æ‘˜è¦</strong><br>     éšç€æ—¥å¸¸ç”Ÿæ´»ä¸­è®¾å¤‡å†…ç½®éº¦å…‹é£ä½¿ç”¨é¢‘ç‡å¢åŠ ä»¥åŠå¯¹åœ¨çº¿æœåŠ¡çš„ä¾èµ–ç¨‹åº¦ä¸æ–­æå‡ï¼Œé’ˆå¯¹é”®ç›˜çš„å£°å­¦ä¾§ä¿¡é“æ”»å‡»ï¼ˆASCAsï¼‰é£é™©åŠ å‰§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å˜å‹å™¨ï¼ˆVTsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æé«˜æ­¤ç±»æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œé€‚ç”¨æ€§æ–¹é¢çš„ä½œç”¨ã€‚ç›¸è¾ƒäºå…ˆå‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å–å¾—äº†é‡å¤§è¿›å±•ï¼ŒCoAtNetæ¨¡å‹çš„è¡¨ç°å°¤ä¸ºçªå‡ºï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚åœ¨æ™ºèƒ½æ‰‹æœºå’ŒZoomä¸Šè®°å½•é”®å‡»æ—¶ï¼Œç›¸è¾ƒäºä¹‹å‰åŸºå‡†æµ‹è¯•ï¼ŒCoAtNetçš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†5.0%å’Œ5.9%ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å˜å‹å™¨æ¶æ„å’Œè¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­è¡¨ç°æœ€ä½³çš„VTæ¨¡å‹ä¸CoAtNetè¡¨ç°ç›¸è¿‘ã€‚ä¸€å¤§è¿›æ­¥åœ¨äºå¼•å…¥äº†ä¸€ç§ç”¨äºç°å®ä¸–ç•Œçš„å™ªå£°ç¼“è§£æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡ç†è§£ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å˜ˆæ‚ç¯å¢ƒä¸­æ£€æµ‹å’Œçº æ­£é”™è¯¯çš„é”®å‡»ï¼Œæé«˜ASCAæ€§èƒ½ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨Low-Rank Adaptationï¼ˆLoRAï¼‰è¿›è¡Œå¾®è°ƒçš„å°å‹è¯­è¨€æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œæ€§èƒ½å ªæ¯”æ‹¥æœ‰æ›´å¤šå‚æ•°çš„å¤§å‹æ¨¡å‹ï¼Œæ¯”ä¾‹ä¸º67å€ã€‚è§†è§‰å˜å‹å™¨å’Œè¯­è¨€æ¨¡å‹çš„ç»“åˆæé«˜äº†ASCAç¼“è§£çš„å®é™…é€‚ç”¨æ€§ï¼Œæ ‡å¿—ç€è¿™äº›æŠ€æœ¯é¦–æ¬¡è¢«ç”¨äºè§£å†³ç°å®åœºæ™¯ä¸­çš„ASCAså’Œé”™è¯¯æ ¡æ­£é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éº¦å…‹é£åœ¨æ—¥å¸¸è®¾å¤‡ä¸­çš„æ™®åŠä»¥åŠåœ¨çº¿æœåŠ¡çš„ä¾èµ–å¢åŠ åŠ å‰§äº†é’ˆå¯¹é”®ç›˜çš„å£°å­¦ä¾§ä¿¡é“æ”»å‡»ï¼ˆASCAsï¼‰é£é™©ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å˜å‹å™¨ï¼ˆVTsï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¢å¼ºASCAsæœ‰æ•ˆæ€§æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>CoAtNetæ¨¡å‹å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡å¹¶é¢†å…ˆè¡Œä¸šæ ‡å‡†ã€‚</li>
<li>å¼•å…¥ä¸€ç§ç”¨äºç°å®ä¸–ç•Œçš„å™ªå£°ç¼“è§£æ–¹æ³•ä»¥å¢å¼ºå£°å­¦ä¾§ä¿¡é“æ”»å‡»æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›æ£€æµ‹å’Œçº æ­£å™ªå£°ç¯å¢ƒä¸­çš„é”™è¯¯é”®å‡»ã€‚</li>
<li>ç»“åˆè§†è§‰å˜å‹å™¨å’Œè¯­è¨€æ¨¡å‹æé«˜ASCAç¼“è§£çš„å®é™…é€‚ç”¨æ€§ã€‚è¿™æ ‡å¿—ç€é¦–æ¬¡è¿ç”¨è¿™äº›æŠ€æœ¯è§£å†³ç°å®åœºæ™¯ä¸­çš„ASCAså’Œé”™è¯¯æ ¡æ­£é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5265c9c8a7219913d1e01a89746d432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7e0789cfcf7a3534a02cd33bdb6d7fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6270588840f86de4b3007016d05efed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95841e816f35d0005d7d3dcf6fc7b494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-121a024fbd0291d120525b176d1e92a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d777c2c1c996729ab6ad726912d93ab9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df9744b47b68b12857ca7c62aa417381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e43243b366021a4492deab8d780a2037.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebde3e029be267ee99597e4de0aab12b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-21/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-21/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-40982bd7658f1fcab229dbfb2d2a5dec.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-21  Autellix An Efficient Serving Engine for LLM Agents as General Programs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-85cf51f3b681081b64e1b1d4e62c8145.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  AV-Flow Transforming Text to Audio-Visual Human-like Interactions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11666.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
