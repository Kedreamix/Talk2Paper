<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-04  Dist Loss Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0e42441625531801b0ca8db53946cdf8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint"><a href="#Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint" class="headerlink" title="Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint"></a>Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint</h2><p><strong>Authors:Guangkun Nie, Gongzheng Tang, Shenda Hong</strong></p>
<p>Imbalanced data distributions are prevalent in real-world scenarios, posing significant challenges in both imbalanced classification and imbalanced regression tasks. They often cause deep learning models to overfit in areas of high sample density (many-shot regions) while underperforming in areas of low sample density (few-shot regions). This characteristic restricts the utility of deep learning models in various sectors, notably healthcare, where areas with few-shot data hold greater clinical relevance. While recent studies have shown the benefits of incorporating distribution information in imbalanced classification tasks, such strategies are rarely explored in imbalanced regression. In this paper, we address this issue by introducing a novel loss function, termed Dist Loss, designed to minimize the distribution distance between the model’s predictions and the target labels in a differentiable manner, effectively integrating distribution information into model training. Dist Loss enables deep learning models to regularize their output distribution during training, effectively enhancing their focus on few-shot regions. We have conducted extensive experiments across three datasets spanning computer vision and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results demonstrate that Dist Loss effectively mitigates the negative impact of imbalanced data distribution on model performance, achieving state-of-the-art results in sparse data regions. Furthermore, Dist Loss is easy to integrate, complementing existing methods. </p>
<blockquote>
<p>不平衡数据分布在实际场景中普遍存在，给不平衡分类和不平衡回归任务带来了重大挑战。它们经常导致深度学习模型在高样本密度区域（多镜头区域）过度拟合，而在低样本密度区域（少镜头区域）表现不佳。这一特点限制了深度学习模型在各行业的应用价值，尤其在医疗领域，少镜头数据区域具有更大的临床意义。虽然最近的研究表明在不平衡分类任务中融入分布信息具有优势，但在不平衡回归中很少探索此类策略。本文旨在解决这个问题，通过引入一种新型损失函数（称为Dist Loss），旨在以可区分的方式最小化模型预测与目标标签之间的分布距离，有效地将分布信息集成到模型训练中。Dist Loss使深度学习模型能够在训练过程中规范其输出分布，有效提高对少镜头区域的关注度。我们在跨越计算机视觉和医疗领域的三个数据集上进行了广泛实验：IMDB-WIKI-DIR、AgeDB-DIR和ECG-Ka-DIR。结果表明，Dist Loss有效减轻了不平衡数据分布对模型性能的负面影响，在稀疏数据区域取得了最先进的成果。此外，Dist Loss易于集成，可以辅助现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15216v3">PDF</a> </p>
<p><strong>Summary</strong><br>深度学习模型在处理不平衡数据分布时面临挑战，特别是在分类和回归任务中。新提出的Dist Loss损失函数能够最小化模型预测与目标标签之间的分布距离，提高模型在样本稀少区域的性能。实验结果表明，Dist Loss能有效缓解不平衡数据分布对模型性能的负面影响，实现稀疏数据区域的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>不平衡数据分布在现实场景中普遍存在，对深度学习的分类和回归任务带来挑战。</li>
<li>深度学习模型在样本密集区域容易过拟合，而在样本稀少区域表现不佳。</li>
<li>Dist Loss是一种新型损失函数，旨在最小化模型预测与目标标签之间的分布距离。</li>
<li>Dist Loss能够使深度学习模型在训练过程中正则化其输出分布，从而提高对样本稀少区域的关注。</li>
<li>Dist Loss易于集成，可以与其他方法相结合。</li>
<li>实验结果表明，Dist Loss能有效提高模型在稀疏数据区域的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15216">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-584d84b953ba2c8e3c94b1b34885e40a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e42441625531801b0ca8db53946cdf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6b33ffef72f0152ca7ef1894df27945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4b1abb7af5ca6233459bcb94a6a1d1a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="One-Policy-to-Run-Them-All-an-End-to-end-Learning-Approach-to-Multi-Embodiment-Locomotion"><a href="#One-Policy-to-Run-Them-All-an-End-to-end-Learning-Approach-to-Multi-Embodiment-Locomotion" class="headerlink" title="One Policy to Run Them All: an End-to-end Learning Approach to   Multi-Embodiment Locomotion"></a>One Policy to Run Them All: an End-to-end Learning Approach to   Multi-Embodiment Locomotion</h2><p><strong>Authors:Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo</strong></p>
<p>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world. </p>
<blockquote>
<p>深度强化学习技术正在实现最新的稳健型腿足运动学研究成果。尽管存在多种腿足平台，如四足、人形和六足等，但该领域仍然缺乏一个单一的学习框架，能够轻松有效地控制所有这些不同的体现形式，并可能以零或少数几次转移的方式应用到未见过的机器人体现形式上。我们引入URMA，即统一机器人形态架构，以弥补这一空白。我们的框架将端到端多任务强化学习方法引入到腿足机器人的领域，使得学习到的策略能够控制任何类型的机器人形态。我们的方法的关键思想是让网络学习一个抽象的步态控制器，该控制器可以无缝地在各种形态之间共享，这得益于我们的形态无关编码器和解码器。这种灵活架构可以被视为构建腿足机器人运动基础模型的第一步。我们的实验表明，URMA可以在多种体现形式上学习步态策略，并且可以轻松地将其转移到仿真和实际世界中的未见过的机器人平台上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06366v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>深度强化学习在稳健的腿部运动方面取得了最新的成果。尽管存在多种腿部平台，如四足、人形和六足等，但领域里仍缺乏一个单一的学习框架，能够轻松有效地控制所有这些不同的体现形式，并可能零成本或少成本转移到看不见的机器人体现形式。我们推出URMA，即统一机器人形态架构，以弥补这一空白。我们的框架将终端到终端的多任务强化学习带到了腿部机器人领域，使学习到的策略能够控制任何类型的机器人形态。我们的方法的关键思想是让网络学习一个抽象的步态控制器，由于我们的形态无关编码器和解码器，该控制器可以在形态之间无缝共享。这一灵活架构可以看作是构建腿部机器人运动基础模型的一个潜在的第一步。实验表明，URMA可以在多种形态上学习步态策略，并能轻松地在仿真和真实环境中转移到未知的机器人平台上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度强化学习在腿部运动方面取得最新成果。</li>
<li>存在多种腿部平台，但缺乏一个统一的学习框架来控制它们。</li>
<li>URMA（统一机器人形态架构）填补了这一空白。</li>
<li>URMA框架采用端到端的多任务强化学习。</li>
<li>网络学习抽象的步态控制器，可在不同的机器人形态之间无缝共享。</li>
<li>URMA的灵活架构是构建腿部机器人运动基础模型的初步尝试。</li>
<li>实验显示URMA能在多种形态上学习步态策略，并轻松转移到未知机器人平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06366">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-51c5d41f1805c61159f59d11384bf3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c5e4a698d12a75b9833104af8f409e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Open-FinLLMs-Open-Multimodal-Large-Language-Models-for-Financial-Applications"><a href="#Open-FinLLMs-Open-Multimodal-Large-Language-Models-for-Financial-Applications" class="headerlink" title="Open-FinLLMs: Open Multimodal Large Language Models for Financial   Applications"></a>Open-FinLLMs: Open Multimodal Large Language Models for Financial   Applications</h2><p><strong>Authors:Jimin Huang, Mengxi Xiao, Dong Li, Zihao Jiang, Yuzhe Yang, Yifei Zhang, Lingfei Qian, Yan Wang, Xueqing Peng, Yang Ren, Ruoyu Xiang, Zhengyu Chen, Xiao Zhang, Yueru He, Weiguang Han, Shunian Chen, Lihang Shen, Daniel Kim, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Guojun Xiong, Zhiwei Liu, Zheheng Luo, Zhiyuan Yao, Ruey-Ling Weng, Meikang Qiu, Kaleb E Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jian-Yun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Qianqian Xie, Sophia Ananiadou, Junichi Tsujii</strong></p>
<p>Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit{Open-FinLLMs}, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE">https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE</a>) and models under OSI-approved licenses. </p>
<blockquote>
<p>金融LLM（大型预训练语言模型）在推进金融任务和特定领域应用方面展现出巨大的潜力。然而，它们受到有限语料库、弱多模态能力和狭窄评估范围的限制，使得它们不太适合实际应用。为了解决这个问题，我们推出了”Open-FinLLMs”，这是首个开源的多模态金融LLM，旨在处理文本、表格、时间序列和图表数据等多样任务，并在零样本、少样本和微调场景中表现出色。该套件包括在全面的52亿令牌语料库上预训练的FinLLaMA、使用573K金融指令进行微调的FinLLaMA-Instruct以及通过143万对多模态调优增强功能的FinLLaVA，以实现强大的跨模态推理。我们全面评估了Open-FinLLMs在14个金融任务、30个数据集和4个多模态任务中的表现，包括零样本、少样本和监督微调场景，并引入了两个新的多模态评估数据集。我们的结果表明，Open-FinLLMs在金融NLP、决策制定和多模态任务方面优于先进的金融和通用LLM，如GPT-4，凸显了它们解决现实挑战的潜力。为了促进学术界和产业界的创新和合作，我们发布了所有代码（<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE%EF%BC%89%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E9%81%B5%E5%BE%AAOSI%E6%89%B9%E5%87%86%E8%AE%B8%E5%8F%AF%E8%AF%81%E3%80%82">https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE）和模型，并遵循OSI批准许可证。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11878v2">PDF</a> 33 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>金融LLM在推进金融任务和特定领域应用方面有很大潜力，但受限于稀缺语料库、弱多模态能力和狭窄评估范围，不太适合现实世界应用。为解决这一问题，我们推出Open-FinLLMs，首个开源多模态金融LLM，可处理文本、表格、时间序列和图表数据的多样化任务，在零样本、少样本和微调设置中都表现出色。包括基于52亿令牌语料库的预训练FinLLaMA、以57.3万条金融指令精细调教的FinLLaMA-Instruct，以及通过143万对多模态调优增强能力的FinLLaVA。我们全面评估了Open-FinLLMs在14个金融任务、30个数据集和4个多模态任务中的表现，并引入两个新的多模态评估数据集。结果表明，Open-FinLLMs在金融NLP、决策和多模态任务方面优于GPT-4等先进金融和通用LLM，展现出解决现实挑战的巨大潜力。我们发布所有代码和模型，以促进学术界和产业界的创新和合作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融LLM具有推进金融任务和领域特定应用的潜力。</li>
<li>当前金融LLM受限于稀缺语料库、弱多模态能力和狭窄评估。</li>
<li>Open-FinLLMs旨在解决这些问题，具备处理多样化任务的能力。</li>
<li>Open-FinLLMs包括预训练的FinLLaMA、精细调教的FinLLaMA-Instruct和增强多模态推理的FinLLaVA。</li>
<li>在多个金融任务和多模态任务中，Open-FinLLMs表现出超越先进金融和通用LLM的潜力。</li>
<li>Open-FinLLMs的推出旨在促进学术界和产业界的合作与创新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e30ed238ac0ba295ce521568ae71898.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa49f929da62393c629f6fb1b495802a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fd0aab99f7fe9d7a6c1f2dbac620edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e9612cc117dbc57d39fa5d3bd741fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aec7904e48a74a229c97ca71091b713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34499c65ed467378adc45c3f2a6ebce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94ef1cad4236f9af17a9b38bb7112334.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Non-Determinism-of-“Deterministic”-LLM-Settings"><a href="#Non-Determinism-of-“Deterministic”-LLM-Settings" class="headerlink" title="Non-Determinism of “Deterministic” LLM Settings"></a>Non-Determinism of “Deterministic” LLM Settings</h2><p><strong>Authors:Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, Breck Baldwin</strong></p>
<p>LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs under settings expected to be deterministic. Yet the questions of how pervasive this is, and with what impact on results, have not to our knowledge been systematically investigated. We investigate non-determinism in five LLMs configured to be deterministic when applied to eight common tasks in across 10 runs, in both zero-shot and few-shot settings. We see accuracy variations up to 15% across naturally occurring runs with a gap of best possible performance to worst possible performance up to 70%. In fact, none of the LLMs consistently delivers repeatable accuracy across all tasks, much less identical output strings. Sharing preliminary results with insiders has revealed that non-determinism perhaps essential to the efficient use of compute resources via co-mingled data in input buffers so this issue is not going away anytime soon. To better quantify our observations, we introduce metrics focused on quantifying determinism, TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement rate of parsed-out answers. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/breckbaldwin/llm-stability">https://github.com/breckbaldwin/llm-stability</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的实践者通常注意到，在预期为确定性设置的条件下，相同输入的输出了可能会发生变动。然而，至于这种现象的普遍性以及对结果的影响程度，据我们了解尚未有系统性的研究。我们在五个被配置为确定性的LLM上进行了调查，这些LLM在零样本和少样本设置下应用于八个常见任务，并进行了十次运行。我们看到自然运行之间的准确率变动高达15%，最佳可能性能与最差可能性能之间的差距高达70%。事实上，没有任何一个LLM在所有任务上都能提供一致的准确率，更别提输出完全相同的字符串了。与业内专家的初步结果分享显示，非确定性对于通过输入缓冲区中的混合数据实现计算资源的有效利用可能是至关重要的，因此这一问题在短期内不会得到解决。为了更好地量化我们的观察结果，我们引入了专注于量化确定性的指标，包括TARr@N（N次运行中原始输出的总协议率）和TARa@N（解析出的答案的总协议率）。我们的代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/breckbaldwin/llm-stability%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/breckbaldwin/llm-stability上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04667v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在实践中表现出非确定性行为，对相同输入的输出在不同运行设置下会有变化。本文对此进行了系统调查，在五个LLM模型中对八个常见任务进行十次运行测试，发现在零样本和少样本设置下，准确度变化高达15%，最佳与最差性能之间的差距最大可达70%。共享初步结果表明，非确定性对于有效利用计算资源可能是必要的。本文引入了专注于量化确定性的指标TARr@N和TARa@N，分别用于衡量原始输出和解析答案的共识程度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM模型在实践中表现出非确定性行为，即相同输入在不同运行设置下会产生不同的输出。</li>
<li>系统调查了五个LLM模型在八个常见任务中的非确定性表现，发现准确度变化大，最佳与最差性能差距显著。</li>
<li>LLM模型在所有任务上无法保持一致的准确性，且很少产生完全相同的输出字符串。</li>
<li>非确定性对于计算资源的有效利用可能是必要的，可能与输入缓冲区中的混合数据有关。</li>
<li>引入TARr@N和TARa@N指标，用于量化LLM模型的确定性评估。</li>
<li>该研究的代码和数据已公开可用。</li>
<li>此问题（非确定性）在短期内不会得到解决。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7178d8edec69ddb50c4c311912880c58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57ba502db41809b9f4bd595e10100c4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39028cb9ec004d1009befdbd4a0ccf57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f05dcfdf69034867430ae4f7f636bca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ee95d7f91024e5de695549a4b6d9e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3865210646e8acdf7b122781f6e5d086.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FsPONER-Few-shot-Prompt-Optimization-for-Named-Entity-Recognition-in-Domain-specific-Scenarios"><a href="#FsPONER-Few-shot-Prompt-Optimization-for-Named-Entity-Recognition-in-Domain-specific-Scenarios" class="headerlink" title="FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in   Domain-specific Scenarios"></a>FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in   Domain-specific Scenarios</h2><p><strong>Authors:Yongjian Tang, Rakebul Hasan, Thomas Runkler</strong></p>
<p>Large Language Models (LLMs) have provided a new pathway for Named Entity Recognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting methods avoid the need for training, conserve substantial computational resources, and rely on minimal annotated data. Previous studies have achieved comparable performance to fully supervised BERT-based fine-tuning approaches on general NER benchmarks. However, none of the previous approaches has investigated the efficiency of LLM-based few-shot learning in domain-specific scenarios. To address this gap, we introduce FsPONER, a novel approach for optimizing few-shot prompts, and evaluate its performance on domain-specific NER datasets, with a focus on industrial manufacturing and maintenance, while using multiple LLMs – GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna. FsPONER consists of three few-shot selection methods based on random sampling, TF-IDF vectors, and a combination of both. We compare these methods with a general-purpose GPT-NER method as the number of few-shot examples increases and evaluate their optimal NER performance against fine-tuned BERT and LLaMA 2-chat. In the considered real-world scenarios with data scarcity, FsPONER with TF-IDF surpasses fine-tuned models by approximately 10% in F1 score. </p>
<blockquote>
<p>大型语言模型（LLM）为命名实体识别（NER）任务提供了新的途径。与微调相比，LLM驱动的提示方法无需训练，可以节省大量计算资源，并依赖极少量的注释数据。之前的研究已在通用NER基准测试上实现了与完全监督的BERT微调方法相当的性能。然而，以前的任何方法都没有研究LLM在特定领域的少样本学习中的效率。为了弥补这一空白，我们引入了FsPONER，这是一种优化少样本提示的新方法，并评估其在特定领域的NER数据集上的性能，重点关注工业制造和维护领域，同时使用多个LLM——GPT-4-32K、GPT-3.5 Turbo、LLaMA 2聊天和Vicuna。FsPONER包括三种基于随机采样、TF-IDF向量和两者组合的少样本选择方法。我们随着少样本示例数量的增加，将这些方法与通用GPT-NER方法进行比较，并评估其最佳NER性能与微调BERT和LLaMA 2聊天的性能。在数据稀缺的现实世界场景中，使用TF-IDF的FsPONER在F1分数上超越了微调模型约10%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.08035v2">PDF</a> accepted in the main track at the 27th European Conference on   Artificial Intelligence (ECAI-2024)</p>
<p><strong>Summary</strong></p>
<p>LLMs在命名实体识别（NER）任务中展现出巨大潜力，采用提示方法而无需训练，节省了计算资源，并依赖少量标注数据。本文介绍了一种新型的基于LLM的少样本学习方法FsPONER，并重点对工业制造和维护领域的特定数据集进行评估。FsPONER包含三种基于随机采样、TF-IDF向量和两者结合的少样本选择方法，在某些场景下表现优异，尤其是在数据稀缺的现实中超越微调BERT模型约10%的F1分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs为NER任务提供了新的途径，采用提示方法无需训练，节省计算资源。</li>
<li>FsPONER是一种新型的基于LLM的少样本学习方法，用于优化NER任务的性能。</li>
<li>FsPONER在特定领域数据集（如工业制造和维护）上表现出优异性能。</li>
<li>FsPONER包含三种少样本选择方法：随机采样、TF-IDF向量和两者的组合。</li>
<li>在某些场景下，FsPONER超越微调BERT模型约10%的F1分数。</li>
<li>使用多个LLMs（如GPT-4-32K、GPT-3.5-Turbo、LLaMA 2-chat和Vicuna）进行评估，显示LLMs在NER任务中的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.08035">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2ca0650a75a7f2299f1a896b5e5e1b01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fe19fa6605fbb1ffb967afb10eca039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ec2e877584eb5a2e98da6221f578622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87036e19cd03921b9702b0a13b9d4300.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6868cb4486a6678ac1790149545339fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-821f376586c2575bf1fbfa4f094a84dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-591339d7e845bbbaf3257162ec2b182c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1c38a41951bd2b08896e40c6b6b751.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Induction-Heads-as-an-Essential-Mechanism-for-Pattern-Matching-in-In-context-Learning"><a href="#Induction-Heads-as-an-Essential-Mechanism-for-Pattern-Matching-in-In-context-Learning" class="headerlink" title="Induction Heads as an Essential Mechanism for Pattern Matching in   In-context Learning"></a>Induction Heads as an Essential Mechanism for Pattern Matching in   In-context Learning</h2><p><strong>Authors:Joy Crosbie, Ekaterina Shutova</strong></p>
<p>Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting. We analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the model’s ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present fine-grained evidence for the role that the induction mechanism plays in ICL. </p>
<blockquote>
<p>大型语言模型（LLM）显示出通过上下文学习（ICL）学习和执行复杂任务的能力。然而，对其内部机制的全面理解仍然缺乏。本文探讨了归纳头在少量上下文学习设置中的角色。我们分析了两种最先进的模型，即Llama-3-8B和InternLM2-20B在抽象模式识别和NLP任务上的表现。我们的结果表明，即使是最小的归纳头消融也会导致抽象模式识别任务的上下文学习性能下降高达约32%，将性能降低到接近随机水平。对于NLP任务，这种消融大大降低了模型从例子中受益的能力，使得少量的上下文学习性能接近于零样本提示。我们进一步使用注意力消除法来禁用特定的归纳模式，并为归纳机制在上下文学习中的作用提供精细的证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07011v3">PDF</a> 9 pages, 7 figures; Code link added</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）通过上下文学习（ICL）展现出了令人瞩目的学习和执行复杂任务的能力，但其内部机制仍缺乏全面理解。本文在少样本ICL环境中，探讨了归纳头的作用。我们对两种最新模型Llama-3-8B和InternLM2-20B在抽象模式识别和NLP任务上进行了分析。结果表明，即使进行少量的归纳头消融也会导致ICL性能下降约至原来的最大达约达32%，严重影响抽象模式识别任务的性能。对于NLP任务，消融使模型几乎丧失从示例中受益的能力，使少样本ICL性能接近零样本提示性能。我们进一步使用注意力剔除技术来禁用特定的归纳模式，并为归纳机制在ICL中的角色提供了精细的证据。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）通过上下文学习（ICL）表现出强大的学习和执行任务的能力。</li>
<li>归纳头在LLMs的ICL中起到关键作用。</li>
<li>对归纳头进行消融会导致LLMs在抽象模式识别任务上的性能下降高达约32%。</li>
<li>对于NLP任务，消融归纳头会使模型丧失从示例中学习的能力，使少样本ICL性能接近零样本提示性能。</li>
<li>通过注意力剔除技术，可以禁用特定的归纳模式。</li>
<li>论文为归纳机制在ICL中的角色提供了精细的证据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07011">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-73c681ed6c799a8902c0af88ee810b0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015847748a3ec7f38cceae23c6f17ec0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-095b6e529312fbcb97d72c343346bab7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33cbce6f40491999f5f77ad8e1bbf98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216f6d799d22c42694fa4a50630d0bd0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CoMM-A-Coherent-Interleaved-Image-Text-Dataset-for-Multimodal-Understanding-and-Generation"><a href="#CoMM-A-Coherent-Interleaved-Image-Text-Dataset-for-Multimodal-Understanding-and-Generation" class="headerlink" title="CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal   Understanding and Generation"></a>CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal   Understanding and Generation</h2><p><strong>Authors:Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, Long Chen</strong></p>
<p>Interleaved image-text generation has emerged as a crucial multimodal task, aiming at creating sequences of interleaved visual and textual content given a query. Despite notable advancements in recent multimodal large language models (MLLMs), generating integrated image-text sequences that exhibit narrative coherence and entity and style consistency remains challenging due to poor training data quality. To address this gap, we introduce CoMM, a high-quality Coherent interleaved image-text MultiModal dataset designed to enhance the coherence, consistency, and alignment of generated multimodal content. Initially, CoMM harnesses raw data from diverse sources, focusing on instructional content and visual storytelling, establishing a foundation for coherent and consistent content. To further refine the data quality, we devise a multi-perspective filter strategy that leverages advanced pre-trained models to ensure the development of sentences, consistency of inserted images, and semantic alignment between them. Various quality evaluation metrics are designed to prove the high quality of the filtered dataset. Meanwhile, extensive few-shot experiments on various downstream tasks demonstrate CoMM’s effectiveness in significantly enhancing the in-context learning capabilities of MLLMs. Moreover, we propose four new tasks to evaluate MLLMs’ interleaved generation abilities, supported by a comprehensive evaluation framework. We believe CoMM opens a new avenue for advanced MLLMs with superior multimodal in-context learning and understanding ability. </p>
<blockquote>
<p>交互式图像文本生成已经成为一项关键的多模态任务，其目标是根据查询生成交替出现的视觉和文本内容序列。尽管最近的多模态大型语言模型（MLLM）取得了显著的进展，但生成具有叙事连贯性和实体及风格一致性的集成图像文本序列仍然是一个挑战，主要是由于训练数据质量差。为了解决这一差距，我们引入了CoMM，这是一个高质量的多模态连贯交互式图像文本数据集，旨在提高生成的多模态内容的一致性、连贯性和对齐性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10462v3">PDF</a> 22 pages, Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为CoMM的高质量图像文本多媒体数据集，该数据集用于提高生成多媒体内容的连贯性、一致性和对齐性。通过从各种资源中获取原始数据，并专注于指令内容和视觉叙事，建立连贯和一致内容的基础。采用多视角过滤策略，利用先进的预训练模型确保句子发展、插入图像的一致性和语义对齐。经过质量评估证明，过滤后的数据集质量高。此外，在各种下游任务上的少量样本实验证明了CoMM在提高多媒体大型语言模型的上下文学习能力方面的有效性。同时，提出了四个新任务来评估多媒体大型语言模型的交错生成能力，并由综合评估框架支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoMM是一个高质量的图像文本多媒体数据集，旨在提高生成多媒体内容的连贯性、一致性和对齐性。</li>
<li>该数据集从各种资源获取原始数据，专注于指令内容和视觉叙事。</li>
<li>采用多视角过滤策略，利用先进的预训练模型进行质量提升。</li>
<li>过滤后的数据集经过质量评估证明表现优异。</li>
<li>在下游任务上的少量样本实验证明了CoMM的有效性。</li>
<li>提出了四个新任务来评估多媒体大型语言模型的交错生成能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10462">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a026ea4741403a3453c41bb401d549e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a523c5d785c1c508e2bb462651972eb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f01793bec58c0b001ce864e134922eb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-627d5c4cf51e678fa9b50a44943b2198.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Consistency-Guided-Asynchronous-Contrastive-Tuning-for-Few-Shot-Class-Incremental-Tuning-of-Foundation-Models"><a href="#Consistency-Guided-Asynchronous-Contrastive-Tuning-for-Few-Shot-Class-Incremental-Tuning-of-Foundation-Models" class="headerlink" title="Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot   Class-Incremental Tuning of Foundation Models"></a>Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot   Class-Incremental Tuning of Foundation Models</h2><p><strong>Authors:Shuvendu Roy, Elham Dolatabadi, Arash Afkanpour, Ali Etemad</strong></p>
<p>We propose Consistency-guided Asynchronous Contrastive Tuning (CoACT), a novel method for continuously tuning foundation models to learn new classes in few-shot settings. CoACT consists of three key components:(i) asynchronous contrastive tuning, which learns new classes by including LoRA modules in the pre-trained encoder while enforcing consistency between two asynchronous encoders; (ii) controlled fine-tuning, which facilitates effective tuning of a subset of the foundation model; and (iii) consistency-guided incremental tuning, which enforces additional regularization during later sessions to reduce forgetting of the learned classes. We evaluate our proposed solution on Few-Shot Class-Incremental Learning (FSCIL) as well as a new and more challenging setup called Few-Shot Class-Incremental Tuning (FSCIT), which facilitates the continual tuning of vision foundation models to learn new classes with only a few samples per class. Unlike traditional FSCIL, FSCIT does not require a large in-distribution base session for initial fully supervised training prior to the incremental few-shot sessions. We conduct extensive evaluations across 16 diverse datasets, demonstrating the effectiveness of CoACT in both FSCIL and FSCIT setups. CoACT outperforms existing methods by up to 5.02% in FSCIL and up to 12.51% in FSCIT for individual datasets, with an average improvement of 2.47%. Furthermore, CoACT exhibits reduced forgetting and enhanced robustness in low-shot experiments. Detailed ablation and sensitivity studies highlight the contribution of each component of CoACT. We make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL">https://github.com/ShuvenduRoy/CoACT-FSCIL</a>. </p>
<blockquote>
<p>我们提出了一种新的方法，名为一致性引导异步对比调优（CoACT），用于在少量样本设置下连续调整基础模型以学习新类别。CoACT包含三个关键组件：(i) 异步对比调优，通过在预训练编码器中包含LoRA模块，同时强制两个异步编码器之间的一致性，来学习新类别；(ii) 受控微调，便于有效地调整基础模型的一个子集；(iii) 一致性引导增量调优，在后续会话期间强制执行额外的正则化，以减少已学习类别的遗忘。我们在少量类别增量学习（FSCIL）以及称为少量类别增量调整（FSCIT）的新颖且更具挑战性的设置上评估了我们提出的解决方案。FSCIT促进了视觉基础模型在只有每个类别几个样本的情况下不断学习新类别。与传统的FSCIL不同，FSCIT不需要在增量少量会话之前进行大规模内部基础会话的初始完全监督训练。我们在16个不同的数据集上进行了广泛评估，证明了CoACT在FSCIL和FSCIT设置中的有效性。在FSCIL中，CoACT将现有方法的性能提高了高达5.02%，在FSCIT中提高了高达12.51%，针对单个数据集的平均改进为2.47%。此外，CoACT在低样本实验中表现出减少遗忘和增强稳健性的特点。详细的消融和敏感性研究突出了CoACT每个组件的贡献。我们在<a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL">https://github.com/ShuvenduRoy/CoACT-FSCIL</a> 公开了我们的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16625v2">PDF</a> Accepted in Transactions on Machine Learning Research (TMLR)</p>
<p><strong>摘要</strong><br>    本研究提出一种名为Consistency-guided Asynchronous Contrastive Tuning（CoACT）的新方法，用于在少量样本情况下连续调整基础模型以学习新类别。CoACT包含三个关键组件：（i）异步对比调整，通过包含LoRA模块在预训练编码器中学习新类别，同时强制两个异步编码器之间的一致性；（ii）受控微调，促进基础模型子集的有效调整；（iii）一致性引导增量调整，在后续会话期间强制执行附加正则化，以减少已学习类别的遗忘。本研究在少量类别增量学习（FSCIL）以及称为少量类别增量调整（FSCIT）的新挑战设置上评估了所提方法的有效性，后者促进了视觉基础模型连续调整以学习新类别，并且每类仅使用少量样本。与传统FSCIL不同，FSCIT不需要大量内部基本会话进行初始完全监督训练，再进行增量少量会话。在16个不同数据集上的广泛评估表明，CoACT在FSCIL和FSCIT设置中均表现出卓越效果。在FSCIL中，CoACT较现有方法最多高出5.02%，在FSCIT中最多高出12.51%。总体而言，CoACT减少了遗忘并在低样本实验中表现出增强的稳健性。详细的消融和敏感性研究突出了CoACT每个组件的贡献。我们的代码已公开发布在<a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL%E3%80%82">https://github.com/ShuvenduRoy/CoACT-FSCIL。</a></p>
<p><strong>要点掌握</strong></p>
<ol>
<li>CoACT是一种用于在少量样本情况下连续调整基础模型学习新类别的新方法。</li>
<li>CoACT包含三个关键组件：异步对比调整、受控微调、一致性引导增量调整。</li>
<li>CoACT在FSCIL和FSCIT两种设置上进行了评估，并表现出卓越的效果。</li>
<li>CoACT在FSCIL和FSCIT中都较现有方法有显著提高，平均提高了2.47%。</li>
<li>CoACT减少了遗忘并在低样本实验中表现出增强的稳健性。</li>
<li>公开的代码地址是<a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL%E3%80%82">https://github.com/ShuvenduRoy/CoACT-FSCIL。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16625">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc67cf47cb925dc4a543fc2d2d38a91a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-238502985f1e8c5476ce96a3237f0d26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-412ee5146513d678245a74d463d6953d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning"><a href="#VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning" class="headerlink" title="VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"></a>VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning</h2><p><strong>Authors:Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</strong></p>
<p>Large language models (LLMs) famously exhibit emergent in-context learning (ICL) – the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model’s weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>. </p>
<blockquote>
<p>大型语言模型（LLM）展现出一种称为情境涌现学习（ICL）的能力，即使用少量示例快速适应新任务，而无需更新模型权重。建立在大型语言模型之上的视觉大型语言模型（VLLM）在识别、推理和接地等领域取得了显著进展。然而，对多模态ICL的研究主要集中在视觉问答和图像描述生成等方面，这些领域并没有充分利用ICL的优势，也没有测试其局限性。多模态ICL的更广泛的能力和局限性尚未被充分探索。在这项研究中，我们引入了全面的基准测试VL-ICL Bench，用于多模态情境学习，涵盖了一系列任务，这些任务涉及图像和文本作为输入和输出，以及从感知到推理和长语境长度的不同类型挑战。我们用这个基准测试套件评估了最先进的VLLM的能力，揭示了它们的各种优势和劣势，并表明即使是最先进的模型，如GPT-4，也会发现这些任务具有挑战性。通过突出一系列新的ICL任务以及现有模型的相关优势和局限性，我们希望我们的数据集将激发未来对增强VLLM情境学习能力的研究，并激发利用VLLM ICL的新应用。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13164v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型展现出新兴的在情境内学习（ICL）能力，能够通过少量示例迅速适应新任务。在此基础上，视觉大型语言模型（VLLMs）在识别、推理和接地等领域取得了显著进展。然而，多模态ICL的探究主要集中在少数视觉问答和图像描述上，这并未充分利用ICL的优势，也没有测试其局限性。本研究引入了一个全面的多模态情境内学习基准VL-ICL Bench，涵盖了一系列涉及图像和文本作为输入和输出的任务，以及从感知到推理和长语境长度的不同类型挑战。我们评估了最先进VLLMs的能力，揭示了它们的各种优势和劣势，并显示即使是最先进的模型，如GPT-4，也会发现这些任务具有挑战性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型展现出情境内学习能力，能通过少量示例迅速适应新任务。</li>
<li>视觉大型语言模型在识别、推理和接地等领域取得显著进展。</li>
<li>当前多模态ICL研究主要集中在视觉问答和图像描述上，未能充分利用ICL的优势和测试其局限性。</li>
<li>引入了一个全面的多模态情境内学习基准VL-ICL Bench，涵盖广泛的任务类型，包括图像和文本作为输入和输出，涉及不同类型的挑战。</li>
<li>评估了最先进VLLMs的能力，揭示了其优势和劣势。</li>
<li>现有最先进的模型，如GPT-4，在面对这些新任务时仍会面临挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b999c539bc388e044c297bd9cc8f8c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615437abc2988c94e3bfd80abd3eedc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75bfc5198c7b62621bde4c7858123edc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02ef11c2143c9259c5c08e36eac8cbab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e860e2d71c7734fe3e5ee796c28356f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ea006b54cbff3ce97de70c07152c8b38.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-04-04  Vision-RWKV Efficient and Scalable Visual Perception with RWKV-Like   Architectures
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5965dd9f74dff732e9f8ee7471fb097c.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-04-04  GECKO Gigapixel Vision-Concept Contrastive Pretraining in   Histopathology
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
