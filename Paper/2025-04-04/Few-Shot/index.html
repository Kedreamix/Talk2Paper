<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Dist Loss Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0e42441625531801b0ca8db53946cdf8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint"><a href="#Dist-Loss-Enhancing-Regression-in-Few-Shot-Region-through-Distribution-Distance-Constraint" class="headerlink" title="Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint"></a>Dist Loss: Enhancing Regression in Few-Shot Region through Distribution   Distance Constraint</h2><p><strong>Authors:Guangkun Nie, Gongzheng Tang, Shenda Hong</strong></p>
<p>Imbalanced data distributions are prevalent in real-world scenarios, posing significant challenges in both imbalanced classification and imbalanced regression tasks. They often cause deep learning models to overfit in areas of high sample density (many-shot regions) while underperforming in areas of low sample density (few-shot regions). This characteristic restricts the utility of deep learning models in various sectors, notably healthcare, where areas with few-shot data hold greater clinical relevance. While recent studies have shown the benefits of incorporating distribution information in imbalanced classification tasks, such strategies are rarely explored in imbalanced regression. In this paper, we address this issue by introducing a novel loss function, termed Dist Loss, designed to minimize the distribution distance between the modelâ€™s predictions and the target labels in a differentiable manner, effectively integrating distribution information into model training. Dist Loss enables deep learning models to regularize their output distribution during training, effectively enhancing their focus on few-shot regions. We have conducted extensive experiments across three datasets spanning computer vision and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results demonstrate that Dist Loss effectively mitigates the negative impact of imbalanced data distribution on model performance, achieving state-of-the-art results in sparse data regions. Furthermore, Dist Loss is easy to integrate, complementing existing methods. </p>
<blockquote>
<p>ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒåœ¨å®é™…åœºæ™¯ä¸­æ™®éå­˜åœ¨ï¼Œç»™ä¸å¹³è¡¡åˆ†ç±»å’Œä¸å¹³è¡¡å›å½’ä»»åŠ¡å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å®ƒä»¬ç»å¸¸å¯¼è‡´æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é«˜æ ·æœ¬å¯†åº¦åŒºåŸŸï¼ˆå¤šé•œå¤´åŒºåŸŸï¼‰è¿‡åº¦æ‹Ÿåˆï¼Œè€Œåœ¨ä½æ ·æœ¬å¯†åº¦åŒºåŸŸï¼ˆå°‘é•œå¤´åŒºåŸŸï¼‰è¡¨ç°ä¸ä½³ã€‚è¿™ä¸€ç‰¹ç‚¹é™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å„è¡Œä¸šçš„åº”ç”¨ä»·å€¼ï¼Œå°¤å…¶åœ¨åŒ»ç–—é¢†åŸŸï¼Œå°‘é•œå¤´æ•°æ®åŒºåŸŸå…·æœ‰æ›´å¤§çš„ä¸´åºŠæ„ä¹‰ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜åœ¨ä¸å¹³è¡¡åˆ†ç±»ä»»åŠ¡ä¸­èå…¥åˆ†å¸ƒä¿¡æ¯å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨ä¸å¹³è¡¡å›å½’ä¸­å¾ˆå°‘æ¢ç´¢æ­¤ç±»ç­–ç•¥ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°ï¼ˆç§°ä¸ºDist Lossï¼‰ï¼Œæ—¨åœ¨ä»¥å¯åŒºåˆ†çš„æ–¹å¼æœ€å°åŒ–æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ï¼Œæœ‰æ•ˆåœ°å°†åˆ†å¸ƒä¿¡æ¯é›†æˆåˆ°æ¨¡å‹è®­ç»ƒä¸­ã€‚Dist Lossä½¿æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§„èŒƒå…¶è¾“å‡ºåˆ†å¸ƒï¼Œæœ‰æ•ˆæé«˜å¯¹å°‘é•œå¤´åŒºåŸŸçš„å…³æ³¨åº¦ã€‚æˆ‘ä»¬åœ¨è·¨è¶Šè®¡ç®—æœºè§†è§‰å’ŒåŒ»ç–—é¢†åŸŸçš„ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼šIMDB-WIKI-DIRã€AgeDB-DIRå’ŒECG-Ka-DIRã€‚ç»“æœè¡¨æ˜ï¼ŒDist Lossæœ‰æ•ˆå‡è½»äº†ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒå¯¹æ¨¡å‹æ€§èƒ½çš„è´Ÿé¢å½±å“ï¼Œåœ¨ç¨€ç–æ•°æ®åŒºåŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚æ­¤å¤–ï¼ŒDist Lossæ˜“äºé›†æˆï¼Œå¯ä»¥è¾…åŠ©ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15216v3">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸­ã€‚æ–°æå‡ºçš„Dist LossæŸå¤±å‡½æ•°èƒ½å¤Ÿæœ€å°åŒ–æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ï¼Œæé«˜æ¨¡å‹åœ¨æ ·æœ¬ç¨€å°‘åŒºåŸŸçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDist Lossèƒ½æœ‰æ•ˆç¼“è§£ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒå¯¹æ¨¡å‹æ€§èƒ½çš„è´Ÿé¢å½±å“ï¼Œå®ç°ç¨€ç–æ•°æ®åŒºåŸŸçš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒåœ¨ç°å®åœºæ™¯ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹æ·±åº¦å­¦ä¹ çš„åˆ†ç±»å’Œå›å½’ä»»åŠ¡å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ ·æœ¬å¯†é›†åŒºåŸŸå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè€Œåœ¨æ ·æœ¬ç¨€å°‘åŒºåŸŸè¡¨ç°ä¸ä½³ã€‚</li>
<li>Dist Lossæ˜¯ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨æœ€å°åŒ–æ¨¡å‹é¢„æµ‹ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„åˆ†å¸ƒè·ç¦»ã€‚</li>
<li>Dist Lossèƒ½å¤Ÿä½¿æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ­£åˆ™åŒ–å…¶è¾“å‡ºåˆ†å¸ƒï¼Œä»è€Œæé«˜å¯¹æ ·æœ¬ç¨€å°‘åŒºåŸŸçš„å…³æ³¨ã€‚</li>
<li>Dist Lossæ˜“äºé›†æˆï¼Œå¯ä»¥ä¸å…¶ä»–æ–¹æ³•ç›¸ç»“åˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDist Lossèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹åœ¨ç¨€ç–æ•°æ®åŒºåŸŸçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-584d84b953ba2c8e3c94b1b34885e40a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e42441625531801b0ca8db53946cdf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6b33ffef72f0152ca7ef1894df27945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4b1abb7af5ca6233459bcb94a6a1d1a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="One-Policy-to-Run-Them-All-an-End-to-end-Learning-Approach-to-Multi-Embodiment-Locomotion"><a href="#One-Policy-to-Run-Them-All-an-End-to-end-Learning-Approach-to-Multi-Embodiment-Locomotion" class="headerlink" title="One Policy to Run Them All: an End-to-end Learning Approach to   Multi-Embodiment Locomotion"></a>One Policy to Run Them All: an End-to-end Learning Approach to   Multi-Embodiment Locomotion</h2><p><strong>Authors:Nico Bohlinger, Grzegorz Czechmanowski, Maciej Krupka, Piotr Kicki, Krzysztof Walas, Jan Peters, Davide Tateo</strong></p>
<p>Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world. </p>
<blockquote>
<p>æ·±åº¦å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ­£åœ¨å®ç°æœ€æ–°çš„ç¨³å¥å‹è…¿è¶³è¿åŠ¨å­¦ç ”ç©¶æˆæœã€‚å°½ç®¡å­˜åœ¨å¤šç§è…¿è¶³å¹³å°ï¼Œå¦‚å››è¶³ã€äººå½¢å’Œå…­è¶³ç­‰ï¼Œä½†è¯¥é¢†åŸŸä»ç„¶ç¼ºä¹ä¸€ä¸ªå•ä¸€çš„å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè½»æ¾æœ‰æ•ˆåœ°æ§åˆ¶æ‰€æœ‰è¿™äº›ä¸åŒçš„ä½“ç°å½¢å¼ï¼Œå¹¶å¯èƒ½ä»¥é›¶æˆ–å°‘æ•°å‡ æ¬¡è½¬ç§»çš„æ–¹å¼åº”ç”¨åˆ°æœªè§è¿‡çš„æœºå™¨äººä½“ç°å½¢å¼ä¸Šã€‚æˆ‘ä»¬å¼•å…¥URMAï¼Œå³ç»Ÿä¸€æœºå™¨äººå½¢æ€æ¶æ„ï¼Œä»¥å¼¥è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†ç«¯åˆ°ç«¯å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ–¹æ³•å¼•å…¥åˆ°è…¿è¶³æœºå™¨äººçš„é¢†åŸŸï¼Œä½¿å¾—å­¦ä¹ åˆ°çš„ç­–ç•¥èƒ½å¤Ÿæ§åˆ¶ä»»ä½•ç±»å‹çš„æœºå™¨äººå½¢æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®æ€æƒ³æ˜¯è®©ç½‘ç»œå­¦ä¹ ä¸€ä¸ªæŠ½è±¡çš„æ­¥æ€æ§åˆ¶å™¨ï¼Œè¯¥æ§åˆ¶å™¨å¯ä»¥æ— ç¼åœ°åœ¨å„ç§å½¢æ€ä¹‹é—´å…±äº«ï¼Œè¿™å¾—ç›Šäºæˆ‘ä»¬çš„å½¢æ€æ— å…³ç¼–ç å™¨å’Œè§£ç å™¨ã€‚è¿™ç§çµæ´»æ¶æ„å¯ä»¥è¢«è§†ä¸ºæ„å»ºè…¿è¶³æœºå™¨äººè¿åŠ¨åŸºç¡€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒURMAå¯ä»¥åœ¨å¤šç§ä½“ç°å½¢å¼ä¸Šå­¦ä¹ æ­¥æ€ç­–ç•¥ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°å°†å…¶è½¬ç§»åˆ°ä»¿çœŸå’Œå®é™…ä¸–ç•Œä¸­çš„æœªè§è¿‡çš„æœºå™¨äººå¹³å°ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06366v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨ç¨³å¥çš„è…¿éƒ¨è¿åŠ¨æ–¹é¢å–å¾—äº†æœ€æ–°çš„æˆæœã€‚å°½ç®¡å­˜åœ¨å¤šç§è…¿éƒ¨å¹³å°ï¼Œå¦‚å››è¶³ã€äººå½¢å’Œå…­è¶³ç­‰ï¼Œä½†é¢†åŸŸé‡Œä»ç¼ºä¹ä¸€ä¸ªå•ä¸€çš„å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿè½»æ¾æœ‰æ•ˆåœ°æ§åˆ¶æ‰€æœ‰è¿™äº›ä¸åŒçš„ä½“ç°å½¢å¼ï¼Œå¹¶å¯èƒ½é›¶æˆæœ¬æˆ–å°‘æˆæœ¬è½¬ç§»åˆ°çœ‹ä¸è§çš„æœºå™¨äººä½“ç°å½¢å¼ã€‚æˆ‘ä»¬æ¨å‡ºURMAï¼Œå³ç»Ÿä¸€æœºå™¨äººå½¢æ€æ¶æ„ï¼Œä»¥å¼¥è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†ç»ˆç«¯åˆ°ç»ˆç«¯çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ å¸¦åˆ°äº†è…¿éƒ¨æœºå™¨äººé¢†åŸŸï¼Œä½¿å­¦ä¹ åˆ°çš„ç­–ç•¥èƒ½å¤Ÿæ§åˆ¶ä»»ä½•ç±»å‹çš„æœºå™¨äººå½¢æ€ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®æ€æƒ³æ˜¯è®©ç½‘ç»œå­¦ä¹ ä¸€ä¸ªæŠ½è±¡çš„æ­¥æ€æ§åˆ¶å™¨ï¼Œç”±äºæˆ‘ä»¬çš„å½¢æ€æ— å…³ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œè¯¥æ§åˆ¶å™¨å¯ä»¥åœ¨å½¢æ€ä¹‹é—´æ— ç¼å…±äº«ã€‚è¿™ä¸€çµæ´»æ¶æ„å¯ä»¥çœ‹ä½œæ˜¯æ„å»ºè…¿éƒ¨æœºå™¨äººè¿åŠ¨åŸºç¡€æ¨¡å‹çš„ä¸€ä¸ªæ½œåœ¨çš„ç¬¬ä¸€æ­¥ã€‚å®éªŒè¡¨æ˜ï¼ŒURMAå¯ä»¥åœ¨å¤šç§å½¢æ€ä¸Šå­¦ä¹ æ­¥æ€ç­–ç•¥ï¼Œå¹¶èƒ½è½»æ¾åœ°åœ¨ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸­è½¬ç§»åˆ°æœªçŸ¥çš„æœºå™¨äººå¹³å°ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨è…¿éƒ¨è¿åŠ¨æ–¹é¢å–å¾—æœ€æ–°æˆæœã€‚</li>
<li>å­˜åœ¨å¤šç§è…¿éƒ¨å¹³å°ï¼Œä½†ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„å­¦ä¹ æ¡†æ¶æ¥æ§åˆ¶å®ƒä»¬ã€‚</li>
<li>URMAï¼ˆç»Ÿä¸€æœºå™¨äººå½¢æ€æ¶æ„ï¼‰å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚</li>
<li>URMAæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>ç½‘ç»œå­¦ä¹ æŠ½è±¡çš„æ­¥æ€æ§åˆ¶å™¨ï¼Œå¯åœ¨ä¸åŒçš„æœºå™¨äººå½¢æ€ä¹‹é—´æ— ç¼å…±äº«ã€‚</li>
<li>URMAçš„çµæ´»æ¶æ„æ˜¯æ„å»ºè…¿éƒ¨æœºå™¨äººè¿åŠ¨åŸºç¡€æ¨¡å‹çš„åˆæ­¥å°è¯•ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºURMAèƒ½åœ¨å¤šç§å½¢æ€ä¸Šå­¦ä¹ æ­¥æ€ç­–ç•¥ï¼Œå¹¶è½»æ¾è½¬ç§»åˆ°æœªçŸ¥æœºå™¨äººå¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06366">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-51c5d41f1805c61159f59d11384bf3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c5e4a698d12a75b9833104af8f409e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Open-FinLLMs-Open-Multimodal-Large-Language-Models-for-Financial-Applications"><a href="#Open-FinLLMs-Open-Multimodal-Large-Language-Models-for-Financial-Applications" class="headerlink" title="Open-FinLLMs: Open Multimodal Large Language Models for Financial   Applications"></a>Open-FinLLMs: Open Multimodal Large Language Models for Financial   Applications</h2><p><strong>Authors:Jimin Huang, Mengxi Xiao, Dong Li, Zihao Jiang, Yuzhe Yang, Yifei Zhang, Lingfei Qian, Yan Wang, Xueqing Peng, Yang Ren, Ruoyu Xiang, Zhengyu Chen, Xiao Zhang, Yueru He, Weiguang Han, Shunian Chen, Lihang Shen, Daniel Kim, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Guojun Xiong, Zhiwei Liu, Zheheng Luo, Zhiyuan Yao, Ruey-Ling Weng, Meikang Qiu, Kaleb E Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jian-Yun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Qianqian Xie, Sophia Ananiadou, Junichi Tsujii</strong></p>
<p>Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit{Open-FinLLMs}, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE">https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE</a>) and models under OSI-approved licenses. </p>
<blockquote>
<p>é‡‘èLLMï¼ˆå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰åœ¨æ¨è¿›é‡‘èä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸåº”ç”¨æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å—åˆ°æœ‰é™è¯­æ–™åº“ã€å¼±å¤šæ¨¡æ€èƒ½åŠ›å’Œç‹­çª„è¯„ä¼°èŒƒå›´çš„é™åˆ¶ï¼Œä½¿å¾—å®ƒä»¬ä¸å¤ªé€‚åˆå®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†â€Open-FinLLMsâ€ï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æºçš„å¤šæ¨¡æ€é‡‘èLLMï¼Œæ—¨åœ¨å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—å’Œå›¾è¡¨æ•°æ®ç­‰å¤šæ ·ä»»åŠ¡ï¼Œå¹¶åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥å¥—ä»¶åŒ…æ‹¬åœ¨å…¨é¢çš„52äº¿ä»¤ç‰Œè¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„FinLLaMAã€ä½¿ç”¨573Ké‡‘èæŒ‡ä»¤è¿›è¡Œå¾®è°ƒçš„FinLLaMA-Instructä»¥åŠé€šè¿‡143ä¸‡å¯¹å¤šæ¨¡æ€è°ƒä¼˜å¢å¼ºåŠŸèƒ½çš„FinLLaVAï¼Œä»¥å®ç°å¼ºå¤§çš„è·¨æ¨¡æ€æ¨ç†ã€‚æˆ‘ä»¬å…¨é¢è¯„ä¼°äº†Open-FinLLMsåœ¨14ä¸ªé‡‘èä»»åŠ¡ã€30ä¸ªæ•°æ®é›†å’Œ4ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç›‘ç£å¾®è°ƒåœºæ™¯ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒOpen-FinLLMsåœ¨é‡‘èNLPã€å†³ç­–åˆ¶å®šå’Œå¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢ä¼˜äºå…ˆè¿›çš„é‡‘èå’Œé€šç”¨LLMï¼Œå¦‚GPT-4ï¼Œå‡¸æ˜¾äº†å®ƒä»¬è§£å†³ç°å®æŒ‘æˆ˜çš„æ½œåŠ›ã€‚ä¸ºäº†ä¿ƒè¿›å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„åˆ›æ–°å’Œåˆä½œï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ‰€æœ‰ä»£ç ï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE%EF%BC%89%E5%92%8C%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E9%81%B5%E5%BE%AAOSI%E6%89%B9%E5%87%86%E8%AE%B8%E5%8F%AF%E8%AF%81%E3%80%82">https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSEï¼‰å’Œæ¨¡å‹ï¼Œå¹¶éµå¾ªOSIæ‰¹å‡†è®¸å¯è¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11878v2">PDF</a> 33 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>é‡‘èLLMåœ¨æ¨è¿›é‡‘èä»»åŠ¡å’Œç‰¹å®šé¢†åŸŸåº”ç”¨æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œä½†å—é™äºç¨€ç¼ºè¯­æ–™åº“ã€å¼±å¤šæ¨¡æ€èƒ½åŠ›å’Œç‹­çª„è¯„ä¼°èŒƒå›´ï¼Œä¸å¤ªé€‚åˆç°å®ä¸–ç•Œåº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºOpen-FinLLMsï¼Œé¦–ä¸ªå¼€æºå¤šæ¨¡æ€é‡‘èLLMï¼Œå¯å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ã€æ—¶é—´åºåˆ—å’Œå›¾è¡¨æ•°æ®çš„å¤šæ ·åŒ–ä»»åŠ¡ï¼Œåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚åŒ…æ‹¬åŸºäº52äº¿ä»¤ç‰Œè¯­æ–™åº“çš„é¢„è®­ç»ƒFinLLaMAã€ä»¥57.3ä¸‡æ¡é‡‘èæŒ‡ä»¤ç²¾ç»†è°ƒæ•™çš„FinLLaMA-Instructï¼Œä»¥åŠé€šè¿‡143ä¸‡å¯¹å¤šæ¨¡æ€è°ƒä¼˜å¢å¼ºèƒ½åŠ›çš„FinLLaVAã€‚æˆ‘ä»¬å…¨é¢è¯„ä¼°äº†Open-FinLLMsåœ¨14ä¸ªé‡‘èä»»åŠ¡ã€30ä¸ªæ•°æ®é›†å’Œ4ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªæ–°çš„å¤šæ¨¡æ€è¯„ä¼°æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼ŒOpen-FinLLMsåœ¨é‡‘èNLPã€å†³ç­–å’Œå¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢ä¼˜äºGPT-4ç­‰å…ˆè¿›é‡‘èå’Œé€šç”¨LLMï¼Œå±•ç°å‡ºè§£å†³ç°å®æŒ‘æˆ˜çš„å·¨å¤§æ½œåŠ›ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰ä»£ç å’Œæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„åˆ›æ–°å’Œåˆä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èLLMå…·æœ‰æ¨è¿›é‡‘èä»»åŠ¡å’Œé¢†åŸŸç‰¹å®šåº”ç”¨çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰é‡‘èLLMå—é™äºç¨€ç¼ºè¯­æ–™åº“ã€å¼±å¤šæ¨¡æ€èƒ½åŠ›å’Œç‹­çª„è¯„ä¼°ã€‚</li>
<li>Open-FinLLMsæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå…·å¤‡å¤„ç†å¤šæ ·åŒ–ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>Open-FinLLMsåŒ…æ‹¬é¢„è®­ç»ƒçš„FinLLaMAã€ç²¾ç»†è°ƒæ•™çš„FinLLaMA-Instructå’Œå¢å¼ºå¤šæ¨¡æ€æ¨ç†çš„FinLLaVAã€‚</li>
<li>åœ¨å¤šä¸ªé‡‘èä»»åŠ¡å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼ŒOpen-FinLLMsè¡¨ç°å‡ºè¶…è¶Šå…ˆè¿›é‡‘èå’Œé€šç”¨LLMçš„æ½œåŠ›ã€‚</li>
<li>Open-FinLLMsçš„æ¨å‡ºæ—¨åœ¨ä¿ƒè¿›å­¦æœ¯ç•Œå’Œäº§ä¸šç•Œçš„åˆä½œä¸åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e30ed238ac0ba295ce521568ae71898.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa49f929da62393c629f6fb1b495802a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fd0aab99f7fe9d7a6c1f2dbac620edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e9612cc117dbc57d39fa5d3bd741fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aec7904e48a74a229c97ca71091b713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34499c65ed467378adc45c3f2a6ebce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94ef1cad4236f9af17a9b38bb7112334.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Non-Determinism-of-â€œDeterministicâ€-LLM-Settings"><a href="#Non-Determinism-of-â€œDeterministicâ€-LLM-Settings" class="headerlink" title="Non-Determinism of â€œDeterministicâ€ LLM Settings"></a>Non-Determinism of â€œDeterministicâ€ LLM Settings</h2><p><strong>Authors:Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, Breck Baldwin</strong></p>
<p>LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs under settings expected to be deterministic. Yet the questions of how pervasive this is, and with what impact on results, have not to our knowledge been systematically investigated. We investigate non-determinism in five LLMs configured to be deterministic when applied to eight common tasks in across 10 runs, in both zero-shot and few-shot settings. We see accuracy variations up to 15% across naturally occurring runs with a gap of best possible performance to worst possible performance up to 70%. In fact, none of the LLMs consistently delivers repeatable accuracy across all tasks, much less identical output strings. Sharing preliminary results with insiders has revealed that non-determinism perhaps essential to the efficient use of compute resources via co-mingled data in input buffers so this issue is not going away anytime soon. To better quantify our observations, we introduce metrics focused on quantifying determinism, TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement rate of parsed-out answers. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/breckbaldwin/llm-stability">https://github.com/breckbaldwin/llm-stability</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®è·µè€…é€šå¸¸æ³¨æ„åˆ°ï¼Œåœ¨é¢„æœŸä¸ºç¡®å®šæ€§è®¾ç½®çš„æ¡ä»¶ä¸‹ï¼Œç›¸åŒè¾“å…¥çš„è¾“å‡ºäº†å¯èƒ½ä¼šå‘ç”Ÿå˜åŠ¨ã€‚ç„¶è€Œï¼Œè‡³äºè¿™ç§ç°è±¡çš„æ™®éæ€§ä»¥åŠå¯¹ç»“æœçš„å½±å“ç¨‹åº¦ï¼Œæ®æˆ‘ä»¬äº†è§£å°šæœªæœ‰ç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªè¢«é…ç½®ä¸ºç¡®å®šæ€§çš„LLMä¸Šè¿›è¡Œäº†è°ƒæŸ¥ï¼Œè¿™äº›LLMåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹åº”ç”¨äºå…«ä¸ªå¸¸è§ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œäº†åæ¬¡è¿è¡Œã€‚æˆ‘ä»¬çœ‹åˆ°è‡ªç„¶è¿è¡Œä¹‹é—´çš„å‡†ç¡®ç‡å˜åŠ¨é«˜è¾¾15%ï¼Œæœ€ä½³å¯èƒ½æ€§èƒ½ä¸æœ€å·®å¯èƒ½æ€§èƒ½ä¹‹é—´çš„å·®è·é«˜è¾¾70%ã€‚äº‹å®ä¸Šï¼Œæ²¡æœ‰ä»»ä½•ä¸€ä¸ªLLMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½èƒ½æä¾›ä¸€è‡´çš„å‡†ç¡®ç‡ï¼Œæ›´åˆ«æè¾“å‡ºå®Œå…¨ç›¸åŒçš„å­—ç¬¦ä¸²äº†ã€‚ä¸ä¸šå†…ä¸“å®¶çš„åˆæ­¥ç»“æœåˆ†äº«æ˜¾ç¤ºï¼Œéç¡®å®šæ€§å¯¹äºé€šè¿‡è¾“å…¥ç¼“å†²åŒºä¸­çš„æ··åˆæ•°æ®å®ç°è®¡ç®—èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨å¯èƒ½æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› æ­¤è¿™ä¸€é—®é¢˜åœ¨çŸ­æœŸå†…ä¸ä¼šå¾—åˆ°è§£å†³ã€‚ä¸ºäº†æ›´å¥½åœ°é‡åŒ–æˆ‘ä»¬çš„è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“æ³¨äºé‡åŒ–ç¡®å®šæ€§çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬TARr@Nï¼ˆNæ¬¡è¿è¡Œä¸­åŸå§‹è¾“å‡ºçš„æ€»åè®®ç‡ï¼‰å’ŒTARa@Nï¼ˆè§£æå‡ºçš„ç­”æ¡ˆçš„æ€»åè®®ç‡ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/breckbaldwin/llm-stability%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/breckbaldwin/llm-stabilityä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04667v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®è·µä¸­è¡¨ç°å‡ºéç¡®å®šæ€§è¡Œä¸ºï¼Œå¯¹ç›¸åŒè¾“å…¥çš„è¾“å‡ºåœ¨ä¸åŒè¿è¡Œè®¾ç½®ä¸‹ä¼šæœ‰å˜åŒ–ã€‚æœ¬æ–‡å¯¹æ­¤è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ï¼Œåœ¨äº”ä¸ªLLMæ¨¡å‹ä¸­å¯¹å…«ä¸ªå¸¸è§ä»»åŠ¡è¿›è¡Œåæ¬¡è¿è¡Œæµ‹è¯•ï¼Œå‘ç°åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå‡†ç¡®åº¦å˜åŒ–é«˜è¾¾15%ï¼Œæœ€ä½³ä¸æœ€å·®æ€§èƒ½ä¹‹é—´çš„å·®è·æœ€å¤§å¯è¾¾70%ã€‚å…±äº«åˆæ­¥ç»“æœè¡¨æ˜ï¼Œéç¡®å®šæ€§å¯¹äºæœ‰æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºå¯èƒ½æ˜¯å¿…è¦çš„ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸“æ³¨äºé‡åŒ–ç¡®å®šæ€§çš„æŒ‡æ ‡TARr@Nå’ŒTARa@Nï¼Œåˆ†åˆ«ç”¨äºè¡¡é‡åŸå§‹è¾“å‡ºå’Œè§£æç­”æ¡ˆçš„å…±è¯†ç¨‹åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹åœ¨å®è·µä¸­è¡¨ç°å‡ºéç¡®å®šæ€§è¡Œä¸ºï¼Œå³ç›¸åŒè¾“å…¥åœ¨ä¸åŒè¿è¡Œè®¾ç½®ä¸‹ä¼šäº§ç”Ÿä¸åŒçš„è¾“å‡ºã€‚</li>
<li>ç³»ç»Ÿè°ƒæŸ¥äº†äº”ä¸ªLLMæ¨¡å‹åœ¨å…«ä¸ªå¸¸è§ä»»åŠ¡ä¸­çš„éç¡®å®šæ€§è¡¨ç°ï¼Œå‘ç°å‡†ç¡®åº¦å˜åŒ–å¤§ï¼Œæœ€ä½³ä¸æœ€å·®æ€§èƒ½å·®è·æ˜¾è‘—ã€‚</li>
<li>LLMæ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šæ— æ³•ä¿æŒä¸€è‡´çš„å‡†ç¡®æ€§ï¼Œä¸”å¾ˆå°‘äº§ç”Ÿå®Œå…¨ç›¸åŒçš„è¾“å‡ºå­—ç¬¦ä¸²ã€‚</li>
<li>éç¡®å®šæ€§å¯¹äºè®¡ç®—èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨å¯èƒ½æ˜¯å¿…è¦çš„ï¼Œå¯èƒ½ä¸è¾“å…¥ç¼“å†²åŒºä¸­çš„æ··åˆæ•°æ®æœ‰å…³ã€‚</li>
<li>å¼•å…¥TARr@Nå’ŒTARa@NæŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–LLMæ¨¡å‹çš„ç¡®å®šæ€§è¯„ä¼°ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å’Œæ•°æ®å·²å…¬å¼€å¯ç”¨ã€‚</li>
<li>æ­¤é—®é¢˜ï¼ˆéç¡®å®šæ€§ï¼‰åœ¨çŸ­æœŸå†…ä¸ä¼šå¾—åˆ°è§£å†³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7178d8edec69ddb50c4c311912880c58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57ba502db41809b9f4bd595e10100c4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39028cb9ec004d1009befdbd4a0ccf57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f05dcfdf69034867430ae4f7f636bca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ee95d7f91024e5de695549a4b6d9e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3865210646e8acdf7b122781f6e5d086.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FsPONER-Few-shot-Prompt-Optimization-for-Named-Entity-Recognition-in-Domain-specific-Scenarios"><a href="#FsPONER-Few-shot-Prompt-Optimization-for-Named-Entity-Recognition-in-Domain-specific-Scenarios" class="headerlink" title="FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in   Domain-specific Scenarios"></a>FsPONER: Few-shot Prompt Optimization for Named Entity Recognition in   Domain-specific Scenarios</h2><p><strong>Authors:Yongjian Tang, Rakebul Hasan, Thomas Runkler</strong></p>
<p>Large Language Models (LLMs) have provided a new pathway for Named Entity Recognition (NER) tasks. Compared with fine-tuning, LLM-powered prompting methods avoid the need for training, conserve substantial computational resources, and rely on minimal annotated data. Previous studies have achieved comparable performance to fully supervised BERT-based fine-tuning approaches on general NER benchmarks. However, none of the previous approaches has investigated the efficiency of LLM-based few-shot learning in domain-specific scenarios. To address this gap, we introduce FsPONER, a novel approach for optimizing few-shot prompts, and evaluate its performance on domain-specific NER datasets, with a focus on industrial manufacturing and maintenance, while using multiple LLMs â€“ GPT-4-32K, GPT-3.5-Turbo, LLaMA 2-chat, and Vicuna. FsPONER consists of three few-shot selection methods based on random sampling, TF-IDF vectors, and a combination of both. We compare these methods with a general-purpose GPT-NER method as the number of few-shot examples increases and evaluate their optimal NER performance against fine-tuned BERT and LLaMA 2-chat. In the considered real-world scenarios with data scarcity, FsPONER with TF-IDF surpasses fine-tuned models by approximately 10% in F1 score. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡æä¾›äº†æ–°çš„é€”å¾„ã€‚ä¸å¾®è°ƒç›¸æ¯”ï¼ŒLLMé©±åŠ¨çš„æç¤ºæ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå¯ä»¥èŠ‚çœå¤§é‡è®¡ç®—èµ„æºï¼Œå¹¶ä¾èµ–æå°‘é‡çš„æ³¨é‡Šæ•°æ®ã€‚ä¹‹å‰çš„ç ”ç©¶å·²åœ¨é€šç”¨NERåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸å®Œå…¨ç›‘ç£çš„BERTå¾®è°ƒæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»¥å‰çš„ä»»ä½•æ–¹æ³•éƒ½æ²¡æœ‰ç ”ç©¶LLMåœ¨ç‰¹å®šé¢†åŸŸçš„å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ•ˆç‡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FsPONERï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–å°‘æ ·æœ¬æç¤ºçš„æ–°æ–¹æ³•ï¼Œå¹¶è¯„ä¼°å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„NERæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œé‡ç‚¹å…³æ³¨å·¥ä¸šåˆ¶é€ å’Œç»´æŠ¤é¢†åŸŸï¼ŒåŒæ—¶ä½¿ç”¨å¤šä¸ªLLMâ€”â€”GPT-4-32Kã€GPT-3.5 Turboã€LLaMA 2èŠå¤©å’ŒVicunaã€‚FsPONERåŒ…æ‹¬ä¸‰ç§åŸºäºéšæœºé‡‡æ ·ã€TF-IDFå‘é‡å’Œä¸¤è€…ç»„åˆçš„å°‘æ ·æœ¬é€‰æ‹©æ–¹æ³•ã€‚æˆ‘ä»¬éšç€å°‘æ ·æœ¬ç¤ºä¾‹æ•°é‡çš„å¢åŠ ï¼Œå°†è¿™äº›æ–¹æ³•ä¸é€šç”¨GPT-NERæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯„ä¼°å…¶æœ€ä½³NERæ€§èƒ½ä¸å¾®è°ƒBERTå’ŒLLaMA 2èŠå¤©çš„æ€§èƒ½ã€‚åœ¨æ•°æ®ç¨€ç¼ºçš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œä½¿ç”¨TF-IDFçš„FsPONERåœ¨F1åˆ†æ•°ä¸Šè¶…è¶Šäº†å¾®è°ƒæ¨¡å‹çº¦10%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.08035v2">PDF</a> accepted in the main track at the 27th European Conference on   Artificial Intelligence (ECAI-2024)</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œé‡‡ç”¨æç¤ºæ–¹æ³•è€Œæ— éœ€è®­ç»ƒï¼ŒèŠ‚çœäº†è®¡ç®—èµ„æºï¼Œå¹¶ä¾èµ–å°‘é‡æ ‡æ³¨æ•°æ®ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºLLMçš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•FsPONERï¼Œå¹¶é‡ç‚¹å¯¹å·¥ä¸šåˆ¶é€ å’Œç»´æŠ¤é¢†åŸŸçš„ç‰¹å®šæ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚FsPONERåŒ…å«ä¸‰ç§åŸºäºéšæœºé‡‡æ ·ã€TF-IDFå‘é‡å’Œä¸¤è€…ç»“åˆçš„å°‘æ ·æœ¬é€‰æ‹©æ–¹æ³•ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„ç°å®ä¸­è¶…è¶Šå¾®è°ƒBERTæ¨¡å‹çº¦10%çš„F1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsä¸ºNERä»»åŠ¡æä¾›äº†æ–°çš„é€”å¾„ï¼Œé‡‡ç”¨æç¤ºæ–¹æ³•æ— éœ€è®­ç»ƒï¼ŒèŠ‚çœè®¡ç®—èµ„æºã€‚</li>
<li>FsPONERæ˜¯ä¸€ç§æ–°å‹çš„åŸºäºLLMçš„å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–NERä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>FsPONERåœ¨ç‰¹å®šé¢†åŸŸæ•°æ®é›†ï¼ˆå¦‚å·¥ä¸šåˆ¶é€ å’Œç»´æŠ¤ï¼‰ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>FsPONERåŒ…å«ä¸‰ç§å°‘æ ·æœ¬é€‰æ‹©æ–¹æ³•ï¼šéšæœºé‡‡æ ·ã€TF-IDFå‘é‡å’Œä¸¤è€…çš„ç»„åˆã€‚</li>
<li>åœ¨æŸäº›åœºæ™¯ä¸‹ï¼ŒFsPONERè¶…è¶Šå¾®è°ƒBERTæ¨¡å‹çº¦10%çš„F1åˆ†æ•°ã€‚</li>
<li>ä½¿ç”¨å¤šä¸ªLLMsï¼ˆå¦‚GPT-4-32Kã€GPT-3.5-Turboã€LLaMA 2-chatå’ŒVicunaï¼‰è¿›è¡Œè¯„ä¼°ï¼Œæ˜¾ç¤ºLLMsåœ¨NERä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.08035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2ca0650a75a7f2299f1a896b5e5e1b01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fe19fa6605fbb1ffb967afb10eca039.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ec2e877584eb5a2e98da6221f578622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87036e19cd03921b9702b0a13b9d4300.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6868cb4486a6678ac1790149545339fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-821f376586c2575bf1fbfa4f094a84dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-591339d7e845bbbaf3257162ec2b182c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1c38a41951bd2b08896e40c6b6b751.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Induction-Heads-as-an-Essential-Mechanism-for-Pattern-Matching-in-In-context-Learning"><a href="#Induction-Heads-as-an-Essential-Mechanism-for-Pattern-Matching-in-In-context-Learning" class="headerlink" title="Induction Heads as an Essential Mechanism for Pattern Matching in   In-context Learning"></a>Induction Heads as an Essential Mechanism for Pattern Matching in   In-context Learning</h2><p><strong>Authors:Joy Crosbie, Ekaterina Shutova</strong></p>
<p>Large language models (LLMs) have shown a remarkable ability to learn and perform complex tasks through in-context learning (ICL). However, a comprehensive understanding of its internal mechanisms is still lacking. This paper explores the role of induction heads in a few-shot ICL setting. We analyse two state-of-the-art models, Llama-3-8B and InternLM2-20B on abstract pattern recognition and NLP tasks. Our results show that even a minimal ablation of induction heads leads to ICL performance decreases of up to ~32% for abstract pattern recognition tasks, bringing the performance close to random. For NLP tasks, this ablation substantially decreases the modelâ€™s ability to benefit from examples, bringing few-shot ICL performance close to that of zero-shot prompts. We further use attention knockout to disable specific induction patterns, and present fine-grained evidence for the role that the induction mechanism plays in ICL. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å­¦ä¹ å’Œæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹å…¶å†…éƒ¨æœºåˆ¶çš„å…¨é¢ç†è§£ä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡æ¢è®¨äº†å½’çº³å¤´åœ¨å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ è®¾ç½®ä¸­çš„è§’è‰²ã€‚æˆ‘ä»¬åˆ†æäº†ä¸¤ç§æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå³Llama-3-8Bå’ŒInternLM2-20Båœ¨æŠ½è±¡æ¨¡å¼è¯†åˆ«å’ŒNLPä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å°çš„å½’çº³å¤´æ¶ˆèä¹Ÿä¼šå¯¼è‡´æŠ½è±¡æ¨¡å¼è¯†åˆ«ä»»åŠ¡çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ä¸‹é™é«˜è¾¾çº¦32%ï¼Œå°†æ€§èƒ½é™ä½åˆ°æ¥è¿‘éšæœºæ°´å¹³ã€‚å¯¹äºNLPä»»åŠ¡ï¼Œè¿™ç§æ¶ˆèå¤§å¤§é™ä½äº†æ¨¡å‹ä»ä¾‹å­ä¸­å—ç›Šçš„èƒ½åŠ›ï¼Œä½¿å¾—å°‘é‡çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½æ¥è¿‘äºé›¶æ ·æœ¬æç¤ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨æ³¨æ„åŠ›æ¶ˆé™¤æ³•æ¥ç¦ç”¨ç‰¹å®šçš„å½’çº³æ¨¡å¼ï¼Œå¹¶ä¸ºå½’çº³æœºåˆ¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„ä½œç”¨æä¾›ç²¾ç»†çš„è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07011v3">PDF</a> 9 pages, 7 figures; Code link added</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å±•ç°å‡ºäº†ä»¤äººç©ç›®çš„å­¦ä¹ å’Œæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶ä»ç¼ºä¹å…¨é¢ç†è§£ã€‚æœ¬æ–‡åœ¨å°‘æ ·æœ¬ICLç¯å¢ƒä¸­ï¼Œæ¢è®¨äº†å½’çº³å¤´çš„ä½œç”¨ã€‚æˆ‘ä»¬å¯¹ä¸¤ç§æœ€æ–°æ¨¡å‹Llama-3-8Bå’ŒInternLM2-20Båœ¨æŠ½è±¡æ¨¡å¼è¯†åˆ«å’ŒNLPä»»åŠ¡ä¸Šè¿›è¡Œäº†åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå³ä½¿è¿›è¡Œå°‘é‡çš„å½’çº³å¤´æ¶ˆèä¹Ÿä¼šå¯¼è‡´ICLæ€§èƒ½ä¸‹é™çº¦è‡³åŸæ¥çš„æœ€å¤§è¾¾çº¦è¾¾32%ï¼Œä¸¥é‡å½±å“æŠ½è±¡æ¨¡å¼è¯†åˆ«ä»»åŠ¡çš„æ€§èƒ½ã€‚å¯¹äºNLPä»»åŠ¡ï¼Œæ¶ˆèä½¿æ¨¡å‹å‡ ä¹ä¸§å¤±ä»ç¤ºä¾‹ä¸­å—ç›Šçš„èƒ½åŠ›ï¼Œä½¿å°‘æ ·æœ¬ICLæ€§èƒ½æ¥è¿‘é›¶æ ·æœ¬æç¤ºæ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨æ³¨æ„åŠ›å‰”é™¤æŠ€æœ¯æ¥ç¦ç”¨ç‰¹å®šçš„å½’çº³æ¨¡å¼ï¼Œå¹¶ä¸ºå½’çº³æœºåˆ¶åœ¨ICLä¸­çš„è§’è‰²æä¾›äº†ç²¾ç»†çš„è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„å­¦ä¹ å’Œæ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å½’çº³å¤´åœ¨LLMsçš„ICLä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å¯¹å½’çº³å¤´è¿›è¡Œæ¶ˆèä¼šå¯¼è‡´LLMsåœ¨æŠ½è±¡æ¨¡å¼è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸‹é™é«˜è¾¾çº¦32%ã€‚</li>
<li>å¯¹äºNLPä»»åŠ¡ï¼Œæ¶ˆèå½’çº³å¤´ä¼šä½¿æ¨¡å‹ä¸§å¤±ä»ç¤ºä¾‹ä¸­å­¦ä¹ çš„èƒ½åŠ›ï¼Œä½¿å°‘æ ·æœ¬ICLæ€§èƒ½æ¥è¿‘é›¶æ ·æœ¬æç¤ºæ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›å‰”é™¤æŠ€æœ¯ï¼Œå¯ä»¥ç¦ç”¨ç‰¹å®šçš„å½’çº³æ¨¡å¼ã€‚</li>
<li>è®ºæ–‡ä¸ºå½’çº³æœºåˆ¶åœ¨ICLä¸­çš„è§’è‰²æä¾›äº†ç²¾ç»†çš„è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-73c681ed6c799a8902c0af88ee810b0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015847748a3ec7f38cceae23c6f17ec0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-095b6e529312fbcb97d72c343346bab7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e33cbce6f40491999f5f77ad8e1bbf98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216f6d799d22c42694fa4a50630d0bd0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CoMM-A-Coherent-Interleaved-Image-Text-Dataset-for-Multimodal-Understanding-and-Generation"><a href="#CoMM-A-Coherent-Interleaved-Image-Text-Dataset-for-Multimodal-Understanding-and-Generation" class="headerlink" title="CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal   Understanding and Generation"></a>CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal   Understanding and Generation</h2><p><strong>Authors:Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, Long Chen</strong></p>
<p>Interleaved image-text generation has emerged as a crucial multimodal task, aiming at creating sequences of interleaved visual and textual content given a query. Despite notable advancements in recent multimodal large language models (MLLMs), generating integrated image-text sequences that exhibit narrative coherence and entity and style consistency remains challenging due to poor training data quality. To address this gap, we introduce CoMM, a high-quality Coherent interleaved image-text MultiModal dataset designed to enhance the coherence, consistency, and alignment of generated multimodal content. Initially, CoMM harnesses raw data from diverse sources, focusing on instructional content and visual storytelling, establishing a foundation for coherent and consistent content. To further refine the data quality, we devise a multi-perspective filter strategy that leverages advanced pre-trained models to ensure the development of sentences, consistency of inserted images, and semantic alignment between them. Various quality evaluation metrics are designed to prove the high quality of the filtered dataset. Meanwhile, extensive few-shot experiments on various downstream tasks demonstrate CoMMâ€™s effectiveness in significantly enhancing the in-context learning capabilities of MLLMs. Moreover, we propose four new tasks to evaluate MLLMsâ€™ interleaved generation abilities, supported by a comprehensive evaluation framework. We believe CoMM opens a new avenue for advanced MLLMs with superior multimodal in-context learning and understanding ability. </p>
<blockquote>
<p>äº¤äº’å¼å›¾åƒæ–‡æœ¬ç”Ÿæˆå·²ç»æˆä¸ºä¸€é¡¹å…³é”®çš„å¤šæ¨¡æ€ä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯æ ¹æ®æŸ¥è¯¢ç”Ÿæˆäº¤æ›¿å‡ºç°çš„è§†è§‰å’Œæ–‡æœ¬å†…å®¹åºåˆ—ã€‚å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ç”Ÿæˆå…·æœ‰å™äº‹è¿è´¯æ€§å’Œå®ä½“åŠé£æ ¼ä¸€è‡´æ€§çš„é›†æˆå›¾åƒæ–‡æœ¬åºåˆ—ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºè®­ç»ƒæ•°æ®è´¨é‡å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†CoMMï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€è¿è´¯äº¤äº’å¼å›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆçš„å¤šæ¨¡æ€å†…å®¹çš„ä¸€è‡´æ€§ã€è¿è´¯æ€§å’Œå¯¹é½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10462v3">PDF</a> 22 pages, Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCoMMçš„é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¤šåª’ä½“æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”¨äºæé«˜ç”Ÿæˆå¤šåª’ä½“å†…å®¹çš„è¿è´¯æ€§ã€ä¸€è‡´æ€§å’Œå¯¹é½æ€§ã€‚é€šè¿‡ä»å„ç§èµ„æºä¸­è·å–åŸå§‹æ•°æ®ï¼Œå¹¶ä¸“æ³¨äºæŒ‡ä»¤å†…å®¹å’Œè§†è§‰å™äº‹ï¼Œå»ºç«‹è¿è´¯å’Œä¸€è‡´å†…å®¹çš„åŸºç¡€ã€‚é‡‡ç”¨å¤šè§†è§’è¿‡æ»¤ç­–ç•¥ï¼Œåˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ç¡®ä¿å¥å­å‘å±•ã€æ’å…¥å›¾åƒçš„ä¸€è‡´æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚ç»è¿‡è´¨é‡è¯„ä¼°è¯æ˜ï¼Œè¿‡æ»¤åçš„æ•°æ®é›†è´¨é‡é«˜ã€‚æ­¤å¤–ï¼Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å°‘é‡æ ·æœ¬å®éªŒè¯æ˜äº†CoMMåœ¨æé«˜å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæå‡ºäº†å››ä¸ªæ–°ä»»åŠ¡æ¥è¯„ä¼°å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤é”™ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ç”±ç»¼åˆè¯„ä¼°æ¡†æ¶æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoMMæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„å›¾åƒæ–‡æœ¬å¤šåª’ä½“æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆå¤šåª’ä½“å†…å®¹çš„è¿è´¯æ€§ã€ä¸€è‡´æ€§å’Œå¯¹é½æ€§ã€‚</li>
<li>è¯¥æ•°æ®é›†ä»å„ç§èµ„æºè·å–åŸå§‹æ•°æ®ï¼Œä¸“æ³¨äºæŒ‡ä»¤å†…å®¹å’Œè§†è§‰å™äº‹ã€‚</li>
<li>é‡‡ç”¨å¤šè§†è§’è¿‡æ»¤ç­–ç•¥ï¼Œåˆ©ç”¨å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè´¨é‡æå‡ã€‚</li>
<li>è¿‡æ»¤åçš„æ•°æ®é›†ç»è¿‡è´¨é‡è¯„ä¼°è¯æ˜è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å°‘é‡æ ·æœ¬å®éªŒè¯æ˜äº†CoMMçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºäº†å››ä¸ªæ–°ä»»åŠ¡æ¥è¯„ä¼°å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤é”™ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.10462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a026ea4741403a3453c41bb401d549e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a523c5d785c1c508e2bb462651972eb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f01793bec58c0b001ce864e134922eb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-627d5c4cf51e678fa9b50a44943b2198.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Consistency-Guided-Asynchronous-Contrastive-Tuning-for-Few-Shot-Class-Incremental-Tuning-of-Foundation-Models"><a href="#Consistency-Guided-Asynchronous-Contrastive-Tuning-for-Few-Shot-Class-Incremental-Tuning-of-Foundation-Models" class="headerlink" title="Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot   Class-Incremental Tuning of Foundation Models"></a>Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot   Class-Incremental Tuning of Foundation Models</h2><p><strong>Authors:Shuvendu Roy, Elham Dolatabadi, Arash Afkanpour, Ali Etemad</strong></p>
<p>We propose Consistency-guided Asynchronous Contrastive Tuning (CoACT), a novel method for continuously tuning foundation models to learn new classes in few-shot settings. CoACT consists of three key components:(i) asynchronous contrastive tuning, which learns new classes by including LoRA modules in the pre-trained encoder while enforcing consistency between two asynchronous encoders; (ii) controlled fine-tuning, which facilitates effective tuning of a subset of the foundation model; and (iii) consistency-guided incremental tuning, which enforces additional regularization during later sessions to reduce forgetting of the learned classes. We evaluate our proposed solution on Few-Shot Class-Incremental Learning (FSCIL) as well as a new and more challenging setup called Few-Shot Class-Incremental Tuning (FSCIT), which facilitates the continual tuning of vision foundation models to learn new classes with only a few samples per class. Unlike traditional FSCIL, FSCIT does not require a large in-distribution base session for initial fully supervised training prior to the incremental few-shot sessions. We conduct extensive evaluations across 16 diverse datasets, demonstrating the effectiveness of CoACT in both FSCIL and FSCIT setups. CoACT outperforms existing methods by up to 5.02% in FSCIL and up to 12.51% in FSCIT for individual datasets, with an average improvement of 2.47%. Furthermore, CoACT exhibits reduced forgetting and enhanced robustness in low-shot experiments. Detailed ablation and sensitivity studies highlight the contribution of each component of CoACT. We make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL">https://github.com/ShuvenduRoy/CoACT-FSCIL</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåä¸ºä¸€è‡´æ€§å¼•å¯¼å¼‚æ­¥å¯¹æ¯”è°ƒä¼˜ï¼ˆCoACTï¼‰ï¼Œç”¨äºåœ¨å°‘é‡æ ·æœ¬è®¾ç½®ä¸‹è¿ç»­è°ƒæ•´åŸºç¡€æ¨¡å‹ä»¥å­¦ä¹ æ–°ç±»åˆ«ã€‚CoACTåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š(i) å¼‚æ­¥å¯¹æ¯”è°ƒä¼˜ï¼Œé€šè¿‡åœ¨é¢„è®­ç»ƒç¼–ç å™¨ä¸­åŒ…å«LoRAæ¨¡å—ï¼ŒåŒæ—¶å¼ºåˆ¶ä¸¤ä¸ªå¼‚æ­¥ç¼–ç å™¨ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæ¥å­¦ä¹ æ–°ç±»åˆ«ï¼›(ii) å—æ§å¾®è°ƒï¼Œä¾¿äºæœ‰æ•ˆåœ°è°ƒæ•´åŸºç¡€æ¨¡å‹çš„ä¸€ä¸ªå­é›†ï¼›(iii) ä¸€è‡´æ€§å¼•å¯¼å¢é‡è°ƒä¼˜ï¼Œåœ¨åç»­ä¼šè¯æœŸé—´å¼ºåˆ¶æ‰§è¡Œé¢å¤–çš„æ­£åˆ™åŒ–ï¼Œä»¥å‡å°‘å·²å­¦ä¹ ç±»åˆ«çš„é—å¿˜ã€‚æˆ‘ä»¬åœ¨å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä»¥åŠç§°ä¸ºå°‘é‡ç±»åˆ«å¢é‡è°ƒæ•´ï¼ˆFSCITï¼‰çš„æ–°é¢–ä¸”æ›´å…·æŒ‘æˆ˜æ€§çš„è®¾ç½®ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„è§£å†³æ–¹æ¡ˆã€‚FSCITä¿ƒè¿›äº†è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åªæœ‰æ¯ä¸ªç±»åˆ«å‡ ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ä¸æ–­å­¦ä¹ æ–°ç±»åˆ«ã€‚ä¸ä¼ ç»Ÿçš„FSCILä¸åŒï¼ŒFSCITä¸éœ€è¦åœ¨å¢é‡å°‘é‡ä¼šè¯ä¹‹å‰è¿›è¡Œå¤§è§„æ¨¡å†…éƒ¨åŸºç¡€ä¼šè¯çš„åˆå§‹å®Œå…¨ç›‘ç£è®­ç»ƒã€‚æˆ‘ä»¬åœ¨16ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œè¯æ˜äº†CoACTåœ¨FSCILå’ŒFSCITè®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨FSCILä¸­ï¼ŒCoACTå°†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½æé«˜äº†é«˜è¾¾5.02%ï¼Œåœ¨FSCITä¸­æé«˜äº†é«˜è¾¾12.51%ï¼Œé’ˆå¯¹å•ä¸ªæ•°æ®é›†çš„å¹³å‡æ”¹è¿›ä¸º2.47%ã€‚æ­¤å¤–ï¼ŒCoACTåœ¨ä½æ ·æœ¬å®éªŒä¸­è¡¨ç°å‡ºå‡å°‘é—å¿˜å’Œå¢å¼ºç¨³å¥æ€§çš„ç‰¹ç‚¹ã€‚è¯¦ç»†çš„æ¶ˆèå’Œæ•æ„Ÿæ€§ç ”ç©¶çªå‡ºäº†CoACTæ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL">https://github.com/ShuvenduRoy/CoACT-FSCIL</a> å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16625v2">PDF</a> Accepted in Transactions on Machine Learning Research (TMLR)</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºConsistency-guided Asynchronous Contrastive Tuningï¼ˆCoACTï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹è¿ç»­è°ƒæ•´åŸºç¡€æ¨¡å‹ä»¥å­¦ä¹ æ–°ç±»åˆ«ã€‚CoACTåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆiï¼‰å¼‚æ­¥å¯¹æ¯”è°ƒæ•´ï¼Œé€šè¿‡åŒ…å«LoRAæ¨¡å—åœ¨é¢„è®­ç»ƒç¼–ç å™¨ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶å¼ºåˆ¶ä¸¤ä¸ªå¼‚æ­¥ç¼–ç å™¨ä¹‹é—´çš„ä¸€è‡´æ€§ï¼›ï¼ˆiiï¼‰å—æ§å¾®è°ƒï¼Œä¿ƒè¿›åŸºç¡€æ¨¡å‹å­é›†çš„æœ‰æ•ˆè°ƒæ•´ï¼›ï¼ˆiiiï¼‰ä¸€è‡´æ€§å¼•å¯¼å¢é‡è°ƒæ•´ï¼Œåœ¨åç»­ä¼šè¯æœŸé—´å¼ºåˆ¶æ‰§è¡Œé™„åŠ æ­£åˆ™åŒ–ï¼Œä»¥å‡å°‘å·²å­¦ä¹ ç±»åˆ«çš„é—å¿˜ã€‚æœ¬ç ”ç©¶åœ¨å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä»¥åŠç§°ä¸ºå°‘é‡ç±»åˆ«å¢é‡è°ƒæ•´ï¼ˆFSCITï¼‰çš„æ–°æŒ‘æˆ˜è®¾ç½®ä¸Šè¯„ä¼°äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåè€…ä¿ƒè¿›äº†è§†è§‰åŸºç¡€æ¨¡å‹è¿ç»­è°ƒæ•´ä»¥å­¦ä¹ æ–°ç±»åˆ«ï¼Œå¹¶ä¸”æ¯ç±»ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬ã€‚ä¸ä¼ ç»ŸFSCILä¸åŒï¼ŒFSCITä¸éœ€è¦å¤§é‡å†…éƒ¨åŸºæœ¬ä¼šè¯è¿›è¡Œåˆå§‹å®Œå…¨ç›‘ç£è®­ç»ƒï¼Œå†è¿›è¡Œå¢é‡å°‘é‡ä¼šè¯ã€‚åœ¨16ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCoACTåœ¨FSCILå’ŒFSCITè®¾ç½®ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ•ˆæœã€‚åœ¨FSCILä¸­ï¼ŒCoACTè¾ƒç°æœ‰æ–¹æ³•æœ€å¤šé«˜å‡º5.02%ï¼Œåœ¨FSCITä¸­æœ€å¤šé«˜å‡º12.51%ã€‚æ€»ä½“è€Œè¨€ï¼ŒCoACTå‡å°‘äº†é—å¿˜å¹¶åœ¨ä½æ ·æœ¬å®éªŒä¸­è¡¨ç°å‡ºå¢å¼ºçš„ç¨³å¥æ€§ã€‚è¯¦ç»†çš„æ¶ˆèå’Œæ•æ„Ÿæ€§ç ”ç©¶çªå‡ºäº†CoACTæ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL%E3%80%82">https://github.com/ShuvenduRoy/CoACT-FSCILã€‚</a></p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>CoACTæ˜¯ä¸€ç§ç”¨äºåœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹è¿ç»­è°ƒæ•´åŸºç¡€æ¨¡å‹å­¦ä¹ æ–°ç±»åˆ«çš„æ–°æ–¹æ³•ã€‚</li>
<li>CoACTåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå¼‚æ­¥å¯¹æ¯”è°ƒæ•´ã€å—æ§å¾®è°ƒã€ä¸€è‡´æ€§å¼•å¯¼å¢é‡è°ƒæ•´ã€‚</li>
<li>CoACTåœ¨FSCILå’ŒFSCITä¸¤ç§è®¾ç½®ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶è¡¨ç°å‡ºå“è¶Šçš„æ•ˆæœã€‚</li>
<li>CoACTåœ¨FSCILå’ŒFSCITä¸­éƒ½è¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ï¼Œå¹³å‡æé«˜äº†2.47%ã€‚</li>
<li>CoACTå‡å°‘äº†é—å¿˜å¹¶åœ¨ä½æ ·æœ¬å®éªŒä¸­è¡¨ç°å‡ºå¢å¼ºçš„ç¨³å¥æ€§ã€‚</li>
<li>å…¬å¼€çš„ä»£ç åœ°å€æ˜¯<a target="_blank" rel="noopener" href="https://github.com/ShuvenduRoy/CoACT-FSCIL%E3%80%82">https://github.com/ShuvenduRoy/CoACT-FSCILã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc67cf47cb925dc4a543fc2d2d38a91a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-238502985f1e8c5476ce96a3237f0d26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-412ee5146513d678245a74d463d6953d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning"><a href="#VL-ICL-Bench-The-Devil-in-the-Details-of-Multimodal-In-Context-Learning" class="headerlink" title="VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"></a>VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning</h2><p><strong>Authors:Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</strong></p>
<p>Large language models (LLMs) famously exhibit emergent in-context learning (ICL) â€“ the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the modelâ€™s weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä¸€ç§ç§°ä¸ºæƒ…å¢ƒæ¶Œç°å­¦ä¹ ï¼ˆICLï¼‰çš„èƒ½åŠ›ï¼Œå³ä½¿ç”¨å°‘é‡ç¤ºä¾‹å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œè€Œæ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ã€‚å»ºç«‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸Šçš„è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰åœ¨è¯†åˆ«ã€æ¨ç†å’Œæ¥åœ°ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹å¤šæ¨¡æ€ICLçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç”Ÿæˆç­‰æ–¹é¢ï¼Œè¿™äº›é¢†åŸŸå¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨ICLçš„ä¼˜åŠ¿ï¼Œä¹Ÿæ²¡æœ‰æµ‹è¯•å…¶å±€é™æ€§ã€‚å¤šæ¨¡æ€ICLçš„æ›´å¹¿æ³›çš„èƒ½åŠ›å’Œå±€é™æ€§å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•VL-ICL Benchï¼Œç”¨äºå¤šæ¨¡æ€æƒ…å¢ƒå­¦ä¹ ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠå›¾åƒå’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºï¼Œä»¥åŠä»æ„ŸçŸ¥åˆ°æ¨ç†å’Œé•¿è¯­å¢ƒé•¿åº¦çš„ä¸åŒç±»å‹æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç”¨è¿™ä¸ªåŸºå‡†æµ‹è¯•å¥—ä»¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„VLLMçš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„å„ç§ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œå¹¶è¡¨æ˜å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4ï¼Œä¹Ÿä¼šå‘ç°è¿™äº›ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é€šè¿‡çªå‡ºä¸€ç³»åˆ—æ–°çš„ICLä»»åŠ¡ä»¥åŠç°æœ‰æ¨¡å‹çš„ç›¸å…³ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®é›†å°†æ¿€å‘æœªæ¥å¯¹å¢å¼ºVLLMæƒ…å¢ƒå­¦ä¹ èƒ½åŠ›çš„ç ”ç©¶ï¼Œå¹¶æ¿€å‘åˆ©ç”¨VLLM ICLçš„æ–°åº”ç”¨ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ys-zong/VL-ICL">https://github.com/ys-zong/VL-ICL</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13164v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºæ–°å…´çš„åœ¨æƒ…å¢ƒå†…å­¦ä¹ ï¼ˆICLï¼‰èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å°‘é‡ç¤ºä¾‹è¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰åœ¨è¯†åˆ«ã€æ¨ç†å’Œæ¥åœ°ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€ICLçš„æ¢ç©¶ä¸»è¦é›†ä¸­åœ¨å°‘æ•°è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä¸Šï¼Œè¿™å¹¶æœªå……åˆ†åˆ©ç”¨ICLçš„ä¼˜åŠ¿ï¼Œä¹Ÿæ²¡æœ‰æµ‹è¯•å…¶å±€é™æ€§ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æƒ…å¢ƒå†…å­¦ä¹ åŸºå‡†VL-ICL Benchï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—æ¶‰åŠå›¾åƒå’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºçš„ä»»åŠ¡ï¼Œä»¥åŠä»æ„ŸçŸ¥åˆ°æ¨ç†å’Œé•¿è¯­å¢ƒé•¿åº¦çš„ä¸åŒç±»å‹æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›VLLMsçš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„å„ç§ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œå¹¶æ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4ï¼Œä¹Ÿä¼šå‘ç°è¿™äº›ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒ…å¢ƒå†…å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½é€šè¿‡å°‘é‡ç¤ºä¾‹è¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ã€æ¨ç†å’Œæ¥åœ°ç­‰é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€ICLç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä¸Šï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ICLçš„ä¼˜åŠ¿å’Œæµ‹è¯•å…¶å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æƒ…å¢ƒå†…å­¦ä¹ åŸºå‡†VL-ICL Benchï¼Œæ¶µç›–å¹¿æ³›çš„ä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºï¼Œæ¶‰åŠä¸åŒç±»å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°äº†æœ€å…ˆè¿›VLLMsçš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†å…¶ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚</li>
<li>ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4ï¼Œåœ¨é¢å¯¹è¿™äº›æ–°ä»»åŠ¡æ—¶ä»ä¼šé¢ä¸´æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b999c539bc388e044c297bd9cc8f8c98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615437abc2988c94e3bfd80abd3eedc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75bfc5198c7b62621bde4c7858123edc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02ef11c2143c9259c5c08e36eac8cbab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e860e2d71c7734fe3e5ee796c28356f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-04/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ea006b54cbff3ce97de70c07152c8b38.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Vision-RWKV Efficient and Scalable Visual Perception with RWKV-Like   Architectures
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5965dd9f74dff732e9f8ee7471fb097c.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  GECKO Gigapixel Vision-Concept Contrastive Pretraining in   Histopathology
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
