<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-04-04  Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-018de950e3915df3e363e45a3d79498c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks"><a href="#Are-you-really-listening-Boosting-Perceptual-Awareness-in-Music-QA-Benchmarks" class="headerlink" title="Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks"></a>Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks</h2><p><strong>Authors:Yongyi Zang, Sean O’Brien, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack</strong></p>
<p>Large Audio Language Models (LALMs), where pretrained text LLMs are finetuned with audio input, have made remarkable progress in music understanding. However, current evaluation methodologies exhibit critical limitations: on the leading Music Question Answering benchmark, MuchoMusic, text-only LLMs without audio perception capabilities achieve surprisingly high accuracy of up to 56.4%, on par or above most LALMs. Furthermore, when presented with random Gaussian noise instead of actual audio, LALMs still perform significantly above chance. These findings suggest existing benchmarks predominantly assess reasoning abilities rather than audio perception. To overcome this challenge, we present RUListening: Robust Understanding through Listening, a framework that enhances perceptual evaluation in Music-QA benchmarks. We introduce the Perceptual Index (PI), a quantitative metric that measures a question’s reliance on audio perception by analyzing log probability distributions from text-only language models. Using this metric, we generate synthetic, challenging distractors to create QA pairs that necessitate genuine audio perception. When applied to MuchoMusic, our filtered dataset successfully forces models to rely on perceptual information-text-only LLMs perform at chance levels, while LALMs similarly deteriorate when audio inputs are replaced with noise. These results validate our framework’s effectiveness in creating benchmarks that more accurately evaluate audio perception capabilities. </p>
<blockquote>
<p>音频大语言模型（LALMs）在音乐理解方面取得了显著进展，这些模型是通过在预训练的文本大型语言模型（LLMs）上进行微调，以音频输入进行训练。然而，现有的评估方法存在重要局限性：在领先的Music Question Answering基准测试MuchoMusic上，没有音频感知能力的纯文本LLMs竟然达到了高达56.4%的惊人准确率，与大多数LALM持平或更高。此外，当面对随机高斯噪声而非实际音频时，LALMs的表现仍然显著超过随机水平。这些发现表明，现有的基准测试主要评估的是推理能力，而不是音频感知能力。为了应对这一挑战，我们提出了RUListening：通过倾听实现稳健理解，这是一个增强音乐问答基准测试中感知评估的框架。我们引入了感知指数（PI）这一量化指标，它通过分析纯文本语言模型的日志概率分布来衡量问题对音频感知的依赖程度。使用这一指标，我们生成了合成挑战干扰项，以创建需要真正音频感知的问答对。在MuchoMusic上的应用显示，我们的过滤数据集成功迫使模型依赖感知信息——纯文本LLMs的表现达到了随机水平，而当将LALM的音频输入替换为噪声时，其表现也大幅下降。这些结果验证了我们框架在创建更准确地评估音频感知能力的基准测试中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00369v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模音频语言模型（LALMs）在音乐理解方面取得了显著进展，但现有评估方法存在严重局限性。研究表明，在没有音频感知能力的纯文本LLMs上，现有的音乐问答基准测试如MuchoMusic的准确率令人惊讶地高达56.4%，与许多LALMs不相上下或更高。当面对随机高斯噪声时，LALMs的表现仍然显著高于平均水平。这暗示现有的基准测试主要评估的是推理能力而非音频感知能力。为此，我们提出了RUListening框架，通过听力增强音乐问答基准测试的感知评估。我们引入了感知指数（PI）这一量化指标，通过分析纯文本语言模型的日志概率分布来衡量问题对音频感知的依赖程度。利用这一指标，我们生成了需要真实音频感知的QA对作为合成挑战干扰项。在MuchoMusic的应用中，我们的筛选数据集成功迫使模型依赖于感知信息——纯文本LLMs的表现达到了机会水平，而LALMs在音频输入被替换为噪声时也同样表现不佳。这验证了我们框架在创建更准确评估音频感知能力的基准测试中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LALMs在音乐理解方面取得显著进展，但现有评估方法存在局限性。</li>
<li>纯文本LLMs在Music Question Answering基准测试上的表现令人惊讶。</li>
<li>现有基准测试主要评估的是推理能力而非音频感知能力。</li>
<li>引入RUListening框架和Perceptual Index（PI）指标以加强音乐问答基准测试中的感知评估。</li>
<li>利用PI指标生成合成挑战干扰项以提高评估准确性。</li>
<li>在筛选数据集中，纯文本LLMs的表现达到机会水平，说明它们不依赖音频感知。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3f244078c20c8addb5f35f5894c1170.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-894bc19d39dfb80370cdbe57f7aa0100.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4887062fdf95aebc8fd2536c4d5d3d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c98d6f92555a3768667bad5ebcd8682.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7122aa8bcee9d851402f42e29c1ddd58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce319e27432554d5fd748b69aeadb690.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DeepDubber-V1-Towards-High-Quality-and-Dialogue-Narration-Monologue-Adaptive-Movie-Dubbing-Via-Multi-Modal-Chain-of-Thoughts-Reasoning-Guidance"><a href="#DeepDubber-V1-Towards-High-Quality-and-Dialogue-Narration-Monologue-Adaptive-Movie-Dubbing-Via-Multi-Modal-Chain-of-Thoughts-Reasoning-Guidance" class="headerlink" title="DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue   Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance"></a>DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue   Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance</h2><p><strong>Authors:Junjie Zheng, Zihao Chen, Chaofan Ding, Xinhan Di</strong></p>
<p>Current movie dubbing technology can generate the desired voice from a given speech prompt, ensuring good synchronization between speech and visuals while accurately conveying the intended emotions. However, in movie dubbing, key aspects such as adapting to different dubbing styles, handling dialogue, narration, and monologue effectively, and understanding subtle details like the age and gender of speakers, have not been well studied. To address this challenge, we propose a framework of multi-modal large language model. First, it utilizes multimodal Chain-of-Thought (CoT) reasoning methods on visual inputs to understand dubbing styles and fine-grained attributes. Second, it generates high-quality dubbing through large speech generation models, guided by multimodal conditions. Additionally, we have developed a movie dubbing dataset with CoT annotations. The evaluation results demonstrate a performance improvement over state-of-the-art methods across multiple datasets. In particular, for the evaluation metrics, the SPK-SIM and EMO-SIM increases from 82.48% to 89.74%, 66.24% to 78.88% for dubbing setting 2.0 on V2C Animation dataset, LSE-D and MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 for dubbing setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to 83.42 and WER decreases from 52.69% to 23.20% for initial reasoning setting on proposed CoT-Movie-Dubbing dataset in the comparison with the state-of-the art models. </p>
<blockquote>
<p>当前的电影配音技术可以根据给定的语音提示生成所需的声音，确保语音和视觉之间的良好同步，同时准确传达预期的情绪。然而，在电影配音中，如何适应不同的配音风格、有效处理对话、旁白和独白，以及理解如说话人的年龄和性别等细微细节等方面尚未得到很好的研究。为了应对这一挑战，我们提出了一个多模态大语言模型框架。首先，它利用视觉输入的链式思维（Chain-of-Thought，CoT）推理方法，理解配音风格和精细属性。其次，通过大型语音生成模型生成高质量的配音，由多模态条件引导。此外，我们还开发了一个带有CoT注释的电影配音数据集。评估结果表明，与最先进的方法相比，我们在多个数据集上的性能有所提高。具体而言，对于评估指标，V2C Animation数据集的配音设置2.0中，SPK-SIM和EMO-SIM分别从82.48%提高到89.74%，从66.24%提高到78.88%；Grid数据集的配音设置2.0中，LSE-D和MCD-SL分别从14.79降至到14.63，从5.24降至到4.74；与最先进模型相比，在提出的CoT-Movie-Dubbing数据集上进行初步推理设置时，SPK-SIM从64.03%提高到83.42%，WER从52.69%降至到23.20%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23660v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>摘要</strong></p>
<p>当前电影配音技术可从给定的语音提示生成所需的语音，确保语音与视觉之间的良好同步，同时准确传达预期的情绪。然而，在电影配音中，如何适应不同的配音风格、有效处理对话、旁白和独白，以及理解如说话人的年龄和性别等细微之处尚未得到很好的研究。为应对这一挑战，我们提出了多模态大型语言模型框架。首先，它利用视觉输入的链式思维（CoT）推理方法，理解配音风格和精细属性。其次，通过大型语音生成模型生成高质量配音，受多模态条件的引导。此外，我们还开发了带有CoT注释的电影配音数据集。评估结果表明，与最新方法相比，我们的方法在多个数据集上的表现有所提高。特别是在评估指标方面，V2C动画数据集的说话人相似性（SPK-SIM）和情感相似性（EMO-SIM）分别从82.48%提高到89.74%，66.24%提高到78.88%；Grid数据集的说话人相似性（LSE-D）和语调相似性（MCD-SL）分别从从降低至的对比实验中在提出的数据集上的说话人相似性（SPK-SIM）从原来的提升至，单词错误率（WER）从原来的降至。</p>
<p><strong>关键见解</strong></p>
<p>一、当前电影配音技术已能够同步语音和视觉，准确传达情绪。但仍需研究如何适应不同的配音风格和处理对话、旁白和独白等。<br>二、提出了多模态大型语言模型框架来解决上述问题，它利用视觉输入的链式思维（CoT）推理方法来理解配音风格和精细属性。<br>三、通过大型语音生成模型生成高质量配音，此过程受多模态条件引导。<br>四、开发了带有CoT注释的电影配音数据集以促进研究。<br>五、与现有方法相比，该框架在多个数据集上的表现有所提升。<br>六、在V2C动画数据集上的实验结果显示，SPK-SIM和EMO-SIM指标有明显提升。<br>七、在Grid数据集上的实验结果显示说话人相似性和语调相似性有所改善。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23660">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fc5ee49ef0b6f667d475d787613844d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42464c87df81b2e865fcb29544188de6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f89e56211abb82d4a8eb3e75f076e215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-862807fe09cf0fc2741ea16e70ca8a2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-820ea6cba6d2d3300f25867b3bd41411.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Integrating-Large-Language-Models-For-Monte-Carlo-Simulation-of-Chemical-Reaction-Networks"><a href="#Integrating-Large-Language-Models-For-Monte-Carlo-Simulation-of-Chemical-Reaction-Networks" class="headerlink" title="Integrating Large Language Models For Monte Carlo Simulation of Chemical   Reaction Networks"></a>Integrating Large Language Models For Monte Carlo Simulation of Chemical   Reaction Networks</h2><p><strong>Authors:Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, Vaghawan Ojha</strong></p>
<p>Chemical reaction network is an important method for modeling and exploring complex biological processes, bio-chemical interactions and the behavior of different dynamics in system biology. But, formulating such reaction kinetics takes considerable time. In this paper, we leverage the efficiency of modern large language models to automate the stochastic monte carlo simulation of chemical reaction networks and enable the simulation through the reaction description provided in the form of natural languages. We also integrate this process into widely used simulation tool Copasi to further give the edge and ease to the modelers and researchers. In this work, we show the efficacy and limitations of the modern large language models to parse and create reaction kinetics for modelling complex chemical reaction processes. </p>
<blockquote>
<p>化学反应网络是模拟和探索复杂生物过程、生物化学交互和系统生物学中不同动态行为的重要方法。但是，构建这样的反应动力学需要花费大量时间。在本文中，我们利用现代大型语言模型的效率，自动进行化学反应网络的随机蒙特卡洛模拟，并通过自然语言形式提供的反应描述来实现模拟。我们还将该过程整合到广泛使用的模拟工具Copasi中，以便为建模人员和研究人员提供优势和便利。在这项工作中，我们展示了现代大型语言模型在解析和创建反应动力学以模拟复杂的化学反应过程中的效果和局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21178v1">PDF</a> Accepted on MadeAI 2025 Conference</p>
<p><strong>Summary</strong><br>化学反应网络是系统生物学中模拟和探索复杂生物过程、生物化学相互作用以及不同动力学行为的重要方法，但构建反应动力学需要耗费大量时间。本研究利用现代大型语言模型的效率，实现了化学反应网络的随机蒙特卡洛模拟的自动化，并通过自然语言形式提供的反应描述进行模拟。同时，研究将这一过程整合到广泛使用的模拟工具Copasi中，为建模人员和研究人员提供了优势和便利。本研究展示了现代大型语言模型在解析和创建反应动力学以模拟复杂化学反应过程的效果和局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>化学反应网络是模拟复杂生物过程的关键方法。</li>
<li>利用现代大型语言模型可自动化化学反应网络的蒙特卡洛模拟。</li>
<li>自然语言的反应描述可用于实现模拟。</li>
<li>将该过程整合到Copasi等模拟工具中，为建模和研究提供了便利。</li>
<li>大型语言模型在解析和创建反应动力学方面表现出效果和局限性。</li>
<li>此方法提高了模拟复杂化学反应过程的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0a22983696be9477538a365a12ed6596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f985b98e1c0252e1ae46f0ba31203740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2b7f9c702565be49365b292216ec788.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a947db43d28fe761180c92c9a156df5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-208b93fcbfdac29f2292073dc75ed1ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f576e97911c89bcec3f541befb9f41f6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="StreamMind-Unlocking-Full-Frame-Rate-Streaming-Video-Dialogue-through-Event-Gated-Cognition"><a href="#StreamMind-Unlocking-Full-Frame-Rate-Streaming-Video-Dialogue-through-Event-Gated-Cognition" class="headerlink" title="StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through   Event-Gated Cognition"></a>StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through   Event-Gated Cognition</h2><p><strong>Authors:Xin Ding, Hao Wu, Yifan Yang, Shiqi Jiang, Donglin Bai, Zhibo Chen, Ting Cao</strong></p>
<p>With the rise of real-world human-AI interaction applications, such as AI assistants, the need for Streaming Video Dialogue is critical. To address this need, we introduce StreamMind, a video LLM framework that achieves ultra-FPS streaming video processing (100 fps on a single A100) and enables proactive, always-on responses in real time, without explicit user intervention.   To solve the key challenge of the contradiction between linear video streaming speed and quadratic transformer computation cost, we propose a novel perception-cognition interleaving paradigm named ‘’event-gated LLM invocation’’, in contrast to the existing per-time-step LLM invocation. By introducing a Cognition Gate network between the video encoder and the LLM, LLM is only invoked when relevant events occur. To realize the event feature extraction with constant cost, we propose Event-Preserving Feature Extractor (EPFE) based on state-space method, generating a single perception token for spatiotemporal features. These techniques enable the video LLM with full-FPS perception and real-time cognition response.   Experiments on Ego4D and SoccerNet streaming tasks, as well as standard offline benchmarks, demonstrate state-of-the-art performance in both model capability and real-time efficiency, paving the way for ultra-high-FPS applications, such as Game AI and interactive media. The code and data is available at <a target="_blank" rel="noopener" href="https://aka.ms/StreamMind">https://aka.ms/StreamMind</a>. </p>
<blockquote>
<p>随着人工智能助手等现实世界人机交互应用的兴起，对流式视频对话的需求变得至关重要。为了应对这一需求，我们推出了StreamMind，这是一款视频LLM框架，可实现超FPS流式视频处理（单个A100上可达100帧&#x2F;秒），并可在无需用户明确干预的情况下，实时主动响应。</p>
</blockquote>
<p>为了解决线性视频流速度与传播器计算成本之间的主要矛盾，我们提出了一种名为“事件门控LLM调用”的新型感知认知交错范式，这与现有的按时间步长LLM调用形成对比。通过在视频编码器和LLM之间引入认知门网络，只有在相关事件发生时才会调用LLM。为了实现恒定成本的事件特征提取，我们基于状态空间方法提出了事件保留特征提取器（EPFE），为时空特征生成单个感知令牌。这些技术使视频LLM具备全FPS感知和实时认知响应能力。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06220v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着人工智能助手等现实世界中人机交互应用的出现，对Streaming Video Dialogue的需求愈发重要。为应对这一需求，我们推出StreamMind，一种视频LLM框架，可实现超高帧率（Ultra-FPS）的视频处理（在单个A100上可达100帧&#x2F;秒），无需用户干预即可实现主动实时响应。为解决线性视频流速度与二次方转换器计算成本之间的主要矛盾，我们提出了一种名为“事件门控LLM调用”的新型感知认知交替范式，与现有的按时间步长LLM调用形成对比。通过在视频编码器和LLM之间引入认知门网络，仅在发生相关事件时才调用LLM。为实现恒定成本的事件特征提取，我们基于状态空间方法提出了事件保留特征提取器（EPFE），为时空特征生成单个感知令牌。这些技术使视频LLM具备全帧率感知和实时认知响应能力。在Ego4D、SoccerNet流媒体任务以及标准离线基准测试上的实验证明了其在模型能力和实时效率方面的卓越性能，为超高帧率应用如游戏AI和交互式媒体铺平了道路。相关代码和数据可通过<a target="_blank" rel="noopener" href="https://aka.ms/StreamMind%E8%AE%BF%E9%97%AE%E3%80%82">https://aka.ms/StreamMind访问。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>StreamMind是一个视频LLM框架，支持超高帧率（Ultra-FPS）的视频处理，实现实时主动响应。</li>
<li>引入了一种新型感知认知交替范式——“事件门控LLM调用”，以提高效率。</li>
<li>通过在视频编码器和LLM之间添加认知门网络，仅在有相关事件发生时才调用LLM。</li>
<li>提出了事件保留特征提取器（EPFE），以恒定成本实现事件特征提取。</li>
<li>StreamMind可实现全帧率感知和实时认知响应的视频LLM。</li>
<li>在多个流媒体任务和标准离线基准测试上表现出卓越性能。</li>
<li>代码和数据可通过<a target="_blank" rel="noopener" href="https://aka.ms/StreamMind%E8%AE%BF%E9%97%AE%EF%BC%8C%E4%B8%BA%E8%B6%85%E9%AB%98%E5%B8%A7%E7%8E%87%E5%BA%94%E7%94%A8%E5%A6%82%E6%B8%B8%E6%88%8FAI%E5%92%8C%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%AA%92%E4%BD%93%E6%8F%90%E4%BE%9B%E4%BA%86%E5%8F%AF%E8%83%BD%E3%80%82">https://aka.ms/StreamMind访问，为超高帧率应用如游戏AI和交互式媒体提供了可能。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-31f16313e09db10aa620456dc0e7a7ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-256aef52eb786d6f7eba53fc8197aab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eeb115a5203f28e0dbc4b903ecc95cf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4365d3f35c533643161154d734be9aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fcbb9d2d7eeb9a2ea5b44ccce00a126.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a102c40fc07abe5e8f07d5023053f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51b4063d3a52b916e7cbc63094b6274e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c1fd060dfe8d17a63331c806ea89c54.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CosForce-A-Force-Based-General-Pedestrian-Model-with-Anticipation-and-Reaction-Mechanisms"><a href="#CosForce-A-Force-Based-General-Pedestrian-Model-with-Anticipation-and-Reaction-Mechanisms" class="headerlink" title="CosForce: A Force-Based General Pedestrian Model with Anticipation and   Reaction Mechanisms"></a>CosForce: A Force-Based General Pedestrian Model with Anticipation and   Reaction Mechanisms</h2><p><strong>Authors:Jinghui Wang, Wei Lv, Shuchao Cao, Chenglin Guo</strong></p>
<p>In this study, we developed a force-based general pedestrian model named CosForce. To the best of our knowledge, this may represent the simplest version of the force-based method. The model employs cosine functions to characterize asymmetric interactions, implicitly incorporating anticipation and reaction mechanisms. By focusing on binary interactions, the CosForce model provides new insights into pedestrian modeling while achieving linear computational complexity. Two specific scenarios in crowd dynamics were analyzed: self-organization (entropy decrease) and crowd collapse (entropy increase). The average normalized speed and order parameter were introduced to quantitatively describe the processes of crowd dynamics. Quantitative evaluations demonstrate that phase separation in crowds is effectively reproduced by the model, including lane formation, stripe formation, and cross-channel formation. Next, in the simulation of mass gathering, within a density-accumulating scenario, processes of critical phase transition in high-density crowds are clearly revealed through time series observations of the order parameter. These findings provide valuable insights into crowd dynamics. </p>
<blockquote>
<p>在这项研究中，我们开发了一种基于力的通用行人模型，名为CosForce。据我们所知，这可能是基于力方法中最简单的版本。该模型采用余弦函数来表征不对称相互作用，隐含地融入了预期和反应机制。通过关注二元交互，CosForce模型在行人建模方面提供了新的见解，同时实现了线性计算复杂度。分析了人群动力学中的两个特定场景：自组织（熵减少）和人群拥挤（熵增加）。引入平均归一化速度和秩序参数来定量描述人群动力学的过程。定量评估表明，该模型有效地再现了人群中的相分离，包括车道形成、条纹形成和跨通道形成。接下来，在密度累积场景中模拟大规模人群聚集的过程时，通过秩序参数的时间序列观察，清晰地揭示了高密度人群中的临界相变过程。这些发现对人群动力学提供了有价值的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10746v3">PDF</a> 28 pages, 25 figures</p>
<p><strong>Summary</strong></p>
<p>本研究开发了一种基于力的一般行人模型——CosForce。据我们所知，这可能是基于力的方法中最简单的版本。该模型采用余弦函数来表征不对称相互作用，隐含地融入了预期和反应机制。通过关注二元交互，CosForce模型为行人建模提供了新的见解，同时实现了线性计算复杂性。分析了人群动力学的两个特定场景：自组织（熵减少）和人群崩溃（熵增加）。引入平均归一化速度和秩序参数来定量描述人群动力学过程。定量评估表明，该模型有效地再现了人群中的相位分离，包括车道形成、条纹形成和跨通道形成。在模拟密度累积场景的大规模聚集时，通过秩序参数的时间序列观察，清晰地揭示了高密度人群中的临界相变过程。这些发现对人群动力学提供了有价值的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开发了名为CosForce的基于力的通用行人模型，可能是基于力的方法中最简单的版本。</li>
<li>该模型采用余弦函数表征不对称相互作用，包含预期和反应机制。</li>
<li>CosForce模型关注二元交互，为行人建模提供新见解，实现线性计算复杂性。</li>
<li>分析了人群动力学的两个特定场景：自组织和人群崩溃（熵的增加和减少）。</li>
<li>通过平均归一化速度和秩序参数定量描述人群动力学过程。</li>
<li>模型有效再现人群中的相位分离，包括车道、条纹和跨通道的形成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-040aec2eb602781b544a3273a7437a6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdb9b644dff6bef3595f2bfa7ec0d7d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0d3ec3aeec1f6c5e34245db144fd58e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e022a304cd0ae847b30d8606bff0aaf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880b1f42119d94834bc3610ce646bce6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-End-to-End-Model-for-Photo-Sharing-Multi-modal-Dialogue-Generation"><a href="#An-End-to-End-Model-for-Photo-Sharing-Multi-modal-Dialogue-Generation" class="headerlink" title="An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation"></a>An End-to-End Model for Photo-Sharing Multi-modal Dialogue Generation</h2><p><strong>Authors:Peiming Guo, Sinuo Liu, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang</strong></p>
<p>Photo-Sharing Multi-modal dialogue generation requires a dialogue agent not only to generate text responses but also to share photos at the proper moment. Using image text caption as the bridge, a pipeline model integrates an image caption model, a text generation model, and an image generation model to handle this complex multi-modal task. However, representing the images with text captions may loss important visual details and information and cause error propagation in the complex dialogue system. Besides, the pipeline model isolates the three models separately because discrete image text captions hinder end-to-end gradient propagation. We propose the first end-to-end model for photo-sharing multi-modal dialogue generation, which integrates an image perceptron and an image generator with a large language model. The large language model employs the Q-Former to perceive visual images in the input end. For image generation in the output end, we propose a dynamic vocabulary transformation matrix and use straight-through and gumbel-softmax techniques to align the large language model and stable diffusion model and achieve end-to-end gradient propagation. We perform experiments on PhotoChat and DialogCC datasets to evaluate our end-to-end model. Compared with pipeline models, the end-to-end model gains state-of-the-art performances on various metrics of text and image generation. More analysis experiments also verify the effectiveness of the end-to-end model for photo-sharing multi-modal dialogue generation. </p>
<blockquote>
<p>图片共享多模态对话生成要求对话代理不仅仅生成文本响应，还要在适当的时刻共享照片。以图像文本标题作为桥梁，管道模型将图像标题模型、文本生成模型和图像生成模型集成在一起，以处理这个复杂的多模态任务。然而，用文本标题表示图像可能会丢失重要的视觉细节和信息，并在复杂对话系统中引起误差传播。此外，管道模型将这三个模型分开处理，因为离散的图像文本标题阻碍了端到端的梯度传播。我们提出了第一个针对图片共享多模态对话生成端到端模型，该模型将图像感知器和图像生成器与大型语言模型集成在一起。大型语言模型采用Q-Former来感知输入端的视觉图像。在输出端进行图像生成时，我们提出了动态词汇转换矩阵，并使用直通和gumbel-softmax技术对齐大型语言模型和稳定扩散模型，实现端到端梯度传播。我们在PhotoChat和DialogCC数据集上对我们的端到端模型进行了实验评估。与管道模型相比，端到端模型在文本和图像生成的各项指标上达到了最先进的性能。更多的分析实验也验证了端到端模型在图片共享多模态对话生成中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08650v2">PDF</a> Accepted by ICME2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了首个端到端的照片共享多模态对话生成模型，该模型集成了图像感知器、图像生成器与大型语言模型。相较于使用图像文本描述作为桥梁的管道模型，该端到端模型能更好地处理图像与文本的融合，避免了信息损失和误差传播问题。通过Q-Former感知视觉图像，输出端采用动态词汇转换矩阵，结合直通和Gumbel-softmax技术，实现大型语言模型和稳定扩散模型的对接，达到端到端梯度传播。实验证明，该模型在PhotoChat和DialogCC数据集上取得了最先进的性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态对话生成需要对话代理不仅生成文本响应，还需在适当时候分享照片。</li>
<li>管道模型通过图像文本描述来整合模型，但可能丢失视觉细节和信息，并导致误差传播。</li>
<li>首次提出端到端的照片共享多模态对话生成模型，集成了图像感知器、图像生成器与大型语言模型。</li>
<li>采用Q-Former作为大型语言模型的一部分，以感知视觉图像。</li>
<li>在输出端采用动态词汇转换矩阵，结合直通和Gumbel-softmax技术，实现端到端梯度传播。</li>
<li>模型在PhotoChat和DialogCC数据集上表现优异，达到 state-of-the-art 的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-dbb46f7a081ec2ce10fb2cbc2f1b2c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edfcf7a3dfba741f02a6fa40ecc615d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-018de950e3915df3e363e45a3d79498c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f31eb855558ef8f2d8df6944c307e237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5279ee584426b2976515283ab8f0428f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Safety-Aware-Human-Lead-Vehicle-Platooning-by-Proactively-Reacting-to-Uncertain-Human-Behaving"><a href="#Safety-Aware-Human-Lead-Vehicle-Platooning-by-Proactively-Reacting-to-Uncertain-Human-Behaving" class="headerlink" title="Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to   Uncertain Human Behaving"></a>Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to   Uncertain Human Behaving</h2><p><strong>Authors:Jia Hu, Shuhan Wang, Yiming Zhang, Haoran Wang, Zhilong Liu, Guangzhi Cao</strong></p>
<p>Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation. By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making. However, state-of-the-art HL-CACC technology still has a great limitation on driving safety due to the lack of considering the leading human driver’s uncertain behavior. In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC). It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV). The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficiency for real-time implementation. The proposed controller is evaluated on a PreScan&amp;Simulink simulation platform. Real vehicle trajectory data is collected for the calibration of the simulation. Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brakes; iii) is confirmed with string stability. The computation time is approximately 3.2 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU. This indicates the proposed controller is ready for real-time implementation. </p>
<blockquote>
<p>人类引导的合作式自适应巡航控制（HL-CACC）被视为一种具有前景的车辆编队技术，在实际应用中具有广阔前景。通过利用人为驾驶车辆（HV）作为车队领导者，HL-CACC降低了感知和决策的成本，并增强了其可靠性。然而，目前最先进的HL-CACC技术由于在考虑领先的人为驾驶员的不确定性行为方面的缺失，在驾驶安全方面仍存在很大的局限性。本研究设计了一种基于随机模型预测控制（SMPC）的HL-CACC控制器。它能够预测领先的人车互联车辆（CHV）的驾驶意图。所设计的控制器具有以下特点：一、在振荡交通中增强了感知安全性；二、确保应对急刹时的安全；三、实时实现计算效率高。该控制器在PreScan＆Simulink仿真平台上进行了评估。收集真实车辆轨迹数据对仿真进行校准。结果表明，该控制器：一、在振荡交通中提高了感知安全性达19.17%；二、在应对急刹时提高了实际安全性达7.76%；三、具有串稳定性。在配备Intel i5-13500H CPU的笔记本电脑上运行时，计算时间约为3.2毫秒。这表明该控制器已准备好进行实时应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07556v3">PDF</a> </p>
<p><strong>Summary</strong><br>     基于随机模型预测控制（SMPC）设计的人领航协同自适应巡航控制（HL-CACC）控制器，能预测领先的人驾驶车辆的驾驶意图，提高感知安全、保障行车安全，适合实时应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HL-CACC被视为具有前景的车辆编队技术，利用人工驾驶车辆作为车队领导者，降低成本并增强感知和决策可靠性。</li>
<li>当前HL-CACC技术在驾驶安全上存在局限性，因为未考虑领先的人驾驶行为的不确定性。</li>
<li>基于SMPC设计的HL-CACC控制器能预测领先的人驾驶车辆的驾驶意图。</li>
<li>该控制器增强了在振荡交通中的感知安全、保障了对急刹车的安全，并具备实时实施的计算效率。</li>
<li>在PreScan&amp;Simulink仿真平台上评估该控制器，利用实际车辆轨迹数据进行校准。</li>
<li>结果显示，该控制器提高了感知安全19.17%，实际安全对抗急刹车提高7.76%，并具备队列稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07556">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-03c119da6e840c14f7bb21bcec6cf260.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-291a7737b02746de5de67cca80548f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3ba8317b8be8f31bdfc0ea8618b95f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32b85910a96109a378086c60e7752b9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01eaed5d7f8d8bd95635e7a9b4a6a76b.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-04-04  Monocular and Generalizable Gaussian Talking Head Animation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f576e97911c89bcec3f541befb9f41f6.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-04-04  Are you really listening? Boosting Perceptual Awareness in Music-QA   Benchmarks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
