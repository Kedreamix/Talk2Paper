<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0cb003298d1d12e0a7c9c740f48cff60.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis"><a href="#Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis" class="headerlink" title="Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis"></a>Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis</h2><p><strong>Authors:Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar</strong></p>
<p>Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins. </p>
<blockquote>
<p>è¿‘æœŸï¼Œåœ¨3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹é¢çš„è¿›å±•ï¼Œåœ¨å®æ—¶3Dé‡å»ºå’Œæ–°å‹è§†å›¾åˆæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤§å‹ã€æ— çº¦æŸçš„ç¯å¢ƒä¸­è¡¨ç°è¾ƒå·®ï¼Œå…¶ä¸­ç¨€ç–å’Œä¸å‡åŒ€çš„è¾“å…¥è¦†ç›–ã€çŸ­æš‚çš„é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œä¸ä¸€è‡´çš„ç›¸æœºè®¾ç½®å¯¼è‡´è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†GS-Diffï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤šè§†å›¾æ‰©æ•£æ¨¡å‹å¼•å¯¼çš„æ–°å‹3DGSæ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›å±€é™æ€§ã€‚é€šè¿‡ç”Ÿæˆä»¥å¤šè§†å›¾è¾“å…¥ä¸ºæ¡ä»¶çš„ä¼ªè§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†çº¦æŸä¸è¶³çš„3Dé‡å»ºé—®é¢˜è½¬åŒ–ä¸ºå®šä½æ˜ç¡®çš„é—®é¢˜ï¼Œå³ä½¿åœ¨ç¨€ç–æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥ä¼˜åŒ–ã€‚GS-Diffè¿˜èåˆäº†å¤šé¡¹å¢å¼ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•ç›®æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å„å‘å¼‚æ€§æ­£åˆ™åŒ–å’Œå…ˆè¿›çš„æ¸²æŸ“æŠ€æœ¯ï¼Œä»¥åº”å¯¹ç°å®ç¯å¢ƒä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-Diffå§‹ç»ˆæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01960v1">PDF</a> WACV ULTRRA Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>3DGSä¸NeRFçš„æœ€æ–°è¿›å±•åœ¨å®æ—¶3Dé‡å»ºå’Œæ–°å‹è§†è§’åˆæˆæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ä½†åœ¨å¤§è§„æ¨¡ã€æ— çº¦æŸçš„ç¯å¢ƒä¸­ï¼Œè¿™äº›æ–¹æ³•åœ¨é¢ä¸´ç¨€ç–å’Œä¸å‡åŒ€è¾“å…¥è¦†ç›–ã€ä¸´æ—¶é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œä¸ä¸€è‡´çš„ç›¸æœºè®¾ç½®ç­‰é—®é¢˜æ—¶è¡¨ç°æ¬ ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºGS-Diffï¼Œä¸€ç§å—å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¼•å¯¼çš„æ–°å‹3DGSæ¡†æ¶ã€‚é€šè¿‡ç”ŸæˆåŸºäºå¤šè§†è§’è¾“å…¥çš„ä¼ªè§‚å¯Ÿå€¼ï¼ŒGS-Diffå°†æ¬ çº¦æŸçš„3Dé‡å»ºé—®é¢˜è½¬åŒ–ä¸ºè‰¯å¥½çš„é—®é¢˜ï¼Œå³ä½¿åœ¨ç¨€ç–æ•°æ®æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒGS-Diffè¿˜åŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•çœ¼æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å¼‚å‘æ€§æ­£åˆ™åŒ–ä»¥åŠå…ˆè¿›çš„æ …æ ¼åŒ–æŠ€æœ¯ç­‰å¢å¼ºåŠŸèƒ½ï¼Œä»¥åº”å¯¹ç°å®åœºæ™¯ä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚åœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-Diffæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSä¸NeRFåœ¨å®æ—¶3Dé‡å»ºå’Œè§†è§’åˆæˆä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡ã€æ— çº¦æŸç¯å¢ƒä¸­ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–å’Œä¸å‡åŒ€çš„è¾“å…¥è¦†ç›–ã€ä¸´æ—¶é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œç›¸æœºè®¾ç½®ä¸ä¸€è‡´ç­‰ã€‚</li>
<li>GS-Diffæ˜¯ä¸€ç§æ–°å‹3DGSæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆä¼ªè§‚å¯Ÿå€¼è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå°†æ¬ çº¦æŸçš„3Dé‡å»ºè½¬åŒ–ä¸ºè‰¯å¥½çº¦æŸçš„é—®é¢˜ã€‚</li>
<li>GS-Diffé‡‡ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¼•å¯¼ï¼Œå®ç°å³ä½¿åœ¨ç¨€ç–æ•°æ®ä¸‹çš„ç¨³å¥ä¼˜åŒ–ã€‚</li>
<li>GS-Diffé›†æˆäº†å¤šç§å¢å¼ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•çœ¼æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å¼‚å‘æ€§æ­£åˆ™åŒ–å’Œå…ˆè¿›çš„æ …æ ¼åŒ–æŠ€æœ¯ï¼Œä»¥åº”å¯¹ç°å®åœºæ™¯ä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚</li>
<li>GS-Diffåœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1ce0624dfdfb12bb64cdccf46f365b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41d8bfdbe85e56ceb5eb98088339d893.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GaussianLSS-â€“-Toward-Real-world-BEV-Perception-Depth-Uncertainty-Estimation-via-Gaussian-Splatting"><a href="#GaussianLSS-â€“-Toward-Real-world-BEV-Perception-Depth-Uncertainty-Estimation-via-Gaussian-Splatting" class="headerlink" title="GaussianLSS â€“ Toward Real-world BEV Perception: Depth Uncertainty   Estimation via Gaussian Splatting"></a>GaussianLSS â€“ Toward Real-world BEV Perception: Depth Uncertainty   Estimation via Gaussian Splatting</h2><p><strong>Authors:Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen</strong></p>
<p>Birdâ€™s-eye view (BEV) perception has gained significant attention because it provides a unified representation to fuse multiple view images and enables a wide range of down-stream autonomous driving tasks, such as forecasting and planning. Recent state-of-the-art models utilize projection-based methods which formulate BEV perception as query learning to bypass explicit depth estimation. While we observe promising advancements in this paradigm, they still fall short of real-world applications because of the lack of uncertainty modeling and expensive computational requirement. In this work, we introduce GaussianLSS, a novel uncertainty-aware BEV perception framework that revisits unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm, and enhances them with depth un-certainty modeling. GaussianLSS represents spatial dispersion by learning a soft depth mean and computing the variance of the depth distribution, which implicitly captures object extents. We then transform the depth distribution into 3D Gaussians and rasterize them to construct uncertainty-aware BEV features. We evaluate GaussianLSS on the nuScenes dataset, achieving state-of-the-art performance compared to unprojection-based methods. In particular, it provides significant advantages in speed, running 2.5x faster, and in memory efficiency, using 0.3x less memory compared to projection-based methods, while achieving competitive performance with only a 0.4% IoU difference. </p>
<blockquote>
<p>é¸Ÿç°è§†å›¾ï¼ˆBEVï¼‰æ„ŸçŸ¥å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒä¸ºå¤šè§†å›¾å›¾åƒèåˆæä¾›äº†ç»Ÿä¸€è¡¨ç¤ºï¼Œå¹¶èƒ½å¤Ÿå®ç°å¤šç§ä¸‹æ¸¸è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ï¼Œå¦‚é¢„æµ‹å’Œè§„åˆ’ã€‚æœ€è¿‘çš„æœ€å…ˆè¿›æ¨¡å‹é‡‡ç”¨åŸºäºæŠ•å½±çš„æ–¹æ³•ï¼Œå°†BEVæ„ŸçŸ¥åˆ¶å®šä¸ºæŸ¥è¯¢å­¦ä¹ ï¼Œä»¥ç»•è¿‡æ˜ç¡®çš„æ·±åº¦ä¼°è®¡ã€‚è™½ç„¶æˆ‘ä»¬åœ¨è¿™ä¸€èŒƒå¼ä¸­çœ‹åˆ°äº†æœ‰å‰æ™¯çš„è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶å› ä¸ºç¼ºä¹ä¸ç¡®å®šæ€§å»ºæ¨¡å’Œæ˜‚è´µçš„è®¡ç®—è¦æ±‚è€Œéš¾ä»¥åº”ç”¨äºç°å®ä¸–ç•Œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†GaussianLSSï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥BEVæ„ŸçŸ¥æ¡†æ¶ï¼Œå®ƒé‡æ–°å®¡è§†äº†åŸºäºéæŠ•å½±çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æå‡-å¹³é“º-å°„å‡»ï¼ˆLSSï¼‰èŒƒå¼ï¼Œå¹¶é€šè¿‡æ·±åº¦ä¸ç¡®å®šæ€§å»ºæ¨¡å¢å¼ºäº†å®ƒä»¬ã€‚GaussianLSSé€šè¿‡å­¦ä¹ è½¯æ·±åº¦å‡å€¼å¹¶è®¡ç®—æ·±åº¦åˆ†å¸ƒçš„æ–¹å·®æ¥è¡¨ç¤ºç©ºé—´åˆ†æ•£æ€§ï¼Œè¿™éšå«åœ°æ•è·äº†å¯¹è±¡èŒƒå›´ã€‚ç„¶åæˆ‘ä»¬å°†æ·±åº¦åˆ†å¸ƒè½¬åŒ–ä¸º3Dé«˜æ–¯åˆ†å¸ƒå¹¶è¿›è¡Œæ …æ ¼åŒ–ï¼Œä»¥æ„å»ºå…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„BEVç‰¹å¾ã€‚æˆ‘ä»¬åœ¨nuscenesæ•°æ®é›†ä¸Šè¯„ä¼°äº†GaussianLSSçš„æ€§èƒ½ï¼Œä¸åŸºäºéæŠ•å½±çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®ƒåœ¨é€Ÿåº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¿è¡Œé€Ÿåº¦ä¸ºåŸºäºæŠ•å½±çš„æ–¹æ³•çš„2.5å€ï¼Œåœ¨å†…å­˜æ•ˆç‡æ–¹é¢ä¹Ÿå…·æœ‰ä¼˜åŠ¿ï¼Œä½¿ç”¨æ¯”åŸºäºæŠ•å½±çš„æ–¹æ³•å°‘0.3å€çš„å†…å­˜ï¼ŒåŒæ—¶ä»…åœ¨IoUæ–¹é¢å­˜åœ¨0.4%çš„å·®å¼‚ä¾¿å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01957v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†GaussianLSSï¼Œä¸€ç§åŸºäºä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰æ„ŸçŸ¥æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡æ–°å®¡è§†äº†åŸºäºéæŠ•å½±çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯Lift-Splat-Shootï¼ˆLSSï¼‰èŒƒå¼ï¼Œå¹¶é€šè¿‡æ·±åº¦ä¸ç¡®å®šæ€§å»ºæ¨¡è¿›è¡Œå¢å¼ºã€‚GaussianLSSé€šè¿‡å­¦ä¹ è½¯æ·±åº¦å‡å€¼å¹¶è®¡ç®—æ·±åº¦åˆ†å¸ƒçš„æ–¹å·®æ¥è¡¨ç¤ºç©ºé—´åˆ†å¸ƒï¼Œä»è€Œéšå¼æ•è·å¯¹è±¡èŒƒå›´ã€‚ç„¶åï¼Œå°†æ·±åº¦åˆ†å¸ƒè½¬æ¢ä¸ºä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œè¿›è¡Œæ …æ ¼åŒ–ä»¥æ„å»ºå…·æœ‰æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„BEVç‰¹å¾ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒGaussianLSSç›¸è¾ƒäºåŸºäºéæŠ•å½±çš„æ–¹æ³•å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œé€Ÿåº¦å¿«2.5å€ï¼Œå†…å­˜æ•ˆç‡æé«˜0.3å€ï¼ŒåŒæ—¶ä¸åŸºäºæŠ•å½±çš„æ–¹æ³•ç›¸æ¯”ï¼ŒIoUå·®å¼‚ä»…0.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaussianLSSæ˜¯ä¸€ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰æ„ŸçŸ¥æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åŸºäºéæŠ•å½±æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯Lift-Splat-Shootï¼ˆLSSï¼‰èŒƒå¼ã€‚</li>
<li>GaussianLSSé€šè¿‡æ·±åº¦ä¸ç¡®å®šæ€§å»ºæ¨¡è¿›è¡Œå¢å¼ºã€‚</li>
<li>é€šè¿‡å­¦ä¹ è½¯æ·±åº¦å‡å€¼å’Œè®¡ç®—æ·±åº¦åˆ†å¸ƒçš„æ–¹å·®æ¥è¡¨ç¤ºç©ºé—´åˆ†å¸ƒã€‚</li>
<li>å°†æ·±åº¦åˆ†å¸ƒè½¬æ¢ä¸ºä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶è¿›è¡Œæ …æ ¼åŒ–ä»¥æ„å»ºä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„BEVç‰¹å¾ã€‚</li>
<li>åœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒGaussianLSSå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œé€Ÿåº¦ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0fff6ee53cadab3442bbca5ace717bc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7339336e6eb1f00486630ae4946f87fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f73f51e96852fb51455f8ebaf150e34b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f483396fdf57f70db3cede0c2127b48.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BOGausS-Better-Optimized-Gaussian-Splatting"><a href="#BOGausS-Better-Optimized-Gaussian-Splatting" class="headerlink" title="BOGausS: Better Optimized Gaussian Splatting"></a>BOGausS: Better Optimized Gaussian Splatting</h2><p><strong>Authors:StÃ©phane Pateux, Matthieu Gendrin, Luce Morin, ThÃ©o Ladune, Xiaoran Jiang</strong></p>
<p>3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view synthesis. Its framework provides fast and high-fidelity rendering. Although less complex than other solutions such as Neural Radiance Fields (NeRF), there are still some challenges building smaller models without sacrificing quality. In this study, we perform a careful analysis of 3DGS training process and propose a new optimization methodology. Our Better Optimized Gaussian Splatting (BOGausS) solution is able to generate models up to ten times lighter than the original 3DGS with no quality degradation, thus significantly boosting the performance of Gaussian Splatting compared to the state of the art. </p>
<blockquote>
<p>ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†ç‚¹åˆæˆé«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚å…¶æ¡†æ¶æä¾›å¿«é€Ÿä¸”é«˜ä¿çœŸæ¸²æŸ“ã€‚è™½ç„¶ç›¸æ¯”äºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç­‰å…¶ä»–è§£å†³æ–¹æ¡ˆï¼Œå®ƒçš„å¤æ‚æ€§è¾ƒä½ï¼Œä½†åœ¨ä¸ç‰ºç‰²è´¨é‡çš„æƒ…å†µä¸‹æ„å»ºå°å‹æ¨¡å‹ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹3DGSè®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†è®¤çœŸåˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬ä¼˜åŒ–çš„é«˜æ–¯å–·æº…ï¼ˆBOGausSï¼‰è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿç”Ÿæˆæ¯”åŸå§‹3DGSè½»åå€çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¸é™ä½è´¨é‡ï¼Œä»è€Œæå¤§åœ°æé«˜äº†é«˜æ–¯å–·æº…ä¸æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºä¸‰ç»´é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼ˆ3DGSï¼‰çš„åˆ›æ–°è§†è§’åˆæˆè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¯¹å…¶æ¡†æ¶è¿›è¡Œä¼˜åŒ–æ”¹è¿›ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå³æ›´å¥½çš„ä¼˜åŒ–é«˜æ–¯è´´å›¾ï¼ˆBOGausSï¼‰ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ¯”åŸå§‹3DGSæ¨¡å‹è½»åå€çš„æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†é«˜æ–¯è´´å›¾æŠ€æœ¯çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ–°è§†è§’åˆæˆè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¿«é€Ÿå’Œé«˜ä¿çœŸæ¸²æŸ“çš„ç‰¹ç‚¹ã€‚</li>
<li>ä¸å…¶ä»–è§£å†³æ–¹æ¡ˆï¼ˆå¦‚NeRFï¼‰ç›¸æ¯”ï¼Œæ„å»ºè¾ƒå°æ¨¡å‹çš„åŒæ—¶ä¿æŒè´¨é‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡å¯¹3DGSè®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œå¹¶æå‡ºäº†æ–°çš„ä¼˜åŒ–æ–¹æ³•â€”â€”BOGausSã€‚</li>
<li>BOGausSèƒ½å¤Ÿåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ¯”åŸå§‹3DGSæ¨¡å‹è½»åå€çš„æ¨¡å‹ã€‚</li>
<li>BOGausSæ˜¾è‘—æé«˜äº†é«˜æ–¯è´´å›¾æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºä¸‰ç»´å›¾å½¢æ¸²æŸ“é¢†åŸŸæä¾›äº†æ–°çš„ä¼˜åŒ–æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c8cb2eb3263f67d85f48043937cfced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e0bdb62b9bc6ae2299197a6852b669.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="3DBonsai-Structure-Aware-Bonsai-Modeling-Using-Conditioned-3D-Gaussian-Splatting"><a href="#3DBonsai-Structure-Aware-Bonsai-Modeling-Using-Conditioned-3D-Gaussian-Splatting" class="headerlink" title="3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian   Splatting"></a>3DBonsai: Structure-Aware Bonsai Modeling Using Conditioned 3D Gaussian   Splatting</h2><p><strong>Authors:Hao Wu, Hao Wang, Ruochong Li, Xuran Ma, Hui Xiong</strong></p>
<p>Recent advancements in text-to-3D generation have shown remarkable results by leveraging 3D priors in combination with 2D diffusion. However, previous methods utilize 3D priors that lack detailed and complex structural information, limiting them to generating simple objects and presenting challenges for creating intricate structures such as bonsai. In this paper, we propose 3DBonsai, a novel text-to-3D framework for generating 3D bonsai with complex structures. Technically, we first design a trainable 3D space colonization algorithm to produce bonsai structures, which are then enhanced through random sampling and point cloud augmentation to serve as the 3D Gaussian priors. We introduce two bonsai generation pipelines with distinct structural levels: fine structure conditioned generation, which initializes 3D Gaussians using a 3D structure prior to produce detailed and complex bonsai, and coarse structure conditioned generation, which employs a multi-view structure consistency module to align 2D and 3D structures. Moreover, we have compiled a unified 2D and 3D Chinese-style bonsai dataset. Our experimental results demonstrate that 3DBonsai significantly outperforms existing methods, providing a new benchmark for structure-aware 3D bonsai generation. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°3Dç”Ÿæˆçš„è¿›å±•ï¼Œé€šè¿‡åˆ©ç”¨3Då…ˆéªŒå’Œ2Dæ‰©æ•£çš„ç»“åˆï¼Œå–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•ä½¿ç”¨çš„3Då…ˆéªŒç¼ºä¹è¯¦ç»†å’Œå¤æ‚çš„ç»“æ„ä¿¡æ¯ï¼Œä»…é™äºç”Ÿæˆç®€å•ç‰©ä½“ï¼Œå¯¹äºåˆ›å»ºå¤æ‚çš„ç»“æ„å¦‚ç›†æ™¯ç­‰å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†3DBonsaiï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å¤æ‚ç»“æ„çš„3Dç›†æ™¯ã€‚æŠ€æœ¯ä¸Šï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§å¯è®­ç»ƒçš„3Dç©ºé—´æ®–æ°‘åŒ–ç®—æ³•æ¥ç”Ÿæˆç›†æ™¯ç»“æ„ï¼Œç„¶åé€šè¿‡éšæœºé‡‡æ ·å’Œç‚¹äº‘å¢å¼ºæ¥å¢å¼ºè¿™äº›ç»“æ„ï¼Œä½œä¸º3Dé«˜æ–¯å…ˆéªŒã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§å…·æœ‰ä¸åŒç»“æ„å±‚æ¬¡çš„ç›†æ™¯ç”Ÿæˆæµç¨‹ï¼šç²¾ç»†ç»“æ„æ¡ä»¶ç”Ÿæˆï¼Œä½¿ç”¨3Dç»“æ„å…ˆéªŒåˆå§‹åŒ–3Dé«˜æ–¯æ¥ç”Ÿæˆè¯¦ç»†è€Œå¤æ‚çš„ç›†æ™¯ï¼›ç²—ç³™ç»“æ„æ¡ä»¶ç”Ÿæˆï¼Œé‡‡ç”¨å¤šè§†è§’ç»“æ„ä¸€è‡´æ€§æ¨¡å—æ¥å¯¹é½2Då’Œ3Dç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¼–è¯‘äº†ä¸€ä¸ªç»Ÿä¸€çš„2Då’Œ3Dä¸­å¼ç›†æ™¯æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œ3DBonsaiæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºç»“æ„æ„ŸçŸ¥çš„3Dç›†æ™¯ç”Ÿæˆæä¾›äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01619v1">PDF</a> Accepted by ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º3DBonsaiçš„æ–°å‹æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰å¤æ‚ç»“æ„çš„3Dç›†æ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡å¯è®­ç»ƒçš„3Dç©ºé—´æ®–æ°‘åŒ–ç®—æ³•æ¥ç”Ÿæˆç›†æ™¯ç»“æ„ï¼Œå¹¶é€šè¿‡éšæœºé‡‡æ ·å’Œç‚¹äº‘å¢å¼ºæŠ€æœ¯æ¥ä¼˜åŒ–è¿™äº›ç»“æ„ï¼Œå°†å…¶ä½œä¸º3Dé«˜æ–¯å…ˆéªŒã€‚æ–‡ç« è¿˜ä»‹ç»äº†ä¸¤ç§å…·æœ‰ä¸åŒç»“æ„å±‚æ¬¡çš„ç›†æ™¯ç”Ÿæˆç®¡é“ï¼Œåˆ†åˆ«æ˜¯ç²¾ç»†ç»“æ„æ¡ä»¶ç”Ÿæˆå’Œç²—ç•¥ç»“æ„æ¡ä»¶ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ3DBonsaiåœ¨ç»“æ„æ„ŸçŸ¥çš„3Dç›†æ™¯ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DBonsaiæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå…·æœ‰å¤æ‚ç»“æ„çš„3Dç›†æ™¯çš„æ–°å‹æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å¯è®­ç»ƒçš„3Dç©ºé—´æ®–æ°‘åŒ–ç®—æ³•æ¥ç”Ÿæˆç›†æ™¯çš„åŸºæœ¬ç»“æ„ã€‚</li>
<li>é€šè¿‡éšæœºé‡‡æ ·å’Œç‚¹äº‘å¢å¼ºæŠ€æœ¯ä¼˜åŒ–ç”Ÿæˆçš„ç›†æ™¯ç»“æ„ï¼Œå°†å…¶ä½œä¸º3Dé«˜æ–¯å…ˆéªŒã€‚</li>
<li>æ–‡ç« ä¸­ä»‹ç»äº†ä¸¤ç§ç›†æ™¯ç”Ÿæˆç®¡é“ï¼šç²¾ç»†ç»“æ„æ¡ä»¶ç”Ÿæˆå’Œç²—ç•¥ç»“æ„æ¡ä»¶ç”Ÿæˆï¼Œåˆ†åˆ«é€‚ç”¨äºä¸åŒçš„éœ€æ±‚ã€‚</li>
<li>3DBonsaiç¼–è¯‘äº†ä¸€ä¸ªç»Ÿä¸€çš„2Då’Œ3Dä¸­å¼ç›†æ™¯æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œ3DBonsaiåœ¨ç»“æ„æ„ŸçŸ¥çš„3Dç›†æ™¯ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-107e2a9aaa4e0b92e00a1db0563acd30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb7212ffd12da5c472cf48b312384ecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22141218ce502f4666540850c82d32d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cac6e22d69e6101731ac942b37973a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb36fbbaf276d14e562578e6b3e61ce4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="High-fidelity-3D-Object-Generation-from-Single-Image-with-RGBN-Volume-Gaussian-Reconstruction-Model"><a href="#High-fidelity-3D-Object-Generation-from-Single-Image-with-RGBN-Volume-Gaussian-Reconstruction-Model" class="headerlink" title="High-fidelity 3D Object Generation from Single Image with RGBN-Volume   Gaussian Reconstruction Model"></a>High-fidelity 3D Object Generation from Single Image with RGBN-Volume   Gaussian Reconstruction Model</h2><p><strong>Authors:Yiyang Shen, Kun Zhou, He Wang, Yin Yang, Tianjia Shao</strong></p>
<p>Recently single-view 3D generation via Gaussian splatting has emerged and developed quickly. They learn 3D Gaussians from 2D RGB images generated from pre-trained multi-view diffusion (MVD) models, and have shown a promising avenue for 3D generation through a single image. Despite the current progress, these methods still suffer from the inconsistency jointly caused by the geometric ambiguity in the 2D images, and the lack of structure of 3D Gaussians, leading to distorted and blurry 3D object generation. In this paper, we propose to fix these issues by GS-RGBN, a new RGBN-volume Gaussian Reconstruction Model designed to generate high-fidelity 3D objects from single-view images. Our key insight is a structured 3D representation can simultaneously mitigate the afore-mentioned two issues. To this end, we propose a novel hybrid Voxel-Gaussian representation, where a 3D voxel representation contains explicit 3D geometric information, eliminating the geometric ambiguity from 2D images. It also structures Gaussians during learning so that the optimization tends to find better local optima. Our 3D voxel representation is obtained by a fusion module that aligns RGB features and surface normal features, both of which can be estimated from 2D images. Extensive experiments demonstrate the superiority of our methods over prior works in terms of high-quality reconstruction results, robust generalization, and good efficiency. </p>
<blockquote>
<p>è¿‘æœŸï¼Œé€šè¿‡é«˜æ–¯è´´ç‰‡æŠ€æœ¯è¿›è¡Œå•è§†å›¾3Dç”Ÿæˆçš„æ–¹æ³•è¿…é€Ÿå…´èµ·å’Œå‘å±•ã€‚è¿™äº›æ–¹æ³•ä»ç”±é¢„è®­ç»ƒçš„å¤šè§†å›¾æ‰©æ•£ï¼ˆMVDï¼‰æ¨¡å‹ç”Ÿæˆçš„2D RGBå›¾åƒä¸­å­¦ä¹ 3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸ºé€šè¿‡å•å¼ å›¾åƒè¿›è¡Œ3Dç”Ÿæˆå±•ç¤ºäº†å‰æ™¯ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å—åˆ°ç”±2Då›¾åƒä¸­çš„å‡ ä½•æ¨¡ç³Šå’Œ3Dé«˜æ–¯ç»“æ„ç¼ºå¤±è”åˆå¼•èµ·çš„ä¸ä¸€è‡´æ€§çš„å›°æ‰°ï¼Œå¯¼è‡´ç”Ÿæˆçš„3Då¯¹è±¡å¤±çœŸå’Œæ¨¡ç³Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡GS-RGBNè§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RGBNä½“ç§¯é«˜æ–¯é‡å»ºæ¨¡å‹ï¼Œæ—¨åœ¨ä»å•è§†å›¾å›¾åƒç”Ÿæˆé«˜ä¿çœŸ3Då¯¹è±¡ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œç»“æ„åŒ–çš„3Dè¡¨ç¤ºå¯ä»¥åŒæ—¶ç¼“è§£ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆä½“ç´ -é«˜æ–¯è¡¨ç¤ºï¼Œå…¶ä¸­3Dä½“ç´ è¡¨ç¤ºåŒ…å«æ˜ç¡®çš„3Då‡ ä½•ä¿¡æ¯ï¼Œæ¶ˆé™¤äº†æ¥è‡ª2Då›¾åƒçš„å‡ ä½•æ¨¡ç³Šã€‚å®ƒè¿˜åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹é«˜æ–¯è¿›è¡Œç»“æ„åŒ–ï¼Œä½¿å¾—ä¼˜åŒ–æ›´è¶‹å‘äºæ‰¾åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜è§£ã€‚æˆ‘ä»¬çš„3Dä½“ç´ è¡¨ç¤ºæ˜¯é€šè¿‡èåˆæ¨¡å—è·å¾—çš„ï¼Œè¯¥æ¨¡å—å¯¹é½RGBç‰¹å¾å’Œè¡¨é¢æ³•çº¿ç‰¹å¾ï¼Œè¿™ä¸¤ä¸ªç‰¹å¾éƒ½å¯ä»¥ä»2Då›¾åƒä¸­ä¼°è®¡å‡ºæ¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜è´¨é‡é‡å»ºç»“æœã€ç¨³å¥çš„é€šç”¨æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01512v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„RGBNä½“ç§¯é«˜æ–¯é‡å»ºæ¨¡å‹GS-RGBNï¼Œç”¨äºä»å•è§†å›¾å›¾åƒç”Ÿæˆé«˜ä¿çœŸåº¦çš„3Då¯¹è±¡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å­˜åœ¨çš„å‡ ä½•æ¨¡ç³Šä¸é«˜æ–¯ç»“æ„ç¼ºå¤±çš„é—®é¢˜ï¼Œè¯¥æ–‡é‡‡ç”¨äº†ä¸€ç§æ–°çš„æ··åˆä½“ç´ -é«˜æ–¯è¡¨ç¤ºæ³•ï¼Œé€šè¿‡èåˆRGBç‰¹å¾å’Œè¡¨é¢æ³•çº¿ç‰¹å¾æ¥æ„å»ºç»“æ„åŒ–çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå®ç°äº†é«˜è´¨é‡çš„ä¸‰ç»´é‡å»ºç»“æœã€‚è¯¥æ–¹æ³•ä¼˜åŒ–äº†å±€éƒ¨ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶è¡¨ç°å‡ºä¼˜å¼‚çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•è§†å›¾3Dç”Ÿæˆé€šè¿‡é«˜æ–¯æ‹¼è´´æ–¹æ³•å¿«é€Ÿå‘å±•ã€‚ä½†æ˜¯ç°æœ‰çš„æ–¹æ³•ä»å­˜åœ¨ç”±äºŒç»´å›¾åƒçš„å‡ ä½•æ¨¡ç³Šå’Œé«˜æ–¯ç»“æ„ç¼ºå¤±å¼•èµ·çš„å¤±çœŸå’Œæ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åŸºäºRGBNä½“ç§¯çš„é«˜æ–¯é‡å»ºæ¨¡å‹GS-RGBNæ¥è§£å†³ã€‚</li>
<li>æ ¸å¿ƒæ€è·¯æ˜¯é‡‡ç”¨æ··åˆä½“ç´ -é«˜æ–¯è¡¨ç¤ºæ³•ï¼Œå°†RGBç‰¹å¾å’Œè¡¨é¢æ³•çº¿ç‰¹å¾èåˆï¼Œæ„å»ºç»“æ„åŒ–çš„ä¸‰ç»´è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•åŒ…å«æ˜ç¡®çš„å‡ ä½•ä¿¡æ¯å¹¶ä¼˜åŒ–äº†é«˜æ–¯å­¦ä¹ è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°è§£å†³äº†äºŒç»´å›¾åƒä¸­çš„å‡ ä½•æ¨¡ç³Šé—®é¢˜ï¼Œå¹¶æé«˜äº†ç”Ÿæˆä¸‰ç»´å¯¹è±¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡èåˆæ¨¡å—è·å–ä¸‰ç»´ä½“ç´ è¡¨ç¤ºæ³•ï¼Œè¯¥æ¨¡å—å¯ä»¥å¯¹RGBç‰¹å¾å’Œè¡¨é¢æ³•çº¿ç‰¹å¾è¿›è¡Œå¯¹é½ä¼°ç®—ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤§é‡å®éªŒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„é‡å»ºè´¨é‡ã€æ›´å¼ºçš„é²æ£’æ€§å’Œè‰¯å¥½çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d4acb84650e4bedc219b6ad85734113.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30a6fb92c3418526effa7ca734eb3f34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e1f089e6c3c29b40a553e3fa8b74c0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c8b4d630a5700ab1e771bd96223bd02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53b58ec8055682c2b0644fed3d50d25e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment"><a href="#Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment" class="headerlink" title="Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment"></a>Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment</h2><p><strong>Authors:Ziteng Cui, Xuangeng Chu, Tatsuya Harada</strong></p>
<p>Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions â€“ including low-light, overexposure, and varying exposure â€“ while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality. </p>
<blockquote>
<p>åœ¨å¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œç…§æ˜æ¡ä»¶ä¸‹æ•æ‰é«˜è´¨é‡ç…§ç‰‡æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶å…‰ç…§ï¼ˆä¾‹å¦‚ä½å…‰ç¯å¢ƒï¼‰å’Œç›¸æœºæ›å…‰è®¾ç½®ï¼ˆä¾‹å¦‚æ›å…‰æ—¶é—´ï¼‰éƒ½ä¼šæ˜¾è‘—å½±å“å›¾åƒè´¨é‡ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨å¤šè§†è§’åœºæ™¯ä¸­å°¤ä¸ºçªå‡ºï¼Œå…¶ä¸­ä¸åŒè§†è§’çš„å…‰ç…§å’Œå›¾åƒä¿¡å·å¤„ç†å™¨ï¼ˆISPï¼‰è®¾ç½®å˜åŒ–ä¼šå¼•å…¥å…‰åº¦ä¸ä¸€è‡´æ€§ã€‚è¿™ç§å…‰ç…§é€€åŒ–å’Œè§†è§’ç›¸å…³çš„å˜åŒ–ç»™åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰çš„æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰æ¡†æ¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Luminance-GSï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨3DGSåœ¨å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å…‰ç…§æ¡ä»¶ä¸‹å®ç°é«˜è´¨é‡æ–°è§†è§’åˆæˆç»“æœçš„æ–°æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨æ¯è§†å›¾é¢œè‰²çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼ŒLuminance-GSåœ¨å„ç§å…‰ç…§æ¡ä»¶ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å’Œå¯å˜æ›å…‰ï¼ŒåŒæ—¶ä¸æ”¹å˜åŸå§‹3DGSçš„æ˜¾å¼è¡¨ç¤ºã€‚ä¸ä¹‹å‰çš„NeRFå’Œ3DGSåŸºå‡†çº¿ç›¸æ¯”ï¼ŒLuminance-GSæä¾›äº†å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œå¹¶æé«˜äº†é‡å»ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01503v1">PDF</a> CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://cuiziteng.github.io/Luminance_GS_web/">https://cuiziteng.github.io/Luminance_GS_web/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹çœŸå®ä¸–ç•Œå¤æ‚å…‰ç…§æ¡ä»¶ä¸‹æ‹æ‘„é«˜è´¨é‡ç…§ç‰‡çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åŸºäº3DGSçš„æ–°æ–¹æ³•Luminance-GSï¼Œå®ç°äº†ä¸åŒè§†è§’çš„é«˜è´¨é‡Œåˆæˆå›¾åƒã€‚Luminance-GSåˆ©ç”¨è§†å›¾è‰²å½©çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´æŠ€æœ¯ï¼Œåœ¨å¤šç§å…‰ç…§æ¡ä»¶ä¸‹ï¼ˆåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å…‰å’Œæ›å…‰å˜åŒ–ï¼‰å–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹3DGSæ˜¾å¼è¡¨ç¤ºçš„å®Œæ•´æ€§ã€‚ä¸åŸºäºNeRFå’Œ3DGSçš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒLuminance-GSå®ç°äº†å®æ—¶æ¸²æŸ“é€Ÿåº¦çš„æå‡å’Œé‡å»ºè´¨é‡çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡å›¾åƒæ•è·é¢ä¸´æŒ‘æˆ˜ï¼šçœŸå®ä¸–ç•Œå¤æ‚çš„å…‰ç…§æ¡ä»¶å’Œç›¸æœºæ›å…‰è®¾ç½®å¯¹å›¾åƒè´¨é‡äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>å¤šè§†è§’åœºæ™¯ä¸­çš„å…‰ç…§å˜åŒ–å¼•å…¥å…‰åº¦ä¸ä¸€è‡´æ€§ï¼Œå¯¹åŸºäºNeRFå’Œ3DGSçš„æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰æ¡†æ¶æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„Luminance-GSæ˜¯ä¸€ç§åˆ©ç”¨è§†å›¾è‰²å½©çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´æŠ€æœ¯çš„æ–¹æ³•ï¼Œè§£å†³äº†åœ¨ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„é«˜è´¨é‡æ–°è§†è§’åˆæˆé—®é¢˜ã€‚</li>
<li>Luminance-GSåœ¨ä¸æ”¹å˜åŸå§‹3DGSæ˜¾å¼è¡¨ç¤ºçš„å‰æä¸‹ï¼Œåœ¨å¤šç§å…‰ç…§æ¡ä»¶ä¸‹å–å¾—å“è¶Šæˆæœã€‚</li>
<li>ä¸ç°æœ‰åŸºäºNeRFå’Œ3DGSçš„æ–¹æ³•ç›¸æ¯”ï¼ŒLuminance-GSå…·æœ‰æ›´å¿«çš„å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œæ›´é«˜çš„é‡å»ºè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯ç”¨äºæ”¹å–„çœŸå®ä¸–ç•Œå¤æ‚ç¯å¢ƒä¸‹çš„å›¾åƒå¤„ç†å’Œæ¸²æŸ“æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35c5c92e224606e546fd03cf5090f540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d178aa2c017ef369fb2414f73a079403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d77b7cade783144fb29ae6054c1885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8064f88da9ae906527cf71cbf1d9a992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c2ded93e6712755127a771f8516b4c3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Inverse-Rendering-with-Approximated-Global-Illumination"><a href="#3D-Gaussian-Inverse-Rendering-with-Approximated-Global-Illumination" class="headerlink" title="3D Gaussian Inverse Rendering with Approximated Global Illumination"></a>3D Gaussian Inverse Rendering with Approximated Global Illumination</h2><p><strong>Authors:Zirui Wu, Jianteng Chen, Laijian Li, Shaoteng Wu, Zhikai Zhu, Kang Xu, Martin R. Oswald, Jie Song</strong></p>
<p>3D Gaussian Splatting shows great potential in reconstructing photo-realistic 3D scenes. However, these methods typically bake illumination into their representations, limiting their use for physically-based rendering and scene editing. Although recent inverse rendering approaches aim to decompose scenes into material and lighting components, they often rely on simplifying assumptions that fail when editing. We present a novel approach that enables efficient global illumination for 3D Gaussians Splatting through screen-space ray tracing. Our key insight is that a substantial amount of indirect light can be traced back to surfaces visible within the current view frustum. Leveraging this observation, we augment the direct shading computed by 3D Gaussians with Monte-Carlo screen-space ray-tracing to capture one-bounce indirect illumination. In this way, our method enables realistic global illumination without sacrificing the computational efficiency and editability benefits of 3D Gaussians. Through experiments, we show that the screen-space approximation we utilize allows for indirect illumination and supports real-time rendering and editing. Code, data, and models will be made available at our project page: <a target="_blank" rel="noopener" href="https://wuzirui.github.io/gs-ssr">https://wuzirui.github.io/gs-ssr</a>. </p>
<blockquote>
<p>ä¸‰ç»´é«˜æ–¯æ›²é¢æ˜ å°„åœ¨é‡å»ºé€¼çœŸçš„ä¸‰ç»´åœºæ™¯æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å°†å…‰ç…§èå…¥å…¶è¡¨ç¤ºä¸­ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨åŸºäºç‰©ç†çš„æ¸²æŸ“å’Œåœºæ™¯ç¼–è¾‘ä¸­çš„ä½¿ç”¨ã€‚å°½ç®¡æœ€è¿‘çš„é€†å‘æ¸²æŸ“æ–¹æ³•æ—¨åœ¨å°†åœºæ™¯åˆ†è§£ä¸ºæè´¨å’Œå…‰ç…§ç»„ä»¶ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–äºç®€åŒ–çš„å‡è®¾ï¼Œåœ¨ç¼–è¾‘æ—¶è¿™äº›å‡è®¾ä¼šå¤±æ•ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å±å¹•ç©ºé—´å…‰çº¿è¿½è¸ªä¸ºä¸‰ç»´é«˜æ–¯æ›²é¢æ˜ å°„å®ç°é«˜æ•ˆçš„å…¨å±€å…‰ç…§ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå¤§é‡çš„é—´æ¥å…‰å¯ä»¥å›æº¯åˆ°å½“å‰è§†é”¥å†…å¯è§çš„æ›²é¢ã€‚åˆ©ç”¨è¿™ä¸€è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬ç”¨è’™ç‰¹å¡ç½—å±å¹•ç©ºé—´å…‰çº¿è¿½è¸ªæ¥å¢å¼ºä¸‰ç»´é«˜æ–¯è®¡ç®—çš„ç›´æ¥ç€è‰²ï¼Œä»¥æ•æ‰ä¸€æ¬¡å¼¹è·³é—´æ¥ç…§æ˜ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²è®¡ç®—æ•ˆç‡å’Œç¼–è¾‘ä¾¿åˆ©æ€§çš„æƒ…å†µä¸‹å®ç°é€¼çœŸçš„å…¨å±€å…‰ç…§ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æ‰€ä½¿ç”¨çš„å±å¹•ç©ºé—´è¿‘ä¼¼å…è®¸é—´æ¥ç…§æ˜å¹¶æ”¯æŒå®æ—¶æ¸²æŸ“å’Œç¼–è¾‘ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æä¾›ï¼š<a target="_blank" rel="noopener" href="https://wuzirui.github.io/gs-ssr%E3%80%82">https://wuzirui.github.io/gs-ssrã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå±å¹•ç©ºé—´å…‰çº¿è¿½è¸ªæŠ€æœ¯çš„ä¸‰ç»´é«˜æ–¯è´´å›¾æ–°æ–¹æ³•ï¼Œå¯æœ‰æ•ˆå®ç°å…¨å±€å…‰ç…§æ•ˆæœã€‚è¯¥æ–¹æ³•ç»“åˆä¸‰ç»´é«˜æ–¯è´´å›¾çš„ç›´æ¥ç€è‰²å’Œè’™ç‰¹å¡ç½—å±å¹•ç©ºé—´å…‰çº¿è¿½è¸ªï¼Œæ•è·ä¸€æ¬¡é—´æ¥ç…§æ˜ï¼Œå…·æœ‰è®¡ç®—æ•ˆç‡é«˜å’Œå¯ç¼–è¾‘æ€§å¼ºçš„ä¼˜ç‚¹ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•å¯å®ç°å®æ—¶æ¸²æŸ“å’Œç¼–è¾‘ï¼Œæ”¯æŒé—´æ¥ç…§æ˜æ•ˆæœã€‚æ›´å¤šè¯¦æƒ…å¯è®¿é—®é¡¹ç›®é¡µé¢ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€ä¸‰ç»´é«˜æ–¯è´´å›¾åœ¨é‡å»ºçœŸå®æ„Ÿä¸‰ç»´åœºæ™¯æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚<br>äºŒã€ç°æœ‰æ–¹æ³•é€šå¸¸å°†ç…§æ˜èå…¥å…¶è¡¨ç¤ºä¸­ï¼Œé™åˆ¶äº†å…¶åœ¨ç‰©ç†æ¸²æŸ“å’Œåœºæ™¯ç¼–è¾‘æ–¹é¢çš„åº”ç”¨ã€‚<br>ä¸‰ã€é€†å‘æ¸²æŸ“æ–¹æ³•æ—¨åœ¨å°†åœºæ™¯åˆ†è§£ä¸ºææ–™å’Œç…§æ˜ç»„ä»¶ï¼Œä½†åœ¨ç¼–è¾‘æ—¶ç®€åŒ–å‡è®¾å¸¸å¸¸å¤±æ•ˆã€‚<br>å››ã€æ–°æ–¹æ³•é€šè¿‡å±å¹•ç©ºé—´å…‰çº¿è¿½è¸ªæŠ€æœ¯ä¸ºä¸‰ç»´é«˜æ–¯è´´å›¾å®ç°äº†é«˜æ•ˆçš„å…¨å±€ç…§æ˜ã€‚<br>äº”ã€å…³é”®æ´å¯ŸåŠ›åœ¨äºå¤§é‡é—´æ¥å…‰å¯ä»¥è¿½æº¯å›åˆ°å½“å‰è§†é”¥ä½“å¯è§çš„è¡¨é¢ã€‚<br>å…­ã€è¯¥æ–¹æ³•ç»“åˆäº†ç›´æ¥ç€è‰²å’Œé—´æ¥ç…§æ˜ï¼Œå®ç°äº†çœŸå®æ„Ÿçš„å…¨å±€ç…§æ˜ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡å’Œç¼–è¾‘æ€§ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-836038039070303ff2235b5daa201dff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b329ff41b13e7e598ec9d98fad48d61a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65587206f8ca41767bff14dc92f29a6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57a165f188cc33c70030ba7a1bdd1586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ef6fcb29b7402d3ec9526e58c6cd510.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DropGaussian-Structural-Regularization-for-Sparse-view-Gaussian-Splatting"><a href="#DropGaussian-Structural-Regularization-for-Sparse-view-Gaussian-Splatting" class="headerlink" title="DropGaussian: Structural Regularization for Sparse-view Gaussian   Splatting"></a>DropGaussian: Structural Regularization for Sparse-view Gaussian   Splatting</h2><p><strong>Authors:Hyunwoo Park, Gun Ryu, Wonjun Kim</strong></p>
<p>Recently, 3D Gaussian splatting (3DGS) has gained considerable attentions in the field of novel view synthesis due to its fast performance while yielding the excellent image quality. However, 3DGS in sparse-view settings (e.g., three-view inputs) often faces with the problem of overfitting to training views, which significantly drops the visual quality of novel view images. Many existing approaches have tackled this issue by using strong priors, such as 2D generative contextual information and external depth signals. In contrast, this paper introduces a prior-free method, so-called DropGaussian, with simple changes in 3D Gaussian splatting. Specifically, we randomly remove Gaussians during the training process in a similar way of dropout, which allows non-excluded Gaussians to have larger gradients while improving their visibility. This makes the remaining Gaussians to contribute more to the optimization process for rendering with sparse input views. Such simple operation effectively alleviates the overfitting problem and enhances the quality of novel view synthesis. By simply applying DropGaussian to the original 3DGS framework, we can achieve the competitive performance with existing prior-based 3DGS methods in sparse-view settings of benchmark datasets without any additional complexity. The code and model are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/DCVL-3D/DropGaussian">https://github.com/DCVL-3D/DropGaussian</a> release. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç”±äºå…¶åœ¨æä¾›å‡ºè‰²å›¾åƒè´¨é‡çš„åŒæ—¶å±•ç°å‡ºé«˜æ•ˆçš„æ€§èƒ½ï¼Œ3Dé«˜æ–¯æ’å€¼ï¼ˆ3DGSï¼‰åœ¨æ–°å‹è§†å›¾åˆæˆé¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œåœ¨ç¨€ç–è§†å›¾ç¯å¢ƒï¼ˆä¾‹å¦‚ä¸‰è§†å›¾è¾“å…¥ï¼‰ä¸­ï¼Œ3DGSç»å¸¸é¢ä¸´è¿‡åº¦æ‹Ÿåˆè®­ç»ƒè§†å›¾çš„é—®é¢˜ï¼Œè¿™æ˜¾è‘—é™ä½äº†æ–°è§†å›¾çš„å›¾åƒè´¨é‡ã€‚è®¸å¤šç°æœ‰æ–¹æ³•é€šè¿‡åˆ©ç”¨å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚äºŒç»´ç”Ÿæˆä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå¤–éƒ¨æ·±åº¦ä¿¡å·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ— éœ€å…ˆéªŒçš„æ–¹æ³•ï¼Œç§°ä¸ºDropGaussianï¼Œå®ƒé€šè¿‡ç®€å•çš„æ”¹å˜3Dé«˜æ–¯æ’å€¼æ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºåˆ é™¤é«˜æ–¯åˆ†é‡ï¼Œç±»ä¼¼äºdropoutçš„æ–¹å¼ï¼Œè¿™å…è®¸æœªè¢«æ’é™¤çš„é«˜æ–¯åˆ†é‡æ‹¥æœ‰æ›´å¤§çš„æ¢¯åº¦å¹¶æé«˜å…¶å¯è§æ€§ã€‚è¿™ä½¿å¾—å‰©ä½™çš„é«˜æ–¯åˆ†é‡å¯¹ä½¿ç”¨ç¨€ç–è¾“å…¥è§†å›¾è¿›è¡Œæ¸²æŸ“çš„ä¼˜åŒ–è¿‡ç¨‹è´¡çŒ®æ›´å¤§ã€‚è¿™ç§ç®€å•çš„æ“ä½œæœ‰æ•ˆåœ°å‡è½»äº†è¿‡åº¦æ‹Ÿåˆé—®é¢˜å¹¶æé«˜äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚é€šè¿‡ç®€å•åœ°å°†DropGaussianåº”ç”¨äºåŸå§‹3DGSæ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŸºå‡†æ•°æ®é›†çš„ç¨€ç–è§†å›¾ç¯å¢ƒä¸­å®ç°ä¸ç°æœ‰åŸºäºå…ˆéªŒçš„3DGSæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”æ²¡æœ‰ä»»ä½•é¢å¤–çš„å¤æ‚æ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/DCVL-3D/DropGaussian%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/DCVL-3D/DropGaussianå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00773v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºDropGaussiançš„å…ˆéªŒæ–¹æ³•ï¼Œé€šè¿‡éšæœºç§»é™¤è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ–¯å‡½æ•°ï¼Œæ”¹å–„äº†ä¸‰ç»´é«˜æ–¯é‡‡æ ·åœ¨ç¨€ç–è§†å›¾ä¸‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡äº†æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚æ­¤æ–¹æ³•ç®€å•æ˜“è¡Œï¼Œåªéœ€åº”ç”¨äºåŸå§‹çš„ä¸‰ç»´é«˜æ–¯é‡‡æ ·æ¡†æ¶ï¼Œå³å¯åœ¨åŸºå‡†æ•°æ®é›†çš„ç¨€ç–è§†å›¾è®¾ç½®ä¸­ï¼Œå®ç°ä¸ç°æœ‰åŸºäºå…ˆéªŒçš„ä¸‰ç»´é«˜æ–¯é‡‡æ ·æ–¹æ³•ç›¸å½“çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D Gaussian Splattingï¼ˆ3DGSï¼‰åœ¨ç¨€ç–è§†å›¾ä¸‹ä¼šé¢ä¸´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯¼è‡´æ–°è§†å›¾å›¾åƒè´¨é‡ä¸‹é™ã€‚</li>
<li>DropGaussianæ˜¯ä¸€ç§æ–°é¢–çš„å…ˆéªŒæ–¹æ³•ï¼Œé€šè¿‡éšæœºç§»é™¤è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ–¯å‡½æ•°æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DropGaussianå…è®¸æœªè¢«æ’é™¤çš„é«˜æ–¯å‡½æ•°æ‹¥æœ‰æ›´å¤§çš„æ¢¯åº¦ï¼Œæå‡å…¶å¯è§æ€§ï¼Œä½¿å‰©ä½™çš„é«˜æ–¯å‡½æ•°å¯¹ä¼˜åŒ–è¿‡ç¨‹åšå‡ºæ›´å¤§è´¡çŒ®ã€‚</li>
<li>DropGaussianæ–¹æ³•ç®€å•æ˜“è¡Œï¼Œèƒ½æœ‰æ•ˆç¼“è§£è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæå‡æ–°è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li>
<li>DropGaussianèƒ½åœ¨ä¸ä½¿ç”¨é¢å¤–çš„å¤æ‚æ€§çš„æƒ…å†µä¸‹ï¼Œå®ç°ä¸ç°æœ‰åŸºäºå…ˆéªŒçš„3DGSæ–¹æ³•åœ¨ç¨€ç–è§†å›¾è®¾ç½®ä¸­çš„ç«äº‰æ€§èƒ½ã€‚</li>
<li>DropGaussiançš„æºä»£ç å’Œæ¨¡å‹å·²ç»å…¬å¼€å‘å¸ƒåœ¨ç›¸å…³GitHubé“¾æ¥ä¸Šã€‚å¯¹äºå…¬ä¼—å¯å…è´¹è·å–å¹¶ä½¿ç”¨è¿™ä¸€èµ„æºç”¨äºå­¦ä¹ å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95124b0be226fded441d398aee159433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-089d83b70a5dabf4996c1224f9817787.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98f79cb33863ba8bf22796614e80c474.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94002c2942f6bed02a3b34dbf298fc68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0c76c79ba42b2e0c9f682c8a8d4e53.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UnIRe-Unsupervised-Instance-Decomposition-for-Dynamic-Urban-Scene-Reconstruction"><a href="#UnIRe-Unsupervised-Instance-Decomposition-for-Dynamic-Urban-Scene-Reconstruction" class="headerlink" title="UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene   Reconstruction"></a>UnIRe: Unsupervised Instance Decomposition for Dynamic Urban Scene   Reconstruction</h2><p><strong>Authors:Yunxuan Mao, Rong Xiong, Yue Wang, Yiyi Liao</strong></p>
<p>Reconstructing and decomposing dynamic urban scenes is crucial for autonomous driving, urban planning, and scene editing. However, existing methods fail to perform instance-aware decomposition without manual annotations, which is crucial for instance-level scene editing.We propose UnIRe, a 3D Gaussian Splatting (3DGS) based approach that decomposes a scene into a static background and individual dynamic instances using only RGB images and LiDAR point clouds. At its core, we introduce 4D superpoints, a novel representation that clusters multi-frame LiDAR points in 4D space, enabling unsupervised instance separation based on spatiotemporal correlations. These 4D superpoints serve as the foundation for our decomposed 4D initialization, i.e., providing spatial and temporal initialization to train a dynamic 3DGS for arbitrary dynamic classes without requiring bounding boxes or object templates.Furthermore, we introduce a smoothness regularization strategy in both 2D and 3D space, further improving the temporal stability.Experiments on benchmark datasets show that our method outperforms existing methods in decomposed dynamic scene reconstruction while enabling accurate and flexible instance-level editing, making it a practical solution for real-world applications. </p>
<blockquote>
<p>é‡å»ºå’Œåˆ†è§£åŠ¨æ€åŸå¸‚åœºæ™¯å¯¹è‡ªåŠ¨é©¾é©¶ã€åŸå¸‚è§„åˆ’å’Œåœºæ™¯ç¼–è¾‘è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•åœ¨æ²¡æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„æƒ…å†µä¸‹æ‰§è¡Œå®ä¾‹æ„ŸçŸ¥åˆ†è§£ï¼Œè¿™å¯¹äºå®ä¾‹çº§åœºæ™¯ç¼–è¾‘è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†UnIReï¼Œè¿™æ˜¯ä¸€ç§åŸºäº3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ï¼Œå®ƒä»…ä½¿ç”¨RGBå›¾åƒå’Œæ¿€å…‰é›·è¾¾ç‚¹äº‘å°†åœºæ™¯åˆ†è§£ä¸ºé™æ€èƒŒæ™¯å’Œå•ä¸ªåŠ¨æ€å®ä¾‹ã€‚åœ¨æ ¸å¿ƒéƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼•å…¥äº†4Dè¶…ç‚¹ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥åœ¨4Dç©ºé—´ä¸­èšé›†å¤šå¸§æ¿€å…‰é›·è¾¾ç‚¹ï¼ŒåŸºäºæ—¶ç©ºç›¸å…³æ€§å®ç°æ— ç›‘ç£å®ä¾‹åˆ†ç¦»ã€‚è¿™äº›4Dè¶…ç‚¹æ˜¯æˆ‘ä»¬åˆ†è§£çš„4Dåˆå§‹åŒ–çš„åŸºç¡€ï¼Œå³æä¾›ç©ºé—´å’Œæ—¶é—´çš„åˆå§‹åŒ–ï¼Œä»¥è®­ç»ƒåŠ¨æ€3DGSï¼Œè€Œæ— éœ€è¾¹ç•Œæ¡†æˆ–å¯¹è±¡æ¨¡æ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨2Då’Œ3Dç©ºé—´ä¸­å¼•å…¥äº†å¹³æ»‘æ­£åˆ™åŒ–ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ—¶é—´ç¨³å®šæ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†è§£åŠ¨æ€åœºæ™¯é‡å»ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†å‡†ç¡®çµæ´»çš„å®ä¾‹çº§ç¼–è¾‘ï¼Œä½¿å…¶æˆä¸ºç°å®ä¸–ç•Œåº”ç”¨çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00763v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     åŠ¨æ€åŸå¸‚åœºæ™¯çš„é‡å»ºå’Œåˆ†è§£å¯¹äºè‡ªåŠ¨é©¾é©¶ã€åŸå¸‚è§„åˆ’å’Œåœºæ™¯ç¼–è¾‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•åœ¨ä¸ä¾èµ–äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹å®ç°å®ä¾‹æ„ŸçŸ¥åˆ†è§£ï¼Œè¿™å¯¹äºå®ä¾‹çº§åˆ«çš„åœºæ™¯ç¼–è¾‘è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†UnIReï¼Œä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼æ¥ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ï¼Œä»…ä½¿ç”¨RGBå›¾åƒå’Œæ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®ï¼Œå°†åœºæ™¯åˆ†è§£ä¸ºé™æ€èƒŒæ™¯å’Œç‹¬ç«‹çš„åŠ¨æ€å®ä¾‹ã€‚æˆ‘ä»¬å¼•å…¥äº†4Dè¶…çº§ç‚¹è¿™ä¸€æ–°å‹è¡¨ç¤ºï¼Œé€šè¿‡æ—¶ç©ºç›¸å…³æ€§å¯¹å¤šå¸§æ¿€å…‰é›·è¾¾ç‚¹åœ¨4Dç©ºé—´è¿›è¡Œèšç±»ï¼Œå®ç°äº†æ— éœ€æ ‡æ³¨æ¡†æˆ–å¯¹è±¡æ¨¡æ¿çš„ä»»æ„åŠ¨æ€ç±»çš„åŠ¨æ€ä¸‰ç»´é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨äºŒç»´å’Œä¸‰ç»´ç©ºé—´ä¸­éƒ½å¼•å…¥äº†å¹³æ»‘æ­£åˆ™åŒ–ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ—¶é—´ç¨³å®šæ€§ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†è§£åŠ¨æ€åœºæ™¯é‡å»ºæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶èƒ½å¤Ÿå®ç°ç²¾ç¡®çµæ´»çš„å®ä¾‹çº§åˆ«ç¼–è¾‘ï¼Œä½¿å…¶æˆä¸ºå®é™…åº”ç”¨ä¸­çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŠ¨æ€åŸå¸‚åœºæ™¯çš„é‡å»ºå’Œåˆ†è§£å¯¹è‡ªåŠ¨é©¾é©¶ã€åŸå¸‚è§„åˆ’å’Œåœºæ™¯ç¼–è¾‘è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹å®ä¾‹æ„ŸçŸ¥åˆ†è§£çš„èƒ½åŠ›ï¼Œéœ€è¦æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>UnIReæ–¹æ³•åŸºäºRGBå›¾åƒå’Œæ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®åˆ†è§£åœºæ™¯ã€‚</li>
<li>å¼•å…¥4Dè¶…çº§ç‚¹è¡¨ç¤ºï¼Œå®ç°åŸºäºæ—¶ç©ºç›¸å…³æ€§çš„å®ä¾‹åˆ†ç¦»ã€‚</li>
<li>ä¸éœ€è¦æ ‡æ³¨æ¡†æˆ–å¯¹è±¡æ¨¡æ¿è¿›è¡ŒåŠ¨æ€ä¸‰ç»´é‡å»ºã€‚</li>
<li>åœ¨äºŒç»´å’Œä¸‰ç»´ç©ºé—´å¼•å…¥å¹³æ»‘æ­£åˆ™åŒ–ç­–ç•¥æé«˜æ—¶é—´ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49b49aa05eb3c55d5ac0241199aea96f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9322b735f7ea1460d30571f9e159c80.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4db28f4480b9bd8b3ab1f145405897ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ea1665f7fd36ac0436dd433aa8d759f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Monocular-and-Generalizable-Gaussian-Talking-Head-Animation"><a href="#Monocular-and-Generalizable-Gaussian-Talking-Head-Animation" class="headerlink" title="Monocular and Generalizable Gaussian Talking Head Animation"></a>Monocular and Generalizable Gaussian Talking Head Animation</h2><p><strong>Authors:Shengjie Gong, Haojie Li, Jiapeng Tang, Dongming Hu, Shuangping Huang, Hao Chen, Tianshui Chen, Zhuoman Liu</strong></p>
<p>In this work, we introduce Monocular and Generalizable Gaussian Talking Head Animation (MGGTalk), which requires monocular datasets and generalizes to unseen identities without personalized re-training. Compared with previous 3D Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or tedious personalized learning&#x2F;inference, MGGtalk enables more practical and broader applications. However, in the absence of multi-view and personalized training data, the incompleteness of geometric and appearance information poses a significant challenge. To address these challenges, MGGTalk explores depth information to enhance geometric and facial symmetry characteristics to supplement both geometric and appearance features. Initially, based on the pixel-wise geometric information obtained from depth estimation, we incorporate symmetry operations and point cloud filtering techniques to ensure a complete and precise position parameter for 3DGS. Subsequently, we adopt a two-stage strategy with symmetric priors for predicting the remaining 3DGS parameters. We begin by predicting Gaussian parameters for the visible facial regions of the source image. These parameters are subsequently utilized to improve the prediction of Gaussian parameters for the non-visible regions. Extensive experiments demonstrate that MGGTalk surpasses previous state-of-the-art methods, achieving superior performance across various metrics. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å•ç›®é€šç”¨é«˜æ–¯è¯´è¯äººå¤´åŠ¨ç”»ï¼ˆMGGTalkï¼‰æŠ€æœ¯ï¼Œå®ƒåªéœ€è¦å•ç›®æ•°æ®é›†ï¼Œå¹¶èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„èº«ä»½è€Œæ— éœ€ä¸ªæ€§åŒ–å†è®­ç»ƒã€‚ä¸ä¹‹å‰éœ€è¦éš¾ä»¥è·å–çš„å¤šè§†è§’æ•°æ®é›†æˆ–ç¹çä¸ªæ€§åŒ–å­¦ä¹ &#x2F;æ¨æ–­çš„3Dé«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒMGGTalkä½¿å®é™…åº”ç”¨å’Œæ›´å¹¿æ³›çš„åº”ç”¨æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰å¤šè§†è§’å’Œä¸ªæ€§åŒ–è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå‡ ä½•å’Œå¤–è§‚ä¿¡æ¯çš„å®Œæ•´æ€§æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒMGGTalkæ¢ç´¢æ·±åº¦ä¿¡æ¯ä»¥å¢å¼ºå‡ ä½•å’Œé¢éƒ¨å¯¹ç§°ç‰¹å¾ï¼Œä»¥è¡¥å……å‡ ä½•å’Œå¤–è§‚ç‰¹å¾ã€‚é¦–å…ˆï¼ŒåŸºäºä»æ·±åº¦ä¼°è®¡è·å¾—çš„åƒç´ çº§å‡ ä½•ä¿¡æ¯ï¼Œæˆ‘ä»¬ç»“åˆäº†å¯¹ç§°æ“ä½œå’Œç‚¹äº‘æ»¤æ³¢æŠ€æœ¯ï¼Œä»¥ç¡®ä¿3DGSçš„å®Œæ•´å’Œç²¾ç¡®çš„ä½ç½®å‚æ•°ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨å…·æœ‰å¯¹ç§°å…ˆéªŒçš„ä¸¤é˜¶æ®µç­–ç•¥æ¥é¢„æµ‹å‰©ä½™çš„3DGSå‚æ•°ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹æºå›¾åƒçš„å¯è§é¢éƒ¨åŒºåŸŸé¢„æµ‹é«˜æ–¯å‚æ•°ã€‚è¿™äº›å‚æ•°éšåç”¨äºæ”¹è¿›éå¯è§åŒºåŸŸçš„é«˜æ–¯å‚æ•°é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMGGTalkè¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00665v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å•ç›®é€šç”¨é«˜æ–¯è¯´è¯äººå¤´åŠ¨ç”»ï¼ˆMGGTalkï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åŸºäºå•ç›®æ•°æ®é›†ï¼Œå¯æ¨å¹¿åˆ°æœªè§è¿‡çš„èº«ä»½è€Œæ— éœ€ä¸ªæ€§åŒ–å†è®­ç»ƒã€‚ä¸ä¹‹å‰éœ€è¦éš¾ä»¥è·å–çš„å¤šè§†è§’æ•°æ®é›†æˆ–ç¹çä¸ªæ€§åŒ–å­¦ä¹ &#x2F;æ¨æ–­çš„3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒMGGTalkå…·æœ‰æ›´å®é™…å’Œæ›´å¹¿æ³›çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³ç¼ºä¹å¤šè§†è§’å’Œä¸ªæ€§åŒ–è®­ç»ƒæ•°æ®æ‰€å¸¦æ¥çš„å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ä¸å®Œæ•´çš„é—®é¢˜ï¼ŒMGGTalkæ¢ç´¢äº†æ·±åº¦ä¿¡æ¯ä»¥å¢å¼ºå‡ ä½•å’Œé¢éƒ¨å¯¹ç§°ç‰¹å¾ï¼Œä»¥è¡¥å……å‡ ä½•å’Œå¤–è§‚ç‰¹å¾ã€‚é€šè¿‡æ·±åº¦ä¼°è®¡è·å¾—çš„åƒç´ çº§å‡ ä½•ä¿¡æ¯ï¼Œç»“åˆå¯¹ç§°æ“ä½œå’Œç‚¹äº‘è¿‡æ»¤æŠ€æœ¯ï¼Œç¡®ä¿3DGSçš„ä½ç½®å‚æ•°å®Œæ•´ä¸”ç²¾ç¡®ã€‚é‡‡ç”¨å…·æœ‰å¯¹ç§°å…ˆéªŒçš„ä¸¤é˜¶æ®µç­–ç•¥æ¥é¢„æµ‹å‰©ä½™çš„3DGSå‚æ•°ã€‚é¦–å…ˆé¢„æµ‹æºå›¾åƒå¯è§é¢éƒ¨åŒºåŸŸçš„é«˜æ–¯å‚æ•°ï¼Œç„¶åç”¨äºæ”¹è¿›éå¯è§åŒºåŸŸçš„é«˜æ–¯å‚æ•°é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMGGTalkè¶…è¶Šç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MGGTalkæ˜¯ä¸€ç§åŸºäºå•ç›®æ•°æ®é›†çš„é«˜è°ˆè¯´è¯äººå¤´åŠ¨ç”»æŠ€æœ¯ï¼Œå¯æ¨å¹¿åˆ°æœªè§è¿‡çš„èº«ä»½ï¼Œæ— éœ€ä¸ªæ€§åŒ–å†è®­ç»ƒã€‚</li>
<li>ä¸éœ€è¦å¤šè§†è§’æ•°æ®é›†çš„3DGSæ–¹æ³•ç›¸æ¯”ï¼ŒMGGTalkæ›´å…·å®é™…åº”ç”¨æ€§å’Œå¹¿æ³›æ€§ã€‚</li>
<li>MGGTalké€šè¿‡æ¢ç´¢æ·±åº¦ä¿¡æ¯æ¥è§£å†³å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ä¸å®Œæ•´çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨åƒç´ çº§å‡ ä½•ä¿¡æ¯ç»“åˆå¯¹ç§°æ“ä½œå’Œç‚¹äº‘è¿‡æ»¤æŠ€æœ¯ï¼Œç¡®ä¿3DGSä½ç½®å‚æ•°çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MGGTalké‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œåˆ©ç”¨å¯¹ç§°å…ˆéªŒæ¥é¢„æµ‹3DGSçš„å‰©ä½™å‚æ•°ã€‚</li>
<li>MGGTalké€šè¿‡é¢„æµ‹æºå›¾åƒå¯è§é¢éƒ¨åŒºåŸŸçš„é«˜æ–¯å‚æ•°ï¼Œè¿›è€Œæ”¹è¿›éå¯è§åŒºåŸŸçš„é«˜æ–¯å‚æ•°é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0cb9b66d7917ae04893d14574dae1c4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cb003298d1d12e0a7c9c740f48cff60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7c5a4511645c758a6f3a3b91b815bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-840be5e63daf121b55e3faed614e8921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-631e2e86c91179320867246457faaffc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86b01b898aab33579c041759577708ef.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Coca-Splat-Collaborative-Optimization-for-Camera-Parameters-and-3D-Gaussians"><a href="#Coca-Splat-Collaborative-Optimization-for-Camera-Parameters-and-3D-Gaussians" class="headerlink" title="Coca-Splat: Collaborative Optimization for Camera Parameters and 3D   Gaussians"></a>Coca-Splat: Collaborative Optimization for Camera Parameters and 3D   Gaussians</h2><p><strong>Authors:Jiamin Wu, Hongyang Li, Xiaoke Jiang, Yuan Yao, Lei Zhang</strong></p>
<p>In this work, we introduce Coca-Splat, a novel approach to addressing the challenges of sparse view pose-free scene reconstruction and novel view synthesis (NVS) by jointly optimizing camera parameters with 3D Gaussians. Inspired by deformable DEtection TRansformer, we design separate queries for 3D Gaussians and camera parameters and update them layer by layer through deformable Transformer layers, enabling joint optimization in a single network. This design demonstrates better performance because to accurately render views that closely approximate ground-truth images relies on precise estimation of both 3D Gaussians and camera parameters. In such a design, the centers of 3D Gaussians are projected onto each view by camera parameters to get projected points, which are regarded as 2D reference points in deformable cross-attention. With camera-aware multi-view deformable cross-attention (CaMDFA), 3D Gaussians and camera parameters are intrinsically connected by sharing the 2D reference points. Additionally, 2D reference point determined rays (RayRef) defined from camera centers to the reference points assist in modeling relationship between 3D Gaussians and camera parameters through RQ-decomposition on an overdetermined system of equations derived from the rays, enhancing the relationship between 3D Gaussians and camera parameters. Extensive evaluation shows that our approach outperforms previous methods, both pose-required and pose-free, on RealEstate10K and ACID within the same pose-free setting. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Coca-Splatè¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡è”åˆä¼˜åŒ–ç›¸æœºå‚æ•°ä¸ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒæ¥è§£å†³æ— å§¿æ€åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•çµæ„Ÿæ¥æºäºå¯å˜å½¢æ£€æµ‹å˜å‹å™¨ï¼ˆDETRï¼‰ï¼Œæˆ‘ä»¬ä¸ºä¸‰ç»´é«˜æ–¯åˆ†å¸ƒå’Œç›¸æœºå‚æ•°è®¾è®¡äº†å•ç‹¬çš„æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¯å˜å½¢Transformerå±‚é€å±‚æ›´æ–°å®ƒä»¬ï¼Œä»è€Œèƒ½å¤Ÿåœ¨å•ä¸ªç½‘ç»œä¸­å®ç°è”åˆä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡å±•ç¤ºäº†æ›´å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºå‡†ç¡®æ¸²æŸ“æ¥è¿‘çœŸå®å›¾åƒçš„è§†å›¾ä¾èµ–äºå¯¹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒå’Œç›¸æœºå‚æ•°çš„ç²¾ç¡®ä¼°è®¡ã€‚åœ¨è¿™ç§è®¾è®¡ä¸­ï¼Œä¸‰ç»´é«˜æ–¯åˆ†å¸ƒçš„ä¸­å¿ƒé€šè¿‡ç›¸æœºå‚æ•°æŠ•å½±åˆ°æ¯ä¸ªè§†å›¾ä¸Šï¼Œå¾—åˆ°æŠ•å½±ç‚¹ï¼Œè¿™äº›ç‚¹è¢«è§†ä¸ºå¯å˜å½¢äº¤å‰æ³¨æ„åŠ›ä¸­çš„äºŒç»´å‚è€ƒç‚¹ã€‚é€šè¿‡ç›¸æœºæ„ŸçŸ¥çš„å¤šè§†è§’å¯å˜å½¢äº¤å‰æ³¨æ„åŠ›ï¼ˆCaMDFAï¼‰ï¼Œä¸‰ç»´é«˜æ–¯åˆ†å¸ƒå’Œç›¸æœºå‚æ•°é€šè¿‡å…±äº«äºŒç»´å‚è€ƒç‚¹å›ºæœ‰åœ°è¿æ¥åœ¨ä¸€èµ·ã€‚æ­¤å¤–ï¼Œä»ç›¸æœºä¸­å¿ƒåˆ°å‚è€ƒç‚¹çš„äºŒç»´å‚è€ƒç‚¹ç¡®å®šå°„çº¿ï¼ˆRayRefï¼‰æœ‰åŠ©äºé€šè¿‡å°„çº¿å¾—å‡ºçš„è¶…å®šç³»ç»Ÿæ–¹ç¨‹çš„RQåˆ†è§£æ¥å»ºæ¨¡ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒä¸ç›¸æœºå‚æ•°ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå¢å¼ºä¸¤è€…ä¹‹é—´çš„è”ç³»ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®åœ°äº§10Kå’Œé…¸åŒ–å‰‚çš„ç›¸åŒæ— å§¿æ€è®¾ç½®ä¸‹ï¼Œç›¸å¯¹äºä»¥å‰çš„æ–¹æ³•ï¼ˆåŒ…æ‹¬éœ€è¦å§¿æ€å’Œä¸éœ€å§¿æ€çš„æ–¹æ³•ï¼‰è¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00639v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Coca-Splatæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ— å§¿æ€åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆï¼ˆNVSï¼‰çš„æŒ‘æˆ˜çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è”åˆä¼˜åŒ–ç›¸æœºå‚æ•°å’Œ3Dé«˜æ–¯å€¼æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚é€šè¿‡å¯å˜å½¢æ£€æµ‹å˜æ¢å™¨ï¼ˆDETRï¼‰çš„çµæ„Ÿï¼Œä¸º3Dé«˜æ–¯å€¼å’Œç›¸æœºå‚æ•°è®¾è®¡ç‹¬ç«‹çš„æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¯å˜å½¢å˜æ¢å™¨é€å±‚è¿›è¡Œæ›´æ–°ï¼Œä½¿ä¸¤è€…åœ¨ä¸€ä¸ªç½‘ç»œä¸­è”åˆä¼˜åŒ–ã€‚è¯¥è®¾è®¡å› å‡†ç¡®æ¸²æŸ“ä¾èµ–äºä¸¤è€…ç²¾ç¡®ä¼°è®¡çš„è§†å›¾è€Œè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚è®¾è®¡å°†3Dé«˜æ–¯å€¼çš„ä¸­å¿ƒé€šè¿‡ç›¸æœºå‚æ•°æŠ•å½±åˆ°æ¯ä¸ªè§†å›¾ä¸Šï¼Œå¾—åˆ°æŠ•å½±ç‚¹ï¼Œè¢«è§†ä¸ºå¯å˜å½¢äº¤å‰æ³¨æ„ä¸­çš„2Då‚è€ƒç‚¹ã€‚é€šè¿‡å…±äº«è¿™äº›å‚è€ƒç‚¹ï¼Œç›¸æœºæ„ŸçŸ¥çš„å¤šè§†è§’å¯å˜å½¢äº¤å‰æ³¨æ„ï¼ˆCaMDFAï¼‰å°†3Dé«˜æ–¯å€¼å’Œç›¸æœºå‚æ•°å†…åœ¨è”ç³»èµ·æ¥ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®šä¹‰ä»ç›¸æœºä¸­å¿ƒåˆ°å‚è€ƒç‚¹çš„å°„çº¿ï¼Œå»ºç«‹3Dé«˜æ–¯ä¸ç›¸æœºå‚æ•°ä¹‹é—´çš„å…³ç³»æ¨¡å‹ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚è¯„ä¼°è¡¨æ˜ï¼Œåœ¨ç›¸åŒæ— å§¿æ€è®¾ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨RealEstate10Kå’ŒACIDä¸Šå‡ä¼˜äºä¹‹å‰çš„å§¿æ€è¦æ±‚å’Œæ— å§¿æ€æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Coca-Splatæ˜¯ä¸€ç§è§£å†³ç¨€ç–è§†è§’æ— å§¿æ€åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæŒ‘æˆ˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è”åˆä¼˜åŒ–ç›¸æœºå‚æ•°å’Œ3Dé«˜æ–¯å€¼æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>è®¾è®¡çµæ„Ÿæ¥æºäºå¯å˜å½¢æ£€æµ‹å˜æ¢å™¨ï¼ˆDETRï¼‰ï¼Œä¸º3Dé«˜æ–¯å€¼å’Œç›¸æœºå‚æ•°åˆ†åˆ«è®¾è®¡æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å¯å˜å½¢å˜æ¢å™¨è¿›è¡Œè”åˆä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡æŠ•å½±3Dé«˜æ–¯å€¼çš„ä¸­å¿ƒåˆ°æ¯ä¸ªè§†å›¾ä¸Šè·å¾—æŠ•å½±ç‚¹ï¼Œè¿™äº›ç‚¹è¢«è§†ä¸º2Då‚è€ƒç‚¹ã€‚</li>
<li>é€šè¿‡å…±äº«2Då‚è€ƒç‚¹ï¼Œå°†ç›¸æœºæ„ŸçŸ¥çš„å¤šè§†è§’å¯å˜å½¢äº¤å‰æ³¨æ„ï¼ˆCaMDFAï¼‰ç”¨äºè¿æ¥3Dé«˜æ–¯å€¼å’Œç›¸æœºå‚æ•°ã€‚</li>
<li>é€šè¿‡å®šä¹‰ä»ç›¸æœºä¸­å¿ƒåˆ°å‚è€ƒç‚¹çš„å°„çº¿æ¥å¢å¼ºæ¨¡å‹å…³ç³»ï¼Œè¿›ä¸€æ­¥è¿æ¥3Dé«˜æ–¯å€¼å’Œç›¸æœºå‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af51fd954941dbf3477f12f0c7305fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b62ac268e9078bcb0f3f0972541cd75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be405b69a258b7267e40ec75dee1a453.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c07c638e3a4e5c5a03b996b79c82f5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Distilling-Multi-view-Diffusion-Models-into-3D-Generators"><a href="#Distilling-Multi-view-Diffusion-Models-into-3D-Generators" class="headerlink" title="Distilling Multi-view Diffusion Models into 3D Generators"></a>Distilling Multi-view Diffusion Models into 3D Generators</h2><p><strong>Authors:Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</strong></p>
<p>We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacherâ€™s probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: <a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DD3Gï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é«˜æ–¯æ‹¼è´´æŠ€æœ¯å°†å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰è’¸é¦åˆ°3Dç”Ÿæˆå™¨çš„æ–¹æ³•ã€‚DD3Gé€šè¿‡æ¨¡æ‹ŸMV-DMçš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œå‹ç¼©å¹¶é›†æˆäº†å¤§é‡çš„è§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ï¼Œç¡®ä¿è’¸é¦åçš„ç”Ÿæˆå™¨æ¯”ä»…å¯¹3Dæ•°æ®è¿›è¡Œè®­ç»ƒçš„ç”Ÿæˆå™¨å…·æœ‰æ›´å¥½çš„é€šç”¨æ€§ã€‚ä¸ä¹‹å‰çš„å¹³å‡ä¼˜åŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å°†MV-DMå’Œ3Dç”Ÿæˆå™¨çš„è¡¨ç¤ºç©ºé—´å¯¹é½ï¼Œä»¥å°†æ•™å¸ˆçš„æ¦‚ç‡æµä¼ è¾“ç»™å­¦ç”Ÿï¼Œä»è€Œé¿å…ç”±æ¦‚ç‡é‡‡æ ·å¼•èµ·çš„ä¼˜åŒ–ç›®æ ‡ä¸ä¸€è‡´ã€‚æ¦‚ç‡æµçš„å¼•å…¥å’Œ3Dé«˜æ–¯ä¸­å„ç§å±æ€§çš„è€¦åˆç»™ç”Ÿæˆè¿‡ç¨‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PEPDç”Ÿæˆå™¨ï¼Œå®ƒç”±æ¨¡å¼æå–å’Œæ¸è¿›è§£ç ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°èåˆæ¦‚ç‡æµï¼Œå¹¶åœ¨0.06ç§’å†…å°†å•å¹…å›¾åƒè½¬æ¢ä¸º3Dé«˜æ–¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘çŸ¥è¯†æŸå¤±å¹¶å…‹æœç¨€ç–è§†å›¾ç›‘ç£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè”åˆä¼˜åŒ–ç›®æ ‡ï¼Œé€šè¿‡æ˜¾å¼ç›‘ç£å’Œéšå¼éªŒè¯ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚æˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„2Dç”Ÿæˆæ¨¡å‹ï¼Œç¼–è¯‘äº†12ä¸‡å¼ é«˜è´¨é‡RGBAå›¾åƒè¿›è¡Œè’¸é¦ã€‚åœ¨åˆæˆæ•°æ®é›†å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/]%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://qinbaigao.github.io/DD3G_project/]ä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00457v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DD3Gæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿå¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼ˆMV-DMï¼‰çš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹ï¼Œå°†MV-DMå‹ç¼©å¹¶è½¬åŒ–ä¸ºä¸€ä¸ªåŸºäºé«˜æ–¯å¡«å……çš„3Dç”Ÿæˆå™¨ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç¡®å®šæ€§è’¸é¦æŠ€æœ¯ï¼Œé€šè¿‡æ¨¡æ‹ŸMV-DMçš„ODEè½¨è¿¹ï¼Œæ•´åˆè§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ï¼Œæé«˜ç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPEPDçš„ç”Ÿæˆå™¨ç»“æ„ï¼Œä»¥é«˜æ•ˆèåˆæ¦‚ç‡æµå¹¶å¿«é€Ÿå°†å•å›¾åƒè½¬æ¢ä¸ºä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚åŒæ—¶ï¼Œä¸ºäº†é™ä½çŸ¥è¯†æŸå¤±å¹¶å…‹æœç¨€ç–è§†å›¾ç›‘ç£é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ç§è”åˆä¼˜åŒ–ç›®æ ‡ï¼Œç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DD3Gæ˜¯ä¸€ä¸ªåŸºäºå¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„æ¨¡æ‹Ÿå¹¶å°†å…¶æ•´åˆä¸º3Dç”Ÿæˆå™¨çš„æ–¹æ¡ˆã€‚å®ƒä½¿ç”¨äº†ç¡®å®šæ€§è’¸é¦æŠ€æœ¯æé«˜äº†ç”Ÿæˆå™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DD3Gä½¿ç”¨æ¨¡æ‹Ÿçš„å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰è½¨è¿¹æ¥å‹ç¼©å’Œæ•´åˆè§†è§‰å’Œç©ºé—´å‡ ä½•çŸ¥è¯†ã€‚å®ƒé€šè¿‡é¿å…ä½¿ç”¨æ¦‚ç‡é‡‡æ ·æŠ€æœ¯ï¼Œå®ç°äº†æ›´å¥½çš„æ¨¡å‹å¯¹é½å’Œä¼˜åŒ–ç›®æ ‡ä¸€è‡´æ€§ã€‚</li>
<li>PEPDç”Ÿæˆå™¨ç»“æ„ä½¿å¾—é«˜æ•ˆèåˆæ¦‚ç‡æµæˆä¸ºå¯èƒ½ï¼Œèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…å°†å•å›¾åƒè½¬æ¢ä¸ºä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚è¿™ç§ç»“æ„æé«˜äº†ç”Ÿæˆè¿‡ç¨‹çš„æ•ˆç‡å’Œè´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1072ca818acce967788bf801b1c2be1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd5ced50961b209c9ff6ef2cd5c0663e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b487ff167a29735997ac5d402d1d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eb6f5bc327dbaba705f3566712c6fc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b572b2194b81b3883cafe56abae3366.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ADGaussian-Generalizable-Gaussian-Splatting-for-Autonomous-Driving-with-Multi-modal-Inputs"><a href="#ADGaussian-Generalizable-Gaussian-Splatting-for-Autonomous-Driving-with-Multi-modal-Inputs" class="headerlink" title="ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with   Multi-modal Inputs"></a>ADGaussian: Generalizable Gaussian Splatting for Autonomous Driving with   Multi-modal Inputs</h2><p><strong>Authors:Qi Song, Chenghong Li, Haotong Lin, Sida Peng, Rui Huang</strong></p>
<p>We present a novel approach, termed ADGaussian, for generalizable street scene reconstruction. The proposed method enables high-quality rendering from single-view input. Unlike prior Gaussian Splatting methods that primarily focus on geometry refinement, we emphasize the importance of joint optimization of image and depth features for accurate Gaussian prediction. To this end, we first incorporate sparse LiDAR depth as an additional input modality, formulating the Gaussian prediction process as a joint learning framework of visual information and geometric clue. Furthermore, we propose a multi-modal feature matching strategy coupled with a multi-scale Gaussian decoding model to enhance the joint refinement of multi-modal features, thereby enabling efficient multi-modal Gaussian learning. Extensive experiments on two large-scale autonomous driving datasets, Waymo and KITTI, demonstrate that our ADGaussian achieves state-of-the-art performance and exhibits superior zero-shot generalization capabilities in novel-view shifting. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºADGaussianï¼Œç”¨äºå¯æ³›åŒ–çš„è¡—é“åœºæ™¯é‡å»ºã€‚æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿä»å•è§†å›¾è¾“å…¥å®ç°é«˜è´¨é‡æ¸²æŸ“ã€‚ä¸ä¸»è¦å…³æ³¨å‡ ä½•ç²¾åŒ–çš„å…ˆå‰çš„é«˜æ–¯Splattingæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬å¼ºè°ƒå›¾åƒå’Œæ·±åº¦ç‰¹å¾è”åˆä¼˜åŒ–çš„é‡è¦æ€§ï¼Œä»¥å®ç°å‡†ç¡®çš„é«˜æ–¯é¢„æµ‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆçº³å…¥ç¨€ç–æ¿€å…‰é›·è¾¾æ·±åº¦ä½œä¸ºé™„åŠ è¾“å…¥æ¨¡å¼ï¼Œå°†é«˜æ–¯é¢„æµ‹è¿‡ç¨‹åˆ¶å®šä¸ºè§†è§‰ä¿¡æ¯å’Œå‡ ä½•çº¿ç´¢çš„è”åˆå­¦ä¹ æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼ç‰¹å¾åŒ¹é…ç­–ç•¥ï¼Œç»“åˆå¤šå°ºåº¦é«˜æ–¯è§£ç æ¨¡å‹ï¼Œä»¥å¢å¼ºå¤šæ¨¡å¼ç‰¹å¾çš„è”åˆä¼˜åŒ–ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡å¼é«˜æ–¯å­¦ä¹ ã€‚åœ¨Waymoå’ŒKITTIä¸¤ä¸ªå¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ADGaussianè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–°å‹è§†è§’è½¬æ¢ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00437v1">PDF</a> The project page can be found at   <a target="_blank" rel="noopener" href="https://maggiesong7.github.io/research/ADGaussian/">https://maggiesong7.github.io/research/ADGaussian/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ–¹æ³•ï¼Œåä¸ºADGaussianï¼Œç”¨äºå¯æ³›åŒ–çš„è¡—é“åœºæ™¯é‡å»ºã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ä»å•è§†è§’è¾“å…¥çš„é«˜è´¨é‡æ¸²æŸ“ã€‚ä¸åŒäºä»¥å¾€ä¸»è¦å…³æ³¨å‡ ä½•ç²¾ä¿®çš„Gaussian Splattingæ–¹æ³•ï¼ŒADGaussianå¼ºè°ƒå›¾åƒå’Œæ·±åº¦ç‰¹å¾è”åˆä¼˜åŒ–å¯¹äºå‡†ç¡®Gaussiané¢„æµ‹çš„é‡è¦æ€§ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œå¼•å…¥äº†ç¨€ç–æ¿€å…‰é›·è¾¾æ·±åº¦ä½œä¸ºé¢å¤–çš„è¾“å…¥æ¨¡å¼ï¼Œå°†Gaussiané¢„æµ‹è¿‡ç¨‹å…¬å¼åŒ–ä¸ºè§†è§‰ä¿¡æ¯å’Œå‡ ä½•çº¿ç´¢çš„è”åˆå­¦ä¹ æ¡†æ¶ã€‚åŒæ—¶ï¼Œæå‡ºäº†å¤šæ¨¡å¼ç‰¹å¾åŒ¹é…ç­–ç•¥å’Œå¤šå°ºåº¦Gaussianè§£ç æ¨¡å‹ï¼Œä»¥å¼ºåŒ–å¤šæ¨¡å¼ç‰¹å¾çš„è”åˆä¼˜åŒ–ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„å¤šæ¨¡å¼Gaussianå­¦ä¹ ã€‚åœ¨Waymoå’ŒKITTIä¸¤ä¸ªå¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒADGaussianè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æ–°å‹è§†è§’è½¬æ¢ä¸­å±•ç°å‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ADGaussianæ˜¯ä¸€ç§ç”¨äºè¡—é“åœºæ™¯é‡å»ºçš„æ–°å‹æ–¹æ³•ï¼Œå¯ä»å•è§†è§’å®ç°é«˜è´¨é‡æ¸²æŸ“ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒADGaussianå¼ºè°ƒå›¾åƒå’Œæ·±åº¦ç‰¹å¾çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥äº†ç¨€ç–æ¿€å…‰é›·è¾¾æ·±åº¦ä½œä¸ºé¢å¤–çš„è¾“å…¥æ¨¡å¼ã€‚</li>
<li>å°†Gaussiané¢„æµ‹è¿‡ç¨‹å…¬å¼åŒ–ä¸ºè§†è§‰ä¿¡æ¯å’Œå‡ ä½•çº¿ç´¢çš„è”åˆå­¦ä¹ æ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨äº†å¤šæ¨¡å¼ç‰¹å¾åŒ¹é…ç­–ç•¥å’Œå¤šå°ºåº¦Gaussianè§£ç æ¨¡å‹ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šï¼ŒADGaussianè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-959f05fec71966ca94a116309585ce7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29f65fb3ec19edf84518304a3e72bf8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37ccf38b568dfd547f566ee0cd5cf368.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b031f425fd82748bf94a468dc0c0c2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f01b6711dbb4887771d23b140a99fbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d6a5e13d2360b8df89f39dc4a979040.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Scene4U-Hierarchical-Layered-3D-Scene-Reconstruction-from-Single-Panoramic-Image-for-Your-Immerse-Exploration"><a href="#Scene4U-Hierarchical-Layered-3D-Scene-Reconstruction-from-Single-Panoramic-Image-for-Your-Immerse-Exploration" class="headerlink" title="Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single   Panoramic Image for Your Immerse Exploration"></a>Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single   Panoramic Image for Your Immerse Exploration</h2><p><strong>Authors:Zilong Huang, Jun He, Junyan Ye, Lihan Jiang, Weijia Li, Yiping Chen, Ting Han</strong></p>
<p>The reconstruction of immersive and realistic 3D scenes holds significant practical importance in various fields of computer vision and computer graphics. Typically, immersive and realistic scenes should be free from obstructions by dynamic objects, maintain global texture consistency, and allow for unrestricted exploration. The current mainstream methods for image-driven scene construction involves iteratively refining the initial image using a moving virtual camera to generate the scene. However, previous methods struggle with visual discontinuities due to global texture inconsistencies under varying camera poses, and they frequently exhibit scene voids caused by foreground-background occlusions. To this end, we propose a novel layered 3D scene reconstruction framework from panoramic image, named Scene4U. Specifically, Scene4U integrates an open-vocabulary segmentation model with a large language model to decompose a real panorama into multiple layers. Then, we employs a layered repair module based on diffusion model to restore occluded regions using visual cues and depth information, generating a hierarchical representation of the scene. The multi-layer panorama is then initialized as a 3D Gaussian Splatting representation, followed by layered optimization, which ultimately produces an immersive 3D scene with semantic and structural consistency that supports free exploration. Scene4U outperforms state-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE, while also achieving the fastest training speed. Additionally, to demonstrate the robustness of Scene4U and allow users to experience immersive scenes from various landmarks, we build WorldVista3D dataset for 3D scene reconstruction, which contains panoramic images of globally renowned sites. The implementation code and dataset will be released at <a target="_blank" rel="noopener" href="https://github.com/LongHZ140516/Scene4U">https://github.com/LongHZ140516/Scene4U</a> . </p>
<blockquote>
<p>æ²‰æµ¸å¼ä¸é€¼çœŸçš„ä¸‰ç»´åœºæ™¯é‡å»ºåœ¨è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦çš„å„ä¸ªé¢†åŸŸéƒ½å…·æœ‰é‡è¦çš„å®é™…æ„ä¹‰ã€‚é€šå¸¸ï¼Œæ²‰æµ¸å¼ä¸”é€¼çœŸçš„åœºæ™¯åº”è¯¥ä¸å—åŠ¨æ€ç‰©ä½“çš„é®æŒ¡å½±å“ï¼Œä¿æŒå…¨å±€çº¹ç†çš„ä¸€è‡´æ€§ï¼Œå¹¶å…è®¸æ— é™åˆ¶çš„æ¢ç´¢ã€‚å½“å‰ä¸»æµçš„å›¾åƒé©±åŠ¨åœºæ™¯æ„å»ºæ–¹æ³•é€šè¿‡ç§»åŠ¨è™šæ‹Ÿç›¸æœºå¯¹åˆå§‹å›¾åƒè¿›è¡Œè¿­ä»£ä¼˜åŒ–ä»¥ç”Ÿæˆåœºæ™¯ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ–¹æ³•åœ¨åº”å¯¹å› ç›¸æœºå§¿æ€å˜åŒ–å¯¼è‡´çš„å…¨å±€çº¹ç†ä¸ä¸€è‡´é—®é¢˜æ—¶å­˜åœ¨è§†è§‰ä¸è¿ç»­çš„é—®é¢˜ï¼Œå¹¶ä¸”ç»å¸¸ç”±äºå‰æ™¯ä¸èƒŒæ™¯çš„é®æŒ¡è€Œå¯¼è‡´åœºæ™¯ç©ºæ´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å…¨æ™¯å›¾åƒåˆ†å±‚ä¸‰ç»´åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œåä¸ºScene4Uã€‚å…·ä½“æ¥è¯´ï¼ŒScene4Uç»“åˆäº†å¼€æ”¾è¯æ±‡åˆ†å‰²æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†çœŸå®å…¨æ™¯å›¾åƒåˆ†è§£æˆå¤šä¸ªå±‚æ¬¡ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å±‚ä¿®å¤æ¨¡å—ï¼Œåˆ©ç”¨è§†è§‰çº¿ç´¢å’Œæ·±åº¦ä¿¡æ¯æ¢å¤è¢«é®æŒ¡çš„åŒºåŸŸï¼Œç”Ÿæˆåœºæ™¯çš„å±‚æ¬¡è¡¨ç¤ºã€‚å¤šå±‚å…¨æ™¯å›¾åƒè¢«åˆå§‹åŒ–ä¸ºä¸‰ç»´é«˜æ–¯é£æº…è¡¨ç¤ºï¼Œéšåè¿›è¡Œåˆ†å±‚ä¼˜åŒ–ï¼Œæœ€ç»ˆç”Ÿæˆå…·æœ‰è¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§çš„æ²‰æµ¸å¼ä¸‰ç»´åœºæ™¯ï¼Œæ”¯æŒè‡ªç”±æ¢ç´¢ã€‚Scene4Uåœ¨LPIPSä¸Šæé«˜äº†24.24%ï¼Œåœ¨BRISQUEä¸Šæé«˜äº†24.4%ï¼ŒåŒæ—¶è¾¾åˆ°äº†æœ€å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯æ˜Scene4Uçš„é²æ£’æ€§å¹¶å…è®¸ç”¨æˆ·ä»å„ç§åœ°æ ‡ä½“éªŒæ²‰æµ¸å¼åœºæ™¯ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç”¨äºä¸‰ç»´åœºæ™¯é‡å»ºçš„WorldVista3Dæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å…¨çƒçŸ¥åæ™¯ç‚¹çš„å…¨æ™¯å›¾åƒã€‚å®æ–½ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/LongHZ140516/Scene4U%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/LongHZ140516/Scene4Uå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00387v1">PDF</a> CVPR 2025, 11 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºScene4Uçš„æ–°å‹åˆ†å±‚3Dåœºæ™¯é‡å»ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»å…¨æ™¯å›¾åƒå‡ºå‘ï¼Œé€šè¿‡å¼€æ”¾å¼è¯æ±‡è¡¨åˆ†å‰²æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆï¼Œå°†çœŸå®å…¨æ™¯å›¾åƒåˆ†è§£ä¸ºå¤šå±‚ã€‚åˆ©ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å±‚ä¿®å¤æ¨¡å—ï¼Œé€šè¿‡è§†è§‰çº¿ç´¢å’Œæ·±åº¦ä¿¡æ¯æ¢å¤é®æŒ¡åŒºåŸŸï¼Œç”Ÿæˆåœºæ™¯çš„å±‚æ¬¡è¡¨ç¤ºã€‚å¤šå±‚å…¨æ™¯å›¾åƒåˆå§‹åŒ–ä¸º3Dé«˜æ–¯å–·æº…è¡¨ç¤ºï¼Œéšåè¿›è¡Œåˆ†å±‚ä¼˜åŒ–ï¼Œæœ€ç»ˆç”Ÿæˆå…·æœ‰è¯­ä¹‰å’Œç»“æ„ä¸€è‡´æ€§çš„æ²‰æµ¸å¼3Dåœºæ™¯ï¼Œæ”¯æŒè‡ªç”±æ¢ç´¢ã€‚Scene4Uæ€§èƒ½ä¼˜è¶Šï¼Œåœ¨LPIPSå’ŒBRISQUEæŒ‡æ ‡ä¸Šåˆ†åˆ«æé«˜äº†24.24%å’Œ24.40%ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦æœ€å¿«ã€‚ä¸ºå±•ç¤ºScene4Uçš„ç¨³å¥æ€§å¹¶è®©ç”¨æˆ·ä½“éªŒæ¥è‡ªä¸åŒåœ°æ ‡çš„æ²‰æµ¸å¼åœºæ™¯ï¼Œå»ºç«‹äº†WorldVista3Dæ•°æ®é›†ï¼ŒåŒ…å«å…¨çƒçŸ¥åæ™¯ç‚¹çš„å…¨æ™¯å›¾åƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ²‰æµ¸å¼3Dåœºæ™¯é‡å»ºåœ¨è®¡ç®—æœºè§†è§‰å’Œè®¡ç®—æœºå›¾å½¢å­¦é¢†åŸŸå…·æœ‰å®é™…é‡è¦æ€§ã€‚</li>
<li>å½“å‰ä¸»æµæ–¹æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–åˆå§‹å›¾åƒç”Ÿæˆåœºæ™¯ï¼Œä½†å­˜åœ¨å…¨çƒçº¹ç†ä¸ä¸€è‡´å’Œåœºæ™¯ç©ºæ´çš„é—®é¢˜ã€‚</li>
<li>Scene4Uæ¡†æ¶é‡‡ç”¨åˆ†å±‚é‡å»ºç­–ç•¥ï¼Œç»“åˆå¼€æ”¾å¼è¯æ±‡è¡¨åˆ†å‰²æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å…¨æ™¯å›¾åƒã€‚</li>
<li>åˆ©ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å±‚ä¿®å¤æ¨¡å—æ¢å¤é®æŒ¡åŒºåŸŸï¼Œç”Ÿæˆå…·æœ‰å±‚æ¬¡ç»“æ„çš„åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>Scene4Uåœ¨LPIPSå’ŒBRISQUEæŒ‡æ ‡ä¸Šæ˜¾è‘—æé«˜ï¼Œä¸”è®­ç»ƒé€Ÿåº¦æœ€å¿«ã€‚</li>
<li>ä¸ºScene4Uæ¡†æ¶å»ºç«‹äº†WorldVista3Dæ•°æ®é›†ï¼ŒåŒ…å«å…¨çƒçŸ¥åæ™¯ç‚¹çš„å…¨æ™¯å›¾åƒã€‚</li>
<li>Scene4Uçš„ç¨³å¥æ€§å¾—åˆ°éªŒè¯ï¼Œå¯å¹¿æ³›åº”ç”¨äºä¸åŒåœ°æ ‡çš„æ²‰æµ¸å¼åœºæ™¯ä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ae6928fafacd93199dc360b22b8aa1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7fda8beee2f40b60e23c957c67baf38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16a9ade343b44b937be5d84a40e9542e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76fa402ff110b28e80254231ed53be26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-725260b66dddbc4cefbee81102e3cfa0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors"><a href="#LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors" class="headerlink" title="LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors"></a>LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors</h2><p><strong>Authors:Han Zhou, Wei Dong, Jun Chen</strong></p>
<p>Directly employing 3D Gaussian Splatting (3DGS) on images with adverse illumination conditions exhibits considerable difficulty in achieving high-quality, normally-exposed representations due to: (1) The limited Structure from Motion (SfM) points estimated in adverse illumination scenarios fail to capture sufficient scene details; (2) Without ground-truth references, the intensive information loss, significant noise, and color distortion pose substantial challenges for 3DGS to produce high-quality results; (3) Combining existing exposure correction methods with 3DGS does not achieve satisfactory performance due to their individual enhancement processes, which lead to the illumination inconsistency between enhanced images from different viewpoints. To address these issues, we propose LITA-GS, a novel illumination-agnostic novel view synthesis method via reference-free 3DGS and physical priors. Firstly, we introduce an illumination-invariant physical prior extraction pipeline. Secondly, based on the extracted robust spatial structure prior, we develop the lighting-agnostic structure rendering strategy, which facilitates the optimization of the scene structure and object appearance. Moreover, a progressive denoising module is introduced to effectively mitigate the noise within the light-invariant representation. We adopt the unsupervised strategy for the training of LITA-GS and extensive experiments demonstrate that LITA-GS surpasses the state-of-the-art (SOTA) NeRF-based method while enjoying faster inference speed and costing reduced training time. The code is released at <a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS">https://github.com/LowLevelAI/LITA-GS</a>. </p>
<blockquote>
<p>ç›´æ¥å¯¹ä¸è‰¯ç…§æ˜æ¡ä»¶ä¸‹çš„å›¾åƒåº”ç”¨3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰åœ¨å®ç°é«˜è´¨é‡ã€æ­£å¸¸æ›å…‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨ç›¸å½“å¤§çš„å›°éš¾ï¼Œå› ä¸ºï¼šï¼ˆ1ï¼‰åœ¨ä¸è‰¯ç…§æ˜åœºæ™¯ä¸­ä¼°è®¡çš„è¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰ç‚¹æœ‰é™ï¼Œæ— æ³•æ•æ‰è¶³å¤Ÿçš„åœºæ™¯ç»†èŠ‚ï¼›ï¼ˆ2ï¼‰æ²¡æœ‰çœŸå®å‚è€ƒï¼Œå¤§é‡ä¿¡æ¯ä¸¢å¤±ã€æ˜¾è‘—å™ªå£°å’Œé¢œè‰²å¤±çœŸç»™3DGSå¸¦æ¥å¾ˆå¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥äº§ç”Ÿé«˜è´¨é‡ç»“æœï¼›ï¼ˆ3ï¼‰å°†ç°æœ‰æ›å…‰æ ¡æ­£æ–¹æ³•ä¸3DGSç›¸ç»“åˆå¹¶æœªå–å¾—ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œå› ä¸ºå®ƒä»¬çš„ä¸ªåˆ«å¢å¼ºå¤„ç†è¿‡ç¨‹å¯¼è‡´ä»ä¸åŒè§†ç‚¹å¢å¼ºçš„å›¾åƒä¹‹é—´ç…§æ˜ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LITA-GSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— ç…§æ˜æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ï¼Œé€šè¿‡æ— å‚è€ƒçš„3DGSå’Œç‰©ç†å…ˆéªŒæ¥å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ã€‚å…¶æ¬¡ï¼ŒåŸºäºæå–çš„ç¨³å¥ç©ºé—´ç»“æ„å…ˆéªŒï¼Œæˆ‘ä»¬å¼€å‘äº†å…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œæœ‰åŠ©äºä¼˜åŒ–åœºæ™¯ç»“æ„å’Œå¯¹è±¡å¤–è§‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ¸è¿›é™å™ªæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°å‡è½»å…‰ä¸å˜è¡¨ç¤ºä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬é‡‡ç”¨æ— ç›‘ç£ç­–ç•¥å¯¹LITA-GSè¿›è¡Œè®­ç»ƒï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒLITA-GSè¶…è¶Šäº†åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼ŒåŒæ—¶æ‹¥æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS%E3%80%82">https://github.com/LowLevelAI/LITA-GSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00219v1">PDF</a> Accepted by CVPR 2025. 3DGS, Adverse illumination conditions,   Reference-free, Physical priors</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é‡‡ç”¨3Dé«˜æ–¯è´´ç‰‡æŠ€æœ¯å¯¹ä¸è‰¯ç…§æ˜æ¡ä»¶ä¸‹çš„å›¾åƒè¿›è¡Œç›´æ¥å¤„ç†ï¼Œéš¾ä»¥å®ç°é«˜è´¨é‡çš„æ­£å¸¸æ›å…‰è¡¨ç¤ºã€‚ä¸»è¦ç”±äºä»¥ä¸‹å‡ ç‚¹ï¼šä¸€æ˜¯åœ¨ä¸è‰¯ç…§æ˜åœºæ™¯ä¸­ä¼°è®¡çš„æœ‰é™ç»“æ„å…‰ç‚¹æ— æ³•æ•æ‰è¶³å¤Ÿçš„åœºæ™¯ç»†èŠ‚ï¼›äºŒæ˜¯ç”±äºç¼ºä¹åœ°é¢çœŸå®å‚è€ƒï¼Œå¤§é‡ä¿¡æ¯ä¸¢å¤±ã€æ˜¾è‘—å™ªå£°å’Œé¢œè‰²å¤±çœŸç»™3DGSå¸¦æ¥å·¨å¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥äº§ç”Ÿé«˜è´¨é‡ç»“æœï¼›ä¸‰æ˜¯ç°æœ‰çš„æ›å…‰æ ¡æ­£æ–¹æ³•ä¸3DGSçš„ç»“åˆå¹¶æœªè¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œå› ä¸ºå„è‡ªçš„å¢å¼ºè¿‡ç¨‹å¯¼è‡´ä»ä¸åŒè§†è§’å¢å¼ºçš„å›¾åƒä¹‹é—´ç…§æ˜ä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºLITA-GSçš„æ–°å‹å…‰ç…§æ— å…³æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ï¼Œé‡‡ç”¨æ— å‚è€ƒçš„3DGSå’Œç‰©ç†å…ˆéªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ã€‚å…¶æ¬¡ï¼ŒåŸºäºæå–çš„ç¨³å¥ç©ºé—´ç»“æ„å…ˆéªŒï¼Œæˆ‘ä»¬å¼€å‘äº†å…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œæœ‰åŠ©äºä¼˜åŒ–åœºæ™¯ç»“æ„å’Œå¯¹è±¡å¤–è§‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ¸è¿›é™å™ªæ¨¡å—ï¼Œå¯æœ‰æ•ˆå‡è½»å…‰ä¸å˜è¡¨ç¤ºä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬é‡‡ç”¨æ— ç›‘ç£ç­–ç•¥å¯¹LITA-GSè¿›è¡Œè®­ç»ƒï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒLITA-GSè¶…è¶Šäº†åŸºäºNeRFçš„æœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶æ‹¥æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS%E3%80%82">https://github.com/LowLevelAI/LITA-GSã€‚</a></p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>3Dé«˜æ–¯è´´ç‰‡æŠ€æœ¯ï¼ˆ3DGSï¼‰åœ¨ä¸è‰¯ç…§æ˜æ¡ä»¶ä¸‹ç›´æ¥åº”ç”¨äºå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å®ç°é«˜è´¨é‡çš„æ­£å¸¸æ›å…‰è¡¨ç¤ºã€‚</li>
<li>æŒ‘æˆ˜åŒ…æ‹¬ï¼šç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰ç‚¹çš„ä¼°è®¡ä¸è¶³ã€ä¿¡æ¯ä¸¢å¤±ã€å™ªå£°å’Œé¢œè‰²å¤±çœŸç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•LITA-GSï¼Œé€šè¿‡æ— å‚è€ƒçš„3DGSå’Œç‰©ç†å…ˆéªŒè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LITA-GSåŒ…å«ï¼šå…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ã€å…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ã€æ¸è¿›é™å™ªæ¨¡å—ã€‚</li>
<li>LITA-GSè¶…è¶ŠåŸºäºNeRFçš„å½“å‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´çŸ­çš„è®­ç»ƒæ—¶é—´ã€‚</li>
<li>ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨æŒ‡å®šGitHubä»“åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-724b60f30c6aa43bb38a6f183871b706.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17aa639b3cdddc86604f9ed53f258f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37b58252ecdf8d6303c5f74971ec484.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f92eeb53810a3b1e93299a7488f9dee.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Visual-Acoustic-Fields"><a href="#Visual-Acoustic-Fields" class="headerlink" title="Visual Acoustic Fields"></a>Visual Acoustic Fields</h2><p><strong>Authors:Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, Xueyan Zou, Paul Liang, Hanspeter Pfister, Xiaolong Wang</strong></p>
<p>Objects produce different sounds when hit, and humans can intuitively infer how an object might sound based on its appearance and material properties. Inspired by this intuition, we propose Visual Acoustic Fields, a framework that bridges hitting sounds and visual signals within a 3D space using 3D Gaussian Splatting (3DGS). Our approach features two key modules: sound generation and sound localization. The sound generation module leverages a conditional diffusion model, which takes multiscale features rendered from a feature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the sound localization module enables querying the 3D scene, represented by the feature-augmented 3DGS, to localize hitting positions based on the sound sources. To support this framework, we introduce a novel pipeline for collecting scene-level visual-sound sample pairs, achieving alignment between captured images, impact locations, and corresponding sounds. To the best of our knowledge, this is the first dataset to connect visual and acoustic signals in a 3D context. Extensive experiments on our dataset demonstrate the effectiveness of Visual Acoustic Fields in generating plausible impact sounds and accurately localizing impact sources. Our project page is at <a target="_blank" rel="noopener" href="https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/">https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/</a>. </p>
<blockquote>
<p>å½“ç‰©ä½“è¢«å‡»ä¸­æ—¶ä¼šäº§ç”Ÿä¸åŒçš„å£°éŸ³ï¼Œäººç±»å¯ä»¥æ ¹æ®ç‰©ä½“çš„å¤–è§‚å’Œææ–™å±æ€§ç›´è§‚åœ°æ¨æ–­å‡ºç‰©ä½“å¯èƒ½å‘å‡ºçš„å£°éŸ³ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰å£°å­¦åœºï¼ˆVisual Acoustic Fieldsï¼‰è¿™ä¸€æ¦‚å¿µï¼Œé€šè¿‡ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰æŠ€æœ¯ï¼Œåœ¨ä¸‰ç»´ç©ºé—´ä¸­æ¶èµ·å‡»æ‰“å£°éŸ³ä¸è§†è§‰ä¿¡å·çš„æ¡¥æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šå£°éŸ³ç”Ÿæˆå’Œå£°éŸ³å®šä½ã€‚å£°éŸ³ç”Ÿæˆæ¨¡å—é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç‰¹å¾å¢å¼ºå‹ä¸‰ç»´é«˜æ–¯æ‹¼è´´æ‰€å‘ˆç°çš„å¤šå°ºåº¦ç‰¹å¾ä¸ºåŸºç¡€ï¼Œç”Ÿæˆé€¼çœŸçš„å‡»æ‰“å£°éŸ³ã€‚åŒæ—¶ï¼Œå£°éŸ³å®šä½æ¨¡å—èƒ½å¤ŸæŸ¥è¯¢ç”±ç‰¹å¾å¢å¼ºå‹ä¸‰ç»´é«˜æ–¯æ‹¼è´´æ‰€è¡¨ç¤ºçš„ä¸‰ç»´åœºæ™¯ï¼Œæ ¹æ®å£°æºæ¥ç¡®å®šå‡»æ‰“ä½ç½®ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹ç®¡é“ï¼Œç”¨äºæ”¶é›†åœºæ™¯çº§åˆ«çš„è§†è§‰-å£°éŸ³æ ·æœ¬å¯¹ï¼Œå®ç°æ•è·å›¾åƒã€å†²å‡»ä½ç½®å’Œç›¸åº”å£°éŸ³ä¹‹é—´çš„å¯¹é½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªåœ¨ä¸‰ç»´ç¯å¢ƒä¸­è¿æ¥è§†è§‰å’Œå£°éŸ³ä¿¡å·çš„æ•°æ®åº“ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®åº“ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº†è§†è§‰å£°å­¦åœºåœ¨ç”Ÿæˆåˆç†å†²å‡»å£°å’Œå‡†ç¡®å®šä½å†²å‡»æºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/%E3%80%82">https://yuelei0428.github.io/projects/Visual-Acoustic-Fields/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24270v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Visual Acoustic Fieldsæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨3Dé«˜æ–¯å»¶å±•ï¼ˆ3DGSï¼‰æŠ€æœ¯ï¼Œåœ¨3Dç©ºé—´ä¸­æ¶èµ·æ‰“å‡»å£°éŸ³ä¸è§†è§‰ä¿¡å·ä¹‹é—´çš„æ¡¥æ¢ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå£°éŸ³ç”Ÿæˆå’Œå£°éŸ³å®šä½ã€‚å£°éŸ³ç”Ÿæˆæ¨¡å—é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ç‰¹å¾å¢å¼ºçš„3DGSç”Ÿæˆé€¼çœŸçš„æ‰“å‡»å£°éŸ³ã€‚è€Œå£°éŸ³å®šä½æ¨¡å—åˆ™å…è®¸æ ¹æ®å£°éŸ³æºæŸ¥è¯¢ç”±ç‰¹å¾å¢å¼ºçš„3DGSè¡¨ç¤ºçš„3Dåœºæ™¯ï¼Œä»¥å®šä½æ‰“å‡»ä½ç½®ã€‚ä¸ºæ”¯æŒæ­¤æ¡†æ¶ï¼Œè¿˜å¼•å…¥äº†æ–°å‹åœºæ™¯çº§è§†è§‰å£°éŸ³æ ·æœ¬é‡‡é›†ç®¡é“ï¼Œå®ç°äº†å›¾åƒæ•è·ã€å½±å“ä½ç½®å’Œç›¸åº”å£°éŸ³ä¹‹é—´çš„å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Visual Acoustic Fieldsæ¡†æ¶ç»“åˆäº†è§†è§‰å’Œå£°éŸ³ä¿¡å·ï¼Œåœ¨3Dç©ºé—´ä¸­æ¨¡æ‹Ÿç‰©ä½“æ‰“å‡»çš„å£°éŸ³ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«å£°éŸ³ç”Ÿæˆå’Œå£°éŸ³å®šä½ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚</li>
<li>å£°éŸ³ç”Ÿæˆæ¨¡å—é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨3Dé«˜æ–¯å»¶å±•æŠ€æœ¯ç”ŸæˆçœŸå®æ„Ÿçš„æ‰“å‡»å£°éŸ³ã€‚</li>
<li>å£°éŸ³å®šä½æ¨¡å—èƒ½æ ¹æ®å£°éŸ³æºåœ¨3Dåœºæ™¯ä¸­çš„ç‰¹å¾è¿›è¡Œå®šä½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„åœºæ™¯çº§è§†è§‰-å£°éŸ³æ ·æœ¬é‡‡é›†ç®¡é“ï¼Œå®ç°äº†å›¾åƒã€å½±å“ä½ç½®å’Œå¯¹åº”å£°éŸ³ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>è¿™æ˜¯é¦–ä¸ªåœ¨3Dç¯å¢ƒä¸­è¿æ¥è§†è§‰å’Œå£°éŸ³ä¿¡å·çš„æ•°æ®åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6a102da00d348a0dd15c6dab4bbdd7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7682554236ff07b3f8a46d143fb41fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef9ccc99249166af3bcde8cbc3b3cfb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a128d068c7a4c5bdee009af42ccb1260.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-806cf5f9e96851a1a63b7b7628944523.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VizFlyt-Perception-centric-Pedagogical-Framework-For-Autonomous-Aerial-Robots"><a href="#VizFlyt-Perception-centric-Pedagogical-Framework-For-Autonomous-Aerial-Robots" class="headerlink" title="VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial   Robots"></a>VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial   Robots</h2><p><strong>Authors:Kushagra Srivastava, Rutwik Kulkarni, Manoj Velmurugan, Nitin J. Sanket</strong></p>
<p>Autonomous aerial robots are becoming commonplace in our lives. Hands-on aerial robotics courses are pivotal in training the next-generation workforce to meet the growing market demands. Such an efficient and compelling course depends on a reliable testbed. In this paper, we present VizFlyt, an open-source perception-centric Hardware-In-The-Loop (HITL) photorealistic testing framework for aerial robotics courses. We utilize pose from an external localization system to hallucinate real-time and photorealistic visual sensors using 3D Gaussian Splatting. This enables stress-free testing of autonomy algorithms on aerial robots without the risk of crashing into obstacles. We achieve over 100Hz of system update rate. Lastly, we build upon our past experiences of offering hands-on aerial robotics courses and propose a new open-source and open-hardware curriculum based on VizFlyt for the future. We test our framework on various course projects in real-world HITL experiments and present the results showing the efficacy of such a system and its large potential use cases. Code, datasets, hardware guides and demo videos are available at <a target="_blank" rel="noopener" href="https://pear.wpi.edu/research/vizflyt.html">https://pear.wpi.edu/research/vizflyt.html</a> </p>
<blockquote>
<p>è‡ªä¸»ç©ºä¸­æœºå™¨äººæ­£åœ¨æˆ‘ä»¬çš„ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éã€‚åŠ¨æ‰‹çš„ç©ºä¸­æœºå™¨äººè¯¾ç¨‹å¯¹äºåŸ¹å…»æ–°ä¸€ä»£åŠ³åŠ¨åŠ›ä»¥æ»¡è¶³ä¸æ–­å¢é•¿çš„å¸‚åœºéœ€æ±‚è‡³å…³é‡è¦ã€‚è¿™æ ·çš„é«˜æ•ˆå’Œæœ‰å¸å¼•åŠ›çš„è¯¾ç¨‹ä¾èµ–äºå¯é çš„æµ‹è¯•å¹³å°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VizFlytï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç©ºä¸­æœºå™¨äººè¯¾ç¨‹çš„ä»¥æ„ŸçŸ¥ä¸ºä¸­å¿ƒçš„å¼€æºç¡¬ä»¶åœ¨ç¯ï¼ˆHITLï¼‰é€¼çœŸæµ‹è¯•æ¡†æ¶ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªå¤–éƒ¨å®šä½ç³»ç»Ÿçš„å§¿æ€ï¼Œé€šè¿‡3Dé«˜æ–¯æ³¼æº…æŠ€æœ¯æ¨¡æ‹Ÿå®æ—¶å’Œé€¼çœŸçš„è§†è§‰ä¼ æ„Ÿå™¨ã€‚è¿™å¯ä»¥åœ¨æ— å‹åŠ›çš„æƒ…å†µä¸‹æµ‹è¯•ç©ºä¸­æœºå™¨äººçš„è‡ªä¸»ç®—æ³•ï¼Œé¿å…äº†ä¸éšœç¢ç‰©çš„ç¢°æ’é£é™©ã€‚æˆ‘ä»¬å®ç°äº†è¶…è¿‡100Hzçš„ç³»ç»Ÿæ›´æ–°ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬åŸºäºæä¾›åŠ¨æ‰‹ç©ºä¸­æœºå™¨äººè¯¾ç¨‹çš„è¿‡å»ç»éªŒï¼Œä»¥VizFlytä¸ºåŸºç¡€ï¼Œä¸ºæœªæ¥æå‡ºä¸€ä¸ªæ–°çš„å¼€æºå’Œå¼€æ”¾ç¡¬ä»¶çš„è¯¾ç¨‹å¤§çº²ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„HITLå®éªŒä¸­å¯¹å„ç§è¯¾ç¨‹é¡¹ç›®æµ‹è¯•äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ä»¥åŠå…¶å·¨å¤§çš„æ½œåœ¨ç”¨ä¾‹ã€‚ä»£ç ã€æ•°æ®é›†ã€ç¡¬ä»¶æŒ‡å—å’Œæ¼”ç¤ºè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://pear.wpi.edu/research/vizflyt.html%E6%89%BE%E5%88%B0%E3%80%82">https://pear.wpi.edu/research/vizflyt.htmlæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22876v2">PDF</a> Accepted at ICRA 2025. Projected Page:   <a target="_blank" rel="noopener" href="https://pear.wpi.edu/research/vizflyt.html">https://pear.wpi.edu/research/vizflyt.html</a></p>
<p><strong>Summary</strong><br>     å¯è§†åŒ–æ— äººæœºé£è¡Œæ¨¡æ‹Ÿå¹³å°ï¼ˆVizFlytï¼‰ä½œä¸ºæ„ŸçŸ¥ä¸ºä¸­å¿ƒç¯èŠ‚çš„ç¡¬ä»¶åœ¨ç¯æµ‹è¯•æ¡†æ¶ï¼Œä¸ºæ— äººæœºè¯¾ç¨‹æä¾›äº†å¼€æ”¾æºä»£ç çš„æµ‹è¯•ç¯å¢ƒã€‚è¯¥å¹³å°åˆ©ç”¨å¤–éƒ¨å®šä½ç³»ç»Ÿçš„å§¿æ€ä¿¡æ¯ï¼Œé€šè¿‡ä¸‰ç»´é«˜æ–¯æ‰©å±•æŠ€æœ¯æ¨¡æ‹ŸçœŸå®ä¼ æ„Ÿå™¨è§†è§‰åœºæ™¯ï¼Œå®ç°å¯¹æ— äººæœºè‡ªä¸»ç®—æ³•çš„æ— éšœç¢å®æ—¶æµ‹è¯•ã€‚ç³»ç»Ÿæ›´æ–°ç‡é«˜è¾¾æ¯ç§’ç™¾å¸§ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜åŸºäºVizFlytå¹³å°æå‡ºäº†å¼€æ”¾æºä»£ç å’Œå¼€æ”¾ç¡¬ä»¶çš„æœªæ¥è¯¾ç¨‹è®¾è®¡æ€è·¯ã€‚æœ‰å…³ä¿¡æ¯å‡å¯é€šè¿‡ç›¸å…³é“¾æ¥æŸ¥è¯¢ã€‚è¯¥é¡¹ç›®ä¿ƒè¿›äº†å¯¹æ— äººæœºçš„å®é™…æ¨¡æ‹Ÿç ”ç©¶ï¼Œå¯¹æ¨åŠ¨æ— äººæœºå¸‚åœºå‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¯è§†åŒ–æ— äººæœºé£è¡Œæ¨¡æ‹Ÿå¹³å°ï¼ˆVizFlytï¼‰ä¸ºæ— äººæœºè¯¾ç¨‹æä¾›äº†é‡è¦æµ‹è¯•ç¯å¢ƒã€‚</li>
<li>åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ‰©å±•æŠ€æœ¯å®ç°å®æ—¶ä¼ æ„Ÿå™¨è§†è§‰æ¨¡æ‹Ÿã€‚</li>
<li>è¯¥å¹³å°æ”¯æŒæ— éšœç¢æµ‹è¯•æ— äººæœºè‡ªä¸»ç®—æ³•ï¼Œé™ä½æµ‹è¯•é£é™©ã€‚</li>
<li>ç³»ç»Ÿæ›´æ–°ç‡é«˜è¾¾æ¯ç§’ç™¾å¸§ä»¥ä¸Šï¼Œä¿è¯æµ‹è¯•æ•ˆç‡ã€‚</li>
<li>åŸºäºVizFlytå¹³å°æå‡ºäº†å¼€æ”¾æºä»£ç å’Œç¡¬ä»¶çš„æœªæ¥è¯¾ç¨‹è®¾è®¡æ–°æ€è·¯ã€‚</li>
<li>æä¾›åœ¨çº¿è®¿é—®æ•™å­¦èµ„æºåŠç›¸å…³é¡¹ç›®çš„æŒ‡å—ï¼Œæœ‰åŠ©äºæ— äººæœºçš„æ•™è‚²åŠç ”å‘å®è·µã€‚</li>
<li>æ­¤æ¡†æ¶éªŒè¯äº†å¤§è§„æ¨¡ä½¿ç”¨æ½œèƒ½å’Œå¯¹å¸‚åœºæœªæ¥å‘å±•çš„é‡è¦ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b303e23ac3d943c1feba3c495acb8c08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b83c4e706a59d82aba27a4811e18211d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fac04436535b6c88f2944c2a8f6ffc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6f5bdf66618d4c16cc7399933951ab1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis"><a href="#Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis" class="headerlink" title="Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis"></a>Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis</h2><p><strong>Authors:Shuai Shen, Wanhua Li, Yunpeng Zhang, Weipeng Hu, Yap-Peng Tan</strong></p>
<p>Talking head synthesis has become a key research area in computer graphics and multimedia, yet most existing methods often struggle to balance generation quality with computational efficiency. In this paper, we present a novel approach that leverages an Audio Factorization Plane (Audio-Plane) based Gaussian Splatting for high-quality and real-time talking head generation. For modeling a dynamic talking head, 4D volume representation is needed. However, directly storing a dense 4D grid is impractical due to the high cost and lack of scalability for longer durations. We overcome this challenge with the proposed Audio-Plane, where the 4D volume representation is decomposed into audio-independent space planes and audio-dependent planes. This provides a compact and interpretable feature representation for talking head, facilitating more precise audio-aware spatial encoding and enhanced audio-driven lip dynamic modeling. To further improve speech dynamics, we develop a dynamic splatting method that helps the network more effectively focus on modeling the dynamics of the mouth region. Extensive experiments demonstrate that by integrating these innovations with the powerful Gaussian Splatting, our method is capable of synthesizing highly realistic talking videos in real time while ensuring precise audio-lip synchronization. Synthesized results are available in <a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/">https://sstzal.github.io/Audio-Plane/</a>. </p>
<blockquote>
<p>è®ºæ–‡æ‘˜è¦ï¼šè¯´è¯äººå¤´éƒ¨åˆæˆå·²æˆä¸ºè®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“é¢†åŸŸçš„ä¸€ä¸ªå…³é”®ç ”ç©¶ç‚¹ï¼Œç„¶è€Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨åŸºäºéŸ³é¢‘åˆ†è§£å¹³é¢ï¼ˆAudio-Planeï¼‰çš„é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯æ¥è¿›è¡Œé«˜è´¨é‡å®æ—¶è¯´è¯å¤´éƒ¨ç”Ÿæˆã€‚ä¸ºäº†æ¨¡æ‹ŸåŠ¨æ€çš„è¯´è¯å¤´éƒ¨ï¼Œéœ€è¦é‡‡ç”¨4Dä½“ç§¯è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç›´æ¥å­˜å‚¨å¯†é›†çš„4Dç½‘æ ¼å¹¶ä¸å®é™…ï¼Œå› ä¸ºæˆæœ¬é«˜æ˜‚ä¸”å¯¹äºæ›´é•¿æ—¶é—´çš„æ‰©å±•æ€§ä¸è¶³ã€‚æˆ‘ä»¬å…‹æœäº†è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†éŸ³é¢‘å¹³é¢ï¼Œå…¶ä¸­å°†4Dä½“ç§¯è¡¨ç¤ºåˆ†è§£ä¸ºéŸ³é¢‘ç‹¬ç«‹çš„ç©ºé—´å¹³é¢å’ŒéŸ³é¢‘ä¾èµ–çš„å¹³é¢ã€‚è¿™ä¸ºè¯´è¯å¤´éƒ¨æä¾›äº†ç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºï¼Œä¿ƒè¿›äº†æ›´ç²¾ç¡®çš„å£°éŸ³æ„ŸçŸ¥ç©ºé—´ç¼–ç å’Œå¢å¼ºçš„éŸ³é¢‘é©±åŠ¨å˜´å”‡åŠ¨æ€å»ºæ¨¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹å–„è¯­éŸ³åŠ¨æ€ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŠ¨æ€æ¶‚æŠ¹æ–¹æ³•ï¼Œå¸®åŠ©ç½‘ç»œæ›´æœ‰æ•ˆåœ°ä¸“æ³¨äºå˜´éƒ¨åŒºåŸŸçš„åŠ¨æ€å»ºæ¨¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å°†è¿™äº›åˆ›æ–°ä¸å¼ºå¤§çš„é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç¡®ä¿ç²¾ç¡®éŸ³é¢‘-å˜´å”‡åŒæ­¥çš„æƒ…å†µä¸‹ï¼Œå®æ—¶åˆæˆé«˜åº¦é€¼çœŸçš„å¯¹è¯è§†é¢‘ã€‚åˆæˆç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/%E6%9F%A5%E7%9C%8B%E3%80%82">https://sstzal.github.io/Audio-Plane/æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22605v1">PDF</a> </p>
<p><strong>Summary</strong><br>å®æ—¶å¤´éƒ¨åˆæˆæ˜¯ç ”ç©¶è®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“é¢†åŸŸçš„ä¸€ä¸ªå…³é”®è¯¾é¢˜ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéŸ³é¢‘åˆ†è§£å¹³é¢ï¼ˆAudio-Planeï¼‰çš„é«˜æ–¯æº…å°„æ³•ï¼Œç”¨äºé«˜è´¨é‡å®æ—¶å¤´éƒ¨ç”Ÿæˆã€‚é€šè¿‡åˆ†è§£å››ç»´ä½“ç§¯è¡¨ç¤ºä¸ºéŸ³é¢‘ç‹¬ç«‹ç©ºé—´å¹³é¢å’ŒéŸ³é¢‘ä¾èµ–å¹³é¢ï¼Œå…‹æœäº†ç›´æ¥å­˜å‚¨å¯†é›†å››ç»´ç½‘æ ¼çš„ä¸åˆ‡å®é™…å’Œä¸å¯æ‰©å±•çš„é—®é¢˜ã€‚è¿™ä¸ºå¤´éƒ¨æä¾›äº†ç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºï¼Œä¿ƒè¿›äº†æ›´ç²¾ç¡®çš„éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç å’Œå¢å¼ºçš„éŸ³é¢‘é©±åŠ¨å”‡éƒ¨åŠ¨æ€å»ºæ¨¡ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„å®éªŒè¯æ˜ï¼Œç»“åˆé«˜æ–¯æº…å°„æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®æ—¶åˆæˆé«˜åº¦é€¼çœŸçš„å¯¹è¯è§†é¢‘ï¼ŒåŒæ—¶ç¡®ä¿ç²¾ç¡®çš„éŸ³é¢‘-å”‡éƒ¨åŒæ­¥ã€‚åˆæˆç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/">é“¾æ¥</a>æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®æ—¶å¤´éƒ¨åˆæˆæ˜¯è®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“é¢†åŸŸçš„é‡è¦ç ”ç©¶å†…å®¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†åŸºäºéŸ³é¢‘åˆ†è§£å¹³é¢ï¼ˆAudio-Planeï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºé«˜è´¨é‡å®æ—¶å¤´éƒ¨ç”Ÿæˆã€‚</li>
<li>Audio-Planeå…‹æœäº†ç›´æ¥å­˜å‚¨å››ç»´ä½“ç§¯è¡¨ç¤ºçš„ä¸åˆ‡å®é™…å’Œä¸å¯æ‰©å±•é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ç´§å‡‘ä¸”å¯è§£é‡Šçš„ç‰¹å¾è¡¨ç¤ºï¼Œä¿ƒè¿›äº†éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç å’Œå”‡éƒ¨åŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>ç»“åˆé«˜æ–¯æº…å°„æ³•ï¼Œå®ç°äº†é«˜åº¦é€¼çœŸçš„å®æ—¶å¯¹è¯è§†é¢‘åˆæˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dc3945e146a1125a5d769517b491305e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c343fd56b0ea0f2ccfabdd9058dcf16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb17023d93340211b982eb56e0a42e41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c0d22507b0d089e075c77b6ec79cfee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14c4fa59a61d1b3a409ba585990402d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08c3de0ff764017706147587374a97e4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting"><a href="#RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting" class="headerlink" title="RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting"></a>RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting</h2><p><strong>Authors:Qiyu Dai, Xingyu Ni, Qianfan Shen, Wenzheng Chen, Baoquan Chen, Mengyu Chu</strong></p>
<p>We consider the problem of adding dynamic rain effects to in-the-wild scenes in a physically-correct manner. Recent advances in scene modeling have made significant progress, with NeRF and 3DGS techniques emerging as powerful tools for reconstructing complex scenes. However, while effective for novel view synthesis, these methods typically struggle with challenging scene editing tasks, such as physics-based rain simulation. In contrast, traditional physics-based simulations can generate realistic rain effects, such as raindrops and splashes, but they often rely on skilled artists to carefully set up high-fidelity scenes. This process lacks flexibility and scalability, limiting its applicability to broader, open-world environments. In this work, we introduce RainyGS, a novel approach that leverages the strengths of both physics-based modeling and 3DGS to generate photorealistic, dynamic rain effects in open-world scenes with physical accuracy. At the core of our method is the integration of physically-based raindrop and shallow water simulation techniques within the fast 3DGS rendering framework, enabling realistic and efficient simulations of raindrop behavior, splashes, and reflections. Our method supports synthesizing rain effects at over 30 fps, offering users flexible control over rain intensity â€“ from light drizzles to heavy downpours. We demonstrate that RainyGS performs effectively for both real-world outdoor scenes and large-scale driving scenarios, delivering more photorealistic and physically-accurate rain effects compared to state-of-the-art methods. Project page can be found at <a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/">https://pku-vcl-geometry.github.io/RainyGS/</a> </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘ä»¥ç‰©ç†æ­£ç¡®çš„æ–¹å¼ä¸ºè‡ªç„¶åœºæ™¯æ·»åŠ åŠ¨æ€ä¸‹é›¨æ•ˆæœçš„é—®é¢˜ã€‚æœ€è¿‘åœºæ™¯å»ºæ¨¡æ–¹é¢çš„è¿›å±•å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼ŒNeRFå’Œ3DGSæŠ€æœ¯ä½œä¸ºé‡å»ºå¤æ‚åœºæ™¯çš„å¼ºå¤§å·¥å…·è€Œå´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›æ–¹æ³•åœ¨åˆæˆæ–°è§†è§’æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸é¢ä¸´åŸºäºç‰©ç†çš„ä¸‹é›¨æ¨¡æ‹Ÿç­‰æŒ‘æˆ˜æ€§åœºæ™¯ç¼–è¾‘ä»»åŠ¡æ—¶æ„Ÿåˆ°å›°éš¾ã€‚ç›¸åï¼Œä¼ ç»Ÿçš„åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå¯ä»¥äº§ç”Ÿé€¼çœŸçš„ä¸‹é›¨æ•ˆæœï¼Œå¦‚é›¨æ»´å’Œé£æº…çš„æ°´èŠ±ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç†Ÿç»ƒçš„è‰ºæœ¯å®¶æ¥ä»”ç»†è®¾ç½®é«˜ä¿çœŸåœºæ™¯ã€‚è¿™ä¸ªè¿‡ç¨‹ç¼ºä¹çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›ã€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†RainyGSï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œ3DGSä¼˜åŠ¿ç”Ÿæˆå¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­çš„é€¼çœŸåŠ¨æ€ä¸‹é›¨æ•ˆæœçš„æ–°æ–¹æ³•ï¼Œå…·æœ‰ç‰©ç†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åœ¨å¿«é€Ÿçš„3DGSæ¸²æŸ“æ¡†æ¶å†…æ•´åˆåŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œèƒ½å¤ŸçœŸå®æœ‰æ•ˆåœ°æ¨¡æ‹Ÿé›¨æ»´è¡Œä¸ºã€é£æº…å’Œæ°´é¢åå°„ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åˆæˆè¶…è¿‡30å¸§&#x2F;ç§’çš„ä¸‹é›¨æ•ˆæœï¼Œè®©ç”¨æˆ·çµæ´»åœ°æ§åˆ¶é›¨å¼ºåº¦â€”â€”ä»è½»å¾®ç»†é›¨åˆ°å€¾ç›†å¤§é›¨ã€‚æˆ‘ä»¬è¯æ˜RainyGSå¯¹äºçœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯éƒ½è¡¨ç°æœ‰æ•ˆï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´é€¼çœŸå’Œæ›´ç‰©ç†å‡†ç¡®çš„ä¸‹é›¨æ•ˆæœã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21442v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRainyGSçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œ3DGSæŠ€æœ¯ï¼Œå¯åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰ç‰©ç†å‡†ç¡®æ€§çš„é«˜é€¼çœŸåº¦åŠ¨æ€é›¨æ°´æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡å¿«é€Ÿ3DGSæ¸²æŸ“æ¡†æ¶é›†æˆåŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œå®ç°äº†çœŸå®é«˜æ•ˆçš„é›¨æ»´è¡Œä¸ºã€é£æº…å’Œåå°„æ¨¡æ‹Ÿã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆè¶…è¿‡30å¸§çš„é™é›¨æ•ˆæœï¼Œç”¨æˆ·å¯ä»¥çµæ´»æ§åˆ¶é›¨å¼ºåº¦ï¼Œå®ç°ä»è½»é›¨åˆ°æš´é›¨çš„æ¨¡æ‹Ÿã€‚RainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°å¾—åˆ°äº†å±•ç¤ºï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´é€¼çœŸå’Œæ›´å‡†ç¡®çš„é›¨æ°´æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•RainyGSï¼Œç»“åˆäº†ç‰©ç†å»ºæ¨¡å’Œ3DGSæŠ€æœ¯æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„é™é›¨æ•ˆæœã€‚</li>
<li>RainyGSä½¿ç”¨åŸºäºç‰©ç†çš„é›¨æ»´æ¨¡æ‹Ÿå’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œç¡®ä¿é›¨æ°´æ•ˆæœçš„é€¼çœŸåº¦å’Œç‰©ç†å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­æ¨¡æ‹Ÿå„ç§é›¨å¼ºåº¦ï¼Œä»è½»é›¨åˆ°æš´é›¨ã€‚</li>
<li>RainyGSé€šè¿‡å¿«é€Ÿ3DGSæ¸²æŸ“æ¡†æ¶å®ç°äº†é«˜æ•ˆæ¨¡æ‹Ÿï¼Œå¯ä»¥åˆæˆè¶…è¿‡30å¸§çš„é™é›¨æ•ˆæœã€‚</li>
<li>ç”¨æˆ·å¯ä»¥çµæ´»æ§åˆ¶é›¨å¼ºåº¦ï¼Œæä¾›äº†ä¸°å¯Œçš„æ¨¡æ‹Ÿé€‰é¡¹ã€‚</li>
<li>RainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­çš„è¡¨ç°å¾—åˆ°äº†éªŒè¯å’Œå±•ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c3b19f41ae58db25446cd92dae747c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d92736acb1d72a0482a1e4739ae0fadb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2540fe3d87f579b141ef35602ce18620.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models"><a href="#4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models" class="headerlink" title="4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models"></a>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models</h2><p><strong>Authors:Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister</strong></p>
<p>Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries. </p>
<blockquote>
<p>å­¦ä¹ å››ç»´è¯­è¨€é¢†åŸŸä»¥å®ç°åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿå’Œæ— æ—¶é™çš„è¯­è¨€æŸ¥è¯¢å¯¹äºè®¸å¤šå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶LangSplatæˆåŠŸåœ°å°†CLIPç‰¹æ€§èå…¥ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºä¸­ï¼Œå®ç°äº†åœ¨ä¸‰ç»´é™æ€åœºæ™¯ä¸­çš„ç²¾ç¡®æ€§å’Œé«˜æ•ˆæ€§ï¼Œä½†å®ƒæ— æ³•å¤„ç†åŠ¨æ€å››ç»´é¢†åŸŸï¼Œå› ä¸ºCLIPæ˜¯ä¸ºé™æ€å›¾åƒæ–‡æœ¬ä»»åŠ¡è®¾è®¡çš„ï¼Œæ— æ³•æ•è·è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚ç°å®ä¸–ç•Œçš„ç¯å¢ƒæœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„ï¼Œç‰©ä½“è¯­ä¹‰ä¼šéšæ—¶é—´æ¼”å˜ã€‚æ„å»ºç²¾ç¡®çš„å››ç»´è¯­è¨€é¢†åŸŸéœ€è¦è·å–åƒç´ å¯¹é½çš„ã€é¢å‘å¯¹è±¡çš„è§†é¢‘ç‰¹å¾ï¼Œè€Œå½“å‰è§†è§‰æ¨¡å‹å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å››ç»´LangSplatï¼Œå®ƒå­¦ä¹ å››ç»´è¯­è¨€é¢†åŸŸä»¥é«˜æ•ˆåœ°å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ— å…³æˆ–æ—¶é—´æ•æ„Ÿçš„æ— é™åˆ¶è¯æ±‡æŸ¥è¯¢ã€‚å››ç»´LangSplatç»•è¿‡äº†ä»è§†è§‰ç‰¹å¾ä¸­å­¦ä¹ è¯­è¨€é¢†åŸŸï¼Œè€Œæ˜¯ç›´æ¥ä»æ ¹æ®é¢å‘å¯¹è±¡çš„è§†é¢‘å­—å¹•ç”Ÿæˆæ–‡æœ¬ä¸­å­¦ä¹ ï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€é¢å‘å¯¹è±¡è§†é¢‘æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œå¼•å¯¼MLLMsä¸ºè§†é¢‘ä¸­çš„å¯¹è±¡ç”Ÿæˆè¯¦ç»†ã€æ—¶é—´ä¸€è‡´ã€é«˜è´¨é‡çš„å­—å¹•ã€‚è¿™äº›å­—å¹•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç¼–ç ä¸ºé«˜è´¨é‡çš„å¥å­åµŒå…¥ï¼Œç„¶åä½œä¸ºåƒç´ å¯¹é½çš„ã€é¢å‘å¯¹è±¡çš„ç‰¹å¾ç›‘ç£ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´å®ç°å¼€æ”¾å¼è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ã€‚æˆ‘ä»¬è®¤è¯†åˆ°å››ç»´åœºæ™¯ä¸­çš„å¯¹è±¡åœ¨çŠ¶æ€ä¹‹é—´å‘ˆç°å‡ºå¹³æ»‘çš„è¿‡æ¸¡ï¼Œå› æ­¤è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªçŠ¶æ€å¯å˜å½¢ç½‘ç»œæ¥æœ‰æ•ˆåœ°å¯¹è¿™äº›éšæ—¶é—´å˜åŒ–çš„è¿ç»­å˜åŒ–è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œå››ç»´LangSplatå¯¹æ—¶é—´æ•æ„Ÿå’Œæ—¶é—´æ— å…³çš„æ— é™åˆ¶è¯æ±‡æŸ¥è¯¢éƒ½è¾¾åˆ°äº†ç²¾ç¡®å’Œé«˜æ•ˆçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10437v2">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://4d-langsplat.github.io/">https://4d-langsplat.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å­¦ä¹ 4Dè¯­è¨€å­—æ®µçš„é‡è¦æ€§ï¼Œè¯¥è¯­è¨€å­—æ®µå¯ä»¥å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿæ€§å’Œå¼€æ”¾æ€§è¯æ±‡æŸ¥è¯¢ã€‚è™½ç„¶LangSplatæˆåŠŸåœ°å°†CLIPç‰¹å¾èå…¥3Dé«˜æ–¯è¡¨ç¤ºï¼Œä½†åœ¨å¤„ç†åŠ¨æ€4Då­—æ®µæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†4D LangSplatï¼Œå®ƒé€šè¿‡ç›´æ¥ä»ç”±å¯¹è±¡çº§è§†é¢‘å­—å¹•ç”Ÿæˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­¦ä¹ è¯­è¨€å­—æ®µï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯¹è±¡çº§è§†é¢‘æç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œå¼•å¯¼MLLMsä¸ºè§†é¢‘ä¸­çš„å¯¹è±¡ç”Ÿæˆè¯¦ç»†ã€æ—¶é—´ä¸€è‡´çš„é«˜è´¨é‡å­—å¹•ã€‚è¿™äº›å­—å¹•è¢«ç¼–ç æˆé«˜è´¨é‡å¥å­åµŒå…¥ï¼Œä½œä¸ºåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å®šç‰¹å¾ç›‘ç£ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´è¿›è¡Œå¼€æ”¾å¼è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§çŠ¶æ€å¯å˜å½¢ç½‘ç»œï¼Œä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿå¯¹è±¡åœ¨4Dåœºæ™¯ä¸­çš„è¿ç»­çŠ¶æ€å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ4D LangSplatåœ¨æ—¶é—´å’Œéæ—¶é—´æ•æ„Ÿæ€§çš„å¼€æ”¾å¼è¯æ±‡æŸ¥è¯¢æ–¹é¢éƒ½å–å¾—äº†ç²¾ç¡®å’Œé«˜æ•ˆçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å­¦ä¹ 4Dè¯­è¨€å­—æ®µå¯¹äºå¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿæ€§å’Œå¼€æ”¾æ€§è¯­è¨€æŸ¥è¯¢è‡³å…³é‡è¦ã€‚</li>
<li>LangSplatåœ¨3Dé™æ€åœºæ™¯ä¸­å…·æœ‰ç²¾ç¡®æ€§å’Œæ•ˆç‡ï¼Œä½†åœ¨å¤„ç†åŠ¨æ€4Då­—æ®µæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>4D LangSplaté€šè¿‡ç›´æ¥ä»å¯¹è±¡çº§è§†é¢‘å­—å¹•ç”Ÿæˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å­¦ä¹ è¯­è¨€å­—æ®µã€‚</li>
<li>å¤šæ¨¡æ€å¯¹è±¡çº§è§†é¢‘æç¤ºæ–¹æ³•ç”Ÿæˆè¯¦ç»†ã€æ—¶é—´ä¸€è‡´çš„é«˜è´¨é‡å­—å¹•ã€‚</li>
<li>å¯¹è±¡çº§å­—å¹•è¢«ç¼–ç ä¸ºå¥å­åµŒå…¥ï¼Œä½œä¸ºåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å®šç‰¹å¾ç›‘ç£ã€‚</li>
<li>å…±äº«åµŒå…¥ç©ºé—´æ”¯æŒå¼€æ”¾å¼è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ã€‚</li>
<li>çŠ¶æ€å¯å˜å½¢ç½‘ç»œç”¨äºæ¨¡æ‹Ÿå¯¹è±¡åœ¨4Dåœºæ™¯ä¸­çš„è¿ç»­çŠ¶æ€å˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42e8870b6a1f97704a697b4d7463e412.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3082f1495a058b1d2ccbd09f68a5087.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-98f79cb33863ba8bf22796614e80c474.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
