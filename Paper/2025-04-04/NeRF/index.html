<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis"><a href="#Diffusion-Guided-Gaussian-Splatting-for-Large-Scale-Unconstrained-3D-Reconstruction-and-Novel-View-Synthesis" class="headerlink" title="Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis"></a>Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis</h2><p><strong>Authors:Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, Rakesh Kumar</strong></p>
<p>Recent advancements in 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF) have achieved impressive results in real-time 3D reconstruction and novel view synthesis. However, these methods struggle in large-scale, unconstrained environments where sparse and uneven input coverage, transient occlusions, appearance variability, and inconsistent camera settings lead to degraded quality. We propose GS-Diff, a novel 3DGS framework guided by a multi-view diffusion model to address these limitations. By generating pseudo-observations conditioned on multi-view inputs, our method transforms under-constrained 3D reconstruction problems into well-posed ones, enabling robust optimization even with sparse data. GS-Diff further integrates several enhancements, including appearance embedding, monocular depth priors, dynamic object modeling, anisotropy regularization, and advanced rasterization techniques, to tackle geometric and photometric challenges in real-world settings. Experiments on four benchmarks demonstrate that GS-Diff consistently outperforms state-of-the-art baselines by significant margins. </p>
<blockquote>
<p>è¿‘æœŸï¼Œåœ¨ä¸‰ç»´é«˜æ–¯å±•å¸ƒï¼ˆ3DGSï¼‰å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ–¹é¢çš„è¿›å±•ï¼Œå·²åœ¨å®æ—¶ä¸‰ç»´é‡å»ºå’Œæ–°å‹è§†è§’åˆæˆé¢†åŸŸå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤§è§„æ¨¡ã€æ— çº¦æŸçš„ç¯å¢ƒä¸­è¡¨ç°è¾ƒå·®ï¼Œç¨€ç–å’Œä¸å‡åŒ€çš„è¾“å…¥è¦†ç›–ã€æš‚æ—¶çš„é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œä¸ä¸€è‡´çš„ç›¸æœºè®¾ç½®å¯¼è‡´è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†GS-Diffï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¼•å¯¼ä¸‹çš„ä¸‰ç»´é«˜æ–¯å±•å¸ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›å±€é™æ€§ã€‚é€šè¿‡ç”ŸæˆåŸºäºå¤šè§†è§’è¾“å…¥çš„ä¼ªè§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†ç¼ºä¹çº¦æŸçš„ä¸‰ç»´é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºå®šä¹‰æ˜ç¡®çš„é—®é¢˜ï¼Œå³ä½¿åœ¨ç¨€ç–æ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥ä¼˜åŒ–ã€‚GS-Diffè¿˜èåˆäº†å¤šç§å¢å¼ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•ç›®æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å„å‘å¼‚æ€§æ­£åˆ™åŒ–å’Œå…ˆè¿›çš„æ¸²æŸ“æŠ€æœ¯ï¼Œä»¥åº”å¯¹ç°å®ç¯å¢ƒä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-Diffå§‹ç»ˆæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01960v1">PDF</a> WACV ULTRRA Workshop 2025</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºé«˜æ–¯ç»˜åˆ¶æŠ€æœ¯å’Œç¥ç»è¾å°„åœºçš„æœ€æ–°è¿›å±•ï¼Œå·²ç»æˆåŠŸåº”ç”¨äºå®æ—¶ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤§è§„æ¨¡ã€æ— çº¦æŸç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–å’Œä¸å‡åŒ€è¾“å…¥è¦†ç›–ã€ç¬æ—¶é®æŒ¡ã€å¤–è§‚å˜åŒ–å’Œä¸ä¸€è‡´çš„ç›¸æœºè®¾ç½®å¯¼è‡´è´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºGS-Diffï¼Œä¸€ç§æ–°å‹çš„å¤šè§†è§’æ‰©æ•£æ¨¡å‹å¼•å¯¼çš„ä¸‰ç»´é«˜æ–¯ç»˜åˆ¶æ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›å±€é™æ€§ã€‚é€šè¿‡ç”ŸæˆåŸºäºå¤šè§†è§’è¾“å…¥çš„ä¼ªè§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†çº¦æŸä¸è¶³çš„ä¸‰ç»´é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºç»“æ„è‰¯å¥½çš„é—®é¢˜ï¼Œå³ä½¿åœ¨ç¨€ç–æ•°æ®ä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥ä¼˜åŒ–ã€‚GS-Diffè¿›ä¸€æ­¥ç»“åˆäº†å¤–è§‚åµŒå…¥ã€å•ç›®æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€å¼‚å‘æ€§æ­£åˆ™åŒ–å’Œé«˜çº§å…‰æ …åŒ–æŠ€æœ¯ç­‰å¤šä¸ªæ”¹è¿›ç‚¹ï¼Œä»¥è§£å†³çœŸå®ç¯å¢ƒä¸­çš„å‡ ä½•å’Œå…‰åº¦æŒ‘æˆ˜ã€‚åœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGS-Diffæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ul>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹ä¸‰ç»´é«˜æ–¯ç»˜åˆ¶æ¡†æ¶GS-Diffï¼Œé€‚ç”¨äºå®æ—¶ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆã€‚</li>
<li>GS-Diffé€šè¿‡ç”Ÿæˆä¼ªè§‚å¯Ÿç»“æœè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤§å‹æ— çº¦æŸç¯å¢ƒä¸­çš„å±€é™æ€§ã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œå°†çº¦æŸä¸è¶³çš„ä¸‰ç»´é‡å»ºé—®é¢˜è½¬åŒ–ä¸ºç»“æ„è‰¯å¥½çš„é—®é¢˜ã€‚</li>
<li>GS-Diffé›†æˆäº†å¤šç§æŠ€æœ¯æ”¹è¿›ï¼ŒåŒ…æ‹¬å¤–è§‚åµŒå…¥ã€å•ç›®æ·±åº¦å…ˆéªŒã€åŠ¨æ€å¯¹è±¡å»ºæ¨¡ç­‰ï¼Œä»¥åº”å¯¹çœŸå®ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01960">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-592a6485d72b21b72232c5a2ce2bb71f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1ce0624dfdfb12bb64cdccf46f365b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41d8bfdbe85e56ceb5eb98088339d893.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BOGausS-Better-Optimized-Gaussian-Splatting"><a href="#BOGausS-Better-Optimized-Gaussian-Splatting" class="headerlink" title="BOGausS: Better Optimized Gaussian Splatting"></a>BOGausS: Better Optimized Gaussian Splatting</h2><p><strong>Authors:StÃ©phane Pateux, Matthieu Gendrin, Luce Morin, ThÃ©o Ladune, Xiaoran Jiang</strong></p>
<p>3D Gaussian Splatting (3DGS) proposes an efficient solution for novel view synthesis. Its framework provides fast and high-fidelity rendering. Although less complex than other solutions such as Neural Radiance Fields (NeRF), there are still some challenges building smaller models without sacrificing quality. In this study, we perform a careful analysis of 3DGS training process and propose a new optimization methodology. Our Better Optimized Gaussian Splatting (BOGausS) solution is able to generate models up to ten times lighter than the original 3DGS with no quality degradation, thus significantly boosting the performance of Gaussian Splatting compared to the state of the art. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹æ–°å‹è§†è§’åˆæˆçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå³ä¸‰ç»´é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼ˆ3DGSï¼‰ã€‚å…¶æ¡†æ¶å¯å®ç°å¿«é€Ÿä¸”é«˜ä¿çœŸæ¸²æŸ“ã€‚è™½ç„¶ç›¸è¾ƒäºå…¶ä»–è§£å†³æ–¹æ¡ˆï¼ˆå¦‚ç¥ç»è¾å°„åœºNeRFï¼‰å¤æ‚åº¦è¾ƒä½ï¼Œä½†åœ¨æ„å»ºä¸ç‰ºç‰²è´¨é‡çš„å°å‹æ¨¡å‹æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¯¹3DGSè®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ›´å¥½ä¼˜åŒ–é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼ˆBOGausSï¼‰è§£å†³æ–¹æ¡ˆç”Ÿæˆçš„æ¨¡å‹é‡é‡ä¸ºåŸå§‹ä¸‰ç»´é«˜æ–¯è´´å›¾çš„ååˆ†ä¹‹ä¸€ä¸”æ²¡æœ‰ä»»ä½•è´¨é‡é™çº§ï¼Œä»è€Œæå¤§åœ°æé«˜äº†é«˜æ–¯è´´å›¾æŠ€æœ¯åœ¨å½“å‰é¢†åŸŸçš„æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰€æä¾›æ–‡æœ¬ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ä¸‰ç»´é«˜æ–¯æ¨¡ç³ŠæŠ€æœ¯ï¼ˆBOGausSï¼‰ï¼Œä¼˜åŒ–äº†ç°æœ‰çš„3DGSæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡æ–°å‹ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé‡é‡æ¯”åŸå§‹æ¨¡å‹è½»åå€çš„æ¨¡å‹ï¼Œä¸”æ²¡æœ‰è´¨é‡æŸå¤±ï¼Œæ˜¾è‘—æå‡äº†é«˜æ–¯æ¨¡ç³ŠæŠ€æœ¯çš„æ€§èƒ½ã€‚è¿™é¡¹æ–°æŠ€æœ¯é€‚ç”¨äºé«˜è´¨é‡çš„ä¸‰ç»´åœºæ™¯æ¸²æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä»æ–‡æœ¬ä¸­æå–çš„ä¸ƒä¸ªä¸»è¦è§è§£ï¼š</p>
<ul>
<li>ç ”ç©¶äººå‘˜æ·±å…¥åˆ†æäº†ç°æœ‰çš„ä¸‰ç»´é«˜æ–¯æ¨¡ç³ŠæŠ€æœ¯ï¼ˆ3DGSï¼‰è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•å‘½åä¸ºBOGausSï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²è´¨é‡çš„å‰æä¸‹ç”Ÿæˆé‡é‡ä»…ä¸ºåŸå§‹æ¨¡å‹ååˆ†ä¹‹ä¸€çš„æ¨¡å‹ã€‚</li>
<li>BOGausSæ˜¾è‘—æå‡äº†é«˜æ–¯æ¨¡ç³ŠæŠ€æœ¯çš„æ€§èƒ½ï¼Œä½¿å…¶æ›´åŠ é«˜æ•ˆå’Œçµæ´»ã€‚</li>
<li>BOGausSåœ¨æ–°å‹è§†åˆæˆé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿé«˜è´¨é‡åœ°æ¸²æŸ“ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>æ–°æ–¹æ³•é’ˆå¯¹è¾ƒå°çš„æ¨¡å‹è®¾è®¡ï¼Œä¸ºè§£å†³åœ¨æ¨¡å‹å¤§å°ä¸è´¨é‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
<li>è¯¥ç ”ç©¶æŒ‘æˆ˜äº†ç°æœ‰çš„æ¨¡å‹ä¼˜åŒ–ç†è®ºï¼Œå¹¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’Œå¯ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c8cb2eb3263f67d85f48043937cfced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e0bdb62b9bc6ae2299197a6852b669.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment"><a href="#Luminance-GS-Adapting-3D-Gaussian-Splatting-to-Challenging-Lighting-Conditions-with-View-Adaptive-Curve-Adjustment" class="headerlink" title="Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment"></a>Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting   Conditions with View-Adaptive Curve Adjustment</h2><p><strong>Authors:Ziteng Cui, Xuangeng Chu, Tatsuya Harada</strong></p>
<p>Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions â€“ including low-light, overexposure, and varying exposure â€“ while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality. </p>
<blockquote>
<p>åœ¨å¤šç§çœŸå®ä¸–ç•Œå…‰ç…§æ¡ä»¶ä¸‹æ•è·é«˜è´¨é‡ç…§ç‰‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªç„¶å…‰ç…§ï¼ˆä¾‹å¦‚ä½å…‰ç¯å¢ƒï¼‰å’Œç›¸æœºæ›å…‰è®¾ç½®ï¼ˆä¾‹å¦‚æ›å…‰æ—¶é—´ï¼‰éƒ½ä¼šæ˜¾è‘—å½±å“å›¾åƒè´¨é‡ã€‚åœ¨å¤šè§†è§’åœºæ™¯ä¸­ï¼Œè¿™ä¸€æŒ‘æˆ˜æ›´ä¸ºçªå‡ºï¼Œä¸åŒè§†è§’çš„å…‰ç…§å’Œå›¾åƒä¿¡å·å¤„ç†ï¼ˆISPï¼‰è®¾ç½®å˜åŒ–ä¼šå¼•å…¥å…‰åº¦ä¸ä¸€è‡´æ€§ã€‚è¿™ç§å…‰ç…§é€€åŒ–å’Œè§†è§’ç›¸å…³çš„å˜åŒ–ç»™åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æç”»ï¼ˆ3DGSï¼‰çš„æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰æ¡†æ¶å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Luminance-GSï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æç”»åœ¨å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å…‰ç…§æ¡ä»¶ä¸‹å®ç°é«˜è´¨é‡æ–°è§†è§’åˆæˆç»“æœçš„æ–°æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨æ¯è§†å›¾é¢œè‰²çŸ©é˜µæ˜ å°„å’Œè§†å›¾è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼ŒLuminance-GSåœ¨å„ç§å…‰ç…§æ¡ä»¶ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å…‰å’Œå¯å˜æ›å…‰ï¼ŒåŒæ—¶ä¸æ”¹å˜åŸå§‹ä¸‰ç»´é«˜æ–¯æç”»çš„æ˜¾å¼è¡¨ç¤ºã€‚ä¸ä¹‹å‰çš„NeRFå’Œ3DGSåŸºçº¿ç›¸æ¯”ï¼ŒLuminance-GSæä¾›äº†å®æ—¶çš„æ¸²æŸ“é€Ÿåº¦ï¼Œå¹¶æé«˜äº†é‡å»ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01503v1">PDF</a> CVPR 2025, project page:   <a target="_blank" rel="noopener" href="https://cuiziteng.github.io/Luminance_GS_web/">https://cuiziteng.github.io/Luminance_GS_web/</a></p>
<p><strong>Summary</strong><br>é«˜éš¾åº¦å…‰ç…§æ¡ä»¶ä¸‹çš„å¤šè§†è§’åˆæˆæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œæ–°çš„Luminance-GSæ–¹æ³•åˆ©ç”¨3DGSå®ç°é«˜è´¨é‡åˆæˆã€‚é€šè¿‡æ¯è§†è§’è‰²å½©çŸ©é˜µæ˜ å°„å’Œè§†å·®è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼ŒLuminance-GSåœ¨å¤šç§å…‰ç…§æ¡ä»¶ä¸‹å–å¾—æœ€ä¼˜ç»“æœï¼ŒåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å’Œä¸åŒçš„æ›å…‰ã€‚è¯¥æ–¹æ³•ä¿æŒåŸå§‹3DGSæ˜¾å¼è¡¨ç¤ºä¸å˜ï¼ŒåŒæ—¶æä¾›å®æ—¶æ¸²æŸ“é€Ÿåº¦å’Œæ”¹å–„çš„é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰ç…§æ¡ä»¶å¯¹å›¾åƒè´¨é‡æœ‰æ˜¾è‘—å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè§†è§’åœºæ™¯ä¸­ï¼Œå¼•èµ·å…‰åº¦ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ä¼ ç»ŸNeRFå’Œ3DGSæ–¹æ³•åœ¨é¢ä¸´å„ç§å¤æ‚å…‰ç…§æ¡ä»¶æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ–°æ–¹æ³•Luminance-GSé€šè¿‡æ¯è§†è§’è‰²å½©çŸ©é˜µæ˜ å°„å’Œè§†å·®è‡ªé€‚åº”æ›²çº¿è°ƒæ•´ï¼ŒæˆåŠŸåº”å¯¹å¤šç§å¤æ‚å…‰ç…§æ¡ä»¶ã€‚</li>
<li>Luminance-GSåœ¨å¤šç§å…‰ç…§æ¡ä»¶ä¸‹å–å¾—æœ€ä¼˜ç»“æœï¼ŒåŒ…æ‹¬ä½å…‰ã€è¿‡æ›å’Œä¸åŒçš„æ›å…‰ç¯å¢ƒã€‚</li>
<li>è¯¥æ–¹æ³•ç»´æŒåŸå§‹3DGSçš„æ˜¾å¼è¡¨ç¤ºä¸å˜ã€‚</li>
<li>ä¸ä¹‹å‰çš„NeRFå’Œ3DGSåŸºçº¿ç›¸æ¯”ï¼ŒLuminance-GSæä¾›å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35c5c92e224606e546fd03cf5090f540.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d178aa2c017ef369fb2414f73a079403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d77b7cade783144fb29ae6054c1885.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8064f88da9ae906527cf71cbf1d9a992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c2ded93e6712755127a771f8516b4c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œç»™æ‰˜ç®¡è¿™äº›æ¨¡å‹çš„æœåŠ¡æä¾›å•†å¸¦æ¥äº†å„ç§ä¼¦ç†å’Œæ³•å¾‹æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå†…å®¹åˆ é™¤æŠ€æœ¯ï¼ˆCRTsï¼‰ä½œä¸ºæ§åˆ¶è¾“å‡ºè€Œæ— éœ€å…¨é¢å†è®­ç»ƒçš„ç ”ç©¶é¢†åŸŸæ­£æ—¥ç›Šå…´èµ·ã€‚è¿‘æœŸçš„å·¥ä½œæ¢ç´¢äº†åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ä½¿ç”¨æœºå™¨é—å¿˜ï¼ˆMachine Unlearningï¼‰æ¥è§£å†³å†…å®¹åˆ é™¤é—®é¢˜ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç ”ç©¶çš„é‡ç‚¹ä¸»è¦é›†ä¸­åœ¨æ‰©æ•£æ¨¡å‹ä¸Šï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­çš„é—å¿˜ä»ç„¶è¢«å¤§å¤§å¿½è§†ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºText-to-Unlearnæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿä»…ä½¿ç”¨æ–‡æœ¬æç¤ºä»é¢„è®­ç»ƒçš„GANsä¸­é€‰æ‹©æ€§é—å¿˜æ¦‚å¿µï¼Œå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜å’Œåœ¨äººè„¸è®­ç»ƒæ¨¡å‹ä¸­çš„è¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ç­‰ç²¾ç»†ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä»æœ€å…ˆè¿›çš„å›¾åƒæ–‡æœ¬å¯¹é½æŒ‡æ ‡ä¸­å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨é—å¿˜è¯„ä¼°æ–¹æ³•ï¼Œå¯¹é—å¿˜æ–¹æ³•è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒText-to-Unlearnæ˜¯é¦–ä¸ªç”¨äºGANsçš„è·¨æ¨¡æ€é—å¿˜æ¡†æ¶ï¼Œåœ¨ç®¡ç†ç”Ÿæˆæ¨¡å‹è¡Œä¸ºæ–¹é¢ä»£è¡¨äº†çµæ´»è€Œé«˜æ•ˆçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„GANsæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºText-to-Unlearnçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºæœ‰é€‰æ‹©åœ°é—å¿˜æ¦‚å¿µï¼Œå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ä»¥åŠåœ¨äººè„¸æ¨¡å‹ä¸Šæ‰§è¡Œè¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ç­‰ç²¾ç»†ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æŒ‡å¯¼æ— å­¦ä¹ æµç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›å¯ä¼¸ç¼©ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„è‡ªåŠ¨æ— å­¦ä¹ è¯„ä¼°æ–¹æ³•åŸºäºå…ˆè¿›çš„å›¾åƒæ–‡æœ¬å¯¹é½æŒ‡æ ‡ï¼Œå…¨é¢åˆ†ææ— å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚Text-to-Unlearnæ˜¯é¦–ä¸ªè·¨æ¨¡æ€çš„GANsæ— å­¦ä¹ æ¡†æ¶ï¼Œçµæ´»é«˜æ•ˆåœ°ç®¡ç†ç”Ÿæˆæ¨¡å‹çš„è¡Œä¸ºã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>Text-to-Unlearnæ¡†æ¶èƒ½å¤Ÿåœ¨é¢„è®­ç»ƒçš„GANsæ¨¡å‹ä¸­é€šè¿‡æ–‡æœ¬æç¤ºé€‰æ‹©æ€§é—å¿˜æ¦‚å¿µã€‚</li>
<li>è¯¥æ¡†æ¶å¯å®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ç­‰ç²¾ç»†ä»»åŠ¡ï¼Œå¦‚äººè„¸æ¨¡å‹ä¸Šçš„è¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ã€‚</li>
<li>åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æŒ‡å¯¼æ— å­¦ä¹ æµç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå›¾åƒæ–‡æœ¬å¯¹é½æŒ‡æ ‡çš„è‡ªåŠ¨æ— å­¦ä¹ è¯„ä¼°æ–¹æ³•ï¼Œå…¨é¢è¯„ä¼°æ— å­¦ä¹ æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Text-to-Unlearnæ˜¯é¦–ä¸ªé’ˆå¯¹GANsçš„è·¨æ¨¡æ€æ— å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºç®¡ç†ç”Ÿæˆæ¨¡å‹è¡Œä¸ºæä¾›äº†çµæ´»é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç›®å‰è¯¥å·¥ä½œè§£å†³äº†é•¿æœŸä»¥æ¥åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­æœªè¢«å……åˆ†ç ”ç©¶çš„æ— å­¦ä¹ é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Neural-Pruning-for-3D-Scene-Reconstruction-Efficient-NeRF-Acceleration"><a href="#Neural-Pruning-for-3D-Scene-Reconstruction-Efficient-NeRF-Acceleration" class="headerlink" title="Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration"></a>Neural Pruning for 3D Scene Reconstruction: Efficient NeRF Acceleration</h2><p><strong>Authors:Tianqi Ding, Dawei Xiang, Pablo Rivas, Liang Dong</strong></p>
<p>Neural Radiance Fields (NeRF) have become a popular 3D reconstruction approach in recent years. While they produce high-quality results, they also demand lengthy training times, often spanning days. This paper studies neural pruning as a strategy to address these concerns. We compare pruning approaches, including uniform sampling, importance-based methods, and coreset-based techniques, to reduce the model size and speed up training. Our findings show that coreset-driven pruning can achieve a 50% reduction in model size and a 35% speedup in training, with only a slight decrease in accuracy. These results suggest that pruning can be an effective method for improving the efficiency of NeRF models in resource-limited settings. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¿‘å¹´æ¥å·²æˆä¸ºæµè¡Œçš„3Dé‡å»ºæ–¹æ³•ã€‚è™½ç„¶å®ƒä»¬èƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œä½†ä¹Ÿéœ€è¦é•¿æ—¶é—´çš„è®­ç»ƒï¼Œé€šå¸¸æŒç»­æ•°å¤©ã€‚æœ¬æ–‡é’ˆå¯¹ç¥ç»ä¿®å‰ªä½œä¸ºä¸€ç§è§£å†³è¿™äº›é—®é¢˜çš„ç­–ç•¥è¿›è¡Œäº†ç ”ç©¶ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¿®å‰ªæ–¹æ³•ï¼ŒåŒ…æ‹¬å‡åŒ€é‡‡æ ·ã€åŸºäºé‡è¦æ€§çš„æ–¹æ³•å’ŒåŸºäºæ ¸å¿ƒé›†çš„æŠ€æœ¯ï¼Œä»¥å‡å°æ¨¡å‹å¤§å°å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ ¸å¿ƒé›†çš„ä¿®å‰ªå¯ä»¥å®ç°æ¨¡å‹å¤§å°å‡å°‘50%ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜35%ï¼ŒåŒæ—¶ä»…ç•¥å¾®é™ä½å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœæš—ç¤ºï¼Œåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œä¿®å‰ªå¯èƒ½æ˜¯æé«˜NeRFæ¨¡å‹æ•ˆç‡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00950v1">PDF</a> 12 pages, 4 figures, accepted by International Conference on the AI   Revolution: Research, Ethics, and Society (AIR-RES 2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ˜¯è¿‘å¹´æ¥æµè¡Œçš„3Dé‡å»ºæ–¹æ³•ï¼Œè™½ç„¶èƒ½äº§ç”Ÿé«˜è´¨é‡ç»“æœï¼Œä½†è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œå¸¸éœ€æ•°æ—¥ã€‚æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡ç¥ç»å‰ªæç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒçš„å‰ªææ–¹æ³•ï¼ŒåŒ…æ‹¬å‡åŒ€é‡‡æ ·ã€åŸºäºé‡è¦æ€§çš„æ–¹æ³•å’ŒåŸºäºæ ¸å¿ƒé›†çš„æŠ€å·§ï¼Œä»¥å‡å°æ¨¡å‹å¤§å°å¹¶åŠ é€Ÿè®­ç»ƒã€‚ç ”ç©¶å‘ç°ï¼ŒåŸºäºæ ¸å¿ƒé›†çš„å‰ªææ–¹æ³•èƒ½åœ¨ä¿æŒè½»å¾®ç²¾åº¦æŸå¤±çš„å‰æä¸‹ï¼Œå®ç°æ¨¡å‹å¤§å°å‡å°‘50%ï¼Œè®­ç»ƒé€Ÿåº¦æå‡35%ã€‚è¿™è¡¨æ˜åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå‰ªææ˜¯æå‡NeRFæ¨¡å‹æ•ˆç‡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>NeRFå·²æˆä¸ºæµè¡Œçš„3Dé‡å»ºæ–¹æ³•ï¼Œä½†è®­ç»ƒæ—¶é—´é•¿æ˜¯å…¶ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç¥ç»å‰ªæä½œä¸ºä¸€ç§ç­–ç•¥è¢«ç ”ç©¶ï¼Œæ—¨åœ¨è§£å†³è®­ç»ƒæ—¶é—´é•¿å’Œæ¨¡å‹ä½“ç§¯å¤§çš„é—®é¢˜ã€‚</li>
<li>å¯¹æ¯”äº†å¤šç§å‰ªææ–¹æ³•ï¼ŒåŒ…æ‹¬å‡åŒ€é‡‡æ ·ã€åŸºäºé‡è¦æ€§å’ŒåŸºäºæ ¸å¿ƒé›†çš„æŠ€å·§ã€‚</li>
<li>åŸºäºæ ¸å¿ƒé›†çš„å‰ªææ–¹æ³•èƒ½åœ¨å‡å°æ¨¡å‹å¤§å°ï¼ˆ50%ï¼‰å’ŒåŠ é€Ÿè®­ç»ƒï¼ˆ35%ï¼‰æ–¹é¢å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>å‰ªææ–¹æ³•åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­èƒ½æœ‰æ•ˆæå‡NeRFæ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>è½»å¾®ç²¾åº¦æŸå¤±æ˜¯å‰ªæç­–ç•¥éœ€è¦å¹³è¡¡çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-03357584e64e5f750a3549f963e69108.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0a0c65908f5341c99362f273efc7be2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a6e94fe72a3744b86645df943d43711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbab8306837a8bb33c92d1d3b371e755.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Data-Cleansing-for-GANs"><a href="#Data-Cleansing-for-GANs" class="headerlink" title="Data Cleansing for GANs"></a>Data Cleansing for GANs</h2><p><strong>Authors:Naoyuki Terashita, Hiroki Ohashi, Satoshi Hara</strong></p>
<p>As the application of generative adversarial networks (GANs) expands, it becomes increasingly critical to develop a unified approach that improves performance across various generative tasks. One effective strategy that applies to any machine learning task is identifying harmful instances, whose removal improves the performance. While previous studies have successfully estimated these harmful training instances in supervised settings, their approaches are not easily applicable to GANs. The challenge lies in two requirements of the previous approaches that do not apply to GANs. First, previous approaches require that the absence of a training instance directly affects the parameters. However, in the training for GANs, the instances do not directly affect the generatorâ€™s parameters since they are only fed into the discriminator. Second, previous approaches assume that the change in loss directly quantifies the harmfulness of the instance to a modelâ€™s performance, while common types of GAN losses do not always reflect the generative performance. To overcome the first challenge, we propose influence estimation methods that use the Jacobian of the generatorâ€™s gradient with respect to the discriminatorâ€™s parameters (and vice versa). Such a Jacobian represents the indirect effect between two models: how removing an instance from the discriminatorâ€™s training changes the generatorâ€™s parameters. Second, we propose an instance evaluation scheme that measures the harmfulness of each training instance based on how a GAN evaluation metric (e.g., Inception score) is expected to change by the instanceâ€™s removal. Furthermore, we demonstrate that removing the identified harmful instances significantly improves the generative performance on various GAN evaluation metrics. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åº”ç”¨ä¸æ–­æ‰©å¤§ï¼Œå¼€å‘ä¸€ç§é€‚ç”¨äºå„ç§ç”Ÿæˆä»»åŠ¡çš„ç»Ÿä¸€æ–¹æ³•å˜å¾—è‡³å…³é‡è¦ã€‚é€‚ç”¨äºä»»ä½•æœºå™¨å­¦ä¹ ä»»åŠ¡çš„æœ‰æ•ˆçš„ç­–ç•¥æ˜¯è¯†åˆ«æœ‰å®³å®ä¾‹ï¼Œç§»é™¤å®ƒä»¬å¯ä»¥æé«˜æ€§èƒ½ã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶å·²ç»åœ¨æœ‰ç›‘ç£ç¯å¢ƒä¸­æˆåŠŸåœ°ä¼°è®¡äº†è¿™äº›æœ‰å®³çš„è®­ç»ƒå®ä¾‹ï¼Œä½†å®ƒä»¬çš„æ–¹æ³•å¹¶ä¸æ˜“äºåº”ç”¨äºGANsã€‚æŒ‘æˆ˜åœ¨äºä»¥å‰çš„æ–¹æ³•çš„ä¸¤ä¸ªè¦æ±‚ä¸é€‚ç”¨äºGANsã€‚é¦–å…ˆï¼Œä»¥å‰çš„æ–¹æ³•è¦æ±‚è®­ç»ƒå®ä¾‹çš„ç¼ºå¤±ç›´æ¥å½±å“å‚æ•°ã€‚ç„¶è€Œï¼Œåœ¨GANsçš„è®­ç»ƒä¸­ï¼Œå®ä¾‹å¹¶ä¸ä¼šç›´æ¥å½±å“ç”Ÿæˆå™¨çš„å‚æ•°ï¼Œå› ä¸ºå®ƒä»¬åªè¾“å…¥åˆ°åˆ¤åˆ«å™¨ä¸­ã€‚å…¶æ¬¡ï¼Œä»¥å‰çš„æ–¹æ³•å‡è®¾æŸå¤±çš„å˜åŒ–ç›´æ¥é‡åŒ–å®ä¾‹å¯¹æ¨¡å‹æ€§èƒ½çš„å±å®³æ€§ï¼Œè€Œå¸¸è§çš„GANæŸå¤±å¹¶ä¸æ€»æ˜¯åæ˜ ç”Ÿæˆæ€§èƒ½ã€‚ä¸ºäº†å…‹æœç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨ç”Ÿæˆå™¨æ¢¯åº¦ç›¸å¯¹äºåˆ¤åˆ«å™¨å‚æ•°çš„é›…å¯æ¯”ï¼ˆä»¥åŠåä¹‹ï¼‰æ¥ä¼°è®¡å½±å“çš„æ–¹æ³•ã€‚è¿™æ ·çš„é›…å¯æ¯”ä»£è¡¨äº†ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„é—´æ¥å½±å“ï¼šç§»é™¤å®ä¾‹å¯¹åˆ¤åˆ«å™¨è®­ç»ƒå¦‚ä½•æ”¹å˜ç”Ÿæˆå™¨çš„å‚æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ä¾‹è¯„ä¼°æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåŸºäºGANè¯„ä¼°æŒ‡æ ‡ï¼ˆä¾‹å¦‚Inceptionåˆ†æ•°ï¼‰çš„é¢„æœŸå˜åŒ–æ¥æµ‹é‡æ¯ä¸ªè®­ç»ƒå®ä¾‹çš„å±å®³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œç§»é™¤å·²è¯†åˆ«å‡ºçš„æœ‰å®³å®ä¾‹å¯ä»¥æ˜¾è‘—æé«˜å„ç§GANè¯„ä¼°æŒ‡æ ‡çš„ç”Ÿæˆæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00603v1">PDF</a> Accepted for IEEE Transactions on Neural Networks and Learning   Systems (TNNLS, 2025). Journal extention of   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=opHLcXxYTC">https://openreview.net/forum?id=opHLcXxYTC</a>_</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åº”ç”¨ä¸­ï¼Œè¯†åˆ«å¹¶ç§»é™¤æœ‰å®³å®ä¾‹çš„é‡è¦æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºGANsï¼Œå› æ­¤æå‡ºé€šè¿‡è®¡ç®—ç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨å‚æ•°é—´çš„é›…å¯æ¯”çŸ©é˜µæ¥ä¼°è®¡å½±å“çš„æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºGANè¯„ä»·æŒ‡æ ‡å˜åŒ–æ¥è¯„ä¼°æ¯ä¸ªè®­ç»ƒå®ä¾‹å±å®³æ€§çš„æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œç§»é™¤æœ‰å®³å®ä¾‹èƒ½æ˜¾è‘—æé«˜GANçš„ç”Ÿæˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANsåœ¨ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œéœ€è¦å¼€å‘ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¥æé«˜å…¶åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>è¯†åˆ«å¹¶ç§»é™¤æœ‰å®³å®ä¾‹æ˜¯ä¸€ç§æœ‰æ•ˆæé«˜æœºå™¨å­¦ä¹ æ€§èƒ½çš„ç­–ç•¥ã€‚</li>
<li>ä»¥å¾€çš„ç ”ç©¶æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºGANsï¼Œå› ä¸ºGANsçš„è®­ç»ƒè¿‡ç¨‹ä¸­å®ä¾‹ä¸ç›´æ¥å½±å“ç”Ÿæˆå™¨çš„å‚æ•°ã€‚</li>
<li>æå‡ºé€šè¿‡è®¡ç®—é›…å¯æ¯”çŸ©é˜µæ¥ä¼°è®¡å®ä¾‹å¯¹GANæ€§èƒ½çš„å½±å“ï¼Œè¯¥çŸ©é˜µè¡¨ç¤ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨å‚æ•°ä¹‹é—´çš„é—´æ¥å½±å“ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºGANè¯„ä»·æŒ‡æ ‡å˜åŒ–æ¥è¯„ä¼°æ¯ä¸ªè®­ç»ƒå®ä¾‹å±å®³æ€§çš„æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œç§»é™¤æœ‰å®³å®ä¾‹èƒ½æ˜¾è‘—æé«˜GANçš„ç”Ÿæˆæ€§èƒ½åœ¨å„ç§è¯„ä»·æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2e44131fb54901bb97be4d234ea012a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7400537dee6e46df021ca29ff2e23dfe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors"><a href="#LITA-GS-Illumination-Agnostic-Novel-View-Synthesis-via-Reference-Free-3D-Gaussian-Splatting-and-Physical-Priors" class="headerlink" title="LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors"></a>LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free   3D Gaussian Splatting and Physical Priors</h2><p><strong>Authors:Han Zhou, Wei Dong, Jun Chen</strong></p>
<p>Directly employing 3D Gaussian Splatting (3DGS) on images with adverse illumination conditions exhibits considerable difficulty in achieving high-quality, normally-exposed representations due to: (1) The limited Structure from Motion (SfM) points estimated in adverse illumination scenarios fail to capture sufficient scene details; (2) Without ground-truth references, the intensive information loss, significant noise, and color distortion pose substantial challenges for 3DGS to produce high-quality results; (3) Combining existing exposure correction methods with 3DGS does not achieve satisfactory performance due to their individual enhancement processes, which lead to the illumination inconsistency between enhanced images from different viewpoints. To address these issues, we propose LITA-GS, a novel illumination-agnostic novel view synthesis method via reference-free 3DGS and physical priors. Firstly, we introduce an illumination-invariant physical prior extraction pipeline. Secondly, based on the extracted robust spatial structure prior, we develop the lighting-agnostic structure rendering strategy, which facilitates the optimization of the scene structure and object appearance. Moreover, a progressive denoising module is introduced to effectively mitigate the noise within the light-invariant representation. We adopt the unsupervised strategy for the training of LITA-GS and extensive experiments demonstrate that LITA-GS surpasses the state-of-the-art (SOTA) NeRF-based method while enjoying faster inference speed and costing reduced training time. The code is released at <a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS">https://github.com/LowLevelAI/LITA-GS</a>. </p>
<blockquote>
<p>ç›´æ¥å¯¹ä¸è‰¯ç…§æ˜æ¡ä»¶ä¸‹çš„å›¾åƒåº”ç”¨ä¸‰ç»´é«˜æ–¯è´´ç‰‡ï¼ˆ3DGSï¼‰åœ¨å®ç°é«˜è´¨é‡çš„æ­£å¸¸æ›å…‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨ç›¸å½“å¤§çš„å›°éš¾ï¼ŒåŸå› å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰åœ¨ä¸è‰¯ç…§æ˜åœºæ™¯ä¸­ä¼°è®¡çš„è¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰ç‚¹æœ‰é™ï¼Œæ— æ³•æ•è·è¶³å¤Ÿçš„åœºæ™¯ç»†èŠ‚ï¼›ï¼ˆ2ï¼‰æ²¡æœ‰åœ°é¢çœŸå®å‚è€ƒï¼Œå¯†é›†çš„ä¿¡æ¯ä¸¢å¤±ã€æ˜¾è‘—çš„å™ªå£°å’Œé¢œè‰²å¤±çœŸç»™3DGSå¸¦æ¥å¾ˆå¤§æŒ‘æˆ˜ï¼Œéš¾ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼›ï¼ˆ3ï¼‰å°†ç°æœ‰çš„æ›å…‰æ ¡æ­£æ–¹æ³•ä¸3DGSç›¸ç»“åˆå¹¶ä¸èƒ½å®ç°ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬çš„ä¸ªåˆ«å¢å¼ºå¤„ç†è¿‡ç¨‹å¯¼è‡´ä»ä¸åŒè§†è§’å¢å¼ºçš„å›¾åƒä¹‹é—´ç…§æ˜ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LITA-GSï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— ç…§æ˜å‚è€ƒæ–°è§†è§’åˆæˆæ–¹æ³•ï¼Œé€šè¿‡æ— å‚è€ƒçš„3DGSå’Œç‰©ç†å…ˆéªŒæ¥å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…‰ç…§ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ã€‚å…¶æ¬¡ï¼ŒåŸºäºæå–çš„ç¨³å¥ç©ºé—´ç»“æ„å…ˆéªŒï¼Œæˆ‘ä»¬å¼€å‘äº†å…‰ç…§æ— å…³çš„ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œè¿™æœ‰åŠ©äºä¼˜åŒ–åœºæ™¯ç»“æ„å’Œç‰©ä½“å¤–è§‚ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ¸è¿›çš„å»å™ªæ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°å‡è½»å…‰ä¸å˜è¡¨ç¤ºä¸­çš„å™ªå£°ã€‚æˆ‘ä»¬é‡‡ç”¨æ— ç›‘ç£ç­–ç•¥å¯¹LITA-GSè¿›è¡Œè®­ç»ƒï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒLITA-GSè¶…è¶Šäº†åŸºäºç¥ç»è¾å°„åœºè¡¨ç¤ºï¼ˆNeRFï¼‰çš„æœ€æ–°æ–¹æ³•ï¼ŒåŒæ—¶äº«æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œå‡å°‘çš„è®­ç»ƒæ—¶é—´ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LowLevelAI/LITA-GS%E3%80%82">https://github.com/LowLevelAI/LITA-GSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00219v1">PDF</a> Accepted by CVPR 2025. 3DGS, Adverse illumination conditions,   Reference-free, Physical priors</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åœ¨ä¸è‰¯å…‰ç…§æ¡ä»¶ä¸‹ç›´æ¥åº”ç”¨3Dé«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰çš„å›°éš¾ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰ç‚¹ä¼°è®¡ä¸è¶³ã€ä¿¡æ¯ä¸¢å¤±ä¸¥é‡ã€å™ªå£°å’Œè‰²å½©å¤±çœŸç­‰é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºLITA-GSçš„æ–°å‹æ— å‚è€ƒ3DGSæ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥ç…§æ˜ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ï¼Œå¼€å‘ç…§æ˜æ— å…³ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œå¹¶å¼•å…¥æ¸è¿›é™å™ªæ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼ŒLITA-GSåœ¨NeRFæ–¹æ³•çš„åŸºç¡€ä¸Šè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†é€Ÿåº¦å’Œé™ä½äº†è®­ç»ƒæ—¶é—´ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä¸è‰¯å…‰ç…§æ¡ä»¶ä¸‹ç›´æ¥åº”ç”¨3Dé«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰é¢ä¸´å›°éš¾ï¼Œå¦‚ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰ç‚¹ä¼°è®¡ä¸è¶³ã€ä¿¡æ¯ä¸¢å¤±ä¸¥é‡ç­‰ã€‚</li>
<li>LITA-GSæ–¹æ³•å¼•å…¥ç…§æ˜ä¸å˜ç‰©ç†å…ˆéªŒæå–ç®¡é“ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>LITA-GSåŸºäºæå–çš„ç¨³å¥ç©ºé—´ç»“æ„å…ˆéªŒï¼Œå¼€å‘ç…§æ˜æ— å…³ç»“æ„æ¸²æŸ“ç­–ç•¥ï¼Œä¼˜åŒ–åœºæ™¯ç»“æ„å’Œç‰©ä½“å¤–è§‚ã€‚</li>
<li>LITA-GSé‡‡ç”¨æ¸è¿›é™å™ªæ¨¡å—ï¼Œæœ‰æ•ˆå‡è½»å…‰ç…§ä¸å˜è¡¨ç¤ºä¸­çš„å™ªå£°ã€‚</li>
<li>LITA-GSé‡‡ç”¨æ— ç›‘ç£ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½è¶…è¶Šç°æœ‰NeRFæ–¹æ³•ï¼ŒåŒæ—¶æé«˜æ¨ç†é€Ÿåº¦å’Œé™ä½è®­ç»ƒæ—¶é—´ã€‚</li>
<li>LITA-GSçš„ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼Œä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-724b60f30c6aa43bb38a6f183871b706.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17aa639b3cdddc86604f9ed53f258f16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c37b58252ecdf8d6303c5f74971ec484.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f92eeb53810a3b1e93299a7488f9dee.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer"><a href="#ERUPT-Efficient-Rendering-with-Unposed-Patch-Transformer" class="headerlink" title="ERUPT: Efficient Rendering with Unposed Patch Transformer"></a>ERUPT: Efficient Rendering with Unposed Patch Transformer</h2><p><strong>Authors:Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor</strong></p>
<p>This work addresses the problem of novel view synthesis in diverse scenes from small collections of RGB images. We propose ERUPT (Efficient Rendering with Unposed Patch Transformer) a state-of-the-art scene reconstruction model capable of efficient scene rendering using unposed imagery. We introduce patch-based querying, in contrast to existing pixel-based queries, to reduce the compute required to render a target view. This makes our model highly efficient both during training and at inference, capable of rendering at 600 fps on commercial hardware. Notably, our model is designed to use a learned latent camera pose which allows for training using unposed targets in datasets with sparse or inaccurate ground truth camera pose. We show that our approach can generalize on large real-world data and introduce a new benchmark dataset (MSVS-1M) for latent view synthesis using street-view imagery collected from Mapillary. In contrast to NeRF and Gaussian Splatting, which require dense imagery and precise metadata, ERUPT can render novel views of arbitrary scenes with as few as five unposed input images. ERUPT achieves better rendered image quality than current state-of-the-art methods for unposed image synthesis tasks, reduces labeled data requirements by ~95% and decreases computational requirements by an order of magnitude, providing efficient novel view synthesis for diverse real-world scenes. </p>
<blockquote>
<p>æœ¬æ–‡è§£å†³äº†ä»å°å‹RGBå›¾åƒé›†åˆä¸­å¯¹å¤šæ ·åœºæ™¯è¿›è¡Œæ–°å‹è§†è§’åˆæˆçš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ERUPTï¼ˆæ— é¢„è®¾è¡¥ä¸è½¬æ¢å™¨é«˜æ•ˆæ¸²æŸ“ï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§æœ€å…ˆè¿›çš„åœºæ™¯é‡å»ºæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ— é¢„è®¾å›¾åƒä¸­å®ç°é«˜æ•ˆåœºæ™¯æ¸²æŸ“ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºè¡¥ä¸çš„æŸ¥è¯¢ï¼Œä¸ç°æœ‰çš„åŸºäºåƒç´ çš„æŸ¥è¯¢ç›¸æ¯”ï¼Œå‡å°‘äº†æ¸²æŸ“ç›®æ ‡è§†å›¾æ‰€éœ€çš„è®¡ç®—é‡ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½éå¸¸é«˜æ•ˆï¼Œèƒ½å¤Ÿåœ¨å•†ç”¨ç¡¬ä»¶ä¸Šä»¥600å¸§&#x2F;ç§’çš„é€Ÿåº¦è¿›è¡Œæ¸²æŸ“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨å­¦ä¹ åˆ°çš„æ½œåœ¨ç›¸æœºå§¿æ€ï¼Œè¿™å…è®¸åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨æ— é¢„è®¾ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå³ä½¿æ•°æ®é›†å…·æœ‰ç¨€ç–æˆ–ä¸å‡†ç¡®çš„çœŸå®ç›¸æœºå§¿æ€ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œæ³›åŒ–ï¼Œå¹¶ä½¿ç”¨ä»Mapillaryæ”¶é›†çš„è¡—é“è§†å›¾å›¾åƒå¼•å…¥äº†æ–°çš„åŸºå‡†æ•°æ®é›†ï¼ˆMSVS-1Mï¼‰ç”¨äºæ½œåœ¨è§†è§’åˆæˆã€‚ä¸éœ€è¦å¯†é›†å›¾åƒå’Œç²¾ç¡®å…ƒæ•°æ®çš„NeRFå’ŒGaussian Splattingç›¸æ¯”ï¼ŒERUPTä»…ä½¿ç”¨äº”å¼ æ— é¢„è®¾è¾“å…¥å›¾åƒå°±èƒ½æ¸²æŸ“ä»»æ„åœºæ™¯çš„æ–°è§†è§’ã€‚ERUPTåœ¨æ— é¢„è®¾å›¾åƒåˆæˆä»»åŠ¡æ–¹é¢å®ç°äº†æ¯”å½“å‰æœ€å…ˆè¿›æ–¹æ³•æ›´å¥½çš„æ¸²æŸ“å›¾åƒè´¨é‡ï¼Œå°†æ ‡æ³¨æ•°æ®éœ€æ±‚å‡å°‘äº†çº¦95ï¼…ï¼Œå¹¶å°†è®¡ç®—éœ€æ±‚å‡å°‘äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œä¸ºå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œåœºæ™¯æä¾›äº†é«˜æ•ˆçš„æ–°å‹è§†è§’åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24374v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºERUPTï¼ˆåŸºäºæ— å§¿æ€å›¾åƒå—çš„Transformeré«˜æ•ˆæ¸²æŸ“ï¼‰çš„åœºæ™¯é‡å»ºæ¨¡å‹ï¼Œç”¨äºè§£å†³ä»ä¸åŒRGBå›¾åƒé›†åˆä¸­åˆæˆæ–°è§†è§’çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å¼•å…¥åŸºäºå›¾åƒå—çš„æŸ¥è¯¢æ–¹å¼ï¼Œç›¸è¾ƒäºç°æœ‰çš„åŸºäºåƒç´ çš„æŸ¥è¯¢ï¼Œå¤§å¹…é™ä½äº†æ¸²æŸ“ç›®æ ‡è§†è§’æ‰€éœ€çš„è®¡ç®—é‡ï¼ŒåŒæ—¶åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½è¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œå¯åœ¨å•†ç”¨ç¡¬ä»¶ä¸Šå®ç°600å¸§çš„æ¸²æŸ“é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å­¦ä¹ åˆ°çš„æ½œåœ¨ç›¸æœºå§¿æ€ï¼Œå¯åœ¨æ•°æ®é›†ç¨€ç–æˆ–åœ°é¢çœŸå®ç›¸æœºå§¿æ€ä¸å‡†ç¡®çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚ERUPTæ¨¡å‹èƒ½å¤Ÿåœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œæ¨å¹¿ï¼Œå¹¶å¼•å…¥æ–°çš„åŸºå‡†æ•°æ®é›†MSVS-1Mï¼Œç”¨äºæ½œåœ¨è§†è§’åˆæˆçš„è¡—é“è§†å›¾å›¾åƒé‡‡é›†ã€‚ç›¸è¾ƒäºéœ€è¦å¯†é›†å›¾åƒå’Œç²¾ç¡®å…ƒæ•°æ®çš„NeRFå’Œé«˜æ–¯è´´å›¾æ–¹æ³•ï¼ŒERUPTä»…éœ€å°‘é‡æ— å§¿æ€è¾“å…¥å›¾åƒå°±èƒ½åˆæˆæ–°çš„è§†è§’ã€‚æ­¤å¤–ï¼ŒERUPTåœ¨æ— å§¿æ€å›¾åƒåˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œå°†æ ‡è®°æ•°æ®éœ€æ±‚é™ä½äº†çº¦95%ï¼Œå¹¶å°†è®¡ç®—éœ€æ±‚é™ä½äº†ä¸€ä¸ªæ•°é‡çº§ï¼Œä¸ºå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œåœºæ™¯æä¾›äº†é«˜æ•ˆçš„æ–°è§†è§’åˆæˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ERUPTæ¨¡å‹è§£å†³äº†ä»ä¸åŒRGBå›¾åƒé›†åˆä¸­åˆæˆæ–°è§†è§’çš„é—®é¢˜ã€‚</li>
<li>ERUPTå¼•å…¥åŸºäºå›¾åƒå—çš„æŸ¥è¯¢æ–¹å¼ï¼Œæé«˜æ¸²æŸ“æ•ˆç‡ã€‚</li>
<li>ERUPTæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½è¡¨ç°å‡ºæé«˜çš„æ•ˆç‡ï¼Œå¯åœ¨å•†ç”¨ç¡¬ä»¶ä¸Šå®ç°é«˜é€Ÿæ¸²æŸ“ã€‚</li>
<li>ERUPTé‡‡ç”¨å­¦ä¹ åˆ°çš„æ½œåœ¨ç›¸æœºå§¿æ€ï¼Œé€‚åº”æ•°æ®é›†å§¿æ€ä¿¡æ¯ä¸å‡†ç¡®çš„æƒ…å†µã€‚</li>
<li>ERUPTæ¨¡å‹èƒ½å¤Ÿåœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè¿›è¡Œæ¨å¹¿ï¼Œå¹¶å¼•å…¥æ–°çš„åŸºå‡†æ•°æ®é›†MSVS-1Mã€‚</li>
<li>ERUPTæ¨¡å‹ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œåœ¨æ— å§¿æ€å›¾åƒåˆæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æ›´å¥½çš„æ¸²æŸ“è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ca11a43b27495e28893890b44f8abb7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5bda767db22f2952901ef41d0f05e35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81bcaa3ca5eff325183266b91206f977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2449e77485b4db1e24f8a3a2589fa36.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Style-Quantization-for-Data-Efficient-GAN-Training"><a href="#Style-Quantization-for-Data-Efficient-GAN-Training" class="headerlink" title="Style Quantization for Data-Efficient GAN Training"></a>Style Quantization for Data-Efficient GAN Training</h2><p><strong>Authors:Jian Wang, Xin Lan, Jizhe Zhou, Yuxin Tian, Jiancheng Lv</strong></p>
<p>Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific real data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled &#96;&#96;styleâ€™â€™ space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method. </p>
<blockquote>
<p>åœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰é€šå¸¸éš¾ä»¥å¯¼èˆªå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨è¾“å…¥æ½œåœ¨ç©ºé—´ã€‚å› æ­¤ï¼Œåœ¨ç¨€ç–è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­ä»ç›¸é‚»å˜é‡ç”Ÿæˆçš„å›¾åƒåœ¨çœŸå®æ€§æ–¹é¢å¯èƒ½å­˜åœ¨é‡å¤§å·®å¼‚ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQ-GANè¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆå¢å¼ºCRã€‚è¯¥æ–¹æ³•å°†ç¨€ç–ã€è¿ç»­çš„è¾“å…¥æ½œåœ¨ç©ºé—´è½¬æ¢ä¸ºç´§å‡‘ã€ç»“æ„åŒ–çš„ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œä½¿æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„çœŸå®æ•°æ®ç‚¹ï¼Œä»è€Œæé«˜CRæ€§èƒ½ã€‚æˆ‘ä»¬ä¸æ˜¯ç›´æ¥è¿›è¡Œé‡åŒ–ï¼Œè€Œæ˜¯é¦–å…ˆå°†è¾“å…¥æ½œåœ¨å˜é‡æ˜ å°„åˆ°ä¸é‚£ä¹ˆçº ç¼ çš„â€œé£æ ¼â€ç©ºé—´ï¼Œç„¶åä½¿ç”¨å¯å­¦ä¹ çš„ä»£ç æœ¬è¿›è¡Œé‡åŒ–ã€‚è¿™ä½¿å¾—æ¯ä¸ªé‡åŒ–ä»£ç èƒ½å¤Ÿæ§åˆ¶ä¸åŒçš„å˜é‡å› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æœ€ä½³ä¼ è¾“è·ç¦»ï¼Œå°†ä»£ç æœ¬ä»£ç ä¸åŸºç¡€æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­æå–çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå°†å¤–éƒ¨çŸ¥è¯†åµŒå…¥åˆ°ä»£ç æœ¬ä¸­ï¼Œå¹¶å»ºç«‹è¯­ä¹‰ä¸°å¯Œçš„è¯æ±‡è¡¨ï¼Œé€‚å½“æè¿°è®­ç»ƒæ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ¤åˆ«å™¨ç¨³å¥æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸‹ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨å¯¼èˆªå’Œæœ‰æ•ˆæ¢ç´¢è¾“å…¥æ½œåœ¨ç©ºé—´æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚å½“åœ¨ç¨€ç–çš„è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­ä»ç›¸é‚»å˜é‡ç”Ÿæˆå›¾åƒæ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°ç°å®æ„Ÿçš„æ˜¾è‘—å·®å¼‚ï¼Œå¯¼è‡´æ¬¡ä¼˜çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰ç»“æœã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQ-GANè¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆï¼Œå¢å¼ºCRæ•ˆæœã€‚å®ƒå°†ç¨€ç–çš„è¿ç»­è¾“å…¥æ½œåœ¨ç©ºé—´è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œä½¿æ¯ä¸ªå…ƒç´ éƒ½èƒ½å¯¹åº”ä¸€ä¸ªçœŸå®çš„æ•°æ®ç‚¹ï¼Œä»è€Œæé«˜CRæ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨å¯å­¦ä¹ çš„ä»£ç æœ¬è¿›è¡Œé‡åŒ–ï¼Œè€Œéç›´æ¥é‡åŒ–ï¼Œé¦–å…ˆå°†è¾“å…¥æ½œåœ¨å˜é‡æ˜ å°„åˆ°ä¸€ä¸ªè¾ƒå°‘çº ç¼ çš„â€œé£æ ¼â€ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¼˜åŒ–äº†è¿è¾“è·ç¦»ï¼Œä½¿ä»£ç æœ¬ä»£ç ä¸åŸºç¡€æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­æå–çš„ç‰¹å¾ä¿æŒä¸€è‡´ï¼Œå°†å¤–éƒ¨çŸ¥è¯†åµŒå…¥ä»£ç æœ¬ä¸­ï¼Œå»ºç«‹äº†ä¸°å¯Œçš„è¯­ä¹‰è¯æ±‡è¡¨æ¥æè¿°è®­ç»ƒæ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ¤åˆ«å™¨ç¨³å¥æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANsåœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸‹åœ¨æ¢ç´¢å’Œå¯¼èˆªæ½œåœ¨ç©ºé—´æ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>åœ¨ç¨€ç–çš„è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­ç”Ÿæˆçš„å›¾åƒå¯èƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç°å®æ„Ÿå·®å¼‚ã€‚</li>
<li>æå‡ºSQ-GANæ–¹æ³•ï¼Œé€šè¿‡é£æ ¼ç©ºé—´é‡åŒ–å¢å¼ºä¸€è‡´æ€§æ­£åˆ™åŒ–æ•ˆæœã€‚</li>
<li>å°†è¿ç»­è¾“å…¥æ½œåœ¨ç©ºé—´è½¬åŒ–ä¸ºç¦»æ•£ä»£ç†ç©ºé—´ï¼Œæé«˜CRæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨å¯å­¦ä¹ çš„ä»£ç æœ¬è¿›è¡Œé‡åŒ–ï¼Œä½¿æ¯ä¸ªé‡åŒ–ä»£ç å¯¹åº”ä¸€ä¸ªçœŸå®æ•°æ®ç‚¹ã€‚</li>
<li>ä¼˜åŒ–è¿è¾“è·ç¦»ä»¥å¯¹é½ä»£ç æœ¬ä»£ç ä¸è®­ç»ƒæ•°æ®çš„ç‰¹å¾ï¼ŒåµŒå…¥å¤–éƒ¨çŸ¥è¯†å¹¶ä¸°å¯Œè¯­ä¹‰è¯æ±‡è¡¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b991e157764a9b779fa1b915f2277ca.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Strain-distribution-in-GaN-AlN-superlattices-grown-on-AlN-sapphire-templates-comparison-of-X-ray-diffraction-and-photoluminescence-studies"><a href="#Strain-distribution-in-GaN-AlN-superlattices-grown-on-AlN-sapphire-templates-comparison-of-X-ray-diffraction-and-photoluminescence-studies" class="headerlink" title="Strain distribution in GaN&#x2F;AlN superlattices grown on AlN&#x2F;sapphire   templates: comparison of X-ray diffraction and photoluminescence studies"></a>Strain distribution in GaN&#x2F;AlN superlattices grown on AlN&#x2F;sapphire   templates: comparison of X-ray diffraction and photoluminescence studies</h2><p><strong>Authors:Aleksandra Wierzbicka, Agata Kaminska, Kamil Sobczak, Dawid Jankowski, Kamil Koronski, Pawel Strak, Marta Sobanska, Zbigniew R. Zytkiewicz</strong></p>
<p>Series of GaN&#x2F;AlN superlattices (SLs) with various periods and the same thicknesses of GaN quantum wells and AlN barriers have been investigated. X-ray diffraction, photoluminescence (PL) and transmission electron microscopy (TEM) techniques were used to study the influence of thickness of AlN and GaN sublayers on strain distribution in GaN&#x2F;AlN SL structures. Detailed X-ray diffraction measurements demonstrate that the strain occurring in SLs generally decreases with an increase of well&#x2F;barrier thickness. Fitting of X-ray diffraction curves allowed determining the real thicknesses of the GaN wells and AlN barriers. Since blurring of the interfaces causes deviation of calculated data from experimental results the quality of the interfaces has been evaluated as well and compared with results of TEM measurements. For the samples with thinner wells&#x2F;barriers the presence of pin-holes and threading dislocations has been observed in TEM measurements. The best quality of interfaces has been found for the sample with a well&#x2F;barrier thickness of 3 nm. Finally, PL spectra showed that due to Quantum-Confined Stark Effect the PL peak energies of the SLs decreased with increasing the width of the GaN quantum wells and AlN barriers. The effect is well modelled by ab initio calculations based on the density functional theory applied for tetragonally strained structures of the same geometry using a full tensorial representation of the strain in the SLs. </p>
<blockquote>
<p>å·²ç»ç ”ç©¶äº†å…·æœ‰ä¸åŒå‘¨æœŸå’Œç›¸åŒGaNé‡å­é˜±å’ŒAlNåŠ¿å’åšåº¦çš„GaN&#x2F;AlNè¶…æ™¶æ ¼ï¼ˆSLsï¼‰ç³»åˆ—ã€‚ä½¿ç”¨Xå°„çº¿è¡å°„ã€å…‰è‡´å‘å…‰ï¼ˆPLï¼‰å’Œé€å°„ç”µå­æ˜¾å¾®é•œï¼ˆTEMï¼‰æŠ€æœ¯ï¼Œç ”ç©¶äº†AlNå’ŒGaNäºšå±‚åšåº¦å¯¹GaN&#x2F;AlN SLç»“æ„ä¸­åº”å˜åˆ†å¸ƒçš„å½±å“ã€‚è¯¦ç»†çš„Xå°„çº¿è¡å°„æµ‹é‡ç»“æœè¡¨æ˜ï¼ŒSLsä¸­çš„åº”å˜é€šå¸¸ä¼šéšç€é˜±&#x2F;åŠ¿å’åšåº¦çš„å¢åŠ è€Œå‡å°ã€‚é€šè¿‡æ‹ŸåˆXå°„çº¿è¡å°„æ›²çº¿ï¼Œå¯ä»¥ç¡®å®šGaNé˜±å’ŒAlNåŠ¿å’çš„å®é™…åšåº¦ã€‚ç”±äºç•Œé¢æ¨¡ç³Šå¯¼è‡´è®¡ç®—æ•°æ®ä¸å®éªŒç»“æœä¹‹é—´å­˜åœ¨åå·®ï¼Œå› æ­¤è¿˜å¯¹ç•Œé¢è´¨é‡è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸TEMæµ‹é‡ç»“æœè¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨è¾ƒè–„é˜±&#x2F;åŠ¿å’çš„æ ·å“ä¸­ï¼Œé€šè¿‡TEMæµ‹é‡è§‚å¯Ÿåˆ°å­˜åœ¨é’ˆå­”å’Œè´¯ç©¿ä½é”™ã€‚åœ¨é˜±&#x2F;åŠ¿å’åšåº¦ä¸º3nmçš„æ ·å“ä¸­å‘ç°ç•Œé¢è´¨é‡æœ€ä½³ã€‚æœ€åï¼ŒPLå…‰è°±æ˜¾ç¤ºï¼Œç”±äºé‡å­é™åˆ¶æ–¯å¡”å…‹æ•ˆåº”ï¼ŒSLsçš„PLå³°èƒ½é‡éšç€GaNé‡å­é˜±å’ŒAlNåŠ¿å’å®½åº¦çš„å¢åŠ è€Œé™ä½ã€‚è¯¥æ•ˆåº”å¯ä»¥é€šè¿‡åŸºäºå¯†åº¦æ³›å‡½ç†è®ºçš„ç¬¬ä¸€æ€§åŸç†è®¡ç®—è¿›è¡Œå¾ˆå¥½çš„æ¨¡æ‹Ÿï¼Œè¯¥è®¡ç®—é€‚ç”¨äºå…·æœ‰ç›¸åŒå‡ ä½•å½¢çŠ¶çš„å››è¾¹å½¢åº”å˜ç»“æ„ï¼Œå¹¶ä½¿ç”¨SLsä¸­çš„åº”å˜çš„å…¨å¼ é‡è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22294v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç ”ç©¶äº†ä¸åŒå‘¨æœŸçš„GaN&#x2F;AlNè¶…æ™¶æ ¼ï¼ˆSLsï¼‰ç»“æ„ï¼Œé‡‡ç”¨Xå°„çº¿è¡å°„ã€å…‰è‡´å‘å…‰å’Œé€å°„ç”µå­æ˜¾å¾®é•œæŠ€æœ¯ï¼Œæ¢è®¨äº†AlNå’ŒGaNäºšå±‚åšåº¦å¯¹GaN&#x2F;AlNè¶…æ™¶æ ¼ç»“æ„åº”å˜åˆ†å¸ƒçš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œéšç€é˜±&#x2F;åŠ¿å’åšåº¦çš„å¢åŠ ï¼Œè¶…æ™¶æ ¼ä¸­çš„åº”å˜ä¸€èˆ¬ä¼šå‡å°ã€‚é€šè¿‡Xå°„çº¿è¡å°„æ›²çº¿çš„æ‹Ÿåˆï¼Œç¡®å®šäº†GaNé˜±å’ŒAlNåŠ¿å’çš„å®é™…åšåº¦ã€‚åŒæ—¶ï¼Œå¯¹ç•Œé¢çš„è´¨é‡è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸é€å°„ç”µå­æ˜¾å¾®é•œçš„æµ‹é‡ç»“æœè¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨è¾ƒè–„çš„é˜±&#x2F;åŠ¿å’æ ·å“ä¸­ï¼Œé€å°„ç”µå­æ˜¾å¾®é•œè§‚å¯Ÿåˆ°å­˜åœ¨é’ˆå­”å’Œèºçº¹ä½é”™ã€‚åœ¨é˜±&#x2F;åŠ¿å’åšåº¦ä¸º3nmçš„æ ·å“ä¸­è·å¾—äº†æœ€ä½³ç•Œé¢è´¨é‡ã€‚æ­¤å¤–ï¼Œå…‰è‡´å‘å…‰å…‰è°±è¡¨æ˜ï¼Œç”±äºé‡å­é™åˆ¶æ–¯å¡”å…‹æ•ˆåº”ï¼ŒSLsçš„å‘å…‰å³°èƒ½é‡éšç€GaNé‡å­é˜±å’ŒAlNåŠ¿å’å®½åº¦çš„å¢åŠ è€Œé™ä½ã€‚è¿™ä¸€æ•ˆåº”é€šè¿‡åŸºäºå¯†åº¦æ³›å‡½ç†è®ºçš„ç¬¬ä¸€æ€§åŸç†è®¡ç®—å¾—åˆ°äº†å¾ˆå¥½çš„æ¨¡æ‹Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaN&#x2F;AlNè¶…æ™¶æ ¼ç³»åˆ—çš„å‘¨æœŸå’Œé‡å­é˜±åšåº¦å¯¹æ¯”ç ”ç©¶è¡¨æ˜ï¼Œåº”å˜éšé˜±&#x2F;åŠ¿å’åšåº¦çš„å¢åŠ è€Œå‡å°ã€‚</li>
<li>é€šè¿‡Xå°„çº¿è¡å°„æµ‹é‡ç¡®å®šäº†GaNé˜±å’ŒAlNåŠ¿å’çš„å®é™…åšåº¦ã€‚</li>
<li>ç•Œé¢è´¨é‡çš„è¯„ä¼°å‘ç°ï¼Œè¾ƒè–„é˜±&#x2F;åŠ¿å’çš„æ ·å“ä¸­å­˜åœ¨é’ˆå­”å’Œèºçº¹ä½é”™ã€‚</li>
<li>æœ€ä½³ç•Œé¢è´¨é‡åœ¨é˜±&#x2F;åŠ¿å’åšåº¦ä¸º3nmçš„æ ·å“ä¸­å‘ç°ã€‚</li>
<li>å…‰è‡´å‘å…‰å…‰è°±æ­ç¤ºäº†é‡å­é™åˆ¶æ–¯å¡”å…‹æ•ˆåº”åœ¨è¶…æ™¶æ ¼ä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
<li>éšç€GaNé‡å­é˜±å’ŒAlNåŠ¿å’å®½åº¦çš„å¢åŠ ï¼Œå‘å…‰å³°èƒ½é‡é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-17d727948c34a80e619067de4b3d016b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8583095db588e171c5422232ba83dc1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ABC-GS-Alignment-Based-Controllable-Style-Transfer-for-3D-Gaussian-Splatting"><a href="#ABC-GS-Alignment-Based-Controllable-Style-Transfer-for-3D-Gaussian-Splatting" class="headerlink" title="ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian   Splatting"></a>ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian   Splatting</h2><p><strong>Authors:Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li</strong></p>
<p>3D scene stylization approaches based on Neural Radiance Fields (NeRF) achieve promising results by optimizing with Nearest Neighbor Feature Matching (NNFM) loss. However, NNFM loss does not consider global style information. In addition, the implicit representation of NeRF limits their fine-grained control over the resulting scenes. In this paper, we introduce ABC-GS, a novel framework based on 3D Gaussian Splatting to achieve high-quality 3D style transfer. To this end, a controllable matching stage is designed to achieve precise alignment between scene content and style features through segmentation masks. Moreover, a style transfer loss function based on feature alignment is proposed to ensure that the outcomes of style transfer accurately reflect the global style of the reference image. Furthermore, the original geometric information of the scene is preserved with the depth loss and Gaussian regularization terms. Extensive experiments show that our ABC-GS provides controllability of style transfer and achieves stylization results that are more faithfully aligned with the global style of the chosen artistic reference. Our homepage is available at <a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/ABC-GS-website">https://vpx-ecnu.github.io/ABC-GS-website</a>. </p>
<blockquote>
<p>åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„3Dåœºæ™¯é£æ ¼åŒ–æ–¹æ³•é€šè¿‡æœ€è¿‘é‚»ç‰¹å¾åŒ¹é…ï¼ˆNNFMï¼‰æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼ŒNNFMæŸå¤±å¹¶æ²¡æœ‰è€ƒè™‘å…¨å±€é£æ ¼ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒNeRFçš„éšå¼è¡¨ç¤ºé™åˆ¶äº†å…¶å¯¹ç»“æœåœºæ™¯çš„ç»†ç²’åº¦æ§åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ABC-GSï¼Œä¸€ä¸ªåŸºäº3Dé«˜æ–¯å–·æ¶‚çš„æ–°å‹æ¡†æ¶ï¼Œä»¥å®ç°é«˜è´¨é‡çš„ä¸‰ç»´é£æ ¼è½¬ç§»ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¯æ§çš„åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡åˆ†å‰²æ©è†œå®ç°åœºæ™¯å†…å®¹ä¸é£æ ¼ç‰¹å¾çš„ç²¾ç¡®å¯¹é½ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾å¯¹é½çš„é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°ï¼Œä»¥ç¡®ä¿é£æ ¼è½¬ç§»çš„ç»“æœå‡†ç¡®åæ˜ å‚è€ƒå›¾åƒçš„å…¨å±€é£æ ¼ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ·±åº¦æŸå¤±å’Œé«˜æ–¯æ­£åˆ™åŒ–é¡¹ä¿ç•™äº†åœºæ™¯çš„åŸå‡ ä½•ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ABC-GSæä¾›äº†é£æ ¼è½¬ç§»çš„æ§åˆ¶èƒ½åŠ›ï¼Œå¹¶å®ç°äº†ä¸æ‰€é€‰è‰ºæœ¯å‚è€ƒçš„å…¨å±€é£æ ¼æ›´å¿ å®å¯¹é½çš„é£æ ¼åŒ–ç»“æœã€‚æˆ‘ä»¬çš„ä¸»é¡µå¯åœ¨<a target="_blank" rel="noopener" href="https://vpx-ecnu.github.io/ABC-GS-website%E8%AE%BF%E9%97%AE%E3%80%82">https://vpx-ecnu.github.io/ABC-GS-websiteè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22218v1">PDF</a> 10 pages, 14 figures</p>
<p><strong>Summary</strong><br>åŸºäºNeRFçš„3Dåœºæ™¯é£æ ¼åŒ–æ–¹æ³•é€šè¿‡é‡‡ç”¨æœ€è¿‘é‚»ç‰¹å¾åŒ¹é…ï¼ˆNNFMï¼‰æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ä½†NNFMæŸå¤±æ²¡æœ‰è€ƒè™‘å…¨å±€é£æ ¼ä¿¡æ¯ï¼Œä¸”NeRFçš„éšå¼è¡¨ç¤ºé™åˆ¶äº†åœºæ™¯çš„ç²¾ç»†æ§åˆ¶ã€‚æœ¬æ–‡æå‡ºABC-GSæ¡†æ¶ï¼ŒåŸºäº3Dé«˜æ–¯æ‹¼è´´å®ç°é«˜è´¨é‡3Dé£æ ¼è½¬ç§»ã€‚è®¾è®¡å¯æ§åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡åˆ†å‰²æ©è†œå®ç°åœºæ™¯å†…å®¹ä¸é£æ ¼ç‰¹å¾çš„ç²¾ç¡®å¯¹é½ã€‚æå‡ºåŸºäºç‰¹å¾å¯¹é½çš„é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°ï¼Œç¡®ä¿é£æ ¼è½¬ç§»ç»“æœå‡†ç¡®åæ˜ å‚è€ƒå›¾åƒçš„å…¨å±€é£æ ¼ã€‚åŒæ—¶ä¿ç•™åœºæ™¯çš„åŸå§‹å‡ ä½•ä¿¡æ¯ï¼Œé€šè¿‡æ·±åº¦æŸå¤±å’Œé«˜æ–¯æ­£åˆ™åŒ–é¡¹å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dåœºæ™¯é£æ ¼åŒ–æ–¹æ³•åŸºäºNeRFå’ŒNNFMæŸå¤±ä¼˜åŒ–å–å¾—è¿›å±•ã€‚</li>
<li>NNFMæŸå¤±ä¸è€ƒè™‘å…¨å±€é£æ ¼ä¿¡æ¯ï¼ŒNeRFçš„éšå¼è¡¨ç¤ºé™åˆ¶äº†ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>ABC-GSæ¡†æ¶å®ç°é«˜è´¨é‡3Dé£æ ¼è½¬ç§»ï¼ŒåŸºäº3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯ã€‚</li>
<li>å¼•å…¥å¯æ§åŒ¹é…é˜¶æ®µï¼Œé€šè¿‡åˆ†å‰²æ©è†œç²¾ç¡®å¯¹é½åœºæ™¯å†…å®¹ä¸é£æ ¼ç‰¹å¾ã€‚</li>
<li>æå‡ºåŸºäºç‰¹å¾å¯¹é½çš„é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°ï¼Œç¡®ä¿åæ˜ å…¨å±€é£æ ¼ã€‚</li>
<li>ä¿ç•™åŸå§‹åœºæ™¯çš„å‡ ä½•ä¿¡æ¯ï¼Œé€šè¿‡æ·±åº¦æŸå¤±å’Œé«˜æ–¯æ­£åˆ™åŒ–é¡¹å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c150b593acd3ad8af8bbbc67d19774d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f0a34dcf472a776172956adfe1b8f8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80b6c3426d1ed93e72af69a2318f832.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f89d45b2378711221b582477857f78d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fc0b4d14604ddcbec4769021841d33.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting"><a href="#RainyGS-Efficient-Rain-Synthesis-with-Physically-Based-Gaussian-Splatting" class="headerlink" title="RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting"></a>RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian   Splatting</h2><p><strong>Authors:Qiyu Dai, Xingyu Ni, Qianfan Shen, Wenzheng Chen, Baoquan Chen, Mengyu Chu</strong></p>
<p>We consider the problem of adding dynamic rain effects to in-the-wild scenes in a physically-correct manner. Recent advances in scene modeling have made significant progress, with NeRF and 3DGS techniques emerging as powerful tools for reconstructing complex scenes. However, while effective for novel view synthesis, these methods typically struggle with challenging scene editing tasks, such as physics-based rain simulation. In contrast, traditional physics-based simulations can generate realistic rain effects, such as raindrops and splashes, but they often rely on skilled artists to carefully set up high-fidelity scenes. This process lacks flexibility and scalability, limiting its applicability to broader, open-world environments. In this work, we introduce RainyGS, a novel approach that leverages the strengths of both physics-based modeling and 3DGS to generate photorealistic, dynamic rain effects in open-world scenes with physical accuracy. At the core of our method is the integration of physically-based raindrop and shallow water simulation techniques within the fast 3DGS rendering framework, enabling realistic and efficient simulations of raindrop behavior, splashes, and reflections. Our method supports synthesizing rain effects at over 30 fps, offering users flexible control over rain intensity â€“ from light drizzles to heavy downpours. We demonstrate that RainyGS performs effectively for both real-world outdoor scenes and large-scale driving scenarios, delivering more photorealistic and physically-accurate rain effects compared to state-of-the-art methods. Project page can be found at <a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/">https://pku-vcl-geometry.github.io/RainyGS/</a> </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘ä»¥ç‰©ç†æ­£ç¡®çš„æ–¹å¼ç»™è‡ªç„¶åœºæ™¯æ·»åŠ åŠ¨æ€é›¨æ°´æ•ˆæœçš„é—®é¢˜ã€‚æœ€è¿‘åœºæ™¯å»ºæ¨¡æ–¹é¢çš„è¿›å±•å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼ŒNeRFå’Œ3DGSæŠ€æœ¯ä½œä¸ºé‡å»ºå¤æ‚åœºæ™¯çš„å¼ºå¤§å·¥å…·è€Œå´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œè™½ç„¶è¿™äº›æ–¹æ³•åœ¨åˆæˆæ–°è§†è§’æ–¹é¢å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ç¼–è¾‘ä»»åŠ¡ï¼ˆå¦‚åŸºäºç‰©ç†çš„é›¨æ°´æ¨¡æ‹Ÿï¼‰æ—¶é‡åˆ°å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå¯ä»¥äº§ç”Ÿé€¼çœŸçš„é›¨æ°´æ•ˆæœï¼Œå¦‚é›¨æ»´å’Œé£æº…çš„æ°´èŠ±ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç†Ÿç»ƒçš„è‰ºæœ¯å®¶æ¥ä»”ç»†è®¾ç½®é«˜ä¿çœŸåœºæ™¯ã€‚è¿™ä¸ªè¿‡ç¨‹ç¼ºä¹çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨æ›´å¹¿æ³›ã€å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RainyGSï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œ3DGSçš„ä¼˜ç‚¹ï¼Œä»¥åœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰ç‰©ç†å‡†ç¡®æ€§çš„é€¼çœŸåŠ¨æ€é›¨æ°´æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åœ¨å¿«é€Ÿçš„3DGSæ¸²æŸ“æ¡†æ¶å†…æ•´åˆåŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°é›¨æ»´è¡Œä¸ºã€é£æº…å’Œæ°´é¢åå°„çš„é€¼çœŸå’Œé«˜æ•ˆæ¨¡æ‹Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒä»¥è¶…è¿‡30å¸§&#x2F;ç§’çš„é€Ÿåº¦åˆæˆé›¨æ°´æ•ˆæœï¼Œè®©ç”¨æˆ·å¯ä»¥çµæ´»æ§åˆ¶é›¨æ°´çš„å¼ºåº¦ï¼Œä»è½»å¾®çš„ç»†é›¨åˆ°å€¾ç›†å¤§é›¨ã€‚æˆ‘ä»¬è¯æ˜äº†RainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´é€¼çœŸå’Œæ›´ç‰©ç†å‡†ç¡®çš„é›¨æ°´æ•ˆæœã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://pku-vcl-geometry.github.io/RainyGS/%E6%89%BE%E5%88%B0%E3%80%82">https://pku-vcl-geometry.github.io/RainyGS/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21442v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶åœ¨å¤æ‚åœºæ™¯ä¸­æ·»åŠ åŠ¨æ€é›¨æ•ˆçš„éš¾é¢˜ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§ç»“åˆç‰©ç†å»ºæ¨¡å’Œ3DGSæŠ€æœ¯çš„æ–°å‹æ–¹æ³•RainyGSã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰ç‰©ç†å‡†ç¡®æ€§çš„é«˜é€¼çœŸåº¦åŠ¨æ€é›¨æ•ˆï¼Œèåˆäº†åŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œåœ¨å¿«é€Ÿ3DGSæ¸²æŸ“æ¡†æ¶å†…å®ç°äº†å¯¹é›¨æ»´è¡Œä¸ºã€æº…èµ·å’Œåå°„çš„é€¼çœŸä¸”é«˜æ•ˆçš„æ¨¡æ‹Ÿã€‚ç”¨æˆ·å¯çµæ´»æ§åˆ¶é›¨åŠ¿å¼ºåº¦ï¼Œå¹¶æ”¯æŒåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­çš„æœ‰æ•ˆåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡è€ƒè™‘åœ¨é‡å¤–åœºæ™¯ä¸­å¢åŠ å…·æœ‰ç‰©ç†æ­£ç¡®æ€§çš„åŠ¨æ€é›¨æ•ˆé—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†RainyGSæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç‰©ç†å»ºæ¨¡å’Œ3DGSæŠ€æœ¯æ¥ç”Ÿæˆé€¼çœŸçš„åŠ¨æ€é›¨æ•ˆã€‚</li>
<li>RainyGSæ–¹æ³•èƒ½å¤Ÿåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰ç‰©ç†å‡†ç¡®æ€§çš„é«˜é€¼çœŸåº¦é›¨æ•ˆã€‚</li>
<li>è¯¥æ–¹æ³•èåˆäº†åŸºäºç‰©ç†çš„é›¨æ»´å’Œæµ…æ°´æ¨¡æ‹ŸæŠ€æœ¯ï¼Œåœ¨å¿«é€Ÿ3DGSæ¸²æŸ“æ¡†æ¶å†…å®ç°äº†å¯¹é›¨æ»´è¡Œä¸ºç­‰çš„é€¼çœŸæ¨¡æ‹Ÿã€‚</li>
<li>ç”¨æˆ·å¯ä»¥çµæ´»æ§åˆ¶é›¨åŠ¿å¼ºåº¦ï¼Œå®ç°ä»ç»†é›¨åˆ°å¤§é›¨çš„æ¨¡æ‹Ÿã€‚</li>
<li>RainyGSåœ¨çœŸå®æˆ·å¤–åœºæ™¯å’Œå¤§è§„æ¨¡é©¾é©¶åœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœè‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c3b19f41ae58db25446cd92dae747c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d92736acb1d72a0482a1e4739ae0fadb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2540fe3d87f579b141ef35602ce18620.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LandMarkSystem-Technical-Report"><a href="#LandMarkSystem-Technical-Report" class="headerlink" title="LandMarkSystem Technical Report"></a>LandMarkSystem Technical Report</h2><p><strong>Authors:Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, Rong Fu, Hengjie Li</strong></p>
<p>3D reconstruction is vital for applications in autonomous driving, virtual reality, augmented reality, and the metaverse. Recent advancements such as Neural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformed the field, yet traditional deep learning frameworks struggle to meet the increasing demands for scene quality and scale. This paper introduces LandMarkSystem, a novel computing framework designed to enhance multi-scale scene reconstruction and rendering. By leveraging a componentized model adaptation layer, LandMarkSystem supports various NeRF and 3DGS structures while optimizing computational efficiency through distributed parallel computing and model parameter offloading. Our system addresses the limitations of existing frameworks, providing dedicated operators for complex 3D sparse computations, thus facilitating efficient training and rapid inference over extensive scenes. Key contributions include a modular architecture, a dynamic loading strategy for limited resources, and proven capabilities across multiple representative algorithms.This comprehensive solution aims to advance the efficiency and effectiveness of 3D reconstruction tasks.To facilitate further research and collaboration, the source code and documentation for the LandMarkSystem project are publicly available in an open-source repository, accessing the repository at: <a target="_blank" rel="noopener" href="https://github.com/InternLandMark/LandMarkSystem">https://github.com/InternLandMark/LandMarkSystem</a>. </p>
<blockquote>
<p>ä¸‰ç»´é‡å»ºåœ¨è‡ªåŠ¨é©¾é©¶ã€è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œå…ƒå®‡å®™ç­‰åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ï¼Œå·²ç»æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¡†æ¶éš¾ä»¥æ»¡è¶³æ—¥ç›Šå¢é•¿çš„åœºæ™¯è´¨é‡å’Œè§„æ¨¡çš„éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†LandMarkSystemï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šå°ºåº¦åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ã€‚é€šè¿‡åˆ©ç”¨ç»„ä»¶åŒ–æ¨¡å‹é€‚é…å±‚ï¼ŒLandMarkSystemæ”¯æŒå„ç§NeRFå’Œ3DGSç»“æ„ï¼ŒåŒæ—¶é€šè¿‡åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—å’Œæ¨¡å‹å‚æ•°å¸è½½ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè§£å†³äº†ç°æœ‰æ¡†æ¶çš„å±€é™æ€§ï¼Œä¸ºå¤æ‚çš„ä¸‰ç»´ç¨€ç–è®¡ç®—æä¾›äº†ä¸“ç”¨æ“ä½œç¬¦ï¼Œä»è€Œå®ç°äº†å¤§è§„æ¨¡åœºæ™¯çš„å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬æ¨¡å—åŒ–æ¶æ„ã€æœ‰é™èµ„æºçš„åŠ¨æ€åŠ è½½ç­–ç•¥ä»¥åŠåœ¨å¤šç§ä»£è¡¨æ€§ç®—æ³•ä¸­çš„èƒ½åŠ›éªŒè¯ã€‚è¯¥å…¨é¢è§£å†³æ–¹æ¡ˆæ—¨åœ¨æé«˜ä¸‰ç»´é‡å»ºä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚ä¸ºäº†æ–¹ä¾¿è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåˆä½œï¼ŒLandMarkSystemé¡¹ç›®çš„æºä»£ç å’Œæ–‡æ¡£å·²åœ¨å¼€æºä»“åº“ä¸­å…¬å¼€å¯ç”¨ï¼Œå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/InternLandMark/LandMarkSystem%E3%80%82">https://github.com/InternLandMark/LandMarkSystemã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21364v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£ä¸‰ç»´é‡å»ºè®¡ç®—æ¡†æ¶LandMarkSystemç ”ç©¶æå‡ºã€‚é‡‡ç”¨ç»„ä»¶åŒ–æ¨¡å‹è‡ªé€‚åº”å±‚æŠ€æœ¯ï¼Œç»“åˆNeRFä¸å¤šç§ä¼ ç»Ÿæ¡†æ¶çš„ä¼˜ç‚¹æå‡åœºæ™¯æ¸²æŸ“æ•ˆæœä¸æ•ˆç‡ã€‚æ”¯æŒåˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—ä¸æ¨¡å‹å‚æ•°å¸è½½ï¼Œå®ç°æ¨¡å—åŒ–æ¶æ„ä¸åŠ¨æ€åŠ è½½ç­–ç•¥ï¼Œé¢å‘å¤§è§„æ¨¡åœºæ™¯æä¾›é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥é¡¹ç›®å…¬å¼€æºä»£ç ä¸æ–‡æ¡£ï¼Œä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶å’Œåˆä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LandMarkSystemæ˜¯ä¸€ä¸ªæ–°å‹ä¸‰ç»´é‡å»ºè®¡ç®—æ¡†æ¶ï¼Œåˆ©ç”¨NeRFå’Œ3DGSæŠ€æœ¯çš„ä¼˜ç‚¹å¢å¼ºå¤šå°ºåº¦åœºæ™¯é‡å»ºå’Œæ¸²æŸ“ã€‚</li>
<li>å¼•å…¥ç»„ä»¶åŒ–æ¨¡å‹è‡ªé€‚åº”å±‚æŠ€æœ¯ï¼Œä¼˜åŒ–è®¡ç®—æ•ˆç‡å¹¶æ”¯æŒå¤šç§NeRFå’Œ3DGSç»“æ„ã€‚</li>
<li>é€šè¿‡åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—å’Œæ¨¡å‹å‚æ•°å¸è½½è§£å†³ç°æœ‰æ¡†æ¶çš„é™åˆ¶ã€‚</li>
<li>å®ç°æ¨¡å—åŒ–æ¶æ„å’ŒåŠ¨æ€åŠ è½½ç­–ç•¥ï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„åœºæ™¯ã€‚</li>
<li>åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šæä¾›é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¡¹ç›®å…¬å¼€æºä»£ç å’Œæ–‡æ¡£ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåˆä½œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21364">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cab89666ecae86575af634424164eee.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UGNA-VPR-A-Novel-Training-Paradigm-for-Visual-Place-Recognition-Based-on-Uncertainty-Guided-NeRF-Augmentation"><a href="#UGNA-VPR-A-Novel-Training-Paradigm-for-Visual-Place-Recognition-Based-on-Uncertainty-Guided-NeRF-Augmentation" class="headerlink" title="UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based   on Uncertainty-Guided NeRF Augmentation"></a>UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based   on Uncertainty-Guided NeRF Augmentation</h2><p><strong>Authors:Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, Xieyuanli Chen</strong></p>
<p>Visual place recognition (VPR) is crucial for robots to identify previously visited locations, playing an important role in autonomous navigation in both indoor and outdoor environments. However, most existing VPR datasets are limited to single-viewpoint scenarios, leading to reduced recognition accuracy, particularly in multi-directional driving or feature-sparse scenes. Moreover, obtaining additional data to mitigate these limitations is often expensive. This paper introduces a novel training paradigm to improve the performance of existing VPR networks by enhancing multi-view diversity within current datasets through uncertainty estimation and NeRF-based data augmentation. Specifically, we initially train NeRF using the existing VPR dataset. Then, our devised self-supervised uncertainty estimation network identifies places with high uncertainty. The poses of these uncertain places are input into NeRF to generate new synthetic observations for further training of VPR networks. Additionally, we propose an improved storage method for efficient organization of augmented and original training data. We conducted extensive experiments on three datasets and tested three different VPR backbone networks. The results demonstrate that our proposed training paradigm significantly improves VPR performance by fully utilizing existing data, outperforming other training approaches. We further validated the effectiveness of our approach on self-recorded indoor and outdoor datasets, consistently demonstrating superior results. Our dataset and code have been released at \href{<a target="_blank" rel="noopener" href="https://github.com/nubot-nudt/UGNA-VPR%7D%7Bhttps://github.com/nubot-nudt/UGNA-VPR%7D">https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}</a>. </p>
<blockquote>
<p>è§†è§‰å®šä½ï¼ˆVPRï¼‰å¯¹äºæœºå™¨äººè¯†åˆ«å…ˆå‰è®¿é—®è¿‡çš„ä½ç½®è‡³å…³é‡è¦ï¼Œåœ¨å®¤å†…å’Œå®¤å¤–ç¯å¢ƒçš„è‡ªä¸»å¯¼èˆªä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„VPRæ•°æ®é›†ä»…é™äºå•ä¸€è§†ç‚¹çš„åœºæ™¯ï¼Œå¯¼è‡´è¯†åˆ«ç²¾åº¦é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ–¹å‘é©¾é©¶æˆ–ç‰¹å¾ç¨€ç–çš„åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œè·å–é¢å¤–çš„æ•°æ®æ¥ç¼“è§£è¿™äº›é™åˆ¶é€šå¸¸æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡å’ŒåŸºäºNeRFçš„æ•°æ®å¢å¼ºæ¥æé«˜ç°æœ‰VPRç½‘ç»œæ€§èƒ½ï¼Œå¢å¼ºå½“å‰æ•°æ®é›†å†…çš„å¤šè§†å›¾å¤šæ ·æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç°æœ‰çš„VPRæ•°æ®é›†è®­ç»ƒNeRFã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡çš„è‡ªç›‘ç£ä¸ç¡®å®šæ€§ä¼°è®¡ç½‘ç»œä¼šè¯†åˆ«å‡ºä¸ç¡®å®šæ€§è¾ƒé«˜çš„ä½ç½®ã€‚è¿™äº›ä¸ç¡®å®šä½ç½®çš„å§¿åŠ¿è¢«è¾“å…¥åˆ°NeRFä¸­ï¼Œä»¥ç”Ÿæˆæ–°çš„åˆæˆè§‚å¯Ÿç»“æœï¼Œç”¨äºè¿›ä¸€æ­¥è®­ç»ƒVPRç½‘ç»œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ•°æ®å­˜å‚¨æ–¹æ³•ï¼Œç”¨äºæœ‰æ•ˆåœ°ç»„ç»‡å¢å¼ºå’ŒåŸå§‹è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶æµ‹è¯•äº†ä¸‰ç§ä¸åŒçš„VPRä¸»å¹²ç½‘ç»œã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„è®­ç»ƒèŒƒå¼å……åˆ†åˆ©ç”¨äº†ç°æœ‰æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†VPRæ€§èƒ½ï¼Œè¶…è¿‡äº†å…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬åœ¨è‡ªå·±æ”¶é›†çš„å®¤å†…å’Œå®¤å¤–æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå§‹ç»ˆè¡¨ç°å‡ºä¼˜è¶Šçš„ç»“æœã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/nubot-nudt/UGNA-VPR">https://github.com/nubot-nudt/UGNA-VPR</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21338v1">PDF</a> Accepted to IEEE Robotics and Automation Letters (RA-L)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡å’ŒåŸºäºNeRFçš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¢å¼ºç°æœ‰VPRç½‘ç»œçš„å¤šè§†è§’å¤šæ ·æ€§ï¼Œæé«˜å…¶åœ¨å¤šæ–¹å‘é©¾é©¶å’Œç‰¹å¾ç¨€ç–åœºæ™¯ä¸­çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚é€šè¿‡è®­ç»ƒNeRFä½¿ç”¨ç°æœ‰VPRæ•°æ®é›†ï¼Œå¼•å…¥è‡ªæˆ‘ç›‘ç£çš„ä¸ç¡®å®šæ€§ä¼°è®¡ç½‘ç»œæ¥è¯†åˆ«é«˜ä¸ç¡®å®šæ€§åœ°ç‚¹ï¼Œå¹¶å°†å…¶å§¿æ€è¾“å…¥NeRFç”Ÿæˆæ–°çš„åˆæˆè§‚æµ‹æ•°æ®ï¼Œç”¨äºè¿›ä¸€æ­¥è®­ç»ƒVPRç½‘ç»œã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›å­˜å‚¨æ–¹æ³•ï¼Œä»¥ä¾¿æ›´æœ‰æ•ˆåœ°ç»„ç»‡åŸå§‹å’Œå¢å¼ºè®­ç»ƒæ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å……åˆ†åˆ©ç”¨ç°æœ‰æ•°æ®ï¼Œæ˜¾è‘—æé«˜VPRæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨æé«˜è§†è§‰åœºæ‰€è¯†åˆ«ï¼ˆVPRï¼‰ç½‘ç»œçš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡å’ŒNeRFæŠ€æœ¯å¢å¼ºå¤šè§†è§’å¤šæ ·æ€§ï¼Œå…‹æœç°æœ‰VPRæ•°æ®é›†çš„å•è§†è§’å±€é™ã€‚</li>
<li>åˆ©ç”¨è‡ªæˆ‘ç›‘ç£çš„ä¸ç¡®å®šæ€§ä¼°è®¡ç½‘ç»œè¯†åˆ«é«˜ä¸ç¡®å®šæ€§åœ°ç‚¹ï¼Œå¹¶é€šè¿‡NeRFç”Ÿæˆæ–°çš„åˆæˆè§‚æµ‹æ•°æ®ã€‚</li>
<li>æå‡ºä¸€ç§æ”¹è¿›å­˜å‚¨æ–¹æ³•ï¼Œæœ‰æ•ˆç»„ç»‡åŸå§‹å’Œå¢å¼ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæµ‹è¯•ä¸‰ç§ä¸åŒçš„VPRéª¨å¹²ç½‘ç»œï¼Œè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†VPRæ€§èƒ½ã€‚</li>
<li>åœ¨è‡ªå½•çš„å®¤å†…å’Œå®¤å¤–æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå‡å–å¾—ä¼˜è¶Šç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d64fae156815e3a3aeec368229be729.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d535b9c8da77d7048c470fa9913f5b84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45e66ba7990fba5569f0f1451df6dfa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bad45da56d9377a5101018b25da7ffb3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction"><a href="#NeRFPrior-Learning-Neural-Radiance-Field-as-a-Prior-for-Indoor-Scene-Reconstruction" class="headerlink" title="NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction"></a>NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene   Reconstruction</h2><p><strong>Authors:Wenyuan Zhang, Emily Yue-ting Jia, Junsheng Zhou, Baorui Ma, Kanle Shi, Yu-Shen Liu, Zhizhong Han</strong></p>
<p>Recently, it has shown that priors are vital for neural implicit functions to reconstruct high-quality surfaces from multi-view RGB images. However, current priors require large-scale pre-training, and merely provide geometric clues without considering the importance of color. In this paper, we present NeRFPrior, which adopts a neural radiance field as a prior to learn signed distance fields using volume rendering for surface reconstruction. Our NeRF prior can provide both geometric and color clues, and also get trained fast under the same scene without additional data. Based on the NeRF prior, we are enabled to learn a signed distance function (SDF) by explicitly imposing a multi-view consistency constraint on each ray intersection for surface inference. Specifically, at each ray intersection, we use the density in the prior as a coarse geometry estimation, while using the color near the surface as a clue to check its visibility from another view angle. For the textureless areas where the multi-view consistency constraint does not work well, we further introduce a depth consistency loss with confidence weights to infer the SDF. Our experimental results outperform the state-of-the-art methods under the widely used benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç ”ç©¶è¡¨æ˜å…ˆéªŒçŸ¥è¯†å¯¹äºç¥ç»éšå¼å‡½æ•°ä»å¤šè§†è§’RGBå›¾åƒé‡å»ºé«˜è´¨é‡è¡¨é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆéªŒçŸ¥è¯†éœ€è¦å¤§é‡é¢„è®­ç»ƒï¼Œå¹¶ä¸”ä»…æä¾›å‡ ä½•çº¿ç´¢ï¼Œè€Œæœªè€ƒè™‘é¢œè‰²çš„é‡è¦æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NeRFPriorï¼Œå®ƒé‡‡ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒçŸ¥è¯†ï¼Œä½¿ç”¨ä½“ç§¯æ¸²æŸ“æ¥å­¦ä¹ æœ‰å‘è·ç¦»åœºè¿›è¡Œè¡¨é¢é‡å»ºã€‚æˆ‘ä»¬çš„NeRFå…ˆéªŒå¯ä»¥æä¾›å‡ ä½•å’Œé¢œè‰²çº¿ç´¢ï¼Œå¹¶ä¸”åœ¨åŒä¸€åœºæ™¯ä¸‹æ— éœ€é¢å¤–æ•°æ®å³å¯å¿«é€Ÿè¿›è¡Œè®­ç»ƒã€‚åŸºäºNeRFå…ˆéªŒï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡åœ¨æ¯æ¡å°„çº¿äº¤ç‚¹å¤„æ˜¾å¼æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ¥å­¦ä¹ æœ‰å‘è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰è¿›è¡Œè¡¨é¢æ¨æ–­ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯æ¡å°„çº¿äº¤ç‚¹å¤„ï¼Œæˆ‘ä»¬ä½¿ç”¨å…ˆéªŒä¸­çš„å¯†åº¦ä½œä¸ºç²—ç•¥çš„å‡ ä½•ä¼°è®¡ï¼ŒåŒæ—¶ä½¿ç”¨æ¥è¿‘è¡¨é¢çš„é¢œè‰²ä½œä¸ºä»å¦ä¸€ä¸ªè§†è§’æ£€æŸ¥å…¶å¯è§æ€§çš„çº¿ç´¢ã€‚å¯¹äºçº¹ç†ç¼ºå¤±åŒºåŸŸï¼Œå¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ— æ³•è‰¯å¥½å·¥ä½œï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†å¸¦æœ‰ç½®ä¿¡æƒé‡çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¶…è¿‡äº†åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18361v2">PDF</a> Accepted by CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://wen-yuan-zhang.github.io/NeRFPrior/">https://wen-yuan-zhang.github.io/NeRFPrior/</a></p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†NeRFPriorï¼Œä½¿ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒæ¥å­¦ä¹ ç¬¦å·è·ç¦»åœºï¼Œé€šè¿‡ä½“ç§¯æ¸²æŸ“è¿›è¡Œè¡¨é¢é‡å»ºã€‚NeRFPriorä¸ä»…èƒ½æä¾›å‡ ä½•å’Œé¢œè‰²çº¿ç´¢ï¼Œè¿˜åœ¨åŒä¸€åœºæ™¯ä¸‹æ— éœ€é¢å¤–æ•°æ®å³å¯å¿«é€Ÿè®­ç»ƒã€‚é€šè¿‡æ˜ç¡®å¯¹æ¯æ¡å°„çº¿äº¤ç‚¹æ–½åŠ å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸï¼Œå®ç°è¡¨é¢æ¨æ–­å­¦ä¹ ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰ã€‚åœ¨çº¹ç†ç¼ºå¤±åŒºåŸŸï¼Œå¼•å…¥å¸¦æœ‰ç½®ä¿¡æƒé‡çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFï¼Œå®éªŒç»“æœä¼˜äºç°æœ‰å‰æ²¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFPriorä½¿ç”¨ç¥ç»è¾å°„åœºä½œä¸ºå…ˆéªŒï¼Œèƒ½åŒæ—¶æä¾›å‡ ä½•å’Œé¢œè‰²çº¿ç´¢ã€‚</li>
<li>æ— éœ€å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œé¢å¤–æ•°æ®ï¼ŒNeRFPriorèƒ½å¿«é€Ÿé€‚åº”åŒä¸€åœºæ™¯ã€‚</li>
<li>é€šè¿‡ä½“ç§¯æ¸²æŸ“å’Œç¬¦å·è·ç¦»åœºå­¦ä¹ è¡¨é¢é‡å»ºã€‚</li>
<li>å¼•å…¥å¤šè§†è§’ä¸€è‡´æ€§çº¦æŸæ¥å­¦ä¹ ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰ã€‚</li>
<li>åœ¨çº¹ç†ç¼ºå¤±åŒºåŸŸï¼Œä½¿ç”¨æ·±åº¦ä¸€è‡´æ€§æŸå¤±æ¥æ¨æ–­SDFã€‚</li>
<li>NeRFPriorçš„å®éªŒç»“æœä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dced1b075b30f7831c4352987fd1efeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-329c32e673c33ffb395103bf6fad45f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529d9fe25e668a950d63460ca52e07fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a736b9c84b1f2095dd212bde8fde2f75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2894203a097369e7df754db7fa9dca19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22dcbf7ed51c78afe9d29df43db16aba.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Scalable-Real2Sim-Physics-Aware-Asset-Generation-Via-Robotic-Pick-and-Place-Setups"><a href="#Scalable-Real2Sim-Physics-Aware-Asset-Generation-Via-Robotic-Pick-and-Place-Setups" class="headerlink" title="Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic   Pick-and-Place Setups"></a>Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic   Pick-and-Place Setups</h2><p><strong>Authors:Nicholas Pfaff, Evelyn Fu, Jeremy Binagia, Phillip Isola, Russ Tedrake</strong></p>
<p>Simulating object dynamics from real-world perception shows great promise for digital twins and robotic manipulation but often demands labor-intensive measurements and expertise. We present a fully automated Real2Sim pipeline that generates simulation-ready assets for real-world objects through robotic interaction. Using only a robotâ€™s joint torque sensors and an external camera, the pipeline identifies visual geometry, collision geometry, and physical properties such as inertial parameters. Our approach introduces a general method for extracting high-quality, object-centric meshes from photometric reconstruction techniques (e.g., NeRF, Gaussian Splatting) by employing alpha-transparent training while explicitly distinguishing foreground occlusions from background subtraction. We validate the full pipeline through extensive experiments, demonstrating its effectiveness across diverse objects. By eliminating the need for manual intervention or environment modifications, our pipeline can be integrated directly into existing pick-and-place setups, enabling scalable and efficient dataset creation. Project page (with code and data): <a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/">https://scalable-real2sim.github.io/</a>. </p>
<blockquote>
<p>é€šè¿‡ä»ç°å®ä¸–ç•Œæ„ŸçŸ¥æ¨¡æ‹Ÿç‰©ä½“åŠ¨æ€ï¼Œæ•°å­—å­ªç”Ÿå’Œæœºå™¨äººæ“ä½œé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†è¿™é€šå¸¸éœ€è¦åŠ³åŠ¨å¯†é›†å‹çš„æµ‹é‡å’Œä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„Real2Simç®¡é“ï¼Œé€šè¿‡æœºå™¨äººäº¤äº’ä¸ºç°å®ä¸–ç•Œç‰©ä½“ç”Ÿæˆæ¨¡æ‹Ÿå°±ç»ªèµ„äº§ã€‚ä»…ä½¿ç”¨æœºå™¨äººçš„å…³èŠ‚æ‰­çŸ©ä¼ æ„Ÿå™¨å’Œå¤–éƒ¨ç›¸æœºï¼Œè¯¥ç®¡é“å¯ä»¥è¯†åˆ«è§†è§‰å‡ ä½•ã€ç¢°æ’å‡ ä½•ä»¥åŠç‰©ç†å±æ€§ï¼ˆä¾‹å¦‚æƒ¯æ€§å‚æ•°ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨alphaé€æ˜è®­ç»ƒï¼Œåˆ©ç”¨å…‰åº¦é‡å»ºæŠ€æœ¯ï¼ˆä¾‹å¦‚NeRFã€é«˜æ–¯å–·ç»˜ï¼‰æå–é«˜è´¨é‡çš„å¯¹è±¡ä¸­å¿ƒç½‘æ ¼ï¼ŒåŒæ—¶æ˜ç¡®åŒºåˆ†å‰æ™¯é®æŒ¡å’ŒèƒŒæ™¯å‡æ³•ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†æ•´ä¸ªç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§å¯¹è±¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡æ¶ˆé™¤å¯¹äººå·¥å¹²é¢„æˆ–ç¯å¢ƒä¿®æ”¹çš„éœ€æ±‚ï¼Œæˆ‘ä»¬çš„ç®¡é“å¯ä»¥ç›´æ¥é›†æˆåˆ°ç°æœ‰çš„æ‹¾å–å’Œæ”¾ç½®è®¾ç½®ä¸­ï¼Œä»è€Œå®ç°å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®åº“åˆ›å»ºã€‚é¡¹ç›®é¡µé¢ï¼ˆå«ä»£ç å’Œæ•°æ®ï¼‰ï¼š<a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/%E3%80%82">https://scalable-real2sim.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00370v2">PDF</a> Website: <a target="_blank" rel="noopener" href="https://scalable-real2sim.github.io/">https://scalable-real2sim.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ç§å…¨è‡ªåŠ¨çš„Real2Simç®¡é“ï¼Œç”¨äºé€šè¿‡æœºå™¨äººäº¤äº’ä¸ºçœŸå®ä¸–ç•Œç‰©ä½“ç”Ÿæˆæ¨¡æ‹Ÿä»¿çœŸèµ„äº§ã€‚åˆ©ç”¨æœºå™¨äººçš„å…³èŠ‚æ‰­çŸ©ä¼ æ„Ÿå™¨å’Œå¤–éƒ¨ç›¸æœºï¼Œè¯¥ç®¡é“èƒ½å¤Ÿè¯†åˆ«ç‰©ä½“çš„è§†è§‰å‡ ä½•ã€ç¢°æ’å‡ ä½•å’Œç‰©ç†å±æ€§ï¼ˆå¦‚æƒ¯æ€§å‚æ•°ï¼‰ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§é‡‡ç”¨alphaé€æ˜è®­ç»ƒæ³•ä»å…‰åº¦é‡å»ºæŠ€æœ¯ï¼ˆå¦‚NeRFã€é«˜æ–¯æ‹¼æ¥æŠ€æœ¯ï¼‰ä¸­æå–é«˜è´¨é‡ç‰©ä½“ä¸­å¿ƒç½‘æ ¼çš„æ–¹æ³•ï¼Œå¹¶èƒ½æ˜ç¡®åŒºåˆ†å‰æ™¯é®æŒ¡å’ŒèƒŒæ™¯å‡æ³•ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥ç®¡é“åœ¨å¤šç§ç‰©ä½“ä¸Šéƒ½å±•ç°å‡ºäº†è‰¯å¥½çš„æ•ˆæœã€‚å®ƒèƒ½å¤Ÿç›´æ¥é›†æˆç°æœ‰çš„æ‹¾å–å’Œæ”¾ç½®è®¾ç½®ï¼Œæ— éœ€äººå·¥å¹²é¢„æˆ–ç¯å¢ƒæ”¹åŠ¨ï¼Œä»è€Œå®ç°äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ•°æ®é›†åˆ›å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§å…¨è‡ªåŠ¨çš„Real2Simç®¡é“ï¼Œç”¨äºç”Ÿæˆæ¨¡æ‹Ÿä»¿çœŸèµ„äº§ã€‚</li>
<li>é€šè¿‡æœºå™¨äººäº¤äº’ï¼Œåˆ©ç”¨æœºå™¨äººçš„å…³èŠ‚æ‰­çŸ©ä¼ æ„Ÿå™¨å’Œå¤–éƒ¨ç›¸æœºè¿›è¡Œè¯†åˆ«ã€‚</li>
<li>èƒ½å¤Ÿè¯†åˆ«ç‰©ä½“çš„è§†è§‰å‡ ä½•ã€ç¢°æ’å‡ ä½•å’Œç‰©ç†å±æ€§ï¼ˆå¦‚æƒ¯æ€§å‚æ•°ï¼‰ã€‚</li>
<li>é‡‡ç”¨äº†alphaé€æ˜è®­ç»ƒæ³•ï¼Œèƒ½å¤Ÿä»å…‰åº¦é‡å»ºæŠ€æœ¯ä¸­æå–é«˜è´¨é‡ç‰©ä½“ä¸­å¿ƒç½‘æ ¼ã€‚</li>
<li>èƒ½å¤Ÿæ˜ç¡®åŒºåˆ†å‰æ™¯é®æŒ¡å’ŒèƒŒæ™¯å‡æ³•ã€‚</li>
<li>ç»è¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼Œè¯¥ç®¡é“åœ¨å¤šç§ç‰©ä½“ä¸Šéƒ½å±•ç°å‡ºäº†è‰¯å¥½æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2035f6c4be743de83b6b6c3884690b27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba7a1307374e31e9533da7b441b4c2b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffc53775a0f4dbe213849bd232a6e089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf64c8b4433f1f326ffa94b2421a4db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca287e62ba891a0564adb80d7d49c127.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ActiveGAMER-Active-GAussian-Mapping-through-Efficient-Rendering"><a href="#ActiveGAMER-Active-GAussian-Mapping-through-Efficient-Rendering" class="headerlink" title="ActiveGAMER: Active GAussian Mapping through Efficient Rendering"></a>ActiveGAMER: Active GAussian Mapping through Efficient Rendering</h2><p><strong>Authors:Liyan Chen, Huangying Zhan, Kevin Chen, Xiangyu Xu, Qingan Yan, Changjiang Cai, Yi Xu</strong></p>
<p>We introduce ActiveGAMER, an active mapping system that utilizes 3D Gaussian Splatting (3DGS) to achieve high-quality, real-time scene mapping and exploration. Unlike traditional NeRF-based methods, which are computationally demanding and restrict active mapping performance, our approach leverages the efficient rendering capabilities of 3DGS, allowing effective and efficient exploration in complex environments. The core of our system is a rendering-based information gain module that dynamically identifies the most informative viewpoints for next-best-view planning, enhancing both geometric and photometric reconstruction accuracy. ActiveGAMER also integrates a carefully balanced framework, combining coarse-to-fine exploration, post-refinement, and a global-local keyframe selection strategy to maximize reconstruction completeness and fidelity. Our system autonomously explores and reconstructs environments with state-of-the-art geometric and photometric accuracy and completeness, significantly surpassing existing approaches in both aspects. Extensive evaluations on benchmark datasets such as Replica and MP3D highlight ActiveGAMERâ€™s effectiveness in active mapping tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ActiveGAMERï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰å®ç°é«˜è´¨é‡ã€å®æ—¶åœºæ™¯æ˜ å°„å’Œæ¢ç´¢çš„ä¸»åŠ¨æ˜ å°„ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„åŸºäºNeRFçš„æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œé™åˆ¶äº†ä¸»åŠ¨æ˜ å°„çš„æ€§èƒ½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨3DGSçš„é«˜æ•ˆæ¸²æŸ“èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯åŸºäºæ¸²æŸ“çš„ä¿¡æ¯å¢ç›Šæ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤ŸåŠ¨æ€åœ°è¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„è§‚ç‚¹ï¼Œç”¨äºè§„åˆ’ä¸‹ä¸€ä¸ªæœ€ä½³è§†å›¾ï¼Œæé«˜å‡ ä½•å’Œå…‰åº¦é‡å»ºçš„ç²¾åº¦ã€‚ActiveGAMERè¿˜é›†æˆäº†ä¸€ä¸ªç²¾å¿ƒå¹³è¡¡çš„æ–¹æ¡ˆï¼Œç»“åˆäº†ä»ç²—åˆ°ç»†çš„æ¢ç´¢ã€åæœŸä¼˜åŒ–ä»¥åŠå…¨å±€å±€éƒ¨å…³é”®å¸§é€‰æ‹©ç­–ç•¥ï¼Œä»¥æœ€å¤§åŒ–é‡å»ºçš„å®Œæ•´æ€§å’Œä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä»¥æœ€å…ˆè¿›çš„å‡ ä½•å’Œå…‰åº¦ç²¾åº¦å’Œå®Œæ•´æ€§è‡ªä¸»åœ°æ¢ç´¢å’Œé‡å»ºç¯å¢ƒï¼Œåœ¨è¿™ä¸¤ä¸ªæ–¹é¢éƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚åœ¨Replicaå’ŒMP3Dç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°å‡¸æ˜¾äº†ActiveGAMERåœ¨ä¸»åŠ¨æ˜ å°„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06897v2">PDF</a> Accepted to CVPR2025</p>
<p><strong>Summary</strong><br>ActiveGAMERæ˜¯ä¸€ç§åˆ©ç”¨3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯å®ç°é«˜è´¨é‡å®æ—¶åœºæ™¯æ˜ å°„å’Œæ¢ç´¢çš„ä¸»åŠ¨æ˜ å°„ç³»ç»Ÿã€‚ç›¸è¾ƒäºä¼ ç»Ÿè®¡ç®—é‡å¤§ä¸”é™åˆ¶ä¸»åŠ¨æ˜ å°„æ€§èƒ½çš„NeRFæ–¹æ³•ï¼ŒActiveGAMERé‡‡ç”¨é«˜æ•ˆçš„æ¸²æŸ“èƒ½åŠ›ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ¢ç´¢ã€‚å…¶æ ¸å¿ƒæ˜¯åŸºäºæ¸²æŸ“çš„ä¿¡æ¯å¢ç›Šæ¨¡å—ï¼Œèƒ½åŠ¨æ€è¯†åˆ«æœ€å…·ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œä¸‹ä¸€æ­¥æœ€ä½³è§†è§’è§„åˆ’ï¼Œæé«˜å‡ ä½•å’Œå…‰åº¦é‡å»ºçš„å‡†ç¡®æ€§ã€‚ActiveGAMERè¿˜ç»“åˆäº†ç²—åˆ°ç»†çš„æ¢ç´¢ã€åæœŸä¼˜åŒ–å’Œå…¨å±€å±€éƒ¨å…³é”®å¸§é€‰æ‹©ç­–ç•¥ï¼Œæ—¨åœ¨æœ€å¤§åŒ–é‡å»ºçš„å®Œæ•´æ€§å’Œé€¼çœŸåº¦ã€‚è¯¥ç³»ç»Ÿåœ¨å‡ ä½•å’Œå…‰åº¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ActiveGAMERåœ¨ä¸»åŠ¨æ˜ å°„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ActiveGAMERåˆ©ç”¨3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯å®ç°é«˜è´¨é‡å®æ—¶åœºæ™¯æ˜ å°„å’Œæ¢ç´¢ã€‚</li>
<li>ä¸ä¼ ç»ŸNeRFæ–¹æ³•ç›¸æ¯”ï¼ŒActiveGAMERé‡‡ç”¨é«˜æ•ˆæ¸²æŸ“æŠ€æœ¯ä¼˜åŒ–å¤æ‚ç¯å¢ƒçš„æ¢ç´¢ã€‚</li>
<li>æ ¸å¿ƒåŸºäºæ¸²æŸ“çš„ä¿¡æ¯å¢ç›Šæ¨¡å—å¯åŠ¨æ€è¯†åˆ«æœ€ä½³è§†è§’ä»¥ä¼˜åŒ–å‡ ä½•å’Œå…‰åº¦é‡å»ºå‡†ç¡®æ€§ã€‚</li>
<li>ActiveGAMERç»“åˆäº†å¤šç§ç­–ç•¥ï¼Œå¦‚ç²—åˆ°ç»†æ¢ç´¢ã€åæœŸä¼˜åŒ–å’Œå…¨å±€å±€éƒ¨å…³é”®å¸§é€‰æ‹©ï¼Œä»¥æœ€å¤§åŒ–é‡å»ºè´¨é‡ã€‚</li>
<li>ç³»ç»Ÿåœ¨å‡ ä½•å’Œå…‰åº¦å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ee2068cd514667fdf8590d3278fd8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec14eb4cb933ec03aae06000294635e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50bac8affe1e91cfbfd2b6b121faa689.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39039f9725c9caaa76b25907768bed3e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Light-Transport-aware-Diffusion-Posterior-Sampling-for-Single-View-Reconstruction-of-3D-Volumes"><a href="#Light-Transport-aware-Diffusion-Posterior-Sampling-for-Single-View-Reconstruction-of-3D-Volumes" class="headerlink" title="Light Transport-aware Diffusion Posterior Sampling for Single-View   Reconstruction of 3D Volumes"></a>Light Transport-aware Diffusion Posterior Sampling for Single-View   Reconstruction of 3D Volumes</h2><p><strong>Authors:Ludwic Leonard, Nils Thuerey, Ruediger Westermann</strong></p>
<p>We introduce a single-view reconstruction technique of volumetric fields in which multiple light scattering effects are omnipresent, such as in clouds. We model the unknown distribution of volumetric fields using an unconditional diffusion model trained on a novel benchmark dataset comprising 1,000 synthetically simulated volumetric density fields. The neural diffusion model is trained on the latent codes of a novel, diffusion-friendly, monoplanar representation. The generative model is used to incorporate a tailored parametric diffusion posterior sampling technique into different reconstruction tasks. A physically-based differentiable volume renderer is employed to provide gradients with respect to light transport in the latent space. This stands in contrast to classic NeRF approaches and makes the reconstructions better aligned with observed data. Through various experiments, we demonstrate single-view reconstruction of volumetric clouds at a previously unattainable quality. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½“ç§¯åœºå•è§†å›¾é‡å»ºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ™®éå­˜åœ¨å¤šæ¬¡æ•£å°„æ•ˆåº”ï¼Œå¦‚äº‘å±‚ä¸­çš„æƒ…å†µã€‚æˆ‘ä»¬ä½¿ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹å¯¹ä½“ç§¯åœºçš„æœªçŸ¥åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œè¯¥æ¨¡å‹æ˜¯åœ¨æ–°å‹åŸºå‡†æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œè¯¥æ•°æ®é›†åŒ…å«1000ä¸ªåˆæˆæ¨¡æ‹Ÿçš„ä½“ç§¯å¯†åº¦åœºã€‚ç¥ç»æ‰©æ•£æ¨¡å‹æ˜¯åœ¨ä¸€ç§æ–°å‹çš„ã€å¯¹æ‰©æ•£å‹å¥½çš„å•å¹³é¢è¡¨ç¤ºçš„æ½œåœ¨ä»£ç ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ç”Ÿæˆæ¨¡å‹è¢«ç”¨äºå°†å®šåˆ¶çš„å‚æ•°åŒ–æ‰©æ•£åé‡‡æ ·æŠ€æœ¯èå…¥ä¸åŒçš„é‡å»ºä»»åŠ¡ä¸­ã€‚é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®ä½“ç§¯æ¸²æŸ“å™¨ï¼Œä»¥æä¾›æ½œåœ¨ç©ºé—´ä¸­å…³äºå…‰ä¼ è¾“çš„æ¢¯åº¦ã€‚è¿™ä¸ä¼ ç»Ÿçš„NeRFæ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œä½¿é‡å»ºç»“æœæ›´å¥½åœ°ä¸è§‚æµ‹æ•°æ®å¯¹é½ã€‚é€šè¿‡å„ç§å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½“ç§¯äº‘çš„å•è§†å›¾é‡å»ºï¼Œè¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05226v3">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å•è§†å›¾é‡å»ºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥é‡å»ºå‡ºå……æ»¡å¤šé‡æ•£å°„æ•ˆæœçš„ä½“ç§¯åœºï¼Œå¦‚äº‘å±‚ã€‚è¯¥ç ”ç©¶é‡‡ç”¨åŸºäºæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„å»ºæ¨¡æ–¹æ³•ï¼Œåˆ©ç”¨æ–°å‹åŸºå‡†æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ•°æ®é›†åŒ…å«ä¸€åƒä¸ªåˆæˆæ¨¡æ‹Ÿçš„ä½“ç§¯å¯†åº¦åœºã€‚ç ”ç©¶å›¢é˜Ÿè¿˜ä½¿ç”¨äº†ä¸€ç§é’ˆå¯¹æ‰©æ•£çš„ä¸ªæ€§åŒ–å‚æ•°åŒ–é‡å»ºæŠ€æœ¯ï¼Œå¹¶ç»“åˆä½¿ç”¨åŸºäºç‰©ç†çš„å¯å¾®ä½“ç§¯æ¸²æŸ“å™¨ã€‚è¿™æ”¹å–„äº†ä¼ ç»ŸNeRFæ–¹æ³•çš„ä¸è¶³ï¼Œä½¿é‡å»ºç»“æœæ›´åŠ è´´è¿‘çœŸå®æ•°æ®è§‚æµ‹ç»“æœï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„é«˜è´¨é‡å•è§†å›¾é‡å»ºäº‘å±‚æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å•è§†å›¾é‡å»ºä½“ç§¯åœºçš„é‡å»ºæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿå¤„ç†å¤šé‡æ•£å°„æ•ˆåº”ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹å¯¹ä½“ç§¯åœºè¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨æ–°å‹åŸºå‡†æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å«åˆæˆæ¨¡æ‹Ÿçš„ä½“ç§¯å¯†åº¦åœºã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨é’ˆå¯¹æ‰©æ•£çš„ä¸ªæ€§åŒ–å‚æ•°åŒ–é‡å»ºæŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é‡‡ç”¨åŸºäºç‰©ç†çš„å¯å¾®ä½“ç§¯æ¸²æŸ“å™¨ï¼Œä½¿é‡å»ºç»“æœæ›´åŠ è´´è¿‘çœŸå®æ•°æ®è§‚æµ‹ç»“æœã€‚</li>
<li>è¯¥æŠ€æœ¯å®ç°äº†é«˜è´¨é‡çš„é‡å»ºäº‘å±‚æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11566dd50cc743473d60ef3912138912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a06d3ba44d73061de32f149c95dd8d86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7503e4f9aa5d5234ea882157ec7087e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-765075193dfa447cd0ad9bd7a3954830.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9be618087546de4ff8ec70027a91ab5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43b74ad375be4fbaa283ef2166a6bff1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting"><a href="#Self-Calibrating-4D-Novel-View-Synthesis-from-Monocular-Videos-Using-Gaussian-Splatting" class="headerlink" title="Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting"></a>Self-Calibrating 4D Novel View Synthesis from Monocular Videos Using   Gaussian Splatting</h2><p><strong>Authors:Fang Li, Hao Zhang, Narendra Ahuja</strong></p>
<p>Gaussian Splatting (GS) has significantly elevated scene reconstruction efficiency and novel view synthesis (NVS) accuracy compared to Neural Radiance Fields (NeRF), particularly for dynamic scenes. However, current 4D NVS methods, whether based on GS or NeRF, primarily rely on camera parameters provided by COLMAP and even utilize sparse point clouds generated by COLMAP for initialization, which lack accuracy as well are time-consuming. This sometimes results in poor dynamic scene representation, especially in scenes with large object movements, or extreme camera conditions e.g. small translations combined with large rotations. Some studies simultaneously optimize the estimation of camera parameters and scenes, supervised by additional information like depth, optical flow, etc. obtained from off-the-shelf models. Using this unverified information as ground truth can reduce robustness and accuracy, which does frequently occur for long monocular videos (with e.g. &gt; hundreds of frames). We propose a novel approach that learns a high-fidelity 4D GS scene representation with self-calibration of camera parameters. It includes the extraction of 2D point features that robustly represent 3D structure, and their use for subsequent joint optimization of camera parameters and 3D structure towards overall 4D scene optimization. We demonstrate the accuracy and time efficiency of our method through extensive quantitative and qualitative experimental results on several standard benchmarks. The results show significant improvements over state-of-the-art methods for 4D novel view synthesis. The source code will be released soon at <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a>. </p>
<blockquote>
<p>é«˜æ–¯é‡‡æ ·ï¼ˆGSï¼‰ä¸ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†åœºæ™¯é‡å»ºæ•ˆç‡å’Œæ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŠ¨æ€åœºæ™¯ã€‚ç„¶è€Œï¼Œå½“å‰çš„å››ç»´NVSæ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºGSè¿˜æ˜¯NeRFï¼Œä¸»è¦ä¾èµ–äºCOLMAPæä¾›çš„ç›¸æœºå‚æ•°ï¼Œç”šè‡³ä½¿ç”¨COLMAPç”Ÿæˆçš„ç¨€ç–ç‚¹äº‘è¿›è¡Œåˆå§‹åŒ–ï¼Œè¿™åœ¨ç²¾åº¦å’Œæ—¶é—´ä¸Šéƒ½å­˜åœ¨ä¸è¶³ã€‚è¿™æœ‰æ—¶ä¼šå¯¼è‡´å¯¹åŠ¨æ€åœºæ™¯çš„è¡¨ç¤ºä¸ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹ç‰©ä½“ç§»åŠ¨æˆ–æç«¯ç›¸æœºæ¡ä»¶ä¸‹ï¼ˆä¾‹å¦‚ï¼Œå°å¹³ç§»ä¸å¤§å¹…æ—‹è½¬ï¼‰ã€‚ä¸€äº›ç ”ç©¶åŒæ—¶ä¼˜åŒ–ç›¸æœºå‚æ•°çš„ä¼°è®¡å’Œåœºæ™¯ï¼Œé€šè¿‡ç°æˆçš„æ¨¡å‹è·å¾—æ·±åº¦ã€å…‰æµç­‰é¢å¤–ä¿¡æ¯è¿›è¡Œç›‘ç£ã€‚ä½¿ç”¨æœªç»éªŒè¯çš„ä¿¡æ¯ä½œä¸ºçœŸå®æ ‡å‡†å¯èƒ½ä¼šé™ä½ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ï¼Œè¿™åœ¨é•¿å•ç›®è§†é¢‘ï¼ˆä¾‹å¦‚æ•°ç™¾å¸§ï¼‰ä¸­ç»å¸¸å‘ç”Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘æ ¡å‡†ç›¸æœºå‚æ•°å­¦ä¹ é«˜ä¿çœŸå››ç»´GSåœºæ™¯è¡¨ç¤ºã€‚å®ƒåŒ…æ‹¬æå–ç¨³å¥åœ°ä»£è¡¨ä¸‰ç»´ç»“æ„çš„äºŒç»´ç‚¹ç‰¹å¾ï¼Œä»¥åŠå®ƒä»¬éšåçš„è”åˆä¼˜åŒ–ç›¸æœºå‚æ•°å’Œä¸‰ç»´ç»“æ„ä»¥è¿›è¡Œæ•´ä½“çš„å››ç»´åœºæ™¯ä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å‡ é¡¹æ ‡å‡†åŸºå‡†æµ‹è¯•è¿›è¡Œäº†å¤§é‡å®šé‡å’Œå®šæ€§çš„å®éªŒç»“æœï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ—¶é—´æ•ˆç‡æ–¹é¢çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°å››ç»´æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æºä»£ç å¾ˆå¿«å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/fangli333/SC-4DGSå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01042v3">PDF</a> GitHub Page: <a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS">https://github.com/fangli333/SC-4DGS</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé«˜æ–¯æ¶‚æŠ¹ï¼ˆGSï¼‰çš„4Dåœºæ™¯è¡¨ç¤ºæ–°æ–¹æ³•ï¼Œèƒ½è‡ªå­¦ä¿®æ­£ç›¸æœºå‚æ•°ï¼Œæé«˜åŠ¨æ€åœºæ™¯çš„é‡å»ºæ•ˆç‡å’Œæ–°å‹è§†è§’åˆæˆï¼ˆNVSï¼‰çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ç¨³å¥çš„2Dç‚¹ç‰¹å¾æ¥ä»£è¡¨3Dç»“æ„ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè”åˆä¼˜åŒ–ç›¸æœºå‚æ•°å’Œ3Dç»“æ„ï¼Œè¾¾åˆ°æ•´ä½“çš„4Dåœºæ™¯ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Gaussian Splatting (GS)æå‡äº†åœºæ™¯é‡å»ºæ•ˆç‡å’Œæ–°å‹è§†è§’åˆæˆï¼ˆNVSï¼‰çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€åœºæ™¯æ–¹é¢ã€‚</li>
<li>ç°æœ‰çš„4D NVSæ–¹æ³•ä¸»è¦ä¾èµ–COLMAPæä¾›çš„ç›¸æœºå‚æ•°ï¼Œå¯¼è‡´ç²¾åº¦å’Œæ—¶é—´æ•ˆç‡ä¸Šçš„é—®é¢˜ã€‚</li>
<li>æ–‡ä¸­æå‡ºçš„æ–¹æ³•èƒ½è‡ªå­¦ä¿®æ­£ç›¸æœºå‚æ•°ï¼Œå®ç°é«˜ä¿çœŸåº¦çš„4D GSåœºæ™¯è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æå–2Dç‚¹ç‰¹å¾æ¥ç¨³å¥åœ°ä»£è¡¨3Dç»“æ„ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè”åˆä¼˜åŒ–ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šç»è¿‡å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æºä»£ç å°†å¾ˆå¿«åœ¨<a target="_blank" rel="noopener" href="https://github.com/fangli333/SC-4DGS%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/fangli333/SC-4DGSä¸Šå‘å¸ƒã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aeb8b6be9ed822de16b6ab6a254689b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943f2448f91afb60a72f6a93466398e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46743dd4fd6de272e99deea3ad94b4ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb752928248c98233389d6a68f5cbb84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94b1a6297fd9bb1be9cdc3fbe53fc163.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MixRT-Mixed-Neural-Representations-For-Real-Time-NeRF-Rendering"><a href="#MixRT-Mixed-Neural-Representations-For-Real-Time-NeRF-Rendering" class="headerlink" title="MixRT: Mixed Neural Representations For Real-Time NeRF Rendering"></a>MixRT: Mixed Neural Representations For Real-Time NeRF Rendering</h2><p><strong>Authors:Chaojian Li, Bichen Wu, Peter Vajda, Yingyan Celine Lin</strong></p>
<p>Neural Radiance Field (NeRF) has emerged as a leading technique for novel view synthesis, owing to its impressive photorealistic reconstruction and rendering capability. Nevertheless, achieving real-time NeRF rendering in large-scale scenes has presented challenges, often leading to the adoption of either intricate baked mesh representations with a substantial number of triangles or resource-intensive ray marching in baked representations. We challenge these conventions, observing that high-quality geometry, represented by meshes with substantial triangles, is not necessary for achieving photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF representation that includes a low-quality mesh, a view-dependent displacement map, and a compressed NeRF model. This design effectively harnesses the capabilities of existing graphics hardware, thus enabling real-time NeRF rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering framework, our proposed MixRT attains real-time rendering speeds on edge devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop), better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360 datasets), and a smaller storage size (less than 80% compared to state-of-the-art methods). </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²ç»æˆä¸ºä¸€ç§é¢†å…ˆçš„æ–°å‹è§†å›¾åˆæˆæŠ€æœ¯ï¼Œå› å…¶ä»¤äººå°è±¡æ·±åˆ»çš„é€¼çœŸé‡å»ºå’Œæ¸²æŸ“èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡åœºæ™¯ä¸­å®ç°å®æ—¶NeRFæ¸²æŸ“å´å­˜åœ¨æŒ‘æˆ˜ï¼Œé€šå¸¸ä¼šå¯¼è‡´é‡‡ç”¨å¤æ‚çš„çƒ˜ç„™ç½‘æ ¼è¡¨ç¤ºæ³•ï¼Œå…¶ä¸­åŒ…å«å¤§é‡çš„ä¸‰è§’å½¢ï¼Œæˆ–è€…é‡‡ç”¨èµ„æºå¯†é›†å‹çš„çƒ˜ç„™è¡¨ç¤ºä¸­çš„å…‰çº¿è¡Œè¿›æŠ€æœ¯ã€‚æˆ‘ä»¬æŒ‘æˆ˜è¿™äº›ä¼ ç»Ÿè§‚å¿µï¼Œè§‚å¯Ÿåˆ°é«˜è´¨é‡çš„å‡ ä½•å½¢çŠ¶ï¼Œç”±åŒ…å«å¤§é‡ä¸‰è§’å½¢çš„ç½‘æ ¼è¡¨ç¤ºï¼Œå¹¶ä¸æ˜¯å®ç°é€¼çœŸæ¸²æŸ“è´¨é‡æ‰€å¿…éœ€çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MixRTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„NeRFè¡¨ç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä½è´¨é‡ç½‘æ ¼ã€è§†å·®å›¾ï¼ˆview-dependent displacement mapï¼‰å’Œå‹ç¼©NeRFæ¨¡å‹ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°åˆ©ç”¨äº†ç°æœ‰å›¾å½¢ç¡¬ä»¶çš„åŠŸèƒ½ï¼Œä»è€Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶NeRFæ¸²æŸ“ã€‚å€ŸåŠ©é«˜åº¦ä¼˜åŒ–çš„WebGLæ¸²æŸ“æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºçš„MixRTåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°äº†å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ˆåœ¨MacBook M1 Proç¬”è®°æœ¬ç”µè„‘ä¸Šä»¥1280 x 720çš„åˆ†è¾¨ç‡è¶…è¿‡30 FPSï¼‰ï¼Œæ›´å¥½çš„æ¸²æŸ“è´¨é‡ï¼ˆåœ¨Unbounded-360æ•°æ®é›†å®¤å†…åœºæ™¯ä¸­é«˜å‡º0.2 PSNRï¼‰ï¼Œä»¥åŠæ›´å°çš„å­˜å‚¨å¤§å°ï¼ˆä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å‡å°‘äº†ä¸åˆ°80%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.11841v5">PDF</a> Accepted by 3DVâ€™24. Project Page: <a target="_blank" rel="noopener" href="https://licj15.github.io/MixRT/">https://licj15.github.io/MixRT/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Neural Radiance Fieldï¼ˆNeRFï¼‰æŠ€æœ¯åœ¨æ–°å‹è§†å›¾åˆæˆæ–¹é¢çš„å‰æ²¿åº”ç”¨ï¼Œæå‡ºä¸€ç§åä¸ºMixRTçš„æ–°å‹NeRFè¡¨ç¤ºæ–¹æ³•ï¼Œç»“åˆä½è´¨é‡ç½‘æ ¼ã€è§†å›¾ç›¸å…³ä½ç§»å›¾å’Œå‹ç¼©NeRFæ¨¡å‹ï¼Œå®ç°äº†å®æ—¶NeRFæ¸²æŸ“ã€‚MixRTåˆ©ç”¨ä¼˜åŒ–çš„WebGLæ¸²æŸ“æ¡†æ¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼ŒåŒæ—¶æé«˜æ¸²æŸ“è´¨é‡å’Œå‡å°å­˜å‚¨å¤§å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯å·²æˆä¸ºæ–°å‹è§†å›¾åˆæˆçš„é¢†å…ˆæŠ€æœ¯ï¼Œå…·æœ‰æƒŠäººçš„é€¼çœŸé‡å»ºå’Œæ¸²æŸ“èƒ½åŠ›ã€‚</li>
<li>å®ç°å¤§è§„æ¨¡åœºæ™¯ä¸­çš„å®æ—¶NeRFæ¸²æŸ“å…·æœ‰æŒ‘æˆ˜ï¼Œé€šå¸¸é‡‡ç”¨å¤æ‚çš„çƒ˜ç„™ç½‘æ ¼è¡¨ç¤ºæ³•æˆ–èµ„æºå¯†é›†å‹çš„å°„çº¿è¿½è¸ªæ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æŒ‘æˆ˜äº†è¿™äº›ä¼ ç»Ÿæ–¹æ³•ï¼ŒæŒ‡å‡ºé«˜è´¨é‡å‡ ä½•å½¢çŠ¶ï¼ˆç”±å¤§é‡ä¸‰è§’å½¢æ„æˆçš„ç½‘æ ¼è¡¨ç¤ºï¼‰å¹¶ä¸æ˜¯å®ç°é€¼çœŸæ¸²æŸ“è´¨é‡çš„å¿…è¦æ¡ä»¶ã€‚</li>
<li>æå‡ºäº†MixRTï¼Œä¸€ç§æ–°å‹çš„NeRFè¡¨ç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä½è´¨é‡ç½‘æ ¼ã€è§†å›¾ç›¸å…³ä½ç§»å›¾å’Œå‹ç¼©NeRFæ¨¡å‹ï¼Œæœ‰æ•ˆç»“åˆç°æœ‰å›¾å½¢ç¡¬ä»¶çš„èƒ½åŠ›ã€‚</li>
<li>MixRTåˆ©ç”¨é«˜åº¦ä¼˜åŒ–çš„WebGLæ¸²æŸ“æ¡†æ¶ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶æ¸²æŸ“é€Ÿåº¦ï¼Œå¯åœ¨MacBook M1 Proç¬”è®°æœ¬ç”µè„‘ä¸Šä»¥è¶…è¿‡30 FPSçš„é€Ÿç‡è¿›è¡Œ1280 x 720åˆ†è¾¨ç‡çš„æ¸²æŸ“ã€‚</li>
<li>MixRTçš„æ¸²æŸ“è´¨é‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å®¤å†…åœºæ™¯çš„Unbounded-360æ•°æ®é›†ä¸ŠPSNRå€¼æé«˜äº†0.2ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.11841">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c34dfa8d80e73c73f189bda7be9230e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44d66c65781dd7094b22b4ed21552bd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16ddd4478152e5c6a4f02bc6b86875f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-729c2c4e7c400fc17f0e1faacd580d64.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c7b0c6b8320afd10d25f4c857613fcf9.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  ILLUME+ Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-283a61d43de86c60c11763eb2b0d954f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Diffusion-Guided Gaussian Splatting for Large-Scale Unconstrained 3D   Reconstruction and Novel View Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
