<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Prompting Forgetting Unlearning in GANs via Textual Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    33 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œç»™æ‰˜ç®¡è¿™äº›æ¨¡å‹çš„æœåŠ¡æä¾›å•†å¸¦æ¥äº†å„ç§ä¼¦ç†å’Œæ³•å¾‹æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œå†…å®¹ç§»é™¤æŠ€æœ¯ï¼ˆCRTsï¼‰ä½œä¸ºç ”ç©¶çš„ä¸€ä¸ªä¸æ–­å¢é•¿é¢†åŸŸå‡ºç°ï¼Œæ—¨åœ¨æ§åˆ¶è¾“å‡ºè€Œæ— éœ€è¿›è¡Œå…¨é¢å†è®­ç»ƒã€‚è¿‘æœŸçš„ç ”ç©¶æ¢ç´¢äº†ç”Ÿæˆæ¨¡å‹ä¸­æœºå™¨é—å¿˜çš„ä½¿ç”¨ä»¥è§£å†³å†…å®¹ç§»é™¤é—®é¢˜ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç ”ç©¶çš„é‡ç‚¹åœ¨æ‰©æ•£æ¨¡å‹ä¸Šï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­çš„é—å¿˜ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºText-to-Unlearnæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ–‡æœ¬æç¤ºä»é¢„è®­ç»ƒçš„GANsä¸­é€‰æ‹©æ€§é—å¿˜æ¦‚å¿µçš„æ–°æ¡†æ¶ï¼Œå¯å®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜å’Œåœ¨äººè„¸è®­ç»ƒæ¨¡å‹ä¸­çš„è¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ç­‰ç²¾ç»†ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨é—å¿˜è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ”¹ç¼–è‡ªæœ€å…ˆè¿›çš„å›¾åƒæ–‡æœ¬å¯¹é½æŒ‡æ ‡ï¼Œå¯¹é—å¿˜æ–¹æ³•è¿›è¡Œå…¨é¢åˆ†æã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒText-to-Unlearnæ˜¯ç¬¬ä¸€ä¸ªç”¨äºGANsçš„è·¨æ¨¡å¼é—å¿˜æ¡†æ¶ï¼Œä»£è¡¨ç€ç®¡ç†ç”Ÿæˆæ¨¡å‹è¡Œä¸ºçš„ä¸€ä¸ªçµæ´»è€Œé«˜æ•ˆçš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºText-to-Unlearnçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºé€‰æ‹©æ€§åœ°é—å¿˜é¢„è®­ç»ƒçš„GANæ¨¡å‹ä¸­çš„æ¦‚å¿µï¼Œå®ç°ç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ä»¥åŠè¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ç­‰ç²¾ç»†ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒï¼Œæä¾›å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-Unlearnæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©æ€§åœ°é—å¿˜GANæ¨¡å‹ä¸­çš„ç‰¹å®šæ¦‚å¿µã€‚</li>
<li>æ¡†æ¶æ”¯æŒç‰¹å¾é—å¿˜ã€èº«ä»½é—å¿˜ä»¥åŠè¡¨æƒ…å’Œå¤šå±æ€§ç§»é™¤ç­‰ç²¾ç»†ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥å¼•å¯¼é—å¿˜è¿‡ç¨‹ã€‚</li>
<li>Text-to-Unlearnä¸éœ€è¦é¢å¤–çš„æ•°æ®é›†æˆ–ç›‘ç£å¾®è°ƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨é—å¿˜è¯„ä¼°æ–¹æ³•ï¼Œæ¥è‡ªé€‚åº”åˆ†æé—å¿˜æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Text-to-Unlearnæ˜¯é¦–ä¸ªé’ˆå¯¹GANsçš„è·¨æ¨¡æ€é—å¿˜æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generalization-aware-Remote-Sensing-Change-Detection-via-Domain-agnostic-Learning"><a href="#Generalization-aware-Remote-Sensing-Change-Detection-via-Domain-agnostic-Learning" class="headerlink" title="Generalization-aware Remote Sensing Change Detection via Domain-agnostic   Learning"></a>Generalization-aware Remote Sensing Change Detection via Domain-agnostic   Learning</h2><p><strong>Authors:Qi Zang, Shuang Wang, Dong Zhao, Dou Quan, Yang Hu, Licheng Jiao</strong></p>
<p>Change detection has essential significance for the regionâ€™s development, in which pseudo-changes between bitemporal images induced by imaging environmental factors are key challenges. Existing transformation-based methods regard pseudo-changes as a kind of style shift and alleviate it by transforming bitemporal images into the same style using generative adversarial networks (GANs). However, their efforts are limited by two drawbacks: 1) Transformed images suffer from distortion that reduces feature discrimination. 2) Alignment hampers the model from learning domain-agnostic representations that degrades performance on scenes with domain shifts from the training data. Therefore, oriented from pseudo-changes caused by style differences, we present a generalizable domain-agnostic difference learning network (DonaNet). For the drawback 1), we argue for local-level statistics as style proxies to assist against domain shifts. For the drawback 2), DonaNet learns domain-agnostic representations by removing domain-specific style of encoded features and highlighting the class characteristics of objects. In the removal, we propose a domain difference removal module to reduce feature variance while preserving discriminative properties and propose its enhanced version to provide possibilities for eliminating more style by decorrelating the correlation between features. In the highlighting, we propose a cross-temporal generalization learning strategy to imitate latent domain shifts, thus enabling the model to extract feature representations more robust to shifts actively. Extensive experiments conducted on three public datasets demonstrate that DonaNet outperforms existing state-of-the-art methods with a smaller model size and is more robust to domain shift. </p>
<blockquote>
<p>å˜åŒ–æ£€æµ‹å¯¹åŒºåŸŸå‘å±•å…·æœ‰é‡å¤§æ„ä¹‰ï¼Œå…¶ä¸­ç”±æˆåƒç¯å¢ƒå› ç´ å¼•èµ·çš„åŒæ—¶ç›¸å›¾åƒä¹‹é—´çš„ä¼ªå˜åŒ–æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºå˜æ¢çš„æ–¹æ³•å°†ä¼ªå˜åŒ–è§†ä¸ºä¸€ç§é£æ ¼è½¬å˜ï¼Œå¹¶é€šè¿‡ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å°†åŒæ—¶ç›¸å›¾åƒè½¬æ¢ä¸ºç›¸åŒé£æ ¼æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œä»–ä»¬çš„åŠªåŠ›å—é™äºä¸¤ä¸ªç¼ºç‚¹ï¼š1ï¼‰å˜æ¢åçš„å›¾åƒä¼šå‡ºç°å¤±çœŸï¼Œé™ä½ç‰¹å¾è¾¨åˆ«èƒ½åŠ›ã€‚2ï¼‰å¯¹é½é—®é¢˜é˜»ç¢äº†æ¨¡å‹å­¦ä¹ é¢†åŸŸæ— å…³çš„è¡¨ç¤ºï¼Œé™ä½äº†åœ¨åœºæ™¯åŸŸä»è®­ç»ƒæ•°æ®ä¸­è¿ç§»æ—¶çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œé’ˆå¯¹ç”±é£æ ¼å·®å¼‚å¼•èµ·çš„ä¼ªå˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„é¢†åŸŸæ— å…³å·®å¼‚å­¦ä¹ ç½‘ç»œï¼ˆDonaNetï¼‰ã€‚é’ˆå¯¹ç¼ºç‚¹ä¸€ï¼Œæˆ‘ä»¬ä¸»å¼ ä½¿ç”¨å±€éƒ¨çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºé£æ ¼ä»£ç†æ¥å¸®åŠ©å¯¹æŠ—é¢†åŸŸè¿ç§»ã€‚é’ˆå¯¹ç¼ºç‚¹äºŒï¼ŒDonaNeté€šè¿‡å­¦ä¹ å»é™¤ç¼–ç ç‰¹å¾çš„ç‰¹å®šé¢†åŸŸé£æ ¼å¹¶çªå‡ºå¯¹è±¡çš„ç±»ç‰¹å¾æ¥å­¦ä¹ é¢†åŸŸæ— å…³çš„è¡¨ç¤ºã€‚åœ¨å»é™¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢†åŸŸå·®å¼‚å»é™¤æ¨¡å—ï¼Œä»¥é™ä½ç‰¹å¾æ–¹å·®çš„åŒæ—¶ä¿ç•™åˆ¤åˆ«å±æ€§ï¼Œå¹¶æå‡ºäº†å…¶å¢å¼ºç‰ˆï¼Œé€šè¿‡è§£ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§æ¥æ¶ˆé™¤æ›´å¤šé£æ ¼çš„å¯èƒ½æ€§ã€‚åœ¨çªå‡ºè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨æ—¶é—´æ³›åŒ–å­¦ä¹ ç­–ç•¥æ¥æ¨¡æ‹Ÿæ½œåœ¨é¢†åŸŸè¿ç§»ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´ä¸»åŠ¨åœ°æå–å¯¹è¿ç§»æ›´ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDonaNetåœ¨è¾ƒå°çš„æ¨¡å‹è§„æ¨¡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¯¹é¢†åŸŸè¿ç§»æ›´å…·é²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00543v1">PDF</a> </p>
<p><strong>Summary</strong><br>    é’ˆå¯¹ç¯å¢ƒå˜åŒ–å¯¼è‡´çš„ä¼ªå˜åŒ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªé€šç”¨çš„é¢†åŸŸæ— å…³å·®å¼‚å­¦ä¹ ç½‘ç»œï¼ˆDonaNetï¼‰ã€‚é€šè¿‡åˆ©ç”¨å±€éƒ¨çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ä½œä¸ºé£æ ¼ä»£ç†æ¥å¯¹æŠ—é¢†åŸŸåç§»ï¼ŒåŒæ—¶å­¦ä¹ é¢†åŸŸæ— å…³çš„è¡¨ç¤ºï¼Œé€šè¿‡å»é™¤ç‰¹å¾ä¸­çš„ç‰¹å®šé£æ ¼å¹¶çªå‡ºå¯¹è±¡ç±»åˆ«ç‰¹å¾æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚DonaNetèƒ½æœ‰æ•ˆå»é™¤é£æ ¼å¹¶çªå‡ºé‰´åˆ«å±æ€§ï¼Œå®éªŒç»“æœè¯æ˜äº†å…¶åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ªå˜åŒ–æ˜¯ç¯å¢ƒæˆåƒå› ç´ å¯¼è‡´çš„åŒæ—¶æ€å›¾åƒé—´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å°†åŒæ—¶æ€å›¾åƒè½¬æ¢ä¸ºåŒä¸€é£æ ¼æ¥è§£å†³ä¼ªå˜åŒ–é—®é¢˜ï¼Œä½†å­˜åœ¨å›¾åƒå¤±çœŸå’Œæ¨¡å‹å¯¹é½é—®é¢˜ã€‚</li>
<li>DonaNeté€šè¿‡åˆ©ç”¨å±€éƒ¨çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯æ¥å¯¹æŠ—é¢†åŸŸåç§»ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DonaNeté€šè¿‡å­¦ä¹ å»é™¤ç‰¹å¾ä¸­çš„ç‰¹å®šé£æ ¼å¹¶çªå‡ºå¯¹è±¡ç±»åˆ«ç‰¹å¾æ¥å­¦ä¹ é¢†åŸŸæ— å…³çš„è¡¨ç¤ºã€‚</li>
<li>DonaNeté€šè¿‡æå‡ºçš„é¢†åŸŸå·®å¼‚å»é™¤æ¨¡å—å‡å°‘äº†ç‰¹å¾æ–¹å·®ï¼ŒåŒæ—¶ä¿ç•™äº†é‰´åˆ«å±æ€§ã€‚</li>
<li>DonaNetåœ¨æ¨¡ä»¿æ½œåœ¨é¢†åŸŸåç§»çš„ç­–ç•¥ä¸Šè¿›è¡Œäº†åˆ›æ–°ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´ç¨³å¥åœ°æå–ç‰¹å¾è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94e2ab5722a06ee39c0d1c0f889f9228.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00252e745c0efcbd5516b4fdf1b25f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003d0bd2f0461f2d1142c62cc8b171a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a745f243bed1d9d2c88122ae501482e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bfeba95aacc4bb03d5d0980b859c450.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Collaborative-Advantage-of-Low-level-Information-on-Generalizable-AI-Generated-Image-Detection"><a href="#Exploring-the-Collaborative-Advantage-of-Low-level-Information-on-Generalizable-AI-Generated-Image-Detection" class="headerlink" title="Exploring the Collaborative Advantage of Low-level Information on   Generalizable AI-Generated Image Detection"></a>Exploring the Collaborative Advantage of Low-level Information on   Generalizable AI-Generated Image Detection</h2><p><strong>Authors:Ziyin Zhou, Ke Sun, Zhongxi Chen, Xianming Lin, Yunpeng Luo, Ke Yan, Shouhong Ding, Xiaoshuai Sun</strong></p>
<p>Existing state-of-the-art AI-Generated image detection methods mostly consider extracting low-level information from RGB images to help improve the generalization of AI-Generated image detection, such as noise patterns. However, these methods often consider only a single type of low-level information, which may lead to suboptimal generalization. Through empirical analysis, we have discovered a key insight: different low-level information often exhibits generalization capabilities for different types of forgeries. Furthermore, we found that simple fusion strategies are insufficient to leverage the detection advantages of each low-level and high-level information for various forgery types. Therefore, we propose the Adaptive Low-level Experts Injection (ALEI) framework. Our approach introduces Lora Experts, enabling the backbone network, which is trained with high-level semantic RGB images, to accept and learn knowledge from different low-level information. We utilize a cross-attention method to adaptively fuse these features at intermediate layers. To prevent the backbone network from losing the modeling capabilities of different low-level features during the later stages of modeling, we developed a Low-level Information Adapter that interacts with the features extracted by the backbone network. Finally, we propose Dynamic Feature Selection, which dynamically selects the most suitable features for detecting the current image to maximize generalization detection capability. Extensive experiments demonstrate that our method, finetuned on only four categories of mainstream ProGAN data, performs excellently and achieves state-of-the-art results on multiple datasets containing unseen GAN and Diffusion methods. </p>
<blockquote>
<p>ç°æœ‰æœ€å…ˆè¿›çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ–¹æ³•å¤§å¤šæ˜¯ä»RGBå›¾åƒä¸­æå–ä½çº§ä¿¡æ¯æ¥å¸®åŠ©æé«˜AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚å™ªå£°æ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åªè€ƒè™‘ä¸€ç§ç±»å‹çš„ä½çº§ä¿¡æ¯ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¬¡ä¼˜çš„æ³›åŒ–æ•ˆæœã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®è§‚ç‚¹ï¼šä¸åŒçš„ä½çº§ä¿¡æ¯å¾€å¾€å¯¹ä¸åŒç±»å‹çš„ä¼ªé€ è¡¨ç°å‡ºä¸åŒçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç®€å•çš„èåˆç­–ç•¥ä¸è¶³ä»¥åˆ©ç”¨æ¯ç§ä½çº§å’Œé«˜çº§ä¿¡æ¯å¯¹ä¸åŒä¼ªé€ ç±»å‹çš„æ£€æµ‹ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ä½çº§ä¸“å®¶æ³¨å…¥ï¼ˆALEIï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†Loraä¸“å®¶ï¼Œä½¿ç»è¿‡é«˜çº§è¯­ä¹‰RGBå›¾åƒè®­ç»ƒçš„éª¨å¹²ç½‘ç»œèƒ½å¤Ÿæ¥å—å’Œå­¦ä¹ æ¥è‡ªä¸åŒä½çº§ä¿¡æ¯çš„çŸ¥è¯†ã€‚æˆ‘ä»¬ä½¿ç”¨äº¤å‰æ³¨æ„æ–¹æ³•æ¥è‡ªé€‚åº”åœ°èåˆä¸­é—´å±‚çš„è¿™äº›ç‰¹å¾ã€‚ä¸ºäº†é˜²æ­¢éª¨å¹²ç½‘ç»œåœ¨å»ºæ¨¡åæœŸå¤±å»å¯¹ä¸åŒä½çº§ç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä½çº§ä¿¡æ¯é€‚é…å™¨ï¼Œå®ƒä¸éª¨å¹²ç½‘ç»œæå–çš„ç‰¹å¾è¿›è¡Œäº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€ç‰¹å¾é€‰æ‹©ï¼Œå®ƒåŠ¨æ€é€‰æ‹©æœ€é€‚åˆæ£€æµ‹å½“å‰å›¾åƒçš„ç‰¹å¾ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜æ³›åŒ–æ£€æµ‹èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨å››ç§ä¸»æµProGANæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå³å¯åœ¨å¤šæ•°æ®é›†ä¸Šå–å¾—å‡ºè‰²çš„è¡¨ç°ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬æœªè§è¿‡çš„GANå’ŒDiffusionæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00463v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç°æœ‰AIç”Ÿæˆå›¾åƒæ£€æµ‹çš„æ–¹æ³•ä¸»è¦é€šè¿‡æå–RGBå›¾åƒçš„åº•å±‚ä¿¡æ¯æ¥æé«˜æ£€æµ‹æ³›åŒ–èƒ½åŠ›ï¼Œå¦‚å™ªå£°æ¨¡å¼ã€‚ä½†è¿™ç§æ–¹æ³•å¾€å¾€åªè€ƒè™‘å•ä¸€ç±»å‹çš„åº•å±‚ä¿¡æ¯ï¼Œå¯èƒ½å¯¼è‡´æ³›åŒ–æ•ˆæœä¸ä½³ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒåº•å±‚ä¿¡æ¯å¯¹ä¸åŒç±»å‹çš„ä¼ªé€ å›¾åƒå…·æœ‰ä¸åŒçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæé«˜æ£€æµ‹å„ç§ä¼ªé€ ç±»å‹çš„èƒ½åŠ›ï¼Œæå‡ºäº†è‡ªé€‚åº”åº•å±‚ä¸“å®¶æ³¨å…¥ï¼ˆALEIï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥Loraä¸“å®¶ï¼Œä½¿è®­ç»ƒæœ‰é«˜çº§è¯­ä¹‰RGBå›¾åƒçš„èƒŒæ™¯ç½‘ç»œèƒ½å¤Ÿæ¥å—å¹¶å­¦ä¹ å„ç§åº•å±‚ä¿¡æ¯çš„çŸ¥è¯†ã€‚ä½¿ç”¨è·¨æ³¨æ„åŠ›æ–¹æ³•åœ¨ä¸­é—´å±‚è‡ªé€‚åº”èåˆè¿™äº›ç‰¹å¾ã€‚ä¸ºé˜²æ­¢èƒŒæ™¯ç½‘ç»œåœ¨åæœŸå»ºæ¨¡ä¸­å¤±å»å¯¹åº•å±‚ç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå¼€å‘äº†åº•å±‚ä¿¡æ¯é€‚é…å™¨ï¼Œä¸èƒŒæ™¯ç½‘ç»œæå–çš„ç‰¹å¾è¿›è¡Œäº¤äº’ã€‚æœ€åï¼Œæå‡ºäº†åŠ¨æ€ç‰¹å¾é€‰æ‹©ï¼Œèƒ½å¤ŸåŠ¨æ€é€‰æ‹©æœ€é€‚åˆæ£€æµ‹å½“å‰å›¾åƒçš„ç‰¹å¾ï¼Œä»¥æœ€å¤§åŒ–æ³›åŒ–æ£€æµ‹èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŒ…å«æœªè§è¿‡çš„GANå’Œæ‰©æ•£æ–¹æ³•çš„æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸»è¦ä¾èµ–RGBå›¾åƒçš„åº•å±‚ä¿¡æ¯æ¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ä¸åŒåº•å±‚ä¿¡æ¯å¯¹ä¸åŒç±»å‹çš„ä¼ªé€ å›¾åƒå…·æœ‰ä¸åŒçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ALEIæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥Loraä¸“å®¶ç»“åˆåº•å±‚å’Œé«˜å±‚æ¬¡ä¿¡æ¯ã€‚</li>
<li>ä½¿ç”¨è·¨æ³¨æ„åŠ›æ–¹æ³•åœ¨ä¸­é—´å±‚èåˆç‰¹å¾ã€‚</li>
<li>å¼€å‘äº†åº•å±‚ä¿¡æ¯é€‚é…å™¨ï¼Œä¿æŒå¯¹åº•å±‚ç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>åŠ¨æ€ç‰¹å¾é€‰æ‹©èƒ½æœ€å¤§åŒ–æ³›åŒ–æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°ä¼˜å¼‚æ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7f5cd4ab23586ef487d5a14ffcc78f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f5186b30e80346598418f6afa89ae5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1efa71886031e529de5d78070607556f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Style-Quantization-for-Data-Efficient-GAN-Training"><a href="#Style-Quantization-for-Data-Efficient-GAN-Training" class="headerlink" title="Style Quantization for Data-Efficient GAN Training"></a>Style Quantization for Data-Efficient GAN Training</h2><p><strong>Authors:Jian Wang, Xin Lan, Jizhe Zhou, Yuxin Tian, Jiancheng Lv</strong></p>
<p>Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific real data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled &#96;&#96;styleâ€™â€™ space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method. </p>
<blockquote>
<p>åœ¨æœ‰é™æ•°æ®è®¾ç½®ä¸‹ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å¾€å¾€éš¾ä»¥å¯¼èˆªå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨è¾“å…¥æ½œåœ¨ç©ºé—´ã€‚å› æ­¤ï¼Œä»ç¨€ç–è¾“å…¥æ½œåœ¨ç©ºé—´ä¸­çš„ç›¸é‚»å˜é‡ç”Ÿæˆçš„å›¾åƒåœ¨é€¼çœŸåº¦æ–¹é¢å¯èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä»è€Œå¯¼è‡´æ¬¡ä¼˜çš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SQ-GANè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆæ¥å¢å¼ºCRã€‚è¯¥æ–¹æ³•å°†ç¨€ç–ã€è¿ç»­çš„è¾“å…¥æ½œåœ¨ç©ºé—´è½¬æ¢ä¸ºç´§å‡‘ã€ç»“æ„åŒ–çš„ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œä½¿æ¯ä¸ªå…ƒç´ éƒ½èƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„çœŸå®æ•°æ®ç‚¹ï¼Œä»è€Œæé«˜CRæ€§èƒ½ã€‚æˆ‘ä»¬ä¸æ˜¯ç›´æ¥è¿›è¡Œé‡åŒ–ï¼Œè€Œæ˜¯é¦–å…ˆå°†è¾“å…¥æ½œåœ¨å˜é‡æ˜ å°„åˆ°ä¸€ä¸ªä¸é‚£ä¹ˆçº ç¼ çš„â€œé£æ ¼â€ç©ºé—´ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„ä»£ç æœ¬è¿›è¡Œé‡åŒ–ã€‚è¿™ä½¿å¾—æ¯ä¸ªé‡åŒ–çš„ä»£ç èƒ½å¤Ÿæ§åˆ¶ä¸åŒçš„å˜åŒ–å› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†æœ€ä½³ä¼ è¾“è·ç¦»ï¼Œå°†ä»£ç æœ¬ä¸­çš„ä»£ç ä¸åŸºç¡€æ¨¡å‹ä»è®­ç»ƒæ•°æ®ä¸­æå–çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå°†å¤–éƒ¨çŸ¥è¯†åµŒå…¥åˆ°ä»£ç æœ¬ä¸­ï¼Œå»ºç«‹äº†ä¸€ä¸ªè¯­ä¹‰ä¸°å¯Œçš„è¯æ±‡è¡¨ï¼Œé€‚å½“åœ°æè¿°äº†è®­ç»ƒæ•°æ®é›†ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ¤åˆ«å™¨é²æ£’æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸‹ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨è¾“å…¥æ½œåœ¨ç©ºé—´çš„å¯¼èˆªå’Œæœ‰æ•ˆæ¢ç´¢æ–¹é¢å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SQ-GANï¼Œé€šè¿‡å¼•å…¥é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆï¼Œå¢å¼ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•å°†ç¨€ç–çš„è¿ç»­è¾“å…¥æ½œåœ¨ç©ºé—´è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–ç¦»æ•£ä»£ç†ç©ºé—´ï¼Œä½¿æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªçœŸå®æ•°æ®ç‚¹ï¼Œä»è€Œæé«˜CRæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ¤åˆ«å™¨é²æ£’æ€§å’Œç”Ÿæˆè´¨é‡æ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸‹ï¼ŒGANsåœ¨å¯¼èˆªå’Œæœ‰æ•ˆæ¢ç´¢è¾“å…¥æ½œåœ¨ç©ºé—´æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„SQ-GANé€šè¿‡é£æ ¼ç©ºé—´é‡åŒ–æ–¹æ¡ˆå¢å¼ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼ˆCRï¼‰æ€§èƒ½ã€‚</li>
<li>SQ-GANå°†ç¨€ç–çš„è¿ç»­è¾“å…¥æ½œåœ¨ç©ºé—´è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–ç¦»æ•£ä»£ç†ç©ºé—´ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿æ¯ä¸ªé‡åŒ–ä»£ç å¯¹åº”ä¸€ä¸ªçœŸå®æ•°æ®ç‚¹ï¼Œæé«˜CRæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨å­¦ä¹ ä»£ç æœ¬è¿›è¡Œé‡åŒ–ï¼Œä½¿æ¯ä¸ªé‡åŒ–ä»£ç æ§åˆ¶ä¸åŒçš„å˜é‡å› ç´ ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¼ è¾“è·ç¦»ï¼Œå°†ä»£ç æœ¬ä»£ç ä¸åŸºç¡€æ¨¡å‹æå–çš„ç‰¹å¾å¯¹é½ï¼ŒåµŒå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œå»ºç«‹ä¸°å¯Œçš„è¯­ä¹‰è¯æ±‡è¡¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b991e157764a9b779fa1b915f2277ca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges"><a href="#Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges" class="headerlink" title="Deterministic Medical Image Translation via High-fidelity Brownian   Bridges"></a>Deterministic Medical Image Translation via High-fidelity Brownian   Bridges</h2><p><strong>Authors:Qisheng He, Nicholas Summerfield, Peiyong Wang, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Recent studies have shown that diffusion models produce superior synthetic images when compared to Generative Adversarial Networks (GANs). However, their outputs are often non-deterministic and lack high fidelity to the ground truth due to the inherent randomness. In this paper, we propose a novel High-fidelity Brownian bridge model (HiFi-BBrg) for deterministic medical image translations. Our model comprises two distinct yet mutually beneficial mappings: a generation mapping and a reconstruction mapping. The Brownian bridge training process is guided by the fidelity loss and adversarial training in the reconstruction mapping. This ensures that translated images can be accurately reversed to their original forms, thereby achieving consistent translations with high fidelity to the ground truth. Our extensive experiments on multiple datasets show HiFi-BBrg outperforms state-of-the-art methods in multi-modal image translation and multi-image super-resolution. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒæ–¹é¢è¡¨ç°æ›´ä¼˜è¶Šã€‚ç„¶è€Œï¼Œç”±äºå…¶å†…åœ¨çš„éšæœºæ€§ï¼Œå®ƒä»¬çš„è¾“å‡ºé€šå¸¸æ˜¯éç¡®å®šçš„ï¼Œå¹¶ä¸”å¯¹çœŸå®æƒ…å†µçš„ä¿çœŸåº¦ä¸é«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒç¿»è¯‘çš„æ–°å‹é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªç‹¬ç‰¹è€Œç›¸äº’æœ‰ç›Šçš„æ˜ å°„ï¼šç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ã€‚å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ç”±é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸåº¦æŸå¤±å’Œå¯¹æŠ—æ€§è®­ç»ƒå¼•å¯¼ã€‚è¿™ç¡®ä¿äº†ç¿»è¯‘åçš„å›¾åƒå¯ä»¥å‡†ç¡®åœ°æ¢å¤åˆ°å…¶åŸå§‹å½¢å¼ï¼Œä»è€Œå®ç°ä¸çœŸå®æƒ…å†µé«˜åº¦ä¸€è‡´çš„ç¿»è¯‘ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHiFi-BBrgåœ¨è·¨æ¨¡æ€å›¾åƒç¿»è¯‘å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22531v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ€è¿‘ç ”ç©¶è¡¨æ˜æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒæ–¹é¢ä¼˜äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œä½†å…¶è¾“å‡ºé€šå¸¸å…·æœ‰éç¡®å®šæ€§ä¸”å¯¹çœŸå®æ•°æ®ç¼ºä¹é«˜ä¿çœŸåº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ç”¨äºç¡®å®šæ€§åŒ»å­¦å›¾åƒè½¬æ¢ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªç‹¬ç‰¹ä¸”ç›¸äº’ä¿ƒè¿›çš„æ˜ å°„ï¼šç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ã€‚å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹ç”±é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸæŸå¤±å’Œå¯¹æŠ—è®­ç»ƒå¼•å¯¼ï¼Œç¡®ä¿ç¿»è¯‘åçš„å›¾åƒå¯ä»¥å‡†ç¡®è¿˜åŸä¸ºåŸå§‹å½¢å¼ï¼Œä»è€Œå®ç°é«˜ä¿çœŸåº¦çš„ä¸€è‡´ç¿»è¯‘ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiFi-BBrgåœ¨å¤šæ¨¡æ€å›¾åƒè½¬æ¢å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆåˆæˆå›¾åƒæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä½†å­˜åœ¨éç¡®å®šæ€§å’Œå¯¹çœŸå®æ•°æ®ç¼ºä¹é«˜ä¿çœŸåº¦çš„é—®é¢˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜ä¿çœŸå¸ƒæœ—æ¡¥æ¨¡å‹ï¼ˆHiFi-BBrgï¼‰ç”¨äºåŒ»å­¦å›¾åƒè½¬æ¢ã€‚</li>
<li>HiFi-BBrgæ¨¡å‹åŒ…å«ä¸¤ä¸ªå…³é”®æ˜ å°„ï¼šç”Ÿæˆæ˜ å°„å’Œé‡å»ºæ˜ å°„ï¼Œè¿™ä¸¤ä¸ªæ˜ å°„ç›¸äº’ä¿ƒè¿›ã€‚</li>
<li>å¸ƒæœ—æ¡¥è®­ç»ƒè¿‡ç¨‹é€šè¿‡é‡å»ºæ˜ å°„ä¸­çš„ä¿çœŸæŸå¤±å’Œå¯¹æŠ—è®­ç»ƒæ¥å¼•å¯¼ï¼Œç¡®ä¿å›¾åƒç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚</li>
<li>HiFi-BBrgèƒ½å¤Ÿå®ç°é«˜ä¿çœŸåº¦çš„ä¸€è‡´ç¿»è¯‘ï¼Œå…‹æœç°æœ‰æŠ€æœ¯çš„ä¸è¶³ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHiFi-BBrgåœ¨å¤šæ¨¡æ€å›¾åƒè½¬æ¢å’Œå¤šå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89b720f2289ff49b5d424b1d1bb39466.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42c63a9a154920b748723ea6f60d03e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b4c05035d61db74240beef8ac3ded3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74dd698f33980edc4ea7e2552f99da80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c4519fed2f33f938908a378ab11a3e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5c85f0176bb823177a0938160c03c26.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder"><a href="#Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder" class="headerlink" title="Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder"></a>Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder</h2><p><strong>Authors:Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang</strong></p>
<p>Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œåœ¨å…¨æ–°é¢†åŸŸå®ç°è¿ç»­å±æ€§çš„ç²¾ç¡®æ§åˆ¶ï¼Œå°¤å…¶æ˜¯åŒæ—¶æ§åˆ¶å¤šä¸ªå±æ€§ï¼ˆä¾‹å¦‚ï¼Œåƒçœ¼ç›çå¼€ç¨‹åº¦æˆ–æ±½è½¦å®½åº¦è¿™æ ·çš„æ•°å€¼ï¼‰ä»…é€šè¿‡æ–‡æœ¬æŒ‡å¯¼ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§ï¼ˆAttï¼‰é€‚é…å™¨ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å³æ’å³ç”¨æ¨¡å—ï¼Œæ—¨åœ¨åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç»†çš„å¤šå±æ€§æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»ä¸€ç»„æ ·æœ¬å›¾åƒä¸­å­¦ä¹ å•ä¸ªæ§åˆ¶é€‚é…å™¨ï¼Œè¿™äº›å›¾åƒå¯ä»¥æ˜¯æœªé…å¯¹çš„ï¼Œå¹¶åŒ…å«å¤šä¸ªè§†è§‰å±æ€§ã€‚Att-Adapteråˆ©ç”¨è§£è€¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè‡ªç„¶åœ°åè°ƒå¤šä¸ªåŸŸå±æ€§ä¸æ–‡æœ¬æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜å°†æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCVAEï¼‰å¼•å…¥åˆ°Att-Adapterä¸­ï¼Œä»¥å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œé€‚åº”è§†è§‰ä¸–ç•Œçš„å¤šæ ·æ€§ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒAtt-Adapteråœ¨æ§åˆ¶è¿ç»­å±æ€§æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºäºLoRAçš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å¤§äº†æ§åˆ¶èŒƒå›´ï¼Œå¹¶æ”¹å–„äº†å¤šä¸ªå±æ€§ä¹‹é—´çš„è§£çº ç¼ ï¼Œè¶…è¶Šäº†åŸºäºStyleGANçš„æŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAtt-Adapteréå¸¸çµæ´»ï¼Œæ— éœ€é…å¯¹åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“åœ¨å•ä¸ªæ¨¡å‹å†…æ‰©å±•åˆ°å¤šä¸ªå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11937v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å·²ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œï¼Œä½¿ç”¨çº¯æ–‡æœ¬æŒ‡å¯¼åœ¨æ–°çš„é¢†åŸŸï¼ˆå¦‚çœ¼ç›çå¼€ç¨‹åº¦æˆ–æ±½è½¦å®½åº¦ç­‰æ•°å€¼ï¼‰å®ç°è¿ç»­å±æ€§çš„ç²¾ç¡®æ§åˆ¶ï¼Œå°¤å…¶æ˜¯åŒæ—¶æ§åˆ¶å¤šä¸ªå±æ€§ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§é€‚é…å™¨ï¼ˆAtt-Adapterï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å³æ’å³ç”¨æ¨¡å—ï¼Œæ—¨åœ¨åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç»†ç²’åº¦çš„å¤šå±æ€§æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»ä¸€ç»„æœªé…å¯¹çš„åŒ…å«å¤šä¸ªè§†è§‰å±æ€§çš„æ ·æœ¬å›¾åƒä¸­å­¦ä¹ å•ä¸ªæ§åˆ¶é€‚é…å™¨ã€‚Att-Adapteråˆ©ç”¨è§£è€¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œè‡ªç„¶åœ°åè°ƒæ–‡æœ¬æ¡ä»¶ä¸å¤šä¸ªåŸŸå±æ€§ã€‚æˆ‘ä»¬è¿˜å°†æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰å¼•å…¥åˆ°Att-Adapterä¸­ï¼Œä»¥å‡è½»è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œé€‚åº”è§†è§‰ä¸–ç•Œçš„å¤šæ ·æ€§ã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAtt-Adapteråœ¨æ§åˆ¶è¿ç»­å±æ€§æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºäºLoRAçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å¤§äº†æ§åˆ¶èŒƒå›´ï¼Œå¹¶æ”¹å–„äº†å¤šä¸ªå±æ€§ä¹‹é—´çš„è§£çº ç¼ ï¼Œè¶…è¶Šäº†StyleGANæŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAtt-Adapteréå¸¸çµæ´»ï¼Œæ— éœ€é…å¯¹åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä¸”å¯ä»¥è½»æ¾æ‰©å±•åˆ°å•ä¸ªæ¨¡å‹å†…çš„å¤šä¸ªå±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ã€‚</li>
<li>å®ç°æ–°é¢†åŸŸä¸­å¤šä¸ªå±æ€§çš„ç²¾ç¡®æ§åˆ¶æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>å±æ€§é€‚é…å™¨ï¼ˆAtt-Adapterï¼‰æ˜¯ä¸€ç§æ–°å‹æ¨¡å—ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå®ç°ç²¾ç»†ç²’åº¦çš„å¤šå±æ€§æ§åˆ¶ã€‚</li>
<li>Att-Adapterèƒ½ä»æœªé…å¯¹çš„æ ·æœ¬å›¾åƒä¸­å­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨è§£è€¦äº¤å‰æ³¨æ„åŠ›æ¨¡å—å’Œæ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰å®ç°æ›´è‡ªç„¶çš„å±æ€§åè°ƒä¸é€‚åº”è§†è§‰ä¸–ç•Œçš„å¤šæ ·æ€§ã€‚</li>
<li>Att-Adapteråœ¨æ§åˆ¶è¿ç»­å±æ€§æ–¹é¢ä¼˜äºåŸºäºLoRAçš„æ–¹æ³•ï¼Œå¹¶èƒ½æ‰©å¤§æ§åˆ¶èŒƒå›´ï¼Œæ”¹å–„å¤šä¸ªå±æ€§é—´çš„è§£çº ç¼ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50629aed4fae0a17a1c42ccc011f8cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d986435c039237a7e970cd0226135aa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-875e25fedbf0e4b14a9f58e93517bcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf7c2ca605970f534731b4f292f64f6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participantâ€™s consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSynShotçš„æ–°æ–¹æ³•ï¼Œç”¨äºåŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°é•œå¤´é©±åŠ¨å¤´éƒ¨åŒ–èº«å€’ç½®ã€‚æˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œéœ€è¦å¤§é‡çš„ä¸åŒåºåˆ—ï¼Œè€Œè¿™äº›åºåˆ—çš„å›¾åƒå’Œé«˜è´¨é‡è·Ÿè¸ªç½‘æ ¼å¹¶ä¸æ€»æ˜¯å¯ç”¨ã€‚å…¶æ¬¡ï¼ŒçœŸå®æ•°æ®çš„ä½¿ç”¨å—åˆ°ä¸¥æ ¼ç›‘ç®¡ï¼ˆä¾‹å¦‚ï¼Œæ ¹æ®ã€Šé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ã€‹ï¼Œå½“å‚ä¸è€…åŒæ„æ’¤å›æ—¶ï¼Œéœ€è¦é¢‘ç¹åˆ é™¤æ¨¡å‹å’Œæ•°åˆ é™¤æ¨¡å‹å’Œæ•°æ®æ®ï¼‰ã€‚ä¸å—è¿™äº›çº¦æŸçš„åˆæˆæ•°æ®æ˜¯ä¸€ä¸ªå¸å¼•äººçš„é€‰æ‹©ã€‚ç¬¬ä¸‰ï¼Œæœ€å…ˆè¿›çš„å•ç›®åŒ–èº«æ¨¡å‹éš¾ä»¥æ¨å¹¿åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…ï¼Œç¼ºä¹å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶ä¸”ç»å¸¸è¿‡åº¦é€‚åº”ç‰¹å®šçš„è§†è§’åˆ†å¸ƒã€‚å—ä»…ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»åŒ…å«ä¸åŒèº«ä»½ã€è¡¨æƒ…å’Œè§†è§’çš„å¤§é‡åˆæˆå¤´éƒ¨æ•°æ®ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹ã€‚å‡­å€Ÿå°‘æ•°è¾“å…¥å›¾åƒï¼ŒSynShotå¾®è°ƒäº†é¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒï¼Œä»¥å¼¥åˆé¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œå»ºæ¨¡ä¸€ä¸ªå¯¹æ–°å‹è¡¨æƒ…å’Œè§†è§’é€šç”¨çš„é€¼çœŸå¤´éƒ¨åŒ–èº«ã€‚æˆ‘ä»¬ä½¿ç”¨3Dé«˜æ–¯è´´å›¾æŠ€æœ¯å’Œå·ç§¯ç¼–ç å™¨-è§£ç å™¨å¯¹å¤´éƒ¨åŒ–èº«è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥ç¼–ç å™¨-è§£ç å™¨è¾“å‡ºUVçº¹ç†ç©ºé—´çš„é«˜æ–¯å‚æ•°ã€‚ä¸ºäº†è€ƒè™‘å¤´éƒ¨å„éƒ¨åˆ†çš„å»ºæ¨¡å¤æ‚æ€§ï¼ˆä¾‹å¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡åµŒå…¥å…ˆéªŒæ¥æ˜ç¡®æ§åˆ¶æ¯ä¸ªéƒ¨åˆ†çš„åŸå§‹æ•°é‡ã€‚ä¸æœ€å…ˆè¿›çš„å•ç›®å’ŒåŸºäºGANçš„æ–¹æ³•ç›¸æ¯”ï¼ŒSynShotåœ¨æ–°å‹è§†è§’å’Œè¡¨æƒ…åˆæˆæ–¹é¢æœ‰äº†æ˜¾ç€æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v3">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SynShotæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°é•œå¤´å³å¯é©±åŠ¨å¤´éƒ¨åŒ–èº«åæ¼”æŠ€æœ¯çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å¤šæ ·åºåˆ—å›¾åƒä¸é«˜è´¨é‡è¿½è¸ªç½‘æ ¼çš„è®­ç»ƒæ•°æ®ã€çœŸå®æ•°æ®ä½¿ç”¨å—é™ä»¥åŠå½“å‰å•çœ¼åŒ–èº«æ¨¡å‹åœ¨æ–°è§†è§’å’Œè¡¨æƒ…ä¸‹çš„æ³›åŒ–èƒ½åŠ›å¼±ã€‚é€šè¿‡ä»å¤§é‡åˆæˆå¤´éƒ¨æ•°æ®ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹ï¼ŒSynShotåªéœ€å°‘é‡è¾“å…¥å›¾åƒå³å¯å¾®è°ƒé¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒï¼Œä»¥ç¼©å°é¢†åŸŸå·®è·ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªèƒ½æ³›åŒ–åˆ°æ–°è¡¨æƒ…å’Œè§†è§’çš„å…‰ç…§å¤´éƒ¨åŒ–èº«ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸‰ç»´é«˜æ–¯è´´ç‰‡æŠ€æœ¯å’Œå·ç§¯ç¼–ç å™¨-è§£ç å™¨å»ºæ¨¡å¤´éƒ¨åŒ–èº«ï¼Œè¾“å‡ºUVçº¹ç†ç©ºé—´çš„é«˜æ–¯å‚æ•°ã€‚ä¸ºäº†å¤„ç†å¤´éƒ¨ä¸åŒéƒ¨åˆ†çš„å»ºæ¨¡å¤æ‚æ€§ï¼ˆå¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼Œè¯¥æ–¹æ³•å…·æœ‰å¯¹æ¯ä¸ªéƒ¨åˆ†åŸå§‹æ•°é‡çš„ä¸Šé‡‡æ ·è¿›è¡Œæ˜¾å¼æ§åˆ¶çš„èƒ½åŠ›ã€‚ç›¸è¾ƒäºå½“å‰å•çœ¼å’ŒåŸºäºGANçš„æ–¹æ³•ï¼ŒSynShotæå¤§åœ°æå‡äº†æ–°å‹è§†è§’å’Œè¡¨æƒ…çš„åˆæˆæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€SynShotæ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°é•œå¤´å³å¯é©±åŠ¨å¤´éƒ¨åŒ–èº«åæ¼”æŠ€æœ¯çš„æ–°æ–¹æ³•ã€‚<br>äºŒã€è¯¥æ–¹æ³•è§£å†³äº†è®­ç»ƒ3Dç”Ÿæˆç½‘ç»œæ‰€é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šç¼ºä¹å¤šæ ·åºåˆ—å›¾åƒä¸é«˜è´¨é‡è¿½è¸ªç½‘æ ¼çš„è®­ç»ƒæ•°æ®ã€çœŸå®æ•°æ®ä½¿ç”¨å—é™ä»¥åŠå•çœ¼åŒ–èº«æ¨¡å‹åœ¨æ–°è§†è§’å’Œè¡¨æƒ…ä¸‹çš„æ³›åŒ–éš¾é¢˜ã€‚<br>ä¸‰ã€é€šè¿‡ä»åˆæˆå¤´éƒ¨æ•°æ®ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹ï¼ŒSynShotèƒ½å¤Ÿåˆ©ç”¨å°‘é‡è¾“å…¥å›¾åƒå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆæ³›åŒ–æ€§å¼ºçš„å…‰ç…§å¤´éƒ¨åŒ–èº«ã€‚<br>å››ã€é‡‡ç”¨ä¸‰ç»´é«˜æ–¯è´´ç‰‡æŠ€æœ¯å’Œå·ç§¯ç¼–ç å™¨-è§£ç å™¨è¿›è¡Œå¤´éƒ¨åŒ–èº«å»ºæ¨¡ï¼Œæå‡æ–°å‹è§†è§’å’Œè¡¨æƒ…çš„åˆæˆæ•ˆæœã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d0bd43aa12bbc98f505f9bcecfbb35ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c6c599bcc7d81e1ada08fc3bb3d40bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00244ec435f68322181bf9ee515280f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4f38e20d8fd6d3e48e671855e82f200.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-950e2b8244c0fac09198e2129caece27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2bfaf37bfdfe73c4a5d22dd4d3c8c0a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Singular-Value-Scaling-Efficient-Generative-Model-Compression-via-Pruned-Weights-Refinement"><a href="#Singular-Value-Scaling-Efficient-Generative-Model-Compression-via-Pruned-Weights-Refinement" class="headerlink" title="Singular Value Scaling: Efficient Generative Model Compression via   Pruned Weights Refinement"></a>Singular Value Scaling: Efficient Generative Model Compression via   Pruned Weights Refinement</h2><p><strong>Authors:Hyeonjin Kim, Jaejun Yoo</strong></p>
<p>While pruning methods effectively maintain model performance without extra training costs, they often focus solely on preserving crucial connections, overlooking the impact of pruned weights on subsequent fine-tuning or distillation, leading to inefficiencies. Moreover, most compression techniques for generative models have been developed primarily for GANs, tailored to specific architectures like StyleGAN, and research into compressing Diffusion models has just begun. Even more, these methods are often applicable only to GANs or Diffusion models, highlighting the need for approaches that work across both model types. In this paper, we introduce Singular Value Scaling (SVS), a versatile technique for refining pruned weights, applicable to both model types. Our analysis reveals that pruned weights often exhibit dominant singular vectors, hindering fine-tuning efficiency and leading to suboptimal performance compared to random initialization. Our method enhances weight initialization by minimizing the disparities between singular values of pruned weights, thereby improving the fine-tuning process. This approach not only guides the compressed model toward superior solutions but also significantly speeds up fine-tuning. Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS improves compression performance across model types without additional training costs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/LAIT-CVLab/Singular-Value-Scaling">https://github.com/LAIT-CVLab/Singular-Value-Scaling</a>. </p>
<blockquote>
<p>è™½ç„¶å‰ªææ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä¿æŒæ¨¡å‹æ€§èƒ½è€Œæ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬ï¼Œä½†å®ƒä»¬é€šå¸¸åªä¸“æ³¨äºä¿ç•™å…³é”®è¿æ¥ï¼Œè€Œå¿½è§†äº†å‰ªææƒé‡å¯¹åç»­å¾®è°ƒæˆ–è’¸é¦çš„å½±å“ï¼Œä»è€Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç”Ÿæˆæ¨¡å‹çš„å‹ç¼©æŠ€æœ¯ä¸»è¦æ˜¯ä¸ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å¼€å‘çš„ï¼Œä¸“ä¸ºç‰¹å®šçš„æ¶æ„ï¼ˆå¦‚StyleGANï¼‰å®šåˆ¶ï¼Œè€Œå¯¹Diffusionæ¨¡å‹çš„å‹ç¼©ç ”ç©¶æ‰åˆšåˆšå¼€å§‹ã€‚è€Œä¸”ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä»…é€‚ç”¨äºGANsæˆ–Diffusionæ¨¡å‹ï¼Œè¿™çªæ˜¾äº†å¯¹åœ¨è¿™ä¸¤ç§æ¨¡å‹ç±»å‹ä¸­éƒ½é€‚ç”¨çš„æ–¹æ³•çš„éœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¥‡å¼‚å€¼ç¼©æ”¾ï¼ˆSingular Value Scaling, SVSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç‚¼å‰ªææƒé‡çš„é€šç”¨æŠ€æœ¯ï¼Œé€‚ç”¨äºè¿™ä¸¤ç§æ¨¡å‹ç±»å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå‰ªææƒé‡é€šå¸¸è¡¨ç°å‡ºä¸»è¦çš„å¥‡å¼‚å‘é‡ï¼Œé˜»ç¢å¾®è°ƒæ•ˆç‡ï¼Œå¯¼è‡´ä¸éšæœºåˆå§‹åŒ–ç›¸æ¯”æ€§èƒ½ä¸ä½³ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æœ€å°åŒ–å‰ªææƒé‡çš„å¥‡å¼‚å€¼ä¹‹é—´çš„å·®å¼‚æ¥å¢å¼ºæƒé‡åˆå§‹åŒ–ï¼Œä»è€Œæ”¹è¿›å¾®è°ƒè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å¼•å¯¼å‹ç¼©æ¨¡å‹è¾¾åˆ°æ›´ä¼˜çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸”è¿˜å¤§å¤§åŠ å¿«äº†å¾®è°ƒé€Ÿåº¦ã€‚åœ¨StyleGAN2ã€StyleGAN3å’ŒDDPMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSVSæé«˜äº†å„ç§æ¨¡å‹çš„å‹ç¼©æ€§èƒ½ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/LAIT-CVlab/Singular-Value-Scaling">https://github.com/LAIT-CVlab/Singular-Value-Scaling</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17387v3">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¥‡å¼‚å€¼ç¼©æ”¾ï¼ˆSVSï¼‰çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨ä¼˜åŒ–ä¿®å‰ªæƒé‡ï¼Œå¹¶é€‚ç”¨äºç”Ÿæˆæ¨¡å‹ä¸­çš„GANå’Œæ‰©æ•£æ¨¡å‹ã€‚åˆ†æè¡¨æ˜ï¼Œä¿®å‰ªçš„æƒé‡å¾€å¾€è¡¨ç°å‡ºæ˜¾è‘—çš„å¥‡å¼‚å‘é‡ï¼Œå½±å“å¾®è°ƒæ•ˆç‡å¹¶å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚SVSæŠ€æœ¯é€šè¿‡æœ€å°åŒ–ä¿®å‰ªæƒé‡çš„å¥‡å¼‚å€¼å·®å¼‚ï¼Œæ”¹è¿›æƒé‡åˆå§‹åŒ–ï¼Œä»è€Œæé«˜å¾®è°ƒè¿‡ç¨‹æ•ˆç‡ï¼Œå¼•å¯¼å‹ç¼©æ¨¡å‹è¾¾åˆ°æ›´å¥½çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶åŠ å¿«å¾®è°ƒé€Ÿåº¦ã€‚å®éªŒè¯æ˜ï¼ŒSVSæŠ€æœ¯åœ¨StyleGAN2ã€StyleGAN3å’ŒDDPMä¸Šå‡èƒ½æé«˜å‹ç¼©æ€§èƒ½ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿®å‰ªæ–¹æ³•è™½èƒ½æœ‰æ•ˆç»´æŒæ¨¡å‹æ€§èƒ½å¹¶èŠ‚çœè®­ç»ƒæˆæœ¬ï¼Œä½†å¾€å¾€åªå…³æ³¨ä¿ç•™å…³é”®è¿æ¥ï¼Œå¿½è§†äº†ä¿®å‰ªæƒé‡å¯¹åç»­å¾®è°ƒæˆ–è’¸é¦çš„å½±å“ï¼Œå¯¼è‡´æ•ˆç‡ä¸é«˜ã€‚</li>
<li>å¤§å¤šæ•°ç”Ÿæˆæ¨¡å‹çš„å‹ç¼©æŠ€æœ¯ä¸»è¦é’ˆå¯¹GANså’Œç‰¹å®šçš„æ¶æ„ï¼ˆå¦‚StyleGANï¼‰å¼€å‘ï¼Œå¯¹æ‰©æ•£æ¨¡å‹çš„å‹ç¼©ç ”ç©¶æ‰åˆšåˆšå¼€å§‹ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•å¾€å¾€åªé€‚ç”¨äºGANsæˆ–æ‰©æ•£æ¨¡å‹ï¼Œéœ€è¦å¼€å‘é€‚ç”¨äºä¸¤ç§æ¨¡å‹ç±»å‹çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†å¥‡å¼‚å€¼ç¼©æ”¾ï¼ˆSVSï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–ä¿®å‰ªæƒé‡çš„é€šç”¨æ–¹æ³•ï¼Œé€‚ç”¨äºGANså’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>åˆ†ææ˜¾ç¤ºï¼Œä¿®å‰ªçš„æƒé‡å…·æœ‰æ˜¾è‘—çš„å¥‡å¼‚å‘é‡ï¼Œè¿™ä¼šå½±å“å¾®è°ƒæ•ˆç‡å¹¶å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>SVSæŠ€æœ¯é€šè¿‡æœ€å°åŒ–ä¿®å‰ªæƒé‡çš„å¥‡å¼‚å€¼å·®å¼‚ï¼Œæ”¹è¿›äº†æƒé‡çš„åˆå§‹åŒ–ï¼Œä»è€Œæé«˜äº†å¾®è°ƒè¿‡ç¨‹çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdcb59522aa39379132cc98d65b497ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b22610d8a4b4132bf3b8eb9c0ae9f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c056a7695e123f11901bf9674a6943c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0791ccc6a2153eb93236d15332918536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f588bca72b144b936ab08dce0375678b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-414b1d39995f2454b12679ac03d13d69.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  FRAME Floor-aligned Representation for Avatar Motion from Egocentric   Video
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Prompting Forgetting Unlearning in GANs via Textual Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18723.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
