<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN 方向最新论文已更新，请持续关注 Update in 2025-04-04  Prompting Forgetting Unlearning in GANs via Textual Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>当前先进的生成模型展现出强大的图像生成能力，给托管这些模型的服务提供商带来了各种伦理和法律挑战。因此，内容移除技术（CRTs）作为研究的一个不断增长领域出现，旨在控制输出而无需进行全面再训练。近期的研究探索了生成模型中机器遗忘的使用以解决内容移除问题。然而，此类研究的重点在扩散模型上，生成对抗网络（GANs）中的遗忘仍然未被充分探索。我们通过提出Text-to-Unlearn来解决这一问题，这是一个使用文本提示从预训练的GANs中选择性遗忘概念的新框架，可实现特征遗忘、身份遗忘和在人脸训练模型中的表情和多属性移除等精细任务。我们的方法利用自然语言描述来引导遗忘过程，无需额外的数据集或监督微调，提供可伸缩和高效的解决方案。为了评估其有效性，我们引入了一种自动遗忘评估方法，该方法改编自最先进的图像文本对齐指标，对遗忘方法进行全面分析。据我们所知，Text-to-Unlearn是第一个用于GANs的跨模式遗忘框架，代表着管理生成模型行为的一个灵活而高效的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Text-to-Unlearn的新型框架，该框架能够通过文本提示选择性地遗忘预训练的GAN模型中的概念，实现特征遗忘、身份遗忘以及表情和多属性移除等精细任务。该方法利用自然语言描述来引导遗忘过程，无需额外的数据集或监督微调，提供可伸缩和高效的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-Unlearn框架能够选择性地遗忘GAN模型中的特定概念。</li>
<li>框架支持特征遗忘、身份遗忘以及表情和多属性移除等精细任务。</li>
<li>该方法利用自然语言描述来引导遗忘过程。</li>
<li>Text-to-Unlearn不需要额外的数据集或监督微调。</li>
<li>引入了一种自动遗忘评估方法，来自适应分析遗忘方法的有效性。</li>
<li>Text-to-Unlearn是首个针对GANs的跨模态遗忘框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Generalization-aware-Remote-Sensing-Change-Detection-via-Domain-agnostic-Learning"><a href="#Generalization-aware-Remote-Sensing-Change-Detection-via-Domain-agnostic-Learning" class="headerlink" title="Generalization-aware Remote Sensing Change Detection via Domain-agnostic   Learning"></a>Generalization-aware Remote Sensing Change Detection via Domain-agnostic   Learning</h2><p><strong>Authors:Qi Zang, Shuang Wang, Dong Zhao, Dou Quan, Yang Hu, Licheng Jiao</strong></p>
<p>Change detection has essential significance for the region’s development, in which pseudo-changes between bitemporal images induced by imaging environmental factors are key challenges. Existing transformation-based methods regard pseudo-changes as a kind of style shift and alleviate it by transforming bitemporal images into the same style using generative adversarial networks (GANs). However, their efforts are limited by two drawbacks: 1) Transformed images suffer from distortion that reduces feature discrimination. 2) Alignment hampers the model from learning domain-agnostic representations that degrades performance on scenes with domain shifts from the training data. Therefore, oriented from pseudo-changes caused by style differences, we present a generalizable domain-agnostic difference learning network (DonaNet). For the drawback 1), we argue for local-level statistics as style proxies to assist against domain shifts. For the drawback 2), DonaNet learns domain-agnostic representations by removing domain-specific style of encoded features and highlighting the class characteristics of objects. In the removal, we propose a domain difference removal module to reduce feature variance while preserving discriminative properties and propose its enhanced version to provide possibilities for eliminating more style by decorrelating the correlation between features. In the highlighting, we propose a cross-temporal generalization learning strategy to imitate latent domain shifts, thus enabling the model to extract feature representations more robust to shifts actively. Extensive experiments conducted on three public datasets demonstrate that DonaNet outperforms existing state-of-the-art methods with a smaller model size and is more robust to domain shift. </p>
<blockquote>
<p>变化检测对区域发展具有重大意义，其中由成像环境因素引起的双时相图像之间的伪变化是关键挑战。现有的基于变换的方法将伪变化视为一种风格转变，并通过使用生成对抗网络（GAN）将双时相图像转换为相同风格来缓解这一问题。然而，他们的努力受限于两个缺点：1）变换后的图像会出现失真，降低特征辨别能力。2）对齐问题阻碍了模型学习领域无关的表示，降低了在场景域从训练数据中迁移时的性能。因此，针对由风格差异引起的伪变化，我们提出了一种通用的领域无关差异学习网络（DonaNet）。针对缺点一，我们主张使用局部级别的统计信息作为风格代理来帮助对抗领域迁移。针对缺点二，DonaNet通过学习去除编码特征的特定领域风格并突出对象的类特征来学习领域无关的表示。在去除过程中，我们提出了一个领域差异去除模块，以降低特征方差的同时保留判别属性，并提出了其增强版，通过解特征之间的相关性来消除更多风格的可能性。在突出过程中，我们提出了一种跨时间泛化学习策略来模拟潜在领域迁移，从而使模型能够更主动地提取对迁移更稳健的特征表示。在三个公共数据集上进行的广泛实验表明，DonaNet在较小的模型规模上优于现有最先进的方法，并且对领域迁移更具鲁棒性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00543v1">PDF</a> </p>
<p><strong>Summary</strong><br>    针对环境变化导致的伪变化问题，提出了一个通用的领域无关差异学习网络（DonaNet）。通过利用局部级别的统计信息作为风格代理来对抗领域偏移，同时学习领域无关的表示，通过去除特征中的特定风格并突出对象类别特征来解决现有方法的局限。DonaNet能有效去除风格并突出鉴别属性，实验结果证明了其在三个公开数据集上的优越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>伪变化是环境成像因素导致的双时态图像间的主要挑战之一。</li>
<li>现有方法通过将双时态图像转换为同一风格来解决伪变化问题，但存在图像失真和模型对齐问题。</li>
<li>DonaNet通过利用局部级别的统计信息来对抗领域偏移，解决了上述问题。</li>
<li>DonaNet通过学习去除特征中的特定风格并突出对象类别特征来学习领域无关的表示。</li>
<li>DonaNet通过提出的领域差异去除模块减少了特征方差，同时保留了鉴别属性。</li>
<li>DonaNet在模仿潜在领域偏移的策略上进行了创新，使其能够更稳健地提取特征表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00543">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94e2ab5722a06ee39c0d1c0f889f9228.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00252e745c0efcbd5516b4fdf1b25f93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003d0bd2f0461f2d1142c62cc8b171a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a745f243bed1d9d2c88122ae501482e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bfeba95aacc4bb03d5d0980b859c450.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Collaborative-Advantage-of-Low-level-Information-on-Generalizable-AI-Generated-Image-Detection"><a href="#Exploring-the-Collaborative-Advantage-of-Low-level-Information-on-Generalizable-AI-Generated-Image-Detection" class="headerlink" title="Exploring the Collaborative Advantage of Low-level Information on   Generalizable AI-Generated Image Detection"></a>Exploring the Collaborative Advantage of Low-level Information on   Generalizable AI-Generated Image Detection</h2><p><strong>Authors:Ziyin Zhou, Ke Sun, Zhongxi Chen, Xianming Lin, Yunpeng Luo, Ke Yan, Shouhong Ding, Xiaoshuai Sun</strong></p>
<p>Existing state-of-the-art AI-Generated image detection methods mostly consider extracting low-level information from RGB images to help improve the generalization of AI-Generated image detection, such as noise patterns. However, these methods often consider only a single type of low-level information, which may lead to suboptimal generalization. Through empirical analysis, we have discovered a key insight: different low-level information often exhibits generalization capabilities for different types of forgeries. Furthermore, we found that simple fusion strategies are insufficient to leverage the detection advantages of each low-level and high-level information for various forgery types. Therefore, we propose the Adaptive Low-level Experts Injection (ALEI) framework. Our approach introduces Lora Experts, enabling the backbone network, which is trained with high-level semantic RGB images, to accept and learn knowledge from different low-level information. We utilize a cross-attention method to adaptively fuse these features at intermediate layers. To prevent the backbone network from losing the modeling capabilities of different low-level features during the later stages of modeling, we developed a Low-level Information Adapter that interacts with the features extracted by the backbone network. Finally, we propose Dynamic Feature Selection, which dynamically selects the most suitable features for detecting the current image to maximize generalization detection capability. Extensive experiments demonstrate that our method, finetuned on only four categories of mainstream ProGAN data, performs excellently and achieves state-of-the-art results on multiple datasets containing unseen GAN and Diffusion methods. </p>
<blockquote>
<p>现有最先进的AI生成图像检测的方法大多是从RGB图像中提取低级信息来帮助提高AI生成图像检测的泛化能力，例如噪声模式。然而，这些方法通常只考虑一种类型的低级信息，可能会导致次优的泛化效果。通过实证分析，我们发现了一个关键观点：不同的低级信息往往对不同类型的伪造表现出不同的泛化能力。此外，我们发现简单的融合策略不足以利用每种低级和高级信息对不同伪造类型的检测优势。因此，我们提出了自适应低级专家注入（ALEI）框架。我们的方法引入了Lora专家，使经过高级语义RGB图像训练的骨干网络能够接受和学习来自不同低级信息的知识。我们使用交叉注意方法来自适应地融合中间层的这些特征。为了防止骨干网络在建模后期失去对不同低级特征的建模能力，我们开发了一个低级信息适配器，它与骨干网络提取的特征进行交互。最后，我们提出了动态特征选择，它动态选择最适合检测当前图像的特征，以最大限度地提高泛化检测能力。大量实验表明，我们的方法仅在四种主流ProGAN数据上进行微调，即可在多数据集上取得出色的表现，达到业界领先水平，包括未见过的GAN和Diffusion方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00463v1">PDF</a> </p>
<p><strong>Summary</strong><br>     现有AI生成图像检测的方法主要通过提取RGB图像的底层信息来提高检测泛化能力，如噪声模式。但这种方法往往只考虑单一类型的底层信息，可能导致泛化效果不佳。研究发现，不同底层信息对不同类型的伪造图像具有不同的泛化能力。为提高检测各种伪造类型的能力，提出了自适应底层专家注入（ALEI）框架。该框架引入Lora专家，使训练有高级语义RGB图像的背景网络能够接受并学习各种底层信息的知识。使用跨注意力方法在中间层自适应融合这些特征。为防止背景网络在后期建模中失去对底层特征的建模能力，开发了底层信息适配器，与背景网络提取的特征进行交互。最后，提出了动态特征选择，能够动态选择最适合检测当前图像的特征，以最大化泛化检测能力。实验表明，该方法在多个包含未见过的GAN和扩散方法的数据集上表现优异，达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前AI生成图像检测主要依赖RGB图像的底层信息来提高泛化能力，但存在局限性。</li>
<li>不同底层信息对不同类型的伪造图像具有不同的泛化能力。</li>
<li>提出了ALEI框架，通过引入Lora专家结合底层和高层次信息。</li>
<li>使用跨注意力方法在中间层融合特征。</li>
<li>开发了底层信息适配器，保持对底层特征的建模能力。</li>
<li>动态特征选择能最大化泛化检测能力。</li>
<li>在多个数据集上实现优异性能，达到最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00463">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f7f5cd4ab23586ef487d5a14ffcc78f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f5186b30e80346598418f6afa89ae5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1efa71886031e529de5d78070607556f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Style-Quantization-for-Data-Efficient-GAN-Training"><a href="#Style-Quantization-for-Data-Efficient-GAN-Training" class="headerlink" title="Style Quantization for Data-Efficient GAN Training"></a>Style Quantization for Data-Efficient GAN Training</h2><p><strong>Authors:Jian Wang, Xin Lan, Jizhe Zhou, Yuxin Tian, Jiancheng Lv</strong></p>
<p>Under limited data setting, GANs often struggle to navigate and effectively exploit the input latent space. Consequently, images generated from adjacent variables in a sparse input latent space may exhibit significant discrepancies in realism, leading to suboptimal consistency regularization (CR) outcomes. To address this, we propose \textit{SQ-GAN}, a novel approach that enhances CR by introducing a style space quantization scheme. This method transforms the sparse, continuous input latent space into a compact, structured discrete proxy space, allowing each element to correspond to a specific real data point, thereby improving CR performance. Instead of direct quantization, we first map the input latent variables into a less entangled &#96;&#96;style’’ space and apply quantization using a learnable codebook. This enables each quantized code to control distinct factors of variation. Additionally, we optimize the optimal transport distance to align the codebook codes with features extracted from the training data by a foundation model, embedding external knowledge into the codebook and establishing a semantically rich vocabulary that properly describes the training dataset. Extensive experiments demonstrate significant improvements in both discriminator robustness and generation quality with our method. </p>
<blockquote>
<p>在有限数据设置下，生成对抗网络（GANs）往往难以导航并有效地利用输入潜在空间。因此，从稀疏输入潜在空间中的相邻变量生成的图像在逼真度方面可能存在显著差异，从而导致次优的一致性正则化（CR）结果。为了解决这一问题，我们提出了SQ-GAN这一新方法，它通过引入风格空间量化方案来增强CR。该方法将稀疏、连续的输入潜在空间转换为紧凑、结构化的离散代理空间，使每个元素都能对应一个特定的真实数据点，从而提高CR性能。我们不是直接进行量化，而是首先将输入潜在变量映射到一个不那么纠缠的“风格”空间，并使用可学习的代码本进行量化。这使得每个量化的代码能够控制不同的变化因素。此外，我们优化了最佳传输距离，将代码本中的代码与基础模型从训练数据中提取的特征进行对齐，将外部知识嵌入到代码本中，建立了一个语义丰富的词汇表，适当地描述了训练数据集。大量实验证明，我们的方法在判别器鲁棒性和生成质量方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在有限数据环境下，生成对抗网络（GANs）在输入潜在空间的导航和有效探索方面常面临挑战。针对这一问题，本文提出了SQ-GAN，通过引入风格空间量化方案，增强一致性正则化（CR）的性能。该方法将稀疏的连续输入潜在空间转化为紧凑的结构化离散代理空间，使每个元素对应一个真实数据点，从而提高CR性能。实验表明，该方法在判别器鲁棒性和生成质量方面均有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在有限数据环境下，GANs在导航和有效探索输入潜在空间时面临挑战。</li>
<li>提出的SQ-GAN通过风格空间量化方案增强一致性正则化（CR）性能。</li>
<li>SQ-GAN将稀疏的连续输入潜在空间转化为紧凑的结构化离散代理空间。</li>
<li>该方法使每个量化代码对应一个真实数据点，提高CR性能。</li>
<li>采用学习代码本进行量化，使每个量化代码控制不同的变量因素。</li>
<li>通过优化传输距离，将代码本代码与基础模型提取的特征对齐，嵌入外部知识，建立丰富的语义词汇表。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b991e157764a9b779fa1b915f2277ca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges"><a href="#Deterministic-Medical-Image-Translation-via-High-fidelity-Brownian-Bridges" class="headerlink" title="Deterministic Medical Image Translation via High-fidelity Brownian   Bridges"></a>Deterministic Medical Image Translation via High-fidelity Brownian   Bridges</h2><p><strong>Authors:Qisheng He, Nicholas Summerfield, Peiyong Wang, Carri Glide-Hurst, Ming Dong</strong></p>
<p>Recent studies have shown that diffusion models produce superior synthetic images when compared to Generative Adversarial Networks (GANs). However, their outputs are often non-deterministic and lack high fidelity to the ground truth due to the inherent randomness. In this paper, we propose a novel High-fidelity Brownian bridge model (HiFi-BBrg) for deterministic medical image translations. Our model comprises two distinct yet mutually beneficial mappings: a generation mapping and a reconstruction mapping. The Brownian bridge training process is guided by the fidelity loss and adversarial training in the reconstruction mapping. This ensures that translated images can be accurately reversed to their original forms, thereby achieving consistent translations with high fidelity to the ground truth. Our extensive experiments on multiple datasets show HiFi-BBrg outperforms state-of-the-art methods in multi-modal image translation and multi-image super-resolution. </p>
<blockquote>
<p>最近的研究表明，与生成对抗网络（GANs）相比，扩散模型在生成合成图像方面表现更优越。然而，由于其内在的随机性，它们的输出通常是非确定的，并且对真实情况的保真度不高。在本文中，我们提出了一种用于确定性医学图像翻译的新型高保真布朗桥模型（HiFi-BBrg）。我们的模型包括两个独特而相互有益的映射：生成映射和重建映射。布朗桥训练过程由重建映射中的保真度损失和对抗性训练引导。这确保了翻译后的图像可以准确地恢复到其原始形式，从而实现与真实情况高度一致的翻译。我们在多个数据集上的广泛实验表明，HiFi-BBrg在跨模态图像翻译和多图像超分辨率方面优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22531v1">PDF</a> </p>
<p><strong>Summary</strong>：最近研究表明扩散模型在生成合成图像方面优于生成对抗网络（GANs），但其输出通常具有非确定性且对真实数据缺乏高保真度。本文提出了一种新型的高保真布朗桥模型（HiFi-BBrg）用于确定性医学图像转换。该模型包含两个独特且相互促进的映射：生成映射和重建映射。布朗桥训练过程由重建映射中的保真损失和对抗训练引导，确保翻译后的图像可以准确还原为原始形式，从而实现高保真度的一致翻译。在多个数据集上的实验表明，HiFi-BBrg在多模态图像转换和多图像超分辨率方面优于现有先进技术。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>扩散模型在生成合成图像方面表现出优越性，但存在非确定性和对真实数据缺乏高保真度的问题。</li>
<li>论文提出了一种新型的高保真布朗桥模型（HiFi-BBrg）用于医学图像转换。</li>
<li>HiFi-BBrg模型包含两个关键映射：生成映射和重建映射，这两个映射相互促进。</li>
<li>布朗桥训练过程通过重建映射中的保真损失和对抗训练来引导，确保图像翻译的准确性。</li>
<li>HiFi-BBrg能够实现高保真度的一致翻译，克服现有技术的不足。</li>
<li>在多个数据集上的实验表明，HiFi-BBrg在多模态图像转换和多图像超分辨率方面优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-89b720f2289ff49b5d424b1d1bb39466.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42c63a9a154920b748723ea6f60d03e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10b4c05035d61db74240beef8ac3ded3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74dd698f33980edc4ea7e2552f99da80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c4519fed2f33f938908a378ab11a3e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5c85f0176bb823177a0938160c03c26.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder"><a href="#Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder" class="headerlink" title="Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder"></a>Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder</h2><p><strong>Authors:Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang</strong></p>
<p>Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著的成绩。然而，在全新领域实现连续属性的精确控制，尤其是同时控制多个属性（例如，像眼睛睁开程度或汽车宽度这样的数值）仅通过文本指导仍然是一个巨大的挑战。为了解决这个问题，我们引入了属性（Att）适配器，这是一种新型即插即用模块，旨在在预训练的扩散模型中实现精细的多属性控制。我们的方法从一组样本图像中学习单个控制适配器，这些图像可以是未配对的，并包含多个视觉属性。Att-Adapter利用解耦交叉注意力模块，自然地协调多个域属性与文本条件。我们还将条件变分自动编码器（CVAE）引入到Att-Adapter中，以减轻过拟合问题，适应视觉世界的多样性。在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线。此外，我们的方法扩大了控制范围，并改善了多个属性之间的解纠缠，超越了基于StyleGAN的技术。值得注意的是，Att-Adapter非常灵活，无需配对合成数据进行训练，并且很容易在单个模型内扩展到多个属性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11937v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型已生成高质量图像方面取得了显著成效。然而，使用纯文本指导在新的领域（如眼睛睁开程度或汽车宽度等数值）实现连续属性的精确控制，尤其是同时控制多个属性，仍然是一个重大挑战。为了解决这个问题，我们引入了属性适配器（Att-Adapter），这是一种新型即插即用模块，旨在在预训练的扩散模型中实现精细粒度的多属性控制。我们的方法从一组未配对的包含多个视觉属性的样本图像中学习单个控制适配器。Att-Adapter利用解耦交叉注意力模块，自然地协调文本条件与多个域属性。我们还将条件变分自编码器（CVAE）引入到Att-Adapter中，以减轻过拟合问题，适应视觉世界的多样性。在公开数据集上的评估显示，Att-Adapter在控制连续属性方面优于所有基于LoRA的方法。此外，我们的方法扩大了控制范围，并改善了多个属性之间的解纠缠，超越了StyleGAN技术。值得注意的是，Att-Adapter非常灵活，无需配对合成数据进行训练，且可以轻松扩展到单个模型内的多个属性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I扩散模型在生成高质量图像方面表现出显著性能。</li>
<li>实现新领域中多个属性的精确控制是一大挑战。</li>
<li>属性适配器（Att-Adapter）是一种新型模块，旨在解决这一挑战，实现精细粒度的多属性控制。</li>
<li>Att-Adapter能从未配对的样本图像中学习。</li>
<li>利用解耦交叉注意力模块和条件变分自编码器（CVAE）实现更自然的属性协调与适应视觉世界的多样性。</li>
<li>Att-Adapter在控制连续属性方面优于基于LoRA的方法，并能扩大控制范围，改善多个属性间的解纠缠。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-50629aed4fae0a17a1c42ccc011f8cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d986435c039237a7e970cd0226135aa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-875e25fedbf0e4b14a9f58e93517bcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf7c2ca605970f534731b4f292f64f6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommodate a situation when a participant’s consent is withdrawn). Synthetic data, free from these constraints, is an appealing alternative. Third, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to SOTA monocular and GAN-based methods, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>我们提出了一种名为SynShot的新方法，用于基于合成先验的少数镜头驱动头部化身倒置。我们解决了三个主要挑战。首先，训练可控的3D生成网络需要大量的不同序列，而这些序列的图像和高质量跟踪网格并不总是可用。其次，真实数据的使用受到严格监管（例如，根据《通用数据保护条例》，当参与者同意撤回时，需要频繁删除模型和数删除模型和数据据）。不受这些约束的合成数据是一个吸引人的选择。第三，最先进的单目化身模型难以推广到新的视角和表情，缺乏强大的先验知识，并且经常过度适应特定的视角分布。受仅使用合成数据训练的机器学习模型的启发，我们提出了一种方法，该方法从包含不同身份、表情和视角的大量合成头部数据中学习先验模型。凭借少数输入图像，SynShot微调了预训练的合成先验，以弥合领域之间的差距，建模一个对新型表情和视角通用的逼真头部化身。我们使用3D高斯贴图技术和卷积编码器-解码器对头部化身进行建模，该编码器-解码器输出UV纹理空间的高斯参数。为了考虑头部各部分的建模复杂性（例如皮肤和头发），我们通过嵌入先验来明确控制每个部分的原始数量。与最先进的单目和基于GAN的方法相比，SynShot在新型视角和表情合成方面有了显着提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v3">PDF</a> Accepted to CVPR25 Website: <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了SynShot方法，这是一种基于合成先验的少数镜头即可驱动头部化身反演技术的新方法。该方法解决了三大挑战：缺乏多样序列图像与高质量追踪网格的训练数据、真实数据使用受限以及当前单眼化身模型在新视角和表情下的泛化能力弱。通过从大量合成头部数据中学习先验模型，SynShot只需少量输入图像即可微调预训练的合成先验，以缩小领域差距，并建立一个能泛化到新表情和视角的光照头部化身。该方法采用三维高斯贴片技术和卷积编码器-解码器建模头部化身，输出UV纹理空间的高斯参数。为了处理头部不同部分的建模复杂性（如皮肤和头发），该方法具有对每个部分原始数量的上采样进行显式控制的能力。相较于当前单眼和基于GAN的方法，SynShot极大地提升了新型视角和表情的合成效果。</p>
<p><strong>Key Takeaways</strong></p>
<p>一、SynShot是一种基于合成先验的少数镜头即可驱动头部化身反演技术的新方法。<br>二、该方法解决了训练3D生成网络所面临的三大挑战：缺乏多样序列图像与高质量追踪网格的训练数据、真实数据使用受限以及单眼化身模型在新视角和表情下的泛化难题。<br>三、通过从合成头部数据中学习先验模型，SynShot能够利用少量输入图像微调预训练模型，生成泛化性强的光照头部化身。<br>四、采用三维高斯贴片技术和卷积编码器-解码器进行头部化身建模，提升新型视角和表情的合成效果。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d0bd43aa12bbc98f505f9bcecfbb35ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c6c599bcc7d81e1ada08fc3bb3d40bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00244ec435f68322181bf9ee515280f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4f38e20d8fd6d3e48e671855e82f200.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-950e2b8244c0fac09198e2129caece27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2bfaf37bfdfe73c4a5d22dd4d3c8c0a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Singular-Value-Scaling-Efficient-Generative-Model-Compression-via-Pruned-Weights-Refinement"><a href="#Singular-Value-Scaling-Efficient-Generative-Model-Compression-via-Pruned-Weights-Refinement" class="headerlink" title="Singular Value Scaling: Efficient Generative Model Compression via   Pruned Weights Refinement"></a>Singular Value Scaling: Efficient Generative Model Compression via   Pruned Weights Refinement</h2><p><strong>Authors:Hyeonjin Kim, Jaejun Yoo</strong></p>
<p>While pruning methods effectively maintain model performance without extra training costs, they often focus solely on preserving crucial connections, overlooking the impact of pruned weights on subsequent fine-tuning or distillation, leading to inefficiencies. Moreover, most compression techniques for generative models have been developed primarily for GANs, tailored to specific architectures like StyleGAN, and research into compressing Diffusion models has just begun. Even more, these methods are often applicable only to GANs or Diffusion models, highlighting the need for approaches that work across both model types. In this paper, we introduce Singular Value Scaling (SVS), a versatile technique for refining pruned weights, applicable to both model types. Our analysis reveals that pruned weights often exhibit dominant singular vectors, hindering fine-tuning efficiency and leading to suboptimal performance compared to random initialization. Our method enhances weight initialization by minimizing the disparities between singular values of pruned weights, thereby improving the fine-tuning process. This approach not only guides the compressed model toward superior solutions but also significantly speeds up fine-tuning. Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS improves compression performance across model types without additional training costs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/LAIT-CVLab/Singular-Value-Scaling">https://github.com/LAIT-CVLab/Singular-Value-Scaling</a>. </p>
<blockquote>
<p>虽然剪枝方法可以有效地保持模型性能而无需额外的训练成本，但它们通常只专注于保留关键连接，而忽视了剪枝权重对后续微调或蒸馏的影响，从而导致效率低下。此外，针对生成模型的压缩技术主要是为生成对抗网络（GANs）开发的，专为特定的架构（如StyleGAN）定制，而对Diffusion模型的压缩研究才刚刚开始。而且，这些方法往往仅适用于GANs或Diffusion模型，这突显了对在这两种模型类型中都适用的方法的需求。在本文中，我们介绍了奇异值缩放（Singular Value Scaling, SVS），这是一种精炼剪枝权重的通用技术，适用于这两种模型类型。我们的分析表明，剪枝权重通常表现出主要的奇异向量，阻碍微调效率，导致与随机初始化相比性能不佳。我们的方法通过最小化剪枝权重的奇异值之间的差异来增强权重初始化，从而改进微调过程。这种方法不仅引导压缩模型达到更优的解决方案，而且还大大加快了微调速度。在StyleGAN2、StyleGAN3和DDPM上的广泛实验表明，SVS提高了各种模型的压缩性能，且无需额外的训练成本。我们的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/LAIT-CVlab/Singular-Value-Scaling">https://github.com/LAIT-CVlab/Singular-Value-Scaling</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17387v3">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为奇异值缩放（SVS）的技术，该技术旨在优化修剪权重，并适用于生成模型中的GAN和扩散模型。分析表明，修剪的权重往往表现出显著的奇异向量，影响微调效率并导致性能下降。SVS技术通过最小化修剪权重的奇异值差异，改进权重初始化，从而提高微调过程效率，引导压缩模型达到更好的解决方案，同时加快微调速度。实验证明，SVS技术在StyleGAN2、StyleGAN3和DDPM上均能提高压缩性能，且无需额外的训练成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>修剪方法虽能有效维持模型性能并节省训练成本，但往往只关注保留关键连接，忽视了修剪权重对后续微调或蒸馏的影响，导致效率不高。</li>
<li>大多数生成模型的压缩技术主要针对GANs和特定的架构（如StyleGAN）开发，对扩散模型的压缩研究才刚刚开始。</li>
<li>目前的方法往往只适用于GANs或扩散模型，需要开发适用于两种模型类型的方法。</li>
<li>本文提出了奇异值缩放（SVS）技术，这是一种优化修剪权重的通用方法，适用于GANs和扩散模型。</li>
<li>分析显示，修剪的权重具有显著的奇异向量，这会影响微调效率并导致性能下降。</li>
<li>SVS技术通过最小化修剪权重的奇异值差异，改进了权重的初始化，从而提高了微调过程的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cdcb59522aa39379132cc98d65b497ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b22610d8a4b4132bf3b8eb9c0ae9f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c056a7695e123f11901bf9674a6943c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0791ccc6a2153eb93236d15332918536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f588bca72b144b936ab08dce0375678b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-414b1d39995f2454b12679ac03d13d69.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-04-04  FRAME Floor-aligned Representation for Avatar Motion from Egocentric   Video
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-20f7a357be35bf0e24ef0897b3f51b03.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-04-04  Prompting Forgetting Unlearning in GANs via Textual Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18723.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
