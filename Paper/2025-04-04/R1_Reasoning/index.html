<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  FortisAVQA and MAVEN a Benchmark Dataset and Debiasing Framework for   Robust Multimodal Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f564bc8a70aca3fa18fce60f6bb5e4f8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="FortisAVQA-and-MAVEN-a-Benchmark-Dataset-and-Debiasing-Framework-for-Robust-Multimodal-Reasoning"><a href="#FortisAVQA-and-MAVEN-a-Benchmark-Dataset-and-Debiasing-Framework-for-Robust-Multimodal-Reasoning" class="headerlink" title="FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for   Robust Multimodal Reasoning"></a>FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for   Robust Multimodal Reasoning</h2><p><strong>Authors:Jie Ma, Zhitao Gao, Qi Chai, Jun Liu, Pinghui Wang, Jing Tao, Zhou Su</strong></p>
<p>Audio-Visual Question Answering (AVQA) is a challenging multimodal reasoning task requiring intelligent systems to answer natural language queries based on paired audio-video inputs accurately. However, existing AVQA approaches often suffer from overfitting to dataset biases, leading to poor robustness. Moreover, current datasets may not effectively diagnose these methods. To address these challenges, we first introduce a novel dataset, FortisAVQA, constructed in two stages: (1) rephrasing questions in the test split of the public MUSIC-AVQA dataset and (2) introducing distribution shifts across questions. The first stage expands the test space with greater diversity, while the second enables a refined robustness evaluation across rare, frequent, and overall question distributions. Second, we introduce a robust Multimodal Audio-Visual Epistemic Network (MAVEN) that leverages a multifaceted cycle collaborative debiasing strategy to mitigate bias learning. Experimental results demonstrate that our architecture achieves state-of-the-art performance on FortisAVQA, with a notable improvement of 7.81%. Extensive ablation studies on both datasets validate the effectiveness of our debiasing components. Additionally, our evaluation reveals the limited robustness of existing multimodal QA methods. We also verify the plug-and-play capability of our strategy by integrating it with various baseline models across both datasets. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/reml-group/fortisavqa">https://github.com/reml-group/fortisavqa</a>. </p>
<blockquote>
<p>éŸ³é¢‘è§†è§‰é—®ç­”ï¼ˆAVQAï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ç³»ç»Ÿæ ¹æ®é…å¯¹çš„éŸ³é¢‘è§†é¢‘è¾“å…¥å‡†ç¡®å›ç­”è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AVQAæ–¹æ³•å¸¸å¸¸é­å—æ•°æ®é›†åè§çš„è¿‡åº¦æ‹Ÿåˆå½±å“ï¼Œå¯¼è‡´ç¨³å¥æ€§è¾ƒå·®ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†å¯èƒ½æ— æ³•æœ‰æ•ˆåœ°è¯Šæ–­è¿™äº›æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†FortisAVQAï¼Œå®ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µæ„å»ºï¼šï¼ˆ1ï¼‰é‡æ–°è¡¨è¿°å…¬å¼€MUSIC-AVQAæ•°æ®é›†æµ‹è¯•åˆ†å‰²ä¸­çš„é—®é¢˜ï¼›ï¼ˆ2ï¼‰å¼•å…¥é—®é¢˜é—´çš„åˆ†å¸ƒåç§»ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¢åŠ æ›´å¤šå¤šæ ·æ€§æ¥æ‰©å¤§æµ‹è¯•ç©ºé—´ï¼Œè€Œç¬¬äºŒé˜¶æ®µåˆ™èƒ½é’ˆå¯¹ç½•è§ã€å¸¸è§å’Œæ€»ä½“é—®é¢˜åˆ†å¸ƒè¿›è¡Œç²¾ç»†çš„ç¨³å¥æ€§è¯„ä¼°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨³å¥çš„å¤šæ¨¡æ€éŸ³é¢‘è§†è§‰è®¤çŸ¥ç½‘ç»œï¼ˆMAVENï¼‰ï¼Œå®ƒåˆ©ç”¨å¤šé¢å¾ªç¯åä½œå»åç­–ç•¥æ¥ç¼“è§£åç½®å­¦ä¹ ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨FortisAVQAä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œæœ‰7.81%çš„æ˜¾è‘—æ”¹å–„ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„å»åç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•çš„æœ‰é™ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šä¸å„ç§åŸºçº¿æ¨¡å‹é›†æˆï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„ç­–ç•¥çš„å³æ’å³ç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/reml-group/fortisavqa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/reml-group/fortisavqaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00487v2">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘è§†è§‰é—®ç­”ï¼ˆAVQAï¼‰çš„æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨è¿‡åº¦æ‹Ÿåˆæ•°æ®é›†åå·®çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†æ–°å‹æ•°æ®é›†FortisAVQAä¸ç¨³å¥çš„å¤šæ¨¡æ€éŸ³é¢‘è§†è§‰è®¤çŸ¥ç½‘ç»œï¼ˆMAVENï¼‰ã€‚FortisAVQAåˆ†ä¸¤ä¸ªé˜¶æ®µæ„å»ºï¼ŒåŒ…æ‹¬é‡æ–°è¡¨è¿°é—®é¢˜å’Œå¼•å…¥åˆ†å¸ƒåç§»ã€‚MAVENé‡‡ç”¨å¤šé¢å¾ªç¯åä½œå»åç­–ç•¥ï¼Œä»¥å‡è½»åè¯¯å­¦ä¹ ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMAVENåœ¨FortisAVQAä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ”¹è¿›ç‡è¾¾7.81%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•çš„æœ‰é™ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰é—®ç­”ï¼ˆAVQAï¼‰æ˜¯ä¸€ä¸ªè¦æ±‚æ™ºèƒ½ç³»ç»ŸåŸºäºé…å¯¹éŸ³è§†é¢‘è¾“å…¥å‡†ç¡®å›ç­”è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰AVQAæ–¹æ³•å­˜åœ¨è¿‡åº¦æ‹Ÿåˆæ•°æ®é›†åå·®çš„é—®é¢˜ï¼Œå¯¼è‡´ç¨³å¥æ€§è¾ƒå·®ã€‚</li>
<li>FortisAVQAæ•°æ®é›†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µæ„å»ºï¼šé‡æ–°è¡¨è¿°é—®é¢˜å’Œå¼•å…¥åˆ†å¸ƒåç§»ï¼Œä»¥æ‰©å¤§æµ‹è¯•ç©ºé—´å¹¶æå‡ç¨³å¥æ€§è¯„ä¼°çš„ç²¾ç»†åº¦ã€‚</li>
<li>MAVENåˆ©ç”¨å¤šé¢å¾ªç¯åä½œå»åç­–ç•¥ï¼Œæœ‰æ•ˆå‡è½»åè¯¯å­¦ä¹ ã€‚</li>
<li>MAVENåœ¨FortisAVQAä¸Šå®ç°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæ”¹è¿›ç‡è¾¾7.81%ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†å»åç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶å¯¹ç°æœ‰å¤šæ¨¡æ€é—®ç­”æ–¹æ³•çš„æœ‰é™ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0256c2c85914a452081c9034b6d3301a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48bdd66daf76e8560aa93c1fda68c877.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b50a4cf06b648a2d4039e8467ea9cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d92526b128d27f980e9f635e2c4b77d6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLM-Assisted-Proactive-Threat-Intelligence-for-Automated-Reasoning"><a href="#LLM-Assisted-Proactive-Threat-Intelligence-for-Automated-Reasoning" class="headerlink" title="LLM-Assisted Proactive Threat Intelligence for Automated Reasoning"></a>LLM-Assisted Proactive Threat Intelligence for Automated Reasoning</h2><p><strong>Authors:Shuva Paul, Farhad Alemi, Richard Macwan</strong></p>
<p>Successful defense against dynamically evolving cyber threats requires advanced and sophisticated techniques. This research presents a novel approach to enhance real-time cybersecurity threat detection and response by integrating large language models (LLMs) and Retrieval-Augmented Generation (RAG) systems with continuous threat intelligence feeds. Leveraging recent advancements in LLMs, specifically GPT-4o, and the innovative application of RAG techniques, our approach addresses the limitations of traditional static threat analysis by incorporating dynamic, real-time data sources. We leveraged RAG to get the latest information in real-time for threat intelligence, which is not possible in the existing GPT-4o model. We employ the Patrowl framework to automate the retrieval of diverse cybersecurity threat intelligence feeds, including Common Vulnerabilities and Exposures (CVE), Common Weakness Enumeration (CWE), Exploit Prediction Scoring System (EPSS), and Known Exploited Vulnerabilities (KEV) databases, and integrate these with the all-mpnet-base-v2 model for high-dimensional vector embeddings, stored and queried in Milvus. We demonstrate our systemâ€™s efficacy through a series of case studies, revealing significant improvements in addressing recently disclosed vulnerabilities, KEVs, and high-EPSS-score CVEs compared to the baseline GPT-4o. This work not only advances the role of LLMs in cybersecurity but also establishes a robust foundation for the development of automated intelligent cyberthreat information management systems, addressing crucial gaps in current cybersecurity practices. </p>
<blockquote>
<p>æˆåŠŸæŠµå¾¡åŠ¨æ€æ¼”å˜çš„ç½‘ç»œå¨èƒéœ€è¦å…ˆè¿›ä¸”å¤æ‚çš„æŠ€æœ¯ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡é›†æˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸æŒç»­å¨èƒæƒ…æŠ¥é¦ˆé€ï¼Œå¢å¼ºå®æ—¶ç½‘ç»œå®‰å…¨å¨èƒæ£€æµ‹å’Œå“åº”ã€‚æˆ‘ä»¬å€ŸåŠ©LLMçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯GPT-4oï¼Œä»¥åŠRAGæŠ€æœ¯çš„åˆ›æ–°åº”ç”¨ï¼Œé€šè¿‡èå…¥åŠ¨æ€ã€å®æ—¶æ•°æ®æºï¼Œå…‹æœäº†ä¼ ç»Ÿé™æ€å¨èƒåˆ†æçš„å±€é™æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨RAGå®æ—¶è·å–æœ€æ–°çš„å¨èƒæƒ…æŠ¥ä¿¡æ¯ï¼Œè¿™æ˜¯ç°æœ‰GPT-4oæ¨¡å‹æ— æ³•åšåˆ°çš„ã€‚æˆ‘ä»¬é‡‡ç”¨Patrowlæ¡†æ¶è‡ªåŠ¨æ£€ç´¢å¤šç§ç½‘ç»œå®‰å…¨å¨èƒæƒ…æŠ¥é¦ˆé€ï¼ŒåŒ…æ‹¬é€šç”¨æ¼æ´å’Œæš´éœ²ï¼ˆCVEï¼‰ã€é€šç”¨å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰ã€æ¼æ´åˆ©ç”¨é¢„æµ‹è¯„åˆ†ç³»ç»Ÿï¼ˆEPSSï¼‰å’Œå·²çŸ¥æ¼æ´ï¼ˆKEVï¼‰æ•°æ®åº“ï¼Œå¹¶å°†å…¶ä¸all-mpnet-base-v2æ¨¡å‹é›†æˆç”¨äºé«˜ç»´å‘é‡åµŒå…¥ï¼Œè¯¥åµŒå…¥å­˜å‚¨åœ¨Milvusä¸­å¹¶å¯è¿›è¡ŒæŸ¥è¯¢ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œä¸åŸºçº¿GPT-4oç›¸æ¯”ï¼Œåœ¨è§£å†³æœ€è¿‘æŠ«éœ²çš„æ¼æ´ã€KEVå’Œé«˜EPSSåˆ†æ•°CVEæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ¨è¿›äº†LLMåœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ä½œç”¨ï¼Œè€Œä¸”ä¸ºå¼€å‘è‡ªåŠ¨åŒ–çš„æ™ºèƒ½ç½‘ç»œå¨èƒä¿¡æ¯ç®¡ç†ç³»ç»Ÿçš„å¼€å‘å¥ å®šäº†åšå®åŸºç¡€ï¼Œè§£å†³äº†å½“å‰ç½‘ç»œå®‰å…¨å®è·µä¸­çš„å…³é”®ç©ºç™½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00428v1">PDF</a> 10 Pages, 1 Figure</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå®æ—¶å¢å¼ºç½‘ç»œå®‰å…¨å¨èƒæ£€æµ‹å’Œå“åº”çš„æ–°å‹æ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡èå…¥åŠ¨æ€å®æ—¶æ•°æ®æºï¼Œè§£å†³äº†ä¼ ç»Ÿé™æ€å¨èƒåˆ†æçš„å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨RAGæŠ€æœ¯å®æ—¶è·å–å¨èƒæƒ…æŠ¥ï¼Œé‡‡ç”¨Patrowlæ¡†æ¶è‡ªåŠ¨åŒ–æ£€ç´¢ç½‘ç»œå®‰å…¨å¨èƒæƒ…æŠ¥æºï¼Œå¹¶æ•´åˆé«˜ç»´å‘é‡åµŒå…¥æ¨¡å‹all-mpnet-base-v2ï¼Œå­˜å‚¨åœ¨Milvusä¸­è¿›è¡ŒæŸ¥è¯¢ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤„ç†æœ€æ–°æŠ«éœ²çš„æ¼æ´ã€å·²çŸ¥æ¼æ´å’Œé«˜EPSSåˆ†æ•°çš„CVEæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œä¸ä»…æ¨åŠ¨äº†LLMsåœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ï¼Œè¿˜ä¸ºå¼€å‘è‡ªåŠ¨åŒ–æ™ºèƒ½ç½‘ç»œå¨èƒä¿¡æ¯ç®¡ç†ç³»ç»Ÿçš„ç¨³å¥åŸºç¡€å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸæœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ç”¨äºå¢å¼ºå®æ—¶ç½‘ç»œå®‰å…¨å¨èƒæ£€æµ‹å’Œå“åº”ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡èå…¥åŠ¨æ€å®æ—¶æ•°æ®æºè§£å†³äº†ä¼ ç»Ÿé™æ€å¨èƒåˆ†æçš„å±€é™æ€§ã€‚</li>
<li>RAGæŠ€æœ¯è¢«ç”¨äºå®æ—¶è·å–å¨èƒæƒ…æŠ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„åˆ›æ–°ç‚¹ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿä½¿ç”¨Patrowlæ¡†æ¶è‡ªåŠ¨åŒ–æ£€ç´¢å¤šç§ç½‘ç»œå®‰å…¨å¨èƒæƒ…æŠ¥æºï¼Œæé«˜äº†æƒ…æŠ¥è·å–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆall-mpnet-base-v2æ¨¡å‹çš„é«˜ç»´å‘é‡åµŒå…¥æŠ€æœ¯ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„ç½‘ç»œå®‰å…¨æ•°æ®ã€‚</li>
<li>æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿçš„å¤„ç†æ€§èƒ½åœ¨å¤„ç†æœ€æ–°æŠ«éœ²çš„æ¼æ´å’Œå·²çŸ¥æ¼æ´æ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20357570ed8ae997ff94820dc19e1d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23e4decc84f1b7ba6ea09f4830f04638.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b626e4a214982fab38196172ed239aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5184dd2f981f3b845a3be43334de2420.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Semantic-Mastery-Enhancing-LLMs-with-Advanced-Natural-Language-Understanding"><a href="#Semantic-Mastery-Enhancing-LLMs-with-Advanced-Natural-Language-Understanding" class="headerlink" title="Semantic Mastery: Enhancing LLMs with Advanced Natural Language   Understanding"></a>Semantic Mastery: Enhancing LLMs with Advanced Natural Language   Understanding</h2><p><strong>Authors:Mohanakrishnan Hariharan</strong></p>
<p>Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‰§è¡ŒNLPä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢æœ‰äº†å¾ˆå¤§æé«˜ã€‚ç„¶è€Œï¼Œæ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰ç†è§£ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œæ›´å¾®å¦™çš„æ¨ç†ä»ç„¶éš¾ä»¥è·å¾—ã€‚æœ¬æ–‡è®¨è®ºäº†é‡‡ç”¨æœ€å…ˆè¿›çš„çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ›´å…ˆè¿›çš„NLUæŠ€æœ¯æ¨è¿›LLMsçš„å‘å±•ï¼Œå¦‚è¯­ä¹‰è§£æã€çŸ¥è¯†é›†æˆå’Œä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬åˆ†æäº†ä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†å›¾è°±ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»¥åŠä¸äººç±»æ°´å¹³ç†è§£ç›¸åŒ¹é…çš„å¾®è°ƒç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†åŸºäºè½¬æ¢å™¨çš„æ¶æ„ã€å¯¹æ¯”å­¦ä¹ å’Œæ··åˆç¬¦å·-ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Œè§£å†³åœ¨æ‰§è¡Œå¤æ‚NLPä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯ç”Ÿæˆï¼‰æ—¶æ¶‰åŠçš„äº‹å®è§†è§’çš„å¹»è§‰ã€æ¨¡ç³Šå’Œä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜è¯­ä¹‰ç²¾åº¦å¯¹å¢å¼ºAIé©±åŠ¨çš„è¯­è¨€ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†ç¼©å°ç»Ÿè®¡è¯­è¨€æ¨¡å‹å’ŒçœŸæ­£çš„è‡ªç„¶è¯­è¨€ç†è§£ä¹‹é—´å·®è·çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00409v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨NLPä»»åŠ¡ä¸­èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œä½†åœ¨æ·±å±‚è¯­ä¹‰ç†è§£ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œæ›´å¾®å¦™çš„æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚è®ºæ–‡è®¨è®ºäº†ä½¿ç”¨æ›´å…ˆè¿›çš„NLUæŠ€æœ¯ï¼Œå¦‚è¯­ä¹‰è§£æã€çŸ¥è¯†é›†æˆå’Œä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•æ¥æé«˜LLMçš„æ€§èƒ½ã€‚è®ºæ–‡è¿˜åˆ†æäº†ç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ä¸äººçš„ç†è§£ç›¸åŒ¹é…çš„å¾®è°ƒç­–ç•¥çš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æ¢è®¨äº†åŸºäºè½¬æ¢å™¨çš„æ¶æ„ã€å¯¹æ¯”å­¦ä¹ å’Œæ··åˆç¬¦å·ç¥ç»æ–¹æ³•çš„é‡è¦æ€§ï¼Œä»¥è§£å†³åœ¨æ‰§è¡Œå¤æ‚NLPä»»åŠ¡æ—¶æ¶‰åŠçš„äº‹å®è§‚ç‚¹çš„é—®é¢˜ï¼Œå¦‚é—®ç­”ã€æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯ç”Ÿæˆä¸­çš„è™šæ„ã€æ¨¡ç³Šå’Œä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜è¯­ä¹‰ç²¾ç¡®æ€§å¯¹å¢å¼ºAIé©±åŠ¨çš„è¯­è¨€ç³»ç»Ÿçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ï¼Œä»¥ç¼©å°ç»Ÿè®¡è¯­è¨€æ¨¡å‹ä¸çœŸæ­£çš„è‡ªç„¶è¯­è¨€ç†è§£ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨NLPä»»åŠ¡ä¸­ä»æœ‰æ·±å±‚è¯­ä¹‰ç†è§£ã€ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œæ¨ç†çš„æŒ‘æˆ˜ã€‚</li>
<li>å…ˆè¿›çš„NLUæŠ€æœ¯ï¼Œå¦‚è¯­ä¹‰è§£æã€çŸ¥è¯†é›†æˆå’Œä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ ï¼Œè¢«ç”¨æ¥æé«˜LLMæ€§èƒ½ã€‚</li>
<li>ç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¸äººçš„ç†è§£ç›¸åŒ¹é…çš„å¾®è°ƒç­–ç•¥æ˜¯å…³é”®æ–¹æ³•ã€‚</li>
<li>åŸºäºè½¬æ¢å™¨çš„æ¶æ„ã€å¯¹æ¯”å­¦ä¹ å’Œæ··åˆç¬¦å·ç¥ç»æ–¹æ³•å¯¹äºè§£å†³å¤æ‚NLPä»»åŠ¡ä¸­çš„é—®é¢˜å¾ˆé‡è¦ã€‚</li>
<li>è¯­ä¹‰ç²¾ç¡®æ€§å¯¹å¢å¼ºAIé©±åŠ¨çš„è¯­è¨€ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†è§£å†³è™šæ„ã€æ¨¡ç³Šå’Œä¸ä¸€è‡´æ€§é—®é¢˜çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6e3abd07e1b2d089fde4a9b1ccc5362.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073db9337469068d21851777bb23e352.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8dfb86e506e48c979df1018054393f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f58bb221f44b57b5fc37fcc4157686f6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VerifiAgent-a-Unified-Verification-Agent-in-Language-Model-Reasoning"><a href="#VerifiAgent-a-Unified-Verification-Agent-in-Language-Model-Reasoning" class="headerlink" title="VerifiAgent: a Unified Verification Agent in Language Model Reasoning"></a>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</h2><p><strong>Authors:Jiuzhou Han, Wray Buntine, Ehsan Shareghi</strong></p>
<p>Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Jiuzhouh/VerifiAgent">https://github.com/Jiuzhouh/VerifiAgent</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç»å¸¸äº§ç”Ÿä¸å¯é æˆ–é”™è¯¯çš„å›åº”ã€‚ç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸æ˜¯æ¨¡å‹ç‰¹å®šæˆ–é¢†åŸŸé™åˆ¶çš„ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨ç†ä»»åŠ¡ä¸­ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†VerifiAgentï¼Œä¸€ä¸ªç»Ÿä¸€çš„éªŒè¯ä»£ç†ï¼Œå®ƒé›†æˆäº†ä¸¤ä¸ªå±‚æ¬¡çš„éªŒè¯ï¼šå…ƒéªŒè¯ï¼Œè¯„ä¼°æ¨¡å‹å“åº”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼›ä»¥åŠåŸºäºå·¥å…·çš„è‡ªé€‚åº”éªŒè¯ï¼Œå…¶ä¸­VerifiAgentæ ¹æ®æ¨ç†ç±»å‹ï¼ˆåŒ…æ‹¬æ•°å­¦ã€é€»è¾‘æˆ–å¸¸è¯†æ¨ç†ï¼‰è‡ªä¸»åœ°é€‰æ‹©é€‚å½“çš„éªŒè¯å·¥å…·ã€‚è¿™ç§è‡ªé€‚åº”æ–¹æ³•ç¡®ä¿äº†ä¸åŒéªŒè¯åœºæ™¯ä¸‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¼˜äºåŸºå‡†éªŒè¯æ–¹æ³•ï¼ˆä¾‹å¦‚æ¼”ç»éªŒè¯å™¨ã€å‘åéªŒè¯å™¨ï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœçš„åé¦ˆè¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚VerifiAgentè¿˜å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºæ¨ç†æ‰©å±•ï¼Œä¸æ•°å­¦æ¨ç†é¢†åŸŸä¸­çš„ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æ›´å°‘çš„ç”Ÿæˆæ ·æœ¬å’Œæˆæœ¬å®ç°æ›´å¥½çš„ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jiuzhouh/VerifiAgent">https://github.com/Jiuzhouh/VerifiAgent</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00406v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å“åº”å¯èƒ½ä¸å¯é æˆ–å‡ºé”™ã€‚ç°æœ‰çš„éªŒè¯æ–¹æ³•é€šå¸¸æ¨¡å‹ç‰¹å®šæˆ–å±€é™äºç‰¹å®šé¢†åŸŸï¼Œéœ€è¦å¤§é‡è®¡ç®—èµ„æºï¼Œä¸”åœ¨è·¨ä¸åŒæ¨ç†ä»»åŠ¡æ—¶ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºVerifiAgentç»Ÿä¸€éªŒè¯ä»£ç†ï¼Œå®ƒé›†æˆäº†ä¸¤ä¸ªçº§åˆ«çš„éªŒè¯ï¼šå…ƒéªŒè¯è¯„ä¼°æ¨¡å‹å“åº”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ï¼Œå·¥å…·è‡ªé€‚åº”éªŒè¯åˆ™æ ¹æ®æ¨ç†ç±»å‹è‡ªåŠ¨é€‰æ‹©é€‚å½“çš„éªŒè¯å·¥å…·ï¼ŒåŒ…æ‹¬æ•°å­¦ã€é€»è¾‘å’Œå¸¸è¯†æ¨ç†ç­‰ã€‚æ­¤æ–¹æ³•ç¡®ä¿äº†ä¸åŒéªŒè¯åœºæ™¯ä¸‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVerifiAgentåœ¨æ‰€æœ‰æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºåŸºçº¿éªŒè¯æ–¹æ³•ï¼Œå¹¶èƒ½é€šè¿‡åˆ©ç”¨éªŒè¯ç»“æœåé¦ˆè¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ•°å­¦æ¨ç†é¢†åŸŸçš„åº”ç”¨å¯ä»¥å®ç°è¾ƒå°‘çš„ç”Ÿæˆæ ·æœ¬å’Œæˆæœ¬ï¼Œä¼˜äºç°æœ‰çš„æµç¨‹å¥–åŠ±æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è™½å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å“åº”å¯èƒ½å­˜åœ¨ä¸å¯é æˆ–é”™è¯¯çš„æƒ…å†µã€‚</li>
<li>ç°æœ‰éªŒè¯æ–¹æ³•é€šå¸¸å…·æœ‰æ¨¡å‹ç‰¹å®šæ€§æˆ–å±€é™äºç‰¹å®šé¢†åŸŸï¼Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—å¤§ï¼Œç¼ºä¹è·¨ä¸åŒæ¨ç†ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>VerifiAgentæ˜¯ä¸€ç§ç»Ÿä¸€éªŒè¯ä»£ç†ï¼Œé€šè¿‡å…ƒéªŒè¯å’Œå·¥å…·è‡ªé€‚åº”éªŒè¯ä¸¤ä¸ªçº§åˆ«çš„æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>å…ƒéªŒè¯è¯„ä¼°æ¨¡å‹å“åº”çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>å·¥å…·è‡ªé€‚åº”éªŒè¯æ ¹æ®æ¨ç†ç±»å‹è‡ªåŠ¨é€‰æ‹©é€‚å½“çš„éªŒè¯å·¥å…·ã€‚</li>
<li>VerifiAgentåœ¨è·¨ä¸åŒæ¨ç†ä»»åŠ¡æ—¶è¡¨ç°å‡ºä¼˜å¼‚çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚</li>
<li>VerifiAgentçš„å®éªŒç»“æœä¼˜äºåŸºçº¿éªŒè¯æ–¹æ³•ï¼Œå¹¶èƒ½è¿›ä¸€æ­¥æé«˜æ¨ç†å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†é¢†åŸŸå®ç°äº†è¾ƒå°‘çš„ç”Ÿæˆæ ·æœ¬å’Œæˆæœ¬ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db5392ab7a55fde392bbcb05c19c58a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-609a375cd65dd0e8e4d26393720af139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdca427370dd07709ad4083e1f9c641d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5403abcf0766c8702c65fdc6fdbcf35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07db16be276f245ab62af92d8c9fae33.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="JudgeLRM-Large-Reasoning-Models-as-a-Judge"><a href="#JudgeLRM-Large-Reasoning-Models-as-a-Judge" class="headerlink" title="JudgeLRM: Large Reasoning Models as a Judge"></a>JudgeLRM: Large Reasoning Models as a Judge</h2><p><strong>Authors:Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He</strong></p>
<p>The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°è€…çš„å…´èµ·ï¼Œå®ƒä¸ºäººç±»æ ‡æ³¨æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç”¨äºæ³•å®˜çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å¾€å¾€åœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸæ—¶è¡¨ç°ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥LLMæ³•å®˜æ˜¯å¦çœŸæ­£å—ç›Šäºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¯„ä¼°ä»»åŠ¡ä¸­çš„æ¨ç†éœ€æ±‚è¿›è¡Œè¯¦å°½åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç›‘ç£å¾®è°ƒæ€§èƒ½æå‡ä¸éœ€è¦æ¨ç†çš„æ ·æœ¬æ¯”ä¾‹ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³ï¼Œè¿™çªæ˜¾äº†ç›‘ç£å¾®è°ƒåœ¨è¿™ç§åœºæ™¯ä¸‹çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†JudgeLRMï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥åˆ¤æ–­ä¸ºå¯¼å‘çš„LLMå®¶æ—ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿ç”¨ä»¥æ³•å®˜ã€ç»“æœé©±åŠ¨çš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚JudgeLRMæ¨¡å‹åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šäº†ç›‘ç£å¾®è°ƒè°ƒä¼˜çš„æ¨¡å‹å’Œæœ€æ–°çš„æ¨ç†æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒJudgeLRM-3Bè¶…è¶Šäº†GPT-4ï¼Œè€ŒJudgeLRM-7Båœ¨F1åˆ†æ•°ä¸Šè¶…è¶Šäº†DeepSeek-R1è¾¾2.79%ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„æ³•å®˜ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00050v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°è€…æä¾›äº†å¯è§„æ¨¡åŒ–æ›¿ä»£äººå·¥æ ‡æ³¨çš„æ–¹æ¡ˆï¼Œä½†åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å¸¸å¸¸è¡¨ç°ä¸è¶³ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶LLMè¯„ä¼°è€…æ˜¯å¦çœŸæ­£å—ç›Šäºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹ä¸åŒè¯„ä¼°ä»»åŠ¡ä¸­æ¨ç†éœ€æ±‚çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°SFTæ€§èƒ½æå‡ä¸éœ€è¦æ¨ç†çš„æ ·æœ¬æ¯”ä¾‹ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³ï¼Œçªæ˜¾äº†SFTåœ¨è¿™äº›åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†JudgeLRMç³»åˆ—æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä»¥åˆ¤æ–­ä¸ºå¯¼å‘çš„LLMï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŸºäºåˆ¤æ–­ç»“æœçš„å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚JudgeLRMç³»åˆ—æ¨¡å‹åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„è¯„ä¼°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ™®éä¼˜äºSFTè°ƒä¼˜æ¨¡å‹å’Œç°æœ‰å…ˆè¿›æ¨ç†æ¨¡å‹ã€‚ç‰¹åˆ«åœ°ï¼ŒJudgeLRM-3Bè¶…è¶Šäº†GPT-4ï¼Œè€ŒJudgeLRM-7Båœ¨F1åˆ†æ•°ä¸Šè¶…è¶Šäº†DeepSeek-R1è¾¾2.79%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°è€…æä¾›äº†å¯è§„æ¨¡åŒ–æ›¿ä»£äººå·¥æ ‡æ³¨çš„å¯èƒ½ã€‚</li>
<li>ç°æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸè¡¨ç°æœ‰é™ã€‚</li>
<li>SFTæ€§èƒ½æå‡ä¸éœ€è¦æ¨ç†çš„æ ·æœ¬æ¯”ä¾‹ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³ã€‚</li>
<li>JudgeLRMç³»åˆ—æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥åˆ¤æ–­ä¸ºå¯¼å‘ã€‚</li>
<li>JudgeLRMç³»åˆ—æ¨¡å‹åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„è¯„ä¼°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>JudgeLRM-3Båœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-4ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef1feec4d257ab938f4075daada3b033.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad4a6484d7f5f5624dc0bed5f9aa8a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ee613896536303315d96d990bf08d2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02e9ddb7c20e91fd6eb25397da45485b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b7e56823b3727150e03d9584e6d656.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rec-R1-Bridging-Generative-Large-Language-Models-and-User-Centric-Recommendation-Systems-via-Reinforcement-Learning"><a href="#Rec-R1-Bridging-Generative-Large-Language-Models-and-User-Centric-Recommendation-Systems-via-Reinforcement-Learning" class="headerlink" title="Rec-R1: Bridging Generative Large Language Models and User-Centric   Recommendation Systems via Reinforcement Learning"></a>Rec-R1: Bridging Generative Large Language Models and User-Centric   Recommendation Systems via Reinforcement Learning</h2><p><strong>Authors:Jiacheng Lin, Tian Wang, Kun Qian</strong></p>
<p>We propose Rec-R1, a general reinforcement learning framework that bridges large language models (LLMs) with recommendation systems through closed-loop optimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1 directly optimizes LLM generation using feedback from a fixed black-box recommendation model, without relying on synthetic SFT data from proprietary models such as GPT-4o. This avoids the substantial cost and effort required for data distillation. To verify the effectiveness of Rec-R1, we evaluate it on two representative tasks: product search and sequential recommendation. Experimental results demonstrate that Rec-R1 not only consistently outperforms prompting- and SFT-based methods, but also achieves significant gains over strong discriminative baselines, even when used with simple retrievers such as BM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM, unlike SFT, which often impairs instruction-following and reasoning. These findings suggest Rec-R1 as a promising foundation for continual task-specific adaptation without catastrophic forgetting. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Rec-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡é—­ç¯ä¼˜åŒ–å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ¨èç³»ç»Ÿè”ç³»èµ·æ¥ã€‚ä¸åŒäºæç¤ºå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRec-R1ç›´æ¥ä½¿ç”¨æ¥è‡ªå›ºå®šé»‘ç®±æ¨èæ¨¡å‹çš„åé¦ˆæ¥ç›´æ¥ä¼˜åŒ–LLMçš„ç”Ÿæˆï¼Œè€Œæ— éœ€ä¾èµ–æ¥è‡ªä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰çš„åˆæˆSFTæ•°æ®ã€‚è¿™é¿å…äº†æ•°æ®è’¸é¦æ‰€éœ€çš„å¤§é‡æˆæœ¬å’Œæ—¶é—´ã€‚ä¸ºäº†éªŒè¯Rec-R1çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼šäº§å“æœç´¢å’Œåºåˆ—æ¨èã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRec-R1ä¸ä»…å§‹ç»ˆä¼˜äºåŸºäºæç¤ºå’ŒSFTçš„æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä½¿ç”¨ç®€å•çš„æ£€ç´¢å™¨ï¼ˆå¦‚BM25ï¼‰æ—¶ï¼Œç”šè‡³æ¯”å¼ºå¤§çš„åˆ¤åˆ«åŸºçº¿ä¹Ÿæœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œä¸ç»å¸¸æŸå®³æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›çš„SFTä¸åŒï¼ŒRec-R1ä¿ç•™äº†LLMçš„é€šç”¨èƒ½åŠ›ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒRec-R1æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ¡†æ¶ï¼Œä¸ºä¸æ–­é€‚åº”ç‰¹å®šä»»åŠ¡è€Œä¸å‘ç”Ÿç¾éš¾æ€§é—å¿˜å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24289v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRec-R1çš„é€šç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡é—­ç¯ä¼˜åŒ–å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ¨èç³»ç»Ÿç›¸ç»“åˆã€‚ä¸åŒäºæç¤ºå’ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ–¹æ³•ï¼ŒRec-R1ç›´æ¥ä½¿ç”¨æ¥è‡ªå›ºå®šé»‘ç®±æ¨èæ¨¡å‹çš„åé¦ˆæ¥ç›´æ¥ä¼˜åŒ–LLMç”Ÿæˆï¼Œæ— éœ€ä¾èµ–å¦‚GPT-4oç­‰ä¸“æœ‰æ¨¡å‹çš„åˆæˆSFTæ•°æ®ï¼Œä»è€Œé¿å…äº†æ•°æ®è’¸é¦æ‰€éœ€çš„å¤§é‡æˆæœ¬å’Œæ—¶é—´ã€‚åœ¨å…¸å‹çš„äº§å“æœç´¢å’Œåºåˆ—æ¨èä»»åŠ¡ä¸Šï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒRec-R1ä¸ä»…å§‹ç»ˆä¼˜äºåŸºäºæç¤ºå’ŒSFTçš„æ–¹æ³•ï¼Œè€Œä¸”åœ¨é‡‡ç”¨ç®€å•æ£€ç´¢å™¨å¦‚BM25æ—¶ä¹Ÿèƒ½æ˜¾è‘—è¶…è¶Šå¼ºå¤§çš„åˆ¤åˆ«åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒRec-R1ä¿ç•™äº†LLMçš„é€šç”¨èƒ½åŠ›ï¼Œä¸åƒSFTé‚£æ ·å¾€å¾€ä¼šæŸå®³æŒ‡ä»¤è·Ÿéšå’Œæ¨ç†èƒ½åŠ›ã€‚è¿™è¡¨æ˜Rec-R1æ˜¯åœ¨ä¸æ–­é€‚åº”ç‰¹å®šä»»åŠ¡è¿‡ç¨‹ä¸­é¿å…ç¾éš¾æ€§é—å¿˜çš„å¯é åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶Rec-R1ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ¨èç³»ç»Ÿã€‚</li>
<li>Rec-R1é€šè¿‡é—­ç¯ä¼˜åŒ–ç›´æ¥ä¼˜åŒ–LLMç”Ÿæˆï¼Œä¸ä¾èµ–åˆæˆç›‘ç£å¾®è°ƒæ•°æ®æˆ–ä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>åœ¨äº§å“æœç´¢å’Œåºåˆ—æ¨èä»»åŠ¡ä¸Šï¼ŒRec-R1è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†åŸºäºæç¤ºå’Œç›‘ç£å¾®è°ƒçš„æ–¹æ³•ã€‚</li>
<li>Rec-R1åœ¨é‡‡ç”¨ç®€å•æ£€ç´¢å™¨æ—¶ä¹Ÿèƒ½è¶…è¶Šå¼ºå¤§çš„åˆ¤åˆ«åŸºå‡†æ¨¡å‹ï¼Œæ˜¾ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>Rec-R1ä¿ç•™äº†LLMçš„é€šç”¨èƒ½åŠ›ï¼Œä¸åŒäºæŸäº›æ–¹æ³•ä¼šæŸå®³æŒ‡ä»¤è·Ÿéšå’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†Rec-R1åœ¨è¿ç»­ä»»åŠ¡é€‚åº”ä¸­é¿å…ç¾éš¾æ€§é—å¿˜çš„æ½œåŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-021899e0e3d56e36d456ad53b65b3d7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0432ea20e2f44bd1dc25151342e0efcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b48449f68f1493a667d373fdd9ee0cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55941a4b36bb974a7d4601d46a640cda.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FakeScope-Large-Multimodal-Expert-Model-for-Transparent-AI-Generated-Image-Forensics"><a href="#FakeScope-Large-Multimodal-Expert-Model-for-Transparent-AI-Generated-Image-Forensics" class="headerlink" title="FakeScope: Large Multimodal Expert Model for Transparent AI-Generated   Image Forensics"></a>FakeScope: Large Multimodal Expert Model for Transparent AI-Generated   Image Forensics</h2><p><strong>Authors:Yixuan Li, Yu Tian, Yipo Huang, Wei Lu, Shiqi Wang, Weisi Lin, Anderson Rocha</strong></p>
<p>The rapid and unrestrained advancement of generative artificial intelligence (AI) presents a double-edged sword: while enabling unprecedented creativity, it also facilitates the generation of highly convincing deceptive content, undermining societal trust. As image generation techniques become increasingly sophisticated, detecting synthetic images is no longer just a binary task: it necessitates interpretable, context-aware methodologies that enhance trustworthiness and transparency. However, existing detection models primarily focus on classification, offering limited explanatory insights into image authenticity. In this work, we propose FakeScope, an expert multimodal model (LMM) tailored for AI-generated image forensics, which not only identifies AI-synthetic images with high accuracy but also provides rich, interpretable, and query-driven forensic insights. We first construct FakeChain dataset that contains linguistic authenticity reasoning based on visual trace evidence, developed through a novel human-machine collaborative framework. Building upon it, we further present FakeInstruct, the largest multimodal instruction tuning dataset containing 2 million visual instructions tailored to enhance forensic awareness in LMMs. FakeScope achieves state-of-the-art performance in both closed-ended and open-ended forensic scenarios. It can distinguish synthetic images with high accuracy while offering coherent and insightful explanations, free-form discussions on fine-grained forgery attributes, and actionable enhancement strategies. Notably, despite being trained exclusively on qualitative hard labels, FakeScope demonstrates remarkable zero-shot quantitative capability on detection, enabled by our proposed token-based probability estimation strategy. Furthermore, FakeScope exhibits strong generalization and in-the-wild ability, ensuring its applicability in real-world scenarios. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”ŸæˆæŠ€æœ¯çš„è¿…çŒ›ä¸”ä¸å—é™åˆ¶çš„å‘å±•å‘ˆç°å‡ºä¸€æŠŠåŒåˆƒå‰‘çš„ç‰¹æ€§ï¼šè™½ç„¶å®ƒä¿ƒè¿›äº†å‰æ‰€æœªæœ‰çš„åˆ›é€ åŠ›ï¼Œä½†ä¹Ÿæ–¹ä¾¿äº†é«˜åº¦æ¬ºéª—æ€§çš„å†…å®¹çš„ç”Ÿæˆï¼Œä»è€Œç ´åäº†ç¤¾ä¼šä¿¡ä»»ã€‚éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„æ—¥ç›Šæˆç†Ÿï¼Œæ£€æµ‹åˆæˆå›¾åƒä¸å†ä»…ä»…æ˜¯äºŒåˆ†ç±»ä»»åŠ¡ï¼šå®ƒéœ€è¦è§£é‡Šæ€§å¼ºã€å…·æœ‰è¯­å¢ƒæ„è¯†çš„æ–¹æ³•æ¥å¢å¼ºå¯ä¿¡åº¦å’Œé€æ˜åº¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€æµ‹æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨åˆ†ç±»ä¸Šï¼Œå¯¹äºå›¾åƒçœŸå®æ€§çš„è§£é‡Šæ€§æ´å¯Ÿæœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FakeScopeï¼Œä¸€ä¸ªé’ˆå¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒå–è¯çš„ä¸“ä¸šå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ã€‚å®ƒä¸ä»…èƒ½å¤Ÿé«˜åº¦å‡†ç¡®åœ°è¯†åˆ«AIåˆæˆçš„å›¾åƒï¼Œè€Œä¸”è¿˜æä¾›ä¸°å¯Œã€å¯è§£é‡Šã€æŸ¥è¯¢é©±åŠ¨çš„å–è¯æ´å¯Ÿã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†FakeChainæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŸºäºè§†è§‰ç—•è¿¹è¯æ®çš„è¯­è¨€çœŸå®æ€§æ¨ç†ï¼Œé€šè¿‡æ–°å‹çš„äººæœºåä½œæ¡†æ¶å¼€å‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¨å‡ºäº†FakeInstructï¼Œè¿™æ˜¯æœ€å¤§çš„å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ŒåŒ…å«200ä¸‡æ¡æ—¨åœ¨æé«˜LMMså–è¯æ„è¯†çš„è§†è§‰æŒ‡ä»¤ã€‚FakeScopeåœ¨å°é—­å’Œå¼€æ”¾å–è¯åœºæ™¯ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒèƒ½å¤Ÿå‡†ç¡®åœ°åŒºåˆ†åˆæˆå›¾åƒï¼ŒåŒæ—¶æä¾›è¿è´¯ä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„è§£é‡Šã€å¯¹ç»†å¾®ä¼ªé€ å±æ€§çš„è‡ªç”±å½¢å¼è®¨è®ºå’Œå¯è¡Œçš„å¢å¼ºç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡ä»…é€šè¿‡å®šæ€§ç¡¬æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼ŒFakeScopeåœ¨æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºäº†å¼•äººæ³¨ç›®çš„é›¶æ ·æœ¬å®šé‡èƒ½åŠ›ï¼Œè¿™æ˜¯ç”±æˆ‘ä»¬æå‡ºçš„åŸºäºæ ‡è®°çš„æ¦‚ç‡ä¼°è®¡ç­–ç•¥æ‰€å®ç°çš„ã€‚æ­¤å¤–ï¼ŒFakeScopeå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’Œé‡å¤–èƒ½åŠ›ï¼Œç¡®ä¿å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.24267v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•å¯¹ç¤¾ä¼šä¿¡ä»»çš„å½±å“ã€‚éšç€å›¾åƒç”ŸæˆæŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæ£€æµ‹åˆæˆå›¾åƒçš„éœ€æ±‚æ„ˆå‘è¿«åˆ‡ï¼Œéœ€è¦å¯è§£é‡Šã€å…·å¤‡æƒ…å¢ƒæ„è¯†çš„æ£€æµ‹æ–¹æ³•æ¥å¢å¼ºä¿¡ä»»åº¦å’Œé€æ˜åº¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†FakeScopeï¼Œä¸€ä¸ªé’ˆå¯¹AIç”Ÿæˆå›¾åƒé‰´å®šçš„ä¸“å®¶å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ã€‚FakeScopeä¸ä»…èƒ½å‡†ç¡®é‰´å®šAIç”Ÿæˆçš„å›¾åƒï¼Œè€Œä¸”æä¾›ä¸°å¯Œã€å¯è§£é‡Šã€æŸ¥è¯¢é©±åŠ¨çš„é‰´å®šæ´å¯Ÿã€‚é€šè¿‡æ„å»ºFakeChainæ•°æ®é›†å’ŒFakeInstructæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ŒFakeScopeåœ¨å°é—­å’Œå¼€æ”¾é‰´å®šåœºæ™¯ä¸­å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚å®ƒå¯ç²¾ç»†åŒºåˆ†åˆæˆå›¾åƒï¼Œæä¾›è¿è´¯çš„æ·±å…¥è§£é‡Šã€è‡ªç”±è®¨è®ºä¼ªé€ å±æ€§çš„ç»†èŠ‚ä»¥åŠå¯è¡Œçš„æ”¹è¿›ç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFakeScopeä»…é€šè¿‡å®šæ€§ç¡¬æ ‡ç­¾è®­ç»ƒï¼Œå´å±•ç°å‡ºé›¶æ ·æœ¬å®šé‡æ£€æµ‹èƒ½åŠ›ï¼Œä¸”å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’Œå®é™…åº”ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†åˆ›é€ åŠ›æå‡çš„åŒæ—¶ï¼Œä¹Ÿä¿ƒè¿›äº†æ¬ºéª—æ€§å†…å®¹çš„ç”Ÿæˆï¼Œå¯¹ç¤¾ä¼šä¿¡ä»»é€ æˆå½±å“ã€‚</li>
<li>å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—æ£€æµ‹åˆæˆå›¾åƒå˜å¾—å¤æ‚ï¼Œéœ€è¦å¯è§£é‡Šã€å…·å¤‡æƒ…å¢ƒæ„è¯†çš„æ£€æµ‹æ–¹æ³•æ¥å¢å¼ºä¿¡ä»»åº¦å’Œé€æ˜åº¦ã€‚</li>
<li>FakeScopeæ˜¯ä¸€ä¸ªé’ˆå¯¹AIç”Ÿæˆå›¾åƒé‰´å®šçš„ä¸“å®¶å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ï¼Œèƒ½å‡†ç¡®é‰´å®šAIç”Ÿæˆçš„å›¾åƒï¼Œå¹¶æä¾›ä¸°å¯Œçš„å¯è§£é‡Šæ€§æ´å¯Ÿã€‚</li>
<li>FakeScopeé€šè¿‡æ„å»ºFakeChainæ•°æ®é›†å’ŒFakeInstructæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œé€‚ç”¨äºå°é—­å’Œå¼€æ”¾çš„é‰´å®šåœºæ™¯ã€‚</li>
<li>FakeScopeå…·å¤‡é›¶æ ·æœ¬å®šé‡æ£€æµ‹èƒ½åŠ›ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨æ½œåŠ›ã€‚</li>
<li>FakeScopeä¸ä»…èƒ½å‡†ç¡®é‰´å®šåˆæˆå›¾åƒï¼Œè¿˜èƒ½æä¾›å…³äºä¼ªé€ å±æ€§çš„æ·±å…¥è§£é‡Šå’Œè¡ŒåŠ¨å»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.24267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-694bdba20e34d207a3c0dbc9db54f26e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd6f7f99f737955e1da167a0c876124.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f74842ca58f90816dd3859a5c8c51e26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d7f04a757a2d8760ea837a36c99b828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5220b28658718646564e588b6fcba51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29097bbc089fcafef3b02be6e3349c2a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Boosting-MLLM-Reasoning-with-Text-Debiased-Hint-GRPO"><a href="#Boosting-MLLM-Reasoning-with-Text-Debiased-Hint-GRPO" class="headerlink" title="Boosting MLLM Reasoning with Text-Debiased Hint-GRPO"></a>Boosting MLLM Reasoning with Text-Debiased Hint-GRPO</h2><p><strong>Authors:Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, Jingyuan Chen, Chang Yao, Jie Song</strong></p>
<p>MLLM reasoning has drawn widespread research for its excellent problem-solving capability. Current reasoning methods fall into two types: PRM, which supervises the intermediate reasoning steps, and ORM, which supervises the final results. Recently, DeepSeek-R1 has challenged the traditional view that PRM outperforms ORM, which demonstrates strong generalization performance using an ORM method (i.e., GRPO). However, current MLLMâ€™s GRPO algorithms still struggle to handle challenging and complex multimodal reasoning tasks (e.g., mathematical reasoning). In this work, we reveal two problems that impede the performance of GRPO on the MLLM: Low data utilization and Text-bias. Low data utilization refers to that GRPO cannot acquire positive rewards to update the MLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses image condition and solely relies on text condition for generation after GRPO training. To tackle these problems, this work proposes Hint-GRPO that improves data utilization by adaptively providing hints for samples of varying difficulty, and text-bias calibration that mitigates text-bias by calibrating the token prediction logits with image condition in test-time. Experiment results on three base MLLMs across eleven datasets demonstrate that our proposed methods advance the reasoning capability of original MLLM by a large margin, exhibiting superior performance to existing MLLM reasoning methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hqhQAQ/Hint-GRPO">https://github.com/hqhQAQ/Hint-GRPO</a>. </p>
<blockquote>
<p>MLLMæ¨ç†å› å…¶å‡ºè‰²çš„è§£å†³é—®é¢˜çš„èƒ½åŠ›è€Œå¼•èµ·äº†å¹¿æ³›çš„ç ”ç©¶ã€‚å½“å‰çš„æ¨ç†æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šPRMï¼Œå®ƒç›‘ç£ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œè€ŒORMåˆ™ç›‘ç£æœ€ç»ˆç»“æœã€‚æœ€è¿‘ï¼ŒDeepSeek-R1æŒ‘æˆ˜äº†ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºPRMä¼˜äºORMï¼Œæ¼”ç¤ºäº†ä½¿ç”¨ORMæ–¹æ³•ï¼ˆå³GRPOï¼‰çš„å¼ºå¤§æ³›åŒ–æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„MLLMçš„GRPOç®—æ³•åœ¨åº”å¯¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦æ¨ç†ï¼‰æ—¶ä»ç„¶é‡åˆ°å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†é˜»ç¢GRPOåœ¨MLLMä¸Šæ€§èƒ½çš„ä¸¤ä¸ªé—®é¢˜ï¼šæ•°æ®åˆ©ç”¨ç‡ä½å’Œæ–‡æœ¬åå·®ã€‚æ•°æ®åˆ©ç”¨ç‡ä½æ˜¯æŒ‡GRPOæ— æ³•è·å–ç§¯æçš„å¥–åŠ±æ¥æ›´æ–°MLLMåœ¨å›°éš¾æ ·æœ¬ä¸Šçš„è¡¨ç°ï¼›æ–‡æœ¬åå·®æ˜¯ä¸€ç§ç°è±¡ï¼Œå³åœ¨GRPOè®­ç»ƒåï¼ŒMLLMç»•è¿‡å›¾åƒæ¡ä»¶è€Œä»…ä¾èµ–æ–‡æœ¬æ¡ä»¶è¿›è¡Œç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†Hint-GRPOï¼Œå®ƒé€šè¿‡è‡ªé€‚åº”åœ°ä¸ºä¸åŒéš¾åº¦çš„æ ·æœ¬æä¾›æç¤ºæ¥æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œä»¥åŠæ–‡æœ¬åå·®æ ¡æ­£ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶é€šè¿‡å›¾åƒæ¡ä»¶æ ¡å‡†æ ‡è®°é¢„æµ‹é€»è¾‘æ¥å‡è½»æ–‡æœ¬åå·®ã€‚åœ¨ä¸‰ä¸ªåŸºç¡€MLLMä¸Šçš„åä¸€ä¸ªæ•°æ®é›†è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¤§å¹…åº¦æé«˜äº†åŸå§‹MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨ç°æœ‰çš„MLLMæ¨ç†æ–¹æ³•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hqhQAQ/Hint-GRPO%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hqhQAQ/Hint-GRPOä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23905v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MLLMæ¨ç†ä¸­çš„ä¸¤ç§å¸¸è§æ–¹æ³•PRMå’ŒORMï¼ŒæŒ‡å‡ºä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºPRMä¼˜äºORMã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨ORMæ–¹æ³•çš„DeepSeek-R1å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚å½“å‰MLLMçš„GRPOç®—æ³•åœ¨å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦æ¨ç†ï¼‰æ—¶ä»å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡æ­ç¤ºäº†å½±å“GRPOåœ¨MLLMä¸Šæ€§èƒ½çš„ä¸¤å¤§é—®é¢˜ï¼šæ•°æ®åˆ©ç”¨ç‡ä½å’Œæ–‡æœ¬åå·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Hint-GRPOå’Œæ–‡æœ¬åå·®æ ¡æ­£æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•å¤§å¹…æé«˜äº†MLLMçš„æ¨ç†èƒ½åŠ›ï¼Œä¼˜äºç°æœ‰çš„MLLMæ¨ç†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMæ¨ç†ä¸­å¸¸è§çš„æ–¹æ³•åŒ…æ‹¬PRMå’ŒORMï¼Œå…¶ä¸­ORMæ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>å½“å‰MLLMçš„GRPOç®—æ³•åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°å­¦æ¨ç†ã€‚</li>
<li>æ­ç¤ºäº†GRPOåœ¨MLLMä¸Šçš„ä¸¤å¤§é—®é¢˜ï¼šæ•°æ®åˆ©ç”¨ç‡ä½å’Œæ–‡æœ¬åå·®ã€‚</li>
<li>Hint-GRPOæ–¹æ³•é€šè¿‡è‡ªé€‚åº”æä¾›ä¸åŒéš¾åº¦æ ·æœ¬çš„æç¤ºæ¥æ”¹å–„æ•°æ®åˆ©ç”¨ç‡ã€‚</li>
<li>æ–‡æœ¬åå·®æ ¡æ­£æ–¹æ³•é€šè¿‡æ ¡å‡†æµ‹è¯•æ—¶çš„å›¾åƒæ¡ä»¶ä»¤ç‰Œé¢„æµ‹æ—¥å¿—æ¥å‡è½»æ–‡æœ¬åå·®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¤§å¹…æé«˜äº†MLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18eebf559a648d28e8b957a5a627ad81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60cedbec22196b51702a17a0897652fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b98c175e061d02eed052b6f2ced6c7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abb3051b683d48c2e393a27aabb84bab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-306f22fd57cef77018e348bbb916751f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Crossing-the-Reward-Bridge-Expanding-RL-with-Verifiable-Rewards-Across-Diverse-Domains"><a href="#Crossing-the-Reward-Bridge-Expanding-RL-with-Verifiable-Rewards-Across-Diverse-Domains" class="headerlink" title="Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across   Diverse Domains"></a>Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across   Diverse Domains</h2><p><strong>Authors:Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, Dong Yu</strong></p>
<p>Reinforcement learning with verifiable rewards (RLVR) has demonstrated significant success in enhancing mathematical reasoning and coding performance of large language models (LLMs), especially when structured reference answers are accessible for verification. However, its extension to broader, less structured domains remains unexplored. In this work, we investigate the effectiveness and scalability of RLVR across diverse real-world domains including medicine, chemistry, psychology, economics, and education, where structured reference answers are typically unavailable. We reveal that binary verification judgments on broad-domain tasks exhibit high consistency across various LLMs provided expert-written reference answers exist. Motivated by this finding, we utilize a generative scoring technique that yields soft, model-based reward signals to overcome limitations posed by binary verifications, especially in free-form, unstructured answer scenarios. We further demonstrate the feasibility of training cross-domain generative reward models using relatively small (7B) LLMs without the need for extensive domain-specific annotation. Through comprehensive experiments, our RLVR framework establishes clear performance gains, significantly outperforming state-of-the-art open-source aligned models such as Qwen2.5-72B and DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach notably enhances the robustness, flexibility, and scalability of RLVR, representing a substantial step towards practical reinforcement learning applications in complex, noisy-label scenarios. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦æ¨ç†å’Œç¼–ç æ€§èƒ½æ–¹é¢å·²å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨å¯è·å¾—ç»“æ„åŒ–å‚è€ƒç­”æ¡ˆè¿›è¡ŒéªŒè¯çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå…¶åœ¨æ›´å¹¿æ³›ã€ç»“æ„åŒ–ç¨‹åº¦è¾ƒä½çš„é¢†åŸŸçš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†RLVRåœ¨ä¸åŒç°å®ä¸–ç•Œé¢†åŸŸï¼ˆåŒ…æ‹¬åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦ã€ç»æµå­¦å’Œæ•™è‚²ï¼‰çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­é€šå¸¸æ²¡æœ‰ç»“æ„åŒ–çš„å‚è€ƒç­”æ¡ˆå¯ç”¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¹¿æ³›çš„é¢†åŸŸä»»åŠ¡ä¸­ï¼Œåªè¦å­˜åœ¨ä¸“å®¶ç¼–å†™çš„å‚è€ƒç­”æ¡ˆï¼ŒäºŒå…ƒéªŒè¯åˆ¤æ–­åœ¨å„ç§LLMä¸­çš„ä¸€è‡´æ€§å°±å¾ˆé«˜ã€‚å—æ­¤å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç”Ÿæˆè¯„åˆ†æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯äº§ç”ŸåŸºäºæ¨¡å‹çš„è½¯å¥–åŠ±ä¿¡å·ï¼Œä»¥å…‹æœäºŒå…ƒéªŒè¯å¸¦æ¥çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç”±å½¢å¼ã€éç»“æ„åŒ–ç­”æ¡ˆåœºæ™¯ä¸­ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯æ˜äº†ä½¿ç”¨ç›¸å¯¹è¾ƒå°çš„ï¼ˆ7Bï¼‰LLMè®­ç»ƒè·¨åŸŸç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ€§ï¼Œæ— éœ€è¿›è¡Œå¤§é‡çš„é¢†åŸŸç‰¹å®šæ ‡æ³¨ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬çš„RLVRæ¡†æ¶åœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜æ˜¾çš„æå‡ï¼Œåœ¨è‡ªç”±å½¢å¼è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„å¼€æºå¯¹é½æ¨¡å‹ï¼Œå¦‚Qwen 2.5-72Bå’ŒDeepSeek-R1-Distill-Qwen-32Bç­‰å¤šä¸ªé¢†åŸŸä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†RLVRçš„é²æ£’æ€§ã€çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»£è¡¨äº†åœ¨å®é™…å¤æ‚ã€å™ªå£°æ ‡ç­¾åœºæ™¯ä¸­å®ç°å¼ºåŒ–å­¦ä¹ åº”ç”¨çš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23829v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦æ¨ç†å’Œç¼–ç¨‹æ€§èƒ½æå‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¯è·å¾—ç»“æ„åŒ–å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å…¶åœ¨åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦ã€ç»æµå­¦å’Œæ•™è‚²ç­‰é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­é€šå¸¸æ²¡æœ‰ç»“æ„åŒ–çš„å‚è€ƒç­”æ¡ˆã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å­˜åœ¨ä¸“å®¶ç¼–å†™çš„å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹ï¼Œå¯¹å¹¿æ³›é¢†åŸŸçš„ä»»åŠ¡è¿›è¡ŒäºŒå…ƒéªŒè¯åˆ¤æ–­åœ¨å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´è¡¨ç°å‡ºé«˜åº¦çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†å…‹æœäºŒå…ƒéªŒè¯åœ¨è‡ªç”±å½¢å¼ã€éç»“æ„åŒ–ç­”æ¡ˆåœºæ™¯ä¸­çš„å±€é™æ€§ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨ç”Ÿæˆè¯„åˆ†æŠ€æœ¯äº§ç”ŸåŸºäºæ¨¡å‹çš„è½¯å¥–åŠ±ä¿¡å·ã€‚åœ¨ä¸éœ€è¦å¹¿æ³›é¢†åŸŸç‰¹å®šæ³¨é‡Šçš„æƒ…å†µä¸‹ï¼Œæœ¬ç ”ç©¶å±•ç¤ºäº†ä½¿ç”¨ç›¸å¯¹è¾ƒå°ï¼ˆ7Bï¼‰çš„LLMè®­ç»ƒè·¨é¢†åŸŸç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ€§ã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼ŒRLVRæ¡†æ¶åœ¨è‡ªç”±å½¢å¼è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºQwen2.5-72Bå’ŒDeepSeek-R1-Distill-Qwen-32Bç­‰æœ€æ–°å¼€æºå¯¹é½æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†å…¶æ€§èƒ½ã€‚æœ¬ç ”ç©¶å¢å¼ºäº†RLVRçš„ç¨³å¥æ€§ã€çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œæœç€å¤æ‚å™ªå£°æ ‡ç­¾åœºæ™¯çš„å®é™…å¼ºåŒ–å­¦ä¹ åº”ç”¨è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RLVRæŠ€æœ¯åœ¨å¢å¼ºæ•°å­¦æ¨ç†å’Œç¼–ç¨‹æ€§èƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶åœ¨æœ‰ç»“æ„åŒ–å‚è€ƒç­”æ¡ˆçš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨åŒ»å­¦ã€åŒ–å­¦ã€å¿ƒç†å­¦ã€ç»æµå­¦å’Œæ•™è‚²ç­‰å¤šæ ·ä¸”éç»“æ„åŒ–çš„é¢†åŸŸä¸­æ¢ç´¢äº†RLVRçš„æœ‰æ•ˆæ€§åŠæ‰©å±•æ€§ã€‚</li>
<li>å‘ç°ä¸“å®¶ç¼–å†™çš„å‚è€ƒç­”æ¡ˆåœ¨è·¨å¤§å‹è¯­è¨€æ¨¡å‹çš„äºŒå…ƒéªŒè¯åˆ¤æ–­ä¸­è¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆè¯„åˆ†æŠ€æœ¯äº§ç”Ÿè½¯å¥–åŠ±ä¿¡å·ï¼Œå…‹æœåœ¨éç»“æ„åŒ–ç­”æ¡ˆåœºæ™¯çš„äºŒå…ƒéªŒè¯å±€é™ã€‚</li>
<li>åœ¨æ— éœ€å¹¿æ³›çš„é¢†åŸŸç‰¹å®šæ ‡æ³¨æƒ…å†µä¸‹ï¼Œå±•ç¤ºäº†è®­ç»ƒè·¨é¢†åŸŸç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„å¯è¡Œæ€§ã€‚</li>
<li>RLVRæ¡†æ¶æ˜¾è‘—ä¼˜äºå…¶ä»–æœ€æ–°å¼€æºæ¨¡å‹ï¼Œå¢å¼ºäº†æ€§èƒ½ã€ç¨³å¥æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0f20cd5e32b129190a9cec75f57a5276.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4be8cf54b64cd9b989b47052740924ad.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HOIGen-1M-A-Large-scale-Dataset-for-Human-Object-Interaction-Video-Generation"><a href="#HOIGen-1M-A-Large-scale-Dataset-for-Human-Object-Interaction-Video-Generation" class="headerlink" title="HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video   Generation"></a>HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video   Generation</h2><p><strong>Authors:Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, Xiaodong He, Wu Liu</strong></p>
<p>Text-to-video (T2V) generation has made tremendous progress in generating complicated scenes based on texts. However, human-object interaction (HOI) often cannot be precisely generated by current T2V models due to the lack of large-scale videos with accurate captions for HOI. To address this issue, we introduce HOIGen-1M, the first largescale dataset for HOI Generation, consisting of over one million high-quality videos collected from diverse sources. In particular, to guarantee the high quality of videos, we first design an efficient framework to automatically curate HOI videos using the powerful multimodal large language models (MLLMs), and then the videos are further cleaned by human annotators. Moreover, to obtain accurate textual captions for HOI videos, we design a novel video description method based on a Mixture-of-Multimodal-Experts (MoME) strategy that not only generates expressive captions but also eliminates the hallucination by individual MLLM. Furthermore, due to the lack of an evaluation framework for generated HOI videos, we propose two new metrics to assess the quality of generated videos in a coarse-to-fine manner. Extensive experiments reveal that current T2V models struggle to generate high-quality HOI videos and confirm that our HOIGen-1M dataset is instrumental for improving HOI video generation. Project webpage is available at <a target="_blank" rel="noopener" href="https://liuqi-creat.github.io/HOIGen.github.io">https://liuqi-creat.github.io/HOIGen.github.io</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆåœ¨åŸºäºæ–‡æœ¬ç”Ÿæˆå¤æ‚åœºæ™¯æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘å¤§è§„æ¨¡å¸¦æœ‰å‡†ç¡®å­—å¹•çš„äººæœºäº¤äº’ï¼ˆHOIï¼‰è§†é¢‘ï¼Œå½“å‰T2Væ¨¡å‹å¾€å¾€æ— æ³•ç²¾ç¡®ç”ŸæˆHOIã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HOIGen-1Mï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºHOIç”Ÿæˆçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«ä»å„ç§æ¥æºæ”¶é›†çš„è¶…è¿‡ä¸€ç™¾ä¸‡é«˜æ¸…è§†é¢‘ã€‚ç‰¹åˆ«æ˜¯ä¸ºäº†ä¿è¯è§†é¢‘çš„é«˜è´¨é‡ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è‡ªåŠ¨ç­›é€‰HOIè§†é¢‘ï¼Œç„¶åè§†é¢‘è¿›ä¸€æ­¥ç”±äººå·¥æ³¨é‡Šè€…è¿›è¡Œæ¸…ç†ã€‚æ­¤å¤–ï¼Œä¸ºäº†è·å¾—HOIè§†é¢‘çš„å‡†ç¡®æ–‡æœ¬å­—å¹•ï¼Œæˆ‘ä»¬åŸºäºæ··åˆå¤šæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰ç­–ç•¥è®¾è®¡äº†ä¸€ç§æ–°çš„è§†é¢‘æè¿°æ–¹æ³•ï¼Œå®ƒä¸ä»…ç”Ÿæˆäº†å¯Œæœ‰è¡¨ç°åŠ›çš„å­—å¹•ï¼Œè¿˜æ¶ˆé™¤äº†å•ä¸ªMLLMäº§ç”Ÿçš„å¹»è§‰ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹ç”Ÿæˆçš„HOIè§†é¢‘çš„è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆçš„è§†é¢‘è´¨é‡ï¼Œä»ç²—ç•¥åˆ°ç²¾ç»†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›®å‰çš„T2Væ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡HOIè§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶è¯å®æˆ‘ä»¬çš„HOIGen-1Mæ•°æ®é›†å¯¹äºæ”¹è¿›HOIè§†é¢‘ç”Ÿæˆéå¸¸é‡è¦ã€‚é¡¹ç›®ç½‘é¡µå¯åœ¨<a target="_blank" rel="noopener" href="https://liuqi-creat.github.io/HOIGen.github.io%E8%AE%BF%E9%97%AE%E3%80%82">https://liuqi-creat.github.io/HOIGen.github.ioè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23715v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆä¸­çš„äººä¸ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰éš¾ä»¥å‡†ç¡®ç”Ÿæˆçš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†HOIGen-1Mæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡é«˜è´¨é‡çš„è§†é¢‘ã€‚é€šè¿‡è®¾è®¡é«˜æ•ˆçš„è‡ªåŠ¨ç­›é€‰æ¡†æ¶å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¿è¯äº†è§†é¢‘çš„é«˜è´¨é‡ã€‚åŒæ—¶ï¼Œæå‡ºäº†åŸºäºæ··åˆå¤šæ¨¡æ€ä¸“å®¶ç­–ç•¥çš„è§†é¢‘æè¿°æ–¹æ³•ï¼Œä»¥è§£å†³å¯¹HOIè§†é¢‘çš„å‡†ç¡®æ–‡æœ¬æè¿°é—®é¢˜ã€‚ç”±äºç¼ºä¹è¯„ä»·æ¡†æ¶ï¼Œè¿˜æå‡ºäº†ä¸¤ç§æ–°çš„è¯„ä»·æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰T2Væ¨¡å‹åœ¨ç”ŸæˆHOIè§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒHOIGen-1Mæ•°æ®é›†å¯¹æ”¹è¿›HOIè§†é¢‘ç”Ÿæˆè‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆåœ¨ç”ŸæˆåŒ…å«äººä¸ç‰©ä½“äº¤äº’ï¼ˆHOIï¼‰çš„åœºæ™¯æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>HOIGen-1Mæ•°æ®é›†æ˜¯è§£å†³æ­¤é—®é¢˜çš„é¦–æ¬¡å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡é«˜è´¨é‡çš„è§†é¢‘ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨ç­›é€‰æ¡†æ¶å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç¡®ä¿è§†é¢‘è´¨é‡ã€‚</li>
<li>æå‡ºåŸºäºæ··åˆå¤šæ¨¡æ€ä¸“å®¶ï¼ˆMoMEï¼‰ç­–ç•¥çš„è§†é¢‘æè¿°æ–¹æ³•ï¼Œä»¥ç”Ÿæˆå‡†ç¡®çš„æ–‡æœ¬æè¿°å¹¶æ¶ˆé™¤ä¸ªä½“MLLMçš„å¹»è§‰ã€‚</li>
<li>ç¼ºä¹è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ç”Ÿæˆçš„HOIè§†é¢‘çš„è´¨é‡ï¼Œå› æ­¤æå‡ºä¸¤ç§æ–°çš„è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>å®éªŒè¡¨æ˜å½“å‰T2Væ¨¡å‹åœ¨ç”ŸæˆHOIè§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d171fde81786b4f4af0e916d750a227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b08e17418db32dafdeab7c9c1117fb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0212259fef038debae31a3a76f3d656b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f6db98cc2c1add9e0a3a8b474e7170.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e0b9c91edd0936c311f96006f6ccd2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5eee0b3bf8c342f8cc103e63934254c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ToRL-Scaling-Tool-Integrated-RL"><a href="#ToRL-Scaling-Tool-Integrated-RL" class="headerlink" title="ToRL: Scaling Tool-Integrated RL"></a>ToRL: Scaling Tool-Integrated RL</h2><p><strong>Authors:Xuefeng Li, Haoyang Zou, Pengfei Liu</strong></p>
<p>We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for training large language models (LLMs) to autonomously use computational tools via reinforcement learning. Unlike supervised fine-tuning, ToRL allows models to explore and discover optimal strategies for tool use. Experiments with Qwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3% accuracy on AIME~24, surpassing reinforcement learning without tool integration by 14% and the best existing Tool-Integrated Reasoning (TIR) model by 17%. Further analysis reveals emergent behaviors such as strategic tool invocation, self-regulation of ineffective code, and dynamic adaptation between computational and analytical reasoning, all arising purely through reward-driven learning. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ToRLï¼ˆå·¥å…·é›†æˆå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªä¸»ä½¿ç”¨è®¡ç®—å·¥å…·çš„æ¡†æ¶ã€‚ä¸ç›‘ç£å¾®è°ƒä¸åŒï¼ŒToRLå…è®¸æ¨¡å‹æ¢ç´¢å’Œå‘ç°å·¥å…·ä½¿ç”¨çš„æœ€ä¼˜ç­–ç•¥ã€‚ä½¿ç”¨Qwen2.5-Mathæ¨¡å‹çš„å®éªŒæ˜¾ç¤ºå‡ºäº†æ˜¾è‘—çš„æ”¹è¿›ï¼šToRL-7Båœ¨AIME 24ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†43.3%ï¼Œæ¯”æ²¡æœ‰å·¥å…·é›†æˆçš„å¼ºåŒ–å­¦ä¹ é«˜å‡º14%ï¼Œå¹¶ä¸”æ¯”ç°æœ‰çš„æœ€ä½³å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰æ¨¡å‹é«˜å‡º17%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†æ–°å…´çš„è¡Œä¸ºï¼Œå¦‚æˆ˜ç•¥æ€§åœ°è°ƒç”¨å·¥å…·ã€è‡ªæˆ‘è°ƒæ§æ— æ•ˆä»£ç ä»¥åŠåœ¨è®¡ç®—æ¨ç†å’Œåˆ†ææ¨ç†ä¹‹é—´çš„åŠ¨æ€é€‚åº”ï¼Œæ‰€æœ‰è¿™äº›è¡Œä¸ºéƒ½çº¯ç²¹æ˜¯é€šè¿‡å¥–åŠ±é©±åŠ¨çš„å­¦ä¹ è€Œäº§ç”Ÿçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23383v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ æ¡†æ¶ToRLè¢«æå‡ºç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»ä½¿ç”¨è®¡ç®—å·¥å…·ã€‚ä¸ç›‘ç£å¾®è°ƒä¸åŒï¼ŒToRLå…è®¸æ¨¡å‹æ¢ç´¢å¹¶å‘ç°å·¥å…·ä½¿ç”¨çš„æœ€ä½³ç­–ç•¥ã€‚å®éªŒæ˜¾ç¤ºï¼Œä½¿ç”¨ToRLæ¡†æ¶è®­ç»ƒçš„Qwen2.5-Mathæ¨¡å‹åœ¨AIME 24ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†43.3%ï¼Œç›¸è¾ƒäºä¸ä½¿ç”¨å·¥å…·æ•´åˆçš„å¼ºåŒ–å­¦ä¹ æé«˜äº†14%ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„æœ€ä½³å·¥å…·æ•´åˆæ¨ç†æ¨¡å‹17%ã€‚è¯¥æ¡†æ¶å¼•å‘äº†è¯¸å¦‚ç­–ç•¥æ€§å·¥å…·è°ƒç”¨ã€è‡ªæˆ‘è°ƒæ§æ— æ•ˆä»£ç ä»¥åŠè®¡ç®—ä¸åˆ†ææ¨ç†é—´çš„åŠ¨æ€é€‚åº”ç­‰æ˜¾è‘—è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ToRLæ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¯ä½¿å…¶è‡ªä¸»ä½¿ç”¨è®¡ç®—å·¥å…·ã€‚</li>
<li>ToRLæ¡†æ¶é‡‡ç”¨å¥–åŠ±é©±åŠ¨çš„å­¦ä¹ æ–¹å¼ï¼Œå…è®¸æ¨¡å‹è‡ªä¸»æ¢ç´¢å’Œå‘ç°å·¥å…·ä½¿ç”¨çš„æœ€ä½³ç­–ç•¥ã€‚</li>
<li>Qwen2.5-Mathæ¨¡å‹åœ¨AIME 24ä¸Šçš„å‡†ç¡®ç‡é€šè¿‡ToRLæ¡†æ¶è®­ç»ƒåæ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°43.3%ã€‚</li>
<li>ToRLæ¡†æ¶çš„å®éªŒç»“æœæ˜¾è‘—ä¼˜äºä¸ä½¿ç”¨å·¥å…·æ•´åˆçš„å¼ºåŒ–å­¦ä¹ ä»¥åŠç°æœ‰çš„æœ€ä½³å·¥å…·æ•´åˆæ¨ç†æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨ToRLæ¡†æ¶çš„æ¨¡å‹å±•ç°å‡ºç­–ç•¥æ€§å·¥å…·è°ƒç”¨ã€è‡ªæˆ‘è°ƒæ§æ— æ•ˆä»£ç ç­‰æ˜¾è‘—è¡Œä¸ºã€‚</li>
<li>æ¨¡å‹åœ¨åŠ¨æ€é€‚åº”è®¡ç®—å’Œåˆ†ææ¨ç†ä¹‹é—´è¡¨ç°å‡ºçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4c2207a6b5dd0f3276ea3f64b4d2206.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96493b29fc1650bb78aba4283d2bec52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e37b31354de1ffe6bc878be62ca28f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb760d92de357a5413c3dcc3ea276e41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb2c1faa59c31a0d0add55fcc357b74d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1537e5a7dd2abbc1a5ac694ee7fd3a74.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Reasoning-SQL-Reinforcement-Learning-with-SQL-Tailored-Partial-Rewards-for-Reasoning-Enhanced-Text-to-SQL"><a href="#Reasoning-SQL-Reinforcement-Learning-with-SQL-Tailored-Partial-Rewards-for-Reasoning-Enhanced-Text-to-SQL" class="headerlink" title="Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards   for Reasoning-Enhanced Text-to-SQL"></a>Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards   for Reasoning-Enhanced Text-to-SQL</h2><p><strong>Authors:Mohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini, Amin Saberi, Sercan â€œO. Arik</strong></p>
<p>Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ¶‰åŠå¤šä¸ªæ¨ç†å¯†é›†å‹çš„å­ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°æ®åº“æ¨¡å¼ç†è§£å’Œç²¾ç¡®çš„SQLæŸ¥è¯¢æ„å»ºã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æ¨ç†è·¯å¾„å’Œå½’çº³åè§ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å…¶æ•´ä½“æ•ˆæœã€‚å—æœ€è¿‘æˆåŠŸåº”ç”¨çš„æ¨ç†å¢å¼ºæ¨¡å‹ï¼ˆå¦‚DeepSeek R1å’ŒOpenAI o1ï¼‰çš„å¯å‘ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡å¥–åŠ±é©±åŠ¨çš„è‡ªæˆ‘æ¢ç´¢æ¥å¢å¼ºæ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºæ–‡æœ¬åˆ°SQLä»»åŠ¡é‡èº«å®šåˆ¶äº†ä¸€å¥—æ–°çš„éƒ¨åˆ†å¥–åŠ±ã€‚æˆ‘ä»¬çš„å¥–åŠ±é›†åŒ…æ‹¬æ¨¡å¼é“¾æ¥ã€AIåé¦ˆã€nå…ƒç›¸ä¼¼æ€§å’Œè¯­æ³•æ£€æŸ¥ï¼Œä¸“é—¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­æ™®éå­˜åœ¨çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜ç¡®é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•å†…åœ¨æ¨ç†æŠ€èƒ½ï¼Œä»¥å‡†ç¡®ç”ŸæˆSQLæŸ¥è¯¢ã€‚åœ¨ä¸åŒè§„æ¨¡æ¨¡å‹çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜ä»…ä½¿ç”¨æˆ‘ä»¬æå‡ºå¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ˆRLè®­ç»ƒï¼‰åœ¨å‡†ç¡®åº¦å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å§‹ç»ˆä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è®­ç»ƒçš„14Bå‚æ•°æ¨¡å‹åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºæ›´å¤§çš„ä¸“æœ‰æ¨¡å‹ï¼Œä¾‹å¦‚o3-minié«˜å‡º4%å’ŒGemini-1.5-Pro-002é«˜å‡º3%ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬åœ¨éƒ¨åˆ†å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ä¸­æå‡ºçš„ç­–ç•¥åœ¨æé«˜æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23157v2">PDF</a> Mohammadreza Pourreza and Shayan Talaei contributed equally to this   work</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„å¥–åŠ±è®¾å®šï¼ŒåŒ…æ‹¬æ¨¡å¼é“¾æ¥ã€AIåé¦ˆã€n-gramç›¸ä¼¼æ€§å’Œè¯­æ³•æ£€æŸ¥ç­‰éƒ¨åˆ†å¥–åŠ±ï¼Œæ—¨åœ¨è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè¯¥å¥–åŠ±è®¾å®šé¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•å†…åœ¨æ¨ç†æŠ€èƒ½ï¼Œä»¥å®ç°å‡†ç¡®çš„SQLæŸ¥è¯¢ç”Ÿæˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸æ‰€æå‡ºçš„å¥–åŠ±è®¾å®šï¼Œç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œèƒ½æŒç»­æé«˜å‡†ç¡®æ€§å¹¶æå‡æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨BIRDåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æå‡ºçš„å°å‹æ¨¡å‹åœ¨è¡¨ç°ä¸Šè¶…è¿‡äº†æ›´å¤§è§„æ¨¡çš„ä¸“æœ‰æ¨¡å‹ã€‚è¿™çªæ˜¾äº†é’ˆå¯¹æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ä¸éƒ¨åˆ†å¥–åŠ±è®¾å®šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬åˆ°SQLä»»åŠ¡æ¶‰åŠå¤šä¸ªæ¨ç†å¯†é›†å­ä»»åŠ¡ï¼Œå¦‚è‡ªç„¶è¯­è¨€ç†è§£ã€æ•°æ®åº“æ¨¡å¼ç†è§£å’Œç²¾ç¡®SQLæŸ¥è¯¢å½¢æˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æ¨ç†è·¯å¾„å’Œå½’çº³åè§ï¼Œè¿™å¯èƒ½é™åˆ¶äº†å…¶æ•´ä½“æ•ˆæœã€‚</li>
<li>æå‡ºä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„æ–°å‹éƒ¨åˆ†å¥–åŠ±è®¾å®šï¼ŒåŒ…æ‹¬æ¨¡å¼é“¾æ¥ã€AIåé¦ˆç­‰ï¼Œä»¥è§£å†³å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±ç¨€ç–é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é¼“åŠ±å¤§å‹è¯­è¨€æ¨¡å‹å‘å±•å†…åœ¨æ¨ç†æŠ€èƒ½ã€‚</li>
<li>ä»…ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸æ‰€æå‡ºçš„å¥–åŠ±è®¾å®šï¼Œç›¸è¾ƒäºç›‘ç£å¾®è°ƒèƒ½æé«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7dd37b3e3cfd0bd3048d55cfd819c494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fa9d084ca10003b8b84e85bcf7f37e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94eb93ed40707c196a0d226fae8c3c00.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-DeepSeek-Reason-Like-a-Surgeon-An-Empirical-Evaluation-for-Vision-Language-Understanding-in-Robotic-Assisted-Surgery"><a href="#Can-DeepSeek-Reason-Like-a-Surgeon-An-Empirical-Evaluation-for-Vision-Language-Understanding-in-Robotic-Assisted-Surgery" class="headerlink" title="Can DeepSeek Reason Like a Surgeon? An Empirical Evaluation for   Vision-Language Understanding in Robotic-Assisted Surgery"></a>Can DeepSeek Reason Like a Surgeon? An Empirical Evaluation for   Vision-Language Understanding in Robotic-Assisted Surgery</h2><p><strong>Authors:Boyi Ma, Yanguang Zhao, Jie Wang, Guankun Wang, Kun Yuan, Tong Chen, Long Bai, Hongliang Ren</strong></p>
<p>DeepSeek series have demonstrated outstanding performance in general scene understanding, question-answering (QA), and text generation tasks, owing to its efficient training paradigm and strong reasoning capabilities. In this study, we investigate the dialogue capabilities of the DeepSeek model in robotic surgery scenarios, focusing on tasks such as Single Phrase QA, Visual QA, and Detailed Description. The Single Phrase QA tasks further include sub-tasks such as surgical instrument recognition, action understanding, and spatial position analysis. We conduct extensive evaluations using publicly available datasets, including EndoVis18 and CholecT50, along with their corresponding dialogue data. Our comprehensive evaluation results indicate that, when provided with specific prompts, DeepSeek-V3 performs well in surgical instrument and tissue recognition tasks However, DeepSeek-V3 exhibits significant limitations in spatial position analysis and struggles to understand surgical actions accurately. Additionally, our findings reveal that, under general prompts, DeepSeek-V3 lacks the ability to effectively analyze global surgical concepts and fails to provide detailed insights into surgical scenarios. Based on our observations, we argue that the DeepSeek-V3 is not ready for vision-language tasks in surgical contexts without fine-tuning on surgery-specific datasets. </p>
<blockquote>
<p>DeepSeekç³»åˆ—æ¨¡å‹åœ¨æ•´ä½“åœºæ™¯ç†è§£ã€é—®ç­”ï¼ˆQAï¼‰å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œè¿™å½’åŠŸäºå…¶é«˜æ•ˆçš„è®­ç»ƒæ¨¡å¼å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶DeepSeekæ¨¡å‹åœ¨æœºå™¨äººæ‰‹æœ¯åœºæ™¯ä¸­çš„å¯¹è¯èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨è¯¸å¦‚ç®€å•é—®ç­”ã€è§†è§‰é—®ç­”å’Œè¯¦ç»†æè¿°ç­‰ä»»åŠ¡ã€‚ç®€å•é—®ç­”ä»»åŠ¡è¿›ä¸€æ­¥åŒ…æ‹¬æ‰‹æœ¯å™¨æ¢°è¯†åˆ«ã€åŠ¨ä½œç†è§£å’Œç©ºé—´ä½ç½®åˆ†æç­‰å­ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨äº†åŒ…æ‹¬EndoVis18å’ŒCholecT50åœ¨å†…çš„å…¬å¼€æ•°æ®é›†åŠå…¶ç›¸åº”çš„å¯¹è¯æ•°æ®è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æˆ‘ä»¬çš„å…¨é¢è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨æä¾›ç‰¹å®šæç¤ºçš„æƒ…å†µä¸‹ï¼ŒDeepSeek-V3åœ¨æ‰‹æœ¯å™¨æ¢°å’Œç»„ç»‡è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚ç„¶è€Œï¼ŒDeepSeek-V3åœ¨ç©ºé—´ä½ç½®åˆ†ææ–¹é¢å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œä¸”éš¾ä»¥å‡†ç¡®ç†è§£æ‰‹æœ¯åŠ¨ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜å‘ç°ï¼Œåœ¨ä¸€èˆ¬æç¤ºä¸‹ï¼ŒDeepSeek-V3ç¼ºä¹åˆ†æå…¨å±€æ‰‹æœ¯æ¦‚å¿µçš„èƒ½åŠ›ï¼Œæ— æ³•ä¸ºæ‰‹æœ¯åœºæ™¯æä¾›è¯¦ç»†çš„è§è§£ã€‚åŸºäºæˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬è®¤ä¸ºDeepSeek-V3åœ¨æœªè¿›è¡Œæ‰‹æœ¯ç‰¹å®šæ•°æ®é›†å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œå°šä¸å…·å¤‡å¤„ç†æ‰‹æœ¯ä¸Šä¸‹æ–‡ä¸­çš„è§†è§‰è¯­è¨€ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23130v2">PDF</a> Technical Report</p>
<p><strong>Summary</strong><br>DeepSeekæ¨¡å‹åœ¨åœºæ™¯ç†è§£ã€é—®ç­”å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†DeepSeekæ¨¡å‹åœ¨æœºå™¨äººæ‰‹æœ¯åœºæ™¯ä¸­çš„å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ­è¯­é—®ç­”ã€è§†è§‰é—®ç­”å’Œè¯¦ç»†æè¿°ç­‰ä»»åŠ¡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒDeepSeek-V3åœ¨æ‰‹æœ¯å™¨æ¢°å’Œç»„ç»‡è¯†åˆ«æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç©ºé—´ä½ç½®åˆ†æå’Œæ‰‹æœ¯åŠ¨ä½œç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚å¯¹äºä¸€èˆ¬æç¤ºï¼ŒDeepSeek-V3ç¼ºä¹åˆ†æå…¨å±€æ‰‹æœ¯æ¦‚å¿µçš„èƒ½åŠ›ï¼Œæ— æ³•æä¾›æ‰‹æœ¯åœºæ™¯çš„è¯¦ç»†è§è§£ã€‚å› æ­¤ï¼Œå»ºè®®å¯¹æ‰‹æœ¯ç‰¹å®šæ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ‰‹æœ¯åœºæ™¯ä¸­çš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeekæ¨¡å‹åœ¨åœºæ™¯ç†è§£ã€é—®ç­”å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†DeepSeekæ¨¡å‹åœ¨æœºå™¨äººæ‰‹æœ¯åœºæ™¯ä¸­çš„å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ­è¯­é—®ç­”ã€è§†è§‰é—®ç­”å’Œè¯¦ç»†æè¿°ç­‰ä»»åŠ¡ã€‚</li>
<li>DeepSeek-V3åœ¨æ‰‹æœ¯å™¨æ¢°å’Œç»„ç»‡è¯†åˆ«æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚</li>
<li>DeepSeek-V3åœ¨ç©ºé—´ä½ç½®åˆ†æå’Œæ‰‹æœ¯åŠ¨ä½œç†è§£æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>å¯¹äºä¸€èˆ¬æç¤ºï¼ŒDeepSeek-V3ç¼ºä¹åˆ†æå…¨å±€æ‰‹æœ¯æ¦‚å¿µçš„èƒ½åŠ›ã€‚</li>
<li>DeepSeek-V3éœ€è¦æä¾›æ‰‹æœ¯ç‰¹å®šæ•°æ®é›†çš„å¾®è°ƒï¼Œä»¥åº”å¯¹æ‰‹æœ¯åœºæ™¯ä¸­çš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-257cb6bb5bca4ebb1775cb8cb402ad0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a43d4bf22d05bb6dfba535fb9fdcc013.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35ad46013153f402d9780c517d60d457.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VGRP-Bench-Visual-Grid-Reasoning-Puzzle-Benchmark-for-Large-Vision-Language-Models"><a href="#VGRP-Bench-Visual-Grid-Reasoning-Puzzle-Benchmark-for-Large-Vision-Language-Models" class="headerlink" title="VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large   Vision-Language Models"></a>VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large   Vision-Language Models</h2><p><strong>Authors:Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine SÃ¼sstrunk, Filippos Kokkinos</strong></p>
<p>Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, as it reflects their ability to engage in structured reasoning - an essential skill for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, highlighting fundamental limitations in their puzzle-solving capabilities. Most importantly, through systematic experiments, we identify and analyze key factors influencing LVLMsâ€™ puzzle-solving performance, including the number of clues, grid size, and rule complexity. Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. We will release VGRP-Bench to facilitate further research on LVLMs for complex, real-world problem-solving. Project page: <a target="_blank" rel="noopener" href="https://yufan-ren.com/subpage/VGRP-Bench/">https://yufan-ren.com/subpage/VGRP-Bench/</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†éœ€è¦ç²¾ç¡®æ„ŸçŸ¥ã€è§„åˆ™ç†è§£å’Œé€»è¾‘æ¨ç†çš„è°œé¢˜æ—¶é¢ä¸´å›°éš¾ã€‚åœ¨è¿™ä¸ªé¢†åŸŸè¯„ä¼°å’Œæé«˜å®ƒä»¬çš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œè¿™åæ˜ äº†å®ƒä»¬è¿›è¡Œç»“æ„åŒ–æ¨ç†çš„èƒ½åŠ›â€”â€”è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„åŸºæœ¬æŠ€èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ²¡æœ‰è¿›è¡Œé¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œé€šå¸¸ç¼ºä¹å¯¹æ¨ç†çš„ä¸“æ³¨ï¼Œå¹¶ä¸”æ— æ³•å»ºç«‹ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†VGRP-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰ç½‘æ ¼æ¨ç†è°œé¢˜åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«20ä¸ªå¤šæ ·åŒ–çš„è°œé¢˜ã€‚VGRP-Benchæ¶µç›–å¤šä¸ªéš¾åº¦çº§åˆ«ï¼Œä¸ä»…åŒ…æ‹¬å¯¹ç°æœ‰èŠå¤©LVLMsï¼ˆä¾‹å¦‚GPT-4oï¼‰çš„å®éªŒï¼Œè¿˜åŒ…æ‹¬å¯¹æ¨ç†LVLMsï¼ˆä¾‹å¦‚Gemini-Thinkingï¼‰çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LVLMsåœ¨è¿™äº›è°œé¢˜ä¸­ä¹Ÿé¢ä¸´å›°éš¾ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨è§£è°œèƒ½åŠ›ä¸Šçš„æ ¹æœ¬å±€é™æ€§ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œé€šè¿‡ç³»ç»Ÿçš„å®éªŒï¼Œæˆ‘ä»¬ç¡®å®šäº†å½±å“LVLMsè§£è°œæ€§èƒ½çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬çº¿ç´¢çš„æ•°é‡ã€ç½‘æ ¼å¤§å°å’Œè§„åˆ™å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç­–ç•¥ï¼Œå¯ä»¥åœ¨è®­ç»ƒåä½¿ç”¨ï¼šå¯¹è§£å†³æ–¹æ¡ˆçš„SFTï¼ˆS-SFTï¼‰å’Œå¯¹åˆæˆæ¨ç†è¿‡ç¨‹çš„SFTï¼ˆR-SFTï¼‰ã€‚è™½ç„¶è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½æ˜¾è‘—æé«˜è®­ç»ƒè°œé¢˜çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨æœªè§è¿‡çš„è°œé¢˜ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬å°†å‘å¸ƒVGRP-Benchï¼Œä»¥ä¿ƒè¿›å¯¹LVLMsè¿›è¡Œå¤æ‚ã€ç°å®ä¸–ç•Œçš„è§£å†³é—®é¢˜çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://yufan-ren.com/subpage/VGRP-Bench/%E3%80%82">https://yufan-ren.com/subpage/VGRP-Bench/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23064v2">PDF</a> 8 pages; Project page: <a target="_blank" rel="noopener" href="https://yufan-ren.com/subpage/VGRP-Bench/">https://yufan-ren.com/subpage/VGRP-Bench/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§£å†³éœ€è¦ç²¾ç¡®æ„ŸçŸ¥ã€è§„åˆ™ç†è§£å’Œé€»è¾‘æ¨ç†çš„è°œé¢˜æ—¶çš„æŒ‘æˆ˜ã€‚ä½œè€…é’ˆå¯¹ç°æœ‰è¯„ä¼°æ–¹å¼çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰ç½‘æ ¼æ¨ç†è°œé¢˜åŸºå‡†æµ‹è¯•ï¼ˆVGRP-Benchï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°LVLMsçš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LVLMsåœ¨è¿™äº›è°œé¢˜ä¸Šä¹Ÿå­˜åœ¨å›°éš¾ï¼Œå¹¶æŒ‡å‡ºäº†å½±å“LVLMsè§£è°œèƒ½åŠ›çš„å…³é”®å› ç´ ã€‚åŒæ—¶ï¼Œæ¢ç´¢äº†ä¸¤ç§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç­–ç•¥ï¼Œå³åŸºäºè§£å†³æ–¹æ¡ˆçš„SFTï¼ˆS-SFTï¼‰å’ŒåŸºäºåˆæˆæ¨ç†è¿‡ç¨‹çš„SFTï¼ˆR-SFTï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨æé«˜å·²è®­ç»ƒè°œé¢˜æ€§èƒ½çš„åŒæ—¶ï¼Œå¯¹æœªè§è¿‡çš„è°œé¢˜æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsåœ¨è§£å†³éœ€è¦ç²¾ç¡®æ„ŸçŸ¥ã€è§„åˆ™ç†è§£å’Œé€»è¾‘æ¨ç†çš„è°œé¢˜æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹å¼å­˜åœ¨ä¸è¶³ï¼Œç¼ºä¹ä¸“é—¨é’ˆå¯¹LVLMsæ¨ç†èƒ½åŠ›çš„ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>VGRP-Benchæ—¨åœ¨è¯„ä¼°LVLMsçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šç§éš¾åº¦çº§åˆ«çš„20ä¸ªå¤šæ ·åŒ–è°œé¢˜ã€‚</li>
<li>æœ€å…ˆè¿›çš„LVLMsåœ¨è°œé¢˜è§£å†³ä¸Šä»æœ‰å›°éš¾ï¼Œè¡¨æ˜å­˜åœ¨åŸºç¡€èƒ½åŠ›é™åˆ¶ã€‚</li>
<li>å½±å“LVLMsè§£è°œèƒ½åŠ›çš„å…³é”®å› ç´ åŒ…æ‹¬çº¿ç´¢æ•°é‡ã€ç½‘æ ¼å¤§å°å’Œè§„åˆ™å¤æ‚æ€§ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ç­–ç•¥å¯ä»¥æé«˜LVLMsåœ¨è®­ç»ƒè°œé¢˜ä¸Šçš„æ€§èƒ½ï¼Œä½†å¯¹æœªè§è°œé¢˜çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-744f1be2b1df1e547a7ee4e2b346da3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515dad0084494691fe3cee96c21a1cbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82d5217368685ce986bb50261e00fda0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-095b349e3c3f42b52c7e5ef59e2548b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cf816cf4eec631d4fbdbd6f66ecb01c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fc3f70d30e6fb291ed43668798e8aad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Adaptive-Interactive-Navigation-of-Quadruped-Robots-using-Large-Language-Models"><a href="#Adaptive-Interactive-Navigation-of-Quadruped-Robots-using-Large-Language-Models" class="headerlink" title="Adaptive Interactive Navigation of Quadruped Robots using Large Language   Models"></a>Adaptive Interactive Navigation of Quadruped Robots using Large Language   Models</h2><p><strong>Authors:Kangjie Zhou, Yao Mu, Haoyang Song, Yi Zeng, Pengying Wu, Han Gao, Chang Liu</strong></p>
<p>Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within free space, struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this gap, we propose an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to reach originally unavailable goals. Specifically, we present a primitive tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning method featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan modification in unknown environments. Comprehensive simulations and experiments have demonstrated our methodâ€™s effectiveness and adaptivity in diverse scenarios. The supplementary video is available at page: <a target="_blank" rel="noopener" href="https://youtu.be/W5ttPnSap2g">https://youtu.be/W5ttPnSap2g</a>. </p>
<blockquote>
<p>æœºå™¨äººå¯¼èˆªåœ¨å¤æ‚ç¯å¢ƒä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¯¼èˆªæ–¹æ³•ä¸»è¦å…³æ³¨åœ¨è‡ªç”±ç©ºé—´å†…ç”Ÿæˆæœ€ä¼˜è½¨è¿¹ï¼Œä½†åœ¨ç¼ºä¹åˆ°è¾¾ç›®æ ‡ç‚¹çš„å¯è¡Œè·¯å¾„çš„ç¯å¢ƒä¸­ï¼Œå¦‚ç¾åŒºæˆ–æ‚ä¹±çš„ä»“åº“ç­‰ï¼Œè¿™äº›æ–¹æ³•ä¼šé¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”äº¤äº’å¯¼èˆªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸»åŠ¨ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ä»¥åˆ›å»ºå¯è¡Œçš„è·¯å¾„ï¼Œä»è€Œè¾¾åˆ°åŸæœ¬æ— æ³•åˆ°è¾¾çš„ç›®æ ‡ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»»åŠ¡è§„åˆ’åŸå§‹æ ‘ï¼Œé€šè¿‡æœ‰æ•ˆçš„æ¨ç†æ¥ç¡®å®šäº¤äº’å¯¹è±¡å’Œåºåˆ—ã€‚ä¸ºäº†ç¡®ä¿å­ä»»åŠ¡çš„ç¨³å¥æ‰§è¡Œï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥é¢„è®­ç»ƒä¸€ä¸ªåŒ…å«é€šç”¨è¿åŠ¨èƒ½åŠ›å’Œäº¤äº’è¡Œä¸ºçš„ç»¼åˆæŠ€èƒ½åº“ï¼Œç”¨äºè¿åŠ¨è§„åˆ’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„é‡æ–°è§„åˆ’æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªåŸºäºLLMçš„æ¨¡å—ï¼šä¸€ä¸ªä½œä¸ºçµæ´»é‡æ–°è§„åˆ’è§¦å‘å™¨çš„é¡¾é—®å’Œä¸€ä¸ªç”¨äºè‡ªä¸»è®¡åˆ’è°ƒæ•´çš„é€ æ—å¸ˆã€‚ä¸æ ‘ç»“æ„ç›¸ç»“åˆï¼Œé‡æ–°è§„åˆ’æœºåˆ¶ä¾¿äºèŠ‚ç‚¹çš„æ·»åŠ å’Œä¿®å‰ªï¼Œèƒ½å¤Ÿåœ¨æœªçŸ¥ç¯å¢ƒä¸­å¿«é€Ÿä¿®æ”¹è®¡åˆ’ã€‚ç»¼åˆæ¨¡æ‹Ÿå’Œå®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚è¡¥å……è§†é¢‘å¯åœ¨é¡µé¢è§‚çœ‹ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/W5ttPnSap2g%E3%80%82">https://youtu.be/W5ttPnSap2gã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22942v1">PDF</a> 10 pages, 9 figures</p>
<p><strong>Summary</strong>ï¼š<br>åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°æœºå™¨äººå¯¼èˆªæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„å¯¼èˆªæ–¹æ³•ä¸»è¦å…³æ³¨åœ¨è‡ªç”±ç©ºé—´å†…ç”Ÿæˆæœ€ä¼˜è½¨è¿¹ï¼Œä½†åœ¨å¦‚ç¾éš¾ç°åœºæˆ–æ‹¥æŒ¤ä»“åº“ç­‰ç¼ºä¹å¯è¡Œè·¯å¾„çš„ç¯å¢ƒé‡Œè¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”äº¤äº’å¯¼èˆªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¸»åŠ¨ä¸ç¯å¢ƒäº¤äº’ä»¥åˆ›å»ºåˆ°è¾¾åŸæœ¬ä¸å¯è¾¾ç›®æ ‡çš„å¯è¡Œè·¯å¾„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºä»»åŠ¡è§„åˆ’çš„åŸç”Ÿæ ‘æ¨¡å‹ï¼Œå¹¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰æ•ˆæ¨ç†ï¼Œä»¥ç¡®å®šäº¤äº’å¯¹è±¡å’Œåºåˆ—ã€‚ä¸ºä¿è¯ç¨³å¥çš„æ¬¡ä»»åŠ¡æ‰§è¡Œï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥é¢„è®­ç»ƒåŒ…å«å„ç§è¿åŠ¨å’Œè¡Œä¸ºæŠ€èƒ½çš„æŠ€èƒ½åº“ï¼Œç”¨äºè¿åŠ¨è§„åˆ’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡è§„åˆ’æ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å—ï¼šä½œä¸ºçµæ´»é‡è§„åˆ’è§¦å‘å™¨çš„é¡¾é—®å’Œç”¨äºè‡ªä¸»è®¡åˆ’è°ƒæ•´çš„é€ æ—å¸ˆã€‚ç»“åˆæ ‘ç»“æ„ï¼Œé‡è§„åˆ’æœºåˆ¶ä¾¿äºèŠ‚ç‚¹å¢åŠ å’Œä¿®å‰ªï¼Œèƒ½åœ¨æœªçŸ¥ç¯å¢ƒä¸­å¿«é€Ÿè°ƒæ•´è®¡åˆ’ã€‚ä»¿çœŸå’Œå®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§è¡¥å……è§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/W5ttPnSap2g">https://youtu.be/W5ttPnSap2g</a>ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœºå™¨äººå¯¼èˆªåœ¨å¤æ‚ç¯å¢ƒä¸­ä»ç„¶æ˜¯ä¸€ä¸ªç ”ç©¶æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹å¯è¡Œè·¯å¾„çš„ç¯å¢ƒä¸­ã€‚</li>
<li>ä¼ ç»Ÿå¯¼èˆªæ–¹æ³•ä¸»è¦å…³æ³¨åœ¨è‡ªç”±ç©ºé—´å†…ç”Ÿæˆæœ€ä¼˜è½¨è¿¹ï¼Œä½†åœ¨æŸäº›ç¯å¢ƒä¸‹æ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”äº¤äº’å¯¼èˆªæ–¹æ³•ï¼Œèƒ½ä¸»åŠ¨ä¸ç¯å¢ƒäº¤äº’ä»¥åˆ›å»ºå¯è¡Œè·¯å¾„ã€‚</li>
<li>ä½¿ç”¨åŸç”Ÿæ ‘æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä»»åŠ¡è§„åˆ’å’Œæ¨ç†ï¼Œç¡®å®šäº¤äº’å¯¹è±¡å’Œåºåˆ—ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒåŒ…å«å„ç§è¿åŠ¨å’Œè¡Œä¸ºæŠ€èƒ½çš„æŠ€èƒ½åº“ï¼Œç”¨äºè¿åŠ¨è§„åˆ’ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡è§„åˆ’æ–¹æ³•ï¼ŒåŒ…å«ä¸¤ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å—ï¼Œä»¥åº”å¯¹æœªçŸ¥ç¯å¢ƒä¸­çš„å¿«é€Ÿè®¡åˆ’è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e142cd0d0b06a7ddea96809beac13325.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed4915166824052907a809d5d7e17d68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35bf2682f3dba4b23299438226db64f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca60c32e0474c2c4fec8d7e4a63270f2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Q-Insight-Understanding-Image-Quality-via-Visual-Reinforcement-Learning"><a href="#Q-Insight-Understanding-Image-Quality-via-Visual-Reinforcement-Learning" class="headerlink" title="Q-Insight: Understanding Image Quality via Visual Reinforcement Learning"></a>Q-Insight: Understanding Image Quality via Visual Reinforcement Learning</h2><p><strong>Authors:Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, Jian Zhang</strong></p>
<p>Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods in both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization to comparison reasoning tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/lwq20020127/Q-Insight">https://github.com/lwq20020127/Q-Insight</a>. </p>
<blockquote>
<p>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä¸“æ³¨äºå›¾åƒçš„ä¸»è§‚è§†è§‰è´¨é‡ï¼Œåœ¨å›¾åƒé‡å»ºã€å‹ç¼©å’Œç”Ÿæˆç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æ‹“å®½äº†IQAçš„èŒƒå›´ï¼Œæœç€ç»¼åˆå›¾åƒè´¨é‡ç†è§£çš„æ–¹å‘å‘å±•ï¼Œè¿™ç»“åˆäº†å†…å®¹åˆ†æã€é€€åŒ–æ„ŸçŸ¥å’Œæ¯”è¾ƒæ¨ç†ï¼Œè¶…è¶Šäº†ä»…ä»…çš„æ•°å­—è¯„åˆ†ã€‚ä¹‹å‰çš„åŸºäºMLLMçš„æ–¹æ³•é€šå¸¸è¦ä¹ˆç”Ÿæˆç¼ºä¹è§£é‡Šæ€§çš„æ•°å­—åˆ†æ•°ï¼Œè¦ä¹ˆä¸¥é‡ä¾èµ–äºä½¿ç”¨å¤§è§„æ¨¡æ³¨é‡Šæ•°æ®é›†è¿›è¡Œçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æä¾›æè¿°æ€§è¯„ä¼°ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„Q-Insightæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å»ºç«‹åœ¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¹‹ä¸Šï¼Œå±•ç°å‡ºå¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œç”¨äºç†è§£å›¾åƒè´¨é‡ï¼Œåªéœ€æœ‰é™çš„ä¸»è§‚è¯„åˆ†å’Œé€€åŒ–æ ‡ç­¾ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ï¼Œè”åˆä¼˜åŒ–è¯„åˆ†å›å½’å’Œé€€åŒ–æ„ŸçŸ¥ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†å®ƒä»¬çš„ç›¸äº’ä¼˜åŠ¿ï¼Œæé«˜äº†æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒQ-Insightåœ¨è¯„åˆ†å›å½’å’Œé€€åŒ–æ„ŸçŸ¥ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶åœ¨é›¶æ ·æœ¬æ¯”è¾ƒæ¨ç†ä»»åŠ¡ä¸Šå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lwq20020127/Q-Insight%E3%80%82">https://github.com/lwq20020127/Q-Insightã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22679v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œåä¸ºQ-Insightï¼Œç”¨äºå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œèƒ½å¤Ÿæ·±å…¥ç†è§£å›¾åƒè´¨é‡ï¼ŒåŒæ—¶åªéœ€å°‘é‡çš„è¯„åˆ†å’Œé™çº§æ ‡ç­¾ã€‚é€šè¿‡ä¼˜åŒ–è¯„åˆ†å›å½’å’Œé™çº§æ„ŸçŸ¥ä»»åŠ¡ï¼Œç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œè¯¥æ–¹æ³•å……åˆ†åˆ©ç”¨ä¸¤è€…çš„äº’ç›Šä¼˜åŠ¿æå‡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒQ-Insightåœ¨è¯„åˆ†å›å½’å’Œé™çº§æ„ŸçŸ¥ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨æ¯”è¾ƒæ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰å…³æ³¨å›¾åƒçš„æ„ŸçŸ¥è§†è§‰è´¨é‡ï¼Œåœ¨å›¾åƒé‡å»ºã€å‹ç¼©å’Œç”Ÿæˆç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ‹“å®½äº†IQAçš„èŒƒå›´ï¼Œä½¿å…¶èƒ½å¤Ÿå…¨é¢ç†è§£å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬å†…å®¹åˆ†æã€é€€åŒ–æ„ŸçŸ¥å’Œæ¯”è¾ƒæ¨ç†ç­‰æ–¹é¢ã€‚</li>
<li>ä»¥å¾€çš„MLLMæ–¹æ³•è¦ä¹ˆäº§ç”Ÿç¼ºä¹å¯è§£é‡Šæ€§çš„æ•°å€¼åˆ†æ•°ï¼Œè¦ä¹ˆä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†è¿›è¡Œæè¿°æ€§è¯„ä¼°ï¼Œé™åˆ¶äº†å…¶çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚</li>
<li>Q-Insightæ–¹æ³•ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæé«˜äº†å›¾åƒè´¨é‡ç†è§£çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Q-Insightæ–¹æ³•åªéœ€å°‘é‡çš„è¯„åˆ†å’Œé™çº§æ ‡ç­¾ï¼Œé€šè¿‡ä¼˜åŒ–è¯„åˆ†å›å½’å’Œé™çº§æ„ŸçŸ¥ä»»åŠ¡ï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°ï¼Œå®ç°å¢å¼ºæ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒQ-Insightåœ¨è¯„åˆ†å›å½’å’Œé™çº§æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e629e02f2f11a26e0e8d7a4e4742f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-692b7e00701e03e520218d53d314d6f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e63fec0fbc4ddb79c3739ea3a86a04f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ed52c24329c00356653f50973ded2d3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="QuestBench-Can-LLMs-ask-the-right-question-to-acquire-information-in-reasoning-tasks"><a href="#QuestBench-Can-LLMs-ask-the-right-question-to-acquire-information-in-reasoning-tasks" class="headerlink" title="QuestBench: Can LLMs ask the right question to acquire information in   reasoning tasks?"></a>QuestBench: Can LLMs ask the right question to acquire information in   reasoning tasks?</h2><p><strong>Authors:Belinda Z. Li, Been Kim, Zi Wang</strong></p>
<p>Recently, a large amount of work has focused on improving large language modelsâ€™ (LLMsâ€™) performance on reasoning benchmarks such as math and logic. However, past work has largely assumed that tasks are well-defined. In the real world, queries to LLMs are often underspecified, only solvable through acquiring missing information. We formalize this as a constraint satisfaction problem (CSP) with missing variable assignments. Using a special case of this formalism where only one necessary variable assignment is missing, we can rigorously evaluate an LLMâ€™s ability to identify the minimal necessary question to ask and quantify axes of difficulty levels for each problem. We present QuestBench, a set of underspecified reasoning tasks solvable by asking at most one question, which includes: (1) Logic-Q: Logical reasoning tasks with one missing proposition, (2) Planning-Q: PDDL planning problems with initial states that are partially-observed, (3) GSM-Q: Human-annotated grade school math problems with one missing variable assignment, and (4) GSME-Q: a version of GSM-Q where word problems are translated into equations by human annotators. The LLM is tasked with selecting the correct clarification question(s) from a list of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their accuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that the ability to solve well-specified reasoning problems may not be sufficient for success on our benchmark: models have difficulty identifying the right question to ask, even when they can solve the fully specified version of the problem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even when explicitly presented with the option to predict &#96;&#96;not sure.â€™â€™ This highlights the need for deeper investigation into modelsâ€™ information acquisition capabilities. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§é‡çš„å·¥ä½œé›†ä¸­åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯¸å¦‚æ•°å­¦å’Œé€»è¾‘ç­‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿‡å»çš„å·¥ä½œå¤§å¤šå‡è®¾ä»»åŠ¡æ˜¯æ˜ç¡®ç•Œå®šå¥½çš„ã€‚åœ¨çœŸå®ä¸–ç•Œä¸­ï¼Œå¯¹LLMçš„æŸ¥è¯¢é€šå¸¸æ˜¯æœªæ˜ç¡®æŒ‡å®šçš„ï¼Œåªèƒ½é€šè¿‡è·å–ç¼ºå¤±çš„ä¿¡æ¯æ¥è§£å†³ã€‚æˆ‘ä»¬å°†è¿™æ­£å¼åŒ–ä¸ºä¸€ä¸ªå…·æœ‰ç¼ºå¤±å˜é‡èµ‹å€¼çš„çº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰ã€‚é€šè¿‡ä½¿ç”¨è¿™ç§å½¢å¼çš„ç‰¹æ®Šæƒ…å†µï¼Œå…¶ä¸­ä»…ç¼ºå°‘ä¸€ä¸ªå¿…è¦çš„å˜é‡èµ‹å€¼ï¼Œæˆ‘ä»¬å¯ä»¥ä¸¥æ ¼è¯„ä¼°LLMç¡®å®šè¦è¯¢é—®çš„æœ€å°å¿…è¦é—®é¢˜çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜é‡åŒ–éš¾åº¦çº§åˆ«çš„è½´ã€‚æˆ‘ä»¬æå‡ºäº†QuestBenchï¼Œè¿™æ˜¯ä¸€ç»„æœ€å¤šé€šè¿‡é—®ä¸€ä¸ªé—®é¢˜å°±å¯ä»¥è§£å†³çš„æœªæ˜ç¡®æŒ‡å®šçš„æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰Logic-Qï¼šé€»è¾‘æ¨ç†ä»»åŠ¡ä¸­æœ‰ä¸€ä¸ªç¼ºå¤±çš„å‘½é¢˜ï¼›ï¼ˆ2ï¼‰Planning-Qï¼šPDDLè§„åˆ’é—®é¢˜çš„åˆå§‹çŠ¶æ€æ˜¯éƒ¨åˆ†è§‚å¯Ÿåˆ°çš„ï¼›ï¼ˆ3ï¼‰GSM-Qï¼šäººç±»æ³¨é‡Šçš„å°å­¦æ•°å­¦é—®é¢˜ä¸­æœ‰ä¸€ä¸ªç¼ºå¤±çš„å˜é‡èµ‹å€¼ï¼›ï¼ˆ4ï¼‰GSME-Qï¼šGSM-Qçš„ä¸€ä¸ªç‰ˆæœ¬ï¼Œå…¶ä¸­äººç±»æ³¨é‡Šè€…å°†æ–‡å­—é—®é¢˜ç¿»è¯‘ä¸ºæ–¹ç¨‹å¼ã€‚LLMçš„ä»»åŠ¡æ˜¯ä»é€‰é¡¹åˆ—è¡¨ä¸­é€‰æ‹©æ­£ç¡®çš„æ¾„æ¸…é—®é¢˜ã€‚è™½ç„¶æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨GSM-Qå’ŒGSME-Qæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨Logic-Qå’ŒPlanning-Qä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º40-50%ã€‚åˆ†æè¡¨æ˜ï¼Œè§£å†³æ˜ç¡®æŒ‡å®šçš„æ¨ç†é—®é¢˜çš„èƒ½åŠ›å¯èƒ½ä¸è¶³ä»¥åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æˆåŠŸï¼šå³ä½¿èƒ½å¤Ÿè§£å†³å®Œå…¨æŒ‡å®šçš„ç‰ˆæœ¬çš„é—®é¢˜ï¼Œæ¨¡å‹ä¹Ÿå¾ˆéš¾ç¡®å®šè¦é—®çš„æ­£ç¡®é—®é¢˜ã€‚æ­¤å¤–ï¼Œåœ¨Planning-Qé¢†åŸŸï¼ŒLLMå¾€å¾€ä¸ä¼šçŠ¹è±«ï¼Œå³ä½¿æ˜ç¡®æä¾›äº†é€‰æ‹©â€œä¸ç¡®å®šâ€çš„é€‰é¡¹ã€‚è¿™çªæ˜¾äº†å¯¹æ¨¡å‹çš„è·å–ä¿¡æ¯èƒ½åŠ›è¿›è¡Œæ›´æ·±å…¥è°ƒæŸ¥çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22674v1">PDF</a> Code and dataset are available at   \url{<a target="_blank" rel="noopener" href="https://github.com/google-deepmind/questbench%7D">https://github.com/google-deepmind/questbench}</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åº”å¯¹æ•°å­¦å’Œé€»è¾‘ç­‰æ¨ç†åŸºå‡†æµ‹è¯•æ–¹é¢çš„æ€§èƒ½æå‡å·²ç»å¸å¼•äº†å¤§é‡çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿‡å»çš„å·¥ä½œå¾€å¾€å‡å®šä»»åŠ¡å·²ç»å®šä¹‰æ˜ç¡®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯¹LLMsçš„æŸ¥è¯¢é€šå¸¸æ˜¯æœªå…·ä½“æŒ‡å®šçš„ï¼Œåªæœ‰é€šè¿‡è·å–ç¼ºå¤±ä¿¡æ¯æ‰èƒ½è§£å†³ã€‚æœ¬ç ”ç©¶å°†å…¶å½¢å¼åŒ–ä¸ºä¸€ä¸ªå¸¦æœ‰ç¼ºå¤±å˜é‡èµ‹å€¼çš„çº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰ã€‚é€šè¿‡è¿™ä¸€å½¢å¼ä¸»ä¹‰çš„ç‰¹æ®Šæ¡ˆä¾‹ï¼Œæˆ‘ä»¬èƒ½ä¸¥æ ¼è¯„ä¼°LLMåœ¨è¯†åˆ«æœ€å°å¿…è¦é—®é¢˜è¯¢é—®æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ä¸ºæ¯ä¸ªé—®é¢˜é‡åŒ–éš¾åº¦ç­‰çº§ã€‚æœ¬ç ”ç©¶æ¨å‡ºäº†QuestBenchï¼Œè¿™æ˜¯ä¸€å¥—å¯é€šè¿‡æœ€å¤šä¸€ä¸ªé—®é¢˜è§£å†³çš„æœªå…·ä½“æŒ‡å®šçš„æ¨ç†ä»»åŠ¡ã€‚åŒ…æ‹¬é€»è¾‘Qã€è§„åˆ’Qã€å°å­¦æ•°å­¦Qä»¥åŠå°å­¦æ–¹ç¨‹Qã€‚å°½ç®¡å‰æ²¿æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é€»è¾‘å’Œè§„åˆ’é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º40-50%ã€‚åˆ†æè¡¨æ˜ï¼Œè§£å†³æ˜ç¡®è§„å®šçš„é—®é¢˜çš„èƒ½åŠ›å¯èƒ½ä¸è¶³ä»¥åœ¨æˆ‘ä»¬è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—æˆåŠŸï¼šæ¨¡å‹åœ¨è¯†åˆ«æ­£ç¡®é—®é¢˜æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå³ä½¿å®ƒä»¬å¯ä»¥è§£å†‘å®Œå…¨æŒ‡å®šçš„é—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œåœ¨è§„åˆ’Qé¢†åŸŸï¼ŒLLMså¾€å¾€ä¸å€¾å‘äºè¿›è¡Œä¸ç¡®å®šæ€§çš„é¢„æµ‹ã€‚è¿™å‡¸æ˜¾äº†å¯¹æ¨¡å‹ä¿¡æ¯è·å–èƒ½åŠ›çš„æ·±å…¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´çš„ä»»åŠ¡å¾€å¾€æ˜¯æœªå…·ä½“æŒ‡å®šçš„ï¼Œéœ€è¦è·å–ç¼ºå¤±ä¿¡æ¯æ¥è§£å†³ã€‚</li>
<li>ç ”ç©¶è€…å°†è¿™ç±»é—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ä¸ªçº¦æŸæ»¡è¶³é—®é¢˜ï¼ˆCSPï¼‰ï¼Œå¹¶å‘ç°LLMsåœ¨è¯†åˆ«å¿…è¦é—®é¢˜æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>QuestBenchåŒ…å«ä¸€ç³»åˆ—æœªå…·ä½“æŒ‡å®šçš„æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬é€»è¾‘ã€è§„åˆ’ã€å°å­¦æ•°å­¦å’Œå°å­¦æ–¹ç¨‹é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é€»è¾‘å’Œè§„åˆ’é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡è¾ƒä½ï¼Œè¡¨æ˜è§£å†³æ˜ç¡®è§„å®šçš„é—®é¢˜çš„èƒ½åŠ›å¯èƒ½ä¸è¶³ä»¥åº”å¯¹æ›´å¤æ‚çš„å®é™…ä»»åŠ¡ã€‚</li>
<li>LLMsåœ¨è¯†åˆ«æ­£ç¡®é—®é¢˜æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå³ä½¿å®ƒä»¬å¯ä»¥è§£å†‘å®Œå…¨æŒ‡å®šçš„é—®é¢˜ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>åœ¨è§„åˆ’é¢†åŸŸï¼ŒLLMså¾€å¾€ä¸å€¾å‘äºé¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œè¿™å¯èƒ½å¯¹æ¨¡å‹çš„æ€§èƒ½å’Œå®é™…åº”ç”¨äº§ç”Ÿå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-212e18495271c356c0b9aafddce637e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6953fc369eff820b6455738e0ac1b6a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training"><a href="#Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training" class="headerlink" title="Unicorn: Text-Only Data Synthesis for Vision Language Model Training"></a>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</h2><p><strong>Authors:Xiaomin Yu, Pengxiang Ding, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, Kejian Wu, Zhaoxin Fan, Ziyue Qiao, Donglin Wang</strong></p>
<p>Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Yu-xm/Unicorn.git">https://github.com/Yu-xm/Unicorn.git</a>. </p>
<blockquote>
<p>è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šå¸¸éœ€è¦å¤§è§„æ¨¡çš„é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹ï¼Œä½†æ”¶é›†æˆ–åˆæˆæ­¤ç±»æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ–‡æœ¬æ•°æ®ä¸°å¯Œä¸”ä»·æ ¼ä½å»‰ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šæ˜¯å¦å¯ä»¥ä»çº¯æ–‡æœ¬ä¸­åˆæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Ÿä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè·¨é›†æˆçš„ä¸‰é˜¶æ®µå¤šæ¨¡æ€æ•°æ®åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šUnicorn-1.2Må’ŒUnicorn-471K-Instructionã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼šå¤šæ ·åŒ–æ ‡é¢˜æ•°æ®åˆæˆä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰©å±•ç¨€ç–æ ‡é¢˜ç§å­ï¼Œæ„å»ºäº†120ä¸‡æ¡è¯­ä¹‰å¤šæ ·çš„é«˜è´¨é‡æ ‡é¢˜ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼šæŒ‡ä»¤è°ƒæ•´æ•°æ®ç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬å°†47.1ä¸‡æ¡æ ‡é¢˜è¿›ä¸€æ­¥å¤„ç†æˆå¤šå›åˆæŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†ã€‚æœ€åï¼Œåœ¨ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡æ€è¡¨ç¤ºè½¬ç§»ä¸­ï¼Œè¿™äº›æ–‡æœ¬æ ‡é¢˜è¡¨ç¤ºè¢«è½¬åŒ–ä¸ºè§†è§‰è¡¨ç¤ºï¼Œä»è€Œäº§ç”Ÿå¤šæ ·åŒ–çš„åˆæˆå›¾åƒè¡¨ç¤ºã€‚è¿™ä¸‰ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºç”¨äºé¢„è®­ç»ƒçš„Unicorn-1.2Må’Œç”¨äºæŒ‡ä»¤è°ƒæ•´çš„Unicorn-471K-Instructionï¼Œè€Œæ— éœ€ä¾èµ–çœŸå®å›¾åƒã€‚é€šè¿‡æ¶ˆé™¤å¯¹çœŸå®å›¾åƒçš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºVLMsè®­ç»ƒæä¾›äº†æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yu-xm/Unicorn.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yu-xm/Unicorn.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22655v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨èåˆçš„ä¸‰é˜¶æ®µå¤šåª’ä½“æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è®­ç»ƒæ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µç”Ÿæˆäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šUnicorn-1.2Må’ŒUnicorn-471K-Instructionã€‚é¦–å…ˆï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰©å±•ç¨€ç–çš„æ ‡é¢˜ç§å­ï¼Œæ„å»º1.2Mè¯­ä¹‰å¤šæ ·çš„é«˜è´¨é‡æ ‡é¢˜ã€‚æ¥ç€ï¼Œå°†471Kæ ‡é¢˜è¿›ä¸€æ­¥å¤„ç†æˆå¤šè½®æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†ã€‚æœ€åï¼Œå°†è¿™äº›æ–‡æœ¬æ ‡é¢˜è¡¨ç¤ºè½¬åŒ–ä¸ºè§†è§‰è¡¨ç¤ºï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆå›¾åƒè¡¨ç¤ºã€‚è¯¥æ¡†æ¶æ¶ˆé™¤äº†å¯¹çœŸå®å›¾åƒçš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä¸ºVLMsè®­ç»ƒæä¾›äº†æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§è·¨èåˆçš„ä¸‰é˜¶æ®µå¤šåª’ä½“æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”ŸæˆVLMsçš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>é€šè¿‡è¯¥æ¡†æ¶ï¼Œç”Ÿæˆäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šUnicorn-1.2Må’ŒUnicorn-471K-Instructionã€‚</li>
<li>åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰©å±•ç¨€ç–æ ‡é¢˜ç§å­ï¼Œç”Ÿæˆè¯­ä¹‰å¤šæ ·çš„é«˜è´¨é‡æ ‡é¢˜ã€‚</li>
<li>åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†æ ‡é¢˜è¿›ä¸€æ­¥å¤„ç†æˆå¤šè½®æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†ã€‚</li>
<li>åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œå°†æ–‡æœ¬æ ‡é¢˜è¡¨ç¤ºè½¬åŒ–ä¸ºè§†è§‰è¡¨ç¤ºï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„åˆæˆå›¾åƒã€‚</li>
<li>è¯¥æ¡†æ¶æ¶ˆé™¤äº†å¯¹çœŸå®å›¾åƒçš„ä¾èµ–ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿è¯äº†æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b5a6c5dab262e94f2d924815911a4af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1606f982b34f4fedf82ca91a3b18e688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c80cdd04d88de4dc6bdf90eac117cf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15b81106d17df4069bd1e3f743a551ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae6dd40a40239345a8267f6d75dcf9c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f564bc8a70aca3fa18fce60f6bb5e4f8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CPPO-Accelerating-the-Training-of-Group-Relative-Policy-Optimization-Based-Reasoning-Models"><a href="#CPPO-Accelerating-the-Training-of-Group-Relative-Policy-Optimization-Based-Reasoning-Models" class="headerlink" title="CPPO: Accelerating the Training of Group Relative Policy   Optimization-Based Reasoning Models"></a>CPPO: Accelerating the Training of Group Relative Policy   Optimization-Based Reasoning Models</h2><p><strong>Authors:Zhihang Lin, Mingbao Lin, Yuan Xie, Rongrong Ji</strong></p>
<p>This paper introduces Completion Pruning Policy Optimization (CPPO) to accelerate the training of reasoning models based on Group Relative Policy Optimization (GRPO). GRPO, while effective, incurs high training costs due to the need for sampling multiple completions for each question. Our experiment and theoretical analysis reveals that the number of completions impacts model accuracy yet increases training time multiplicatively, and not all completions contribute equally to policy training â€“ their contribution depends on their relative advantage. To address these issues, we propose CPPO, which prunes completions with low absolute advantages, significantly reducing the number needed for gradient calculation and updates. Additionally, we introduce a dynamic completion allocation strategy to maximize GPU utilization by incorporating additional questions, further enhancing training efficiency. Experimental results demonstrate that CPPO achieves up to $8.32\times$ speedup on GSM8K and $3.51\times$ on Math while preserving or even enhancing the accuracy compared to the original GRPO. We release our code at <a target="_blank" rel="noopener" href="https://github.com/lzhxmu/CPPO">https://github.com/lzhxmu/CPPO</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å®Œæˆå‰ªæç­–ç•¥ä¼˜åŒ–ï¼ˆCPPOï¼‰ï¼Œä»¥åŠ é€ŸåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ¨ç†æ¨¡å‹çš„è®­ç»ƒã€‚è™½ç„¶GRPOå¾ˆæœ‰æ•ˆï¼Œä½†ç”±äºéœ€è¦å¯¹æ¯ä¸ªé—®é¢˜é‡‡æ ·å¤šä¸ªå®Œæˆé¡¹ï¼Œå…¶è®­ç»ƒæˆæœ¬å¾ˆé«˜ã€‚æˆ‘ä»¬çš„å®éªŒå’Œç†è®ºåˆ†æè¡¨æ˜ï¼Œå®Œæˆé¡¹çš„æ•°é‡ä¼šå½±å“æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”ä¼šå‘ˆå€åœ°å¢åŠ è®­ç»ƒæ—¶é—´ï¼Œå¹¶éæ‰€æœ‰çš„å®Œæˆé¡¹éƒ½å¯¹ç­–ç•¥è®­ç»ƒè´¡çŒ®ç›¸åŒâ€”â€”å®ƒä»¬çš„è´¡çŒ®å–å†³äºå…¶ç›¸å¯¹ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CPPOï¼Œå®ƒé€šè¿‡å‰ªé™¤å…·æœ‰ä½ç»å¯¹ä¼˜åŠ¿çš„å®Œæˆé¡¹ï¼Œæ˜¾è‘—å‡å°‘äº†ç”¨äºæ¢¯åº¦è®¡ç®—å’Œæ›´æ–°çš„å®Œæˆé¡¹æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŠ¨æ€å®Œæˆé¡¹åˆ†é…ç­–ç•¥ï¼Œé€šè¿‡åˆå¹¶é¢å¤–çš„é—®é¢˜æ¥æœ€å¤§é™åº¦åœ°æé«˜GPUçš„åˆ©ç”¨ç‡ï¼Œè¿›ä¸€æ­¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCPPOåœ¨GSM8Kä¸Šå®ç°äº†æœ€é«˜è¾¾8.32å€çš„é€Ÿåº¦æå‡ï¼Œåœ¨æ•°å­¦ä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜è¾¾3.51å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒæˆ–ç”šè‡³æé«˜äº†ä¸åŸå§‹GRPOç›¸æ¯”çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lzhxmu/CPPO%E3%80%82">https://github.com/lzhxmu/CPPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22342v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå®Œæˆä¿®å‰ªç­–ç•¥ä¼˜åŒ–ï¼ˆCPPOï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€ŸåŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ¨ç†æ¨¡å‹è®­ç»ƒã€‚ä¸ºè§£å†³GRPOé«˜è®­ç»ƒæˆæœ¬çš„é—®é¢˜ï¼ŒCPPOé€šè¿‡ä¿®å‰ªä½ä¼˜åŠ¿å®Œæˆçš„æ•°é‡å‡å°‘è®¡ç®—æ¢¯åº¦æ‰€éœ€çš„å®Œæˆæ•°é‡ã€‚åŒæ—¶ï¼Œå¼•å…¥åŠ¨æ€å®Œæˆåˆ†é…ç­–ç•¥ä»¥æé«˜GPUåˆ©ç”¨ç‡ï¼Œè¿›ä¸€æ­¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCPPOåœ¨GSM8Kä¸Šå®ç°äº†æœ€é«˜è¾¾8.32å€çš„åŠ é€Ÿï¼Œåœ¨æ•°å­¦ä¸Šå®ç°äº†æœ€é«˜è¾¾3.51å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†ä¸åŸå§‹GRPOçš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CPPOæ—¨åœ¨åŠ é€ŸåŸºäºGRPOçš„æ¨ç†æ¨¡å‹è®­ç»ƒã€‚</li>
<li>GRPOè™½ç„¶æœ‰æ•ˆï¼Œä½†è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œéœ€ä¸ºæ¯ä¸ªé—®é¢˜é‡‡æ ·å¤šä¸ªå®Œæˆã€‚</li>
<li>å®éªŒå’Œç†è®ºåˆ†ææ˜¾ç¤ºå®Œæˆæ•°é‡å½±å“æ¨¡å‹å‡†ç¡®æ€§ï¼Œå¹¶å‘ˆå€å¢åŠ è®­ç»ƒæ—¶é—´ã€‚</li>
<li>CPPOé€šè¿‡ä¿®å‰ªä½ä¼˜åŠ¿å®Œæˆå‡å°‘è®¡ç®—æ¢¯åº¦æ‰€éœ€çš„å®Œæˆæ•°é‡ã€‚</li>
<li>å¼•å…¥åŠ¨æ€å®Œæˆåˆ†é…ç­–ç•¥ä»¥æé«˜GPUåˆ©ç”¨ç‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºCPPOæ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚</li>
<li>ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/lzhxmu/CPPO%E3%80%82">https://github.com/lzhxmu/CPPOã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa30b01d837a2a7bcccbed4e5e5a146d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-817a7924db6f46942187b897b6b388d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72774cecb0ab860cab030b4de4e0d62c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Exploring-Data-Scaling-Trends-and-Effects-in-Reinforcement-Learning-from-Human-Feedback"><a href="#Exploring-Data-Scaling-Trends-and-Effects-in-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Exploring Data Scaling Trends and Effects in Reinforcement Learning from   Human Feedback"></a>Exploring Data Scaling Trends and Effects in Reinforcement Learning from   Human Feedback</h2><p><strong>Authors:Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, Lin Yan</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methodsâ€™ effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®—æ³•æ”¹è¿›ä¸Šï¼Œä½†æç¤ºæ•°æ®æ„å»ºçš„é‡è¦æ€§å´è¢«å¿½è§†äº†ã€‚æœ¬æ–‡è§£å†³äº†è¿™ä¸€ç©ºç™½é¢†åŸŸçš„é—®é¢˜ï¼Œé€šè¿‡æ¢ç´¢RLHFæ€§èƒ½æ‰©å±•ä¸­æ•°æ®é©±åŠ¨çš„ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å¥–åŠ±ç ´è§£å’Œå“åº”å¤šæ ·æ€§é™ä½çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆå¥–åŠ±ç³»ç»Ÿï¼Œç»“åˆæ¨ç†ä»»åŠ¡éªŒè¯å™¨ï¼ˆRTVï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰ï¼Œä»¥ç¼“è§£å¥–åŠ±ç ´è§£çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°å‹çš„æç¤ºé€‰æ‹©æ–¹æ³•Pre-PPOï¼Œä»¥ä¿æŒå“åº”çš„å¤šæ ·æ€§å¹¶æé«˜å­¦ä¹ æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ—©æœŸåœ¨RLHFè®­ç»ƒä¸­ä¼˜å…ˆè¿›è¡Œæ•°å­¦å’Œç¼–ç ä»»åŠ¡å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªæ¨¡å‹å¤§å°ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•å’Œå¯æ‰©å±•æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒRTVæœ€ä¸å®¹æ˜“å—åˆ°å¥–åŠ±ç ´è§£çš„å½±å“ï¼Œå…¶æ¬¡æ˜¯å¸¦æœ‰çœŸå®å¥–åŠ±çš„GenRMï¼Œç„¶åæ˜¯å¸¦æœ‰SFTæœ€ä½³Nå“åº”çš„GenRMã€‚æˆ‘ä»¬çš„ç­–ç•¥èƒ½å¤Ÿè¿…é€Ÿæ•æ‰åˆ°å¾®å¦™çš„ç‰¹å®šä»»åŠ¡å·®å¼‚ï¼Œä»è€Œå¤§å¤§æé«˜äº†æ•´ä½“çš„RLHFæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ä»”ç»†æ„å»ºæ•°æ®çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†å…‹æœRLHFä¸­æ€§èƒ½éšœç¢çš„å®é™…æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22230v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„å…³é”®åœ¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸äººç±»åé¦ˆçš„ç»“åˆï¼ˆRLHFï¼‰ã€‚è¿‘æœŸç ”ç©¶å¤šå…³æ³¨ç®—æ³•æ”¹è¿›ï¼Œå¿½ç•¥äº†æç¤ºæ•°æ®æ„å»ºçš„é‡è¦æ€§ã€‚æœ¬æ–‡é€šè¿‡æ¢ç©¶RLHFæ€§èƒ½æå‡çš„æ•°æ®é©±åŠ¨ç“¶é¢ˆï¼Œè§£å†³äº†å¥–åŠ±ç ´è§£å’Œå“åº”å¤šæ ·æ€§é™ä½çš„é—®é¢˜ã€‚å¼•å…¥ç»“åˆæ¨ç†ä»»åŠ¡éªŒè¯å™¨ï¼ˆRTVï¼‰å’Œç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGenRMï¼‰çš„æ··åˆå¥–åŠ±ç³»ç»Ÿï¼Œä»¥ç¼“è§£å¥–åŠ±ç ´è§£é—®é¢˜ã€‚åŒæ—¶æå‡ºä¸€ç§æ–°å‹çš„æç¤ºé€‰æ‹©æ–¹æ³•Pre-PPOï¼Œä»¥ç»´æŒå“åº”å¤šæ ·æ€§å¹¶æé«˜å­¦ä¹ æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æ—©æœŸåœ¨RLHFè®­ç»ƒä¸­ä¼˜å…ˆè¿›è¡Œæ•°å­¦å’Œç¼–ç ä»»åŠ¡èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ¨¡å‹å°ºå¯¸ä¸Šå‡æœ‰æ•ˆä¸”å¯æ‰©å±•ã€‚å…¶ä¸­RTVæœ€ä¸æ˜“å—åˆ°å¥–åŠ±ç ´è§£çš„å½±å“ï¼Œå…¶æ¬¡æ˜¯ç»“åˆçœŸå®æƒ…å†µçš„GenRMï¼Œæœ€åæ˜¯ä½¿ç”¨SFT Best-of-Nå“åº”çš„GenRMã€‚æœ¬æ–‡çš„ç­–ç•¥æœ‰åŠ©äºè¿…é€Ÿæ•æ‰å¾®å¦™çš„ç‰¹å®šä»»åŠ¡åŒºåˆ«ï¼Œæ˜¾è‘—æé«˜äº†æ•´ä½“çš„RLHFæ€§èƒ½ã€‚å¼ºè°ƒäº†ä»”ç»†æ„å»ºæ•°æ®çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå…‹æœRLHFä¸­çš„æ€§èƒ½éšœç¢æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>æ•°æ®æ„å»ºå¯¹äºRLHFæ€§èƒ½è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬è§£å†³å¥–åŠ±ç ´è§£å’Œç»´æŒå“åº”å¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥æ··åˆå¥–åŠ±ç³»ç»Ÿï¼ˆç»“åˆRTVå’ŒGenRMï¼‰ä»¥ç¼“è§£å¥–åŠ±ç ´è§£é—®é¢˜ã€‚</li>
<li>æå‡ºæ–°å‹æç¤ºé€‰æ‹©æ–¹æ³•Pre-PPOï¼Œæ—¨åœ¨æé«˜å­¦ä¹ æ•ˆç‡å¹¶ç»´æŒå“åº”å¤šæ ·æ€§ã€‚</li>
<li>æ—©æœŸè®­ç»ƒä¸­çš„æ•°å­¦å’Œç¼–ç ä»»åŠ¡å¯¹RLHFæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
<li>å®éªŒéªŒè¯æ‰€ææ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å°ºå¯¸ä¸Šçš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab7b0e5a8f8103642e14a1d7633223b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d69547334cb0e7c26bbea74fde47264.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1633e30a7972c59880f093054363e81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77d251321084d149295d28c35f0a113c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fd0720d589f11ae109f58dcbb423620.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by   Generating Realistic Dermoscopic Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ec05533b496a742a58c549d69e4bd8a0.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  TimeSearch Hierarchical Video Search with Spotlight and Reflection for   Human-like Long Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
