<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-04  ILLUME+ Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-e3a59a108dd9892170ffcc61518b1904.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    21.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    86 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement"><a href="#ILLUME-Illuminating-Unified-MLLM-with-Dual-Visual-Tokenization-and-Diffusion-Refinement" class="headerlink" title="ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement"></a>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement</h2><p><strong>Authors:Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, Hang Xu</strong></p>
<p>We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: <a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/">https://illume-unified-mllm.github.io/</a>. </p>
<blockquote>
<p>我们提出了ILLUME+，它利用双重视觉标记化和扩散解码器来提高深度语义理解和高保真图像生成能力。现有的统一模型在统一模型中同时处理三种基本能力（理解、生成和编辑）时遇到了困难。像变色龙和EMU3这样的模型使用VQGAN进行图像离散化，由于缺乏深度语义交互，它们在视觉理解任务上落后于LLaVA等专家模型。为了缓解这一问题，LaViT和ILLUME采用语义编码器进行标记化，但在图像编辑方面表现欠佳，纹理保存能力较弱。与此同时，Janus系列将输入和输出图像表示分离，限制了它们无缝处理交织的图像文本理解和生成的能力。相比之下，ILLUME+引入了一个统一的双重视觉标记器DualViTok，它既保留了精细纹理又保留了文本对齐语义，同时实现了一种粗到细的图像表示策略用于多模态理解和生成。此外，我们采用扩散模型作为图像解码器来提高生成质量和高效的超分辨率。ILLUME+在统一的MLLM中遵循连续输入、离散输出的方案，并采用渐进式训练程序，支持视觉标记器、MLLM和扩散解码器之间的动态分辨率调整。这种设计使得跨不同任务的上下文感知图像编辑和生成更加灵活高效。ILLUME+（3B）在多模态理解、生成和编辑基准测试中表现出与现有统一MLLM和专用模型相当的性能。凭借其强大的性能，ILLUME+为未来多模态应用提供了可扩展和通用的基础。项目页面：<a target="_blank" rel="noopener" href="https://illume-unified-mllm.github.io/%E3%80%82">https://illume-unified-mllm.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01934v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了ILLUME+模型，该模型利用双重视觉标记和扩散解码器，改进了深度语义理解和高保真图像生成。现有统一模型在三项基本能力上难以兼顾：理解、生成和编辑。而ILLUME+通过引入统一双重视觉标记器DualViTok和扩散模型，实现了精细纹理和文本对齐语义的保留，并采用了从粗到细的多模态理解和生成策略。此外，其连续输入、离散输出的方案以及渐进式训练程序，使模型在灵活高效的上下文感知图像编辑和生成方面表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ILLUME+模型结合了双重视觉标记和扩散解码器，提升了语义理解和图像生成能力。</li>
<li>现有统一模型在理解、生成和编辑方面存在困难，而ILLUME+通过引入新技术弥补了这些不足。</li>
<li>ILLUME+采用统一双重视觉标记器DualViTok，能同时保留精细纹理和文本对齐语义。</li>
<li>扩散模型的引入增强了图像生成的品质和超分辨率效率。</li>
<li>ILLUME+采用连续输入、离散输出的方案，实现了灵活且高效的上下文感知图像编辑和生成。</li>
<li>该模型的渐进式训练程序支持动态分辨率调整，适应不同的视觉标记器、MLLM和扩散解码器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f52434e3dacf1616e9d13b50bd68274d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc92edbce4f88d4f781481b545d1475d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6b0e79eb6b7dd27324f19e04b9430c4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Implicit-Bias-Injection-Attacks-against-Text-to-Image-Diffusion-Models"><a href="#Implicit-Bias-Injection-Attacks-against-Text-to-Image-Diffusion-Models" class="headerlink" title="Implicit Bias Injection Attacks against Text-to-Image Diffusion Models"></a>Implicit Bias Injection Attacks against Text-to-Image Diffusion Models</h2><p><strong>Authors:Huayang Huang, Xiangye Jin, Jiaxu Miao, Yu Wu</strong></p>
<p>The proliferation of text-to-image diffusion models (T2I DMs) has led to an increased presence of AI-generated images in daily life. However, biased T2I models can generate content with specific tendencies, potentially influencing people’s perceptions. Intentional exploitation of these biases risks conveying misleading information to the public. Current research on bias primarily addresses explicit biases with recognizable visual patterns, such as skin color and gender. This paper introduces a novel form of implicit bias that lacks explicit visual features but can manifest in diverse ways across various semantic contexts. This subtle and versatile nature makes this bias challenging to detect, easy to propagate, and adaptable to a wide range of scenarios. We further propose an implicit bias injection attack framework (IBI-Attacks) against T2I diffusion models by precomputing a general bias direction in the prompt embedding space and adaptively adjusting it based on different inputs. Our attack module can be seamlessly integrated into pre-trained diffusion models in a plug-and-play manner without direct manipulation of user input or model retraining. Extensive experiments validate the effectiveness of our scheme in introducing bias through subtle and diverse modifications while preserving the original semantics. The strong concealment and transferability of our attack across various scenarios further underscore the significance of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Hannah1102/IBI-attacks">https://github.com/Hannah1102/IBI-attacks</a>. </p>
<blockquote>
<p>文本到图像的扩散模型（T2I DM）的激增导致了日常生活中AI生成的图像出现频率增加。然而，存在偏向的T2I模型可能会生成具有特定倾向的内容，从而可能影响人们的感知。故意利用这些偏见有可能向公众传递误导信息。目前关于偏见的研究主要集中在具有可识别视觉模式的明显偏见上，例如肤色和性别。本文介绍了一种新型隐式偏见，这种偏见没有明确的视觉特征，但可以在各种语义上下文中以多种方式表现。这种细微且多功能的特性使这种偏见难以检测、易于传播并且适应多种场景。我们进一步提出了针对文本到图像扩散模型的隐式偏见注入攻击框架（IBI-Attacks），通过预先计算提示嵌入空间中的一般偏见方向，并根据不同的输入进行自适应调整。我们的攻击模块可以无缝集成到预训练的扩散模型中，以即插即用方式运行，无需直接操作用户输入或重新训练模型。大量实验验证了我们的方案通过细微和多样的修改引入偏见的有效性，同时保持了原始语义。我们攻击的强隐蔽性和跨场景的转移性进一步突出了我们的方法的重要性。相关代码可访问<a target="_blank" rel="noopener" href="https://github.com/Hannah1102/IBI-attacks%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hannah1102/IBI-attacks获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01819v1">PDF</a> Accept to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>文本到图像扩散模型（T2I DMs）的普及使得AI生成的图像在日常生活中出现频率增加。然而，存在偏见的T2I模型可能生成具有特定倾向的内容，影响人们的感知。故意利用这些偏见可能向公众传递误导信息。当前关于偏见的研究主要关注具有明显视觉模式，如肤色和性别的显式偏见。本文介绍了一种新型的隐式偏见，它没有明显的视觉特征，但可以在各种语义上下文中以多种方式表现。这种隐式偏见的微妙性和通用性使其难以检测、易于传播并且适应性强。我们进一步提出了针对T2I扩散模型的隐式偏见注入攻击框架（IBI-Attacks），通过在提示嵌入空间中预先计算一般的偏见方向并基于不同的输入进行适应性调整。我们的攻击模块可以无缝集成到预训练的扩散模型中，以即插即用方式工作，无需直接操作用户输入或重新训练模型。大量实验证明了我们方案的有效性，通过细微和多样的修改引入偏见，同时保留原始语义。我们的攻击的强隐蔽性和跨场景的传染性进一步强调了我们的方法的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI生成的图像在日常生活中越来越普遍，但存在偏见的T2I模型可能生成具有特定倾向的内容，影响人们的感知。</li>
<li>当前研究主要关注具有明显视觉特征的显式偏见，但存在新型隐式偏见缺乏直接的可识别视觉特征。</li>
<li>隐式偏见可以灵活地在各种语义上下文中表现，使其难以检测且易于传播。</li>
<li>提出了一种新的攻击框架IBI-Attacks，可以通过微调提示嵌入空间中的偏见方向来影响T2I扩散模型的输出。</li>
<li>IBI-Attacks可以无缝集成到预训练的扩散模型中，无需直接操作用户输入或重新训练模型。</li>
<li>实验证明IBI-Attacks通过细微和多样的修改有效引入偏见，同时保留原始语义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ee07af5670699b2de3e093ac19e9fec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56e8eae2229d4c3dd8044ddd7d86008c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68726db3cd26d8a3d557e4b7b529fa26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b91862d6fcbe708c3d40e9b0c526f699.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Diffusion-Recommender-Model"><a href="#Hyperbolic-Diffusion-Recommender-Model" class="headerlink" title="Hyperbolic Diffusion Recommender Model"></a>Hyperbolic Diffusion Recommender Model</h2><p><strong>Authors:Meng Yuan, Yutian Xiao, Wei Chen, Chu Zhao, Deqing Wang, Fuzhen Zhuang</strong></p>
<p>Diffusion models (DMs) have emerged as the new state-of-the-art family of deep generative models. To gain deeper insights into the limitations of diffusion models in recommender systems, we investigate the fundamental structural disparities between images and items. Consequently, items often exhibit distinct anisotropic and directional structures that are less prevalent in images. However, the traditional forward diffusion process continuously adds isotropic Gaussian noise, causing anisotropic signals to degrade into noise, which impairs the semantically meaningful representations in recommender systems.   Inspired by the advancements in hyperbolic spaces, we propose a novel \textit{\textbf{H}yperbolic} \textit{\textbf{D}iffusion} \textit{\textbf{R}ecommender} \textit{\textbf{M}odel} (named HDRM). Unlike existing directional diffusion methods based on Euclidean space, the intrinsic non-Euclidean structure of hyperbolic space makes it particularly well-adapted for handling anisotropic diffusion processes. In particular, we begin by formulating concepts to characterize latent directed diffusion processes within a geometrically grounded hyperbolic space. Subsequently, we propose a novel hyperbolic latent diffusion process specifically tailored for users and items. Drawing upon the natural geometric attributes of hyperbolic spaces, we impose structural restrictions on the space to enhance hyperbolic diffusion propagation, thereby ensuring the preservation of the intrinsic topology of user-item graphs. Extensive experiments on three benchmark datasets demonstrate the effectiveness of HDRM. </p>
<blockquote>
<p>扩散模型（DMs）已经作为先进的深度生成模型家族崭露头角。为了深入了解扩散模型在推荐系统中的局限性，我们研究了图像和项目之间基本结构上的差异。因此，项目通常表现出独特的各向异性和方向性结构，这在图像中并不常见。然而，传统的正向扩散过程不断添加各向同性高斯噪声，导致各向异性信号降为噪声，从而损害了推荐系统中的语义表示。受到双曲空间发展的启发，我们提出了一种新型的<strong>H</strong>yperbolic <strong>D</strong>iffusion <strong>R</strong>ecommender <strong>M</strong>odel（简称HDRM）。与基于欧几里得空间的现有方向性扩散方法不同，双曲空间的内在非欧几里得结构特别适合于处理各向异性扩散过程。具体来说，我们首先制定概念，以在几何基础上的双曲空间中刻画潜在的定向扩散过程。随后，我们针对用户和项目提出了一种新型的双曲潜在扩散过程。利用双曲空间的自然几何属性，我们对空间施加结构约束，以增强双曲扩散传播，从而确保用户-项目图的内蕴拓扑得到保留。在三个基准数据集上的大量实验证明了HDRM的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01541v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型（DMs）作为最新的深度生成模型主流技术，在推荐系统中存在局限性。本文探究了图像和物品之间基本结构差异导致的挑战。物品通常具有独特的定向结构，而传统前向扩散过程添加的是同向高斯噪声，这会导致定向信号降为噪声，从而影响推荐系统的语义表示。受双曲空间发展的启发，我们提出了新型双曲扩散推荐模型（HDRM）。与基于欧几里得空间的定向扩散方法不同，双曲空间的非欧几里得结构特别适合处理定向扩散过程。我们在双曲空间中构建了描述潜在定向扩散过程的概念框架，并针对用户和物品提出了新型双曲潜在扩散过程。借助双曲空间的自然几何属性，我们对空间施加结构约束，增强双曲扩散传播，确保用户-物品图内在拓扑的保留。在三个基准数据集上的广泛实验证明了HDRM的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在推荐系统中存在局限性，需要更深入的研究图像和物品之间的结构差异。</li>
<li>物品通常具有独特的定向结构，而传统扩散过程添加的同向噪声不利于语义表示。</li>
<li>提出新型双曲扩散推荐模型（HDRM），适应非欧几里得结构处理定向扩散过程。</li>
<li>在双曲空间中构建了描述潜在定向扩散过程的概念框架。</li>
<li>针对用户和物品提出了新型双曲潜在扩散过程，增强扩散传播并保留用户-物品图的内在拓扑。</li>
<li>通过广泛实验证明，HDRM在三个基准数据集上表现出有效性。</li>
<li>HDRM模型能够为解决扩散模型在推荐系统中的局限性提供新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c71f33ecd3ab19d66d21464bcd7a427.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00fad0193c1bfe8f80e6c54132fb36c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94a3bc9dff25e7af83478322492d6318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03dd31fcf13d799605c183f2c939250e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f8d4b96b7b3ba34f5a427df451deebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb5b6507ee46ecb428a45582d9a6bb9c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="From-Easy-to-Hard-Building-a-Shortcut-for-Differentially-Private-Image-Synthesis"><a href="#From-Easy-to-Hard-Building-a-Shortcut-for-Differentially-Private-Image-Synthesis" class="headerlink" title="From Easy to Hard: Building a Shortcut for Differentially Private Image   Synthesis"></a>From Easy to Hard: Building a Shortcut for Differentially Private Image   Synthesis</h2><p><strong>Authors:Kecen Li, Chen Gong, Xiaochen Li, Yuzhong Zhao, Xinwen Hou, Tianhao Wang</strong></p>
<p>Differentially private (DP) image synthesis aims to generate synthetic images from a sensitive dataset, alleviating the privacy leakage concerns of organizations sharing and utilizing synthetic images. Although previous methods have significantly progressed, especially in training diffusion models on sensitive images with DP Stochastic Gradient Descent (DP-SGD), they still suffer from unsatisfactory performance. In this work, inspired by curriculum learning, we propose a two-stage DP image synthesis framework, where diffusion models learn to generate DP synthetic images from easy to hard. Unlike existing methods that directly use DP-SGD to train diffusion models, we propose an easy stage in the beginning, where diffusion models learn simple features of the sensitive images. To facilitate this easy stage, we propose to use &#96;central images’, simply aggregations of random samples of the sensitive dataset. Intuitively, although those central images do not show details, they demonstrate useful characteristics of all images and only incur minimal privacy costs, thus helping early-phase model training. We conduct experiments to present that on the average of four investigated image datasets, the fidelity and utility metrics of our synthetic images are 33.1% and 2.1% better than the state-of-the-art method. </p>
<blockquote>
<p>差分隐私（DP）图像合成旨在从敏感数据集中生成合成图像，从而减轻组织和机构共享和利用合成图像时的隐私泄露担忧。尽管之前的方法已经有了显著进展，特别是在使用差分隐私随机梯度下降法（DP-SGD）对敏感图像进行扩散模型训练方面，但它们仍然存在着性能不佳的问题。在这项工作中，我们受到课程学习的启发，提出了一个两阶段的DP图像合成框架，其中扩散模型从易到难学习生成DP合成图像。与现有方法直接使用DP-SGD训练扩散模型不同，我们在开始时提出了一个简单阶段，让扩散模型学习敏感图像的简单特征。为了促进这一简单阶段，我们建议使用“中心图像”，即敏感数据集随机样本的简单集合。直观地看，虽然这些中心图像没有显示细节，但它们展示了所有图像的有用特征，并且只产生微小的隐私成本，从而有助于早期阶段的模型训练。我们进行了实验，结果显示在所调查的四个图像数据集上，我们的合成图像的保真度和效用指标平均比现有最佳方法高出33.1%和2.1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01395v1">PDF</a> Accepted at IEEE S&amp;P (Oakland) 2025; code available at   <a target="_blank" rel="noopener" href="https://github.com/SunnierLee/DP-FETA">https://github.com/SunnierLee/DP-FETA</a></p>
<p><strong>Summary</strong><br>差分隐私（DP）图像合成旨在从敏感数据集中生成合成图像，以缓解组织和公众对共享和利用合成图像时的隐私泄露担忧。尽管之前的方法，特别是在使用DP随机梯度下降（DP-SGD）训练扩散模型方面取得了显著进展，但它们仍表现出不满意的性能。在这项工作中，我们受到课程学习的启发，提出了一个两阶段的DP图像合成框架，其中扩散模型从易到难学习生成DP合成图像。与直接使用DP-SGD训练扩散模型的方法不同，我们在开始时提出了一个简单阶段，其中扩散模型学习敏感图像的基本特征。为了促进这一简单阶段，我们建议使用“中心图像”，即敏感数据集随机样本的简单聚合。虽然这些中心图像不显示细节，但它们展示了所有图像的有用特征并且只产生了最小的隐私成本，从而有助于早期阶段的模型训练。实验表明，在调查的四个图像数据集上，我们生成的合成图像的保真度和效用指标比现有最佳方法平均提高了33.1%和2.1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>差分隐私（DP）图像合成旨在解决共享和利用合成图像时的隐私泄露问题。</li>
<li>现有方法虽然使用DP-SGD训练扩散模型有所进展，但性能仍不理想。</li>
<li>提出的两阶段DP图像合成框架允许扩散模型从简单到复杂地学习生成DP合成图像。</li>
<li>在模型训练的初始阶段，使用“中心图像”促进简单阶段的学习。</li>
<li>中心图像是敏感数据集随机样本的简单聚合，能够提供图像的有用特征且隐私成本较低。</li>
<li>实验结果表明，与现有最佳方法相比，所提出的方法在合成图像的保真度和效用指标上有所改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01395">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-58b9f71beef6f32493d3cc50f11c9131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3a59a108dd9892170ffcc61518b1904.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5950abc0d33401bbdd76dfc65d2f93a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78e83401a11da22e25f7853e9ad2999f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8128766f7acb6dd9a39a37893db4562.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7218dc417e1af5c4e584533c7c216eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Safeguarding-Vision-Language-Models-Mitigating-Vulnerabilities-to-Gaussian-Noise-in-Perturbation-based-Attacks"><a href="#Safeguarding-Vision-Language-Models-Mitigating-Vulnerabilities-to-Gaussian-Noise-in-Perturbation-based-Attacks" class="headerlink" title="Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks"></a>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks</h2><p><strong>Authors:Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yichen Fu, Yichun Feng, Kin-man Lam</strong></p>
<p>Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned &#x2F; misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM">https://github.com/JarvisUSTC/DiffPure-RobustVLM</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）通过融入视觉信息扩展了大语言模型（LLMs）的功能，但它们仍然容易受到越狱攻击的影响，特别是在处理嘈杂或损坏的图像时。尽管现有的VLMs在训练过程中采取了安全措施来缓解这种攻击，但与噪声增强视觉输入相关的漏洞却被忽视了。在这项工作中，我们发现缺少噪声增强训练会导致关键的安全漏洞：许多VLMs容易受到甚至简单的扰动，如高斯噪声的影响。为了应对这一挑战，我们提出了Robust-VLGuard，这是一个多模式安全数据集，包含对齐的&#x2F;未对齐的图像文本对，结合噪声增强微调，在降低攻击成功率的同时保持VLM的功能性。针对基于更强优化的视觉扰动攻击，我们提出了DiffPure-VLM，利用扩散模型将对抗性扰动转化为高斯噪声，通过具有噪声增强安全微调的VLM进行防御。实验结果表明，扩散模型的分布转移属性与我们的微调VLMs非常吻合，显著减轻了不同强度下的对抗性扰动。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JarvisUSTC/DiffPure-RobustVLM找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01308v1">PDF</a> </p>
<p><strong>Summary</strong><br>     视觉语言模型（VLM）通过融入视觉信息扩展了大语言模型（LLM）的功能，但它们仍易受攻击，特别是在处理噪声或损坏的图像时。尽管现有VLM在训练期间采取安全措施来减轻这些攻击，但噪声增强视觉输入相关的漏洞却被忽视。本研究发现缺失噪声增强训练会造成关键的安全漏洞：许多VLM容易受到如高斯噪声等简单干扰的影响。为解决此问题，我们提出Robust-VLGuard，这是一种多模式安全数据集，包含对齐&#x2F;未对齐的图像文本对，结合噪声增强微调，以降低攻击成功率同时保持VLM的功能性。针对更优化的视觉扰动攻击，我们提出DiffPure-VLM，利用扩散模型将对抗性扰动转化为高斯类噪声，可通过具有噪声增强安全微调的VLM进行防御。实验结果表明，扩散模型的分布转移属性与我们的微调VLMs非常契合，显著减轻了不同强度的对抗性扰动。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DiffPure-RobustVLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/JarvisUSTC/DiffPure-RobustVLM获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs虽然功能强大，但在处理噪声或损坏图像时易受攻击。</li>
<li>现有VLM训练中的安全措施忽视了噪声增强视觉输入相关的漏洞。</li>
<li>缺乏噪声增强训练会导致VLMs易受简单扰动的影响。</li>
<li>提出Robust-VLGuard数据集，包含图像文本对，旨在增强VLM对噪声的鲁棒性。</li>
<li>针对优化视觉扰动攻击，利用扩散模型提出DiffPure-VLM方法。</li>
<li>DiffPure-VLM能将对抗性扰动转化为高斯类噪声，通过噪声增强安全微调的VLM进行防御。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01308">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ce86214a4d6408b2e819cb382fe3b769.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a559cc1bf6190fd25d58077a4bb91799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9417fd9d3eff057332ab3a5c50f7ee49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3078b631421816dfb957834f0811276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49923dfda67b907568219914f9e733cb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance"><a href="#Prompting-Forgetting-Unlearning-in-GANs-via-Textual-Guidance" class="headerlink" title="Prompting Forgetting: Unlearning in GANs via Textual Guidance"></a>Prompting Forgetting: Unlearning in GANs via Textual Guidance</h2><p><strong>Authors:Piyush Nagasubramaniam, Neeraj Karamchandani, Chen Wu, Sencun Zhu</strong></p>
<p>State-of-the-art generative models exhibit powerful image-generation capabilities, introducing various ethical and legal challenges to service providers hosting these models. Consequently, Content Removal Techniques (CRTs) have emerged as a growing area of research to control outputs without full-scale retraining. Recent work has explored the use of Machine Unlearning in generative models to address content removal. However, the focus of such research has been on diffusion models, and unlearning in Generative Adversarial Networks (GANs) has remained largely unexplored. We address this gap by proposing Text-to-Unlearn, a novel framework that selectively unlearns concepts from pre-trained GANs using only text prompts, enabling feature unlearning, identity unlearning, and fine-grained tasks like expression and multi-attribute removal in models trained on human faces. Leveraging natural language descriptions, our approach guides the unlearning process without requiring additional datasets or supervised fine-tuning, offering a scalable and efficient solution. To evaluate its effectiveness, we introduce an automatic unlearning assessment method adapted from state-of-the-art image-text alignment metrics, providing a comprehensive analysis of the unlearning methodology. To our knowledge, Text-to-Unlearn is the first cross-modal unlearning framework for GANs, representing a flexible and efficient advancement in managing generative model behavior. </p>
<blockquote>
<p>当前先进的生成模型展现出强大的图像生成能力，给托管这些模型的服务提供商带来了各种伦理和法律挑战。因此，内容删除技术（CRTs）作为一个研究领域日益凸显，旨在控制输出而无需进行全面再训练。近期的研究探索了在生成模型中使用机器遗忘（Machine Unlearning）来解决内容删除问题。然而，这类研究的重点主要放在扩散模型上，生成对抗网络（GANs）中的遗忘却一直被忽视。我们通过提出Text-to-Unlearn这一新型框架来解决这一问题，该框架能够仅通过文本提示从预训练的GANs中选择性遗忘概念，从而实现特征遗忘、身份遗忘以及在人脸训练模型中的表情和多属性移除等精细任务。我们的方法利用自然语言描述来引导无学习进程，无需额外的数据集或监督微调，提供了一种可扩展且高效的解决方案。为了评估其有效性，我们引入了一种自动遗忘评估方法，该方法改编自当前先进的图像文本对齐指标，对遗忘方法进行了全面分析。据我们所知，Text-to-Unlearn是首个用于GANs的跨模态遗忘框架，代表着在管理生成模型行为方面的灵活和高效进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01218v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Text-to-Unlearn的新框架，用于选择性遗忘预训练GANs中的概念。该框架仅使用文本提示进行特征遗忘、身份遗忘以及精细任务，如面部表情和多属性移除。它利用自然语言描述引导遗忘过程，无需额外的数据集或监督微调，提供了一种可扩展和高效的解决方案。该框架还是首个针对GANs的跨模态遗忘框架，灵活高效地管理生成模型的行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-Unlearn框架可以针对预训练的GANs进行选择性概念遗忘。</li>
<li>该框架通过文本提示实现特征遗忘、身份遗忘以及面部表情和多属性移除等精细任务。</li>
<li>Text-to-Unlearn利用自然语言描述来引导遗忘过程，无需额外的数据集或监督微调。</li>
<li>该框架采用自动遗忘评估方法进行效果评估。</li>
<li>Text-to-Unlearn是首个针对GANs的跨模态遗忘框架。</li>
<li>该框架有助于管理生成模型的行为，提高模型的灵活性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ce7efed644f29823794ac569b2343bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314f1d89fb12216a96e360c7bfb5fa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe9c1683527b12a7c4c12aeb2691c2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a3ef7b00893ade8c040e80318e82c73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9292e701a6915ddc2aca931bd67329a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-422d801be0aac01f8bb39eb64f2e6abf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28943ea8a45a22e30fa7a6639aa783f0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction"><a href="#AnimeGamer-Infinite-Anime-Life-Simulation-with-Next-Game-State-Prediction" class="headerlink" title="AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction"></a>AnimeGamer: Infinite Anime Life Simulation with Next Game State   Prediction</h2><p><strong>Authors:Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan</strong></p>
<p>Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer">https://github.com/TencentARC/AnimeGamer</a>. </p>
<blockquote>
<p>图像和视频合成的最新进展为生成游戏领域带来了新的前景。一个特别吸引人的应用是将动漫电影中的角色转变为可互动的游戏实体。这允许玩家通过语言指令沉浸在他们最喜欢的角色的动态动漫世界中，进行生活模拟游戏。这类游戏被定义为无限游戏，因为它们消除了预先设定的边界和固定的游戏规则，玩家可以通过开放式的语言与游戏世界互动，体验不断发展和变化的故事情节和环境。最近，一种开创性的无限动漫生活模拟方法采用大型语言模型（LLM）将多轮文本对话翻译成图像生成的指令。然而，它忽略了历史视觉上下文，导致游戏过程不一致。此外，它只能生成静态图像，无法融入动态元素，从而无法提供引人入胜的游戏体验。在这项工作中，我们提出了AnimeGamer，它建立在多模态大型语言模型（MLLM）之上，用于生成每个游戏状态，包括描述角色动作和状态更新的动态动画镜头，如图1所示。我们引入了新型的动作感知多模态表示来代表动画镜头，可以使用视频扩散模型将其解码成高质量的视频片段。通过获取历史动画镜头表示作为上下文并预测后续表示，AnimeGamer可以生成具有上下文一致性和令人满意的动力学的游戏。通过自动化指标和人为评估的广泛评估表明，AnimeGamer在游戏体验的各个方面都优于现有方法。相关代码和检查点可在<a target="_blank" rel="noopener" href="https://github.com/TencentARC/AnimeGamer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TencentARC/AnimeGamer上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.01014v1">PDF</a> Project released at: <a target="_blank" rel="noopener" href="https://howe125.github.io/AnimeGamer.github.io/">https://howe125.github.io/AnimeGamer.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了近期图像和视频合成技术的进展在生成游戏领域的新应用，特别是在将动漫电影中的角色转化为可互动实体方面。玩家可以通过语言指令沉浸于动态的动漫世界，并体验无限的游戏情节和环境。然而，现有方法忽视了历史视觉上下文，导致游戏体验的不一致性，并且只能生成静态图像。为此，本文提出了基于多模态大型语言模型（MLLMs）的AnimeGamer，能够生成包括角色动作和状态更新的游戏状态。通过引入新型的动作感知多模态表示，AnimeGamer能够解码成高质量的视频片段，并在考虑历史动画镜头表示的基础上预测后续表示，从而实现具有上下文一致性和满意动态效果的游戏生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动漫角色在生成游戏中成为可互动实体，通过语言指令实现动态动漫世界的沉浸体验。</li>
<li>现有方法忽视历史视觉上下文，导致游戏体验的不一致性。</li>
<li>现有方法只能生成静态图像，缺乏动态效果，影响游戏互动性。</li>
<li>AnimeGamer基于多模态大型语言模型（MLLMs），能够生成包括角色动作和状态更新的游戏状态。</li>
<li>AnimeGamer引入动作感知多模态表示，可解码为高质量视频片段。</li>
<li>AnimeGamer考虑历史动画镜头表示，实现游戏生成的上下文一致性和满意动态效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.01014">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd0ebf6fe3158be837dc673128def6b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd29e104e47e0ff54d6807a15db67d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-696e28d03c6550c4c5c82861b2ad2d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27966b67be617eeb22bb1d6a7c5065e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f3546a6de895403e043b8e44e47447f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cef586b07bbc87c8a676a1da28206c4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TurboFill-Adapting-Few-step-Text-to-image-Model-for-Fast-Image-Inpainting"><a href="#TurboFill-Adapting-Few-step-Text-to-image-Model-for-Fast-Image-Inpainting" class="headerlink" title="TurboFill: Adapting Few-step Text-to-image Model for Fast Image   Inpainting"></a>TurboFill: Adapting Few-step Text-to-image Model for Fast Image   Inpainting</h2><p><strong>Authors:Liangbin Xie, Daniil Pakhomov, Zhonghao Wang, Zongze Wu, Ziyan Chen, Yuqian Zhou, Haitian Zheng, Zhifei Zhang, Zhe Lin, Jiantao Zhou, Chao Dong</strong></p>
<p>This paper introduces TurboFill, a fast image inpainting model that enhances a few-step text-to-image diffusion model with an inpainting adapter for high-quality and efficient inpainting. While standard diffusion models generate high-quality results, they incur high computational costs. We overcome this by training an inpainting adapter on a few-step distilled text-to-image model, DMD2, using a novel 3-step adversarial training scheme to ensure realistic, structurally consistent, and visually harmonious inpainted regions. To evaluate TurboFill, we propose two benchmarks: DilationBench, which tests performance across mask sizes, and HumanBench, based on human feedback for complex prompts. Experiments show that TurboFill outperforms both multi-step BrushNet and few-step inpainting methods, setting a new benchmark for high-performance inpainting tasks. Our project page: <a target="_blank" rel="noopener" href="https://liangbinxie.github.io/projects/TurboFill/">https://liangbinxie.github.io/projects/TurboFill/</a> </p>
<blockquote>
<p>本文介绍了TurboFill，这是一种快速图像修复模型，它通过修复适配器提高了文本到图像扩散模型的效率，以实现高质量和高效的图像修复。虽然标准扩散模型可以生成高质量的结果，但它们计算成本较高。我们通过训练一个修复适配器来解决这个问题，该适配器在蒸馏的文本到图像模型DMD2上进行了几步训练，并使用了一种新颖的3步对抗性训练方案，以确保修复的区域的逼真性、结构一致性和视觉和谐性。为了评估TurboFill的性能，我们提出了两个基准测试：DilationBench，用于测试不同掩膜大小的性能；HumanBench，基于人类反馈对复杂提示进行评估。实验表明，TurboFill在高性能修复任务方面超越了多步BrushNet和少数步骤修复方法，并设定了新的基准。我们的项目页面：<a target="_blank" rel="noopener" href="https://liangbinxie.github.io/projects/TurboFill/%EF%BC%88%E6%B3%A8%EF%BC%9A%E7%94%B1%E4%BA%8E%E6%A0%BC%E5%BC%8F%E9%99%90%E5%88%B6%E6%97%A0%E6%B3%95%E5%B1%95%E7%A4%BA%E9%93%BE%E6%8E%A5%EF%BC%89%E3%80%82">https://liangbinxie.github.io/projects/TurboFill/（注：由于格式限制无法展示链接）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00996v1">PDF</a> Project webpage available at   <a target="_blank" rel="noopener" href="https://liangbinxie.github.io/projects/TurboFill/">https://liangbinxie.github.io/projects/TurboFill/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了TurboFill，这是一种快速图像修复模型。它改进了文本到图像的少量步骤扩散模型，通过使用修复适配器来提高修复质量和效率。尽管标准扩散模型生成的结果质量很高，但它们计算成本高昂。本研究通过在一个蒸馏的文本到图像模型DMD2上训练修复适配器来克服这一问题，并采用一种新颖的3步对抗训练方案来确保修复区域真实、结构一致和视觉和谐。为了评估TurboFill的性能，我们提出了两个基准测试：DilationBench（测试不同掩膜大小的性能）和HumanBench（基于人类反馈对复杂提示进行评估）。实验表明，TurboFill在高性能修复任务上优于多步骤的BrushNet和少量步骤的修复方法，为图像修复任务设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TurboFill是一个快速图像修复模型，基于文本到图像的少量步骤扩散模型进行改进。</li>
<li>通过使用修复适配器，TurboFill提高了修复的质量和效率。</li>
<li>标准扩散模型虽然生成高质量结果，但计算成本高昂。</li>
<li>TurboFill通过在一个蒸馏的文本到图像模型DMD2上训练修复适配器来降低计算成本。</li>
<li>采用新颖的3步对抗训练方案确保修复区域真实、结构一致和视觉和谐。</li>
<li>TurboFill在高性能修复任务上表现出优异性能，优于多步骤和少量步骤的修复方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00996">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce0cdd27817814cb3f016ebc1cb6387b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-669db6627a7a3bb90a76ffffa8365123.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce755e2f32dbb9e8c127c5f621014800.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff7c45a61830a4ed0e684da442590787.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Data-free-Knowledge-Distillation-with-Diffusion-Models"><a href="#Data-free-Knowledge-Distillation-with-Diffusion-Models" class="headerlink" title="Data-free Knowledge Distillation with Diffusion Models"></a>Data-free Knowledge Distillation with Diffusion Models</h2><p><strong>Authors:Xiaohua Qi, Renda Li, Long Peng, Qiang Ling, Jun Yu, Ziyi Chen, Peng Chang, Mei Han, Jing Xiao</strong></p>
<p>Recently Data-Free Knowledge Distillation (DFKD) has garnered attention and can transfer knowledge from a teacher neural network to a student neural network without requiring any access to training data. Although diffusion models are adept at synthesizing high-fidelity photorealistic images across various domains, existing methods cannot be easiliy implemented to DFKD. To bridge that gap, this paper proposes a novel approach based on diffusion models, DiffDFKD. Specifically, DiffDFKD involves targeted optimizations in two key areas. Firstly, DiffDFKD utilizes valuable information from teacher models to guide the pre-trained diffusion models’ data synthesis, generating datasets that mirror the training data distribution and effectively bridge domain gaps. Secondly, to reduce computational burdens, DiffDFKD introduces Latent CutMix Augmentation, an efficient technique, to enhance the diversity of diffusion model-generated images for DFKD while preserving key attributes for effective knowledge transfer. Extensive experiments validate the efficacy of DiffDFKD, yielding state-of-the-art results exceeding existing DFKD approaches. We release our code at <a target="_blank" rel="noopener" href="https://github.com/xhqi0109/DiffDFKD">https://github.com/xhqi0109/DiffDFKD</a>. </p>
<blockquote>
<p>最近，无数据知识蒸馏（DFKD）引起了人们的关注，它可以将知识从一个教师神经网络转移到一个学生神经网络，而无需访问任何训练数据。尽管扩散模型擅长在各种领域合成高保真照片级图像，但现有方法无法轻易应用于DFKD。为了填补这一空白，本文提出了一种基于扩散模型的新方法，名为DiffDFKD。具体来说，DiffDFKD涉及两个关键领域的目标优化。首先，DiffDFKD利用教师模型中的有价值信息来指导预训练扩散模型的数据合成，生成反映训练数据分布的数据集，有效地弥合领域差距。其次，为了减少计算负担，DiffDFKD引入了一种高效的Latent CutMix Augmentation技术，以增强扩散模型生成图像的多样性，同时保留关键属性以实现有效的知识转移。大量实验验证了DiffDFKD的有效性，其性能优于现有DFKD方法。我们在<a target="_blank" rel="noopener" href="https://github.com/xhqi0109/DiffDFKD">https://github.com/xhqi0109/DiffDFKD</a>发布我们的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00870v1">PDF</a> Accepted by ICME2025</p>
<p><strong>Summary</strong><br>数据无关的知识的蒸馏方法近来受到关注，可将知识从教师神经网络转移到学生神经网络，无需访问训练数据。尽管扩散模型擅长合成跨域的高保真度照片级图像，但现有方法难以应用于数据无关的知识的蒸馏。为了填补这一空白，本文提出了一种基于扩散模型的全新方法，名为DiffDFKD。DiffDFKD在两个方面进行了针对性优化。首先，它利用教师模型的有价值信息来指导预训练的扩散模型的数据合成，生成反映训练数据分布的数据集，有效缩短差距。其次，为了减少计算负担，DiffDFKD引入了潜空间混合增强技术（Latent CutMix Augmentation），在保持关键属性的同时增强扩散模型生成图像的多样性，实现有效的知识转移。实验验证DiffDFKD的有效性，其性能优于现有数据无关的知识的蒸馏方法。我们的代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/xhqi0109/DiffDFKD%E3%80%82">https://github.com/xhqi0109/DiffDFKD。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据无关的知识的蒸馏（DFKD）能够在不需要访问训练数据的情况下，从教师神经网络转移知识到学生神经网络。</li>
<li>扩散模型在合成跨域的高保真度图像方面具有优势，但现有方法难以应用于DFKD。</li>
<li>DiffDFKD方法利用教师模型的信息来指导扩散模型的数据合成，生成反映训练数据分布的数据集。</li>
<li>DiffDFKD引入Latent CutMix Augmentation技术，增强扩散模型生成图像的多样性并减少计算负担。</li>
<li>实验结果显示DiffDFKD的有效性，其性能超越现有DFKD方法。</li>
<li>该研究发布的代码位于<a target="_blank" rel="noopener" href="https://github.com/xhqi0109/DiffDFKD%EF%BC%8C%E4%BE%9B%E5%85%AC%E4%BC%97%E8%AE%BF%E9%97%AE%E5%92%8C%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/xhqi0109/DiffDFKD，供公众访问和使用。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00870">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9274a7091b7e8d49a5827ae6855f3a71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-144391497f54f507fc4384ee7d8f3115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-116e9cfcc0ed080256dba2e47b8c39c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9363f4de8a069f97fc3c363681d513b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bee94dc557e6a5547b53523e7ecf8f1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-Based-Size-Variable-Virtual-Try-On-Technology-and-Evaluation-Method"><a href="#Diffusion-Model-Based-Size-Variable-Virtual-Try-On-Technology-and-Evaluation-Method" class="headerlink" title="Diffusion Model-Based Size Variable Virtual Try-On Technology and   Evaluation Method"></a>Diffusion Model-Based Size Variable Virtual Try-On Technology and   Evaluation Method</h2><p><strong>Authors:Shufang Zhang, Hang Qian, Minxue Ni, Yaxuan Li, Wenxin Ding, Jun Liu</strong></p>
<p>With the rapid development of e-commerce, virtual try-on technology has become an essential tool to satisfy consumers’ personalized clothing preferences. Diffusion-based virtual try-on systems aim to naturally align garments with target individuals, generating realistic and detailed try-on images. However, existing methods overlook the importance of garment size variations in meeting personalized consumer needs. To address this, we propose a novel virtual try-on method named SV-VTON, which introduces garment sizing concepts into virtual try-on tasks. The SV-VTON method first generates refined masks for multiple garment sizes, then integrates these masks with garment images at varying proportions, enabling virtual try-on simulations across different sizes. In addition, we developed a specialized size evaluation module to quantitatively assess the accuracy of size variations. This module calculates differences between generated size increments and international sizing standards, providing objective measurements of size accuracy. To further validate SV-VTON’s generalization capability across different models, we conducted experiments on multiple SOTA Diffusion models. The results demonstrate that SV-VTON consistently achieves precise multi-size virtual try-on across various SOTA models, and validates the effectiveness and rationality of the proposed method, significantly fulfilling users’ personalized multi-size virtual try-on requirements. </p>
<blockquote>
<p>随着电子商务的快速发展，虚拟试穿技术已成为满足消费者个性化服装需求的重要工具。基于扩散的虚拟试穿系统旨在将服装自然地与目标个体对齐，生成逼真且详细的试穿图像。然而，现有方法忽视了服装尺寸变化在满足个性化消费者需求方面的重要性。为了解决这一问题，我们提出了一种新型的虚拟试穿方法，名为SV-VTON，它将服装尺寸概念引入虚拟试穿任务中。SV-VTON方法首先为多种服装尺寸生成精细的掩膜，然后将这些掩膜与不同比例的服装图像集成，从而实现不同尺寸的虚拟试穿模拟。此外，我们还开发了一个专门的尺寸评估模块，定量评估尺寸变化的准确性。该模块计算生成的尺寸增量与国际尺寸标准之间的差异，为尺寸精度提供客观测量。为了进一步验证SV-VTON在不同模型上的泛化能力，我们在多个最先进的扩散模型上进行了实验。结果表明，SV-VTON在各种最先进的模型上均实现了精确的多尺寸虚拟试穿，验证了所提方法的有效性和合理性，充分满足了用户的个性化多尺寸虚拟试穿需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00562v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的虚拟试穿技术为满足消费者的个性化服装需求提供了重要工具。现有方法忽略了服装尺寸变化的重要性，为此我们提出了一种名为SV-VTON的新型虚拟试穿方法。SV-VTON引入服装尺寸概念，为不同尺寸的服装生成精细遮罩，并与服装图像按比例集成，实现跨不同尺寸的虚拟试穿模拟。此外，我们还开发了一个专门的尺寸评估模块来定量评估尺寸变化的准确性。实验证明，SV-VTON在多个先进扩散模型上均能实现精确的多尺寸虚拟试穿，验证了方法的有效性和合理性，充分满足了用户的个性化多尺寸虚拟试穿需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>虚拟试穿技术已成为满足消费者个性化服装需求的重要工具。</li>
<li>现有虚拟试穿方法忽略了服装尺寸变化的重要性。</li>
<li>SV-VTON方法引入服装尺寸概念，生成精细遮罩并与服装图像按比例集成。</li>
<li>SV-VTON实现了跨不同尺寸的虚拟试穿模拟。</li>
<li>开发了一个专门的尺寸评估模块来定量评估尺寸变化的准确性。</li>
<li>实验证明SV-VTON在多个先进扩散模型上均能实现精确的多尺寸虚拟试穿。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7643b6c42935833d796a9e09cdb558ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb266029db41a4fc2d60f300a95d9faf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9df426ff5fefca75b735693cc4b09dc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b59e0daced40d8092599213159a7ab6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Distilling-Multi-view-Diffusion-Models-into-3D-Generators"><a href="#Distilling-Multi-view-Diffusion-Models-into-3D-Generators" class="headerlink" title="Distilling Multi-view Diffusion Models into 3D Generators"></a>Distilling Multi-view Diffusion Models into 3D Generators</h2><p><strong>Authors:Hao Qin, Luyuan Chen, Ming Kong, Mengxu Lu, Qiang Zhu</strong></p>
<p>We introduce DD3G, a formulation that Distills a multi-view Diffusion model (MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and integrates extensive visual and spatial geometric knowledge from the MV-DM by simulating its ordinary differential equation (ODE) trajectory, ensuring the distilled generator generalizes better than those trained solely on 3D data. Unlike previous amortized optimization approaches, we align the MV-DM and 3D generator representation spaces to transfer the teacher’s probabilistic flow to the student, thus avoiding inconsistencies in optimization objectives caused by probabilistic sampling. The introduction of probabilistic flow and the coupling of various attributes in 3D Gaussians introduce challenges in the generation process. To tackle this, we propose PEPD, a generator consisting of Pattern Extraction and Progressive Decoding phases, which enables efficient fusion of probabilistic flow and converts a single image into 3D Gaussians within 0.06 seconds. Furthermore, to reduce knowledge loss and overcome sparse-view supervision, we design a joint optimization objective that ensures the quality of generated samples through explicit supervision and implicit verification. Leveraging existing 2D generation models, we compile 120k high-quality RGBA images for distillation. Experiments on synthetic and public datasets demonstrate the effectiveness of our method. Our project is available at: <a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/">https://qinbaigao.github.io/DD3G_project/</a> </p>
<blockquote>
<p>我们介绍了DD3G，这是一种通过高斯拼贴技术将多视图扩散模型（MV-DM）蒸馏到3D生成器的方法。DD3G通过模拟MV-DM的普通微分方程（ODE）轨迹，压缩并集成了丰富的视觉和空间几何知识，确保蒸馏出的生成器比仅使用3D数据训练的生成器具有更好的泛化性能。与以前的一次性优化方法不同，我们对MV-DM和3D生成器的表示空间进行对齐，以将教师的概率流传输给学生，从而避免了由概率采样引起的优化目标不一致的问题。概率流的引入和3D高斯中各种属性的耦合给生成过程带来了挑战。为解决这一问题，我们提出了PEPD生成器，它由模式提取和渐进解码两个阶段组成，能够高效融合概率流，并在0.06秒内将单幅图像转换为3D高斯。此外，为了减少知识损失并克服稀疏视图监督，我们设计了一个联合优化目标，通过显式监督和隐式验证确保生成样本的质量。我们利用现有的2D生成模型，编译了12万张高质量RGBA图像进行蒸馏。在合成和公共数据集上的实验证明了我们的方法的有效性。我们的项目在：[<a target="_blank" rel="noopener" href="https://qinbaigao.github.io/DD3G_project/]">https://qinbaigao.github.io/DD3G_project/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00457v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了DD3G方法，它将多视图扩散模型（MV-DM）蒸馏到3D生成器中。通过模拟MV-DM的常微分方程（ODE）轨迹，DD3G压缩并集成了视觉和几何空间知识。与以往优化方法不同，DD3G通过对齐MV-DM和3D生成器的表示空间，将教师的概率流转移给学生，避免了由概率采样引起的目标不一致。为了应对引入的概率流和3D高斯中的属性耦合所带来的生成挑战，DD3G提出了PEPD生成器，包括模式提取和渐进解码阶段，实现了高效融合概率流并将单幅图像转换为3D高斯。此外，为了减少知识损失并克服稀疏视图监督问题，DD3G设计了一个联合优化目标，通过明确监督和隐性验证确保生成样本的质量。实验表明，该方法在合成和公开数据集上均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DD3G将多视图扩散模型（MV-DM）蒸馏到3D生成器中。</li>
<li>通过模拟MV-DM的ODE轨迹，DD3G整合视觉和几何空间知识。</li>
<li>DD3G避免概率采样引起的目标不一致，通过对齐MV-DM和3D生成器的表示空间。</li>
<li>PEPD生成器用于高效融合概率流，将单幅图像转换为3D高斯。</li>
<li>为减少知识损失和克服稀疏视图监督问题，DD3G设计联合优化目标。</li>
<li>实验证明DD3G在合成和公开数据集上的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1072ca818acce967788bf801b1c2be1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd5ced50961b209c9ff6ef2cd5c0663e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7b487ff167a29735997ac5d402d1d2d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6eb6f5bc327dbaba705f3566712c6fc2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b572b2194b81b3883cafe56abae3366.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Physically-Plausible-Video-Generation-via-VLM-Planning"><a href="#Towards-Physically-Plausible-Video-Generation-via-VLM-Planning" class="headerlink" title="Towards Physically Plausible Video Generation via VLM Planning"></a>Towards Physically Plausible Video Generation via VLM Planning</h2><p><strong>Authors:Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia</strong></p>
<p>Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories&#x2F;changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories&#x2F;changes to guide the video generation of a VDM. As the predicted motion trajectories&#x2F;changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: <a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">https://madaoer.github.io/projects/physically_plausible_video_generation</a>. </p>
<blockquote>
<p>视频扩散模型（VDMs）近年来取得了显著进展，能够生成高度逼真的视频，并作为世界模拟器引起了社区的广泛关注。然而，尽管VDMs具有强大的能力，但由于对物理的固有理解不足，它们往往无法生成物理上合理的视频，导致动态和事件序列不正确。为了解决这一局限性，我们提出了一种新的两阶段图像到视频生成框架，该框架显式地结合了物理学。在第一阶段，我们采用视觉语言模型（VLM）作为粗粒度运动规划器，通过思维链和物理感知推理来预测粗略的运动轨迹&#x2F;变化，这些预测的运动轨迹&#x2F;变化近似于现实世界中的物理动态，同时确保帧间一致性。在第二阶段，我们使用预测的运动轨迹&#x2F;变化来指导VDM的视频生成。由于预测的运动轨迹&#x2F;变化是粗略的，因此在推理过程中加入了噪声，为VDM生成具有更多细节的运动提供了自由。大量的实验结果表明，我们的框架可以生成物理上合理的运动，并且与其他方法的比较评估凸显了我们方法的显著优势。更多视频结果请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://madaoer.github.io/projects/physically_plausible_video_generation">https://madaoer.github.io/projects/physically_plausible_video_generation</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23368v2">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新颖的两阶段图像到视频生成框架，旨在解决视频扩散模型（VDMs）在生成物理上合理视频方面的局限性。该框架通过引入视觉语言模型（VLM）作为粗粒度运动规划器，结合思维链和物理感知推理，预测大致的运动轨迹&#x2F;变化，以模拟现实世界中的物理动态，并确保帧间一致性。在第二阶段，利用预测的运动轨迹&#x2F;变化引导VDM的视频生成。实验结果表明，该框架能够生成物理上合理的运动，相比现有方法具有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频扩散模型（VDMs）在生成高度逼真的视频方面取得了显著进展，但作为世界模拟器，仍存在物理理解上的局限性。</li>
<li>提出的两阶段图像到视频生成框架通过引入物理显式整合来解决VDMs的这一局限性。</li>
<li>第一阶段使用视觉语言模型（VLM）作为粗粒度运动规划器，预测大致的运动轨迹&#x2F;变化，模拟现实物理动态。</li>
<li>第二阶段利用预测的运动轨迹&#x2F;变化来指导VDM的视频生成。</li>
<li>该框架通过添加噪声在推理过程中提供VDM生成具有更多细节运动的自由度。</li>
<li>实验结果表明，该框架能生成物理上合理的运动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9a706d651493dcdc95cba43025ee37fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b04ae2d0e383733eb4d82dfa222dfdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f7af36f51e83f4b35c713450e1da9c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a8879d6375211516ac02353e31b767b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Target-Aware-Video-Diffusion-Models"><a href="#Target-Aware-Video-Diffusion-Models" class="headerlink" title="Target-Aware Video Diffusion Models"></a>Target-Aware Video Diffusion Models</h2><p><strong>Authors:Taeksoo Kim, Hanbyul Joo</strong></p>
<p>We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor’s movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target’s spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis. </p>
<blockquote>
<p>我们提出了一种目标感知视频扩散模型，该模型从输入图像中生成视频，其中演员在与指定目标进行交互时执行指定的动作。目标由分割掩膜定义，所需动作则通过文本提示进行描述。与现有的可控图像到视频扩散模型不同，这些模型通常依赖于密集的结构或运动线索来引导演员的动作朝向目标，而我们的目标感知模型仅需要简单的掩膜来指示目标，利用预训练模型的泛化能力来产生合理的动作。这使得我们的方法对于人机交互（HOI）场景特别有效，在这些场景中提供精确的动作指导具有挑战性，并且进一步使视频扩散模型能够在机器人等应用中进行高级动作规划。我们通过将目标掩膜作为附加输入来扩展基线模型，构建了我们的目标感知模型。为了强制实施目标感知，我们引入了一个特殊令牌，该令牌在文本提示中编码目标的空间信息。然后，我们使用新的跨注意损失来微调我们的模型，该损失与我们的精选数据集一起使用，以对齐与此令牌相关的跨注意图与输入的目标掩膜。为了进一步提高性能，我们选择性地将此损失应用于最语义相关的transformer块和注意区域。实验结果表明，我们的目标感知模型在生成演员与指定目标准确交互的视频方面优于现有解决方案。我们进一步通过两个下游应用证明了其有效性：视频内容创建和零镜头3D人机交互运动合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18950v2">PDF</a> The project page is available at <a target="_blank" rel="noopener" href="https://taeksuu.github.io/tavid/">https://taeksuu.github.io/tavid/</a></p>
<p><strong>摘要</strong></p>
<p>本文提出了一种目标感知视频扩散模型，该模型可以从输入图像中生成视频，其中演员与指定目标进行交互，同时执行指定动作。目标通过分割掩膜定义，所需动作则通过文本提示描述。与现有的可控图像到视频扩散模型不同，这些模型通常依赖于密集的结构或运动线索来引导演员的动作朝向目标，我们的目标感知模型只需简单掩膜来指示目标，利用预训练模型的泛化能力来产生合理动作。这使得我们的方法在人机交互场景中特别有效，在那里提供精确的动作指导具有挑战性，并进一步使视频扩散模型在机器人技术的高级动作规划应用中使用成为可能。我们通过扩展基线模型来构建我们的目标感知模型，以将目标掩膜作为附加输入。为了强制实施目标感知，我们引入了一个特殊令牌，该令牌在文本提示中编码目标的空间信息。然后，我们使用定制的数据集对模型进行微调，并使用一种新型交叉注意力损失来对齐与此令牌相关的交叉注意力图与输入目标掩膜。为了进一步提高性能，我们选择性地将此损失应用于最语义相关的变压器块和注意力区域。实验结果表明，我们的目标感知模型在生成演员与指定目标准确交互的视频方面优于现有解决方案。我们还通过两个下游应用进一步证明了其有效性：视频内容创建和零射击3D人机交互运动合成。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种目标感知视频扩散模型，用于从输入图像生成视频，其中演员与指定目标交互执行动作。</li>
<li>通过简单掩膜指示目标，无需密集的结构或运动线索。</li>
<li>利用预训练模型的泛化能力产生合理动作，特别适用于人机交互场景。</li>
<li>通过扩展基线模型并引入特殊令牌和交叉注意力损失来构建目标感知模型。</li>
<li>通过对模型进行微调并使用新型交叉注意力损失来提高性能，选择性应用该损失以关注最语义相关的部分。</li>
<li>在生成演员与指定目标准确交互的视频方面，该模型优于现有解决方案。</li>
<li>模型在视频内容创建和零射击3D人机交互运动合成等下游应用中表现出有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18950">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d055e0abf2be79da543eaf2b207e9fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0689334fcc83e4a844b9dd7f2c78cad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-984c49e727dcf862943098a2b0b01e11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6a06fe99785103110350b8067363c31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce374f79c976ed47c260611aa08f479b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adapting-Video-Diffusion-Models-for-Time-Lapse-Microscopy"><a href="#Adapting-Video-Diffusion-Models-for-Time-Lapse-Microscopy" class="headerlink" title="Adapting Video Diffusion Models for Time-Lapse Microscopy"></a>Adapting Video Diffusion Models for Time-Lapse Microscopy</h2><p><strong>Authors:Alexander Holmberg, Nils Mechtel, Wei Ouyang</strong></p>
<p>We present a domain adaptation of video diffusion models to generate highly realistic time-lapse microscopy videos of cell division in HeLa cells. Although state-of-the-art generative video models have advanced significantly for natural videos, they remain underexplored in microscopy domains. To address this gap, we fine-tune a pretrained video diffusion model on microscopy-specific sequences, exploring three conditioning strategies: (1) text prompts derived from numeric phenotypic measurements (e.g., proliferation rates, migration speeds, cell-death frequencies), (2) direct numeric embeddings of phenotype scores, and (3) image-conditioned generation, where an initial microscopy frame is extended into a complete video sequence. Evaluation using biologically meaningful morphological, proliferation, and migration metrics demonstrates that fine-tuning substantially improves realism and accurately captures critical cellular behaviors such as mitosis and migration. Notably, the fine-tuned model also generalizes beyond the training horizon, generating coherent cell dynamics even in extended sequences. However, precisely controlling specific phenotypic characteristics remains challenging, highlighting opportunities for future work to enhance conditioning methods. Our results demonstrate the potential for domain-specific fine-tuning of generative video models to produce biologically plausible synthetic microscopy data, supporting applications such as in-silico hypothesis testing and data augmentation. </p>
<blockquote>
<p>我们针对视频扩散模型进行了领域适配，以生成高度逼真的时间显微镜视频，展示HeLa细胞的分裂过程。尽管针对自然视频的最新生成视频模型已经取得了显著的进展，但在显微镜领域仍缺乏研究。为了弥补这一空白，我们对预训练的视频扩散模型进行了微调，以适应显微镜特定序列，并探索了三种条件策略：（1）从数值表型测量值派生的文本提示（例如增殖率、迁移速度和细胞死亡频率），（2）表型分数的直接数值嵌入，（3）图像条件生成，其中将初始显微镜图像帧扩展为完整的视频序列。使用具有生物学意义的形态学、增殖和迁移指标进行的评估表明，微调显著提高了逼真度，并能准确捕捉关键细胞行为，如分裂和迁移。值得注意的是，微调后的模型在训练范围之外也具有通用性，即使在扩展序列中也能产生连贯的细胞动态。然而，精确控制特定的表型特征仍然具有挑战性，这突显了未来增强条件方法的机会。我们的结果证明了针对特定领域对生成视频模型进行微调的潜力，可以生成生物学上合理的合成显微镜数据，支持虚拟假设测试和数据增强等应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18583v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对显微视频扩散模型的领域适应研究，该研究通过微调预训练的视频扩散模型，生成了高度逼真的显微时间序列视频，展示了细胞分裂过程。该研究探索了三种条件策略，包括基于数值表型测量的文本提示、直接数值嵌入表型分数和图像条件生成。评估表明，微调显著提高了逼真度，并准确捕捉了关键细胞行为，如分裂和迁移。此外，该模型还能在训练范围之外进行推广，生成连贯的细胞动态序列。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究对视频扩散模型进行了领域适应，以生成高度逼真的显微时间序列视频，展示细胞分裂过程。</li>
<li>研究探索了三种条件策略来生成视频，包括使用文本提示、直接数值嵌入和图像条件生成。</li>
<li>评估表明，微调后的模型在生成逼真度方面有了显著提高，并能准确捕捉细胞的关键行为。</li>
<li>模型能够推广至训练范围之外，生成连贯的细胞动态序列。</li>
<li>研究强调了精确控制特定表型特征的挑战，并指出了未来改进条件方法的机会。</li>
<li>研究结果展示了领域特定微调生成视频模型生产生物学上合理的合成显微数据的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f2f42c15cf0168979ec5b2219da92326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a22349d600f03d4cf94348fc400cee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7b0c6b8320afd10d25f4c857613fcf9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder"><a href="#Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder" class="headerlink" title="Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder"></a>Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder</h2><p><strong>Authors:Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang</strong></p>
<p>Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著的性能。然而，在全新领域实现连续属性的精确控制，尤其是同时控制多个属性，仅通过文本指导仍存在重大挑战，例如数值属性，如眼睛睁开程度或汽车宽度。为了解决这一问题，我们引入了属性（Att）适配器，这是一种新型即插即用模块，旨在在预训练的扩散模型中实现精细的多属性控制。我们的方法从一组样本图像中学习单个控制适配器，这些图像可以是未配对的，并包含多个视觉属性。Att-Adapter利用解耦交叉注意力模块，自然地协调多个域属性与文本条件。我们进一步将条件变分自编码器（CVAE）引入到Att-Adapter中，以减轻过拟合问题，适应视觉世界的多样性。在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线。此外，我们的方法扩大了控制范围，并改善了多个属性之间的解纠缠，超越了StyleGAN技术。值得注意的是，Att-Adapter非常灵活，无需配对合成数据进行训练，并且很容易在单个模型中扩展到多个属性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11937v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著成效，但在新领域实现连续属性（如眼睛开合或汽车宽度等数值）的精确控制，尤其是同时控制多个属性时，仍然面临挑战。为解决这一问题，我们引入了属性适配器（Att-Adapter），这是一种新型即插即用模块，旨在在预训练的扩散模型中实现精细粒度的多属性控制。我们的方法通过从一组样本图像中学习单个控制适配器，这些图像可以是未配对的并且包含多个视觉属性。Att-Adapter利用解耦交叉注意模块，自然地协调文本条件与多个域属性。为进一步缓解过拟合问题并匹配视觉世界的多样性，我们还在Att-Adapter中引入了条件变分自编码器（CVAE）。在两项公开数据集上的评估显示，Att-Adapter在控制连续属性方面优于所有基于LoRA的方法。此外，我们的方法扩大了控制范围，并改善了多个属性之间的解纠缠，超越了StyleGAN技术。值得注意的是，Att-Adapter非常灵活，无需配对合成数据进行训练，并且易于扩展到单个模型中的多个属性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I Diffusion Models在生成高质量图像方面表现出卓越性能。</li>
<li>实现连续属性的精确控制，特别是在新领域面临挑战。</li>
<li>引入Attribute (Att) Adapter模块，旨在解决多属性控制问题。</li>
<li>Att-Adapter通过样本图像学习单个控制适配器，处理多个视觉属性。</li>
<li>利用解耦交叉注意模块和条件变分自编码器（CVAE）增强性能。</li>
<li>在公开数据集上的评估显示Att-Adapter在控制连续属性方面优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-50629aed4fae0a17a1c42ccc011f8cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d986435c039237a7e970cd0226135aa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-875e25fedbf0e4b14a9f58e93517bcda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bf7c2ca605970f534731b4f292f64f6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation"><a href="#StarGen-A-Spatiotemporal-Autoregression-Framework-with-Video-Diffusion-Model-for-Scalable-and-Controllable-Scene-Generation" class="headerlink" title="StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation"></a>StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion   Model for Scalable and Controllable Scene Generation</h2><p><strong>Authors:Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, Guofeng Zhang</strong></p>
<p>Recent advances in large reconstruction and generative models have significantly improved scene reconstruction and novel view generation. However, due to compute limitations, each inference with these large models is confined to a small area, making long-range consistent scene generation challenging. To address this, we propose StarGen, a novel framework that employs a pre-trained video diffusion model in an autoregressive manner for long-range scene generation. The generation of each video clip is conditioned on the 3D warping of spatially adjacent images and the temporally overlapping image from previously generated clips, improving spatiotemporal consistency in long-range scene generation with precise pose control. The spatiotemporal condition is compatible with various input conditions, facilitating diverse tasks, including sparse view interpolation, perpetual view generation, and layout-conditioned city generation. Quantitative and qualitative evaluations demonstrate StarGen’s superior scalability, fidelity, and pose accuracy compared to state-of-the-art methods. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen">https://zju3dv.github.io/StarGen</a>. </p>
<blockquote>
<p>近期重建和生成模型方面的进展极大地推动了场景重建和新颖视角生成的技术。然而，由于计算限制，这些大型模型的每次推断都局限于小范围，使得大范围一致的场景生成面临挑战。为了解决这个问题，我们提出了StarGen，这是一个采用预训练视频扩散模型的新型框架，以自回归的方式进行大范围场景生成。每个视频剪辑的生成都是以空间相邻图像的3D扭曲和先前生成的剪辑中时间上重叠的图像为条件，提高了大范围场景生成中的时空一致性，并实现了精确的姿势控制。时空条件与各种输入条件兼容，促进了包括稀疏视图插值、永久视图生成和布局控制城市生成在内的各种任务。定量和定性评估表明，与最新方法相比，StarGen在可扩展性、保真度和姿势准确性方面具有优势。项目页面：<a target="_blank" rel="noopener" href="https://zju3dv.github.io/StarGen%E3%80%82">https://zju3dv.github.io/StarGen。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05763v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型重建和生成模型的最新进展极大地改进了场景重建和新颖视角生成。然而，由于计算限制，这些大型模型的每次推理都局限于小范围，使得长距离一致场景生成面临挑战。为解决此问题，我们提出StarGen框架，采用预训练的视频扩散模型以自回归方式进行长距离场景生成。每个视频片段的生成以相邻图像的3D变形和先前生成的片段中时间上重叠的图像为条件，提高了长距离场景生成的时空一致性并实现了精确的姿态控制。这种时空条件与各种输入条件兼容，促进多种任务，包括稀疏视图插值、永久视图生成和布局控制城市生成。评估和实验证明StarGen在可扩展性、保真度和姿态准确性方面优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型重建和生成模型的最新进展推动了场景重建和新颖视角生成的进步。</li>
<li>由于计算限制，现有模型在长距离一致场景生成方面面临挑战。</li>
<li>StarGen框架采用预训练的视频扩散模型以自回归方式解决这一问题。</li>
<li>StarGen利用相邻图像的3D变形和先前生成的片段中的时间重叠图像来生成视频片段。</li>
<li>这提高了长距离场景生成的时空一致性，并实现了精确的姿态控制。</li>
<li>StarGen的时空条件适用于多种任务，包括稀疏视图插值、永久视图生成和布局控制城市生成。</li>
<li>评估和实验显示，StarGen在可扩展性、保真度和姿态准确性方面优于现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aeb6cbc6173a013f5744e3bf4cd04a26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cf0e8dcc9abb373f6f850fcd56b32a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa1a7dec6813a97f986d23fee4a718fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a973b1603e00e9a3843ad9f288cc4aa7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Controllable-Human-Image-Generation-with-Personalized-Multi-Garments"><a href="#Controllable-Human-Image-Generation-with-Personalized-Multi-Garments" class="headerlink" title="Controllable Human Image Generation with Personalized Multi-Garments"></a>Controllable Human Image Generation with Personalized Multi-Garments</h2><p><strong>Authors:Yisol Choi, Sangkyung Kwak, Sihyun Yu, Hyungwon Choi, Jinwoo Shin</strong></p>
<p>We present BootComp, a novel framework based on text-to-image diffusion models for controllable human image generation with multiple reference garments. Here, the main bottleneck is data acquisition for training: collecting a large-scale dataset of high-quality reference garment images per human subject is quite challenging, i.e., ideally, one needs to manually gather every single garment photograph worn by each human. To address this, we propose a data generation pipeline to construct a large synthetic dataset, consisting of human and multiple-garment pairs, by introducing a model to extract any reference garment images from each human image. To ensure data quality, we also propose a filtering strategy to remove undesirable generated data based on measuring perceptual similarities between the garment presented in human image and extracted garment. Finally, by utilizing the constructed synthetic dataset, we train a diffusion model having two parallel denoising paths that use multiple garment images as conditions to generate human images while preserving their fine-grained details. We further show the wide-applicability of our framework by adapting it to different types of reference-based generation in the fashion domain, including virtual try-on, and controllable human image generation with other conditions, e.g., pose, face, etc. </p>
<blockquote>
<p>我们提出了BootComp，这是一个基于文本到图像扩散模型的新型框架，用于可控的人体图像生成，具有多重参考服装。在这里，训练中的主要瓶颈是数据采集：针对每个受试者收集大规模的高质量参考服装图像数据集相当具有挑战性，即理想情况下，需要手动收集每个人所穿的每一件服装照片。为了解决这一问题，我们提出了一种数据生成管道，通过引入一个模型从每个人体图像中提取任何参考服装图像，来构建一个人体和多重服装配对的大型合成数据集。为了保证数据质量，我们还提出了一种过滤策略，通过测量人体图像中展示的服装和提取的服装之间的感知相似性来去除不良生成数据。最后，通过利用构建的合成数据集，我们训练了一个扩散模型，该模型具有两个并行降噪路径，使用多重服装图像作为条件来生成人体图像，同时保留其精细细节。我们还通过将该框架适应于时尚领域的不同参考基础生成任务来展示其广泛的应用性，包括虚拟试穿和其他可控人体图像生成条件，如姿势、面部等。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16801v3">PDF</a> CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://omnious.github.io/BootComp">https://omnious.github.io/BootComp</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于文本到图像扩散模型的BootComp新框架，用于可控的人体图像生成。针对训练数据获取的挑战，提出了构建大型合成数据集的方法，通过引入模型从每个人体图像中提取任何参考服装图像。同时，通过测量人体图像中的服装与提取的服装之间的感知相似性，提出了过滤策略以确保数据质量。利用构建好的合成数据集训练了一个拥有两条并行去噪路径的扩散模型，该模型能够以多张服装图像为条件生成人体图像，同时保留精细的细节。该框架还被适应于时尚领域的不同类型参考基础生成任务，如虚拟试衣、以其他条件如姿态、面部表情等可控的人体图像生成等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BootComp是一个基于文本到图像扩散模型的新框架，用于可控的人体图像生成。</li>
<li>训练数据获取是主要的瓶颈，因此提出了一种构建大型合成数据集的方法，通过模型从人体图像中提取参考服装图像。</li>
<li>提出了一种过滤策略，以确保数据质量，该策略基于测量人体图像中的服装与提取的服装之间的感知相似性。</li>
<li>利用合成数据集训练了一个扩散模型，该模型具有两条并行去噪路径，可以以多张服装图像为条件生成人体图像并保留细节。</li>
<li>该框架适用于多种参考基础生成任务，包括虚拟试衣和其他条件可控的人体图像生成等。</li>
<li>该框架在解决数据获取难题的同时，也推动了可控人体图像生成技术的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd98c487a23772ec4410193f2a94a72f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d7c476f5915e2ac8f62fd124f5a6009.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3a202bbb2a39030553983d628d790a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-128ca770e8247efd150d306a3e46d22c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67ae76ce4d023263f329c7f8592b9f37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deacd110450f01464eda12994b1e7aa9.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ControlSR-Taming-Diffusion-Models-for-Consistent-Real-World-Image-Super-Resolution"><a href="#ControlSR-Taming-Diffusion-Models-for-Consistent-Real-World-Image-Super-Resolution" class="headerlink" title="ControlSR: Taming Diffusion Models for Consistent Real-World Image Super   Resolution"></a>ControlSR: Taming Diffusion Models for Consistent Real-World Image Super   Resolution</h2><p><strong>Authors:Yuhao Wan, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Ming-Ming Cheng, Bo Li</strong></p>
<p>We present ControlSR, a new method that can tame Diffusion Models for consistent real-world image super-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to activate more generative priors of text-to-image diffusion models to make the output high-resolution (HR) images look better. However, since these methods rely too much on the generative priors, the content of the output images is often inconsistent with the input LR ones. To mitigate the above issue, in this work, we tame Diffusion Models by effectively utilizing LR information to impose stronger constraints on the control signals from ControlNet in the latent space. We show that our method can produce higher-quality control signals, which enables the super-resolution results to be more consistent with the LR image and leads to clearer visual results. In addition, we also propose an inference strategy that imposes constraints in the latent space using LR information, allowing for the simultaneous improvement of fidelity and generative ability. Experiments demonstrate that our model can achieve better performance across multiple metrics on several test sets and generate more consistent SR results with LR images than existing methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/ControlSR">https://github.com/HVision-NKU/ControlSR</a>. </p>
<blockquote>
<p>我们提出了ControlSR，这是一种新的方法，可以驯化扩散模型，用于实现一致的现实世界图像超分辨率（Real-ISR）。之前的Real-ISR模型主要关注如何激活文本到图像扩散模型的生产性先验知识，以使输出的高分辨率（HR）图像看起来更好。然而，由于这些方法过于依赖生成先验知识，输出图像的内容往往与输入的LR图像不一致。为了解决上述问题，在这项工作中，我们通过有效利用LR信息对ControlNet的控制信号施加更强的约束来驯化扩散模型。我们证明了我们的方法可以产生更高质量的控制信号，这使得超分辨率的结果更加符合LR图像，从而带来更清晰的视觉效果。此外，我们还提出了一种利用LR信息在潜在空间施加约束的推理策略，可以同时提高保真度和生成能力。实验表明，我们的模型在多个测试集上通过多种指标实现了更好的性能，与现有方法相比，生成的SR结果与LR图像更加一致。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/ControlSR%E3%80%82">https://github.com/HVision-NKU/ControlSR。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14279v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>控制SR方法能够驯服扩散模型，实现真实世界图像超分辨率（Real-ISR）的一致性。该方法有效利用低分辨率（LR）信息，对控制信号施加更强约束，产生更高质量的控制信号，使超分辨率结果与LR图像更一致，视觉结果更清晰。同时，提出一种利用LR信息在潜在空间施加约束的推理策略，提高了保真度和生成能力。实验证明，该模型在多个测试集上表现优异，与现有方法相比，生成SR结果与LR图像更一致。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ControlSR是一种新方法，可以驯服扩散模型，实现真实世界图像超分辨率（Real-ISR）的一致性。</li>
<li>该方法有效利用低分辨率（LR）信息，对控制信号施加更强约束。</li>
<li>ControlSR产生更高质量的控制信号，使超分辨率结果与LR图像更一致。</li>
<li>提出一种在潜在空间利用LR信息施加约束的推理策略，同时提高保真度和生成能力。</li>
<li>实验证明，ControlSR模型在多个测试集上表现优异。</li>
<li>与现有方法相比，ControlSR生成的SR结果与LR图像更一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14279">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b0b46cddf2e22051e57d5e293d8088e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac42168fbf311c7bb7c8abf9ebbc82f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e19c6e6264667d03ff87b49b1e1fe308.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea065ba4652c5ab44d264f2cf1639546.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bf479f31577052bffabda3cd3b18b92.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Automated-Filtering-of-Human-Feedback-Data-for-Aligning-Text-to-Image-Diffusion-Models"><a href="#Automated-Filtering-of-Human-Feedback-Data-for-Aligning-Text-to-Image-Diffusion-Models" class="headerlink" title="Automated Filtering of Human Feedback Data for Aligning Text-to-Image   Diffusion Models"></a>Automated Filtering of Human Feedback Data for Aligning Text-to-Image   Diffusion Models</h2><p><strong>Authors:Yongjin Yang, Sihyeon Kim, Hojung Jung, Sangmin Bae, SangMook Kim, Se-Young Yun, Kimin Lee</strong></p>
<p>Fine-tuning text-to-image diffusion models with human feedback is an effective method for aligning model behavior with human intentions. However, this alignment process often suffers from slow convergence due to the large size and noise present in human feedback datasets. In this work, we propose FiFA, a novel automated data filtering algorithm designed to enhance the fine-tuning of diffusion models using human feedback datasets with direct preference optimization (DPO). Specifically, our approach selects data by solving an optimization problem to maximize three components: preference margin, text quality, and text diversity. The concept of preference margin is used to identify samples that are highly informative in addressing the noisy nature of feedback dataset, which is calculated using a proxy reward model. Additionally, we incorporate text quality, assessed by large language models to prevent harmful contents, and consider text diversity through a k-nearest neighbor entropy estimator to improve generalization. Finally, we integrate all these components into an optimization process, with approximating the solution by assigning importance score to each data pair and selecting the most important ones. As a result, our method efficiently filters data automatically, without the need for manual intervention, and can be applied to any large-scale dataset. Experimental results show that FiFA significantly enhances training stability and achieves better performance, being preferred by humans 17% more, while using less than 0.5% of the full data and thus 1% of the GPU hours compared to utilizing full human feedback datasets. </p>
<blockquote>
<p>使用人类反馈对文本到图像的扩散模型进行微调是使模型行为与人类意图相一致的有效方法。然而，由于人类反馈数据集规模大且存在噪声，这种对齐过程往往存在收敛缓慢的问题。在这项工作中，我们提出了FiFA，这是一种新型自动化数据过滤算法，旨在利用人类反馈数据集对扩散模型进行微调，并结合直接偏好优化（DPO）。具体来说，我们的方法通过解决优化问题来选择数据，以最大化三个组成部分：偏好幅度、文本质量和文本多样性。偏好幅度的概念用于识别在解决反馈数据集的噪声本质时具有高度信息量的样本，它是通过使用代理奖励模型来计算的。此外，我们结合文本质量（通过大型语言模型进行评估，以防止有害内容），并通过k近邻熵估计器考虑文本多样性，以提高泛化能力。最后，我们将所有这些组件整合到优化过程中，通过为每个数据对分配重要性分数并选择最重要的数据来近似解决方案。因此，我们的方法能够自动过滤数据，无需人工干预，并可应用于任何大规模数据集。实验结果表明，FiFA显著提高了训练稳定性，并取得了更好的性能，人类偏好率提高了17%，同时使用的数据不到全数据的0.5%，与使用全人类反馈数据集相比，GPU小时数减少了1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10166v2">PDF</a> ICLR 2025; Project Page available at :   <a target="_blank" rel="noopener" href="https://sprain02.github.io/FiFA/">https://sprain02.github.io/FiFA/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的自动化数据过滤算法FiFA，用于优化文本到图像扩散模型利用人类反馈数据集进行微调的过程。通过解决优化问题，FiFA最大化偏好幅度、文本质量和文本多样性三个组成部分，从而提高训练稳定性和性能。实验结果显示，FiFA能自动过滤数据，无需人工干预，能应用于大规模数据集，显著提高训练稳定性和性能，人类偏好度高17%，同时仅使用0.5%的数据和1%的GPU小时数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FiFA算法能有效优化文本到图像扩散模型使用人类反馈数据集进行微调的过程。</li>
<li>FiFA通过最大化偏好幅度、文本质量和文本多样性三个组成部分来提高训练效果和模型性能。</li>
<li>偏好幅度用于识别反馈数据集中噪声大的样本，通过代理奖励模型计算。</li>
<li>文本质量通过大型语言模型评估，以防止有害内容。</li>
<li>文本多样性通过k近邻熵估计器来提高模型的泛化能力。</li>
<li>FiFA算法能自动过滤数据，无需手动干预，可应用于大规模数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6138b9ab0ed1ae7de171b283dd1a14c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c28d81c94d7bbbba83f52e9ce3aee047.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f6c12a6424198dbad9a5750ce417c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48496c8453686246038c8a3c55b38801.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-in-3D-Vision-A-Survey"><a href="#Diffusion-Models-in-3D-Vision-A-Survey" class="headerlink" title="Diffusion Models in 3D Vision: A Survey"></a>Diffusion Models in 3D Vision: A Survey</h2><p><strong>Authors:Zhen Wang, Dongyuan Li, Yaozu Wu, Tianyu He, Jiang Bian, Renhe Jiang</strong></p>
<p>In recent years, 3D vision has become a crucial field within computer vision, powering a wide range of applications such as autonomous driving, robotics, augmented reality, and medical imaging. This field relies on accurate perception, understanding, and reconstruction of 3D scenes from 2D images or text data sources. Diffusion models, originally designed for 2D generative tasks, offer the potential for more flexible, probabilistic methods that can better capture the variability and uncertainty present in real-world 3D data. In this paper, we review the state-of-the-art methods that use diffusion models for 3D visual tasks, including but not limited to 3D object generation, shape completion, point-cloud reconstruction, and scene construction. We provide an in-depth discussion of the underlying mathematical principles of diffusion models, outlining their forward and reverse processes, as well as the various architectural advancements that enable these models to work with 3D datasets. We also discuss the key challenges in applying diffusion models to 3D vision, such as handling occlusions and varying point densities, and the computational demands of high-dimensional data. Finally, we discuss potential solutions, including improving computational efficiency, enhancing multimodal fusion, and exploring the use of large-scale pretraining for better generalization across 3D tasks. This paper serves as a foundation for future exploration and development in this rapidly evolving field. </p>
<blockquote>
<p>近年来，3D视觉已成为计算机视觉领域的一个重要分支，为自动驾驶、机器人技术、增强现实和医学影像等多个应用领域提供了支持。该领域依赖于从2D图像或文本数据源对3D场景进行准确感知、理解和重建。扩散模型最初是为2D生成任务而设计的，具有更大的灵活性，能提供概率方法，更好地捕捉真实世界3D数据中的可变性和不确定性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04738v3">PDF</a> </p>
<p><strong>Summary</strong><br>     近年来，三维视觉已成为计算机视觉领域的关键分支，广泛应用于自动驾驶、机器人、增强现实和医学影像等领域。扩散模型原本用于二维生成任务，具有灵活捕捉现实世界三维数据中的变化和不确定性的潜力。本文综述了使用扩散模型进行三维视觉任务的最新方法，深入探讨了扩散模型的数学原理、正向和反向过程以及各种架构进展。同时，本文还讨论了将扩散模型应用于三维视觉的主要挑战和潜在解决方案。本文为未来在这一快速演变领域的探索和发展奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>三维视觉是计算机视觉领域的重要分支，广泛应用于多个领域。</li>
<li>扩散模型在三维视觉任务中具有巨大潜力，可以灵活捕捉现实世界数据的变异性。</li>
<li>扩散模型的数学原理、正向和反向过程在文中得到详细解释。</li>
<li>扩散模型在三维视觉任务中的应用包括三维物体生成、形状补全、点云重建和场景构建等。</li>
<li>应用于三维视觉的主要挑战包括处理遮挡和点密度变化以及高维数据的计算需求。</li>
<li>提高计算效率、增强多模式融合和探索大规模预训练是潜在解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04738">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c5510fc9c411527f6b03faaf820fbdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f58afa7aa8030111915ce88f88bfb9b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42352d2558e2eddcf7b8d17a5181cf60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9a31b55b506dfdfbe27ff08d70f494a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0c694e293a2e888b477d0cfe72590b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-cb3b6f3635d1eecc5dacd6980ba7b48c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-04  BioAtt Anatomical Prior Driven Low-Dose CT Denoising
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c7b0c6b8320afd10d25f4c857613fcf9.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-04  ILLUME+ Illuminating Unified MLLM with Dual Visual Tokenization and   Diffusion Refinement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15230.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
