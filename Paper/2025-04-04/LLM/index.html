<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  CADFormer Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c4198d82bea3dd405c405082c9f941d8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-04-æ›´æ–°"><a href="#2025-04-04-æ›´æ–°" class="headerlink" title="2025-04-04 æ›´æ–°"></a>2025-04-04 æ›´æ–°</h1><h2 id="CADFormer-Fine-Grained-Cross-modal-Alignment-and-Decoding-Transformer-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#CADFormer-Fine-Grained-Cross-modal-Alignment-and-Decoding-Transformer-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation"></a>CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation</h2><p><strong>Authors:Maofu Liu, Xin Jiang, Xiaokang Zhang</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task, aiming to segment specific target objects in remote sensing (RS) images based on a given language expression. Existing RRSIS methods typically employ coarse-grained unidirectional alignment approaches to obtain multimodal features, and they often overlook the critical role of language features as contextual information during the decoding process. Consequently, these methods exhibit weak object-level correspondence between visual and language features, leading to incomplete or erroneous predicted masks, especially when handling complex expressions and intricate RS image scenes. To address these challenges, we propose a fine-grained cross-modal alignment and decoding Transformer, CADFormer, for RRSIS. Specifically, we design a semantic mutual guidance alignment module (SMGAM) to achieve both vision-to-language and language-to-vision alignment, enabling comprehensive integration of visual and textual features for fine-grained cross-modal alignment. Furthermore, a textual-enhanced cross-modal decoder (TCMD) is introduced to incorporate language features during decoding, using refined textual information as context to enhance the relationship between cross-modal features. To thoroughly evaluate the performance of CADFormer, especially for inconspicuous targets in complex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which includes larger high-resolution RS image patches and semantically richer language expressions. Extensive experiments on the RRSIS-HR dataset and the popular RRSIS-D dataset demonstrate the effectiveness and superiority of CADFormer. Datasets and source codes will be available at <a target="_blank" rel="noopener" href="https://github.com/zxk688">https://github.com/zxk688</a>. </p>
<blockquote>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„è¯­è¨€è¡¨è¾¾å¯¹é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒä¸­çš„ç‰¹å®šç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚ç°æœ‰çš„RRSISæ–¹æ³•é€šå¸¸é‡‡ç”¨ç²—ç²’åº¦å•å‘å¯¹é½æ–¹æ³•è·å¾—å¤šæ¨¡æ€ç‰¹å¾ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†è¯­è¨€ç‰¹å¾åœ¨è§£ç è¿‡ç¨‹ä¸­çš„å…³é”®ä½œç”¨ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•åœ¨è§†è§‰å’Œè¯­è¨€ç‰¹å¾ä¹‹é—´è¡¨ç°å‡ºè¾ƒå¼±çš„å¯¹è±¡çº§å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´é¢„æµ‹æ©è†œä¸å®Œæ•´æˆ–é”™è¯¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„è¡¨è¾¾å¼å’Œé¥æ„Ÿçš„ç»†è…»å›¾åƒåœºæ™¯æ—¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹RRSISçš„ç²¾ç»†ç²’åº¦è·¨æ¨¡æ€å¯¹é½å’Œè§£ç Transformerï¼Œåä¸ºCADFormerã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯­ä¹‰ç›¸äº’å¼•å¯¼å¯¹é½æ¨¡å—ï¼ˆSMGAMï¼‰æ¥å®ç°è§†è§‰åˆ°è¯­è¨€ä»¥åŠè¯­è¨€åˆ°è§†è§‰çš„å¯¹é½ï¼Œå®ç°å¯¹è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„å…¨é¢èåˆï¼Œå®ç°ç²¾ç»†ç²’åº¦çš„è·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–‡æœ¬å¢å¼ºè·¨æ¨¡æ€è§£ç å™¨ï¼ˆTCMDï¼‰ï¼Œåœ¨è§£ç è¿‡ç¨‹ä¸­èå…¥è¯­è¨€ç‰¹å¾ï¼Œåˆ©ç”¨ç²¾ç‚¼çš„æ–‡æœ¬ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¢å¼ºè·¨æ¨¡æ€ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°CADFormerçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤æ‚åœºæ™¯ä¸­ä¸æ˜æ˜¾çš„ç›®æ ‡ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„RRSISæ•°æ®é›†ï¼Œåä¸ºRRSIS-HRï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬æ›´å¤§çš„é«˜åˆ†è¾¨ç‡RSå›¾åƒè¡¥ä¸å’Œè¯­ä¹‰æ›´ä¸°å¯Œçš„è¯­è¨€è¡¨è¾¾ã€‚åœ¨RRSIS-HRæ•°æ®é›†å’Œæµè¡Œçš„RRSIS-Dæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†CADFormerçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚æ•°æ®é›†å’Œæºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zxk688">https://github.com/zxk688</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23456v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡ï¼ˆRRSISï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾ç»†çš„è·¨æ¨¡æ€å¯¹é½ä¸è§£ç Transformerï¼ˆCADFormerï¼‰ã€‚å®ƒè®¾è®¡äº†è¯­ä¹‰äº’å¯¼å‘å¯¹é½æ¨¡å—ï¼ˆSMGAMï¼‰å®ç°è§†è§‰ä¸è¯­è¨€çš„åŒå‘å¯¹é½ï¼Œå¹¶å¼•å…¥äº†æ–‡æœ¬å¢å¼ºè·¨æ¨¡æ€è§£ç å™¨ï¼ˆTCMDï¼‰åœ¨è§£ç è¿‡ç¨‹ä¸­èå…¥è¯­è¨€ç‰¹å¾ã€‚ä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ„å»ºäº†æ–°çš„RRSISæ•°æ®é›†RRSIS-HRã€‚å®éªŒè¡¨æ˜ï¼ŒCADFormeråœ¨RRSIS-HRåŠæµè¡Œæ•°æ®é›†RRSIS-Dä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RRSISæ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„è¯­è¨€è¡¨è¾¾å¯¹é¥æ„Ÿå›¾åƒä¸­çš„ç‰¹å®šç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨ç²—ç²’åº¦çš„å•å‘å¯¹é½æ–¹æ³•è·å¾—å¤šæ¨¡æ€ç‰¹å¾ï¼Œå¿½è§†äº†è¯­è¨€ç‰¹å¾ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>CADFormeré€šè¿‡è®¾è®¡SMGAMæ¨¡å—å®ç°è§†è§‰ä¸è¯­è¨€çš„åŒå‘ç²¾ç»†å¯¹é½ã€‚</li>
<li>TCMDè§£ç å™¨åœ¨è§£ç è¿‡ç¨‹ä¸­èå…¥äº†è¯­è¨€ç‰¹å¾ï¼Œåˆ©ç”¨ç²¾ç‚¼çš„æ–‡æœ¬ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>ä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ„å»ºäº†æ–°çš„RRSISæ•°æ®é›†RRSIS-HRï¼ŒåŒ…å«å¤§å°ºåº¦é«˜åˆ†è¾¨ç‡é¥æ„Ÿå›¾åƒå’Œä¸°å¯Œçš„è¯­è¨€è¡¨è¾¾ã€‚</li>
<li>åœ¨RRSIS-HRåŠRRSIS-Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜CADFormerå…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3bd40f76e978ec2253da487287569be9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccbcdaad1110d35330f0dc0636a7a2e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b40f4010a95e815d4d595b15de803e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3f96982cd38744ce6fecdd55f92ca65.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EagleVision-Object-level-Attribute-Multimodal-LLM-for-Remote-Sensing"><a href="#EagleVision-Object-level-Attribute-Multimodal-LLM-for-Remote-Sensing" class="headerlink" title="EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing"></a>EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing</h2><p><strong>Authors:Hongxiang Jiang, Jihao Yin, Qixiong Wang, Jiaqi Feng, Guo Chen</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have demonstrated impressive results in various visual tasks. However, in remote sensing (RS), high resolution and small proportion of objects pose challenges to existing MLLMs, which struggle with object-centric tasks, particularly in precise localization and fine-grained attribute description for each object. These RS MLLMs have not yet surpassed classical visual perception models, as they only provide coarse image understanding, leading to limited gains in real-world scenarios. To address this gap, we establish EagleVision, an MLLM tailored for remote sensing that excels in object detection and attribute comprehension. Equipped with the Attribute Disentangle module, EagleVision learns disentanglement vision tokens to express distinct attributes. To support object-level visual-language alignment, we construct EVAttrs-95K, the first large-scale object attribute understanding dataset in RS for instruction tuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves state-of-the-art performance on both fine-grained object detection and object attribute understanding tasks, highlighting the mutual promotion between detection and understanding capabilities in MLLMs. The code, model, data, and demo will be available at <a target="_blank" rel="noopener" href="https://github.com/XiangTodayEatsWhat/EagleVision">https://github.com/XiangTodayEatsWhat/EagleVision</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨é¥æ„Ÿï¼ˆRSï¼‰é¢†åŸŸï¼Œé«˜åˆ†è¾¨ç‡å’Œå°æ¯”ä¾‹çš„å¯¹è±¡å¯¹ç°æœ‰çš„MLLMsæ„æˆäº†æŒ‘æˆ˜ï¼Œå®ƒä»¬åœ¨ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šè¡¨ç°æŒ£æ‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¯ä¸ªå¯¹è±¡çš„ç²¾ç¡®å®šä½å’Œç²¾ç»†ç²’åº¦å±æ€§æè¿°æ–¹é¢ã€‚è¿™äº›ç”¨äºé¥æ„Ÿçš„MLLMså°šæœªè¶…è¶Šç»å…¸è§†è§‰æ„ŸçŸ¥æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åªæä¾›ç²—ç•¥çš„å›¾åƒç†è§£ï¼Œå¯¼è‡´åœ¨çœŸå®åœºæ™¯ä¸­çš„æ”¶ç›Šæœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å»ºç«‹äº†EagleVisionï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿå®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ“…é•¿å¯¹è±¡æ£€æµ‹å’Œå±æ€§ç†è§£ã€‚é…å¤‡äº†å±æ€§åˆ†ç¦»æ¨¡å—åï¼ŒEagleVisionå­¦ä¹ åˆ†ç¦»è§†è§‰ä»¤ç‰Œä»¥è¡¨è¾¾ä¸åŒçš„å±æ€§ã€‚ä¸ºäº†æ”¯æŒå¯¹è±¡çº§åˆ«çš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œæˆ‘ä»¬æ„å»ºäº†EVAttrs-95Kæ•°æ®é›†ï¼Œè¿™æ˜¯é¥æ„Ÿé¢†åŸŸä¸­ç”¨äºæŒ‡ä»¤è°ƒæ•´çš„ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡å¯¹è±¡å±æ€§ç†è§£æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„è¯„ä¼°åŸºå‡†EVBenchã€‚EagleVisionåœ¨ç²¾ç»†ç²’åº¦å¯¹è±¡æ£€æµ‹å’Œå¯¹è±¡å±æ€§ç†è§£ä»»åŠ¡ä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾å‡ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ£€æµ‹å’Œç†è§£èƒ½åŠ›ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚ä»£ç ã€æ¨¡å‹ã€æ•°æ®å’Œæ¼”ç¤ºå°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/XiangTodayEatsWhat/EagleVision%E3%80%82">https://github.com/XiangTodayEatsWhat/EagleVisionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23330v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨é¥æ„Ÿé¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç²¾ç¡®å®šä½å’Œç²¾ç»†å±æ€§æè¿°ç­‰å¯¹è±¡ä¸­å¿ƒä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EagleVisionæ¨¡å‹ï¼Œå¹¶è®¾è®¡äº†Attribute Disentangleæ¨¡å—ä»¥è¿›è¡Œç²¾ç»†åŒ–å¯¹è±¡å±æ€§ç†è§£ã€‚æˆ‘ä»¬å»ºç«‹äº†EVAttrs-95Kæ•°æ®é›†å’ŒEVBenchè¯„ä¼°åŸºå‡†ä»¥æ”¯æŒå¯¹è±¡çº§è§†è§‰è¯­è¨€å¯¹é½ã€‚EagleVisionåœ¨ç²¾ç»†ç²’åº¦å¯¹è±¡æ£€æµ‹å’Œå¯¹è±¡å±æ€§ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é¥æ„Ÿï¼ˆRSï¼‰é¢†åŸŸçš„å¯¹è±¡ä¸­å¿ƒä»»åŠ¡ä¸Šé‡åˆ°äº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç¡®å®šä½å’Œç²¾ç»†å±æ€§æè¿°æ–¹é¢ã€‚</li>
<li>ç°æœ‰MLLMsåœ¨é¥æ„Ÿåº”ç”¨ä¸­å°šæœªè¶…è¶Šç»å…¸è§†è§‰æ„ŸçŸ¥æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬ä»…æä¾›ç²—ç•¥çš„å›¾åƒç†è§£ã€‚</li>
<li>EagleVisionæ˜¯ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿé¢†åŸŸçš„MLLMï¼Œæ“…é•¿å¯¹è±¡æ£€æµ‹å’Œå±æ€§ç†è§£ã€‚</li>
<li>Attribute Disentangleæ¨¡å—ä½¿EagleVisionèƒ½å¤Ÿå­¦ä¹ è§£çº ç¼ çš„è§†è§‰ä»¤ç‰Œä»¥è¡¨è¾¾ä¸åŒçš„å±æ€§ã€‚</li>
<li>ä¸ºæ”¯æŒå¯¹è±¡çº§è§†è§‰è¯­è¨€å¯¹é½ï¼Œå»ºç«‹äº†EVAttrs-95Kæ•°æ®é›†å’ŒEVBenchè¯„ä¼°åŸºå‡†ã€‚</li>
<li>EagleVisionåœ¨ç²¾ç»†ç²’åº¦å¯¹è±¡æ£€æµ‹å’Œå¯¹è±¡å±æ€§ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</li>
<li>æ£€æµ‹å’Œç†è§£èƒ½åŠ›åœ¨MLLMsä¸­ç›¸äº’ä¿ƒè¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-077bae7a0c808d43230fc2f5cd5cb5bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11fe065fbfc6fbb680d2cfc1dfd4ba50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34fa62c5a977cb805a41c67ead9cb834.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5b31b1640012a150899780328c4cd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f59b1ce761f96019c4008b4aaa3d2c68.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RefChartQA-Grounding-Visual-Answer-on-Chart-Images-through-Instruction-Tuning"><a href="#RefChartQA-Grounding-Visual-Answer-on-Chart-Images-through-Instruction-Tuning" class="headerlink" title="RefChartQA: Grounding Visual Answer on Chart Images through Instruction   Tuning"></a>RefChartQA: Grounding Visual Answer on Chart Images through Instruction   Tuning</h2><p><strong>Authors:Alexander Vogel, Omar Moured, Yufan Chen, Jiaming Zhang, Rainer Stiefelhagen</strong></p>
<p>Recently, Vision Language Models (VLMs) have increasingly emphasized document visual grounding to achieve better human-computer interaction, accessibility, and detailed understanding. However, its application to visualizations such as charts remains under-explored due to the inherent complexity of interleaved visual-numerical relationships in chart images. Existing chart understanding methods primarily focus on answering questions without explicitly identifying the visual elements that support their predictions. To bridge this gap, we introduce RefChartQA, a novel benchmark that integrates Chart Question Answering (ChartQA) with visual grounding, enabling models to refer elements at multiple granularities within chart images. Furthermore, we conduct a comprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across different categories. Our experiments demonstrate that incorporating spatial awareness via grounding improves response accuracy by over 15%, reducing hallucinations, and improving model reliability. Additionally, we identify key factors influencing text-spatial alignment, such as architectural improvements in TinyChart, which leverages a token-merging module for enhanced feature fusion. Our dataset is open-sourced for community development and further advancements. All models and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¶Šæ¥è¶Šå¼ºè°ƒæ–‡æ¡£çš„è§†è§‰å®šä½ï¼Œä»¥å®ç°æ›´å¥½çš„äººæœºäº¤äº’ã€å¯è®¿é—®æ€§å’Œæ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œç”±äºå…¶å›¾è¡¨å›¾åƒä¸­äº¤ç»‡çš„è§†è§‰æ•°å­—å…³ç³»å›ºæœ‰çš„å¤æ‚æ€§ï¼Œå…¶åœ¨å›¾è¡¨ç­‰å¯è§†åŒ–ä¸­çš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚ç°æœ‰çš„å›¾è¡¨ç†è§£æ–¹æ³•ä¸»è¦å…³æ³¨å›ç­”é—®é¢˜ï¼Œè€Œæ²¡æœ‰æ˜ç¡®è¯†åˆ«æ”¯æŒå…¶é¢„æµ‹çš„è§†è§‰å…ƒç´ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefChartQAï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ä¸è§†è§‰å®šä½ç›¸ç»“åˆçš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å›¾è¡¨å›¾åƒå†…çš„å¤šä¸ªç²’åº¦ä¸Šå‚è€ƒå…ƒç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡ä»¤è°ƒæ•´äº†5ä¸ªä¸åŒç±»åˆ«çš„æœ€å…ˆè¿›VLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å®šä½èå…¥ç©ºé—´æ„è¯†æé«˜äº†è¶…è¿‡15%çš„å“åº”å‡†ç¡®æ€§ï¼Œå‡å°‘äº†å¹»è§‰ï¼Œæé«˜äº†æ¨¡å‹å¯é æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†å½±å“æ–‡æœ¬ç©ºé—´å¯¹é½çš„å…³é”®å› ç´ ï¼Œå¦‚TinyChartä¸­çš„æ¶æ„æ”¹è¿›ï¼Œå®ƒåˆ©ç”¨ä»¤ç‰Œåˆå¹¶æ¨¡å—å¢å¼ºç‰¹å¾èåˆã€‚æˆ‘ä»¬çš„æ•°æ®é›†å·²å¼€æºä¾›ç¤¾åŒºå‘å±•å’Œè¿›ä¸€æ­¥è¿›æ­¥ã€‚æ‰€æœ‰æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/moured/RefChartQAä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23131v1">PDF</a> All models and code will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a></p>
<p><strong>Summary</strong>ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ­£é€æ¸é‡è§†æ–‡æ¡£è§†è§‰å®šä½ï¼Œä»¥æé«˜äººæœºäº¤äº’ã€å¯è®¿é—®æ€§å’Œæ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œç”±äºå›¾è¡¨å›¾åƒä¸­è§†è§‰ä¸æ•°å€¼å…³ç³»çš„å›ºæœ‰å¤æ‚æ€§ï¼Œå…¶åœ¨å›¾è¡¨ç­‰å¯è§†åŒ–æ–¹é¢çš„åº”ç”¨ä»è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¼•å…¥äº†RefChartQAåŸºå‡†æµ‹è¯•ï¼Œå®ƒå°†å›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ä¸è§†è§‰å®šä½ç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å›¾è¡¨å›¾åƒä¸­çš„å¤šä¸ªç²’åº¦çº§åˆ«ä¸Šå¼•ç”¨å…ƒç´ ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å®šä½èå…¥ç©ºé—´æ„è¯†å¯ä»¥æé«˜å“åº”å‡†ç¡®æ€§è¶…è¿‡15%ï¼Œå‡å°‘å¹»è§‰å¹¶æé«˜æ¨¡å‹å¯é æ€§ã€‚æ­¤å¤–ï¼Œå…¬å¼€çš„æ•°æ®é›†å¯ä¾›ç¤¾åŒºå¼€å‘å’Œè¿›ä¸€æ­¥è¿›å±•ä¹‹ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Vision Language Models (VLMs) å¼ºè°ƒæ–‡æ¡£è§†è§‰å®šä½çš„é‡è¦æ€§ï¼Œä»¥å¢å¼ºäººæœºäº¤äº’å’Œæ·±å…¥ç†è§£ã€‚</li>
<li>VLMsåœ¨å›¾è¡¨å¯è§†åŒ–æ–¹é¢çš„åº”ç”¨ä»å¤„äºæ¢ç´¢é˜¶æ®µï¼Œå› ä¸ºéœ€è¦å¤„ç†è§†è§‰ä¸æ•°å€¼å…³ç³»çš„å¤æ‚æ€§ã€‚</li>
<li>RefChartQAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç»“åˆäº†å›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰å’Œè§†è§‰å®šä½ã€‚</li>
<li>é€šè¿‡èå…¥ç©ºé—´æ„è¯†ï¼Œå®šä½å¯ä»¥æé«˜å“åº”å‡†ç¡®æ€§è¶…è¿‡15%ï¼Œå¹¶å‡å°‘å¹»è§‰ã€‚</li>
<li>RefChartQAæ•°æ®é›†å…¬å¼€ä¾›ç¤¾åŒºä½¿ç”¨ï¼Œä¿ƒè¿›è¿›ä¸€æ­¥çš„å‘å±•ã€‚</li>
<li>å®éªŒè¯„ä¼°äº†äº”ç§æœ€æ–°VLMsçš„ä¸åŒç±»åˆ«ï¼Œæ˜¾ç¤ºæ¨¡å‹åœ¨å®šä½æ–¹é¢çš„æ”¹è¿›å¯¹ç»“æœæœ‰ç§¯æå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4198d82bea3dd405c405082c9f941d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b4289ae7d243fcc6ae5abfd779b27c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72193e9cdddf05e9852341443bbfd1ea.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training"><a href="#Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training" class="headerlink" title="Unicorn: Text-Only Data Synthesis for Vision Language Model Training"></a>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</h2><p><strong>Authors:Xiaomin Yu, Pengxiang Ding, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, Kejian Wu, Zhaoxin Fan, Ziyue Qiao, Donglin Wang</strong></p>
<p>Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Yu-xm/Unicorn.git">https://github.com/Yu-xm/Unicorn.git</a>. </p>
<blockquote>
<p>è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šå¸¸éœ€è¦å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å›¾æ–‡å¯¹æ•°æ®ï¼Œä½†æ”¶é›†æˆ–åˆæˆè¿™æ ·çš„æ•°æ®æˆæœ¬å¾ˆé«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ–‡æœ¬æ•°æ®éå¸¸ä¸°å¯Œä¸”ä»·æ ¼ä½å»‰ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šæ˜¯å¦å¯ä»¥ä»çº¯æ–‡æœ¬åˆæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Ÿä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè·¨èåˆçš„ä¸‰é˜¶æ®µå¤šæ¨¡æ€æ•°æ®åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šUnicorn-1.2Må’ŒUnicorn-471K-Instructionã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼šå¤šæ ·åŒ–æ ‡é¢˜æ•°æ®åˆæˆä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰©å±•ç¨€ç–æ ‡é¢˜ç§å­æ¥æ„å»º120ä¸‡å¼ è¯­ä¹‰ä¸°å¯Œçš„é«˜è´¨é‡å›¾ç‰‡æ ‡é¢˜ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼šæŒ‡ä»¤è°ƒæ•´æ•°æ®ç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬å°†47.1ä¸‡å¼ å›¾ç‰‡çš„æ ‡é¢˜è¿›ä¸€æ­¥å¤„ç†æˆå¤šè½®æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ï¼Œä»¥æ”¯æŒå¤æ‚æ¨ç†ã€‚æœ€åï¼Œåœ¨ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡æ€è¡¨ç¤ºè½¬æ¢ä¸­ï¼Œè¿™äº›æ–‡æœ¬æ ‡é¢˜è¡¨ç¤ºè¢«è½¬æ¢æˆè§†è§‰è¡¨ç¤ºï¼Œä»è€Œäº§ç”Ÿå¤šæ ·åŒ–çš„åˆæˆå›¾åƒè¡¨ç¤ºã€‚è¿™ä¸‰é˜¶æ®µçš„è¿‡ç¨‹ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºç”¨äºé¢„è®­ç»ƒçš„Unicorn-1.2Må’Œç”¨äºæŒ‡ä»¤è°ƒæ•´çš„Unicorn-471K-Instructionï¼Œè€Œæ— éœ€ä¾èµ–çœŸå®å›¾åƒã€‚é€šè¿‡æ¶ˆé™¤å¯¹çœŸå®å›¾åƒçš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºVLMsè®­ç»ƒæä¾›äº†æˆæœ¬ä½å»‰ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Yu-xm/Unicorn.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yu-xm/Unicorn.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22655v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨èåˆçš„ä¸‰é˜¶æ®µå¤šåª’ä½“æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é€šè¿‡çº¯æ–‡æœ¬æ–¹å¼åˆæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šå¤šæ ·åŒ–æ ‡é¢˜æ•°æ®åˆæˆã€æŒ‡ä»¤è°ƒæ•´æ•°æ®ç”Ÿæˆå’Œæ¨¡æ€è¡¨ç¤ºè½¬ç§»ã€‚æœ€ç»ˆæ„å»ºäº†ç”¨äºé¢„è®­ç»ƒçš„Unicorn-1.2Mæ•°æ®é›†å’Œç”¨äºæŒ‡ä»¤è°ƒæ•´çš„Unicorn-471K-Instructionæ•°æ®é›†ã€‚è¯¥æ¡†æ¶æ¶ˆé™¤äº†å¯¹çœŸå®å›¾åƒçš„ä¾èµ–ï¼ŒåŒæ—¶ä¿æŒäº†æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä¸ºè§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒæä¾›äº†æˆæœ¬æ•ˆç›Šé«˜ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§è·¨èåˆçš„ä¸‰é˜¶æ®µå¤šåª’ä½“æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆè§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>æ¡†æ¶åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šå¤šæ ·åŒ–æ ‡é¢˜æ•°æ®åˆæˆã€æŒ‡ä»¤è°ƒæ•´æ•°æ®ç”Ÿæˆå’Œæ¨¡æ€è¡¨ç¤ºè½¬ç§»ã€‚</li>
<li>é€šè¿‡çº¯æ–‡æœ¬æ–¹å¼åˆæˆé«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œæ— éœ€ä¾èµ–çœŸå®å›¾åƒã€‚</li>
<li>æ„å»ºäº†ç”¨äºé¢„è®­ç»ƒçš„Unicorn-1.2Mæ•°æ®é›†å’Œç”¨äºæŒ‡ä»¤è°ƒæ•´çš„Unicorn-471K-Instructionæ•°æ®é›†ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿä¿æŒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶æé«˜æˆæœ¬æ•ˆç›Šå’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç¨€ç–æ ‡é¢˜ç§å­çš„æ‰©å±•ï¼Œç”Ÿæˆè¯­ä¹‰ä¸Šå¤šæ ·åŒ–çš„é«˜è´¨é‡æ ‡é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b5a6c5dab262e94f2d924815911a4af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1606f982b34f4fedf82ca91a3b18e688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c80cdd04d88de4dc6bdf90eac117cf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b81106d17df4069bd1e3f743a551ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6dd40a40239345a8267f6d75dcf9c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f564bc8a70aca3fa18fce60f6bb5e4f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Integrating-Artificial-Intelligence-with-Human-Expertise-An-In-depth-Analysis-of-ChatGPTâ€™s-Capabilities-in-Generating-Metamorphic-Relations"><a href="#Integrating-Artificial-Intelligence-with-Human-Expertise-An-In-depth-Analysis-of-ChatGPTâ€™s-Capabilities-in-Generating-Metamorphic-Relations" class="headerlink" title="Integrating Artificial Intelligence with Human Expertise: An In-depth   Analysis of ChatGPTâ€™s Capabilities in Generating Metamorphic Relations"></a>Integrating Artificial Intelligence with Human Expertise: An In-depth   Analysis of ChatGPTâ€™s Capabilities in Generating Metamorphic Relations</h2><p><strong>Authors:Yifan Zhang, Dave Towey, Matthew Pike, Quang-Hung Luu, Huai Liu, Tsong Yueh Chen</strong></p>
<p>Context: This paper provides an in-depth examination of the generation and evaluation of Metamorphic Relations (MRs) using GPT models developed by OpenAI, with a particular focus on the capabilities of GPT-4 in software testing environments.   Objective: The aim is to examine the quality of MRs produced by GPT-3.5 and GPT-4 for a specific System Under Test (SUT) adopted from an earlier study, and to introduce and apply an improved set of evaluation criteria for a diverse range of SUTs.   Method: The initial phase evaluates MRs generated by GPT-3.5 and GPT-4 using criteria from a prior study, followed by an application of an enhanced evaluation framework on MRs created by GPT-4 for a diverse range of nine SUTs, varying from simple programs to complex systems incorporating AI&#x2F;ML components. A custom-built GPT evaluator, alongside human evaluators, assessed the MRs, enabling a direct comparison between automated and human evaluation methods.   Results: The study finds that GPT-4 outperforms GPT-3.5 in generating accurate and useful MRs. With the advanced evaluation criteria, GPT-4 demonstrates a significant ability to produce high-quality MRs across a wide range of SUTs, including complex systems incorporating AI&#x2F;ML components.   Conclusions: GPT-4 exhibits advanced capabilities in generating MRs suitable for various applications. The research underscores the growing potential of AI in software testing, particularly in the generation and evaluation of MRs, and points towards the complementarity of human and AI skills in this domain. </p>
<blockquote>
<p>æœ¬æ–‡æ·±å…¥æ¢è®¨äº†ä½¿ç”¨OpenAIå¼€å‘çš„GPTæ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯GPT-4ï¼‰åœ¨è½¯ä»¶æµ‹è¯•ç¯å¢ƒä¸­ç”Ÿæˆå’Œè¯„ä»·Metamorphic Relations (MRs)çš„æ–¹æ³•ã€‚æ–‡ç« æ—¨åœ¨ç ”ç©¶GPT-3.5å’ŒGPT-4åœ¨ç‰¹å®šç³»ç»Ÿæµ‹è¯•ï¼ˆSUTï¼‰ç¯å¢ƒä¸‹ç”ŸæˆMRsçš„è´¨é‡ï¼Œå¹¶ä¸ºå¤šç§SUTså¼•å…¥å’Œåº”ç”¨äº†ä¸€å¥—æ”¹è¿›çš„è¯„ä»·æ ‡å‡†ã€‚åˆå§‹é˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®å…ˆå‰çš„ç ”ç©¶æ ‡å‡†è¯„ä¼°äº†GPT-3.5å’ŒGPT-4ç”Ÿæˆçš„MRsï¼Œç„¶ååº”ç”¨äº†ä¸€ä¸ªå¢å¼ºçš„è¯„ä»·æ¡†æ¶å¯¹GPT-4ä¸ºä¹ç§ä¸åŒçš„SUTsç”Ÿæˆçš„MRsè¿›è¡Œäº†è¯„ä»·ï¼Œè¿™äº›ç³»ç»Ÿä»ç®€å•çš„ç¨‹åºåˆ°å¤æ‚çš„åŒ…å«äººå·¥æ™ºèƒ½&#x2F;æœºå™¨å­¦ä¹ ç»„ä»¶çš„ç³»ç»Ÿä¸ç­‰ã€‚æˆ‘ä»¬è‡ªå®šä¹‰çš„GPTè¯„ä»·å·¥å…·å’Œäººç±»è¯„ä»·è€…å…±åŒå¯¹MRsè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–å’Œäººç±»è¯„ä»·æ–¹æ³•ä¹‹é—´çš„ç›´æ¥æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-4åœ¨ç”Ÿæˆå‡†ç¡®æœ‰ç”¨çš„MRsæ–¹é¢ä¼˜äºGPT-3.5ã€‚é€šè¿‡å…ˆè¿›çš„è¯„ä»·æ ‡å‡†ï¼ŒGPT-4æ˜¾ç¤ºå‡ºåœ¨å„ç§SUTsä¸Šç”Ÿæˆé«˜è´¨é‡MRsçš„æ˜¾è‘—èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤æ‚çš„åŒ…å«äººå·¥æ™ºèƒ½&#x2F;æœºå™¨å­¦ä¹ ç»„ä»¶çš„ç³»ç»Ÿã€‚è¿™è¡¨æ˜GPT-4åœ¨å„ç§åº”ç”¨ä¸­ç”ŸæˆMRsæ–¹é¢è¡¨ç°å‡ºé«˜çº§èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†äººå·¥æ™ºèƒ½åœ¨è½¯ä»¶æµ‹è¯•é¢†åŸŸï¼ˆç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå’Œè¯„ä»·MRsæ–¹é¢ï¼‰çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†äººç±»å’Œäººå·¥æ™ºèƒ½æŠ€èƒ½åœ¨è¿™ä¸ªé¢†åŸŸçš„äº’è¡¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22141v1">PDF</a> Submitted to Information and Software Technology</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ·±å…¥æ¢è®¨äº†ä½¿ç”¨OpenAIå¼€å‘çš„GPTæ¨¡å‹ç”Ÿæˆå’Œè¯„ä¼°å½¢æ€å…³ç³»ï¼ˆMRsï¼‰çš„è¿‡ç¨‹ï¼Œé‡ç‚¹ç ”ç©¶äº†GPT-4åœ¨è½¯ä»¶æµ‹è¯•ç¯å¢ƒä¸­çš„èƒ½åŠ›ã€‚æ–‡ç« æ—¨åœ¨è¯„ä¼°GPT-3.5å’ŒGPT-4å¯¹ç‰¹å®šç³»ç»Ÿï¼ˆé‡‡ç”¨æ—©æœŸç ”ç©¶ä¸­çš„ç³»ç»Ÿï¼‰ç”ŸæˆMRsçš„è´¨é‡ï¼Œå¹¶ä¸ºå¤šç§ç³»ç»Ÿå¼•å…¥å¹¶åº”ç”¨æ”¹è¿›çš„è¯„ä»·æ ‡å‡†ã€‚ç ”ç©¶æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨æ—©æœŸç ”ç©¶çš„æ ‡å‡†å¯¹GPT-3.5å’ŒGPT-4ç”Ÿæˆçš„MRsè¿›è¡Œè¯„ä¼°ï¼Œä»¥åŠå¯¹GPT-4ä¸ºä¹ç§ä¸åŒç³»ç»Ÿç”Ÿæˆçš„MRsåº”ç”¨æ”¹è¿›çš„è¯„ä»·æ¡†æ¶ã€‚ç»“åˆè‡ªå®šä¹‰çš„GPTè¯„ä¼°å™¨å’Œäººç±»è¯„ä¼°è€…ï¼Œå¯¹MRsè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†è‡ªåŠ¨ä¸äººç±»è¯„ä¼°æ–¹æ³•çš„ç›´æ¥æ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-4åœ¨ç”Ÿæˆå‡†ç¡®å’Œæœ‰ç”¨çš„MRsæ–¹é¢ä¼˜äºGPT-3.5ã€‚ä½¿ç”¨æ”¹è¿›çš„è¯„ä»·æ ‡å‡†ï¼ŒGPT-4æ˜¾ç¤ºå‡ºåœ¨å„ç§ç³»ç»Ÿï¼ˆåŒ…æ‹¬åŒ…å«äººå·¥æ™ºèƒ½&#x2F;æœºå™¨å­¦ä¹ ç»„ä»¶çš„å¤æ‚ç³»ç»Ÿï¼‰ä¸­äº§ç”Ÿé«˜è´¨é‡MRsçš„é‡è¦èƒ½åŠ›ã€‚æœ¬æ–‡çš„ç»“è®ºæ˜¯ï¼ŒGPT-4åœ¨ç”Ÿæˆé€‚ç”¨äºå„ç§åº”ç”¨çš„MRsæ–¹é¢å±•ç°äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œå¼ºè°ƒäººå·¥æ™ºèƒ½åœ¨è½¯ä»¶æµ‹è¯•é¢†åŸŸçš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå’Œè¯„ä¼°MRsæ–¹é¢ã€‚å®ƒè¡¨æ˜äººç±»å’Œäººå·¥æ™ºèƒ½çš„æŠ€èƒ½åœ¨è¿™ä¸ªé¢†åŸŸçš„äº’è¡¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GPT-4åœ¨ç”Ÿæˆå½¢æ€å…³ç³»ï¼ˆMRsï¼‰æ–¹é¢è¡¨ç°å‡ºä¼˜äºGPT-3.5çš„æ€§èƒ½ã€‚</li>
<li>GPT-4èƒ½å¤Ÿåœ¨å„ç§ç³»ç»Ÿï¼ˆåŒ…æ‹¬åŒ…å«AI&#x2F;MLç»„ä»¶çš„å¤æ‚ç³»ç»Ÿï¼‰ä¸­äº§ç”Ÿé«˜è´¨é‡çš„MRsã€‚</li>
<li>æ–‡ç« é€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰çš„GPTè¯„ä¼°å™¨å’Œäººç±»è¯„ä¼°è€…ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–å’Œäººç±»è¯„ä¼°æ–¹æ³•çš„ç›´æ¥æ¯”è¾ƒã€‚</li>
<li>GPTæ¨¡å‹åœ¨è½¯ä»¶æµ‹è¯•é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå’Œè¯„ä¼°MRsæ–¹é¢ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†äººå·¥æ™ºèƒ½åœ¨è½¯ä»¶æµ‹è¯•ä¸­çš„æˆé•¿æ½œåŠ›ã€‚</li>
<li>åœ¨ç”Ÿæˆå’Œè¯„ä¼°MRsæ–¹é¢ï¼Œäººç±»å’ŒAIçš„æŠ€èƒ½è¡¨ç°å‡ºäº’è¡¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1765ae94db9731490f1fdaef0938d38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09dc2c7d924adc3bfe8184792ab58592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd2603ac13adbaab2cb9ff1385dfa79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd60bd61fa30ef4778b2b03e8e0b793a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a57e9ab95956c56224cf87b4e9827eec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f029f4adf95c61ec168e3fb6b3cbc4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TEMPLE-Temporal-Preference-Learning-of-Video-LLMs-via-Difficulty-Scheduling-and-Pre-SFT-Alignment"><a href="#TEMPLE-Temporal-Preference-Learning-of-Video-LLMs-via-Difficulty-Scheduling-and-Pre-SFT-Alignment" class="headerlink" title="TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment"></a>TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment</h2><p><strong>Authors:Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun</strong></p>
<p>Video Large Language Models (Video LLMs) have achieved significant success by leveraging a two-stage paradigm: pretraining on large-scale video-text data for vision-language alignment, followed by supervised fine-tuning (SFT) for task-specific capabilities. However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and reliance on the next-token prediction paradigm during training. To address these limitations, we propose TEMPLE (TEMporal Preference Learning), a systematic framework that enhances Video LLMsâ€™ temporal reasoning capabilities through Direct Preference Optimization (DPO). To facilitate this, we introduce an automated preference data generation pipeline that systematically constructs preference pairs by selecting videos that are rich in temporal information, designing video-specific perturbation strategies, and finally evaluating model responses on clean and perturbed video inputs. Our temporal alignment features two key innovations: curriculum learning which that progressively increases perturbation difficulty to improve model robustness and adaptability; and â€œPre-SFT Alignmentâ€™â€™, applying preference optimization before instruction tuning to prioritize fine-grained temporal comprehension. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. We further analyze the transferability of DPO data across architectures and the role of difficulty scheduling in optimization. Our findings highlight our TEMPLE as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE">https://github.com/lscpku/TEMPLE</a>. </p>
<blockquote>
<p>è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰é€šè¿‡é‡‡ç”¨ä¸¤é˜¶æ®µèŒƒå¼å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡è§†é¢‘æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œç„¶åé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è·å¾—ç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºæ•°æ®ä¸­çš„æ—¶é—´å¯¹åº”å…³ç³»è¾ƒå¼±ä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼ï¼Œå› æ­¤åœ¨æ—¶é—´æ¨ç†æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TEMPLEï¼ˆæ—¶ç©ºåå¥½å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¢å¼ºè§†é¢‘LLMçš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–åå¥½æ•°æ®ç”Ÿæˆç®¡é“ï¼Œè¯¥ç®¡é“é€šè¿‡é€‰æ‹©æ—¶é—´ä¿¡æ¯ä¸°å¯Œçš„è§†é¢‘ã€è®¾è®¡é’ˆå¯¹è§†é¢‘çš„æ‰°åŠ¨ç­–ç•¥ï¼Œä»¥åŠè¯„ä¼°æ¨¡å‹å¯¹å¹²å‡€å’Œæ‰°åŠ¨è§†é¢‘è¾“å…¥çš„å“åº”æ¥ç³»ç»Ÿåœ°æ„å»ºåå¥½å¯¹ã€‚æˆ‘ä»¬çš„æ—¶é—´å¯¹é½æœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šè¯¾ç¨‹å­¦ä¹ ï¼Œé€æ­¥å¢åŠ æ‰°åŠ¨éš¾åº¦ä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œé€‚åº”æ€§ï¼›ä»¥åŠâ€œPre-SFTå¯¹é½â€ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´ä¹‹å‰åº”ç”¨åå¥½ä¼˜åŒ–ï¼Œä»¥ä¼˜å…ˆè¿›è¡Œç²¾ç»†çš„æ—¶é—´ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨ç›¸å¯¹è¾ƒå°‘è‡ªæˆ‘ç”Ÿæˆçš„DPOæ•°æ®æ—¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸ŠæŒç»­æé«˜è§†é¢‘LLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†DPOæ•°æ®åœ¨ä¸åŒæ¶æ„ä¹‹é—´çš„å¯è¿ç§»æ€§ä»¥åŠéš¾åº¦è°ƒåº¦åœ¨ä¼˜åŒ–ä¸­çš„è§’è‰²ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒTEMPLEå¯ä»¥ä½œä¸ºåŸºäºSFTçš„æ–¹æ³•çš„å¯æ‰©å±•å’Œé«˜æ•ˆçš„è¡¥å……ï¼Œä¸ºå¼€å‘å¯é çš„è§†é¢‘LLMé“ºå¹³äº†é“è·¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lscpku/TEMPLEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16929v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†TEMPLEæ¡†æ¶ï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æå‡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨åŒ–åå¥½æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€‰æ‹©å¯Œå«æ—¶é—´ä¿¡æ¯çš„è§†é¢‘ã€è®¾è®¡è§†é¢‘ç‰¹å®šæ‰°åŠ¨ç­–ç•¥ï¼Œå¹¶è¯„ä¼°æ¨¡å‹å¯¹å¹²å‡€å’Œæ‰°åŠ¨è§†é¢‘è¾“å…¥çš„å“åº”ã€‚å¼•å…¥è¯¾ç¨‹å­¦ä¹ æœºåˆ¶å’Œâ€œPre-SFTå¯¹é½â€ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒä¹‹å‰åº”ç”¨åå¥½ä¼˜åŒ–ï¼Œä¼˜å…ˆæé«˜ç²¾ç»†ç²’åº¦çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†Video LLMçš„æ€§èƒ½ï¼Œä¸”ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„DPOæ•°æ®é›†ç›¸å¯¹è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video LLMsé‡‡ç”¨ä¸¤é˜¶æ®µèŒƒå¼ï¼šé¢„è®­ç»ƒå¤§è§„æ¨¡è§†é¢‘æ–‡æœ¬æ•°æ®å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œç„¶åé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è·å¾—ç‰¹å®šä»»åŠ¡èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æ—¶é—´æ¨ç†å›°éš¾ï¼Œå› æ•°æ®ä¸­çš„æ—¶é—´å¯¹åº”å…³ç³»è¾ƒå¼±ï¼Œä¸”è®­ç»ƒæ—¶ä¾èµ–ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼ã€‚</li>
<li>TEMPLEæ¡†æ¶é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æå‡Video LLMsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TEMPLEå¼•å…¥è‡ªåŠ¨åŒ–åå¥½æ•°æ®ç”Ÿæˆç®¡é“ï¼ŒåŒ…æ‹¬é€‰æ‹©å¯Œå«æ—¶é—´ä¿¡æ¯çš„è§†é¢‘ã€è®¾è®¡è§†é¢‘ç‰¹å®šæ‰°åŠ¨ç­–ç•¥ï¼Œå¹¶è¯„ä¼°æ¨¡å‹å“åº”ã€‚</li>
<li>è¯¾ç¨‹å­¦ä¹ æœºåˆ¶å’Œâ€œPre-SFTå¯¹é½â€æ˜¯æé«˜æ¨¡å‹å¯¹æ—¶é—´ä¿¡æ¯ç†è§£çš„å…³é”®åˆ›æ–°ã€‚</li>
<li>å®éªŒè¯æ˜TEMPLEæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†Video LLMæ€§èƒ½ï¼Œä¸”ä½¿ç”¨è‡ªæˆ‘ç”Ÿæˆçš„DPOæ•°æ®é›†ç›¸å¯¹è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5510be75ebe851a3144f1b3a10d7e22c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e7d12dbbb5194fa383c16648b21567a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be6a348d7181b26b10b8be3bcacdd034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f5aa05b29fc3af80cf08594d4f77c34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abd74dc5d9651e2ca31dfde7f587f11b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning"><a href="#Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning" class="headerlink" title="Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning"></a>Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning</h2><p><strong>Authors: NVIDIA,  :, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, Zhe Zhang</strong></p>
<p>Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1">https://github.com/nvidia-cosmos/cosmos-reason1</a>. </p>
<blockquote>
<p>ç‰©ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿéœ€è¦åœ¨ç‰©ç†ä¸–ç•Œä¸­æ„ŸçŸ¥ã€ç†è§£å’Œæ‰§è¡Œå¤æ‚çš„åŠ¨ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Cosmos-Reason1æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸€ç³»åˆ—é•¿æ€è€ƒè¿‡ç¨‹ç†è§£ç‰©ç†ä¸–ç•Œï¼Œå¹¶ä»¥è‡ªç„¶è¯­è¨€ç”Ÿæˆé€‚å½“çš„å†³ç­–ï¼ˆä¾‹å¦‚ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ç‰©ç†äººå·¥æ™ºèƒ½æ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†ã€‚ä¸ºäº†è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œæˆ‘ä»¬ä½¿ç”¨å±‚æ¬¡æœ¬ä½“æ¥æ•è·å…³äºç©ºé—´ã€æ—¶é—´å’Œç‰©ç†çš„åŸºæœ¬å¸¸è¯†ã€‚ä¸ºäº†è¿›è¡Œå®ä½“æ¨ç†ï¼Œæˆ‘ä»¬ä¾èµ–äºä¸€ä¸ªäºŒç»´æœ¬ä½“ï¼Œè¯¥æœ¬ä½“å¯ä»¥æ¦‚æ‹¬ä¸åŒçš„ç‰©ç†å®ä½“ã€‚åŸºäºè¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå³Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚æˆ‘ä»¬åœ¨å››ä¸ªé˜¶æ®µå¯¹æ•°æ®è¿›è¡Œæ•´ç†å¹¶å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼šè§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç‰©ç†äººå·¥æ™ºèƒ½SFTå’Œç‰©ç†äººå·¥æ™ºèƒ½å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºåè®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æ ¹æ®æˆ‘ä»¬çš„æœ¬ä½“è®ºå»ºç«‹äº†ç‰©ç†å¸¸è¯†å’Œå®ä½“æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç‰©ç†äººå·¥æ™ºèƒ½SFTå’Œå¼ºåŒ–å­¦ä¹ å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ä¸ºäº†æ–¹ä¾¿ç‰©ç†äººå·¥æ™ºèƒ½çš„å¼€å‘ï¼Œæˆ‘ä»¬å°†åœ¨NVIDIA Open Model Licenseä¸‹æä¾›æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1">https://github.com/nvidia-cosmos/cosmos-reason1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15558v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†é’ˆå¯¹ç‰©ç†AIç³»ç»Ÿçš„ç ”ç©¶ï¼Œé‡ç‚¹ä»‹ç»äº†Cosmos-Reason1æ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·å¤‡ç†è§£ç‰©ç†ä¸–ç•Œå¹¶ç”Ÿæˆé€‚å½“å†³ç­–çš„èƒ½åŠ›ï¼Œé€šè¿‡é•¿é“¾æ€ç»´æ¨ç†è¿‡ç¨‹å®ç°ã€‚è®ºæ–‡å®šä¹‰äº†ç‰©ç†AIæ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œä¾§é‡äºç‰©ç†å¸¸è¯†å’Œä½“ç°æ¨ç†ã€‚ä¸ºè¡¨è¾¾ç‰©ç†å¸¸è¯†ï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–æœ¬ä½“è®ºæ•æ‰å…³äºç©ºé—´ã€æ—¶é—´å’Œç‰©ç†å­¦çš„æ ¹æœ¬çŸ¥è¯†ã€‚ä¸ºä½“ç°æ¨ç†ï¼Œä¾èµ–äºŒç»´æœ¬ä½“è®ºï¼Œåœ¨ä¸åŒç‰©ç†ä½“ç°ä¸­æ¦‚æ‹¬å…±æ€§ã€‚åŸºäºæ­¤ï¼Œå¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚æ¨¡å‹è®­ç»ƒåˆ†å››é˜¶æ®µï¼šè§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒã€ç‰©ç†AIç›‘ç£å’Œç‰©ç†AIå¼ºåŒ–å­¦ä¹ ã€‚è¯„ä¼°æ¨¡å‹æ—¶ï¼Œæ ¹æ®æœ¬ä½“è®ºå»ºç«‹äº†ç‰©ç†å¸¸è¯†å’Œä½“ç°æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºï¼Œç‰©ç†AIç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ å¸¦æ¥æ˜¾è‘—æ”¹è¿›ã€‚ä¸ºæ–¹ä¾¿ç‰©ç†AIçš„å‘å±•ï¼Œæ¨¡å‹å’Œä»£ç å°†åœ¨NVIDIA Open Model Licenseä¸‹æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cosmos-Reason1æ¨¡å‹å…·å¤‡ç†è§£ç‰©ç†ä¸–ç•Œå¹¶ç”Ÿæˆé€‚å½“å†³ç­–çš„èƒ½åŠ›ï¼Œé€šè¿‡é•¿é“¾æ€ç»´æ¨ç†è¿‡ç¨‹å®ç°ã€‚</li>
<li>è®ºæ–‡å®šä¹‰äº†ç‰©ç†AIæ¨ç†çš„å…³é”®èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç‰©ç†å¸¸è¯†å’Œä½“ç°æ¨ç†ã€‚</li>
<li>é‡‡ç”¨å±‚æ¬¡åŒ–æœ¬ä½“è®ºå’ŒäºŒç»´æœ¬ä½“è®ºæ¥ä»£è¡¨ç‰©ç†å¸¸è¯†å’Œä½“ç°æ¨ç†ã€‚</li>
<li>å¼€å‘äº†ä¸¤ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹Cosmos-Reason1-8Bå’ŒCosmos-Reason1-56Bã€‚</li>
<li>æ¨¡å‹è®­ç»ƒåŒ…æ‹¬å››ä¸ªé˜¶æ®µï¼šè§†è§‰é¢„è®­ç»ƒã€ä¸€èˆ¬ç›‘ç£å¾®è°ƒã€ç‰©ç†AIç›‘ç£ã€ç‰©ç†AIå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>è¯„ä¼°æ¨¡å‹æ—¶ï¼Œå»ºç«‹äº†åŸºäºæœ¬ä½“è®ºçš„ç‰©ç†å¸¸è¯†å’Œä½“ç°æ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5180c62fc77f538fc50442309d0ced5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856bc8687ff4a4cacb12a7ffe717b60f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec9f52c985fc47aa47580339277604c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-731456c5726227d06f6f44bf8457ddbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99659305b782ad2bbf56ee38fd36b36c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Post-Training-Quantization-for-Diffusion-Transformer-via-Hierarchical-Timestep-Grouping"><a href="#Post-Training-Quantization-for-Diffusion-Transformer-via-Hierarchical-Timestep-Grouping" class="headerlink" title="Post-Training Quantization for Diffusion Transformer via Hierarchical   Timestep Grouping"></a>Post-Training Quantization for Diffusion Transformer via Hierarchical   Timestep Grouping</h2><p><strong>Authors:Ning Ding, Jing Han, Yuchuan Tian, Chao Xu, Kai Han, Yehui Tang</strong></p>
<p>Diffusion Transformer (DiT) has now become the preferred choice for building image generation models due to its great generation capability. Unlike previous convolution-based UNet models, DiT is purely composed of a stack of transformer blocks, which renders DiT excellent in scalability like large language models. However, the growing model size and multi-step sampling paradigm bring about considerable pressure on deployment and inference. In this work, we propose a post-training quantization framework tailored for Diffusion Transforms to tackle these challenges. We firstly locate that the quantization difficulty of DiT mainly originates from the time-dependent channel-specific outliers. We propose a timestep-aware shift-and-scale strategy to smooth the activation distribution to reduce the quantization error. Secondly, based on the observation that activations of adjacent timesteps have similar distributions, we utilize a hierarchical clustering scheme to divide the denoising timesteps into multiple groups. We further design a re-parameterization scheme which absorbs the quantization parameters into nearby module to avoid redundant computations. Comprehensive experiments demonstrate that out PTQ method successfully quantize the Diffusion Transformer into 8-bit weight and 8-bit activation (W8A8) with state-of-the-art FiD score. And our method can further quantize DiT model into 4-bit weight and 8-bit activation (W4A8) without sacrificing generation quality. </p>
<blockquote>
<p>æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰ç”±äºå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç°å·²æˆä¸ºæ„å»ºå›¾åƒç”Ÿæˆæ¨¡å‹çš„é¦–é€‰ã€‚ä¸ä¹‹å‰çš„åŸºäºå·ç§¯çš„UNetæ¨¡å‹ä¸åŒï¼ŒDiTå®Œå…¨ç”±ä¸€å †è½¬æ¢å™¨å—ç»„æˆï¼Œè¿™ä½¿å¾—DiTåœ¨å¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°±åƒå¤§å‹è¯­è¨€æ¨¡å‹ä¸€æ ·ã€‚ç„¶è€Œï¼Œä¸æ–­å¢é•¿çš„æ¨¡å‹å¤§å°å’Œå¤šæ­¥é‡‡æ ·èŒƒå¼ç»™éƒ¨ç½²å’Œæ¨ç†å¸¦æ¥äº†å·¨å¤§çš„å‹åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ‰©æ•£è½¬æ¢æå‡ºäº†ä¸€ç§åè®­ç»ƒé‡åŒ–æ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–å…ˆå‘ç°DiTçš„é‡åŒ–éš¾åº¦ä¸»è¦æºäºæ—¶é—´ä¾èµ–çš„ç‰¹å®šé€šé“å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´æ„ŸçŸ¥ç§»ä½å’Œç¼©æ”¾ç­–ç•¥ï¼Œä»¥å¹³æ»‘æ¿€æ´»åˆ†å¸ƒï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®ã€‚å…¶æ¬¡ï¼ŒåŸºäºç›¸é‚»æ—¶é—´æ­¥çš„æ¿€æ´»å…·æœ‰ç›¸ä¼¼åˆ†å¸ƒçš„è§‚æµ‹ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†å±‚èšç±»æ–¹æ¡ˆå°†å»å™ªæ—¶é—´æ­¥åˆ†ä¸ºå¤šä¸ªç»„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§é‡æ–°å‚æ•°åŒ–æ–¹æ¡ˆï¼Œå°†é‡åŒ–å‚æ•°å¸æ”¶åˆ°é™„è¿‘çš„æ¨¡å—ä¸­ï¼Œä»¥é¿å…å†—ä½™è®¡ç®—ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PTQæ–¹æ³•æˆåŠŸåœ°å°†æ‰©æ•£è½¬æ¢å™¨é‡åŒ–åˆ°8ä½æƒé‡å’Œ8ä½æ¿€æ´»ï¼ˆW8A8ï¼‰ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„FIDåˆ†æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥å°†DiTæ¨¡å‹è¿›ä¸€æ­¥é‡åŒ–åˆ°4ä½æƒé‡å’Œ8ä½æ¿€æ´»ï¼ˆW4A8ï¼‰ï¼Œè€Œä¸ä¼šç‰ºç‰²ç”Ÿæˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06930v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„å®šåˆ¶åè®­ç»ƒé‡åŒ–æ¡†æ¶ï¼Œè§£å†³äº†æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†ä¸­çš„å‹åŠ›é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼ŒDiTæ¨¡å‹çš„é‡åŒ–éš¾åº¦ä¸»è¦æ¥æºäºæ—¶åºç‰¹å®šçš„é€šé“å¼‚å¸¸å€¼ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ—¶åºæ„ŸçŸ¥ç§»ä½ç¼©æ”¾ç­–ç•¥æ¥å¹³æ»‘æ¿€æ´»åˆ†å¸ƒä»¥é™ä½é‡åŒ–è¯¯å·®ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ç›¸é‚»æ—¶åºæ¿€æ´»åˆ†å¸ƒçš„ç›¸ä¼¼æ€§è¿›è¡Œå±‚æ¬¡èšç±»åˆ†ç»„ï¼Œè®¾è®¡äº†å‚æ•°åŒ–æ–¹æ¡ˆä»¥é¿å…å†—ä½™è®¡ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥é‡åŒ–æ–¹æ³•å¯å°†æ‰©æ•£å˜æ¢å™¨æ¨¡å‹æˆåŠŸé‡åŒ–ä¸ºæƒé‡å’Œæ¿€æ´»å‡ä¸º8ä½çš„æ¨¡å‹ï¼ˆW8A8ï¼‰ï¼Œä¸”FIDå¾—åˆ†é¢†å…ˆã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å°†DiTæ¨¡å‹è¿›ä¸€æ­¥é‡åŒ–ä¸ºæƒé‡ä¸º4ä½ã€æ¿€æ´»ä¸º8ä½çš„æ¨¡å‹ï¼ˆW4A8ï¼‰ï¼Œä¸”ä¸å½±å“ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformer (DiT)å·²æˆä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¼˜é€‰æ–¹æ¡ˆï¼Œå…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å¾—åˆ°å¹¿æ³›è®¤å¯ã€‚</li>
<li>DiTé¢ä¸´æ¨¡å‹è§„æ¨¡æ‰©å¤§å’Œå¤šæ­¥é‡‡æ ·å¸¦æ¥çš„éƒ¨ç½²å’Œæ¨ç†å‹åŠ›ã€‚</li>
<li>é’ˆå¯¹DiTçš„é‡åŒ–éš¾åº¦ï¼Œç ”ç©¶å‘ç°ä¸»è¦æºäºæ—¶åºç‰¹å®šçš„é€šé“å¼‚å¸¸å€¼ã€‚</li>
<li>æå‡ºæ—¶åºæ„ŸçŸ¥ç§»ä½ç¼©æ”¾ç­–ç•¥ä»¥å¹³æ»‘æ¿€æ´»åˆ†å¸ƒï¼Œé™ä½é‡åŒ–è¯¯å·®ã€‚</li>
<li>åˆ©ç”¨ç›¸é‚»æ—¶åºæ¿€æ´»åˆ†å¸ƒçš„ç›¸ä¼¼æ€§è¿›è¡Œå±‚æ¬¡èšç±»åˆ†ç»„ã€‚</li>
<li>è®¾è®¡äº†é¿å…å†—ä½™è®¡ç®—çš„å‚æ•°åŒ–æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35c1af07217c138c342caba80cf219d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30473515ddbdf7544074ddf20d791574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ded74726a85b45af236fb39b7e3562b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc52c37b37e796b0e2deafe36b380c57.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Forgetting-Transformer-Softmax-Attention-with-a-Forget-Gate"><a href="#Forgetting-Transformer-Softmax-Attention-with-a-Forget-Gate" class="headerlink" title="Forgetting Transformer: Softmax Attention with a Forget Gate"></a>Forgetting Transformer: Softmax Attention with a Forget Gate</h2><p><strong>Authors:Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville</strong></p>
<p>An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformerâ€™s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a â€œProâ€ block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer">https://github.com/zhixuan-lin/forgetting-transformer</a>. </p>
<blockquote>
<p>ç°ä»£å¾ªç¯åºåˆ—æ¨¡å‹çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†æ˜¯é—å¿˜é—¨ã€‚è™½ç„¶Transformeræ²¡æœ‰æ˜ç¡®çš„é€’å½’å½¢å¼ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§é€šè¿‡æ•°æ®ç›¸å…³çš„æ–¹å¼é™ä½æœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›åˆ†æ•°ï¼Œè‡ªç„¶åœ°å°†é—å¿˜é—¨çº³å…¥Transformerçš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†è¿™ç§æ³¨æ„åŠ›æœºåˆ¶å‘½åä¸ºé—å¿˜æ³¨æ„åŠ›ï¼Œå¹¶å°†å¾—åˆ°çš„æ¨¡å‹å‘½åä¸ºé—å¿˜è½¬æ¢å™¨ï¼ˆFoXï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºFoXåœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ã€é•¿åº¦æ‰©å±•å’ŒçŸ­ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºTransformerï¼ŒåŒæ—¶åœ¨é•¿ä¸Šä¸‹æ–‡ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸Transformerç›¸å½“ã€‚æ­¤å¤–ï¼Œå®ƒä¸FlashAttentionç®—æ³•å…¼å®¹ï¼Œä¸éœ€è¦ä»»ä½•ä½ç½®åµŒå…¥ã€‚åŒ…æ‹¬â€œæµ·åº•æé’ˆâ€æµ‹è¯•åœ¨å†…çš„å‡ é¡¹åˆ†æè¡¨æ˜ï¼ŒFoXåœ¨ä¿æŒTransformeråœ¨é•¿ä¸Šä¸‹æ–‡æ–¹é¢çš„ä¼˜åŠ¿çš„åŒæ—¶ï¼Œä¹Ÿä¼˜äºå¾ªç¯åºåˆ—æ¨¡å‹å¦‚Mamba-2ã€HGRN2å’ŒDeltaNetç­‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§â€œä¸“ä¸šâ€å—è®¾è®¡ï¼Œå®ƒç»“åˆäº†å¾ªç¯åºåˆ—æ¨¡å‹ä¸­çš„ä¸€äº›å¸¸è§æ¶æ„ç»„ä»¶ï¼Œå‘ç°å®ƒèƒ½æ˜¾è‘—æé«˜FoXå’ŒTransformerçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer%E3%80%82">https://github.com/zhixuan-lin/forgetting-transformerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02130v2">PDF</a> Published as a conference paper at ICLR 2025; Fixed an issue with the   attention map visualization</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨Transformeræ¨¡å‹ä¸­å¼•å…¥é—å¿˜é—¨ï¼ˆForget Gateï¼‰çš„æœºåˆ¶ï¼Œç§°ä¸ºé—å¿˜æ³¨æ„åŠ›ï¼ˆForgetting Attentionï¼‰ï¼Œå¹¶ç”±æ­¤æå‡ºäº†é—å¿˜å˜å‹å™¨ï¼ˆFoXï¼‰æ¨¡å‹ã€‚FoXæ¨¡å‹åœ¨é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡ã€é•¿åº¦æ‰©å±•å’ŒçŸ­æ–‡æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¼˜äºTransformerï¼ŒåŒæ—¶åœ¨é•¿æ–‡æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸Transformerç›¸å½“ã€‚æ­¤å¤–ï¼ŒFoXä¸FlashAttentionç®—æ³•å…¼å®¹ï¼Œæ— éœ€ä½ç½®åµŒå…¥ã€‚åˆ†æå’Œæµ‹è¯•æ˜¾ç¤ºï¼ŒFoXä¿ç•™äº†Transformeråœ¨é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†æŸäº›é€’å½’åºåˆ—æ¨¡å‹ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§ç»“åˆé€’å½’åºåˆ—æ¨¡å‹å¸¸è§æ¶æ„ç»„ä»¶çš„â€œProâ€å—è®¾è®¡ï¼Œæ˜¾è‘—æé«˜äº†FoXå’ŒTransformerçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é—å¿˜é—¨æœºåˆ¶è¢«è‡ªç„¶åœ°èå…¥åˆ°Transformeræ¨¡å‹ä¸­ï¼Œé€šè¿‡æ•°æ®ç›¸å…³çš„æ–¹å¼å¯¹æœªæ ‡å‡†åŒ–çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œä¸‹æƒé‡å¤„ç†ã€‚</li>
<li>æå‡ºçš„FoXæ¨¡å‹åœ¨é•¿æ–‡æœ¬è¯­è¨€å»ºæ¨¡ã€é•¿åº¦æ‰©å±•å’ŒçŸ­æ–‡æœ¬ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºTransformerçš„æ€§èƒ½ã€‚</li>
<li>FoXæ¨¡å‹ä¸FlashAttentionç®—æ³•å…¼å®¹ï¼Œä¸”ä¸éœ€è¦ä½¿ç”¨ä½ç½®åµŒå…¥ã€‚</li>
<li>åˆ†æè¡¨æ˜ï¼ŒFoXæ¨¡å‹ä¿ç•™äº†Transformeråœ¨é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>ä¸æŸäº›é€’å½’åºåˆ—æ¨¡å‹ç›¸æ¯”ï¼ŒFoXæ¨¡å‹è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>å¼•å…¥çš„â€œProâ€å—è®¾è®¡ç»“åˆäº†é€’å½’åºåˆ—æ¨¡å‹çš„å¸¸è§æ¶æ„ç»„ä»¶ï¼Œæ˜¾è‘—æé«˜äº†FoXå’ŒTransformerçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cab32a20f130fbe4b5184bdf16a322e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66cdcb70392f3a49eba2eef23bd0e870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc12c8258b772edb4bb7df66467166af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e012daee8b9ff118bcc6a15e73ebb43a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Code-Generation-A-Comprehensive-Survey-of-Challenges-Techniques-Evaluation-and-Applications"><a href="#Large-Language-Models-for-Code-Generation-A-Comprehensive-Survey-of-Challenges-Techniques-Evaluation-and-Applications" class="headerlink" title="Large Language Models for Code Generation: A Comprehensive Survey of   Challenges, Techniques, Evaluation, and Applications"></a>Large Language Models for Code Generation: A Comprehensive Survey of   Challenges, Techniques, Evaluation, and Applications</h2><p><strong>Authors:Nam Huynh, Beiyu Lin</strong></p>
<p>Large Language Models (LLMs) have demonstrated their remarkable capabilities in numerous fields. This survey focuses on how LLMs empower users, regardless of their technical background, to use human languages to automatically generate executable code. We begin with understanding LLMsâ€™ limitations and challenges in automated code generation. Subsequently, we review various fine-tuning techniques designed to enhance both the performance and adaptability of LLMs in code generation tasks. We then review the existing metrics and benchmarks for evaluations to assess model performance based on fine-tuning techniques. Finally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot, ToolGen) in code generation tasks to illustrate their roles and functionalities. This survey provides a comprehensive overview of LLMs for code generation, helps researchers in diverse fields better understand the current state-of-the-art technologies, and offers the potential of effectively leveraging LLMs for code generation tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤šä¸ªé¢†åŸŸå±•ç¤ºäº†å…¶å“è¶Šçš„èƒ½åŠ›ã€‚è¿™ç¯‡ç»¼è¿°çš„é‡ç‚¹æ˜¯ï¼Œæ— è®ºç”¨æˆ·çš„æŠ€æœ¯èƒŒæ™¯å¦‚ä½•ï¼ŒLLMå¦‚ä½•èµ‹èƒ½ç”¨æˆ·ä½¿ç”¨è‡ªç„¶è¯­è¨€è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œä»£ç ã€‚æˆ‘ä»¬é¦–å…ˆäº†è§£LLMåœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„å±€é™æ€§å’ŒæŒ‘æˆ˜ã€‚éšåï¼Œæˆ‘ä»¬å›é¡¾äº†å„ç§å¾®è°ƒæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜LLMåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚ç„¶åæˆ‘ä»¬å›é¡¾äº†åŸºäºå¾®è°ƒæŠ€æœ¯çš„è¯„ä¼°ç°æœ‰æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†LLMï¼ˆä¾‹å¦‚CodeLlamaã€GitHub Copilotã€ToolGenï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä»¥è¯´æ˜å®ƒä»¬çš„ä½œç”¨å’ŒåŠŸèƒ½ã€‚è¿™ç¯‡ç»¼è¿°ä¸ºä»£ç ç”Ÿæˆé¢†åŸŸçš„LLMæä¾›äº†å…¨é¢çš„æ¦‚è¿°ï¼Œæœ‰åŠ©äºä¸åŒé¢†åŸŸçš„ç ”ç©¶è€…æ›´å¥½åœ°äº†è§£å½“å‰æœ€æ–°æŠ€æœ¯ï¼Œå¹¶æä¾›äº†æœ‰æ•ˆåˆ©ç”¨LLMè¿›è¡Œä»£ç ç”Ÿæˆä»»åŠ¡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01245v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°äº†å“è¶Šçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ‹èƒ½ç”¨æˆ·åˆ©ç”¨è‡ªç„¶è¯­è¨€è‡ªåŠ¨ç”Ÿæˆå¯æ‰§è¡Œä»£ç æ–¹é¢ã€‚æœ¬ç»¼è¿°å…ˆæ¢è®¨LLMsåœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢çš„å±€é™å’ŒæŒ‘æˆ˜ï¼Œå†è¯„è¿°å„ç§å¢å¼ºæ¨¡å‹æ€§èƒ½å’Œé€‚åº”æ€§çš„å¾®è°ƒæŠ€æœ¯ï¼Œå¹¶è¯„ä»·åŸºäºè¿™äº›æŠ€æœ¯çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œå±•ç¤ºLLMsåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨å®ä¾‹ï¼ˆå¦‚CodeLlamaã€GitHub Copilotã€ToolGenç­‰ï¼‰ï¼Œå¸®åŠ©ä¸åŒé¢†åŸŸçš„ç ”ç©¶è€…äº†è§£æœ€æ–°æŠ€æœ¯ï¼Œå¹¶ä¸ºæœ‰æ•ˆåˆ©ç”¨LLMsè¿›è¡Œä»£ç ç”Ÿæˆä»»åŠ¡æä¾›æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”¨æˆ·åˆ©ç”¨è‡ªç„¶è¯­è¨€è‡ªåŠ¨ç”Ÿæˆä»£ç æ–¹é¢ã€‚</li>
<li>LLMsåœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆæ–¹é¢å­˜åœ¨å±€é™å’ŒæŒ‘æˆ˜ï¼Œéœ€è¦ç ”ç©¶æœ‰æ•ˆçš„å¾®è°ƒæŠ€æœ¯æ¥æé«˜æ€§èƒ½å’Œé€‚åº”æ€§ã€‚</li>
<li>å­˜åœ¨å¤šç§è¯„ä¼°LLMsåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­æ€§èƒ½çš„æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>LLMsåœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨å®ä¾‹åŒ…æ‹¬CodeLlamaã€GitHub Copilotå’ŒToolGenç­‰ã€‚</li>
<li>è¿™äº›åº”ç”¨å±•ç¤ºäº†LLMsåœ¨èµ‹èƒ½ç”¨æˆ·ã€æé«˜å¼€å‘æ•ˆç‡å’Œä¿ƒè¿›è‡ªåŠ¨åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç»¼åˆè¯„è¿°LLMsåœ¨ä»£ç ç”Ÿæˆæ–¹é¢çš„åº”ç”¨æœ‰åŠ©äºç ”ç©¶è€…äº†è§£å½“å‰æœ€æ–°æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-599bd4eb94fa35c6535af0f741fb746d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcfa68b83c55f05263f04ec907f884de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45c381107acb9268bc201c143c4e5d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc9fd147a4d00a90c4a26ddcdbb51f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb59751a08d47be98740267103c57d83.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation"><a href="#Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation" class="headerlink" title="Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?"></a>Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?</h2><p><strong>Authors:Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park</strong></p>
<p>Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in usersâ€™ item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMsâ€™ ability to understand usersâ€™ item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec">https://github.com/Sein-Kim/LLM-SRec</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºå…¶å…ˆè¿›çš„æ–‡æœ¬ç†è§£èƒ½åŠ›å’Œä¸Šä¸‹æ–‡æ„è¯†ï¼Œæœ€è¿‘è¢«å…¬è®¤ä¸ºæ¨èå·¥å…·ä¸­çš„æœ‰å‰é€”çš„å·¥å…·ã€‚å°½ç®¡å½“å‰åœ¨é¡ºåºæ¨èåœºæ™¯ä¸‹è®­ç»ƒå’Œè¯„ä¼°åŸºäºLLMçš„æ¨èï¼ˆLLM4Recï¼‰æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹æ˜¯å¦ç†è§£ç”¨æˆ·é¡¹ç›®äº¤äº’åºåˆ—ä¸­å›ºæœ‰çš„é¡ºåºä¿¡æ¯å´è¢«å¤§å¤§å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ä¸€ç³»åˆ—å®éªŒè¯æ˜ï¼Œç°æœ‰çš„LLM4Recæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½æ²¡æœ‰å®Œå…¨æ•è·é¡ºåºä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºLLMçš„é¡ºåºæ¨èå™¨ï¼Œç§°ä¸ºLLM-SRecã€‚è¿™æ˜¯ä¸€ç§é€šè¿‡è’¸é¦ä»é¢„è®­ç»ƒçš„CF-SRecæ¨¡å‹ä¸­æå–çš„ç”¨æˆ·è¡¨ç¤ºåˆ°LLMä¸­ï¼Œå¢å¼ºé¡ºåºä¿¡æ¯èå…¥LLMçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLLM-SRecå¢å¼ºäº†LLMç†è§£ç”¨æˆ·é¡¹ç›®äº¤äº’åºåˆ—çš„èƒ½åŠ›ï¼Œæœ€ç»ˆæé«˜äº†æ¨èæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰çš„éœ€è¦å¾®è°ƒLLMçš„LLM4Recæ¨¡å‹ä¸åŒï¼ŒLLM-SRecä»…é€šè¿‡è®­ç»ƒä¸€äº›è½»é‡çº§çš„MLPå°±å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec%E3%80%82">https://github.com/Sein-Kim/LLM-SRecã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13909v3">PDF</a> </p>
<p><strong>Summary</strong><br>LLMsç”¨äºæ¨èç³»ç»Ÿå…·æœ‰å…ˆè¿›æ–‡æœ¬ç†è§£å’Œè¯­å¢ƒæ„ŸçŸ¥èƒ½åŠ›ï¼Œä½†ç°æœ‰LLM4Recæ¨¡å‹å¿½ç•¥äº†å¯¹ç”¨æˆ·ç‰©å“äº¤äº’åºåˆ—ä¸­çš„é¡ºåºä¿¡æ¯çš„ç†è§£ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºLLMçš„é¡ºåºæ¨èå™¨LLM-SRecï¼Œé€šè¿‡è’¸é¦é¢„è®­ç»ƒCF-SRecæ¨¡å‹ä¸­çš„ç”¨æˆ·è¡¨ç¤ºåˆ°LLMsä¸­ï¼Œå¢å¼ºäº†å¯¹é¡ºåºä¿¡æ¯çš„æ•´åˆèƒ½åŠ›ï¼Œæé«˜æ¨èæ€§èƒ½ï¼Œä¸”åªéœ€è®­ç»ƒå°‘é‡è½»é‡çº§MLPsï¼Œå®ç”¨æ€§è¾ƒé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æ¨èç³»ç»Ÿä¸­å±•ç°å‡ºå¼ºå¤§çš„æ–‡æœ¬ç†è§£å’Œè¯­å¢ƒæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰LLM4Recæ¨¡å‹å¿½ç•¥äº†ç”¨æˆ·ç‰©å“äº¤äº’åºåˆ—ä¸­çš„é¡ºåºä¿¡æ¯ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å®éªŒè¯æ˜ç°æœ‰LLM4Recæ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­æœªèƒ½å……åˆ†æ•æ‰é¡ºåºä¿¡æ¯ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºLLMçš„é¡ºåºæ¨èå™¨LLM-SRecï¼Œèƒ½æœ‰æ•ˆæ•´åˆé¡ºåºä¿¡æ¯ã€‚</li>
<li>LLM-SRecé€šè¿‡è’¸é¦é¢„è®­ç»ƒCF-SRecæ¨¡å‹ä¸­çš„ç”¨æˆ·è¡¨ç¤ºåˆ°LLMsä¸­ï¼Œå¢å¼ºäº†ç†è§£ç”¨æˆ·ç‰©å“äº¤äº’åºåˆ—çš„èƒ½åŠ›ã€‚</li>
<li>LLM-SRecæé«˜äº†æ¨èæ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>LLM-SRecåªéœ€è®­ç»ƒå°‘é‡è½»é‡çº§MLPsï¼Œå…·æœ‰å®ç”¨æ€§ï¼Œé€‚ç”¨äºçœŸå®ä¸–ç•Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4fb78631dc36d262d46bbb6a52d0db77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62a650a40c6af80db5555293e00112d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1307b97ddd357ce63767b177741ceb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac00f70619628c5d1e3f8b7c93cccaf6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis"><a href="#Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis" class="headerlink" title="Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis"></a>Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis</h2><p><strong>Authors:Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, Ning Wang, Meijuan An, Qingliang Meng, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian</strong></p>
<p>DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations for DeepSeek Series models from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we conduct a systematic evaluation of the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, their corresponding 4-bit quantized models, and the reasoning model QwQ-32B using the enhanced A-Eval benchmark, A-Eval-2.0. Through a comparative analysis of original instruction-tuned models and their distilled counterparts, we investigate how reasoning enhancements impact performance across diverse practical tasks. To assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications. Based on the quantification results, we develop a model selection handbook that clearly illustrates the relation among models, their capabilities and practical applications. This handbook enables users to select the most cost-effective models without efforts, ensuring optimal performance and resource efficiency in real-world applications. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmarks and periodically update this paper to provide more comprehensive and accurate evaluation results. Please refer to the latest version of the paper for the most current results and conclusions. </p>
<blockquote>
<p>DeepSeek-R1ä»¥å…¶ä½è®­ç»ƒæˆæœ¬å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›è€Œè‘—ç§°ï¼Œå·²åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œé’ˆå¯¹DeepSeekç³»åˆ—æ¨¡å‹ä»ç°å®åº”ç”¨è§’åº¦çš„è¯¦ç»†è¯„ä¼°ä»ç„¶ç¼ºä¹ï¼Œè¿™ä½¿å¾—ç”¨æˆ·éš¾ä»¥ä¸ºå…¶ç‰¹å®šéœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹DeepSeek-V3ã€DeepSeek-R1ã€DeepSeek-R1-Distill-Qwenç³»åˆ—ã€DeepSeek-R1-Distill-Llamaç³»åˆ—ã€å…¶å¯¹åº”çš„4ä½é‡åŒ–æ¨¡å‹ä»¥åŠæ¨ç†æ¨¡å‹QwQ-32Bè¿›è¡Œäº†ç³»ç»Ÿçš„è¯„ä¼°ï¼Œä½¿ç”¨çš„æ˜¯å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ï¼Œå³A-Eval-2.0ã€‚é€šè¿‡å¯¹åŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸å…¶è’¸é¦å¯¹åº”ç‰©çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ¨ç†å¢å¼ºå¦‚ä½•å½±å“å„ç§å®é™…ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†ååŠ©ç”¨æˆ·è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œæˆ‘ä»¬é€šè¿‡æ€§èƒ½åˆ†çº§åˆ†ç±»é‡åŒ–äº†DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚åŸºäºé‡åŒ–ç»“æœï¼Œæˆ‘ä»¬å¼€å‘äº†æ¨¡å‹é€‰æ‹©æ‰‹å†Œï¼Œè¯¥æ‰‹å†Œæ¸…æ™°åœ°è¯´æ˜äº†å„æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€å…¶èƒ½åŠ›å’Œå®é™…åº”ç”¨ã€‚è¯¥æ‰‹å†Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ï¼Œç¡®ä¿åœ¨çœŸå®åº”ç”¨ä¸­çš„æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬åŠªåŠ›å»ºç«‹å…¨é¢ã€å®¢è§‚ã€æƒå¨çš„è¯„ä»·åŸºå‡†ï¼Œä½†æµ‹è¯•æ ·æœ¬çš„é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒçš„ç‰¹å¾ä»¥åŠè¯„ä»·æ ‡å‡†çš„è®¾å®šä¸å¯é¿å…åœ°ä¼šç»™è¯„ä»·ç»“æœå¸¦æ¥ä¸€å®šçš„åå·®ã€‚æˆ‘ä»¬å°†ä¸æ–­ä¼˜åŒ–è¯„ä¼°åŸºå‡†ï¼Œå¹¶å®šæœŸæ›´æ–°æœ¬æ–‡ï¼Œä»¥æä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚æœ€æ–°çš„ç»“æœå’Œç»“è®ºè¯·å‚è€ƒæœ¬æ–‡çš„æœ€æ–°ç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11164v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>DeepSeek-R1æ¨¡å‹ä»¥å…¶ä½è®­ç»ƒæˆæœ¬ä¸å‡ºè‰²çš„æ¨ç†èƒ½åŠ›è€ŒçŸ¥åï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚ç„¶è€Œï¼Œå…³äºDeepSeekç³»åˆ—æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„è¯¦ç»†è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œä½¿å¾—ç”¨æˆ·éš¾ä»¥é€‰æ‹©æœ€é€‚åˆè‡ªèº«éœ€æ±‚çš„DeepSeekæ¨¡å‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹DeepSeek-V3ã€DeepSeek-R1ã€DeepSeek-R1-Distill-Qwenç³»åˆ—ã€DeepSeek-R1-Distill-Llamaç³»åˆ—ã€å…¶å¯¹åº”çš„4ä½é‡åŒ–æ¨¡å‹ä»¥åŠæ¨ç†æ¨¡å‹QwQ-32Bè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„è¯„ä¼°ï¼Œé‡‡ç”¨äº†å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ï¼Œå³A-Eval-2.0ã€‚æˆ‘ä»¬æ¯”è¾ƒåˆ†æäº†åŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸å…¶è’¸é¦å¯¹åº”ç‰©ï¼Œæ¢è®¨æ¨ç†å¢å¼ºå¦‚ä½•å½±å“å„ç§å®é™…ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ºäº†å¸®åŠ©ç”¨æˆ·é€‰æ‹©æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡æ€§èƒ½åˆ†çº§é‡åŒ–äº†DeepSeekæ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶æ®æ­¤å¼€å‘äº†æ¨¡å‹é€‰æ‹©æ‰‹å†Œï¼Œæ¸…æ™°åœ°è¯´æ˜äº†å„æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€å…¶èƒ½åŠ›å’Œå®é™…åº”ç”¨ã€‚è¯¥æ‰‹å†Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·è½»æ¾é€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹ï¼Œç¡®ä¿åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å®ç°æœ€ä½³æ€§èƒ½å’Œèµ„æºæ•ˆç‡ã€‚éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œå°½ç®¡æˆ‘ä»¬åŠªåŠ›å»ºç«‹å…¨é¢ã€å®¢è§‚ã€æƒå¨çš„è¯„ä¼°åŸºå‡†ï¼Œä½†æµ‹è¯•æ ·æœ¬çš„é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒçš„ç‰¹å¾ä»¥åŠè¯„ä¼°æ ‡å‡†çš„è®¾å®šå¯èƒ½ä¼šç»™è¯„ä¼°ç»“æœå¸¦æ¥ä¸€å®šçš„åå·®ã€‚æˆ‘ä»¬å°†æŒç»­ä¼˜åŒ–è¯„ä¼°åŸºå‡†å¹¶å®šæœŸæ›´æ–°æœ¬æ–‡ï¼Œä»¥æä¾›æ›´å…¨é¢ã€å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚æœ€æ–°ç»“æœå’Œç»“è®ºè¯·å‚é˜…æœ¬æ–‡æœ€æ–°ç‰ˆæœ¬ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DeepSeek-R1æ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä½†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚</li>
<li>é€šè¿‡å¯¹DeepSeekç³»åˆ—æ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œé‡‡ç”¨äº†å¢å¼ºçš„A-EvalåŸºå‡†æµ‹è¯•ï¼Œå³A-Eval-2.0ã€‚</li>
<li>æ¯”è¾ƒåˆ†æäº†åŸå§‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸è’¸é¦æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>é€šè¿‡æ€§èƒ½åˆ†çº§é‡åŒ–æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼Œä¸ºç”¨æˆ·æä¾›æ›´æ¸…æ™°çš„æ¨¡å‹é€‰æ‹©ä¾æ®ã€‚</li>
<li>å¼€å‘äº†æ¨¡å‹é€‰æ‹©æ‰‹å†Œï¼Œå¸®åŠ©ç”¨æˆ·è½»æ¾é€‰æ‹©æœ€é€‚åˆå…¶éœ€æ±‚çš„æˆæœ¬æ•ˆç›Šé«˜çš„æ¨¡å‹ã€‚</li>
<li>è¯„ä¼°è¿‡ç¨‹ä¸­å­˜åœ¨çš„åå·®ä¸»è¦æºäºæµ‹è¯•æ ·æœ¬é€‰æ‹©ã€æ•°æ®åˆ†å¸ƒç‰¹å¾ä»¥åŠè¯„ä¼°æ ‡å‡†è®¾å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c777be450e91965811e9692983405e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-610ca94921a22b69af95da15ac53b01e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac7f7a774c0bcd5382ddb0ddce07ac34.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation"><a href="#TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation" class="headerlink" title="TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"></a>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</h2><p><strong>Authors:Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li</strong></p>
<p>In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each major task further contains three subtasks, while each subtask comprises 5,000 test samples. Given the inherent complexity of open molecule generation evaluation, we also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations as well as potential areas for improvement in text-guided molecule discovery. Furthermore, we propose OpenMolIns, a specialized instruction tuning dataset established for solving challenges raised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5% on TOMG-Bench. Our codes and datasets are available through <a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench">https://github.com/phenixace/TOMG-Bench</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¼€æ”¾åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆTOMG-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾åŸŸåˆ†å­ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚TOMG-BenchåŒ…å«ä¸‰å¤§ä»»åŠ¡çš„æ•°æ®é›†ï¼šåˆ†å­ç¼–è¾‘ï¼ˆMolEditï¼‰ã€åˆ†å­ä¼˜åŒ–ï¼ˆMolOptï¼‰å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆï¼ˆMolCustomï¼‰ã€‚æ¯ä¸ªä¸»è¦ä»»åŠ¡è¿˜åŒ…æ‹¬ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡åŒ…å«5000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚è€ƒè™‘åˆ°å¼€æ”¾åˆ†å­ç”Ÿæˆè¯„ä»·çš„å›ºæœ‰å¤æ‚æ€§ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°ç³»ç»Ÿï¼Œå¸®åŠ©è¡¡é‡ç”Ÿæˆåˆ†å­çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¯¹25ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å…¨é¢åŸºå‡†æµ‹è¯•æ­ç¤ºäº†å½“å‰æ–‡æœ¬æŒ‡å¯¼åˆ†å­å‘ç°æ–¹é¢çš„å±€é™æ€§ä»¥åŠæ½œåœ¨çš„æ”¹è¿›é¢†åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†OpenMolInsï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè§£å†³TOMG-Benchæå‡ºçš„æŒ‘æˆ˜è€Œå»ºç«‹çš„ä¸“ä¸šæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚åœ¨OpenMolInsä¸Šå¾®è°ƒçš„Llama3.1-8Bèƒ½å¤Ÿè¶…è¶Šæ‰€æœ‰å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨TOMG-Benchä¸Šçš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†GPT-3.5 Turboçš„46.5%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/phenixace/TOMG-Benchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14642v2">PDF</a> The first benchmark for text-based open molecule generation</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¼€æ”¾åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆTOMG-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼€æ”¾åŸŸåˆ†å­ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚TOMG-BenchåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼šåˆ†å­ç¼–è¾‘ï¼ˆMolEditï¼‰ã€åˆ†å­ä¼˜åŒ–ï¼ˆMolOptï¼‰å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆï¼ˆMolCustomï¼‰ã€‚æ¯ä¸ªä¸»è¦ä»»åŠ¡åŒ…å«ä¸‰ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªå­ä»»åŠ¡åŒ…å«5000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚ä¸ºè¯„ä¼°ç”Ÿæˆçš„åˆ†å­è´¨é‡å’Œå‡†ç¡®æ€§ï¼Œè¿˜å¼€å‘äº†è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿã€‚å¯¹25ä¸ªLLMsçš„å…¨é¢åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ–‡æœ¬å¼•å¯¼åˆ†å­å‘ç°é¢†åŸŸçš„å½“å‰å±€é™æ€§å’Œæ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚æ­¤å¤–ï¼Œä¸ºåº”å¯¹TOMG-Benchæå‡ºçš„æŒ‘æˆ˜ï¼Œè¿˜æ¨å‡ºäº†OpenMolInsä¸“ç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚åœ¨OpenMolInsä¸Šç»è¿‡å¾®è°ƒçš„Llama3.1-8Bæ€§èƒ½è¶…è¶Šæ‰€æœ‰å¼€æºé€šç”¨LLMsï¼Œåœ¨TOMG-Benchä¸Šçš„è¡¨ç°æ¯”GPT-3.5-turboé«˜å‡º46.5%ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/phenixace/TOMG-Benchè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æå‡ºäº†åŸºäºæ–‡æœ¬çš„å¼€æ”¾åˆ†å­ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆTOMG-Benchï¼‰ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨å¼€æ”¾åŸŸåˆ†å­ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>TOMG-BenchåŒ…å«ä¸‰ä¸ªä¸»è¦ä»»åŠ¡ï¼šåˆ†å­ç¼–è¾‘ã€åˆ†å­ä¼˜åŒ–å’Œå®šåˆ¶åˆ†å­ç”Ÿæˆã€‚</li>
<li>å¼€å‘äº†è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿï¼Œä»¥è¡¡é‡ç”Ÿæˆåˆ†å­çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¯¹25ä¸ªLLMsçš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†æ–‡æœ¬å¼•å¯¼åˆ†å­å‘ç°é¢†åŸŸçš„å±€é™æ€§å’Œæ½œåœ¨æ”¹è¿›æ–¹å‘ã€‚</li>
<li>æ¨å‡ºäº†OpenMolInsä¸“ç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œä»¥åº”å¯¹TOMG-Benchçš„æŒ‘æˆ˜ã€‚</li>
<li>Llama3.1-8Båœ¨OpenMolInsä¸Šç»è¿‡å¾®è°ƒåï¼Œæ€§èƒ½è¶…è¶Šå…¶ä»–LLMsï¼Œåœ¨TOMG-Benchä¸Šçš„è¡¨ç°æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8130a767c60f10a723bccdc370f1acf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8257366b714dcbec0abe5e65531e38b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b237bef2e1a2b0be7e8e9e171c9b14a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a02607bd2028b26e161245a43483d6c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VERA-Explainable-Video-Anomaly-Detection-via-Verbalized-Learning-of-Vision-Language-Models"><a href="#VERA-Explainable-Video-Anomaly-Detection-via-Verbalized-Learning-of-Vision-Language-Models" class="headerlink" title="VERA: Explainable Video Anomaly Detection via Verbalized Learning of   Vision-Language Models"></a>VERA: Explainable Video Anomaly Detection via Verbalized Learning of   Vision-Language Models</h2><p><strong>Authors:Muchao Ye, Weiyang Liu, Pan He</strong></p>
<p>The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰å»ºç«‹äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼šåˆ©ç”¨VLMsåŒæ—¶æ£€æµ‹å¼‚å¸¸å¹¶ä¸ºå†³ç­–æä¾›å¯ç†è§£çš„è§£é‡Šã€‚ç°æœ‰å·¥ä½œå¾€å¾€å‡è®¾VADæ‰€éœ€çš„å¤æ‚æ¨ç†è¶…å‡ºäº†é¢„è®­ç»ƒVLMsçš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­èå…¥äº†ä¸“é—¨çš„æ¨ç†æ¨¡å—ï¼Œè¦ä¹ˆé€šè¿‡é¢å¤–çš„è®­ç»ƒä¾èµ–äºæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†æ¥é€‚åº”VLMsè¿›è¡ŒVADã€‚ç„¶è€Œï¼Œè¿™äº›ç­–ç•¥é€šå¸¸ä¼šå¯¼è‡´å·¨å¤§çš„è®¡ç®—æˆæœ¬æˆ–æ•°æ®æ³¨é‡Šå¼€é”€ã€‚ä¸ºäº†åº”å¯¹å¯è§£é‡ŠVADä¸­çš„è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºVERAçš„è¨€è¯­åŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒä½¿VLMsèƒ½å¤Ÿæ‰§è¡ŒVADè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼ŒVERAè‡ªåŠ¨å°†VADæ‰€éœ€çš„å¤æ‚æ¨ç†åˆ†è§£ä¸ºå¯¹æ›´ç®€å•ã€æ›´é›†ä¸­çš„å¼•å¯¼é—®é¢˜çš„åæ€ï¼Œæ•æ‰ä¸åŒçš„å¼‚å¸¸æ¨¡å¼ã€‚å®ƒå°†è¿™äº›åæ€é—®é¢˜è§†ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œé€šè¿‡å­¦ä¹ è€…ä¸ä¼˜åŒ–å™¨VLMsä¹‹é—´çš„æ•°æ®é©±åŠ¨è¨€è¯­äº¤äº’æ¥ä¼˜åŒ–å®ƒä»¬ï¼Œä½¿ç”¨ç²—ç•¥æ ‡è®°çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVERAå°†å­¦ä¹ åˆ°çš„é—®é¢˜åµŒå…¥åˆ°æ¨¡å‹æç¤ºä¸­ï¼Œä»¥æŒ‡å¯¼VLMsç”Ÿæˆåˆ†æ®µçº§åˆ«çš„å¼‚å¸¸åˆ†æ•°ï¼Œç„¶åé€šè¿‡åœºæ™¯å’Œæ—¶æ€ä¸Šä¸‹æ–‡çš„èåˆå°†è¿™äº›åˆ†æ•°ç»†åŒ–ä¸ºå¸§çº§åˆ«åˆ†æ•°ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVERAå­¦ä¹ çš„é—®é¢˜é«˜åº¦è‡ªé€‚åº”ï¼Œæ˜¾è‘—æé«˜äº†VLMsåœ¨VADæ–¹é¢çš„æ£€æµ‹æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01095v3">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰ä¸­çš„æ–°åº”ç”¨æ¨¡å¼ã€‚ä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸éœ€è¦é¢å¤–çš„æ¨ç†æ¨¡å—æˆ–æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†æ¥é€‚åº”VADéœ€æ±‚ï¼Œè¿™å¸¦æ¥äº†å¤§é‡çš„è®¡ç®—æˆæœ¬æˆ–æ•°æ®æ ‡æ³¨è´Ÿæ‹…ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVERAçš„è¨€è¯­åŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿VLMsèƒ½å¤Ÿåœ¨æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æƒ…å†µä¸‹è¿›è¡ŒVADã€‚VERAé€šè¿‡æ•°æ®é©±åŠ¨çš„è¨€è¯­äº¤äº’ï¼Œå°†å¤æ‚çš„VADæ¨ç†åˆ†è§£ä¸ºå¯¹ç®€å•æŒ‡å¯¼é—®é¢˜çš„åæ€ï¼Œè¿›è€Œä¼˜åŒ–è¿™äº›åæ€é—®é¢˜ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVERAå°†å­¦ä¹ çš„é—®é¢˜åµŒå…¥æ¨¡å‹æç¤ºä¸­ï¼ŒæŒ‡å¯¼VLMsç”Ÿæˆåˆ†æ®µçº§åˆ«çš„å¼‚å¸¸åˆ†æ•°ï¼Œå¹¶é€šè¿‡åœºæ™¯å’Œæ—¶é—´ä¸Šä¸‹æ–‡çš„èåˆï¼Œå°†å…¶ç»†åŒ–ä¸ºå¸§çº§åˆ«çš„åˆ†æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVERAå­¦ä¹ çš„é—®é¢˜å…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œæ˜¾è‘—æé«˜äº†VLMsåœ¨VADä¸­çš„æ£€æµ‹æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰ä¸­å»ºç«‹äº†æ–°çš„åº”ç”¨æ¨¡å¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¸¸å¸¸éœ€è¦é¢å¤–çš„æ¨ç†æ¨¡å—æˆ–æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œå¸¦æ¥è®¡ç®—æˆæœ¬å’Œæ•°æ®æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>VERAæ¡†æ¶ä½¿VLMsæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°å³å¯è¿›è¡ŒVADã€‚</li>
<li>VERAé€šè¿‡æ•°æ®é©±åŠ¨çš„è¨€è¯­äº¤äº’è‡ªåŠ¨åˆ†è§£å¤æ‚çš„VADæ¨ç†ï¼Œå¹¶ä¼˜åŒ–åæ˜ é—®é¢˜ã€‚</li>
<li>VERAå°†å­¦ä¹ çš„é—®é¢˜åµŒå…¥æ¨¡å‹æç¤ºä¸­ï¼ŒæŒ‡å¯¼ç”Ÿæˆåˆ†æ®µçº§åˆ«çš„å¼‚å¸¸åˆ†æ•°ã€‚</li>
<li>é€šè¿‡åœºæ™¯å’Œæ—¶é—´ä¸Šä¸‹æ–‡çš„èåˆï¼ŒVERAå°†å¼‚å¸¸åˆ†æ•°ç»†åŒ–ä¸ºå¸§çº§åˆ«çš„åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c85e22d8b5f135a92c0d541cbeb815c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908c2ac23a100ac670b04d06d85c32aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08761fc33c0544f59ac03c00a0401b20.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Human-Motion-Instruction-Tuning"><a href="#Human-Motion-Instruction-Tuning" class="headerlink" title="Human Motion Instruction Tuning"></a>Human Motion Instruction Tuning</h2><p><strong>Authors:Lei Li, Sen Jia, Jianhao Wang, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang</strong></p>
<p>This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the modelâ€™s ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction. Our code and models are available on the project website: <a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo">https://github.com/ILGLJ/LLaMo</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†LLaMoï¼ˆå¤§å‹è¯­è¨€å’Œäººç±»è¿åŠ¨åŠ©ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºäººç±»è¿åŠ¨æŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡å¼æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„å°†éè¯­è¨€è¾“å…¥ï¼ˆå¦‚è§†é¢‘æˆ–è¿åŠ¨åºåˆ—ï¼‰è½¬æ¢ä¸ºè¯­è¨€æ ‡è®°çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸åŒï¼ŒLLaMoä»¥åŸå§‹å½¢å¼ä¿ç•™è¿åŠ¨æ¥è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚è¿™ç§æ–¹æ³•ä¿ç•™äº†è¿åŠ¨ç‰¹å®šçš„ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚åœ¨æ ‡è®°åŒ–æ—¶å¾€å¾€ä¼šå‡å°‘ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è§£é‡Šå¤æ‚äººç±»è¡Œä¸ºçš„èƒ½åŠ›ã€‚é€šè¿‡åŒæ—¶å¤„ç†è§†é¢‘å’Œè¿åŠ¨æ•°æ®ä»¥åŠæ–‡æœ¬è¾“å…¥ï¼ŒLLaMoå®ç°äº†ä»¥äººä¸ºä¸­å¿ƒçš„åˆ†æçµæ´»æ€§ã€‚åœ¨äººç±»è¡Œä¸ºå’Œä¸“ä¸šæ´»åŠ¨ç­‰é«˜å¤æ‚åº¦é¢†åŸŸçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLLaMoæœ‰æ•ˆåœ°æ•è·äº†ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œæé«˜äº†è¿åŠ¨å¯†é›†å‹åœºæ™¯ä¸­çš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›LLaMoèƒ½ä¸ºä»ä½“è‚²åˆ†æåˆ°è¡Œä¸ºé¢„æµ‹å…·æœ‰å¹¿æ³›åº”ç”¨é¢†åŸŸçš„æœªæ¥å¤šæ¨¡å¼AIç³»ç»Ÿæä¾›åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨é¡¹ç›®ç½‘ç«™ä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo%E3%80%82">https://github.com/ILGLJ/LLaMoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16805v4">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>LLaMoæ¡†æ¶æ˜¯ä¸€ç§ç”¨äºäººç±»è¿åŠ¨æŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œèƒ½å¤Ÿä¿æŒè¿åŠ¨çš„æœ¬ä½“å½¢å¼è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œæå‡æ¨¡å‹å¯¹äººç±»å¤æ‚è¡Œä¸ºçš„è§£è¯»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaMoæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºäººç±»è¿åŠ¨æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>ä¸ä¼ ç»Ÿå°†éè¯­è¨€è¾“å…¥è½¬æ¢ä¸ºè¯­è¨€æ ‡è®°çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸åŒï¼ŒLLaMoä¿æŒè¿åŠ¨çš„æœ¬ä½“å½¢å¼è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>LLaMoé€šè¿‡å¤„ç†è§†é¢‘å’Œè¿åŠ¨æ•°æ®ä¸æ–‡æœ¬è¾“å…¥ï¼Œå®ç°äº†çµæ´»ã€ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åˆ†æã€‚</li>
<li>LLaMoèƒ½æœ‰æ•ˆæ•æ‰é«˜å¤æ‚åº¦é¢†åŸŸçš„ç‰¹å®šçŸ¥è¯†ï¼Œæå‡åœ¨è¿åŠ¨å¯†é›†å‹åœºæ™¯ä¸­çš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>LLaMoæ¡†æ¶åœ¨è¯¸å¦‚ä½“è‚²åˆ†æå’Œè¡Œä¸ºé¢„æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>LLaMoçš„ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨é¡¹ç›®çš„ç½‘ç«™ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-54a053ff97423f4f3b0a52bfe32db42a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51b1f741f7ed474b7e200c54b63fd0a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68d11099a7db5bf896a993681ba04562.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Is-â€˜Rightâ€™-Right-Enhancing-Object-Orientation-Understanding-in-Multimodal-Large-Language-Models-through-Egocentric-Instruction-Tuning"><a href="#Is-â€˜Rightâ€™-Right-Enhancing-Object-Orientation-Understanding-in-Multimodal-Large-Language-Models-through-Egocentric-Instruction-Tuning" class="headerlink" title="Is â€˜Rightâ€™ Right? Enhancing Object Orientation Understanding in   Multimodal Large Language Models through Egocentric Instruction Tuning"></a>Is â€˜Rightâ€™ Right? Enhancing Object Orientation Understanding in   Multimodal Large Language Models through Egocentric Instruction Tuning</h2><p><strong>Authors:Ji Hyeok Jung, Eun Tae Kim, Seoyeon Kim, Joo Ho Lee, Bumsoo Kim, Buru Chang</strong></p>
<p>Multimodal large language models (MLLMs) act as essential interfaces, connecting humans with AI technologies in multimodal applications. However, current MLLMs face challenges in accurately interpreting object orientation in images due to inconsistent orientation annotations in training data, hindering the development of a coherent orientation understanding. To overcome this, we propose egocentric instruction tuning, which aligns MLLMsâ€™ orientation understanding with the userâ€™s perspective, based on a consistent annotation standard derived from the userâ€™s egocentric viewpoint. We first generate egocentric instruction data that leverages MLLMsâ€™ ability to recognize object details and applies prior knowledge for orientation understanding. Using this data, we perform instruction tuning to enhance the modelâ€™s capability for accurate orientation interpretation. In addition, we introduce EgoOrientBench, a benchmark that evaluates MLLMsâ€™ orientation understanding across three tasks using images collected from diverse domains. Experimental results on this benchmark show that egocentric instruction tuning significantly improves orientation understanding without compromising overall MLLM performance. The instruction data and benchmark dataset are available on our project page at <a target="_blank" rel="noopener" href="https://github.com/jhCOR/EgoOrientBench">https://github.com/jhCOR/EgoOrientBench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºäººç±»ä¸å¤šæ¨¡æ€åº”ç”¨ç¨‹åºä¸­çš„AIæŠ€æœ¯ä¹‹é—´çš„é‡è¦æ¥å£ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®ä¸­çš„æ–¹å‘æ ‡æ³¨ä¸ä¸€è‡´ï¼Œå½“å‰çš„MLLMsåœ¨å‡†ç¡®è§£é‡Šå›¾åƒä¸­çš„å¯¹è±¡æ–¹å‘æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™é˜»ç¢äº†è¿è´¯çš„æ–¹å‘ç†è§£çš„å‘å±•ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼ˆç¬¬ä¸€äººç§°è§†è§’ï¼‰çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼Œå®ƒå°†MLLMsçš„æ–¹å‘ç†è§£ä¸ç”¨æˆ·çš„è§†è§’å¯¹é½ï¼ŒåŸºäºä»ç”¨æˆ·çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§’å¾—å‡ºçš„ä¸€è‡´æ ‡æ³¨æ ‡å‡†ã€‚æˆ‘ä»¬é¦–å…ˆç”Ÿæˆåˆ©ç”¨MLLMsè¯†åˆ«å¯¹è±¡ç»†èŠ‚å’Œåº”ç”¨æ–¹å‘ç†è§£çš„å…ˆéªŒçŸ¥è¯†çš„è‡ªæˆ‘ä¸­å¿ƒæŒ‡ä»¤æ•°æ®ã€‚ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬æ‰§è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥å¢å¼ºæ¨¡å‹å¯¹å‡†ç¡®æ–¹å‘è§£é‡Šçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†EgoOrientBenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•é€šè¿‡ä»å¤šä¸ªé¢†åŸŸæ”¶é›†çš„å›¾åƒå¯¹MLLMsçš„æ–¹å‘ç†è§£èƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä»»åŠ¡ã€‚åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒæ•´åœ¨ä¸å½±å“MLLMæ•´ä½“æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜äº†æ–¹å‘ç†è§£èƒ½åŠ›ã€‚æŒ‡ä»¤æ•°æ®å’ŒåŸºå‡†æ•°æ®é›†å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://github.com/jhCOR/EgoOrientBench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jhCOR/EgoOrientBenchä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16761v2">PDF</a> CVPR2025 Camera-ready</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºäººç±»ä¸AIæŠ€æœ¯ä¹‹é—´çš„å…³é”®æ¥å£ï¼Œåœ¨å¤šæ¨¡æ€åº”ç”¨ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®ä¸­æ–¹ä½æ ‡æ³¨çš„ä¸ä¸€è‡´æ€§ï¼ŒMLLMsåœ¨å‡†ç¡®è§£é‡Šå›¾åƒä¸­çš„å¯¹è±¡æ–¹ä½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œåˆ¶çº¦äº†å…¶æ–¹ä½ç†è§£çš„è¿è´¯æ€§å‘å±•ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ç”¨æˆ·è§†è§’ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼ŒåŸºäºç”¨æˆ·è§†è§’çš„ä¸€è‡´æ ‡æ³¨æ ‡å‡†æ¥å¯¹é½MLLMsçš„æ–¹ä½ç†è§£ã€‚æˆ‘ä»¬é¦–å…ˆç”Ÿæˆä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤æ•°æ®ï¼Œåˆ©ç”¨MLLMsè¯†åˆ«å¯¹è±¡ç»†èŠ‚çš„èƒ½åŠ›ï¼Œå¹¶åº”ç”¨æ–¹ä½ç†è§£çš„å…ˆéªŒçŸ¥è¯†ã€‚åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œæé«˜æ¨¡å‹å‡†ç¡®è§£é‡Šæ–¹ä½çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†EgoOrientBenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸­è¯„ä¼°MLLMsçš„æ–¹ä½ç†è§£ï¼Œä½¿ç”¨äº†æ¥è‡ªä¸åŒé¢†åŸŸçš„å›¾åƒã€‚åœ¨EgoOrientBenchä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä»¥ç”¨æˆ·è§†è§’ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒæ•´åœ¨æ˜¾è‘—æé«˜æ–¹ä½ç†è§£çš„åŒæ—¶ï¼Œä¸æŸå®³MLLMçš„æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€åº”ç”¨ä¸­ä½œä¸ºäººç±»ä¸AIä¹‹é—´çš„æ¥å£ï¼Œå…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>è®­ç»ƒæ•°æ®ä¸­æ–¹ä½æ ‡æ³¨çš„ä¸ä¸€è‡´æ€§å½±å“äº†MLLMså‡†ç¡®è§£é‡Šå›¾åƒä¸­çš„å¯¹è±¡æ–¹ä½ã€‚</li>
<li>æå‡ºä»¥ç”¨æˆ·è§†è§’ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼Œä»¥æ”¹å–„MLLMsçš„æ–¹ä½ç†è§£è¿è´¯æ€§ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆçš„ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤æ•°æ®ï¼Œç»“åˆMLLMsçš„è¯†åˆ«èƒ½åŠ›å’Œå…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤è°ƒæ•´æé«˜æ¨¡å‹å‡†ç¡®è§£é‡Šæ–¹ä½çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥EgoOrientBenchåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°MLLMsåœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ–¹ä½ç†è§£æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e11583df710bcd80abc68f5880408e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73db525671778fe6dab339072b95eae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36513dbb0d6fe6e72ecb552459971320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07299a7377b10387a1ed8159e85e3f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ed62787356adf4a2eb506a7681e700.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MC-LLaVA-Multi-Concept-Personalized-Vision-Language-Model"><a href="#MC-LLaVA-Multi-Concept-Personalized-Vision-Language-Model" class="headerlink" title="MC-LLaVA: Multi-Concept Personalized Vision-Language Model"></a>MC-LLaVA: Multi-Concept Personalized Vision-Language Model</h2><p><strong>Authors:Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang</strong></p>
<p>Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies investigate VLM personalization to understand user-provided concepts. However, they mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes the first multi-concept personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the costs related to joint training, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location confidence maps for enhanced recognition and grounding capabilities. To advance multi-concept personalization research, we further contribute a high-quality instruction tuning dataset. We carefully collect images with multiple characters and objects from movies and manually generate question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive qualitative and quantitative experiments demonstrate that MC-LLaVA can achieve impressive multi-concept personalized responses, paving the way for VLMs to become better user-specific assistants. The code and dataset will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/arctanxarc/MC-LLaVA">https://github.com/arctanxarc/MC-LLaVA</a>. </p>
<blockquote>
<p>å½“å‰çš„è¯­è¨€è§†è§‰æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¦‚è§†è§‰é—®ç­”ã€‚ä¸ºäº†æå‡ç”¨æˆ·ä½“éªŒï¼Œè¿‘æœŸçš„ç ”ç©¶è‡´åŠ›äºå¯¹VLMè¿›è¡Œä¸ªæ€§åŒ–ï¼Œä»¥ç†è§£ç”¨æˆ·æä¾›çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦å…³æ³¨å•ä¸€æ¦‚å¿µçš„ä¸ªæ€§åŒ–ï¼Œå¿½ç•¥äº†å¤šä¸ªæ¦‚å¿µçš„å­˜åœ¨å’Œç›¸äº’ä½œç”¨ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¤šæ¦‚å¿µä¸ªæ€§åŒ–èŒƒå¼MC-LLaVAã€‚å…·ä½“æ¥è¯´ï¼ŒMC-LLaVAé‡‡ç”¨äº†ä¸€ç§å¤šæ¦‚å¿µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªè®­ç»ƒæ­¥éª¤ä¸­æœ‰æ•ˆåœ°é›†æˆå¤šä¸ªæ¦‚å¿µã€‚ä¸ºäº†é™ä½è”åˆè®­ç»ƒçš„æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ªæ€§åŒ–çš„æ–‡æœ¬æç¤ºï¼Œåˆ©ç”¨è§†è§‰ä»¤ç‰Œä¿¡æ¯æ¥åˆå§‹åŒ–æ¦‚å¿µä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸ªæ€§åŒ–çš„è§†è§‰æç¤ºï¼Œé€šè¿‡æ±‡æ€»ä½ç½®ç½®ä¿¡å›¾æ¥å¢å¼ºè¯†åˆ«å’Œæ¥åœ°èƒ½åŠ›ã€‚ä¸ºäº†æ¨åŠ¨å¤šæ¦‚å¿µä¸ªæ€§åŒ–ç ”ç©¶çš„å‘å±•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚æˆ‘ä»¬ä»ç”µå½±ä¸­ç²¾å¿ƒæ”¶é›†äº†å«æœ‰å¤šä¸ªè§’è‰²å’Œå¯¹è±¡çš„å›¾åƒï¼Œå¹¶æ‰‹åŠ¨ä¸ºå¤šä¸ªæ¦‚å¿µåœºæ™¯ç”Ÿæˆé—®ç­”æ ·æœ¬ï¼Œå…·æœ‰å‡ºè‰²çš„å¤šæ ·æ€§ã€‚ç»¼åˆçš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼ŒMC-LLaVAå¯ä»¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„å¤šæ¦‚å¿µä¸ªæ€§åŒ–å“åº”ï¼Œä¸ºVLMsæˆä¸ºæ›´å¥½çš„ç”¨æˆ·ç‰¹å®šåŠ©æ‰‹é“ºå¹³äº†é“è·¯ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/arctanxarc/MC-LLaVA%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/arctanxarc/MC-LLaVAä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11706v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¦‚å¿µä¸ªæ€§åŒ–è§†è§‰è¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹å¿½ç•¥å¤šæ¦‚å¿µçš„é—®é¢˜ï¼Œæå‡ºMC-LLaVAæ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ¦‚å¿µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œåœ¨å•ä¸€è®­ç»ƒæ­¥éª¤ä¸­æœ‰æ•ˆæ•´åˆå¤šä¸ªæ¦‚å¿µã€‚åˆ©ç”¨è§†è§‰æ ‡è®°ä¿¡æ¯åˆå§‹åŒ–æ¦‚å¿µæ ‡è®°ï¼Œé™ä½è”åˆè®­ç»ƒæˆæœ¬ã€‚å¼•å…¥ä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºå’Œä¸ªæ€§åŒ–è§†è§‰æç¤ºï¼Œæé«˜è¯†åˆ«å’Œå¤šæ¨¡æ€å®šä½èƒ½åŠ›ã€‚è´¡çŒ®é«˜è´¨é‡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œä¿ƒè¿›å¤šæ¦‚å¿µä¸ªæ€§åŒ–ç ”ç©¶çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œå¦‚è§†è§‰é—®ç­”ã€‚</li>
<li>è¿‘æœŸç ”ç©¶ç€çœ¼äºVLMä¸ªæ€§åŒ–ï¼Œä»¥ç†è§£ç”¨æˆ·æä¾›çš„æ¦‚å¿µã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•æ¦‚å¿µä¸ªæ€§åŒ–ï¼Œå¿½ç•¥äº†å¤šæ¦‚å¿µçš„å­˜åœ¨å’Œç›¸äº’ä½œç”¨ã€‚</li>
<li>MC-LLaVAæ¨¡å‹é¦–æ¬¡æå‡ºå¤šæ¦‚å¿µä¸ªæ€§åŒ–èŒƒå¼ã€‚</li>
<li>MC-LLaVAé‡‡ç”¨å¤šæ¦‚å¿µæŒ‡ä»¤è°ƒæ•´ç­–ç•¥ï¼Œåœ¨å•ä¸€è®­ç»ƒæ­¥éª¤ä¸­æ•´åˆå¤šä¸ªæ¦‚å¿µã€‚</li>
<li>åˆ©ç”¨è§†è§‰æ ‡è®°ä¿¡æ¯åˆå§‹åŒ–æ¦‚å¿µæ ‡è®°ï¼Œé™ä½è”åˆè®­ç»ƒæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff0588fed58b96b95a8344ea2ffb4f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2075661619ba41019437cf247b33ef23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a0842e0a2d456dbf4d45bf98d6195d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3030174a11a73b336e1ac75622104458.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a138bc677a822b68424c8e7ee15f6052.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a8a62d63fe32f510729dfeebedf4e24.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Formal-Framework-for-Understanding-Length-Generalization-in-Transformers"><a href="#A-Formal-Framework-for-Understanding-Length-Generalization-in-Transformers" class="headerlink" title="A Formal Framework for Understanding Length Generalization in   Transformers"></a>A Formal Framework for Understanding Length Generalization in   Transformers</h2><p><strong>Authors:Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn</strong></p>
<p>A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers. </p>
<blockquote>
<p>å¯¹äºè½¬æ¢å™¨æ¥è¯´ï¼Œä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯æ¨å¹¿åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡çš„åºåˆ—é•¿åº¦ã€‚å°½ç®¡ä¹‹å‰çš„å·¥ä½œå·²ç»è¯æ˜ï¼Œè½¬æ¢å™¨çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›å–å†³äºä»»åŠ¡ï¼Œä½†å…³äºè¿™ä¸€ç°è±¡çš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¥è°¨çš„ç†è®ºæ¡†æ¶ï¼Œå¯¹å¸¦æœ‰å¯å­¦ä¹ ç»å¯¹ä½ç½®ç¼–ç çš„å› æœè½¬æ¢å™¨çš„é•¿åº¦æ³›åŒ–è¿›è¡Œåˆ†æã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ»ç”»äº†åœ¨ç†æƒ³åŒ–æ¨ç†æ–¹æ¡ˆä¸‹ï¼Œä½¿ç”¨åŸºäºèŒƒæ•°çš„æ­£åˆ™åŒ–å™¨ï¼Œå¯ä»¥ä»è¶³å¤Ÿé•¿çš„è¾“å…¥ä¸­è¯†åˆ«å‡ºçš„é‚£äº›å…·æœ‰ç»å¯¹ä½ç½®ç¼–ç çš„å‡½æ•°ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºä¸€ç³»åˆ—ä¸°å¯Œçš„é—®é¢˜è¯æ˜é•¿åº¦æ³›åŒ–çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†è¯¥ç†è®ºåœ¨ç®—æ³•å’Œå½¢å¼è¯­è¨€ä»»åŠ¡çš„é•¿åº¦æ³›åŒ–æˆåŠŸå’Œå¤±è´¥æ–¹é¢çš„é¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç†è®ºä¸ä»…è§£é‡Šäº†å¤§é‡ç»éªŒè§‚å¯Ÿç»“æœï¼Œè€Œä¸”ä¸ºé¢„æµ‹è½¬æ¢å™¨ä¸­çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02140v2">PDF</a> 85 pages, 9 figures, 11 tables. Accepted for publication at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªä¸¥è°¨çš„ç†è®ºæ¡†æ¶ï¼Œç”¨äºåˆ†æå…·æœ‰å¯å­¦ä¹ ç»å¯¹ä½ç½®ç¼–ç çš„å› æœå˜å‹å™¨ä¸­çš„é•¿åº¦æ³›åŒ–é—®é¢˜ã€‚é€šè¿‡ç†æƒ³åŒ–æ¨ç†æ–¹æ¡ˆå’ŒåŸºäºèŒƒæ•°çš„æ­£åˆ™åŒ–å™¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»è¶³å¤Ÿé•¿çš„è¾“å…¥ä¸­è¯†åˆ«å‡ºé‚£äº›å¯è¯†åˆ«çš„åŠŸèƒ½ï¼Œä»è€Œè¯æ˜å¯¹ä¸€ç³»åˆ—é—®é¢˜çš„é•¿åº¦æ³›åŒ–çš„å¯èƒ½æ€§ã€‚å®éªŒéªŒè¯äº†è¯¥ç†è®ºåœ¨é¢„æµ‹å„ç§ç®—æ³•å’Œå½¢å¼è¯­è¨€ä»»åŠ¡çš„é•¿åº¦æ³›åŒ–æˆåŠŸå’Œå¤±è´¥æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç†è®ºä¸ä»…è§£é‡Šäº†å¹¿æ³›çš„å®éªŒè§‚å¯Ÿç»“æœï¼Œè€Œä¸”ä¸ºé¢„æµ‹å˜å‹å™¨çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œç”¨äºåˆ†æå…·æœ‰å¯å­¦ä¹ ç»å¯¹ä½ç½®ç¼–ç çš„å› æœå˜å‹å™¨ä¸­çš„é•¿åº¦æ³›åŒ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç†æƒ³åŒ–æ¨ç†æ–¹æ¡ˆå’ŒåŸºäºèŒƒæ•°çš„æ­£åˆ™åŒ–å™¨ï¼Œç ”ç©¶èƒ½å¤Ÿè¯†åˆ«è¶³å¤Ÿé•¿è¾“å…¥ä¸­çš„å¯è¯†åˆ«åŠŸèƒ½ã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†å¯¹äºä¸€ç³»åˆ—é—®é¢˜çš„é•¿åº¦æ³›åŒ–çš„å¯èƒ½æ€§ã€‚</li>
<li>å®éªŒéªŒè¯äº†è¯¥ç†è®ºåœ¨é¢„æµ‹å„ç§ç®—æ³•å’Œå½¢å¼è¯­è¨€ä»»åŠ¡çš„é•¿åº¦æ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥ç†è®ºä¸ä»…è§£é‡Šäº†å¹¿æ³›çš„å®éªŒè§‚å¯Ÿç»“æœã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºé¢„æµ‹å˜å‹å™¨çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°æ€è·¯ã€‚</li>
<li>æ­¤ç†è®ºæ¡†æ¶æœ‰åŠ©äºç†è§£ä¸ºä½•åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œå˜å‹å™¨èƒ½å¤ŸæˆåŠŸæ³›åŒ–åˆ°æ¯”è®­ç»ƒæœŸé—´è§‚å¯Ÿåˆ°çš„æ›´é•¿çš„åºåˆ—ï¼Œè€Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­åˆ™å¯èƒ½å¤±è´¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05c431618e967710b683df750bebbe9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37d9704c0d099710ef77ea8a508e0bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d74a9d7a10c282f489f405ec119ba1b4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OmniBench-Towards-The-Future-of-Universal-Omni-Language-Models"><a href="#OmniBench-Towards-The-Future-of-Universal-Omni-Language-Models" class="headerlink" title="OmniBench: Towards The Future of Universal Omni-Language Models"></a>OmniBench: Towards The Future of Universal Omni-Language Models</h2><p><strong>Authors:Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have focused on integrating multiple modalities, yet their ability to simultaneously process and reason across different inputs remains underexplored. We introduce OmniBench, a novel benchmark designed to evaluate modelsâ€™ ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define language models capable of such tri-modal processing as omni-language models (OLMs). OmniBench features high-quality human annotations that require integrated understanding across all modalities. Our evaluation reveals that: i) open-source OLMs show significant limitations in instruction-following and reasoning in tri-modal contexts; and ii) most baseline models perform poorly (around 50% accuracy) even with textual alternatives to image&#x2F;audio inputs. To address these limitations, we develop OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We advocate for developing more robust tri-modal integration techniques and training strategies to enhance OLM performance. Codes and data could be found at our repo (<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/OmniBench">https://github.com/multimodal-art-projection/OmniBench</a>). </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›å±•ä¸»è¦èšç„¦äºå¤šç§æ¨¡æ€çš„é›†æˆï¼Œä½†å®ƒä»¬åœ¨å¤„ç†ä¸åŒè¾“å…¥çš„åŒæ—¶è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ä»ç„¶è¢«å¿½è§†ã€‚æˆ‘ä»¬å¼•å…¥äº†OmniBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬è¾“å…¥ä¸Šçš„è¯†åˆ«ã€è§£é‡Šå’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å®šä¹‰èƒ½å¤Ÿè¿›è¡Œè¿™ç§ä¸‰æ¨¡æ€å¤„ç†çš„è¯­è¨€æ¨¡å‹ä¸ºå…¨è¯­è¨€æ¨¡å‹ï¼ˆOLMsï¼‰ã€‚OmniBenchä»¥é«˜è´¨é‡çš„äººåŠ›æ³¨é‡Šä¸ºç‰¹è‰²ï¼Œéœ€è¦å…¨é¢ç†è§£æ‰€æœ‰æ¨¡æ€ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°ï¼šiï¼‰å¼€æºOLMåœ¨ä¸‰æ¨¡æ€ç¯å¢ƒä¸‹çš„æŒ‡ä»¤æ‰§è¡Œå’Œæ¨ç†èƒ½åŠ›å­˜åœ¨æ˜æ˜¾å±€é™ï¼›iiï¼‰å³ä½¿ä½¿ç”¨æ–‡æœ¬æ›¿ä»£å›¾åƒ&#x2F;éŸ³é¢‘è¾“å…¥ï¼Œå¤§å¤šæ•°åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ä¹Ÿè¾ƒå·®ï¼ˆå‡†ç¡®ç‡çº¦ä¸º50%ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼€å‘äº†OmniInstructï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«9ä¸‡å…­åƒä¸ªæ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒOLMã€‚æˆ‘ä»¬æå€¡å¼€å‘æ›´ç¨³å¥çš„ä¸‰æ¨¡æ€é›†æˆæŠ€æœ¯å’Œè®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜OLMçš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨æˆ‘ä»¬çš„ä»“åº“ä¸­æ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/OmniBench%EF%BC%89%E3%80%82">https://github.com/multimodal-art-projection/OmniBenchï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15272v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹åŸºå‡†æµ‹è¯•â€”â€”OmniBenchï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åŒæ—¶å¤„ç†è§†è§‰ã€å£°éŸ³å’Œæ–‡æœ¬è¾“å…¥çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ä¸‰æ¨¡æ€ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¹¶æå‡ºOmniInstructæ•°æ®é›†ç”¨äºè®­ç»ƒèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€æ•°æ®çš„è¯­è¨€æ¨¡å‹ã€‚å¼ºè°ƒå‘å±•æ›´å¼ºå¤§çš„ä¸‰æ¨¡æ€é›†æˆæŠ€æœ¯å’Œè®­ç»ƒç­–ç•¥ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€OmniBenchåŸºå‡†æµ‹è¯•è¢«è®¾è®¡ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åŒæ—¶å¤„ç†è§†è§‰ã€å£°éŸ³å’Œæ–‡æœ¬è¾“å…¥çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è¯†åˆ«ã€è§£é‡Šå’Œæ¨ç†ã€‚<br>äºŒã€ç°æœ‰å¼€æºæ¨¡å‹åœ¨å¤„ç†ä¸‰æ¨¡æ€ä¸Šä¸‹æ–‡æ—¶çš„æŒ‡ä»¤éµå¾ªå’Œæ¨ç†èƒ½åŠ›å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚<br>ä¸‰ã€å¤§å¤šæ•°åŸºå‡†æ¨¡å‹å³ä½¿ä½¿ç”¨æ–‡æœ¬æ›¿ä»£å›¾åƒ&#x2F;éŸ³é¢‘è¾“å…¥ï¼Œå…¶å‡†ç¡®ç‡ä¹Ÿåªæœ‰å¤§çº¦50%ã€‚<br>å››ã€ä¸ºäº†æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†OmniInstructæ•°æ®é›†ï¼ŒåŒ…å«96Kæ ·æœ¬çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ã€‚<br>äº”ã€OmniBenchå¼ºè°ƒå¼€å‘æ›´å¼ºå¤§çš„ä¸‰æ¨¡æ€é›†æˆæŠ€æœ¯å’Œè®­ç»ƒç­–ç•¥çš„å¿…è¦æ€§ã€‚<br>å…­ã€OmniBenchçš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨æŒ‡å®šä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8dd5fae1a0fcc5118afea7573433b2b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7701f076b69e733e1efc018ab5d404df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bec75f3188c8c0c18cef5afbc8f10f68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dd6c40e3aa975affdca19585c3c51c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68df14a4ad25a76d250205cb0a11e2bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac1b1f6c109c75ed09adb25eeecf5e14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6547e34a7776769d11fadda0f924efe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-391ed846392ec13b560ca702b92ae5d5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy"><a href="#Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy" class="headerlink" title="Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?"></a>Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?</h2><p><strong>Authors:Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill</strong></p>
<p>Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on recognition accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œåªè§£ç ç¦»æ•£æ ‡è®°çš„è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—äº†é‡å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå…³äºä¸åŒæ¨¡æ€å¦‚ä½•å½±å“ç‰¹å®šåœºæ™¯çš„æ€§èƒ½çš„ç³»ç»Ÿæ€§åˆ†æä»ç„¶æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šç§æ¨¡æ€å¯¹åˆæˆæ•°æ®é›†å’Œç°å®ä¸–ç•Œæ•°æ®é›†è¯†åˆ«ç²¾åº¦çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼šï¼ˆ1ï¼‰é›†æˆæ›´å¤šæ¨¡æ€å¯ä»¥æé«˜ç²¾åº¦ï¼›å°¤å…¶æ˜¯æˆ‘ä»¬çš„è®ºæ–‡æ®æˆ‘ä»¬æ‰€çŸ¥é¦–æ¬¡å±•ç¤ºäº†ç»“åˆéŸ³é¢‘ã€å›¾åƒä¸Šä¸‹æ–‡å’Œå˜´å”‡ä¿¡æ¯çš„ä¼˜åŠ¿ï¼›ï¼ˆ2ï¼‰å›¾åƒä½œä¸ºè¯­éŸ³è¯†åˆ«çš„è¾…åŠ©æ¨¡æ€åœ¨ä¸­ç­‰å™ªå£°æ°´å¹³ä¸‹æä¾›æœ€å¤§çš„ä¼˜åŠ¿ï¼Œæ­¤å¤–ï¼Œå®ƒä»¬è¡¨ç°å‡ºä¸å˜´å”‡ç§»åŠ¨ç­‰å›ºæœ‰åŒæ­¥æ¨¡æ€ä¸åŒçš„è¶‹åŠ¿ï¼›ï¼ˆ3ï¼‰å½“åœ¨é¢„å¤„ç†æ­¥éª¤ä¸­è¿‡æ»¤æ‰æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯æ—¶ï¼Œåˆæˆæ•°æ®é›†å’Œç°å®ä¸–ç•Œæ•°æ®é›†çš„æ€§èƒ½éƒ½ä¼šæœ‰æ‰€æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09221v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¯¹åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯†åˆ«ç²¾åº¦çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œé›†æˆå¤šæ¨¡æ€èƒ½æé«˜å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯ç»“åˆéŸ³é¢‘ã€å›¾åƒä¸Šä¸‹æ–‡å’Œå”‡éƒ¨ä¿¡æ¯çš„ç»„åˆæ•ˆç›Šæ˜¾è‘—ï¼›å›¾åƒä½œä¸ºè¯­éŸ³è¯†åˆ«çš„ä¸€ç§è¡¥å……æ¨¡å¼åœ¨ä¸­åº¦å™ªéŸ³æ°´å¹³ä¸‹æä¾›æœ€å¤§å¸®åŠ©ï¼Œå¹¶ä¸”ä¸å”‡éƒ¨åŠ¨ä½œç­‰å†…åœ¨åŒæ­¥æ¨¡å¼ç›¸æ¯”è¡¨ç°å‡ºä¸åŒçš„è¶‹åŠ¿ï¼›åœ¨é¢„å¤„ç†æ­¥éª¤ä¸­è¿‡æ»¤æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯å¯ä»¥æé«˜åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†çš„è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç»“åˆå¤šç§æ¨¡æ€ï¼ˆå¦‚éŸ³é¢‘ã€å›¾åƒä¸Šä¸‹æ–‡å’Œå”‡éƒ¨ä¿¡æ¯ï¼‰èƒ½æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨è¯­éŸ³è¯†åˆ«ä¸­ï¼Œå›¾åƒä½œä¸ºè¡¥å……æ¨¡æ€åœ¨ä¸­åº¦å™ªéŸ³æ°´å¹³ä¸‹å°¤ä¸ºé‡è¦ã€‚</li>
<li>ä¸å†…åœ¨åŒæ­¥æ¨¡å¼ï¼ˆå¦‚å”‡éƒ¨åŠ¨ä½œï¼‰ç›¸æ¯”ï¼Œå›¾åƒçš„å½±å“å±•ç°å‡ºç‹¬ç‰¹è¶‹åŠ¿ã€‚</li>
<li>è¿‡æ»¤æœ€ç›¸å…³çš„è§†è§‰ä¿¡æ¯ä½œä¸ºé¢„å¤„ç†æ­¥éª¤å¯ä»¥æé«˜è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†å¤šæ¨¡æ€é›†æˆåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ•ˆç›Šã€‚</li>
<li>æœ¬æ–‡æ˜¯é¦–æ¬¡å±•ç¤ºç»“åˆéŸ³é¢‘ã€å›¾åƒä¸Šä¸‹æ–‡å’Œå”‡éƒ¨ä¿¡æ¯å¸¦æ¥çš„ç›Šå¤„çš„è®ºæ–‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0bafdeb6780cbecb1d6946db37408f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24bd934af7376e7ba1d87df26c16c8fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064b23bff11e7ddbdea0ae473c99fbe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ddb7e89b731e981061fc5224f1f57d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0cd001349ee2c56f86b5ce767c93af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8823ea5bbfb3feab68473343aed2150f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-95b2d0fb29b18389b1c180b7a3c48137.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  v-CLR View-Consistent Learning for Open-World Instance Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fd0720d589f11ae109f58dcbb423620.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-04  Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by   Generating Realistic Dermoscopic Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17663.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
