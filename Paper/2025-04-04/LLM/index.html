<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-04  CADFormer Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c4198d82bea3dd405c405082c9f941d8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-04-更新"><a href="#2025-04-04-更新" class="headerlink" title="2025-04-04 更新"></a>2025-04-04 更新</h1><h2 id="CADFormer-Fine-Grained-Cross-modal-Alignment-and-Decoding-Transformer-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#CADFormer-Fine-Grained-Cross-modal-Alignment-and-Decoding-Transformer-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation"></a>CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer   for Referring Remote Sensing Image Segmentation</h2><p><strong>Authors:Maofu Liu, Xin Jiang, Xiaokang Zhang</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task, aiming to segment specific target objects in remote sensing (RS) images based on a given language expression. Existing RRSIS methods typically employ coarse-grained unidirectional alignment approaches to obtain multimodal features, and they often overlook the critical role of language features as contextual information during the decoding process. Consequently, these methods exhibit weak object-level correspondence between visual and language features, leading to incomplete or erroneous predicted masks, especially when handling complex expressions and intricate RS image scenes. To address these challenges, we propose a fine-grained cross-modal alignment and decoding Transformer, CADFormer, for RRSIS. Specifically, we design a semantic mutual guidance alignment module (SMGAM) to achieve both vision-to-language and language-to-vision alignment, enabling comprehensive integration of visual and textual features for fine-grained cross-modal alignment. Furthermore, a textual-enhanced cross-modal decoder (TCMD) is introduced to incorporate language features during decoding, using refined textual information as context to enhance the relationship between cross-modal features. To thoroughly evaluate the performance of CADFormer, especially for inconspicuous targets in complex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which includes larger high-resolution RS image patches and semantically richer language expressions. Extensive experiments on the RRSIS-HR dataset and the popular RRSIS-D dataset demonstrate the effectiveness and superiority of CADFormer. Datasets and source codes will be available at <a target="_blank" rel="noopener" href="https://github.com/zxk688">https://github.com/zxk688</a>. </p>
<blockquote>
<p>远程遥感图像分割（RRSIS）是一项具有挑战性的任务，旨在根据给定的语言表达对遥感（RS）图像中的特定目标对象进行分割。现有的RRSIS方法通常采用粗粒度单向对齐方法获得多模态特征，但它们往往忽略了语言特征在解码过程中的关键作用作为上下文信息。因此，这些方法在视觉和语言特征之间表现出较弱的对象级对应关系，导致预测掩膜不完整或错误，尤其是在处理复杂的表达式和遥感的细腻图像场景时。为了解决这些挑战，我们提出了一种针对RRSIS的精细粒度跨模态对齐和解码Transformer，名为CADFormer。具体来说，我们设计了一个语义相互引导对齐模块（SMGAM）来实现视觉到语言以及语言到视觉的对齐，实现对视觉和文本特征的全面融合，实现精细粒度的跨模态对齐。此外，还引入了一种文本增强跨模态解码器（TCMD），在解码过程中融入语言特征，利用精炼的文本信息作为上下文，增强跨模态特征之间的关系。为了全面评估CADFormer的性能，尤其是针对复杂场景中不明显的目标，我们构建了一个新的RRSIS数据集，名为RRSIS-HR，该数据集包括更大的高分辨率RS图像补丁和语义更丰富的语言表达。在RRSIS-HR数据集和流行的RRSIS-D数据集上的大量实验证明了CADFormer的有效性和优越性。数据集和源代码将在<a target="_blank" rel="noopener" href="https://github.com/zxk688">https://github.com/zxk688</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23456v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对远程遥感图像分割任务（RRSIS）中的挑战，本文提出了一种精细的跨模态对齐与解码Transformer（CADFormer）。它设计了语义互导向对齐模块（SMGAM）实现视觉与语言的双向对齐，并引入了文本增强跨模态解码器（TCMD）在解码过程中融入语言特征。为评估模型性能，构建了新的RRSIS数据集RRSIS-HR。实验表明，CADFormer在RRSIS-HR及流行数据集RRSIS-D上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RRSIS是一个挑战性任务，旨在根据给定的语言表达对遥感图像中的特定目标对象进行分割。</li>
<li>现有方法通常使用粗粒度的单向对齐方法获得多模态特征，忽视了语言特征作为上下文信息的重要性。</li>
<li>CADFormer通过设计SMGAM模块实现视觉与语言的双向精细对齐。</li>
<li>TCMD解码器在解码过程中融入了语言特征，利用精炼的文本信息作为上下文，增强了跨模态特征之间的关系。</li>
<li>为评估模型性能，构建了新的RRSIS数据集RRSIS-HR，包含大尺度高分辨率遥感图像和丰富的语言表达。</li>
<li>在RRSIS-HR及RRSIS-D数据集上的实验表明CADFormer具有优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23456">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3bd40f76e978ec2253da487287569be9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccbcdaad1110d35330f0dc0636a7a2e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b40f4010a95e815d4d595b15de803e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3f96982cd38744ce6fecdd55f92ca65.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EagleVision-Object-level-Attribute-Multimodal-LLM-for-Remote-Sensing"><a href="#EagleVision-Object-level-Attribute-Multimodal-LLM-for-Remote-Sensing" class="headerlink" title="EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing"></a>EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing</h2><p><strong>Authors:Hongxiang Jiang, Jihao Yin, Qixiong Wang, Jiaqi Feng, Guo Chen</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have demonstrated impressive results in various visual tasks. However, in remote sensing (RS), high resolution and small proportion of objects pose challenges to existing MLLMs, which struggle with object-centric tasks, particularly in precise localization and fine-grained attribute description for each object. These RS MLLMs have not yet surpassed classical visual perception models, as they only provide coarse image understanding, leading to limited gains in real-world scenarios. To address this gap, we establish EagleVision, an MLLM tailored for remote sensing that excels in object detection and attribute comprehension. Equipped with the Attribute Disentangle module, EagleVision learns disentanglement vision tokens to express distinct attributes. To support object-level visual-language alignment, we construct EVAttrs-95K, the first large-scale object attribute understanding dataset in RS for instruction tuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves state-of-the-art performance on both fine-grained object detection and object attribute understanding tasks, highlighting the mutual promotion between detection and understanding capabilities in MLLMs. The code, model, data, and demo will be available at <a target="_blank" rel="noopener" href="https://github.com/XiangTodayEatsWhat/EagleVision">https://github.com/XiangTodayEatsWhat/EagleVision</a>. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLMs）的进展在各种视觉任务中取得了令人印象深刻的结果。然而，在遥感（RS）领域，高分辨率和小比例的对象对现有的MLLMs构成了挑战，它们在以对象为中心的任务上表现挣扎，特别是在每个对象的精确定位和精细粒度属性描述方面。这些用于遥感的MLLMs尚未超越经典视觉感知模型，因为它们只提供粗略的图像理解，导致在真实场景中的收益有限。为了解决这一差距，我们建立了EagleVision，这是一个针对遥感定制的大型语言模型，擅长对象检测和属性理解。配备了属性分离模块后，EagleVision学习分离视觉令牌以表达不同的属性。为了支持对象级别的视觉语言对齐，我们构建了EVAttrs-95K数据集，这是遥感领域中用于指令调整的第一个大规模对象属性理解数据集，以及一个新的评估基准EVBench。EagleVision在精细粒度对象检测和对象属性理解任务上都实现了最先进的性能，这凸显出在大型语言模型中检测和理解能力之间的相互促进。代码、模型、数据和演示将发布在<a target="_blank" rel="noopener" href="https://github.com/XiangTodayEatsWhat/EagleVision%E3%80%82">https://github.com/XiangTodayEatsWhat/EagleVision。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23330v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>大语言模型在遥感领域面临挑战，如精确定位和精细属性描述等对象中心任务。为此，我们提出了EagleVision模型，并设计了Attribute Disentangle模块以进行精细化对象属性理解。我们建立了EVAttrs-95K数据集和EVBench评估基准以支持对象级视觉语言对齐。EagleVision在精细粒度对象检测和对象属性理解任务上取得了最新性能表现。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>多模态大型语言模型（MLLMs）在遥感（RS）领域的对象中心任务上遇到了挑战，特别是在精确定位和精细属性描述方面。</li>
<li>现有MLLMs在遥感应用中尚未超越经典视觉感知模型，因为它们仅提供粗略的图像理解。</li>
<li>EagleVision是一个针对遥感领域的MLLM，擅长对象检测和属性理解。</li>
<li>Attribute Disentangle模块使EagleVision能够学习解纠缠的视觉令牌以表达不同的属性。</li>
<li>为支持对象级视觉语言对齐，建立了EVAttrs-95K数据集和EVBench评估基准。</li>
<li>EagleVision在精细粒度对象检测和对象属性理解任务上取得了最新性能。</li>
<li>检测和理解能力在MLLMs中相互促进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23330">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-077bae7a0c808d43230fc2f5cd5cb5bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11fe065fbfc6fbb680d2cfc1dfd4ba50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34fa62c5a977cb805a41c67ead9cb834.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf5b31b1640012a150899780328c4cd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f59b1ce761f96019c4008b4aaa3d2c68.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RefChartQA-Grounding-Visual-Answer-on-Chart-Images-through-Instruction-Tuning"><a href="#RefChartQA-Grounding-Visual-Answer-on-Chart-Images-through-Instruction-Tuning" class="headerlink" title="RefChartQA: Grounding Visual Answer on Chart Images through Instruction   Tuning"></a>RefChartQA: Grounding Visual Answer on Chart Images through Instruction   Tuning</h2><p><strong>Authors:Alexander Vogel, Omar Moured, Yufan Chen, Jiaming Zhang, Rainer Stiefelhagen</strong></p>
<p>Recently, Vision Language Models (VLMs) have increasingly emphasized document visual grounding to achieve better human-computer interaction, accessibility, and detailed understanding. However, its application to visualizations such as charts remains under-explored due to the inherent complexity of interleaved visual-numerical relationships in chart images. Existing chart understanding methods primarily focus on answering questions without explicitly identifying the visual elements that support their predictions. To bridge this gap, we introduce RefChartQA, a novel benchmark that integrates Chart Question Answering (ChartQA) with visual grounding, enabling models to refer elements at multiple granularities within chart images. Furthermore, we conduct a comprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across different categories. Our experiments demonstrate that incorporating spatial awareness via grounding improves response accuracy by over 15%, reducing hallucinations, and improving model reliability. Additionally, we identify key factors influencing text-spatial alignment, such as architectural improvements in TinyChart, which leverages a token-merging module for enhanced feature fusion. Our dataset is open-sourced for community development and further advancements. All models and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a>. </p>
<blockquote>
<p>最近，视觉语言模型（VLMs）越来越强调文档的视觉定位，以实现更好的人机交互、可访问性和深入理解。然而，由于其图表图像中交织的视觉数字关系固有的复杂性，其在图表等可视化中的应用仍待探索。现有的图表理解方法主要关注回答问题，而没有明确识别支持其预测的视觉元素。为了弥补这一空白，我们引入了RefChartQA，这是一个将图表问答（ChartQA）与视觉定位相结合的新型基准测试，使模型能够在图表图像内的多个粒度上参考元素。此外，我们通过指令调整了5个不同类别的最先进VLMs进行了全面评估。我们的实验表明，通过定位融入空间意识提高了超过15%的响应准确性，减少了幻觉，提高了模型可靠性。另外，我们还确定了影响文本空间对齐的关键因素，如TinyChart中的架构改进，它利用令牌合并模块增强特征融合。我们的数据集已开源供社区发展和进一步进步。所有模型和代码将在<a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/moured/RefChartQA上公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23131v1">PDF</a> All models and code will be publicly available at   <a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a></p>
<p><strong>Summary</strong>：视觉语言模型（VLMs）正逐渐重视文档视觉定位，以提高人机交互、可访问性和深入理解。然而，由于图表图像中视觉与数值关系的固有复杂性，其在图表等可视化方面的应用仍被忽视。为了弥补这一空白，引入了RefChartQA基准测试，它将图表问答（ChartQA）与视觉定位相结合，使模型能够在图表图像中的多个粒度级别上引用元素。实验表明，通过定位融入空间意识可以提高响应准确性超过15%，减少幻觉并提高模型可靠性。此外，公开的数据集可供社区开发和进一步进展之用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Vision Language Models (VLMs) 强调文档视觉定位的重要性，以增强人机交互和深入理解。</li>
<li>VLMs在图表可视化方面的应用仍处于探索阶段，因为需要处理视觉与数值关系的复杂性。</li>
<li>RefChartQA是一个新的基准测试，结合了图表问答（ChartQA）和视觉定位。</li>
<li>通过融入空间意识，定位可以提高响应准确性超过15%，并减少幻觉。</li>
<li>RefChartQA数据集公开供社区使用，促进进一步的发展。</li>
<li>实验评估了五种最新VLMs的不同类别，显示模型在定位方面的改进对结果有积极影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4198d82bea3dd405c405082c9f941d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b4289ae7d243fcc6ae5abfd779b27c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72193e9cdddf05e9852341443bbfd1ea.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training"><a href="#Unicorn-Text-Only-Data-Synthesis-for-Vision-Language-Model-Training" class="headerlink" title="Unicorn: Text-Only Data Synthesis for Vision Language Model Training"></a>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</h2><p><strong>Authors:Xiaomin Yu, Pengxiang Ding, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, Kejian Wu, Zhaoxin Fan, Ziyue Qiao, Donglin Wang</strong></p>
<p>Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Yu-xm/Unicorn.git">https://github.com/Yu-xm/Unicorn.git</a>. </p>
<blockquote>
<p>训练视觉语言模型（VLM）通常需要大规模、高质量的图文对数据，但收集或合成这样的数据成本很高。相比之下，文本数据非常丰富且价格低廉，这引发了一个问题：是否可以从纯文本合成高质量的多模态训练数据？为解决这一问题，我们提出了一个跨融合的三阶段多模态数据合成框架，该框架生成了两个数据集：Unicorn-1.2M和Unicorn-471K-Instruction。在第一阶段：多样化标题数据合成中，我们通过利用大型语言模型（LLM）扩展稀疏标题种子来构建120万张语义丰富的高质量图片标题。在第二阶段：指令调整数据生成中，我们将47.1万张图片的标题进一步处理成多轮指令调整任务，以支持复杂推理。最后，在第三阶段：模态表示转换中，这些文本标题表示被转换成视觉表示，从而产生多样化的合成图像表示。这三阶段的过程使我们能够构建用于预训练的Unicorn-1.2M和用于指令调整的Unicorn-471K-Instruction，而无需依赖真实图像。通过消除对真实图像的依赖，同时保持数据的质量和多样性，我们的框架为VLMs训练提供了成本低廉且可扩展的解决方案。代码可通过<a target="_blank" rel="noopener" href="https://github.com/Yu-xm/Unicorn.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yu-xm/Unicorn.git获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22655v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种跨融合的三阶段多媒体数据合成框架，用于生成用于视觉语言模型训练的数据集。该框架通过纯文本方式合成高质量的多模态训练数据，分为三个阶段：多样化标题数据合成、指令调整数据生成和模态表示转移。最终构建了用于预训练的Unicorn-1.2M数据集和用于指令调整的Unicorn-471K-Instruction数据集。该框架消除了对真实图像的依赖，同时保持了数据的质量和多样性，为视觉语言模型训练提供了成本效益高且可扩展的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种跨融合的三阶段多媒体数据合成框架，用于生成视觉语言模型训练数据集。</li>
<li>框架分为三个阶段：多样化标题数据合成、指令调整数据生成和模态表示转移。</li>
<li>通过纯文本方式合成高质量的多模态训练数据，无需依赖真实图像。</li>
<li>构建了用于预训练的Unicorn-1.2M数据集和用于指令调整的Unicorn-471K-Instruction数据集。</li>
<li>框架能够保持数据的质量和多样性，同时提高成本效益和可扩展性。</li>
<li>利用大型语言模型（LLMs）进行稀疏标题种子的扩展，生成语义上多样化的高质量标题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b5a6c5dab262e94f2d924815911a4af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1606f982b34f4fedf82ca91a3b18e688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c80cdd04d88de4dc6bdf90eac117cf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b81106d17df4069bd1e3f743a551ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6dd40a40239345a8267f6d75dcf9c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f564bc8a70aca3fa18fce60f6bb5e4f8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Integrating-Artificial-Intelligence-with-Human-Expertise-An-In-depth-Analysis-of-ChatGPT’s-Capabilities-in-Generating-Metamorphic-Relations"><a href="#Integrating-Artificial-Intelligence-with-Human-Expertise-An-In-depth-Analysis-of-ChatGPT’s-Capabilities-in-Generating-Metamorphic-Relations" class="headerlink" title="Integrating Artificial Intelligence with Human Expertise: An In-depth   Analysis of ChatGPT’s Capabilities in Generating Metamorphic Relations"></a>Integrating Artificial Intelligence with Human Expertise: An In-depth   Analysis of ChatGPT’s Capabilities in Generating Metamorphic Relations</h2><p><strong>Authors:Yifan Zhang, Dave Towey, Matthew Pike, Quang-Hung Luu, Huai Liu, Tsong Yueh Chen</strong></p>
<p>Context: This paper provides an in-depth examination of the generation and evaluation of Metamorphic Relations (MRs) using GPT models developed by OpenAI, with a particular focus on the capabilities of GPT-4 in software testing environments.   Objective: The aim is to examine the quality of MRs produced by GPT-3.5 and GPT-4 for a specific System Under Test (SUT) adopted from an earlier study, and to introduce and apply an improved set of evaluation criteria for a diverse range of SUTs.   Method: The initial phase evaluates MRs generated by GPT-3.5 and GPT-4 using criteria from a prior study, followed by an application of an enhanced evaluation framework on MRs created by GPT-4 for a diverse range of nine SUTs, varying from simple programs to complex systems incorporating AI&#x2F;ML components. A custom-built GPT evaluator, alongside human evaluators, assessed the MRs, enabling a direct comparison between automated and human evaluation methods.   Results: The study finds that GPT-4 outperforms GPT-3.5 in generating accurate and useful MRs. With the advanced evaluation criteria, GPT-4 demonstrates a significant ability to produce high-quality MRs across a wide range of SUTs, including complex systems incorporating AI&#x2F;ML components.   Conclusions: GPT-4 exhibits advanced capabilities in generating MRs suitable for various applications. The research underscores the growing potential of AI in software testing, particularly in the generation and evaluation of MRs, and points towards the complementarity of human and AI skills in this domain. </p>
<blockquote>
<p>本文深入探讨了使用OpenAI开发的GPT模型（特别是GPT-4）在软件测试环境中生成和评价Metamorphic Relations (MRs)的方法。文章旨在研究GPT-3.5和GPT-4在特定系统测试（SUT）环境下生成MRs的质量，并为多种SUTs引入和应用了一套改进的评价标准。初始阶段，我们根据先前的研究标准评估了GPT-3.5和GPT-4生成的MRs，然后应用了一个增强的评价框架对GPT-4为九种不同的SUTs生成的MRs进行了评价，这些系统从简单的程序到复杂的包含人工智能&#x2F;机器学习组件的系统不等。我们自定义的GPT评价工具和人类评价者共同对MRs进行了评估，实现了自动化和人类评价方法之间的直接比较。研究发现，GPT-4在生成准确有用的MRs方面优于GPT-3.5。通过先进的评价标准，GPT-4显示出在各种SUTs上生成高质量MRs的显著能力，包括复杂的包含人工智能&#x2F;机器学习组件的系统。这表明GPT-4在各种应用中生成MRs方面表现出高级能力。该研究强调了人工智能在软件测试领域（特别是在生成和评价MRs方面）的巨大潜力，并指出了人类和人工智能技能在这个领域的互补性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22141v1">PDF</a> Submitted to Information and Software Technology</p>
<p><strong>摘要</strong></p>
<p>本文深入探讨了使用OpenAI开发的GPT模型生成和评估形态关系（MRs）的过程，重点研究了GPT-4在软件测试环境中的能力。文章旨在评估GPT-3.5和GPT-4对特定系统（采用早期研究中的系统）生成MRs的质量，并为多种系统引入并应用改进的评价标准。研究方法包括使用早期研究的标准对GPT-3.5和GPT-4生成的MRs进行评估，以及对GPT-4为九种不同系统生成的MRs应用改进的评价框架。结合自定义的GPT评估器和人类评估者，对MRs进行了评估，实现了自动与人类评估方法的直接比较。研究发现，GPT-4在生成准确和有用的MRs方面优于GPT-3.5。使用改进的评价标准，GPT-4显示出在各种系统（包括包含人工智能&#x2F;机器学习组件的复杂系统）中产生高质量MRs的重要能力。本文的结论是，GPT-4在生成适用于各种应用的MRs方面展现了先进的性能，强调人工智能在软件测试领域的潜力，特别是在生成和评估MRs方面。它表明人类和人工智能的技能在这个领域的互补性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>GPT-4在生成形态关系（MRs）方面表现出优于GPT-3.5的性能。</li>
<li>GPT-4能够在各种系统（包括包含AI&#x2F;ML组件的复杂系统）中产生高质量的MRs。</li>
<li>文章通过使用自定义的GPT评估器和人类评估者，实现了自动化和人类评估方法的直接比较。</li>
<li>GPT模型在软件测试领域具有潜力，特别是在生成和评估MRs方面。</li>
<li>研究强调了人工智能在软件测试中的成长潜力。</li>
<li>在生成和评估MRs方面，人类和AI的技能表现出互补性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1765ae94db9731490f1fdaef0938d38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09dc2c7d924adc3bfe8184792ab58592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd2603ac13adbaab2cb9ff1385dfa79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd60bd61fa30ef4778b2b03e8e0b793a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a57e9ab95956c56224cf87b4e9827eec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f029f4adf95c61ec168e3fb6b3cbc4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TEMPLE-Temporal-Preference-Learning-of-Video-LLMs-via-Difficulty-Scheduling-and-Pre-SFT-Alignment"><a href="#TEMPLE-Temporal-Preference-Learning-of-Video-LLMs-via-Difficulty-Scheduling-and-Pre-SFT-Alignment" class="headerlink" title="TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment"></a>TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty   Scheduling and Pre-SFT Alignment</h2><p><strong>Authors:Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun</strong></p>
<p>Video Large Language Models (Video LLMs) have achieved significant success by leveraging a two-stage paradigm: pretraining on large-scale video-text data for vision-language alignment, followed by supervised fine-tuning (SFT) for task-specific capabilities. However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and reliance on the next-token prediction paradigm during training. To address these limitations, we propose TEMPLE (TEMporal Preference Learning), a systematic framework that enhances Video LLMs’ temporal reasoning capabilities through Direct Preference Optimization (DPO). To facilitate this, we introduce an automated preference data generation pipeline that systematically constructs preference pairs by selecting videos that are rich in temporal information, designing video-specific perturbation strategies, and finally evaluating model responses on clean and perturbed video inputs. Our temporal alignment features two key innovations: curriculum learning which that progressively increases perturbation difficulty to improve model robustness and adaptability; and “Pre-SFT Alignment’’, applying preference optimization before instruction tuning to prioritize fine-grained temporal comprehension. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. We further analyze the transferability of DPO data across architectures and the role of difficulty scheduling in optimization. Our findings highlight our TEMPLE as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE">https://github.com/lscpku/TEMPLE</a>. </p>
<blockquote>
<p>视频大语言模型（Video LLMs）通过采用两阶段范式取得了显著的成功：首先在大规模视频文本数据上进行预训练，以实现视觉语言对齐，然后通过监督微调（SFT）获得特定任务的能力。然而，现有方法由于数据中的时间对应关系较弱以及在训练过程中依赖于下一个标记预测范式，因此在时间推理方面遇到了困难。为了解决这个问题，我们提出了TEMPLE（时空偏好学习），这是一个系统框架，它通过直接偏好优化（DPO）增强视频LLM的时间推理能力。为了促进这一点，我们引入了一个自动化偏好数据生成管道，该管道通过选择时间信息丰富的视频、设计针对视频的扰动策略，以及评估模型对干净和扰动视频输入的响应来系统地构建偏好对。我们的时间对齐有两个关键的创新点：课程学习，逐步增加扰动难度以提高模型的鲁棒性和适应性；以及“Pre-SFT对齐”，在指令调整之前应用偏好优化，以优先进行精细的时间理解。大量实验表明，我们的方法在使用相对较少自我生成的DPO数据时，能够在多个基准测试上持续提高视频LLM的性能。我们还分析了DPO数据在不同架构之间的可迁移性以及难度调度在优化中的角色。我们的研究结果表明，TEMPLE可以作为基于SFT的方法的可扩展和高效的补充，为开发可靠的视频LLM铺平了道路。代码可在<a target="_blank" rel="noopener" href="https://github.com/lscpku/TEMPLE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lscpku/TEMPLE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16929v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了TEMPLE框架，通过直接偏好优化（DPO）提升视频大型语言模型（Video LLMs）的时间推理能力。该框架通过自动化偏好数据生成管道，选择富含时间信息的视频、设计视频特定扰动策略，并评估模型对干净和扰动视频输入的响应。引入课程学习机制和“Pre-SFT对齐”，在指令微调之前应用偏好优化，优先提高精细粒度的时间理解能力。实验证明，该方法在多个基准测试上提高了Video LLM的性能，且使用自我生成的DPO数据集相对较小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video LLMs采用两阶段范式：预训练大规模视频文本数据实现视觉语言对齐，然后通过监督微调（SFT）获得特定任务能力。</li>
<li>现有方法面临时间推理困难，因数据中的时间对应关系较弱，且训练时依赖下一个标记预测范式。</li>
<li>TEMPLE框架通过直接偏好优化（DPO）提升Video LLMs的时间推理能力。</li>
<li>TEMPLE引入自动化偏好数据生成管道，包括选择富含时间信息的视频、设计视频特定扰动策略，并评估模型响应。</li>
<li>课程学习机制和“Pre-SFT对齐”是提高模型对时间信息理解的关键创新。</li>
<li>实验证明TEMPLE方法在多个基准测试上提高了Video LLM性能，且使用自我生成的DPO数据集相对较小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16929">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5510be75ebe851a3144f1b3a10d7e22c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e7d12dbbb5194fa383c16648b21567a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be6a348d7181b26b10b8be3bcacdd034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f5aa05b29fc3af80cf08594d4f77c34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abd74dc5d9651e2ca31dfde7f587f11b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning"><a href="#Cosmos-Reason1-From-Physical-Common-Sense-To-Embodied-Reasoning" class="headerlink" title="Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning"></a>Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning</h2><p><strong>Authors: NVIDIA,  :, Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, Jinwei Gu, Siddharth Gururani, Imad El Hanafi, Zekun Hao, Jacob Huffman, Jingyi Jin, Brendan Johnson, Rizwan Khan, George Kurian, Elena Lantz, Nayeon Lee, Zhaoshuo Li, Xuan Li, Tsung-Yi Lin, Yen-Chen Lin, Ming-Yu Liu, Alice Luo, Andrew Mathau, Yun Ni, Lindsey Pavao, Wei Ping, David W. Romero, Misha Smelyanskiy, Shuran Song, Lyne Tchapmi, Andrew Z. Wang, Boxin Wang, Haoxiang Wang, Fangyin Wei, Jiashu Xu, Yao Xu, Xiaodong Yang, Zhuolin Yang, Xiaohui Zeng, Zhe Zhang</strong></p>
<p>Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1">https://github.com/nvidia-cosmos/cosmos-reason1</a>. </p>
<blockquote>
<p>物理人工智能系统需要在物理世界中感知、理解和执行复杂的动作。在本文中，我们介绍了Cosmos-Reason1模型，该模型能够通过一系列长思考过程理解物理世界，并以自然语言生成适当的决策（例如下一步行动）。我们首先定义物理人工智能推理的关键能力，重点关注物理常识和实体推理。为了表示物理常识，我们使用层次本体来捕获关于空间、时间和物理的基本常识。为了进行实体推理，我们依赖于一个二维本体，该本体可以概括不同的物理实体。基于这些能力，我们开发了两个多模态大型语言模型，即Cosmos-Reason1-8B和Cosmos-Reason1-56B。我们在四个阶段对数据进行整理并对模型进行训练：视觉预训练、一般监督微调（SFT）、物理人工智能SFT和物理人工智能强化学习（RL）作为后训练。为了评估我们的模型，我们根据我们的本体论建立了物理常识和实体推理的综合基准测试。评估结果表明，物理人工智能SFT和强化学习带来了显著的改进。为了方便物理人工智能的开发，我们将在NVIDIA Open Model License下提供我们的代码和预训练模型：<a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-reason1">https://github.com/nvidia-cosmos/cosmos-reason1</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15558v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文介绍了针对物理AI系统的研究，重点介绍了Cosmos-Reason1模型。该模型具备理解物理世界并生成适当决策的能力，通过长链思维推理过程实现。论文定义了物理AI推理的关键能力，侧重于物理常识和体现推理。为表达物理常识，采用层次化本体论捕捉关于空间、时间和物理学的根本知识。为体现推理，依赖二维本体论，在不同物理体现中概括共性。基于此，开发了两个多模态大型语言模型Cosmos-Reason1-8B和Cosmos-Reason1-56B。模型训练分四阶段：视觉预训练、一般监督微调、物理AI监督和物理AI强化学习。评估模型时，根据本体论建立了物理常识和体现推理的综合基准测试。结果显示，物理AI监督和强化学习带来显著改进。为方便物理AI的发展，模型和代码将在NVIDIA Open Model License下提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cosmos-Reason1模型具备理解物理世界并生成适当决策的能力，通过长链思维推理过程实现。</li>
<li>论文定义了物理AI推理的关键能力，包括物理常识和体现推理。</li>
<li>采用层次化本体论和二维本体论来代表物理常识和体现推理。</li>
<li>开发了两个多模态大型语言模型Cosmos-Reason1-8B和Cosmos-Reason1-56B。</li>
<li>模型训练包括四个阶段：视觉预训练、一般监督微调、物理AI监督、物理AI强化学习。</li>
<li>评估模型时，建立了基于本体论的物理常识和体现推理的综合基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15558">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5180c62fc77f538fc50442309d0ced5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-856bc8687ff4a4cacb12a7ffe717b60f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ec9f52c985fc47aa47580339277604c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-731456c5726227d06f6f44bf8457ddbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99659305b782ad2bbf56ee38fd36b36c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Post-Training-Quantization-for-Diffusion-Transformer-via-Hierarchical-Timestep-Grouping"><a href="#Post-Training-Quantization-for-Diffusion-Transformer-via-Hierarchical-Timestep-Grouping" class="headerlink" title="Post-Training Quantization for Diffusion Transformer via Hierarchical   Timestep Grouping"></a>Post-Training Quantization for Diffusion Transformer via Hierarchical   Timestep Grouping</h2><p><strong>Authors:Ning Ding, Jing Han, Yuchuan Tian, Chao Xu, Kai Han, Yehui Tang</strong></p>
<p>Diffusion Transformer (DiT) has now become the preferred choice for building image generation models due to its great generation capability. Unlike previous convolution-based UNet models, DiT is purely composed of a stack of transformer blocks, which renders DiT excellent in scalability like large language models. However, the growing model size and multi-step sampling paradigm bring about considerable pressure on deployment and inference. In this work, we propose a post-training quantization framework tailored for Diffusion Transforms to tackle these challenges. We firstly locate that the quantization difficulty of DiT mainly originates from the time-dependent channel-specific outliers. We propose a timestep-aware shift-and-scale strategy to smooth the activation distribution to reduce the quantization error. Secondly, based on the observation that activations of adjacent timesteps have similar distributions, we utilize a hierarchical clustering scheme to divide the denoising timesteps into multiple groups. We further design a re-parameterization scheme which absorbs the quantization parameters into nearby module to avoid redundant computations. Comprehensive experiments demonstrate that out PTQ method successfully quantize the Diffusion Transformer into 8-bit weight and 8-bit activation (W8A8) with state-of-the-art FiD score. And our method can further quantize DiT model into 4-bit weight and 8-bit activation (W4A8) without sacrificing generation quality. </p>
<blockquote>
<p>扩散转换器（DiT）由于其强大的生成能力，现已成为构建图像生成模型的首选。与之前的基于卷积的UNet模型不同，DiT完全由一堆转换器块组成，这使得DiT在可扩展性方面表现出色，就像大型语言模型一样。然而，不断增长的模型大小和多步采样范式给部署和推理带来了巨大的压力。在这项工作中，我们针对扩散转换提出了一种后训练量化框架，以解决这些挑战。我们首先发现DiT的量化难度主要源于时间依赖的特定通道异常值。我们提出了一种时间感知移位和缩放策略，以平滑激活分布，从而减少量化误差。其次，基于相邻时间步的激活具有相似分布的观测，我们采用分层聚类方案将去噪时间步分为多个组。我们进一步设计了一种重新参数化方案，将量化参数吸收到附近的模块中，以避免冗余计算。综合实验表明，我们的PTQ方法成功地将扩散转换器量化到8位权重和8位激活（W8A8），具有最先进的FID分数。我们的方法还可以将DiT模型进一步量化到4位权重和8位激活（W4A8），而不会牺牲生成质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06930v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了针对扩散变换器（DiT）的定制后训练量化框架，解决了模型部署和推理中的压力问题。研究发现，DiT模型的量化难度主要来源于时序特定的通道异常值。针对此问题，本文提出了一个时序感知移位缩放策略来平滑激活分布以降低量化误差。此外，利用相邻时序激活分布的相似性进行层次聚类分组，设计了参数化方案以避免冗余计算。实验表明，该量化方法可将扩散变换器模型成功量化为权重和激活均为8位的模型（W8A8），且FID得分领先。同时，该方法还能将DiT模型进一步量化为权重为4位、激活为8位的模型（W4A8），且不影响生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformer (DiT)已成为图像生成模型的优选方案，其强大的生成能力得到广泛认可。</li>
<li>DiT面临模型规模扩大和多步采样带来的部署和推理压力。</li>
<li>针对DiT的量化难度，研究发现主要源于时序特定的通道异常值。</li>
<li>提出时序感知移位缩放策略以平滑激活分布，降低量化误差。</li>
<li>利用相邻时序激活分布的相似性进行层次聚类分组。</li>
<li>设计了避免冗余计算的参数化方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-35c1af07217c138c342caba80cf219d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30473515ddbdf7544074ddf20d791574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ded74726a85b45af236fb39b7e3562b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc52c37b37e796b0e2deafe36b380c57.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Forgetting-Transformer-Softmax-Attention-with-a-Forget-Gate"><a href="#Forgetting-Transformer-Softmax-Attention-with-a-Forget-Gate" class="headerlink" title="Forgetting Transformer: Softmax Attention with a Forget Gate"></a>Forgetting Transformer: Softmax Attention with a Forget Gate</h2><p><strong>Authors:Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville</strong></p>
<p>An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer’s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a “Pro” block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer">https://github.com/zhixuan-lin/forgetting-transformer</a>. </p>
<blockquote>
<p>现代循环序列模型的一个重要组成部分是遗忘门。虽然Transformer没有明确的递归形式，我们展示了一种通过数据相关的方式降低未归一化的注意力分数，自然地将遗忘门纳入Transformer的方法。我们将这种注意力机制命名为遗忘注意力，并将得到的模型命名为遗忘转换器（FoX）。我们展示FoX在长上下文语言建模、长度扩展和短上下文下游任务上的性能优于Transformer，同时在长上下文下游任务上的表现与Transformer相当。此外，它与FlashAttention算法兼容，不需要任何位置嵌入。包括“海底捞针”测试在内的几项分析表明，FoX在保持Transformer在长上下文方面的优势的同时，也优于循环序列模型如Mamba-2、HGRN2和DeltaNet等。我们还引入了一种“专业”块设计，它结合了循环序列模型中的一些常见架构组件，发现它能显著提高FoX和Transformer的性能。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/zhixuan-lin/forgetting-transformer%E3%80%82">https://github.com/zhixuan-lin/forgetting-transformer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02130v2">PDF</a> Published as a conference paper at ICLR 2025; Fixed an issue with the   attention map visualization</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在Transformer模型中引入遗忘门（Forget Gate）的机制，称为遗忘注意力（Forgetting Attention），并由此提出了遗忘变压器（FoX）模型。FoX模型在长文本语言建模、长度扩展和短文本下游任务上优于Transformer，同时在长文本下游任务上的表现与Transformer相当。此外，FoX与FlashAttention算法兼容，无需位置嵌入。分析和测试显示，FoX保留了Transformer在长文本上下文方面的优势，超越了某些递归序列模型。同时，本文还引入了一种结合递归序列模型常见架构组件的“Pro”块设计，显著提高了FoX和Transformer的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遗忘门机制被自然地融入到Transformer模型中，通过数据相关的方式对未标准化的注意力分数进行下权重处理。</li>
<li>提出的FoX模型在长文本语言建模、长度扩展和短文本下游任务上表现出优于Transformer的性能。</li>
<li>FoX模型与FlashAttention算法兼容，且不需要使用位置嵌入。</li>
<li>分析表明，FoX模型保留了Transformer在长文本上下文方面的优势。</li>
<li>与某些递归序列模型相比，FoX模型表现更优。</li>
<li>引入的“Pro”块设计结合了递归序列模型的常见架构组件，显著提高了FoX和Transformer的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02130">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4cab32a20f130fbe4b5184bdf16a322e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66cdcb70392f3a49eba2eef23bd0e870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc12c8258b772edb4bb7df66467166af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e012daee8b9ff118bcc6a15e73ebb43a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Code-Generation-A-Comprehensive-Survey-of-Challenges-Techniques-Evaluation-and-Applications"><a href="#Large-Language-Models-for-Code-Generation-A-Comprehensive-Survey-of-Challenges-Techniques-Evaluation-and-Applications" class="headerlink" title="Large Language Models for Code Generation: A Comprehensive Survey of   Challenges, Techniques, Evaluation, and Applications"></a>Large Language Models for Code Generation: A Comprehensive Survey of   Challenges, Techniques, Evaluation, and Applications</h2><p><strong>Authors:Nam Huynh, Beiyu Lin</strong></p>
<p>Large Language Models (LLMs) have demonstrated their remarkable capabilities in numerous fields. This survey focuses on how LLMs empower users, regardless of their technical background, to use human languages to automatically generate executable code. We begin with understanding LLMs’ limitations and challenges in automated code generation. Subsequently, we review various fine-tuning techniques designed to enhance both the performance and adaptability of LLMs in code generation tasks. We then review the existing metrics and benchmarks for evaluations to assess model performance based on fine-tuning techniques. Finally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot, ToolGen) in code generation tasks to illustrate their roles and functionalities. This survey provides a comprehensive overview of LLMs for code generation, helps researchers in diverse fields better understand the current state-of-the-art technologies, and offers the potential of effectively leveraging LLMs for code generation tasks. </p>
<blockquote>
<p>大型语言模型（LLM）已在多个领域展示了其卓越的能力。这篇综述的重点是，无论用户的技术背景如何，LLM如何赋能用户使用自然语言自动生成可执行代码。我们首先了解LLM在自动代码生成方面的局限性和挑战。随后，我们回顾了各种微调技术，旨在提高LLM在代码生成任务中的性能和适应性。然后我们回顾了基于微调技术的评估现有指标和基准测试，以评估模型性能。最后，我们探讨了LLM（例如CodeLlama、GitHub Copilot、ToolGen）在代码生成任务中的应用，以说明它们的作用和功能。这篇综述为代码生成领域的LLM提供了全面的概述，有助于不同领域的研究者更好地了解当前最新技术，并提供了有效利用LLM进行代码生成任务的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01245v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多个领域展现了卓越的能力，特别是在赋能用户利用自然语言自动生成可执行代码方面。本综述先探讨LLMs在自动化代码生成方面的局限和挑战，再评述各种增强模型性能和适应性的微调技术，并评价基于这些技术的模型性能评估指标和基准测试。最后，展示LLMs在代码生成任务中的应用实例（如CodeLlama、GitHub Copilot、ToolGen等），帮助不同领域的研究者了解最新技术，并为有效利用LLMs进行代码生成任务提供潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已在多个领域展现出强大的能力，特别是在用户利用自然语言自动生成代码方面。</li>
<li>LLMs在自动化代码生成方面存在局限和挑战，需要研究有效的微调技术来提高性能和适应性。</li>
<li>存在多种评估LLMs在代码生成任务中性能的指标和基准测试。</li>
<li>LLMs在代码生成任务中的应用实例包括CodeLlama、GitHub Copilot和ToolGen等。</li>
<li>这些应用展示了LLMs在赋能用户、提高开发效率和促进自动化方面的巨大潜力。</li>
<li>综合评述LLMs在代码生成方面的应用有助于研究者了解当前最新技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01245">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-599bd4eb94fa35c6535af0f741fb746d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcfa68b83c55f05263f04ec907f884de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45c381107acb9268bc201c143c4e5d17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc9fd147a4d00a90c4a26ddcdbb51f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb59751a08d47be98740267103c57d83.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation"><a href="#Lost-in-Sequence-Do-Large-Language-Models-Understand-Sequential-Recommendation" class="headerlink" title="Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?"></a>Lost in Sequence: Do Large Language Models Understand Sequential   Recommendation?</h2><p><strong>Authors:Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park</strong></p>
<p>Large Language Models (LLMs) have recently emerged as promising tools for recommendation thanks to their advanced textual understanding ability and context-awareness. Despite the current practice of training and evaluating LLM-based recommendation (LLM4Rec) models under a sequential recommendation scenario, we found that whether these models understand the sequential information inherent in users’ item interaction sequences has been largely overlooked. In this paper, we first demonstrate through a series of experiments that existing LLM4Rec models do not fully capture sequential information both during training and inference. Then, we propose a simple yet effective LLM-based sequential recommender, called LLM-SRec, a method that enhances the integration of sequential information into LLMs by distilling the user representations extracted from a pre-trained CF-SRec model into LLMs. Our extensive experiments show that LLM-SRec enhances LLMs’ ability to understand users’ item interaction sequences, ultimately leading to improved recommendation performance. Furthermore, unlike existing LLM4Rec models that require fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by training only a few lightweight MLPs, highlighting its practicality in real-world applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec">https://github.com/Sein-Kim/LLM-SRec</a>. </p>
<blockquote>
<p>大型语言模型（LLM）由于其先进的文本理解能力和上下文意识，最近被公认为推荐工具中的有前途的工具。尽管当前在顺序推荐场景下训练和评估基于LLM的推荐（LLM4Rec）模型，但我们发现这些模型是否理解用户项目交互序列中固有的顺序信息却被大大忽视了。在本文中，我们首先通过一系列实验证明，现有的LLM4Rec模型在训练和推理过程中都没有完全捕获顺序信息。然后，我们提出了一种简单有效的基于LLM的顺序推荐器，称为LLM-SRec。这是一种通过蒸馏从预训练的CF-SRec模型中提取的用户表示到LLM中，增强顺序信息融入LLM的方法。我们的大量实验表明，LLM-SRec增强了LLM理解用户项目交互序列的能力，最终提高了推荐性能。此外，与现有的需要微调LLM的LLM4Rec模型不同，LLM-SRec仅通过训练一些轻量级的MLP就实现了最先进的性能，这凸显了其在现实世界应用中的实用性。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/Sein-Kim/LLM-SRec%E3%80%82">https://github.com/Sein-Kim/LLM-SRec。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13909v3">PDF</a> </p>
<p><strong>Summary</strong><br>LLMs用于推荐系统具有先进文本理解和语境感知能力，但现有LLM4Rec模型忽略了对用户物品交互序列中的顺序信息的理解。本文提出一种简单有效的基于LLM的顺序推荐器LLM-SRec，通过蒸馏预训练CF-SRec模型中的用户表示到LLMs中，增强了对顺序信息的整合能力，提高推荐性能，且只需训练少量轻量级MLPs，实用性较高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在推荐系统中展现出强大的文本理解和语境感知能力。</li>
<li>现有LLM4Rec模型忽略了用户物品交互序列中的顺序信息。</li>
<li>本文通过实验证明现有LLM4Rec模型在训练和推理过程中未能充分捕捉顺序信息。</li>
<li>提出一种基于LLM的顺序推荐器LLM-SRec，能有效整合顺序信息。</li>
<li>LLM-SRec通过蒸馏预训练CF-SRec模型中的用户表示到LLMs中，增强了理解用户物品交互序列的能力。</li>
<li>LLM-SRec提高了推荐性能，达到最新水平。</li>
<li>LLM-SRec只需训练少量轻量级MLPs，具有实用性，适用于真实世界应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4fb78631dc36d262d46bbb6a52d0db77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62a650a40c6af80db5555293e00112d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa1307b97ddd357ce63767b177741ceb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac00f70619628c5d1e3f8b7c93cccaf6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis"><a href="#Quantifying-the-Capability-Boundary-of-DeepSeek-Models-An-Application-Driven-Performance-Analysis" class="headerlink" title="Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis"></a>Quantifying the Capability Boundary of DeepSeek Models: An   Application-Driven Performance Analysis</h2><p><strong>Authors:Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, Ning Wang, Meijuan An, Qingliang Meng, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian</strong></p>
<p>DeepSeek-R1, known for its low training cost and exceptional reasoning capabilities, has achieved state-of-the-art performance on various benchmarks. However, detailed evaluations for DeepSeek Series models from the perspective of real-world applications are lacking, making it challenging for users to select the most suitable DeepSeek models for their specific needs. To address this gap, we conduct a systematic evaluation of the DeepSeek-V3, DeepSeek-R1, DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, their corresponding 4-bit quantized models, and the reasoning model QwQ-32B using the enhanced A-Eval benchmark, A-Eval-2.0. Through a comparative analysis of original instruction-tuned models and their distilled counterparts, we investigate how reasoning enhancements impact performance across diverse practical tasks. To assist users in model selection, we quantify the capability boundary of DeepSeek models through performance tier classifications. Based on the quantification results, we develop a model selection handbook that clearly illustrates the relation among models, their capabilities and practical applications. This handbook enables users to select the most cost-effective models without efforts, ensuring optimal performance and resource efficiency in real-world applications. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmarks and periodically update this paper to provide more comprehensive and accurate evaluation results. Please refer to the latest version of the paper for the most current results and conclusions. </p>
<blockquote>
<p>DeepSeek-R1以其低训练成本和出色的推理能力而著称，已在各种基准测试上达到了最先进的性能。然而，针对DeepSeek系列模型从现实应用角度的详细评估仍然缺乏，这使得用户难以为其特定需求选择最合适的DeepSeek模型。为了弥补这一空白，我们对DeepSeek-V3、DeepSeek-R1、DeepSeek-R1-Distill-Qwen系列、DeepSeek-R1-Distill-Llama系列、其对应的4位量化模型以及推理模型QwQ-32B进行了系统的评估，使用的是增强的A-Eval基准测试，即A-Eval-2.0。通过对原始指令调整模型与其蒸馏对应物的比较分析，我们研究了推理增强如何影响各种实际任务上的性能。为了协助用户进行模型选择，我们通过性能分级分类量化了DeepSeek模型的能力边界。基于量化结果，我们开发了模型选择手册，该手册清晰地说明了各模型之间的关系、其能力和实际应用。该手册使用户能够轻松选择最具成本效益的模型，确保在真实应用中的最佳性能和资源效率。值得注意的是，尽管我们努力建立全面、客观、权威的评价基准，但测试样本的选择、数据分布的特征以及评价标准的设定不可避免地会给评价结果带来一定的偏差。我们将不断优化评估基准，并定期更新本文，以提供更全面和准确的评估结果。最新的结果和结论请参考本文的最新版本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11164v4">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>DeepSeek-R1模型以其低训练成本与出色的推理能力而知名，在各种基准测试中表现卓越。然而，关于DeepSeek系列模型在真实世界应用中的详细评估仍然不足，使得用户难以选择最适合自身需求的DeepSeek模型。为解决此问题，我们对DeepSeek-V3、DeepSeek-R1、DeepSeek-R1-Distill-Qwen系列、DeepSeek-R1-Distill-Llama系列、其对应的4位量化模型以及推理模型QwQ-32B进行了系统性的评估，采用了增强的A-Eval基准测试，即A-Eval-2.0。我们比较分析了原始指令调整模型与其蒸馏对应物，探讨推理增强如何影响各种实际任务的性能。为了帮助用户选择模型，我们通过性能分级量化了DeepSeek模型的能力边界，并据此开发了模型选择手册，清晰地说明了各模型之间的关系、其能力和实际应用。该手册旨在帮助用户轻松选择最具成本效益的模型，确保在真实世界应用中实现最佳性能和资源效率。需要指出的是，尽管我们努力建立全面、客观、权威的评估基准，但测试样本的选择、数据分布的特征以及评估标准的设定可能会给评估结果带来一定的偏差。我们将持续优化评估基准并定期更新本文，以提供更全面、准确的评估结果。最新结果和结论请参阅本文最新版本。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DeepSeek-R1模型在多种基准测试中表现卓越，但其在实际应用中的性能评估仍然不足。</li>
<li>通过对DeepSeek系列模型的全面评估，采用了增强的A-Eval基准测试，即A-Eval-2.0。</li>
<li>比较分析了原始指令调整模型与蒸馏模型的推理能力，展示了其在实际任务中的性能差异。</li>
<li>通过性能分级量化模型能力边界，为用户提供更清晰的模型选择依据。</li>
<li>开发了模型选择手册，帮助用户轻松选择最适合其需求的成本效益高的模型。</li>
<li>评估过程中存在的偏差主要源于测试样本选择、数据分布特征以及评估标准设定。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11164">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c777be450e91965811e9692983405e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-610ca94921a22b69af95da15ac53b01e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac7f7a774c0bcd5382ddb0ddce07ac34.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation"><a href="#TOMG-Bench-Evaluating-LLMs-on-Text-based-Open-Molecule-Generation" class="headerlink" title="TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation"></a>TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation</h2><p><strong>Authors:Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li</strong></p>
<p>In this paper, we propose Text-based Open Molecule Generation Benchmark (TOMG-Bench), the first benchmark to evaluate the open-domain molecule generation capability of LLMs. TOMG-Bench encompasses a dataset of three major tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom). Each major task further contains three subtasks, while each subtask comprises 5,000 test samples. Given the inherent complexity of open molecule generation evaluation, we also developed an automated evaluation system that helps measure both the quality and the accuracy of the generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the current limitations as well as potential areas for improvement in text-guided molecule discovery. Furthermore, we propose OpenMolIns, a specialized instruction tuning dataset established for solving challenges raised by TOMG-Bench. Fine-tuned on OpenMolIns, Llama3.1-8B could outperform all the open-source general LLMs, even surpassing GPT-3.5-turbo by 46.5% on TOMG-Bench. Our codes and datasets are available through <a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench">https://github.com/phenixace/TOMG-Bench</a>. </p>
<blockquote>
<p>本文提出了基于文本的开放分子生成基准测试（TOMG-Bench），这是第一个评估大型语言模型在开放域分子生成能力方面的基准测试。TOMG-Bench包含三大任务的数据集：分子编辑（MolEdit）、分子优化（MolOpt）和定制分子生成（MolCustom）。每个主要任务还包括三个子任务，每个子任务包含5000个测试样本。考虑到开放分子生成评价的固有复杂性，我们还开发了一个自动评估系统，帮助衡量生成分子的质量和准确性。我们对25个大型语言模型进行的全面基准测试揭示了当前文本指导分子发现方面的局限性以及潜在的改进领域。此外，我们提出了OpenMolIns，这是一个专门为解决TOMG-Bench提出的挑战而建立的专业指令调整数据集。在OpenMolIns上微调的Llama3.1-8B能够超越所有开源的大型语言模型，在TOMG-Bench上的表现甚至超过了GPT-3.5 Turbo的46.5%。我们的代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/phenixace/TOMG-Bench获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14642v2">PDF</a> The first benchmark for text-based open molecule generation</p>
<p><strong>Summary</strong>：</p>
<p>本文提出了基于文本的开放分子生成基准测试（TOMG-Bench），这是第一个评估大型语言模型（LLMs）在开放域分子生成能力方面的基准测试。TOMG-Bench包括三个主要任务：分子编辑（MolEdit）、分子优化（MolOpt）和定制分子生成（MolCustom）。每个主要任务包含三个子任务，每个子任务包含5000个测试样本。为评估生成的分子质量和准确性，还开发了自动化评估系统。对25个LLMs的全面基准测试揭示了文本引导分子发现领域的当前局限性和潜在改进方向。此外，为应对TOMG-Bench提出的挑战，还推出了OpenMolIns专用指令调整数据集。在OpenMolIns上经过微调的Llama3.1-8B性能超越所有开源通用LLMs，在TOMG-Bench上的表现比GPT-3.5-turbo高出46.5%。相关代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/phenixace/TOMG-Bench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/phenixace/TOMG-Bench获取。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了基于文本的开放分子生成基准测试（TOMG-Bench），用于评估LLMs在开放域分子生成方面的能力。</li>
<li>TOMG-Bench包含三个主要任务：分子编辑、分子优化和定制分子生成。</li>
<li>开发了自动化评估系统，以衡量生成分子的质量和准确性。</li>
<li>对25个LLMs的基准测试揭示了文本引导分子发现领域的局限性和潜在改进方向。</li>
<li>推出了OpenMolIns专用指令调整数据集，以应对TOMG-Bench的挑战。</li>
<li>Llama3.1-8B在OpenMolIns上经过微调后，性能超越其他LLMs，在TOMG-Bench上的表现显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14642">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8130a767c60f10a723bccdc370f1acf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8257366b714dcbec0abe5e65531e38b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b237bef2e1a2b0be7e8e9e171c9b14a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a02607bd2028b26e161245a43483d6c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VERA-Explainable-Video-Anomaly-Detection-via-Verbalized-Learning-of-Vision-Language-Models"><a href="#VERA-Explainable-Video-Anomaly-Detection-via-Verbalized-Learning-of-Vision-Language-Models" class="headerlink" title="VERA: Explainable Video Anomaly Detection via Verbalized Learning of   Vision-Language Models"></a>VERA: Explainable Video Anomaly Detection via Verbalized Learning of   Vision-Language Models</h2><p><strong>Authors:Muchao Ye, Weiyang Liu, Pan He</strong></p>
<p>The rapid advancement of vision-language models (VLMs) has established a new paradigm in video anomaly detection (VAD): leveraging VLMs to simultaneously detect anomalies and provide comprehendible explanations for the decisions. Existing work in this direction often assumes the complex reasoning required for VAD exceeds the capabilities of pretrained VLMs. Consequently, these approaches either incorporate specialized reasoning modules during inference or rely on instruction tuning datasets through additional training to adapt VLMs for VAD. However, such strategies often incur substantial computational costs or data annotation overhead. To address these challenges in explainable VAD, we introduce a verbalized learning framework named VERA that enables VLMs to perform VAD without model parameter modifications. Specifically, VERA automatically decomposes the complex reasoning required for VAD into reflections on simpler, more focused guiding questions capturing distinct abnormal patterns. It treats these reflective questions as learnable parameters and optimizes them through data-driven verbal interactions between learner and optimizer VLMs, using coarsely labeled training data. During inference, VERA embeds the learned questions into model prompts to guide VLMs in generating segment-level anomaly scores, which are then refined into frame-level scores via the fusion of scene and temporal contexts. Experimental results on challenging benchmarks demonstrate that the learned questions of VERA are highly adaptable, significantly improving both detection performance and explainability of VLMs for VAD. </p>
<blockquote>
<p>视觉语言模型（VLMs）的快速发展为视频异常检测（VAD）建立了一种新的范式：利用VLMs同时检测异常并为决策提供可理解的解释。现有工作往往假设VAD所需的复杂推理超出了预训练VLMs的能力。因此，这些方法要么在推理过程中融入了专门的推理模块，要么通过额外的训练依赖于指令调整数据集来适应VLMs进行VAD。然而，这些策略通常会导致巨大的计算成本或数据注释开销。为了应对可解释VAD中的这些挑战，我们引入了一种名为VERA的言语化学习框架，它使VLMs能够执行VAD而无需修改模型参数。具体来说，VERA自动将VAD所需的复杂推理分解为对更简单、更集中的引导问题的反思，捕捉不同的异常模式。它将这些反思问题视为可学习的参数，通过学习者与优化器VLMs之间的数据驱动言语交互来优化它们，使用粗略标记的训练数据。在推理过程中，VERA将学习到的问题嵌入到模型提示中，以指导VLMs生成分段级别的异常分数，然后通过场景和时态上下文的融合将这些分数细化为帧级别分数。在具有挑战性的基准测试上的实验结果表明，VERA学习的问题高度自适应，显著提高了VLMs在VAD方面的检测性能和解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01095v3">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉语言模型（VLMs）在视频异常检测（VAD）中的新应用模式。传统方法常常需要额外的推理模块或指令调整数据集来适应VAD需求，这带来了大量的计算成本或数据标注负担。为解决这些问题，本文提出了一种名为VERA的言语化学习框架，使VLMs能够在无需修改模型参数的情况下进行VAD。VERA通过数据驱动的言语交互，将复杂的VAD推理分解为对简单指导问题的反思，进而优化这些反思问题。在推理过程中，VERA将学习的问题嵌入模型提示中，指导VLMs生成分段级别的异常分数，并通过场景和时间上下文的融合，将其细化为帧级别的分数。实验结果表明，VERA学习的问题具有高度适应性，显著提高了VLMs在VAD中的检测性能和解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在视频异常检测（VAD）中建立了新的应用模式。</li>
<li>现有方法常常需要额外的推理模块或指令调整数据集，带来计算成本和数据标注负担。</li>
<li>VERA框架使VLMs无需修改模型参数即可进行VAD。</li>
<li>VERA通过数据驱动的言语交互自动分解复杂的VAD推理，并优化反映问题。</li>
<li>VERA将学习的问题嵌入模型提示中，指导生成分段级别的异常分数。</li>
<li>通过场景和时间上下文的融合，VERA将异常分数细化为帧级别的分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c85e22d8b5f135a92c0d541cbeb815c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908c2ac23a100ac670b04d06d85c32aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08761fc33c0544f59ac03c00a0401b20.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Human-Motion-Instruction-Tuning"><a href="#Human-Motion-Instruction-Tuning" class="headerlink" title="Human Motion Instruction Tuning"></a>Human Motion Instruction Tuning</h2><p><strong>Authors:Lei Li, Sen Jia, Jianhao Wang, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang</strong></p>
<p>This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the model’s ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction. Our code and models are available on the project website: <a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo">https://github.com/ILGLJ/LLaMo</a>. </p>
<blockquote>
<p>本文介绍了LLaMo（大型语言和人类运动助理），这是一个用于人类运动指令调整的多模式框架。与传统的将非语言输入（如视频或运动序列）转换为语言标记的指令调整方法不同，LLaMo以原始形式保留运动来进行指令调整。这种方法保留了运动特定的细节，这些细节在标记化时往往会减少，从而提高了模型解释复杂人类行为的能力。通过同时处理视频和运动数据以及文本输入，LLaMo实现了以人为中心的分析灵活性。在人类行为和专业活动等高复杂度领域的实验评估表明，LLaMo有效地捕获了特定领域的知识，提高了运动密集型场景中的理解和预测能力。我们希望LLaMo能为从体育分析到行为预测具有广泛应用领域的未来多模式AI系统提供基础。我们的代码和模型可在项目网站上获得：<a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo%E3%80%82">https://github.com/ILGLJ/LLaMo。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16805v4">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>LLaMo框架是一种用于人类运动指令调整的多模态框架，能够保持运动的本体形式进行指令调整，提升模型对人类复杂行为的解读能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaMo是一种多模态框架，用于人类运动指令调整。</li>
<li>与传统将非语言输入转换为语言标记的指令调整方法不同，LLaMo保持运动的本体形式进行指令调整。</li>
<li>LLaMo通过处理视频和运动数据与文本输入，实现了灵活、以人类为中心的分析。</li>
<li>LLaMo能有效捕捉高复杂度领域的特定知识，提升在运动密集型场景中的理解和预测能力。</li>
<li>LLaMo框架在诸如体育分析和行为预测等领域具有广泛的应用前景。</li>
<li>LLaMo的代码和模型已发布在项目的网站上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16805">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-54a053ff97423f4f3b0a52bfe32db42a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51b1f741f7ed474b7e200c54b63fd0a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68d11099a7db5bf896a993681ba04562.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Is-‘Right’-Right-Enhancing-Object-Orientation-Understanding-in-Multimodal-Large-Language-Models-through-Egocentric-Instruction-Tuning"><a href="#Is-‘Right’-Right-Enhancing-Object-Orientation-Understanding-in-Multimodal-Large-Language-Models-through-Egocentric-Instruction-Tuning" class="headerlink" title="Is ‘Right’ Right? Enhancing Object Orientation Understanding in   Multimodal Large Language Models through Egocentric Instruction Tuning"></a>Is ‘Right’ Right? Enhancing Object Orientation Understanding in   Multimodal Large Language Models through Egocentric Instruction Tuning</h2><p><strong>Authors:Ji Hyeok Jung, Eun Tae Kim, Seoyeon Kim, Joo Ho Lee, Bumsoo Kim, Buru Chang</strong></p>
<p>Multimodal large language models (MLLMs) act as essential interfaces, connecting humans with AI technologies in multimodal applications. However, current MLLMs face challenges in accurately interpreting object orientation in images due to inconsistent orientation annotations in training data, hindering the development of a coherent orientation understanding. To overcome this, we propose egocentric instruction tuning, which aligns MLLMs’ orientation understanding with the user’s perspective, based on a consistent annotation standard derived from the user’s egocentric viewpoint. We first generate egocentric instruction data that leverages MLLMs’ ability to recognize object details and applies prior knowledge for orientation understanding. Using this data, we perform instruction tuning to enhance the model’s capability for accurate orientation interpretation. In addition, we introduce EgoOrientBench, a benchmark that evaluates MLLMs’ orientation understanding across three tasks using images collected from diverse domains. Experimental results on this benchmark show that egocentric instruction tuning significantly improves orientation understanding without compromising overall MLLM performance. The instruction data and benchmark dataset are available on our project page at <a target="_blank" rel="noopener" href="https://github.com/jhCOR/EgoOrientBench">https://github.com/jhCOR/EgoOrientBench</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）作为人类与多模态应用程序中的AI技术之间的重要接口。然而，由于训练数据中的方向标注不一致，当前的MLLMs在准确解释图像中的对象方向方面面临挑战，这阻碍了连贯的方向理解的发展。为了克服这一问题，我们提出了以自我为中心（第一人称视角）的指令调整方法，它将MLLMs的方向理解与用户的视角对齐，基于从用户的自我中心视角得出的一致标注标准。我们首先生成利用MLLMs识别对象细节和应用方向理解的先验知识的自我中心指令数据。使用这些数据，我们执行指令调整，以增强模型对准确方向解释的能力。此外，我们还介绍了EgoOrientBench基准测试，该基准测试通过从多个领域收集的图像对MLLMs的方向理解能力进行评估，包括三个任务。在此基准测试上的实验结果表明，以自我为中心的指令调整在不影响MLLM整体性能的前提下，显著提高了方向理解能力。指令数据和基准数据集可在我们的项目页面<a target="_blank" rel="noopener" href="https://github.com/jhCOR/EgoOrientBench%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jhCOR/EgoOrientBench上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16761v2">PDF</a> CVPR2025 Camera-ready</p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型（MLLMs）作为人类与AI技术之间的关键接口，在多模态应用中扮演着重要角色。然而，由于训练数据中方位标注的不一致性，MLLMs在准确解释图像中的对象方位方面存在挑战，制约了其方位理解的连贯性发展。为解决这个问题，我们提出了以用户视角为中心的指令调整方法，基于用户视角的一致标注标准来对齐MLLMs的方位理解。我们首先生成以用户为中心的指令数据，利用MLLMs识别对象细节的能力，并应用方位理解的先验知识。利用这些数据，我们进行指令调整，提高模型准确解释方位的能力。此外，我们还介绍了EgoOrientBench基准测试，该测试在三个任务中评估MLLMs的方位理解，使用了来自不同领域的图像。在EgoOrientBench上的实验结果表明，以用户视角为中心的指令调整在显著提高方位理解的同时，不损害MLLM的整体性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在多模态应用中作为人类与AI之间的接口，具有关键作用。</li>
<li>训练数据中方位标注的不一致性影响了MLLMs准确解释图像中的对象方位。</li>
<li>提出以用户视角为中心的指令调整方法，以改善MLLMs的方位理解连贯性。</li>
<li>利用生成的以用户为中心的指令数据，结合MLLMs的识别能力和先验知识。</li>
<li>通过指令调整提高模型准确解释方位的能力。</li>
<li>引入EgoOrientBench基准测试，评估MLLMs在不同任务中的方位理解性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5e11583df710bcd80abc68f5880408e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73db525671778fe6dab339072b95eae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36513dbb0d6fe6e72ecb552459971320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07299a7377b10387a1ed8159e85e3f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ed62787356adf4a2eb506a7681e700.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MC-LLaVA-Multi-Concept-Personalized-Vision-Language-Model"><a href="#MC-LLaVA-Multi-Concept-Personalized-Vision-Language-Model" class="headerlink" title="MC-LLaVA: Multi-Concept Personalized Vision-Language Model"></a>MC-LLaVA: Multi-Concept Personalized Vision-Language Model</h2><p><strong>Authors:Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang</strong></p>
<p>Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies investigate VLM personalization to understand user-provided concepts. However, they mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes the first multi-concept personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the costs related to joint training, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location confidence maps for enhanced recognition and grounding capabilities. To advance multi-concept personalization research, we further contribute a high-quality instruction tuning dataset. We carefully collect images with multiple characters and objects from movies and manually generate question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive qualitative and quantitative experiments demonstrate that MC-LLaVA can achieve impressive multi-concept personalized responses, paving the way for VLMs to become better user-specific assistants. The code and dataset will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/arctanxarc/MC-LLaVA">https://github.com/arctanxarc/MC-LLaVA</a>. </p>
<blockquote>
<p>当前的语言视觉模型（VLMs）在多种任务中表现出卓越的能力，如视觉问答。为了提升用户体验，近期的研究致力于对VLM进行个性化，以理解用户提供的概念。然而，它们主要关注单一概念的个性化，忽略了多个概念的存在和相互作用，这限制了其在现实世界中的应用性。本文提出了首个多概念个性化范式MC-LLaVA。具体来说，MC-LLaVA采用了一种多概念指令调整策略，能够在单个训练步骤中有效地集成多个概念。为了降低联合训练的成本，我们提出了一种个性化的文本提示，利用视觉令牌信息来初始化概念令牌。此外，我们在推理过程中引入了个性化的视觉提示，通过汇总位置置信图来增强识别和接地能力。为了推动多概念个性化研究的发展，我们进一步贡献了一个高质量的指令调整数据集。我们从电影中精心收集了含有多个角色和对象的图像，并手动为多个概念场景生成问答样本，具有出色的多样性。综合的定性和定量实验表明，MC-LLaVA可以实现令人印象深刻的多概念个性化响应，为VLMs成为更好的用户特定助手铺平了道路。代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/arctanxarc/MC-LLaVA%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/arctanxarc/MC-LLaVA上公开提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11706v3">PDF</a> </p>
<p><strong>Summary</strong><br>多概念个性化视觉语言模型研究。针对现有模型忽略多概念的问题，提出MC-LLaVA模型，采用多概念指令调整策略，在单一训练步骤中有效整合多个概念。利用视觉标记信息初始化概念标记，降低联合训练成本。引入个性化文本提示和个性化视觉提示，提高识别和多模态定位能力。贡献高质量指令调整数据集，促进多概念个性化研究的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前视觉语言模型（VLMs）在多种任务上表现出卓越的能力，如视觉问答。</li>
<li>近期研究着眼于VLM个性化，以理解用户提供的概念。</li>
<li>现有研究主要关注单概念个性化，忽略了多概念的存在和相互作用。</li>
<li>MC-LLaVA模型首次提出多概念个性化范式。</li>
<li>MC-LLaVA采用多概念指令调整策略，在单一训练步骤中整合多个概念。</li>
<li>利用视觉标记信息初始化概念标记，降低联合训练成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11706">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ff0588fed58b96b95a8344ea2ffb4f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2075661619ba41019437cf247b33ef23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a0842e0a2d456dbf4d45bf98d6195d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3030174a11a73b336e1ac75622104458.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a138bc677a822b68424c8e7ee15f6052.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a8a62d63fe32f510729dfeebedf4e24.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="A-Formal-Framework-for-Understanding-Length-Generalization-in-Transformers"><a href="#A-Formal-Framework-for-Understanding-Length-Generalization-in-Transformers" class="headerlink" title="A Formal Framework for Understanding Length Generalization in   Transformers"></a>A Formal Framework for Understanding Length Generalization in   Transformers</h2><p><strong>Authors:Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn</strong></p>
<p>A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers. </p>
<blockquote>
<p>对于转换器来说，一个主要挑战是推广到训练过程中未见过的序列长度。尽管之前的工作已经证明，转换器的长度泛化能力取决于任务，但关于这一现象的理论理解仍然有限。在这项工作中，我们引入了一个严谨的理论框架，对带有可学习绝对位置编码的因果转换器的长度泛化进行分析。特别是，我们刻画了在理想化推理方案下，使用基于范数的正则化器，可以从足够长的输入中识别出的那些具有绝对位置编码的函数。这使我们能够为一系列丰富的问题证明长度泛化的可能性。我们通过实验验证了该理论在算法和形式语言任务的长度泛化成功和失败方面的预测能力。我们的理论不仅解释了大量经验观察结果，而且为预测转换器中的长度泛化能力开辟了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02140v2">PDF</a> 85 pages, 9 figures, 11 tables. Accepted for publication at ICLR 2025</p>
<p><strong>Summary</strong><br>     本研究介绍了一个严谨的理论框架，用于分析具有可学习绝对位置编码的因果变压器中的长度泛化问题。通过理想化推理方案和基于范数的正则化器，我们能够从足够长的输入中识别出那些可识别的功能，从而证明对一系列问题的长度泛化的可能性。实验验证了该理论在预测各种算法和形式语言任务的长度泛化成功和失败方面的有效性。该理论不仅解释了广泛的实验观察结果，而且为预测变压器的长度泛化能力开辟了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究提出了一个理论框架，用于分析具有可学习绝对位置编码的因果变压器中的长度泛化问题。</li>
<li>通过理想化推理方案和基于范数的正则化器，研究能够识别足够长输入中的可识别功能。</li>
<li>研究证明了对于一系列问题的长度泛化的可能性。</li>
<li>实验验证了该理论在预测各种算法和形式语言任务的长度泛化方面的有效性。</li>
<li>该理论不仅解释了广泛的实验观察结果。</li>
<li>该研究为预测变压器的长度泛化能力提供了新思路。</li>
<li>此理论框架有助于理解为何在某些任务中，变压器能够成功泛化到比训练期间观察到的更长的序列，而在其他任务中则可能失败。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02140">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-05c431618e967710b683df750bebbe9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37d9704c0d099710ef77ea8a508e0bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d74a9d7a10c282f489f405ec119ba1b4.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="OmniBench-Towards-The-Future-of-Universal-Omni-Language-Models"><a href="#OmniBench-Towards-The-Future-of-Universal-Omni-Language-Models" class="headerlink" title="OmniBench: Towards The Future of Universal Omni-Language Models"></a>OmniBench: Towards The Future of Universal Omni-Language Models</h2><p><strong>Authors:Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have focused on integrating multiple modalities, yet their ability to simultaneously process and reason across different inputs remains underexplored. We introduce OmniBench, a novel benchmark designed to evaluate models’ ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define language models capable of such tri-modal processing as omni-language models (OLMs). OmniBench features high-quality human annotations that require integrated understanding across all modalities. Our evaluation reveals that: i) open-source OLMs show significant limitations in instruction-following and reasoning in tri-modal contexts; and ii) most baseline models perform poorly (around 50% accuracy) even with textual alternatives to image&#x2F;audio inputs. To address these limitations, we develop OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We advocate for developing more robust tri-modal integration techniques and training strategies to enhance OLM performance. Codes and data could be found at our repo (<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/OmniBench">https://github.com/multimodal-art-projection/OmniBench</a>). </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLM）进展主要聚焦于多种模态的集成，但它们在处理不同输入的同时进行推理的能力仍然被忽视。我们引入了OmniBench，这是一个新的基准测试，旨在评估模型在视觉、听觉和文本输入上的识别、解释和推理能力。我们定义能够进行这种三模态处理的语言模型为全语言模型（OLMs）。OmniBench以高质量的人力注释为特色，需要全面理解所有模态。我们的评估发现：i）开源OLM在三模态环境下的指令执行和推理能力存在明显局限；ii）即使使用文本替代图像&#x2F;音频输入，大多数基准模型的性能也较差（准确率约为50%）。为了解决这些局限性，我们开发了OmniInstruct，这是一个包含9万六千个样本的指令调整数据集，用于训练OLM。我们提倡开发更稳健的三模态集成技术和训练策略，以提高OLM的性能。代码和数据可以在我们的仓库中找到（<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/OmniBench%EF%BC%89%E3%80%82">https://github.com/multimodal-art-projection/OmniBench）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15272v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>最新研究提出了一种新型基准测试——OmniBench，用于评估多模态大型语言模型（MLLMs）同时处理视觉、声音和文本输入的能力。研究发现现有模型在处理三模态上下文时存在显著局限性，并提出OmniInstruct数据集用于训练能够处理多模态数据的语言模型。强调发展更强大的三模态集成技术和训练策略以提高模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<p>一、OmniBench基准测试被设计用于评估语言模型同时处理视觉、声音和文本输入的能力，包括识别、解释和推理。<br>二、现有开源模型在处理三模态上下文时的指令遵循和推理能力存在显著局限性。<br>三、大多数基准模型即使使用文本替代图像&#x2F;音频输入，其准确率也只有大约50%。<br>四、为了改善模型性能，研究团队开发了OmniInstruct数据集，包含96K样本的指令调整数据。<br>五、OmniBench强调开发更强大的三模态集成技术和训练策略的必要性。<br>六、OmniBench的代码和数据可以在指定仓库中找到。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15272">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8dd5fae1a0fcc5118afea7573433b2b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7701f076b69e733e1efc018ab5d404df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bec75f3188c8c0c18cef5afbc8f10f68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dd6c40e3aa975affdca19585c3c51c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68df14a4ad25a76d250205cb0a11e2bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac1b1f6c109c75ed09adb25eeecf5e14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6547e34a7776769d11fadda0f924efe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-391ed846392ec13b560ca702b92ae5d5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy"><a href="#Multi-modal-Speech-Transformer-Decoders-When-Do-Multiple-Modalities-Improve-Accuracy" class="headerlink" title="Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?"></a>Multi-modal Speech Transformer Decoders: When Do Multiple Modalities   Improve Accuracy?</h2><p><strong>Authors:Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill</strong></p>
<p>Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on recognition accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step. </p>
<blockquote>
<p>最近，只解码离散标记的语言模型在自动语音识别领域取得了重大成功。然而，关于不同模态如何影响特定场景的性能的系统性分析仍然有限。在本文中，我们研究了多种模态对合成数据集和现实世界数据集识别精度的影响。我们的实验表明：（1）集成更多模态可以提高精度；尤其是我们的论文据我们所知首次展示了结合音频、图像上下文和嘴唇信息的优势；（2）图像作为语音识别的辅助模态在中等噪声水平下提供最大的优势，此外，它们表现出与嘴唇移动等固有同步模态不同的趋势；（3）当在预处理步骤中过滤掉最相关的视觉信息时，合成数据集和现实世界数据集的性能都会有所提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09221v2">PDF</a> </p>
<p><strong>Summary</strong>：本文研究了多模态对合成和真实世界数据集上的识别精度的影响。实验表明，集成多模态能提高准确性，尤其是结合音频、图像上下文和唇部信息的组合效益显著；图像作为语音识别的一种补充模式在中度噪音水平下提供最大帮助，并且与唇部动作等内在同步模式相比表现出不同的趋势；在预处理步骤中过滤最相关的视觉信息可以提高合成和真实世界数据集的识别性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>结合多种模态（如音频、图像上下文和唇部信息）能提高识别准确性。</li>
<li>在语音识别中，图像作为补充模态在中度噪音水平下尤为重要。</li>
<li>与内在同步模式（如唇部动作）相比，图像的影响展现出独特趋势。</li>
<li>过滤最相关的视觉信息作为预处理步骤可以提高识别性能。</li>
<li>实验结果证明了多模态集成在合成和真实世界数据集上的效益。</li>
<li>本文是首次展示结合音频、图像上下文和唇部信息带来的益处的论文。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0bafdeb6780cbecb1d6946db37408f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24bd934af7376e7ba1d87df26c16c8fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064b23bff11e7ddbdea0ae473c99fbe7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ddb7e89b731e981061fc5224f1f57d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0cd001349ee2c56f86b5ce767c93af8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8823ea5bbfb3feab68473343aed2150f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-95b2d0fb29b18389b1c180b7a3c48137.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-04-04  v-CLR View-Consistent Learning for Open-World Instance Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fd0720d589f11ae109f58dcbb423620.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-04-04  Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by   Generating Realistic Dermoscopic Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17663.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
