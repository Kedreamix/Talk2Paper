<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-27  Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    30 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-27-æ›´æ–°"><a href="#2024-12-27-æ›´æ–°" class="headerlink" title="2024-12-27 æ›´æ–°"></a>2024-12-27 æ›´æ–°</h1><h2 id="Unleashing-the-Temporal-Spatial-Reasoning-Capacity-of-GPT-for-Training-Free-Audio-and-Language-Referenced-Video-Object-Segmentation"><a href="#Unleashing-the-Temporal-Spatial-Reasoning-Capacity-of-GPT-for-Training-Free-Audio-and-Language-Referenced-Video-Object-Segmentation" class="headerlink" title="Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation"></a>Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation</h2><p><strong>Authors:Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu</strong></p>
<p>In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2) pipeline to explore the training-free paradigm for audio and language-referenced video object segmentation, namely AVS and RVOS tasks. The intuitive solution leverages GroundingDINO to identify the target object from a single frame and SAM 2 to segment the identified object throughout the video, which is less robust to spatiotemporal variations due to a lack of video context exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt. Within GPT-PS, two task-specific Chain-of-Thought prompts are designed to unleash GPTâ€™s temporal-spatial reasoning capacity by guiding GPT to make selections based on a comprehensive understanding of video and reference information. Furthermore, we propose a Language-Binded Reference Unification (LBRU) module to convert audio signals into language-formatted references, thereby unifying the formats of AVS and RVOS tasks in the same pipeline. Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2 pipeline achieves performances comparable to or even better than fully-supervised fine-tuning methods. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/appletea233/AL-Ref-SAM2">https://github.com/appletea233/AL-Ref-SAM2</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéŸ³é¢‘-è¯­è¨€å‚è€ƒçš„SAM 2ï¼ˆAL-Ref-SAM 2ï¼‰ç®¡é“ï¼Œç”¨äºæ¢ç´¢éŸ³é¢‘å’Œè¯­è¨€å‚è€ƒçš„è§†é¢‘å¯¹è±¡åˆ†å‰²çš„æ— è®­ç»ƒèŒƒå¼ï¼Œå³AVSå’ŒRVOSä»»åŠ¡ã€‚è¯¥ç›´è§‚è§£å†³æ–¹æ¡ˆåˆ©ç”¨GroundingDINOä»å•å¸§ä¸­è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œå¹¶ä½¿ç”¨SAM 2åœ¨æ•´ä¸ªè§†é¢‘ä¸­åˆ†å‰²å·²è¯†åˆ«çš„å¯¹è±¡ã€‚ç”±äºç¼ºä¹å¯¹è§†é¢‘ä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œè¯¥æ–¹æ¡ˆå¯¹æ—¶ç©ºå˜åŒ–ä¸å¤ªç¨³å¥ã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„AL-Ref-SAM 2ç®¡é“ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„GPTè¾…åŠ©æ”¯ç‚¹é€‰æ‹©ï¼ˆGPT-PSï¼‰æ¨¡å—ï¼Œä»¥æŒ‡å¯¼GPT-4æ‰§è¡Œä¸¤æ­¥æ—¶ç©ºæ¨ç†ï¼Œä»è€ŒæŒ‰é¡ºåºé€‰æ‹©æ”¯ç‚¹å¸§å’Œæ”¯ç‚¹æ¡†ï¼Œä¸ºSAM 2æä¾›é«˜è´¨é‡çš„åˆå§‹å¯¹è±¡æç¤ºã€‚åœ¨GPT-PSä¸­ï¼Œè®¾è®¡äº†ä¸¤ä¸ªé’ˆå¯¹ä»»åŠ¡çš„æ€ç»´é“¾æç¤ºï¼Œé€šè¿‡å¼•å¯¼GPTåŸºäºè§†é¢‘å’Œå‚è€ƒä¿¡æ¯çš„å…¨é¢ç†è§£è¿›è¡Œé€‰æ‹©ï¼Œä»¥æ¿€å‘GPTçš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­è¨€ç»‘å®šå‚è€ƒç»Ÿä¸€ï¼ˆLBRUï¼‰æ¨¡å—ï¼Œå°†éŸ³é¢‘ä¿¡å·è½¬æ¢ä¸ºè¯­è¨€æ ¼å¼çš„å‚è€ƒï¼Œä»è€Œç»Ÿä¸€ç®¡é“ä¸­AVSå’ŒRVOSä»»åŠ¡æ ¼å¼ã€‚åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— è®­ç»ƒAL-Ref-SAM 2ç®¡é“çš„æ€§èƒ½å¯ä¸å®Œå…¨ç›‘ç£çš„å¾®è°ƒæ–¹æ³•ç›¸åª²ç¾ç”šè‡³æ›´å¥½ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/appletea233/AL-Ref-SAM2">https://github.com/appletea233/AL-Ref-SAM2</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15876v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéŸ³é¢‘-è¯­è¨€å‚è€ƒçš„SAM 2ï¼ˆAL-Ref-SAM 2ï¼‰ç®¡é“ï¼Œç”¨äºæ¢ç´¢æ— éœ€è®­ç»ƒå³å¯è¿›è¡ŒéŸ³é¢‘å’Œè§†é¢‘å‚è€ƒçš„å¯¹è±¡åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨GroundingDINOè¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œä½¿ç”¨SAM 2åœ¨è§†é¢‘ä¸­å¯¹è¯†åˆ«å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚ä¸ºäº†å…‹æœç”±äºç¼ºä¹è§†é¢‘ä¸Šä¸‹æ–‡æ¢ç´¢å¯¼è‡´çš„æ—¶ç©ºå˜åŒ–é²æ£’æ€§ä¸è¶³çš„é—®é¢˜ï¼Œå¼•å…¥äº†GPTè¾…åŠ©çš„æ¢è½´é€‰æ‹©ï¼ˆGPT-PSï¼‰æ¨¡å—ï¼ŒæŒ‡å¯¼GPT-4è¿›è¡Œä¸¤æ­¥æ—¶ç©ºæ¨ç†ï¼Œé€‰æ‹©æ¢è½´å¸§å’Œæ¢è½´æ¡†ï¼Œä¸ºSAM 2æä¾›é«˜è´¨é‡çš„å¯¹è±¡åˆå§‹æç¤ºã€‚åŒæ—¶ï¼Œæå‡ºäº†è¯­è¨€ç»‘å®šå‚è€ƒç»Ÿä¸€ï¼ˆLBRUï¼‰æ¨¡å—ï¼Œå°†éŸ³é¢‘ä¿¡å·è½¬æ¢ä¸ºè¯­è¨€æ ¼å¼çš„å‚è€ƒï¼Œç»Ÿä¸€AVSå’ŒRVOSä»»åŠ¡åœ¨åŒä¸€ç®¡é“ä¸­çš„æ ¼å¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥è®­ç»ƒå…è´¹çš„AL-Ref-SAM 2ç®¡é“çš„æ€§èƒ½å¯ä¸æˆ–ä¼˜äºå®Œå…¨ç›‘ç£çš„å¾®è°ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºéŸ³é¢‘-è¯­è¨€å‚è€ƒçš„SAM 2ç®¡é“ï¼ˆAL-Ref-SAM 2ï¼‰ï¼Œç”¨äºæ— éœ€è®­ç»ƒçš„éŸ³é¢‘å’Œè§†é¢‘å‚è€ƒå¯¹è±¡åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨GroundingDINOè¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œé€šè¿‡SAM 2è¿›è¡Œè§†é¢‘åˆ†å‰²ã€‚</li>
<li>å¼•å…¥GPTè¾…åŠ©çš„æ¢è½´é€‰æ‹©ï¼ˆGPT-PSï¼‰æ¨¡å—ä»¥æé«˜è§†é¢‘åˆ†å‰²çš„é²æ£’æ€§ã€‚</li>
<li>GPT-PSæ¨¡å—é€šè¿‡æŒ‡å¯¼GPT-4è¿›è¡Œä¸¤æ­¥æ—¶ç©ºæ¨ç†æ¥é€‰æ‹©æ¢è½´å¸§å’Œæ¢è½´æ¡†ã€‚</li>
<li>æå‡ºè¯­è¨€ç»‘å®šå‚è€ƒç»Ÿä¸€ï¼ˆLBRUï¼‰æ¨¡å—ï¼Œå°†éŸ³é¢‘ä¿¡å·è½¬æ¢ä¸ºè¯­è¨€æ ¼å¼å‚è€ƒï¼Œç»Ÿä¸€ä¸åŒä»»åŠ¡çš„æ ¼å¼ã€‚</li>
<li>åœ¨ä¸¤ç§ä»»åŠ¡ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAL-Ref-SAM 2ç®¡é“æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-29d81c609b54a1597ec2499111f32127.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-11b59583ae221e351d4da6fe6afb2862.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b3c91e8a38026cfee85a54d33c4d01f8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8231f40e1fa73337fa488e4efd96af82.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d3f23d1ea89c51c4d6304a92fe3a50d5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c1bf18e0f0064614577191dd30f4991a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SpikingSSMs-Learning-Long-Sequences-with-Sparse-and-Parallel-Spiking-State-Space-Models"><a href="#SpikingSSMs-Learning-Long-Sequences-with-Sparse-and-Parallel-Spiking-State-Space-Models" class="headerlink" title="SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking   State Space Models"></a>SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking   State Space Models</h2><p><strong>Authors:Shuaijie Shen, Chao Wang, Renzhuo Huang, Yan Zhong, Qinghai Guo, Zhichao Lu, Jianguo Zhang, Luziwei Leng</strong></p>
<p>Known as low energy consumption networks, spiking neural networks (SNNs) have gained a lot of attention within the past decades. While SNNs are increasing competitive with artificial neural networks (ANNs) for vision tasks, they are rarely used for long sequence tasks, despite their intrinsic temporal dynamics. In this work, we develop spiking state space models (SpikingSSMs) for long sequence learning by leveraging on the sequence learning abilities of state space models (SSMs). Inspired by dendritic neuron structure, we hierarchically integrate neuronal dynamics with the original SSM block, meanwhile realizing sparse synaptic computation. Furthermore, to solve the conflict of event-driven neuronal dynamics with parallel computing, we propose a light-weight surrogate dynamic network which accurately predicts the after-reset membrane potential and compatible to learnable thresholds, enabling orders of acceleration in training speed compared with conventional iterative methods. On the long range arena benchmark task, SpikingSSM achieves competitive performance to state-of-the-art SSMs meanwhile realizing on average 90% of network sparsity. On language modeling, our network significantly surpasses existing spiking large language models (spikingLLMs) on the WikiText-103 dataset with only a third of the model size, demonstrating its potential as backbone architecture for low computation cost LLMs. </p>
<blockquote>
<p>è¢«ç§°ä¸ºä½åŠŸè€—ç¥ç»ç½‘ç»œï¼Œè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰åœ¨è¿‡å»çš„å‡ åå¹´ä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶SNNsåœ¨è§†è§‰ä»»åŠ¡æ–¹é¢ä¸äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNsï¼‰çš„ç«äº‰æ—¥ç›Šæ¿€çƒˆï¼Œä½†å®ƒä»¬å¾ˆå°‘ç”¨äºé•¿åºåˆ—ä»»åŠ¡ï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰å†…åœ¨çš„æ—¶ç©ºåŠ¨æ€ç‰¹æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åºåˆ—å­¦ä¹ èƒ½åŠ›ï¼Œå¼€å‘ç”¨äºé•¿åºåˆ—å­¦ä¹ çš„è„‰å†²çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSpikingSSMsï¼‰ã€‚å—æ ‘çªç¥ç»å…ƒç»“æ„çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ†å±‚åœ°å°†ç¥ç»å…ƒåŠ¨æ€ä¸åŸå§‹SSMå—é›†æˆï¼ŒåŒæ—¶å®ç°ç¨€ç–çªè§¦è®¡ç®—ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³äº‹ä»¶é©±åŠ¨ç¥ç»å…ƒåŠ¨åŠ›å­¦ä¸å¹¶è¡Œè®¡ç®—ä¹‹é—´çš„å†²çªï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„æ›¿ä»£åŠ¨æ€ç½‘ç»œï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹é‡ç½®åçš„è†œç”µä½å¹¶ä¸”ä¸å¯å­¦ä¹ çš„é˜ˆå€¼å…¼å®¹ï¼Œä¸å¸¸è§„è¿­ä»£æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒé€Ÿåº¦æé«˜äº†å‡ ä¸ªæ•°é‡çº§ã€‚åœ¨è¿œç¨‹åŒºåŸŸåŸºå‡†ä»»åŠ¡ä¸Šï¼ŒSpikingSSMå®ç°äº†ä¸æœ€æ–°SSMçš„ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†å¹³å‡90%çš„ç½‘ç»œç¨€ç–æ€§ã€‚åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç½‘ç»œåœ¨WikiText-103æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„è„‰å†²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆspikingLLMsï¼‰ï¼Œå¹¶ä¸”åªæœ‰ä¸‰åˆ†ä¹‹ä¸€æ¨¡å‹å¤§å°ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºä½è®¡ç®—æˆæœ¬LLMçš„éª¨å¹²æ¶æ„çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14909v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°–å³°çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSpikingSSMsï¼‰åœ¨é•¿åºåˆ—å­¦ä¹ æ–¹é¢çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹ç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åºåˆ—å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨æ ‘çªç¥ç»å…ƒç»“æ„çš„å¯å‘ï¼Œå®ç°äº†ç¥ç»å…ƒåŠ¨åŠ›å­¦çš„å±‚æ¬¡é›†æˆå’Œç¨€ç–çªè§¦è®¡ç®—ã€‚ä¸ºè§£å†³äº‹ä»¶é©±åŠ¨ç¥ç»å…ƒåŠ¨åŠ›å­¦ä¸å¹¶è¡Œè®¡ç®—çš„å†²çªï¼Œæå‡ºäº†è½»é‡çº§æ›¿ä»£åŠ¨æ€ç½‘ç»œï¼Œå¯å‡†ç¡®é¢„æµ‹é‡ç½®åçš„è†œç”µä½å¹¶ä¸å­¦ä¹ é˜ˆå€¼å…¼å®¹ï¼Œä½¿å¾—è®­ç»ƒé€Ÿåº¦æ¯”ä¼ ç»Ÿè¿­ä»£æ–¹æ³•æœ‰æ‰€åŠ å¿«ã€‚åœ¨é•¿èŒƒå›´åŒºåŸŸåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSpikingSSMå®ç°äº†ä¸æœ€æ–°SSMæŠ€æœ¯çš„ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶å®ç°äº†å¹³å‡90%çš„ç½‘ç»œç¨€ç–æ€§ã€‚åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢ï¼Œè¯¥ç½‘ç»œåœ¨WikiText-103æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å°–å³°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆspikingLLMsï¼‰ï¼Œä»…ä½¿ç”¨äº†ä¸‰åˆ†ä¹‹ä¸€å¤§å°çš„æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºä½è®¡ç®—æˆæœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ½œåœ¨æ¶æ„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spiking state space models (SpikingSSMs)ç»“åˆäº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„åºåˆ—å­¦ä¹ èƒ½åŠ›ï¼Œç”¨äºé•¿åºåˆ—å­¦ä¹ ã€‚</li>
<li>å±‚çº§ç»“åˆç¥ç»å…ƒåŠ¨åŠ›å­¦å’ŒåŸå§‹SSMæ¨¡å—ï¼Œå—åˆ°æ ‘çªç¥ç»å…ƒç»“æ„çš„å¯å‘ã€‚</li>
<li>æå‡ºè½»é‡çº§æ›¿ä»£åŠ¨æ€ç½‘ç»œï¼Œè§£å†³äº‹ä»¶é©±åŠ¨ç¥ç»å…ƒåŠ¨åŠ›å­¦ä¸å¹¶è¡Œè®¡ç®—çš„å†²çªã€‚</li>
<li>SpikingSSMåœ¨é•¿èŒƒå›´åŒºåŸŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸æœ€æ–°SSMæŠ€æœ¯çš„ç«äº‰æ€§èƒ½ï¼Œå¹¶å®ç°äº†ç½‘ç»œç¨€ç–æ€§ã€‚</li>
<li>åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢ï¼Œè¯¥ç½‘ç»œåœ¨WikiText-103æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å°–å³°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆspikingLLMsï¼‰ã€‚</li>
<li>è¯¥ç½‘ç»œæ¶æ„å…·æœ‰æ½œåŠ›æˆä¸ºä½è®¡ç®—æˆæœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„éª¨å¹²æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.14909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3c406ce708aaddbd0d21ca371d64015e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-39b41e0498396fd9e345e35b01107f84.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-49b5cd643fde0b70b6b37d44e45d85c8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-db1f077e142795679d313a643dc4fa46.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d52d3e7936315cbc193c94a461000adb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Security-Attacks-on-LLM-based-Code-Completion-Tools"><a href="#Security-Attacks-on-LLM-based-Code-Completion-Tools" class="headerlink" title="Security Attacks on LLM-based Code Completion Tools"></a>Security Attacks on LLM-based Code Completion Tools</h2><p><strong>Authors:Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang</strong></p>
<p>The rapid development of large language models (LLMs) has significantly advanced code completion capabilities, giving rise to a new generation of LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these tools possess unique workflows, integrating multiple information sources as input and prioritizing code suggestions over natural language interaction, which introduces distinct security challenges. Additionally, LCCTs often rely on proprietary code datasets for training, raising concerns about the potential exposure of sensitive data. This paper exploits these distinct characteristics of LCCTs to develop targeted attack methodologies on two critical security risks: jailbreaking and training data extraction attacks. Our experimental results expose significant vulnerabilities within LCCTs, including a 99.4% success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate on Amazon Q. Furthermore, We successfully extracted sensitive user data from GitHub Copilot, including 54 real email addresses and 314 physical addresses associated with GitHub usernames. Our study also demonstrates that these code-based attack methods are effective against general-purpose LLMs, such as the GPT series, highlighting a broader security misalignment in the handling of code by modern LLMs. These findings underscore critical security challenges associated with LCCTs and suggest essential directions for strengthening their security frameworks. The example code and attack samples from our research are provided at <a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">https://github.com/Sensente/Security-Attacks-on-LCCTs</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æå‡äº†ä»£ç è¡¥å…¨åŠŸèƒ½ï¼Œå¹¶å‚¬ç”Ÿäº†ä¸€ä»£åŸºäºLLMçš„ä»£ç è¡¥å…¨å·¥å…·ï¼ˆLCCTsï¼‰ã€‚ä¸ä¸€èˆ¬ç”¨é€”çš„LLMä¸åŒï¼Œè¿™äº›å·¥å…·æ‹¥æœ‰ç‹¬ç‰¹çš„å·¥ä½œæµç¨‹ï¼Œå®ƒä»¬æ•´åˆå¤šç§ä¿¡æ¯æºä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¼˜å…ˆäºè‡ªç„¶è¯­è¨€äº¤äº’ç»™å‡ºä»£ç å»ºè®®ï¼Œè¿™ä¸ºä»£ç è¡¥å…¨å·¥å…·å¸¦æ¥ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒLCCTé€šå¸¸ä¾èµ–ä¸“æœ‰ä»£ç æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¼•å‘äº†å…³äºæ½œåœ¨æ•æ„Ÿæ•°æ®æ³„éœ²çš„æ‹…å¿§ã€‚æœ¬æ–‡åˆ©ç”¨LCCTçš„è¿™äº›ç‹¬ç‰¹ç‰¹ç‚¹ï¼Œé’ˆå¯¹ä¸¤ç§å…³é”®å®‰å…¨é£é™©åˆ¶å®šäº†æœ‰é’ˆå¯¹æ€§çš„æ”»å‡»æ–¹æ³•ï¼šè¶Šç‹±æ”»å‡»å’Œè®­ç»ƒæ•°æ®æå–æ”»å‡»ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ­ç¤ºäº†LCCTsä¸­å­˜åœ¨çš„é‡å¤§æ¼æ´ï¼ŒåŒ…æ‹¬å¯¹GitHub Copilotçš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡ä¸º99.4%ï¼Œå¯¹äºšé©¬é€ŠQçš„æˆåŠŸç‡ä¸º46.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æˆåŠŸåœ°ä»GitHub Copilotä¸­æå–äº†ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ï¼ŒåŒ…æ‹¬54ä¸ªçœŸå®ç”µå­é‚®ä»¶åœ°å€å’Œä¸GitHubç”¨æˆ·åç›¸å…³çš„314ä¸ªå®ä½“åœ°å€ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œè¿™äº›åŸºäºä»£ç çš„æ”»å‡»æ–¹æ³•å¯¹é€šç”¨LLMï¼ˆå¦‚GPTç³»åˆ—ï¼‰åŒæ ·æœ‰æ•ˆï¼Œè¿™å‡¸æ˜¾äº†ç°ä»£LLMåœ¨å¤„ç†ä»£ç æ–¹é¢çš„å®‰å…¨é…ç½®ä¸å½“é—®é¢˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†LCCTé¢ä¸´çš„å…³é”®å®‰å…¨æŒ‘æˆ˜ï¼Œå¹¶ä¸ºåŠ å¼ºå…¶å®‰å…¨æ¡†æ¶æä¾›äº†é‡è¦æ–¹å‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸­çš„ç¤ºä¾‹ä»£ç å’Œæ”»å‡»æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">https://github.com/Sensente/Security-Attacks-on-LCCTs</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11006v3">PDF</a> Paper accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†ä»£ç è¡¥å…¨èƒ½åŠ›çš„æ˜¾è‘—å¢å¼ºï¼Œå‡ºç°äº†æ–°ä¸€ä»£åŸºäºLLMçš„ä»£ç è¡¥å…¨å·¥å…·ï¼ˆLCCTsï¼‰ã€‚è¿™äº›å·¥å…·å…·æœ‰ç‹¬ç‰¹çš„å·¥ä½œæµç¨‹ï¼Œæ•´åˆå¤šç§ä¿¡æ¯æºä½œä¸ºè¾“å…¥ï¼Œä¼˜å…ˆæä¾›ä»£ç å»ºè®®è€Œéè‡ªç„¶è¯­è¨€äº¤äº’ï¼Œä»è€Œå¸¦æ¥äº†æ–°çš„å®‰å…¨æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒLCCTsé€šå¸¸ä¾èµ–ä¸“æœ‰ä»£ç æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¼•å‘äº†å…³äºæ½œåœ¨æ•æ„Ÿæ•°æ®æ³„éœ²çš„æ‹…å¿§ã€‚æœ¬æ–‡åˆ©ç”¨LCCTsçš„ç‹¬ç‰¹ç‰¹ç‚¹ï¼Œé’ˆå¯¹ä¸¤é¡¹å…³é”®å®‰å…¨é£é™©è®¾è®¡äº†æœ‰é’ˆå¯¹æ€§çš„æ”»å‡»æ–¹æ³•ï¼šè¶Šç‹±æ”»å‡»å’Œè®­ç»ƒæ•°æ®æå–æ”»å‡»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLCCTså­˜åœ¨é‡å¤§æ¼æ´ï¼ŒåŒ…æ‹¬å¯¹GitHub Copilotçš„è¶Šç‹±æ”»å‡»æˆåŠŸç‡é«˜è¾¾99.4%ï¼Œå¯¹äºšé©¬é€ŠQçš„æˆåŠŸç‡ä¸º46.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æˆåŠŸä»GitHub Copilotæå–äº†ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ï¼ŒåŒ…æ‹¬54ä¸ªçœŸå®ç”µå­é‚®ä»¶åœ°å€å’Œ314ä¸ªä¸GitHubç”¨æˆ·åå…³è”çš„ç‰©ç†åœ°å€ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œè¿™äº›åŸºäºä»£ç çš„æ”»å‡»æ–¹æ³•å¯¹é€šç”¨LLMï¼ˆå¦‚GPTç³»åˆ—ï¼‰åŒæ ·æœ‰æ•ˆï¼Œçªæ˜¾äº†ç°ä»£LLMåœ¨å¤„ç†ä»£ç æ—¶çš„å¹¿æ³›å®‰å…¨åå·®ã€‚æœ¬ç ”ç©¶çš„ç¤ºä¾‹ä»£ç å’Œæ”»å‡»æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†ä»£ç è¡¥å…¨å·¥å…·ï¼ˆLCCTsï¼‰çš„è¿›æ­¥ã€‚</li>
<li>LCCTså…·æœ‰ç‹¬ç‰¹çš„å·¥ä½œæµç¨‹å’Œå®‰å…¨æŒ‘æˆ˜ï¼Œæ¶‰åŠå¤šä¸ªä¿¡æ¯æºçš„æ•´åˆå’Œä»£ç å»ºè®®çš„ä¼˜å…ˆæä¾›ã€‚</li>
<li>LCCTsä¾èµ–ä¸“æœ‰ä»£ç æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¼•å‘æ•æ„Ÿæ•°æ®æ³„éœ²çš„æ‹…å¿§ã€‚</li>
<li>é’ˆå¯¹LCCTså­˜åœ¨è¶Šç‹±æ”»å‡»å’Œè®­ç»ƒæ•°æ®æå–æ”»å‡»çš„é‡å¤§æ¼æ´ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºGitHub Copilotç­‰LCCTså·¥å…·å­˜åœ¨é«˜é£é™©æ¼æ´ï¼Œæ”»å‡»è€…å¯æˆåŠŸæå–ç”¨æˆ·æ•æ„Ÿæ•°æ®ã€‚</li>
<li>è¿™äº›æ”»å‡»æ–¹æ³•å¯¹é€šç”¨LLMåŒæ ·æœ‰æ•ˆï¼Œæ˜¾ç¤ºç°ä»£LLMåœ¨å¤„ç†ä»£ç æ—¶çš„å®‰å…¨ç¼ºé™·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-194caa365833fd9f98d39865a3f5521c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5ef0e203f35c596121a9b31bedd5003e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96ffdb2421a926b5021a29eb466a1351.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3898f8a714d76e2e08a1dc4ab7bf4a80.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Advancing-Mental-Health-Pre-Screening-A-New-Custom-GPT-for-Psychological-Distress-Assessment"><a href="#Advancing-Mental-Health-Pre-Screening-A-New-Custom-GPT-for-Psychological-Distress-Assessment" class="headerlink" title="Advancing Mental Health Pre-Screening: A New Custom GPT for   Psychological Distress Assessment"></a>Advancing Mental Health Pre-Screening: A New Custom GPT for   Psychological Distress Assessment</h2><p><strong>Authors:Jinwen Tang, Yi Shang</strong></p>
<p>This study introduces â€˜Psycho Analystâ€™, a custom GPT model based on OpenAIâ€™s GPT-4, optimized for pre-screening mental health disorders. Enhanced with DSM-5, PHQ-8, detailed data descriptions, and extensive training data, the model adeptly decodes nuanced linguistic indicators of mental health disorders. It utilizes a dual-task framework that includes binary classification and a three-stage PHQ-8 score computation involving initial assessment, detailed breakdown, and independent assessment, showcasing refined analytic capabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1 scores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of 2.89 and 3.69 in PHQ-8 scoring. These results highlight the modelâ€™s precision and transformative potential in enhancing public mental health support, improving accessibility, cost-effectiveness, and serving as a second opinion for professionals. </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†â€œå¿ƒç†åˆ†æä¸“å®¶â€ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºOpenAIçš„GPT-4çš„è‡ªå®šä¹‰GPTæ¨¡å‹ï¼Œç»è¿‡ä¼˜åŒ–ï¼Œç”¨äºé¢„å…ˆç­›é€‰ç²¾ç¥å¥åº·éšœç¢ã€‚é€šè¿‡DSM-5ã€PHQ-8ã€è¯¦ç»†æ•°æ®æè¿°å’Œå¤§é‡çš„è®­ç»ƒæ•°æ®å¢å¼ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç†Ÿç»ƒåœ°è§£ç ç²¾ç¥å¥åº·éšœç¢çš„å¾®å¦™è¯­è¨€æŒ‡æ ‡ã€‚å®ƒé‡‡ç”¨åŒé‡ä»»åŠ¡æ¡†æ¶ï¼ŒåŒ…æ‹¬äºŒå…ƒåˆ†ç±»å’Œæ¶‰åŠåˆæ­¥è¯„ä¼°ã€è¯¦ç»†åˆ†è§£å’Œç‹¬ç«‹è¯„ä¼°çš„ä¸‰é˜¶æ®µPHQ-8è¯„åˆ†è®¡ç®—ï¼Œå±•ç¤ºäº†ç²¾ç»†çš„åˆ†æèƒ½åŠ›ã€‚ä½¿ç”¨DAIC-WOZæ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºF1å’Œå®è§‚F1å¾—åˆ†åˆ†åˆ«ä¸º0.929å’Œ0.949ï¼Œåœ¨PHQ-8è¯„åˆ†ä¸­å¹³å‡ç»å¯¹è¯¯å·®å’Œå‡æ–¹æ ¹è¯¯å·®æœ€ä½ï¼Œåˆ†åˆ«ä¸º2.89å’Œ3.69ã€‚è¿™äº›ç»“æœçªå‡ºäº†è¯¥æ¨¡å‹çš„ç²¾ç¡®åº¦å’Œåœ¨å¢å¼ºå…¬ä¼—ç²¾ç¥å¥åº·æ”¯æŒã€æé«˜å¯åŠæ€§ã€æˆæœ¬æ•ˆç›Šä»¥åŠä½œä¸ºä¸“ä¸šäººå£«çš„ç¬¬äºŒæ„è§æ–¹é¢çš„å˜é©æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01614v2">PDF</a> Accepted by IEEE CogMI 2024</p>
<p><strong>Summary</strong></p>
<p>ä¸€é¡¹ç ”ç©¶æ¨å‡ºäº†åä¸ºâ€œç²¾ç¥åˆ†æä¸“å®¶â€çš„GPTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºOpenAIçš„GPT-4æ„å»ºï¼Œä¸“é—¨ç”¨äºé¢„å…ˆç­›é€‰ç²¾ç¥å¥åº·ç–¾ç—…ã€‚è¯¥æ¨¡å‹ç»“åˆäº†DSM-5ã€PHQ-8ç­‰æ ‡å‡†å·¥å…·ï¼Œé€šè¿‡è¯¦ç»†çš„æ•°æ®æè¿°å’Œå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿç²¾å‡†è§£è¯»è¯­è¨€ä¸­å¾®å¦™çš„å¿ƒç†å¥åº·éšœç¢è¿¹è±¡ã€‚é‡‡ç”¨åŒé‡ä»»åŠ¡æ¡†æ¶ï¼ŒåŒ…æ‹¬äºŒå…ƒåˆ†ç±»å’Œæ¶‰åŠåˆæ­¥è¯„ä¼°ã€è¯¦ç»†åˆ†è§£å’Œç‹¬ç«‹è¯„ä¼°çš„ä¸‰é˜¶æ®µPHQ-8è¯„åˆ†è®¡ç®—ï¼Œå±•ç°å‡ºç²¾ç»†çš„åˆ†æèƒ½åŠ›ã€‚ä½¿ç”¨DAIC-WOZæ•°æ®é›†éªŒè¯ï¼Œæ¨¡å‹çš„F1å’Œå®è§‚F1å¾—åˆ†åˆ†åˆ«ä¸º0.929å’Œ0.949ï¼Œåœ¨PHQ-8è¯„åˆ†æ–¹é¢çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œå‡æ–¹æ ¹è¯¯å·®æœ€ä½ï¼Œåˆ†åˆ«ä¸º2.89å’Œ3.69ã€‚è¿™ä¸€æˆæœå±•ç°äº†æ¨¡å‹åœ¨å…¬ä¼—ç²¾ç¥å¥åº·æ”¯æŒæ–¹é¢çš„ç²¾å‡†åº¦å’Œé©æ–°æ½œåŠ›ï¼Œæé«˜äº†å¯åŠæ€§å’Œæˆæœ¬æ•ˆç›Šï¼Œå¯æˆä¸ºä¸“ä¸šäººå£«çš„ç¬¬äºŒæ„è§å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œPsycho Analystâ€æ˜¯åŸºäºGPT-4çš„å®šåˆ¶æ¨¡å‹ï¼Œä¸“ä¸ºé¢„ç­›ç²¾ç¥å¥åº·ç–¾ç—…è®¾è®¡ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†DSM-5ã€PHQ-8ç­‰æ ‡å‡†å·¥å…·ï¼Œå…·å¤‡è§£è¯»è¯­è¨€ä¸­çš„ç²¾ç¥å¥åº·éšœç¢è¿¹è±¡çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨åŒé‡ä»»åŠ¡æ¡†æ¶ï¼ŒåŒ…æ‹¬äºŒå…ƒåˆ†ç±»å’ŒPHQ-8è¯„åˆ†è®¡ç®—çš„ä¸‰é˜¶æ®µè¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œç»DAIC-WOZæ•°æ®é›†éªŒè¯ï¼ŒF1å’Œå®è§‚F1å¾—åˆ†é«˜ã€‚</li>
<li>æ¨¡å‹åœ¨PHQ-8è¯„åˆ†æ–¹é¢çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œå‡æ–¹æ ¹è¯¯å·®è¾¾åˆ°è¾ƒä½æ°´å¹³ã€‚</li>
<li>æ¨¡å‹åœ¨å…¬ä¼—ç²¾ç¥å¥åº·æ”¯æŒæ–¹é¢å…·å¤‡ç²¾å‡†åº¦å’Œé©æ–°æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-73225adde3f9f3011ba6b3913c7dbcff.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e86a39a9e574397ca3585de75a700199.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5618ab2c3912992cafd51a46376ded6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bdecf9a16bf092e368193b066c56dc36.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DART-Math-Difficulty-Aware-Rejection-Tuning-for-Mathematical-Problem-Solving"><a href="#DART-Math-Difficulty-Aware-Rejection-Tuning-for-Mathematical-Problem-Solving" class="headerlink" title="DART-Math: Difficulty-Aware Rejection Tuning for Mathematical   Problem-Solving"></a>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical   Problem-Solving</h2><p><strong>Authors:Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He</strong></p>
<p>Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries. Hypothesizing that difficult queries are crucial to learn complex reasoning, we propose Difficulty-Aware Rejection Tuning (DART), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples. Utilizing DART, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4. We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called DART-MATH. In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, DART-MATH outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. </p>
<blockquote>
<p>è§£å†³æ•°å­¦é—®é¢˜éœ€è¦é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œè¿™æ„æˆäº†æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚ä»¥å‰çš„å·¥ä½œé€šå¸¸ä»ä¸“æœ‰æ¨¡å‹ä¸­åˆæˆæ•°æ®æ¥å¢å¼ºç°æœ‰æ•°æ®é›†ï¼Œç„¶åé€šè¿‡æŒ‡ä»¤å¾®è°ƒæ¥è·å¾—é¡¶çº§ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ•°æ®é›†çš„åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬å­˜åœ¨ä¸¥é‡çš„åå‘ç®€å•æŸ¥è¯¢çš„åè§ï¼Œå¯¹äºæœ€å…·æŒ‘æˆ˜æ€§çš„æŸ¥è¯¢ï¼Œç»å¸¸æ— æ³•ç”Ÿæˆä»»ä½•æ­£ç¡®çš„å›åº”ã€‚æˆ‘ä»¬å‡è®¾å›°éš¾æŸ¥è¯¢å¯¹äºå­¦ä¹ å¤æ‚æ¨ç†è‡³å…³é‡è¦ï¼Œå› æ­¤æå‡ºäº†å›°éš¾æ„ŸçŸ¥æ‹’ç»è°ƒæ•´ï¼ˆDARTï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆé˜¶æ®µä¸ºå›°éš¾æŸ¥è¯¢åˆ†é…æ›´å¤šçš„è¯•éªŒï¼Œä½¿å®ƒä»¬åœ¨å›°éš¾æ ·æœ¬ä¸Šè¿›è¡Œæ›´å¹¿æ³›çš„è®­ç»ƒã€‚åˆ©ç”¨DARTï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸“æ³¨äºå›°éš¾æŸ¥è¯¢çš„æ•°å­¦é—®é¢˜æ±‚è§£æ–°æ•°æ®é›†ï¼Œå¹¶ä¸”è¿™äº›æ•°æ®é›†æ¯”ä»¥å‰çš„è¦å°å¾—å¤šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åˆæˆè¿‡ç¨‹ä»…ä¾èµ–äºä¸€ä¸ª7Bå¤§å°çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼Œä¸éœ€è¦å¸¸ç”¨çš„ä¸“æœ‰GPT-4ã€‚æˆ‘ä»¬åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒäº†ä»7Båˆ°70Bçš„å„ç§åŸºç¡€æ¨¡å‹ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç³»åˆ—å¼ºå¤§çš„æ¨¡å‹ï¼Œç§°ä¸ºDART-MATHã€‚åœ¨6ä¸ªæ•°å­¦åŸºå‡†çš„å…¨é¢é¢†åŸŸå†…éƒ¨å’Œé¢†åŸŸå¤–éƒ¨è¯„ä¼°ä¸­ï¼ŒDART-MATHæ˜¾è‘—ä¼˜äºæ™®é€šçš„æ‹’ç»è°ƒæ•´ï¼Œå¹¶ä¸”åœ¨ä¹‹å‰çš„æŠ€æœ¯ä¸­è¡¨ç°ä¼˜è¶Šæˆ–ç›¸å½“ï¼Œå°½ç®¡å®ƒä½¿ç”¨çš„æ•°æ®é›†æ›´å°ï¼Œè€Œä¸”æ²¡æœ‰ä½¿ç”¨ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœå°†æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†å®šä½ä¸ºæ¨è¿›æ•°å­¦é—®é¢˜æ±‚è§£æ–¹é¢æœ€æœ‰æ•ˆä¸”æœ€ç»æµçš„å…¬å¼€èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13690v2">PDF</a> NeurIPS 2024. Data and model checkpoints are available at   <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/dart-math">https://github.com/hkust-nlp/dart-math</a></p>
<p><strong>Summary</strong>ï¼šè§£å†³æ•°å­¦é—®é¢˜éœ€è¦é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹æå‡ºäº†æŒ‘æˆ˜ã€‚è¿‡å»çš„ç ”ç©¶é€šå¸¸é€šè¿‡åˆæˆæ•°æ®å’Œå¾®è°ƒæŒ‡ä»¤æ¥å¢å¼ºç°æœ‰æ•°æ®é›†ä»¥å®ç°é¡¶å°–ç»“æœã€‚ç„¶è€Œï¼Œåˆ†æå‘ç°è¿™äº›æ•°æ®é›†å­˜åœ¨ä¸¥é‡åå‘ç®€å•æŸ¥è¯¢çš„é—®é¢˜ï¼Œå¯¹æœ€å¤æ‚çš„æŸ¥è¯¢å¾€å¾€æ— æ³•ç»™å‡ºæ­£ç¡®å›åº”ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºéš¾åº¦æ„ŸçŸ¥æ‹’ç»è°ƒæ•´ï¼ˆDARTï¼‰æ–¹æ³•ï¼Œåœ¨åˆæˆé˜¶æ®µä¸ºå›°éš¾æŸ¥è¯¢åˆ†é…æ›´å¤šè¯•éªŒæ¬¡æ•°ï¼Œä»¥ä¾¿å¯¹å›°éš¾æ ·æœ¬è¿›è¡Œæ›´å¹¿æ³›çš„è®­ç»ƒã€‚ä½¿ç”¨DARTï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸“æ³¨äºå›°éš¾æŸ¥è¯¢çš„æ•°å­¦é—®é¢˜æ±‚è§£æ–°æ•°æ®é›†ï¼Œä¸”æ¯”ä¹‹å‰çš„æ•°æ®é›†å°å¾—å¤šã€‚åˆæˆè¿‡ç¨‹ä»…ä¾èµ–äº7Bå¤§å°çš„å¼€æ”¾æƒé‡æ¨¡å‹ï¼Œæœªä½¿ç”¨å¸¸è§çš„ä¸“æœ‰GPT-4ã€‚æˆ‘ä»¬åœ¨ä¸åŒå¤§å°ï¼ˆä»7Båˆ°70Bï¼‰çš„åŸºå‡†æ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå½¢æˆä¸€ç³»åˆ—å¼ºå¤§çš„DART-MATHæ¨¡å‹ã€‚åœ¨6ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå…¨é¢çš„åŸŸå†…å’ŒåŸŸå¤–è¯„ä¼°ï¼ŒDART-MATHæ˜¾è‘—ä¼˜äºç®€å•çš„æ‹’ç»è°ƒæ•´ï¼Œä¸”ä¼˜äºæˆ–ç›¸å½“äºä»¥å‰çš„æŠ€æœ¯ï¼Œå°½ç®¡ä½¿ç”¨äº†è¾ƒå°çš„æ•°æ®é›†ä¸”æ²¡æœ‰ä¸“æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœå°†æˆ‘ä»¬çš„åˆæˆæ•°æ®é›†å®šä½ä¸ºæ¨è¿›æ•°å­¦é—®é¢˜æ±‚è§£æ–¹é¢æœ€æœ‰æ•ˆä¸”æœ€ç»æµçš„å…¬å¼€èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§£å†³æ•°å­¦é—®é¢˜éœ€è¦é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå¯¹å¤§è¯­è¨€æ¨¡å‹æå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨åå‘ç®€å•æŸ¥è¯¢çš„é—®é¢˜ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚æŸ¥è¯¢çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºéš¾åº¦æ„ŸçŸ¥æ‹’ç»è°ƒæ•´ï¼ˆDARTï¼‰æ–¹æ³•ï¼Œä¸ºå›°éš¾æŸ¥è¯¢åˆ†é…æ›´å¤šè®­ç»ƒæœºä¼šã€‚</li>
<li>ä½¿ç”¨DARTåˆ›å»ºçš„æ–°æ•°æ®é›†ä¸“æ³¨äºå›°éš¾æŸ¥è¯¢ï¼Œä¸”è§„æ¨¡è¾ƒå°ã€‚</li>
<li>åˆæˆè¿‡ç¨‹æœªä¾èµ–ä¸“æœ‰GPT-4æ¨¡å‹ï¼Œä»…ä½¿ç”¨7Bå¼€æ”¾æƒé‡æ¨¡å‹ã€‚</li>
<li>DART-MATHæ¨¡å‹åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç®€å•æ‹’ç»è°ƒæ•´å’Œå…ˆå‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.13690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fa4172a198e6ada57dce64c29c0ff81a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a8a54e6bffa70a6a2e07f6b758cd9224.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b8a98d733fec6fdd81179fe6e93495b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-573653138b354a9ce223669e03b904ed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BEADs-Bias-Evaluation-Across-Domains"><a href="#BEADs-Bias-Evaluation-Across-Domains" class="headerlink" title="BEADs: Bias Evaluation Across Domains"></a>BEADs: Bias Evaluation Across Domains</h2><p><strong>Authors:Shaina Raza, Mizanur Rahman, Michael R. Zhang</strong></p>
<p>Recent advancements in large language models (LLMs) have greatly enhanced natural language processing (NLP) applications. Nevertheless, these models often inherit biases from their training data. Despite the availability of various datasets for bias detection, most are limited to one or two NLP tasks (typically classification or evaluation) and lack comprehensive evaluations across a broader range of NLP tasks. To address this gap, we introduce the Bias Evaluations Across Domains BEADs dataset, designed to support a wide array of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key focus of this paper is the gold label dataset that is annotated by GPT4 for scalabilty and verified by experts to ensure high reliability. BEADs provides data for both fine-tuning, including classification and language generation tasks, and for evaluating LLMs. Our findings indicate that BEADs effectively identifies numerous biases when fine-tuned on this dataset. It also reduces biases when used for fine-tuning language generation task, while preserving language quality. The results also reveal some prevalent demographic biases in LLMs when BEADs is used for evaluation in demographic task. We provide the BEADs dataset for detecting biases in various domains, and this dataset is readily usable for responsible AI development and application. The dataset can be accessed at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/shainar/BEAD">https://huggingface.co/datasets/shainar/BEAD</a> . </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æå¤§åœ°å¢å¼ºäº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä»å…¶è®­ç»ƒæ•°æ®ä¸­ç»§æ‰¿äº†åè§ã€‚å°½ç®¡å­˜åœ¨å„ç§ç”¨äºæ£€æµ‹åè§çš„çš„æ•°æ®é›†ï¼Œä½†å¤§å¤šæ•°ä»…é™äºä¸€ä¸ªæˆ–ä¸¤ä¸ªNLPä»»åŠ¡ï¼ˆé€šå¸¸ä¸ºåˆ†ç±»æˆ–è¯„ä¼°ï¼‰ï¼Œå¹¶ä¸”åœ¨æ›´å¹¿æ³›çš„NLPä»»åŠ¡èŒƒå›´å†…ç¼ºä¹ç»¼åˆè¯„ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨åŸŸåè§è¯„ä¼°BEADsæ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒå¹¿æ³›çš„NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€ä»¤ç‰Œåˆ†ç±»ã€åè§é‡åŒ–å’Œè‰¯æ€§è¯­è¨€ç”Ÿæˆã€‚æœ¬æ–‡çš„ä¸€ä¸ªé‡ç‚¹æ˜¯ä½¿ç”¨GPT4è¿›è¡Œæ ‡æ³¨çš„é‡‘æ ‡ç­¾æ•°æ®é›†ï¼Œä»¥å®ç°å¯æ‰©å±•æ€§ï¼Œå¹¶é€šè¿‡ä¸“å®¶éªŒè¯ä»¥ç¡®ä¿é«˜å¯é æ€§ã€‚BEADsæ—¢å¯ç”¨äºå¾®è°ƒï¼ˆåŒ…æ‹¬åˆ†ç±»å’Œè¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼‰ï¼Œä¹Ÿå¯ç”¨äºè¯„ä¼°LLMã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨BEADsæ•°æ®é›†ä¸Šå¾®è°ƒæ—¶ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°è¯†åˆ«å‡ºå¤§é‡åè§ã€‚åœ¨ç”¨äºå¾®è°ƒè¯­è¨€ç”Ÿæˆä»»åŠ¡æ—¶ï¼Œå®ƒè¿˜èƒ½å‡å°‘åè§ï¼ŒåŒæ—¶ä¿æŒè¯­è¨€è´¨é‡ã€‚ç»“æœè¿˜æ˜¾ç¤ºï¼Œåœ¨ä½¿ç”¨BEADsè¿›è¡Œäººå£ç»Ÿè®¡ä»»åŠ¡è¯„ä¼°æ—¶ï¼ŒLLMä¸­å­˜åœ¨ä¸€äº›æ™®éçš„äººå£ç»Ÿè®¡åè§ã€‚æˆ‘ä»¬æä¾›BEADsæ•°æ®é›†ç”¨äºæ£€æµ‹å„ç§é¢†åŸŸçš„åè§ï¼Œæ­¤æ•°æ®é›†å¯ç«‹å³ç”¨äºè´Ÿè´£ä»»çš„AIå¼€å‘å’Œåº”ç”¨ã€‚è¯¥æ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/shainar/bead%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/shainar/BEADè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04220v4">PDF</a> under review</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æå¤§åœ°ä¿ƒè¿›äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨çš„å‘å±•ï¼Œä½†è¿™äº›æ¨¡å‹å¾€å¾€ç»§æ‰¿äº†è®­ç»ƒæ•°æ®ä¸­çš„åè§ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ•°æ®é›†åœ¨åè§æ£€æµ‹æ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è·¨åŸŸåè§è¯„ä¼°æ•°æ®é›†ï¼ˆBEADsï¼‰ï¼Œæ”¯æŒå¤šç§NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€ä»¤ç‰Œåˆ†ç±»ã€åè§é‡åŒ–å’Œè‰¯æ€§è¯­è¨€ç”Ÿæˆã€‚è¯¥æ•°æ®é›†é€šè¿‡GPT4è¿›è¡Œæ ‡æ³¨ï¼Œä»¥æé«˜å¯æ‰©å±•æ€§ï¼Œå¹¶ç”±ä¸“å®¶éªŒè¯ä»¥ç¡®ä¿é«˜å¯é æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒBEADsåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èƒ½æœ‰æ•ˆè¯†åˆ«å¤šç§åè§ï¼Œåœ¨è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ç”¨äºå¾®è°ƒæ—¶ï¼Œèƒ½å‡å°‘åè§å¹¶ä¿æŒè¯­è¨€è´¨é‡ã€‚ç„¶è€Œï¼Œåœ¨ç”¨äºè¯„ä¼°äººå£ç»Ÿè®¡ä»»åŠ¡æ—¶ï¼Œæ­ç¤ºäº†LLMä¸­ä¸€äº›æ™®éçš„äººå£ç»Ÿè®¡åè§ã€‚æˆ‘ä»¬æä¾›BEADsæ•°æ®é›†ç”¨äºæ£€æµ‹ä¸åŒé¢†åŸŸçš„åè§ï¼Œä¾¿äºç”¨äºå¼€å‘å’Œåº”ç”¨è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æœ€æ–°è¿›å±•ä¿ƒè¿›äº†NLPåº”ç”¨çš„å‘å±•ï¼Œä½†å­˜åœ¨ç»§æ‰¿åè§çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰åè§æ£€æµ‹æ•°æ®é›†é€šå¸¸ä»…é™äºä¸€ä¸ªæˆ–ä¸¤ä¸ªNLPä»»åŠ¡ï¼Œç¼ºä¹å…¨é¢çš„è¯„ä¼°ã€‚</li>
<li>ä»‹ç»äº†BEADsæ•°æ®é›†ï¼Œæ”¯æŒå¤šç§NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€ä»¤ç‰Œåˆ†ç±»ã€åè§é‡åŒ–å’Œè‰¯æ€§è¯­è¨€ç”Ÿæˆã€‚</li>
<li>BEADsæ•°æ®é›†é€šè¿‡GPT4è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ç”±ä¸“å®¶éªŒè¯ä»¥ç¡®ä¿å¯é æ€§ã€‚</li>
<li>BEADsèƒ½æœ‰æ•ˆè¯†åˆ«å¤šç§åè§ï¼Œå¹¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å‡å°‘åè§ã€‚</li>
<li>åœ¨è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒBEADsç”¨äºå¾®è°ƒæ—¶èƒ½ä¿æŒè¯­è¨€è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b3d6b63a86e0f20f78a8d906c349592b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2805ce181e1c74c4101c8dfbcdd13856.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-599226bf964dadefdf3d688e3b814a24.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VIoTGPT-Learning-to-Schedule-Vision-Tools-in-LLMs-towards-Intelligent-Video-Internet-of-Things"><a href="#VIoTGPT-Learning-to-Schedule-Vision-Tools-in-LLMs-towards-Intelligent-Video-Internet-of-Things" class="headerlink" title="VIoTGPT: Learning to Schedule Vision Tools in LLMs towards Intelligent   Video Internet of Things"></a>VIoTGPT: Learning to Schedule Vision Tools in LLMs towards Intelligent   Video Internet of Things</h2><p><strong>Authors:Yaoyao Zhong, Mengshi Qi, Rui Wang, Yuhan Qiu, Yang Zhang, Huadong Ma</strong></p>
<p>Video Internet of Things (VIoT) has shown full potential in collecting an unprecedented volume of video data. How to schedule the domain-specific perceiving models and analyze the collected videos uniformly, efficiently, and especially intelligently to accomplish complicated tasks is challenging. To address the challenge, we build VIoTGPT, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to analyze multimedia data collaboratively. To support VIoTGPT and related future works, we meticulously crafted the VIoT-Tool dataset, including the training dataset and the benchmark involving 11 representative vision models across three categories based on semi-automatic annotations. To guide LLM to act as the intelligent agent towards intelligent VIoT, we resort to the ReAct instruction tuning method based on VIoT-Tool to learn the tool capability. Quantitative and qualitative experiments and analyses demonstrate the effectiveness of VIoTGPT. We believe VIoTGPT contributes to improving human-centered experiences in VIoT applications. The project website is <a target="_blank" rel="noopener" href="https://github.com/zhongyy/VIoTGPT">https://github.com/zhongyy/VIoTGPT</a>. </p>
<blockquote>
<p>ç‰©è”ç½‘è§†é¢‘ï¼ˆVIoTï¼‰å·²ç»æ˜¾ç¤ºå‡ºæ”¶é›†å‰æ‰€æœªæœ‰çš„å¤§é‡è§†é¢‘æ•°æ®çš„å·¨å¤§æ½œåŠ›ã€‚å¦‚ä½•ä¸ºç‰¹å®šé¢†åŸŸæ„ŸçŸ¥æ¨¡å‹å®‰æ’æ—¥ç¨‹ï¼Œå¹¶ç»Ÿä¸€ã€é«˜æ•ˆã€å°¤å…¶æ˜¯æ™ºèƒ½åœ°åˆ†ææ‰€æ”¶é›†çš„è§†é¢‘ï¼Œä»¥å®Œæˆå¤æ‚ä»»åŠ¡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„VIoTGPTæ¡†æ¶ï¼Œä»¥ä¾¿æ­£ç¡®ä¸äººç±»äº¤äº’ã€æŸ¥è¯¢çŸ¥è¯†è§†é¢‘ï¼Œå¹¶è°ƒç”¨è§†è§‰æ¨¡å‹ååŒåˆ†æå¤šåª’ä½“æ•°æ®ã€‚ä¸ºäº†æ”¯æŒVIoTGPTå’Œç›¸å…³çš„æœªæ¥å·¥ä½œï¼Œæˆ‘ä»¬ç²¾å¿ƒåˆ¶ä½œäº†VIoT-Toolæ•°æ®é›†ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•é›†ï¼Œæ¶‰åŠåŸºäºåŠè‡ªåŠ¨æ³¨é‡Šçš„ä¸‰ä¸ªç±»åˆ«ä¸­çš„11ä¸ªä»£è¡¨æ€§è§†è§‰æ¨¡å‹ã€‚ä¸ºäº†å¼•å¯¼LLMä½œä¸ºæ™ºèƒ½ç‰©è”ç½‘çš„æ™ºèƒ½ä»£ç†ï¼Œæˆ‘ä»¬å€ŸåŠ©åŸºäºVIoT-Toolçš„ReActæŒ‡ä»¤è°ƒæ•´æ–¹æ³•å­¦ä¹ å·¥å…·èƒ½åŠ›ã€‚å®šé‡å’Œå®šæ€§çš„å®éªŒä¸åˆ†æè¯æ˜äº†VIoTGPTçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡VIoTGPTæœ‰åŠ©äºæé«˜ä»¥äººç±»ä¸ºä¸­å¿ƒçš„ç‰©è”ç½‘è§†é¢‘åº”ç”¨ä½“éªŒã€‚é¡¹ç›®ç½‘ç«™æ˜¯<a target="_blank" rel="noopener" href="https://github.com/zhongyy/VIoTGPT%E3%80%82">https://github.com/zhongyy/VIoTGPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00401v2">PDF</a> AAAI 2025, 12 pages</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘ç‰©è”ç½‘ï¼ˆVIoTï¼‰æ”¶é›†äº†å¤§é‡å‰æ‰€æœªæœ‰çš„è§†é¢‘æ•°æ®ï¼Œå¦‚ä½•è°ƒåº¦ç‰¹å®šé¢†åŸŸçš„æ„ŸçŸ¥æ¨¡å‹å¹¶ç»Ÿä¸€åˆ†æè¿™äº›æ•°æ®ï¼Œä»¥å®Œæˆå¤æ‚ä»»åŠ¡æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†åŸºäºLLMçš„VIoTGPTæ¡†æ¶ï¼Œå¯æ­£ç¡®ä¸äººç±»äº’åŠ¨ï¼ŒæŸ¥è¯¢è§†é¢‘çŸ¥è¯†å¹¶è°ƒç”¨è§†è§‰æ¨¡å‹ä»¥ååŒåˆ†æå¤šåª’ä½“æ•°æ®ã€‚ä¸ºæ”¯æŒVIoTGPTåŠç›¸å…³æœªæ¥å·¥ä½œï¼Œæˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†VIoT-Toolæ•°æ®é›†ï¼ŒåŒ…å«è®­ç»ƒæ•°æ®é›†å’Œæ¶‰åŠä¸‰ä¸ªç±»åˆ«ä¸­11ç§ä»£è¡¨æ€§è§†è§‰æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼ŒåŸºäºåŠè‡ªåŠ¨æ³¨é‡Šã€‚ä¸ºå¼•å¯¼LLMæˆä¸ºé¢å‘æ™ºèƒ½VIoTçš„æ™ºèƒ½ä»£ç†ï¼Œæˆ‘ä»¬å€ŸåŠ©VIoT-Toolé‡‡ç”¨ReActæŒ‡ä»¤è°ƒæ•´æ–¹æ³•å­¦ä¹ å·¥å…·èƒ½åŠ›ã€‚å®šé‡å’Œå®šæ€§å®éªŒä¸åˆ†æè¯æ˜äº†VIoTGPTçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡VIoTGPTæœ‰åŠ©äºæé«˜ä»¥äººç±»ä¸ºä¸­å¿ƒåœ¨VIoTåº”ç”¨ä¸­çš„ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIoTå±•ç°äº†å‰æ‰€æœªæœ‰çš„è§†é¢‘æ•°æ®æ”¶é›†æ½œåŠ›ã€‚</li>
<li>è°ƒåº¦ç‰¹å®šé¢†åŸŸçš„æ„ŸçŸ¥æ¨¡å‹å¹¶åˆ†æè§†é¢‘æ•°æ®æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>VIoTGPTæ¡†æ¶åŸºäºLLMå»ºç«‹ï¼Œæ”¯æŒäººç±»äº’åŠ¨ã€è§†é¢‘çŸ¥è¯†æŸ¥è¯¢å’Œå¤šåª’ä½“æ•°æ®ååŒåˆ†æã€‚</li>
<li>VIoT-Toolæ•°æ®é›†åŒ…å«è®­ç»ƒæ•°æ®å’Œæ¶µç›–å¤šç§è§†è§‰æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>é‡‡ç”¨ReActæŒ‡ä»¤è°ƒæ•´æ–¹æ³•å¼•å¯¼LLMé€‚åº”æ™ºèƒ½VIoTè§’è‰²ã€‚</li>
<li>VIoTGPTçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å®šé‡å’Œå®šæ€§å®éªŒçš„æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-76c809847456c2382d93dd3013c569e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-692363ca797689739dd577bd3d1ff66e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2b656f24b01f8ff044e7448ec5fae1c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-83766c9c451be4dbc6b1a40d57e99b0c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e7e55dad4ae7006be3d05ada5fa012d9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;æœ¬ç¯‡
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-27  Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a25a8cff2f35a042343caae6099f6e12.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-26  Extracting triples from dialogues for conversational social agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">7760.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
