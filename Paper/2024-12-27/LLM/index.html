<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2024-12-27  Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-27-更新"><a href="#2024-12-27-更新" class="headerlink" title="2024-12-27 更新"></a>2024-12-27 更新</h1><h2 id="Unleashing-the-Temporal-Spatial-Reasoning-Capacity-of-GPT-for-Training-Free-Audio-and-Language-Referenced-Video-Object-Segmentation"><a href="#Unleashing-the-Temporal-Spatial-Reasoning-Capacity-of-GPT-for-Training-Free-Audio-and-Language-Referenced-Video-Object-Segmentation" class="headerlink" title="Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation"></a>Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation</h2><p><strong>Authors:Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu</strong></p>
<p>In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2) pipeline to explore the training-free paradigm for audio and language-referenced video object segmentation, namely AVS and RVOS tasks. The intuitive solution leverages GroundingDINO to identify the target object from a single frame and SAM 2 to segment the identified object throughout the video, which is less robust to spatiotemporal variations due to a lack of video context exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt. Within GPT-PS, two task-specific Chain-of-Thought prompts are designed to unleash GPT’s temporal-spatial reasoning capacity by guiding GPT to make selections based on a comprehensive understanding of video and reference information. Furthermore, we propose a Language-Binded Reference Unification (LBRU) module to convert audio signals into language-formatted references, thereby unifying the formats of AVS and RVOS tasks in the same pipeline. Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2 pipeline achieves performances comparable to or even better than fully-supervised fine-tuning methods. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/appletea233/AL-Ref-SAM2">https://github.com/appletea233/AL-Ref-SAM2</a>. </p>
<blockquote>
<p>本文提出了一种基于音频-语言参考的SAM 2（AL-Ref-SAM 2）管道，用于探索音频和语言参考的视频对象分割的无训练范式，即AVS和RVOS任务。该直观解决方案利用GroundingDINO从单帧中识别目标对象，并使用SAM 2在整个视频中分割已识别的对象。由于缺乏对视频上下文探索，该方案对时空变化不太稳健。因此，在我们的AL-Ref-SAM 2管道中，我们提出了一种新颖的GPT辅助支点选择（GPT-PS）模块，以指导GPT-4执行两步时空推理，从而按顺序选择支点帧和支点框，为SAM 2提供高质量的初始对象提示。在GPT-PS中，设计了两个针对任务的思维链提示，通过引导GPT基于视频和参考信息的全面理解进行选择，以激发GPT的时空推理能力。此外，我们提出了一种语言绑定参考统一（LBRU）模块，将音频信号转换为语言格式的参考，从而统一管道中AVS和RVOS任务格式。在这两项任务上的大量实验表明，我们的无训练AL-Ref-SAM 2管道的性能可与完全监督的微调方法相媲美甚至更好。代码可通过以下网址获得：<a target="_blank" rel="noopener" href="https://github.com/appletea233/AL-Ref-SAM2">https://github.com/appletea233/AL-Ref-SAM2</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15876v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于音频-语言参考的SAM 2（AL-Ref-SAM 2）管道，用于探索无需训练即可进行音频和视频参考的对象分割任务。该方法利用GroundingDINO识别目标对象，使用SAM 2在视频中对识别对象进行分割。为了克服由于缺乏视频上下文探索导致的时空变化鲁棒性不足的问题，引入了GPT辅助的枢轴选择（GPT-PS）模块，指导GPT-4进行两步时空推理，选择枢轴帧和枢轴框，为SAM 2提供高质量的对象初始提示。同时，提出了语言绑定参考统一（LBRU）模块，将音频信号转换为语言格式的参考，统一AVS和RVOS任务在同一管道中的格式。实验表明，该训练免费的AL-Ref-SAM 2管道的性能可与或优于完全监督的微调方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于音频-语言参考的SAM 2管道（AL-Ref-SAM 2），用于无需训练的音频和视频参考对象分割任务。</li>
<li>利用GroundingDINO识别目标对象，通过SAM 2进行视频分割。</li>
<li>引入GPT辅助的枢轴选择（GPT-PS）模块以提高视频分割的鲁棒性。</li>
<li>GPT-PS模块通过指导GPT-4进行两步时空推理来选择枢轴帧和枢轴框。</li>
<li>提出语言绑定参考统一（LBRU）模块，将音频信号转换为语言格式参考，统一不同任务的格式。</li>
<li>在两种任务上进行的广泛实验表明，AL-Ref-SAM 2管道性能优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15876">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-29d81c609b54a1597ec2499111f32127.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-11b59583ae221e351d4da6fe6afb2862.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b3c91e8a38026cfee85a54d33c4d01f8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8231f40e1fa73337fa488e4efd96af82.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d3f23d1ea89c51c4d6304a92fe3a50d5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c1bf18e0f0064614577191dd30f4991a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SpikingSSMs-Learning-Long-Sequences-with-Sparse-and-Parallel-Spiking-State-Space-Models"><a href="#SpikingSSMs-Learning-Long-Sequences-with-Sparse-and-Parallel-Spiking-State-Space-Models" class="headerlink" title="SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking   State Space Models"></a>SpikingSSMs: Learning Long Sequences with Sparse and Parallel Spiking   State Space Models</h2><p><strong>Authors:Shuaijie Shen, Chao Wang, Renzhuo Huang, Yan Zhong, Qinghai Guo, Zhichao Lu, Jianguo Zhang, Luziwei Leng</strong></p>
<p>Known as low energy consumption networks, spiking neural networks (SNNs) have gained a lot of attention within the past decades. While SNNs are increasing competitive with artificial neural networks (ANNs) for vision tasks, they are rarely used for long sequence tasks, despite their intrinsic temporal dynamics. In this work, we develop spiking state space models (SpikingSSMs) for long sequence learning by leveraging on the sequence learning abilities of state space models (SSMs). Inspired by dendritic neuron structure, we hierarchically integrate neuronal dynamics with the original SSM block, meanwhile realizing sparse synaptic computation. Furthermore, to solve the conflict of event-driven neuronal dynamics with parallel computing, we propose a light-weight surrogate dynamic network which accurately predicts the after-reset membrane potential and compatible to learnable thresholds, enabling orders of acceleration in training speed compared with conventional iterative methods. On the long range arena benchmark task, SpikingSSM achieves competitive performance to state-of-the-art SSMs meanwhile realizing on average 90% of network sparsity. On language modeling, our network significantly surpasses existing spiking large language models (spikingLLMs) on the WikiText-103 dataset with only a third of the model size, demonstrating its potential as backbone architecture for low computation cost LLMs. </p>
<blockquote>
<p>被称为低功耗神经网络，脉冲神经网络（SNNs）在过去的几十年中引起了广泛关注。虽然SNNs在视觉任务方面与人工神经网络（ANNs）的竞争日益激烈，但它们很少用于长序列任务，尽管它们具有内在的时空动态特性。在这项工作中，我们通过利用状态空间模型（SSMs）的序列学习能力，开发用于长序列学习的脉冲状态空间模型（SpikingSSMs）。受树突神经元结构的启发，我们分层地将神经元动态与原始SSM块集成，同时实现稀疏突触计算。此外，为了解决事件驱动神经元动力学与并行计算之间的冲突，我们提出了一个轻量级的替代动态网络，该网络能够准确预测重置后的膜电位并且与可学习的阈值兼容，与常规迭代方法相比，训练速度提高了几个数量级。在远程区域基准任务上，SpikingSSM实现了与最新SSM的竞争性能，同时实现了平均90%的网络稀疏性。在语言建模方面，我们的网络在WikiText-103数据集上的表现显著超越了现有的脉冲大型语言模型（spikingLLMs），并且只有三分之一模型大小，显示出其作为低计算成本LLM的骨干架构的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14909v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了尖峰状态空间模型（SpikingSSMs）在长序列学习方面的应用。该模型结合了状态空间模型（SSMs）的序列学习能力，并利用树突神经元结构的启发，实现了神经元动力学的层次集成和稀疏突触计算。为解决事件驱动神经元动力学与并行计算的冲突，提出了轻量级替代动态网络，可准确预测重置后的膜电位并与学习阈值兼容，使得训练速度比传统迭代方法有所加快。在长范围区域基准测试中，SpikingSSM实现了与最新SSM技术的竞争性能，同时实现了平均90%的网络稀疏性。在语言建模方面，该网络在WikiText-103数据集上的表现显著优于现有的尖峰大型语言模型（spikingLLMs），仅使用了三分之一大小的模型，显示出其作为低计算成本大型语言模型（LLMs）的潜在架构价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Spiking state space models (SpikingSSMs)结合了状态空间模型（SSMs）的序列学习能力，用于长序列学习。</li>
<li>层级结合神经元动力学和原始SSM模块，受到树突神经元结构的启发。</li>
<li>提出轻量级替代动态网络，解决事件驱动神经元动力学与并行计算的冲突。</li>
<li>SpikingSSM在长范围区域基准测试中实现了与最新SSM技术的竞争性能，并实现了网络稀疏性。</li>
<li>在语言建模方面，该网络在WikiText-103数据集上的表现优于现有的尖峰大型语言模型（spikingLLMs）。</li>
<li>该网络架构具有潜力成为低计算成本的大型语言模型（LLMs）的骨干架构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.14909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3c406ce708aaddbd0d21ca371d64015e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-39b41e0498396fd9e345e35b01107f84.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-49b5cd643fde0b70b6b37d44e45d85c8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-db1f077e142795679d313a643dc4fa46.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d52d3e7936315cbc193c94a461000adb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Security-Attacks-on-LLM-based-Code-Completion-Tools"><a href="#Security-Attacks-on-LLM-based-Code-Completion-Tools" class="headerlink" title="Security Attacks on LLM-based Code Completion Tools"></a>Security Attacks on LLM-based Code Completion Tools</h2><p><strong>Authors:Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang</strong></p>
<p>The rapid development of large language models (LLMs) has significantly advanced code completion capabilities, giving rise to a new generation of LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these tools possess unique workflows, integrating multiple information sources as input and prioritizing code suggestions over natural language interaction, which introduces distinct security challenges. Additionally, LCCTs often rely on proprietary code datasets for training, raising concerns about the potential exposure of sensitive data. This paper exploits these distinct characteristics of LCCTs to develop targeted attack methodologies on two critical security risks: jailbreaking and training data extraction attacks. Our experimental results expose significant vulnerabilities within LCCTs, including a 99.4% success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate on Amazon Q. Furthermore, We successfully extracted sensitive user data from GitHub Copilot, including 54 real email addresses and 314 physical addresses associated with GitHub usernames. Our study also demonstrates that these code-based attack methods are effective against general-purpose LLMs, such as the GPT series, highlighting a broader security misalignment in the handling of code by modern LLMs. These findings underscore critical security challenges associated with LCCTs and suggest essential directions for strengthening their security frameworks. The example code and attack samples from our research are provided at <a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">https://github.com/Sensente/Security-Attacks-on-LCCTs</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的快速发展极大地提升了代码补全功能，并催生了一代基于LLM的代码补全工具（LCCTs）。与一般用途的LLM不同，这些工具拥有独特的工作流程，它们整合多种信息源作为输入，并优先于自然语言交互给出代码建议，这为代码补全工具带来独特的挑战。此外，LCCT通常依赖专有代码数据集进行训练，引发了关于潜在敏感数据泄露的担忧。本文利用LCCT的这些独特特点，针对两种关键安全风险制定了有针对性的攻击方法：越狱攻击和训练数据提取攻击。我们的实验结果揭示了LCCTs中存在的重大漏洞，包括对GitHub Copilot的越狱攻击成功率为99.4%，对亚马逊Q的成功率为46.3%。此外，我们还成功地从GitHub Copilot中提取了用户的敏感数据，包括54个真实电子邮件地址和与GitHub用户名相关的314个实体地址。我们的研究还表明，这些基于代码的攻击方法对通用LLM（如GPT系列）同样有效，这凸显了现代LLM在处理代码方面的安全配置不当问题。这些发现强调了LCCT面临的关键安全挑战，并为加强其安全框架提供了重要方向。我们的研究中的示例代码和攻击样本可在<a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">https://github.com/Sensente/Security-Attacks-on-LCCTs</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11006v3">PDF</a> Paper accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的快速发展推动了代码补全能力的显著增强，出现了新一代基于LLM的代码补全工具（LCCTs）。这些工具具有独特的工作流程，整合多种信息源作为输入，优先提供代码建议而非自然语言交互，从而带来了新的安全挑战。此外，LCCTs通常依赖专有代码数据集进行训练，引发了关于潜在敏感数据泄露的担忧。本文利用LCCTs的独特特点，针对两项关键安全风险设计了有针对性的攻击方法：越狱攻击和训练数据提取攻击。实验结果显示，LCCTs存在重大漏洞，包括对GitHub Copilot的越狱攻击成功率高达99.4%，对亚马逊Q的成功率为46.3%。此外，我们还成功从GitHub Copilot提取了用户的敏感数据，包括54个真实电子邮件地址和314个与GitHub用户名关联的物理地址。我们的研究还表明，这些基于代码的攻击方法对通用LLM（如GPT系列）同样有效，突显了现代LLM在处理代码时的广泛安全偏差。本研究的示例代码和攻击样本可在<a target="_blank" rel="noopener" href="https://github.com/Sensente/Security-Attacks-on-LCCTs">链接</a>找到。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM的快速发展推动了代码补全工具（LCCTs）的进步。</li>
<li>LCCTs具有独特的工作流程和安全挑战，涉及多个信息源的整合和代码建议的优先提供。</li>
<li>LCCTs依赖专有代码数据集进行训练，引发敏感数据泄露的担忧。</li>
<li>针对LCCTs存在越狱攻击和训练数据提取攻击的重大漏洞。</li>
<li>实验显示GitHub Copilot等LCCTs工具存在高风险漏洞，攻击者可成功提取用户敏感数据。</li>
<li>这些攻击方法对通用LLM同样有效，显示现代LLM在处理代码时的安全缺陷。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-194caa365833fd9f98d39865a3f5521c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5ef0e203f35c596121a9b31bedd5003e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96ffdb2421a926b5021a29eb466a1351.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3898f8a714d76e2e08a1dc4ab7bf4a80.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Advancing-Mental-Health-Pre-Screening-A-New-Custom-GPT-for-Psychological-Distress-Assessment"><a href="#Advancing-Mental-Health-Pre-Screening-A-New-Custom-GPT-for-Psychological-Distress-Assessment" class="headerlink" title="Advancing Mental Health Pre-Screening: A New Custom GPT for   Psychological Distress Assessment"></a>Advancing Mental Health Pre-Screening: A New Custom GPT for   Psychological Distress Assessment</h2><p><strong>Authors:Jinwen Tang, Yi Shang</strong></p>
<p>This study introduces ‘Psycho Analyst’, a custom GPT model based on OpenAI’s GPT-4, optimized for pre-screening mental health disorders. Enhanced with DSM-5, PHQ-8, detailed data descriptions, and extensive training data, the model adeptly decodes nuanced linguistic indicators of mental health disorders. It utilizes a dual-task framework that includes binary classification and a three-stage PHQ-8 score computation involving initial assessment, detailed breakdown, and independent assessment, showcasing refined analytic capabilities. Validation with the DAIC-WOZ dataset reveals F1 and Macro-F1 scores of 0.929 and 0.949, respectively, along with the lowest MAE and RMSE of 2.89 and 3.69 in PHQ-8 scoring. These results highlight the model’s precision and transformative potential in enhancing public mental health support, improving accessibility, cost-effectiveness, and serving as a second opinion for professionals. </p>
<blockquote>
<p>本研究介绍了“心理分析专家”，这是一个基于OpenAI的GPT-4的自定义GPT模型，经过优化，用于预先筛选精神健康障碍。通过DSM-5、PHQ-8、详细数据描述和大量的训练数据增强，该模型能够熟练地解码精神健康障碍的微妙语言指标。它采用双重任务框架，包括二元分类和涉及初步评估、详细分解和独立评估的三阶段PHQ-8评分计算，展示了精细的分析能力。使用DAIC-WOZ数据集进行验证，结果显示F1和宏观F1得分分别为0.929和0.949，在PHQ-8评分中平均绝对误差和均方根误差最低，分别为2.89和3.69。这些结果突出了该模型的精确度和在增强公众精神健康支持、提高可及性、成本效益以及作为专业人士的第二意见方面的变革潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01614v2">PDF</a> Accepted by IEEE CogMI 2024</p>
<p><strong>Summary</strong></p>
<p>一项研究推出了名为“精神分析专家”的GPT模型，该模型基于OpenAI的GPT-4构建，专门用于预先筛选精神健康疾病。该模型结合了DSM-5、PHQ-8等标准工具，通过详细的数据描述和大量的训练数据，能够精准解读语言中微妙的心理健康障碍迹象。采用双重任务框架，包括二元分类和涉及初步评估、详细分解和独立评估的三阶段PHQ-8评分计算，展现出精细的分析能力。使用DAIC-WOZ数据集验证，模型的F1和宏观F1得分分别为0.929和0.949，在PHQ-8评分方面的平均绝对误差和均方根误差最低，分别为2.89和3.69。这一成果展现了模型在公众精神健康支持方面的精准度和革新潜力，提高了可及性和成本效益，可成为专业人士的第二意见参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>“Psycho Analyst”是基于GPT-4的定制模型，专为预筛精神健康疾病设计。</li>
<li>模型结合了DSM-5、PHQ-8等标准工具，具备解读语言中的精神健康障碍迹象的能力。</li>
<li>采用双重任务框架，包括二元分类和PHQ-8评分计算的三阶段过程。</li>
<li>模型展现出色的性能，经DAIC-WOZ数据集验证，F1和宏观F1得分高。</li>
<li>模型在PHQ-8评分方面的平均绝对误差和均方根误差达到较低水平。</li>
<li>模型在公众精神健康支持方面具备精准度和革新潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01614">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-73225adde3f9f3011ba6b3913c7dbcff.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e86a39a9e574397ca3585de75a700199.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5618ab2c3912992cafd51a46376ded6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bdecf9a16bf092e368193b066c56dc36.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DART-Math-Difficulty-Aware-Rejection-Tuning-for-Mathematical-Problem-Solving"><a href="#DART-Math-Difficulty-Aware-Rejection-Tuning-for-Mathematical-Problem-Solving" class="headerlink" title="DART-Math: Difficulty-Aware Rejection Tuning for Mathematical   Problem-Solving"></a>DART-Math: Difficulty-Aware Rejection Tuning for Mathematical   Problem-Solving</h2><p><strong>Authors:Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He</strong></p>
<p>Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries. Hypothesizing that difficult queries are crucial to learn complex reasoning, we propose Difficulty-Aware Rejection Tuning (DART), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples. Utilizing DART, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4. We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called DART-MATH. In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, DART-MATH outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. </p>
<blockquote>
<p>解决数学问题需要高级推理能力，对于大型语言模型来说，这构成了显著的挑战。以前的工作通常从专有模型中合成数据来增强现有数据集，然后通过指令微调来获得顶级结果。然而，我们对这些数据集的分析表明，它们存在严重的偏向简单查询的偏见，对于最具挑战性的查询，经常无法生成任何正确的回应。我们假设困难查询对于学习复杂推理至关重要，因此提出了困难感知拒绝调整（DART）的方法，该方法在合成阶段为困难查询分配更多的试验，使它们在困难样本上进行更广泛的训练。利用DART，我们已经创建了专注于困难查询的数学问题求解新数据集，并且这些数据集比以前的要小得多。值得注意的是，我们的合成过程仅依赖于一个7B大小的开放权重模型，不需要常用的专有GPT-4。我们在自己的数据集上微调了从7B到70B的各种基础模型，从而产生了一系列强大的模型，称为DART-MATH。在6个数学基准的全面领域内部和领域外部评估中，DART-MATH显著优于普通的拒绝调整，并且在之前的技术中表现优越或相当，尽管它使用的数据集更小，而且没有使用专有模型。此外，我们的结果将我们的合成数据集定位为推进数学问题求解方面最有效且最经济的公开资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.13690v2">PDF</a> NeurIPS 2024. Data and model checkpoints are available at   <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/dart-math">https://github.com/hkust-nlp/dart-math</a></p>
<p><strong>Summary</strong>：解决数学问题需要高级推理能力，对大语言模型提出了挑战。过去的研究通常通过合成数据和微调指令来增强现有数据集以实现顶尖结果。然而，分析发现这些数据集存在严重偏向简单查询的问题，对最复杂的查询往往无法给出正确回应。因此，我们提出难度感知拒绝调整（DART）方法，在合成阶段为困难查询分配更多试验次数，以便对困难样本进行更广泛的训练。使用DART，我们创建了专注于困难查询的数学问题求解新数据集，且比之前的数据集小得多。合成过程仅依赖于7B大小的开放权重模型，未使用常见的专有GPT-4。我们在不同大小（从7B到70B）的基准模型上进行微调，形成一系列强大的DART-MATH模型。在6个数学基准测试上进行全面的域内和域外评估，DART-MATH显著优于简单的拒绝调整，且优于或相当于以前的技术，尽管使用了较小的数据集且没有专有模型。此外，我们的结果将我们的合成数据集定位为推进数学问题求解方面最有效且最经济的公开资源。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>解决数学问题需要高级推理能力，对大语言模型提出了挑战。</li>
<li>现有数据集存在偏向简单查询的问题，难以应对复杂查询的挑战。</li>
<li>提出难度感知拒绝调整（DART）方法，为困难查询分配更多训练机会。</li>
<li>使用DART创建的新数据集专注于困难查询，且规模较小。</li>
<li>合成过程未依赖专有GPT-4模型，仅使用7B开放权重模型。</li>
<li>DART-MATH模型在多个数学基准测试中表现优异，优于简单拒绝调整和先前技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.13690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fa4172a198e6ada57dce64c29c0ff81a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a8a54e6bffa70a6a2e07f6b758cd9224.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b8a98d733fec6fdd81179fe6e93495b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-573653138b354a9ce223669e03b904ed.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BEADs-Bias-Evaluation-Across-Domains"><a href="#BEADs-Bias-Evaluation-Across-Domains" class="headerlink" title="BEADs: Bias Evaluation Across Domains"></a>BEADs: Bias Evaluation Across Domains</h2><p><strong>Authors:Shaina Raza, Mizanur Rahman, Michael R. Zhang</strong></p>
<p>Recent advancements in large language models (LLMs) have greatly enhanced natural language processing (NLP) applications. Nevertheless, these models often inherit biases from their training data. Despite the availability of various datasets for bias detection, most are limited to one or two NLP tasks (typically classification or evaluation) and lack comprehensive evaluations across a broader range of NLP tasks. To address this gap, we introduce the Bias Evaluations Across Domains BEADs dataset, designed to support a wide array of NLP tasks, including text classification, token classification, bias quantification, and benign language generation. A key focus of this paper is the gold label dataset that is annotated by GPT4 for scalabilty and verified by experts to ensure high reliability. BEADs provides data for both fine-tuning, including classification and language generation tasks, and for evaluating LLMs. Our findings indicate that BEADs effectively identifies numerous biases when fine-tuned on this dataset. It also reduces biases when used for fine-tuning language generation task, while preserving language quality. The results also reveal some prevalent demographic biases in LLMs when BEADs is used for evaluation in demographic task. We provide the BEADs dataset for detecting biases in various domains, and this dataset is readily usable for responsible AI development and application. The dataset can be accessed at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/shainar/BEAD">https://huggingface.co/datasets/shainar/BEAD</a> . </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步极大地增强了自然语言处理（NLP）应用的能力。然而，这些模型往往从其训练数据中继承了偏见。尽管存在各种用于检测偏见的的数据集，但大多数仅限于一个或两个NLP任务（通常为分类或评估），并且在更广泛的NLP任务范围内缺乏综合评估。为了弥补这一空白，我们引入了跨域偏见评估BEADs数据集，旨在支持广泛的NLP任务，包括文本分类、令牌分类、偏见量化和良性语言生成。本文的一个重点是使用GPT4进行标注的金标签数据集，以实现可扩展性，并通过专家验证以确保高可靠性。BEADs既可用于微调（包括分类和语言生成任务），也可用于评估LLM。我们的研究结果表明，在BEADs数据集上微调时，它能有效地识别出大量偏见。在用于微调语言生成任务时，它还能减少偏见，同时保持语言质量。结果还显示，在使用BEADs进行人口统计任务评估时，LLM中存在一些普遍的人口统计偏见。我们提供BEADs数据集用于检测各种领域的偏见，此数据集可立即用于负责任的AI开发和应用。该数据集可通过<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/shainar/bead%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/shainar/BEAD访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04220v4">PDF</a> under review</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）的最新进展极大地促进了自然语言处理（NLP）应用的发展，但这些模型往往继承了训练数据中的偏见。为了解决现有数据集在偏见检测方面的局限性，我们推出了跨域偏见评估数据集（BEADs），支持多种NLP任务，包括文本分类、令牌分类、偏见量化和良性语言生成。该数据集通过GPT4进行标注，以提高可扩展性，并由专家验证以确保高可靠性。研究表明，BEADs在微调过程中能有效识别多种偏见，在语言生成任务中用于微调时，能减少偏见并保持语言质量。然而，在用于评估人口统计任务时，揭示了LLM中一些普遍的人口统计偏见。我们提供BEADs数据集用于检测不同领域的偏见，便于用于开发和应用负责任的人工智能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的最新进展促进了NLP应用的发展，但存在继承偏见的问题。</li>
<li>现有偏见检测数据集通常仅限于一个或两个NLP任务，缺乏全面的评估。</li>
<li>介绍了BEADs数据集，支持多种NLP任务，包括文本分类、令牌分类、偏见量化和良性语言生成。</li>
<li>BEADs数据集通过GPT4进行标注，并由专家验证以确保可靠性。</li>
<li>BEADs能有效识别多种偏见，并在微调过程中减少偏见。</li>
<li>在语言生成任务中，BEADs用于微调时能保持语言质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b3d6b63a86e0f20f78a8d906c349592b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2805ce181e1c74c4101c8dfbcdd13856.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-599226bf964dadefdf3d688e3b814a24.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VIoTGPT-Learning-to-Schedule-Vision-Tools-in-LLMs-towards-Intelligent-Video-Internet-of-Things"><a href="#VIoTGPT-Learning-to-Schedule-Vision-Tools-in-LLMs-towards-Intelligent-Video-Internet-of-Things" class="headerlink" title="VIoTGPT: Learning to Schedule Vision Tools in LLMs towards Intelligent   Video Internet of Things"></a>VIoTGPT: Learning to Schedule Vision Tools in LLMs towards Intelligent   Video Internet of Things</h2><p><strong>Authors:Yaoyao Zhong, Mengshi Qi, Rui Wang, Yuhan Qiu, Yang Zhang, Huadong Ma</strong></p>
<p>Video Internet of Things (VIoT) has shown full potential in collecting an unprecedented volume of video data. How to schedule the domain-specific perceiving models and analyze the collected videos uniformly, efficiently, and especially intelligently to accomplish complicated tasks is challenging. To address the challenge, we build VIoTGPT, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to analyze multimedia data collaboratively. To support VIoTGPT and related future works, we meticulously crafted the VIoT-Tool dataset, including the training dataset and the benchmark involving 11 representative vision models across three categories based on semi-automatic annotations. To guide LLM to act as the intelligent agent towards intelligent VIoT, we resort to the ReAct instruction tuning method based on VIoT-Tool to learn the tool capability. Quantitative and qualitative experiments and analyses demonstrate the effectiveness of VIoTGPT. We believe VIoTGPT contributes to improving human-centered experiences in VIoT applications. The project website is <a target="_blank" rel="noopener" href="https://github.com/zhongyy/VIoTGPT">https://github.com/zhongyy/VIoTGPT</a>. </p>
<blockquote>
<p>物联网视频（VIoT）已经显示出收集前所未有的大量视频数据的巨大潜力。如何为特定领域感知模型安排日程，并统一、高效、尤其是智能地分析所收集的视频，以完成复杂任务是一个挑战。为了应对这一挑战，我们构建了基于大语言模型（LLM）的VIoTGPT框架，以便正确与人类交互、查询知识视频，并调用视觉模型协同分析多媒体数据。为了支持VIoTGPT和相关的未来工作，我们精心制作了VIoT-Tool数据集，包括训练数据集和基准测试集，涉及基于半自动注释的三个类别中的11个代表性视觉模型。为了引导LLM作为智能物联网的智能代理，我们借助基于VIoT-Tool的ReAct指令调整方法学习工具能力。定量和定性的实验与分析证明了VIoTGPT的有效性。我们相信VIoTGPT有助于提高以人类为中心的物联网视频应用体验。项目网站是<a target="_blank" rel="noopener" href="https://github.com/zhongyy/VIoTGPT%E3%80%82">https://github.com/zhongyy/VIoTGPT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00401v2">PDF</a> AAAI 2025, 12 pages</p>
<p><strong>Summary</strong></p>
<p>视频物联网（VIoT）收集了大量前所未有的视频数据，如何调度特定领域的感知模型并统一分析这些数据，以完成复杂任务是一大挑战。为解决此问题，我们建立了基于LLM的VIoTGPT框架，可正确与人类互动，查询视频知识并调用视觉模型以协同分析多媒体数据。为支持VIoTGPT及相关未来工作，我们精心构建了VIoT-Tool数据集，包含训练数据集和涉及三个类别中11种代表性视觉模型的基准测试，基于半自动注释。为引导LLM成为面向智能VIoT的智能代理，我们借助VIoT-Tool采用ReAct指令调整方法学习工具能力。定量和定性实验与分析证明了VIoTGPT的有效性。我们相信VIoTGPT有助于提高以人类为中心在VIoT应用中的体验。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIoT展现了前所未有的视频数据收集潜力。</li>
<li>调度特定领域的感知模型并分析视频数据是一大挑战。</li>
<li>VIoTGPT框架基于LLM建立，支持人类互动、视频知识查询和多媒体数据协同分析。</li>
<li>VIoT-Tool数据集包含训练数据和涵盖多种视觉模型的基准测试。</li>
<li>采用ReAct指令调整方法引导LLM适应智能VIoT角色。</li>
<li>VIoTGPT的有效性得到了定量和定性实验的支持。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00401">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-76c809847456c2382d93dd3013c569e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-692363ca797689739dd577bd3d1ff66e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2b656f24b01f8ff044e7448ec5fae1c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-83766c9c451be4dbc6b1a40d57e99b0c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e7e55dad4ae7006be3d05ada5fa012d9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-27/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-27/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="far fa-dot-circle"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b169c2049bd4c74cfa72d3579703085c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2024-12-27  Unleashing the Temporal-Spatial Reasoning Capacity of GPT for   Training-Free Audio and Language Referenced Video Object Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-26/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a25a8cff2f35a042343caae6099f6e12.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2024-12-26  Extracting triples from dialogues for conversational social agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">7760.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
