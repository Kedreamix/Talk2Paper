<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-15  HybridVLA Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-15-更新"><a href="#2025-03-15-更新" class="headerlink" title="2025-03-15 更新"></a>2025-03-15 更新</h1><h2 id="HybridVLA-Collaborative-Diffusion-and-Autoregression-in-a-Unified-Vision-Language-Action-Model"><a href="#HybridVLA-Collaborative-Diffusion-and-Autoregression-in-a-Unified-Vision-Language-Action-Model" class="headerlink" title="HybridVLA: Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model"></a>HybridVLA: Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model</h2><p><strong>Authors:Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang</strong></p>
<p>Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations. </p>
<blockquote>
<p>最近，用于常识推理的视觉语言模型（VLMs）的进步推动了视觉语言行动（VLA）模型的发展，使机器人能够执行通用操作。尽管现有的自回归VLA方法利用大规模预训练知识，但它们会破坏行动的连续性。同时，一些VLA方法加入了一个额外的扩散头来预测连续行动，仅依赖于VLM提取的特征，这限制了其推理能力。在本文中，我们介绍了HybridVLA，这是一个统一框架，能够在一个大型语言模型中无缝集成自回归和扩散策略的优点，而不是简单地连接它们。为了弥合生成差距，我们提出了一种协作训练配方，将扩散建模直接注入下一个令牌预测。通过这一配方，我们发现这两种行动预测不仅相互加强，而且在不同任务上的表现各不相同。因此，我们设计了一种协作行动集成机制，自适应地融合这两种预测，从而实现更稳健的控制。在实验中，HybridVLA在各种仿真和真实任务中表现出超越先前最先进的VLA方法的效果，包括单臂和双臂机器人，同时在以前未见过的配置中表现出稳定的操作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10631v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了HybridVLA框架，该框架结合了自回归和扩散策略的优势，在一个大型语言模型中无缝集成，以强化机器人执行通用操作的能力。通过协作训练策略，将扩散建模直接注入下一个令牌预测中，以缩小生成差距。此外，还设计了一种协作行动集成机制，自适应地融合两种预测，以实现更稳健的控制。在模拟和真实世界的实验中，HybridVLA在单臂和双臂机器人任务中均优于先前的主流VLA方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HybridVLA是一个新的框架，结合了自回归和扩散策略的优势，增强了机器人执行通用操作的能力。</li>
<li>协作训练策略将扩散建模直接注入下一个令牌预测，以缩小生成差距。</li>
<li>自回归和扩散策略在不同的任务中表现出不同的性能。</li>
<li>提出了一个协作行动集成机制，自适应地融合两种动作预测。</li>
<li>HybridVLA在模拟和真实世界的实验中均表现出优异性能。</li>
<li>该方法在单臂和双臂机器人任务中都优于先前的主流VLA方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10631">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ed5327c076ca82e33a6c93e1dbe54782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b2c6dd7255677ad343553ffa3e15dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee4487719b66c6f17bf0b5274865e77.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniGoal-Towards-Universal-Zero-shot-Goal-oriented-Navigation"><a href="#UniGoal-Towards-Universal-Zero-shot-Goal-oriented-Navigation" class="headerlink" title="UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"></a>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</h2><p><strong>Authors:Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu</strong></p>
<p>In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods. </p>
<blockquote>
<p>本文提出了一个通用的零样本目标导向导航框架。现有的零样本方法基于大型语言模型（LLM）构建特定任务的推理框架，整体流程差异较大，且无法在不同类型的目标之间推广。为了实现通用零样本导航的目标，我们提出了一种统一图表示方法，以融合不同的目标，包括对象类别、实例图像和文本描述。此外，我们将观察结果转换为在线维护的场景图。通过这种一致的场景和目标表示，我们能够保留大部分结构信息，相较于纯文本能够利用LLM进行明确的基于图的推理。具体来说，我们在每个时间点上对场景图和目标图进行图匹配，并根据不同的匹配状态提出不同的策略来生成长期探索目标。当零匹配时，代理首先迭代搜索目标的子图。随着部分匹配的出现，代理然后利用坐标投影和锚点配对来对齐来推断目标位置。最后是场景图修正和目标验证以完成完美匹配。我们还引入了一个黑名单机制，以实现阶段之间的稳健切换。在几个基准测试上的广泛实验表明，我们的UniGoal使用单一模型在三项研究的导航任务上实现了最先进的零样本性能，甚至超越了特定任务的零样本方法和监督通用方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10630v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong>：<br>该文提出了一种基于通用图表示的零样本目标导向导航框架。通过统一图表示，将不同目标（包括对象类别、实例图像和文本描述）融合在一起，实现了零样本导航。文中提出了场景图的在线维护，保留了大多数结构信息，并采用了大型语言模型进行基于图的显式推理。通过图匹配和不同的探索策略，实现了对探索目标的长期规划。此外，还引入了一个黑名单机制来增强阶段的稳健性切换。实验表明，所提出的UniGoal框架在三种导航任务上实现了最先进的零样本性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>论文提出了一个通用的零样本目标导向导航框架，旨在实现跨不同目标的零样本导航。</li>
<li>通过统一图表示，融合了不同的目标，包括对象类别、实例图像和文本描述。</li>
<li>采用了大型语言模型进行基于图的显式推理，保留了结构信息。</li>
<li>场景图的在线维护使得导航更加精确和灵活。</li>
<li>通过图匹配和不同的探索策略，实现了对探索目标的长期规划。</li>
<li>UniGoal框架具有强大的性能，在多个基准测试中实现了最先进的零样本导航性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cc4497a6e4df9e873931ce68f0c60570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0329636c003965d65e238236a3890ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eee4bcb569b6e71d0dd579dee86fdd94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3d3ceed1548a7749765019a747f43c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transformers-without-Normalization"><a href="#Transformers-without-Normalization" class="headerlink" title="Transformers without Normalization"></a>Transformers without Normalization</h2><p><strong>Authors:Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</strong></p>
<p>Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) &#x3D; \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks. </p>
<blockquote>
<p>标准化层在现代神经网络中无处不在，并一直被视为至关重要。本研究表明，不使用标准化的Transformer可以通过一种非常简单的技术实现相同或更好的性能。我们引入了动态双曲函数（DyT），作为一种元素级操作$DyT(x) &#x3D; \tanh(\alpha x)$，作为Transformer中标准化层的即插即用替代品。DyT的灵感来源于这样一个观察：Transformer中的层标准化通常会产生类似于双曲函数（tanh）的S形输入输出映射。通过融入DyT，不使用标准化的Transformer可以匹配或超过其标准化对应物的性能，而且大部分情况下不需要调整超参数。我们在多种设置中验证了使用DyT的Transformer的有效性，包括从识别到生成、监督学习到自我监督学习、计算机视觉到语言模型等。这些发现挑战了标准化层在现代神经网络中不可或缺的传统理解，并为深入了解其在深度网络中的作用提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10622v1">PDF</a> CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://jiachenzhu.github.io/DyT/">https://jiachenzhu.github.io/DyT/</a></p>
<p><strong>Summary</strong><br>现代神经网络中普遍存在的归一化层被认为是必要的，但这项工作展示了不使用归一化的Transformer通过使用一种非常简单的技术也能达到相同或更好的性能。引入动态双曲函数（DyT）作为Transformer中归一化层的替代方案。DyT受启发于Transformer中层归一化产生的tanh-like输入-输出映射的观察。通过结合DyT，不使用归一化的Transformer可以匹配或超过使用归一化的同类模型的性能，并且大多不需要调整超参数。验证了DyT在多种设置中的有效性，包括识别与生成任务、监督学习与自监督学习以及计算机视觉和语言模型等领域。这些发现挑战了常规认知，即归一化层在现代神经网络中是不可或缺的，并为深入了解其在深度网络中的作用提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformers不使用归一化层也能实现良好性能。</li>
<li>提出了动态双曲函数（DyT）作为Transformer中归一化层的替代方案。</li>
<li>DyT是基于观察Transformer中层归一化的输入-输出映射特性而设计的。</li>
<li>使用DyT的Transformer模型可以匹配或超过使用归一化的模型的性能。</li>
<li>在多种任务设置中都验证了DyT的有效性，包括识别、生成、监督学习、自监督学习等。</li>
<li>这些发现挑战了常规认知，即归一化层在现代神经网络中是不可或缺的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1be3de9c01997296a09af7138e4440b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff9bff21d4f1df9594a1739aeab691c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33e9af8c548232f2696d74480ba43b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6493e10be8d54a9f671c035a57030fa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce12bbe850e908af0d9aac240d133cce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec54fb90313cb0e17c41338f3ad6445e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb40328db8275518338f3b9eddb654e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4f820f9d190e892c9c0a68f1418e6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4bfe828938c86bcb7885777e8f08d90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-796b4c307f63ef1478b69a7a872d96f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Siege-Autonomous-Multi-Turn-Jailbreaking-of-Large-Language-Models-with-Tree-Search"><a href="#Siege-Autonomous-Multi-Turn-Jailbreaking-of-Large-Language-Models-with-Tree-Search" class="headerlink" title="Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with   Tree Search"></a>Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with   Tree Search</h2><p><strong>Authors:Andy Zhou</strong></p>
<p>We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models. </p>
<blockquote>
<p>我们介绍了Siege，这是一个多轮对抗性框架，通过树搜索的视角来模拟大型语言模型（LLM）安全性的逐渐侵蚀。不同于依赖精心设计的提示的单轮越狱，Siege以广度优先的方式扩展每一轮的对话，分支出多个对抗性提示，这些提示利用之前响应的部分合规性。通过跟踪这些逐步的政策泄露并将其重新注入后续的查询中，Siege揭示了微小的让步是如何累积成完全禁止的输出的。在JailbreakBench数据集上的评估表明，Siege在GPT-3.5 turbo上实现了100%的成功率，在GPT-4的单轮多运行中达到了97%的成功率，使用的查询次数少于基准测试如Crescendo或GOAT。这种树搜索方法提供了一个深入的视角，来观察模型保障措施在连续的对话回合中是如何退化的，这突显了对语言模型进行稳健的多轮测试程序的紧迫性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10619v1">PDF</a> Accepted to ICLR 2025 Trustworthy LLM</p>
<p><strong>Summary</strong>：我们推出了Siege，这是一个多回合对抗性框架，它通过树搜索的角度来模拟大型语言模型（LLM）安全性的逐渐侵蚀。不同于依赖精心设计的单一提示的单回合越狱，Siege以广度优先的方式扩展对话的每一回合，分支出多个对抗性提示，利用之前回应的部分合规性进行攻击。通过跟踪这些累积的政策漏洞并将其重新注入后续查询，Siege揭示了微小的让步是如何累积成完全不允许的输出的。在JailbreakBench数据集上的评估显示，Siege在一次多回合运行中实现了GPT-3.5turbo的100%成功率，GPT-4上的成功率为97%，查询次数少于基线如Crescendo或GOAT。这种树搜索方法提供了一个深入的视角，可以观察到模型保障措施如何在连续的对话回合中降级，强调了为语言模型实施稳健的多回合测试程序的紧迫性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Siege是一个多回合对抗性框架，模拟LLM安全性的逐渐侵蚀。</li>
<li>它采用树搜索方法来扩展对话，并在每一回合中分支出多个对抗性提示。</li>
<li>Siege通过跟踪政策漏洞并将其重新注入后续查询，揭示了LLM的安全问题。</li>
<li>在JailbreakBench数据集上，Siege在GPT-3.5turbo上的成功率为100%，在GPT-4上为97%。</li>
<li>相比于基线方法如Crescendo或GOAT，Siege使用了更少的查询次数。</li>
<li>Siege揭示了模型保障措施如何在连续的对话回合中逐渐失效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bd20267ebb6c68543078f2f747ab6b78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0c015ec3f503bb28babd836e17fe59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02984606309b20effe7f61be32d9db3d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Compositional-Subspace-Representation-Fine-tuning-for-Adaptive-Large-Language-Models"><a href="#Compositional-Subspace-Representation-Fine-tuning-for-Adaptive-Large-Language-Models" class="headerlink" title="Compositional Subspace Representation Fine-tuning for Adaptive Large   Language Models"></a>Compositional Subspace Representation Fine-tuning for Adaptive Large   Language Models</h2><p><strong>Authors:Andy Zhou</strong></p>
<p>Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead. </p>
<blockquote>
<p>适应大型语言模型进行多任务学习可能会导致跨技能干扰，即某项技能的提升会损害另一项技能。虽然LoRA等方法在权重层面施加正交性约束，但它们并没有完全解决隐藏状态表示中的干扰问题。我们提出了组合子空间表示微调（CS-ReFT），这是一种基于表示的新方法，学习多个正交子空间变换，每个变换都专注于一种独特的技能，并通过轻量级路由器进行组合。通过在隐藏状态中隔离这些子空间编辑，而不是权重矩阵，CS-ReFT更有效地防止了跨任务冲突。在AlpacaEval基准测试中，将CS-ReFT应用于Llama-2-7B模型，取得了93.94%的胜率，超过了GPT-3.5 Turbo（86.30%），同时仅需要模型参数的0.0098%。这些发现表明，通过简单路由器组合的专业表示编辑，可以显著提高多任务指令的执行力，且几乎不产生额外开销。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10617v1">PDF</a> Accepted to ICLR 2025 SCOPE</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在多任务适应过程中会出现技能间干扰问题，即提升某项技能的同时会降以及其他技能的性能。现有方法如LoRA虽在权重层面施加正交性约束，但并未完全解决隐藏状态表示中的干扰问题。本文提出一种基于表示的方法——Compositional Subspace Representation Fine-tuning（CS-ReFT），学习多个正交的子空间变换，每个变换专长于一种技能，并通过轻量级路由器进行组合。通过隔离隐藏状态中的子空间编辑，而非权重矩阵，CS-ReFT更有效地防止了跨任务冲突。在AlpacaEval基准测试中，将CS-ReFT应用于Llama-2-7B模型，取得了93.94%的高胜率，超越了GPT-3.5 Turbo（86.30%），并且仅需要模型参数的0.0098%。研究结果表明，通过简单路由器组合的专业化表示编辑能显著提高多任务指令执行效率，且带来的额外开销很小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在多任务适应时会面临技能间干扰问题。</li>
<li>现有方法如LoRA并不完全解决隐藏状态表示中的干扰。</li>
<li>CS-ReFT通过学习多个正交的子空间变换来解决跨任务干扰问题。</li>
<li>每个子空间变换在CS-ReFT中专注于一种特定技能。</li>
<li>CS-ReFT通过轻量级路由器组合这些子空间变换。</li>
<li>CS-ReFT在AlpacaEval基准测试中取得了显著成果，胜过GPT-3.5 Turbo。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10617">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-00ee9975ea4757911fefa69dbb40331c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1fff38f869d9c1bbf6bcf7162e2727e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55fc31a6a13c51dfbe16493b5258e774.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization"><a href="#R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization" class="headerlink" title="R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization"></a>R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization</h2><p><strong>Authors:Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen</strong></p>
<p>Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. </p>
<blockquote>
<p>大型语言模型在复杂的文本任务中表现出了显著的理解能力。然而，多模态推理，这需要整合视觉和文本信息，仍然是一个巨大的挑战。现有的视觉语言模型往往难以有效地分析和理解视觉内容，导致在复杂的推理任务上表现不佳。此外，缺乏全面的基准测试阻碍了多模态推理能力的准确评估。在本文中，我们介绍了R1-Onevision，一个旨在弥合视觉感知和深度推理之间差距的多模态推理模型。为实现这一目标，我们提出了一种跨模态推理管道，将图像转换为正式的纹理表示，从而实现基于精确语言的推理。通过这个管道，我们构建了R1-Onevision数据集，该数据集在各个领域提供了详细、分步骤的多模态推理注释。我们进一步通过监督微调强化学习来开发R1-Onevision模型，培养先进的推理和稳健的泛化能力。为了全面评估不同等级的多模态推理性能，我们推出了与人的教育阶段相对应的标准R1-Onevision-Bench，涵盖从初中到大学及以后的考试。实验结果表明，R1-Onevision取得了最先进的性能，在多个具有挑战性的多模态推理基准测试中优于GPT-4o和Qwen2.5-VL等模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10615v1">PDF</a> Code and Model: <a target="_blank" rel="noopener" href="https://github.com/Fancy-MLLM/R1-onevision">https://github.com/Fancy-MLLM/R1-onevision</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型在复杂文本任务中展现出强大的推理能力，但在需要整合视觉和文本信息的多模态推理方面仍面临挑战。现有视觉语言模型在分析和理解视觉内容方面存在不足，导致在复杂推理任务上的性能不佳。针对这些问题，本文提出了R1-Onevision多模态推理模型，通过跨模态推理管道将图像转化为正式文本表示，实现精确的语言推理。借助该管道，构建了R1-Onevision数据集，提供不同领域详细的逐步多模态推理注释。通过监督微调强化学习，进一步发展了R1-Onevision模型的先进推理和稳健泛化能力。为全面评估不同等级的多模态推理性能，本文还引入了与人类教育阶段相符的R1-Onevision-Bench基准测试，涵盖从初中到大学及以后的考试。实验结果表明，R1-Onevision在多个具有挑战性的多模态推理基准测试上实现了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在复杂文本任务中展现出强大的推理能力，但在多模态推理方面仍有显著挑战。</li>
<li>现有视觉语言模型在分析和理解视觉内容方面存在不足。</li>
<li>R1-Onevision多模态推理模型通过跨模态推理管道转化图像为文本表示，实现精确语言推理。</li>
<li>R1-Onevision数据集提供多模态推理的详细注释，涵盖不同领域。</li>
<li>R1-Onevision模型通过监督微调和强化学习进一步发展了先进推理和泛化能力。</li>
<li>R1-Onevision-Bench基准测试评估不同等级的多模态推理性能，与人类的各个阶段教育相符。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb8237035f928e8b62af18ec3e240ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16752ac75fa0fe28dc5b056a5e187cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4d139b9a615271f0b61530bae848ae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa35ecce8741f0e38a5eef55a96232db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d79e2c222974a93705052dae305edd64.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TruthPrInt-Mitigating-LVLM-Object-Hallucination-Via-Latent-Truthful-Guided-Pre-Intervention"><a href="#TruthPrInt-Mitigating-LVLM-Object-Hallucination-Via-Latent-Truthful-Guided-Pre-Intervention" class="headerlink" title="TruthPrInt: Mitigating LVLM Object Hallucination Via Latent   Truthful-Guided Pre-Intervention"></a>TruthPrInt: Mitigating LVLM Object Hallucination Via Latent   Truthful-Guided Pre-Intervention</h2><p><strong>Authors:Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu</strong></p>
<p>Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the “overall truthfulness” of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as “per-token” hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist “generic truthful directions” shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/TruthPrInt">https://github.com/jinhaoduan/TruthPrInt</a>. </p>
<blockquote>
<p>对象幻觉（OH）已被认为是大型视觉语言模型（LVLMs）面临的主要可信挑战之一。大型语言模型（LLM）的最新进展表明，隐藏状态等内部状态编码了生成响应的“整体真实性”。然而，关于LVLMs的内部状态如何发挥作用，以及它们是否可以作为“逐词”幻觉指标的问题尚未得到充分研究，这对于缓解OH至关重要。在本文中，我们首先深入探讨了LVLM内部状态与OH问题的关系，并发现：（1）LVLM的内部状态是幻觉行为的高特异性逐词指标。（2）不同的LVLM在共同的潜在子空间中编码了幻觉的通用模式，这表明存在各种LVLM共享的“通用真实方向”。基于这些发现，我们提出了真实引导预干预（TruthPrInt），它首先学习LVLM解码的真实方向，然后在LVLM解码时进行真实引导推理时间干预。为了进一步增强跨LVLM和跨数据幻觉检测的可转移性，我们提出ComnHallu，通过构建和对齐幻觉潜在子空间。我们在广泛的实验设置中对TruthPrInt进行了评估，包括域内和域外场景，以及流行的LVLM和OH基准测试。实验结果表明，TruthPrInt显著优于最新方法。代码将在<a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/TruthPrInt">https://github.com/jinhaoduan/TruthPrInt</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10602v1">PDF</a> 15 pages, 9 figures, the first two authors contributed equally</p>
<p><strong>摘要</strong></p>
<p>大型视觉语言模型（LVLMs）中的对象幻觉（OH）被公认为是主要的可信挑战之一。研究发现，LVLM的内部状态（如隐藏状态）编码了生成响应的“整体真实性”。然而，尚不清楚LVLM的内部状态如何发挥作用，以及它们是否能作为“每个标记”的幻觉指标来减轻OH。本文通过深入研究LVLM内部状态与OH问题的关系，发现（1）LVLM内部状态是高特异性的、针对每个标记的幻觉行为指标。（2）不同的LVLM在共同的潜在子空间中编码幻觉的通用模式，这表明各种LVLM共享“通用的真实方向”。基于此，本文提出了真实引导预干预（TruthPrInt），该方法首先学习LVLM解码的真实方向，然后在LVLM解码过程中进行真实引导推理干预。此外，本文还提出了ComnHallu，通过构建和对齐幻觉潜在子空间，增强跨LVLM和跨数据幻觉检测的迁移性。在广泛的实验设置（包括域内和域外场景、流行LVLM和OH基准测试）中评估TruthPrInt，结果表明TruthPrInt显著优于最新方法。代码将发布在<a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/TruthPrInt%E3%80%82">https://github.com/jinhaoduan/TruthPrInt。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>LVLM的内部状态是评估生成响应真实性的重要指标。</li>
<li>LVLM的内部状态可以作为“每个标记”的幻觉行为的特异性指标。</li>
<li>不同的LVLM在潜在子空间中编码幻觉的通用模式，存在“通用的真实方向”。</li>
<li>提出了真实引导预干预（TruthPrInt）方法，通过学习LVLM解码的真实方向，在推理时进行干预。</li>
<li>ComnHallu方法通过构建和对齐幻觉潜在子空间，增强了幻觉检测的迁移性和跨模型、跨数据的检测能力。</li>
<li>TruthPrInt在广泛的实验设置中表现出显著的性能提升，包括在域内和域外的场景、流行的LVLM和OH基准测试上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1316c5ffe80b249da2f0d262d822d6f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-927962f082af7881e922e9da1531d13b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06bff57bcc8c7c1696a7f8de398137a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0597cd06fb19f5bbfeaa717aa6226a44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9d624b4d4e8ab7f8f335da3d4a1e785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43072cbae91fd9e0b729a876734344a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unlock-the-Power-of-Unlabeled-Data-in-Language-Driving-Model"><a href="#Unlock-the-Power-of-Unlabeled-Data-in-Language-Driving-Model" class="headerlink" title="Unlock the Power of Unlabeled Data in Language Driving Model"></a>Unlock the Power of Unlabeled Data in Language Driving Model</h2><p><strong>Authors:Chaoqun Wang, Jie Yang, Xiaobin Hong, Ruimao Zhang</strong></p>
<p>Recent Vision-based Large Language Models~(VisionLLMs) for autonomous driving have seen rapid advancements. However, such promotion is extremely dependent on large-scale high-quality annotated data, which is costly and labor-intensive. To address this issue, we propose unlocking the value of abundant yet unlabeled data to improve the language-driving model in a semi-supervised learning manner. Specifically, we first introduce a series of template-based prompts to extract scene information, generating questions that create pseudo-answers for the unlabeled data based on a model trained with limited labeled data. Next, we propose a Self-Consistency Refinement method to improve the quality of these pseudo-annotations, which are later used for further training. By utilizing a pre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving Model (LDM) for driving scene question-answering, outperforming previous state-of-the-art methods. Extensive experiments on the DriveLM benchmark show that our approach performs well with just 5% labeled data, achieving competitive performance against models trained with full datasets. In particular, our LDM achieves 44.85% performance with limited labeled data, increasing to 54.27% when using unlabeled data, while models trained with full datasets reach 60.68% on the DriveLM benchmark. </p>
<blockquote>
<p>近年来，基于视觉的大型语言模型（VisionLLMs）在自动驾驶领域取得了快速发展。然而，这种进步极度依赖于大规模高质量标注数据，这成本高昂且劳动密集。为了解决这个问题，我们提出了以半监督学习的方式，解锁大量未标注数据的价值，以提高驾驶语言模型的表现。具体来说，我们首先引入一系列基于模板的提示来提取场景信息，生成问题，这些问题会根据有限标注数据训练的模型为未标注数据提供伪答案。接下来，我们提出了一种自洽细化方法，以提高这些伪标注的质量，用于进一步的训练。通过利用预训练的VisionLLM（例如InternVL），我们建立了一个强大的语言驾驶模型（LDM），用于驾驶场景问答，超越了之前的最先进方法。在DriveLM基准测试上的大量实验表明，我们的方法只需5%的标注数据就能表现良好，与全数据集训练的模型相比具有竞争力。尤其值得一提的是，我们的LDM在有限标注数据的情况下达到44.85%的性能，在使用未标注数据后提高到54.27%，而全数据集训练的模型在DriveLM基准测试上达到60.68%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10586v1">PDF</a> Accepted by ICRA2025</p>
<p><strong>Summary</strong></p>
<p>大规模高质量标注数据对基于视觉的大型语言模型（VisionLLMs）在自动驾驶领域的应用至关重要，但获取这些数据成本高昂且劳动密集。为应对这一问题，本研究提出利用丰富的未标注数据，以半监督学习方式提升语言驾驶模型的价值。通过基于模板的提示生成伪答案，再利用自我一致性优化方法提高伪标注的质量，并将其用于进一步训练。利用预训练的VisionLLM（如InternVL），为驾驶场景问答构建了强大的语言驾驶模型（LDM），表现优于先前的方法。在DriveLM基准测试上，仅使用5%标注数据的方法表现良好，使用未标注数据时性能进一步提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶领域中的VisionLLMs面临依赖大规模高质量标注数据的问题。</li>
<li>为解决此问题，研究提出了利用丰富的未标注数据以半监督学习方式提升语言驾驶模型的方法。</li>
<li>通过基于模板的提示生成伪答案，并使用自我一致性优化方法提高伪标注质量。</li>
<li>利用预训练的VisionLLM构建了强大的LDM用于驾驶场景问答。</li>
<li>该方法在DriveLM基准测试上表现良好，仅使用5%标注数据时性能可观。</li>
<li>使用未标注数据可进一步提升模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10586">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e46eabcd480d26b675d1a3f29162982e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f45ea99187122ade180647e419fec4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f786c432d55ae80759eff7ee8e010d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c085af18222771611c76d3caa7ae76f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb94a99f41ea3e3804b01f24bc306f04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-218de8773ee9ecefd9154adab3c04404.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-Mathematical-Reasoning-in-DeepSeek-Models-A-Comparative-Study-of-Large-Language-Models"><a href="#Unveiling-the-Mathematical-Reasoning-in-DeepSeek-Models-A-Comparative-Study-of-Large-Language-Models" class="headerlink" title="Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative   Study of Large Language Models"></a>Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative   Study of Large Language Models</h2><p><strong>Authors:Afrar Jahin, Arif Hassan Zidan, Yu Bao, Shizhe Liang, Tianming Liu, Wei Zhang</strong></p>
<p>With the rapid evolution of Artificial Intelligence (AI), Large Language Models (LLMs) have reshaped the frontiers of various fields, spanning healthcare, public health, engineering, science, agriculture, education, arts, humanities, and mathematical reasoning. Among these advancements, DeepSeek models have emerged as noteworthy contenders, demonstrating promising capabilities that set them apart from their peers. While previous studies have conducted comparative analyses of LLMs, few have delivered a comprehensive evaluation of mathematical reasoning across a broad spectrum of LLMs. In this work, we aim to bridge this gap by conducting an in-depth comparative study, focusing on the strengths and limitations of DeepSeek models in relation to their leading counterparts. In particular, our study systematically evaluates the mathematical reasoning performance of two DeepSeek models alongside five prominent LLMs across three independent benchmark datasets. The findings reveal several key insights: 1). DeepSeek-R1 consistently achieved the highest accuracy on two of the three datasets, demonstrating strong mathematical reasoning capabilities. 2). The distilled variant of LLMs significantly underperformed compared to its peers, highlighting potential drawbacks in using distillation techniques. 3). In terms of response time, Gemini 2.0 Flash demonstrated the fastest processing speed, outperforming other models in efficiency, which is a crucial factor for real-time applications. Beyond these quantitative assessments, we delve into how architecture, training, and optimization impact LLMs’ mathematical reasoning. Moreover, our study goes beyond mere performance comparison by identifying key areas for future advancements in LLM-driven mathematical reasoning. This research enhances our understanding of LLMs’ mathematical reasoning and lays the groundwork for future advancements </p>
<blockquote>
<p>随着人工智能（AI）的迅速发展，大型语言模型（LLM）已经重塑了各个领域的边界，涵盖了医疗保健、公共卫生、工程、科学、农业、教育、艺术、人文和数学推理等多个领域。在这些进展中，DeepSeek模型表现出引人注目的能力，使其在同行业中的竞争中脱颖而出。虽然之前的研究已经对LLM进行了比较分析，但很少有研究对LLM的广泛数学推理能力进行全面评估。在这项工作中，我们旨在通过进行深入的对比研究来填补这一空白，重点关注DeepSeek模型与其领先同行在数学推理方面的优势和局限性。特别是，我们的研究系统地评估了两个DeepSeek模型与五种主要LLM在数学推理方面的性能表现，涉及三个独立的基准数据集。研究结果表明：1. DeepSeek-R1在两个数据集中的准确率始终最高，表现出强大的数学推理能力。2. 与其同行相比，蒸馏变体LLM的表现明显较差，这突显了使用蒸馏技术可能存在的潜在缺陷。3. 在响应时间方面，Gemini 2.0 Flash的处理速度最快，在效率方面优于其他模型，这对于实时应用是一个至关重要的因素。除了这些定量评估外，我们还深入探讨了架构、训练和优化如何影响LLM的数学推理能力。此外，我们的研究不仅局限于性能比较，还确定了LLM驱动的数学推理未来发展的关键领域。这项研究增强了我们对于LLM数学推理的理解，并为未来的进步奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着人工智能的快速发展，大型语言模型（LLM）已经重塑了多个领域的前沿，包括医疗、公共卫生、工程、科学、农业、教育、艺术、人文和数学推理等。DeepSeek模型表现出令人瞩目的能力，本研究旨在通过深入研究对比DeepSeek模型与其他领先的大型语言模型在数学推理方面的优势和局限性，填补当前研究的空白。研究结果表明DeepSeek-R1在多数数据集上表现最佳，蒸馏型LLM表现较差，Gemini 2.0 Flash在响应时间方面表现最佳。此外，本研究还探讨了架构、训练和优化对LLM数学推理的影响，并指出了未来LLM驱动的数学推理发展的关键领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1在多数数据集上展现出强大的数学推理能力。</li>
<li>蒸馏型LLM相较于其他模型表现较差，这突显了使用蒸馏技术的一些潜在缺点。</li>
<li>Gemini 2.0 Flash在响应时间方面表现最佳，具有较高的处理效率，适用于实时应用。</li>
<li>LLM的数学推理能力受到其架构、训练和优化的影响。</li>
<li>研究结果强调了未来在LLM驱动的数学推理发展中需要关注的关键领域。</li>
<li>本研究增强了我们对LLM数学推理的理解，并为未来的进步奠定了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9da35c5617ab8e527b3a95bb07a1d4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2029a26a2c658dde3ddb8c0b6cf5d56a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec02691a416c672424aeb16f36e5d699.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models"><a href="#ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models" class="headerlink" title="ASIDE: Architectural Separation of Instructions and Data in Language   Models"></a>ASIDE: Architectural Separation of Instructions and Data in Language   Models</h2><p><strong>Authors:Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Soroush Tabesh, Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert</strong></p>
<p>Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose an architectural change, ASIDE, that allows the model to clearly separate between instructions and data by using separate embeddings for them. Instead of training the embeddings from scratch, we propose a method to convert an existing model to ASIDE form by using two copies of the original model’s embeddings layer, and applying an orthogonal rotation to one of them. We demonstrate the effectiveness of our method by showing (1) highly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations. </p>
<blockquote>
<p>尽管大型语言模型表现出色，但它们缺乏基本的安全功能，这使得它们容易受到众多恶意攻击的影响。特别是，先前的工作已经确定了指令和数据之间缺乏内在分离是提示注入攻击成功的根本原因。在这项工作中，我们提出了一种架构更改，即ASIDE，它允许模型通过为指令和数据使用单独的嵌入来清晰地分离它们。我们并不提议从头开始训练嵌入，而是提出了一种将现有模型转换为ASIDE形式的方法，该方法使用原始模型的嵌入层副本，并对其中一个应用正交旋转。我们通过展示（1）在保持模型能力不损失的情况下，指令-数据分离得分大幅提高；（2）即使在未进行专门的安全训练的情况下，提示注入基准测试也有竞争力结果来证明我们方法的有效性。此外，我们还通过模型表示的分析研究了该方法的工作原理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10566v1">PDF</a> ICLR 2025 Workshop on Building Trust in Language Models and   Applications</p>
<p><strong>摘要</strong></p>
<p>尽管大型语言模型的性能显著，但它们缺乏基本的安全特性，这使得它们容易受到多次恶意攻击。先前的工作已经识别出指令和数据之间缺乏内在分离是提示注入攻击成功的根本原因。在这项工作中，我们提出了一种架构改变——ASIDE，它允许模型通过为指令和数据使用单独的嵌入来清晰地分离它们。我们提出了一种将现有模型转换为ASIDE形式的方法，而不是从头开始训练嵌入，该方法包括使用原始模型嵌入层的两个副本，并对其中一个应用正交旋转。我们通过（1）提高指令-数据分离分数而不会影响模型能力，（2）在提示注入基准测试上获得具有竞争力的结果（即使在没有专门的安全训练的情况下），来证明我们方法的有效性。此外，我们还通过模型表示的分析研究了我们的方法的工作原理。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>大型语言模型缺乏基本的安全特性，容易受到恶意攻击。</li>
<li>指令和数据之间缺乏内在分离是提示注入攻击成功的关键因素。</li>
<li>提出了一种新的架构ASIDE，能够清晰地分离指令和数据。</li>
<li>转换现有模型到ASIDE形式的方法是通过使用两个嵌入层副本并应用正交旋转。</li>
<li>该方法在不损失模型能力的前提下提高了指令与数据的分离效果。</li>
<li>在提示注入基准测试上，该方法取得了具有竞争力的结果，无需专门的安全训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8a22beb539251ecaa7a912a519612106.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-678be6aac11410aae872cb795f9b4b80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad333b018139374a375e1b17410ef877.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74becbf01084ab3c8832ee8d60cdda12.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="KUDA-Keypoints-to-Unify-Dynamics-Learning-and-Visual-Prompting-for-Open-Vocabulary-Robotic-Manipulation"><a href="#KUDA-Keypoints-to-Unify-Dynamics-Learning-and-Visual-Prompting-for-Open-Vocabulary-Robotic-Manipulation" class="headerlink" title="KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for   Open-Vocabulary Robotic Manipulation"></a>KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for   Open-Vocabulary Robotic Manipulation</h2><p><strong>Authors:Zixian Liu, Mingtong Zhang, Yunzhu Li</strong></p>
<p>With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at <a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io/">http://kuda-dynamics.github.io</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）和视觉语言模型（VLM）的快速发展，开放式词汇机器人操纵系统在开发方面已取得了重大进展。然而，许多现有方法忽视了物体动力学的重要性，限制了它们在更复杂、动态的任务中的应用。在这项工作中，我们介绍了KUDA，这是一个开放式词汇操纵系统，它通过关键点整合动力学学习和视觉提示，利用VLMs和基于学习的神经动力学模型。我们的关键见解是，基于关键点的目标规格可以同时被VLMs解释，并可以有效地转化为基于模型的规划的成本函数。给定语言指令和视觉观察，KUDA首先为RGB图像分配关键点，并查询VLM以生成目标规格。这些抽象的基于关键点的表示然后被转换成成本函数，使用学习到的动力学模型进行优化，以产生机器人轨迹。我们在各种操作任务上评估了KUDA，包括跨不同对象类别的自由形式语言指令、多对象交互以及可变形或颗粒状物体，证明了我们的框架的有效性。项目页面可在<a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io查看./">http://kuda-dynamics.github.io查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10546v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io/">http://kuda-dynamics.github.io</a></p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLMs）和视觉语言模型（VLMs）的快速发展，开放词汇机器人操纵系统取得了显著进展。然而，许多现有方法忽视了物体动力学的重要性，限制了它们在更复杂、动态任务中的应用。本研究介绍了KUDA，一个开放词汇的操纵系统，它通过关键点整合动力学学习和视觉提示，利用VLMs和学习型神经动力学模型。KUDA的关键见解是，基于关键点的目标规格可以同时被VLMs解读，并能有效地转化为模型基础规划的成本函数。给定语言指令和视觉观察，KUDA首先将关键点分配给RGB图像并查询VLM以生成目标规格。这些抽象的基于关键点的表示然后被转化为成本函数，使用学习到的动力学模型进行优化以产生机器人轨迹。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）和视觉语言模型（VLMs）的快速发展推动了开放词汇机器人操纵系统的进步。</li>
<li>现有方法在复杂、动态任务中的应用受限于对物体动力学的忽视。</li>
<li>KUDA是一个开放词汇的操纵系统，通过关键点整合动力学学习和视觉提示。</li>
<li>KUDA利用VLMs和学习型神经动力学模型，基于关键点的目标规格可以同时被VLMs解读并转化为成本函数。</li>
<li>KUDA通过语言指令和视觉观察来分配关键点，生成目标规格，并将其转化为机器人操作的轨迹。</li>
<li>KUDA在多种操作任务上的表现得到了验证，包括自由形式的语言指令、多对象交互、可变形或颗粒状物体等。</li>
<li>KUDA项目页面提供了更多详细信息：<a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io./">http://kuda-dynamics.github.io。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10546">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-499863b8d20f82f943bab3a89b521449.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4573e12edb6afe2f10bba7630dc196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23df30b12939cce86956cceb6381be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15c1dd4a05c357e276a2529e9e1381dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60366fa6c56174ae2210e902523ab307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9aa63d6392bd19b68c69820f6bb8c74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a4cf12967b3bf673e4af01ac9164c6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PiSA-A-Self-Augmented-Data-Engine-and-Training-Strategy-for-3D-Understanding-with-Large-Models"><a href="#PiSA-A-Self-Augmented-Data-Engine-and-Training-Strategy-for-3D-Understanding-with-Large-Models" class="headerlink" title="PiSA: A Self-Augmented Data Engine and Training Strategy for 3D   Understanding with Large Models"></a>PiSA: A Self-Augmented Data Engine and Training Strategy for 3D   Understanding with Large Models</h2><p><strong>Authors:Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, Zhen Li</strong></p>
<p>3D Multimodal Large Language Models (MLLMs) have recently made substantial advancements. However, their potential remains untapped, primarily due to the limited quantity and suboptimal quality of 3D datasets. Current approaches attempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but still face modality and domain gaps. To this end, we introduce PiSA-Engine (Point-Self-Augmented-Engine), a new framework for generating instruction point-language datasets enriched with 3D spatial semantics. We observe that existing 3D MLLMs offer a comprehensive understanding of point clouds for annotation, while 2D MLLMs excel at cross-validation by providing complementary information. By integrating holistic 2D and 3D insights from off-the-shelf MLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation. We select PointLLM as the baseline and adopt this co-evolution training framework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally, we identify limitations in previous 3D benchmarks, which often feature coarse language captions and insufficient category diversity, resulting in inaccurate evaluations. To address this gap, we further introduce PiSA-Bench, a comprehensive 3D benchmark covering six key aspects with detailed and diverse labels. Experimental results demonstrate PointLLM-PiSA’s state-of-the-art performance in zero-shot 3D object captioning and generative classification on our PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and 63.75% (+16.25%), respectively. We will release the code, datasets, and benchmark. </p>
<blockquote>
<p>3D多模态大型语言模型（MLLMs）最近取得了重大进展。然而，它们的潜力尚未被发掘，主要是因为3D数据集的数量有限和质量不佳。当前的方法试图从2D MLLMs转移知识来扩展3D指令数据，但仍面临模态和领域差距。为此，我们引入了PiSA-Engine（Point-Self-Augmented-Engine），这是一个用于生成富含3D空间语义的指令点语言数据集的新框架。我们发现现有的3D MLLMs对点云注释有着全面的理解，而2D MLLMs则通过提供补充信息擅长交叉验证。通过整合现成的MLLMs的2D和3D整体见解，PiSA-Engine能够实现高质量数据生成的持续循环。我们选择PointLLM作为基线，并采用这种协同进化训练框架来开发增强的3D MLLM，称为PointLLM-PiSA。此外，我们发现了以前3D基准测试的局限性，这些测试通常具有粗糙的语言标题和不足的类别多样性，导致评估不准确。为了弥补这一差距，我们进一步引入了PiSA-Bench，这是一个全面的3D基准测试，涵盖六个关键方面，具有详细和多样的标签。实验结果表明，PointLLM-PiSA在我们的PiSA-Bench上实现了零样本3D目标描述和生成分类的领先水平，分别提高了46.45%（+8.33%）和63.75%（+16.25%）。我们将发布代码、数据集和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10529v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>基于上述文本，提出一种名为PiSA-Engine的新框架，用于生成富含三维空间语义的指令点语言数据集。通过整合二维和三维的多角度见解，实现了高质量数据生成的连续循环。并提出PiSA-Bench这一全面三维基准测试平台，覆盖六个关键方面并拥有详细多样的标签。实验结果证实其在新方法和基准测试上的优越性。后续将发布相关代码、数据集和基准测试平台。概述完毕。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是基于文本的关键见解：</p>
<ul>
<li>当前大型语言模型（LLM）在三维领域的潜力尚未得到充分发挥，主要原因是缺乏高质量的三维数据集。</li>
<li>提出PiSA-Engine框架，旨在生成富含三维空间语义的指令点语言数据集。该框架结合了二维和三维LLM的优势，实现了高质量数据生成的连续循环。</li>
<li>介绍新的基准测试平台PiSA-Bench，其涵盖了六个关键方面并具备详细的多样性标签，能够更准确地对模型进行评估。</li>
<li>基于PointLLM基线模型和PiSA框架共同演化训练法训练了一个新的模型PointLLM-PiSA。</li>
<li>实验结果证实PointLLM-PiSA在零样本三维对象描述和生成分类任务上表现卓越，实现了显著的性能提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10529">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-62162ac8816e570e451993e6a1d1d610.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13c8509bba9640023545bf1b11a1ccbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-973208431521d7ce788693de488fe1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b11ccd64daf4036c716b9eb709354bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c52204bac3adea4141e511e73fe3009.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e90fd20318e22868e8bf7a2df8697864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ad817c0887841167bbe9eac94a1af47.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AudioX-Diffusion-Transformer-for-Anything-to-Audio-Generation"><a href="#AudioX-Diffusion-Transformer-for-Anything-to-Audio-Generation" class="headerlink" title="AudioX: Diffusion Transformer for Anything-to-Audio Generation"></a>AudioX: Diffusion Transformer for Anything-to-Audio Generation</h2><p><strong>Authors:Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo</strong></p>
<p>Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at <a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a> </p>
<blockquote>
<p>音频和音乐生成在许多应用中已成为至关重要的任务，然而现有方法面临重大局限：它们在孤立的情况下运行，不具备跨模态的统一能力，缺乏高质量的多模态训练数据，难以有效地整合不同的输入。在这项工作中，我们提出了AudioX，一个统一的扩散Transformer模型，用于任何内容到音频和音乐生成。不同于之前的特定领域模型，AudioX可以生成高质量的一般音频和音乐，同时提供灵活的自然语言控制以及无缝处理各种模态，包括文本、视频、图像、音乐和音频。其核心创新之处在于多模态掩模训练策略，该策略会屏蔽跨模态的输入，并迫使模型从被屏蔽的输入中学习，从而产生稳健和统一的跨模态表示。为了解决数据稀缺问题，我们整理了两个综合数据集：基于VGGSound数据集的19万音频字幕的vggsound-caps，以及从V2M数据集中派生的600万音乐字幕的V2M-caps。大量实验表明，AudioX不仅与最先进的专用模型相匹配或表现更好，而且在一个统一架构中处理各种输入模态和生成任务方面表现出惊人的通用性。代码和数据集将可在<a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/%E6%89%BE%E5%88%B0%E3%80%82">https://zeyuet.github.io/AudioX/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10522v1">PDF</a> The code and datasets will be available at   <a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a></p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种名为AudioX的统一扩散转换器模型，该模型可用于任何内容到音频和音乐生成。它具备跨模态的灵活处理能力，可以生成高质量的一般音频和音乐。其关键创新在于多模态掩模训练策略，该策略能够在跨模态输入中掩模输入并迫使模型从掩模输入中学习，从而产生稳健的统一跨模态表示。为解决数据稀缺问题，作者还整理了两个综合数据集。实验表明，AudioX不仅在专用模型上表现卓越，还表现出惊人的跨模态处理能力和生成任务的灵活性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AudioX是一个统一的扩散转换器模型，用于任何内容到音频和音乐生成。</li>
<li>它具备生成高质量一般音频和音乐的能力，同时提供灵活的跨模态处理能力。</li>
<li>AudioX的关键创新在于其多模态掩模训练策略，该策略产生稳健的跨模态表示。</li>
<li>为解决数据稀缺问题，作者整理了两个综合数据集vggsound-caps和V2M-caps。</li>
<li>AudioX在专用模型上表现卓越，并在处理跨模态输入和生成任务时表现出灵活性。</li>
<li>该模型的代码和数据集将可在[<a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/]%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://zeyuet.github.io/AudioX/]上获得。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7962feb85c1b4f0bd5702ba1579d767b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fda29bb30da1f0a10d892593bcfec63a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30d06630359804ef2a6acab6319ed08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22b662cff6ecbc633c7e71c9e7d7cf3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Conformal-Prediction-Sets-for-Deep-Generative-Models-via-Reduction-to-Conformal-Regression"><a href="#Conformal-Prediction-Sets-for-Deep-Generative-Models-via-Reduction-to-Conformal-Regression" class="headerlink" title="Conformal Prediction Sets for Deep Generative Models via Reduction to   Conformal Regression"></a>Conformal Prediction Sets for Deep Generative Models via Reduction to   Conformal Regression</h2><p><strong>Authors:Hooman Shahrokhi, Devjeet Raj Roy, Yan Yan, Venera Arnaoudova, Janaradhan Rao Doppa</strong></p>
<p>We consider the problem of generating valid and small prediction sets by sampling outputs (e.g., software code and natural language text) from a black-box deep generative model for a given input (e.g., textual prompt). The validity of a prediction set is determined by a user-defined binary admissibility function depending on the target application. For example, requiring at least one program in the set to pass all test cases in code generation application. To address this problem, we develop a simple and effective conformal inference algorithm referred to as Generative Prediction Sets (GPS). Given a set of calibration examples and black-box access to a deep generative model, GPS can generate prediction sets with provable guarantees. The key insight behind GPS is to exploit the inherent structure within the distribution over the minimum number of samples needed to obtain an admissible output to develop a simple conformal regression approach over the minimum number of samples. Experiments on multiple datasets for code and math word problems using different large language models demonstrate the efficacy of GPS over state-of-the-art methods. </p>
<blockquote>
<p>我们考虑通过从一个黑箱深度生成模型中对给定输入（如文本提示）进行采样输出（例如软件代码和自然语言文本），来生成有效且小的预测集的问题。预测集的有效性是由用户定义的二进制接纳函数决定的，这取决于目标应用程序。例如，在代码生成应用程序中，要求集合中至少有一个程序通过所有测试用例。为了解决此问题，我们开发了一种简单有效的合规推理算法，称为生成预测集（GPS）。给定一组校准示例和对深度生成模型的黑箱访问权限，GPS可以生成具有可证明保证的预测集。GPS背后的关键见解是利用获得可接纳输出所需的最小样本数分布的内在结构，在此基础上开发一种简单的合规回归方法。在多个数据集上进行的代码和数学文字问题的实验表明，与最先进的方法相比，GPS更为有效。使用了大型语言模型进行了广泛验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10512v1">PDF</a> </p>
<p><strong>Summary</strong><br>在给定输入（如文本提示）的情况下，通过从黑箱深度生成模型中采样输出（如软件代码和自然语言文本）来生成有效且小的预测集是一个难题。为了解决这个问题，我们提出了一种简单有效的基于置信推断的方法，称为生成预测集（GPS）。给定一组校准示例和对深度生成模型的黑箱访问权限，GPS能够生成具有可验证保证的预测集。关键在于利用生成可接受输出所需的最小样本数分布的内在结构，为最小样本数开发一种简单的置信回归方法。在多个数据集上的实验表明，GPS在解决代码和数学问题的应用中优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该问题聚焦于从黑箱深度生成模型中生成有效且小的预测集。</li>
<li>通过一个用户定义的二元接纳函数来判断预测集的有效性。</li>
<li>提出了一种简单有效的基于置信推断的方法——生成预测集（GPS）。</li>
<li>GPS利用生成可接受输出所需的最小样本数分布的内在结构。</li>
<li>GPS通过一种简单的置信回归方法为最小样本数提供可验证保证。</li>
<li>实验结果显示GPS在多个数据集上的表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10512">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-005be3cd5803209008d938d577b86ea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1419ccc4a18ed8e2597678fcc0e295.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-588f080d3ae0be949dbfe29042da6638.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TokenCarve-Information-Preserving-Visual-Token-Compression-in-Multimodal-Large-Language-Models"><a href="#TokenCarve-Information-Preserving-Visual-Token-Compression-in-Multimodal-Large-Language-Models" class="headerlink" title="TokenCarve: Information-Preserving Visual Token Compression in   Multimodal Large Language Models"></a>TokenCarve: Information-Preserving Visual Token Compression in   Multimodal Large Language Models</h2><p><strong>Authors:Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen</strong></p>
<p>Multimodal Large Language Models (MLLMs) are becoming increasingly popular, while the high computational cost associated with multimodal data input, particularly from visual tokens, poses a significant challenge. Existing training-based token compression methods improve inference efficiency but require costly retraining, while training-free methods struggle to maintain performance when aggressively reducing token counts. In this study, we reveal that the performance degradation of MLLM closely correlates with the accelerated loss of information in the attention output matrix. This insight introduces a novel information-preserving perspective, making it possible to maintain performance even under extreme token compression. Based on this finding, we propose TokenCarve, a training-free, plug-and-play, two-stage token compression framework. The first stage employs an Information-Preservation-Guided Selection (IPGS) strategy to prune low-information tokens, while the second stage further leverages IPGS to guide token merging, minimizing information loss. Extensive experiments on 11 datasets and 2 model variants demonstrate the effectiveness of TokenCarve. It can even reduce the number of visual tokens to 22.2% of the original count, achieving a 1.23x speedup in inference, a 64% reduction in KV cache storage, and only a 1.54% drop in accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ShawnTan86/TokenCarve">https://github.com/ShawnTan86/TokenCarve</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）越来越受欢迎，然而与多模态数据输入相关的高计算成本，特别是来自视觉标记的输入，构成了一大挑战。现有的基于训练的标记压缩方法提高了推理效率，但需要昂贵的重新训练，而无训练的方法在大力减少标记数量时却难以维持性能。在这项研究中，我们发现多模态大型语言模型的性能下降与注意力输出矩阵中信息加速丢失密切相关。这一发现引入了一种新的信息保留视角，即使在极端的标记压缩下也能维持性能。基于这一发现，我们提出了TokenCarve，这是一种无需训练、即插即用的两阶段标记压缩框架。第一阶段采用信息保留引导选择（IPGS）策略来删除低信息标记，第二阶段进一步利用IPGS来指导标记合并，尽量减少信息损失。在11个数据集和2个模型变体上的大量实验证明了TokenCarve的有效性。它甚至可以将视觉标记的数量减少到原始数量的22.2%，实现推理速度提高1.23倍，KV缓存存储减少64%，准确率仅下降1.54%。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/ShawnTan86/TokenCarve%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ShawnTan86/TokenCarve上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10501v1">PDF</a> </p>
<p><strong>Summary</strong><br>多媒体大型语言模型（MLLMs）面临高计算成本挑战，特别是在处理视觉符号输入时。现有基于训练的符号压缩方法能提高推理效率，但需要昂贵的重新训练成本，而无需训练的压缩方法则在大幅减少符号计数时难以保持性能。本研究发现MLLM性能下降与注意力输出矩阵中信息的加速丧失密切相关，因此提出了一种新型的信息保留视角，并基于此见解提出了无需训练的TokenCarve框架。该框架采用两阶段符号压缩策略，第一阶段采用信息保留引导选择策略来删除低信息符号，第二阶段进一步利用该策略引导符号合并，尽量减少信息损失。在多个数据集和模型变体上的实验表明TokenCarve的有效性。它可以将视觉符号的数量减少到原始数量的22.2%，提高推理速度1.23倍，降低KV缓存存储64%，且仅损失1.54%的精度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多媒体大型语言模型（MLLMs）在处理视觉符号输入时面临高计算成本挑战。</li>
<li>现有符号压缩方法需要昂贵的重新训练成本或难以在减少符号计数时保持性能。</li>
<li>研究发现MLLM性能下降与注意力输出矩阵中信息的加速丧失有关。</li>
<li>提出了一种新型的信息保留视角来保持性能，即使在极端的符号压缩下。</li>
<li>介绍了TokenCarve框架，这是一个无需训练的、即插即用的两阶段符号压缩框架。</li>
<li>TokenCarve通过采用信息保留引导选择策略来删除和合并低信息符号，以优化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-65293ab3fd83f71893d41b2ee57ffa00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8364eaa66f565daccb158719e74d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd7b91b5f863e9846d41c68d98d3995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e68234c75816647b2af5070e8c52ec61.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MMLU-ProX-A-Multilingual-Benchmark-for-Advanced-Large-Language-Model-Evaluation"><a href="#MMLU-ProX-A-Multilingual-Benchmark-for-Advanced-Large-Language-Model-Evaluation" class="headerlink" title="MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model   Evaluation"></a>MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model   Evaluation</h2><p><strong>Authors:Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Yun Xing, Junjue Wang, Huitao Li, Xin Li, Kunyu Yu, Nan Liu, Qingyu Chen, Douglas Teodoro, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, Irene Li</strong></p>
<p>Traditional benchmarks struggle to evaluate increasingly sophisticated language models in multilingual and culturally diverse contexts. To address this gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark covering 13 typologically diverse languages with approximately 11,829 questions per language. Building on the challenging reasoning-focused design of MMLU-Pro, our framework employs a semi-automatic translation process: translations generated by state-of-the-art large language models (LLMs) are rigorously evaluated by expert annotators to ensure conceptual accuracy, terminological consistency, and cultural relevance. We comprehensively evaluate 25 state-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot prompting strategies, analyzing their performance across linguistic and cultural boundaries. Our experiments reveal consistent performance degradation from high-resource languages to lower-resource ones, with the best models achieving over 70% accuracy on English but dropping to around 40% for languages like Swahili, highlighting persistent gaps in multilingual capabilities despite recent advances. MMLU-ProX is an ongoing project; we are expanding our benchmark by incorporating additional languages and evaluating more language models to provide a more comprehensive assessment of multilingual capabilities. </p>
<blockquote>
<p>传统基准测试在评估日益复杂的多语言和文化背景的语言模型时面临挑战。为了解决这一差距，我们推出了MMLU-ProX，这是一个全面的多语言基准测试，涵盖13种语言形态多样的语言，每种语言大约有11,829个问题。基于具有挑战性的推理导向设计MMLU-Pro，我们的框架采用半自动翻译过程：由最新大型语言模型（LLM）生成的翻译会经过专家注释者的严格评估，以确保概念准确性、术语一致性和文化相关性。我们使用最新的LLM对性能进行综合评估采用具有五次连续性思考的初步思考和零射引发策略来推进分析其在语言和跨文化领域的表现。我们的实验揭示了从资源丰富型语言到资源稀缺型语言的性能持续下降，最佳模型在英语上的准确率超过70%，但在如斯瓦希里语等语言上的准确率降至约40%，这突显了尽管近期有所进展，但在多语言能力方面仍存在持续的差距。MMLU-ProX是一个正在进行中的项目；我们正在通过增加语言和评估更多的语言模型来扩展我们的基准测试，以提供更全面的多语言能力评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10497v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MMLU-ProX是一个针对多语言模型性能评估的新基准测试，覆盖了包括低资源语言在内的13种不同语言。该研究使用了基于大型语言模型（LLM）的半自动翻译过程，并经过专家评估确保翻译的准确性、一致性和文化相关性。该研究分析了多语言环境下的语言模型性能，发现高资源语言到低资源语言的性能退化问题仍然显著存在。MMLU-ProX是一个持续发展的项目，计划在未来继续扩展语言和模型评估范围。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMLU-ProX是一个针对多语言模型性能评估的新基准测试，覆盖了多达13种语言，目的是评估日渐精细的语言模型在多语言和跨文化背景下的表现。</li>
<li>MMLU-ProX通过引入半自动翻译过程并采用最新的大型语言模型进行翻译生成，再经过专家评估来确保翻译的质量与准确性。</li>
<li>该研究使用不同的测试策略评估了语言模型的性能，如基于推理的CoT策略和零样本提示策略。这些策略对语言模型在多语言和跨文化环境下的性能进行了全面评估。</li>
<li>研究发现，从高资源语言到低资源语言的性能退化问题仍然存在，即使在英语等语言上表现最好的模型在斯瓦希里等语言上的准确率也只有大约40%。这凸显了多语言能力方面的持续差距。</li>
<li>MMLU-ProX项目仍在发展中，计划通过增加更多语言和评估更多语言模型来提供更全面的多语言能力评估。这表明该领域的研究和进步仍在持续进行中。</li>
<li>此研究的重点在于展示和分析语言模型在多种不同语言环境下的表现，强调多语言能力的重要性及其挑战。这不仅对于研究者和开发者具有重要意义，也对实际应用中的多语言处理提出了挑战和机遇。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6f3e05495bab0734305b38af9b06bcbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5bab2a8c981b137c281593d9df43ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112ea37b9e998b9775c811b423a2a015.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Whisper-Speaker-Identification-Leveraging-Pre-Trained-Multilingual-Transformers-for-Robust-Speaker-Embeddings"><a href="#Whisper-Speaker-Identification-Leveraging-Pre-Trained-Multilingual-Transformers-for-Robust-Speaker-Embeddings" class="headerlink" title="Whisper Speaker Identification: Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings"></a>Whisper Speaker Identification: Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings</h2><p><strong>Authors:Jakaria Islam Emon, Md Abu Salek, Kazi Tamanna Alam</strong></p>
<p>Speaker identification in multilingual settings presents unique challenges, particularly when conventional models are predominantly trained on English data. In this paper, we propose WSI (Whisper Speaker Identification), a framework that repurposes the encoder of the Whisper automatic speech recognition model pre trained on extensive multilingual data to generate robust speaker embeddings via a joint loss optimization strategy that leverages online hard triplet mining and self supervised Normalized Temperature-scaled Cross Entropy loss. By capitalizing on Whisper language-agnostic acoustic representations, our approach effectively distinguishes speakers across diverse languages and recording conditions. Extensive evaluations on multiple corpora, including VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish, Chinese, and Japanese), and Voxconverse (English), demonstrate that WSI consistently outperforms state-of-the-art baselines, namely Pyannote Embedding, ECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC scores. These results validate our hypothesis that a multilingual pre-trained ASR encoder, combined with joint loss optimization, substantially improves speaker identification performance in non-English languages. </p>
<blockquote>
<p>在多语言环境中进行说话人识别面临着独特的挑战，尤其是在传统的模型主要基于英语数据进行训练的情况下。在本文中，我们提出了WSI（Whisper说话人识别）框架，它重新利用了在大量多语言数据上预训练的Whisper自动语音识别模型的编码器，通过联合损失优化策略生成稳健的说话人嵌入，该策略利用在线硬三元组挖掘和自我监督的归一化温度尺度交叉熵损失。通过利用Whisper语言无关的声学表示，我们的方法有效地在不同的语言和录音条件下区分说话人。在多个语料库上的广泛评估，包括VoxTube（多语言）、JVS（日语）、CallHome（德语、西班牙语、中文和日语）和Voxconverse（英语），证明WSI在平等误差率更低、AUC分数更高的情况下，始终优于Pyannote Embedding、ECAPA TDNN和Xvector等最新基线。这些结果验证了我们假设的正确性，即多语言预训练的ASR编码器与联合损失优化相结合，可以显著提高非英语语言的说话人识别性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10446v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong>：</p>
<p>本文提出了WSI（Whisper语音识别模型中的说话人识别）框架，该框架利用预训练于丰富多语种数据的Whisper自动语音识别模型的编码器生成稳健的说话人嵌入。通过采用在线硬三元组挖掘和自监督的归一化温度缩放交叉熵损失的联合损失优化策略，该框架能有效区分不同语言和录音条件下的说话人。在多个语料库上的广泛评估表明，WSI在平等错误率和AUC得分方面均优于Pyannote嵌入、ECAPA TDNN和Xvector等最新基线模型，验证了多语种预训练ASR编码器结合联合损失优化在非英语语种说话人识别方面的显著提升效果。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>本文提出了WSI框架，旨在解决多语种环境下的说话人识别挑战。</li>
<li>WSI利用预训练于多语种数据的Whisper自动语音识别模型的编码器生成说话人嵌入。</li>
<li>通过联合损失优化策略，WSI能有效区分不同语言和录音条件下的说话人。</li>
<li>广泛评估表明，WSI在多个语料库上的性能优于其他最新基线模型。</li>
<li>WSI在平等错误率和AUC得分方面均表现出色。</li>
<li>验证了在非英语语种说话人识别方面，多语种预训练ASR编码器结合联合损失优化可显著提升效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a0bdb47d4309b81002513c45e2941c27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5134bc6b613250bc8e8cec2b7b35f596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec54433401449c1327c5b908bb83f91f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47edbaddccb1b3c2956900ecb1d2eac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-215ea2545d9f8c9a6d5a0b39e6595fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd5f3c30e7e27880af551dd55aa3719.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models"><a href="#4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models" class="headerlink" title="4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models"></a>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models</h2><p><strong>Authors:Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister</strong></p>
<p>Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries. </p>
<blockquote>
<p>学习4D语言字段以实现动态场景中的时间敏感和开放结束语言查询对于许多实际应用至关重要。虽然LangSplat成功地将CLIP特征转化为3D高斯表示，实现了在3D静态场景中的精度和效率，但它缺乏处理动态4D字段的能力，因为CLIP是为静态图像文本任务设计的，无法捕获视频中的时间动态。现实世界的环境本质上是动态的，物体语义会随时间演变。要建立精确的4D语言字段，必须获得像素对齐的、面向对象的视频特征，而当前视觉模型很难做到这一点。为了解决这些挑战，我们提出了4D LangSplat，它学习4D语言字段以高效地处理动态场景中的时间无关或时间敏感的开放词汇查询。4D LangSplat绕过从视觉特征学习语言字段，而是直接从通过多模态大型语言模型（MLLM）生成的面向对象的视频字幕中学习。具体来说，我们提出了一种多模态面向对象的视频提示方法，包括视觉和文本提示，引导MLLM为视频中的对象生成详细、时间一致、高质量的字幕。这些字幕使用大型语言模型进行编码，以生成高质量句子嵌入，然后作为像素对齐的、面向对象的特征监督，通过共享嵌入空间实现开放式文本查询。我们认识到4D场景中对象的状态转换是平滑的，因此进一步提出了状态可变形网络来有效地模拟这些随时间变化的连续变化。我们在多个基准测试上的结果证明，4D LangSplat对于时间敏感和时间无关的开放式查询都达到了精确和高效的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10437v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://4d-langsplat.github.io/">https://4d-langsplat.github.io</a></p>
<p><strong>摘要</strong></p>
<p>学习4D语言字段以处理动态场景中的时间敏感和开放端语言查询对于许多实际应用至关重要。虽然LangSplat成功地将CLIP特性融入3D高斯表示，实现了在3D静态场景中的精确性和效率，但它无法处理动态的4D字段。现实世界的环境本质上是动态的，物体语义随时间演变。为了构建精确的4D语言字段，必须从视频中获得像素对齐的物体级特征，而当前视觉模型难以实现这一目标。为解决这些挑战，我们提出了4D LangSplat，它能够处理动态场景中的时间无关或时间敏感的开放词汇查询。4D LangSplat绕过从视觉特征学习语言字段，而是直接从由对象级视频字幕生成的多模态大型语言模型（MLLMs）中学习。我们提出了一种多模态对象级视频提示方法，包括视觉和文本提示，引导MLLMs为视频中的对象生成详细、时间连贯的高质量字幕。这些字幕使用大型语言模型编码成高质量句子嵌入，作为像素对齐的对象特定特征监督，通过共享嵌入空间实现开放词汇文本查询。认识到4D场景中对象的平滑状态转换，我们进一步提出了状态可变形网络来有效模拟这些随时间变化的连续变化。我们的多项基准测试结果证明，4D LangSplat对于时间敏感和时间无关的开放词汇查询均能达到精确和高效的结果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>学习4D语言字段对于处理现实世界中动态场景的时间敏感和开放端语言查询至关重要。</li>
<li>LangSplat在3D静态场景中表现良好，但无法处理动态的4D字段。</li>
<li>4D LangSplat通过直接学习从对象级视频字幕生成的多模态大型语言模型（MLLMs）来应对这一挑战。</li>
<li>提出了一种多模态对象级视频提示方法，包括视觉和文本提示，以生成高质量的、与时间相关的对象字幕。</li>
<li>使用大型语言模型将字幕编码成高质量句子嵌入，作为像素对齐的对象特定特征监督。</li>
<li>通过共享嵌入空间，可以实现开放词汇文本查询。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10437">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-31e10094985ed60b63a44eae59c925ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0386885b55626e702f4946956c9ad1ae.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Limitations-of-Vision-Language-Models-in-Understanding-Image-Transforms"><a href="#On-the-Limitations-of-Vision-Language-Models-in-Understanding-Image-Transforms" class="headerlink" title="On the Limitations of Vision-Language Models in Understanding Image   Transforms"></a>On the Limitations of Vision-Language Models in Understanding Image   Transforms</h2><p><strong>Authors:Ahmad Mustafa Anis, Hasnain Ali, Saquib Sarfraz</strong></p>
<p>Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image&#x2F;Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations. </p>
<blockquote>
<p>视觉语言模型（VLMs）在各种下游任务中显示出巨大的潜力，包括图像&#x2F;视频生成、视觉问答、多模态聊天机器人和视频理解。然而，这些模型在基本的图像转换方面经常遇到困难。本文研究了VLMs的图像级理解，特别是OpenAI的CLIP和Google的SigLIP。我们的研究结果表明，这些模型缺乏对图像级增强的理解。为了促进这项研究，我们创建了Flickr8k数据集的增强版本，为每个图像提供所应用转换的详细描述。我们还进一步探讨了这种缺陷对下游任务的影响，特别是在图像编辑方面，并评估了最先进的Image2Image模型在简单转换上的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09837v1">PDF</a> 8 pages, 15 images</p>
<p><strong>Summary</strong><br>     视觉语言模型（VLMs）在下游任务中展现出巨大潜力，包括图像&#x2F;视频生成、视觉问答、多模态聊天机器人和视频理解。然而，这些模型在基本图像转换方面存在困难。本文研究了VLMs的图像级理解，特别是OpenAI的CLIP和Google的SigLIP。研究发现，这些模型无法理解多种图像级增强。为了推进这项研究，我们创建了Flickr8k数据集的增强版本，为每张图像配备详细的转换描述。我们还探讨了这种缺陷对下游任务、尤其是图像编辑的影响，并评估了最新Image2Image模型在简单转换上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在多个下游任务中表现出显著潜力。</li>
<li>VLMs在基本图像转换方面存在挑战。</li>
<li>本文研究了VLMs的图像级理解，特别是CLIP和SigLIP模型。</li>
<li>这些模型无法理解多种图像级增强，表明其局限性。</li>
<li>为了推进研究，创建了一个配备详细转换描述的Flickr8k数据集增强版本。</li>
<li>VLMs的缺陷对下游任务、尤其是图像编辑产生影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09837">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2cb46979643ad8fca7fd95cd099c1368.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c454940760913645b686754cbcd6a46d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6ecc18507cccac1927dc33660f3a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87db6d2273c54cbdeae08981d6ed6c15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b9856e4640fd98b698ce3751e3546eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2731ce35c28573ac02b6438fa41a7d5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b49fd19c024995dbc350d070f825249b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-15  SurgRAW Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5f65fe55be11d251101f7f1bf9c9617e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-15  SciVerse Unveiling the Knowledge Comprehension and Visual Reasoning of   LMMs on Multi-modal Scientific Problems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17204.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
