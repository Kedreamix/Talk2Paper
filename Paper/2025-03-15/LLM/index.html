<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  HybridVLA Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-15-æ›´æ–°"><a href="#2025-03-15-æ›´æ–°" class="headerlink" title="2025-03-15 æ›´æ–°"></a>2025-03-15 æ›´æ–°</h1><h2 id="HybridVLA-Collaborative-Diffusion-and-Autoregression-in-a-Unified-Vision-Language-Action-Model"><a href="#HybridVLA-Collaborative-Diffusion-and-Autoregression-in-a-Unified-Vision-Language-Action-Model" class="headerlink" title="HybridVLA: Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model"></a>HybridVLA: Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model</h2><p><strong>Authors:Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang</strong></p>
<p>Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”¨äºå¸¸è¯†æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„å‘å±•ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œé€šç”¨æ“ä½œã€‚å°½ç®¡ç°æœ‰çš„è‡ªå›å½’VLAæ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçŸ¥è¯†ï¼Œä½†å®ƒä»¬ä¼šç ´åè¡ŒåŠ¨çš„è¿ç»­æ€§ã€‚åŒæ—¶ï¼Œä¸€äº›VLAæ–¹æ³•åŠ å…¥äº†ä¸€ä¸ªé¢å¤–çš„æ‰©æ•£å¤´æ¥é¢„æµ‹è¿ç»­è¡ŒåŠ¨ï¼Œä»…ä¾èµ–äºVLMæå–çš„ç‰¹å¾ï¼Œè¿™é™åˆ¶äº†å…¶æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†HybridVLAï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ— ç¼é›†æˆè‡ªå›å½’å’Œæ‰©æ•£ç­–ç•¥çš„ä¼˜ç‚¹ï¼Œè€Œä¸æ˜¯ç®€å•åœ°è¿æ¥å®ƒä»¬ã€‚ä¸ºäº†å¼¥åˆç”Ÿæˆå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä½œè®­ç»ƒé…æ–¹ï¼Œå°†æ‰©æ•£å»ºæ¨¡ç›´æ¥æ³¨å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ã€‚é€šè¿‡è¿™ä¸€é…æ–¹ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸¤ç§è¡ŒåŠ¨é¢„æµ‹ä¸ä»…ç›¸äº’åŠ å¼ºï¼Œè€Œä¸”åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°å„ä¸ç›¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åä½œè¡ŒåŠ¨é›†æˆæœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°èåˆè¿™ä¸¤ç§é¢„æµ‹ï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„æ§åˆ¶ã€‚åœ¨å®éªŒä¸­ï¼ŒHybridVLAåœ¨å„ç§ä»¿çœŸå’ŒçœŸå®ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰æœ€å…ˆè¿›çš„VLAæ–¹æ³•çš„æ•ˆæœï¼ŒåŒ…æ‹¬å•è‡‚å’ŒåŒè‡‚æœºå™¨äººï¼ŒåŒæ—¶åœ¨ä»¥å‰æœªè§è¿‡çš„é…ç½®ä¸­è¡¨ç°å‡ºç¨³å®šçš„æ“ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10631v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HybridVLAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è‡ªå›å½’å’Œæ‰©æ•£ç­–ç•¥çš„ä¼˜åŠ¿ï¼Œåœ¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ— ç¼é›†æˆï¼Œä»¥å¼ºåŒ–æœºå™¨äººæ‰§è¡Œé€šç”¨æ“ä½œçš„èƒ½åŠ›ã€‚é€šè¿‡åä½œè®­ç»ƒç­–ç•¥ï¼Œå°†æ‰©æ•£å»ºæ¨¡ç›´æ¥æ³¨å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ä¸­ï¼Œä»¥ç¼©å°ç”Ÿæˆå·®è·ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§åä½œè¡ŒåŠ¨é›†æˆæœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°èåˆä¸¤ç§é¢„æµ‹ï¼Œä»¥å®ç°æ›´ç¨³å¥çš„æ§åˆ¶ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­ï¼ŒHybridVLAåœ¨å•è‡‚å’ŒåŒè‡‚æœºå™¨äººä»»åŠ¡ä¸­å‡ä¼˜äºå…ˆå‰çš„ä¸»æµVLAæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HybridVLAæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç»“åˆäº†è‡ªå›å½’å’Œæ‰©æ•£ç­–ç•¥çš„ä¼˜åŠ¿ï¼Œå¢å¼ºäº†æœºå™¨äººæ‰§è¡Œé€šç”¨æ“ä½œçš„èƒ½åŠ›ã€‚</li>
<li>åä½œè®­ç»ƒç­–ç•¥å°†æ‰©æ•£å»ºæ¨¡ç›´æ¥æ³¨å…¥ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ï¼Œä»¥ç¼©å°ç”Ÿæˆå·®è·ã€‚</li>
<li>è‡ªå›å½’å’Œæ‰©æ•£ç­–ç•¥åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸åŒçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåä½œè¡ŒåŠ¨é›†æˆæœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°èåˆä¸¤ç§åŠ¨ä½œé¢„æµ‹ã€‚</li>
<li>HybridVLAåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å•è‡‚å’ŒåŒè‡‚æœºå™¨äººä»»åŠ¡ä¸­éƒ½ä¼˜äºå…ˆå‰çš„ä¸»æµVLAæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed5327c076ca82e33a6c93e1dbe54782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17b2c6dd7255677ad343553ffa3e15dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee4487719b66c6f17bf0b5274865e77.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UniGoal-Towards-Universal-Zero-shot-Goal-oriented-Navigation"><a href="#UniGoal-Towards-Universal-Zero-shot-Goal-oriented-Navigation" class="headerlink" title="UniGoal: Towards Universal Zero-shot Goal-oriented Navigation"></a>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</h2><p><strong>Authors:Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu</strong></p>
<p>In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„é›¶æ ·æœ¬ç›®æ ‡å¯¼å‘å¯¼èˆªæ¡†æ¶ã€‚ç°æœ‰çš„é›¶æ ·æœ¬æ–¹æ³•åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºç‰¹å®šä»»åŠ¡çš„æ¨ç†æ¡†æ¶ï¼Œæ•´ä½“æµç¨‹å·®å¼‚è¾ƒå¤§ï¼Œä¸”æ— æ³•åœ¨ä¸åŒç±»å‹çš„ç›®æ ‡ä¹‹é—´æ¨å¹¿ã€‚ä¸ºäº†å®ç°é€šç”¨é›¶æ ·æœ¬å¯¼èˆªçš„ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€å›¾è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥èåˆä¸åŒçš„ç›®æ ‡ï¼ŒåŒ…æ‹¬å¯¹è±¡ç±»åˆ«ã€å®ä¾‹å›¾åƒå’Œæ–‡æœ¬æè¿°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è§‚å¯Ÿç»“æœè½¬æ¢ä¸ºåœ¨çº¿ç»´æŠ¤çš„åœºæ™¯å›¾ã€‚é€šè¿‡è¿™ç§ä¸€è‡´çš„åœºæ™¯å’Œç›®æ ‡è¡¨ç¤ºï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¿ç•™å¤§éƒ¨åˆ†ç»“æ„ä¿¡æ¯ï¼Œç›¸è¾ƒäºçº¯æ–‡æœ¬èƒ½å¤Ÿåˆ©ç”¨LLMè¿›è¡Œæ˜ç¡®çš„åŸºäºå›¾çš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´ç‚¹ä¸Šå¯¹åœºæ™¯å›¾å’Œç›®æ ‡å›¾è¿›è¡Œå›¾åŒ¹é…ï¼Œå¹¶æ ¹æ®ä¸åŒçš„åŒ¹é…çŠ¶æ€æå‡ºä¸åŒçš„ç­–ç•¥æ¥ç”Ÿæˆé•¿æœŸæ¢ç´¢ç›®æ ‡ã€‚å½“é›¶åŒ¹é…æ—¶ï¼Œä»£ç†é¦–å…ˆè¿­ä»£æœç´¢ç›®æ ‡çš„å­å›¾ã€‚éšç€éƒ¨åˆ†åŒ¹é…çš„å‡ºç°ï¼Œä»£ç†ç„¶ååˆ©ç”¨åæ ‡æŠ•å½±å’Œé”šç‚¹é…å¯¹æ¥å¯¹é½æ¥æ¨æ–­ç›®æ ‡ä½ç½®ã€‚æœ€åæ˜¯åœºæ™¯å›¾ä¿®æ­£å’Œç›®æ ‡éªŒè¯ä»¥å®Œæˆå®Œç¾åŒ¹é…ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªé»‘åå•æœºåˆ¶ï¼Œä»¥å®ç°é˜¶æ®µä¹‹é—´çš„ç¨³å¥åˆ‡æ¢ã€‚åœ¨å‡ ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„UniGoalä½¿ç”¨å•ä¸€æ¨¡å‹åœ¨ä¸‰é¡¹ç ”ç©¶çš„å¯¼èˆªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ç‰¹å®šä»»åŠ¡çš„é›¶æ ·æœ¬æ–¹æ³•å’Œç›‘ç£é€šç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10630v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong>ï¼š<br>è¯¥æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€šç”¨å›¾è¡¨ç¤ºçš„é›¶æ ·æœ¬ç›®æ ‡å¯¼å‘å¯¼èˆªæ¡†æ¶ã€‚é€šè¿‡ç»Ÿä¸€å›¾è¡¨ç¤ºï¼Œå°†ä¸åŒç›®æ ‡ï¼ˆåŒ…æ‹¬å¯¹è±¡ç±»åˆ«ã€å®ä¾‹å›¾åƒå’Œæ–‡æœ¬æè¿°ï¼‰èåˆåœ¨ä¸€èµ·ï¼Œå®ç°äº†é›¶æ ·æœ¬å¯¼èˆªã€‚æ–‡ä¸­æå‡ºäº†åœºæ™¯å›¾çš„åœ¨çº¿ç»´æŠ¤ï¼Œä¿ç•™äº†å¤§å¤šæ•°ç»“æ„ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºå›¾çš„æ˜¾å¼æ¨ç†ã€‚é€šè¿‡å›¾åŒ¹é…å’Œä¸åŒçš„æ¢ç´¢ç­–ç•¥ï¼Œå®ç°äº†å¯¹æ¢ç´¢ç›®æ ‡çš„é•¿æœŸè§„åˆ’ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªé»‘åå•æœºåˆ¶æ¥å¢å¼ºé˜¶æ®µçš„ç¨³å¥æ€§åˆ‡æ¢ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„UniGoalæ¡†æ¶åœ¨ä¸‰ç§å¯¼èˆªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„é›¶æ ·æœ¬ç›®æ ‡å¯¼å‘å¯¼èˆªæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è·¨ä¸åŒç›®æ ‡çš„é›¶æ ·æœ¬å¯¼èˆªã€‚</li>
<li>é€šè¿‡ç»Ÿä¸€å›¾è¡¨ç¤ºï¼Œèåˆäº†ä¸åŒçš„ç›®æ ‡ï¼ŒåŒ…æ‹¬å¯¹è±¡ç±»åˆ«ã€å®ä¾‹å›¾åƒå’Œæ–‡æœ¬æè¿°ã€‚</li>
<li>é‡‡ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºäºå›¾çš„æ˜¾å¼æ¨ç†ï¼Œä¿ç•™äº†ç»“æ„ä¿¡æ¯ã€‚</li>
<li>åœºæ™¯å›¾çš„åœ¨çº¿ç»´æŠ¤ä½¿å¾—å¯¼èˆªæ›´åŠ ç²¾ç¡®å’Œçµæ´»ã€‚</li>
<li>é€šè¿‡å›¾åŒ¹é…å’Œä¸åŒçš„æ¢ç´¢ç­–ç•¥ï¼Œå®ç°äº†å¯¹æ¢ç´¢ç›®æ ‡çš„é•¿æœŸè§„åˆ’ã€‚</li>
<li>UniGoalæ¡†æ¶å…·æœ‰å¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬å¯¼èˆªæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc4497a6e4df9e873931ce68f0c60570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0329636c003965d65e238236a3890ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eee4bcb569b6e71d0dd579dee86fdd94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3d3ceed1548a7749765019a747f43c6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transformers-without-Normalization"><a href="#Transformers-without-Normalization" class="headerlink" title="Transformers without Normalization"></a>Transformers without Normalization</h2><p><strong>Authors:Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</strong></p>
<p>Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) &#x3D; \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks. </p>
<blockquote>
<p>æ ‡å‡†åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­æ— å¤„ä¸åœ¨ï¼Œå¹¶ä¸€ç›´è¢«è§†ä¸ºè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œä¸ä½¿ç”¨æ ‡å‡†åŒ–çš„Transformerå¯ä»¥é€šè¿‡ä¸€ç§éå¸¸ç®€å•çš„æŠ€æœ¯å®ç°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€åŒæ›²å‡½æ•°ï¼ˆDyTï¼‰ï¼Œä½œä¸ºä¸€ç§å…ƒç´ çº§æ“ä½œ$DyT(x) &#x3D; \tanh(\alpha x)$ï¼Œä½œä¸ºTransformerä¸­æ ‡å‡†åŒ–å±‚çš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚DyTçš„çµæ„Ÿæ¥æºäºè¿™æ ·ä¸€ä¸ªè§‚å¯Ÿï¼šTransformerä¸­çš„å±‚æ ‡å‡†åŒ–é€šå¸¸ä¼šäº§ç”Ÿç±»ä¼¼äºåŒæ›²å‡½æ•°ï¼ˆtanhï¼‰çš„Så½¢è¾“å…¥è¾“å‡ºæ˜ å°„ã€‚é€šè¿‡èå…¥DyTï¼Œä¸ä½¿ç”¨æ ‡å‡†åŒ–çš„Transformerå¯ä»¥åŒ¹é…æˆ–è¶…è¿‡å…¶æ ‡å‡†åŒ–å¯¹åº”ç‰©çš„æ€§èƒ½ï¼Œè€Œä¸”å¤§éƒ¨åˆ†æƒ…å†µä¸‹ä¸éœ€è¦è°ƒæ•´è¶…å‚æ•°ã€‚æˆ‘ä»¬åœ¨å¤šç§è®¾ç½®ä¸­éªŒè¯äº†ä½¿ç”¨DyTçš„Transformerçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä»è¯†åˆ«åˆ°ç”Ÿæˆã€ç›‘ç£å­¦ä¹ åˆ°è‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰åˆ°è¯­è¨€æ¨¡å‹ç­‰ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†æ ‡å‡†åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­ä¸å¯æˆ–ç¼ºçš„ä¼ ç»Ÿç†è§£ï¼Œå¹¶ä¸ºæ·±å…¥äº†è§£å…¶åœ¨æ·±åº¦ç½‘ç»œä¸­çš„ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10622v1">PDF</a> CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://jiachenzhu.github.io/DyT/">https://jiachenzhu.github.io/DyT/</a></p>
<p><strong>Summary</strong><br>ç°ä»£ç¥ç»ç½‘ç»œä¸­æ™®éå­˜åœ¨çš„å½’ä¸€åŒ–å±‚è¢«è®¤ä¸ºæ˜¯å¿…è¦çš„ï¼Œä½†è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸ä½¿ç”¨å½’ä¸€åŒ–çš„Transformeré€šè¿‡ä½¿ç”¨ä¸€ç§éå¸¸ç®€å•çš„æŠ€æœ¯ä¹Ÿèƒ½è¾¾åˆ°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚å¼•å…¥åŠ¨æ€åŒæ›²å‡½æ•°ï¼ˆDyTï¼‰ä½œä¸ºTransformerä¸­å½’ä¸€åŒ–å±‚çš„æ›¿ä»£æ–¹æ¡ˆã€‚DyTå—å¯å‘äºTransformerä¸­å±‚å½’ä¸€åŒ–äº§ç”Ÿçš„tanh-likeè¾“å…¥-è¾“å‡ºæ˜ å°„çš„è§‚å¯Ÿã€‚é€šè¿‡ç»“åˆDyTï¼Œä¸ä½¿ç”¨å½’ä¸€åŒ–çš„Transformerå¯ä»¥åŒ¹é…æˆ–è¶…è¿‡ä½¿ç”¨å½’ä¸€åŒ–çš„åŒç±»æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¤§å¤šä¸éœ€è¦è°ƒæ•´è¶…å‚æ•°ã€‚éªŒè¯äº†DyTåœ¨å¤šç§è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è¯†åˆ«ä¸ç”Ÿæˆä»»åŠ¡ã€ç›‘ç£å­¦ä¹ ä¸è‡ªç›‘ç£å­¦ä¹ ä»¥åŠè®¡ç®—æœºè§†è§‰å’Œè¯­è¨€æ¨¡å‹ç­‰é¢†åŸŸã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¸¸è§„è®¤çŸ¥ï¼Œå³å½’ä¸€åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œå¹¶ä¸ºæ·±å…¥äº†è§£å…¶åœ¨æ·±åº¦ç½‘ç»œä¸­çš„ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformersä¸ä½¿ç”¨å½’ä¸€åŒ–å±‚ä¹Ÿèƒ½å®ç°è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†åŠ¨æ€åŒæ›²å‡½æ•°ï¼ˆDyTï¼‰ä½œä¸ºTransformerä¸­å½’ä¸€åŒ–å±‚çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>DyTæ˜¯åŸºäºè§‚å¯ŸTransformerä¸­å±‚å½’ä¸€åŒ–çš„è¾“å…¥-è¾“å‡ºæ˜ å°„ç‰¹æ€§è€Œè®¾è®¡çš„ã€‚</li>
<li>ä½¿ç”¨DyTçš„Transformeræ¨¡å‹å¯ä»¥åŒ¹é…æˆ–è¶…è¿‡ä½¿ç”¨å½’ä¸€åŒ–çš„æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šç§ä»»åŠ¡è®¾ç½®ä¸­éƒ½éªŒè¯äº†DyTçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è¯†åˆ«ã€ç”Ÿæˆã€ç›‘ç£å­¦ä¹ ã€è‡ªç›‘ç£å­¦ä¹ ç­‰ã€‚</li>
<li>è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¸¸è§„è®¤çŸ¥ï¼Œå³å½’ä¸€åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1be3de9c01997296a09af7138e4440b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff9bff21d4f1df9594a1739aeab691c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33e9af8c548232f2696d74480ba43b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6493e10be8d54a9f671c035a57030fa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce12bbe850e908af0d9aac240d133cce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec54fb90313cb0e17c41338f3ad6445e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb40328db8275518338f3b9eddb654e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4f820f9d190e892c9c0a68f1418e6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4bfe828938c86bcb7885777e8f08d90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-796b4c307f63ef1478b69a7a872d96f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Siege-Autonomous-Multi-Turn-Jailbreaking-of-Large-Language-Models-with-Tree-Search"><a href="#Siege-Autonomous-Multi-Turn-Jailbreaking-of-Large-Language-Models-with-Tree-Search" class="headerlink" title="Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with   Tree Search"></a>Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with   Tree Search</h2><p><strong>Authors:Andy Zhou</strong></p>
<p>We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Siegeï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè½®å¯¹æŠ—æ€§æ¡†æ¶ï¼Œé€šè¿‡æ ‘æœç´¢çš„è§†è§’æ¥æ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®‰å…¨æ€§çš„é€æ¸ä¾µèš€ã€‚ä¸åŒäºä¾èµ–ç²¾å¿ƒè®¾è®¡çš„æç¤ºçš„å•è½®è¶Šç‹±ï¼ŒSiegeä»¥å¹¿åº¦ä¼˜å…ˆçš„æ–¹å¼æ‰©å±•æ¯ä¸€è½®çš„å¯¹è¯ï¼Œåˆ†æ”¯å‡ºå¤šä¸ªå¯¹æŠ—æ€§æç¤ºï¼Œè¿™äº›æç¤ºåˆ©ç”¨ä¹‹å‰å“åº”çš„éƒ¨åˆ†åˆè§„æ€§ã€‚é€šè¿‡è·Ÿè¸ªè¿™äº›é€æ­¥çš„æ”¿ç­–æ³„éœ²å¹¶å°†å…¶é‡æ–°æ³¨å…¥åç»­çš„æŸ¥è¯¢ä¸­ï¼ŒSiegeæ­ç¤ºäº†å¾®å°çš„è®©æ­¥æ˜¯å¦‚ä½•ç´¯ç§¯æˆå®Œå…¨ç¦æ­¢çš„è¾“å‡ºçš„ã€‚åœ¨JailbreakBenchæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSiegeåœ¨GPT-3.5 turboä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ï¼Œåœ¨GPT-4çš„å•è½®å¤šè¿è¡Œä¸­è¾¾åˆ°äº†97%çš„æˆåŠŸç‡ï¼Œä½¿ç”¨çš„æŸ¥è¯¢æ¬¡æ•°å°‘äºåŸºå‡†æµ‹è¯•å¦‚Crescendoæˆ–GOATã€‚è¿™ç§æ ‘æœç´¢æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ·±å…¥çš„è§†è§’ï¼Œæ¥è§‚å¯Ÿæ¨¡å‹ä¿éšœæªæ–½åœ¨è¿ç»­çš„å¯¹è¯å›åˆä¸­æ˜¯å¦‚ä½•é€€åŒ–çš„ï¼Œè¿™çªæ˜¾äº†å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œç¨³å¥çš„å¤šè½®æµ‹è¯•ç¨‹åºçš„ç´§è¿«æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10619v1">PDF</a> Accepted to ICLR 2025 Trustworthy LLM</p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æ¨å‡ºäº†Siegeï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå›åˆå¯¹æŠ—æ€§æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ ‘æœç´¢çš„è§’åº¦æ¥æ¨¡æ‹Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®‰å…¨æ€§çš„é€æ¸ä¾µèš€ã€‚ä¸åŒäºä¾èµ–ç²¾å¿ƒè®¾è®¡çš„å•ä¸€æç¤ºçš„å•å›åˆè¶Šç‹±ï¼ŒSiegeä»¥å¹¿åº¦ä¼˜å…ˆçš„æ–¹å¼æ‰©å±•å¯¹è¯çš„æ¯ä¸€å›åˆï¼Œåˆ†æ”¯å‡ºå¤šä¸ªå¯¹æŠ—æ€§æç¤ºï¼Œåˆ©ç”¨ä¹‹å‰å›åº”çš„éƒ¨åˆ†åˆè§„æ€§è¿›è¡Œæ”»å‡»ã€‚é€šè¿‡è·Ÿè¸ªè¿™äº›ç´¯ç§¯çš„æ”¿ç­–æ¼æ´å¹¶å°†å…¶é‡æ–°æ³¨å…¥åç»­æŸ¥è¯¢ï¼ŒSiegeæ­ç¤ºäº†å¾®å°çš„è®©æ­¥æ˜¯å¦‚ä½•ç´¯ç§¯æˆå®Œå…¨ä¸å…è®¸çš„è¾“å‡ºçš„ã€‚åœ¨JailbreakBenchæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSiegeåœ¨ä¸€æ¬¡å¤šå›åˆè¿è¡Œä¸­å®ç°äº†GPT-3.5turboçš„100%æˆåŠŸç‡ï¼ŒGPT-4ä¸Šçš„æˆåŠŸç‡ä¸º97%ï¼ŒæŸ¥è¯¢æ¬¡æ•°å°‘äºåŸºçº¿å¦‚Crescendoæˆ–GOATã€‚è¿™ç§æ ‘æœç´¢æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ·±å…¥çš„è§†è§’ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æ¨¡å‹ä¿éšœæªæ–½å¦‚ä½•åœ¨è¿ç»­çš„å¯¹è¯å›åˆä¸­é™çº§ï¼Œå¼ºè°ƒäº†ä¸ºè¯­è¨€æ¨¡å‹å®æ–½ç¨³å¥çš„å¤šå›åˆæµ‹è¯•ç¨‹åºçš„ç´§è¿«æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Siegeæ˜¯ä¸€ä¸ªå¤šå›åˆå¯¹æŠ—æ€§æ¡†æ¶ï¼Œæ¨¡æ‹ŸLLMå®‰å…¨æ€§çš„é€æ¸ä¾µèš€ã€‚</li>
<li>å®ƒé‡‡ç”¨æ ‘æœç´¢æ–¹æ³•æ¥æ‰©å±•å¯¹è¯ï¼Œå¹¶åœ¨æ¯ä¸€å›åˆä¸­åˆ†æ”¯å‡ºå¤šä¸ªå¯¹æŠ—æ€§æç¤ºã€‚</li>
<li>Siegeé€šè¿‡è·Ÿè¸ªæ”¿ç­–æ¼æ´å¹¶å°†å…¶é‡æ–°æ³¨å…¥åç»­æŸ¥è¯¢ï¼Œæ­ç¤ºäº†LLMçš„å®‰å…¨é—®é¢˜ã€‚</li>
<li>åœ¨JailbreakBenchæ•°æ®é›†ä¸Šï¼ŒSiegeåœ¨GPT-3.5turboä¸Šçš„æˆåŠŸç‡ä¸º100%ï¼Œåœ¨GPT-4ä¸Šä¸º97%ã€‚</li>
<li>ç›¸æ¯”äºåŸºçº¿æ–¹æ³•å¦‚Crescendoæˆ–GOATï¼ŒSiegeä½¿ç”¨äº†æ›´å°‘çš„æŸ¥è¯¢æ¬¡æ•°ã€‚</li>
<li>Siegeæ­ç¤ºäº†æ¨¡å‹ä¿éšœæªæ–½å¦‚ä½•åœ¨è¿ç»­çš„å¯¹è¯å›åˆä¸­é€æ¸å¤±æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd20267ebb6c68543078f2f747ab6b78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0c015ec3f503bb28babd836e17fe59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02984606309b20effe7f61be32d9db3d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Compositional-Subspace-Representation-Fine-tuning-for-Adaptive-Large-Language-Models"><a href="#Compositional-Subspace-Representation-Fine-tuning-for-Adaptive-Large-Language-Models" class="headerlink" title="Compositional Subspace Representation Fine-tuning for Adaptive Large   Language Models"></a>Compositional Subspace Representation Fine-tuning for Adaptive Large   Language Models</h2><p><strong>Authors:Andy Zhou</strong></p>
<p>Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead. </p>
<blockquote>
<p>é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ å¯èƒ½ä¼šå¯¼è‡´è·¨æŠ€èƒ½å¹²æ‰°ï¼Œå³æŸé¡¹æŠ€èƒ½çš„æå‡ä¼šæŸå®³å¦ä¸€é¡¹æŠ€èƒ½ã€‚è™½ç„¶LoRAç­‰æ–¹æ³•åœ¨æƒé‡å±‚é¢æ–½åŠ æ­£äº¤æ€§çº¦æŸï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰å®Œå…¨è§£å†³éšè—çŠ¶æ€è¡¨ç¤ºä¸­çš„å¹²æ‰°é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ç»„åˆå­ç©ºé—´è¡¨ç¤ºå¾®è°ƒï¼ˆCS-ReFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¡¨ç¤ºçš„æ–°æ–¹æ³•ï¼Œå­¦ä¹ å¤šä¸ªæ­£äº¤å­ç©ºé—´å˜æ¢ï¼Œæ¯ä¸ªå˜æ¢éƒ½ä¸“æ³¨äºä¸€ç§ç‹¬ç‰¹çš„æŠ€èƒ½ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±å™¨è¿›è¡Œç»„åˆã€‚é€šè¿‡åœ¨éšè—çŠ¶æ€ä¸­éš”ç¦»è¿™äº›å­ç©ºé—´ç¼–è¾‘ï¼Œè€Œä¸æ˜¯æƒé‡çŸ©é˜µï¼ŒCS-ReFTæ›´æœ‰æ•ˆåœ°é˜²æ­¢äº†è·¨ä»»åŠ¡å†²çªã€‚åœ¨AlpacaEvalåŸºå‡†æµ‹è¯•ä¸­ï¼Œå°†CS-ReFTåº”ç”¨äºLlama-2-7Bæ¨¡å‹ï¼Œå–å¾—äº†93.94%çš„èƒœç‡ï¼Œè¶…è¿‡äº†GPT-3.5 Turboï¼ˆ86.30%ï¼‰ï¼ŒåŒæ—¶ä»…éœ€è¦æ¨¡å‹å‚æ•°çš„0.0098%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé€šè¿‡ç®€å•è·¯ç”±å™¨ç»„åˆçš„ä¸“ä¸šè¡¨ç¤ºç¼–è¾‘ï¼Œå¯ä»¥æ˜¾è‘—æé«˜å¤šä»»åŠ¡æŒ‡ä»¤çš„æ‰§è¡ŒåŠ›ï¼Œä¸”å‡ ä¹ä¸äº§ç”Ÿé¢å¤–å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10617v1">PDF</a> Accepted to ICLR 2025 SCOPE</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡é€‚åº”è¿‡ç¨‹ä¸­ä¼šå‡ºç°æŠ€èƒ½é—´å¹²æ‰°é—®é¢˜ï¼Œå³æå‡æŸé¡¹æŠ€èƒ½çš„åŒæ—¶ä¼šé™ä»¥åŠå…¶ä»–æŠ€èƒ½çš„æ€§èƒ½ã€‚ç°æœ‰æ–¹æ³•å¦‚LoRAè™½åœ¨æƒé‡å±‚é¢æ–½åŠ æ­£äº¤æ€§çº¦æŸï¼Œä½†å¹¶æœªå®Œå…¨è§£å†³éšè—çŠ¶æ€è¡¨ç¤ºä¸­çš„å¹²æ‰°é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¡¨ç¤ºçš„æ–¹æ³•â€”â€”Compositional Subspace Representation Fine-tuningï¼ˆCS-ReFTï¼‰ï¼Œå­¦ä¹ å¤šä¸ªæ­£äº¤çš„å­ç©ºé—´å˜æ¢ï¼Œæ¯ä¸ªå˜æ¢ä¸“é•¿äºä¸€ç§æŠ€èƒ½ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±å™¨è¿›è¡Œç»„åˆã€‚é€šè¿‡éš”ç¦»éšè—çŠ¶æ€ä¸­çš„å­ç©ºé—´ç¼–è¾‘ï¼Œè€Œéæƒé‡çŸ©é˜µï¼ŒCS-ReFTæ›´æœ‰æ•ˆåœ°é˜²æ­¢äº†è·¨ä»»åŠ¡å†²çªã€‚åœ¨AlpacaEvalåŸºå‡†æµ‹è¯•ä¸­ï¼Œå°†CS-ReFTåº”ç”¨äºLlama-2-7Bæ¨¡å‹ï¼Œå–å¾—äº†93.94%çš„é«˜èƒœç‡ï¼Œè¶…è¶Šäº†GPT-3.5 Turboï¼ˆ86.30%ï¼‰ï¼Œå¹¶ä¸”ä»…éœ€è¦æ¨¡å‹å‚æ•°çš„0.0098%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç®€å•è·¯ç”±å™¨ç»„åˆçš„ä¸“ä¸šåŒ–è¡¨ç¤ºç¼–è¾‘èƒ½æ˜¾è‘—æé«˜å¤šä»»åŠ¡æŒ‡ä»¤æ‰§è¡Œæ•ˆç‡ï¼Œä¸”å¸¦æ¥çš„é¢å¤–å¼€é”€å¾ˆå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä»»åŠ¡é€‚åº”æ—¶ä¼šé¢ä¸´æŠ€èƒ½é—´å¹²æ‰°é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚LoRAå¹¶ä¸å®Œå…¨è§£å†³éšè—çŠ¶æ€è¡¨ç¤ºä¸­çš„å¹²æ‰°ã€‚</li>
<li>CS-ReFTé€šè¿‡å­¦ä¹ å¤šä¸ªæ­£äº¤çš„å­ç©ºé—´å˜æ¢æ¥è§£å†³è·¨ä»»åŠ¡å¹²æ‰°é—®é¢˜ã€‚</li>
<li>æ¯ä¸ªå­ç©ºé—´å˜æ¢åœ¨CS-ReFTä¸­ä¸“æ³¨äºä¸€ç§ç‰¹å®šæŠ€èƒ½ã€‚</li>
<li>CS-ReFTé€šè¿‡è½»é‡çº§è·¯ç”±å™¨ç»„åˆè¿™äº›å­ç©ºé—´å˜æ¢ã€‚</li>
<li>CS-ReFTåœ¨AlpacaEvalåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œèƒœè¿‡GPT-3.5 Turboã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00ee9975ea4757911fefa69dbb40331c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1fff38f869d9c1bbf6bcf7162e2727e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55fc31a6a13c51dfbe16493b5258e774.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization"><a href="#R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization" class="headerlink" title="R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization"></a>R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization</h2><p><strong>Authors:Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen</strong></p>
<p>Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ¨ç†ï¼Œè¿™éœ€è¦æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å¾€å¾€éš¾ä»¥æœ‰æ•ˆåœ°åˆ†æå’Œç†è§£è§†è§‰å†…å®¹ï¼Œå¯¼è‡´åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œç¼ºä¹å…¨é¢çš„åŸºå‡†æµ‹è¯•é˜»ç¢äº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„å‡†ç¡®è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†R1-Onevisionï¼Œä¸€ä¸ªæ—¨åœ¨å¼¥åˆè§†è§‰æ„ŸçŸ¥å’Œæ·±åº¦æ¨ç†ä¹‹é—´å·®è·çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€æ¨ç†ç®¡é“ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ­£å¼çš„çº¹ç†è¡¨ç¤ºï¼Œä»è€Œå®ç°åŸºäºç²¾ç¡®è¯­è¨€çš„æ¨ç†ã€‚é€šè¿‡è¿™ä¸ªç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†R1-Onevisionæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨å„ä¸ªé¢†åŸŸæä¾›äº†è¯¦ç»†ã€åˆ†æ­¥éª¤çš„å¤šæ¨¡æ€æ¨ç†æ³¨é‡Šã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ æ¥å¼€å‘R1-Onevisionæ¨¡å‹ï¼ŒåŸ¹å…»å…ˆè¿›çš„æ¨ç†å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ä¸åŒç­‰çº§çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸äººçš„æ•™è‚²é˜¶æ®µç›¸å¯¹åº”çš„æ ‡å‡†R1-Onevision-Benchï¼Œæ¶µç›–ä»åˆä¸­åˆ°å¤§å­¦åŠä»¥åçš„è€ƒè¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Onevisionå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºGPT-4oå’ŒQwen2.5-VLç­‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10615v1">PDF</a> Code and Model: <a target="_blank" rel="noopener" href="https://github.com/Fancy-MLLM/R1-onevision">https://github.com/Fancy-MLLM/R1-onevision</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ–‡æœ¬ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éœ€è¦æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯çš„å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†æå’Œç†è§£è§†è§‰å†…å®¹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸ä½³ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†R1-Onevisionå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡è·¨æ¨¡æ€æ¨ç†ç®¡é“å°†å›¾åƒè½¬åŒ–ä¸ºæ­£å¼æ–‡æœ¬è¡¨ç¤ºï¼Œå®ç°ç²¾ç¡®çš„è¯­è¨€æ¨ç†ã€‚å€ŸåŠ©è¯¥ç®¡é“ï¼Œæ„å»ºäº†R1-Onevisionæ•°æ®é›†ï¼Œæä¾›ä¸åŒé¢†åŸŸè¯¦ç»†çš„é€æ­¥å¤šæ¨¡æ€æ¨ç†æ³¨é‡Šã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥å‘å±•äº†R1-Onevisionæ¨¡å‹çš„å…ˆè¿›æ¨ç†å’Œç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºå…¨é¢è¯„ä¼°ä¸åŒç­‰çº§çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸äººç±»æ•™è‚²é˜¶æ®µç›¸ç¬¦çš„R1-Onevision-BenchåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä»åˆä¸­åˆ°å¤§å­¦åŠä»¥åçš„è€ƒè¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Onevisionåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ–‡æœ¬ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»æœ‰æ˜¾è‘—æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åˆ†æå’Œç†è§£è§†è§‰å†…å®¹æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>R1-Onevisionå¤šæ¨¡æ€æ¨ç†æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€æ¨ç†ç®¡é“è½¬åŒ–å›¾åƒä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œå®ç°ç²¾ç¡®è¯­è¨€æ¨ç†ã€‚</li>
<li>R1-Onevisionæ•°æ®é›†æä¾›å¤šæ¨¡æ€æ¨ç†çš„è¯¦ç»†æ³¨é‡Šï¼Œæ¶µç›–ä¸åŒé¢†åŸŸã€‚</li>
<li>R1-Onevisionæ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å‘å±•äº†å…ˆè¿›æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>R1-Onevision-BenchåŸºå‡†æµ‹è¯•è¯„ä¼°ä¸åŒç­‰çº§çš„å¤šæ¨¡æ€æ¨ç†æ€§èƒ½ï¼Œä¸äººç±»çš„å„ä¸ªé˜¶æ®µæ•™è‚²ç›¸ç¬¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb8237035f928e8b62af18ec3e240ce1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16752ac75fa0fe28dc5b056a5e187cfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4d139b9a615271f0b61530bae848ae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa35ecce8741f0e38a5eef55a96232db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d79e2c222974a93705052dae305edd64.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TruthPrInt-Mitigating-LVLM-Object-Hallucination-Via-Latent-Truthful-Guided-Pre-Intervention"><a href="#TruthPrInt-Mitigating-LVLM-Object-Hallucination-Via-Latent-Truthful-Guided-Pre-Intervention" class="headerlink" title="TruthPrInt: Mitigating LVLM Object Hallucination Via Latent   Truthful-Guided Pre-Intervention"></a>TruthPrInt: Mitigating LVLM Object Hallucination Via Latent   Truthful-Guided Pre-Intervention</h2><p><strong>Authors:Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu</strong></p>
<p>Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the â€œoverall truthfulnessâ€ of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as â€œper-tokenâ€ hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist â€œgeneric truthful directionsâ€ shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at <a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/TruthPrInt">https://github.com/jinhaoduan/TruthPrInt</a>. </p>
<blockquote>
<p>å¯¹è±¡å¹»è§‰ï¼ˆOHï¼‰å·²è¢«è®¤ä¸ºæ˜¯å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é¢ä¸´çš„ä¸»è¦å¯ä¿¡æŒ‘æˆ˜ä¹‹ä¸€ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œéšè—çŠ¶æ€ç­‰å†…éƒ¨çŠ¶æ€ç¼–ç äº†ç”Ÿæˆå“åº”çš„â€œæ•´ä½“çœŸå®æ€§â€ã€‚ç„¶è€Œï¼Œå…³äºLVLMsçš„å†…éƒ¨çŠ¶æ€å¦‚ä½•å‘æŒ¥ä½œç”¨ï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦å¯ä»¥ä½œä¸ºâ€œé€è¯â€å¹»è§‰æŒ‡æ ‡çš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œè¿™å¯¹äºç¼“è§£OHè‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ·±å…¥æ¢è®¨äº†LVLMå†…éƒ¨çŠ¶æ€ä¸OHé—®é¢˜çš„å…³ç³»ï¼Œå¹¶å‘ç°ï¼šï¼ˆ1ï¼‰LVLMçš„å†…éƒ¨çŠ¶æ€æ˜¯å¹»è§‰è¡Œä¸ºçš„é«˜ç‰¹å¼‚æ€§é€è¯æŒ‡æ ‡ã€‚ï¼ˆ2ï¼‰ä¸åŒçš„LVLMåœ¨å…±åŒçš„æ½œåœ¨å­ç©ºé—´ä¸­ç¼–ç äº†å¹»è§‰çš„é€šç”¨æ¨¡å¼ï¼Œè¿™è¡¨æ˜å­˜åœ¨å„ç§LVLMå…±äº«çš„â€œé€šç”¨çœŸå®æ–¹å‘â€ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†çœŸå®å¼•å¯¼é¢„å¹²é¢„ï¼ˆTruthPrIntï¼‰ï¼Œå®ƒé¦–å…ˆå­¦ä¹ LVLMè§£ç çš„çœŸå®æ–¹å‘ï¼Œç„¶ååœ¨LVLMè§£ç æ—¶è¿›è¡ŒçœŸå®å¼•å¯¼æ¨ç†æ—¶é—´å¹²é¢„ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè·¨LVLMå’Œè·¨æ•°æ®å¹»è§‰æ£€æµ‹çš„å¯è½¬ç§»æ€§ï¼Œæˆ‘ä»¬æå‡ºComnHalluï¼Œé€šè¿‡æ„å»ºå’Œå¯¹é½å¹»è§‰æ½œåœ¨å­ç©ºé—´ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„å®éªŒè®¾ç½®ä¸­å¯¹TruthPrIntè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ï¼Œä»¥åŠæµè¡Œçš„LVLMå’ŒOHåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTruthPrIntæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/TruthPrInt">https://github.com/jinhaoduan/TruthPrInt</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10602v1">PDF</a> 15 pages, 9 figures, the first two authors contributed equally</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„å¯¹è±¡å¹»è§‰ï¼ˆOHï¼‰è¢«å…¬è®¤ä¸ºæ˜¯ä¸»è¦çš„å¯ä¿¡æŒ‘æˆ˜ä¹‹ä¸€ã€‚ç ”ç©¶å‘ç°ï¼ŒLVLMçš„å†…éƒ¨çŠ¶æ€ï¼ˆå¦‚éšè—çŠ¶æ€ï¼‰ç¼–ç äº†ç”Ÿæˆå“åº”çš„â€œæ•´ä½“çœŸå®æ€§â€ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šLVLMçš„å†…éƒ¨çŠ¶æ€å¦‚ä½•å‘æŒ¥ä½œç”¨ï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦èƒ½ä½œä¸ºâ€œæ¯ä¸ªæ ‡è®°â€çš„å¹»è§‰æŒ‡æ ‡æ¥å‡è½»OHã€‚æœ¬æ–‡é€šè¿‡æ·±å…¥ç ”ç©¶LVLMå†…éƒ¨çŠ¶æ€ä¸OHé—®é¢˜çš„å…³ç³»ï¼Œå‘ç°ï¼ˆ1ï¼‰LVLMå†…éƒ¨çŠ¶æ€æ˜¯é«˜ç‰¹å¼‚æ€§çš„ã€é’ˆå¯¹æ¯ä¸ªæ ‡è®°çš„å¹»è§‰è¡Œä¸ºæŒ‡æ ‡ã€‚ï¼ˆ2ï¼‰ä¸åŒçš„LVLMåœ¨å…±åŒçš„æ½œåœ¨å­ç©ºé—´ä¸­ç¼–ç å¹»è§‰çš„é€šç”¨æ¨¡å¼ï¼Œè¿™è¡¨æ˜å„ç§LVLMå…±äº«â€œé€šç”¨çš„çœŸå®æ–¹å‘â€ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†çœŸå®å¼•å¯¼é¢„å¹²é¢„ï¼ˆTruthPrIntï¼‰ï¼Œè¯¥æ–¹æ³•é¦–å…ˆå­¦ä¹ LVLMè§£ç çš„çœŸå®æ–¹å‘ï¼Œç„¶ååœ¨LVLMè§£ç è¿‡ç¨‹ä¸­è¿›è¡ŒçœŸå®å¼•å¯¼æ¨ç†å¹²é¢„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ComnHalluï¼Œé€šè¿‡æ„å»ºå’Œå¯¹é½å¹»è§‰æ½œåœ¨å­ç©ºé—´ï¼Œå¢å¼ºè·¨LVLMå’Œè·¨æ•°æ®å¹»è§‰æ£€æµ‹çš„è¿ç§»æ€§ã€‚åœ¨å¹¿æ³›çš„å®éªŒè®¾ç½®ï¼ˆåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ã€æµè¡ŒLVLMå’ŒOHåŸºå‡†æµ‹è¯•ï¼‰ä¸­è¯„ä¼°TruthPrIntï¼Œç»“æœè¡¨æ˜TruthPrIntæ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jinhaoduan/TruthPrInt%E3%80%82">https://github.com/jinhaoduan/TruthPrIntã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LVLMçš„å†…éƒ¨çŠ¶æ€æ˜¯è¯„ä¼°ç”Ÿæˆå“åº”çœŸå®æ€§çš„é‡è¦æŒ‡æ ‡ã€‚</li>
<li>LVLMçš„å†…éƒ¨çŠ¶æ€å¯ä»¥ä½œä¸ºâ€œæ¯ä¸ªæ ‡è®°â€çš„å¹»è§‰è¡Œä¸ºçš„ç‰¹å¼‚æ€§æŒ‡æ ‡ã€‚</li>
<li>ä¸åŒçš„LVLMåœ¨æ½œåœ¨å­ç©ºé—´ä¸­ç¼–ç å¹»è§‰çš„é€šç”¨æ¨¡å¼ï¼Œå­˜åœ¨â€œé€šç”¨çš„çœŸå®æ–¹å‘â€ã€‚</li>
<li>æå‡ºäº†çœŸå®å¼•å¯¼é¢„å¹²é¢„ï¼ˆTruthPrIntï¼‰æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ LVLMè§£ç çš„çœŸå®æ–¹å‘ï¼Œåœ¨æ¨ç†æ—¶è¿›è¡Œå¹²é¢„ã€‚</li>
<li>ComnHalluæ–¹æ³•é€šè¿‡æ„å»ºå’Œå¯¹é½å¹»è§‰æ½œåœ¨å­ç©ºé—´ï¼Œå¢å¼ºäº†å¹»è§‰æ£€æµ‹çš„è¿ç§»æ€§å’Œè·¨æ¨¡å‹ã€è·¨æ•°æ®çš„æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>TruthPrIntåœ¨å¹¿æ³›çš„å®éªŒè®¾ç½®ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬åœ¨åŸŸå†…å’ŒåŸŸå¤–çš„åœºæ™¯ã€æµè¡Œçš„LVLMå’ŒOHåŸºå‡†æµ‹è¯•ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1316c5ffe80b249da2f0d262d822d6f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-927962f082af7881e922e9da1531d13b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06bff57bcc8c7c1696a7f8de398137a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0597cd06fb19f5bbfeaa717aa6226a44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9d624b4d4e8ab7f8f335da3d4a1e785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43072cbae91fd9e0b729a876734344a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unlock-the-Power-of-Unlabeled-Data-in-Language-Driving-Model"><a href="#Unlock-the-Power-of-Unlabeled-Data-in-Language-Driving-Model" class="headerlink" title="Unlock the Power of Unlabeled Data in Language Driving Model"></a>Unlock the Power of Unlabeled Data in Language Driving Model</h2><p><strong>Authors:Chaoqun Wang, Jie Yang, Xiaobin Hong, Ruimao Zhang</strong></p>
<p>Recent Vision-based Large Language Models~(VisionLLMs) for autonomous driving have seen rapid advancements. However, such promotion is extremely dependent on large-scale high-quality annotated data, which is costly and labor-intensive. To address this issue, we propose unlocking the value of abundant yet unlabeled data to improve the language-driving model in a semi-supervised learning manner. Specifically, we first introduce a series of template-based prompts to extract scene information, generating questions that create pseudo-answers for the unlabeled data based on a model trained with limited labeled data. Next, we propose a Self-Consistency Refinement method to improve the quality of these pseudo-annotations, which are later used for further training. By utilizing a pre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving Model (LDM) for driving scene question-answering, outperforming previous state-of-the-art methods. Extensive experiments on the DriveLM benchmark show that our approach performs well with just 5% labeled data, achieving competitive performance against models trained with full datasets. In particular, our LDM achieves 44.85% performance with limited labeled data, increasing to 54.27% when using unlabeled data, while models trained with full datasets reach 60.68% on the DriveLM benchmark. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºè§†è§‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVisionLLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå–å¾—äº†å¿«é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œè¿™ç§è¿›æ­¥æåº¦ä¾èµ–äºå¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œè¿™æˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥åŠç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œè§£é”å¤§é‡æœªæ ‡æ³¨æ•°æ®çš„ä»·å€¼ï¼Œä»¥æé«˜é©¾é©¶è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ä¸€ç³»åˆ—åŸºäºæ¨¡æ¿çš„æç¤ºæ¥æå–åœºæ™¯ä¿¡æ¯ï¼Œç”Ÿæˆé—®é¢˜ï¼Œè¿™äº›é—®é¢˜ä¼šæ ¹æ®æœ‰é™æ ‡æ³¨æ•°æ®è®­ç»ƒçš„æ¨¡å‹ä¸ºæœªæ ‡æ³¨æ•°æ®æä¾›ä¼ªç­”æ¡ˆã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæ´½ç»†åŒ–æ–¹æ³•ï¼Œä»¥æé«˜è¿™äº›ä¼ªæ ‡æ³¨çš„è´¨é‡ï¼Œç”¨äºè¿›ä¸€æ­¥çš„è®­ç»ƒã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„VisionLLMï¼ˆä¾‹å¦‚InternVLï¼‰ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„è¯­è¨€é©¾é©¶æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œç”¨äºé©¾é©¶åœºæ™¯é—®ç­”ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚åœ¨DriveLMåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€5%çš„æ ‡æ³¨æ•°æ®å°±èƒ½è¡¨ç°è‰¯å¥½ï¼Œä¸å…¨æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„LDMåœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¾¾åˆ°44.85%çš„æ€§èƒ½ï¼Œåœ¨ä½¿ç”¨æœªæ ‡æ³¨æ•°æ®åæé«˜åˆ°54.27%ï¼Œè€Œå…¨æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨DriveLMåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°60.68%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10586v1">PDF</a> Accepted by ICRA2025</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®å¯¹åŸºäºè§†è§‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVisionLLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†è·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºåˆ©ç”¨ä¸°å¯Œçš„æœªæ ‡æ³¨æ•°æ®ï¼Œä»¥åŠç›‘ç£å­¦ä¹ æ–¹å¼æå‡è¯­è¨€é©¾é©¶æ¨¡å‹çš„ä»·å€¼ã€‚é€šè¿‡åŸºäºæ¨¡æ¿çš„æç¤ºç”Ÿæˆä¼ªç­”æ¡ˆï¼Œå†åˆ©ç”¨è‡ªæˆ‘ä¸€è‡´æ€§ä¼˜åŒ–æ–¹æ³•æé«˜ä¼ªæ ‡æ³¨çš„è´¨é‡ï¼Œå¹¶å°†å…¶ç”¨äºè¿›ä¸€æ­¥è®­ç»ƒã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„VisionLLMï¼ˆå¦‚InternVLï¼‰ï¼Œä¸ºé©¾é©¶åœºæ™¯é—®ç­”æ„å»ºäº†å¼ºå¤§çš„è¯­è¨€é©¾é©¶æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œè¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚åœ¨DriveLMåŸºå‡†æµ‹è¯•ä¸Šï¼Œä»…ä½¿ç”¨5%æ ‡æ³¨æ•°æ®çš„æ–¹æ³•è¡¨ç°è‰¯å¥½ï¼Œä½¿ç”¨æœªæ ‡æ³¨æ•°æ®æ—¶æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶é¢†åŸŸä¸­çš„VisionLLMsé¢ä¸´ä¾èµ–å¤§è§„æ¨¡é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†åˆ©ç”¨ä¸°å¯Œçš„æœªæ ‡æ³¨æ•°æ®ä»¥åŠç›‘ç£å­¦ä¹ æ–¹å¼æå‡è¯­è¨€é©¾é©¶æ¨¡å‹çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åŸºäºæ¨¡æ¿çš„æç¤ºç”Ÿæˆä¼ªç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨è‡ªæˆ‘ä¸€è‡´æ€§ä¼˜åŒ–æ–¹æ³•æé«˜ä¼ªæ ‡æ³¨è´¨é‡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„VisionLLMæ„å»ºäº†å¼ºå¤§çš„LDMç”¨äºé©¾é©¶åœºæ™¯é—®ç­”ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨DriveLMåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä»…ä½¿ç”¨5%æ ‡æ³¨æ•°æ®æ—¶æ€§èƒ½å¯è§‚ã€‚</li>
<li>ä½¿ç”¨æœªæ ‡æ³¨æ•°æ®å¯è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e46eabcd480d26b675d1a3f29162982e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f45ea99187122ade180647e419fec4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f786c432d55ae80759eff7ee8e010d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c085af18222771611c76d3caa7ae76f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb94a99f41ea3e3804b01f24bc306f04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-218de8773ee9ecefd9154adab3c04404.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-Mathematical-Reasoning-in-DeepSeek-Models-A-Comparative-Study-of-Large-Language-Models"><a href="#Unveiling-the-Mathematical-Reasoning-in-DeepSeek-Models-A-Comparative-Study-of-Large-Language-Models" class="headerlink" title="Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative   Study of Large Language Models"></a>Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative   Study of Large Language Models</h2><p><strong>Authors:Afrar Jahin, Arif Hassan Zidan, Yu Bao, Shizhe Liang, Tianming Liu, Wei Zhang</strong></p>
<p>With the rapid evolution of Artificial Intelligence (AI), Large Language Models (LLMs) have reshaped the frontiers of various fields, spanning healthcare, public health, engineering, science, agriculture, education, arts, humanities, and mathematical reasoning. Among these advancements, DeepSeek models have emerged as noteworthy contenders, demonstrating promising capabilities that set them apart from their peers. While previous studies have conducted comparative analyses of LLMs, few have delivered a comprehensive evaluation of mathematical reasoning across a broad spectrum of LLMs. In this work, we aim to bridge this gap by conducting an in-depth comparative study, focusing on the strengths and limitations of DeepSeek models in relation to their leading counterparts. In particular, our study systematically evaluates the mathematical reasoning performance of two DeepSeek models alongside five prominent LLMs across three independent benchmark datasets. The findings reveal several key insights: 1). DeepSeek-R1 consistently achieved the highest accuracy on two of the three datasets, demonstrating strong mathematical reasoning capabilities. 2). The distilled variant of LLMs significantly underperformed compared to its peers, highlighting potential drawbacks in using distillation techniques. 3). In terms of response time, Gemini 2.0 Flash demonstrated the fastest processing speed, outperforming other models in efficiency, which is a crucial factor for real-time applications. Beyond these quantitative assessments, we delve into how architecture, training, and optimization impact LLMsâ€™ mathematical reasoning. Moreover, our study goes beyond mere performance comparison by identifying key areas for future advancements in LLM-driven mathematical reasoning. This research enhances our understanding of LLMsâ€™ mathematical reasoning and lays the groundwork for future advancements </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿…é€Ÿå‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»é‡å¡‘äº†å„ä¸ªé¢†åŸŸçš„è¾¹ç•Œï¼Œæ¶µç›–äº†åŒ»ç–—ä¿å¥ã€å…¬å…±å«ç”Ÿã€å·¥ç¨‹ã€ç§‘å­¦ã€å†œä¸šã€æ•™è‚²ã€è‰ºæœ¯ã€äººæ–‡å’Œæ•°å­¦æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸã€‚åœ¨è¿™äº›è¿›å±•ä¸­ï¼ŒDeepSeekæ¨¡å‹è¡¨ç°å‡ºå¼•äººæ³¨ç›®çš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨åŒè¡Œä¸šä¸­çš„ç«äº‰ä¸­è„±é¢–è€Œå‡ºã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»å¯¹LLMè¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶å¯¹LLMçš„å¹¿æ³›æ•°å­¦æ¨ç†èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡è¿›è¡Œæ·±å…¥çš„å¯¹æ¯”ç ”ç©¶æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œé‡ç‚¹å…³æ³¨DeepSeekæ¨¡å‹ä¸å…¶é¢†å…ˆåŒè¡Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸¤ä¸ªDeepSeekæ¨¡å‹ä¸äº”ç§ä¸»è¦LLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æ€§èƒ½è¡¨ç°ï¼Œæ¶‰åŠä¸‰ä¸ªç‹¬ç«‹çš„åŸºå‡†æ•°æ®é›†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼š1. DeepSeek-R1åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­çš„å‡†ç¡®ç‡å§‹ç»ˆæœ€é«˜ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚2. ä¸å…¶åŒè¡Œç›¸æ¯”ï¼Œè’¸é¦å˜ä½“LLMçš„è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè¿™çªæ˜¾äº†ä½¿ç”¨è’¸é¦æŠ€æœ¯å¯èƒ½å­˜åœ¨çš„æ½œåœ¨ç¼ºé™·ã€‚3. åœ¨å“åº”æ—¶é—´æ–¹é¢ï¼ŒGemini 2.0 Flashçš„å¤„ç†é€Ÿåº¦æœ€å¿«ï¼Œåœ¨æ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¿™å¯¹äºå®æ—¶åº”ç”¨æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„å› ç´ ã€‚é™¤äº†è¿™äº›å®šé‡è¯„ä¼°å¤–ï¼Œæˆ‘ä»¬è¿˜æ·±å…¥æ¢è®¨äº†æ¶æ„ã€è®­ç»ƒå’Œä¼˜åŒ–å¦‚ä½•å½±å“LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…å±€é™äºæ€§èƒ½æ¯”è¾ƒï¼Œè¿˜ç¡®å®šäº†LLMé©±åŠ¨çš„æ•°å­¦æ¨ç†æœªæ¥å‘å±•çš„å…³é”®é¢†åŸŸã€‚è¿™é¡¹ç ”ç©¶å¢å¼ºäº†æˆ‘ä»¬å¯¹äºLLMæ•°å­¦æ¨ç†çš„ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»é‡å¡‘äº†å¤šä¸ªé¢†åŸŸçš„å‰æ²¿ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€å…¬å…±å«ç”Ÿã€å·¥ç¨‹ã€ç§‘å­¦ã€å†œä¸šã€æ•™è‚²ã€è‰ºæœ¯ã€äººæ–‡å’Œæ•°å­¦æ¨ç†ç­‰ã€‚DeepSeekæ¨¡å‹è¡¨ç°å‡ºä»¤äººç©ç›®çš„èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡æ·±å…¥ç ”ç©¶å¯¹æ¯”DeepSeekæ¨¡å‹ä¸å…¶ä»–é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œå¡«è¡¥å½“å‰ç ”ç©¶çš„ç©ºç™½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜DeepSeek-R1åœ¨å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œè’¸é¦å‹LLMè¡¨ç°è¾ƒå·®ï¼ŒGemini 2.0 Flashåœ¨å“åº”æ—¶é—´æ–¹é¢è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†æ¶æ„ã€è®­ç»ƒå’Œä¼˜åŒ–å¯¹LLMæ•°å­¦æ¨ç†çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥LLMé©±åŠ¨çš„æ•°å­¦æ¨ç†å‘å±•çš„å…³é”®é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1åœ¨å¤šæ•°æ•°æ®é›†ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è’¸é¦å‹LLMç›¸è¾ƒäºå…¶ä»–æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œè¿™çªæ˜¾äº†ä½¿ç”¨è’¸é¦æŠ€æœ¯çš„ä¸€äº›æ½œåœ¨ç¼ºç‚¹ã€‚</li>
<li>Gemini 2.0 Flashåœ¨å“åº”æ—¶é—´æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå…·æœ‰è¾ƒé«˜çš„å¤„ç†æ•ˆç‡ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨ã€‚</li>
<li>LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›å—åˆ°å…¶æ¶æ„ã€è®­ç»ƒå’Œä¼˜åŒ–çš„å½±å“ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼ºè°ƒäº†æœªæ¥åœ¨LLMé©±åŠ¨çš„æ•°å­¦æ¨ç†å‘å±•ä¸­éœ€è¦å…³æ³¨çš„å…³é”®é¢†åŸŸã€‚</li>
<li>æœ¬ç ”ç©¶å¢å¼ºäº†æˆ‘ä»¬å¯¹LLMæ•°å­¦æ¨ç†çš„ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9da35c5617ab8e527b3a95bb07a1d4e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2029a26a2c658dde3ddb8c0b6cf5d56a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec02691a416c672424aeb16f36e5d699.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models"><a href="#ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models" class="headerlink" title="ASIDE: Architectural Separation of Instructions and Data in Language   Models"></a>ASIDE: Architectural Separation of Instructions and Data in Language   Models</h2><p><strong>Authors:Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Soroush Tabesh, Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert</strong></p>
<p>Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose an architectural change, ASIDE, that allows the model to clearly separate between instructions and data by using separate embeddings for them. Instead of training the embeddings from scratch, we propose a method to convert an existing model to ASIDE form by using two copies of the original modelâ€™s embeddings layer, and applying an orthogonal rotation to one of them. We demonstrate the effectiveness of our method by showing (1) highly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹åŸºæœ¬çš„å®‰å…¨åŠŸèƒ½ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°ä¼—å¤šæ¶æ„æ”»å‡»çš„å½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…ˆå‰çš„å·¥ä½œå·²ç»ç¡®å®šäº†æŒ‡ä»¤å’Œæ•°æ®ä¹‹é—´ç¼ºä¹å†…åœ¨åˆ†ç¦»æ˜¯æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸçš„æ ¹æœ¬åŸå› ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¶æ„æ›´æ”¹ï¼Œå³ASIDEï¼Œå®ƒå…è®¸æ¨¡å‹é€šè¿‡ä¸ºæŒ‡ä»¤å’Œæ•°æ®ä½¿ç”¨å•ç‹¬çš„åµŒå…¥æ¥æ¸…æ™°åœ°åˆ†ç¦»å®ƒä»¬ã€‚æˆ‘ä»¬å¹¶ä¸æè®®ä»å¤´å¼€å§‹è®­ç»ƒåµŒå…¥ï¼Œè€Œæ˜¯æå‡ºäº†ä¸€ç§å°†ç°æœ‰æ¨¡å‹è½¬æ¢ä¸ºASIDEå½¢å¼çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨åŸå§‹æ¨¡å‹çš„åµŒå…¥å±‚å‰¯æœ¬ï¼Œå¹¶å¯¹å…¶ä¸­ä¸€ä¸ªåº”ç”¨æ­£äº¤æ—‹è½¬ã€‚æˆ‘ä»¬é€šè¿‡å±•ç¤ºï¼ˆ1ï¼‰åœ¨ä¿æŒæ¨¡å‹èƒ½åŠ›ä¸æŸå¤±çš„æƒ…å†µä¸‹ï¼ŒæŒ‡ä»¤-æ•°æ®åˆ†ç¦»å¾—åˆ†å¤§å¹…æé«˜ï¼›ï¼ˆ2ï¼‰å³ä½¿åœ¨æœªè¿›è¡Œä¸“é—¨çš„å®‰å…¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•ä¹Ÿæœ‰ç«äº‰åŠ›ç»“æœæ¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ¨¡å‹è¡¨ç¤ºçš„åˆ†æç ”ç©¶äº†è¯¥æ–¹æ³•çš„å·¥ä½œåŸç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10566v1">PDF</a> ICLR 2025 Workshop on Building Trust in Language Models and   Applications</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ï¼Œä½†å®ƒä»¬ç¼ºä¹åŸºæœ¬çš„å®‰å…¨ç‰¹æ€§ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°å¤šæ¬¡æ¶æ„æ”»å‡»ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¯†åˆ«å‡ºæŒ‡ä»¤å’Œæ•°æ®ä¹‹é—´ç¼ºä¹å†…åœ¨åˆ†ç¦»æ˜¯æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸçš„æ ¹æœ¬åŸå› ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¶æ„æ”¹å˜â€”â€”ASIDEï¼Œå®ƒå…è®¸æ¨¡å‹é€šè¿‡ä¸ºæŒ‡ä»¤å’Œæ•°æ®ä½¿ç”¨å•ç‹¬çš„åµŒå…¥æ¥æ¸…æ™°åœ°åˆ†ç¦»å®ƒä»¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ç°æœ‰æ¨¡å‹è½¬æ¢ä¸ºASIDEå½¢å¼çš„æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒåµŒå…¥ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨åŸå§‹æ¨¡å‹åµŒå…¥å±‚çš„ä¸¤ä¸ªå‰¯æœ¬ï¼Œå¹¶å¯¹å…¶ä¸­ä¸€ä¸ªåº”ç”¨æ­£äº¤æ—‹è½¬ã€‚æˆ‘ä»¬é€šè¿‡ï¼ˆ1ï¼‰æé«˜æŒ‡ä»¤-æ•°æ®åˆ†ç¦»åˆ†æ•°è€Œä¸ä¼šå½±å“æ¨¡å‹èƒ½åŠ›ï¼Œï¼ˆ2ï¼‰åœ¨æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•ä¸Šè·å¾—å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼ˆå³ä½¿åœ¨æ²¡æœ‰ä¸“é—¨çš„å®‰å…¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼‰ï¼Œæ¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ¨¡å‹è¡¨ç¤ºçš„åˆ†æç ”ç©¶äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å·¥ä½œåŸç†ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹åŸºæœ¬çš„å®‰å…¨ç‰¹æ€§ï¼Œå®¹æ˜“å—åˆ°æ¶æ„æ”»å‡»ã€‚</li>
<li>æŒ‡ä»¤å’Œæ•°æ®ä¹‹é—´ç¼ºä¹å†…åœ¨åˆ†ç¦»æ˜¯æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸçš„å…³é”®å› ç´ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ASIDEï¼Œèƒ½å¤Ÿæ¸…æ™°åœ°åˆ†ç¦»æŒ‡ä»¤å’Œæ•°æ®ã€‚</li>
<li>è½¬æ¢ç°æœ‰æ¨¡å‹åˆ°ASIDEå½¢å¼çš„æ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªåµŒå…¥å±‚å‰¯æœ¬å¹¶åº”ç”¨æ­£äº¤æ—‹è½¬ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸æŸå¤±æ¨¡å‹èƒ½åŠ›çš„å‰æä¸‹æé«˜äº†æŒ‡ä»¤ä¸æ•°æ®çš„åˆ†ç¦»æ•ˆæœã€‚</li>
<li>åœ¨æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæ— éœ€ä¸“é—¨çš„å®‰å…¨è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8a22beb539251ecaa7a912a519612106.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-678be6aac11410aae872cb795f9b4b80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad333b018139374a375e1b17410ef877.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74becbf01084ab3c8832ee8d60cdda12.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="KUDA-Keypoints-to-Unify-Dynamics-Learning-and-Visual-Prompting-for-Open-Vocabulary-Robotic-Manipulation"><a href="#KUDA-Keypoints-to-Unify-Dynamics-Learning-and-Visual-Prompting-for-Open-Vocabulary-Robotic-Manipulation" class="headerlink" title="KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for   Open-Vocabulary Robotic Manipulation"></a>KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for   Open-Vocabulary Robotic Manipulation</h2><p><strong>Authors:Zixian Liu, Mingtong Zhang, Yunzhu Li</strong></p>
<p>With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at <a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io/">http://kuda-dynamics.github.io</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€æ”¾å¼è¯æ±‡æœºå™¨äººæ“çºµç³»ç»Ÿåœ¨å¼€å‘æ–¹é¢å·²å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•å¿½è§†äº†ç‰©ä½“åŠ¨åŠ›å­¦çš„é‡è¦æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¤æ‚ã€åŠ¨æ€çš„ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†KUDAï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾å¼è¯æ±‡æ“çºµç³»ç»Ÿï¼Œå®ƒé€šè¿‡å…³é”®ç‚¹æ•´åˆåŠ¨åŠ›å­¦å­¦ä¹ å’Œè§†è§‰æç¤ºï¼Œåˆ©ç”¨VLMså’ŒåŸºäºå­¦ä¹ çš„ç¥ç»åŠ¨åŠ›å­¦æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼ŒåŸºäºå…³é”®ç‚¹çš„ç›®æ ‡è§„æ ¼å¯ä»¥åŒæ—¶è¢«VLMsè§£é‡Šï¼Œå¹¶å¯ä»¥æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºåŸºäºæ¨¡å‹çš„è§„åˆ’çš„æˆæœ¬å‡½æ•°ã€‚ç»™å®šè¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è§‚å¯Ÿï¼ŒKUDAé¦–å…ˆä¸ºRGBå›¾åƒåˆ†é…å…³é”®ç‚¹ï¼Œå¹¶æŸ¥è¯¢VLMä»¥ç”Ÿæˆç›®æ ‡è§„æ ¼ã€‚è¿™äº›æŠ½è±¡çš„åŸºäºå…³é”®ç‚¹çš„è¡¨ç¤ºç„¶åè¢«è½¬æ¢æˆæˆæœ¬å‡½æ•°ï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„åŠ¨åŠ›å­¦æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œä»¥äº§ç”Ÿæœºå™¨äººè½¨è¿¹ã€‚æˆ‘ä»¬åœ¨å„ç§æ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°äº†KUDAï¼ŒåŒ…æ‹¬è·¨ä¸åŒå¯¹è±¡ç±»åˆ«çš„è‡ªç”±å½¢å¼è¯­è¨€æŒ‡ä»¤ã€å¤šå¯¹è±¡äº¤äº’ä»¥åŠå¯å˜å½¢æˆ–é¢—ç²’çŠ¶ç‰©ä½“ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="http://kuda-dynamics.github.ioæŸ¥çœ‹./">http://kuda-dynamics.github.ioæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10546v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io/">http://kuda-dynamics.github.io</a></p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€æ”¾è¯æ±‡æœºå™¨äººæ“çºµç³»ç»Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•å¿½è§†äº†ç‰©ä½“åŠ¨åŠ›å­¦çš„é‡è¦æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¤æ‚ã€åŠ¨æ€ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†KUDAï¼Œä¸€ä¸ªå¼€æ”¾è¯æ±‡çš„æ“çºµç³»ç»Ÿï¼Œå®ƒé€šè¿‡å…³é”®ç‚¹æ•´åˆåŠ¨åŠ›å­¦å­¦ä¹ å’Œè§†è§‰æç¤ºï¼Œåˆ©ç”¨VLMså’Œå­¦ä¹ å‹ç¥ç»åŠ¨åŠ›å­¦æ¨¡å‹ã€‚KUDAçš„å…³é”®è§è§£æ˜¯ï¼ŒåŸºäºå…³é”®ç‚¹çš„ç›®æ ‡è§„æ ¼å¯ä»¥åŒæ—¶è¢«VLMsè§£è¯»ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°è½¬åŒ–ä¸ºæ¨¡å‹åŸºç¡€è§„åˆ’çš„æˆæœ¬å‡½æ•°ã€‚ç»™å®šè¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è§‚å¯Ÿï¼ŒKUDAé¦–å…ˆå°†å…³é”®ç‚¹åˆ†é…ç»™RGBå›¾åƒå¹¶æŸ¥è¯¢VLMä»¥ç”Ÿæˆç›®æ ‡è§„æ ¼ã€‚è¿™äº›æŠ½è±¡çš„åŸºäºå…³é”®ç‚¹çš„è¡¨ç¤ºç„¶åè¢«è½¬åŒ–ä¸ºæˆæœ¬å‡½æ•°ï¼Œä½¿ç”¨å­¦ä¹ åˆ°çš„åŠ¨åŠ›å­¦æ¨¡å‹è¿›è¡Œä¼˜åŒ–ä»¥äº§ç”Ÿæœºå™¨äººè½¨è¿¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¼€æ”¾è¯æ±‡æœºå™¨äººæ“çºµç³»ç»Ÿçš„è¿›æ­¥ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚ã€åŠ¨æ€ä»»åŠ¡ä¸­çš„åº”ç”¨å—é™äºå¯¹ç‰©ä½“åŠ¨åŠ›å­¦çš„å¿½è§†ã€‚</li>
<li>KUDAæ˜¯ä¸€ä¸ªå¼€æ”¾è¯æ±‡çš„æ“çºµç³»ç»Ÿï¼Œé€šè¿‡å…³é”®ç‚¹æ•´åˆåŠ¨åŠ›å­¦å­¦ä¹ å’Œè§†è§‰æç¤ºã€‚</li>
<li>KUDAåˆ©ç”¨VLMså’Œå­¦ä¹ å‹ç¥ç»åŠ¨åŠ›å­¦æ¨¡å‹ï¼ŒåŸºäºå…³é”®ç‚¹çš„ç›®æ ‡è§„æ ¼å¯ä»¥åŒæ—¶è¢«VLMsè§£è¯»å¹¶è½¬åŒ–ä¸ºæˆæœ¬å‡½æ•°ã€‚</li>
<li>KUDAé€šè¿‡è¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è§‚å¯Ÿæ¥åˆ†é…å…³é”®ç‚¹ï¼Œç”Ÿæˆç›®æ ‡è§„æ ¼ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæœºå™¨äººæ“ä½œçš„è½¨è¿¹ã€‚</li>
<li>KUDAåœ¨å¤šç§æ“ä½œä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†éªŒè¯ï¼ŒåŒ…æ‹¬è‡ªç”±å½¢å¼çš„è¯­è¨€æŒ‡ä»¤ã€å¤šå¯¹è±¡äº¤äº’ã€å¯å˜å½¢æˆ–é¢—ç²’çŠ¶ç‰©ä½“ç­‰ã€‚</li>
<li>KUDAé¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š<a target="_blank" rel="noopener" href="http://kuda-dynamics.github.io./">http://kuda-dynamics.github.ioã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-499863b8d20f82f943bab3a89b521449.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b4573e12edb6afe2f10bba7630dc196.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23df30b12939cce86956cceb6381be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15c1dd4a05c357e276a2529e9e1381dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60366fa6c56174ae2210e902523ab307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9aa63d6392bd19b68c69820f6bb8c74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a4cf12967b3bf673e4af01ac9164c6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PiSA-A-Self-Augmented-Data-Engine-and-Training-Strategy-for-3D-Understanding-with-Large-Models"><a href="#PiSA-A-Self-Augmented-Data-Engine-and-Training-Strategy-for-3D-Understanding-with-Large-Models" class="headerlink" title="PiSA: A Self-Augmented Data Engine and Training Strategy for 3D   Understanding with Large Models"></a>PiSA: A Self-Augmented Data Engine and Training Strategy for 3D   Understanding with Large Models</h2><p><strong>Authors:Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, Zhen Li</strong></p>
<p>3D Multimodal Large Language Models (MLLMs) have recently made substantial advancements. However, their potential remains untapped, primarily due to the limited quantity and suboptimal quality of 3D datasets. Current approaches attempt to transfer knowledge from 2D MLLMs to expand 3D instruction data, but still face modality and domain gaps. To this end, we introduce PiSA-Engine (Point-Self-Augmented-Engine), a new framework for generating instruction point-language datasets enriched with 3D spatial semantics. We observe that existing 3D MLLMs offer a comprehensive understanding of point clouds for annotation, while 2D MLLMs excel at cross-validation by providing complementary information. By integrating holistic 2D and 3D insights from off-the-shelf MLLMs, PiSA-Engine enables a continuous cycle of high-quality data generation. We select PointLLM as the baseline and adopt this co-evolution training framework to develop an enhanced 3D MLLM, termed PointLLM-PiSA. Additionally, we identify limitations in previous 3D benchmarks, which often feature coarse language captions and insufficient category diversity, resulting in inaccurate evaluations. To address this gap, we further introduce PiSA-Bench, a comprehensive 3D benchmark covering six key aspects with detailed and diverse labels. Experimental results demonstrate PointLLM-PiSAâ€™s state-of-the-art performance in zero-shot 3D object captioning and generative classification on our PiSA-Bench, achieving significant improvements of 46.45% (+8.33%) and 63.75% (+16.25%), respectively. We will release the code, datasets, and benchmark. </p>
<blockquote>
<p>3Då¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ½œåŠ›å°šæœªè¢«å‘æ˜ï¼Œä¸»è¦æ˜¯å› ä¸º3Dæ•°æ®é›†çš„æ•°é‡æœ‰é™å’Œè´¨é‡ä¸ä½³ã€‚å½“å‰çš„æ–¹æ³•è¯•å›¾ä»2D MLLMsè½¬ç§»çŸ¥è¯†æ¥æ‰©å±•3DæŒ‡ä»¤æ•°æ®ï¼Œä½†ä»é¢ä¸´æ¨¡æ€å’Œé¢†åŸŸå·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†PiSA-Engineï¼ˆPoint-Self-Augmented-Engineï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¯Œå«3Dç©ºé—´è¯­ä¹‰çš„æŒ‡ä»¤ç‚¹è¯­è¨€æ•°æ®é›†çš„æ–°æ¡†æ¶ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„3D MLLMså¯¹ç‚¹äº‘æ³¨é‡Šæœ‰ç€å…¨é¢çš„ç†è§£ï¼Œè€Œ2D MLLMsåˆ™é€šè¿‡æä¾›è¡¥å……ä¿¡æ¯æ“…é•¿äº¤å‰éªŒè¯ã€‚é€šè¿‡æ•´åˆç°æˆçš„MLLMsçš„2Då’Œ3Dæ•´ä½“è§è§£ï¼ŒPiSA-Engineèƒ½å¤Ÿå®ç°é«˜è´¨é‡æ•°æ®ç”Ÿæˆçš„æŒç»­å¾ªç¯ã€‚æˆ‘ä»¬é€‰æ‹©PointLLMä½œä¸ºåŸºçº¿ï¼Œå¹¶é‡‡ç”¨è¿™ç§ååŒè¿›åŒ–è®­ç»ƒæ¡†æ¶æ¥å¼€å‘å¢å¼ºçš„3D MLLMï¼Œç§°ä¸ºPointLLM-PiSAã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†ä»¥å‰3DåŸºå‡†æµ‹è¯•çš„å±€é™æ€§ï¼Œè¿™äº›æµ‹è¯•é€šå¸¸å…·æœ‰ç²—ç³™çš„è¯­è¨€æ ‡é¢˜å’Œä¸è¶³çš„ç±»åˆ«å¤šæ ·æ€§ï¼Œå¯¼è‡´è¯„ä¼°ä¸å‡†ç¡®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†PiSA-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„3DåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å…­ä¸ªå…³é”®æ–¹é¢ï¼Œå…·æœ‰è¯¦ç»†å’Œå¤šæ ·çš„æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPointLLM-PiSAåœ¨æˆ‘ä»¬çš„PiSA-Benchä¸Šå®ç°äº†é›¶æ ·æœ¬3Dç›®æ ‡æè¿°å’Œç”Ÿæˆåˆ†ç±»çš„é¢†å…ˆæ°´å¹³ï¼Œåˆ†åˆ«æé«˜äº†46.45%ï¼ˆ+8.33%ï¼‰å’Œ63.75%ï¼ˆ+16.25%ï¼‰ã€‚æˆ‘ä»¬å°†å‘å¸ƒä»£ç ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10529v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸Šè¿°æ–‡æœ¬ï¼Œæå‡ºä¸€ç§åä¸ºPiSA-Engineçš„æ–°æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¯Œå«ä¸‰ç»´ç©ºé—´è¯­ä¹‰çš„æŒ‡ä»¤ç‚¹è¯­è¨€æ•°æ®é›†ã€‚é€šè¿‡æ•´åˆäºŒç»´å’Œä¸‰ç»´çš„å¤šè§’åº¦è§è§£ï¼Œå®ç°äº†é«˜è´¨é‡æ•°æ®ç”Ÿæˆçš„è¿ç»­å¾ªç¯ã€‚å¹¶æå‡ºPiSA-Benchè¿™ä¸€å…¨é¢ä¸‰ç»´åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¦†ç›–å…­ä¸ªå…³é”®æ–¹é¢å¹¶æ‹¥æœ‰è¯¦ç»†å¤šæ ·çš„æ ‡ç­¾ã€‚å®éªŒç»“æœè¯å®å…¶åœ¨æ–°æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„ä¼˜è¶Šæ€§ã€‚åç»­å°†å‘å¸ƒç›¸å…³ä»£ç ã€æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å¹³å°ã€‚æ¦‚è¿°å®Œæ¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸‰ç»´é¢†åŸŸçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å‘æŒ¥ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹é«˜è´¨é‡çš„ä¸‰ç»´æ•°æ®é›†ã€‚</li>
<li>æå‡ºPiSA-Engineæ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆå¯Œå«ä¸‰ç»´ç©ºé—´è¯­ä¹‰çš„æŒ‡ä»¤ç‚¹è¯­è¨€æ•°æ®é›†ã€‚è¯¥æ¡†æ¶ç»“åˆäº†äºŒç»´å’Œä¸‰ç»´LLMçš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜è´¨é‡æ•°æ®ç”Ÿæˆçš„è¿ç»­å¾ªç¯ã€‚</li>
<li>ä»‹ç»æ–°çš„åŸºå‡†æµ‹è¯•å¹³å°PiSA-Benchï¼Œå…¶æ¶µç›–äº†å…­ä¸ªå…³é”®æ–¹é¢å¹¶å…·å¤‡è¯¦ç»†çš„å¤šæ ·æ€§æ ‡ç­¾ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åŸºäºPointLLMåŸºçº¿æ¨¡å‹å’ŒPiSAæ¡†æ¶å…±åŒæ¼”åŒ–è®­ç»ƒæ³•è®­ç»ƒäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹PointLLM-PiSAã€‚</li>
<li>å®éªŒç»“æœè¯å®PointLLM-PiSAåœ¨é›¶æ ·æœ¬ä¸‰ç»´å¯¹è±¡æè¿°å’Œç”Ÿæˆåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-62162ac8816e570e451993e6a1d1d610.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13c8509bba9640023545bf1b11a1ccbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-973208431521d7ce788693de488fe1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b11ccd64daf4036c716b9eb709354bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c52204bac3adea4141e511e73fe3009.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e90fd20318e22868e8bf7a2df8697864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ad817c0887841167bbe9eac94a1af47.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AudioX-Diffusion-Transformer-for-Anything-to-Audio-Generation"><a href="#AudioX-Diffusion-Transformer-for-Anything-to-Audio-Generation" class="headerlink" title="AudioX: Diffusion Transformer for Anything-to-Audio Generation"></a>AudioX: Diffusion Transformer for Anything-to-Audio Generation</h2><p><strong>Authors:Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo</strong></p>
<p>Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at <a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a> </p>
<blockquote>
<p>éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆåœ¨è®¸å¤šåº”ç”¨ä¸­å·²æˆä¸ºè‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•é¢ä¸´é‡å¤§å±€é™ï¼šå®ƒä»¬åœ¨å­¤ç«‹çš„æƒ…å†µä¸‹è¿è¡Œï¼Œä¸å…·å¤‡è·¨æ¨¡æ€çš„ç»Ÿä¸€èƒ½åŠ›ï¼Œç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œéš¾ä»¥æœ‰æ•ˆåœ°æ•´åˆä¸åŒçš„è¾“å…¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AudioXï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£Transformeræ¨¡å‹ï¼Œç”¨äºä»»ä½•å†…å®¹åˆ°éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆã€‚ä¸åŒäºä¹‹å‰çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ï¼ŒAudioXå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸€èˆ¬éŸ³é¢‘å’ŒéŸ³ä¹ï¼ŒåŒæ—¶æä¾›çµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ä»¥åŠæ— ç¼å¤„ç†å„ç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†é¢‘ã€å›¾åƒã€éŸ³ä¹å’ŒéŸ³é¢‘ã€‚å…¶æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå¤šæ¨¡æ€æ©æ¨¡è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¼šå±è”½è·¨æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶è¿«ä½¿æ¨¡å‹ä»è¢«å±è”½çš„è¾“å…¥ä¸­å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿç¨³å¥å’Œç»Ÿä¸€çš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸¤ä¸ªç»¼åˆæ•°æ®é›†ï¼šåŸºäºVGGSoundæ•°æ®é›†çš„19ä¸‡éŸ³é¢‘å­—å¹•çš„vggsound-capsï¼Œä»¥åŠä»V2Mæ•°æ®é›†ä¸­æ´¾ç”Ÿçš„600ä¸‡éŸ³ä¹å­—å¹•çš„V2M-capsã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAudioXä¸ä»…ä¸æœ€å…ˆè¿›çš„ä¸“ç”¨æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ï¼Œè€Œä¸”åœ¨ä¸€ä¸ªç»Ÿä¸€æ¶æ„ä¸­å¤„ç†å„ç§è¾“å…¥æ¨¡æ€å’Œç”Ÿæˆä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæƒŠäººçš„é€šç”¨æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†å¯åœ¨<a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/%E6%89%BE%E5%88%B0%E3%80%82">https://zeyuet.github.io/AudioX/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10522v1">PDF</a> The code and datasets will be available at   <a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAudioXçš„ç»Ÿä¸€æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ç”¨äºä»»ä½•å†…å®¹åˆ°éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆã€‚å®ƒå…·å¤‡è·¨æ¨¡æ€çš„çµæ´»å¤„ç†èƒ½åŠ›ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„ä¸€èˆ¬éŸ³é¢‘å’ŒéŸ³ä¹ã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºå¤šæ¨¡æ€æ©æ¨¡è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿåœ¨è·¨æ¨¡æ€è¾“å…¥ä¸­æ©æ¨¡è¾“å…¥å¹¶è¿«ä½¿æ¨¡å‹ä»æ©æ¨¡è¾“å…¥ä¸­å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿç¨³å¥çš„ç»Ÿä¸€è·¨æ¨¡æ€è¡¨ç¤ºã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½œè€…è¿˜æ•´ç†äº†ä¸¤ä¸ªç»¼åˆæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒAudioXä¸ä»…åœ¨ä¸“ç”¨æ¨¡å‹ä¸Šè¡¨ç°å“è¶Šï¼Œè¿˜è¡¨ç°å‡ºæƒŠäººçš„è·¨æ¨¡æ€å¤„ç†èƒ½åŠ›å’Œç”Ÿæˆä»»åŠ¡çš„çµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AudioXæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹ï¼Œç”¨äºä»»ä½•å†…å®¹åˆ°éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆã€‚</li>
<li>å®ƒå…·å¤‡ç”Ÿæˆé«˜è´¨é‡ä¸€èˆ¬éŸ³é¢‘å’ŒéŸ³ä¹çš„èƒ½åŠ›ï¼ŒåŒæ—¶æä¾›çµæ´»çš„è·¨æ¨¡æ€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>AudioXçš„å…³é”®åˆ›æ–°åœ¨äºå…¶å¤šæ¨¡æ€æ©æ¨¡è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥äº§ç”Ÿç¨³å¥çš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚</li>
<li>ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½œè€…æ•´ç†äº†ä¸¤ä¸ªç»¼åˆæ•°æ®é›†vggsound-capså’ŒV2M-capsã€‚</li>
<li>AudioXåœ¨ä¸“ç”¨æ¨¡å‹ä¸Šè¡¨ç°å“è¶Šï¼Œå¹¶åœ¨å¤„ç†è·¨æ¨¡æ€è¾“å…¥å’Œç”Ÿæˆä»»åŠ¡æ—¶è¡¨ç°å‡ºçµæ´»æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å’Œæ•°æ®é›†å°†å¯åœ¨[<a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/]%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://zeyuet.github.io/AudioX/]ä¸Šè·å¾—ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7962feb85c1b4f0bd5702ba1579d767b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fda29bb30da1f0a10d892593bcfec63a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30d06630359804ef2a6acab6319ed08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22b662cff6ecbc633c7e71c9e7d7cf3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Conformal-Prediction-Sets-for-Deep-Generative-Models-via-Reduction-to-Conformal-Regression"><a href="#Conformal-Prediction-Sets-for-Deep-Generative-Models-via-Reduction-to-Conformal-Regression" class="headerlink" title="Conformal Prediction Sets for Deep Generative Models via Reduction to   Conformal Regression"></a>Conformal Prediction Sets for Deep Generative Models via Reduction to   Conformal Regression</h2><p><strong>Authors:Hooman Shahrokhi, Devjeet Raj Roy, Yan Yan, Venera Arnaoudova, Janaradhan Rao Doppa</strong></p>
<p>We consider the problem of generating valid and small prediction sets by sampling outputs (e.g., software code and natural language text) from a black-box deep generative model for a given input (e.g., textual prompt). The validity of a prediction set is determined by a user-defined binary admissibility function depending on the target application. For example, requiring at least one program in the set to pass all test cases in code generation application. To address this problem, we develop a simple and effective conformal inference algorithm referred to as Generative Prediction Sets (GPS). Given a set of calibration examples and black-box access to a deep generative model, GPS can generate prediction sets with provable guarantees. The key insight behind GPS is to exploit the inherent structure within the distribution over the minimum number of samples needed to obtain an admissible output to develop a simple conformal regression approach over the minimum number of samples. Experiments on multiple datasets for code and math word problems using different large language models demonstrate the efficacy of GPS over state-of-the-art methods. </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘é€šè¿‡ä»ä¸€ä¸ªé»‘ç®±æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­å¯¹ç»™å®šè¾“å…¥ï¼ˆå¦‚æ–‡æœ¬æç¤ºï¼‰è¿›è¡Œé‡‡æ ·è¾“å‡ºï¼ˆä¾‹å¦‚è½¯ä»¶ä»£ç å’Œè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼‰ï¼Œæ¥ç”Ÿæˆæœ‰æ•ˆä¸”å°çš„é¢„æµ‹é›†çš„é—®é¢˜ã€‚é¢„æµ‹é›†çš„æœ‰æ•ˆæ€§æ˜¯ç”±ç”¨æˆ·å®šä¹‰çš„äºŒè¿›åˆ¶æ¥çº³å‡½æ•°å†³å®šçš„ï¼Œè¿™å–å†³äºç›®æ ‡åº”ç”¨ç¨‹åºã€‚ä¾‹å¦‚ï¼Œåœ¨ä»£ç ç”Ÿæˆåº”ç”¨ç¨‹åºä¸­ï¼Œè¦æ±‚é›†åˆä¸­è‡³å°‘æœ‰ä¸€ä¸ªç¨‹åºé€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åˆè§„æ¨ç†ç®—æ³•ï¼Œç§°ä¸ºç”Ÿæˆé¢„æµ‹é›†ï¼ˆGPSï¼‰ã€‚ç»™å®šä¸€ç»„æ ¡å‡†ç¤ºä¾‹å’Œå¯¹æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„é»‘ç®±è®¿é—®æƒé™ï¼ŒGPSå¯ä»¥ç”Ÿæˆå…·æœ‰å¯è¯æ˜ä¿è¯çš„é¢„æµ‹é›†ã€‚GPSèƒŒåçš„å…³é”®è§è§£æ˜¯åˆ©ç”¨è·å¾—å¯æ¥çº³è¾“å‡ºæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°åˆ†å¸ƒçš„å†…åœ¨ç»“æ„ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¼€å‘ä¸€ç§ç®€å•çš„åˆè§„å›å½’æ–¹æ³•ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„ä»£ç å’Œæ•°å­¦æ–‡å­—é—®é¢˜çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒGPSæ›´ä¸ºæœ‰æ•ˆã€‚ä½¿ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10512v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç»™å®šè¾“å…¥ï¼ˆå¦‚æ–‡æœ¬æç¤ºï¼‰çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»é»‘ç®±æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­é‡‡æ ·è¾“å‡ºï¼ˆå¦‚è½¯ä»¶ä»£ç å’Œè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼‰æ¥ç”Ÿæˆæœ‰æ•ˆä¸”å°çš„é¢„æµ‹é›†æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºç½®ä¿¡æ¨æ–­çš„æ–¹æ³•ï¼Œç§°ä¸ºç”Ÿæˆé¢„æµ‹é›†ï¼ˆGPSï¼‰ã€‚ç»™å®šä¸€ç»„æ ¡å‡†ç¤ºä¾‹å’Œå¯¹æ·±åº¦ç”Ÿæˆæ¨¡å‹çš„é»‘ç®±è®¿é—®æƒé™ï¼ŒGPSèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¯éªŒè¯ä¿è¯çš„é¢„æµ‹é›†ã€‚å…³é”®åœ¨äºåˆ©ç”¨ç”Ÿæˆå¯æ¥å—è¾“å‡ºæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°åˆ†å¸ƒçš„å†…åœ¨ç»“æ„ï¼Œä¸ºæœ€å°æ ·æœ¬æ•°å¼€å‘ä¸€ç§ç®€å•çš„ç½®ä¿¡å›å½’æ–¹æ³•ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGPSåœ¨è§£å†³ä»£ç å’Œæ•°å­¦é—®é¢˜çš„åº”ç”¨ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥é—®é¢˜èšç„¦äºä»é»‘ç®±æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­ç”Ÿæˆæœ‰æ•ˆä¸”å°çš„é¢„æµ‹é›†ã€‚</li>
<li>é€šè¿‡ä¸€ä¸ªç”¨æˆ·å®šä¹‰çš„äºŒå…ƒæ¥çº³å‡½æ•°æ¥åˆ¤æ–­é¢„æµ‹é›†çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºç½®ä¿¡æ¨æ–­çš„æ–¹æ³•â€”â€”ç”Ÿæˆé¢„æµ‹é›†ï¼ˆGPSï¼‰ã€‚</li>
<li>GPSåˆ©ç”¨ç”Ÿæˆå¯æ¥å—è¾“å‡ºæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°åˆ†å¸ƒçš„å†…åœ¨ç»“æ„ã€‚</li>
<li>GPSé€šè¿‡ä¸€ç§ç®€å•çš„ç½®ä¿¡å›å½’æ–¹æ³•ä¸ºæœ€å°æ ·æœ¬æ•°æä¾›å¯éªŒè¯ä¿è¯ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºGPSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-005be3cd5803209008d938d577b86ea2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb1419ccc4a18ed8e2597678fcc0e295.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-588f080d3ae0be949dbfe29042da6638.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TokenCarve-Information-Preserving-Visual-Token-Compression-in-Multimodal-Large-Language-Models"><a href="#TokenCarve-Information-Preserving-Visual-Token-Compression-in-Multimodal-Large-Language-Models" class="headerlink" title="TokenCarve: Information-Preserving Visual Token Compression in   Multimodal Large Language Models"></a>TokenCarve: Information-Preserving Visual Token Compression in   Multimodal Large Language Models</h2><p><strong>Authors:Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen</strong></p>
<p>Multimodal Large Language Models (MLLMs) are becoming increasingly popular, while the high computational cost associated with multimodal data input, particularly from visual tokens, poses a significant challenge. Existing training-based token compression methods improve inference efficiency but require costly retraining, while training-free methods struggle to maintain performance when aggressively reducing token counts. In this study, we reveal that the performance degradation of MLLM closely correlates with the accelerated loss of information in the attention output matrix. This insight introduces a novel information-preserving perspective, making it possible to maintain performance even under extreme token compression. Based on this finding, we propose TokenCarve, a training-free, plug-and-play, two-stage token compression framework. The first stage employs an Information-Preservation-Guided Selection (IPGS) strategy to prune low-information tokens, while the second stage further leverages IPGS to guide token merging, minimizing information loss. Extensive experiments on 11 datasets and 2 model variants demonstrate the effectiveness of TokenCarve. It can even reduce the number of visual tokens to 22.2% of the original count, achieving a 1.23x speedup in inference, a 64% reduction in KV cache storage, and only a 1.54% drop in accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ShawnTan86/TokenCarve">https://github.com/ShawnTan86/TokenCarve</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œç„¶è€Œä¸å¤šæ¨¡æ€æ•°æ®è¾“å…¥ç›¸å…³çš„é«˜è®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªè§†è§‰æ ‡è®°çš„è¾“å…¥ï¼Œæ„æˆäº†ä¸€å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºè®­ç»ƒçš„æ ‡è®°å‹ç¼©æ–¹æ³•æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œä½†éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒï¼Œè€Œæ— è®­ç»ƒçš„æ–¹æ³•åœ¨å¤§åŠ›å‡å°‘æ ‡è®°æ•°é‡æ—¶å´éš¾ä»¥ç»´æŒæ€§èƒ½ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ä¸æ³¨æ„åŠ›è¾“å‡ºçŸ©é˜µä¸­ä¿¡æ¯åŠ é€Ÿä¸¢å¤±å¯†åˆ‡ç›¸å…³ã€‚è¿™ä¸€å‘ç°å¼•å…¥äº†ä¸€ç§æ–°çš„ä¿¡æ¯ä¿ç•™è§†è§’ï¼Œå³ä½¿åœ¨æç«¯çš„æ ‡è®°å‹ç¼©ä¸‹ä¹Ÿèƒ½ç»´æŒæ€§èƒ½ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†TokenCarveï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„ä¸¤é˜¶æ®µæ ‡è®°å‹ç¼©æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨ä¿¡æ¯ä¿ç•™å¼•å¯¼é€‰æ‹©ï¼ˆIPGSï¼‰ç­–ç•¥æ¥åˆ é™¤ä½ä¿¡æ¯æ ‡è®°ï¼Œç¬¬äºŒé˜¶æ®µè¿›ä¸€æ­¥åˆ©ç”¨IPGSæ¥æŒ‡å¯¼æ ‡è®°åˆå¹¶ï¼Œå°½é‡å‡å°‘ä¿¡æ¯æŸå¤±ã€‚åœ¨11ä¸ªæ•°æ®é›†å’Œ2ä¸ªæ¨¡å‹å˜ä½“ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†TokenCarveçš„æœ‰æ•ˆæ€§ã€‚å®ƒç”šè‡³å¯ä»¥å°†è§†è§‰æ ‡è®°çš„æ•°é‡å‡å°‘åˆ°åŸå§‹æ•°é‡çš„22.2%ï¼Œå®ç°æ¨ç†é€Ÿåº¦æé«˜1.23å€ï¼ŒKVç¼“å­˜å­˜å‚¨å‡å°‘64%ï¼Œå‡†ç¡®ç‡ä»…ä¸‹é™1.54%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShawnTan86/TokenCarve%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ShawnTan86/TokenCarveä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10501v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´é«˜è®¡ç®—æˆæœ¬æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰ç¬¦å·è¾“å…¥æ—¶ã€‚ç°æœ‰åŸºäºè®­ç»ƒçš„ç¬¦å·å‹ç¼©æ–¹æ³•èƒ½æé«˜æ¨ç†æ•ˆç‡ï¼Œä½†éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒæˆæœ¬ï¼Œè€Œæ— éœ€è®­ç»ƒçš„å‹ç¼©æ–¹æ³•åˆ™åœ¨å¤§å¹…å‡å°‘ç¬¦å·è®¡æ•°æ—¶éš¾ä»¥ä¿æŒæ€§èƒ½ã€‚æœ¬ç ”ç©¶å‘ç°MLLMæ€§èƒ½ä¸‹é™ä¸æ³¨æ„åŠ›è¾“å‡ºçŸ©é˜µä¸­ä¿¡æ¯çš„åŠ é€Ÿä¸§å¤±å¯†åˆ‡ç›¸å…³ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¿¡æ¯ä¿ç•™è§†è§’ï¼Œå¹¶åŸºäºæ­¤è§è§£æå‡ºäº†æ— éœ€è®­ç»ƒçš„TokenCarveæ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µç¬¦å·å‹ç¼©ç­–ç•¥ï¼Œç¬¬ä¸€é˜¶æ®µé‡‡ç”¨ä¿¡æ¯ä¿ç•™å¼•å¯¼é€‰æ‹©ç­–ç•¥æ¥åˆ é™¤ä½ä¿¡æ¯ç¬¦å·ï¼Œç¬¬äºŒé˜¶æ®µè¿›ä¸€æ­¥åˆ©ç”¨è¯¥ç­–ç•¥å¼•å¯¼ç¬¦å·åˆå¹¶ï¼Œå°½é‡å‡å°‘ä¿¡æ¯æŸå¤±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹å˜ä½“ä¸Šçš„å®éªŒè¡¨æ˜TokenCarveçš„æœ‰æ•ˆæ€§ã€‚å®ƒå¯ä»¥å°†è§†è§‰ç¬¦å·çš„æ•°é‡å‡å°‘åˆ°åŸå§‹æ•°é‡çš„22.2%ï¼Œæé«˜æ¨ç†é€Ÿåº¦1.23å€ï¼Œé™ä½KVç¼“å­˜å­˜å‚¨64%ï¼Œä¸”ä»…æŸå¤±1.54%çš„ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†è§†è§‰ç¬¦å·è¾“å…¥æ—¶é¢ä¸´é«˜è®¡ç®—æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç¬¦å·å‹ç¼©æ–¹æ³•éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒæˆæœ¬æˆ–éš¾ä»¥åœ¨å‡å°‘ç¬¦å·è®¡æ•°æ—¶ä¿æŒæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°MLLMæ€§èƒ½ä¸‹é™ä¸æ³¨æ„åŠ›è¾“å‡ºçŸ©é˜µä¸­ä¿¡æ¯çš„åŠ é€Ÿä¸§å¤±æœ‰å…³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¿¡æ¯ä¿ç•™è§†è§’æ¥ä¿æŒæ€§èƒ½ï¼Œå³ä½¿åœ¨æç«¯çš„ç¬¦å·å‹ç¼©ä¸‹ã€‚</li>
<li>ä»‹ç»äº†TokenCarveæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ã€å³æ’å³ç”¨çš„ä¸¤é˜¶æ®µç¬¦å·å‹ç¼©æ¡†æ¶ã€‚</li>
<li>TokenCarveé€šè¿‡é‡‡ç”¨ä¿¡æ¯ä¿ç•™å¼•å¯¼é€‰æ‹©ç­–ç•¥æ¥åˆ é™¤å’Œåˆå¹¶ä½ä¿¡æ¯ç¬¦å·ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65293ab3fd83f71893d41b2ee57ffa00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8364eaa66f565daccb158719e74d45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd7b91b5f863e9846d41c68d98d3995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e68234c75816647b2af5070e8c52ec61.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MMLU-ProX-A-Multilingual-Benchmark-for-Advanced-Large-Language-Model-Evaluation"><a href="#MMLU-ProX-A-Multilingual-Benchmark-for-Advanced-Large-Language-Model-Evaluation" class="headerlink" title="MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model   Evaluation"></a>MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model   Evaluation</h2><p><strong>Authors:Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Yun Xing, Junjue Wang, Huitao Li, Xin Li, Kunyu Yu, Nan Liu, Qingyu Chen, Douglas Teodoro, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, Irene Li</strong></p>
<p>Traditional benchmarks struggle to evaluate increasingly sophisticated language models in multilingual and culturally diverse contexts. To address this gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark covering 13 typologically diverse languages with approximately 11,829 questions per language. Building on the challenging reasoning-focused design of MMLU-Pro, our framework employs a semi-automatic translation process: translations generated by state-of-the-art large language models (LLMs) are rigorously evaluated by expert annotators to ensure conceptual accuracy, terminological consistency, and cultural relevance. We comprehensively evaluate 25 state-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot prompting strategies, analyzing their performance across linguistic and cultural boundaries. Our experiments reveal consistent performance degradation from high-resource languages to lower-resource ones, with the best models achieving over 70% accuracy on English but dropping to around 40% for languages like Swahili, highlighting persistent gaps in multilingual capabilities despite recent advances. MMLU-ProX is an ongoing project; we are expanding our benchmark by incorporating additional languages and evaluating more language models to provide a more comprehensive assessment of multilingual capabilities. </p>
<blockquote>
<p>ä¼ ç»ŸåŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°æ—¥ç›Šå¤æ‚çš„å¤šè¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯çš„è¯­è¨€æ¨¡å‹æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMLU-ProXï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–13ç§è¯­è¨€å½¢æ€å¤šæ ·çš„è¯­è¨€ï¼Œæ¯ç§è¯­è¨€å¤§çº¦æœ‰11,829ä¸ªé—®é¢˜ã€‚åŸºäºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†å¯¼å‘è®¾è®¡MMLU-Proï¼Œæˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨åŠè‡ªåŠ¨ç¿»è¯‘è¿‡ç¨‹ï¼šç”±æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ç¿»è¯‘ä¼šç»è¿‡ä¸“å®¶æ³¨é‡Šè€…çš„ä¸¥æ ¼è¯„ä¼°ï¼Œä»¥ç¡®ä¿æ¦‚å¿µå‡†ç¡®æ€§ã€æœ¯è¯­ä¸€è‡´æ€§å’Œæ–‡åŒ–ç›¸å…³æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€æ–°çš„LLMå¯¹æ€§èƒ½è¿›è¡Œç»¼åˆè¯„ä¼°é‡‡ç”¨å…·æœ‰äº”æ¬¡è¿ç»­æ€§æ€è€ƒçš„åˆæ­¥æ€è€ƒå’Œé›¶å°„å¼•å‘ç­–ç•¥æ¥æ¨è¿›åˆ†æå…¶åœ¨è¯­è¨€å’Œè·¨æ–‡åŒ–é¢†åŸŸçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†ä»èµ„æºä¸°å¯Œå‹è¯­è¨€åˆ°èµ„æºç¨€ç¼ºå‹è¯­è¨€çš„æ€§èƒ½æŒç»­ä¸‹é™ï¼Œæœ€ä½³æ¨¡å‹åœ¨è‹±è¯­ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡70%ï¼Œä½†åœ¨å¦‚æ–¯ç“¦å¸Œé‡Œè¯­ç­‰è¯­è¨€ä¸Šçš„å‡†ç¡®ç‡é™è‡³çº¦40%ï¼Œè¿™çªæ˜¾äº†å°½ç®¡è¿‘æœŸæœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨å¤šè¯­è¨€èƒ½åŠ›æ–¹é¢ä»å­˜åœ¨æŒç»­çš„å·®è·ã€‚MMLU-ProXæ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œä¸­çš„é¡¹ç›®ï¼›æˆ‘ä»¬æ­£åœ¨é€šè¿‡å¢åŠ è¯­è¨€å’Œè¯„ä¼°æ›´å¤šçš„è¯­è¨€æ¨¡å‹æ¥æ‰©å±•æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥æä¾›æ›´å…¨é¢çš„å¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10497v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MMLU-ProXæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šè¯­è¨€æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œè¦†ç›–äº†åŒ…æ‹¬ä½èµ„æºè¯­è¨€åœ¨å†…çš„13ç§ä¸åŒè¯­è¨€ã€‚è¯¥ç ”ç©¶ä½¿ç”¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠè‡ªåŠ¨ç¿»è¯‘è¿‡ç¨‹ï¼Œå¹¶ç»è¿‡ä¸“å®¶è¯„ä¼°ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§ã€ä¸€è‡´æ€§å’Œæ–‡åŒ–ç›¸å…³æ€§ã€‚è¯¥ç ”ç©¶åˆ†æäº†å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è¯­è¨€æ¨¡å‹æ€§èƒ½ï¼Œå‘ç°é«˜èµ„æºè¯­è¨€åˆ°ä½èµ„æºè¯­è¨€çš„æ€§èƒ½é€€åŒ–é—®é¢˜ä»ç„¶æ˜¾è‘—å­˜åœ¨ã€‚MMLU-ProXæ˜¯ä¸€ä¸ªæŒç»­å‘å±•çš„é¡¹ç›®ï¼Œè®¡åˆ’åœ¨æœªæ¥ç»§ç»­æ‰©å±•è¯­è¨€å’Œæ¨¡å‹è¯„ä¼°èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMLU-ProXæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šè¯­è¨€æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œè¦†ç›–äº†å¤šè¾¾13ç§è¯­è¨€ï¼Œç›®çš„æ˜¯è¯„ä¼°æ—¥æ¸ç²¾ç»†çš„è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œè·¨æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
<li>MMLU-ProXé€šè¿‡å¼•å…¥åŠè‡ªåŠ¨ç¿»è¯‘è¿‡ç¨‹å¹¶é‡‡ç”¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¿»è¯‘ç”Ÿæˆï¼Œå†ç»è¿‡ä¸“å®¶è¯„ä¼°æ¥ç¡®ä¿ç¿»è¯‘çš„è´¨é‡ä¸å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨ä¸åŒçš„æµ‹è¯•ç­–ç•¥è¯„ä¼°äº†è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¦‚åŸºäºæ¨ç†çš„CoTç­–ç•¥å’Œé›¶æ ·æœ¬æç¤ºç­–ç•¥ã€‚è¿™äº›ç­–ç•¥å¯¹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œè·¨æ–‡åŒ–ç¯å¢ƒä¸‹çš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œä»é«˜èµ„æºè¯­è¨€åˆ°ä½èµ„æºè¯­è¨€çš„æ€§èƒ½é€€åŒ–é—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œå³ä½¿åœ¨è‹±è¯­ç­‰è¯­è¨€ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹åœ¨æ–¯ç“¦å¸Œé‡Œç­‰è¯­è¨€ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰å¤§çº¦40%ã€‚è¿™å‡¸æ˜¾äº†å¤šè¯­è¨€èƒ½åŠ›æ–¹é¢çš„æŒç»­å·®è·ã€‚</li>
<li>MMLU-ProXé¡¹ç›®ä»åœ¨å‘å±•ä¸­ï¼Œè®¡åˆ’é€šè¿‡å¢åŠ æ›´å¤šè¯­è¨€å’Œè¯„ä¼°æ›´å¤šè¯­è¨€æ¨¡å‹æ¥æä¾›æ›´å…¨é¢çš„å¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°ã€‚è¿™è¡¨æ˜è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œè¿›æ­¥ä»åœ¨æŒç»­è¿›è¡Œä¸­ã€‚</li>
<li>æ­¤ç ”ç©¶çš„é‡ç‚¹åœ¨äºå±•ç¤ºå’Œåˆ†æè¯­è¨€æ¨¡å‹åœ¨å¤šç§ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œå¼ºè°ƒå¤šè¯­è¨€èƒ½åŠ›çš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚è¿™ä¸ä»…å¯¹äºç ”ç©¶è€…å’Œå¼€å‘è€…å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä¹Ÿå¯¹å®é™…åº”ç”¨ä¸­çš„å¤šè¯­è¨€å¤„ç†æå‡ºäº†æŒ‘æˆ˜å’Œæœºé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6f3e05495bab0734305b38af9b06bcbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5bab2a8c981b137c281593d9df43ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112ea37b9e998b9775c811b423a2a015.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Whisper-Speaker-Identification-Leveraging-Pre-Trained-Multilingual-Transformers-for-Robust-Speaker-Embeddings"><a href="#Whisper-Speaker-Identification-Leveraging-Pre-Trained-Multilingual-Transformers-for-Robust-Speaker-Embeddings" class="headerlink" title="Whisper Speaker Identification: Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings"></a>Whisper Speaker Identification: Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings</h2><p><strong>Authors:Jakaria Islam Emon, Md Abu Salek, Kazi Tamanna Alam</strong></p>
<p>Speaker identification in multilingual settings presents unique challenges, particularly when conventional models are predominantly trained on English data. In this paper, we propose WSI (Whisper Speaker Identification), a framework that repurposes the encoder of the Whisper automatic speech recognition model pre trained on extensive multilingual data to generate robust speaker embeddings via a joint loss optimization strategy that leverages online hard triplet mining and self supervised Normalized Temperature-scaled Cross Entropy loss. By capitalizing on Whisper language-agnostic acoustic representations, our approach effectively distinguishes speakers across diverse languages and recording conditions. Extensive evaluations on multiple corpora, including VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish, Chinese, and Japanese), and Voxconverse (English), demonstrate that WSI consistently outperforms state-of-the-art baselines, namely Pyannote Embedding, ECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC scores. These results validate our hypothesis that a multilingual pre-trained ASR encoder, combined with joint loss optimization, substantially improves speaker identification performance in non-English languages. </p>
<blockquote>
<p>åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¿›è¡Œè¯´è¯äººè¯†åˆ«é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¼ ç»Ÿçš„æ¨¡å‹ä¸»è¦åŸºäºè‹±è¯­æ•°æ®è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WSIï¼ˆWhisperè¯´è¯äººè¯†åˆ«ï¼‰æ¡†æ¶ï¼Œå®ƒé‡æ–°åˆ©ç”¨äº†åœ¨å¤§é‡å¤šè¯­è¨€æ•°æ®ä¸Šé¢„è®­ç»ƒçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¼–ç å™¨ï¼Œé€šè¿‡è”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ç”Ÿæˆç¨³å¥çš„è¯´è¯äººåµŒå…¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨åœ¨çº¿ç¡¬ä¸‰å…ƒç»„æŒ–æ˜å’Œè‡ªæˆ‘ç›‘ç£çš„å½’ä¸€åŒ–æ¸©åº¦å°ºåº¦äº¤å‰ç†µæŸå¤±ã€‚é€šè¿‡åˆ©ç”¨Whisperè¯­è¨€æ— å…³çš„å£°å­¦è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°åœ¨ä¸åŒçš„è¯­è¨€å’Œå½•éŸ³æ¡ä»¶ä¸‹åŒºåˆ†è¯´è¯äººã€‚åœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬VoxTubeï¼ˆå¤šè¯­è¨€ï¼‰ã€JVSï¼ˆæ—¥è¯­ï¼‰ã€CallHomeï¼ˆå¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­ï¼‰å’ŒVoxconverseï¼ˆè‹±è¯­ï¼‰ï¼Œè¯æ˜WSIåœ¨å¹³ç­‰è¯¯å·®ç‡æ›´ä½ã€AUCåˆ†æ•°æ›´é«˜çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºPyannote Embeddingã€ECAPA TDNNå’ŒXvectorç­‰æœ€æ–°åŸºçº¿ã€‚è¿™äº›ç»“æœéªŒè¯äº†æˆ‘ä»¬å‡è®¾çš„æ­£ç¡®æ€§ï¼Œå³å¤šè¯­è¨€é¢„è®­ç»ƒçš„ASRç¼–ç å™¨ä¸è”åˆæŸå¤±ä¼˜åŒ–ç›¸ç»“åˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜éè‹±è¯­è¯­è¨€çš„è¯´è¯äººè¯†åˆ«æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10446v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†WSIï¼ˆWhisperè¯­éŸ³è¯†åˆ«æ¨¡å‹ä¸­çš„è¯´è¯äººè¯†åˆ«ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒäºä¸°å¯Œå¤šè¯­ç§æ•°æ®çš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¼–ç å™¨ç”Ÿæˆç¨³å¥çš„è¯´è¯äººåµŒå…¥ã€‚é€šè¿‡é‡‡ç”¨åœ¨çº¿ç¡¬ä¸‰å…ƒç»„æŒ–æ˜å’Œè‡ªç›‘ç£çš„å½’ä¸€åŒ–æ¸©åº¦ç¼©æ”¾äº¤å‰ç†µæŸå¤±çš„è”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒè¯­è¨€å’Œå½•éŸ³æ¡ä»¶ä¸‹çš„è¯´è¯äººã€‚åœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒWSIåœ¨å¹³ç­‰é”™è¯¯ç‡å’ŒAUCå¾—åˆ†æ–¹é¢å‡ä¼˜äºPyannoteåµŒå…¥ã€ECAPA TDNNå’ŒXvectorç­‰æœ€æ–°åŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯äº†å¤šè¯­ç§é¢„è®­ç»ƒASRç¼–ç å™¨ç»“åˆè”åˆæŸå¤±ä¼˜åŒ–åœ¨éè‹±è¯­è¯­ç§è¯´è¯äººè¯†åˆ«æ–¹é¢çš„æ˜¾è‘—æå‡æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†WSIæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šè¯­ç§ç¯å¢ƒä¸‹çš„è¯´è¯äººè¯†åˆ«æŒ‘æˆ˜ã€‚</li>
<li>WSIåˆ©ç”¨é¢„è®­ç»ƒäºå¤šè¯­ç§æ•°æ®çš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¼–ç å™¨ç”Ÿæˆè¯´è¯äººåµŒå…¥ã€‚</li>
<li>é€šè¿‡è”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ï¼ŒWSIèƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒè¯­è¨€å’Œå½•éŸ³æ¡ä»¶ä¸‹çš„è¯´è¯äººã€‚</li>
<li>å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒWSIåœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°åŸºçº¿æ¨¡å‹ã€‚</li>
<li>WSIåœ¨å¹³ç­‰é”™è¯¯ç‡å’ŒAUCå¾—åˆ†æ–¹é¢å‡è¡¨ç°å‡ºè‰²ã€‚</li>
<li>éªŒè¯äº†åœ¨éè‹±è¯­è¯­ç§è¯´è¯äººè¯†åˆ«æ–¹é¢ï¼Œå¤šè¯­ç§é¢„è®­ç»ƒASRç¼–ç å™¨ç»“åˆè”åˆæŸå¤±ä¼˜åŒ–å¯æ˜¾è‘—æå‡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0bdb47d4309b81002513c45e2941c27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5134bc6b613250bc8e8cec2b7b35f596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec54433401449c1327c5b908bb83f91f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47edbaddccb1b3c2956900ecb1d2eac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-215ea2545d9f8c9a6d5a0b39e6595fa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd5f3c30e7e27880af551dd55aa3719.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models"><a href="#4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models" class="headerlink" title="4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models"></a>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models</h2><p><strong>Authors:Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister</strong></p>
<p>Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries. </p>
<blockquote>
<p>å­¦ä¹ 4Dè¯­è¨€å­—æ®µä»¥å®ç°åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿå’Œå¼€æ”¾ç»“æŸè¯­è¨€æŸ¥è¯¢å¯¹äºè®¸å¤šå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶LangSplatæˆåŠŸåœ°å°†CLIPç‰¹å¾è½¬åŒ–ä¸º3Dé«˜æ–¯è¡¨ç¤ºï¼Œå®ç°äº†åœ¨3Dé™æ€åœºæ™¯ä¸­çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œä½†å®ƒç¼ºä¹å¤„ç†åŠ¨æ€4Då­—æ®µçš„èƒ½åŠ›ï¼Œå› ä¸ºCLIPæ˜¯ä¸ºé™æ€å›¾åƒæ–‡æœ¬ä»»åŠ¡è®¾è®¡çš„ï¼Œæ— æ³•æ•è·è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚ç°å®ä¸–ç•Œçš„ç¯å¢ƒæœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„ï¼Œç‰©ä½“è¯­ä¹‰ä¼šéšæ—¶é—´æ¼”å˜ã€‚è¦å»ºç«‹ç²¾ç¡®çš„4Dè¯­è¨€å­—æ®µï¼Œå¿…é¡»è·å¾—åƒç´ å¯¹é½çš„ã€é¢å‘å¯¹è±¡çš„è§†é¢‘ç‰¹å¾ï¼Œè€Œå½“å‰è§†è§‰æ¨¡å‹å¾ˆéš¾åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†4D LangSplatï¼Œå®ƒå­¦ä¹ 4Dè¯­è¨€å­—æ®µä»¥é«˜æ•ˆåœ°å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ— å…³æˆ–æ—¶é—´æ•æ„Ÿçš„å¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚4D LangSplatç»•è¿‡ä»è§†è§‰ç‰¹å¾å­¦ä¹ è¯­è¨€å­—æ®µï¼Œè€Œæ˜¯ç›´æ¥ä»é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆçš„é¢å‘å¯¹è±¡çš„è§†é¢‘å­—å¹•ä¸­å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€é¢å‘å¯¹è±¡çš„è§†é¢‘æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œå¼•å¯¼MLLMä¸ºè§†é¢‘ä¸­çš„å¯¹è±¡ç”Ÿæˆè¯¦ç»†ã€æ—¶é—´ä¸€è‡´ã€é«˜è´¨é‡çš„å­—å¹•ã€‚è¿™äº›å­—å¹•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç¼–ç ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡å¥å­åµŒå…¥ï¼Œç„¶åä½œä¸ºåƒç´ å¯¹é½çš„ã€é¢å‘å¯¹è±¡çš„ç‰¹å¾ç›‘ç£ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´å®ç°å¼€æ”¾å¼æ–‡æœ¬æŸ¥è¯¢ã€‚æˆ‘ä»¬è®¤è¯†åˆ°4Dåœºæ™¯ä¸­å¯¹è±¡çš„çŠ¶æ€è½¬æ¢æ˜¯å¹³æ»‘çš„ï¼Œå› æ­¤è¿›ä¸€æ­¥æå‡ºäº†çŠ¶æ€å¯å˜å½¢ç½‘ç»œæ¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿè¿™äº›éšæ—¶é—´å˜åŒ–çš„è¿ç»­å˜åŒ–ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¯æ˜ï¼Œ4D LangSplatå¯¹äºæ—¶é—´æ•æ„Ÿå’Œæ—¶é—´æ— å…³çš„å¼€æ”¾å¼æŸ¥è¯¢éƒ½è¾¾åˆ°äº†ç²¾ç¡®å’Œé«˜æ•ˆçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10437v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://4d-langsplat.github.io/">https://4d-langsplat.github.io</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>å­¦ä¹ 4Dè¯­è¨€å­—æ®µä»¥å¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ•æ„Ÿå’Œå¼€æ”¾ç«¯è¯­è¨€æŸ¥è¯¢å¯¹äºè®¸å¤šå®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶LangSplatæˆåŠŸåœ°å°†CLIPç‰¹æ€§èå…¥3Dé«˜æ–¯è¡¨ç¤ºï¼Œå®ç°äº†åœ¨3Dé™æ€åœºæ™¯ä¸­çš„ç²¾ç¡®æ€§å’Œæ•ˆç‡ï¼Œä½†å®ƒæ— æ³•å¤„ç†åŠ¨æ€çš„4Då­—æ®µã€‚ç°å®ä¸–ç•Œçš„ç¯å¢ƒæœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„ï¼Œç‰©ä½“è¯­ä¹‰éšæ—¶é—´æ¼”å˜ã€‚ä¸ºäº†æ„å»ºç²¾ç¡®çš„4Dè¯­è¨€å­—æ®µï¼Œå¿…é¡»ä»è§†é¢‘ä¸­è·å¾—åƒç´ å¯¹é½çš„ç‰©ä½“çº§ç‰¹å¾ï¼Œè€Œå½“å‰è§†è§‰æ¨¡å‹éš¾ä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†4D LangSplatï¼Œå®ƒèƒ½å¤Ÿå¤„ç†åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´æ— å…³æˆ–æ—¶é—´æ•æ„Ÿçš„å¼€æ”¾è¯æ±‡æŸ¥è¯¢ã€‚4D LangSplatç»•è¿‡ä»è§†è§‰ç‰¹å¾å­¦ä¹ è¯­è¨€å­—æ®µï¼Œè€Œæ˜¯ç›´æ¥ä»ç”±å¯¹è±¡çº§è§†é¢‘å­—å¹•ç”Ÿæˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯¹è±¡çº§è§†é¢‘æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œå¼•å¯¼MLLMsä¸ºè§†é¢‘ä¸­çš„å¯¹è±¡ç”Ÿæˆè¯¦ç»†ã€æ—¶é—´è¿è´¯çš„é«˜è´¨é‡å­—å¹•ã€‚è¿™äº›å­—å¹•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç¼–ç æˆé«˜è´¨é‡å¥å­åµŒå…¥ï¼Œä½œä¸ºåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å®šç‰¹å¾ç›‘ç£ï¼Œé€šè¿‡å…±äº«åµŒå…¥ç©ºé—´å®ç°å¼€æ”¾è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ã€‚è®¤è¯†åˆ°4Dåœºæ™¯ä¸­å¯¹è±¡çš„å¹³æ»‘çŠ¶æ€è½¬æ¢ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†çŠ¶æ€å¯å˜å½¢ç½‘ç»œæ¥æœ‰æ•ˆæ¨¡æ‹Ÿè¿™äº›éšæ—¶é—´å˜åŒ–çš„è¿ç»­å˜åŒ–ã€‚æˆ‘ä»¬çš„å¤šé¡¹åŸºå‡†æµ‹è¯•ç»“æœè¯æ˜ï¼Œ4D LangSplatå¯¹äºæ—¶é—´æ•æ„Ÿå’Œæ—¶é—´æ— å…³çš„å¼€æ”¾è¯æ±‡æŸ¥è¯¢å‡èƒ½è¾¾åˆ°ç²¾ç¡®å’Œé«˜æ•ˆçš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å­¦ä¹ 4Dè¯­è¨€å­—æ®µå¯¹äºå¤„ç†ç°å®ä¸–ç•Œä¸­åŠ¨æ€åœºæ™¯çš„æ—¶é—´æ•æ„Ÿå’Œå¼€æ”¾ç«¯è¯­è¨€æŸ¥è¯¢è‡³å…³é‡è¦ã€‚</li>
<li>LangSplatåœ¨3Dé™æ€åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†æ— æ³•å¤„ç†åŠ¨æ€çš„4Då­—æ®µã€‚</li>
<li>4D LangSplaté€šè¿‡ç›´æ¥å­¦ä¹ ä»å¯¹è±¡çº§è§†é¢‘å­—å¹•ç”Ÿæˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€å¯¹è±¡çº§è§†é¢‘æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„ã€ä¸æ—¶é—´ç›¸å…³çš„å¯¹è±¡å­—å¹•ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†å­—å¹•ç¼–ç æˆé«˜è´¨é‡å¥å­åµŒå…¥ï¼Œä½œä¸ºåƒç´ å¯¹é½çš„å¯¹è±¡ç‰¹å®šç‰¹å¾ç›‘ç£ã€‚</li>
<li>é€šè¿‡å…±äº«åµŒå…¥ç©ºé—´ï¼Œå¯ä»¥å®ç°å¼€æ”¾è¯æ±‡æ–‡æœ¬æŸ¥è¯¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-31e10094985ed60b63a44eae59c925ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0386885b55626e702f4946956c9ad1ae.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="On-the-Limitations-of-Vision-Language-Models-in-Understanding-Image-Transforms"><a href="#On-the-Limitations-of-Vision-Language-Models-in-Understanding-Image-Transforms" class="headerlink" title="On the Limitations of Vision-Language Models in Understanding Image   Transforms"></a>On the Limitations of Vision-Language Models in Understanding Image   Transforms</h2><p><strong>Authors:Ahmad Mustafa Anis, Hasnain Ali, Saquib Sarfraz</strong></p>
<p>Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image&#x2F;Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€èŠå¤©æœºå™¨äººå’Œè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºæœ¬çš„å›¾åƒè½¬æ¢æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡ç ”ç©¶äº†VLMsçš„å›¾åƒçº§ç†è§£ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„CLIPå’ŒGoogleçš„SigLIPã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹ç¼ºä¹å¯¹å›¾åƒçº§å¢å¼ºçš„ç†è§£ã€‚ä¸ºäº†ä¿ƒè¿›è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Flickr8kæ•°æ®é›†çš„å¢å¼ºç‰ˆæœ¬ï¼Œä¸ºæ¯ä¸ªå›¾åƒæä¾›æ‰€åº”ç”¨è½¬æ¢çš„è¯¦ç»†æè¿°ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†è¿™ç§ç¼ºé™·å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ï¼Œå¹¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„Image2Imageæ¨¡å‹åœ¨ç®€å•è½¬æ¢ä¸Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09837v1">PDF</a> 8 pages, 15 images</p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€èŠå¤©æœºå™¨äººå’Œè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºæœ¬å›¾åƒè½¬æ¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡ç ”ç©¶äº†VLMsçš„å›¾åƒçº§ç†è§£ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„CLIPå’ŒGoogleçš„SigLIPã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹æ— æ³•ç†è§£å¤šç§å›¾åƒçº§å¢å¼ºã€‚ä¸ºäº†æ¨è¿›è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Flickr8kæ•°æ®é›†çš„å¢å¼ºç‰ˆæœ¬ï¼Œä¸ºæ¯å¼ å›¾åƒé…å¤‡è¯¦ç»†çš„è½¬æ¢æè¿°ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è¿™ç§ç¼ºé™·å¯¹ä¸‹æ¸¸ä»»åŠ¡ã€å°¤å…¶æ˜¯å›¾åƒç¼–è¾‘çš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†æœ€æ–°Image2Imageæ¨¡å‹åœ¨ç®€å•è½¬æ¢ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>VLMsåœ¨åŸºæœ¬å›¾åƒè½¬æ¢æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†VLMsçš„å›¾åƒçº§ç†è§£ï¼Œç‰¹åˆ«æ˜¯CLIPå’ŒSigLIPæ¨¡å‹ã€‚</li>
<li>è¿™äº›æ¨¡å‹æ— æ³•ç†è§£å¤šç§å›¾åƒçº§å¢å¼ºï¼Œè¡¨æ˜å…¶å±€é™æ€§ã€‚</li>
<li>ä¸ºäº†æ¨è¿›ç ”ç©¶ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé…å¤‡è¯¦ç»†è½¬æ¢æè¿°çš„Flickr8kæ•°æ®é›†å¢å¼ºç‰ˆæœ¬ã€‚</li>
<li>VLMsçš„ç¼ºé™·å¯¹ä¸‹æ¸¸ä»»åŠ¡ã€å°¤å…¶æ˜¯å›¾åƒç¼–è¾‘äº§ç”Ÿå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cb46979643ad8fca7fd95cd099c1368.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c454940760913645b686754cbcd6a46d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6ecc18507cccac1927dc33660f3a38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87db6d2273c54cbdeae08981d6ed6c15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b9856e4640fd98b698ce3751e3546eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2731ce35c28573ac02b6438fa41a7d5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b49fd19c024995dbc350d070f825249b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  SurgRAW Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5f65fe55be11d251101f7f1bf9c9617e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  SciVerse Unveiling the Knowledge Comprehension and Visual Reasoning of   LMMs on Multi-modal Scientific Problems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17204.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
