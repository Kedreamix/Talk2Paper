<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  Imaging Ultrafast Dynamical Diffraction wavefronts of femtosecond   laser-induced lattice distortions inside crystalline semiconductors">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-88c1a2d35388d5c366bd493c38f13a4f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-15-æ›´æ–°"><a href="#2025-03-15-æ›´æ–°" class="headerlink" title="2025-03-15 æ›´æ–°"></a>2025-03-15 æ›´æ–°</h1><h2 id="Imaging-Ultrafast-Dynamical-Diffraction-wavefronts-of-femtosecond-laser-induced-lattice-distortions-inside-crystalline-semiconductors"><a href="#Imaging-Ultrafast-Dynamical-Diffraction-wavefronts-of-femtosecond-laser-induced-lattice-distortions-inside-crystalline-semiconductors" class="headerlink" title="Imaging Ultrafast Dynamical Diffraction wavefronts of femtosecond   laser-induced lattice distortions inside crystalline semiconductors"></a>Imaging Ultrafast Dynamical Diffraction wavefronts of femtosecond   laser-induced lattice distortions inside crystalline semiconductors</h2><p><strong>Authors:Angel RodrÃ­guez-FernÃ¡ndez, Jan-Etienne Pudell, Roman Shayduk, Wonhyuk Jo, James Wrigley, Johannes MÃ¶ller, Peter Zalden, Alexey Zozulya, JÃ¶rg Hallmann, Anders Madsen, Pablo Villanueva-Perez, Zdenek Matej, Thies J. Albert, Dominik Kaczmarek, Klaus Sokolowski-Tinten, Antonowicz Jerzy, Ryszard Sobierajski, Rahimi Mosafer, Oleksii I. Liubchenko, Javier Solis, Jan Siegel</strong></p>
<p>Material processing with femtosecond lasers has attracted enormous attention because of its potential for technology and industry applications. In parallel, time-resolved x-ray diffraction has been successfully used to study ultrafast structural distortion dynamics in semiconductor thin films. Gracing incident x-ray geometry has been also used to look to distortion dynamics, but this technique is only sensitive to the surface of bulk materials with a limited temporal resolution. However, â€˜real-worldâ€™ processing applications deal mostly with bulk materials, which prevent the use of such techniques. For processing applications, a fast and depth-sensitive probe is needed. To address this, we present a novel technique based on ultrafast dynamical diffraction (UDD) capable of imaging transient strain distributions inside bulk crystals upon single-pulse excitation. This pump-probe technique provides a complete picture of the temporal evolution of ultrafast distortion depth profiles. Our measurements were obtained in a thin crystalline Si wafer upon single pulse femtosecond optical excitation revealing that even below the melting threshold strong lattice distortions appear on ps time scales due to the formation and propagation of high-amplitude strain waves into the bulk. </p>
<blockquote>
<p>ææ–™ä½¿ç”¨é£ç§’æ¿€å…‰å¤„ç†å› å…¶æŠ€æœ¯å’Œå·¥ä¸šåº”ç”¨æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚åŒæ—¶ï¼Œæ—¶é—´åˆ†è¾¨Xå°„çº¿è¡å°„å·²æˆåŠŸåº”ç”¨äºç ”ç©¶åŠå¯¼ä½“è–„è†œä¸­çš„è¶…å¿«ç»“æ„ç•¸å˜åŠ¨åŠ›å­¦ã€‚å°½ç®¡å…¥å°„Xå°„çº¿å‡ ä½•ç»“æ„ä¹Ÿè¢«ç”¨äºè§‚å¯Ÿç•¸å˜åŠ¨åŠ›å­¦ï¼Œä½†è¿™ç§æŠ€æœ¯ä»…å¯¹ä½“ææ–™è¡¨é¢æ•æ„Ÿï¼Œä¸”æ—¶é—´åˆ†è¾¨ç‡æœ‰é™ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„å¤„ç†åº”ç”¨å¤§å¤šæ¶‰åŠä½“ææ–™ï¼Œè¿™ä½¿å¾—è¿™äº›æŠ€æœ¯çš„ä½¿ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¶…å¿«åŠ¨æ€è¡å°„ï¼ˆUDDï¼‰çš„æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å•è„‰å†²æ¿€å‘ä¸‹å¯¹ä½“æ™¶ä½“å†…éƒ¨çš„ç¬æ€åº”å˜åˆ†å¸ƒè¿›è¡Œæˆåƒã€‚è¿™ç§æ³µæµ¦æ¢é’ˆæŠ€æœ¯æä¾›äº†è¶…å¿«ç•¸å˜æ·±åº¦åˆ†å¸ƒçš„æ—¶é—´æ¼”åŒ–çš„å®Œæ•´å›¾åƒã€‚æˆ‘ä»¬çš„æµ‹é‡æ˜¯åœ¨å•æ™¶ç¡…è–„ç‰‡ä¸­è¿›è¡Œçš„ï¼Œé€šè¿‡å¯¹å•è„‰å†²é£ç§’å…‰å­¦æ¿€å‘å‘ç°ï¼Œå³ä½¿åœ¨ä½äºç†”åŒ–é˜ˆå€¼çš„æƒ…å†µä¸‹ï¼Œç”±äºé«˜æŒ¯å¹…åº”å˜æ³¢çš„å½¢æˆå’Œå‘ä½“ææ–™çš„ä¼ æ’­ï¼Œçš®ç§’æ—¶é—´å†…ä¹Ÿä¼šå‡ºç°å¼ºçƒˆçš„æ™¶æ ¼ç•¸å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10420v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å…³æ³¨é£ç§’æ¿€å…‰ææ–™åŠ å·¥æŠ€æœ¯åŠå…¶åœ¨åŠå¯¼ä½“è–„è†œä¸­çš„è¶…å¿«ç»“æ„ç•¸å˜åŠ¨åŠ›å­¦ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯æ— æ³•å¯¹å—çŠ¶ææ–™è¿›è¡Œæ·±åº¦æ•æ„Ÿæ¢æµ‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¶…å¿«åŠ¨æ€è¡å°„ï¼ˆUDDï¼‰çš„æ–°å‹æˆåƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨å•è„‰å†²æ¿€å‘ä¸‹æˆåƒæ™¶ä½“å†…éƒ¨çš„ç¬æ€åº”å˜åˆ†å¸ƒï¼Œä¸ºåŠ å·¥åº”ç”¨æä¾›äº†å¿«é€Ÿä¸”æ·±åº¦æ•æ„Ÿçš„æ¢æµ‹æ‰‹æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å•è„‰å†²é£ç§’å…‰å­¦æ¿€å‘ä¸‹ï¼Œå³ä½¿åœ¨ç†”ç‚¹ä»¥ä¸‹ï¼Œç¡…æ™¶åœ†è–„ç‰‡ä¹Ÿä¼šå‡ºç°å¼ºæ™¶æ ¼ç•¸å˜ï¼Œè¡¨ç°ä¸ºé«˜æŒ¯å¹…åº”å˜æ³¢åœ¨ä½“ææ–™ä¸­çš„å½¢æˆå’Œä¼ æ’­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é£ç§’æ¿€å…‰ææ–™åŠ å·¥æŠ€æœ¯å› å…¶æ½œåœ¨çš„æŠ€æœ¯å’Œå·¥ä¸šåº”ç”¨è€Œå¤‡å—å…³æ³¨ã€‚</li>
<li>æ—¶é—´åˆ†è¾¨Xå°„çº¿è¡å°„å·²æˆåŠŸåº”ç”¨äºç ”ç©¶åŠå¯¼ä½“è–„è†œä¸­çš„è¶…å¿«ç»“æ„ç•¸å˜åŠ¨åŠ›å­¦ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯éš¾ä»¥å¤„ç†å—çŠ¶ææ–™çš„æ·±åº¦æ•æ„Ÿæ¢æµ‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè¶…å¿«åŠ¨æ€è¡å°„ï¼ˆUDDï¼‰çš„æ–°å‹æˆåƒæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥æˆåƒæ™¶ä½“å†…éƒ¨çš„ç¬æ€åº”å˜åˆ†å¸ƒã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ºåŠ å·¥åº”ç”¨æä¾›äº†å¿«é€Ÿä¸”æ·±åº¦æ•æ„Ÿçš„æ¢æµ‹æ‰‹æ®µã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å•è„‰å†²é£ç§’å…‰å­¦æ¿€å‘ä¸‹ï¼Œç¡…æ™¶åœ†è–„ç‰‡å‡ºç°å¼ºæ™¶æ ¼ç•¸å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-785a7e12033fb806b9e5193abf4d7f8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7735e122d86f98864780868d96bcd68.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Bilingual-Dual-Head-Deep-Model-for-Parkinsonâ€™s-Disease-Detection-from-Speech"><a href="#Bilingual-Dual-Head-Deep-Model-for-Parkinsonâ€™s-Disease-Detection-from-Speech" class="headerlink" title="Bilingual Dual-Head Deep Model for Parkinsonâ€™s Disease Detection from   Speech"></a>Bilingual Dual-Head Deep Model for Parkinsonâ€™s Disease Detection from   Speech</h2><p><strong>Authors:Moreno La Quatra, Juan Rafael Orozco-Arroyave, Marco Sabato Siniscalchi</strong></p>
<p>This work aims to tackle the Parkinsonâ€™s disease (PD) detection problem from the speech signal in a bilingual setting by proposing an ad-hoc dual-head deep neural architecture for type-based binary classification. One head is specialized for diadochokinetic patterns. The other head looks for natural speech patterns present in continuous spoken utterances. Only one of the two heads is operative accordingly to the nature of the input. Speech representations are extracted from self-supervised learning (SSL) models and wavelet transforms. Adaptive layers, convolutional bottlenecks, and contrastive learning are exploited to reduce variations across languages. Our solution is assessed against two distinct datasets, EWA-DB, and PC-GITA, which cover Slovak and Spanish languages, respectively. Results indicate that conventional models trained on a single language dataset struggle with cross-linguistic generalization, and naive combinations of datasets are suboptimal. In contrast, our model improves generalization on both languages, simultaneously. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³åŒè¯­ç¯å¢ƒä¸‹çš„å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰è¯­éŸ³ä¿¡å·æ£€æµ‹é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç±»å‹äºŒåˆ†ç±»çš„ä¸“ç”¨åŒå¤´æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸€ä¸ªå¤´ä¸“é—¨ç”¨äºå¤„ç†è¿ç»­è¯­éŸ³ä¸­çš„å‘éŸ³è¿åŠ¨æ¨¡å¼ï¼Œå¦ä¸€ä¸ªå¤´åˆ™ç”¨äºå¯»æ‰¾è‡ªç„¶è¯­éŸ³æ¨¡å¼ã€‚æ ¹æ®è¾“å…¥çš„æ€§è´¨ï¼Œåªæœ‰å…¶ä¸­ä¸€ä¸ªå¤´å¤„äºå·¥ä½œçŠ¶æ€ã€‚è¯­éŸ³è¡¨ç¤ºæ˜¯ä»è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹å’Œå°æ³¢å˜æ¢ä¸­æå–çš„ã€‚æˆ‘ä»¬åˆ©ç”¨è‡ªé€‚åº”å±‚ã€å·ç§¯ç“¶é¢ˆå’Œå¯¹æ¯”å­¦ä¹ æ¥å‡å°‘è·¨è¯­è¨€ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†EWA-DBå’ŒPC-GITAä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™ä¸¤ä¸ªæ•°æ®é›†åˆ†åˆ«æ¶µç›–äº†æ–¯æ´›ä¼å…‹è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å•ä¸€è¯­è¨€æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¸¸è§„æ¨¡å‹åœ¨è·¨è¯­è¨€æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œå•çº¯çš„æ•°æ®é›†ç»„åˆæ•ˆæœå¹¶ä¸ç†æƒ³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åŒæ—¶æ”¹å–„ä¸¤ç§è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10301v1">PDF</a> Accepted at ICASSP 2025 - Personal use of this material is permitted.   Permission from IEEE must be obtained for all other uses</p>
<p><strong>Summary</strong><br>å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ£€æµ‹åœ¨åŒè¯­ç¯å¢ƒä¸‹çš„è¯­éŸ³ä¿¡å·ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸“é—¨é’ˆå¯¹ç±»å‹åŒ–äºŒå…ƒåˆ†ç±»çš„åŒå¤´æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸€å¤´ä¸“æ³¨äºè¿ç»­è¯­éŸ³ä¸­çš„è‡ªç„¶è¯­éŸ³æ¨¡å¼ï¼Œå¦ä¸€å¤´ä¸“æ³¨äºå‘éŸ³è¿åŠ¨æ¨¡å¼ã€‚æ ¹æ®è¾“å…¥ç±»å‹é€‰æ‹©æ¿€æ´»ç›¸åº”çš„å¤´éƒ¨ã€‚è¯¥ç ”ç©¶ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹å’Œå°æ³¢å˜æ¢æå–è¯­éŸ³ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è‡ªé€‚åº”å±‚ã€å·ç§¯ç“¶é¢ˆå’Œå¯¹æ¯”å­¦ä¹ æ¥å‡å°‘è¯­è¨€é—´çš„å·®å¼‚ã€‚åœ¨æ–¯æ´›ä¼å…‹å’Œè¥¿ç­ç‰™è¯­æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå•ä¸€è¯­è¨€æ¨¡å‹çš„è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œè€Œæœ¬æ¨¡å‹åœ¨ä¸¤ç§è¯­è¨€ä¸Šçš„æ³›åŒ–èƒ½åŠ›å‡æœ‰æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹åŒè¯­ç¯å¢ƒä¸‹çš„å¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ£€æµ‹é—®é¢˜ï¼Œæå‡ºäº†åŒå¤´æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
<li>åŒå¤´æ¶æ„åˆ†åˆ«ä¸“æ³¨äºå‘éŸ³è¿åŠ¨æ¨¡å¼ä¸è‡ªç„¶è¯­éŸ³æ¨¡å¼ã€‚</li>
<li>ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹å’Œå°æ³¢å˜æ¢æå–è¯­éŸ³ç‰¹å¾ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”å±‚ã€å·ç§¯ç“¶é¢ˆå’Œå¯¹æ¯”å­¦ä¹ å‡å°‘è¯­è¨€é—´çš„å·®å¼‚ã€‚</li>
<li>åœ¨æ–¯æ´›ä¼å…‹å’Œè¥¿ç­ç‰™è¯­æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜å•ä¸€è¯­è¨€æ¨¡å‹çš„è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹åœ¨ä¸¤ç§è¯­è¨€ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f4c43c6bcc3131f3eebca396a1b95fd2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcc5c1c85bdb30d89e55e4b16c1cf244.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ebbcfd42f51ad07aeb6be0f5b98c8a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd02307a382844ae60711455d47b18a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Automatic-quality-control-in-multi-centric-fetal-brain-MRI-super-resolution-reconstruction"><a href="#Automatic-quality-control-in-multi-centric-fetal-brain-MRI-super-resolution-reconstruction" class="headerlink" title="Automatic quality control in multi-centric fetal brain MRI   super-resolution reconstruction"></a>Automatic quality control in multi-centric fetal brain MRI   super-resolution reconstruction</h2><p><strong>Authors:Thomas Sanchez, Vladyslav Zalevsky, Angeline Mihailo, Gerard MartÃ­ Juan, Elisenda Eixarch, Andras Jakab, Vincent Dunet, MÃ©riam Koob, Guillaume Auzias, Meritxell Bach Cuadra</strong></p>
<p>Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$<em>{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$</em>{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC &#x3D; 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model will be released upon acceptance of the paper. </p>
<blockquote>
<p>è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰è¢«è®¤ä¸ºæ˜¯ä¿è¯ç¥ç»å½±åƒå­¦ç ”ç©¶å¯é æ€§çš„å…³é”®å› ç´ å·²ç»å¾ˆä¹…äº†ã€‚å¯¹äºèƒå„¿è„‘éƒ¨MRIæ¥è¯´å°¤å…¶é‡è¦ï¼Œå› ä¸ºä¸æˆäººæˆåƒç›¸æ¯”ï¼Œå…¶é‡‡é›†å’Œå›¾åƒå¤„ç†æŠ€æœ¯æ ‡å‡†åŒ–ç¨‹åº¦è¾ƒä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºèƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨è´¨é‡æ§åˆ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å¤„ç†æ­¥éª¤ï¼Œå…¶ä¸­å¤šä¸ªåšçš„2Dåˆ‡ç‰‡å †å åœ¨ä¸€èµ·æ³¨å†Œï¼Œå¹¶ç»„åˆæˆä¸€ä¸ªå•ä¸€ã€å„å‘åŒæ€§ä¸”æ— ä¼ªå½±çš„T2åŠ æƒä½“ç§¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æå–äº†è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚è¿™ç§æ–¹æ³•éå¸¸é€‚åˆäºé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨å’Œå°æ•°æ®é›†çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨åŸŸå¤–ï¼ˆOODï¼‰ç¯å¢ƒä¸­éªŒè¯äº†FetMRQC_{SR}ï¼Œå¹¶æŠ¥å‘Šäº†é«˜æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ï¼Œå³ä½¿åœ¨é¢å¯¹æ¥è‡ªæœªçŸ¥ç«™ç‚¹æˆ–SRRæ–¹æ³•çš„æ•°æ®æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬è¿˜è°ƒæŸ¥äº†å¤±è´¥çš„æƒ…å†µï¼Œå¹¶è¡¨æ˜è¿™äº›å¤±è´¥å‘ç”Ÿåœ¨45%çš„å›¾åƒä¸­ï¼Œä¸»è¦æ˜¯ç”±äºé…ç½®æ¨¡ç³Šï¼Œä¸“å®¶çš„è¯„åˆ†å­˜åœ¨äº‰è®®ã€‚è¿™äº›ç»“æœä»¤äººé¼“èˆï¼Œå¹¶è¯´æ˜äº†åƒFetMRQC_{SR}è¿™æ ·çš„éæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚ä½•é€‚åº”è¿™ä¸ªå¤šé¢é—®é¢˜ã€‚æˆ‘ä»¬çš„å·¥å…·ä»¥åŠç”¨äºç”Ÿæˆã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„æ‰€æœ‰ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10156v1">PDF</a> 11 pages, 3 figures; Submitted to MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡å…³æ³¨èƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶ã€‚æå‡ºä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æå–è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†è¾ƒå°çš„é—®é¢˜ã€‚åœ¨é¢†åŸŸå¤–ï¼ˆOODï¼‰ç¯å¢ƒä¸‹éªŒè¯äº†FetMRQC_{SR}çš„æ€§èƒ½ï¼Œå³ä½¿é¢å¯¹æ¥è‡ªæœªçŸ¥ç«™ç‚¹æˆ–SRRæ–¹æ³•çš„æ•°æ®ï¼Œä¹Ÿè¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œ45%çš„å›¾åƒå­˜åœ¨å¤±è´¥æ¡ˆä¾‹ï¼Œä¸»è¦ç”±äºæ¨¡ç³Šé…ç½®å¯¼è‡´ä¸“å®¶è¯„åˆ†å­˜åœ¨äº‰è®®ã€‚æœ¬æ–‡å·¥å…·åŠæ‰€æœ‰ç”¨äºç”Ÿæˆã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„ä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è´¨é‡æ§åˆ¶åœ¨ç¥ç»æˆåƒç ”ç©¶ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨èƒå„¿è„‘éƒ¨MRIä¸­ï¼Œå…¶é‡‡é›†å’Œå›¾åƒå¤„ç†æŠ€æœ¯è¾ƒæˆäººæˆåƒæ ‡å‡†åŒ–ç¨‹åº¦è¾ƒä½ã€‚</li>
<li>æœ¬æ–‡å…³æ³¨èƒå„¿è„‘éƒ¨MRIçš„è¶…çº§åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰çš„è´¨é‡æ§åˆ¶ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æå–è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨éæ·±åº¦å­¦ä¹ çš„èƒŒæ™¯ä¸‹è¡¨ç°è‰¯å¥½ï¼Œé€‚ç”¨äºé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨çš„é—®é¢˜ã€‚</li>
<li>åœ¨é¢†åŸŸå¤–ç¯å¢ƒä¸‹éªŒè¯æ€§èƒ½è‰¯å¥½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ï¼Œåœ¨ä¸åŒæ•°æ®æºä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å‘ç°å¤±è´¥æ¡ˆä¾‹å 45%ï¼Œä¸»è¦æ˜¯ç”±äºå›¾åƒé…ç½®æ¨¡ç³Šå¯¼è‡´ä¸“å®¶è¯„åˆ†çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯¥å·¥å…·å’Œä»£ç å°†åœ¨è®ºæ–‡è¢«æ¥å—åå…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-46432cec15b4bf57bf5bc4bd120234b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95eb2a9b36bde144774b751fc34f44a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc8153f3671aa11c8cb7d3223f13b24b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Probing-the-Hot-Gaseous-Halo-of-the-Low-mass-Disk-Galaxy-NGC-7793-with-eROSITA-and-Chandra"><a href="#Probing-the-Hot-Gaseous-Halo-of-the-Low-mass-Disk-Galaxy-NGC-7793-with-eROSITA-and-Chandra" class="headerlink" title="Probing the Hot Gaseous Halo of the Low-mass Disk Galaxy NGC 7793 with   eROSITA and Chandra"></a>Probing the Hot Gaseous Halo of the Low-mass Disk Galaxy NGC 7793 with   eROSITA and Chandra</h2><p><strong>Authors:Lin He, Zhiyuan Li, Meicun Hou, Min Du, Taotao Fang, Wei Cui</strong></p>
<p>Galaxy formation models predict that local galaxies are surrounded by hot X-ray-emitting halos, which are technically difficult to detect due to their extended and low surface brightness nature. Previous X-ray studies have mostly focused on disk galaxies more massive than the Milky Way, with essentially no consensus on the halo X-ray properties at the lower mass end. We utilize the early-released eROSITA and archival Chandra observations to analyze the diffuse X-ray emission of NGC7793, a nearby spiral galaxy with an estimated stellar mass of only $3.2\times 10^9$ $M_{\odot}$. We find evidence for extraplanar hot gas emission from both the radial and vertical soft X-ray intensity profiles, which spreads up to a galactocentric distance of $\sim$ 6 kpc, nearly 30 $%$ more extended than its stellar disk. Analysis of the eROSITA spectra indicates that the hot gas can be characterized by a temperature of $0.18^{+0.02}_{-0.03}$ keV, with 0.5â€“2 keV unabsorbed luminosity of $1.3\times 10^{38}$ erg $s^{-1}$. We compare our results with the IllustrisTNG simulations and find overall consistence on the disk scale, whereas excessive emission at large radii is predicted by TNG50. This work provides the latest detection of hot corona around a low-mass galaxy, putting new constrains on state-of-the-art cosmological simulations. We also verify the detectability of hot circumgalactic medium around even low-mass spirals with future high-resolution X-ray spectrometer such as the Hot Universe Baryon Surveyor. </p>
<blockquote>
<p>æ˜Ÿç³»å½¢æˆæ¨¡å‹é¢„æµ‹ï¼Œå±€éƒ¨æ˜Ÿç³»å‘¨å›´å­˜åœ¨å‘å°„Xå°„çº¿çš„çƒ­æ™•ï¼Œç”±äºå…¶å»¶å±•æ€§å’Œä½è¡¨é¢äº®åº¦ï¼ŒæŠ€æœ¯ä¸Šå¾ˆéš¾æ£€æµ‹åˆ°ã€‚ä¹‹å‰çš„Xå°„çº¿ç ”ç©¶ä¸»è¦é›†ä¸­äºè´¨é‡å¤§äºé“¶æ²³ç³»çš„ç›˜çŠ¶æ˜Ÿç³»ï¼Œå¯¹äºä½è´¨é‡ç«¯æ˜Ÿç³»æ™•çš„Xå°„çº¿ç‰¹æ€§åŸºæœ¬æ— å…±è¯†ã€‚æˆ‘ä»¬åˆ©ç”¨æ—©æœŸå‘å¸ƒçš„eROSITAè§‚æµ‹æ•°æ®å’Œå­˜æ¡£çš„Chandraè§‚æµ‹æ•°æ®æ¥åˆ†æNGC7793çš„æ¼«å°„Xå°„çº¿å‘å°„æƒ…å†µã€‚NGC7793æ˜¯é™„è¿‘çš„ä¸€ä¸ªèºæ—‹æ˜Ÿç³»ï¼Œå…¶æ’æ˜Ÿè´¨é‡ä¼°è®¡ä»…ä¸º$3.2\times 10^9$ $M_{\odot}$ã€‚æˆ‘ä»¬å‘ç°å¾„å‘å’Œå‚ç›´è½¯Xå°„çº¿å¼ºåº¦åˆ†å¸ƒéƒ½æœ‰å¹³é¢å¤–çš„çƒ­æ°”ä½“å‘å°„è¯æ®ï¼Œå®ƒæ‰©å±•åˆ°ç¦»ä¸­å¿ƒçº¦6kpcçš„è·ç¦»ï¼Œæ¯”å…¶æ’æ˜Ÿç›˜å»¶ä¼¸äº†çº¦30%ã€‚å¯¹eROSITAå…‰è°±çš„åˆ†æè¡¨æ˜ï¼Œçƒ­æ°”ä½“ç‰¹å¾æ¸©åº¦çº¦ä¸º$0.18^{+0.02}_{-0.03}$ keVï¼Œæœªå¸æ”¶çš„0.5-2 keVå…‰åº¦ä¸º$1.3\times 10^{38}$ erg s$^{-1}$ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç»“æœä¸IllustrisTNGæ¨¡æ‹Ÿè¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°åœ¨ç£ç›˜å°ºåº¦ä¸Šæ€»ä½“ä¸€è‡´ï¼Œè€ŒTNG50é¢„è®¡åœ¨è¾ƒå¤§åŠå¾„å¤„ä¼šå‡ºç°è¿‡å¤šçš„å‘å°„ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä½è´¨é‡æ˜Ÿç³»å‘¨å›´çƒ­å†•çš„æœ€æ–°æ£€æµ‹ï¼Œå¯¹æœ€æ–°çš„å®‡å®™å­¦æ¨¡æ‹Ÿæå‡ºäº†æ–°çš„çº¦æŸã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†å³ä½¿å¯¹äºä½è´¨é‡èºæ—‹æ˜Ÿç³»ï¼Œæœªæ¥é«˜åˆ†è¾¨ç‡Xå°„çº¿å…‰è°±ä»ªï¼ˆå¦‚Hot Universe Baryon Surveyorï¼‰ä¹Ÿå¯ä»¥æ£€æµ‹åˆ°å›´ç»•å…¶å‘¨å›´çš„çƒ­å®‡å®™ä»‹è´¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10087v1">PDF</a> 17 pages, 7 figures, 1 table, accepted for publication in ApJ</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ‘˜è¦åŸºäºæ˜Ÿç³»å½¢æˆæ¨¡å‹é¢„æµ‹ï¼Œæœ¬åœ°æ˜Ÿç³»å‘¨å›´å­˜åœ¨éš¾ä»¥æ£€æµ‹çš„çƒ­Xå°„çº¿å‘å°„æ™•ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨è´¨é‡é«˜äºé“¶æ²³ç³»çš„å¤§å‹ç›˜æ˜Ÿç³»ï¼Œè€Œå¯¹ä½è´¨é‡æ˜Ÿç³»çš„æ™•Xå°„çº¿ç‰¹æ€§å°šæ— å…±è¯†ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ—©æœŸå‘å¸ƒçš„eROSITAå’Œå­˜æ¡£çš„Chandraè§‚æµ‹æ•°æ®ï¼Œåˆ†æäº†NGC7793è¿™ä¸€é‚»è¿‘èºæ—‹æ˜Ÿç³»çš„å¼¥æ•£Xå°„çº¿å‘å°„ã€‚è¯¥æ˜Ÿç³»çš„æ’æ˜Ÿè´¨é‡ä¼°è®¡ä»…ä¸º$3.2\times 10^9$ $M_{\odot}$ã€‚æˆ‘ä»¬å‘ç°æ¥è‡ªå¾„å‘å’Œå‚ç›´è½¯Xå°„çº¿å¼ºåº¦åˆ†å¸ƒçš„è¯æ®æ˜¾ç¤ºå­˜åœ¨çƒ­æ°”ä½“å‘å°„ï¼Œå…¶æ‰©å±•è‡³è·ç¦»ä¸­å¿ƒçº¦6kpcçš„åœ°æ–¹ï¼Œå‡ ä¹æ¯”å…¶æ’æ˜Ÿç›˜å»¶ä¼¸äº†30%ã€‚å¯¹eROSITAå…‰è°±çš„åˆ†æè¡¨æ˜ï¼Œçƒ­æ°”ä½“å¯ä»¥é€šè¿‡æ¸©åº¦çº¦ä¸º$0.18^{+0.02}_{-0.03}$ keVçš„ç‰¹è´¨æ¥è¡¨å¾ï¼Œæœªå¸æ”¶åœ¨0.5â€“2 keVä¹‹é—´çš„å…‰åº¦çº¦ä¸º $1.3\times 10^{38}$ erg $s^{-1}$ã€‚æˆ‘ä»¬çš„ç»“æœä¸IllustrisTNGæ¨¡æ‹Ÿå¤§è‡´ä¸€è‡´ï¼Œä½†TNG50åœ¨å¤§åŠå¾„å¤„é¢„æµ‹è¿‡åº¦å‘å°„ã€‚è¿™é¡¹ç ”ç©¶ä¸ºä½è´¨é‡æ˜Ÿç³»å‘¨å›´çƒ­æ™•çš„æœ€æ–°æ£€æµ‹æä¾›äº†è¯æ®ï¼Œå¯¹æœ€æ–°çš„å®‡å®™å­¦æ¨¡æ‹Ÿæå‡ºäº†æ–°çš„çº¦æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éªŒè¯äº†æœªæ¥ä½¿ç”¨é«˜åˆ†è¾¨ç‡Xå°„çº¿å…‰è°±ä»ªï¼ˆå¦‚Hot Universe Baryon Surveyorï¼‰æ£€æµ‹ä½è´¨é‡èºæ—‹æ˜Ÿç³»å‘¨å›´çƒ­ç¯å‘¨ä»‹è´¨çš„å¯èƒ½æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ˜Ÿç³»å½¢æˆæ¨¡å‹é¢„æµ‹æœ¬åœ°æ˜Ÿç³»å‘¨å›´å­˜åœ¨éš¾ä»¥æ£€æµ‹çš„çƒ­Xå°„çº¿å‘å°„æ™•ã€‚</li>
<li>å…ˆå‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è´¨é‡è¾ƒå¤§çš„ç›˜æ˜Ÿç³»ä¸Šï¼Œè€Œå¯¹ä½è´¨é‡æ˜Ÿç³»çš„æ™•Xå°„çº¿ç‰¹æ€§äº†è§£è¾ƒå°‘ã€‚</li>
<li>åˆ©ç”¨eROSITAå’ŒChandraè§‚æµ‹æ•°æ®ï¼Œå‘ç°ä½è´¨é‡èºæ—‹æ˜Ÿç³»NGC7793å­˜åœ¨çƒ­æ°”ä½“å‘å°„è¯æ®ã€‚</li>
<li>çƒ­æ°”ä½“å»¶ä¼¸è¶…å‡ºæ’æ˜Ÿç›˜èŒƒå›´ï¼Œè¿‘è‡³ä¸­å¿ƒè·ç¦»çº¦6kpcå¤„ã€‚</li>
<li>eROSITAå…‰è°±åˆ†ææ­ç¤ºçƒ­æ°”ä½“æ¸©åº¦çº¦ä¸º$0.18^{+0.02}_{-0.03}$ keVã€‚</li>
<li>ä¸æ¨¡æ‹Ÿç»“æœå¯¹æ¯”æ˜¾ç¤ºï¼Œå¤§åŠå¾„å¤„çš„å‘å°„åœ¨TNG50æ¨¡å‹ä¸­é¢„æµ‹å¾—è¾ƒå¤šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2320306c36d0614b56c495836c12bfd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb476ac5cb6337b5f98542073d370ac2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0481225357f0ea902cd6a81d5242bac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a0ca083cb2d24c92c27b5de6dcd5d72.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AI-assisted-Early-Detection-of-Pancreatic-Ductal-Adenocarcinoma-on-Contrast-enhanced-CT"><a href="#AI-assisted-Early-Detection-of-Pancreatic-Ductal-Adenocarcinoma-on-Contrast-enhanced-CT" class="headerlink" title="AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on   Contrast-enhanced CT"></a>AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on   Contrast-enhanced CT</h2><p><strong>Authors:Han Liu, Riqiang Gao, Sasa Grbic</strong></p>
<p>Pancreatic ductal adenocarcinoma (PDAC) is one of the most common and aggressive types of pancreatic cancer. However, due to the lack of early and disease-specific symptoms, most patients with PDAC are diagnosed at an advanced disease stage. Consequently, early PDAC detection is crucial for improving patientsâ€™ quality of life and expanding treatment options. In this work, we develop a coarse-to-fine approach to detect PDAC on contrast-enhanced CT scans. First, we localize and crop the region of interest from the low-resolution images, and then segment the PDAC-related structures at a finer scale. Additionally, we introduce two strategies to further boost detection performance: (1) a data-splitting strategy for model ensembling, and (2) a customized post-processing function. We participated in the PANORAMA challenge and ranked 1st place for PDAC detection with an AUROC of 0.9263 and an AP of 0.7243. Our code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/han-liu/PDAC_detection">https://github.com/han-liu/PDAC_detection</a>. </p>
<blockquote>
<p>èƒ°è…ºç™Œå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰æ˜¯èƒ°è…ºç™Œæœ€å¸¸è§ä¸”æœ€å…·æœ‰ä¾µè¢­æ€§çš„ç±»å‹ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œç”±äºæ—©æœŸå’Œç–¾ç—…ç‰¹å¼‚æ€§ç—‡çŠ¶ç¼ºä¹ï¼Œå¤§å¤šæ•°PDACæ‚£è€…åœ¨ç–¾ç—…è¿›å±•é˜¶æ®µæ‰è¢«è¯Šæ–­å‡ºæ¥ã€‚å› æ­¤ï¼Œæ—©æœŸå‘ç°PDACå¯¹äºæ”¹å–„æ‚£è€…ç”Ÿæ´»è´¨é‡å’Œæ‰©å¤§æ²»ç–—é€‰æ‹©è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„PDACæ£€æµ‹æ–¹æ³•æ¥æ£€æµ‹å¢å¼ºCTæ‰«æä¸­çš„èƒ°è…ºç™Œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»ä½åˆ†è¾¨ç‡å›¾åƒä¸­å®šä½å’Œè£å‰ªæ„Ÿå…´è¶£åŒºåŸŸï¼Œç„¶ååœ¨æ›´ç²¾ç»†çš„å°ºåº¦ä¸Šåˆ†å‰²ä¸PDACç›¸å…³çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§ç­–ç•¥æ¥è¿›ä¸€æ­¥æé«˜æ£€æµ‹æ€§èƒ½ï¼šï¼ˆ1ï¼‰ç”¨äºæ¨¡å‹é›†æˆçš„æ•°æ®æ‹†åˆ†ç­–ç•¥ï¼Œï¼ˆ2ï¼‰å®šåˆ¶çš„åå¤„ç†åŠŸèƒ½ã€‚æˆ‘ä»¬å‚åŠ äº†PANORAMAæŒ‘æˆ˜èµ›ï¼Œåœ¨PDACæ£€æµ‹æ–¹é¢è·å¾—ç¬¬ä¸€åï¼ŒAUROCä¸º0.9263ï¼ŒAPä¸º0.7243ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/han-liu/PDAC_detection%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/han-liu/PDAC_detectionä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10068v1">PDF</a> 1st place in the PANORAMA Challenge (Team DTI)</p>
<p><strong>Summary</strong><br>èƒ°è…ºå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰æ˜¯èƒ°è…ºç™Œä¸­æœ€å¸¸è§ä¸”æœ€å…·æœ‰ä¾µè¢­æ€§çš„ç±»å‹ä¹‹ä¸€ã€‚ç”±äºæ—©æœŸå’Œç‰¹å®šç–¾ç—…ç—‡çŠ¶ç¼ºä¹ï¼Œå¤§å¤šæ•°PDACæ‚£è€…åœ¨ç–¾ç—…è¿›å±•åˆ°æ™šæœŸæ—¶æ‰è¢«è¯Šæ–­å‡ºæ¥ã€‚å› æ­¤ï¼Œæ—©æœŸå‘ç°PDACå¯¹äºæ”¹å–„æ‚£è€…ç”Ÿæ´»è´¨é‡å’Œæ‰©å¤§æ²»ç–—é€‰æ‹©è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„æ–¹æ³•ï¼Œåœ¨å¢å¼ºCTæ‰«æä¸­æ£€æµ‹PDACã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»ä½åˆ†è¾¨ç‡å›¾åƒä¸­å®šä½å¹¶è£å‰ªæ„Ÿå…´è¶£åŒºåŸŸï¼Œç„¶ååœ¨æ›´ç²¾ç»†çš„å°ºåº¦ä¸Šåˆ†å‰²ä¸PDACç›¸å…³çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ç§ç­–ç•¥æ¥è¿›ä¸€æ­¥æé«˜æ£€æµ‹æ€§èƒ½ï¼šä¸€æ˜¯æ¨¡å‹é›†æˆçš„æ•°æ®æ‹†åˆ†ç­–ç•¥ï¼ŒäºŒæ˜¯å®šåˆ¶çš„åå¤„ç†åŠŸèƒ½ã€‚æˆ‘ä»¬å‚åŠ äº†PANORAMAæŒ‘æˆ˜èµ›ï¼Œåœ¨PDACæ£€æµ‹æ–¹é¢è·å¾—ç¬¬ä¸€åï¼ŒAUROCä¸º0.9263ï¼ŒAPä¸º0.7243ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨å…¬å¼€è®¿é—®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/han-liu/PDAC_detection">å…¬å¼€é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ°è…ºå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰æ˜¯èƒ°è…ºç™Œä¸­å¸¸è§ä¸”å…·æœ‰ä¾µè¢­æ€§çš„ç±»å‹ï¼Œæ—©æœŸå‘ç°å¯¹æ”¹å–„æ‚£è€…ç”Ÿæ´»è´¨é‡å’Œæ‰©å¤§æ²»ç–—é€‰æ‹©è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç”±ç²—åˆ°ç²¾çš„æ–¹æ³•æ£€æµ‹PDACï¼Œå…ˆåœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸­å®šä½å¹¶è£å‰ªæ„Ÿå…´è¶£åŒºåŸŸï¼Œå†åœ¨æ›´ç²¾ç»†çš„å°ºåº¦ä¸Šåˆ†å‰²ç›¸å…³ç»“æ„ã€‚</li>
<li>ä¸ºæé«˜æ£€æµ‹æ€§èƒ½ï¼Œå¼•å…¥äº†æ¨¡å‹é›†æˆçš„æ•°æ®æ‹†åˆ†ç­–ç•¥å’Œå®šåˆ¶çš„åå¤„ç†åŠŸèƒ½ã€‚</li>
<li>åœ¨PANORAMAæŒ‘æˆ˜èµ›ä¸­ï¼ŒPDACæ£€æµ‹è·å¾—ç¬¬ä¸€åï¼Œæ˜¾ç¤ºå‡ºæ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„ä»£ç å’Œæ¨¡å‹å¯ä¾›å…¬å¼€è®¿é—®ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œå­¦ä¹ ã€‚</li>
<li>AUROCï¼ˆArea Under the Receiver Operating Characteristic Curveï¼‰ä¸º0.9263ï¼Œè¡¨æ˜æ¨¡å‹çš„é«˜å‡†ç¡®æ€§å’Œè‰¯å¥½çš„è¯Šæ–­èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94167e51a20f6a8e7c59a479c37ef244.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49a2ffabaded91416dd7a1efa0754943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e07c6c9695662e8841375de05a3bbc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62d448d16cf04bb54473cb20f5e239c1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="QuickDraw-Fast-Visualization-Analysis-and-Active-Learning-for-Medical-Image-Segmentation"><a href="#QuickDraw-Fast-Visualization-Analysis-and-Active-Learning-for-Medical-Image-Segmentation" class="headerlink" title="QuickDraw: Fast Visualization, Analysis and Active Learning for Medical   Image Segmentation"></a>QuickDraw: Fast Visualization, Analysis and Active Learning for Medical   Image Segmentation</h2><p><strong>Authors:Daniel Syomichev, Padmini Gopinath, Guang-Lin Wei, Eric Chang, Ian Gordon, Amanuel Seifu, Rahul Pemmaraju, Neehar Peri, James Purtilo</strong></p>
<p>Analyzing CT scans, MRIs and X-rays is pivotal in diagnosing and treating diseases. However, detecting and identifying abnormalities from such medical images is a time-intensive process that requires expert analysis and is prone to interobserver variability. To mitigate such issues, machine learning-based models have been introduced to automate and significantly reduce the cost of image segmentation. Despite significant advances in medical image analysis in recent years, many of the latest models are never applied in clinical settings because state-of-the-art models do not easily interface with existing medical image viewers. To address these limitations, we propose QuickDraw, an open-source framework for medical image visualization and analysis that allows users to upload DICOM images and run off-the-shelf models to generate 3D segmentation masks. In addition, our tool allows users to edit, export, and evaluate segmentation masks to iteratively improve state-of-the-art models through active learning. In this paper, we detail the design of our tool and present survey results that highlight the usability of our software. Notably, we find that QuickDraw reduces the time to manually segment a CT scan from four hours to six minutes and reduces machine learning-assisted segmentation time by 10% compared to prior work. Our code and documentation are available at <a target="_blank" rel="noopener" href="https://github.com/qd-seg/quickdraw">https://github.com/qd-seg/quickdraw</a> </p>
<blockquote>
<p>åˆ†æCTæ‰«æã€MRIå’ŒXå…‰å°„çº¿å¯¹ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä»è¿™æ ·çš„åŒ»å­¦å›¾åƒä¸­æ£€æµ‹å’Œè¯†åˆ«å¼‚å¸¸æ˜¯ä¸€ä¸ªè€—æ—¶çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸“å®¶åˆ†æï¼Œå¹¶ä¸”å®¹æ˜“å—è§‚å¯Ÿè€…ä¹‹é—´çš„å·®å¼‚å½±å“ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œå·²ç»å¼•å…¥äº†åŸºäºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è‡ªåŠ¨åŒ–å›¾åƒåˆ†å‰²æŠ€æœ¯æ¥æ˜¾è‘—é™ä½æˆæœ¬ã€‚å°½ç®¡è¿‘å¹´æ¥åœ¨åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è®¸å¤šæœ€æ–°æ¨¡å‹ä»æœªåº”ç”¨äºä¸´åºŠç¯å¢ƒï¼Œå› ä¸ºæœ€å…ˆè¿›çš„æ¨¡å‹ä¸å®¹æ˜“ä¸ç°æœ‰çš„åŒ»å­¦å›¾åƒæŸ¥çœ‹å™¨æ¥å£è¿æ¥ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†QuickDrawï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒå¯è§†åŒ–å’Œåˆ†æçš„å¼€æºæ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·ä¸Šä¼ DICOMå›¾åƒå¹¶è¿è¡Œç°æˆçš„æ¨¡å‹æ¥ç”Ÿæˆ3Dåˆ†å‰²è’™ç‰ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å·¥å…·å…è®¸ç”¨æˆ·ç¼–è¾‘ã€å¯¼å‡ºå’Œè¯„ä¼°åˆ†å‰²è’™ç‰ˆï¼Œé€šè¿‡ä¸»åŠ¨å­¦ä¹ è¿­ä»£æ”¹è¿›æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†å·¥å…·çš„è®¾è®¡ï¼Œå¹¶æä¾›äº†è°ƒæŸ¥ç»“æœï¼Œä»¥çªå‡ºæˆ‘ä»¬è½¯ä»¶çš„å¯ç”¨æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°QuickDrawå°†æ‰‹åŠ¨åˆ†å‰²CTæ‰«æçš„æ—¶é—´ä»å››ä¸ªå°æ—¶å‡å°‘åˆ°å…­åˆ†é’Ÿï¼Œä¸ä½¿ç”¨å…ˆå‰å·¥ä½œç›¸æ¯”å°†æœºå™¨å­¦ä¹ è¾…åŠ©åˆ†å‰²æ—¶é—´å‡å°‘äº†10%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ–‡æ¡£å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/qd-seg/quickdraw%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qd-seg/quickdrawæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09885v1">PDF</a> The first two authors contributed equally. The last three authors   advised equally. This work has been accepted to the International Conference   on Human Computer Interaction (HCII) 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»ç–—å›¾åƒåˆ†æå¯¹äºç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ï¼Œä½†æ‰‹åŠ¨åˆ†æè€—è´¹æ—¶é—´ä¸”å­˜åœ¨è§‚å¯Ÿè€…é—´å·®å¼‚ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹å¯è‡ªåŠ¨åŒ–å›¾åƒåˆ†å‰²é™ä½æˆæœ¬ï¼Œä½†ç°æœ‰æ¨¡å‹éš¾ä»¥ä¸åŒ»ç–—å›¾åƒæŸ¥çœ‹å™¨æ•´åˆã€‚QuickDrawå¼€æºæ¡†æ¶è§£å†³æ­¤é—®é¢˜ï¼Œå…è®¸ä¸Šä¼ DICOMå›¾åƒå¹¶ä½¿ç”¨ç°æˆæ¨¡å‹ç”Ÿæˆ3Dåˆ†å‰²è’™ç‰ˆï¼Œæ”¯æŒç¼–è¾‘ã€å¯¼å‡ºå’Œè¯„ä¼°è’™ç‰ˆï¼Œé€šè¿‡ä¸»åŠ¨å­¦ä¹ è¿­ä»£æ”¹è¿›æ¨¡å‹ã€‚æœ¬ç ”ç©¶è¯¦ç»†é˜è¿°è®¾è®¡å¹¶å±•ç¤ºè½¯ä»¶å¯ç”¨æ€§è°ƒæŸ¥ç»“æœï¼Œå‘ç°QuickDrawå¯å¤§å¹…å‡å°‘æ‰‹åŠ¨åˆ†å‰²CTæ‰«ææ—¶é—´ï¼Œå¹¶é™ä½æœºå™¨å­¦ä¹ è¾…åŠ©åˆ†å‰²æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†æå¯¹ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨è€—æ—¶å’Œè§‚å¯Ÿè€…é—´å·®å¼‚é—®é¢˜ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹å¯è‡ªåŠ¨åŒ–åŒ»ç–—å›¾åƒåˆ†å‰²ï¼Œé™ä½æˆæœ¬ã€‚</li>
<li>ç°æœ‰æ¨¡å‹éš¾ä»¥ä¸åŒ»ç–—å›¾åƒæŸ¥çœ‹å™¨æ•´åˆï¼Œå­˜åœ¨åº”ç”¨éšœç¢ã€‚</li>
<li>QuickDrawæ¡†æ¶è§£å†³æ­¤é—®é¢˜ï¼Œæ”¯æŒDICOMå›¾åƒä¸Šä¼ å’Œç°æˆæ¨¡å‹ä½¿ç”¨ã€‚</li>
<li>QuickDrawå…è®¸ç”Ÿæˆ3Dåˆ†å‰²è’™ç‰ˆï¼Œå¹¶æ”¯æŒç¼–è¾‘ã€å¯¼å‡ºå’Œè¯„ä¼°ã€‚</li>
<li>é€šè¿‡ä¸»åŠ¨å­¦ä¹ ï¼Œå¯è¿­ä»£æ”¹è¿›æ¨¡å‹ã€‚</li>
<li>QuickDrawå¤§å¹…å‡å°‘æ‰‹åŠ¨åˆ†å‰²CTæ‰«ææ—¶é—´ï¼Œå¹¶é™ä½æœºå™¨å­¦ä¹ è¾…åŠ©åˆ†å‰²æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a2fc265a6acbbb09645ca31f62f84e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29995d6108329a0198e90d5283a49b87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff02a4e7c5372661e3f21f9a621899e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14dca65a1e7f8b6d6ec9ff3b61826514.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e153dac156c2d40539d2f187b06bb21.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SeqSAM-Autoregressive-Multiple-Hypothesis-Prediction-for-Medical-Image-Segmentation-using-SAM"><a href="#SeqSAM-Autoregressive-Multiple-Hypothesis-Prediction-for-Medical-Image-Segmentation-using-SAM" class="headerlink" title="SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image   Segmentation using SAM"></a>SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image   Segmentation using SAM</h2><p><strong>Authors:Benjamin Towle, Xin Chen, Ke Zhou</strong></p>
<p>Pre-trained segmentation models are a powerful and flexible tool for segmenting images. Recently, this trend has extended to medical imaging. Yet, often these methods only produce a single prediction for a given image, neglecting inherent uncertainty in medical images, due to unclear object boundaries and errors caused by the annotation tool. Multiple Choice Learning is a technique for generating multiple masks, through multiple learned prediction heads. However, this cannot readily be extended to producing more outputs than its initial pre-training hyperparameters, as the sparse, winner-takes-all loss function makes it easy for one prediction head to become overly dominant, thus not guaranteeing the clinical relevancy of each mask produced. We introduce SeqSAM, a sequential, RNN-inspired approach to generating multiple masks, which uses a bipartite matching loss for ensuring the clinical relevancy of each mask, and can produce an arbitrary number of masks. We show notable improvements in quality of each mask produced across two publicly available datasets. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BenjaminTowle/SeqSAM">https://github.com/BenjaminTowle/SeqSAM</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒåˆ†å‰²æ¨¡å‹æ˜¯åˆ†å‰²å›¾åƒçš„å¼ºå¤§ä¸”çµæ´»çš„å·¥å…·ã€‚æœ€è¿‘ï¼Œè¿™ä¸€è¶‹åŠ¿å·²æ‰©å±•åˆ°åŒ»å­¦æˆåƒé¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºå¯¹è±¡è¾¹ç•Œä¸æ¸…æ™°å’Œæ ‡æ³¨å·¥å…·å¼•èµ·çš„é”™è¯¯ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åªä¸ºç»™å®šçš„å›¾åƒç”Ÿæˆå•ä¸€é¢„æµ‹ç»“æœï¼Œå¿½ç•¥äº†åŒ»å­¦å›¾åƒä¸­çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚å¤šé€‰æ‹©å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡å¤šä¸ªå­¦ä¹ é¢„æµ‹å¤´ç”Ÿæˆå¤šä¸ªæ©è†œçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç”±äºç¨€ç–çš„èƒœè€…å…¨å–æŸå¤±å‡½æ•°ä½¿å¾—ä¸€ä¸ªé¢„æµ‹å¤´å¾ˆå®¹æ˜“è¿‡äºå ä¸»å¯¼åœ°ä½ï¼Œå› æ­¤ä¸èƒ½è½»æ˜“æ‰©å±•åˆ°æ¯”å…¶åˆå§‹é¢„è®­ç»ƒè¶…å‚æ•°æ›´å¤šçš„è¾“å‡ºï¼Œä»è€Œä¸èƒ½ä¿è¯ç”Ÿæˆçš„æ¯ä¸ªæ©è†œçš„åŒ»å­¦ç›¸å…³æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†SeqSAMï¼Œè¿™æ˜¯ä¸€ç§å—RNNå¯å‘çš„ç”Ÿæˆå¤šä¸ªæ©è†œçš„é¡ºåºæ–¹æ³•ï¼Œå®ƒä½¿ç”¨äºŒåˆ†åŒ¹é…æŸå¤±ç¡®ä¿æ¯ä¸ªæ©è†œçš„åŒ»å­¦ç›¸å…³æ€§ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆä»»æ„æ•°é‡çš„æ©è†œã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºç”Ÿæˆçš„æ¯ä¸ªæ©è†œè´¨é‡çš„æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„ä»£ç åœ¨<a target="_blank" rel="noopener" href="https://github.com/BenjaminTowle/SeqSAM%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/BenjaminTowle/SeqSAMä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09797v1">PDF</a> Accepted to ISBI 2025</p>
<p><strong>Summary</strong><br>     é¢„è®­ç»ƒåˆ†å‰²æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä¸­å…·æœ‰å¼ºå¤§å’Œçµæ´»çš„å·¥å…·ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨åŒ»å­¦å½±åƒé¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºåŒ»å­¦å›¾åƒä¸­çš„æ¨¡ç³Šå¯¹è±¡è¾¹ç•Œå’Œæ ‡æ³¨å·¥å…·å¼•èµ·çš„è¯¯å·®ï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åªä¸ºç»™å®šå›¾åƒç”Ÿæˆå•ä¸€é¢„æµ‹ç»“æœï¼Œå¿½ç•¥äº†åŒ»å­¦å›¾åƒä¸­çš„å›ºæœ‰ä¸ç¡®å®šæ€§ã€‚SeqSAMæ˜¯ä¸€ç§ç”Ÿæˆå¤šä¸ªæ©è†œçš„åºè´¯æ–¹æ³•ï¼Œå®ƒå—åˆ°RNNçš„å¯å‘ï¼Œä½¿ç”¨äºŒåˆ†åŒ¹é…æŸå¤±ç¡®ä¿æ¯ä¸ªæ©è†œçš„ä¸´åºŠç›¸å…³æ€§ï¼Œå¹¶èƒ½ç”Ÿæˆä»»æ„æ•°é‡çš„æ©è†œã€‚åœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒSeqSAMç”Ÿæˆçš„æ©è†œè´¨é‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒåˆ†å‰²æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå°¤å…¶åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸã€‚</li>
<li>ç”±äºåŒ»å­¦å›¾åƒçš„æ¨¡ç³Šè¾¹ç•Œå’Œæ ‡æ³¨å·¥å…·è¯¯å·®ï¼Œç°æœ‰çš„æ–¹æ³•å¿½ç•¥äº†å›¾åƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>Multiple Choice Learningæ–¹æ³•é€šè¿‡å¤šä¸ªå­¦ä¹ é¢„æµ‹å¤´ç”Ÿæˆå¤šä¸ªæ©è†œã€‚</li>
<li>SeqSAMæ˜¯ä¸€ç§åºè´¯æ–¹æ³•ï¼Œå—åˆ°RNNå¯å‘ï¼Œå¯ä»¥ç”Ÿæˆä»»æ„æ•°é‡çš„æ©è†œã€‚</li>
<li>SeqSAMä½¿ç”¨äºŒåˆ†åŒ¹é…æŸå¤±ç¡®ä¿æ¯ä¸ªæ©è†œçš„ä¸´åºŠç›¸å…³æ€§ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒSeqSAMç”Ÿæˆçš„æ©è†œè´¨é‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a842b7b8e93e1a005b924e08008a0828.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5138478fec17e89c68e0093bd21fded.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ea1dd199ff88970042516db0665386d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a88a3f0aae6d3e957583c2b18ed5b30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="4D-ACFNet-A-4D-Attention-Mechanism-Based-Prognostic-Framework-for-Colorectal-Cancer-Liver-Metastasis-Integrating-Multimodal-Spatiotemporal-Features"><a href="#4D-ACFNet-A-4D-Attention-Mechanism-Based-Prognostic-Framework-for-Colorectal-Cancer-Liver-Metastasis-Integrating-Multimodal-Spatiotemporal-Features" class="headerlink" title="4D-ACFNet: A 4D Attention Mechanism-Based Prognostic Framework for   Colorectal Cancer Liver Metastasis Integrating Multimodal Spatiotemporal   Features"></a>4D-ACFNet: A 4D Attention Mechanism-Based Prognostic Framework for   Colorectal Cancer Liver Metastasis Integrating Multimodal Spatiotemporal   Features</h2><p><strong>Authors:Zesheng Li, Wei Yang, Yan Su, Yiran Zhu, Yuhan Tang, Haoran Chen, Chengchang Pan, Honggang Qi</strong></p>
<p>Postoperative prognostic prediction for colorectal cancer liver metastasis (CRLM) remains challenging due to tumor heterogeneity, dynamic evolution of the hepatic microenvironment, and insufficient multimodal data fusion. To address these issues, we propose 4D-ACFNet, the first framework that synergistically integrates lightweight spatiotemporal modeling, cross-modal dynamic calibration, and personalized temporal prediction within a unified architecture. Specifically, it incorporates a novel 4D spatiotemporal attention mechanism, which employs spatiotemporal separable convolution (reducing parameter count by 41%) and virtual timestamp encoding to model the interannual evolution patterns of postoperative dynamic processes, such as liver regeneration and steatosis. For cross-modal feature alignment, Transformer layers are integrated to jointly optimize modality alignment loss and disentanglement loss, effectively suppressing scale mismatch and redundant interference in clinical-imaging data. Additionally, we design a dynamic prognostic decision module that generates personalized interannual recurrence risk heatmaps through temporal upsampling and a gated classification head, overcoming the limitations of traditional methods in temporal dynamic modeling and cross-modal alignment. Experiments on 197 CRLM patients demonstrate that the model achieves 100% temporal adjacency accuracy (TAA), with performance significantly surpassing existing approaches. This study establishes the first spatiotemporal modeling paradigm for postoperative dynamic monitoring of CRLM. The proposed framework can be extended to prognostic analysis of multi-cancer metastases, advancing precision surgery from â€œspatial resectionâ€ to â€œspatiotemporal cure.â€ </p>
<blockquote>
<p>å¯¹äºç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰çš„æœ¯åé¢„åé¢„æµ‹ï¼Œç”±äºè‚¿ç˜¤çš„å¼‚è´¨æ€§ã€è‚è„å¾®ç¯å¢ƒçš„åŠ¨æ€æ¼”å˜ä»¥åŠå¤šæ¨¡æ€æ•°æ®èåˆçš„ä¸è¶³ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†4D-ACFNetæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒæ•´åˆäº†è½»é‡çº§æ—¶ç©ºå»ºæ¨¡ã€è·¨æ¨¡æ€åŠ¨æ€æ ¡å‡†å’Œä¸ªæ€§åŒ–æ—¶åºé¢„æµ‹çš„ç»Ÿä¸€æ¶æ„ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒèå…¥äº†ä¸€ç§æ–°é¢–çš„4Dæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œé‡‡ç”¨æ—¶ç©ºå¯åˆ†ç¦»å·ç§¯ï¼ˆå‡å°‘41%çš„å‚æ•°æ•°é‡ï¼‰å’Œè™šæ‹Ÿæ—¶é—´æˆ³ç¼–ç ï¼Œä»¥æ¨¡æ‹Ÿæœ¯ååŠ¨æ€è¿‡ç¨‹çš„å¹´åº¦æ¼”å˜æ¨¡å¼ï¼Œå¦‚è‚è„å†ç”Ÿå’Œè„‚è‚ªè‚å˜æ€§ç­‰ã€‚ä¸ºäº†è¿›è¡Œè·¨æ¨¡æ€ç‰¹å¾å¯¹é½ï¼Œé›†æˆäº†Transformerå±‚ä»¥è”åˆä¼˜åŒ–æ¨¡æ€å¯¹é½æŸå¤±å’Œåˆ†è§£æŸå¤±ï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†ä¸´åºŠå½±åƒæ•°æ®ä¸­å°ºåº¦ä¸åŒ¹é…å’Œå†—ä½™å¹²æ‰°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€é¢„åå†³ç­–æ¨¡å—ï¼Œé€šè¿‡æ—¶åºä¸Šé‡‡æ ·å’Œé—¨æ§åˆ†ç±»å¤´ç”Ÿæˆä¸ªæ€§åŒ–çš„å¹´åº¦å¤å‘é£é™©çƒ­å›¾ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ—¶åºåŠ¨æ€å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢çš„å±€é™æ€§ã€‚åœ¨197åCRLMæ‚£è€…ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„æ—¶é—´é‚»æ¥ç²¾åº¦ï¼ˆTAAï¼‰è¾¾åˆ°100%ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æœ¬ç ”ç©¶ä¸ºCRLMçš„æœ¯ååŠ¨æ€ç›‘æµ‹å»ºç«‹äº†é¦–ä¸ªæ—¶ç©ºå»ºæ¨¡èŒƒå¼ã€‚æ‰€æå‡ºçš„æ¡†æ¶å¯æ‰©å±•åˆ°å¤šç™Œè½¬ç§»çš„é¢„ååˆ†æï¼Œæ¨åŠ¨ç²¾å‡†æ‰‹æœ¯ä»â€œç©ºé—´åˆ‡é™¤â€å‘å±•åˆ°â€œæ—¶ç©ºæ²»æ„ˆâ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09652v1">PDF</a> 8 pages,6 figures,2 tables,submitted to the 33rd ACM International   Conference on Multimedia(ACM MM 2025)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸º4D-ACFNetçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰æœ¯åé¢„åé¢„æµ‹çš„éš¾é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆè½»é‡çº§æ—¶ç©ºå»ºæ¨¡ã€è·¨æ¨¡æ€åŠ¨æ€æ ¡å‡†å’Œä¸ªæ€§åŒ–æ—¶åºé¢„æµ‹ï¼Œå…‹æœäº†è‚¿ç˜¤å¼‚è´¨æ€§ã€è‚è„å¾®ç¯å¢ƒåŠ¨æ€æ¼”å˜å’Œå¤šæ¨¡æ€æ•°æ®èåˆä¸è¶³çš„é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥4Dæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶å’Œè™šæ‹Ÿæ—¶é—´æˆ³ç¼–ç ï¼Œå¯¹æœ¯ååŠ¨æ€è¿‡ç¨‹ï¼ˆå¦‚è‚å†ç”Ÿå’Œè„‚è‚ªå˜æ€§ï¼‰çš„å¹´é™…æ¼”å˜æ¨¡å¼è¿›è¡Œå»ºæ¨¡ã€‚åŒæ—¶ï¼Œé€šè¿‡æ•´åˆTransformerå±‚ä¼˜åŒ–æ¨¡æ€å¯¹é½æŸå¤±å’Œåˆ†ç¦»æŸå¤±ï¼Œæœ‰æ•ˆæŠ‘åˆ¶äº†ä¸´åºŠä¸æˆåƒæ•°æ®ä¸­çš„å°ºåº¦ä¸åŒ¹é…å’Œå†—ä½™å¹²æ‰°ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€é¢„åå†³ç­–æ¨¡å—ï¼Œé€šè¿‡æ—¶åºä¸Šé‡‡æ ·å’Œé—¨æ§åˆ†ç±»å¤´ç”Ÿæˆä¸ªæ€§åŒ–çš„å¹´é™…å¤å‘é£é™©çƒ­å›¾ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ—¶åºåŠ¨æ€å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹é½æ–¹é¢çš„å±€é™æ€§ã€‚åœ¨197åCRLMæ‚£è€…ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹è¾¾åˆ°100%çš„æ—¶é—´é‚»æ¥ç²¾åº¦ï¼ˆTAAï¼‰ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºCRLMçš„æœ¯ååŠ¨æ€ç›‘æµ‹å»ºç«‹äº†é¦–ä¸ªæ—¶ç©ºå»ºæ¨¡èŒƒä¾‹ï¼Œå¹¶ä¸”è¯¥æ¡†æ¶å¯æ‰©å±•åˆ°å¤šç™Œè½¬ç§»çš„é¢„ååˆ†æï¼Œæ¨åŠ¨ç²¾å‡†æ‰‹æœ¯ä»â€œç©ºé—´åˆ‡é™¤â€å‘â€œæ—¶ç©ºæ²»æ„ˆâ€å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>4D-ACFNetæ¡†æ¶è¢«æå‡ºï¼Œæ•´åˆäº†è½»é‡çº§æ—¶ç©ºå»ºæ¨¡ã€è·¨æ¨¡æ€åŠ¨æ€æ ¡å‡†å’Œä¸ªæ€§åŒ–æ—¶åºé¢„æµ‹ã€‚</li>
<li>å¼•å…¥4Dæ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶å’Œè™šæ‹Ÿæ—¶é—´æˆ³ç¼–ç ï¼Œå¯¹æœ¯ååŠ¨æ€è¿‡ç¨‹çš„å¹´é™…æ¼”å˜è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>é€šè¿‡Transformerå±‚ä¼˜åŒ–æ¨¡æ€å¯¹é½ï¼Œå‡å°‘ä¸´åºŠä¸æˆåƒæ•°æ®ä¸­çš„å°ºåº¦ä¸åŒ¹é…å’Œå†—ä½™å¹²æ‰°ã€‚</li>
<li>è®¾è®¡äº†åŠ¨æ€é¢„åå†³ç­–æ¨¡å—ï¼Œç”Ÿæˆä¸ªæ€§åŒ–å¤å‘é£é™©çƒ­å›¾ï¼Œå…‹æœä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨197åCRLMæ‚£è€…ä¸Šè¾¾åˆ°100%çš„æ—¶é—´é‚»æ¥ç²¾åº¦ï¼ˆTAAï¼‰ã€‚</li>
<li>ç ”ç©¶ä¸ºCRLMçš„æœ¯ååŠ¨æ€ç›‘æµ‹å»ºç«‹äº†é¦–ä¸ªæ—¶ç©ºå»ºæ¨¡èŒƒä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc7b24a49ba0b57750032370c7d7c113.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58b0051917daaba9dca26115f8759bb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-648957dc06622f679598234ad416b773.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d84c0544d0ea4bc29c7bf286b99b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed26bc1e0e484ce331e31620a1d66e55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e43078c04670609b37649ab035a1ddc6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-R2D2-Deep-Neural-Network-Series-for-Scalable-Non-Cartesian-Magnetic-Resonance-Imaging"><a href="#The-R2D2-Deep-Neural-Network-Series-for-Scalable-Non-Cartesian-Magnetic-Resonance-Imaging" class="headerlink" title="The R2D2 Deep Neural Network Series for Scalable Non-Cartesian Magnetic   Resonance Imaging"></a>The R2D2 Deep Neural Network Series for Scalable Non-Cartesian Magnetic   Resonance Imaging</h2><p><strong>Authors:Yiwei Chen, Amir Aghabiglou, Shijie Chen, Motahare Torki, Chao Tang, Ruud B. van Heeswijk, Yves Wiaux</strong></p>
<p>We introduce the R2D2 Deep Neural Network (DNN) series paradigm for fast and scalable image reconstruction from highly-accelerated non-Cartesian k-space acquisitions in Magnetic Resonance Imaging (MRI). While unrolled DNN architectures provide a robust image formation approach via data-consistency layers, embedding non-uniform fast Fourier transform operators in a DNN can become impractical to train at large scale, e.g in 2D MRI with a large number of coils, or for higher-dimensional imaging. Plug-and-play approaches that alternate a learned denoiser blind to the measurement setting with a data-consistency step are not affected by this limitation but their highly iterative nature implies slow reconstruction. To address this scalability challenge, we leverage the R2D2 paradigm that was recently introduced to enable ultra-fast reconstruction for large-scale Fourier imaging in radio astronomy. R2D2â€™s reconstruction is formed as a series of residual images iteratively estimated as outputs of DNN modules taking the previous iterationâ€™s data residual as input. The method can be interpreted as a learned version of the Matching Pursuit algorithm. A series of R2D2 DNN modules were sequentially trained in a supervised manner on the fastMRI dataset and validated for 2D multi-coil MRI in simulation and on real data, targeting highly under-sampled radial k-space sampling. Results suggest that a series with only few DNNs achieves superior reconstruction quality over its unrolled incarnation R2D2-Net (whose training is also much less scalable), and over the state-of-the-art diffusion-based â€œDecomposed Diffusion Samplerâ€ approach (also characterised by a slower reconstruction process). </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†R2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—èŒƒå¼ï¼Œç”¨äºä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­é«˜åº¦åŠ é€Ÿçš„éç¬›å¡å°”kç©ºé—´é‡‡é›†å¿«é€Ÿä¸”å¯æ‰©å±•çš„å›¾åƒé‡å»ºã€‚è™½ç„¶å±•å¼€çš„DNNæ¶æ„é€šè¿‡æ•°æ®ä¸€è‡´æ€§å±‚æä¾›äº†ç¨³å¥çš„å›¾åƒå½¢æˆæ–¹æ³•ï¼Œä½†åœ¨å¤§è§„æ¨¡æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨å…·æœ‰å¤§é‡çº¿åœˆçš„äºŒç»´MRIæˆ–æ›´é«˜ç»´æˆåƒä¸­ï¼Œåœ¨DNNä¸­åµŒå…¥éå‡åŒ€å¿«é€Ÿå‚…é‡Œç«‹å¶å˜æ¢ç®—å­è¿›è¡Œè®­ç»ƒå¯èƒ½ä¸åˆ‡å®é™…ã€‚äº¤æ›¿ä½¿ç”¨å¯¹æµ‹é‡è®¾ç½®ç›²ç›®çš„å­¦ä¹ å»å™ªå™¨å’Œæ•°æ®ä¸€è‡´æ€§æ­¥éª¤çš„å³æ’å³ç”¨æ–¹æ³•ä¸å—æ­¤é™åˆ¶ï¼Œä½†å®ƒä»¬çš„é«˜åº¦è¿­ä»£æ€§è´¨æ„å‘³ç€é‡å»ºé€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åˆ©ç”¨R2D2èŒƒå¼ï¼Œè¯¥èŒƒå¼æœ€è¿‘è¢«å¼•å…¥åˆ°å¤©æ–‡æ— çº¿ç”µä¸­çš„å¤§è§„æ¨¡å‚…é‡Œå¶æˆåƒä»¥è¿›è¡Œè¶…å¿«é€Ÿé‡å»ºã€‚R2D2çš„é‡å»ºæ˜¯ç”±ä¸€ç³»åˆ—æ®‹å·®å›¾åƒç»„æˆï¼Œè¿™äº›æ®‹å·®å›¾åƒæ˜¯DNNæ¨¡å—çš„è¾“å‡ºï¼Œä»¥ä¹‹å‰çš„è¿­ä»£æ•°æ®æ®‹å·®ä½œä¸ºè¾“å…¥è€Œè¿­ä»£ä¼°è®¡å¾—å‡ºã€‚è¯¥æ–¹æ³•å¯è§£é‡Šä¸ºåŒ¹é…è¿½è¸ªç®—æ³•çš„å­¦ä¹ ç‰ˆæœ¬ã€‚R2D2 DNNæ¨¡å—ç³»åˆ—ä»¥ç›‘ç£æ–¹å¼åœ¨fastMRIæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é’ˆå¯¹é«˜åº¦æ¬ é‡‡æ ·çš„å¾„å‘kç©ºé—´é‡‡æ ·åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šè¿›è¡Œ2Då¤šçº¿åœˆMRIéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œåªæœ‰å°‘æ•°DNNçš„ç³»åˆ—è¾¾åˆ°äº†ä¼˜äºå…¶å±•å¼€çš„R2D2-Netï¼ˆå…¶è®­ç»ƒä¹Ÿä¸å¤ªå¯æ‰©å±•ï¼‰ä»¥åŠä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„â€œåˆ†è§£æ‰©æ•£é‡‡æ ·å™¨â€æ–¹æ³•çš„é‡å»ºè´¨é‡ï¼ˆè¯¥æ–¹æ³•ä¹Ÿä»¥å…¶è¾ƒæ…¢çš„é‡å»ºè¿‡ç¨‹ä¸ºç‰¹å¾ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09559v2">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºR2D2æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ç³»åˆ—çš„èŒƒå¼ï¼Œç”¨äºä»é«˜åº¦åŠ é€Ÿçš„éç¬›å¡å°”kç©ºé—´é‡‡é›†ä¸­è¿›è¡Œå¿«é€Ÿä¸”å¯ä¼¸ç¼©çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒé‡å»ºã€‚è¯¥èŒƒå¼è§£å†³äº†åœ¨å…·æœ‰å¤§é‡çº¿åœˆçš„äºŒç»´MRIæˆ–æ›´é«˜ç»´åº¦æˆåƒä¸­ï¼Œå°†éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ç®—å­åµŒå…¥DNNä¸­è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒçš„ä¸å®ç”¨æ€§ã€‚é€šè¿‡åˆ©ç”¨R2D2èŒƒå¼ï¼Œå®ç°äº†å¿«é€Ÿé‡å»ºï¼Œè¯¥èŒƒå¼çš„é‡å»ºå½¢å¼æ˜¯ä¸€ç³»åˆ—æ®‹å·®å›¾åƒçš„è¿­ä»£ä¼°è®¡ï¼Œè¿™äº›ä¼°è®¡ä½œä¸ºDNNæ¨¡å—çš„è¾“å‡ºæ¥è·å¾—å‰ä¸€æ¬¡è¿­ä»£çš„å‰©ä½™æ•°æ®ä½œä¸ºè¾“å…¥ã€‚ä¸€ç³»åˆ—R2D2 DNNæ¨¡å—åœ¨fastMRIæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šå¯¹äºŒç»´å¤šçº¿åœˆMRIè¿›è¡ŒéªŒè¯ï¼Œé’ˆå¯¹é«˜åº¦æ¬ é‡‡æ ·çš„å¾„å‘kç©ºé—´é‡‡æ ·ã€‚ç»“æœè¡¨æ˜ï¼Œä»…åŒ…å«å°‘æ•°DNNçš„ç³»åˆ—å®ç°ä¼˜äºå…¶æœªå±•å¼€çš„R2D2-Netç‰ˆæœ¬ï¼ˆå…¶è®­ç»ƒä¹Ÿä¸å¤ªå¯æ‰©å±•ï¼‰ï¼Œå¹¶ä¸”ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„â€œåˆ†è§£æ‰©æ•£é‡‡æ ·å™¨â€æ–¹æ³•ï¼ˆå…¶ç‰¹å¾åœ¨äºè¾ƒæ…¢çš„é‡å»ºè¿‡ç¨‹ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥R2D2 Deep Neural Network (DNN)ç³»åˆ—èŒƒå¼ï¼Œç”¨äºå¿«é€Ÿå’Œå¯ä¼¸ç¼©çš„å›¾åƒé‡å»ºã€‚</li>
<li>è§£å†³åœ¨å¤§å‹å‚…é‡Œå¶æˆåƒä¸­å°†éå‡åŒ€å¿«é€Ÿå‚…é‡Œå¶å˜æ¢åµŒå…¥DNNè¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒçš„ä¸å®ç”¨æ€§é—®é¢˜ã€‚</li>
<li>R2D2èŒƒå¼çš„é‡å»ºæ˜¯ä¸€ç³»åˆ—åŸºäºDNNæ¨¡å—è¾“å‡ºçš„æ®‹å·®å›¾åƒçš„è¿­ä»£ä¼°è®¡ã€‚</li>
<li>R2D2èŒƒå¼å¯ä»¥ç†è§£ä¸ºåŒ¹é…è¿½è¸ªç®—æ³•çš„å­¦ä¹ ç‰ˆæœ¬ã€‚</li>
<li>R2D2 DNNæ¨¡å—åœ¨fastMRIæ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒã€‚</li>
<li>R2D2ç³»åˆ—åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„äºŒç»´å¤šçº¿åœˆMRIéªŒè¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09559">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f657dc7d084b79da07a140cf5275266a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a524737d211f3be291033c04129e9aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6cbd605fb9510cbe541ee99174935e3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14662143e51ba085df24c074e745b545.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NimbleReg-A-light-weight-deep-learning-framework-for-diffeomorphic-image-registration"><a href="#NimbleReg-A-light-weight-deep-learning-framework-for-diffeomorphic-image-registration" class="headerlink" title="NimbleReg: A light-weight deep-learning framework for diffeomorphic   image registration"></a>NimbleReg: A light-weight deep-learning framework for diffeomorphic   image registration</h2><p><strong>Authors:Antoine Legouhy, Ross Callaghan, Nolah Mazet, Vivien Julienne, Hojjat Azadbakht, Hui Zhang</strong></p>
<p>This paper presents NimbleReg, a light-weight deep-learning (DL) framework for diffeomorphic image registration leveraging surface representation of multiple segmented anatomical regions. Deep learning has revolutionized image registration but most methods typically rely on cumbersome gridded representations, leading to hardware-intensive models. Reliable fine-grained segmentations, that are now accessible at low cost, are often used to guide the alignment. Light-weight methods representing segmentations in terms of boundary surfaces have been proposed, but they lack mechanism to support the fusion of multiple regional mappings into an overall diffeomorphic transformation. Building on these advances, we propose a DL registration method capable of aligning surfaces from multiple segmented regions to generate an overall diffeomorphic transformation for the whole ambient space. The proposed model is light-weight thanks to a PointNet backbone. Diffeomoprhic properties are guaranteed by taking advantage of the stationary velocity field parametrization of diffeomorphisms. We demonstrate that this approach achieves alignment comparable to state-of-the-art DL-based registration techniques that consume images. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†NimbleRegï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¡†æ¶ï¼Œç”¨äºåˆ©ç”¨å¤šä¸ªåˆ†å‰²è§£å‰–åŒºåŸŸçš„è¡¨é¢è¡¨ç¤ºè¿›è¡Œå¾®åˆ†åŒèƒšå›¾åƒé…å‡†ã€‚æ·±åº¦å­¦ä¹ å·²ç»å½»åº•æ”¹å˜äº†å›¾åƒé…å‡†ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•é€šå¸¸ä¾èµ–äºç¹ççš„ç½‘æ ¼è¡¨ç¤ºï¼Œå¯¼è‡´ç¡¬ä»¶å¯†é›†å‹æ¨¡å‹ã€‚ç°åœ¨ä½æˆæœ¬å³å¯è·å¾—çš„å¯é ç²¾ç»†åˆ†å‰²é€šå¸¸ç”¨äºå¼•å¯¼å¯¹é½ã€‚è™½ç„¶å·²æœ‰è½»é‡çº§æ–¹æ³•ç”¨è¾¹ç•Œè¡¨é¢è¡¨ç¤ºåˆ†å‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹å°†å¤šä¸ªåŒºåŸŸæ˜ å°„èåˆåˆ°æ•´ä½“å¾®åˆ†åŒèƒšå˜æ¢ä¸­çš„æœºåˆ¶ã€‚åŸºäºè¿™äº›è¿›å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦å­¦ä¹ é…å‡†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå°†å¤šä¸ªåˆ†å‰²åŒºåŸŸçš„è¡¨é¢å¯¹é½ï¼Œä»¥ç”Ÿæˆæ•´ä¸ªç¯å¢ƒç©ºé—´çš„æ•´ä½“å¾®åˆ†åŒèƒšå˜æ¢ã€‚æ‰€ææ¨¡å‹å¾—ç›ŠäºPointNetéª¨å¹²ç½‘è€Œå®ç°è½»é‡åŒ–ã€‚é€šè¿‡åˆ©ç”¨å¾®åˆ†åŒèƒšçš„ç¨³æ€é€Ÿåº¦åœºå‚æ•°åŒ–ï¼Œä¿è¯äº†å¾®åˆ†åŒèƒšå±æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•å®ç°çš„é…å‡†æ•ˆæœä¸æ¶ˆè€—å›¾åƒçš„æœ€æ–°æ·±åº¦å­¦ä¹ é…å‡†æŠ€æœ¯ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07768v1">PDF</a> submitted in MICCAI 2025 conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NimbleRegï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è½»é‡åŒ–å›¾åƒæ³¨å†Œæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤šä¸ªåˆ†å‰²è§£å‰–åŒºåŸŸçš„è¡¨é¢è¡¨ç¤ºæ¥è¿›è¡Œå¾®åˆ†åŒèƒšå›¾åƒæ³¨å†Œã€‚è¯¥æ–¹æ³•åŸºäºPointNetéª¨å¹²ç½‘æ„å»ºè½»é‡åŒ–æ¨¡å‹ï¼Œåˆ©ç”¨è¡¨é¢è¡¨ç¤ºæ³•èåˆå¤šä¸ªåŒºåŸŸæ˜ å°„ï¼Œç”Ÿæˆæ•´ä¸ªç¯å¢ƒçš„å¾®åˆ†åŒèƒšå˜æ¢ã€‚è¿™ç§æ–¹æ³•åœ¨ä¿è¯å¯¹é½è´¨é‡çš„åŒæ—¶ï¼Œæ›´åŠ è½»é‡åŒ–å’Œé«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NimbleRegæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒæ³¨å†Œæ¡†æ¶ï¼Œé€‚ç”¨äºå¾®åˆ†åŒèƒšå›¾åƒæ³¨å†Œã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¤šä¸ªåˆ†å‰²è§£å‰–åŒºåŸŸçš„è¡¨é¢è¡¨ç¤ºè¿›è¡Œå›¾åƒæ³¨å†Œã€‚</li>
<li>NimbleRegé‡‡ç”¨è½»é‡åŒ–æ¨¡å‹è®¾è®¡ï¼ŒåŸºäºPointNetéª¨å¹²ç½‘ã€‚</li>
<li>è¯¥æ–¹æ³•èåˆäº†å¤šä¸ªåŒºåŸŸæ˜ å°„ï¼Œç”Ÿæˆäº†æ•´ä¸ªç¯å¢ƒçš„å¾®åˆ†åŒèƒšå˜æ¢ã€‚</li>
<li>å¾®åˆ†åŒèƒšå±æ€§é€šè¿‡åˆ©ç”¨å¾®åˆ†åŒèƒšçš„ç¨³æ€é€Ÿåº¦åœºå‚æ•°åŒ–æ¥ä¿è¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¸åŸºäºå›¾åƒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸å½“çš„å¯¹é½æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b835111defcab75e8ab786d0c1f56966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea8bf9f442dc75ee0545be7cd6f48ff1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-645165c4ca94276e5e5023ce26f4ea8d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SegResMamba-An-Efficient-Architecture-for-3D-Medical-Image-Segmentation"><a href="#SegResMamba-An-Efficient-Architecture-for-3D-Medical-Image-Segmentation" class="headerlink" title="SegResMamba: An Efficient Architecture for 3D Medical Image Segmentation"></a>SegResMamba: An Efficient Architecture for 3D Medical Image Segmentation</h2><p><strong>Authors:Badhan Kumar Das, Ajay Singh, Saahil Islam, Gengyan Zhao, Andreas Maier</strong></p>
<p>The Transformer architecture has opened a new paradigm in the domain of deep learning with its ability to model long-range dependencies and capture global context and has outpaced the traditional Convolution Neural Networks (CNNs) in many aspects. However, applying Transformer models to 3D medical image datasets presents significant challenges due to their high training time, and memory requirements, which not only hinder scalability but also contribute to elevated CO$_2$ footprint. This has led to an exploration of alternative models that can maintain or even improve performance while being more efficient and environmentally sustainable. Recent advancements in Structured State Space Models (SSMs) effectively address some of the inherent limitations of Transformers, particularly their high memory and computational demands. Inspired by these advancements, we propose an efficient 3D segmentation model for medical imaging called SegResMamba, designed to reduce computation complexity, memory usage, training time, and environmental impact while maintaining high performance. Our model uses less than half the memory during training compared to other state-of-the-art (SOTA) architectures, achieving comparable performance with significantly reduced resource demands. </p>
<blockquote>
<p>Transformeræ¶æ„å‡­å€Ÿå…¶èƒ½å¤Ÿå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå¼€å¯äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œå¹¶ä¸”åœ¨è®¸å¤šæ–¹é¢éƒ½è¶…è¶Šäº†ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚ç„¶è€Œï¼Œå°†Transformeræ¨¡å‹åº”ç”¨äº3DåŒ»å­¦å›¾åƒæ•°æ®é›†å­˜åœ¨å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶è®­ç»ƒæ—¶é—´é•¿å’Œå†…å­˜è¦æ±‚é«˜çš„ç‰¹ç‚¹ä¸ä»…é˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œè¿˜å¯¼è‡´äº†COâ‚‚æ’æ”¾è¶³è¿¹å¢åŠ ã€‚è¿™ä¿ƒä½¿äººä»¬æ¢ç´¢èƒ½å¤Ÿç»´æŒç”šè‡³æé«˜æ€§èƒ½çš„åŒæ—¶æ›´é«˜æ•ˆä¸”ç¯ä¿çš„æ›¿ä»£æ¨¡å‹ã€‚ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„æœ€æ–°è¿›å±•æœ‰æ•ˆåœ°è§£å†³äº†Transformerçš„ä¸€äº›å›ºæœ‰å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚å—è¿™äº›è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„åŒ»å­¦æˆåƒ3Dåˆ†å‰²æ¨¡å‹SegResMambaï¼Œæ—¨åœ¨é™ä½è®¡ç®—å¤æ‚åº¦ã€å†…å­˜ä½¿ç”¨ã€è®­ç»ƒæ—¶é—´ä»¥åŠå¯¹ç¯å¢ƒçš„å½±å“ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚ä¸å…¶ä»–æœ€å…ˆè¿›çš„æ¶æ„ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å†…å­˜å‡å°‘äº†ä¸€åŠä»¥ä¸Šï¼Œå¹¶ä»¥æ˜¾è‘—é™ä½çš„èµ„æºéœ€æ±‚å®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07766v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Transformeræ¶æ„åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸä¸­çš„æ–°èŒƒå¼ï¼Œå…¶èƒ½å¤Ÿå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»å¹¶æ•è·å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå·²åœ¨è®¸å¤šæ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€‚ç„¶è€Œï¼Œå°†Transformeræ¨¡å‹åº”ç”¨äº3DåŒ»å­¦å›¾åƒæ•°æ®é›†æ—¶ï¼Œå­˜åœ¨è®­ç»ƒæ—¶é—´é•¿ã€å†…å­˜è¦æ±‚é«˜ç­‰æŒ‘æˆ˜ï¼Œè¿™ä¸ä»…é˜»ç¢äº†å¯æ‰©å±•æ€§ï¼Œè¿˜å¯¼è‡´äº†äºŒæ°§åŒ–ç¢³è¶³è¿¹çš„å¢åŠ ã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢èƒ½å¤Ÿç»´æŒæˆ–æé«˜æ€§èƒ½åŒæ—¶æ›´å…·æ•ˆç‡å’Œç¯ä¿çš„æ›¿ä»£æ¨¡å‹ã€‚æœ€è¿‘çš„ç»“æ„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„è¿›æ­¥æœ‰æ•ˆåœ°è§£å†³äº†Transformerçš„ä¸€äº›å›ºæœ‰å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶é«˜å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚å—è¿™äº›è¿›æ­¥çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŒ»å­¦æˆåƒ3Dåˆ†å‰²æ¨¡å‹SegResMambaï¼Œæ—¨åœ¨é™ä½è®¡ç®—å¤æ‚åº¦ã€å†…å­˜ä½¿ç”¨ã€è®­ç»ƒæ—¶é—´å’Œç¯å¢ƒå½±å“ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å†…å­˜ä¸åˆ°å…¶ä»–å…ˆè¿›æ¶æ„çš„ä¸€åŠï¼ŒåŒæ—¶å®ç°äº†ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—é™ä½äº†èµ„æºéœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¶æ„å…·æœ‰å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å’Œæ•è·å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„èŒƒå¼ã€‚</li>
<li>å°†Transformeræ¨¡å‹åº”ç”¨äº3DåŒ»å­¦å›¾åƒæ•°æ®é›†æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒæ—¶é—´é•¿ã€å†…å­˜è¦æ±‚é«˜ã€‚</li>
<li>è¿™äº›æŒ‘æˆ˜å¯¼è‡´äºŒæ°§åŒ–ç¢³è¶³è¿¹å¢åŠ ï¼Œéœ€è¦æ¢ç´¢æ›´é«˜æ•ˆã€ç¯ä¿çš„æ›¿ä»£æ¨¡å‹ã€‚</li>
<li>ç»“æ„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„æœ€æ–°è¿›å±•è§£å†³äº†Transformerçš„ä¸€äº›å±€é™æ€§ã€‚</li>
<li>SegResMambaæ¨¡å‹æ˜¯ä¸€ç§é«˜æ•ˆçš„åŒ»å­¦æˆåƒ3Dåˆ†å‰²æ¨¡å‹ï¼Œæ—¨åœ¨é™ä½è®¡ç®—å¤æ‚åº¦ã€å†…å­˜ä½¿ç”¨ã€è®­ç»ƒæ—¶é—´å’Œç¯å¢ƒå½±å“ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>SegResMambaæ¨¡å‹åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å†…å­˜æ˜¾è‘—å°‘äºå…¶ä»–å…ˆè¿›æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c81f3dc1fef4e63315cb3cd4ba1b527c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a20ebb1f4f60e4d8b7e265e24976279.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9dfb8387d746ff1710209a04e8cb73e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f1794c118b8464a3873a976fad1b9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f21858d2aa3a044f1f47f9a1f5aeebc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Interactive-Medical-Image-Analysis-with-Concept-based-Similarity-Reasoning"><a href="#Interactive-Medical-Image-Analysis-with-Concept-based-Similarity-Reasoning" class="headerlink" title="Interactive Medical Image Analysis with Concept-based Similarity   Reasoning"></a>Interactive Medical Image Analysis with Concept-based Similarity   Reasoning</h2><p><strong>Authors:Ta Duc Huy, Sen Kim Tran, Phan Nguyen, Nguyen Hoang Tran, Tran Bao Sam, Anton van den Hengel, Zhibin Liao, Johan W. Verjans, Minh-Son To, Vu Minh Hieu Phan</strong></p>
<p>The ability to interpret and intervene model decisions is important for the adoption of computer-aided diagnosis methods in clinical workflows. Recent concept-based methods link the model predictions with interpretable concepts and modify their activation scores to interact with the model. However, these concepts are at the image level, which hinders the model from pinpointing the exact patches the concepts are activated. Alternatively, prototype-based methods learn representations from training image patches and compare these with test image patches, using the similarity scores for final class prediction. However, interpreting the underlying concepts of these patches can be challenging and often necessitates post-hoc guesswork. To address this issue, this paper introduces the novel Concept-based Similarity Reasoning network (CSR), which offers (i) patch-level prototype with intrinsic concept interpretation, and (ii) spatial interactivity. First, the proposed CSR provides localized explanation by grounding prototypes of each concept on image regions. Second, our model introduces novel spatial-level interaction, allowing doctors to engage directly with specific image areas, making it an intuitive and transparent tool for medical imaging. CSR improves upon prior state-of-the-art interpretable methods by up to 4.5% across three biomedical datasets. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/tadeephuy/InteractCSR">https://github.com/tadeephuy/InteractCSR</a>. </p>
<blockquote>
<p>è§£è¯»å’Œå¹²é¢„æ¨¡å‹å†³ç­–çš„èƒ½åŠ›å¯¹äºåœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­é‡‡ç”¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ€è¿‘çš„æ¦‚å¿µæ–¹æ³•å°†æ¨¡å‹é¢„æµ‹ä¸å¯è§£é‡Šçš„æ¦‚å¿µè”ç³»èµ·æ¥ï¼Œå¹¶ä¿®æ”¹å…¶æ¿€æ´»åˆ†æ•°ä»¥ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œè¿™äº›æ¦‚å¿µå­˜åœ¨äºå›¾åƒå±‚é¢ï¼Œé˜»ç¢äº†æ¨¡å‹ç¡®å®šæ¦‚å¿µæ¿€æ´»çš„ç¡®åˆ‡åŒºåŸŸã€‚åŸºäºåŸå‹çš„æ–¹æ³•åˆ™ä»è®­ç»ƒå›¾åƒå—ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸æµ‹è¯•å›¾åƒå—è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨ç›¸ä¼¼åº¦å¾—åˆ†è¿›è¡Œæœ€ç»ˆç±»åˆ«é¢„æµ‹ã€‚ç„¶è€Œï¼Œè§£é‡Šè¿™äº›å›¾åƒå—çš„æ½œåœ¨æ¦‚å¿µå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œäº‹åçŒœæµ‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†æ–°å‹çš„æ¦‚å¿µç›¸ä¼¼æ€§æ¨ç†ç½‘ç»œï¼ˆCSRï¼‰ï¼Œå®ƒæä¾›äº†ï¼ˆiï¼‰å›¾åƒå—çº§åˆ«çš„åŸå‹å’Œå†…åœ¨æ¦‚å¿µè§£é‡Šï¼Œï¼ˆiiï¼‰ç©ºé—´äº¤äº’æ€§ã€‚é¦–å…ˆï¼Œæ‰€æå‡ºçš„CSRé€šè¿‡åœ¨å›¾åƒåŒºåŸŸä¸Šä¸ºæ¯ä¸ªæ¦‚å¿µæä¾›åŸå‹æ¥æä¾›å±€éƒ¨è§£é‡Šã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¼•å…¥äº†æ–°å‹çš„ç©ºé—´çº§åˆ«äº¤äº’ï¼Œå…è®¸åŒ»ç”Ÿç›´æ¥ä¸ç‰¹å®šçš„å›¾åƒåŒºåŸŸè¿›è¡Œäº¤äº’ï¼Œä½¿å…¶æˆä¸ºåŒ»å­¦å½±åƒçš„ç›´è§‚å’Œé€æ˜å·¥å…·ã€‚CSRåœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šæ”¹è¿›äº†æœ€å…ˆè¿›çš„å¯è§£é‡Šæ–¹æ³•ï¼Œæé«˜äº†é«˜è¾¾4.5%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/tadeephuy/InteractCSR%E3%80%82">https://github.com/tadeephuy/InteractCSRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06873v2">PDF</a> Accepted CVPR2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„æ¦‚å¿µç›¸ä¼¼æ€§æ¨ç†ç½‘ç»œï¼ˆCSRï¼‰ç»“åˆäº†æ¦‚å¿µçº§å’Œè¡¥ä¸çº§çš„è§£é‡Šæ–¹æ³•ï¼Œæé«˜äº†æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§å’Œå¹²é¢„èƒ½åŠ›ã€‚CSRå®ç°äº†æ¦‚å¿µä¸å›¾åƒåŒºåŸŸçš„è”ç³»ï¼Œä¸ºåŒ»ç”Ÿæä¾›äº†ç›´è§‚å’Œé€æ˜çš„è¯Šæ–­å·¥å…·ã€‚è¯¥æ¨¡å‹åœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰è§£é‡Šæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒè¯Šæ–­ä¸­çš„æ¨¡å‹å†³ç­–éœ€è¦å¯è§£é‡Šæ€§å’Œå¯å¹²é¢„æ€§ã€‚</li>
<li>æ¦‚å¿µçº§æ–¹æ³•æœ‰åŠ©äºè§£é‡Šæ¨¡å‹é¢„æµ‹ï¼Œä½†éš¾ä»¥å®šä½å…·ä½“æ¿€æ´»åŒºåŸŸã€‚</li>
<li>è¡¥ä¸çº§æ–¹æ³•é€šè¿‡å›¾åƒåŒºåŸŸå­¦ä¹ è¡¨ç¤ºï¼Œä½†è§£é‡Šåº•å±‚æ¦‚å¿µå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>CSRç½‘ç»œç»“åˆäº†æ¦‚å¿µçº§å’Œè¡¥ä¸çº§çš„è§£é‡Šæ–¹æ³•ï¼Œæä¾›å±€éƒ¨è§£é‡Šå’Œå†…åœ¨æ¦‚å¿µè§£è¯»ã€‚</li>
<li>CSRç½‘ç»œå®ç°äº†ç©ºé—´çº§åˆ«çš„äº¤äº’ï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿç›´æ¥ä¸ç‰¹å®šå›¾åƒåŒºåŸŸäº’åŠ¨ã€‚</li>
<li>CSRç½‘ç»œæé«˜äº†æ¨¡å‹åœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-949c7d71aea29cd3db675475a2074d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90bd9f54860aeea029edc540d67492f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aaf48488a6d44f916ac63fb3788990d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5f8a663e2244983f7b467bc408907fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0eef3f141c3663d8fcbf728486cbdef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc344d20b38c67b3ddf92be0fba069d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Pathological-Prior-Guided-Multiple-Instance-Learning-For-Mitigating-Catastrophic-Forgetting-in-Breast-Cancer-Whole-Slide-Image-Classification"><a href="#Pathological-Prior-Guided-Multiple-Instance-Learning-For-Mitigating-Catastrophic-Forgetting-in-Breast-Cancer-Whole-Slide-Image-Classification" class="headerlink" title="Pathological Prior-Guided Multiple Instance Learning For Mitigating   Catastrophic Forgetting in Breast Cancer Whole Slide Image Classification"></a>Pathological Prior-Guided Multiple Instance Learning For Mitigating   Catastrophic Forgetting in Breast Cancer Whole Slide Image Classification</h2><p><strong>Authors:Weixi Zheng, Aoling Huang. Jingping Yuan, Haoyu Zhao, Zhou Zhao, Yongchao Xu, Thierry GÃ©raud</strong></p>
<p>In histopathology, intelligent diagnosis of Whole Slide Images (WSIs) is essential for automating and objectifying diagnoses, reducing the workload of pathologists. However, diagnostic models often face the challenge of forgetting previously learned data during incremental training on datasets from different sources. To address this issue, we propose a new framework PaGMIL to mitigate catastrophic forgetting in breast cancer WSI classification. Our framework introduces two key components into the common MIL model architecture. First, it leverages microscopic pathological prior to select more accurate and diverse representative patches for MIL. Secondly, it trains separate classification heads for each task and uses macroscopic pathological prior knowledge, treating the thumbnail as a prompt guide (PG) to select the appropriate classification head. We evaluate the continual learning performance of PaGMIL across several public breast cancer datasets. PaGMIL achieves a better balance between the performance of the current task and the retention of previous tasks, outperforming other continual learning methods. Our code will be open-sourced upon acceptance. </p>
<blockquote>
<p>åœ¨ç—…ç†å­¦é¢†åŸŸï¼Œæ™ºèƒ½åœ°å¯¹å…¨ç‰‡å›¾åƒï¼ˆWhole Slide Imagesï¼Œç®€ç§°WSIï¼‰è¿›è¡Œè¯Šæ–­å¯¹äºå®ç°è¯Šæ–­å’Œå®¢è§‚åŒ–è¯Šæ–­çš„è‡ªåŠ¨åŒ–ã€å‡è½»ç—…ç†å­¦å®¶çš„å·¥ä½œé‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¯Šæ–­æ¨¡å‹åœ¨é’ˆå¯¹æ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®é›†è¿›è¡Œå¢é‡è®­ç»ƒæ—¶ï¼Œå¸¸å¸¸é¢ä¸´é—å¿˜å…ˆå‰å­¦ä¹ æ•°æ®çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶PaGMILï¼Œæ—¨åœ¨ç¼“è§£ä¹³è…ºç™ŒWSIåˆ†ç±»ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¸¸è§çš„MILæ¨¡å‹æ¶æ„ä¸­å¼•å…¥äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆï¼Œå®ƒåˆ©ç”¨å¾®è§‚ç—…ç†å…ˆéªŒçŸ¥è¯†æ¥é€‰æ‹©æ›´å‡†ç¡®å’Œå¤šæ ·åŒ–çš„ä»£è¡¨æ€§è¡¥ä¸ç”¨äºMILã€‚å…¶æ¬¡ï¼Œå®ƒä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒäº†å•ç‹¬çš„åˆ†ç±»å¤´ï¼Œå¹¶ä½¿ç”¨å®è§‚ç—…ç†å…ˆéªŒçŸ¥è¯†ï¼Œå°†ç¼©ç•¥å›¾ä½œä¸ºæç¤ºæŒ‡å—ï¼ˆPrompt Guideï¼Œç®€ç§°PGï¼‰æ¥é€‰æ‹©é€‚å½“çš„åˆ†ç±»å¤´ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…¬å…±ä¹³è…ºç™Œæ•°æ®é›†ä¸Šè¯„ä¼°äº†PaGMILçš„è¿ç»­å­¦ä¹ æ•ˆæœã€‚PaGMILåœ¨å¹³è¡¡å½“å‰ä»»åŠ¡æ€§èƒ½å’Œä¿ç•™å…ˆå‰ä»»åŠ¡çŸ¥è¯†æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä¼˜äºå…¶ä»–è¿ç»­å­¦ä¹ æ–¹æ³•ã€‚ä»£ç å°†åœ¨éªŒæ”¶åå¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06056v1">PDF</a> ICASSP2025(Oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¡†æ¶PaGMILï¼Œç”¨äºè§£å†³ç»„ç»‡ç—…ç†å­¦å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰æ™ºèƒ½è¯Šæ–­ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¾®è§‚å’Œå®è§‚ç—…ç†å…ˆéªŒçŸ¥è¯†ï¼Œè¯¥æ¡†æ¶ä¼˜åŒ–äº†å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¨¡å‹æ¶æ„ï¼Œå¹¶æå‡äº†ä¹³è…ºç™ŒWSIåˆ†ç±»çš„è¿ç»­å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒPaGMILåœ¨ä¸åŒå…¬å…±ä¹³è…ºç™Œæ•°æ®é›†ä¸Šçš„æŒç»­å­¦ä¹ æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†å½“å‰ä»»åŠ¡æ€§èƒ½ä¸å…ˆå‰ä»»åŠ¡ä¿ç•™ä¹‹é—´çš„å¹³è¡¡ï¼Œå¹¶ä¼˜äºå…¶ä»–æŒç»­å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½è¯Šæ–­å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰åœ¨ç»„ç»‡ç—…ç†å­¦ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰è¯Šæ–­æ¨¡å‹åœ¨å¢é‡è®­ç»ƒæ—¶é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„æŒ‘æˆ˜ã€‚</li>
<li>PaGMILæ¡†æ¶é€šè¿‡å¼•å…¥å¾®è§‚ç—…ç†å…ˆéªŒçŸ¥è¯†æ¥é€‰æ‹©æ›´å‡†ç¡®å’Œå¤šæ ·åŒ–çš„ä»£è¡¨æ€§è¡¥ä¸ï¼Œä¼˜åŒ–å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ¨¡å‹æ¶æ„ã€‚</li>
<li>PaGMILä½¿ç”¨å®è§‚ç—…ç†å…ˆéªŒçŸ¥è¯†è®­ç»ƒå•ç‹¬çš„ä»»åŠ¡åˆ†ç±»å¤´ï¼Œå¹¶ä½¿ç”¨ç¼©ç•¥å›¾ä½œä¸ºæç¤ºæŒ‡å—æ¥é€‰æ‹©é€‚å½“çš„åˆ†ç±»å¤´ã€‚</li>
<li>PaGMILåœ¨å¤šä¸ªå…¬å…±ä¹³è…ºç™Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†å½“å‰ä»»åŠ¡ä¸å…ˆå‰ä»»åŠ¡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>PaGMILçš„æŒç»­å­¦ä¹ æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-430b4c446d5e9f21d5ea4f1c108c6bae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5e884be1e3057c1e597d8737257afc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16d881582cb2a3c1de58a3ef2029fc0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e13aa9fd323016be032dc2ed1d6eb851.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Interpretable-High-order-Knowledge-Graph-Neural-Network-for-Predicting-Synthetic-Lethality-in-Human-Cancers"><a href="#Interpretable-High-order-Knowledge-Graph-Neural-Network-for-Predicting-Synthetic-Lethality-in-Human-Cancers" class="headerlink" title="Interpretable High-order Knowledge Graph Neural Network for Predicting   Synthetic Lethality in Human Cancers"></a>Interpretable High-order Knowledge Graph Neural Network for Predicting   Synthetic Lethality in Human Cancers</h2><p><strong>Authors:Xuexin Chen, Ruichu Cai, Zhengting Huang, Zijian Li, Jie Zheng, Min Wu</strong></p>
<p>Synthetic lethality (SL) is a promising gene interaction for cancer therapy. Recent SL prediction methods integrate knowledge graphs (KGs) into graph neural networks (GNNs) and employ attention mechanisms to extract local subgraphs as explanations for target gene pairs. However, attention mechanisms often lack fidelity, typically generate a single explanation per gene pair, and fail to ensure trustworthy high-order structures in their explanations. To overcome these limitations, we propose Diverse Graph Information Bottleneck for Synthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithful explanations for the same gene pair and effectively encodes high-order structures. Specifically, we introduce a novel DGIB objective, integrating a Determinant Point Process (DPP) constraint into the standard IB objective, and employ 13 motif-based adjacency matrices to capture high-order structures in gene representations. Experimental results show that DGIB4SL outperforms state-of-the-art baselines and provides multiple explanations for SL prediction, revealing diverse biological mechanisms underlying SL inference. </p>
<blockquote>
<p>åˆæˆè‡´æ­»æ€§ï¼ˆSynthetic Lethality, SLï¼‰æ˜¯ä¸€ç§å…·æœ‰æ½œåŠ›çš„ç™Œç—‡æ²»ç–—åŸºå› äº¤äº’ä½œç”¨ã€‚æœ€è¿‘çš„SLé¢„æµ‹æ–¹æ³•å°†çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰èå…¥å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œå¹¶åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶æå–å±€éƒ¨å­å›¾ä½œä¸ºç›®æ ‡åŸºå› å¯¹çš„è§£é‡Šã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ç¼ºä¹å‡†ç¡®æ€§ï¼Œé€šå¸¸åªä¸ºæ¯ä¸ªåŸºå› å¯¹ç”Ÿæˆä¸€ä¸ªè§£é‡Šï¼Œå¹¶ä¸”æ— æ³•åœ¨è§£é‡Šä¸­ç¡®ä¿å¯é çš„é«˜é˜¶ç»“æ„ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†å›¾è°±çš„åˆæˆè‡´æ­»æ€§å¤šæ ·å›¾ä¿¡æ¯ç“¶é¢ˆï¼ˆDGIB4SLï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ºåŒä¸€åŸºå› å¯¹ç”Ÿæˆå¤šä¸ªå¿ å®è§£é‡Šï¼Œå¹¶æœ‰æ•ˆåœ°ç¼–ç é«˜é˜¶ç»“æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„DGIBç›®æ ‡ï¼Œå°†è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰çº¦æŸé›†æˆåˆ°æ ‡å‡†IBç›®æ ‡ä¸­ï¼Œå¹¶ä½¿ç”¨13ä¸ªåŸºäºåŸºåºçš„é‚»æ¥çŸ©é˜µæ¥æ•è·åŸºå› è¡¨ç¤ºä¸­çš„é«˜é˜¶ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGIB4SLä¼˜äºæœ€æ–°çš„åŸºçº¿æ–¹æ³•ï¼Œä¸ºSLé¢„æµ‹æä¾›äº†å¤šä¸ªè§£é‡Šï¼Œæ­ç¤ºäº†SLæ¨æ–­èƒŒåçš„å¤šç§ç”Ÿç‰©æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06052v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆæˆè‡´æ­»æ€§ï¼ˆSLï¼‰æ˜¯ç™Œç—‡æ²»ç–—ä¸­æœ‰å‰æ™¯çš„åŸºå› äº¤äº’ä½œç”¨ã€‚ä¸ºå…‹æœç°æœ‰SLé¢„æµ‹æ–¹æ³•ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹å¿ å®åº¦ã€é€šå¸¸åªä¸ºåŸºå› å¯¹ç”Ÿæˆå•ä¸€è§£é‡Šä»¥åŠæ— æ³•ç¡®ä¿è§£é‡Šä¸­å¯é çš„é«˜é˜¶ç»“æ„ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†å›¾è°±çš„å›¾ç¥ç»ç½‘ç»œDGIB4SLï¼Œä¸ºåŒä¸€åŸºå› å¯¹ç”Ÿæˆå¤šä¸ªå¿ å®è§£é‡Šï¼Œå¹¶æœ‰æ•ˆç¼–ç é«˜é˜¶ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè‡´æ­»æ€§ï¼ˆSLï¼‰åœ¨ç™Œç—‡æ²»ç–—ä¸­æ˜¯é‡è¦çš„åŸºå› äº¤äº’ã€‚</li>
<li>ç°æœ‰SLé¢„æµ‹æ–¹æ³•ç»“åˆçŸ¥è¯†å›¾è°±å’Œå›¾ç¥ç»ç½‘ç»œï¼Œä½†æ³¨æ„åŠ›æœºåˆ¶å¸¸ç¼ºä¹å¿ å®åº¦ã€‚</li>
<li>DGIB4SLæ–¹æ³•é€šè¿‡å¼•å…¥æ–°é¢–çš„DGIBç›®æ ‡ï¼Œæ•´åˆè¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰çº¦æŸåˆ°æ ‡å‡†IBç›®æ ‡ä¸­ï¼Œè§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>DGIB4SLä½¿ç”¨13ä¸ªåŸºäºæ¨¡å¼çš„é‚»æ¥çŸ©é˜µæ¥æ•æ‰åŸºå› è¡¨ç¤ºä¸­çš„é«˜é˜¶ç»“æ„ã€‚</li>
<li>DGIB4SLç›¸æ¯”æœ€æ–°æŠ€æœ¯åŸºçº¿è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>DGIB4SLä¸ºSLé¢„æµ‹æä¾›å¤šä¸ªè§£é‡Šï¼Œæ­ç¤ºåˆæˆè‡´æ­»æ€§æ¨æ–­èƒŒåçš„å¤šç§ç”Ÿç‰©æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef8aab18e9d21b75bcb393b25697e0ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e09a962d6351d1d76d2df13687af0eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88c1a2d35388d5c366bd493c38f13a4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-364c878f003c413ffc8e1e25fa3c35e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed57fd8ee6ba2c3dd428384b6f28081b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§å¤šä»¥æ‰‹åŠ¨æ–¹å¼è¿›è¡Œï¼Œéœ€è¦é˜…è¯»æ‚£è€…ç—…å†å¹¶å°†æ•°æ®è¾“å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ½œåŠ›é€šè¿‡æé«˜æ•ˆç‡å’Œå¯é æ€§æ¥å¢å¼ºè¿™ä¸€æµç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°å¯¹ä¸‰ç§è‚¿ç˜¤è®°å½•åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸Šä½¿ç”¨äº†ä»1äº¿åˆ°70äº¿æ¨¡å‹å‚æ•°ä¸ç­‰çš„åä¸€ä¸ªä¸åŒå¼€æºLLMsè¿›è¡Œäº†æµ‹è¯•ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›ä»»åŠ¡ä¸Šçš„LLMsæ€§èƒ½ï¼Œå‡†å¤‡äº†ä¸€ä»½åŸºäºæ³Œå°¿ç§‘åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ³¨é‡Šæ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥ç ”ç©¶å°‘é‡ç¤ºä¾‹æç¤ºä¸­ç¤ºä¾‹æ•°é‡çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsçš„ä¸€èˆ¬èƒ½åŠ›ã€‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“å‡ºè‰²ã€‚æ‹¥æœ‰è¾ƒå°‘è®­ç»ƒæ•°æ®æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œå¤§å‹æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªæ³Œå°¿ç§‘ä»¥å¤–çš„å…¶ä»–åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿå¯ä»¥åœ¨å°‘é‡æç¤ºä¸­æ”¹å–„ç»“æœï¼Œè¿™è¯æ˜äº†LLMså¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMsåœ¨è‡ªåŠ¨è‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚å‚æ•°åœ¨7äº¿åˆ°12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼ˆfine-tuningï¼‰å’Œç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹æœªæ¥å¯èƒ½æˆä¸ºä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E7%96%97NLP%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9C%9F%E5%AE%9E%E5%92%8C%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9F%AD%E7%BC%BA%E9%97%AE%E9%A2%98%E7%9A%84%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æ•°æ®é›†ï¼Œä½œä¸ºè§£å†³å¾·å›½åŒ»ç–—NLPé¢†åŸŸä¸­çœŸå®å’Œæ˜“äºè®¿é—®åŸºå‡†æµ‹è¯•æ•°æ®çŸ­ç¼ºé—®é¢˜çš„æ–°æœ‰ä»·å€¼èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v2">PDF</a> 48 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‚¿ç˜¤è®°å½•è‡ªåŠ¨åŒ–æ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹11ç§ä¸åŒè§„æ¨¡ï¼ˆä»1äº¿åˆ°70äº¿å‚æ•°ï¼‰çš„å¼€æºLLMsè¿›è¡Œè¯„ä»·ï¼Œå‘ç°å®ƒä»¬åœ¨è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æ¨¡å‹å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´å’Œç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹æœ‰æœ›åœ¨æœªæ¥æˆä¸ºä¸´åºŠæ–‡æ¡£ç®¡ç†çš„é‡è¦å·¥å…·ã€‚æ•°æ®é›†å·²å‘å¸ƒï¼Œä»¥è§£å†³å¾·è¯­åŒ»ç–—NLPé¢†åŸŸçœŸå®å’Œå¯è®¿é—®åŸºå‡†æ•°æ®çš„çŸ­ç¼ºé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ½œåŠ›æé«˜å¾·å›½è‚¿ç˜¤è®°å½•è¿‡ç¨‹çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>å¯¹11ç§ä¸åŒè§„æ¨¡å’Œå¼€æºçš„LLMsè¿›è¡Œäº†è¯„ä»·ï¼ŒåŒ…æ‹¬ä¸åŒä»»åŠ¡å’Œæ¨¡å‹æ€§èƒ½çš„æ¯”è¾ƒã€‚</li>
<li>Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ¨¡å‹å‚æ•°åœ¨7-12äº¿ä¹‹é—´å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´å’Œç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹å¯åº”ç”¨äºä¸´åºŠæ–‡æ¡£ç®¡ç†ã€‚</li>
<li>æ¨¡å‹çš„æ€§èƒ½ä¸ä»…å–å†³äºæ¨¡å‹æœ¬èº«çš„è§„æ¨¡å’Œè®­ç»ƒæ•°æ®ï¼Œè¿˜å—åˆ°æç¤ºç­–ç•¥å’Œé¢†åŸŸå·®å¼‚çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-764cd5a8d839a45da36d211b0c673397.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fbd2fce63b2cad7e3b967780aa11e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b245a6522b0d161dce0f49b1bb1190c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention"><a href="#Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention" class="headerlink" title="Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention"></a>Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention</h2><p><strong>Authors:Shanwen Wang, Xin Sun, Changrui Chen, Danfeng Hong, Jungong Han</strong></p>
<p>Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ä¸ºé¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„è§£å†³æ–¹æ¡ˆï¼Œå‡è½»äº†åŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡ç­¾çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œé¥æ„Ÿå›¾åƒå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾å’Œé«˜çš„ç±»é—´ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸äº¤å‰æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›ï¼ˆMUCAï¼‰æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒMUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œçº¦æŸç½‘ç»œä¸åŒå±‚ç‰¹å¾å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®ƒæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMUCAåˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œï¼Œé€šè¿‡æ•™å¸ˆç½‘ç»œçš„äº’è¡¥ç‰¹å¾æ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ä¸€è®¾è®¡æœ‰æ•ˆåœ°ç»“åˆäº†å¼±å¢å¼ºå’Œå¼ºå¢å¼ºï¼ˆWAå’ŒSAï¼‰ï¼Œè¿›ä¸€æ­¥æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„åŠç›‘ç£æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼ç‰©ä½“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›åŠç›‘ç£é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10736v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŠç›‘ç£å­¦ä¹ ä¸ºé¥æ„Ÿå›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§æœ‰å¸å¼•åŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œå‡è½»äº†åŠ³åŠ¨åŠ›å¯†é›†çš„åƒç´ çº§æ ‡æ³¨çš„è´Ÿæ‹…ã€‚é’ˆå¯¹é¥æ„Ÿå›¾åƒçš„å¤šå°ºåº¦ç‰¹å¾å’Œç±»é—´é«˜ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠç›‘ç£Multi-Scale Uncertainty and Cross-Teacher-Student Attentionï¼ˆMUCAï¼‰æ¨¡å‹ã€‚MUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒMUCAåˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·è¾¨è¯†åº¦çš„ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼ç‰©ä½“æ–¹é¢å±•ç°æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ æ˜¯è§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­åŠ³åŠ¨åŠ›å¯†é›†æ ‡æ³¨é—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>é¥æ„Ÿå›¾åƒå…·æœ‰å¤šå°ºåº¦ç‰¹å¾å’Œç±»é—´é«˜ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>MUCAæ¨¡å‹é€šè¿‡å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–æé«˜åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>MUCAæ¨¡å‹åˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·è¾¨è¯†åº¦çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å¼±å¢å¼ºå’Œå¼ºå¢å¼ºï¼Œè¿›ä¸€æ­¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†MUCAæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2ef039f55b860571bb4c1e3cbcfc2ec0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0c5331caa60f7e8d315d9de686cebf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-326833c3be16962bc975d7fa47905839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16d53dae20669c3b1ad4cf78f41f1e4a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds"><a href="#CAD-Recode-Reverse-Engineering-CAD-Code-from-Point-Clouds" class="headerlink" title="CAD-Recode: Reverse Engineering CAD Code from Point Clouds"></a>CAD-Recode: Reverse Engineering CAD Code from Point Clouds</h2><p><strong>Authors:Danila Rukhovich, Elona Dupont, Dimitrios Mallis, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</strong></p>
<p>Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and training dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained on a procedurally generated dataset of one million CAD sequences. CAD-Recode significantly outperforms existing methods across the DeepCAD, Fusion360 and real-world CC3D datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡ä¾æ¬¡ç»˜åˆ¶å‚æ•°åŒ–è‰å›¾å¹¶åº”ç”¨CADæ“ä½œæ¥è·å¾—çš„ä¸‰ç»´æ¨¡å‹ã€‚3D CADé€†å‘å·¥ç¨‹çš„é—®é¢˜åœ¨äºä»ç‚¹äº‘ç­‰3Dè¡¨ç¤ºä¸­é‡å»ºè‰å›¾å’ŒCADæ“ä½œåºåˆ—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ–¹é¢çš„æ–°è´¡çŒ®æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼šCADåºåˆ—è¡¨ç¤ºã€ç½‘ç»œè®¾è®¡å’Œè®­ç»ƒæ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†CADè‰å›¾æŒ¤å‹åºåˆ—è¡¨ç¤ºä¸ºPythonä»£ç ã€‚æ‰€æå‡ºçš„CAD-Recodeå°†ç‚¹äº‘è½¬æ¢ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œè¯¥ä»£ç å³å¯é‡å»ºCADæ¨¡å‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹Pythonä»£ç çš„æš´éœ²ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªè¾ƒå°çš„LLMä½œä¸ºCAD-Recodeçš„è§£ç å™¨ï¼Œå¹¶å°†å…¶ä¸ä¸€ä¸ªè½»é‡çº§çš„ç‚¹äº‘æŠ•å½±ä»ªç›¸ç»“åˆã€‚CAD-Recodeæ˜¯åœ¨ç¨‹åºç”Ÿæˆçš„ä¸€ç™¾ä¸‡ä¸ªCADåºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚åœ¨DeepCADã€Fusion360å’Œç°å®ä¸–ç•ŒCC3Dæ•°æ®é›†ä¸­ï¼ŒCAD-Recodeæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„CAD Pythonä»£ç è¾“å‡ºèƒ½è¢«å¸‚é¢ä¸Šçš„LLMè§£é‡Šï¼Œä»è€Œèƒ½å¤Ÿæœ‰ç‚¹äº‘è¿›è¡ŒCADç¼–è¾‘å’Œé’ˆå¯¹CADçš„é—®é¢˜å›ç­”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14042v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è®¡ç®—æœºä¸‰ç»´è¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„é€†å‘å·¥ç¨‹é—®é¢˜ï¼Œé€šè¿‡ä¸‰ä¸ªæ–¹é¢çš„åˆ›æ–°è´¡çŒ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šCADåºåˆ—è¡¨ç¤ºã€ç½‘ç»œè®¾è®¡å’Œè®­ç»ƒæ•°æ®é›†ã€‚è®ºæ–‡å°†CADè‰å›¾æŒ¤å‹åºåˆ—è¡¨ç¤ºä¸ºPythonä»£ç ï¼Œå¹¶æå‡ºCAD-Recodeæ–¹æ³•ï¼Œå°†ç‚¹äº‘è½¬åŒ–ä¸ºPythonä»£ç ï¼Œæ‰§è¡Œåå¯é‡å»ºCADæ¨¡å‹ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹Pythonä»£ç çš„æš´éœ²ï¼Œç»“åˆå°å‹LLMè§£ç å™¨å’Œè½»é‡çº§ç‚¹äº‘æŠ•å½±ä»ªï¼Œåœ¨CAD-Recodeè®­ç»ƒäº†ä¸€åƒä¸‡ä¸ªCADåºåˆ—çš„ç¨‹åºç”Ÿæˆæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ã€‚CAD-Recodeåœ¨æ·±CADã€Fusion360å’Œç°å®ä¸–ç•ŒCC3Dæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å±•ç¤ºçš„CAD Pythonä»£ç è¾“å‡ºèƒ½è¢«å¸‚é¢ä¸Šçš„LLMè§£é‡Šï¼Œä½¿å¾—ä»ç‚¹äº‘ä¸­è¿›è¡ŒCADç¼–è¾‘å’Œå›ç­”CADç›¸å…³é—®é¢˜æˆä¸ºå¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†è®¡ç®—æœºä¸‰ç»´è¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„é€†å‘å·¥ç¨‹é—®é¢˜ï¼Œæ—¨åœ¨ä»ä¸‰ç»´è¡¨ç¤ºï¼ˆå¦‚ç‚¹äº‘ï¼‰é‡å»ºè‰å›¾å’Œè®¾è®¡æ“ä½œåºåˆ—ã€‚</li>
<li>åˆ›æ–°æ€§åœ°ä½¿ç”¨Pythonä»£ç è¡¨ç¤ºCADè‰å›¾æŒ¤å‹åºåˆ—ï¼Œæå‡ºCAD-Recodeæ–¹æ³•ï¼Œå°†ç‚¹äº‘è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„Pythonä»£ç ä»¥é‡å»ºCADæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆè½»é‡çº§ç‚¹äº‘æŠ•å½±ä»ªè¿›è¡Œè§£ç ã€‚</li>
<li>CAD-Recodeåœ¨å¤§å‹ç¨‹åºç”Ÿæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CAD-Recodeè¾“å‡ºçš„Pythonä»£ç å…·æœ‰å¯è§£é‡Šæ€§ï¼Œä¾¿äºCADç¼–è¾‘å’Œå›ç­”ç›¸å…³é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¯åº”ç”¨äºä¸åŒé¢†åŸŸçš„ä¸‰ç»´æ¨¡å‹é‡å»ºå’Œç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e2fdf66af5b25642d8fc0d9fb088eb03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf89e6bda9511c4cc6cac7518e02e412.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c76ec9b20f054caf0df4d48cc65c6a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50ffda0ad121911aa46b1c035b7a1951.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2dc410a27fca72c8bbc7043031ea05e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="CAD-Assistant-Tool-Augmented-VLLMs-as-Generic-CAD-Task-Solvers"><a href="#CAD-Assistant-Tool-Augmented-VLLMs-as-Generic-CAD-Task-Solvers" class="headerlink" title="CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers"></a>CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers</h2><p><strong>Authors:Dimitrios Mallis, Ahmet Serdar Karadeniz, Sebastian Cavada, Danila Rukhovich, Niki Foteinopoulou, Kseniya Cherenkova, Anis Kacem, Djamila Aouada</strong></p>
<p>We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design. Our approach is based on a powerful Vision and Large Language Model (VLLM) as a planner and a tool-augmentation paradigm using CAD-specific tools. CAD-Assistant addresses multimodal user queries by generating actions that are iteratively executed on a Python interpreter equipped with the FreeCAD software, accessed via its Python API. Our framework is able to assess the impact of generated CAD commands on geometry and adapts subsequent actions based on the evolving state of the CAD design. We consider a wide range of CAD-specific tools including a sketch image parameterizer, rendering modules, a 2D cross-section generator, and other specialized routines. CAD-Assistant is evaluated on multiple CAD benchmarks, where it outperforms VLLM baselines and supervised task-specific methods. Beyond existing benchmarks, we qualitatively demonstrate the potential of tool-augmented VLLMs as general-purpose CAD solvers across diverse workflows. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºCADåŠ©æ‰‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºäººå·¥æ™ºèƒ½è¾…åŠ©è®¾è®¡çš„é€šç”¨CADä»£ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºå¼ºå¤§çš„è§†è§‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰ä½œä¸ºè§„åˆ’å™¨ï¼Œå¹¶ä½¿ç”¨CADç‰¹å®šå·¥å…·è¿›è¡Œå·¥å…·å¢å¼ºèŒƒå¼ã€‚CADåŠ©æ‰‹é€šè¿‡ç”ŸæˆåŠ¨ä½œæ¥è§£å†³å¤šæ¨¡å¼ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿™äº›åŠ¨ä½œåœ¨é…å¤‡FreeCADè½¯ä»¶çš„Pythonè§£é‡Šå™¨ä¸Šè¿­ä»£æ‰§è¡Œï¼Œé€šè¿‡å…¶Python APIè¿›è¡Œè®¿é—®ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿè¯„ä¼°ç”Ÿæˆçš„CADå‘½ä»¤å¯¹å‡ ä½•å½¢çŠ¶çš„å½±å“ï¼Œå¹¶æ ¹æ®CADè®¾è®¡çš„ä¸æ–­å˜åŒ–çŠ¶æ€è°ƒæ•´åç»­åŠ¨ä½œã€‚æˆ‘ä»¬è€ƒè™‘äº†å¹¿æ³›çš„CADç‰¹å®šå·¥å…·ï¼ŒåŒ…æ‹¬è‰å›¾å›¾åƒå‚æ•°åŒ–å™¨ã€æ¸²æŸ“æ¨¡å—ã€2Dæ¨ªæˆªé¢ç”Ÿæˆå™¨å’Œå…¶ä»–ä¸“ä¸šä¾‹è¡Œç¨‹åºã€‚CADåŠ©æ‰‹åœ¨å¤šä¸ªCADåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜äºVLLMåŸºå‡†çº¿å’Œæœ‰ç›‘ç£çš„ç‰¹å®šä»»åŠ¡æ–¹æ³•ã€‚é™¤äº†ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬è¿˜ä»å®šæ€§è§’åº¦å±•ç¤ºäº†å·¥å…·å¢å¼ºå‹VLLMåœ¨å¤šæ ·åŒ–å·¥ä½œæµç¨‹ä¸­çš„é€šç”¨CADæ±‚è§£å™¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºå¤§çš„è§†è§‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰ä»¥åŠCADç‰¹å®šå·¥å…·çš„è¾…åŠ©è®¾è®¡åŠ©æ‰‹CAD-Assistantæå‡ºã€‚CAD-Assistantèƒ½å¤Ÿé€šè¿‡Pythonè§£é‡Šå™¨ä¸Šçš„è¿­ä»£æ‰§è¡ŒåŠ¨ä½œå“åº”å¤šæ¨¡æ€ç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶é…å¤‡FreeCADè½¯ä»¶é€šè¿‡å…¶Python APIè®¿é—®ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯„ä¼°ç”Ÿæˆçš„CADå‘½ä»¤å¯¹å‡ ä½•ç»“æ„çš„å½±å“ï¼Œå¹¶æ ¹æ®CADè®¾è®¡çš„ä¸æ–­æ¼”å˜çŠ¶æ€è°ƒæ•´åç»­æ“ä½œã€‚CAD-AssistantåŒ…æ‹¬è‰å›¾å›¾åƒå‚æ•°åŒ–å™¨ã€æ¸²æŸ“æ¨¡å—ã€äºŒç»´æ¨ªæˆªé¢ç”Ÿæˆå™¨å’Œå…¶ä»–ç‰¹å®šä¾‹è¡Œç¨‹åºåœ¨å†…çš„å¤šç§CADç‰¹å®šå·¥å…·ã€‚åœ¨å¤šä¸ªCADåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†CAD-Assistantçš„æ€§èƒ½ï¼Œç›¸è¾ƒäºVLLMåŸºå‡†æµ‹è¯•å’Œç‰¹å®šä»»åŠ¡ç›‘ç£å­¦ä¹ æ–¹æ³•æœ‰æ›´å¥½çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®šæ€§å±•ç¤ºäº†å·¥å…·å¢å¼ºå‹VLLMåœ¨å¤šæ ·åŒ–å·¥ä½œæµç¨‹ä¸­çš„é€šç”¨CADæ±‚è§£æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD-Assistantæ˜¯ä¸€ä¸ªåŸºäºå¼ºå¤§çš„è§†è§‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰çš„é€šç”¨CADè¾…åŠ©å·¥å…·ã€‚</li>
<li>å®ƒé‡‡ç”¨å·¥å…·å¢å¼ºèŒƒå¼ï¼Œä½¿ç”¨CADç‰¹å®šå·¥å…·å“åº”å¤šæ¨¡æ€ç”¨æˆ·æŸ¥è¯¢ã€‚</li>
<li>CAD-Assistantèƒ½å¤Ÿé€šè¿‡Pythonè§£é‡Šå™¨æ‰§è¡ŒåŠ¨ä½œï¼Œå¹¶é…å¤‡FreeCADè½¯ä»¶é€šè¿‡å…¶Python APIè®¿é—®ã€‚</li>
<li>è¯¥æ¡†æ¶å¯ä»¥è¯„ä¼°ç”Ÿæˆçš„CADå‘½ä»¤å¯¹è®¾è®¡å‡ ä½•ç»“æ„çš„å½±å“ï¼Œå¹¶é€‚åº”æ€§åœ°è°ƒæ•´åç»­æ“ä½œã€‚</li>
<li>CAD-AssistantåŒ…æ‹¬å¤šç§CADç‰¹å®šå·¥å…·ï¼Œå¦‚è‰å›¾å›¾åƒå‚æ•°åŒ–å™¨ã€æ¸²æŸ“æ¨¡å—å’ŒäºŒç»´æ¨ªæˆªé¢ç”Ÿæˆå™¨ç­‰ã€‚</li>
<li>åœ¨å¤šä¸ªCADåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCAD-Assistantè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¼˜äºVLLMåŸºå‡†æµ‹è¯•å’Œç‰¹å®šä»»åŠ¡ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24ec27013a2b95cc2beb4ba0ac3af8a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38806c9e39b27333bde9c9f4686e6be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f2e11e047e1dbe79fab4533111634d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5619fb506c477fe2d9419fdbe5526ab.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="High-Quality-Mask-Tuning-Matters-for-Open-Vocabulary-Segmentation"><a href="#High-Quality-Mask-Tuning-Matters-for-Open-Vocabulary-Segmentation" class="headerlink" title="High-Quality Mask Tuning Matters for Open-Vocabulary Segmentation"></a>High-Quality Mask Tuning Matters for Open-Vocabulary Segmentation</h2><p><strong>Authors:Quan-Sheng Zeng, Yunheng Li, Daquan Zhou, Guanbin Li, Qibin Hou, Ming-Ming Cheng</strong></p>
<p>Open-vocabulary image segmentation has been advanced through the synergy between mask generators and vision-language models like Contrastive Language-Image Pre-training (CLIP). Previous approaches focus on generating masks while aligning mask features with text embeddings during training. In this paper, we observe that relying on generated low-quality masks can weaken the alignment of vision and language in regional representations. This motivates us to present a new fine-tuning framework, named MaskCLIP++, which uses ground-truth masks instead of generated masks to enhance the mask classification capability of CLIP. Due to the limited diversity of image segmentation datasets with mask annotations, we propose incorporating a consistency alignment principle during fine-tuning, which alleviates categorical bias toward the fine-tuning dataset. After low-cost fine-tuning, MaskCLIP++ significantly improves the mask classification performance on multi-domain datasets. Combining with the mask generator in previous state-of-the-art mask-based open vocabulary segmentation methods, we achieve performance improvements of +1.7, +2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20 datasets, respectively. Code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/MaskCLIPpp">https://github.com/HVision-NKU/MaskCLIPpp</a> . </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å›¾åƒåˆ†å‰²å·²ç»é€šè¿‡æ©è†œç”Ÿæˆå™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒCLIPï¼‰ä¹‹é—´çš„ååŒä½œç”¨å¾—åˆ°äº†å‘å±•ã€‚ä¹‹å‰çš„æ–¹æ³•ä¾§é‡äºç”Ÿæˆæ©è†œï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†æ©è†œç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¾èµ–ç”Ÿæˆçš„ä½è´¨é‡æ©è†œä¼šå‰Šå¼±åŒºåŸŸè¡¨ç¤ºä¸­çš„è§†è§‰å’Œè¯­è¨€å¯¹é½ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ¡†æ¶ï¼Œåä¸ºMaskCLIP++ï¼Œå®ƒä½¿ç”¨çœŸå®æ©è†œè€Œä¸æ˜¯ç”Ÿæˆçš„æ©è†œï¼Œä»¥æé«˜CLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚ç”±äºå¸¦æœ‰æ©è†œæ ‡æ³¨çš„å›¾åƒåˆ†å‰²æ•°æ®é›†å¤šæ ·æ€§æœ‰é™ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å¼•å…¥ä¸€è‡´æ€§å¯¹é½åŸåˆ™ï¼Œè¿™å‡è½»äº†å¯¹å¾®è°ƒæ•°æ®é›†çš„ç±»åˆ«åè§ã€‚ç»è¿‡ä½æˆæœ¬çš„å¾®è°ƒåï¼ŒMaskCLIP++åœ¨å¤šåŸŸæ•°æ®é›†ä¸Šçš„æ©è†œåˆ†ç±»æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä¸å…ˆå‰æœ€å…ˆè¿›çš„åŸºäºæ©è†œçš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ä¸­çš„æ©è†œç”Ÿæˆå™¨ç›¸ç»“åˆï¼Œæˆ‘ä»¬åœ¨A-847ã€PC-459ã€A-150ã€PC-59å’ŒPAS-20æ•°æ®é›†ä¸Šçš„mIoUåˆ†åˆ«æé«˜äº†+1.7ã€+2.3ã€+2.1ã€+3.1å’Œ+0.3ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HVision-NKU/MaskCLIPpp%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HVision-NKU/MaskCLIPppä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11464v3">PDF</a> Revised version according to comments from reviewers of ICLR2025</p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒæ¡†æ¶MaskCLIP++ï¼Œé‡‡ç”¨çœŸå®æ©è†œä»£æ›¿ç”Ÿæˆæ©è†œä»¥å¢å¼ºCLIPçš„æ©è†œåˆ†ç±»èƒ½åŠ›ã€‚ä¸ºæé«˜æ¨¡å‹åœ¨å¤šé¢†åŸŸæ•°æ®é›†çš„æ©è†œåˆ†ç±»æ€§èƒ½ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§ä¸€è‡´æ€§å¯¹é½åŸåˆ™ã€‚MaskCLIP++ä¸å½“å‰å…ˆè¿›çš„åŸºäºæ©è†œçš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ç›¸ç»“åˆï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MaskCLIP++ä½¿ç”¨çœŸå®æ©è†œä»£æ›¿ç”Ÿæˆæ©è†œè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹å¯¹å›¾åƒåŒºåŸŸè¡¨ç¤ºçš„è§†è§‰å’Œè¯­è¨€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>ä¸€è‡´æ€§å¯¹é½åŸåˆ™ç”¨äºç¼“è§£æ¨¡å‹å¯¹å¾®è°ƒæ•°æ®é›†çš„ç±»åˆ«åè§ï¼Œé€‚ç”¨äºå›¾åƒåˆ†å‰²æ•°æ®é›†æ©è†œæ ‡æ³¨æœ‰é™çš„æƒ…å†µã€‚</li>
<li>MaskCLIP++é€šè¿‡ä½æˆæœ¬çš„å¾®è°ƒæ˜¾è‘—æé«˜äº†æ©è†œåˆ†ç±»æ€§èƒ½ï¼Œåœ¨å¤šé¢†åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚</li>
<li>ç»“åˆå…ˆå‰å…ˆè¿›çš„åŸºäºæ©è†œçš„å¼€æ”¾è¯æ±‡åˆ†å‰²æ–¹æ³•ï¼ŒMaskCLIP++åœ¨A-847ã€PC-459ã€A-150ã€PC-59å’ŒPAS-20æ•°æ®é›†ä¸Šçš„æ€§èƒ½åˆ†åˆ«æå‡äº†+1.7ã€+2.3ã€+2.1ã€+3.1å’Œ+0.3 mIoUã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›çš„MaskCLIP++ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
<li>MaskCLIP++çš„æ–¹æ³•å¯åº”ç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„å¼€æ”¾è¯æ±‡åœºæ™¯ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5fae430a05b935fbf025ff480c6d835.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2a97cd1a5dab81c44ce0c395e3f68da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8293c7bec893a1b9cd4724e755390a24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7f80a10bb4ea8b500b21829b4bb95c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f82992b3e37b312975e67516092eee9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdc9d0c552109fa0117f302a02ffba48.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings"><a href="#CADSpotting-Robust-Panoptic-Symbol-Spotting-on-Large-Scale-CAD-Drawings" class="headerlink" title="CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings"></a>CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings</h2><p><strong>Authors:Jiazuo Mu, Fuyi Yang, Yanshun Zhang, Mingqian Zhang, Junxiong Zhang, Yongjian Luo, Lan Xu, Yujiao Shi, Yingliang Zhang</strong></p>
<p>We introduce CADSpotting, an effective method for panoptic symbol spotting in large-scale architectural CAD drawings. Existing approaches struggle with symbol diversity, scale variations, and overlapping elements in CAD designs. CADSpotting overcomes these challenges by representing primitives through densely sampled points with attributes like coordinates and colors, using a unified 3D point cloud model for robust feature learning. To enable accurate segmentation in large, complex drawings, we further propose a novel Sliding Window Aggregation (SWA) technique, combining weighted voting and Non-Maximum Suppression (NMS). Moreover, we introduce LS-CAD, a new large-scale CAD dataset to support our experiments, with each floorplan covering around 1,000 square meters, significantly larger than previous benchmarks. Experiments on FloorPlanCAD and LS-CAD datasets show that CADSpotting significantly outperforms existing methods. We also demonstrate its practical value through automating parametric 3D reconstruction, enabling interior modeling directly from raw CAD inputs. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†CADSpottingï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤§å‹å»ºç­‘CADå›¾çº¸ä¸­è¿›è¡Œå…¨æ™¯ç¬¦å·è¯†åˆ«çš„é«˜æ•ˆæ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•åœ¨ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’ŒCADè®¾è®¡ä¸­çš„é‡å å…ƒç´ æ–¹é¢å­˜åœ¨å›°éš¾ã€‚CADSpottingé€šè¿‡ç»Ÿä¸€ä½¿ç”¨å¯†é›†é‡‡æ ·ç‚¹äº‘æ¨¡å‹è¿›è¡Œç¨³å¥çš„ç‰¹å¾å­¦ä¹ ï¼Œè¿™äº›ç‚¹å¸¦æœ‰åæ ‡å’Œé¢œè‰²ç­‰å±æ€§ï¼Œå…‹æœäº†è¿™äº›æŒ‘æˆ˜ã€‚ä¸ºäº†åœ¨å¤§å‹å¤æ‚å›¾çº¸ä¸­å®ç°å‡†ç¡®åˆ†å‰²ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ»‘åŠ¨çª—å£èšåˆï¼ˆSWAï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†åŠ æƒæŠ•ç¥¨å’Œéæœ€å¤§æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†LS-CADè¿™ä¸€æ–°çš„å¤§è§„æ¨¡CADæ•°æ®é›†æ¥æ”¯æŒæˆ‘ä»¬çš„å®éªŒï¼Œæ¯ä¸ªå¹³é¢å›¾è¦†ç›–çº¦ä¸€åƒå¹³æ–¹ç±³ï¼Œè¿œè¶…ä¹‹å‰çš„åŸºå‡†æµ‹è¯•é›†ã€‚åœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCADSpottingåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨å‚æ•°åŒ–ä¸‰ç»´é‡å»ºå±•ç¤ºäº†å…¶å®ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿç›´æ¥ä»åŸå§‹CADè¾“å…¥è¿›è¡Œå®¤å†…å»ºæ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07377v3">PDF</a> 18pages, 14 figures, Project web-page:   <a target="_blank" rel="noopener" href="https://dgeneai.github.io/cadspotting-pages/">https://dgeneai.github.io/cadspotting-pages/</a></p>
<p><strong>Summary</strong></p>
<p>CADSpottingæ–¹æ³•èƒ½æœ‰æ•ˆè§£å†³å¤§è§„æ¨¡å»ºç­‘CADç»˜å›¾ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’ŒCADè®¾è®¡ä¸­çš„é‡å å…ƒç´ ç­‰æŒ‘æˆ˜ã€‚CADSpottingé€šè¿‡ç»Ÿä¸€ä½¿ç”¨ä¸‰ç»´ç‚¹äº‘æ¨¡å‹è¡¨ç¤ºåŸå§‹å›¾å½¢ä¸­çš„å¯†é›†é‡‡æ ·ç‚¹ï¼ˆåŒ…å«åæ ‡å’Œé¢œè‰²ç­‰å±æ€§ï¼‰ï¼Œå…‹æœè¿™äº›éš¾é¢˜ã€‚ä¸ºå®ç°å¤§å‹å¤æ‚ç»˜å›¾çš„ç²¾ç¡®åˆ†å‰²ï¼Œç ”ç©¶å›¢é˜Ÿè¿›ä¸€æ­¥æå‡ºæ–°é¢–çš„æ»‘åŠ¨çª—å£èšåˆæŠ€æœ¯ï¼ˆSWAï¼‰ï¼Œç»“åˆåŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºæ–°çš„å¤§è§„æ¨¡CADæ•°æ®é›†LS-CADä»¥æ”¯æŒå®éªŒï¼Œæ¯ä¸ªå¹³é¢å›¾çº¦è¦†ç›–ä¸€åƒå¹³æ–¹ç±³ï¼Œè¿œè¶…ä¹‹å‰çš„åŸºå‡†æµ‹è¯•é›†ã€‚åœ¨FloorPlanCADå’ŒLS-CADæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCADSpottingæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½é€šè¿‡è‡ªåŠ¨åŒ–å‚æ•°åŒ–ä¸‰ç»´é‡å»ºï¼Œå®ç°ä»åŸå§‹CADè¾“å…¥ä¸­è¿›è¡Œå†…éƒ¨å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADSpottingèƒ½æœ‰æ•ˆè§£å†³å¤§å‹å»ºç­‘CADç»˜å›¾ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«éš¾é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´ç¬¦å·å¤šæ ·æ€§ã€å°ºåº¦å˜åŒ–å’Œé‡å å…ƒç´ çš„æŒ‘æˆ˜ã€‚</li>
<li>CADSpottingä½¿ç”¨ç»Ÿä¸€çš„ä¸‰ç»´ç‚¹äº‘æ¨¡å‹è¡¨ç¤ºåŸå§‹å›¾å½¢ï¼Œæœ‰åŠ©äºå…‹æœä¸Šè¿°éš¾é¢˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºæ»‘åŠ¨çª—å£èšåˆæŠ€æœ¯ï¼ˆSWAï¼‰ï¼Œä»¥æé«˜å¤§å‹å¤æ‚ç»˜å›¾çš„ç²¾ç¡®åˆ†å‰²èƒ½åŠ›ã€‚</li>
<li>CADSpottingç»“åˆåŠ æƒæŠ•ç¥¨å’Œéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ¨å‡ºæ–°çš„å¤§è§„æ¨¡CADæ•°æ®é›†LS-CADï¼Œç”¨äºæ”¯æŒå®éªŒï¼Œæ¯ä¸ªå¹³é¢å›¾è¦†ç›–é¢ç§¯è¿œè¶…ä¹‹å‰çš„åŸºå‡†æµ‹è¯•é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67a602918632e548781aaaa3e5df397b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27a03ae8dc9698f04fa5ecf41b39bbd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb04a138a490e534781b5b8955f458ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3392a6e868fe517d64a3e8a9eda6065.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd5e8dbd9a8b83e552d2cefc71727a19.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2b0ff748e2c78988332d150b3e39d0f9.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  StyleSpeaker Audio-Enhanced Fine-Grained Style Modeling for   Speech-Driven 3D Facial Animation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-f29f2208ae501d92e2c304904b0b3dea.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  GoT Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
