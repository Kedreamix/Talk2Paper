<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-03-15  MuDG Taming Multi-modal Diffusion with Gaussian Splatting for Urban   Scene Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-4e2badfa4ca3ef058bd057c9ffb541e5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    44 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-15-更新"><a href="#2025-03-15-更新" class="headerlink" title="2025-03-15 更新"></a>2025-03-15 更新</h1><h2 id="MuDG-Taming-Multi-modal-Diffusion-with-Gaussian-Splatting-for-Urban-Scene-Reconstruction"><a href="#MuDG-Taming-Multi-modal-Diffusion-with-Gaussian-Splatting-for-Urban-Scene-Reconstruction" class="headerlink" title="MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban   Scene Reconstruction"></a>MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban   Scene Reconstruction</h2><p><strong>Authors:Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, Bohan Li, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang</strong></p>
<p>Recent breakthroughs in radiance fields have significantly advanced 3D scene reconstruction and novel view synthesis (NVS) in autonomous driving. Nevertheless, critical limitations persist: reconstruction-based methods exhibit substantial performance deterioration under significant viewpoint deviations from training trajectories, while generation-based techniques struggle with temporal coherence and precise scene controllability. To overcome these challenges, we present MuDG, an innovative framework that integrates Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and geometric priors to condition a multi-modal video diffusion model, synthesizing photorealistic RGB, depth, and semantic outputs for novel viewpoints. This synthesis pipeline enables feed-forward NVS without computationally intensive per-scene optimization, providing comprehensive supervision signals to refine 3DGS representations for rendering robustness enhancement under extreme viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG outperforms existing methods in both reconstruction and synthesis quality. </p>
<blockquote>
<p>近期，辐射场方面的突破极大地推动了自主驾驶中的3D场景重建和新型视角合成（NVS）的发展。然而，仍存在一些关键限制：基于重建的方法在训练轨迹的显著视点偏差下表现出显著的性能下降，而基于生成的技术则难以维持时间连贯性和精确的场景可控性。为了克服这些挑战，我们提出了MuDG，这是一个创新框架，它将多模式扩散模型与高斯喷溅（GS）相结合，用于城市场景重建。MuDG利用聚合的激光雷达点云、RGB和几何先验条件来调控多模式视频扩散模型，合成逼真的RGB、深度和语义输出，用于新颖视角。这一合成管道实现了前馈NVS，无需计算密集的单场景优化，提供全面的监督信号，以在极端视点变化下提高渲染的鲁棒性，细化3DGS表示。在Open Waymo数据集上的实验表明，MuDG在重建和合成质量方面都优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10604v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期，辐射场技术的突破显著推动了自动驾驶中的3D场景重建和新型视角合成（NVS）的发展。然而，仍存在关键局限：基于重建的方法在训练轨迹的显著视角偏差下性能大幅下降，而基于生成的技术则在时间连贯性和精确场景可控性方面面临挑战。为克服这些难题，我们提出了MuDG框架，它结合了多模态扩散模型和高斯拼贴（GS）进行城市场景重建。MuDG利用聚合的激光雷达点云、RGB和几何先验来条件化多模态视频扩散模型，为新型视角合成逼真的RGB、深度和语义输出。这一合成管道实现了前馈NVS，无需计算密集的场景优化，为增强3DGS表示的渲染稳健性提供了全面的监督信号，在极端视角变化下表现尤为出色。在Open Waymo数据集上的实验表明，MuDG在重建和合成质量方面均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>辐射场技术的最新突破已经显著推动了3D场景重建和新型视角合成在自动驾驶领域的发展。</li>
<li>当前方法存在局限性，基于重建的方法在视角偏差较大时性能下降，而基于生成的方法在保持时间连贯性和场景精确可控性方面存在挑战。</li>
<li>MuDG框架结合了多模态扩散模型和高斯拼贴（GS）进行城市场景重建，旨在克服现有方法的局限性。</li>
<li>MuDG利用LiDAR点云、RGB和几何先验信息，为新型视角合成逼真的RGB、深度和语义输出。</li>
<li>合成管道实现了前馈NVS，无需密集的场景优化计算，提供了全面的监督信号，以改进3D表示并增强渲染稳健性。</li>
<li>MuDG在多种数据集上的实验表现优于现有方法，特别是在处理极端视角变化时。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10604">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e2badfa4ca3ef058bd057c9ffb541e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80045cf9aea9fd9a53280dc3cdc43790.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05257f3e40daffc792c49c72c02cc48e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e54691a33ed55be331a86bafd3d76ccc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models"><a href="#4D-LangSplat-4D-Language-Gaussian-Splatting-via-Multimodal-Large-Language-Models" class="headerlink" title="4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models"></a>4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large   Language Models</h2><p><strong>Authors:Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister</strong></p>
<p>Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries. </p>
<blockquote>
<p>学习四维语言场，以实现动态场景中的时间敏感型开放式语言查询，对于许多现实世界应用至关重要。虽然LangSplat成功地将CLIP特性融入三维高斯表示，实现了在三维静态场景中的精度和效率，但它无法处理动态四维场，因为CLIP是为静态图像文本任务设计的，无法捕捉视频中的时间动态。现实世界的环境本质上是动态的，物体语义会随时间演变。构建精确的四维语言场必须获得像素对齐的、面向对象的视频特征，而当前视觉模型很难做到这一点。为了应对这些挑战，我们提出了四维LangSplat，它学习四维语言场，以高效地处理动态场景中的时间无关或时间敏感型开放式词汇查询。四维LangSplat绕过从视觉特征中学习语言场的方法，而是直接从通过面向对象的视频字幕生成文本中学习。具体来说，我们提出了一种多模态面向对象的视频提示方法，包括视觉和文本提示，可以引导多模态大型语言模型（MLLMs）为视频中的对象生成详细、时间连贯的高质量字幕。这些字幕使用大型语言模型进行编码，生成高质量句子嵌入，然后作为像素对齐的、面向对象的特征监督，通过共享嵌入空间实现开放式词汇文本查询。我们认识到四维场景中的对象在状态之间呈现出平滑过渡，因此进一步提出了一种状态可变形网络，以有效地对这些随时间变化的连续变化进行建模。我们在多个基准测试上的结果表明，四维LangSplat对于时间敏感型和时间无关型的开放式词汇查询都达到了精确和高效的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10437v1">PDF</a> CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://4d-langsplat.github.io/">https://4d-langsplat.github.io</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了学习4D语言字段的重要性，该语言字段可以处理动态场景中的时间敏感或无时间敏感的开放词汇查询。为解决当前模型在动态视频处理上的不足，提出了一种名为“四维朗思平板”（4D LangSplat）的方法。它采用对象级别的视频提示来指导大型多媒体语言模型（MLLMs）生成高质量的详细描述对象视频内容的文本字幕。该文本被转换为高质量的句子嵌入，作为像素对齐的对象特定特征监督，为通过共享嵌入空间进行开放式文本查询提供了便利。进一步采用了一种状态可变形网络，有效建模对象在连续时间内变化的特征变化。结果证明了该方法在不同标准上的准确性和高效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>学习四维语言字段对于处理动态场景中的时间敏感和无时间敏感的开放词汇查询至关重要。</li>
<li>当前模型在处理动态视频时面临困难，无法捕捉对象的语义变化。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10437">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-31e10094985ed60b63a44eae59c925ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0386885b55626e702f4946956c9ad1ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VicaSplat-A-Single-Run-is-All-You-Need-for-3D-Gaussian-Splatting-and-Camera-Estimation-from-Unposed-Video-Frames"><a href="#VicaSplat-A-Single-Run-is-All-You-Need-for-3D-Gaussian-Splatting-and-Camera-Estimation-from-Unposed-Video-Frames" class="headerlink" title="VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and   Camera Estimation from Unposed Video Frames"></a>VicaSplat: A Single Run is All You Need for 3D Gaussian Splatting and   Camera Estimation from Unposed Video Frames</h2><p><strong>Authors:Zhiqi Li, Chengrui Dong, Yiming Chen, Zhangchi Huang, Peidong Liu</strong></p>
<p>We present VicaSplat, a novel framework for joint 3D Gaussians reconstruction and camera pose estimation from a sequence of unposed video frames, which is a critical yet underexplored task in real-world 3D applications. The core of our method lies in a novel transformer-based network architecture. In particular, our model starts with an image encoder that maps each image to a list of visual tokens. All visual tokens are concatenated with additional inserted learnable camera tokens. The obtained tokens then fully communicate with each other within a tailored transformer decoder. The camera tokens causally aggregate features from visual tokens of different views, and further modulate them frame-wisely to inject view-dependent features. 3D Gaussian splats and camera pose parameters can then be estimated via different prediction heads. Experiments show that VicaSplat surpasses baseline methods for multi-view inputs, and achieves comparable performance to prior two-view approaches. Remarkably, VicaSplat also demonstrates exceptional cross-dataset generalization capability on the ScanNet benchmark, achieving superior performance without any fine-tuning. Project page: <a target="_blank" rel="noopener" href="https://lizhiqi49.github.io/VicaSplat">https://lizhiqi49.github.io/VicaSplat</a>. </p>
<blockquote>
<p>我们提出了VicaSplat，这是一个新颖的框架，用于从一系列未摆姿势的视频帧中联合重建3D高斯并估计相机姿态，这是现实世界3D应用中一个关键但尚未被充分研究的任务。我们的方法的核心在于一种新型的基于transformer的网络架构。具体来说，我们的模型从一个图像编码器开始，它将每张图像映射到一系列视觉令牌。所有视觉令牌都与附加的插入的可学习相机令牌连接在一起。所获得的令牌之间在定制的transformer解码器中进行充分的通信。相机令牌从来自不同视角的视觉令牌中提取特征，并进一步对其进行调制以注入视角相关的特征。然后可以通过不同的预测头估计出3D高斯地图和相机姿态参数。实验表明，对于多视角输入，VicaSplat超过了基线方法，实现了与现有两视角方法相当的性能。值得注意的是，VicaSplat在ScanNet基准测试上也表现出了出色的跨数据集泛化能力，无需微调即可实现卓越性能。项目页面：<a target="_blank" rel="noopener" href="https://lizhiqi49.github.io/VicaSplat">https://lizhiqi49.github.io/VicaSplat</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10286v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了VicaSplat这一新型框架，该框架可从一系列未定位的视频帧中联合进行3D高斯分布重建和相机姿态估计，这是现实世界中3D应用中的一个关键但尚未充分探索的任务。该方法的核心在于基于transformer的网络架构。模型通过图像编码器将每幅图像映射到一系列视觉令牌，并将这些令牌与额外的可学习相机令牌相结合。这些令牌在一个定制的transformer解码器内进行全面交流。相机令牌从来自不同视图的视觉令牌中提取特征，并对其进行智能调节以注入视角相关的特征。最后通过不同的预测头估计出3D高斯分布和相机姿态参数。实验表明，VicaSplat在多视图输入上超过了基线方法，并实现了与前两个视图方法相当的性能。此外，VicaSplat在ScanNet数据集上也展现出了卓越的跨数据集泛化能力，并且在不进行微调的情况下取得了优越的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VicaSplat是一个针对从视频帧序列进行3D高斯分布重建和相机姿态估计的新型框架。</li>
<li>方法核心在于基于transformer的网络架构，包括图像编码器和定制的transformer解码器。</li>
<li>通过相机令牌注入视角相关的特征，实现跨不同视图的特征融合。</li>
<li>VicaSplat通过不同的预测头估计出3D高斯分布和相机姿态参数。</li>
<li>实验结果表明，该框架在多视图输入上性能卓越。</li>
<li>VicaSplat实现了强大的跨数据集泛化能力，在ScanNet数据集上表现优异。</li>
<li>项目页面提供了更多详细信息：<a target="_blank" rel="noopener" href="https://lizhiqi49.github.io/VicaSplat">链接</a>。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10286">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-83721c46d0260f1599cde0dc5b1aa92c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0334b84eb22efe45ada0cb51b7017e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-972ab0405f170c74af1f8fd06b4f7fee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6030d36f7251a7b43ee61689a2cbd08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d24bc60d2a7b1a055cae61d6bad7f29.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ROODI-Reconstructing-Occluded-Objects-with-Denoising-Inpainters"><a href="#ROODI-Reconstructing-Occluded-Objects-with-Denoising-Inpainters" class="headerlink" title="ROODI: Reconstructing Occluded Objects with Denoising Inpainters"></a>ROODI: Reconstructing Occluded Objects with Denoising Inpainters</h2><p><strong>Authors:Yeonjin Chang, Erqun Dong, Seunghyeon Seo, Nojun Kwak, Kwang Moo Yi</strong></p>
<p>While the quality of novel-view images has improved dramatically with 3D Gaussian Splatting, extracting specific objects from scenes remains challenging. Isolating individual 3D Gaussian primitives for each object and handling occlusions in scenes remain far from being solved. We propose a novel object extraction method based on two key principles: (1) being object-centric by pruning irrelevant primitives; and (2) leveraging generative inpainting to compensate for missing observations caused by occlusions. For pruning, we analyze the local structure of primitives using K-nearest neighbors, and retain only relevant ones. For inpainting, we employ an off-the-shelf diffusion-based inpainter combined with occlusion reasoning, utilizing the 3D representation of the entire scene. Our findings highlight the crucial synergy between pruning and inpainting, both of which significantly enhance extraction performance. We evaluate our method on a standard real-world dataset and introduce a synthetic dataset for quantitative analysis. Our approach outperforms the state-of-the-art, demonstrating its effectiveness in object extraction from complex scenes. </p>
<blockquote>
<p>采用3D高斯贴图技术，全景图像的质量得到了极大提高，但从场景中提取特定物体仍然具有挑战性。隔离每个物体的单个3D高斯基本体并处理场景中的遮挡问题仍有待解决。我们提出了一种基于两个关键原则的新物体提取方法：（1）以物体为中心，剔除无关的基本体；（2）利用生成式补全技术来弥补遮挡造成的观测缺失。对于剔除操作，我们通过分析基本体的局部结构，利用K近邻法只保留相关的基本体。对于补全操作，我们采用现成的基于扩散的补全工具，结合遮挡推理，利用整个场景的3D表示。我们的研究发现，剔除和补全之间的协同作用至关重要，二者都能显著提高提取性能。我们在标准现实世界数据集上评估了我们的方法，并引入了一个合成数据集进行定量分析。我们的方法优于当前最新技术，证明了其在复杂场景中提取物体的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10256v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://yeonjin-chang.github.io/ROODI/">https://yeonjin-chang.github.io/ROODI/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于三维高斯分裂技术的新物体提取方法，该方法针对场景中特定物体的提取挑战进行了优化。该方法包括两个关键原则：一是以物体为中心，剔除无关的原语；二是利用生成补全技术补偿因遮挡造成的观测缺失。通过K近邻分析原语局部结构进行剔除，并结合场景的三维表示和扩散式补全技术处理遮挡问题。该方法在真实和合成数据集上的表现均优于现有技术，显著提高了从复杂场景中提取物体的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D Gaussian Splatting技术虽在生成新视角图像方面取得了显著进步，但场景中的特定物体提取仍然面临挑战。</li>
<li>剔除无关原语并利用生成补全技术补偿遮挡是本文提出的新物体提取方法的关键。</li>
<li>通过K近邻分析原语的局部结构来进行剔除，以提高提取性能。</li>
<li>利用场景的三维表示结合扩散式补全技术处理遮挡问题。</li>
<li>剔除和补全之间的协同作用对于提高物体提取性能至关重要。</li>
<li>在真实和合成数据集上的实验表明，该方法在复杂场景中的物体提取性能优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ebb525070c237f1801874786aad1c40c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0f25ef888b5927ae39644883ebd0660.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10d38ed3ac6554f376341b21b4c4e77b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2453cbcccf3664c45f0078d599e8d1df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f41f7fb83ecdc017d4e69fe368e61464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef70af28f5bad6b66b535963e91c02a7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GS-SDF-LiDAR-Augmented-Gaussian-Splatting-and-Neural-SDF-for-Geometrically-Consistent-Rendering-and-Reconstruction"><a href="#GS-SDF-LiDAR-Augmented-Gaussian-Splatting-and-Neural-SDF-for-Geometrically-Consistent-Rendering-and-Reconstruction" class="headerlink" title="GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for   Geometrically Consistent Rendering and Reconstruction"></a>GS-SDF: LiDAR-Augmented Gaussian Splatting and Neural SDF for   Geometrically Consistent Rendering and Reconstruction</h2><p><strong>Authors:Jianheng Liu, Yunfei Wan, Bowen Wang, Chunran Zheng, Jiarong Lin, Fu Zhang</strong></p>
<p>Digital twins are fundamental to the development of autonomous driving and embodied artificial intelligence. However, achieving high-granularity surface reconstruction and high-fidelity rendering remains a challenge. Gaussian splatting offers efficient photorealistic rendering but struggles with geometric inconsistencies due to fragmented primitives and sparse observational data in robotics applications. Existing regularization methods, which rely on render-derived constraints, often fail in complex environments. Moreover, effectively integrating sparse LiDAR data with Gaussian splatting remains challenging. We propose a unified LiDAR-visual system that synergizes Gaussian splatting with a neural signed distance field. The accurate LiDAR point clouds enable a trained neural signed distance field to offer a manifold geometry field, This motivates us to offer an SDF-based Gaussian initialization for physically grounded primitive placement and a comprehensive geometric regularization for geometrically consistent rendering and reconstruction. Experiments demonstrate superior reconstruction accuracy and rendering quality across diverse trajectories. To benefit the community, the codes will be released at <a target="_blank" rel="noopener" href="https://github.com/hku-mars/GS-SDF">https://github.com/hku-mars/GS-SDF</a>. </p>
<blockquote>
<p>数字孪生对自动驾驶和人工智能实体的发展具有重要意义。然而，实现高粒度表面重建和高保真渲染仍然是一个挑战。高斯斑点法虽可提供高效的逼真渲染，但在机器人应用中，由于碎片化的原始数据和稀疏的观察数据，其在处理几何不一致性方面存在困难。现有的依赖于渲染约束的正则化方法往往在复杂环境中失效。此外，如何将稀疏的激光雷达数据与高斯斑点法有效结合仍然是一个挑战。我们提出了一种统一的激光雷达视觉系统，该系统将高斯斑点法与神经有向距离场相结合。准确的激光雷达点云能够使经过训练的神经有向距离场提供流形几何场，这促使我们提供一种基于SDF的高斯初始化，用于物理基础的原始位置放置和全面的几何正则化，以实现几何一致的渲染和重建。实验表明，该方法在多种轨迹上的重建精度和渲染质量均表现优越。为了造福社区，相关代码将在[<a target="_blank" rel="noopener" href="https://github.com/hku-mars/GS-SDF%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82]">https://github.com/hku-mars/GS-SDF上发布。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10170v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>数字孪生在自动驾驶和人工智能领域扮演重要角色，但实现高粒度表面重建和高保真渲染仍存在挑战。高斯贴图技术虽然能进行逼真的渲染，但在机器人应用中因几何不一致性和稀疏观测数据导致的问题却令人头疼。目前依赖渲染派生约束的正则化方法，在复杂环境中常常失效。同时，如何将稀疏的激光雷达数据与高斯贴图技术有效结合也是一个难题。本研究提出一种统一的激光雷达视觉系统，该系统将高斯贴图技术与神经网络距离场相融合。准确的激光雷达点云数据为训练神经网络提供了几何场数据，基于此，我们提出了基于距离场的初始高斯设置和全面的几何正则化方法，以实现几何一致的渲染和重建。实验证明，该方法在多种轨迹上的重建精度和渲染质量均表现优越。相关代码已发布至GitHub（<a target="_blank" rel="noopener" href="https://github.com/hku-mars/GS-SDF%EF%BC%89%E3%80%82">https://github.com/hku-mars/GS-SDF）。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数字孪生在自动驾驶和人工智能领域有重要作用，但实现高粒度表面重建和高保真渲染仍存在挑战。</li>
<li>高斯贴图技术虽然在机器人应用中能够进行逼真的渲染，但存在几何不一致性和稀疏观测数据的问题。</li>
<li>目前依赖渲染派生约束的正则化方法在复杂环境中常常失效。</li>
<li>激光雷达数据与高斯贴图技术的结合是一大挑战。</li>
<li>本研究提出了一种统一的激光雷达视觉系统，融合了高斯贴图技术与神经网络距离场技术。</li>
<li>该系统利用准确的激光雷达点云数据训练神经网络，并基于此提出了基于距离场的初始高斯设置和全面的几何正则化方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10170">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9afc792b3c2cbdf55f4aebb0cdc027e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-266603759591f866a8142d0c5f8ecd5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca06af105f637b44cc15dd8b322ccd1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-904bfdd54812f38c0731f021a69b920a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfeb6977c5371fd17a1750278253803b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fda6323f966b2bba37d67e5c46c65dbf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="3D-Student-Splatting-and-Scooping"><a href="#3D-Student-Splatting-and-Scooping" class="headerlink" title="3D Student Splatting and Scooping"></a>3D Student Splatting and Scooping</h2><p><strong>Authors:Jialin Zhu, Jiangbei Yue, Feixiang He, He Wang</strong></p>
<p>Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student’s t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%. </p>
<blockquote>
<p>最近，3D高斯拼接（3DGS）为新型视角合成提供了新的框架，并在神经渲染和相关应用中引发了新的研究热潮。由于3DGS正在成为许多模型的基础组件，因此对3DGS本身的任何改进都能带来巨大的好处。为此，我们旨在改进3DGS的基本范式和公式。我们认为，作为一种未归一化的混合模型，它既不需要是高斯分布也不需要是拼接。随后，我们提出了一种新的混合模型，由灵活的Student’s t分布组成，包括正向（拼接）和负向（挖掘）密度。我们将我们的模型命名为学生拼接和挖掘，或SSS。在提高表达性的同时，SSS也给学习带来了新的挑战。因此，我们还提出了一种新的有原则性的采样优化方法。通过跨多个数据集、设置和指标的综合评估比较，我们证明了SSS在质量和参数效率方面优于现有方法，例如在使用相似数量的组件时达到匹配或更好的质量，并在减少组件数量的同时获得相当的结果，最多可达82%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10148v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10148">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6dab72fe2be068e71053b9f94306e9ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1d20edb148f03f0cde59cc49001479f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-439b779631b630c82f0545081f2c8302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79f089d98151e391b3120456f408fb26.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GaussHDR-High-Dynamic-Range-Gaussian-Splatting-via-Learning-Unified-3D-and-2D-Local-Tone-Mapping"><a href="#GaussHDR-High-Dynamic-Range-Gaussian-Splatting-via-Learning-Unified-3D-and-2D-Local-Tone-Mapping" class="headerlink" title="GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D   and 2D Local Tone Mapping"></a>GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D   and 2D Local Tone Mapping</h2><p><strong>Authors:Jinfeng Liu, Lingtong Kong, Bo Li, Dan Xu</strong></p>
<p>High dynamic range (HDR) novel view synthesis (NVS) aims to reconstruct HDR scenes by leveraging multi-view low dynamic range (LDR) images captured at different exposure levels. Current training paradigms with 3D tone mapping often result in unstable HDR reconstruction, while training with 2D tone mapping reduces the model’s capacity to fit LDR images. Additionally, the global tone mapper used in existing methods can impede the learning of both HDR and LDR representations. To address these challenges, we present GaussHDR, which unifies 3D and 2D local tone mapping through 3D Gaussian splatting. Specifically, we design a residual local tone mapper for both 3D and 2D tone mapping that accepts an additional context feature as input. We then propose combining the dual LDR rendering results from both 3D and 2D local tone mapping at the loss level. Finally, recognizing that different scenes may exhibit varying balances between the dual results, we introduce uncertainty learning and use the uncertainties for adaptive modulation. Extensive experiments demonstrate that GaussHDR significantly outperforms state-of-the-art methods in both synthetic and real-world scenarios. </p>
<blockquote>
<p>高动态范围（HDR）新颖视图合成（NVS）旨在利用在不同曝光级别下捕获的多视图低动态范围（LDR）图像重建HDR场景。当前使用3D色调映射的训练模式往往会导致HDR重建不稳定，而使用2D色调映射的训练则降低了模型对LDR图像的拟合能力。此外，现有方法中所使用的全局色调映射器会阻碍HDR和LDR表示的学习。为了解决这些挑战，我们提出了GaussHDR，它通过3D高斯喷涂技术统一了3D和2D局部色调映射。具体来说，我们为3D和2D色调映射设计了残差局部色调映射器，该映射器以额外的上下文特征作为输入。然后，我们在损失级别上提出了结合来自3D和2D局部色调映射的两种LDR渲染结果。最后，我们认识到不同场景在双重结果之间可能存在不同的平衡，因此引入了不确定性学习并使用不确定性进行自适应调制。大量实验表明，GaussHDR在合成场景和真实世界场景中均显著优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10143v1">PDF</a> This paper is accepted by CVPR 2025. Project page is available at   <a target="_blank" rel="noopener" href="https://liujf1226.github.io/GaussHDR">https://liujf1226.github.io/GaussHDR</a></p>
<p><strong>Summary</strong><br>     本文提出一种名为GaussHDR的方法，旨在通过结合3D和2D局部色调映射技术，实现高动态范围（HDR）场景的重构。该方法通过3D高斯平铺技术统一了这两种色调映射，设计了一种接受附加上下文特征输入的残差局部色调映射器。在损失层面结合了来自3D和2D局部色调映射的双重LDR渲染结果，并引入不确定性学习来适应不同场景的需求。实验表明，GaussHDR在合成和真实场景中都显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GaussHDR结合了3D和2D局部色调映射技术，旨在实现HDR场景的稳定重构。</li>
<li>现有训练模式在HDR重建时存在不稳定问题，而GaussHDR通过结合两种色调映射技术解决了这一问题。</li>
<li>GaussHDR设计了一种接受附加上下文特征输入的残差局部色调映射器。</li>
<li>方法在损失层面结合了来自3D和2D局部色调映射的双重LDR渲染结果。</li>
<li>GaussHDR引入了不确定性学习，以应对不同场景中双重结果之间的不同平衡。</li>
<li>实验表明，GaussHDR在合成和真实场景中均显著优于当前主流方法。</li>
<li>GaussHDR能够提高模型的容量，更好地适应LDR图像的拟合。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c45c26929f320dfb4ba657ad9da1e5d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa0025c35a77d47f020c9d25161dcd7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b21355123c421b664159a3fb50515b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-205b6c5085c9ceb9c1b81004c0f171a4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MVGSR-Multi-View-Consistency-Gaussian-Splatting-for-Robust-Surface-Reconstruction"><a href="#MVGSR-Multi-View-Consistency-Gaussian-Splatting-for-Robust-Surface-Reconstruction" class="headerlink" title="MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface   Reconstruction"></a>MVGSR: Multi-View Consistency Gaussian Splatting for Robust Surface   Reconstruction</h2><p><strong>Authors:Chenfeng Hou, Qi Xun Yeo, Mengqi Guo, Yongxin Su, Yanyan Li, Gim Hee Lee</strong></p>
<p>3D Gaussian Splatting (3DGS) has gained significant attention for its high-quality rendering capabilities, ultra-fast training, and inference speeds. However, when we apply 3DGS to surface reconstruction tasks, especially in environments with dynamic objects and distractors, the method suffers from floating artifacts and color errors due to inconsistency from different viewpoints. To address this challenge, we propose Multi-View Consistency Gaussian Splatting for the domain of Robust Surface Reconstruction (\textbf{MVGSR}), which takes advantage of lightweight Gaussian models and a {heuristics-guided distractor masking} strategy for robust surface reconstruction in non-static environments. Compared to existing methods that rely on MLPs for distractor segmentation strategies, our approach separates distractors from static scene elements by comparing multi-view feature consistency, allowing us to obtain precise distractor masks early in training. Furthermore, we introduce a pruning measure based on multi-view contributions to reset transmittance, effectively reducing floating artifacts. Finally, a multi-view consistency loss is applied to achieve high-quality performance in surface reconstruction tasks. Experimental results demonstrate that MVGSR achieves competitive geometric accuracy and rendering fidelity compared to the state-of-the-art surface reconstruction algorithms. More information is available on our project page (<a target="_blank" rel="noopener" href="https://mvgsr.github.io/">https://mvgsr.github.io</a>). </p>
<blockquote>
<p>3D高斯融合（3DGS）因其高质量渲染能力、超快训练和推理速度而受到广泛关注。然而，当我们将其应用于表面重建任务，尤其是在动态物体和干扰物存在的环境中，该方法会因从不同视角的不一致性而出现浮动伪影和颜色错误。为了应对这一挑战，我们提出了针对稳健表面重建领域的多视角一致性高斯融合（MVGSR）。该方法利用轻量级高斯模型和启发式引导干扰物遮挡策略，在非静态环境中实现稳健的表面重建。与现有依赖MLP进行干扰物分割策略的方法相比，我们的方法通过比较多视角特征一致性来区分干扰物和静态场景元素，从而在训练早期就能精确获取干扰物遮挡。此外，我们引入了一种基于多视角贡献的修剪度量来重置透射率，有效减少浮动伪影。最后，应用多视角一致性损失，以实现表面重建任务的高性能。实验结果表明，与最新表面重建算法相比，MVGSR在几何精度和渲染保真度方面达到竞争力水平。更多信息请访问我们的项目页面（<a target="_blank" rel="noopener" href="https://mvgsr.github.io)./">https://mvgsr.github.io）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08093v2">PDF</a> project page <a target="_blank" rel="noopener" href="https://mvgsr.github.io/">https://mvgsr.github.io</a></p>
<p><strong>摘要</strong></p>
<p>基于高效的高斯模型及多视角一致性算法，本研究针对非静态环境中的鲁棒表面重建提出一种新的策略Multi-View Consistency Gaussian Splatting（MVGSR）。在具有动态物体与干扰物的环境下，针对现有的基于3DGS的表面重建技术的局限性问题（如视角不一致造成的漂浮伪影及色彩错误），本文创新性地利用特征一致性识别干扰物的机制、修复对目标数据的视角冲突导致的几何表面及重建着色差异等表面重建任务中的关键问题。相较于依赖MLPs的干扰物分割策略，MVGSR在训练初期就能通过对比多视角特征一致性分离出干扰物与静态场景元素。同时引入了基于多视角贡献的重置透光率的方法来降低漂浮伪影的出现频率，确保了重建表面的质量。实验结果表明，MVGSR在几何精度和渲染保真度方面达到了先进的表面重建算法水平。更多详细信息可参见项目主页。总体来说，此研究工作主要展示了全新的角度分离方法和精准的几何精度处理技术对目标数据进行合理的分解、分离与重建工作的重要性。如需更多细节信息，请访问项目主页进行了解。 </p>
<p><strong>关键见解</strong></p>
<ul>
<li>3DGS在高质量渲染、超快训练和推理速度方面备受关注。但在表面重建任务中，特别是在动态对象和干扰物的环境中，存在由于视角不一致导致的浮动伪影和颜色错误的问题。</li>
<li>提出了一种新的鲁棒表面重建策略——MVGSR（多视角一致性高斯飞溅法）。利用轻量化高斯模型和启发式引导的干扰物掩蔽策略来解决非静态环境中的表面重建问题。此方法在早期训练阶段就通过比较多视角特征一致性来分离干扰物和静态场景元素。相较于依赖MLPs的干扰物分割策略更具优势。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08093">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-994d5ae8c7694b1574f74c1628206a13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b46f7875d1a447c298b802787fa1fc08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-427bc491741fe92903fc3a4ce75aae96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1965062cc362de269e1d9ce932ed1962.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DoF-Gaussian-Controllable-Depth-of-Field-for-3D-Gaussian-Splatting"><a href="#DoF-Gaussian-Controllable-Depth-of-Field-for-3D-Gaussian-Splatting" class="headerlink" title="DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting"></a>DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting</h2><p><strong>Authors:Liao Shen, Tianqi Liu, Huiqiang Sun, Jiaqi Li, Zhiguo Cao, Wei Li, Chen Change Loy</strong></p>
<p>Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable success in representing 3D scenes and generating high-quality, novel views in real-time. However, 3D-GS and its variants assume that input images are captured based on pinhole imaging and are fully in focus. This assumption limits their applicability, as real-world images often feature shallow depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-based imaging model based on geometric optics principles to control DoF effects. To ensure accurate scene geometry, we incorporate depth priors adjusted per scene, and we apply defocus-to-focus adaptation to minimize the gap in the circle of confusion. We also introduce a synthetic dataset to assess refocusing capabilities and the model’s ability to learn precise lens parameters. Our framework is customizable and supports various interactive applications. Extensive experiments confirm the effectiveness of our method. Our project is available at <a target="_blank" rel="noopener" href="https://dof-gaussian.github.io/">https://dof-gaussian.github.io</a>. </p>
<blockquote>
<p>近期三维高斯描点（3D-GS）的进展在表示三维场景和实时生成高质量、新颖视图方面取得了显著的成功。然而，3D-GS及其变体假设输入图像是基于针孔成像捕获的，并且完全在焦点内。这一假设限制了其适用性，因为现实世界的图像通常具有较浅的景深（DoF）。在本文中，我们介绍了DoF-Gaussian，这是一种可控景深的三维高斯描点方法。我们基于几何光学原理开发了一种基于镜头的成像模型来控制景深效应。为确保场景几何的准确性，我们根据场景调整了深度优先事项，并采用了失焦到聚焦的适应方法来减小模糊圆之间的间隙。我们还引入了一个合成数据集来评估重新聚焦能力和模型学习精确镜头参数的能力。我们的框架可定制且支持各种交互应用程序。大量实验证实了我们方法的有效性。我们的项目可在<a target="_blank" rel="noopener" href="https://dof-gaussian.github.io找到./">https://dof-gaussian.github.io找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00746v3">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>3D高斯插值（3D-GS）的最新进展在表示3D场景和实时生成高质量、新颖视图方面取得了显著的成功。然而，该方法和其变体假设输入图像是基于针孔成像并完全聚焦的，这限制了其在现实世界中的应用，因为真实图像往往具有较浅的景深（DoF）。本文介绍了一种可控景深的方法DoF-Gaussian，用于3D-GS。我们基于几何光学原理开发了一种透镜成像模型来控制DoF效应。为确保场景几何的准确性，我们根据场景调整了深度先验知识，并应用了失焦到聚焦的适应来减少模糊圆圈的差距。我们还引入了一个合成数据集来评估我们的方法的重新聚焦能力和学习精确镜头参数的能力。我们的框架支持各种交互式应用，并通过广泛的实验验证了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D-GS在表示3D场景和生成高质量视图方面表现出显著成功。</li>
<li>现有方法假设输入图像完全基于针孔成像并聚焦，限制了其在现实场景的应用。</li>
<li>DoF-Gaussian方法解决了上述问题，通过透镜成像模型控制景深效应。</li>
<li>该方法结合场景深度先验知识，确保场景几何的准确性。</li>
<li>通过失焦到聚焦的适应，减少了模糊差距。</li>
<li>引入合成数据集用于评估重新聚焦能力和模型学习精确镜头参数的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00746">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-608a2f94a73117a7c8828606f10c6e2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb0f076551e3e9eb5a2ea2ef822fd3f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdd2917a6f721b5bc2d9ac5f19ff4e71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-920e17b7ee613895183b83765e081fc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17333ead5ffe22a3aeb553a4389b68f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a9ef275cb34913b2e221526eaaba4e0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Extrapolated-Urban-View-Synthesis-Benchmark"><a href="#Extrapolated-Urban-View-Synthesis-Benchmark" class="headerlink" title="Extrapolated Urban View Synthesis Benchmark"></a>Extrapolated Urban View Synthesis Benchmark</h2><p><strong>Authors:Xiangyu Han, Zhen Jia, Boyi Li, Yan Wang, Boris Ivanovic, Yurong You, Lingjie Liu, Yue Wang, Marco Pavone, Chen Feng, Yiming Li</strong></p>
<p>Photorealistic simulators are essential for the training and evaluation of vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis (NVS), a crucial capability that generates diverse unseen viewpoints to accommodate the broad and continuous pose distribution of AVs. Recent advances in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic rendering at real-time speeds and have been widely used in modeling large-scale driving scenes. However, their performance is commonly evaluated using an interpolated setup with highly correlated training and test views. In contrast, extrapolation, where test views largely deviate from training views, remains underexplored, limiting progress in generalizable simulation technology. To address this gap, we leverage publicly available AV datasets with multiple traversals, multiple vehicles, and multiple cameras to build the first Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct both quantitative and qualitative evaluations of state-of-the-art NVS methods across different evaluation settings. Our results show that current NVS methods are prone to overfitting to training views. Besides, incorporating diffusion priors and improving geometry cannot fundamentally improve NVS under large view changes, highlighting the need for more robust approaches and large-scale training. We will release the data to help advance self-driving and urban robotics simulation technology. </p>
<blockquote>
<p>真实感模拟器对于以视觉为中心的自动驾驶汽车的训练和评估至关重要。其核心是新型视图合成（NVS），这是一种能够生成多种未见观点以适应自动驾驶汽车广泛且连续的姿态分布的关键能力。最近的辐射场进展，如3D高斯绘图板，实现了实时速度的逼真渲染，并已广泛应用于模拟大规模驾驶场景。然而，它们的性能通常使用插值设置进行评估，其中训练和测试观点的关联度很高。相比之下，外推（extrapolation）研究则很少进行探索，即测试视图与训练视图有很大差异，这在通用仿真技术的进步方面构成了一个限制。为了弥补这一差距，我们利用具有多次遍历、多辆车和多摄像头的公开自动驾驶数据集建立了第一个外推城市视图合成（EUVS）基准。同时，我们在不同的评估环境下对最先进的新型视图合成方法进行了定量和定性的评估。结果表明，当前的新型视图合成方法容易过度拟合训练视图。此外，引入扩散先验知识和改进几何结构并不能从根本上改善大视角变化下的新型视图合成技术，这凸显了需要更稳健的方法和大规模训练数据。我们将发布这些数据以帮助推进自动驾驶和城市机器人仿真技术的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05256v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ai4ce.github.io/EUVS-Benchmark/">https://ai4ce.github.io/EUVS-Benchmark/</a></p>
<p><strong>Summary</strong></p>
<p>本文强调真实感模拟器对以视觉为中心的自动驾驶汽车（AVs）训练和评估的重要性。核心在于视点合成（NVS），它能生成多样的未见过视点以适应AVs的广泛和连续姿态分布。虽然使用辐射场的新进展如3D高斯拼贴可以在实时速度下实现逼真的渲染，但现有的评估方法主要使用插值设置，训练视图和测试视图高度相关。为了解决这个问题，本文建立了一个名为“Extrapolated Urban View Synthesis (EUVS)”的基准测试平台，该平台利用公开可用的自动驾驶汽车数据集进行构建。同时，对最先进的NVS方法进行了不同评估环境的定量和定性评估。结果显示，当前NVS方法容易过度拟合训练视图，在大视角变化下融入扩散先验和改进几何并不能从根本上改善NVS，这突显了需要更稳健的方法和大规模训练数据的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>真实感模拟器对自动驾驶汽车的训练和评估至关重要。</li>
<li>真实感模拟器的核心能力是视点合成（NVS），可以生成多样的未见过视点。</li>
<li>当前NVS方法主要使用插值设置进行评估，存在对训练视图的过度拟合问题。</li>
<li>EUVS基准测试平台利用公开可用的自动驾驶汽车数据集进行构建，旨在解决现有评估方法的不足。</li>
<li>评估结果显示，当前NVS方法在处理大视角变化时存在局限性。</li>
<li>融合扩散先验和改进几何在解决大视角变化下的NVS问题时效果有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e56a03885d68afaee403e2614ecae8e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c0e8c282afed8d2b8ada0d1295a2fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41b67939d018e9ab31d36070b639b90f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa7f746179eeca6d6597698f6e1f7c82.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SplatAD-Real-Time-Lidar-and-Camera-Rendering-with-3D-Gaussian-Splatting-for-Autonomous-Driving"><a href="#SplatAD-Real-Time-Lidar-and-Camera-Rendering-with-3D-Gaussian-Splatting-for-Autonomous-Driving" class="headerlink" title="SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting   for Autonomous Driving"></a>SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting   for Autonomous Driving</h2><p><strong>Authors:Georg Hess, Carl Lindström, Maryam Fatemi, Christoffer Petersson, Lennart Svensson</strong></p>
<p>Ensuring the safety of autonomous robots, such as self-driving vehicles, requires extensive testing across diverse driving scenarios. Simulation is a key ingredient for conducting such testing in a cost-effective and scalable way. Neural rendering methods have gained popularity, as they can build simulation environments from collected logs in a data-driven manner. However, existing neural radiance field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer from low rendering speeds, limiting their applicability for large-scale testing. While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods are limited to camera data and are unable to render lidar data essential for autonomous driving. To address these limitations, we propose SplatAD, the first 3DGS-based method for realistic, real-time rendering of dynamic scenes for both camera and lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built algorithms to optimize rendering efficiency. Evaluation across three autonomous driving datasets demonstrates that SplatAD achieves state-of-the-art rendering quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing rendering speed over NeRF-based methods by an order of magnitude. See <a target="_blank" rel="noopener" href="https://research.zenseact.com/publications/splatad/">https://research.zenseact.com/publications/splatad/</a> for our project page. </p>
<blockquote>
<p>确保自主机器人（如自动驾驶汽车）的安全需要在各种驾驶场景中进行广泛的测试。仿真是一种以成本效益高和可扩展的方式开展此类测试的关键要素。神经渲染方法很受欢迎，因为它们可以以数据驱动的方式从收集的日志中构建仿真环境。然而，现有的用于相机和激光雷达数据传感器现实渲染的神经辐射场（NeRF）方法存在渲染速度低的问题，限制了它们在大规模测试中的应用。虽然3D高斯拼贴（3DGS）可以实现实时渲染，但当前的方法仅限于相机数据，而无法呈现对自动驾驶至关重要的激光雷达数据。为了解决这些局限性，我们提出了SplatAD，这是基于3DGS的第一种用于相机和激光雷达数据动态场景现实实时渲染的方法。SplatAD使用专门构建的算法准确建模关键传感器特定现象，如滚降效应、激光雷达强度和激光雷达射线中断，以优化渲染效率。在三个自动驾驶数据集上的评估表明，SplatAD实现了最先进的渲染质量，在NVS和重建方面分别提高了+2 PSNR和+3 PSNR，同时提高了NeRF方法的渲染速度一个数量级。有关我们的项目页面，请访问：[<a target="_blank" rel="noopener" href="https://research.zenseact.com/publications/splatad/]">https://research.zenseact.com/publications/splatad/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16816v3">PDF</a> </p>
<p><strong>Summary</strong><br>     为自主机器人如自动驾驶汽车进行测试，需要跨多种驾驶场景进行大量测试。仿真是一种经济高效且可扩展的测试方式。神经渲染方法能够从收集的数据日志中以数据驱动的方式构建仿真环境。然而，现有的神经辐射场（NeRF）方法在处理和渲染相机和激光雷达数据时速度较慢，不适用于大规模测试。而3D高斯喷射技术（3DGS）可以实现实时渲染，但仅限于相机数据，无法渲染对自动驾驶至关重要的激光雷达数据。为解决这些问题，我们提出了SplatAD，这是一种基于3DGS的实时渲染动态场景的方法，既适用于相机数据也适用于激光雷达数据。SplatAD准确模拟了滚动快门效应、激光雷达强度和激光雷达射线丢失等关键传感器特性。在三个自动驾驶数据集上的评估表明，SplatAD的渲染质量达到业界最佳水平，渲染速度比基于NeRF的方法提高了一个数量级。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主机器人的安全性需要通过多样场景的广泛测试来确保。</li>
<li>仿真测试是经济高效且可扩展的测试方式。</li>
<li>神经渲染方法能够从数据日志构建仿真环境。</li>
<li>现有的NeRF方法在处理和渲染相机和激光雷达数据时存在低渲染速度的问题。</li>
<li>3DGS技术可实现相机数据的实时渲染，但无法处理激光雷达数据。</li>
<li>SplatAD是基于3DGS的方法，能实时渲染动态场景，适用于相机和激光雷达数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4935caab7acca3f0b1ffa58553e1b54d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f9922ae649796c2a66c4acbc9c7dcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f561799265f6b718492fcdcb5a29abae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c108f0225490efa8982432428cc046.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f561799265f6b718492fcdcb5a29abae.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-15  Flow-NeRF Joint Learning of Geometry, Poses, and Dense Flow within   Unified Neural Representations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf38765ec00cc7bc29580000016409fe.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-03-15  X-GAN A Generative AI-Powered Unsupervised Model for High-Precision   Segmentation of Retinal Main Vessels toward Early Detection of Glaucoma
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15821.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
