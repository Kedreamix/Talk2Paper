<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  GoT Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-f29f2208ae501d92e2c304904b0b3dea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    47 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-15-æ›´æ–°"><a href="#2025-03-15-æ›´æ–°" class="headerlink" title="2025-03-15 æ›´æ–°"></a>2025-03-15 æ›´æ–°</h1><h2 id="GoT-Unleashing-Reasoning-Capability-of-Multimodal-Large-Language-Model-for-Visual-Generation-and-Editing"><a href="#GoT-Unleashing-Reasoning-Capability-of-Multimodal-Large-Language-Model-for-Visual-Generation-and-Editing" class="headerlink" title="GoT: Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing"></a>GoT: Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing</h2><p><strong>Authors:Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li</strong></p>
<p>Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at <a target="_blank" rel="noopener" href="https://github.com/rongyaofang/GoT">https://github.com/rongyaofang/GoT</a>. </p>
<blockquote>
<p>å½“å‰å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹æ³•ä¸»è¦å°†æ–‡æœ¬æç¤ºä½œä¸ºç›´æ¥è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œè€Œæ²¡æœ‰å¯¹è§†è§‰æ„å›¾å’Œæ˜ç¡®æ“ä½œè¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†æ€ç»´é“¾ç”Ÿæˆï¼ˆGoTï¼‰è¿™ä¸€æ–°é¢–èŒƒå¼ï¼Œé€šè¿‡æ˜ç¡®çš„è¯­è¨€æ¨ç†è¿‡ç¨‹æ¥ç”Ÿæˆå’Œç¼–è¾‘å›¾åƒï¼Œç„¶åå†è¾“å‡ºå›¾åƒã€‚è¿™ç§æ–¹æ³•å°†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘è½¬å˜ä¸ºä»¥æ¨ç†ä¸ºæŒ‡å¯¼çš„æ¡†æ¶ï¼Œåˆ†æè¯­ä¹‰å…³ç³»å’Œç©ºé—´å¸ƒå±€ã€‚æˆ‘ä»¬å®šä¹‰äº†GoTçš„å…¬å¼ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡GoTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡900ä¸‡ä¸ªæ ·æœ¬ï¼Œè¯¦ç»†çš„æ¨ç†é“¾æ•æ‰äº†è¯­ä¹‰-ç©ºé—´å…³ç³»ã€‚ä¸ºäº†åˆ©ç”¨GoTçš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé›†æˆäº†Qwen2.5-VLè¿›è¡Œæ¨ç†é“¾ç”Ÿæˆï¼Œé€šè¿‡æˆ‘ä»¬æ–°é¢–çš„è¯­ä¹‰-ç©ºé—´æŒ‡å¯¼æ¨¡å—å¢å¼ºç«¯åˆ°ç«¯æ‰©æ•£æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GoTæ¡†æ¶åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å®ç°äº†äº¤äº’å¼è§†è§‰ç”Ÿæˆï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿæ˜ç¡®ä¿®æ”¹æ¨ç†æ­¥éª¤ï¼Œè¿›è¡Œç²¾ç¡®å›¾åƒè°ƒæ•´ã€‚GoTå¼€åˆ›äº†æ¨ç†é©±åŠ¨è§†è§‰ç”Ÿæˆå’Œç¼–è¾‘çš„æ–°æ–¹å‘ï¼Œç”Ÿæˆçš„å›¾åƒæ›´å¥½åœ°ç¬¦åˆäººç±»æ„å›¾ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/rongyaofang/GoT%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/rongyaofang/GoTå…¬å¼€äº†æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10639v1">PDF</a> Dataset and models are released in <a target="_blank" rel="noopener" href="https://github.com/rongyaofang/GoT">https://github.com/rongyaofang/GoT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºç”Ÿæˆå¼æ€ç»´é“¾ï¼ˆGeneration Chain-of-Thoughtï¼ŒGoTï¼‰çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘æ–°èŒƒå¼ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®çš„è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹æ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ï¼Œå®ç°äº†å¯¹è¯­ä¹‰å…³ç³»å’Œç©ºé—´å¸ƒå±€çš„åˆ†æã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†å¤§è§„æ¨¡GoTæ•°æ®é›†ï¼Œå¹¶å¼€å‘äº†ç»“åˆQwen2.5-VLæ¨ç†é“¾ç”Ÿæˆä¸æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå…¶ä¸­èå…¥æ–°å‹è¯­ä¹‰ç©ºé—´å¼•å¯¼æ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼ŒGoTæ¡†æ¶åœ¨ç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒGoTæ”¯æŒäº¤äº’å¼è§†è§‰ç”Ÿæˆï¼Œä½¿ç”¨æˆ·å¯ç²¾ç¡®è°ƒæ•´å›¾åƒç»†èŠ‚ã€‚è¯¥ç ”ç©¶ä¸ºæ¨ç†é©±åŠ¨è§†è§‰ç”Ÿæˆä¸ç¼–è¾‘å¼€åˆ›äº†æ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ç”Ÿæˆå¼æ€ç»´é“¾ï¼ˆGoTï¼‰æ–°èŒƒå¼ï¼Œç”¨äºå›¾åƒç”Ÿæˆä¸ç¼–è¾‘ã€‚</li>
<li>GoTé€šè¿‡è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ï¼Œæ¶‰åŠè¯­ä¹‰å…³ç³»å’Œç©ºé—´å¸ƒå±€åˆ†æã€‚</li>
<li>æ„å»ºå¤§è§„æ¨¡GoTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡900ä¸‡æ ·æœ¬ï¼Œè¯¦ç»†è®°å½•æ¨ç†é“¾ä¸­çš„è¯­ä¹‰ç©ºé—´å…³ç³»ã€‚</li>
<li>å¼€å‘å‡ºç»“åˆQwen2.5-VLä¸æ‰©æ•£æ¨¡å‹çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¹¶èå…¥è¯­ä¹‰ç©ºé—´å¼•å¯¼æ¨¡å—ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºGoTæ¡†æ¶åœ¨ç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶ŠåŸºå‡†æ¨¡å‹ã€‚</li>
<li>GoTæ”¯æŒäº¤äº’å¼è§†è§‰ç”Ÿæˆï¼Œå…è®¸ç”¨æˆ·ç²¾ç¡®è°ƒæ•´å›¾åƒç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a045f0153212d728b24a826020406583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5a9d5272a621f3db6277ffed7e2614d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b1bba85b6372539976092d24981b24e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90e259d326292169340539650ba89083.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Facial-Privacy-Protection-via-Weakening-Diffusion-Purification"><a href="#Enhancing-Facial-Privacy-Protection-via-Weakening-Diffusion-Purification" class="headerlink" title="Enhancing Facial Privacy Protection via Weakening Diffusion Purification"></a>Enhancing Facial Privacy Protection via Weakening Diffusion Purification</h2><p><strong>Authors:Ali Salar, Qing Liu, Yingli Tian, Guoying Zhao</strong></p>
<p>The rapid growth of social media has led to the widespread sharing of individual portrait images, which pose serious privacy risks due to the capabilities of automatic face recognition (AFR) systems for mass surveillance. Hence, protecting facial privacy against unauthorized AFR systems is essential. Inspired by the generation capability of the emerging diffusion models, recent methods employ diffusion models to generate adversarial face images for privacy protection. However, they suffer from the diffusion purification effect, leading to a low protection success rate (PSR). In this paper, we first propose learning unconditional embeddings to increase the learning capacity for adversarial modifications and then use them to guide the modification of the adversarial latent code to weaken the diffusion purification effect. Moreover, we integrate an identity-preserving structure to maintain structural consistency between the original and generated images, allowing human observers to recognize the generated image as having the same identity as the original. Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and LADN, demonstrate the superiority of our approach. The protected faces generated by our method outperform those produced by existing facial privacy protection approaches in terms of transferability and natural appearance. </p>
<blockquote>
<p>éšç€ç¤¾äº¤åª’ä½“çš„å¿«é€Ÿå‘å±•ï¼Œä¸ªäººè‚–åƒç…§ç‰‡çš„å…±äº«æ—¥ç›Šæ™®éï¼Œç”±äºå¤§è§„æ¨¡ç›‘æ§çš„è‡ªåŠ¨äººè„¸è¯†åˆ«ï¼ˆAFRï¼‰ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œè¿™å¸¦æ¥äº†ä¸¥é‡çš„éšç§é£é™©ã€‚å› æ­¤ï¼Œä¿æŠ¤é¢éƒ¨éšç§å…å—æœªç»æˆæƒçš„äººè„¸è¯†åˆ«ç³»ç»Ÿçš„è¯†åˆ«è‡³å…³é‡è¦ã€‚å—æ–°å…´æ‰©æ•£æ¨¡å‹ç”Ÿæˆèƒ½åŠ›çš„å¯å‘ï¼Œæœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹æŠ—æ€§äººè„¸å›¾åƒæ¥è¿›è¡Œéšç§ä¿æŠ¤ã€‚ç„¶è€Œï¼Œå®ƒä»¬å—åˆ°æ‰©æ•£å‡€åŒ–æ•ˆæœçš„å½±å“ï¼Œå¯¼è‡´ä¿æŠ¤æˆåŠŸç‡ï¼ˆPSRï¼‰è¾ƒä½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºå­¦ä¹ æ— æ¡ä»¶åµŒå…¥ï¼Œä»¥å¢åŠ å¯¹æŠ—æ€§ä¿®æ”¹çš„å­¦ä¹ èƒ½åŠ›ï¼Œç„¶åä½¿ç”¨å®ƒä»¬æ¥æŒ‡å¯¼å¯¹æŠ—æ€§æ½œåœ¨ä»£ç çš„ä¿®æ”¹ï¼Œä»¥å‡å¼±æ‰©æ•£å‡€åŒ–æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´åˆäº†èº«ä»½ä¿æŒç»“æ„ï¼Œä»¥ä¿æŒåŸå§‹å’Œç”Ÿæˆå›¾åƒä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ï¼Œä½¿äººç±»è§‚å¯Ÿè€…èƒ½å¤Ÿè®¤å‡ºç”Ÿæˆå›¾åƒä¸åŸå§‹å›¾åƒå…·æœ‰ç›¸åŒçš„èº«ä»½ã€‚åœ¨CelebA-HQå’ŒLADNä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ç”±æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„å—ä¿æŠ¤çš„äººè„¸åœ¨å¯è½¬ç§»æ€§å’Œè‡ªç„¶å¤–è§‚æ–¹é¢ä¼˜äºç°æœ‰é¢éƒ¨éšç§ä¿æŠ¤æ–¹æ³•ç”Ÿæˆçš„äººè„¸ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10350v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“ä¸Šä¸ªäººè‚–åƒç…§ç‰‡çš„å¹¿æ³›åˆ†äº«å¸¦æ¥äº†ä¸¥é‡çš„éšç§é£é™©ï¼Œå› ä¸ºè‡ªåŠ¨é¢éƒ¨è¯†åˆ«ï¼ˆAFRï¼‰ç³»ç»Ÿå¯ä»¥è¿›è¡Œå¤§è§„æ¨¡ç›‘æ§ã€‚ä¿æŠ¤é¢éƒ¨éšç§å…å—æœªç»æˆæƒçš„AFRç³»ç»Ÿä¾µæ‰°è‡³å…³é‡è¦ã€‚å—æ–°å…´æ‰©æ•£æ¨¡å‹ç”Ÿæˆèƒ½åŠ›çš„å¯å‘ï¼Œæœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹æŠ—æ€§é¢éƒ¨å›¾åƒä»¥å®ç°éšç§ä¿æŠ¤ï¼Œä½†å®ƒä»¬å—åˆ°æ‰©æ•£å‡€åŒ–æ•ˆåº”çš„å½±å“ï¼Œå¯¼è‡´ä¿æŠ¤æˆåŠŸç‡ï¼ˆPSRï¼‰è¾ƒä½ã€‚æœ¬æ–‡é¦–å…ˆæå‡ºå­¦ä¹ æ— æ¡ä»¶åµŒå…¥ï¼Œä»¥å¢åŠ å¯¹æŠ—æ€§ä¿®æ”¹çš„å­¦ä¹ å®¹é‡ï¼Œç„¶ååˆ©ç”¨å®ƒä»¬æ¥æŒ‡å¯¼å¯¹æŠ—æ€§æ½œåœ¨ä»£ç çš„ä¿®æ”¹ï¼Œä»¥å‰Šå¼±æ‰©æ•£å‡€åŒ–æ•ˆåº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ•´åˆäº†ä¸€ç§èº«ä»½ä¿ç•™ç»“æ„ï¼Œä»¥ä¿æŒåŸå§‹å›¾åƒå’Œç”Ÿæˆå›¾åƒä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ï¼Œä½¿äººç±»è§‚å¯Ÿè€…èƒ½å¤Ÿè¯†åˆ«ç”Ÿæˆå›¾åƒä¸åŸå§‹å›¾åƒå…·æœ‰ç›¸åŒçš„èº«ä»½ã€‚åœ¨CelebA-HQå’ŒLADNä¸¤ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„å—ä¿æŠ¤äººè„¸åœ¨ä¼ è¾“æ€§å’Œè‡ªç„¶å¤–è§‚æ–¹é¢ä¼˜äºç°æœ‰é¢éƒ¨éšç§ä¿æŠ¤æ–¹æ³•ç”Ÿæˆçš„äººè„¸ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“ä¸Šä¸ªäººè‚–åƒçš„å¹¿æ³›åˆ†äº«å¼•å‘äº†ç”±äºè‡ªåŠ¨é¢éƒ¨è¯†åˆ«ï¼ˆAFRï¼‰ç³»ç»Ÿçš„éšç§é£é™©ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è¢«ç”¨äºç”Ÿæˆå¯¹æŠ—æ€§é¢éƒ¨å›¾åƒä»¥å®ç°éšç§ä¿æŠ¤ï¼Œä½†å­˜åœ¨æ‰©æ•£å‡€åŒ–æ•ˆåº”ï¼Œå¯¼è‡´ä¿æŠ¤æˆåŠŸç‡è¾ƒä½ã€‚</li>
<li>æå‡ºå­¦ä¹ æ— æ¡ä»¶åµŒå…¥ä»¥æé«˜å¯¹æŠ—æ€§ä¿®æ”¹çš„å­¦ä¹ å®¹é‡ï¼Œå¹¶å‰Šå¼±æ‰©æ•£å‡€åŒ–æ•ˆåº”ã€‚</li>
<li>æ•´åˆèº«ä»½ä¿ç•™ç»“æ„ï¼Œä¿æŒåŸå§‹å’Œç”Ÿæˆå›¾åƒä¹‹é—´çš„ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨CelebA-HQå’ŒLADNæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜è¶Šï¼Œç”Ÿæˆçš„å—ä¿æŠ¤äººè„¸åœ¨ä¼ è¾“æ€§å’Œè‡ªç„¶å¤–è§‚æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¯¹æŠ—æœªç»æˆæƒçš„AFRç³»ç»Ÿï¼Œæé«˜é¢éƒ¨éšç§ä¿æŠ¤æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-667dcac9d664408ace3aa4d639ae5bd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a73aabfe62c378096263e206666cae6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9decef1f4bf771f9e2bcb5a9fcd91f14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adacfa8e7958d99977d3f621ab31a272.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AdvPaint-Protecting-Images-from-Inpainting-Manipulation-via-Adversarial-Attention-Disruption"><a href="#AdvPaint-Protecting-Images-from-Inpainting-Manipulation-via-Adversarial-Attention-Disruption" class="headerlink" title="AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial   Attention Disruption"></a>AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial   Attention Disruption</h2><p><strong>Authors:Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, Sung-eui Yoon</strong></p>
<p>The outstanding capability of diffusion models in generating high-quality images poses significant threats when misused by adversaries. In particular, we assume malicious adversaries exploiting diffusion models for inpainting tasks, such as replacing a specific region with a celebrity. While existing methods for protecting images from manipulation in diffusion-based generative models have primarily focused on image-to-image and text-to-image tasks, the challenge of preventing unauthorized inpainting has been rarely addressed, often resulting in suboptimal protection performance. To mitigate inpainting abuses, we propose ADVPAINT, a novel defensive framework that generates adversarial perturbations that effectively disrupt the adversaryâ€™s inpainting tasks. ADVPAINT targets the self- and cross-attention blocks in a target diffusion inpainting model to distract semantic understanding and prompt interactions during image generation. ADVPAINT also employs a two-stage perturbation strategy, dividing the perturbation region based on an enlarged bounding box around the object, enhancing robustness across diverse masks of varying shapes and sizes. Our experimental results demonstrate that ADVPAINTâ€™s perturbations are highly effective in disrupting the adversaryâ€™s inpainting tasks, outperforming existing methods; ADVPAINT attains over a 100-point increase in FID and substantial decreases in precision. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢çš„å‡ºè‰²èƒ½åŠ›ï¼Œå½“è¢«æ¶æ„å¯¹æ‰‹è¯¯ç”¨æ—¶ï¼Œæ„æˆäº†é‡å¤§å¨èƒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‡è®¾æ¶æ„å¯¹æ‰‹æ­£åœ¨åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒä¿®å¤ä»»åŠ¡ï¼Œä¾‹å¦‚ç”¨åäººæ›¿æ¢ç‰¹å®šåŒºåŸŸã€‚è™½ç„¶ç°æœ‰çš„ä¿æŠ¤å›¾åƒå…å—åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹æ“çºµçš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å›¾åƒåˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸Šï¼Œä½†é˜²æ­¢æœªç»æˆæƒçš„å›¾åƒä¿®å¤çš„æŒ‘æˆ˜å¾ˆå°‘å¾—åˆ°é‡è§†ï¼Œå¾€å¾€å¯¼è‡´ä¿æŠ¤æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†ç¼“è§£å›¾åƒä¿®å¤æ»¥ç”¨çš„æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†ADVPAINTè¿™ä¸€æ–°å‹é˜²å¾¡æ¡†æ¶ï¼Œå®ƒç”Ÿæˆå¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œæœ‰æ•ˆåœ°ç ´åäº†å¯¹æ‰‹çš„å›¾åƒä¿®å¤ä»»åŠ¡ã€‚ADVPAINTé’ˆå¯¹ç›®æ ‡æ‰©æ•£ä¿®å¤æ¨¡å‹ä¸­çš„è‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ï¼Œå¹²æ‰°è¯­ä¹‰ç†è§£å’Œå›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„äº¤äº’ã€‚ADVPAINTè¿˜é‡‡ç”¨ä¸¤é˜¶æ®µæ‰°åŠ¨ç­–ç•¥ï¼Œæ ¹æ®å¯¹è±¡å‘¨å›´æ‰©å¤§çš„è¾¹ç•Œæ¡†åˆ’åˆ†æ‰°åŠ¨åŒºåŸŸï¼Œæé«˜äº†åœ¨å„ç§å½¢çŠ¶å’Œå¤§å°çš„é¢å…·ä¸‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒADVPAINTçš„æ‰°åŠ¨åœ¨ç ´åå¯¹æ‰‹çš„å›¾åƒä¿®å¤ä»»åŠ¡æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼›ADVPAINTçš„FIDå¾—åˆ†æé«˜äº†è¶…è¿‡100ç‚¹ï¼Œç²¾ç¡®åº¦ä¹Ÿå¤§å¹…ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10081v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å…·æœ‰å‡ºè‰²èƒ½åŠ›ï¼Œä½†ä¸€æ—¦è¢«æ¶æ„å¯¹æ‰‹æ»¥ç”¨ï¼Œå°±ä¼šæ„æˆé‡å¤§å¨èƒã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ä¸­çš„å›¾åƒåˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡çš„å›¾åƒä¿æŠ¤ï¼Œå¾ˆå°‘è§£å†³é˜²æ­¢æœªç»æˆæƒçš„è¡¥å…¨æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¿æŠ¤æ€§èƒ½ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ADVPAINTï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§æ‰°åŠ¨æ¥æœ‰æ•ˆç ´åå¯¹æ‰‹çš„å›¾åƒè¡¥å…¨ä»»åŠ¡ã€‚ADVPAINTé’ˆå¯¹ç›®æ ‡æ‰©æ•£è¡¥å…¨æ¨¡å‹ä¸­çš„è‡ªæˆ‘å’Œè·¨æ³¨æ„åŠ›å—ï¼Œä»¥åˆ†æ•£è¯­ä¹‰ç†è§£å’Œç”Ÿæˆå›¾åƒæ—¶çš„æç¤ºäº¤äº’ã€‚ADVPAINTè¿˜é‡‡ç”¨ä¸¤é˜¶æ®µæ‰°åŠ¨ç­–ç•¥ï¼Œæ ¹æ®å¯¹è±¡å‘¨å›´æ‰©å¤§çš„è¾¹ç•Œæ¡†åˆ’åˆ†æ‰°åŠ¨åŒºåŸŸï¼Œæé«˜äº†å¯¹å„ç§å½¢çŠ¶å’Œå¤§å°ä¸åŒæ©ç çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒADVPAINTçš„æ‰°åŠ¨åœ¨ç ´åå¯¹æ‰‹çš„å›¾åƒè¡¥å…¨ä»»åŠ¡æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨FIDä¸Šå®ç°äº†è¶…è¿‡100ç‚¹çš„æé«˜ï¼ŒåŒæ—¶åœ¨ç²¾åº¦ä¸Šä¹Ÿæœ‰å¤§å¹…ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„èƒ½åŠ›å¦‚æœè¢«æ¶æ„å¯¹æ‰‹æ»¥ç”¨ï¼Œå°†ä¼šå¸¦æ¥ä¸¥é‡å¨èƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ä¸­çš„å›¾åƒåˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°å›¾åƒä»»åŠ¡çš„ä¿æŠ¤ï¼Œå¯¹é˜²æ­¢æœªç»æˆæƒçš„è¡¥å…¨æŒ‘æˆ˜è§£å†³ä¸è¶³ã€‚</li>
<li>ADVPAINTæ˜¯ä¸€ç§æ–°çš„é˜²å¾¡æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§æ‰°åŠ¨æ¥æœ‰æ•ˆç ´åå¯¹æ‰‹çš„å›¾åƒè¡¥å…¨ä»»åŠ¡ã€‚</li>
<li>ADVPAINTé’ˆå¯¹ç›®æ ‡æ‰©æ•£è¡¥å…¨æ¨¡å‹ä¸­çš„è‡ªæˆ‘å’Œè·¨æ³¨æ„åŠ›å—è¿›è¡Œå¹²æ‰°ã€‚</li>
<li>ADVPAINTé‡‡ç”¨ä¸¤é˜¶æ®µæ‰°åŠ¨ç­–ç•¥æ¥æé«˜å¯¹å„ç§å½¢çŠ¶å’Œå¤§å°ä¸åŒæ©ç çš„é²æ£’æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒADVPAINTåœ¨ç ´åå¯¹æ‰‹çš„å›¾åƒè¡¥å…¨ä»»åŠ¡æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f76ca98a3dc58fa8011f1c95be9ba221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c966f909ebc280de2d6297d0ac49be27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f393eee8b79a836a8d192d52e77c2a8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Investigating-and-Improving-Counter-Stereotypical-Action-Relation-in-Text-to-Image-Diffusion-Models"><a href="#Investigating-and-Improving-Counter-Stereotypical-Action-Relation-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Investigating and Improving Counter-Stereotypical Action Relation in   Text-to-Image Diffusion Models"></a>Investigating and Improving Counter-Stereotypical Action Relation in   Text-to-Image Diffusion Models</h2><p><strong>Authors:Sina Malakouti, Adriana Kovashka</strong></p>
<p>Text-to-image diffusion models consistently fail at generating counter-stereotypical action relationships (e.g., â€œmouse chasing catâ€), defaulting to frequent stereotypes even when explicitly prompted otherwise. Through systematic investigation, we discover this limitation stems from distributional biases rather than inherent model constraints. Our key insight reveals that while models fail on rare compositions when their inversions are common, they can successfully generate similar intermediate compositions (e.g., â€œmouse chasing boyâ€). To test this hypothesis, we develop a Role-Bridging Decomposition framework that leverages these intermediates to gradually teach rare relationships without architectural modifications. We introduce ActionBench, a comprehensive benchmark specifically designed to evaluate action-based relationship generation across stereotypical and counter-stereotypical configurations. Our experiments validate that intermediate compositions indeed facilitate counter-stereotypical generation, with both automatic metrics and human evaluations showing significant improvements over existing approaches. This work not only identifies fundamental biases in current text-to-image systems but demonstrates a promising direction for addressing them through compositional reasoning. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆååˆ»æ¿åŠ¨ä½œå…³ç³»ï¼ˆä¾‹å¦‚â€œè€é¼ è¿½çŒ«â€ï¼‰æ—¶æŒç»­å¤±è´¥ï¼Œå³ä½¿åœ¨æ˜ç¡®æç¤ºä¸‹ä¹Ÿé»˜è®¤é‡‡ç”¨é¢‘ç¹å‡ºç°çš„åˆ»æ¿å°è±¡ã€‚é€šè¿‡ç³»ç»Ÿè°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸€å±€é™æ€§æºäºåˆ†å¸ƒåè§ï¼Œè€Œéå›ºæœ‰çš„æ¨¡å‹çº¦æŸã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè™½ç„¶æ¨¡å‹åœ¨å¸¸è§åè½¬çš„æƒ…å†µä¸‹æ— æ³•å¤„ç†ç½•è§çš„ç»„åˆï¼Œä½†å®ƒä»¬å¯ä»¥æˆåŠŸç”Ÿæˆç±»ä¼¼çš„ä¸­é—´ç»„åˆï¼ˆä¾‹å¦‚ï¼Œâ€œè€é¼ è¿½å­©å­â€ï¼‰ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè§’è‰²æ¡¥æ¥åˆ†è§£æ¡†æ¶ï¼Œåˆ©ç”¨è¿™äº›ä¸­é—´ç»“æ„æ¥é€æ­¥æ•™æˆç½•è§çš„å…³ç³»ï¼Œæ— éœ€è¿›è¡Œæ¶æ„ä¿®æ”¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ActionBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°åˆ»æ¿å’Œéåˆ»æ¿é…ç½®ä¸­çš„åŸºäºåŠ¨ä½œçš„å…³ç³»ç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†ä¸­é—´ç»„åˆç¡®å®æœ‰åŠ©äºååˆ»æ¿ç”Ÿæˆï¼Œè‡ªåŠ¨æŒ‡æ ‡å’Œäººç±»è¯„ä¼°å‡æ˜¾ç¤ºå¯¹ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—æ”¹å–„ã€‚è¿™é¡¹å·¥ä½œä¸ä»…è¯†åˆ«äº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„åŸºç¡€åè§ï¼Œè€Œä¸”å±•ç¤ºäº†ä¸€ä¸ªé€šè¿‡ç»„åˆæ¨ç†è§£å†³å®ƒä»¬çš„å……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10037v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆååˆ»æ¿åŠ¨ä½œå…³ç³»ï¼ˆå¦‚â€œè€é¼ è¿½é€çŒ«â€ï¼‰æ—¶æŒç»­å¤±è´¥ï¼Œå³ä½¿æ˜ç¡®æç¤ºä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºåˆ†å¸ƒåè§è€Œéæ¨¡å‹æœ¬èº«çš„é™åˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹åœ¨åè½¬ç½•è§ç»„åˆæ—¶å¤±è´¥ï¼Œä½†èƒ½æˆåŠŸç”Ÿæˆç±»ä¼¼çš„ä¸­é—´ç»„åˆï¼ˆå¦‚â€œè€é¼ è¿½é€ç”·å­©â€ï¼‰ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸€å‡è®¾ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§åŸºäºè§’è‰²è½¬æ¢åˆ†è§£çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸­é—´çŠ¶æ€é€æ­¥å­¦ä¹ ç½•è§å…³ç³»ï¼Œæ— éœ€å¯¹æ¶æ„è¿›è¡Œä¿®æ”¹ã€‚åŒæ—¶ï¼Œä»–ä»¬å¼•å…¥äº†ActionBenchåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°åŸºäºåŠ¨ä½œçš„å…³ç³»ç”Ÿæˆåœ¨åˆ»æ¿å’Œéåˆ»æ¿é…ç½®ä¸‹çš„è¡¨ç°ã€‚å®éªŒè¯æ˜ï¼Œä¸­é—´ç»„åˆç¡®å®æœ‰åŠ©äºç”Ÿæˆååˆ»æ¿åŠ¨ä½œï¼Œåœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­éƒ½æ˜¾ç¤ºå‡ºå¯¹ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å½“å‰æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿçš„åŸºç¡€åè§ï¼Œè¿˜å±•ç¤ºäº†é€šè¿‡ç»„åˆæ¨ç†è§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆååˆ»æ¿åŠ¨ä½œå…³ç³»æ—¶å­˜åœ¨å¤±è´¥ç°è±¡ã€‚</li>
<li>å¤±è´¥çš„åŸå› è¢«å½’ç»“ä¸ºåˆ†å¸ƒåè§ï¼Œè€Œéæ¨¡å‹æœ¬èº«çš„é™åˆ¶ã€‚</li>
<li>æ¨¡å‹åœ¨ç”Ÿæˆç½•è§ç»„åˆæ—¶é‡åˆ°å›°éš¾ï¼Œä½†èƒ½æˆåŠŸç”Ÿæˆç±»ä¼¼çš„ä¸­é—´ç»„åˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè§’è‰²è½¬æ¢åˆ†è§£çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸­é—´çŠ¶æ€é€æ­¥å­¦ä¹ ç½•è§å…³ç³»ã€‚</li>
<li>å¼•å…¥äº†ActionBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç³»ç»Ÿåœ¨å¤„ç†åŠ¨ä½œå…³ç³»ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä¸­é—´ç»„åˆæœ‰åŠ©äºç”Ÿæˆååˆ»æ¿åŠ¨ä½œï¼Œå¹¶åœ¨è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-715cfb776fcce941234de4a1f46a273c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19125cd6b9108489da8f3151a719a9a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4293ffa44533077b68aac521808e8ec3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Channel-wise-Noise-Scheduled-Diffusion-for-Inverse-Rendering-in-Indoor-Scenes"><a href="#Channel-wise-Noise-Scheduled-Diffusion-for-Inverse-Rendering-in-Indoor-Scenes" class="headerlink" title="Channel-wise Noise Scheduled Diffusion for Inverse Rendering in Indoor   Scenes"></a>Channel-wise Noise Scheduled Diffusion for Inverse Rendering in Indoor   Scenes</h2><p><strong>Authors:JunYong Choi, Min-Cheol Sagong, SeokYeong Lee, Seung-Won Jung, Ig-Jae Kim, Junghyun Cho</strong></p>
<p>We propose a diffusion-based inverse rendering framework that decomposes a single RGB image into geometry, material, and lighting. Inverse rendering is inherently ill-posed, making it difficult to predict a single accurate solution. To address this challenge, recent generative model-based methods aim to present a range of possible solutions. However, finding a single accurate solution and generating diverse solutions can be conflicting. In this paper, we propose a channel-wise noise scheduling approach that allows a single diffusion model architecture to achieve two conflicting objectives. The resulting two diffusion models, trained with different channel-wise noise schedules, can predict a single highly accurate solution and present multiple possible solutions. The experimental results demonstrate the superiority of our two models in terms of both diversity and accuracy, which translates to enhanced performance in downstream applications such as object insertion and material editing. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„é€†å‘æ¸²æŸ“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å°†å•ä¸ªRGBå›¾åƒåˆ†è§£ä¸ºå‡ ä½•ã€æè´¨å’Œå…‰ç…§ã€‚é€†å‘æ¸²æŸ“æœ¬è´¨ä¸Šæ˜¯ç—…æ€çš„ï¼Œå› æ­¤å¾ˆéš¾é¢„æµ‹å•ä¸ªå‡†ç¡®è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ€è¿‘çš„åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•æ—¨åœ¨æå‡ºä¸€ç³»åˆ—å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œæ‰¾åˆ°å•ä¸ªå‡†ç¡®è§£å¹¶ç”Ÿæˆå¤šç§è§£å†³æ–¹æ¡ˆå¯èƒ½ä¼šå­˜åœ¨å†²çªã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šé“å™ªå£°è°ƒåº¦æ–¹æ³•ï¼Œå…è®¸å•ä¸ªæ‰©æ•£æ¨¡å‹æ¶æ„å®ç°ä¸¤ä¸ªç›¸äº’å†²çªçš„ç›®æ ‡ã€‚é€šè¿‡ç”¨ä¸åŒé€šé“å™ªå£°è°ƒåº¦è®­ç»ƒçš„ä¸¤ç§æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹å•ä¸ªé«˜åº¦å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å‘ˆç°å¤šç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¸¤ç§æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¿™è½¬åŒ–ä¸ºä¸‹æ¸¸åº”ç”¨å¦‚å¯¹è±¡æ’å…¥å’Œææ–™ç¼–è¾‘çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09993v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é€†å‘æ¸²æŸ“æ¡†æ¶ï¼Œå¯å°†å•ä¸ªRGBå›¾åƒåˆ†è§£ä¸ºå‡ ä½•ã€æè´¨å’Œå…‰ç…§ä¿¡æ¯ã€‚é€†å‘æ¸²æŸ“æœ¬è´¨ä¸Šæ˜¯ç—…æ€é—®é¢˜ï¼Œéš¾ä»¥é¢„æµ‹å•ä¸€å‡†ç¡®è§£ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é€šé“å™ªå£°è°ƒåº¦æ–¹æ³•ï¼Œä½¿å•ä¸€æ‰©æ•£æ¨¡å‹æ¶æ„åŒæ—¶å®ç°ä¸¤ä¸ªç›¸äº’å†²çªçš„ç›®æ ‡ï¼šé¢„æµ‹å•ä¸€é«˜åº¦å‡†ç¡®çš„è§£å’Œå‘ˆç°å¤šç§å¯èƒ½çš„è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä¸åŒçš„é€šé“å™ªå£°è°ƒåº¦è®­ç»ƒçš„ä¸¤ç§æ‰©æ•£æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œè¿›è€Œåœ¨ç‰©ä½“æ’å…¥å’Œææ–™ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é€†å‘æ¸²æŸ“æ¡†æ¶ï¼Œå¯å°†å›¾åƒåˆ†è§£ä¸ºå‡ ä½•ã€æè´¨å’Œå…‰ç…§ä¿¡æ¯ã€‚</li>
<li>é€†å‘æ¸²æŸ“æ˜¯ç—…æ€é—®é¢˜ï¼Œéš¾ä»¥è·å¾—å•ä¸€å‡†ç¡®è§£ã€‚</li>
<li>é€šé“å™ªå£°è°ƒåº¦æ–¹æ³•ä½¿å•ä¸€æ‰©æ•£æ¨¡å‹èƒ½åŒæ—¶å®ç°é¢„æµ‹å•ä¸€é«˜åº¦å‡†ç¡®çš„è§£å’Œå‘ˆç°å¤šç§å¯èƒ½çš„è§£è¿™ä¸¤ä¸ªç›¸äº’å†²çªçš„ç›®æ ‡ã€‚</li>
<li>é€šè¿‡ä¸åŒçš„é€šé“å™ªå£°è°ƒåº¦è®­ç»ƒçš„ä¸¤ç§æ‰©æ•£æ¨¡å‹åœ¨å¤šæ ·æ€§å’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®éªŒåº”ç”¨ä¸­å®ç°äº†é«˜åº¦å‡†ç¡®çš„è§£ç”Ÿæˆå’Œå¤šç§è§£çš„å‘ˆç°ã€‚</li>
<li>è¿™ç§æ–¹æ³•èƒ½æé«˜ä¸‹æ¸¸åº”ç”¨çš„æ€§èƒ½ï¼Œå¦‚ç‰©ä½“æ’å…¥å’Œææ–™ç¼–è¾‘ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09993">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d6519e1ebcd876d0c8e5f854050c9c89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f29f2208ae501d92e2c304904b0b3dea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-008a2dbfee25a44a824d6fbad717daea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a53ff1277ee8471e556a309cf705ba9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-10702ecc2d89c654a13306c9c7377f94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66d77a0514b6fab22c6bac80300aa1f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d15526446209807ba575087bbe016da2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-facca4e718f69c8bcb3d51e01d4536b5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VideoMerge-Towards-Training-free-Long-Video-Generation"><a href="#VideoMerge-Towards-Training-free-Long-Video-Generation" class="headerlink" title="VideoMerge: Towards Training-free Long Video Generation"></a>VideoMerge: Towards Training-free Long Video Generation</h2><p><strong>Authors:Siyang Zhang, Harry Yang, Ser-Nam Lim</strong></p>
<p>Long video generation remains a challenging and compelling topic in computer vision. Diffusion based models, among the various approaches to video generation, have achieved state of the art quality with their iterative denoising procedures. However, the intrinsic complexity of the video domain renders the training of such diffusion models exceedingly expensive in terms of both data curation and computational resources. Moreover, these models typically operate on a fixed noise tensor that represents the video, resulting in predetermined spatial and temporal dimensions. Although several high quality open-source pretrained video diffusion models, jointly trained on images and videos of varying lengths and resolutions, are available, it is generally not recommended to specify a video length at inference that was not included in the training set. Consequently, these models are not readily adaptable to the direct generation of longer videos by merely increasing the specified video length. In addition to feasibility challenges, long-video generation also encounters quality issues. The domain of long videos is inherently more complex than that of short videos: extended durations introduce greater variability and necessitate long-range temporal consistency, thereby increasing the overall difficulty of the task. We propose VideoMerge, a training-free method that can be seamlessly adapted to merge short videos generated by pretrained text-to-video diffusion model. Our approach preserves the modelâ€™s original expressiveness and consistency while allowing for extended duration and dynamic variation as specified by the user. By leveraging the strengths of pretrained models, our method addresses challenges related to smoothness, consistency, and dynamic content through orthogonal strategies that operate collaboratively to achieve superior quality. </p>
<blockquote>
<p>é•¿è§†é¢‘ç”Ÿæˆä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¸å¼•åŠ›çš„ä¸»é¢˜ã€‚åœ¨å¤šç§è§†é¢‘ç”Ÿæˆæ–¹æ³•ä¸­ï¼ŒåŸºäºæ‰©æ•£çš„æ¨¡å‹ä»¥å…¶è¿­ä»£å»å™ªç¨‹åºè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å“è´¨ã€‚ç„¶è€Œï¼Œè§†é¢‘é¢†åŸŸçš„å›ºæœ‰å¤æ‚æ€§ä½¿å¾—è¿™ç±»æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒåœ¨æ•°æ®æ•´ç†å’Œè®¡ç®—èµ„æºæ–¹é¢æä¸ºæ˜‚è´µã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åœ¨ä¸€ä¸ªä»£è¡¨è§†é¢‘çš„å›ºå®šå™ªå£°å¼ é‡ä¸Šè¿è¡Œï¼Œå¯¼è‡´é¢„å…ˆç¡®å®šçš„ç©ºé—´å’Œæ—¶é—´ç»´åº¦ã€‚å°½ç®¡æœ‰å‡ ä¸ªé«˜è´¨é‡çš„å¼€æºé¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥åœ¨ä¸åŒé•¿åº¦å’Œåˆ†è¾¨ç‡çš„å›¾åƒå’Œè§†é¢‘ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼Œä½†é€šå¸¸ä¸å»ºè®®åœ¨æ¨ç†é˜¶æ®µæŒ‡å®šæœªåœ¨è®­ç»ƒé›†ä¸­åŒ…å«çš„è§†é¢‘é•¿åº¦ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹å¹¶ä¸æ˜“äºä»…é€šè¿‡å¢åŠ æŒ‡å®šçš„è§†é¢‘é•¿åº¦æ¥ç›´æ¥ç”Ÿæˆæ›´é•¿çš„è§†é¢‘ã€‚é™¤äº†å¯è¡Œæ€§æŒ‘æˆ˜å¤–ï¼Œé•¿è§†é¢‘ç”Ÿæˆè¿˜é¢ä¸´è´¨é‡é—®é¢˜ã€‚é•¿è§†é¢‘é¢†åŸŸçš„å¤æ‚æ€§æœ¬è´¨ä¸Šé«˜äºçŸ­è§†é¢‘ï¼šæ›´é•¿çš„æŒç»­æ—¶é—´å¼•å…¥äº†æ›´å¤§çš„å¯å˜æ€§å’Œå¿…è¦çš„é•¿æœŸæ—¶é—´ä¸€è‡´æ€§ï¼Œä»è€Œå¢åŠ äº†ä»»åŠ¡çš„æ€»ä½“éš¾åº¦ã€‚æˆ‘ä»¬æå‡ºäº†VideoMergeï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä»¥æ— ç¼åœ°é€‚åº”åˆå¹¶ç”±é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„çŸ­è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹è¡¨è¾¾åŠ›å’Œä¸€è‡´æ€§ï¼ŒåŒæ—¶å…è®¸ç”¨æˆ·æŒ‡å®šçš„æ‰©å±•æŒç»­æ—¶é—´å’ŒåŠ¨æ€å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ååŒå·¥ä½œçš„æ­£äº¤ç­–ç•¥è§£å†³äº†ä¸å¹³æ»‘åº¦ã€ä¸€è‡´æ€§å’ŒåŠ¨æ€å†…å®¹ç›¸å…³çš„æŒ‘æˆ˜ï¼Œä»è€Œå®ç°ä¼˜è´¨æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09926v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é•¿è§†é¢‘ç”Ÿæˆé—®é¢˜ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†å…ˆè¿›çš„è´¨é‡ï¼Œä½†ç”±äºè§†é¢‘åŸŸçš„å›ºæœ‰å¤æ‚æ€§å’Œè®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œå…¶åº”ç”¨å—åˆ°é™åˆ¶ã€‚æ­¤å¤–ï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ“ä½œå›ºå®šå™ªå£°å¼ é‡è¡¨ç¤ºçš„è§†é¢‘ï¼Œå¯¼è‡´ç©ºé—´å’Œæ—¶é—´ç»´åº¦é¢„å®šï¼Œéš¾ä»¥é€‚åº”ç›´æ¥ç”Ÿæˆæ›´é•¿çš„è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†VideoMergeæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯æ— ç¼é€‚åº”åˆå¹¶ç”±é¢„è®­ç»ƒæ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç”ŸæˆçŸ­è§†é¢‘ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šæ‰©å±•çš„æŒç»­æ—¶é—´å’ŒåŠ¨æ€å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹è¡¨è¾¾åŠ›å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨å¤æ‚æ€§å’Œé«˜æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é€šå¸¸æ“ä½œå›ºå®šå™ªå£°å¼ é‡ï¼Œé™åˆ¶äº†è§†é¢‘ç”Ÿæˆçš„ç©ºé—´å’Œæ—¶é—´ç»´åº¦ã€‚</li>
<li>å°½ç®¡å­˜åœ¨é«˜è´¨é‡çš„å¼€æºé¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä½†ä¸å»ºè®®åœ¨æ¨ç†é˜¶æ®µæŒ‡å®šæœªåœ¨è®­ç»ƒé›†ä¸­åŒ…å«çš„è§†é¢‘é•¿åº¦ã€‚</li>
<li>ç›´æ¥ç”Ÿæˆæ›´é•¿çš„è§†é¢‘é¢ä¸´é€‚åº”æ€§å’Œè´¨é‡æŒ‘æˆ˜ï¼Œéœ€è¦ä¿æŒé•¿æ—¶é—´åºåˆ—çš„ä¸€è‡´æ€§å’ŒåŠ¨æ€å†…å®¹ã€‚</li>
<li>VideoMergeæ–¹æ³•æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯åˆå¹¶çŸ­è§†é¢‘ä»¥ç”Ÿæˆé•¿è§†é¢‘ã€‚</li>
<li>VideoMergeæ–¹æ³•å…è®¸ç”¨æˆ·æŒ‡å®šæ‰©å±•çš„æŒç»­æ—¶é—´å’ŒåŠ¨æ€å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹è¡¨è¾¾åŠ›å’Œä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9aa1729b970aca794a9575093047f201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c96ca6de5c7ce721a2cf8e6f852b5398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b05b42006ccb530270a55b6351fc207.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c122dab3961f171b4bf755b1a84d2c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2dfc5edebc8871fe475a00188ce5029.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0edd49b86d34453b8ec50517f8f2c541.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Shaping-Inductive-Bias-in-Diffusion-Models-through-Frequency-Based-Noise-Control"><a href="#Shaping-Inductive-Bias-in-Diffusion-Models-through-Frequency-Based-Noise-Control" class="headerlink" title="Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise   Control"></a>Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise   Control</h2><p><strong>Authors:Thomas Jiralerspong, Berton Earnshaw, Jason Hartford, Yoshua Bengio, Luca Scimeca</strong></p>
<p>Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption. </p>
<blockquote>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰æ˜¯å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å¤šä¸ªç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ— ä¸ä¼¦æ¯”çš„æˆåŠŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ä¸­æ„å»ºå½’çº³åç½®ï¼Œä»¥æ›´å¥½åœ°é€‚åº”è¦å»ºæ¨¡çš„æ•°æ®çš„ç›®æ ‡åˆ†å¸ƒã€‚å¯¹äºæ‹“æ‰‘ç»“æ„æ•°æ®ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºé¢‘ç‡çš„å™ªå£°æ“ä½œç¬¦ï¼Œä»¥æ•…æ„æ“ä½œå¹¶è®¾ç½®è¿™äº›å½’çº³åç½®ã€‚æˆ‘ä»¬é¦–å…ˆè¡¨æ˜ï¼Œé€‚å½“åœ°æ“ä½œå™ªå£°æ­£å‘è¿‡ç¨‹å¯ä»¥ä½¿DPMä¸“æ³¨äºåˆ†å¸ƒå­¦ä¹ çš„ç‰¹å®šæ–¹é¢ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸åŒçš„æ•°æ®é›†éœ€è¦ä¸åŒçš„å½’çº³åç½®ï¼Œä¸æ ‡å‡†æ‰©æ•£ç›¸æ¯”ï¼Œé€‚å½“çš„åŸºäºé¢‘ç‡çš„å™ªå£°æ§åˆ¶ä¼šæé«˜ç”Ÿæˆæ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å­¦ä¹ æ—¶å¿½ç•¥ç‰¹å®šé¢‘ç‡ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬åœ¨å›¾åƒæŸåå’Œæ¢å¤ä»»åŠ¡ä¸­å±•ç¤ºäº†è¿™ä¸€ç‚¹ï¼Œåœ¨è¯¥ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒDPMåœ¨ä¸¥é‡å™ªå£°æŸååæ¢å¤åŸå§‹ç›®æ ‡åˆ†å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10236v2">PDF</a> Published as workshop paper at DeLTa and FPI workshops, ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰ï¼Œä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥å½’çº³åè§æ¥æ”¹å–„æ•°æ®ç›®æ ‡åˆ†å¸ƒä¸æ¨¡å‹ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚å¯¹äºæ‹“æ‰‘ç»“æ„æ•°æ®ï¼Œæœ¬æ–‡è®¾è®¡äº†ä¸€ç§åŸºäºé¢‘ç‡çš„å™ªå£°æ“ä½œå™¨æ¥æ•…æ„æ“çºµå¹¶è®¾ç½®è¿™äº›å½’çº³åè§ã€‚é€šè¿‡é€‚å½“æ“çºµå™ªå£°å‰å‘è¿‡ç¨‹ï¼ŒDPMså¯ä»¥ä¸“æ³¨äºåˆ†å¸ƒå­¦ä¹ çš„ç‰¹å®šæ–¹é¢ã€‚ä¸åŒæ•°æ®é›†éœ€è¦ä¸åŒçš„å½’çº³åè§ï¼Œè€Œé€‚å½“çš„åŸºäºé¢‘ç‡çš„å™ªå£°æ§åˆ¶å¯æé«˜ç”Ÿæˆæ€§èƒ½ï¼Œä¸æ ‡å‡†æ‰©æ•£ç›¸æ¯”ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å±•ç¤ºäº†åœ¨å­¦ä¹ æ—¶å¿½ç•¥ç‰¹å®šé¢‘ç‡ä¿¡æ¯çš„å¯èƒ½æ€§ï¼Œå¹¶åœ¨å›¾åƒæŸåå’Œæ¢å¤ä»»åŠ¡ä¸­è¿›è¡Œäº†æ¼”ç¤ºï¼Œè®­ç»ƒDPMåœ¨ä¸¥é‡å™ªå£°æŸååæ¢å¤åŸå§‹ç›®æ ‡åˆ†å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDPMsï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—å‰æ‰€æœªæœ‰çš„æˆåŠŸã€‚</li>
<li>å¯¹äºæ‹“æ‰‘ç»“æ„æ•°æ®ï¼Œå¼•å…¥å½’çº³åè§å¯ä»¥æ”¹å–„æ•°æ®ç›®æ ‡åˆ†å¸ƒä¸æ¨¡å‹ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>åŸºäºé¢‘ç‡çš„å™ªå£°æ“ä½œå™¨è¢«è®¾è®¡æ¥æ•…æ„æ“çºµè®¾ç½®è¿™äº›å½’çº³åè§ã€‚</li>
<li>é€šè¿‡é€‚å½“æ“çºµå™ªå£°å‰å‘è¿‡ç¨‹ï¼ŒDPMså¯ä»¥ä¸“æ³¨äºåˆ†å¸ƒå­¦ä¹ çš„ç‰¹å®šæ–¹é¢ï¼Œä»è€Œæé«˜ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸åŒæ•°æ®é›†éœ€è¦ä¸åŒçš„å½’çº³åè§ï¼Œè€ŒåŸºäºé¢‘ç‡çš„å™ªå£°æ§åˆ¶æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ”¹è¿›æ–¹æ³•ã€‚</li>
<li>åœ¨å­¦ä¹ æ—¶å¯ä»¥å¿½ç•¥ç‰¹å®šé¢‘ç‡ä¿¡æ¯ï¼Œè¿™åœ¨å›¾åƒæŸåå’Œæ¢å¤ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5aa4b84ce382d9551efdfc485871f8bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51aafbf000d5bd2a5183dd8b4196b663.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca31968d8b11c21b1ea121fb38e10040.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd944ca6f4601c59e00145e6c891c70e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Arbitrary-steps-Image-Super-resolution-via-Diffusion-Inversion"><a href="#Arbitrary-steps-Image-Super-resolution-via-Diffusion-Inversion" class="headerlink" title="Arbitrary-steps Image Super-resolution via Diffusion Inversion"></a>Arbitrary-steps Image Super-resolution via Diffusion Inversion</h2><p><strong>Authors:Zongsheng Yue, Kang Liao, Chen Change Loy</strong></p>
<p>This study presents a new image super-resolution (SR) technique based on diffusion inversion, aiming at harnessing the rich image priors encapsulated in large pre-trained diffusion models to improve SR performance. We design a Partial noise Prediction strategy to construct an intermediate state of the diffusion model, which serves as the starting sampling point. Central to our approach is a deep noise predictor to estimate the optimal noise maps for the forward diffusion process. Once trained, this noise predictor can be used to initialize the sampling process partially along the diffusion trajectory, generating the desirable high-resolution result. Compared to existing approaches, our method offers a flexible and efficient sampling mechanism that supports an arbitrary number of sampling steps, ranging from one to five. Even with a single sampling step, our method demonstrates superior or comparable performance to recent state-of-the-art approaches. The code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/zsyOAOA/InvSR">https://github.com/zsyOAOA/InvSR</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£åè½¬çš„æ–°å‹å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ‰©æ•£æ¨¡å‹ä¸­å°è£…çš„ä¸°å¯Œå›¾åƒå…ˆéªŒçŸ¥è¯†æ¥æé«˜SRæ€§èƒ½ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å±€éƒ¨å™ªå£°é¢„æµ‹ç­–ç•¥ï¼Œä»¥æ„å»ºæ‰©æ•£æ¨¡å‹çš„ä¸­é—´çŠ¶æ€ï¼Œä½œä¸ºèµ·å§‹é‡‡æ ·ç‚¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ·±åº¦å™ªå£°é¢„æµ‹å™¨ï¼Œç”¨äºä¼°è®¡æ­£å‘æ‰©æ•£è¿‡ç¨‹çš„æœ€ä½³å™ªå£°å›¾ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œè¯¥å™ªå£°é¢„æµ‹å™¨å¯ç”¨äºéƒ¨åˆ†åˆå§‹åŒ–æ²¿æ‰©æ•£è½¨è¿¹çš„é‡‡æ ·è¿‡ç¨‹ï¼Œç”Ÿæˆç†æƒ³çš„é«˜åˆ†è¾¨ç‡ç»“æœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§çµæ´»é«˜æ•ˆçš„é‡‡æ ·æœºåˆ¶ï¼Œæ”¯æŒä»ä¸€åˆ°äº”ä»»æ„çš„é‡‡æ ·æ­¥éª¤æ•°ã€‚å³ä½¿åªæœ‰ä¸€ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¸æœ€æ–°å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜çš„æ€§èƒ½ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zsyOAOA/InvSR%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/zsyOAOA/InvSRä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09013v2">PDF</a> Accepted by CVPR 2025. Project: <a target="_blank" rel="noopener" href="https://github.com/zsyOAOA/InvSR">https://github.com/zsyOAOA/InvSR</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£åæ¼”çš„æ–°å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­å°è£…çš„ä¸°å¯Œå›¾åƒå…ˆéªŒçŸ¥è¯†ï¼Œä»¥æé«˜SRæ€§èƒ½ã€‚è®¾è®¡äº†ä¸€ç§å±€éƒ¨å™ªå£°é¢„æµ‹ç­–ç•¥ï¼Œæ„å»ºäº†æ‰©æ•£æ¨¡å‹çš„ä¸­é—´çŠ¶æ€ï¼Œä½œä¸ºé‡‡æ ·èµ·å§‹ç‚¹ã€‚æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ·±åº¦å™ªå£°é¢„æµ‹å™¨ï¼Œç”¨äºä¼°è®¡å‰å‘æ‰©æ•£è¿‡ç¨‹çš„æœ€ä½³å™ªå£°å›¾ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œè¯¥å™ªå£°é¢„æµ‹å™¨å¯ç”¨äºæ²¿æ‰©æ•£è½¨è¿¹éƒ¨åˆ†åˆå§‹åŒ–é‡‡æ ·è¿‡ç¨‹ï¼Œç”Ÿæˆç†æƒ³çš„é«˜åˆ†è¾¨ç‡ç»“æœã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æä¾›äº†çµæ´»é«˜æ•ˆçš„é‡‡æ ·æœºåˆ¶ï¼Œæ”¯æŒä»ä¸€åˆ°äº”æ­¥çš„ä»»æ„é‡‡æ ·æ­¥éª¤æ•°ã€‚å³ä½¿åªæœ‰ä¸€ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºä¼˜äºæˆ–ç›¸å½“äºæœ€è¿‘å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚æ¨¡å‹å’Œä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/zsyOAOA/InvSR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/zsyOAOA/InvSRä¸Šæä¾›ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£åæ¼”çš„å›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒå…ˆéªŒçŸ¥è¯†æ¥æé«˜è¶…åˆ†è¾¨ç‡æ€§èƒ½ã€‚</li>
<li>è®¾è®¡äº†å±€éƒ¨å™ªå£°é¢„æµ‹ç­–ç•¥ï¼Œæ„å»ºäº†æ‰©æ•£æ¨¡å‹çš„ä¸­é—´çŠ¶æ€ä½œä¸ºé‡‡æ ·èµ·å§‹ç‚¹ã€‚</li>
<li>æ·±åº¦å™ªå£°é¢„æµ‹å™¨ç”¨äºä¼°è®¡å‰å‘æ‰©æ•£è¿‡ç¨‹çš„æœ€ä½³å™ªå£°å›¾ã€‚</li>
<li>æ–¹æ³•æ”¯æŒä»»æ„æ•°é‡çš„é‡‡æ ·æ­¥éª¤ï¼ŒåŒ…æ‹¬ä»ä¸€åˆ°äº”æ­¥ã€‚</li>
<li>å•æ­¥é‡‡æ ·å³å¯è¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7f9b7867d5b47108bfa73531e1fb4e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f760e69eeec0b01dc9ae5c90caed34a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf976f94462104356241c45f30af7a03.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images"><a href="#Hidden-in-the-Noise-Two-Stage-Robust-Watermarking-for-Images" class="headerlink" title="Hidden in the Noise: Two-Stage Robust Watermarking for Images"></a>Hidden in the Noise: Two-Stage Robust Watermarking for Images</h2><p><strong>Authors:Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</strong></p>
<p>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion modelâ€™s initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆå™¨çš„è´¨é‡ä¸æ–­æé«˜ï¼Œæ·±åº¦ä¼ªé€ æˆä¸ºç¤¾ä¼šçƒ­è®®çš„è¯é¢˜ã€‚å›¾åƒæ°´å°å…è®¸æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶AIç”Ÿæˆçš„å†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡æ³¨ï¼Œä»è€Œå‡è½»å…¶é€ æˆçš„ä¼¤å®³ã€‚ç„¶è€Œï¼Œå½“å‰æœ€å…ˆè¿›çš„æ°´å°æŠ€æœ¯ä»ç„¶å®¹æ˜“å—åˆ°ä¼ªé€ å’Œåˆ é™¤æ”»å‡»çš„å½±å“ã€‚è¿™ç§è„†å¼±æ€§éƒ¨åˆ†æ˜¯ç”±äºæ°´å°ä¼šç ´åç”Ÿæˆå›¾åƒçš„åˆ†å¸ƒï¼Œä»è€Œæ— æ„é—´æ³„éœ²æœ‰å…³æ°´å°æŠ€æœ¯çš„ä¿¡æ¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°çš„æ— å¤±çœŸæ°´å°æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ£€æµ‹æ°´å°éœ€è¦æ¯”è¾ƒé‡å»ºçš„å›¾åƒåˆå§‹å™ªå£°ä¸æ‰€æœ‰å…ˆå‰ä½¿ç”¨çš„åˆå§‹å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆæ£€æµ‹çš„ä¸¤é˜¶æ®µæ°´å°æ¡†æ¶ã€‚åœ¨ç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆçš„å‚…é‡Œå¶æ¨¡å¼å¢å¼ºåˆå§‹å™ªå£°ä»¥åµŒå…¥å…³äºæˆ‘ä»¬æ‰€ç”¨åˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ã€‚å¯¹äºæ£€æµ‹é˜¶æ®µï¼Œæˆ‘ä»¬ï¼ˆiï¼‰æ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œï¼ˆiiï¼‰åœ¨ç»™å®šçš„ç»„å†…æœç´¢å¯èƒ½ä¸æˆ‘ä»¬çš„å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚è¿™ç§æ°´å°æ–¹æ³•è¾¾åˆ°äº†å¯¹æŠ—å¤§é‡æ”»å‡»çš„ä¼ªé€ å’Œåˆ é™¤æ“ä½œçš„æœ€æ–°é²æ£’æ€§æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04653v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾åƒæ°´å°æŠ€æœ¯åœ¨é˜²æ­¢æ·±åº¦ä¼ªé€ å›¾åƒæ–¹é¢çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹å½“å‰å›¾åƒæ°´å°æŠ€æœ¯æ˜“è¢«ç¯¡æ”¹å’Œç§»é™¤çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ— éœ€å˜å½¢çš„å›¾åƒæ°´å°æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°è¿›è¡Œæ°´å°åµŒå…¥ä¸æ£€æµ‹ï¼Œå®ç°äº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ°´å°æ£€æµ‹æ¡†æ¶ï¼Œæœ‰æ•ˆæé«˜æ°´å°æ£€æµ‹çš„æ•ˆç‡å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæŠµæŠ—å¤šç§æ”»å‡»æ‰‹æ®µï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ°´å°æŠ€æœ¯å…è®¸æ¨¡å‹æ‰€æœ‰è€…å¯¹å…¶AIç”Ÿæˆçš„å†…å®¹è¿›è¡Œæ£€æµ‹å’Œæ ‡è®°ï¼Œæœ‰åŠ©äºç¼“è§£æ·±åº¦ä¼ªé€ å›¾åƒå¸¦æ¥çš„å±å®³ã€‚</li>
<li>å½“å‰å›¾åƒæ°´å°æŠ€æœ¯é¢ä¸´æ˜“è¢«ç¯¡æ”¹å’Œç§»é™¤çš„é—®é¢˜ï¼Œéƒ¨åˆ†åŸå› åœ¨äºæ°´å°ä¼šå¹²æ‰°ç”Ÿæˆå›¾åƒçš„åˆ†å¸ƒï¼Œä»è€Œæ³„éœ²æ°´å°æŠ€æœ¯ä¿¡æ¯ã€‚</li>
<li>åŸºäºæ‰©æ•£æ¨¡å‹çš„åˆå§‹å™ªå£°è¿›è¡Œå›¾åƒæ°´å°æ˜¯ä¸€ç§æ— éœ€å˜å½¢çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„æ°´å°æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆæ—¶å¢åŠ åˆå§‹å™ªå£°çš„å‚…ç«‹å¶æ¨¡å¼ï¼ŒåµŒå…¥å…³äºåˆå§‹å™ªå£°ç»„çš„ä¿¡æ¯ï¼Œä»¥æé«˜æ£€æµ‹æ•ˆç‡ã€‚</li>
<li>æ°´å°æ£€æµ‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ£€ç´¢ç›¸å…³çš„å™ªå£°ç»„ï¼Œç„¶ååœ¨ç»™å®šçš„å™ªå£°ç»„å†…æœç´¢å¯èƒ½ä¸å›¾åƒåŒ¹é…çš„åˆå§‹å™ªå£°ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†å¯¹å¤šç§æ”»å‡»çš„é«˜é²æ£’æ€§ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e656404da77b2d8e75fad223578b3f0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c408e9179c0ad8c709dd36f5a51ffece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6680abe6520758c46ecb354591555d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-122dbbeb9da300fbbfb0ba4f9ee5a142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b0dd49c947ecd9f04d958b3d9342ce.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EmojiDiff-Advanced-Facial-Expression-Control-with-High-Identity-Preservation-in-Portrait-Generation"><a href="#EmojiDiff-Advanced-Facial-Expression-Control-with-High-Identity-Preservation-in-Portrait-Generation" class="headerlink" title="EmojiDiff: Advanced Facial Expression Control with High Identity   Preservation in Portrait Generation"></a>EmojiDiff: Advanced Facial Expression Control with High Identity   Preservation in Portrait Generation</h2><p><strong>Authors:Liangwei Jiang, Ruida Li, Zhifeng Zhang, Shuo Fang, Chenguang Ma</strong></p>
<p>This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity: (i) fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait; (ii) even coarse-grained expression control can cause facial changes that compromise identity, since they all act on the face. These limitations remain unaddressed by previous generation methods, which primarily rely on coarse control signals or two-stage inference that integrates portrait animation. Here, we introduce EmojiDiff, the first end-to-end solution that enables simultaneous control of extremely detailed expression (RGB-level) and high-fidelity identity in portrait generation. To address the above challenges, EmojiDiff adopts a two-stage scheme involving decoupled training and fine-tuning. For decoupled training, we innovate ID-irrelevant Data Iteration (IDI) to synthesize cross-identity expression pairs by dividing and optimizing the processes of maintaining expression and altering identity, thereby ensuring stable and high-quality data generation. Training the model with this data, we effectively disentangle fine expression features in the expression template from other extraneous information (e.g., identity, skin). Subsequently, we present ID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves rapid reconstruction and joint supervision of identity and expression information, thus aligning identity representations of images with and without expression control. Experimental results demonstrate that our method remarkably outperforms counterparts, achieves precise expression control with highly maintained identity, and generalizes well to various diffusion models. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨å®ç°è‚–åƒç”Ÿæˆä¸­çš„ç²¾ç»†è¡¨æƒ…æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸèº«ä»½ã€‚è¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè¡¨æƒ…å’Œèº«ä»½ä¹‹é—´å­˜åœ¨ç›¸äº’å¹²æ‰°ï¼šï¼ˆiï¼‰ç²¾ç»†è¡¨æƒ…æ§åˆ¶ä¿¡å·ä¸å¯é¿å…åœ°ä¼šå¼•å…¥ä¸å¤–è§‚ç›¸å…³çš„è¯­ä¹‰ï¼ˆä¾‹å¦‚é¢éƒ¨è½®å»“å’Œæ¯”ä¾‹ï¼‰ï¼Œè¿™ä¼šå½±å“ç”Ÿæˆçš„è‚–åƒçš„èº«ä»½ï¼›ï¼ˆiiï¼‰å³ä½¿æ˜¯ç²—ç²’åº¦çš„è¡¨æƒ…æ§åˆ¶ä¹Ÿä¼šå¯¼è‡´é¢éƒ¨å˜åŒ–ï¼ŒæŸå®³èº«ä»½ï¼Œå› ä¸ºå®ƒä»¬éƒ½ä½œç”¨äºé¢éƒ¨ã€‚ä»¥å‰çš„ç”Ÿæˆæ–¹æ³•ä¸»è¦ä¾èµ–äºç²—æ§åˆ¶ä¿¡å·æˆ–é›†æˆè‚–åƒåŠ¨ç”»çš„ä¸¤é˜¶æ®µæ¨ç†ï¼Œæ— æ³•è§£å†³è¿™äº›é™åˆ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº†EmojiDiffï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨è‚–åƒç”Ÿæˆä¸­åŒæ—¶æ§åˆ¶æå…¶è¯¦ç»†çš„è¡¨æƒ…ï¼ˆRGBçº§åˆ«ï¼‰å’Œé«˜ä¿çœŸèº«ä»½ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼ŒEmojiDiffé‡‡ç”¨äº†æ¶‰åŠè§£è€¦è®­ç»ƒå’Œå¾®è°ƒçš„ä¸¤é˜¶æ®µæ–¹æ¡ˆã€‚å¯¹äºè§£è€¦è®­ç»ƒï¼Œæˆ‘ä»¬åˆ›æ–°äº†IDæ— å…³æ•°æ®è¿­ä»£ï¼ˆIDIï¼‰ï¼Œé€šè¿‡åˆ†ç¦»å’Œä¼˜åŒ–ä¿æŒè¡¨æƒ…å’Œæ”¹å˜èº«ä»½çš„è¿‡ç¨‹ï¼Œåˆæˆè·¨èº«ä»½è¡¨æƒ…å¯¹ï¼Œä»è€Œç¡®ä¿ç¨³å®šä¸”é«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚ä½¿ç”¨æ­¤æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°å°†ä»è¡¨æƒ…æ¨¡æ¿ä¸­çš„ç²¾ç»†è¡¨æƒ…ç‰¹å¾ä¸å…¶ä»–å¤–æ¥ä¿¡æ¯ï¼ˆä¾‹å¦‚èº«ä»½ã€çš®è‚¤ï¼‰åˆ†ç¦»ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›è¡Œäº†IDå¢å¼ºå¯¹æ¯”å¯¹é½ï¼ˆICAï¼‰çš„è¿›ä¸€æ­¥å¾®è°ƒã€‚ICAå®ç°äº†èº«ä»½å’Œè¡¨æƒ…ä¿¡æ¯çš„å¿«é€Ÿé‡å»ºå’Œè”åˆç›‘ç£ï¼Œä»è€Œå¯¹é½æœ‰å’Œæ— è¡¨æƒ…æ§åˆ¶å›¾åƒçš„èº«ä»½è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŒè¡Œï¼Œå®ç°äº†ç²¾ç¡®çš„è¡¨æƒ…æ§åˆ¶ï¼ŒåŒæ—¶é«˜åº¦ä¿æŒèº«ä»½ï¼Œå¹¶ä¸”èƒ½å¾ˆå¥½åœ°é€‚åº”å„ç§æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01254v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEmojiDiffçš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨å®ç°è‚–åƒç”Ÿæˆä¸­çš„ç²¾ç»†è¡¨æƒ…æ§åˆ¶å’Œé«˜ä¿çœŸèº«ä»½ä¿æŒã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ¡ˆï¼ŒåŒ…æ‹¬è§£è€¦è®­ç»ƒå’Œå¾®è°ƒï¼Œè§£å†³äº†è¡¨æƒ…ä¸èº«ä»½ä¹‹é—´ç›¸äº’å¹²æ‰°çš„é—®é¢˜ã€‚é€šè¿‡åˆ›æ–°æ€§çš„ID-irrelevant Data Iterationï¼ˆIDIï¼‰æ–¹æ³•ï¼Œåˆæˆè·¨èº«ä»½çš„è¡¨æƒ…å¯¹ï¼Œç¡®ä¿åœ¨ä¿æŒè¡¨æƒ…çš„åŒæ—¶æ”¹å˜èº«ä»½ï¼Œä»è€Œç¡®ä¿ç¨³å®šé«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚éšåï¼Œé€šè¿‡ID-enhanced Contrast Alignmentï¼ˆICAï¼‰è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒï¼Œå®ç°èº«ä»½å’Œè¡¨æƒ…ä¿¡æ¯çš„å¿«é€Ÿé‡å»ºå’Œè”åˆç›‘ç£ï¼Œä½¿å¸¦æœ‰å’Œä¸å¸¦è¡¨æƒ…æ§åˆ¶çš„å›¾åƒèº«ä»½è¡¨ç¤ºå¯¹é½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŒç±»äº§å“ï¼Œå®ç°äº†ç²¾ç¡®çš„è¡¨æƒ…æ§åˆ¶å’Œé«˜åº¦çš„èº«ä»½ä¿æŒï¼Œå¹¶èƒ½å¾ˆå¥½åœ°åº”ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ—¨åœ¨å®ç°è‚–åƒç”Ÿæˆä¸­çš„ç²¾ç»†è¡¨æƒ…æ§åˆ¶ä¸é«˜ä¿çœŸèº«ä»½ä¿æŒï¼Œè§£å†³è¡¨æƒ…ä¸èº«ä»½ç›¸äº’å¹²æ‰°çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åä¸ºEmojiDiffçš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ¡ˆï¼ˆè§£è€¦è®­ç»ƒå’Œå¾®è°ƒï¼‰ã€‚</li>
<li>é€šè¿‡ID-irrelevant Data Iterationï¼ˆIDIï¼‰æ–¹æ³•ï¼Œåˆæˆè·¨èº«ä»½çš„è¡¨æƒ…å¯¹ï¼Œç¡®ä¿æ•°æ®ç”Ÿæˆçš„ç¨³å®šæ€§å’Œé«˜è´¨é‡ã€‚</li>
<li>ID-enhanced Contrast Alignmentï¼ˆICAï¼‰ç”¨äºè¿›ä¸€æ­¥å¾®è°ƒï¼Œå®ç°èº«ä»½å’Œè¡¨æƒ…ä¿¡æ¯çš„å¿«é€Ÿé‡å»ºå’Œè”åˆç›‘ç£ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ç²¾ç¡®çš„è¡¨æƒ…æ§åˆ¶ï¼Œé«˜åº¦ä¿æŒäº†èº«ä»½ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–åŒç±»äº§å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-44dc8aee10f84bb1c9c4f5e797e24832.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf7081fcfbd8cb3b7c69b2ffc8f7ac18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b083d851feed10e030d23aab1dc123a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b49df2108712663f69fa1d34790ed14.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Meissonic-Revitalizing-Masked-Generative-Transformers-for-Efficient-High-Resolution-Text-to-Image-Synthesis"><a href="#Meissonic-Revitalizing-Masked-Generative-Transformers-for-Efficient-High-Resolution-Text-to-Image-Synthesis" class="headerlink" title="Meissonic: Revitalizing Masked Generative Transformers for Efficient   High-Resolution Text-to-Image Synthesis"></a>Meissonic: Revitalizing Masked Generative Transformers for Efficient   High-Resolution Text-to-Image Synthesis</h2><p><strong>Authors:Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, Shuicheng Yan</strong></p>
<p>We present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIMâ€™s performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonicâ€™s capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \times 1024$ resolution images. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Meissonicï¼Œå®ƒå°†éè‡ªå›å½’æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„æ–‡æœ¬åˆ°å›¾åƒçš„æ°´å¹³æå‡åˆ°äº†ä¸SDXLç­‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚é€šè¿‡èå…¥ä¸€ç³»åˆ—æ¶æ„åˆ›æ–°ã€å…ˆè¿›çš„å®šä½ç¼–ç ç­–ç•¥å’Œä¼˜åŒ–é‡‡æ ·æ¡ä»¶ï¼ŒMeissonicå¤§å¤§æé«˜äº†MIMçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œç»“åˆäººç±»åå¥½åˆ†æ•°æ‰€æä¾›çš„å¾®è§‚æ¡ä»¶ï¼Œå¹¶é‡‡ç”¨ç‰¹å¾å‹ç¼©å±‚ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å›¾åƒçš„ä¿çœŸåº¦å’Œåˆ†è¾¨ç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…è¾¾åˆ°äº†ç°æœ‰æ¨¡å‹å¦‚SDXLçš„æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸”åœ¨ç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢å¾€å¾€è¡¨ç°æ›´ä½³ã€‚å¤§é‡å®éªŒéªŒè¯äº†Meissonicçš„èƒ½åŠ›ï¼Œè¡¨æ˜äº†å®ƒåœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆé¢†åŸŸçš„æ½œåŠ›ï¼Œæœ‰æœ›æˆä¸ºä¸€ç§æ–°æ ‡å‡†ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªå¯ä»¥ç”Ÿæˆ$1024 \times 1024$åˆ†è¾¨ç‡å›¾åƒçš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08261v4">PDF</a> Accepted to ICLR 2025. Codes and Supplementary Material:   <a target="_blank" rel="noopener" href="https://github.com/viiika/Meissonic">https://github.com/viiika/Meissonic</a></p>
<p><strong>Summary</strong></p>
<p>Meissonicæ¨¡å‹é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—æ¶æ„åˆ›æ–°ã€å…ˆè¿›çš„å®šä½ç¼–ç ç­–ç•¥ä»¥åŠä¼˜åŒ–é‡‡æ ·æ¡ä»¶ï¼Œå¤§å¹…æå‡äº†éè‡ªå›å½’æ©ç›–å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„æ–‡æœ¬è½¬å›¾åƒèƒ½åŠ›ï¼Œä½¿å…¶ä¸å¦‚SDXLç­‰å…ˆè¿›æ‰©æ•£æ¨¡å‹ç›¸åª²ç¾ã€‚å€ŸåŠ©é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€äººç±»åå¥½å¾—åˆ†æŒ‡å¯¼çš„å¾®æ¡ä»¶ä»¥åŠç‰¹å¾å‹ç¼©å±‚ï¼ŒMeissonicä¸ä»…è¾¾åˆ°äº†ç°æœ‰æ¨¡å‹å¦‚SDXLçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨ç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢æ›´èƒœä¸€ç­¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Meissonicæ¨¡å‹æé«˜äº†éè‡ªå›å½’æ©ç›–å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„æ–‡æœ¬è½¬å›¾åƒèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—æ¶æ„åˆ›æ–°å’Œå…ˆè¿›çš„å®šä½ç¼–ç ç­–ç•¥ï¼ŒMeissonicå®ç°äº†æ€§èƒ½æå‡ã€‚</li>
<li>Meissonicæ¨¡å‹ä½¿ç”¨äº†ä¼˜åŒ–é‡‡æ ·æ¡ä»¶ï¼Œè¿›ä¸€æ­¥æå‡äº†æ•ˆç‡ã€‚</li>
<li>é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€äººç±»åå¥½å¾—åˆ†æŒ‡å¯¼çš„å¾®æ¡ä»¶ä»¥åŠç‰¹å¾å‹ç¼©å±‚çš„åº”ç”¨å¢å¼ºäº†å›¾åƒçš„ä¿çœŸåº¦å’Œåˆ†è¾¨ç‡ã€‚</li>
<li>Meissonicæ¨¡å‹è¾¾åˆ°äº†ç”šè‡³è¶…è¶Šäº†ç°æœ‰æ¨¡å‹å¦‚SDXLçš„æ€§èƒ½ã€‚</li>
<li>Meissonicæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¹¿æ³›å®éªŒéªŒè¯å…¶èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ–‡æœ¬è½¬å›¾åƒåˆæˆæ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6af0fb79207baf862e3fa27849ea4a72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cabb5186727dd92a2a95692931306d10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e418e9b505e1cb2aba5fec875e931324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46e1c7f06c67b8b45041ad24953383f8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VIGFace-Virtual-Identity-Generation-for-Privacy-Free-Face-Recognition"><a href="#VIGFace-Virtual-Identity-Generation-for-Privacy-Free-Face-Recognition" class="headerlink" title="VIGFace: Virtual Identity Generation for Privacy-Free Face Recognition"></a>VIGFace: Virtual Identity Generation for Privacy-Free Face Recognition</h2><p><strong>Authors:Minsoo Kim, Min-Cheol Sagong, Gi Pyo Nam, Junghyun Cho, Ig-Jae Kim</strong></p>
<p>Deep learning-based face recognition continues to face challenges due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns. To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images. Our idea originates from pre-assigning virtual identities in the feature space. Initially, we train the face recognition model using a real face dataset and create a feature space for both real and virtual identities, where virtual prototypes are orthogonal to other prototypes. Subsequently, we train the diffusion model based on the established feature space, enabling it to generate authentic human face images from real prototypes and synthesize virtual face images from virtual prototypes. Our proposed framework provides two significant benefits. Firstly, it shows clear separability between existing individuals and virtual face images, allowing one to create synthetic images with confidence and without concerns about privacy and portrait rights. Secondly, it ensures improved performance through data augmentation by incorporating real existing images. Extensive experiments demonstrate the superiority of our virtual face dataset and framework, outperforming the previous state-of-the-art on various face recognition benchmarks. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„äººè„¸è¯†åˆ«ä»ç„¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä¾èµ–äºä»ç½‘ç»œçˆ¬è™«è·å¾—çš„å¤§é‡æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†çš„æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œå¹¶å¼•å‘äº†ç°å®ä¸–ç•Œä¸­çš„é‡å¤§éšç§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VIGFaceæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆåˆæˆé¢éƒ¨å›¾åƒã€‚æˆ‘ä»¬çš„æƒ³æ³•æºäºåœ¨ç‰¹å¾ç©ºé—´é¢„å…ˆåˆ†é…è™šæ‹Ÿèº«ä»½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨çœŸå®é¢éƒ¨æ•°æ®é›†è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ï¼Œå¹¶ä¸ºçœŸå®å’Œè™šæ‹Ÿèº«ä»½åˆ›å»ºç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­è™šæ‹ŸåŸå‹ä¸å…¶ä»–åŸå‹æ­£äº¤ã€‚éšåï¼Œæˆ‘ä»¬åŸºäºå»ºç«‹çš„ç‰¹å¾ç©ºé—´è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä»çœŸå®åŸå‹ç”ŸæˆçœŸå®çš„äººè„¸å›¾åƒï¼Œå¹¶ä»è™šæ‹ŸåŸå‹åˆæˆè™šæ‹Ÿäººè„¸å›¾åƒã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶æä¾›äº†ä¸¤ä¸ªé‡è¦ä¼˜åŠ¿ã€‚é¦–å…ˆï¼Œå®ƒå®ç°äº†ç°æœ‰ä¸ªä½“å’Œè™šæ‹Ÿäººè„¸å›¾åƒä¹‹é—´çš„æ¸…æ™°å¯åˆ†ç¦»æ€§ï¼Œä½¿äººä»¬èƒ½å¤Ÿè‡ªä¿¡åœ°åˆ›å»ºåˆæˆå›¾åƒï¼Œæ— éœ€æ‹…å¿ƒéšç§å’Œè‚–åƒæƒé—®é¢˜ã€‚å…¶æ¬¡ï¼Œé€šè¿‡èå…¥çœŸå®å­˜åœ¨çš„å›¾åƒï¼Œå®ƒç¡®ä¿äº†é€šè¿‡æ•°æ®å¢å¼ºæé«˜æ€§èƒ½ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„è™šæ‹Ÿäººè„¸æ•°æ®é›†å’Œæ¡†æ¶ä¼˜äºå„ç§äººè„¸è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„å…ˆå‰æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08277v3">PDF</a> Please refer to version 3 if you are citing this paper. Major   updates: (1)Test utilities updated: use AdaFace code. (2)Training method   updated: AdaFace+IR-SE50</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é¢è²Œè¯†åˆ«æ¡†æ¶VIGFaceï¼Œå®ƒåˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ç”Ÿæˆåˆæˆé¢éƒ¨å›¾åƒï¼Œä»¥è§£å†³ä¾èµ–ç½‘ç»œçˆ¬è™«è·å–çš„å¤§é‡çœŸå®é¢éƒ¨å›¾åƒæ•°æ®æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨ç‰¹å¾ç©ºé—´é¢„å…ˆåˆ†é…è™šæ‹Ÿèº«ä»½æ¥ç”Ÿæˆè™šæ‹Ÿé¢éƒ¨åŸå‹ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹ä»¥ç”ŸæˆçœŸå®å’Œè™šæ‹Ÿé¢éƒ¨å›¾åƒã€‚è¯¥æ¡†æ¶å…·æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šä¸€æ˜¯èƒ½å¤Ÿåœ¨ç°æœ‰ä¸ªä½“å’Œè™šæ‹Ÿé¢éƒ¨å›¾åƒä¹‹é—´å®ç°æ¸…æ™°çš„åˆ†ç¦»ï¼Œä¸ºåˆ›å»ºåˆæˆå›¾åƒæä¾›äº†ä¿¡å¿ƒï¼Œæ¶ˆé™¤äº†å¯¹éšç§å’Œè‚–åƒæƒçš„æ‹…å¿§ï¼›äºŒæ˜¯é€šè¿‡å¼•å…¥çœŸå®çš„ç°æœ‰å›¾åƒï¼Œå®ç°äº†æ•°æ®å¢å¼ºï¼Œæé«˜äº†æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥è™šæ‹Ÿé¢éƒ¨æ•°æ®é›†å’Œæ¡†æ¶ä¼˜äºå½“å‰å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å„ç§é¢éƒ¨è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIGFaceæ¡†æ¶è§£å†³äº†æ·±åº¦å­¦ä¹ åœ¨é¢éƒ¨è¯†åˆ«ä¸­ä¾èµ–å¤§é‡çœŸå®æ•°æ®çš„é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆé¢éƒ¨å›¾åƒå‡å°‘äº†æˆæœ¬å’Œå¯¹éšç§çš„æ‹…å¿§ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨ç‰¹å¾ç©ºé—´é¢„å…ˆåˆ†é…è™šæ‹Ÿèº«ä»½æ¥ç”Ÿæˆè™šæ‹Ÿé¢éƒ¨åŸå‹ï¼Œè¿™äº›åŸå‹ä¸çœŸå®åŸå‹æ­£äº¤ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åŸºäºå·²å»ºç«‹çš„ç‰¹æ€§ç©ºé—´è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥ç”ŸæˆçœŸå®å’Œè™šæ‹Ÿçš„é¢éƒ¨å›¾åƒã€‚</li>
<li>VIGFaceæ¡†æ¶èƒ½å¤Ÿç¡®ä¿ç°æœ‰ä¸ªä½“å’Œè™šæ‹Ÿé¢éƒ¨å›¾åƒä¹‹é—´çš„æ¸…æ™°åˆ†ç¦»ï¼Œæé«˜äº†åˆ›å»ºåˆæˆå›¾åƒçš„å¯é æ€§å’Œå®‰å…¨æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥çœŸå®ç°æœ‰å›¾åƒè¿›è¡Œå¢å¼ºï¼Œæé«˜äº†æ•°æ®å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒVIGFaceæ¡†æ¶åœ¨å¤šç§é¢éƒ¨è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.08277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ff611b44eefe0215ea88362128bfa99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74270f3e926e6eec3d369974aa6946d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd578810373754eb1b68b56584ea7c0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae392cd4a1297017fe548deb5b42d71d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f1f2b1599b1f77675c70a5e56260c5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49706a6b918e8443dc8fe9d8d1035901.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-88c1a2d35388d5c366bd493c38f13a4f.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  Imaging Ultrafast Dynamical Diffraction wavefronts of femtosecond   laser-induced lattice distortions inside crystalline semiconductors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f561799265f6b718492fcdcb5a29abae.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  Flow-NeRF Joint Learning of Geometry, Poses, and Dense Flow within   Unified Neural Representations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
