<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-15  SciVerse Unveiling the Knowledge Comprehension and Visual Reasoning of   LMMs on Multi-modal Scientific Problems">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5f65fe55be11d251101f7f1bf9c9617e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-15-更新"><a href="#2025-03-15-更新" class="headerlink" title="2025-03-15 更新"></a>2025-03-15 更新</h1><h2 id="SciVerse-Unveiling-the-Knowledge-Comprehension-and-Visual-Reasoning-of-LMMs-on-Multi-modal-Scientific-Problems"><a href="#SciVerse-Unveiling-the-Knowledge-Comprehension-and-Visual-Reasoning-of-LMMs-on-Multi-modal-Scientific-Problems" class="headerlink" title="SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of   LMMs on Multi-modal Scientific Problems"></a>SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of   LMMs on Multi-modal Scientific Problems</h2><p><strong>Authors:Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng</strong></p>
<p>The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: <a target="_blank" rel="noopener" href="https://sciverse-cuhk.github.io/">https://sciverse-cuhk.github.io</a> </p>
<blockquote>
<p>随着大型多模态模型（LMMs）的快速发展，它们被广泛应用于科学问题求解，但其精细功能仍被探索不足。在本文中，我们介绍了SciVerse，这是一个多模态科学评估基准测试，旨在全面评估五个不同版本共5735个测试实例中的大型多模态模型。我们的目标是研究大型多模态模型的三个关键维度：科学知识理解、多模态内容解释和链式思维（CoT）推理。为了揭示大型多模态模型是否具备足够的科学专业知识，我们首先将每个问题转化为包含不同知识水平的三个版本，即无知识版、精简版和丰富版。然后，为了探究大型多模态模型如何解释多模态科学内容，我们标注了另外两个版本，即视觉丰富版和仅视觉版，从文本到图表提供更多问题的信息。通过比较不同版本的结果，SciVerse系统地检验了大型多模态模型在科研领域的专业知识储备和视觉感知能力。此外，为了严格评估链式思维推理，我们提出了一种新的科学链式思维评估策略，对模型输出中的知识和逻辑错误进行分步评估。我们对SciVerse上的不同大型多模态模型进行了全面评估，揭示了它们在科研能力方面的关键局限性，并为未来发展提供了新见解。项目页面：<a target="_blank" rel="noopener" href="https://sciverse-cuhk.github.io/">https://sciverse-cuhk.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10627v1">PDF</a> Initially released in September 2024. Project page:   <a target="_blank" rel="noopener" href="https://sciverse-cuhk.github.io/">https://sciverse-cuhk.github.io</a></p>
<p><strong>Summary</strong></p>
<p>大型多模态模型（LMMs）在科学问题解答中的应用正日益普及，但其精细功能尚未得到充分探索。本文介绍了SciVerse，这是一个多模态科学评估基准测试，旨在全面评估LMMs在5735个测试实例中的表现，涵盖五个不同版本。本研究旨在探究LMMs的三个关键维度：科学知识理解、多模态内容解读和链式思维（CoT）推理。通过不同版本的测试，SciVerse系统地检验了LMMs的专业知识库存和科学领域的视觉感知技能。此外，我们还提出了一种新的科学CoT评估策略，对模型输出中的知识和逻辑错误进行逐步评估。对SciVerse上不同LMMs的广泛评估揭示了其在科学方面的局限性，并为未来发展提供了新见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SciVerse是一个多模态科学评估基准测试，旨在全面评估大型多模态模型（LMMs）的表现。</li>
<li>SciVerse涵盖了5735个测试实例，分为五个不同版本，以探究LMMs在科学知识理解、多模态内容解读和链式思维推理方面的能力。</li>
<li>通过不同版本的测试，SciVerse评估了LMMs的专业知识库存和视觉感知技能。</li>
<li>提出了一个新的科学链式思维（CoT）评估策略，以逐步评估模型输出中的知识和逻辑错误。</li>
<li>LMMs在科学方面的表现存在局限性。</li>
<li>SciVerse评估结果为LMMs的未来发展提供了新见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9f54344de9022da2cbfc192a24a0097.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97dd2a8daa40717bb3e02033cfab1cec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-242dcbf3579332f29e05366bc3d977a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-983962d75468a96f19e79f200cf72b55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6746038182594e9c9dc61067cfc48638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dbaa078a70731ff0a82a97d417c0cbb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DriveLMM-o1-A-Step-by-Step-Reasoning-Dataset-and-Large-Multimodal-Model-for-Driving-Scenario-Understanding"><a href="#DriveLMM-o1-A-Step-by-Step-Reasoning-Dataset-and-Large-Multimodal-Model-for-Driving-Scenario-Understanding" class="headerlink" title="DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model   for Driving Scenario Understanding"></a>DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model   for Driving Scenario Understanding</h2><p><strong>Authors:Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar, Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham Cholakkal, Ivan Laptev, Rao Muhammad Anwer, Salman Khan</strong></p>
<p>While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at <a target="_blank" rel="noopener" href="https://github.com/ayesha-ishaq/DriveLMM-o1">https://github.com/ayesha-ishaq/DriveLMM-o1</a>. </p>
<blockquote>
<p>虽然大型多模态模型（LMM）在各种视觉问答（VQA）任务中表现出了强大的性能，但某些挑战需要复杂的多步骤推理才能得出准确答案。一个特别具有挑战性的任务是自动驾驶，在做出决策之前，需要进行彻底的认知处理。在这个领域，对视觉线索的连续和解释性理解对于有效的感知、预测和规划至关重要。然而，常见的VQA基准测试通常侧重于最终答案的准确性，而忽视了解答过程中使用的推理方法。此外，现有方法缺乏一个综合框架来评估现实驾驶场景中逐步推理的效果。为了解决这一差距，我们提出了DriveLMM-o1，这是一个专门为推进自动驾驶的逐步视觉推理而设计的新数据集和基准测试。我们的基准测试训练集中包含超过1.8万个VQA示例，测试集中包含超过4000个示例，涵盖关于感知、预测和规划的多样化问题，每个问题都辅以逐步推理，以确保自动驾驶场景中的逻辑推断。我们进一步引入了一种经过我们推理数据集微调的大型多模态模型，在复杂驾驶场景中表现出稳健的性能。此外，我们在所提出的数据集上对各开源和闭源方法进行了基准测试，系统地比较了它们在自动驾驶任务中的推理能力。我们的模型在最终答案准确性方面取得了+7.49%的增益，在推理得分上较之前最佳的开源模型提高了3.62%。我们的框架、数据集和模型可在<a target="_blank" rel="noopener" href="https://github.com/ayesha-ishaq/DriveLMM-o1%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ayesha-ishaq/DriveLMM-o1找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10621v1">PDF</a> 8 pages, 4 figures, 3 tables, github:   <a target="_blank" rel="noopener" href="https://github.com/ayesha-ishaq/DriveLMM-o1">https://github.com/ayesha-ishaq/DriveLMM-o1</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一项针对自动驾驶视觉问答任务的研究，提出一个新的数据集和基准测试标准DriveLMM-o1。该数据集包含超过18k个训练样本和超过4k个测试样本，涵盖感知、预测和规划等多样化问题，并强调逐步推理的重要性。该研究还引入了一个大型多模态模型，经过该数据集微调后，在复杂驾驶场景中表现出稳健性能。相较于当前最佳开源模型，该模型在最终答案准确性和推理得分方面均有显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型在视觉问答任务中表现出强大性能，但在需要复杂多步骤推理的自动驾驶任务中面临挑战。</li>
<li>自动驾驶需要全面理解视觉线索的序列和解释性，以进行有效感知、预测和规划。</li>
<li>现有视觉问答基准测试更多地关注最终答案的准确性，而忽视推理过程的重要性。</li>
<li>引入新的数据集和基准测试标准DriveLMM-o1，包含超过18k个训练样本和超过4k个测试样本，强调逐步推理的重要性。</li>
<li>提出的大型多模态模型经过DriveLMM-o1数据集的微调后，在复杂驾驶场景中表现出稳健性能。</li>
<li>与当前最佳开源模型相比，新模型在最终答案准确性和推理得分方面均有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94e6a86cba95488f21b9b94ea46fc048.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bde750f48495f0ec0226843ef0df0d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b96947be3f964969f947e3a4060efddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a16d22d37aa8ccc069c7cc948fc00ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8582f4ee35ffb69482345cb72df73fa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4216013adcb745c9a970a8b76fb4638.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization"><a href="#R1-Onevision-Advancing-Generalized-Multimodal-Reasoning-through-Cross-Modal-Formalization" class="headerlink" title="R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization"></a>R1-Onevision: Advancing Generalized Multimodal Reasoning through   Cross-Modal Formalization</h2><p><strong>Authors:Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen</strong></p>
<p>Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. </p>
<blockquote>
<p>大型语言模型在复杂的文本任务中表现出了惊人的推理能力。然而，多模态推理，这需要整合视觉和文本信息，仍然是一个巨大的挑战。现有的视觉语言模型往往难以有效地分析和推理视觉内容，导致在复杂的推理任务上的性能不佳。此外，缺乏全面的基准测试阻碍了多模态推理能力的准确评估。在本文中，我们介绍了R1-Onevision，一个旨在弥合视觉感知和深度推理之间差距的多模态推理模型。为此，我们提出了一种跨模态推理管道，将图像转换为正式的纹理表示，从而实现基于精确语言的推理。利用这一管道，我们构建了R1-Onevision数据集，该数据集在各个领域提供了详细、逐步的多模态推理注释。我们进一步通过监督微调强化学习来开发R1-Onevision模型，以培养先进的推理和稳健的泛化能力。为了全面评估不同等级的多模态推理性能，我们推出了R1-Onevision-Bench，这是一个与人类教育阶段相一致的基准测试，涵盖从初中到大学及以后的考试。实验结果表明，R1-Onevision在多个具有挑战性的多模态推理基准测试中达到了最新技术水平，超越了GPT-4o和Qwen2.5-VL等模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10615v1">PDF</a> Code and Model: <a target="_blank" rel="noopener" href="https://github.com/Fancy-MLLM/R1-onevision">https://github.com/Fancy-MLLM/R1-onevision</a></p>
<p><strong>Summary</strong>：大型语言模型在复杂的文本任务中展现出色的推理能力，但在需要整合视觉和文本信息的多模态推理方面仍存在挑战。现有视觉语言模型在分析视觉内容方面表现不足，导致在复杂推理任务上性能不佳。本文介绍R1-Onevision多模态推理模型，通过跨模态推理管道将图像转化为正式文本表示，实现精确的语言推理。构建R1-Onevision数据集并提供跨不同领域的详细、逐步多模态推理注释。通过监督微调与强化学习，培养先进的推理和稳健的泛化能力。为全面评估不同级别的多模态推理性能，本文还引入与人类教育阶段相符的R1-Onevision-Bench基准测试，涵盖从初中到大学及以后的考试。实验结果证明R1-Onevision在多个具有挑战性的多模态推理基准测试上实现卓越性能，优于GPT-4o和Qwen2.5-VL模型。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在多模态推理方面存在挑战，需要整合视觉和文本信息。</li>
<li>现有视觉语言模型在分析视觉内容方面表现不足，导致复杂推理任务性能不佳。</li>
<li>R1-Onevision多模态推理模型通过跨模态推理管道实现图像到文本表示的转化，支持精确语言推理。</li>
<li>R1-Onevision数据集的构建提供了详细、逐步的多模态推理注释，适用于不同领域。</li>
<li>R1-Onevision模型通过监督微调和强化学习培养高级推理和泛化能力。</li>
<li>R1-Onevision-Bench基准测试用于评估不同级别的多模态推理性能，与人类的各个阶段教育相匹配。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cb8237035f928e8b62af18ec3e240ce1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16752ac75fa0fe28dc5b056a5e187cfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4d139b9a615271f0b61530bae848ae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa35ecce8741f0e38a5eef55a96232db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d79e2c222974a93705052dae305edd64.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unveiling-the-Mathematical-Reasoning-in-DeepSeek-Models-A-Comparative-Study-of-Large-Language-Models"><a href="#Unveiling-the-Mathematical-Reasoning-in-DeepSeek-Models-A-Comparative-Study-of-Large-Language-Models" class="headerlink" title="Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative   Study of Large Language Models"></a>Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative   Study of Large Language Models</h2><p><strong>Authors:Afrar Jahin, Arif Hassan Zidan, Yu Bao, Shizhe Liang, Tianming Liu, Wei Zhang</strong></p>
<p>With the rapid evolution of Artificial Intelligence (AI), Large Language Models (LLMs) have reshaped the frontiers of various fields, spanning healthcare, public health, engineering, science, agriculture, education, arts, humanities, and mathematical reasoning. Among these advancements, DeepSeek models have emerged as noteworthy contenders, demonstrating promising capabilities that set them apart from their peers. While previous studies have conducted comparative analyses of LLMs, few have delivered a comprehensive evaluation of mathematical reasoning across a broad spectrum of LLMs. In this work, we aim to bridge this gap by conducting an in-depth comparative study, focusing on the strengths and limitations of DeepSeek models in relation to their leading counterparts. In particular, our study systematically evaluates the mathematical reasoning performance of two DeepSeek models alongside five prominent LLMs across three independent benchmark datasets. The findings reveal several key insights: 1). DeepSeek-R1 consistently achieved the highest accuracy on two of the three datasets, demonstrating strong mathematical reasoning capabilities. 2). The distilled variant of LLMs significantly underperformed compared to its peers, highlighting potential drawbacks in using distillation techniques. 3). In terms of response time, Gemini 2.0 Flash demonstrated the fastest processing speed, outperforming other models in efficiency, which is a crucial factor for real-time applications. Beyond these quantitative assessments, we delve into how architecture, training, and optimization impact LLMs’ mathematical reasoning. Moreover, our study goes beyond mere performance comparison by identifying key areas for future advancements in LLM-driven mathematical reasoning. This research enhances our understanding of LLMs’ mathematical reasoning and lays the groundwork for future advancements </p>
<blockquote>
<p>随着人工智能（AI）的快速发展，大型语言模型（LLM）已经重塑了各个领域的边界，涵盖了医疗、公共卫生、工程、科学、农业、教育、艺术、人文和数学推理等多个领域。在这些进展中，DeepSeek模型表现出色，展现出令人瞩目的能力，使其与同行区分开来。虽然之前的研究已经对LLM进行了比较分析，但很少有研究对一系列LLM的数学推理能力进行全面评估。在这项工作中，我们旨在通过进行深入的对比研究来填补这一空白，重点关注DeepSeek模型与其领先同行的优势和局限性。特别是，我们的研究系统地评估了两个DeepSeek模型和五种突出的LLM在三个独立基准数据集上的数学推理性能。研究发现揭示了几个关键见解：1. DeepSeek-R1在三个数据集中的两个上始终实现了最高精度，表现出强大的数学推理能力。2. 与同行相比，蒸馏变种LLM的表现显著较差，这突显了使用蒸馏技术可能存在的潜在缺陷。3. 在响应时间方面，Gemini 2.0 Flash显示出最快的处理速度，在效率方面超越了其他模型，这对于实时应用是一个关键因素。除了这些定量评估外，我们还深入研究了架构、训练和优化如何影响LLM的数学推理。此外，我们的研究超越了仅仅的性能比较，而是确定了未来在LLM驱动的数学推理方面发展的关键领域。这项研究增强了我们对于LLM数学推理的理解，并为未来的进步奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着人工智能的快速发展，大型语言模型（LLM）已经重塑了多个领域的前沿，其中DeepSeek模型表现突出。然而，关于数学推理能力的全面评估仍有所欠缺。本研究旨在填补这一空白，深入评估DeepSeek模型及其领先同类模型的表现。研究结果显示DeepSeek-R1在两项数据集中表现最佳，展现了强大的数学推理能力。同时，研究也探讨了架构、训练和优化对LLM数学推理能力的影响，并为未来的发展方向提供了关键见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1在多项数据集中展现出色的数学推理能力。</li>
<li>蒸馏型LLM表现较差，提示蒸馏技术在某些应用场景中可能存在局限性。</li>
<li>Gemini 2.0 Flash在响应时间方面表现最佳，具有较高的处理效率。</li>
<li>架构、训练和优化对LLM的数学推理能力有重要影响。</li>
<li>研究结果不仅关注性能比较，还指出了未来LLM驱动的数学推理发展的关键方向。</li>
<li>本研究提高了对LLM数学推理能力的理解，为后续研究提供了基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9da35c5617ab8e527b3a95bb07a1d4e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2029a26a2c658dde3ddb8c0b6cf5d56a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec02691a416c672424aeb16f36e5d699.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Light-R1-Curriculum-SFT-DPO-and-RL-for-Long-COT-from-Scratch-and-Beyond"><a href="#Light-R1-Curriculum-SFT-DPO-and-RL-for-Long-COT-from-Scratch-and-Beyond" class="headerlink" title="Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and   Beyond"></a>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and   Beyond</h2><p><strong>Authors:Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang</strong></p>
<p>This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 &amp; 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL. </p>
<blockquote>
<p>本文介绍了我们在Light-R1系列方面的工作，其中模型、数据和代码均已发布。我们首先专注于从头开始训练长COT模型，特别是从最初不具备长COT能力的模型开始。使用包含两阶段SFT和半在线策略DPO的课程训练配方，我们从Qwen2.5-32B-Instruct训练了Light-R1-32B模型，与DeepSeek-R1-Distill-Qwen-32B相比，其数学性能优越。尽管只在数学数据上进行训练，但Light-R1-32B在其他领域表现出强大的泛化能力。在这项工作的后续阶段，我们强调了为第二阶段SFT构建的3k数据集对增强其他模型的重要好处。通过对此数据集进行微调DeepSeek-R1-Distilled模型，我们获得了7B和14B的新SOTA模型，而32B模型Light-R1-32B-DS与QwQ-32B和DeepSeek-R1表现相当。此外，我们通过将强化学习，特别是GRPO，应用于长COT模型，进一步提高了推理性能。我们成功地用强化学习训练了最终的Light-R1-14B-DS，在14B参数模型中实现数学方面的SOTA性能。Light-R1-14B-DS在AIME24和AIME25的得分分别为74.0和60.2，甚至超越了许多32B模型和DeepSeek-R1-Distill-Llama-70B。其强化学习训练还表现出预期的行为，响应长度和奖励分数同时增加。Light-R1系列工作验证了从头开始训练长COT模型的可行性，展示了SFT数据的艺术，并发布了来自RL的SOTA模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10460v1">PDF</a> all release at <a target="_blank" rel="noopener" href="https://github.com/Qihoo360/Light-R1">https://github.com/Qihoo360/Light-R1</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Light-R1系列的研究工作，涉及模型、数据和代码的发布。研究重点在于从头开始训练具有长期连续输出（COT）能力的模型。通过使用包括两个阶段自训练（SFT）和半在线策略DPO的课程训练配方，成功训练了Light-R1-32B模型，表现出优异的数学性能。该模型在多个领域都展现出强大的泛化能力。后续研究中，构建了3k数据集用于第二阶段SFT，提升了其他模型性能。通过对DeepSeek-R1-Distilled模型进行微调，获得了新的SOTA模型。此外，将强化学习应用于长期COT模型，进一步提高了推理性能。最终，Light-R1系列的研究验证了从头训练长期COT模型的可行性，展示了自训练数据的重要性，并发布了使用强化学习训练的SOTA模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Light-R1系列研究关注于从头开始训练具有长期连续输出（COT）能力的模型。</li>
<li>通过特殊的课程训练配方，成功训练了Light-R1-32B模型，表现出卓越的数学性能。</li>
<li>Light-R1-32B模型在多个领域展现出强大的泛化能力。</li>
<li>构建了3k数据集用于第二阶段自训练，显著提升了其他模型的性能。</li>
<li>通过微调DeepSeek-R1-Distilled模型，获得了新的SOTA模型在7B和14B参数模型中表现优异。</li>
<li>应用强化学习进一步提高了长期COT模型的推理性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5a018a2ae84ca134bc7a5d33c35e32f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64b986dd0b93580da9247a39043841a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-003c8bb5f462c67bbb8e34ba331e0a79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2690354181d8bab7e32618f47f81cddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7f8b6052c48bbaafa0b34d30cbae318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17df686689387cdbb247696390d11abd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="New-Trends-for-Modern-Machine-Translation-with-Large-Reasoning-Models"><a href="#New-Trends-for-Modern-Machine-Translation-with-Large-Reasoning-Models" class="headerlink" title="New Trends for Modern Machine Translation with Large Reasoning Models"></a>New Trends for Modern Machine Translation with Large Reasoning Models</h2><p><strong>Authors:Sinuo Liu, Chenyang Lyu, Minghao Wu, Longyue Wang, Weihua Luo, Kaifu Zhang</strong></p>
<p>Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X-&gt;Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it. </p>
<blockquote>
<p>近期大型推理模型（LRMs）的最新进展，特别是那些采用思维链推理（CoT）的模型，为机器翻译（MT）开辟了新的可能性。本立场论文认为，LRMs通过重新构建翻译作为一个需要上下文、文化和语言理解和推理的动态推理任务，从而极大地转变了传统的神经机器翻译以及基于大型语言模型的机器翻译范式。我们确定了三个基本转变：1）上下文连贯性，LRMs通过跨句和复杂上下文或甚至无上下文的显式推理来解决歧义并保留话语结构；2）文化意图性，使模型能够通过推断说话者的意图、受众期望和社会语言规范来适应输出；3）自我反思，LRMs可以在推理时间进行自我反思，以纠正翻译中可能存在的错误，特别是在极端嘈杂的情况下，显示出比简单的X-&gt;Y翻译更高的稳健性。我们通过展示实证例子来探索翻译中的各种场景，包括风格化翻译、文档级翻译和多模态翻译，这些例子证明了LRMs在翻译中的优越性。我们还发现了LRMs在机器翻译方面的几个有趣现象，包括自动枢轴翻译以及关键挑战，如翻译的过度本地化和推理效率。总之，我们认为LRMs重新定义了翻译系统，不仅仅作为文本转换器，而是作为能够超越文本进行意义推理的多语言认知代理。这种范式转变提醒我们，在更广泛的背景下，要超越传统的翻译场景来思考LRMs在翻译方面的问题——我们可以在它之上实现什么。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）特别是利用思维链（CoT）的模型为机器翻译（MT）带来了全新可能性。该立场论文认为，LRMs通过重新构建翻译作为一个需要上下文、文化和语言理解和推理的动态推理任务，从而极大地改变了传统的神经机器翻译以及基于LLMs的机器翻译范式。包括三大基础转变：上下文连贯性、文化意图和自我反思。通过实证例子展示了LRMs在翻译中的优越性，并指出了自动转换翻译等现象以及过度本地化翻译和推理效率等挑战。总的来说，LRMs重新定义了翻译系统，不仅仅是文本转换器，而是能够超越文本进行意义推理的多语言认知代理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMs通过引入思维链推理，为机器翻译领域带来全新可能性。</li>
<li>LRMs改变了传统的机器翻译范式，将其构建为一个需要上下文、文化和语言理解的动态推理任务。</li>
<li>LRMs实现了三大基础转变：上下文连贯性、文化意图和自我反思。</li>
<li>LRMs通过实证例子展示了在风格化翻译、文档级翻译和多模态翻译等场景中的优越性。</li>
<li>LRMs在机器翻译中展现出自动转换翻译等现象。</li>
<li>LRMs面临过度本地化翻译和推理效率等挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d7013a0bc941d879192584f785e46ecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16ac621e5356ed6f81ab09edccc21589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b394e70b08f89bb678a04428edb81a9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3886beef9c4513ca683095e69141f5f5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VisualPRM-An-Effective-Process-Reward-Model-for-Multimodal-Reasoning"><a href="#VisualPRM-An-Effective-Process-Reward-Model-for-Multimodal-Reasoning" class="headerlink" title="VisualPRM: An Effective Process Reward Model for Multimodal Reasoning"></a>VisualPRM: An Effective Process Reward Model for Multimodal Reasoning</h2><p><strong>Authors:Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao, Jifeng Dai, Wenhai Wang</strong></p>
<p>We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in <a target="_blank" rel="noopener" href="https://internvl.github.io/blog/2025-03-13-VisualPRM/">https://internvl.github.io/blog/2025-03-13-VisualPRM/</a>. </p>
<blockquote>
<p>我们介绍了VisualPRM，这是一个先进的跨模态过程奖励模型（PRM），拥有8B参数，它通过采用最佳N（BoN）评估策略，提高了现有跨模态大型语言模型（MLLMs）在不同模型规模和家族中的推理能力。具体来说，我们的模型改进了三种类型的MLLMs和四种不同模型规模的推理性能。即使应用到高度能力的InternVL2.5-78B上，它在七个跨模态推理基准测试中实现了5.9分的提升。实验结果表明，与结果奖励模型和自我一致性相比，我们的模型在BoN评估中表现出更优越的性能。为了训练跨模态PRMs，我们使用自动化数据管道构建了一个跨模态过程监督数据集VisualPRM400K。为了评估跨模态PRMs，我们提出了VisualProcessBench基准测试，它带有由人类注释的步骤正确性标签，用于衡量PRMs在跨模态推理任务中检测错误步骤的能力。我们希望我们的工作能够激发更多的未来研究，并为MLLMs的发展做出贡献。我们的模型、数据和基准测试已在<a target="_blank" rel="noopener" href="https://internvl.github.io/blog/2025-03-13-VisualPRM/">https://internvl.github.io/blog/2025-03-13-VisualPRM/</a>上发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10291v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视觉PRM是一种先进的跨模态过程奖励模型，具有8B参数，提高了多种不同规模和家族的多模态大语言模型的推理能力。通过最佳N评估策略，我们的模型在三种类型的MLLMs和四个不同的模型规模上取得了显著的改进。此外，我们的模型还表现出了出色的性能，在高度复杂的InternVL模型上实现了跨七个跨模态推理基准测试5.9点的改进。我们构建了视觉PRM数据集和多模态过程基准测试，以支持多模态PRM的训练和评估。我们相信这项工作将激发更多未来的研究并为多模态语言模型的发展做出贡献。具体详情可访问：<a target="_blank" rel="noopener" href="https://internvl.github.io/blog/2025-03-13-VisualPRM/">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<p>一、提出了视觉PRM模型，一种先进的多模态过程奖励模型（PRM），参数规模达到8B。<br>二、该模型增强了多模态大语言模型（MLLMs）的推理能力，适用于不同规模和类型。<br>三、在最佳N评估策略下，视觉PRM模型对三种类型的MLLMs和四个不同规模的模型均有显著提高表现。<br>四、在高度复杂的InternVL模型中，视觉PRM实现了跨七个跨模态推理基准测试5.9点的显著改进。<br>五、为了支持多模态PRMs的训练和评估，构建了视觉PRM数据集和多模态过程基准测试。<br>六、该模型的性能优于结果奖励模型和自一致性模型在最佳N评估中的表现。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10291">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62cb54e2985afc2c9ecdd9d854b259de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a01bacd38109d99edb09ec112e561f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d22c9c7251f0bec958c9aaeb096306a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-112f4971e9901fcf2cd3bb5b117265dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a453ecf55def3f663014964469b1c1b0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SurgRAW-Multi-Agent-Workflow-with-Chain-of-Thought-Reasoning-for-Surgical-Intelligence"><a href="#SurgRAW-Multi-Agent-Workflow-with-Chain-of-Thought-Reasoning-for-Surgical-Intelligence" class="headerlink" title="SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence"></a>SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence</h2><p><strong>Authors:Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, Yueming Jin</strong></p>
<p>Integration of Vision-Language Models (VLMs) in surgical intelligence is hindered by hallucinations, domain knowledge gaps, and limited understanding of task interdependencies within surgical scenes, undermining clinical reliability. While recent VLMs demonstrate strong general reasoning and thinking capabilities, they still lack the domain expertise and task-awareness required for precise surgical scene interpretation. Although Chain-of-Thought (CoT) can structure reasoning more effectively, current approaches rely on self-generated CoT steps, which often exacerbate inherent domain gaps and hallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent framework that delivers transparent, interpretable insights for most tasks in robotic-assisted surgery. By employing specialized CoT prompts across five tasks: instrument recognition, action recognition, action prediction, patient data extraction, and outcome assessment, SurgRAW mitigates hallucinations through structured, domain-aware reasoning. Retrieval-Augmented Generation (RAG) is also integrated to external medical knowledge to bridge domain gaps and improve response reliability. Most importantly, a hierarchical agentic system ensures that CoT-embedded VLM agents collaborate effectively while understanding task interdependencies, with a panel discussion mechanism promotes logical consistency. To evaluate our method, we introduce SurgCoTBench, the first reasoning-based dataset with structured frame-level annotations. With comprehensive experiments, we demonstrate the effectiveness of proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures, achieving the state-of-the-art performance and advancing explainable, trustworthy, and autonomous surgical assistance. </p>
<blockquote>
<p>将视觉语言模型（VLMs）整合到手术智能中面临着诸多挑战，如幻觉、领域知识差距以及手术场景中任务相互依赖性的有限理解，这削弱了其在临床上的可靠性。尽管最新的VLMs表现出强大的通用推理和思维能力，但它们仍然缺乏精确解读手术场景所需的领域专业知识和任务意识。虽然“思维链”（CoT）能够更有效地结构化推理，但当前的方法依赖于自我生成的CoT步骤，这往往会加剧固有的领域差距和幻觉。</p>
</blockquote>
<p>为了克服这一问题，我们推出了SurgRAW，这是一个由CoT驱动的多代理框架，为机器人辅助手术中的大多数任务提供透明、可解释的见解。通过五个任务中的专门CoT提示：仪器识别、动作识别、动作预测、患者数据提取和结果评估，SurgRAW通过结构化、领域感知推理来缓解幻觉。同时集成了增强生成（RAG）以访问外部医学知识，以弥补领域差距并提高响应可靠性。最重要的是，一个分层的多代理系统确保CoT嵌入的VLM代理能够进行有效协作，同时了解任务相互依赖性，而小组讨论机制则促进了逻辑一致性。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10265v1">PDF</a> </p>
<p><strong>Summary</strong>：视觉语言模型（VLMs）在手术智能集成中面临幻觉、领域知识差距和手术场景内任务相互依赖的有限理解等问题，影响临床可靠性。虽然最近的VLMs展现出强大的通用推理和思维能力，但它们仍然缺乏精确解读手术场景所需的领域专业知识和任务意识。为此，提出SurgRAW，一个基于思维链（CoT）的多智能体框架，为机器人辅助手术中的大多数任务提供透明、可解释性的见解。通过五个任务的专门思维链提示，SurgRAW通过结构化、领域知识驱动的推理来缓解幻觉问题。同时集成检索增强生成（RAG）以获取外部医学知识，缩小领域差距并提高响应可靠性。最重要的是，层次化的智能体系确保思维链嵌入的VLM智能体有效协作，理解任务相互依赖性，并通过小组讨论机制促进逻辑一致性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>VLM在手术智能应用中存在幻觉、领域知识差距和任务相互依赖理解有限的问题。</li>
<li>虽然VLM具有强大的通用推理和思维能力，但仍需提高在手术领域的专业知识和任务意识。</li>
<li>SurgRAW框架利用CoT结构来增强手术场景的解读，通过结构化、领域知识驱动的推理缓解幻觉问题。</li>
<li>SurgRAW集成RAG以获取外部医学知识，缩小领域差距并提高响应的可靠性。</li>
<li>层次化的智能体系确保VLM智能体有效协作，理解任务相互依赖性。</li>
<li>SurgRAW引入SurgCoTBench数据集进行方法评估，该数据集是首个基于推理的结构化帧级别注释数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-924dc5051a215b0cb2331628406b2cd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08663a2f6ba4409243f6a1cf84bfeb5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9de395e9305ab5dbde7a7cc0f00dcd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bba250c9d2179659cd5a6bdacb753073.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ImageScope-Unifying-Language-Guided-Image-Retrieval-via-Large-Multimodal-Model-Collective-Reasoning"><a href="#ImageScope-Unifying-Language-Guided-Image-Retrieval-via-Large-Multimodal-Model-Collective-Reasoning" class="headerlink" title="ImageScope: Unifying Language-Guided Image Retrieval via Large   Multimodal Model Collective Reasoning"></a>ImageScope: Unifying Language-Guided Image Retrieval via Large   Multimodal Model Collective Reasoning</h2><p><strong>Authors:Pengfei Luo, Jingbo Zhou, Tong Xu, Yuan Xia, Linli Xu, Enhong Chen</strong></p>
<p>With the proliferation of images in online content, language-guided image retrieval (LGIR) has emerged as a research hotspot over the past decade, encompassing a variety of subtasks with diverse input forms. While the development of large multimodal models (LMMs) has significantly facilitated these tasks, existing approaches often address them in isolation, requiring the construction of separate systems for each task. This not only increases system complexity and maintenance costs, but also exacerbates challenges stemming from language ambiguity and complex image content, making it difficult for retrieval systems to provide accurate and reliable results. To this end, we propose ImageScope, a training-free, three-stage framework that leverages collective reasoning to unify LGIR tasks. The key insight behind the unification lies in the compositional nature of language, which transforms diverse LGIR tasks into a generalized text-to-image retrieval process, along with the reasoning of LMMs serving as a universal verification to refine the results. To be specific, in the first stage, we improve the robustness of the framework by synthesizing search intents across varying levels of semantic granularity using chain-of-thought (CoT) reasoning. In the second and third stages, we then reflect on retrieval results by verifying predicate propositions locally, and performing pairwise evaluations globally. Experiments conducted on six LGIR datasets demonstrate that ImageScope outperforms competitive baselines. Comprehensive evaluations and ablation studies further confirm the effectiveness of our design. </p>
<blockquote>
<p>随着在线内容中图像数量的激增，语言指导的图像检索（LGIR）在过去十年中已成为研究热点，涵盖了各种具有不同输入形式的子任务。尽管大型多模态模型（LMM）的发展极大地促进了这些任务，但现有方法通常孤立地解决它们，需要为每个任务构建单独的系统。这不仅增加了系统复杂性和维护成本，而且还加剧了由语言模糊和复杂的图像内容引起的挑战，使得检索系统难以提供准确和可靠的结果。为此，我们提出了ImageScope，这是一个无需训练的三阶段框架，它利用集体推理来统一LGIR任务。统一的背后关键见解在于语言的组合性质，它将各种LGIR任务转变为一种通用的文本到图像检索过程，加上LMM的推理作为通用的验证来完善结果。具体而言，在第一阶段，我们通过利用链式思维（CoT）推理在不同级别的语义粒度上合成搜索意图，提高了框架的稳健性。在第二和第三阶段，然后我们通过局部验证谓词命题以及全局进行配对评估来反思检索结果。在六个LGIR数据集上进行的实验表明，ImageScope优于竞争基线。综合评估和消融研究进一步证实了我们的设计有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10166v1">PDF</a> WWW 2025</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了随着网络内容中图片数量的增加，语言引导的图像检索（LGIR）已成为过去十年的研究热点。尽管大型多模态模型（LMMs）的发展极大地促进了这些任务，但现有方法通常孤立地解决它们，需要为每个任务构建单独的系统。这不仅增加了系统复杂性和维护成本，还加剧了来自语言模糊和复杂图像内容的挑战，使得检索系统难以提供准确可靠的结果。为此，提出了ImageScope，一个无需训练的三阶段框架，利用集体推理来统一LGIR任务。关键洞察力在于语言的组成性质，它将各种LGIR任务转变为一种通用的文本到图像检索过程，而LMMs的推理则作为通用验证来优化结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言引导的图像检索（LGIR）已成为研究热点，面临系统复杂性和维护成本等问题。</li>
<li>ImageScope是一个无需训练的三阶段框架，旨在统一LGIR任务，提供准确可靠的检索结果。</li>
<li>ImageScope的关键在于利用语言的组成性质，将LGIR任务转变为文本到图像检索过程。</li>
<li>第一阶段通过链式思维（CoT）推理合成不同语义粒度的搜索意图，提高了框架的稳健性。</li>
<li>第二、三阶段通过局部验证谓词命题和全局成对评估进行结果反思。</li>
<li>在六个LGIR数据集上的实验表明，ImageScope优于竞争基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28cb0694a78224dccc6dccf36f009fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb739d9d43c3cc9bc6de3c6f0a84957d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-704c7823e07b22a721daa30635be7520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb2b8dd20029ebe5ab0226cf2f1ca503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39c181245f58016f94578513922308de.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cognitive-Mental-LLM-Leveraging-Reasoning-in-Large-Language-Models-for-Mental-Health-Prediction-via-Online-Text"><a href="#Cognitive-Mental-LLM-Leveraging-Reasoning-in-Large-Language-Models-for-Mental-Health-Prediction-via-Online-Text" class="headerlink" title="Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for   Mental Health Prediction via Online Text"></a>Cognitive-Mental-LLM: Leveraging Reasoning in Large Language Models for   Mental Health Prediction via Online Text</h2><p><strong>Authors:Avinash Patil, Amardeep Kour Gedhu</strong></p>
<p>Large Language Models (LLMs) have demonstrated potential in predicting mental health outcomes from online text, yet traditional classification methods often lack interpretability and robustness. This study evaluates structured reasoning techniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and Tree-of-Thought (ToT)-to improve classification accuracy across multiple mental health datasets sourced from Reddit. We analyze reasoning-driven prompting strategies, including Zero-shot CoT and Few-shot CoT, using key performance metrics such as Balanced Accuracy, F1 score, and Sensitivity&#x2F;Specificity. Our findings indicate that reasoning-enhanced techniques improve classification performance over direct prediction, particularly in complex cases. Compared to baselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained transformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs such as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable gains on datasets like Dreaddit (+0.52% over M-LLM, +0.82% over BERT) and SDCNL (+4.67% over M-LLM, +2.17% over BERT). However, performance declines in Depression Severity, and CSSRS predictions suggest dataset-specific limitations, likely due to our using a more extensive test set. Among prompting strategies, Few-shot CoT consistently outperforms others, reinforcing the effectiveness of reasoning-driven LLMs. Nonetheless, dataset variability highlights challenges in model reliability and interpretability. This study provides a comprehensive benchmark of reasoning-based LLM techniques for mental health text classification. It offers insights into their potential for scalable clinical applications while identifying key challenges for future improvements. </p>
<blockquote>
<p>大型语言模型（LLM）已显示出从在线文本预测心理健康结果的潜力，但传统分类方法往往缺乏可解释性和稳健性。本研究评估了结构化推理技术——思维链（CoT）、自我一致性（SC-CoT）和思维树（ToT）——以提高在多个来自Reddit的心理健康数据集上的分类精度。我们分析了以推理为核心的提示策略，包括零镜头CoT和少镜头CoT，使用平衡精度、F1分数和敏感性&#x2F;特异性等关键性能指标。我们的研究结果表明，通过推理增强的技术提高了直接预测的分类性能，特别是在复杂情况下。与基线方法（如零镜头非CoT提示）以及经过微调预训练的转换器（如BERT和Mental-RoBERTa）以及经过微调开源LLM（如Mental Alpaca和Mental-Flan-T5）相比，以推理为核心的LLM在数据集上取得了显著的成绩，如在Dreaddit上较M-LLM提高0.52%，较BERT提高0.82%，在SDCNL上较M-LLM提高4.67%，较BERT提高2.17%。然而，在抑郁症严重程度和CSSRS预测方面的表现下降，表明存在特定数据集的局限性，这可能是由于我们使用了更广泛的测试集。在提示策略中，少镜头CoT始终表现最佳，进一步证明了以推理为核心的LLM的有效性。然而，数据集的变化突显了模型可靠性和可解释性方面的挑战。本研究为基于推理的LLM技术在心理健康文本分类方面提供了全面的基准测试。它为可扩展的临床应用提供了见解，并指出了未来改进的关键挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10095v1">PDF</a> 8 pages, 4 Figures, 3 tables</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLMs）在预测心理健康结果方面具有潜力，但传统分类方法缺乏可解释性和稳健性。本研究评估了基于结构化推理的技术，包括思维链（CoT）、自我一致性（SC-CoT）和思维树（ToT），以提高多个来自Reddit的心理健康数据集的分类准确性。我们分析了基于推理的提示策略，包括零射CoT和少射CoT，使用平衡精度、F1分数和敏感性&#x2F;特异性等关键性能指标。研究发现，基于推理的技术在直接预测方面表现出改进，特别是在复杂情况下。与基线方法（如零射非CoT提示）以及微调预训练变压器（如BERT和Mental-RoBERTa）和微调开源LLMs（如Mental Alpaca和Mental-Flan-T5）相比，基于推理的LLMs在数据集（如Dreaddit和SDCNL）上实现了显著收益。然而，在抑郁症严重性和CSSRS预测方面的性能下降表明，特定数据集可能存在局限性，这可能是由于使用了更广泛的测试集。在提示策略中，少射CoT表现最好，证实了基于推理的LLMs的有效性。然而，数据集的变化强调了模型可靠性和可解释性的挑战。本研究为基于推理的LLM技术在心理健康文本分类方面提供了全面的基准测试。它为可扩展的临床应用提供了见解，并指出了未来改进的关键挑战。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLMs）在预测心理健康结果方面具有潜力。</li>
<li>传统分类方法缺乏可解释性和稳健性。</li>
<li>结构化推理技术（如思维链和思维树）能提高分类准确性。</li>
<li>基于推理的提示策略（如零射和少射CoT）表现良好。</li>
<li>与基线方法和微调预训练模型相比，基于推理的LLMs在某些数据集上实现了显著收益。</li>
<li>数据集的多样性对模型性能和可解释性构成挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b28b81968b782880701f0efffb24fc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9820f88b053584216b6cb1ecd79a38a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9980ea44e20e355761d5ff240cb0f49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80bab825098126a3e54ec3b5d30ffdcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a05609bde858186c3bde655d9f9c1d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5db2d2d1a6c1d5c76adc5e9babde10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59b00984a235693cdcff72e5f26f78ce.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Why-Does-Your-CoT-Prompt-Not-Work-Theoretical-Analysis-of-Prompt-Space-Complexity-its-Interaction-with-Answer-Space-During-CoT-Reasoning-with-LLMs-A-Recurrent-Perspective"><a href="#Why-Does-Your-CoT-Prompt-Not-Work-Theoretical-Analysis-of-Prompt-Space-Complexity-its-Interaction-with-Answer-Space-During-CoT-Reasoning-with-LLMs-A-Recurrent-Perspective" class="headerlink" title="Why Does Your CoT Prompt (Not) Work? Theoretical Analysis of Prompt   Space Complexity, its Interaction with Answer Space During CoT Reasoning with   LLMs: A Recurrent Perspective"></a>Why Does Your CoT Prompt (Not) Work? Theoretical Analysis of Prompt   Space Complexity, its Interaction with Answer Space During CoT Reasoning with   LLMs: A Recurrent Perspective</h2><p><strong>Authors:Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, Dujian Ding</strong></p>
<p>Despite the remarkable successes of Large Language Models (LLMs), their fundamental Transformer architecture possesses inherent theoretical limitations that restrict their capability to handle reasoning tasks with increasing computational complexity. Chain-of-Thought (CoT) prompting has emerged as a practical solution, supported by several theoretical studies. However, current CoT-based methods (including ToT, GoT, etc.) generally adopt a “one-prompt-fits-all” strategy, using fixed templates (e.g., “think step by step”) across diverse reasoning tasks. This method forces models to navigate an extremely complex prompt space to identify effective reasoning paths. The current prompt designing research are also heavily relying on trial-and-error rather than theoretically informed guidance. In this paper, we provide a rigorous theoretical analysis of the complexity and interplay between two crucial spaces: the prompt space (the space of potential prompt structures) and the answer space (the space of reasoning solutions generated by LLMs) in CoT reasoning. We demonstrate how reliance on a single universal prompt (e.g. think step by step) can negatively impact the theoretical computability of LLMs, illustrating that prompt complexity directly influences the structure and effectiveness of the navigation in answer space. Our analysis highlights that sometimes human supervision is critical for efficiently navigating the prompt space. We theoretically and empirically show that task-specific prompting significantly outperforms unsupervised prompt generation, emphasizing the necessity of thoughtful human guidance in CoT prompting. </p>
<blockquote>
<p>尽管大型语言模型（LLM）取得了显著的成就，但其基本的Transformer架构具有固有的理论局限性，限制了其处理计算复杂度不断增加的推理任务的能力。Chain-of-Thought（CoT）提示已成为一种实用的解决方案，并得到了一些理论研究的支持。然而，当前的基于CoT的方法（包括ToT、GoT等）通常采用“一提示适合所有”的策略，在多样化的推理任务中使用固定的模板（例如，“一步一步思考”）。这种方法迫使模型在极其复杂的提示空间中识别有效的推理路径。当前关于提示设计的研究还严重依赖于试错，而非理论引导的指南。在本文中，我们对两个关键空间之间的复杂性和相互作用进行了严格的理论分析：提示空间（潜在提示结构的空间）和答案空间（由LLM生成的推理解决方案的空间）在CoT推理中。我们展示了依赖单一通用提示（例如“一步一步思考”）如何对LLM的理论计算能力产生负面影响，并说明了提示复杂性直接影响答案空间中的导航结构及其有效性。我们的分析强调，有时人类的监督对于有效地导航提示空间至关重要。我们从理论和实践上证明，任务特定的提示显著优于无监督的提示生成，强调在CoT提示中需要深思熟虑的人类指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10084v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2410.14198</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）在应对复杂推理任务时的理论局限性，并介绍了思维链（CoT）提示作为一种实用解决方案。然而，当前CoT方法采用“一刀切”策略，使用固定模板应对各种推理任务，这限制了模型在识别有效推理路径时的理论计算能力。本文严格分析了提示空间（潜在提示结构）和答案空间（LLMs生成的推理解决方案）之间的复杂性和相互作用。文章指出，依赖单一通用提示可能影响LLMs的理论计算能力，提示复杂性直接影响答案空间的结构和有效性。有时，人类的监督对于有效地导航提示空间至关重要。本文理论和实证地表明，任务特定的提示显著优于无监督的提示生成，强调在CoT提示中需要深思熟虑的人类指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在应对复杂推理任务时存在理论局限性。</li>
<li>思维链（CoT）提示是一种实用的解决方案来应对LLMs的局限性。</li>
<li>当前的CoT方法采用“一刀切”策略，使用固定模板，限制了模型在识别有效推理路径时的能力。</li>
<li>提示空间和答案空间之间的复杂性及相互作用对LLMs的理论计算能力有影响。</li>
<li>依赖单一通用提示可能降低LLMs的性能。</li>
<li>提示的复杂性对答案空间的结构和有效性有直接影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-06474bf1a99095898988bd9807e7d005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9417b8a2ab68aedd2dc547c18494ee3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eea6c9e27b0f30d8f3767244462d7452.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a6f27f5b8982096845ce10e3ccd106e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="How-Do-Multimodal-Large-Language-Models-Handle-Complex-Multimodal-Reasoning-Placing-Them-in-An-Extensible-Escape-Game"><a href="#How-Do-Multimodal-Large-Language-Models-Handle-Complex-Multimodal-Reasoning-Placing-Them-in-An-Extensible-Escape-Game" class="headerlink" title="How Do Multimodal Large Language Models Handle Complex Multimodal   Reasoning? Placing Them in An Extensible Escape Game"></a>How Do Multimodal Large Language Models Handle Complex Multimodal   Reasoning? Placing Them in An Extensible Escape Game</h2><p><strong>Authors:Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu</strong></p>
<p>The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）的快速发展激发了现实和虚拟环境中复杂多模态推理任务的兴趣。这些任务需要协调多种能力，包括视觉感知、视觉推理、空间意识和目标推断。然而，现有的评估主要侧重于最终任务完成，经常将评估简化为孤立的技能，如视觉接地和视觉问答。对于在多模态环境中全面和定量地分析推理过程，人们给予的关注度不够。这对于理解模型行为和超越任务成功的底层推理机制至关重要。为了解决这个问题，我们引入了MM-Escape，这是一个可扩展的基准测试，用于研究多模态推理，其灵感来自于现实世界的逃脱游戏。MM-Escape除了最终的任务完成外，还强调模型中间的行为。为了实现这一点，我们开发了EscapeCraft，这是一个可定制和开放的环境，让模型能够进行自由形式的探索，以评估多模态推理。大量实验表明，无论规模大小，MLLMs都可以成功完成最简单的房间逃脱任务，其中一些表现出类似人类的探索策略。然而，随着任务难度的增加，性能急剧下降。此外，我们观察到不同模型的性能瓶颈各不相同，揭示了其多模态推理能力的不同失败模式和局限性，例如重复轨迹缺乏自适应探索、因视觉空间意识差而陷入角落、以及无效地使用获得的道具（如钥匙）。我们希望我们的工作能揭示多模态推理的新挑战，并揭示MLLMs能力的潜在改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10042v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型（MLLMs）的快速进步激发了现实世界和虚拟环境中复杂多模态推理任务的兴趣。然而，现有评估主要关注任务完成的最终效果，忽视了多模态环境中推理过程的综合定量分析，这对于理解模型行为和推理机制至关重要。为此，我们引入了MM-Escape，一个用于研究多模态推理的可扩展基准测试，并开发了EscapeCraft，一个可定制的开源环境，以评估模型在自由形式探索中的多模态推理能力。实验表明，不同规模的多模态语言模型可以在最简单的房间逃生任务中成功完成任务，但面对难度更大的任务时性能会大幅下降。此外，我们还观察到不同模型之间存在性能瓶颈，揭示了其在多模态推理能力方面的不同失败模式和局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的进步推动了复杂多模态推理任务的发展。</li>
<li>现有评估主要关注任务完成效果，忽视了多模态推理过程的综合分析。</li>
<li>MM-Escape是一个用于研究多模态推理的基准测试，强调模型在任务完成过程中的中间行为。</li>
<li>EscapeCraft是一个可定制的开源环境，用于评估模型在自由形式探索中的多模态推理能力。</li>
<li>MLLMs可以在简单的房间逃生任务中成功完成任务，但难度增加时性能显著下降。</li>
<li>不同模型在多模态推理能力方面存在不同的失败模式和局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-daeaff98b34d2c57cdbb20a7769bf2e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d0e3122dcd039aa2412e1bb9248d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d48a21fa7a775def42bdcb65ef71824.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb3e929bd991c3a2373f2622b765ce82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f65fe55be11d251101f7f1bf9c9617e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="OR-LLM-Agent-Automating-Modeling-and-Solving-of-Operations-Research-Optimization-Problem-with-Reasoning-Large-Language-Model"><a href="#OR-LLM-Agent-Automating-Modeling-and-Solving-of-Operations-Research-Optimization-Problem-with-Reasoning-Large-Language-Model" class="headerlink" title="OR-LLM-Agent: Automating Modeling and Solving of Operations Research   Optimization Problem with Reasoning Large Language Model"></a>OR-LLM-Agent: Automating Modeling and Solving of Operations Research   Optimization Problem with Reasoning Large Language Model</h2><p><strong>Authors:Bowen Zhang, Pengcheng Luo</strong></p>
<p>Operations Research (OR) has been widely applied in various fields such as resource allocation, production planning, and supply chain management. However, addressing real-world OR problems requires OR experts to perform mathematical modeling and programmers to develop solution algorithms. This traditional method, heavily reliant on experts, is costly and has long development cycles, severely limiting the widespread adoption of OR techniques. Few have considered using Artificial Intelligence (AI) to replace professionals to achieve fully automated solutions for OR problems. We propose OR-LLM-Agent, the first AI agent that enables end-to-end automation for solving real-world OR problems. OR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of Large Language Models (LLMs) to translate natural language problem descriptions into formal mathematical models and automatically generate Gurobi solver code. In OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair within a sandbox environment, facilitating the derivation of the final solution. Due to the lack of dedicated benchmark datasets for evaluating the automated solving of OR problems, we construct a benchmark dataset comprising 83 real-world OR problems described in natural language. We conduct comparative experiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini, DeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the highest pass rate of 100% and the highest solution accuracy of 85%, demonstrating the feasibility of automated OR problem-solving. Data and code have been publicly available at <a target="_blank" rel="noopener" href="https://github.com/bwz96sco/or_llm_agent">https://github.com/bwz96sco/or_llm_agent</a>. </p>
<blockquote>
<p>运筹学（OR）已广泛应用于资源分配、生产计划和供应链管理等各个领域。然而，解决现实世界的运筹学问题，需要运筹学专家进行数学建模和程序员开发解决方案算法。这种传统的方法，严重依赖专家，成本高昂且开发周期长，严重限制了运筹学技术的广泛应用。很少有人考虑使用人工智能（AI）来替代专业人员，以实现运筹学问题的全自动解决方案。我们提出OR-LLM-Agent，这是第一个能够实现解决现实世界运筹学问题的端到端自动化的AI代理。OR-LLM-Agent利用大型语言模型（LLM）的链式思维（CoT）推理能力，将自然语言问题描述翻译成正式的数学模型，并自动生成Gurobi求解器代码。在OR-LLM-Agent中，OR-CodeAgent旨在自动化代码执行和修复，在沙箱环境中进行，便于得出最终解决方案。由于缺乏用于评估运筹学问题自动化求解的专用基准数据集，我们构建了一个包含83个用自然语言描述的现实世界运筹学问题的基准数据集。我们与最新技术（SOTA）推理LLM进行了比较实验，包括GPT-o3-mini、DeepSeek-R1和Gemini 2.0 Flash Thinking。OR-LLM-Agent的通过率达到了最高的100%，解决方案的准确性也达到了最高的85%，证明了自动化解决运筹学问题的可行性。数据和代码已公开在<a target="_blank" rel="noopener" href="https://github.com/bwz96sco/or_llm_agent">https://github.com/bwz96sco/or_llm_agent</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10009v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>摘要</strong></p>
<p>本文介绍了操作研究（OR）在资源分配、生产规划和供应链管理等领域的广泛应用。然而，解决现实世界中的OR问题传统上需要依赖专家进行数学建模和程序员开发解决方案算法，这种方法成本高且开发周期长，限制了OR技术的广泛应用。本文提出使用人工智能（AI）代替专业人士，实现OR问题的全自动解决方案。我们提出了OR-LLM-Agent，这是一种能够实现端到端自动化解决现实世界中OR问题的人工智能代理。OR-LLM-Agent利用大型语言模型（LLM）的链式思维（CoT）推理能力，将自然语言问题描述翻译成正式的数学模型，并自动生成Gurobi求解器代码。在OR-LLM-Agent中，设计了OR-CodeAgent在沙箱环境中自动化代码执行和修复，便于得出最终解决方案。由于缺乏专门的基准数据集来评估OR问题的自动化解决程度，我们构建了一个包含83个用自然语言描述的现实生活中OR问题的基准数据集。我们对包括GPT-o3-mini、DeepSeek-R1和Gemini 2.0 Flash Thinking在内的最新推理LLM进行了比较实验。OR-LLM-Agent取得了100%的通过率，解决方案准确性达到85%，证明了自动化解决OR问题的可行性。数据和代码已公开在<a target="_blank" rel="noopener" href="https://github.com/bwz96sco/or_llm_agent">https://github.com/bwz96sco/or_llm_agent</a>。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>OR问题在传统解决方式上依赖专家进行数学建模和程序员开发，过程成本高且周期长。</li>
<li>提出使用AI实现OR问题的全自动解决方案，引入OR-LLM-Agent实现端到端自动化。</li>
<li>OR-LLM-Agent利用LLM的CoT推理能力，将自然语言转化为数学模odel，自动生成求解器代码。</li>
<li>介绍了OR-CodeAgent的设计，用于在沙箱环境中自动化代码执行和修复。</li>
<li>缺乏专门评估OR问题自动化解决的基准数据集，因此构建了包含83个现实问题的基准数据集。</li>
<li>OR-LLM-Agent在比较实验中表现出优异的性能，通过率高达100%，解决方案准确性达85%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10009">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3c1b8aa8cb84670c130c056fb1da7158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0a518baba771874a1a1ec42cf7ced03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43e450a269853824a515ccd20e89f3a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138b2448edab9536b2f7b595f975c057.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ExtremeAIGC-Benchmarking-LMM-Vulnerability-to-AI-Generated-Extremist-Content"><a href="#ExtremeAIGC-Benchmarking-LMM-Vulnerability-to-AI-Generated-Extremist-Content" class="headerlink" title="ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist   Content"></a>ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist   Content</h2><p><strong>Authors:Bhavik Chandna, Mariam Aboujenane, Usman Naseem</strong></p>
<p>Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated extremist content, including photorealistic images and text, which can be used to bypass safety mechanisms and generate harmful outputs. However, existing datasets for evaluating LMM robustness offer limited exploration of extremist content, often lacking AI-generated images, diverse image generation models, and comprehensive coverage of historical events, which hinders a complete assessment of model vulnerabilities. To fill this gap, we introduce ExtremeAIGC, a benchmark dataset and evaluation framework designed to assess LMM vulnerabilities against such content. ExtremeAIGC simulates real-world events and malicious use cases by curating diverse text- and image-based examples crafted using state-of-the-art image generation techniques. Our study reveals alarming weaknesses in LMMs, demonstrating that even cutting-edge safety measures fail to prevent the generation of extremist material. We systematically quantify the success rates of various attack strategies, exposing critical gaps in current defenses and emphasizing the need for more robust mitigation strategies. </p>
<blockquote>
<p>大型多模态模型（LMM）越来越容易受到AI生成的极端内容的影响，包括逼真的图像和文本，这些内容可用于绕过安全机制并产生有害输出。然而，现有的用于评估LMM稳健性的数据集在探索极端内容方面提供了有限的研究，往往缺乏AI生成的图像、多样化的图像生成模型以及对历史事件的综合覆盖，这阻碍了模型漏洞的全面评估。为了填补这一空白，我们引入了ExtremeAIGC，这是一个用于评估LMM对此类内容漏洞的基准数据集和评估框架。ExtremeAIGC通过收集使用最新图像生成技术制作的多样化文本和图像示例，模拟现实世界的事件和恶意用例。我们的研究发现大型多模态模型存在令人担忧的弱点，表明即使是最前沿的安全措施也无法防止极端材料的生成。我们系统地量化了各种攻击策略的成功率，暴露了当前防御手段中的关键差距，并强调需要更稳健的缓解策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09964v1">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>大型多模态模型（LMMs）易受AI生成极端内容的影响，包括逼真的图像和文字。现有评估LMM稳健性的数据集对极端内容的探索有限，缺乏AI生成的图像、多样的图像生成模型和历史的全面覆盖，这阻碍了模型漏洞的完全评估。为解决此问题，我们推出了ExtremeAIGC，一个用于评估LMM对此类内容脆弱性的基准数据集和评估框架。ExtremeAIGC通过策划使用最新图像生成技术制作的多样文本和图像样本，模拟现实世界事件和恶意用例。我们的研究揭示了LMM惊人的弱点，证明即使是最先进的安全措施也无法防止极端材料的生成。我们系统地量化了各种攻击策略的成功率，突显了当前防御措施中的关键漏洞，并强调了需要更稳健的缓解策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型（LMMs）易受AI生成的极端内容影响，包括逼真的图像和文字。</li>
<li>现有数据集在评估LMM对极端内容的稳健性方面存在局限性，缺乏AI生成的图像和全面的历史事件覆盖。</li>
<li>ExtremeAIGC基准数据集和评估框架用于评估LMM对极端内容的脆弱性。</li>
<li>ExtremeAIGC模拟现实事件和恶意用例，通过多样文本和图像样本展示攻击策略。</li>
<li>研究发现LMM存在惊人弱点，即使安全措施也无法完全防止生成极端材料。</li>
<li>系统量化各种攻击策略的成功率，突显当前防御措施的关键漏洞。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09964">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-baf4024b7acf0d4212f27ca0d1476bf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e35689842fa67dabd9b37625b8ac3408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61b4d304e098de40b55811fed663062.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-568dea96fb5523786b96df9801c2ad13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-279649971b9375c0c25a59c15d216a7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db3ac16b10734ff50cfcfaa33a01d894.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15a65b6a92605d8da9cffd5f41b01ef8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Conversational-Gold-Evaluating-Personalized-Conversational-Search-System-using-Gold-Nuggets"><a href="#Conversational-Gold-Evaluating-Personalized-Conversational-Search-System-using-Gold-Nuggets" class="headerlink" title="Conversational Gold: Evaluating Personalized Conversational Search   System using Gold Nuggets"></a>Conversational Gold: Evaluating Personalized Conversational Search   System using Gold Nuggets</h2><p><strong>Authors:Zahra Abbasiantaeb, Simon Lupart, Leif Azzopardi, Jeffery Dalton, Mohammad Aliannejadi</strong></p>
<p>The rise of personalized conversational search systems has been driven by advancements in Large Language Models (LLMs), enabling these systems to retrieve and generate answers for complex information needs. However, the automatic evaluation of responses generated by Retrieval Augmented Generation (RAG) systems remains an understudied challenge. In this paper, we introduce a new resource for assessing the retrieval effectiveness and relevance of response generated by RAG systems, using a nugget-based evaluation framework. Built upon the foundation of TREC iKAT 2023, our dataset extends to the TREC iKAT 2024 collection, which includes 17 conversations and 20,575 relevance passage assessments, together with 2,279 extracted gold nuggets, and 62 manually written gold answers from NIST assessors. While maintaining the core structure of its predecessor, this new collection enables a deeper exploration of generation tasks in conversational settings. Key improvements in iKAT 2024 include: (1) &#96;&#96;gold nuggets’’ – concise, essential pieces of information extracted from relevant passages of the collection – which serve as a foundation for automatic response evaluation; (2) manually written answers to provide a gold standard for response evaluation; (3) unanswerable questions to evaluate model hallucination; (4) expanded user personas, providing richer contextual grounding; and (5) a transition from Personal Text Knowledge Base (PTKB) ranking to PTKB classification and selection. Built on this resource, we provide a framework for long-form answer generation evaluation, involving nuggets extraction and nuggets matching, linked to retrieval. This establishes a solid resource for advancing research in personalized conversational search and long-form answer generation. Our resources are publicly available at <a target="_blank" rel="noopener" href="https://github.com/irlabamsterdam/CONE-RAG">https://github.com/irlabamsterdam/CONE-RAG</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）的进步，个性化对话搜索系统的崛起使得这些系统能够检索和生成针对复杂信息需求的答案。然而，由检索增强生成（RAG）系统生成的响应的自动评估仍然是一个被忽视的挑战。在本文中，我们介绍了一个使用基于片段的评估框架来评估RAG系统生成的检索有效性和响应相关性的新资源。我们的数据集建立在TREC iKAT 2023的基础上，扩展到TREC iKAT 2024集合，其中包括17个对话和20,575个相关段落评估，以及从相关段落中提取的2,279个黄金片段和NIST评估人员编写的62个手动黄金答案。在保持其前身的核心结构的同时，这个新集合使我们对对话环境中的生成任务进行了更深入的探索。iKAT 2024的关键改进包括：（1）“黄金片段”——从集合的相关段落中提取的简洁、必要的信息片段——作为自动响应评估的基础；（2）手动编写的答案，为响应评估提供黄金标准；（3）无法回答的问题以评估模型的幻觉；（4）扩展的用户角色，提供更丰富的上下文背景；（5）从个人文本知识库（PTKB）排名过渡到PTKB分类和选择。基于这一资源，我们提供了一个框架，用于长形式答案生成评估，涉及片段提取和与检索相关的片段匹配。这为个性化对话搜索和长形式答案生成的研究进步建立了坚实的资源。我们的资源在<a target="_blank" rel="noopener" href="https://github.com/irlabamsterdam/CONE-RAG%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/irlabamsterdam/CONE-RAG公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09902v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于大型语言模型（LLMs）的个性化对话搜索系统的进步，并指出评估这些系统生成的响应的检索效果和相关性仍是一个挑战。为此，本文引入了一个新的评估资源，使用基于“精粹”的评估框架来评估RAG系统生成的响应的检索效果。该数据集包括TREC iKAT 2024集合，含有17个对话和超过两万条相关段落评估，以及从相关段落中提取的金精粹和NIST评估者撰写的金标准答案。该数据集的关键改进包括金精粹、手动编写的答案、无法回答的问题、扩展的用户角色以及从个人文本知识库（PTKB）排名到PTKB分类和选择的转变。这些资源为推进个性化对话搜索和长形式答案生成的研究提供了坚实的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）的进展推动了个性化对话搜索系统的崛起。</li>
<li>评估RAG系统生成的响应的检索效果和相关性是一个未充分研究的挑战。</li>
<li>引入了新的资源，使用基于“精粹”的评估框架来评估RAG系统的响应。</li>
<li>TREC iKAT 2024集合包含17个对话、超过两万条相关段落评估。</li>
<li>数据集包括从相关段落中提取的金精粹和手动编写的金标准答案。</li>
<li>数据集的关键改进包括金精粹、无法回答的问题、扩展的用户角色等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09902">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-82c31d2fbef2f5518dfad07c9f94dd73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a722c3c166c8f3b6c68c7a580ec710b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1735ad0124fe15e51a511dfaf987a88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e094ee83bc9afe784e2fff6946ae91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d9971e8ceed62c72775c019e062e961.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Local-Look-Ahead-Guidance-via-Verifier-in-the-Loop-for-Automated-Theorem-Proving"><a href="#Local-Look-Ahead-Guidance-via-Verifier-in-the-Loop-for-Automated-Theorem-Proving" class="headerlink" title="Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem   Proving"></a>Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem   Proving</h2><p><strong>Authors:Sara Rajaee, Kumar Pratik, Gabriele Cesa, Arash Behboodi</strong></p>
<p>The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the model, even for the step-wise rewards, or large quantities of human annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods. In this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model’s reasoning accuracy and efficiency. </p>
<blockquote>
<p>近期最有前景的人工智能推理方法需要应用强化学习（RL）的变种，要么在模型展开的轨迹上，甚至对于逐步奖励，要么需要大量的人工注释轨迹数据。对展开轨迹的依赖使得计算成本和时间非常高昂。特别地，推理轨迹的正确性通常只能在完成时进行判断，这导致强化学习中的奖励稀疏，或需要昂贵的合成数据生成，类似于专家迭代的方法。在这项工作中，我们关注自动定理证明（ATP）任务，并提出了一种新型的在循环验证器设计，它不同于那些依赖整个推理轨迹反馈的现有方法，而是采用自动化验证器在推理过程的每一步提供中间反馈。使用Lean作为验证器，我们从实证上证明，逐步的局部验证提高了模型推理的准确性和效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09730v1">PDF</a> Accepted at ICLR 2025 Workshop on Reasoning and Planning for Large   Language Models</p>
<p><strong>Summary</strong>：<br>最近的人工智能推理方法主要依赖于强化学习（RL）在各种模型轨迹上的应用，包括逐步奖励和大量的人类注释轨迹数据。然而，依赖于展开的轨迹使得计算成本和时间变得非常高昂。本工作专注于自动定理证明（ATP）任务，提出了一种新颖的循环验证器设计，与传统的对整个推理轨迹进行反馈的方法不同，该设计采用自动化验证器在推理过程的每一步提供中间反馈。使用Lean作为验证器，实证表明，逐步的局部验证提高了模型的推理准确性和效率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>人工智能推理方法主要依赖强化学习在各种模型轨迹上的应用。</li>
<li>对展开的推理轨迹的依赖使得计算成本和时间变得非常高昂。</li>
<li>当前工作提出了一个新颖的循环验证器设计，该设计在推理过程的每一步提供中间反馈。</li>
<li>与传统方法不同，新方法采用Lean作为验证器进行逐步的局部验证。</li>
<li>实证研究表明，逐步的局部验证有助于提高模型的推理准确性。</li>
<li>这种方法还提高了推理效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-88acfe342d72ab720227ba929c7a03e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5340a0ed5872d3f8a313a04a566a5474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8d71bd76950a0062b6e3b6de1b534b3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Reasoning-with-LLMs-for-k-anonymity-Estimation"><a href="#Probabilistic-Reasoning-with-LLMs-for-k-anonymity-Estimation" class="headerlink" title="Probabilistic Reasoning with LLMs for k-anonymity Estimation"></a>Probabilistic Reasoning with LLMs for k-anonymity Estimation</h2><p><strong>Authors:Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu</strong></p>
<p>Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a novel numerical reasoning task under uncertainty, focusing on estimating the k-anonymity of user-generated documents containing privacy-sensitive information. We propose BRANCH, which uses LLMs to factorize a joint probability distribution to estimate the k-value-the size of the population matching the given information-by modeling individual pieces of textual information as random variables. The probability of each factor occurring within a population is estimated using standalone LLMs or retrieval-augmented generation systems, and these probabilities are combined into a final k-value. Our experiments show that this method successfully estimates the correct k-value 67% of the time, an 11% increase compared to GPT-4o chain-of-thought reasoning. Additionally, we leverage LLM uncertainty to develop prediction intervals for k-anonymity, which include the correct value in nearly 92% of cases. </p>
<blockquote>
<p>概率推理是人类和人工智能中的关键方面，能够处理决策中的不确定性和模糊性。在本文中，我们介绍了一种新型的概率推理任务，重点估计包含隐私敏感信息的用户生成文档的k匿名度。我们提出了BRANCH方法，它使用大型语言模型对联合概率分布进行分解估计k值（与给定信息匹配的人口数量），并将个别文本信息片段作为随机变量进行建模。通过使用独立的大型语言模型或检索增强生成系统来估计每个因素在人口中的出现概率，并将这些概率合并到最终的k值中。我们的实验表明，该方法成功估计出正确的k值的概率达到67%，比GPT-4的深度思考推理提高了11%。此外，我们还利用大型语言模型的不确定性为k匿名性开发预测区间，在接近92%的情况下包含正确的值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09674v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型数值推理任务——不确定性下的k-匿名性估算。文章重点讨论如何估计包含隐私敏感信息的用户生成文档的k-匿名性。为此，提出了基于大型语言模型（LLM）的方法BRANCH，它通过分解联合概率分布来估算k值。实验表明，该方法成功估算k值的准确率为67%，相较于GPT-4链式思维推理提升了11%。此外，文章还利用LLM的不确定性为k-匿名性开发了预测区间，其包含正确值的比例接近92%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了在不确定性下处理k-匿名性估算的新型数值推理任务。</li>
<li>BRANCH方法利用大型语言模型（LLM）来估算k-匿名性。</li>
<li>BRANCH通过分解联合概率分布来估算k值。</li>
<li>BRANCH方法的实验显示，其成功估算k值的准确率为67%。</li>
<li>与GPT-4链式思维推理相比，BRANCH方法的准确率提升了11%。</li>
<li>利用LLM的不确定性为k-匿名性开发预测区间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09674">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1ca164e42c0f24fbee4145ea74e89b85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd8ec33c90e919b58aa39326dfebcdbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-193068e3afaa29a43b380768c3c49888.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Reasoning-In-The-Wild-Is-Not-Always-Faithful"><a href="#Chain-of-Thought-Reasoning-In-The-Wild-Is-Not-Always-Faithful" class="headerlink" title="Chain-of-Thought Reasoning In The Wild Is Not Always Faithful"></a>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</h2><p><strong>Authors:Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy</strong></p>
<p>Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal non-negligible rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and ChatGPT-4o (7.0%) all answer a notable proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (“implicit post-hoc rationalization”). For example, when separately presented with the questions “Is X bigger than Y?” and “Is Y bigger than X?”, models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior. </p>
<blockquote>
<p>“链式思维（Chain-of-Thought，简称CoT）推理在人工智能领域取得了显著的进步。然而，最近的研究表明，CoT推理并不总是可靠的，即CoT推理并不总是反映模型如何得出结论。迄今为止，大多数研究主要集中在非自然环境下的不忠实情况，即引入明确偏见的环境。与此相反，我们证明了在真实的提示且没有人为偏见的情况下也会出现不忠实的CoT。我们的结果揭示了前沿模型中不可忽略的多种不忠实推理形式的发生率：Sonnet 3.7（16.3%）、DeepSeek R1（5.3%）和ChatGPT-4o（7.0%）都会回答相当比例的问题对时不忠实。具体来说，我们发现模型在回答二元问题时（如“X是否大于Y？”和“Y是否大于X？”），会为自己的答案进行事后合理化。有时模型会提供看似合理的论据来证明两个问题的答案都是“是”或都是“否”，尽管这样的回答在逻辑上是矛盾的。我们还研究了模型在推理过程中出现的修复错误和不忠实的简化策略，即在解决普特南问题（一个困难的基准测试）时，模型采用明显不合逻辑的推理来简化问题。我们的研究对依赖于监控CoT来检测不期望行为的AI安全工作提出了挑战。”</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08679v2">PDF</a> Accepted to the Reasoning and Planning for Large Language Models   Workshop (ICLR 25), 10 main paper pages, 38 appendix pages</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Chain-of-Thought（CoT）推理在先进AI能力中的重要作用，但研究发现CoT推理并不总是忠实于模型的决策过程。过去的研究主要集中在非自然情境下的不忠实现象，而本文则展示了在现实提示下也会出现不忠实的CoT。研究结果显示，前沿模型如Sonnet 3.7、DeepSeek R1和ChatGPT-4o存在不可忽略的不忠实推理现象。模型会在回答二元问题时理性化其隐含偏见（隐性事后合理化），如回答两个逻辑矛盾的问题时，会提供看似合理的论证。此外，还探讨了模型在推理中的修复错误和不忠实的简化策略所带来的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Thought（CoT）推理对先进AI能力有重要作用，但存在不忠实现象。</li>
<li>不忠实CoT可出现在现实提示中，而非仅限于非自然情境。</li>
<li>多个前沿模型存在不忠实推理现象，如Sonnet 3.7、DeepSeek R1和ChatGPT-4o。</li>
<li>模型会在回答二元问题时理性化其隐含偏见。</li>
<li>模型会出现修复错误的情况，即默默地纠正推理中的错误。</li>
<li>模型存在不忠实的简化策略，如在解决Putnam问题时使用明显不合逻辑的推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-325abc4bd6f1c5b0fd43206db87f89f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32bd02d5ebfb07512374011441ace7d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49a797cea6090c18b6e99a5e13dae10c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59dcdced32d25f8d45c1dff7d4db0c86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c222adeb7e9c295431cddb07692fc8bd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-15  HybridVLA Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-14/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cc60871f3eeaf18a8dbd47e44d298385.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-03-14  PersonaBooth Personalized Text-to-Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19211.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
