<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  Whisper Speaker Identification Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a0c2b353825760f828655d046cd905f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    30 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-15-æ›´æ–°"><a href="#2025-03-15-æ›´æ–°" class="headerlink" title="2025-03-15 æ›´æ–°"></a>2025-03-15 æ›´æ–°</h1><h2 id="Whisper-Speaker-Identification-Leveraging-Pre-Trained-Multilingual-Transformers-for-Robust-Speaker-Embeddings"><a href="#Whisper-Speaker-Identification-Leveraging-Pre-Trained-Multilingual-Transformers-for-Robust-Speaker-Embeddings" class="headerlink" title="Whisper Speaker Identification: Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings"></a>Whisper Speaker Identification: Leveraging Pre-Trained Multilingual   Transformers for Robust Speaker Embeddings</h2><p><strong>Authors:Jakaria Islam Emon, Md Abu Salek, Kazi Tamanna Alam</strong></p>
<p>Speaker identification in multilingual settings presents unique challenges, particularly when conventional models are predominantly trained on English data. In this paper, we propose WSI (Whisper Speaker Identification), a framework that repurposes the encoder of the Whisper automatic speech recognition model pre trained on extensive multilingual data to generate robust speaker embeddings via a joint loss optimization strategy that leverages online hard triplet mining and self supervised Normalized Temperature-scaled Cross Entropy loss. By capitalizing on Whisper language-agnostic acoustic representations, our approach effectively distinguishes speakers across diverse languages and recording conditions. Extensive evaluations on multiple corpora, including VoxTube (multilingual), JVS (Japanese), CallHome (German, Spanish, Chinese, and Japanese), and Voxconverse (English), demonstrate that WSI consistently outperforms state-of-the-art baselines, namely Pyannote Embedding, ECAPA TDNN, and Xvector, in terms of lower equal error rates and higher AUC scores. These results validate our hypothesis that a multilingual pre-trained ASR encoder, combined with joint loss optimization, substantially improves speaker identification performance in non-English languages. </p>
<blockquote>
<p>åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­è¿›è¡Œè¯´è¯äººè¯†åˆ«é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ä¸»è¦åŸºäºè‹±è¯­æ•°æ®è®­ç»ƒçš„å¸¸è§„æ¨¡å‹æ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WSIï¼ˆWhisperè¯´è¯äººè¯†åˆ«ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¼–ç å™¨è¿›è¡Œæ”¹é€ çš„æ¡†æ¶ï¼Œè¯¥ç¼–ç å™¨åœ¨å¤§é‡å¤šè¯­è¨€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé€šè¿‡è”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ç”Ÿæˆç¨³å¥çš„è¯´è¯äººåµŒå…¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨åœ¨çº¿ç¡¬ä¸‰å…ƒç»„æŒ–æ˜å’Œè‡ªç›‘ç£çš„å½’ä¸€åŒ–æ¸©åº¦æ¯”ä¾‹äº¤å‰ç†µæŸå¤±ã€‚é€šè¿‡åˆ©ç”¨Whisperçš„è¯­è¨€æ— å…³å£°éŸ³è¡¨ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°åŒºåˆ†ä¸åŒè¯­è¨€å’Œå½•åˆ¶æ¡ä»¶ä¸‹çš„è¯´è¯äººã€‚åœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬VoxTubeï¼ˆå¤šè¯­è¨€ï¼‰ã€JVSï¼ˆæ—¥è¯­ï¼‰ã€CallHomeï¼ˆå¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­ï¼‰å’ŒVoxconverseï¼ˆè‹±è¯­ï¼‰ï¼Œè¯æ˜WSIåœ¨å¹³ç­‰é”™è¯¯ç‡å’ŒAUCåˆ†æ•°æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œå³PyannoteåµŒå…¥ã€ECAPA TDNNå’ŒXvectorã€‚è¿™äº›ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„å‡è®¾ï¼Œå³å¤šè¯­è¨€é¢„è®­ç»ƒçš„ASRç¼–ç å™¨ä¸è”åˆæŸå¤±ä¼˜åŒ–ç›¸ç»“åˆï¼Œå¯ä»¥å¤§å¤§æé«˜éè‹±è¯­è¯­è¨€çš„è¯´è¯äººè¯†åˆ«æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10446v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†WSIï¼ˆWhisperè¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šè¯­ç§ç¯å¢ƒä¸‹çš„è¯´è¯äººè¯†åˆ«ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒäºå¤šè¯­ç§æ•°æ®çš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¼–ç å™¨ç”Ÿæˆç¨³å¥çš„è¯´è¯äººåµŒå…¥ï¼Œé€šè¿‡è”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ï¼Œç»“åˆåœ¨çº¿ç¡¬ä¸‰å…ƒç»„æŒ–æ˜å’Œè‡ªæˆ‘ç›‘ç£çš„å½’ä¸€åŒ–æ¸©åº¦å°ºåº¦äº¤å‰ç†µæŸå¤±ï¼Œæœ‰æ•ˆåŒºåˆ†ä¸åŒè¯­è¨€å’Œå½•éŸ³æ¡ä»¶ä¸‹çš„è¯´è¯äººã€‚åœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒWSIåœ¨å¹³ç­‰é”™è¯¯ç‡å’ŒAUCå¾—åˆ†æ–¹é¢å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¦‚PyannoteåµŒå…¥ã€ECAPA TDNNå’ŒXvectorç­‰æœ€æ–°æŠ€æœ¯ã€‚è¿™éªŒè¯äº†æœ¬æ–‡çš„å‡è®¾ï¼šç»“åˆå¤šè¯­ç§é¢„è®­ç»ƒçš„ASRç¼–ç å™¨å’Œè”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜éè‹±è¯­è¯­è¨€çš„è¯´è¯äººè¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>WSIæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¼–ç å™¨è¿›è¡Œè¯´è¯äººè¯†åˆ«ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šè¯­ç§ç¯å¢ƒä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½æœ‰æ•ˆåŒºåˆ†ä¸åŒè¯­è¨€å’Œå½•éŸ³æ¡ä»¶ä¸‹çš„è¯´è¯äººã€‚</li>
<li>WSIé‡‡ç”¨è”åˆæŸå¤±ä¼˜åŒ–ç­–ç•¥ï¼ŒåŒ…æ‹¬åœ¨çº¿ç¡¬ä¸‰å…ƒç»„æŒ–æ˜å’Œè‡ªæˆ‘ç›‘ç£çš„å½’ä¸€åŒ–æ¸©åº¦å°ºåº¦äº¤å‰ç†µæŸå¤±ã€‚</li>
<li>ç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ¨¡å‹ï¼ŒWSIåœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šçš„è¯„ä¼°ç»“æœè¡¨ç°æ›´ä½³ï¼Œä½“ç°åœ¨æ›´ä½çš„å¹³ç­‰é”™è¯¯ç‡å’Œæ›´é«˜çš„AUCå¾—åˆ†ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0bdb47d4309b81002513c45e2941c27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5134bc6b613250bc8e8cec2b7b35f596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec54433401449c1327c5b908bb83f91f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47edbaddccb1b3c2956900ecb1d2eac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-215ea2545d9f8c9a6d5a0b39e6595fa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd5f3c30e7e27880af551dd55aa3719.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HSEmotion-Team-at-ABAW-8-Competition-Audiovisual-Ambivalence-Hesitancy-Emotional-Mimicry-Intensity-and-Facial-Expression-Recognition"><a href="#HSEmotion-Team-at-ABAW-8-Competition-Audiovisual-Ambivalence-Hesitancy-Emotional-Mimicry-Intensity-and-Facial-Expression-Recognition" class="headerlink" title="HSEmotion Team at ABAW-8 Competition: Audiovisual Ambivalence&#x2F;Hesitancy,   Emotional Mimicry Intensity and Facial Expression Recognition"></a>HSEmotion Team at ABAW-8 Competition: Audiovisual Ambivalence&#x2F;Hesitancy,   Emotional Mimicry Intensity and Facial Expression Recognition</h2><p><strong>Authors:Andrey V. Savchenko</strong></p>
<p>This article presents our results for the eighth Affective Behavior Analysis in-the-Wild (ABAW) competition. We combine facial emotional descriptors extracted by pre-trained models, namely, our EmotiEffLib library, with acoustic features and embeddings of texts recognized from speech. The frame-level features are aggregated and fed into simple classifiers, e.g., multi-layered perceptron (feed-forward neural network with one hidden layer), to predict ambivalence&#x2F;hesitancy and facial expressions. In the latter case, we also use the pre-trained facial expression recognition model to select high-score video frames and prevent their processing with a domain-specific video classifier. The video-level prediction of emotional mimicry intensity is implemented by simply aggregating frame-level features and training a multi-layered perceptron. Experimental results for three tasks from the ABAW challenge demonstrate that our approach significantly increases validation metrics compared to existing baselines. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨ç¬¬å…«å±Šé‡å¤–æƒ…æ„Ÿè¡Œä¸ºåˆ†æï¼ˆABAWï¼‰ç«èµ›çš„ç»“æœã€‚æˆ‘ä»¬å°†é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æå–çš„é¢éƒ¨æƒ…æ„Ÿæè¿°ç¬¦ï¼ˆå³æˆ‘ä»¬çš„EmotiEffLibåº“ï¼‰ä¸ä»è¯­éŸ³ä¸­è¯†åˆ«çš„æ–‡æœ¬çš„å£°éŸ³ç‰¹å¾å’ŒåµŒå…¥ç›¸ç»“åˆã€‚å¸§çº§ç‰¹å¾è¢«èšåˆå¹¶è¾“å…¥åˆ°ç®€å•çš„åˆ†ç±»å™¨ä¸­ï¼Œä¾‹å¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆå…·æœ‰ä¸€å±‚éšè—å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼‰ï¼Œä»¥é¢„æµ‹çŸ›ç›¾å¿ƒç†&#x2F;çŠ¹è±«å’Œé¢éƒ¨è¡¨æƒ…ã€‚åœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨é¢„è®­ç»ƒçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ¨¡å‹æ¥é€‰æ‹©é«˜åˆ†æ•°è§†é¢‘å¸§ï¼Œå¹¶é˜²æ­¢å®ƒä»¬ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„è§†é¢‘åˆ†ç±»å™¨è¿›è¡Œå¤„ç†ã€‚é€šè¿‡ç®€å•åœ°èšåˆå¸§çº§ç‰¹å¾å¹¶è®­ç»ƒå¤šå±‚æ„ŸçŸ¥å™¨æ¥å®ç°æƒ…æ„Ÿæ¨¡ä»¿å¼ºåº¦çš„è§†é¢‘çº§é¢„æµ‹ã€‚æ¥è‡ªABAWæŒ‘æˆ˜çš„ä¸‰ä¸ªä»»åŠ¡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜éªŒè¯æŒ‡æ ‡æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10399v1">PDF</a> submitted to ABAW CVPR 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç¬¬å…«å±Šæƒ…æ„Ÿè¡Œä¸ºåˆ†æç«èµ›ï¼ˆABAWï¼‰ä¸­çš„ç ”ç©¶ç»“æœã€‚è¯¥ç ”ç©¶ç»“åˆäº†é¢„è®­ç»ƒæ¨¡å‹æå–çš„é¢éƒ¨æƒ…æ„Ÿæè¿°ç¬¦ã€å£°å­¦ç‰¹å¾å’Œä»è¯­éŸ³ä¸­è¯†åˆ«çš„æ–‡æœ¬åµŒå…¥ã€‚æ¡†æ¶çº§åˆ«çš„ç‰¹å¾è¢«èšåˆå¹¶è¾“å…¥åˆ°ç®€å•çš„åˆ†ç±»å™¨ä¸­ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä»¥é¢„æµ‹æ¨¡æ£±ä¸¤å¯&#x2F;çŠ¹è±«å’Œé¢éƒ¨è¡¨æƒ…ã€‚åœ¨é¢„æµ‹é¢éƒ¨è¡¨æƒ…æ—¶ï¼Œè¿˜ä½¿ç”¨é¢„è®­ç»ƒçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«æ¨¡å‹é€‰æ‹©é«˜åˆ†æ•°è§†é¢‘å¸§ï¼Œé¿å…ä½¿ç”¨ç‰¹å®šé¢†åŸŸçš„è§†é¢‘åˆ†ç±»å™¨è¿›è¡Œå¤„ç†ã€‚é€šè¿‡ç®€å•èšåˆæ¡†æ¶çº§åˆ«çš„ç‰¹å¾å¹¶è®­ç»ƒå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå®ç°äº†æƒ…æ„Ÿæ¨¡ä»¿å¼ºåº¦çš„è§†é¢‘çº§é¢„æµ‹ã€‚åœ¨ABAWæŒ‘æˆ˜ä¸­çš„ä¸‰ä¸ªä»»åŠ¡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰åŸºçº¿ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜äº†éªŒè¯æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†åœ¨æƒ…æ„Ÿè¡Œä¸ºåˆ†æç«èµ›ï¼ˆABAWï¼‰ä¸­çš„æ–°ç ”ç©¶æˆæœã€‚</li>
<li>ä½¿ç”¨äº†é¢„è®­ç»ƒçš„æ¨¡å‹æ¥æå–é¢éƒ¨æƒ…æ„Ÿæè¿°ç¬¦å’Œæ–‡æœ¬åµŒå…¥ã€‚</li>
<li>ç»“åˆäº†é¢éƒ¨ã€å£°éŸ³å’Œæ–‡æœ¬è¯†åˆ«æŠ€æœ¯æ¥é¢„æµ‹æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>é‡‡ç”¨å¤šå±‚æ„ŸçŸ¥å™¨è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ï¼Œå¹¶åœ¨é¢„æµ‹é¢éƒ¨è¡¨æƒ…æ—¶é‡‡ç”¨ç‰¹å®šç­–ç•¥é€‰æ‹©è§†é¢‘å¸§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç®€å•èšåˆæ¡†æ¶çº§åˆ«çš„ç‰¹å¾å®ç°äº†æƒ…æ„Ÿæ¨¡ä»¿å¼ºåº¦çš„è§†é¢‘çº§é¢„æµ‹ã€‚</li>
<li>åœ¨ABAWæŒ‘æˆ˜çš„ä¸‰ä¸ªä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†éªŒè¯æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-91f1149ecec30ca6ab6a3e2a6353d4a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c655edfbb34e2e6bf4e0789c03a71b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67fcefbda93426c9b9360cff8e9b2e48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f7a83c47abff25b50fbe98921dca99b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bafa04fd532a604ba6a0a65cdbcfce5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7da4336b8b841dfbc3b3820199ca45c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1443c75ec9dbd2693eaf7b0a6e9bbe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Adaptive-Inner-Speech-Text-Alignment-for-LLM-based-Speech-Translation"><a href="#Adaptive-Inner-Speech-Text-Alignment-for-LLM-based-Speech-Translation" class="headerlink" title="Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation"></a>Adaptive Inner Speech-Text Alignment for LLM-based Speech Translation</h2><p><strong>Authors:Henglyu Liu, Andong Chen, Kehai Chen, Xuefeng Bai, Meizhi Zhong, Yuan Qiu, Min Zhang</strong></p>
<p>Recent advancement of large language models (LLMs) has led to significant breakthroughs across various tasks, laying the foundation for the development of LLM-based speech translation systems. Existing methods primarily focus on aligning inputs and outputs across modalities while overlooking deeper semantic alignment within model representations. To address this limitation, we propose an Adaptive Inner Speech-Text Alignment (AI-STA) method to bridge the modality gap by explicitly aligning speech and text representations at selected layers within LLMs. To achieve this, we leverage the optimal transport (OT) theory to quantify fine-grained representation discrepancies between speech and text. Furthermore, we utilize the cross-modal retrieval technique to identify the layers that are best suited for alignment and perform joint training on these layers. Experimental results on speech translation (ST) tasks demonstrate that AI-STA significantly improves the translation performance of large speech-text models (LSMs), outperforming previous state-of-the-art approaches. Our findings highlight the importance of inner-layer speech-text alignment in LLMs and provide new insights into enhancing cross-modal learning. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºå„é¡¹ä»»åŠ¡å¸¦æ¥äº†é‡å¤§çªç ´ï¼Œä¸ºåŸºäºLLMçš„è¯­éŸ³ç¿»è¯‘ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€çš„è¾“å…¥è¾“å‡ºå¯¹é½ï¼Œè€Œå¿½è§†äº†æ¨¡å‹è¡¨ç¤ºå†…çš„æ·±å±‚è¯­ä¹‰å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å†…éƒ¨è¯­éŸ³æ–‡æœ¬å¯¹é½ï¼ˆAI-STAï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ˜ç¡®å¯¹é½LLMä¸­é€‰å®šå±‚ä¸Šçš„è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºæ¥ç¼©å°æ¨¡æ€å·®è·ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºæ¥é‡åŒ–è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„ç»†ç²’åº¦è¡¨ç¤ºå·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨è·¨æ¨¡æ€æ£€ç´¢æŠ€æœ¯æ¥ç¡®å®šæœ€é€‚åˆå¯¹é½çš„å±‚ï¼Œå¹¶å¯¹è¿™äº›å±‚è¿›è¡Œè”åˆè®­ç»ƒã€‚åœ¨è¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAI-STAæ˜¾è‘—æé«˜äº†å¤§å‹è¯­éŸ³æ–‡æœ¬æ¨¡å‹ï¼ˆLSMï¼‰çš„ç¿»è¯‘æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†LLMå†…éƒ¨å±‚è¯­éŸ³æ–‡æœ¬å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¢å¼ºè·¨æ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10211v1">PDF</a> 12 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºåŸºäºLLMçš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œå¸¦æ¥äº†å„é¡¹ä»»åŠ¡çš„æ˜¾è‘—çªç ´ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€è¾“å…¥è¾“å‡ºå¯¹é½è€Œå¿½è§†æ¨¡å‹è¡¨ç¤ºä¸­çš„æ·±å±‚è¯­ä¹‰å¯¹é½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”å†…éƒ¨è¯­éŸ³æ–‡æœ¬å¯¹é½ï¼ˆAI-STAï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ˜ç¡®å¯¹é½LLMå†…é€‰å®šå±‚çš„è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºæ¥ç¼©å°æ¨¡æ€å·®è·ã€‚åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºæ¥é‡åŒ–è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„ç»†å¾®å·®å¼‚ï¼Œå¹¶åˆ©ç”¨è·¨æ¨¡æ€æ£€ç´¢æŠ€æœ¯æ¥ç¡®å®šæœ€é€‚åˆå¯¹é½çš„å±‚ï¼Œå¹¶å¯¹è¿™äº›å±‚è¿›è¡Œè”åˆè®­ç»ƒã€‚åœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒAI-STAæ˜¾è‘—æé«˜äº†å¤§å‹è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼ˆLSMï¼‰çš„ç¿»è¯‘æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†LLMå†…éƒ¨å±‚è¯­éŸ³æ–‡æœ¬å¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¢å¼ºè·¨æ¨¡æ€å­¦ä¹ æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€è¾“å…¥è¾“å‡ºå¯¹é½ï¼Œå¿½è§†æ¨¡å‹å†…éƒ¨çš„æ·±å±‚è¯­ä¹‰å¯¹é½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å†…éƒ¨è¯­éŸ³æ–‡æœ¬å¯¹é½ï¼ˆAI-STAï¼‰æ–¹æ³•ï¼Œä»¥ç¼©å°æ¨¡æ€å·®è·ã€‚</li>
<li>AI-STAæ–¹æ³•é€šè¿‡æ˜ç¡®å¯¹é½LLMå†…é€‰å®šå±‚çš„è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºæ¥å®ç°å¯¹é½ã€‚</li>
<li>åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºé‡åŒ–è¯­éŸ³å’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>ä½¿ç”¨è·¨æ¨¡æ€æ£€ç´¢æŠ€æœ¯ç¡®å®šæœ€é€‚åˆå¯¹é½çš„å±‚ï¼Œå¹¶å¯¹è¿™äº›å±‚è¿›è¡Œè”åˆè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1aeb9d0137f0b08aad100f89b7222cec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de099ec987c243c2a9c8ea21485fb12a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db699bfd8d995d80251275485909f26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da1c44917c4e90e89f2695096fa67f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8b1952ef317cccbe9eee8b87d227f32.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ValSub-Subsampling-Validation-Data-to-Mitigate-Forgetting-during-ASR-Personalization"><a href="#ValSub-Subsampling-Validation-Data-to-Mitigate-Forgetting-during-ASR-Personalization" class="headerlink" title="ValSub: Subsampling Validation Data to Mitigate Forgetting during ASR   Personalization"></a>ValSub: Subsampling Validation Data to Mitigate Forgetting during ASR   Personalization</h2><p><strong>Authors:Haaris Mehmood, Karthikeyan Saravanan, Pablo Peso Parada, David Tuckey, Mete Ozay, Gil Ho Lee, Jungin Lee, Seokyeong Jung</strong></p>
<p>Automatic Speech Recognition (ASR) is widely used within consumer devices such as mobile phones. Recently, personalization or on-device model fine-tuning has shown that adaptation of ASR models towards target user speech improves their performance over rare words or accented speech. Despite these gains, fine-tuning on user data (target domain) risks the personalized model to forget knowledge about its original training distribution (source domain) i.e. catastrophic forgetting, leading to subpar general ASR performance. A simple and efficient approach to combat catastrophic forgetting is to measure forgetting via a validation set that represents the source domain distribution. However, such validation sets are large and impractical for mobile devices. Towards this, we propose a novel method to subsample a substantially large validation set into a smaller one while maintaining the ability to estimate forgetting. We demonstrate the efficacy of such a dataset in mitigating forgetting by utilizing it to dynamically determine the number of ideal fine-tuning epochs. When measuring the deviations in per user fine-tuning epochs against a 50x larger validation set (oracle), our method achieves a lower mean-absolute-error (3.39) compared to randomly selected subsets of the same size (3.78-8.65). Unlike random baselines, our method consistently tracks the oracleâ€™s behaviour across three different forgetting thresholds. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨ç§»åŠ¨ç”µè¯ç­‰æ¶ˆè´¹è®¾å¤‡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æœ€è¿‘ï¼Œä¸ªæ€§åŒ–æˆ–è®¾å¤‡ä¸Šçš„æ¨¡å‹å¾®è°ƒæ˜¾ç¤ºï¼Œé’ˆå¯¹ç›®æ ‡ç”¨æˆ·è¯­éŸ³çš„ASRæ¨¡å‹é€‚åº”æ€§æ”¹è¿›äº†å…¶åœ¨ç½•è§è¯æ±‡æˆ–å¸¦å£éŸ³è¯­éŸ³æ–¹é¢çš„æ€§èƒ½ã€‚å°½ç®¡æœ‰è¿™äº›æ”¶è·ï¼Œä½†åœ¨ç”¨æˆ·æ•°æ®ï¼ˆç›®æ ‡åŸŸï¼‰ä¸Šè¿›è¡Œå¾®è°ƒä¼šä½¿ä¸ªæ€§åŒ–æ¨¡å‹å¿˜è®°å…¶åŸå§‹è®­ç»ƒåˆ†å¸ƒï¼ˆæºåŸŸï¼‰çš„çŸ¥è¯†ï¼Œå³ç¾éš¾æ€§é—å¿˜ï¼Œå¯¼è‡´é€šç”¨çš„ASRæ€§èƒ½ä¸‹é™ã€‚ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•æ˜¯é€šè¿‡ä»£è¡¨æºåŸŸåˆ†å¸ƒçš„éªŒè¯é›†æ¥æµ‹é‡é—å¿˜ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„éªŒè¯é›†å¾ˆå¤§ï¼Œå¯¹äºç§»åŠ¨è®¾å¤‡æ¥è¯´ä¸åˆ‡å®é™…ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä»å¤§é‡éªŒè¯é›†ä¸­æŠ½æ ·å‡ºè¾ƒå°çš„å­é›†ï¼ŒåŒæ—¶ä¿æŒä¼°è®¡é—å¿˜çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡åœ¨åŠ¨æ€ç¡®å®šç†æƒ³å¾®è°ƒå‘¨æœŸæ•°æ—¶åˆ©ç”¨è¯¥æ•°æ®é›†æ¥è¯æ˜æ­¤ç±»æ•°æ®é›†åœ¨ç¼“è§£é—å¿˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å½“æµ‹é‡æ¯ä¸ªç”¨æˆ·çš„å¾®è°ƒå‘¨æœŸä¸50å€å¤§çš„éªŒè¯é›†ï¼ˆç†æƒ³æƒ…å†µï¼‰ä¹‹é—´çš„åå·®æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´ä½çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆ3.39ï¼‰ï¼Œä¸éšæœºé€‰æ‹©çš„ç›¸åŒå¤§å°çš„å­é›†ç›¸æ¯”ï¼ˆ3.78-8.65ï¼‰ã€‚ä¸åŒäºéšæœºåŸºçº¿æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆè·Ÿè¸ªç†æƒ³æƒ…å†µä¸‹çš„è¡Œä¸ºè¡¨ç°ï¼Œè·¨è¶Šä¸‰ä¸ªä¸åŒçš„é—å¿˜é˜ˆå€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09906v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨æ‰‹æœºç­‰æ¶ˆè´¹è®¾å¤‡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç”¨æˆ·æ¨¡å‹å¾®è°ƒæé«˜äº†ç›®æ ‡ç”¨æˆ·å¯¹ASRæ¨¡å‹çš„é€‚åº”æ€§å’Œå¯¹æŸäº›ç”¨æˆ·çš„ç½•è§è¯æ±‡æˆ–å¸¦å£éŸ³è¯­éŸ³çš„è¯†åˆ«æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹ç”¨æˆ·æ•°æ®è¿›è¡Œå¾®è°ƒå¯èƒ½ä½¿æ¨¡å‹é—å¿˜å…¶åŸå§‹è®­ç»ƒåˆ†å¸ƒçš„çŸ¥è¯†ï¼Œå¯¼è‡´ä¸€èˆ¬ASRæ€§èƒ½ä¸‹é™ã€‚ä¸€ç§è§£å†³ç¾éš¾æ€§é—å¿˜çš„ç®€å•æœ‰æ•ˆæ–¹æ³•æ˜¯ä½¿ç”¨ä»£è¡¨åŸå§‹è®­ç»ƒåˆ†å¸ƒçš„æ•°æ®é›†æ¥æµ‹é‡é—å¿˜ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„éªŒè¯é›†å¾ˆå¤§ï¼Œä¸é€‚ç”¨äºç§»åŠ¨è®¾å¤‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†è¾ƒå¤§çš„éªŒè¯é›†ç¼©å‡ä¸ºè¾ƒå°çš„é›†åˆå¹¶ä»ç„¶èƒ½å¤Ÿä¼°è®¡é—å¿˜ã€‚æˆ‘ä»¬è¯æ˜äº†é€šè¿‡è¯¥æ•°æ®é›†åŠ¨æ€ç¡®å®šç†æƒ³å¾®è°ƒå‘¨æœŸçš„æœ‰æ•ˆæ€§ã€‚ä¸æ›´å¤§çš„éªŒè¯é›†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¡¡é‡ç”¨æˆ·å¾®è°ƒå‘¨æœŸçš„åå·®æ–¹é¢å®ç°äº†æ›´ä½çš„å¹³å‡ç»å¯¹è¯¯å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„é—å¿˜é˜ˆå€¼ä¸‹éƒ½èƒ½æŒç»­è¿½è¸ªéªŒè¯é›†çš„èµ°å‘ã€‚æ€»çš„æ¥è¯´ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ASRæ¨¡å‹å¾®è°ƒç­–ç•¥ã€‚é€šè¿‡å¯¹æ•°æ®é›†è¿›è¡Œæ™ºèƒ½é‡‡æ ·æ¥è¯„ä¼°é—å¿˜æƒ…å†µï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„æ¨¡å‹è°ƒæ•´å¹¶æå‡æ€§èƒ½ã€‚åŒæ—¶ç¡®ä¿å³ä½¿åœ¨æœ‰é™çš„èµ„æºæ¡ä»¶ä¸‹ä¹Ÿèƒ½æœ‰æ•ˆè¿è¡Œï¼Œä¸ºè§£å†³ASRé¢ä¸´çš„æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚æ­¤å¤–è¿˜å¯¹æœ‰æ•ˆæ€§èƒ½è¡¨ç°æå‡ºäº†å¼ºæœ‰åŠ›çš„å®è¯æ”¯æŒã€‚è¿™ä¸ä»…å¯ä»¥ä¼˜åŒ–æ¶ˆè´¹è®¾å¤‡çš„ASRæŠ€æœ¯è¿˜å¯ä»¥æ¨å¹¿è‡³å…¶å®ƒè¯­éŸ³è¯†åˆ«åº”ç”¨æˆ–ç³»ç»Ÿè®¾è®¡ä¸­å¹¶æœ‰åŠ©äºæ›´å¹¿æ³›å®ç°é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒæå‡è¡Œä¸šæ•´ä½“å‘å±•é€Ÿåº¦ä¸æˆæœæ™®åŠå¹¿åº¦ã€‚<strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0c2b353825760f828655d046cd905f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74933e4d709a35547b3820a3afda8620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b2237d67b2a52bc13805e8a6b5a13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8df4085a7f1767e16957917f32a7cbfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e19eb069227bcf4c27ea0f8b39b6dd65.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Quantization-for-OpenAIâ€™s-Whisper-Models-A-Comparative-Analysis"><a href="#Quantization-for-OpenAIâ€™s-Whisper-Models-A-Comparative-Analysis" class="headerlink" title="Quantization for OpenAIâ€™s Whisper Models: A Comparative Analysis"></a>Quantization for OpenAIâ€™s Whisper Models: A Comparative Analysis</h2><p><strong>Authors:Allison Andreyev</strong></p>
<p>Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19% and model size by 45%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/allisonandreyev/WhisperQuantization.git">https://github.com/allisonandreyev/WhisperQuantization.git</a> </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹åœ¨å­—å¹•ã€è¯­éŸ³ç¿»è¯‘å’Œå®æ—¶è½¬å½•ç­‰é¢†åŸŸçš„åº”ç”¨å·²ç»å¾—åˆ°å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡ç ”ç©¶äº†Whisperå’Œä¸¤ç§æ¨¡å‹å˜ä½“ï¼šä¸€ç§é€‚ç”¨äºå®æ—¶è¯­éŸ³æµï¼Œå¦ä¸€ç§é€‚ç”¨äºç¦»çº¿è½¬å½•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹è¢«å‘ç°ä¼šäº§ç”Ÿå¹»è§‰å†…å®¹ï¼Œé™ä½äº†è½¬å½•çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œè¾ƒå¤§çš„æ¨¡å‹å˜ä½“è¡¨ç°å‡ºæ›´é«˜çš„å»¶è¿Ÿï¼Œå¹¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²æ—¶å¸¦æ¥æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶åˆ†æäº†ä¸‰ç§Whisperæ¨¡å‹çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼Œå®šæ€§è¯„ä¼°äº†å®ƒä»¬å„è‡ªçš„èƒ½åŠ›ã€‚æ¥ä¸‹æ¥ï¼Œæœ¬ç ”ç©¶é‡åŒ–äº†æ¨¡å‹é‡åŒ–å¯¹å»¶è¿Ÿçš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†å…¶ç”¨äºè¾¹ç¼˜è®¾å¤‡çš„å¯è¡Œæ€§ã€‚æœ¬ç ”ç©¶ä½¿ç”¨å¼€æºLibriSpeechæ•°æ®é›†ï¼Œè¯„ä¼°äº†whispercppçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¹¶å¯¹å…¶ä½¿ç”¨ä¸‰ç§é‡åŒ–æ–¹æ³•ï¼ˆINT4ã€INT5ã€INT8ï¼‰è¿›è¡Œäº†å»¶è¿Ÿåˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œé‡åŒ–å¯ä»¥é™ä½å»¶è¿Ÿ19%ï¼Œç¼©å°æ¨¡å‹ä½“ç§¯45%ï¼ŒåŒæ—¶ä¿æŒè½¬å½•ç²¾åº¦ã€‚è¿™äº›å‘ç°æä¾›äº†å¯¹ä¸åŒWhisperæ¨¡å‹çš„æœ€ä½³ç”¨ä¾‹å’Œè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²å¯èƒ½æ€§çš„æ·±åˆ»è§è§£ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®é›†å’Œå®æ–½ç»†èŠ‚éƒ½å¯åœ¨å…¬å…±GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/allisonandreyev/WhisperQuantization.git%E3%80%82">https://github.com/allisonandreyev/WhisperQuantization.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09905v1">PDF</a> 7 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸­çš„whisperä¸å…¶ä¸¤ç§å˜ç§æ¨¡å‹åœ¨è¯­éŸ³è¯†åˆ«åº”ç”¨æ–¹é¢çš„è¡¨ç°å’ŒæŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°è¿™äº›æ¨¡å‹å­˜åœ¨ç”Ÿæˆè™šæ„å†…å®¹çš„é—®é¢˜ï¼Œé™ä½äº†è½¬å½•çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œå¤§å‹æ¨¡å‹è¡¨ç°å‡ºè¾ƒé«˜çš„å»¶è¿Ÿï¼Œåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼€æºLibriSpeechæ•°æ®é›†å¯¹whispercppè¿›è¡Œé‡åŒ–åˆ†æï¼Œæ¢è®¨äº†ä¸‰ç§é‡åŒ–æ–¹æ³•å¯¹å»¶è¿Ÿå’Œæ¨¡å‹å¤§å°çš„å½±å“ï¼ŒåŒæ—¶è¯„ä¼°äº†å…¶å¯¹è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„å¯è¡Œæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡åŒ–å¯é™ä½å»¶è¿Ÿå¹¶å‡å°æ¨¡å‹å¤§å°ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„è½¬å½•ç²¾åº¦ã€‚è¯¥ç ”ç©¶çš„å‘ç°å¯¹äºä¼˜åŒ–whisperæ¨¡å‹åœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸­çš„ä½¿ç”¨åŠå…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²æä¾›äº†é‡è¦è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WhisperåŠå…¶å˜ç§æ¨¡å‹å¹¿æ³›åº”ç”¨äºè¯­éŸ³è¯†åˆ«é¢†åŸŸï¼Œå¦‚å­—å¹•ã€è¯­éŸ³ç¿»è¯‘å’Œå®æ—¶è½¬å½•ã€‚</li>
<li>è¿™äº›æ¨¡å‹å­˜åœ¨ç”Ÿæˆè™šæ„å†…å®¹çš„é—®é¢˜ï¼Œå½±å“è½¬å½•çš„å¯é æ€§ã€‚</li>
<li>å¤§å‹ASRæ¨¡å‹å…·æœ‰æ›´é«˜çš„å»¶è¿Ÿï¼Œåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡LibriSpeechæ•°æ®é›†å¯¹whispercppè¿›è¡Œé‡åŒ–åˆ†æï¼Œæ¢è®¨äº†é‡åŒ–å¯¹å»¶è¿Ÿå’Œæ¨¡å‹å¤§å°çš„å½±å“ã€‚</li>
<li>é‡åŒ–æ–¹æ³•å¯é™ä½å»¶è¿Ÿå¹¶å‡å°æ¨¡å‹å¤§å°çº¦45%ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„è½¬å½•ç²¾åº¦ã€‚</li>
<li>å…¬å…±GitHubä»“åº“æä¾›äº†ä»£ç ã€æ•°æ®é›†å’Œå®æ–½ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09905">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f8ac7986b49c77c589f4a4e3dfe75ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71061411868c0c1a2ba0b7392349ce40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e371423c699a871b68fe614759f964f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-092edf09236fb296243cc76d12a6d2f6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StyleSpeaker-Audio-Enhanced-Fine-Grained-Style-Modeling-for-Speech-Driven-3D-Facial-Animation"><a href="#StyleSpeaker-Audio-Enhanced-Fine-Grained-Style-Modeling-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="StyleSpeaker: Audio-Enhanced Fine-Grained Style Modeling for   Speech-Driven 3D Facial Animation"></a>StyleSpeaker: Audio-Enhanced Fine-Grained Style Modeling for   Speech-Driven 3D Facial Animation</h2><p><strong>Authors:An Yang, Chenyu Liu, Pengcheng Xia, Jun Du</strong></p>
<p>Speech-driven 3D facial animation is challenging due to the diversity in speaking styles and the limited availability of 3D audio-visual data. Speech predominantly dictates the coarse motion trends of the lip region, while specific styles determine the details of lip motion and the overall facial expressions. Prior works lack fine-grained learning in style modeling and do not adequately consider style biases across varying speech conditions, which reduce the accuracy of style modeling and hamper the adaptation capability to unseen speakers. To address this, we propose a novel framework, StyleSpeaker, which explicitly extracts speaking styles based on speaker characteristics while accounting for style biases caused by different speeches. Specifically, we utilize a style encoder to capture speakersâ€™ styles from facial motions and enhance them according to motion preferences elicited by varying speech conditions. The enhanced styles are then integrated into the coarse motion features via a style infusion module, which employs a set of style primitives to learn fine-grained style representation. Throughout training, we maintain this set of style primitives to comprehensively model the entire style space. Hence, StyleSpeaker possesses robust style modeling capability for seen speakers and can rapidly adapt to unseen speakers without fine-tuning. Additionally, we design a trend loss and a local contrastive loss to improve the synchronization between synthesized motions and speeches. Extensive qualitative and quantitative experiments on three public datasets demonstrate that our method outperforms existing state-of-the-art approaches. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ç”±äºè¯´è¯é£æ ¼çš„å¤šæ ·æ€§å’Œ3Dè§†å¬æ•°æ®çš„æœ‰é™å¯ç”¨æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¯­éŸ³ä¸»è¦å†³å®šå”‡éƒ¨çš„ç²—ç•¥è¿åŠ¨è¶‹åŠ¿ï¼Œè€Œç‰¹å®šé£æ ¼åˆ™å†³å®šå”‡éƒ¨è¿åŠ¨çš„ç»†èŠ‚å’Œæ•´ä½“é¢éƒ¨è¡¨æƒ…ã€‚ä¹‹å‰çš„ç ”ç©¶åœ¨é£æ ¼å»ºæ¨¡æ–¹é¢ç¼ºä¹ç²¾ç»†å­¦ä¹ ï¼Œæ²¡æœ‰å……åˆ†è€ƒè™‘ä¸åŒè¯­éŸ³æ¡ä»¶ä¸‹çš„é£æ ¼åè§ï¼Œè¿™é™ä½äº†é£æ ¼å»ºæ¨¡çš„å‡†ç¡®æ€§ï¼Œå¹¶é˜»ç¢äº†å¯¹æœªè§è¯´è¯è€…çš„é€‚åº”èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶StyleSpeakerï¼Œè¯¥æ¡†æ¶èƒ½å¤ŸåŸºäºè¯´è¯äººç‰¹å¾æ˜ç¡®æå–è¯´è¯é£æ ¼ï¼ŒåŒæ—¶è€ƒè™‘ä¸åŒè¯­éŸ³å¼•èµ·çš„é£æ ¼åè§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨é£æ ¼ç¼–ç å™¨ä»é¢éƒ¨è¿åŠ¨ä¸­æ•æ‰è¯´è¯äººçš„é£æ ¼ï¼Œå¹¶æ ¹æ®ä¸åŒè¯­éŸ³æ¡ä»¶å¼•å‘çš„è¿åŠ¨åå¥½è¿›è¡Œå¢å¼ºã€‚å¢å¼ºåçš„é£æ ¼ç„¶åé€šè¿‡é£æ ¼æ³¨å…¥æ¨¡å—èå…¥åˆ°ç²—ç•¥è¿åŠ¨ç‰¹å¾ä¸­ï¼Œè¯¥æ¨¡å—é‡‡ç”¨ä¸€ç³»åˆ—é£æ ¼åŸå§‹å…ƒç´ æ¥å­¦ä¹ ç²¾ç»†çš„é£æ ¼è¡¨ç¤ºã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¿æŒè¿™ç»„é£æ ¼åŸå§‹å…ƒç´ æ¥å…¨é¢å»ºæ¨¡æ•´ä¸ªé£æ ¼ç©ºé—´ã€‚å› æ­¤ï¼ŒStyleSpeakerå¯¹å·²çŸ¥è¯´è¯äººå…·æœ‰å¼ºå¤§çš„é£æ ¼å»ºæ¨¡èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥å¿«é€Ÿé€‚åº”æœªè§è¯´è¯äººè€Œæ— éœ€å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†è¶‹åŠ¿æŸå¤±å’Œå±€éƒ¨å¯¹æ¯”æŸå¤±æ¥æ”¹å–„åˆæˆè¿åŠ¨å’Œè¯­éŸ³ä¹‹é—´çš„åŒæ­¥æ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºStyleSpeakerçš„æ–°æ¡†æ¶ï¼Œç”¨äºåŸºäºè¯´è¯é£æ ¼çš„3Dé¢éƒ¨åŠ¨ç”»ã€‚è¯¥æ¡†æ¶é€šè¿‡é£æ ¼ç¼–ç å™¨æ•æ‰è¯´è¯äººçš„é£æ ¼ç‰¹å¾ï¼Œå¹¶è€ƒè™‘ä¸åŒè¯­éŸ³å¼•èµ·çš„é£æ ¼åè§ã€‚é€šè¿‡é£æ ¼èåˆæ¨¡å—å°†å¢å¼ºé£æ ¼èå…¥ç²—åŠ¨ä½œç‰¹å¾ï¼Œå­¦ä¹ ç²¾ç»†çš„é£æ ¼è¡¨ç¤ºï¼Œå¹¶è®¾è®¡è¶‹åŠ¿æŸå¤±å’Œå±€éƒ¨å¯¹æ¯”æŸå¤±ä»¥æé«˜åŠ¨ä½œä¸è¯­éŸ³çš„åŒæ­¥æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StyleSpeakeræ¡†æ¶ç”¨äºè§£å†³è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„è¯´è¯é£æ ¼å¤šæ ·æ€§å’Œ3Dè§†å¬æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æ¡†æ¶é€šè¿‡é£æ ¼ç¼–ç å™¨æ•æ‰è¯´è¯äººçš„é£æ ¼ç‰¹å¾ï¼Œå¹¶è€ƒè™‘ä¸åŒè¯­éŸ³æ¡ä»¶å¼•èµ·çš„é£æ ¼åè§ã€‚</li>
<li>åˆ©ç”¨é£æ ¼èåˆæ¨¡å—å°†å¢å¼ºé£æ ¼èå…¥ç²—åŠ¨ä½œç‰¹å¾ï¼Œå­¦ä¹ ç²¾ç»†çš„é£æ ¼è¡¨ç¤ºã€‚</li>
<li>è®¾è®¡è¶‹åŠ¿æŸå¤±å’Œå±€éƒ¨å¯¹æ¯”æŸå¤±ä»¥æé«˜åŠ¨ä½œä¸è¯­éŸ³çš„åŒæ­¥æ€§ã€‚</li>
<li>StyleSpeakerå…·æœ‰ç¨³å¥çš„é£æ ¼å»ºæ¨¡èƒ½åŠ›ï¼Œå¯å¿«é€Ÿé€‚åº”æœªè§è¿‡çš„è¯´è¯äººã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒStyleSpeakerçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5a76db721cefb8e44b226cc32ec5d6ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f326a43ac05867e3bbf611d67ad78628.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72a46b0c757e87dafb56eadf2f7f1228.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37cdf18abd49899c7658e0f34462deaf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bimodal-Connection-Attention-Fusion-for-Speech-Emotion-Recognition"><a href="#Bimodal-Connection-Attention-Fusion-for-Speech-Emotion-Recognition" class="headerlink" title="Bimodal Connection Attention Fusion for Speech Emotion Recognition"></a>Bimodal Connection Attention Fusion for Speech Emotion Recognition</h2><p><strong>Authors:Jiachen Luo, Huy Phan, Lin Wang, Joshua D. Reiss</strong></p>
<p>Multi-modal emotion recognition is challenging due to the difficulty of extracting features that capture subtle emotional differences. Understanding multi-modal interactions and connections is key to building effective bimodal speech emotion recognition systems. In this work, we propose Bimodal Connection Attention Fusion (BCAF) method, which includes three main modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network uses an encoder-decoder architecture to model modality connections between audio and text while leveraging modality-specific features. The bimodal attention network enhances semantic complementation and exploits intra- and inter-modal interactions. The correlative attention network reduces cross-modal noise and captures correlations between audio and text. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing state-of-the-art baselines. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå…¶éš¾ç‚¹åœ¨äºæå–èƒ½å¤Ÿæ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾ã€‚ç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥æ˜¯æ„å»ºæœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å…³é”®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ¨¡æ€è¿æ¥æ³¨æ„åŠ›èåˆï¼ˆBCAFï¼‰æ–¹æ³•ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚äº¤äº’è¿æ¥ç½‘ç»œä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¥å»ºæ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥ï¼ŒåŒæ—¶åˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºäº†è¯­ä¹‰äº’è¡¥æ€§ï¼Œå¹¶æ¢ç´¢äº†æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„äº¤äº’ã€‚ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œé™ä½äº†è·¨æ¨¡æ€å™ªå£°ï¼Œå¹¶æ•æ‰äº†éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨MELDå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BCAFæ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€æ–°åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05858v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„æŒ‘æˆ˜ï¼Œå…³é”®åœ¨äºæå–æ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾ã€‚ä¸ºäº†æ„å»ºæœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿï¼Œç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥æ˜¯å…³é”®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŒæ¨¡æ€è¿æ¥æ³¨æ„åŠ›èåˆï¼ˆBCAFï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ¨¡æ€é—´çš„äº¤äº’å…³ç³»å’Œä¿¡æ¯æ¥å»ºæ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥ï¼ŒåŒæ—¶é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„è¿›è¡Œå»ºæ¨¡ã€‚åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºäº†è¯­ä¹‰äº’è¡¥æ€§ï¼Œå¹¶åˆ©ç”¨äº†æ¨¡æ€å†…çš„äº¤äº’æ€§ã€‚ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œå‡å°‘äº†è·¨æ¨¡æ€å™ªå£°ï¼Œå¹¶æ•æ‰äº†éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨MELDå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BCAFæ–¹æ³•ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«é¢ä¸´æå–æ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚ç‰¹å¾çš„æŒ‘æˆ˜ã€‚</li>
<li>ç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥æ˜¯æ„å»ºæœ‰æ•ˆåŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å…³é”®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†BCAFæ–¹æ³•ï¼ŒåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—æ¥å¤„ç†å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>äº¤äº’è¿æ¥ç½‘ç»œåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å»ºæ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥ã€‚</li>
<li>åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºè¯­ä¹‰äº’è¡¥æ€§å¹¶æ•æ‰æ¨¡æ€å†…çš„äº¤äº’æ€§ã€‚</li>
<li>ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œå‡å°‘è·¨æ¨¡æ€å™ªå£°å¹¶æ•æ‰éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f27519db50689a51e74858d8c352238f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c1b8cf94f9c66af79275f626478a5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edd9520a134d3b9fb9db88cf258b5cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6951a87eef122264c5d3ffb0e1ac5b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d29b3f719366cf26dd167bad0e298b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f228ba52ba5d222be261a00ef566ca81.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EMOVA-Empowering-Language-Models-to-See-Hear-and-Speak-with-Vivid-Emotions"><a href="#EMOVA-Empowering-Language-Models-to-See-Hear-and-Speak-with-Vivid-Emotions" class="headerlink" title="EMOVA: Empowering Language Models to See, Hear and Speak with Vivid   Emotions"></a>EMOVA: Empowering Language Models to See, Hear and Speak with Vivid   Emotions</h2><p><strong>Authors:Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu</strong></p>
<p>GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions. </p>
<blockquote>
<p>GPT-4oæ˜¯ä¸€æ¬¾æ”¯æŒå¸¦æœ‰å¤šç§æƒ…æ„Ÿå’Œè¯­è°ƒçš„å£°éŸ³å¯¹è¯çš„è·¨æ¨¡æ€æ¨¡å‹ï¼Œå®ƒä¸ºè·¨æ¨¡æ€åŸºç¡€æ¨¡å‹çš„å‘å±•æ ‘ç«‹äº†é‡Œç¨‹ç¢‘ã€‚ç„¶è€Œï¼Œå¯¹äºå¼€æºç¤¾åŒºè€Œè¨€ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„ŸçŸ¥å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ç­‰ç«¯åˆ°ç«¯çš„æ•°æ®ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–äºå¤–éƒ¨å·¥å…·è¿›è¡Œè¯­éŸ³å¤„ç†ï¼Œè€Œè¯­éŸ³è¯­è¨€æ¨¡å‹ä»ç„¶å—åˆ°æœ‰é™çš„æˆ–å®Œå…¨æ²¡æœ‰è§†è§‰ç†è§£èƒ½åŠ›çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†æƒ…æ„Ÿæ— å¤„ä¸åœ¨çš„è¯­éŸ³åŠ©æ‰‹ï¼ˆEMOVAï¼‰ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡ç«¯åˆ°ç«¯çš„è¯­éŸ³èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé¢†å…ˆçš„è§†è§‰è¯­è¨€æ€§èƒ½ã€‚é€šè¿‡è¯­ä¹‰å£°å­¦åˆ†æ‹†çš„è¯­éŸ³æ ‡è®°å™¨ï¼Œæˆ‘ä»¬å‘ç°è·¨æ¨¡æ€å¯¹é½èƒ½è¿›ä¸€æ­¥å¢å¼ºäº†è§†è§‰è¯­è¨€å’Œè¯­éŸ³çš„èƒ½åŠ›ç›¸æ¯”ä¼ ç»ŸåŒæ¨¡æ€å¯¹é½çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§é£æ ¼æ¨¡å—ï¼Œç”¨äºçµæ´»çš„è¯­éŸ³é£æ ¼æ§åˆ¶ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå’ŒéŸ³è°ƒã€‚é¦–æ¬¡å®ç°äº†åœ¨è§†è§‰è¯­è¨€å’Œè¯­éŸ³åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¾¾åˆ°äº†æœ€ä½³è¡¨ç°ï¼ŒåŒæ—¶æ”¯æŒå¸¦æœ‰ç”ŸåŠ¨æƒ…æ„Ÿçš„è·¨æ¨¡æ€å£è¯­å¯¹è¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18042v3">PDF</a> Accepted by CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://emova-ollm.github.io/">https://emova-ollm.github.io/</a></p>
<p><strong>Summary</strong><br>     GPT-4oæ¨¡å‹åœ¨æ”¯æŒè·¨æ¨¡æ€å¯¹è¯ä¸­å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰ï¼Œèƒ½å¤Ÿæ”¯æŒå¤šæ ·åŒ–çš„æƒ…æ„Ÿå’Œè¯­è°ƒã€‚ç„¶è€Œï¼Œå…¬å¼€æ•°æ®é›†ä¸­ç«¯åˆ°ç«¯æ„ŸçŸ¥å’Œç”Ÿæˆå›¾åƒã€æ–‡æœ¬å’Œè¯­éŸ³ä»å­˜åœ¨æŒ‘æˆ˜ã€‚EMOVAæ¨¡å‹æ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯è¯­éŸ³èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé¢†å…ˆçš„è§†è§‰è¯­è¨€æ€§èƒ½ã€‚å¼•å…¥è¯­ä¹‰å£°å­¦åˆ†ç¦»çš„è¯­éŸ³åˆ†è¯å™¨ï¼Œå‘ç°å¤šæ¨¡æ€å¯¹é½èƒ½æé«˜è§†è§‰è¯­è¨€å’Œè¯­éŸ³èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¼•å…¥è½»é‡çº§é£æ ¼æ¨¡å—å®ç°çµæ´»çš„è¯­éŸ³é£æ ¼æ§åˆ¶ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå’ŒéŸ³è°ƒã€‚EMOVAé¦–æ¬¡åœ¨è§†è§‰è¯­è¨€å’Œè¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ”¯æŒå…·æœ‰ç”ŸåŠ¨æƒ…æ„Ÿçš„è·¨æ¨¡æ€è¯­éŸ³å¯¹è¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oæ¨¡å‹æ”¯æŒè·¨æ¨¡æ€å¯¹è¯ä¸­å…·æœ‰å¤šæ ·åŒ–çš„æƒ…æ„Ÿå’Œè¯­è°ƒã€‚</li>
<li>EMOVAæ¨¡å‹æ—¨åœ¨å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯è¯­éŸ³èƒ½åŠ›ã€‚</li>
<li>EMOVAæ¨¡å‹å¼•å…¥è¯­ä¹‰å£°å­¦åˆ†ç¦»çš„è¯­éŸ³åˆ†è¯å™¨ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€å¯¹é½èƒ½æé«˜è§†è§‰è¯­è¨€å’Œè¯­éŸ³èƒ½åŠ›ã€‚</li>
<li>è½»é‡çº§é£æ ¼æ¨¡å—å®ç°çµæ´»çš„è¯­éŸ³é£æ ¼æ§åˆ¶ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå’ŒéŸ³è°ƒæ§åˆ¶ã€‚</li>
<li>EMOVAåœ¨è§†è§‰è¯­è¨€å’Œè¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å…ˆè¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8826c564c19392b6b6b2c76b0b833f1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0095ac336e177b6e19a4f9d5dc36fa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1cb9f78dca34a615752f909a05a0879.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-070ec817d9092c84d669b09ccfb1bbb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17c2d3d0fc708bdbc24eec4b4f9a07ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef712702f22c0a59c33b0fa5c0c32106.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf38765ec00cc7bc29580000016409fe.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  X-GAN A Generative AI-Powered Unsupervised Model for High-Precision   Segmentation of Retinal Main Vessels toward Early Detection of Glaucoma
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-62d448d16cf04bb54473cb20f5e239c1.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on   Contrast-enhanced CT
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18179.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
