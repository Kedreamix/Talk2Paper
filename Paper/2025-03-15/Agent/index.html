<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  SurgRAW Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-15-æ›´æ–°"><a href="#2025-03-15-æ›´æ–°" class="headerlink" title="2025-03-15 æ›´æ–°"></a>2025-03-15 æ›´æ–°</h1><h2 id="SurgRAW-Multi-Agent-Workflow-with-Chain-of-Thought-Reasoning-for-Surgical-Intelligence"><a href="#SurgRAW-Multi-Agent-Workflow-with-Chain-of-Thought-Reasoning-for-Surgical-Intelligence" class="headerlink" title="SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence"></a>SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence</h2><p><strong>Authors:Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, Yueming Jin</strong></p>
<p>Integration of Vision-Language Models (VLMs) in surgical intelligence is hindered by hallucinations, domain knowledge gaps, and limited understanding of task interdependencies within surgical scenes, undermining clinical reliability. While recent VLMs demonstrate strong general reasoning and thinking capabilities, they still lack the domain expertise and task-awareness required for precise surgical scene interpretation. Although Chain-of-Thought (CoT) can structure reasoning more effectively, current approaches rely on self-generated CoT steps, which often exacerbate inherent domain gaps and hallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent framework that delivers transparent, interpretable insights for most tasks in robotic-assisted surgery. By employing specialized CoT prompts across five tasks: instrument recognition, action recognition, action prediction, patient data extraction, and outcome assessment, SurgRAW mitigates hallucinations through structured, domain-aware reasoning. Retrieval-Augmented Generation (RAG) is also integrated to external medical knowledge to bridge domain gaps and improve response reliability. Most importantly, a hierarchical agentic system ensures that CoT-embedded VLM agents collaborate effectively while understanding task interdependencies, with a panel discussion mechanism promotes logical consistency. To evaluate our method, we introduce SurgCoTBench, the first reasoning-based dataset with structured frame-level annotations. With comprehensive experiments, we demonstrate the effectiveness of proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures, achieving the state-of-the-art performance and advancing explainable, trustworthy, and autonomous surgical assistance. </p>
<blockquote>
<p>å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ•´åˆåˆ°æ‰‹æœ¯æ™ºèƒ½ä¸­é¢ä¸´ç€å¹»è§†ã€é¢†åŸŸçŸ¥è¯†å·®è·ã€å¯¹æ‰‹æœ¯åœºæ™¯ä¸­ä»»åŠ¡ç›¸äº’ä¾èµ–æ€§çš„æœ‰é™ç†è§£ç­‰éšœç¢ï¼Œè¿™å‰Šå¼±äº†ä¸´åºŠå¯é æ€§ã€‚è™½ç„¶æœ€è¿‘çš„VLMsè¡¨ç°å‡ºäº†å¼ºå¤§çš„é€šç”¨æ¨ç†å’Œæ€ç»´èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹ç²¾ç¡®çš„æ‰‹æœ¯åœºæ™¯è§£é‡Šæ‰€éœ€çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œä»»åŠ¡æ„è¯†ã€‚è™½ç„¶æ€ç»´é“¾ï¼ˆCoTï¼‰å¯ä»¥æ›´æœ‰æ•ˆåœ°ç»“æ„åŒ–æ¨ç†ï¼Œä½†å½“å‰çš„æ–¹æ³•ä¾èµ–äºè‡ªæˆ‘ç”Ÿæˆçš„CoTæ­¥éª¤ï¼Œè¿™å¾€å¾€ä¼šåŠ å‰§å›ºæœ‰çš„é¢†åŸŸå·®è·å’Œå¹»è§†ã€‚</p>
</blockquote>
<p>ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SurgRAWï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ€ç»´é“¾é©±åŠ¨çš„å¤šä»£ç†æ¡†æ¶ï¼Œä¸ºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯ä¸­çš„å¤§å¤šæ•°ä»»åŠ¡æä¾›é€æ˜ã€å¯è§£é‡Šçš„è§è§£ã€‚é€šè¿‡äº”ä¸ªä»»åŠ¡ä¸­çš„ä¸“ä¸šæ€ç»´é“¾æç¤ºï¼šä»ªå™¨è¯†åˆ«ã€åŠ¨ä½œè¯†åˆ«ã€åŠ¨ä½œé¢„æµ‹ã€æ‚£è€…æ•°æ®æå–å’Œç»“æœè¯„ä¼°ï¼ŒSurgRAWé€šè¿‡ç»“æ„åŒ–ã€é¢†åŸŸæ„ŸçŸ¥æ¨ç†æ¥ç¼“è§£å¹»è§†ã€‚è¿˜é›†æˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»¥è·å–å¤–éƒ¨åŒ»å­¦çŸ¥è¯†ï¼Œä»¥å¼¥è¡¥é¢†åŸŸå·®è·å¹¶æé«˜å“åº”å¯é æ€§ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œåˆ†å±‚ä»£ç†ç³»ç»Ÿç¡®ä¿æ€ç»´é“¾åµŒå…¥çš„VLMä»£ç†æœ‰æ•ˆåœ°åä½œï¼ŒåŒæ—¶ç†è§£ä»»åŠ¡ç›¸äº’ä¾èµ–æ€§ï¼Œå°ç»„è®¨è®ºæœºåˆ¶ä¿ƒè¿›äº†é€»è¾‘ä¸€è‡´æ€§ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†SurgCoTBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¨ç†çš„æ•°æ®é›†ï¼Œå…·æœ‰ç»“æ„åŒ–å¸§çº§æ³¨é‡Šã€‚é€šè¿‡å…¨é¢çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„SurgRAWçš„æœ‰æ•ˆæ€§ï¼Œåœ¨12ä¸ªæœºå™¨äººæ‰‹æœ¯ç¨‹åºä¸Šç›¸å¯¹äºåŸºçº¿VLMså®ç°äº†29.32%çš„å‡†ç¡®ç‡æå‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œå¹¶æ¨åŠ¨äº†å¯è§£é‡Šã€å¯ä¿¡å’Œè‡ªä¸»çš„æ‰‹æœ¯è¾…åŠ©æŠ€æœ¯ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10265v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ‰‹æœ¯æ™ºèƒ½é›†æˆä¸­å—åˆ°å¹»è§‰ã€é¢†åŸŸçŸ¥è¯†å·®è·å’Œä»»åŠ¡ç›¸äº’ä¾èµ–æ€§çš„é™åˆ¶ï¼Œå½±å“ä¸´åºŠå¯é æ€§ã€‚è™½ç„¶æœ€è¿‘çš„VLMå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ¨ç†å’Œæ€ç»´èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹ç²¾ç¡®çš„æ‰‹æœ¯åœºæ™¯è§£é‡Šæ‰€éœ€çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œä»»åŠ¡æ„è¯†ã€‚æœ¬æ–‡æå‡ºSurgRAWï¼Œä¸€ä¸ªåŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œä¸ºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯ä¸­çš„å¤§å¤šæ•°ä»»åŠ¡æä¾›é€æ˜ã€å¯è§£é‡Šæ€§çš„è§è§£ã€‚é€šè¿‡äº”ä¸ªä»»åŠ¡çš„ä¸“é—¨æ€ç»´é“¾æç¤ºï¼ŒSurgRAWé€šè¿‡ç»“æ„åŒ–ã€é¢†åŸŸæ„è¯†æ¨ç†ç¼“è§£å¹»è§‰ã€‚åŒæ—¶é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä»¥å¼¥è¡¥é¢†åŸŸçŸ¥è¯†å·®è·å¹¶æé«˜å“åº”å¯é æ€§ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¸€ä¸ªåˆ†å±‚æ™ºèƒ½ä½“ç³»ç¡®ä¿åµŒå…¥æ€ç»´é“¾çš„VLMæ™ºèƒ½ä½“æœ‰æ•ˆåä½œï¼Œç†è§£ä»»åŠ¡ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶é€šè¿‡å°ç»„è®¨è®ºæœºåˆ¶ä¿ƒè¿›é€»è¾‘ä¸€è‡´æ€§ã€‚è¯„ä¼°æ–¹æ³•å¼•å…¥SurgCoTBenchï¼Œé¦–ä¸ªåŸºäºæ¨ç†çš„æ•°æ®é›†ï¼Œå¸¦æœ‰ç»“æ„åŒ–å¸§çº§æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿VLMç›¸æ¯”ï¼ŒSurgRAWå®ç°äº†29.32%çš„å‡†ç¡®ç‡æå‡ï¼Œåœ¨12é¡¹æœºå™¨äººæ‰‹æœ¯ç¨‹åºä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæ¨åŠ¨å¯è§£é‡Šã€å¯ä¿¡å’Œè‡ªä¸»æ‰‹æœ¯è¾…åŠ©çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMåœ¨æ‰‹æœ¯æ™ºèƒ½ä¸­çš„é›†æˆå—åˆ°å¹»è§‰ã€é¢†åŸŸçŸ¥è¯†å·®è·å’Œä»»åŠ¡ç›¸äº’ä¾èµ–æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ€è¿‘çš„VLMè™½ç„¶å…·æœ‰å¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†ç¼ºä¹æ‰‹æœ¯é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’Œä»»åŠ¡æ„è¯†ã€‚</li>
<li>SurgRAWæ¡†æ¶åˆ©ç”¨CoTé©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç»“æ„æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæä¾›å¯è§£é‡Šæ€§çš„æ‰‹æœ¯ä»»åŠ¡è§è§£ã€‚</li>
<li>SurgRAWé€šè¿‡ç»“æ„åŒ–ã€é¢†åŸŸæ„è¯†çš„æ¨ç†æ¥å‡è½»å¹»è§‰ã€‚</li>
<li>é›†æˆRAGæ¥å¼¥è¡¥åŒ»ç–—é¢†åŸŸçš„çŸ¥è¯†å·®è·ï¼Œæé«˜å“åº”çš„å¯é æ€§ã€‚</li>
<li>åˆ†å±‚æ™ºèƒ½ä½“ç³»ç¡®ä¿æ™ºèƒ½ä½“ç†è§£ä»»åŠ¡ç›¸äº’ä¾èµ–æ€§å¹¶æœ‰æ•ˆåä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-924dc5051a215b0cb2331628406b2cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08663a2f6ba4409243f6a1cf84bfeb5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9de395e9305ab5dbde7a7cc0f00dcd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bba250c9d2179659cd5a6bdacb753073.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents"><a href="#LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents" class="headerlink" title="LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents"></a>LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents</h2><p><strong>Authors:Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, Yali Wang</strong></p>
<p>Existing Multimodal Large Language Models (MLLMs) encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools (e.g., search engine, memory banks, OCR, retrieval models) to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our methodology consists of four key steps: 1. Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2. Perception: We design an effective retrieval scheme for long videos, improving the coverage of critical temporal segments while maintaining computational efficiency. 3. Action: Agents answer long video-related questions and exchange reasons. 4. Reflection: We evaluate the performance of each agent in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (including GPT-4o) and open-source models (including InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80% on four mainstream long video understanding tasks. Notably, on the LongVideoBench dataset, LVAgent improves accuracy by up to 14.3% compared with SOTA. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å»ºæ¨¡é•¿è§†é¢‘ä¸­çš„æ—¶é—´ä¸Šä¸‹æ–‡æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç›®å‰ï¼Œä¸»æµçš„åŸºæœ¬ä»£ç†æ–¹æ³•ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢å¼•æ“ã€è®°å¿†åº“ã€OCRã€æ£€ç´¢æ¨¡å‹ï¼‰æ¥ååŠ©å•ä¸ªMLLMå›ç­”é•¿è§†é¢‘é—®é¢˜ã€‚å°½ç®¡æœ‰åŸºäºå·¥å…·çš„æ”¯æŒï¼Œå•ä¸ªMLLMä»ç„¶åªèƒ½å¯¹é•¿è§†é¢‘æœ‰éƒ¨åˆ†ç†è§£ï¼Œå¯¼è‡´æ€§èƒ½æœ‰é™ã€‚ä¸ºäº†æ›´å¥½åœ°åº”å¯¹é•¿è§†é¢‘ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†LVAgentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä½¿å¤šè½®åŠ¨æ€åä½œçš„MLLMä»£ç†å®ç°é•¿è§†é¢‘ç†è§£çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å››ä¸ªå…³é”®æ­¥éª¤ï¼š1.é€‰æ‹©ï¼šæˆ‘ä»¬ä»æ¨¡å‹åº“ä¸­é¢„å…ˆé€‰æ‹©é€‚å½“çš„ä»£ç†ï¼Œæ ¹æ®ä¸åŒçš„ä»»åŠ¡å½¢æˆæœ€ä¼˜ä»£ç†å›¢é˜Ÿã€‚2.æ„ŸçŸ¥ï¼šæˆ‘ä»¬ä¸ºé•¿è§†é¢‘è®¾è®¡äº†æœ‰æ•ˆçš„æ£€ç´¢æ–¹æ¡ˆï¼Œæé«˜äº†å…³é”®æ—¶é—´æ®µçš„è¦†ç›–ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚3.è¡ŒåŠ¨ï¼šä»£ç†å›ç­”ä¸é•¿è§†é¢‘ç›¸å…³çš„é—®é¢˜å¹¶äº¤æµç†ç”±ã€‚4.åæ€ï¼šæˆ‘ä»¬è¯„ä¼°æ¯ä¸€è½®è®¨è®ºä¸­æ¯ä¸ªä»£ç†çš„æ€§èƒ½ï¼Œä¼˜åŒ–ä»£ç†å›¢é˜Ÿè¿›è¡ŒåŠ¨æ€åä½œã€‚é€šè¿‡MLLMä»£ç†çš„å¤šè½®åŠ¨æ€åä½œï¼Œä»£ç†å¯ä»¥è¿­ä»£ä¼˜åŒ–ç­”æ¡ˆã€‚LVAgentæ˜¯ç¬¬ä¸€ä¸ªåœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºæ‰€æœ‰å°é—­æºæ¨¡å‹ï¼ˆåŒ…æ‹¬GPT-4oï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆåŒ…æ‹¬InternVL-2.5å’ŒQwen2-VLï¼‰çš„ä»£ç†ç³»ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„LVAgentåœ¨å››ä¸ªä¸»æµçš„é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¾¾åˆ°äº†80%çš„å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨LongVideoBenchæ•°æ®é›†ä¸Šï¼ŒLVAgentä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾14.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10200v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹é•¿è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œç°æœ‰å•ä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å­˜åœ¨å»ºæ¨¡æ—¶é—´ä¸Šä¸‹æ–‡èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LVAgentæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒå¤šè½®åŠ¨æ€åä½œçš„MLLMä»£ç†ï¼Œä»¥æé«˜å¯¹é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚LVAgenté€šè¿‡é€‰æ‹©ã€æ„ŸçŸ¥ã€è¡ŒåŠ¨å’Œåæ€å››ä¸ªå…³é”®æ­¥éª¤å®ç°å¤šè½®åŠ¨æ€åä½œï¼Œå¹¶åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¶…è¶Šäº†è®¸å¤šå°é—­å’Œå¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œå•ä¸€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å­˜åœ¨å»ºæ¨¡æ—¶é—´ä¸Šä¸‹æ–‡èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>LVAgentæ¡†æ¶æ˜¯é¦–ä¸ªæ”¯æŒå¤šè½®åŠ¨æ€åä½œçš„MLLMä»£ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æ›´å¥½åœ°è§£å†³é•¿è§†é¢‘ä»»åŠ¡ã€‚</li>
<li>LVAgenté€šè¿‡é€‰æ‹©ã€æ„ŸçŸ¥ã€è¡ŒåŠ¨å’Œåæ€å››ä¸ªæ­¥éª¤å®ç°å¤šè½®åŠ¨æ€åä½œã€‚</li>
<li>LVAgentåœ¨ä¸»æµé•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†é«˜è¾¾80%çš„å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨LongVideoBenchæ•°æ®é›†ä¸Šï¼ŒLVAgentç›¸è¾ƒäºç°æœ‰æœ€ä½³æ¨¡å‹ï¼Œæé«˜äº†é«˜è¾¾14.3%çš„å‡†ç¡®ç‡ã€‚</li>
<li>LVAgenté€šè¿‡è®¾è®¡æœ‰æ•ˆçš„é•¿è§†é¢‘æ£€ç´¢æ–¹æ¡ˆï¼Œæé«˜äº†å¯¹å…³é”®æ—¶é—´æ®µçš„è¦†ç›–å¹¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-12d002c85c1bc01456b098e7eb6c3324.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eeddcf790978ed67edc49e7d1fb51bcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e2ab58aabc5234ec6b4d074bb3d0fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5927170d078d15ffc833161cedb26f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8af1ba5b4ab1a9ecc455db683270abd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6578ceaa7dcb305bc5cccc1774d9fec3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="StepMathAgent-A-Step-Wise-Agent-for-Evaluating-Mathematical-Processes-through-Tree-of-Error"><a href="#StepMathAgent-A-Step-Wise-Agent-for-Evaluating-Mathematical-Processes-through-Tree-of-Error" class="headerlink" title="StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes   through Tree-of-Error"></a>StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes   through Tree-of-Error</h2><p><strong>Authors:Shu-Xun Yang, Cunxiang Wang, Yidong Wang, Xiaotao Gu, Minlie Huang, Jie Tang</strong></p>
<p>Evaluating mathematical capabilities is critical for assessing the overall performance of large language models (LLMs). However, existing evaluation methods often focus solely on final answers, resulting in highly inaccurate and uninterpretable evaluation outcomes, as well as their failure to assess proof or open-ended problems. To address these issues, we propose a novel mathematical process evaluation agent based on Tree-of-Error, called StepMathAgent. This agent incorporates four internal core operations: logical step segmentation, step scoring, score aggregation and error tree generation, along with four external extension modules: difficulty calibration, simplicity evaluation, completeness validation and format assessment. Furthermore, we introduce StepMathBench, a benchmark comprising 1,000 step-divided process evaluation instances, derived from 200 high-quality math problems grouped by problem type, subject category and difficulty level. Experiments on StepMathBench show that our proposed StepMathAgent outperforms all state-of-the-art methods, demonstrating human-aligned evaluation preferences and broad applicability to various scenarios. Our data and code are available at <a target="_blank" rel="noopener" href="https://github.com/SHU-XUN/StepMathAgent">https://github.com/SHU-XUN/StepMathAgent</a>. </p>
<blockquote>
<p>è¯„ä¼°æ•°å­¦èƒ½åŠ›æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ•´ä½“æ€§èƒ½çš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å¾€å¾€åªå…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¯¼è‡´è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€éš¾ä»¥è§£é‡Šï¼Œå¹¶ä¸”æ— æ³•è¯„ä¼°è¯æ˜æˆ–å¼€æ”¾å¼é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé”™è¯¯æ ‘çš„æ–°å‹æ•°å­¦è¿‡ç¨‹è¯„ä¼°ä»£ç†ï¼Œç§°ä¸ºStepMathAgentã€‚è¯¥ä»£ç†ç»“åˆäº†å››é¡¹å†…éƒ¨æ ¸å¿ƒæ“ä½œï¼šé€»è¾‘æ­¥éª¤åˆ†å‰²ã€æ­¥éª¤è¯„åˆ†ã€åˆ†æ•°èšåˆå’Œé”™è¯¯æ ‘ç”Ÿæˆï¼Œä»¥åŠå››é¡¹å¤–éƒ¨æ‰©å±•æ¨¡å—ï¼šéš¾åº¦æ ¡å‡†ã€ç®€å•æ€§è¯„ä¼°ã€å®Œæ•´æ€§éªŒè¯å’Œæ ¼å¼è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†StepMathBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1000ä¸ªæŒ‰æ­¥éª¤åˆ’åˆ†çš„è¿‡ç¨‹è¯„ä¼°å®ä¾‹çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›å®ä¾‹æ¥æºäºæŒ‰é—®é¢˜ç±»å‹ã€ä¸»é¢˜ç±»åˆ«å’Œéš¾åº¦æ°´å¹³åˆ†ç»„çš„200ä¸ªé«˜è´¨é‡æ•°å­¦é—®é¢˜ã€‚åœ¨StepMathBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„StepMathAgentä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¡¨ç°å‡ºä¸äººç±»è¯„ä¼°åå¥½ç›¸ç¬¦çš„ç»“æœï¼Œå¹¶å¹¿æ³›é€‚ç”¨äºå„ç§åœºæ™¯ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SHU-XUN/StepMathAgent">https://github.com/SHU-XUN/StepMathAgent</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦èƒ½åŠ›çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºTree-of-Errorçš„æ–°å‹æ•°å­¦è¿‡ç¨‹è¯„ä»·ä»£ç†â€”â€”StepMathAgentã€‚è¯¥ä»£ç†å…·å¤‡é€»è¾‘æ­¥éª¤åˆ†å‰²ã€æ­¥éª¤è¯„åˆ†ã€è¯„åˆ†èšåˆå’Œé”™è¯¯æ ‘ç”Ÿæˆç­‰å››é¡¹å†…éƒ¨æ ¸å¿ƒæ“ä½œï¼Œä»¥åŠéš¾åº¦æ ¡å‡†ã€ç®€å•æ€§è¯„ä»·ã€å®Œæ•´æ€§éªŒè¯å’Œæ ¼å¼è¯„ä¼°ç­‰å››é¡¹å¤–éƒ¨æ‰©å±•æ¨¡å—ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŒ…å«1000ä¸ªåˆ†æ­¥è¿‡ç¨‹è¯„ä»·å®ä¾‹çš„StepMathBenchåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ¥æºäºæŒ‰é—®é¢˜ç±»å‹ã€å­¦ç§‘ç±»åˆ«å’Œéš¾åº¦æ°´å¹³åˆ†ç»„çš„é«˜è´¨é‡æ•°å­¦é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„StepMathAgentä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç°å‡ºä¸äººç±»è¯„ä¼°åå¥½ç›¸ç¬¦çš„è¯„ä»·å’Œå¹¿æ³›é€‚ç”¨çš„åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦èƒ½åŠ›æ˜¯è¡¡é‡å…¶æ•´ä½“æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆï¼Œå¯¼è‡´è¯„ä»·ä¸å‡†ç¡®ã€ä¸å¯è§£é‡Šï¼Œå¹¶ä¸”æ— æ³•è¯„ä¼°è¯æ˜æˆ–å¼€æ”¾æ€§é—®é¢˜ã€‚</li>
<li>StepMathAgentæ˜¯ä¸€ç§åŸºäºTree-of-Errorçš„æ–°å‹æ•°å­¦è¿‡ç¨‹è¯„ä»·ä»£ç†ï¼ŒåŒ…å«å†…éƒ¨æ ¸å¿ƒæ“ä½œå’Œå¤–éƒ¨æ‰©å±•æ¨¡å—ã€‚</li>
<li>StepMathAgentå…·å¤‡é€»è¾‘æ­¥éª¤åˆ†å‰²ã€æ­¥éª¤è¯„åˆ†ã€è¯„åˆ†èšåˆå’Œé”™è¯¯æ ‘ç”Ÿæˆç­‰åŠŸèƒ½ã€‚</li>
<li>StepMathBenchåŸºå‡†æµ‹è¯•åŒ…å«1000ä¸ªåˆ†æ­¥è¿‡ç¨‹è¯„ä»·å®ä¾‹ï¼Œæºäºé«˜è´¨é‡æ•°å­¦é—®é¢˜ï¼Œå¹¶æŒ‰é—®é¢˜ç±»å‹ã€å­¦ç§‘å’Œéš¾åº¦åˆ†ç»„ã€‚</li>
<li>å®éªŒè¡¨æ˜StepMathAgentåœ¨è¯„ä¼°æ•°å­¦è¿‡ç¨‹æ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œä¸”ä¸äººç±»è¯„ä¼°åå¥½ç›¸ç¬¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3e1525e717cf9abdfcb455b4eded9200.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e24d7ca2ec0736b75d28c6bc857bfaa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3738d36dda85d6c47fb44557e3be3ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d69f774712d310c3fc8f05350364bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7548979ad8fa26c414a6fcffaf900a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa88e64b23ca77d2b7de3a37ba38eef1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Revisiting-Multi-Agent-Asynchronous-Online-Optimization-with-Delays-the-Strongly-Convex-Case"><a href="#Revisiting-Multi-Agent-Asynchronous-Online-Optimization-with-Delays-the-Strongly-Convex-Case" class="headerlink" title="Revisiting Multi-Agent Asynchronous Online Optimization with Delays: the   Strongly Convex Case"></a>Revisiting Multi-Agent Asynchronous Online Optimization with Delays: the   Strongly Convex Case</h2><p><strong>Authors:Lingchan Bao, Tong Wei, Yuanyu Wan</strong></p>
<p>We revisit multi-agent asynchronous online optimization with delays, where only one of the agents becomes active for making the decision at each round, and the corresponding feedback is received by all the agents after unknown delays. Although previous studies have established an $O(\sqrt{dT})$ regret bound for this problem, they assume that the maximum delay $d$ is knowable or the arrival order of feedback satisfies a special property, which may not hold in practice. In this paper, we surprisingly find that when the loss functions are strongly convex, these assumptions can be eliminated, and the existing regret bound can be significantly improved to $O(d\log T)$ meanwhile. Specifically, to exploit the strong convexity of functions, we first propose a delayed variant of the classical follow-the-leader algorithm, namely FTDL, which is very simple but requires the full information of functions as feedback. Moreover, to handle the more general case with only the gradient feedback, we develop an approximate variant of FTDL by combining it with surrogate loss functions. Experimental results show that the approximate FTDL outperforms the existing algorithm in the strongly convex case. </p>
<blockquote>
<p>æˆ‘ä»¬é‡æ–°ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“å¼‚æ­¥åœ¨çº¿ä¼˜åŒ–é—®é¢˜ï¼Œè¯¥é—®é¢˜ä¸­å­˜åœ¨å»¶è¿Ÿã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œåªæœ‰ä¸€ä¸ªæ™ºèƒ½ä½“è¢«æ¿€æ´»ä»¥åšå‡ºå†³ç­–ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“åœ¨æœªçŸ¥å»¶è¿Ÿåæ¥æ”¶åˆ°ç›¸åº”çš„åé¦ˆã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²ç»ä¸ºè¿™ä¸ªé—®é¢˜å»ºç«‹äº†O(âˆšdT)çš„é—æ†¾ç•Œé™ï¼Œä½†ä»–ä»¬å‡è®¾æœ€å¤§å»¶è¿Ÿdæ˜¯å¯çŸ¥çš„ï¼Œæˆ–è€…åé¦ˆåˆ°è¾¾é¡ºåºæ»¡è¶³ç‰¹æ®Šå±æ€§ï¼Œè¿™åœ¨å®è·µä¸­å¯èƒ½ä¸æˆç«‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æƒŠè®¶åœ°å‘ç°å½“æŸå¤±å‡½æ•°æ˜¯å¼ºå‡¸çš„æ—¶å€™ï¼Œè¿™äº›å‡è®¾å¯ä»¥è¢«æ¶ˆé™¤ï¼Œå¹¶ä¸”ç°æœ‰çš„é—æ†¾ç•Œé™å¯ä»¥æ˜¾è‘—æ”¹å–„åˆ°O(dlogT)ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†åˆ©ç”¨å‡½æ•°çš„å¼ºå‡¸æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ç»å…¸è·Ÿéšé¢†å¯¼è€…ç®—æ³•çš„å»¶è¿Ÿå˜ä½“ï¼Œå³FTDLï¼Œå®ƒéå¸¸ç®€å•ï¼Œä½†éœ€è¦å‡½æ•°çš„å…¨éƒ¨ä¿¡æ¯ä½œä¸ºåé¦ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†åªæœ‰æ¢¯åº¦åé¦ˆçš„æ›´ä¸€èˆ¬æƒ…å†µï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆæ›¿ä»£æŸå¤±å‡½æ•°ï¼Œå¼€å‘äº†FTDLçš„è¿‘ä¼¼å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿‘ä¼¼FTDLåœ¨å¼ºå‡¸æƒ…å†µä¸‹ä¼˜äºç°æœ‰ç®—æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10013v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“å¼‚æ­¥åœ¨çº¿ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­åªæœ‰ä¸€åæ™ºèƒ½ä½“åœ¨æ¯ä¸ªå›åˆä¸­æ´»è·ƒä»¥åšå‡ºå†³ç­–ï¼Œæ‰€æœ‰æ™ºèƒ½ä½“åœ¨æœªçŸ¥å»¶è¿Ÿåéƒ½ä¼šæ”¶åˆ°åé¦ˆã€‚å°½ç®¡å…ˆå‰çš„ç ”ç©¶ä¸ºè¯¥é—®é¢˜å»ºç«‹äº†O(âˆšdT)çš„åæ‚”ç•Œï¼Œä½†å®ƒä»¬å‡è®¾æœ€å¤§å»¶è¿Ÿdæ˜¯å¯çŸ¥é“çš„æˆ–åé¦ˆåˆ°è¾¾é¡ºåºæ»¡è¶³ç‰¹æ®Šå±æ€§ï¼Œè¿™å¯èƒ½ä¸ç¬¦åˆå®é™…æƒ…å†µã€‚æœ¬æ–‡æƒŠå¥‡åœ°å‘ç°ï¼Œå½“æŸå¤±å‡½æ•°æ˜¯å¼ºå‡¸æ—¶ï¼Œå¯ä»¥æ¶ˆé™¤è¿™äº›å‡è®¾ï¼Œå¹¶å°†ç°æœ‰åæ‚”ç•Œæ˜¾è‘—æ”¹å–„è‡³O(dlogT)ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸ºäº†åˆ©ç”¨å‡½æ•°çš„å¼ºå‡¸æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»å…¸è·Ÿéšé¢†å…ˆè€…ç®—æ³•çš„å»¶è¿Ÿå˜ä½“ï¼Œå³FTDLï¼Œå®ƒéå¸¸ç®€å•ä½†éœ€è¦å‡½æ•°çš„å®Œæ•´ä¿¡æ¯ä½œä¸ºåé¦ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†åªæœ‰æ¢¯åº¦åé¦ˆçš„æ›´ä¸€èˆ¬æƒ…å†µï¼Œæˆ‘ä»¬ç»“åˆæ›¿ä»£æŸå¤±å‡½æ•°å¼€å‘äº†FTDLçš„è¿‘ä¼¼å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿‘ä¼¼FTDLåœ¨å¼ºå‡¸æƒ…å†µä¸‹ä¼˜äºç°æœ‰ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“å¼‚æ­¥åœ¨çº¿ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­æ¶‰åŠå»¶è¿Ÿåé¦ˆå’Œåªæœ‰ä¸€ä¸ªæ™ºèƒ½ä½“åœ¨æ¯ä¸ªå›åˆä¸­æ´»è·ƒåšå†³ç­–çš„åœºæ™¯ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶ä¸ºè¿™ä¸€é—®é¢˜è®¾å®šäº†O(âˆšdT)çš„åæ‚”ç•Œï¼Œä½†å‡è®¾æœ€å¤§å»¶è¿Ÿdå¯çŸ¥æˆ–åé¦ˆåˆ°è¾¾é¡ºåºæœ‰ç‰¹æ®Šå±æ€§ï¼Œè¿™å¯èƒ½ä¸ç¬¦åˆå®é™…ã€‚</li>
<li>å½“æŸå¤±å‡½æ•°æ˜¯å¼ºå‡¸æ—¶ï¼Œå¯ä»¥æ¶ˆé™¤è¿™äº›å‡è®¾ï¼Œå¹¶å°†åæ‚”ç•Œæ”¹å–„è‡³O(dlogT)ã€‚</li>
<li>æå‡ºäº†ç»å…¸è·Ÿéšé¢†å…ˆè€…ç®—æ³•çš„å»¶è¿Ÿå˜ä½“FTDLï¼Œè¯¥ç®—æ³•ç®€å•ä½†è¦æ±‚åé¦ˆåŒ…å«å‡½æ•°çš„å®Œæ•´ä¿¡æ¯ã€‚</li>
<li>ä¸ºäº†å¤„ç†ä»…æœ‰æ¢¯åº¦åé¦ˆçš„æƒ…å†µï¼Œç»“åˆäº†æ›¿ä»£æŸå¤±å‡½æ•°æ¥å¼€å‘FTDLçš„è¿‘ä¼¼å˜ä½“ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿‘ä¼¼FTDLåœ¨å¼ºå‡¸æƒ…å†µä¸‹è¡¨ç°ä¼˜äºç°æœ‰ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ce69f8bbe035238b040c68365deb13ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a27e5b73d4550f50872114877f2dd77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09375b337e25fa3346d537a907c2ea44.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PCLA-A-Framework-for-Testing-Autonomous-Agents-in-the-CARLA-Simulator"><a href="#PCLA-A-Framework-for-Testing-Autonomous-Agents-in-the-CARLA-Simulator" class="headerlink" title="PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator"></a>PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator</h2><p><strong>Authors:Masoud Jamshidiyan Tehrani, Jinhan Kim, Paolo Tonella</strong></p>
<p>Recent research on testing autonomous driving agents has grown significantly, especially in simulation environments. The CARLA simulator is often the preferred choice, and the autonomous agents from the CARLA Leaderboard challenge are regarded as the best-performing agents within this environment. However, researchers who test these agents, rather than training their own ones from scratch, often face challenges in utilizing them within customized test environments and scenarios. To address these challenges, we introduce PCLA (Pretrained CARLA Leaderboard Agents), an open-source Python testing framework that includes nine high-performing pre-trained autonomous agents from the Leaderboard challenges. PCLA is the first infrastructure specifically designed for testing various autonomous agents in arbitrary CARLA environments&#x2F;scenarios. PCLA provides a simple way to deploy Leaderboard agents onto a vehicle without relying on the Leaderboard codebase, it allows researchers to easily switch between agents without requiring modifications to CARLA versions or programming environments, and it is fully compatible with the latest version of CARLA while remaining independent of the Leaderboardâ€™s specific CARLA version. PCLA is publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/MasoudJTehrani/PCLA">https://github.com/MasoudJTehrani/PCLA</a>. </p>
<blockquote>
<p>è¿‘æœŸå…³äºæµ‹è¯•è‡ªåŠ¨é©¾é©¶ä»£ç†çš„ç ”ç©¶å·²æ˜¾è‘—å¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¿çœŸç¯å¢ƒä¸­ã€‚CARLAæ¨¡æ‹Ÿå™¨é€šå¸¸æ˜¯é¦–é€‰ï¼ŒCARLAæ’è¡Œæ¦œæŒ‘æˆ˜ä¸­çš„è‡ªä¸»ä»£ç†è¢«è®¤ä¸ºæ˜¯åœ¨æ­¤ç¯å¢ƒä¸­è¡¨ç°æœ€å¥½çš„ä»£ç†ã€‚ç„¶è€Œï¼Œç ”ç©¶äººå‘˜åœ¨æµ‹è¯•è¿™äº›ä»£ç†è€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒå®ƒä»¬æ—¶ï¼Œå¸¸å¸¸é¢ä¸´åœ¨è‡ªå®šä¹‰æµ‹è¯•ç¯å¢ƒå’Œåœºæ™¯ä¸­ä½¿ç”¨å®ƒä»¬çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PCLAï¼ˆé¢„è®­ç»ƒCARLAæ’è¡Œæ¦œä»£ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºPythonæµ‹è¯•æ¡†æ¶ï¼ŒåŒ…å«æ¥è‡ªæ’è¡Œæ¦œæŒ‘æˆ˜çš„ä¹ä¸ªé«˜æ€§èƒ½é¢„è®­ç»ƒè‡ªä¸»ä»£ç†ã€‚PCLAæ˜¯ä¸“é—¨ä¸ºåœ¨ä»»æ„CARLAç¯å¢ƒ&#x2F;åœºæ™¯ä¸­æµ‹è¯•å„ç§è‡ªä¸»ä»£ç†è€Œè®¾è®¡çš„åŸºç¡€è®¾æ–½ã€‚PCLAæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•å°†æ’è¡Œæ¦œä»£ç†éƒ¨ç½²åˆ°è½¦è¾†ä¸Šï¼Œè€Œæ— éœ€ä¾èµ–æ’è¡Œæ¦œçš„ä»£ç åº“ï¼Œå®ƒå…è®¸ç ”ç©¶äººå‘˜è½»æ¾åœ°åœ¨ä¸ä¿®æ”¹CARLAç‰ˆæœ¬æˆ–ç¼–ç¨‹ç¯å¢ƒçš„æƒ…å†µä¸‹åˆ‡æ¢ä»£ç†ï¼Œå¹¶ä¸”å®ƒä¸æœ€æ–°ç‰ˆæœ¬çš„CARLAå®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶ç‹¬ç«‹äºæ’è¡Œæ¦œçš„ç‰¹å®šCARLAç‰ˆæœ¬ã€‚PCLAå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MasoudJTehrani/PCLA">https://github.com/MasoudJTehrani/PCLA</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09385v2">PDF</a> This work will be published at the FSE 2025 demonstration track</p>
<p><strong>Summary</strong></p>
<p>CARLAæ¨¡æ‹Ÿå™¨ä¸­çš„è‡ªä¸»é©¾é©¶ä»£ç†æµ‹è¯•ç ”ç©¶æ—¥ç›Šå¢å¤šï¼Œä½†ç ”ç©¶è€…åœ¨ä½¿ç”¨å®šåˆ¶æµ‹è¯•ç¯å¢ƒå’Œåœºæ™¯æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ¨å‡ºäº†PCLAï¼ˆé¢„è®­ç»ƒCARLAé¢†å¯¼è€…ä»£ç†ï¼‰æµ‹è¯•æ¡†æ¶ï¼ŒåŒ…å«ä¹ä¸ªé«˜æ€§èƒ½é¢„è®­ç»ƒè‡ªä¸»ä»£ç†ï¼Œå¯åœ¨ä»»æ„CARLAç¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•ã€‚PCLAæä¾›äº†ç®€å•çš„æ–¹æ³•éƒ¨ç½²é¢†å¯¼è€…ä»£ç†åˆ°è½¦è¾†ä¸Šï¼Œæ— éœ€ä¾èµ–é¢†å¯¼è€…ä»£ç åº“ï¼Œå¹¶å…è®¸ç ”ç©¶è€…è½»æ¾åˆ‡æ¢ä»£ç†ï¼ŒåŒæ—¶ä¸æœ€æ–°ç‰ˆæœ¬çš„CARLAå…¼å®¹ï¼Œç‹¬ç«‹äºé¢†å¯¼è€…ç‰¹å®šçš„CARLAç‰ˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€è¿‘å…³äºåœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸­æµ‹è¯•è‡ªä¸»é©¾é©¶ä»£ç†çš„ç ”ç©¶æ˜¾è‘—å¢é•¿ã€‚</li>
<li>ç ”ç©¶è€…åœ¨å®šåˆ¶æµ‹è¯•ç¯å¢ƒå’Œåœºæ™¯ä¸­ä½¿ç”¨ç°æœ‰ä»£ç†æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>PCLAæ˜¯ä¸€ä¸ªä¸ºæµ‹è¯•å„ç§è‡ªä¸»ä»£ç†è€Œè®¾è®¡çš„æµ‹è¯•æ¡†æ¶ã€‚</li>
<li>PCLAåŒ…å«ä¹ä¸ªæ¥è‡ªé¢†å¯¼è€…æ’è¡Œæ¦œçš„é«˜æ€§èƒ½é¢„è®­ç»ƒè‡ªä¸»ä»£ç†ã€‚</li>
<li>PCLAæä¾›äº†ç®€å•çš„æ–¹æ³•éƒ¨ç½²ä»£ç†åˆ°è½¦è¾†ä¸Šï¼Œæ— éœ€ä¾èµ–é¢†å¯¼è€…ä»£ç åº“ã€‚</li>
<li>PCLAå…è®¸ç ”ç©¶è€…è½»æ¾åˆ‡æ¢ä»£ç†ï¼Œæ— éœ€æ›´æ”¹CARLAç‰ˆæœ¬æˆ–ç¼–ç¨‹ç¯å¢ƒã€‚</li>
<li>PCLAä¸æœ€æ–°ç‰ˆæœ¬çš„CARLAå…¼å®¹ï¼Œå¹¶ä¸”ç‹¬ç«‹äºé¢†å¯¼è€…ç‰¹å®šçš„CARLAç‰ˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8861956140edd9f2fc7648554bda48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3908ed5d8b04c177ae0b9e3044720a03.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-to-Detect-Objects-from-Multi-Agent-LiDAR-Scans-without-Manual-Labels"><a href="#Learning-to-Detect-Objects-from-Multi-Agent-LiDAR-Scans-without-Manual-Labels" class="headerlink" title="Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual   Labels"></a>Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual   Labels</h2><p><strong>Authors:Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen Dong, Cheng Wang, Chenglu Wen</strong></p>
<p>Unsupervised 3D object detection serves as an important solution for offline 3D object annotation. However, due to the data sparsity and limited views, the clustering-based label fitting in unsupervised object detection often generates low-quality pseudo-labels. Multi-agent collaborative dataset, which involves the sharing of complementary observations among agents, holds the potential to break through this bottleneck. In this paper, we introduce a novel unsupervised method that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA, without using labels from external. DOtA first uses the internally shared ego-pose and ego-shape of collaborative agents to initialize the detector, leveraging the generalization performance of neural networks to infer preliminary labels. Subsequently,DOtA uses the complementary observations between agents to perform multi-scale encoding on preliminary labels, then decodes high-quality and low-quality labels. These labels are further used as prompts to guide a correct feature learning process, thereby enhancing the performance of the unsupervised object detection task. Extensive experiments on the V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art unsupervised 3D object detection methods. Additionally, we also validate the effectiveness of the DOtA labels under various collaborative perception frameworks.The code is available at <a target="_blank" rel="noopener" href="https://github.com/xmuqimingxia/DOtA">https://github.com/xmuqimingxia/DOtA</a>. </p>
<blockquote>
<p>æ— ç›‘ç£çš„3Dç›®æ ‡æ£€æµ‹æ˜¯ç¦»çº¿3Dç›®æ ‡æ ‡æ³¨çš„é‡è¦è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®ç¨€ç–å’Œè§†å›¾æœ‰é™ï¼Œæ— ç›‘ç£ç›®æ ‡æ£€æµ‹ä¸­çš„åŸºäºèšç±»çš„æ ‡ç­¾æ‹Ÿåˆé€šå¸¸ä¼šäº§ç”Ÿä½è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚å¤šæ™ºèƒ½ä½“ååŒæ•°æ®é›†æ¶‰åŠæ™ºèƒ½ä½“ä¹‹é—´çš„äº’è¡¥è§‚å¯Ÿå…±äº«ï¼Œå…·æœ‰çªç ´è¿™ä¸€ç“¶é¢ˆçš„æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ— ç›‘ç£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»å¤šæ™ºèƒ½ä½“æ¿€å…‰é›·è¾¾æ‰«æä¸­å­¦ä¹ æ£€æµ‹ç›®æ ‡ï¼Œç§°ä¸ºDOtAï¼Œæ— éœ€ä½¿ç”¨æ¥è‡ªå¤–éƒ¨æ ‡ç­¾ã€‚DOtAé¦–å…ˆä½¿ç”¨æ™ºèƒ½ä½“ä¹‹é—´å…±äº«çš„å†…ç½®è‡ªæˆ‘å§¿åŠ¿å’Œè‡ªæˆ‘å½¢çŠ¶æ¥åˆå§‹åŒ–æ£€æµ‹å™¨ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œçš„æ³›åŒ–æ€§èƒ½æ¥æ¨æ–­åˆæ­¥æ ‡ç­¾ã€‚éšåï¼ŒDOtAåˆ©ç”¨æ™ºèƒ½ä½“ä¹‹é—´çš„äº’è¡¥è§‚å¯Ÿå¯¹åˆæ­¥æ ‡ç­¾æ‰§è¡Œå¤šå°ºåº¦ç¼–ç ï¼Œç„¶åè§£ç é«˜è´¨é‡å’Œä½è´¨é‡æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾è¿›ä¸€æ­¥ä½œä¸ºæç¤ºæ¥å¼•å¯¼æ­£ç¡®çš„ç‰¹å¾å­¦ä¹ è¿‡ç¨‹ï¼Œä»è€Œæé«˜æ— ç›‘ç£ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨V2V4Realå’ŒOPV2Væ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DOtAä¼˜äºæœ€æ–°çš„æ— ç›‘ç£3Dç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éªŒè¯äº†åœ¨ä¸åŒååŒæ„ŸçŸ¥æ¡†æ¶ä¸‹DOtAæ ‡ç­¾çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmuqimingxia/DOtA">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08421v2">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong><br>åŸºäºå¤šæ™ºèƒ½ä½“ååŒæ•°æ®é›†çš„æ— äººç›‘ç£3Dç›®æ ‡æ£€æµ‹é€šè¿‡åˆ©ç”¨æ™ºèƒ½ä½“é—´çš„äº’è¡¥è§‚å¯Ÿçªç ´äº†æ£€æµ‹ç“¶é¢ˆã€‚DOtAæ–¹æ³•é€šè¿‡å…±äº«æ™ºèƒ½ä½“çš„å†…éƒ¨å§¿æ€å’Œå½¢çŠ¶è¿›è¡Œåˆæ­¥æ ‡ç­¾æ¨æ–­ï¼Œå¹¶åˆ©ç”¨æ™ºèƒ½ä½“é—´çš„äº’è¡¥è§‚å¯Ÿè¿›è¡Œå¤šå°ºåº¦ç¼–ç ï¼Œç”Ÿæˆé«˜è´¨é‡å’Œä½è´¨é‡æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾è¿›ä¸€æ­¥å¼•å¯¼ç‰¹å¾å­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜äº†æ— äººç›‘ç£çš„3Dç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚åœ¨V2V4Realå’ŒOPV2Væ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒDOtAä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ™ºèƒ½ä½“ååŒæ•°æ®é›†é€šè¿‡æ™ºèƒ½ä½“é—´çš„äº’è¡¥è§‚å¯Ÿå¢å¼ºæ— äººç›‘ç£çš„3Dç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>DOtAæ–¹æ³•ä½¿ç”¨æ™ºèƒ½ä½“çš„å†…éƒ¨å…±äº«å§¿æ€å’Œå½¢çŠ¶åˆå§‹åŒ–æ£€æµ‹å™¨ã€‚</li>
<li>åˆ©ç”¨æ™ºèƒ½ä½“é—´çš„äº’è¡¥è§‚å¯Ÿè¿›è¡Œå¤šå°ºåº¦ç¼–ç ç”Ÿæˆæ ‡ç­¾ã€‚</li>
<li>ç”Ÿæˆçš„é«˜è´¨é‡æ ‡ç­¾å¼•å¯¼ç‰¹å¾å­¦ä¹ è¿‡ç¨‹ä»¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜DOtAæ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>DOtAçš„ä»£ç å·²ç»å…¬å¼€ï¼Œå¯ä¾›ä»–äººä½¿ç”¨å‚è€ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-32caff3b4ab5bb1e070b230871544550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aa4d22053df44aa1ac61d28bc9c00d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8738656748f8fde35a152afd7e259035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f03a725c95b78969a7897ee1fcaa0a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6e65903b919e1bc829ce5a79d01ea7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="KG4Diagnosis-A-Hierarchical-Multi-Agent-LLM-Framework-with-Knowledge-Graph-Enhancement-for-Medical-Diagnosis"><a href="#KG4Diagnosis-A-Hierarchical-Multi-Agent-LLM-Framework-with-Knowledge-Graph-Enhancement-for-Medical-Diagnosis" class="headerlink" title="KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge   Graph Enhancement for Medical Diagnosis"></a>KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge   Graph Enhancement for Medical Diagnosis</h2><p><strong>Authors:Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio</strong></p>
<p>Integrating Large Language Models (LLMs) in healthcare diagnosis demands systematic frameworks that can handle complex medical scenarios while maintaining specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent framework that combines LLMs with automated knowledge graph construction, encompassing 362 common diseases across medical specialties. Our framework mirrors real-world medical systems through a two-tier architecture: a general practitioner (GP) agent for initial assessment and triage, coordinating with specialized agents for in-depth diagnosis in specific domains. The core innovation lies in our end-to-end knowledge graph generation methodology, incorporating: (1) semantic-driven entity and relation extraction optimized for medical terminology, (2) multi-dimensional decision relationship reconstruction from unstructured medical texts, and (3) human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized medical diagnosis systems, with capabilities to incorporate new diseases and medical knowledge. The frameworkâ€™s modular design enables seamless integration of domain-specific enhancements, making it valuable for developing targeted medical diagnosis systems. We provide architectural guidelines and protocols to facilitate adoption across medical contexts. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•´åˆåˆ°åŒ»ç–—è¯Šæ–­ä¸­ï¼Œéœ€è¦èƒ½å¤Ÿå¤„ç†å¤æ‚åŒ»ç–—åœºæ™¯å¹¶ä¿æŒä¸“ä¸šçŸ¥è¯†çš„ç³»ç»Ÿæ€§æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†KG4Diagnosisï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åˆ†å±‚å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒå°†LLMsä¸è‡ªåŠ¨åŒ–çŸ¥è¯†å›¾è°±æ„å»ºç›¸ç»“åˆï¼Œæ¶µç›–362ç§å¸¸è§ç–¾ç—…ï¼Œæ¶‰åŠåŒ»å­¦å„ä¸“ä¸šé¢†åŸŸã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¸¤å±‚æ¶æ„åæ˜ ç°å®åŒ»ç–—ç³»ç»Ÿï¼šä¸€å±‚æ˜¯é€šç”¨å®è·µè€…ï¼ˆGPï¼‰ä»£ç†è¿›è¡Œåˆæ­¥è¯„ä¼°å’Œåˆ†ç±»ï¼Œå¦ä¸€å±‚æ˜¯ä¸ä¸“ä¸šé¢†åŸŸæ·±å…¥è¯Šæ–­çš„ä¸“é—¨ä»£ç†åè°ƒã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºæˆ‘ä»¬ç«¯åˆ°ç«¯çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹åŒ»å­¦æœ¯è¯­ä¼˜åŒ–çš„è¯­ä¹‰é©±åŠ¨å®ä½“å’Œå…³ç³»æå–ï¼Œï¼ˆ2ï¼‰ä»éç»“æ„åŒ–çš„åŒ»ç–—æ–‡æœ¬ä¸­é‡å»ºå¤šç»´å†³ç­–å…³ç³»ï¼Œï¼ˆ3ï¼‰ç”¨äºçŸ¥è¯†æ‰©å±•çš„äººæœºååŒæ¨ç†ã€‚KG4Diagnosisä½œä¸ºä¸€ä¸ªå¯æ‰©å±•çš„åŸºç¡€ï¼Œä¸ºä¸“ä¸šåŒ»ç–—è¯Šæ–­ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„æ”¯æŒï¼Œæœ‰èƒ½åŠ›èå…¥æ–°çš„ç–¾ç—…å’ŒåŒ»å­¦çŸ¥è¯†ã€‚è¯¥æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å¾—èƒ½å¤Ÿæ— ç¼é›†æˆç‰¹å®šé¢†åŸŸçš„å¢å¼ºåŠŸèƒ½ï¼Œå› æ­¤å¯¹äºå¼€å‘æœ‰é’ˆå¯¹æ€§çš„åŒ»ç–—è¯Šæ–­ç³»ç»Ÿå…·æœ‰å¾ˆé«˜çš„ä»·å€¼ã€‚æˆ‘ä»¬æä¾›äº†æ¶æ„æŒ‡å—å’Œåè®®ï¼Œä»¥æ¨åŠ¨å…¶åœ¨å„ç§åŒ»ç–—ç¯å¢ƒä¸­çš„é‡‡ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16833v3">PDF</a> 10 pages,5 figures,published to AAAI-25 Bridge Program</p>
<p><strong>Summary</strong>ï¼šKG4Diagnosisæ˜¯ä¸€ä¸ªç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè‡ªåŠ¨åŒ–çŸ¥è¯†å›¾è°±æ„å»ºçš„æ–°å‹åˆ†çº§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåŒ»ç–—æœåŠ¡ä¸­çš„è¯Šæ–­å·¥ä½œã€‚å®ƒèƒ½å¤Ÿå¤„ç†å¤æ‚çš„åŒ»ç–—åœºæ™¯å¹¶ç»´æŒä¸“ä¸šæ€§çš„çŸ¥è¯†ï¼Œæ¶µç›–äº†åŒ»å­¦é¢†åŸŸçš„362ç§å¸¸è§ç–¾ç—…ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤å±‚ç»“æ„æ¥æ¨¡æ‹ŸçœŸå®çš„åŒ»ç–—ç³»ç»Ÿï¼šåˆçº§åŒ»ç”Ÿè¿›è¡Œåˆæ­¥è¯„ä¼°å’Œåˆ†çº§è¯Šç–—ï¼Œå¹¶ä¸ä¸“ç§‘æ™ºèƒ½ä½“åè°ƒè¿›è¡Œç‰¹å®šé¢†åŸŸçš„æ·±åº¦è¯Šæ–­ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå…¶ç«¯åˆ°ç«¯çš„çŸ¥è¯†å›¾è°±ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬é¢å‘åŒ»å­¦æœ¯è¯­çš„è¯­ä¹‰é©±åŠ¨å®ä½“å’Œå…³ç³»æå–ã€ä»éç»“æ„åŒ–åŒ»ç–—æ–‡æœ¬ä¸­é‡å»ºå¤šç»´å†³ç­–å…³ç³»ä»¥åŠåŸºäºäººå·¥æ¨ç†çš„çŸ¥è¯†æ‰©å±•ã€‚å®ƒä¸ºä¸“ä¸šåŒ»ç–—è¯Šæ–­ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„åŸºç¡€ï¼Œèƒ½å¤Ÿèå…¥æ–°çš„ç–¾ç—…å’ŒåŒ»å­¦çŸ¥è¯†ã€‚æ¡†æ¶çš„æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæ— ç¼é›†æˆç‰¹å®šé¢†åŸŸçš„æ”¹è¿›ï¼Œå¯¹äºå¼€å‘æœ‰é’ˆå¯¹æ€§çš„åŒ»ç–—è¯Šæ–­ç³»ç»Ÿå…·æœ‰é‡è¦ä»·å€¼ã€‚æˆ‘ä»¬æä¾›äº†è·¨åŒ»ç–—ç¯å¢ƒçš„é‡‡ç”¨æŒ‡å—å’Œåè®®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>KG4Diagnosisæ˜¯ä¸€ä¸ªé›†æˆLLMå’Œè‡ªåŠ¨åŒ–çŸ¥è¯†å›¾è°±çš„æ–°å‹è¯Šæ–­æ¡†æ¶ï¼Œæ¶µç›–å¤šç§å¸¸è§ç–¾ç—…ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤å±‚ç»“æ„ï¼Œåˆçº§åŒ»ç”Ÿè¿›è¡Œåˆæ­¥è¯„ä¼°å¹¶ä¸ä¸“ç§‘æ™ºèƒ½ä½“åè°ƒæ·±åº¦è¯Šæ–­ã€‚</li>
<li>KG4Diagnosisçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶çŸ¥è¯†å›¾è°±ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬è¯­ä¹‰é©±åŠ¨çš„å®ä½“å’Œå…³ç³»æå–ã€å¤šç»´å†³ç­–å…³ç³»é‡å»ºå’ŒåŸºäºäººå·¥æ¨ç†çš„çŸ¥è¯†æ‰©å±•ã€‚</li>
<li>æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯èå…¥æ–°çš„ç–¾ç—…å’ŒåŒ»å­¦çŸ¥è¯†ï¼Œä¸”æ¨¡å—åŒ–è®¾è®¡å¯æ— ç¼é›†æˆç‰¹å®šé¢†åŸŸçš„æ”¹è¿›ã€‚</li>
<li>KG4Diagnosisä¸ºä¸“ä¸šåŒ»ç–—è¯Šæ–­ç³»ç»Ÿæä¾›äº†åŸºç¡€ï¼Œå¹¶æä¾›äº†è·¨åŒ»ç–—ç¯å¢ƒçš„é‡‡ç”¨æŒ‡å—å’Œåè®®ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºå¤æ‚çš„åŒ»ç–—åœºæ™¯ï¼Œå¹¶èƒ½å¤Ÿç»´æŒä¸“ä¸šæ€§çš„çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c3c92782c439aee2d8ad680bffdab538.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50afebfc586d931d790904ce420fc4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-144a2f441c16fa0f57c3081d6b171dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb7c4aecc4a233f64b49a8137451e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-336e50ee4a5e3968ea86e2a2fea37f14.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DataEnvGym-Data-Generation-Agents-in-Teacher-Environments-with-Student-Feedback"><a href="#DataEnvGym-Data-Generation-Agents-in-Teacher-Environments-with-Student-Feedback" class="headerlink" title="DataEnvGym: Data Generation Agents in Teacher Environments with Student   Feedback"></a>DataEnvGym: Data Generation Agents in Teacher Environments with Student   Feedback</h2><p><strong>Authors:Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal</strong></p>
<p>The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid, scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agentâ€™s goal is to improve student performance. Students are iteratively trained and evaluated on generated data, and their feedback (in the form of errors or weak skills) is reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 4 domains (math, code, VQA, and tool-use) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms. </p>
<blockquote>
<p>ç›®å‰ï¼Œåˆ›å»ºç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•™å­¦è®­ç»ƒæ•°æ®çš„è¿‡ç¨‹æ˜¯ç”±äººç±»é©±åŠ¨çš„ï¼Œäººç±»æ‰‹åŠ¨åˆ†ææ¨¡å‹çš„å¼±ç‚¹ï¼Œå¹¶è®¡åˆ’å¦‚ä½•åˆ›å»ºèƒ½å¤Ÿæ”¹è¿›å­¦ç”Ÿæ¨¡å‹çš„æ•°æ®ã€‚ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ³¨é‡Šè€…çš„æ–¹æ³•å‡å°‘äº†äººåŠ›æŠ•å…¥ï¼Œä½†ä»éœ€è¦äººç±»æ¥è§£é‡Šè¯„ä¼°åé¦ˆå¹¶æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ä»¥äº§ç”Ÿå­¦ç”Ÿæ‰€éœ€çš„æ•°æ®ã€‚é€šè¿‡åˆ›å»ºè‡ªä¸»æ•°æ®ç”Ÿæˆä»£ç†ï¼ˆæˆ–æ•™å¸ˆï¼‰æ¥è‡ªåŠ¨å®Œæˆè¿™ä¸€åŠ³åŠ¨å¯†é›†å‹è¿‡ç¨‹æ˜¯å¯å–çš„ï¼Œä½†è¿™éœ€è¦èƒ½å¤Ÿæ¨¡æ‹Ÿæ•°æ®åˆ›å»ºä¸­çš„åé¦ˆé©±åŠ¨ã€è¿­ä»£å’Œé—­ç¯çš„ç¯å¢ƒã€‚ä¸ºäº†å®ç°å¯¹è¿™ç±»ä»£ç†åŠå…¶æ¨¡å—è¿›è¡Œå¿«é€Ÿã€å¯æ‰©å±•çš„æµ‹è¯•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DataEnvGymï¼Œä¸€ä¸ªä¸ºæ•™å¸ˆç¯å¢ƒè®¾è®¡çš„æµ‹è¯•å¹³å°ã€‚DataEnvGymå°†æ•°æ®ç”Ÿæˆæ¡†æ¶è®¾å®šä¸ºä¸€é¡¹åºåˆ—å†³ç­–ä»»åŠ¡ï¼Œæ¶‰åŠæ•°æ®ç”Ÿæˆä»£ç†å†…çš„æ•°æ®ç”Ÿæˆç­–ç•¥å’Œç”Ÿæˆå¼•æ“ã€‚å…¶ä¸­æ•°æ®ç”Ÿæˆç­–ç•¥ç”¨äºåˆ¶å®šåˆ›å»ºè®­ç»ƒæ•°æ®çš„è®¡åˆ’ï¼Œè€Œç”Ÿæˆå¼•æ“åˆ™å°†è¯¥è®¡åˆ’è½¬åŒ–ä¸ºå®é™…æ•°æ®ã€‚ä»£ç†ä½äºä¸€ä¸ªæä¾›å­¦ç”Ÿåé¦ˆçš„ç¯å¢ƒä¸­ï¼Œå…¶ç›®æ ‡æ˜¯æé«˜å­¦ç”Ÿçš„è¡¨ç°ã€‚å­¦ç”Ÿåœ¨ç”Ÿæˆçš„è¿­ä»£æ•°æ®å’Œè¯„ä¼°ä¸­ä¸æ–­å­¦ä¹ å’Œè¿›æ­¥ï¼Œä»–ä»¬çš„åé¦ˆï¼ˆä»¥é”™è¯¯æˆ–æŠ€èƒ½ä¸è¶³çš„å½¢å¼ï¼‰ä¼šåœ¨æ¯æ¬¡è¿­ä»£åæŠ¥å‘Šç»™ä»£ç†ã€‚DataEnvGymåŒ…å«å¤šä¸ªæ•™å¸ˆç¯å¢ƒå®ä¾‹ï¼Œæ¶µç›–äº†çŠ¶æ€è¡¨ç¤ºå’Œè¡Œä¸ºç©ºé—´ä¸­çš„ä¸‰ä¸ªç»“æ„å±‚æ¬¡ã€‚æ›´ç»“æ„åŒ–çš„ç¯å¢ƒåŸºäºæ¨æ–­æŠ€èƒ½ï¼Œæä¾›æ›´å¤šçš„å¯è§£é‡Šæ€§å’Œè¯¾ç¨‹æ§åˆ¶ã€‚æˆ‘ä»¬æ”¯æŒå››ä¸ªé¢†åŸŸï¼ˆæ•°å­¦ã€ä»£ç ã€è§†è§‰é—®ç­”å’Œå·¥å…·ä½¿ç”¨ï¼‰ï¼Œå¹¶æµ‹è¯•å¤šä¸ªå­¦ç”Ÿå’Œæ•™å¸ˆã€‚åœ¨æˆ‘ä»¬çš„æ•™å­¦ç¯å¢ƒä¸­ï¼Œç¤ºä¾‹ä»£ç†å¯ä»¥è¿­ä»£åœ°æé«˜å­¦ç”Ÿçš„ä»»åŠ¡è¡¨ç°å’Œè®¾ç½®èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ç¯å¢ƒèƒ½å¤Ÿæ•™æˆä¸åŒæŠ€èƒ½æ°´å¹³å¹¶æµ‹è¯•å…³é”®æ¨¡å—çš„ä¸åŒå˜ä½“ï¼Œè¿™ä¸ºæœªæ¥æ”¹è¿›æ•°æ®ç”Ÿæˆä»£ç†ã€å¼•æ“å’Œåé¦ˆæœºåˆ¶æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06215v3">PDF</a> ICLR 2025 Spotlight; Project Page: <a target="_blank" rel="noopener" href="https://dataenvgym.github.io/">https://DataEnvGym.github.io</a></p>
<p><strong>æ‘˜è¦</strong><br>è®­ç»ƒæ•°æ®ç”Ÿæˆé‡‡ç”¨è‡ªåŠ¨æ–¹å¼å‡å°‘äººå·¥å¹²é¢„æ˜¯å½“å‰ç ”ç©¶çš„çƒ­ç‚¹ã€‚è¯¥æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨DataEnvGymæµ‹è¯•å¹³å°çš„æ–¹æ³•ï¼Œå°†è®­ç»ƒæ•°æ®ç”Ÿæˆè§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–ä»»åŠ¡ã€‚DataEnvGymæä¾›äº†ä¸€ä¸ªæ•™å¸ˆç¯å¢ƒç”¨äºæ•°æ®ç”Ÿæˆä»£ç†çš„æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬æ•°æ®ç”Ÿæˆç­–ç•¥å’Œç”Ÿæˆå¼•æ“ã€‚æ•°æ®ç”Ÿæˆç­–ç•¥è´Ÿè´£åˆ›å»ºè®­ç»ƒæ•°æ®çš„è®¡åˆ’ï¼Œè€Œç”Ÿæˆå¼•æ“è´Ÿè´£å°†æ•°æ®è½¬åŒ–ä¸ºå®é™…åº”ç”¨ã€‚ç¯å¢ƒé€šè¿‡å­¦ç”Ÿåé¦ˆæ¥è¯„ä¼°æ•°æ®è´¨é‡ï¼Œä»£ç†çš„ç›®æ ‡æ˜¯æé«˜å­¦ç”Ÿçš„è¡¨ç°ã€‚é€šè¿‡è¿­ä»£è®­ç»ƒå’Œè¯„ä¼°å­¦ç”Ÿç”Ÿæˆçš„é”™è¯¯æˆ–å¼±æŠ€èƒ½ï¼Œå°†åé¦ˆåé¦ˆç»™ä»£ç†ï¼Œä»¥ä¾¿ä¼˜åŒ–åç»­æ•°æ®ç”Ÿæˆç­–ç•¥ã€‚DataEnvGymåœ¨ä¸åŒç»“æ„å±‚æ¬¡çš„ç¯å¢ƒä¸­æä¾›äº†å¤šä¸ªæ•™å¸ˆç¯å¢ƒçš„å®ä¾‹ï¼Œæ›´ç»“æ„åŒ–çš„ç¯å¢ƒåŸºäºæ¨æ–­æŠ€èƒ½å¹¶æä¾›æ›´å¤šå¯è§£é‡Šæ€§å’Œè¯¾ç¨‹æ§åˆ¶ã€‚æ”¯æŒæ•°å­¦ã€ä»£ç ã€è§†è§‰é—®ç­”å’Œå·¥å…·ä½¿ç”¨ç­‰å››ä¸ªé¢†åŸŸï¼Œå¹¶æµ‹è¯•äº†å¤šä¸ªå­¦ç”Ÿå’Œæ•™å¸ˆã€‚åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œæ•™å¸ˆä»£ç†èƒ½å¤Ÿæé«˜å­¦ç”Ÿè·¨ä»»åŠ¡å’Œè®¾ç½®çš„è¡¨ç°ï¼Œå±•ç¤ºæœªæ¥æ”¹è¿›æ•°æ®ç”Ÿæˆç­–ç•¥çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®­ç»ƒæ•°æ®ç”Ÿæˆçš„è‡ªåŠ¨åŒ–æˆä¸ºè¿«åˆ‡éœ€æ±‚ï¼Œä»¥æ›¿ä»£ç¹ççš„äººåŠ›æ ‡æ³¨å·¥ä½œã€‚</li>
<li>DataEnvGymè¢«æå‡ºä½œä¸ºæ•°æ®ç”Ÿæˆä»£ç†çš„æµ‹è¯•å¹³å°ï¼Œæ¨¡ä»¿çœŸå®çš„åé¦ˆé—­ç¯ç³»ç»Ÿã€‚</li>
<li>è¯¥å¹³å°é€šè¿‡å°†æ•°æ®ç”Ÿæˆçœ‹ä½œä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜æ¥å¤„ç†ï¼Œæ¶‰åŠæ•°æ®ç”Ÿæˆç­–ç•¥å’Œç”Ÿæˆå¼•æ“ä¸¤éƒ¨åˆ†ã€‚</li>
<li>ç¯å¢ƒè®¾è®¡æ³¨é‡ç»“æ„æ€§å’Œè§£é‡Šæ€§ï¼Œå¯æ ¹æ®æ¨æ–­æŠ€èƒ½è¿›è¡Œè¯¾ç¨‹æ§åˆ¶ã€‚</li>
<li>æ”¯æŒå¤šç§é¢†åŸŸçš„æ•°æ®ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ•°å­¦ã€ä»£ç ã€è§†è§‰é—®ç­”å’Œå·¥å…·ä½¿ç”¨ç­‰ã€‚</li>
<li>æ•™å¸ˆä»£ç†èƒ½å¤Ÿåœ¨ä¸åŒä»»åŠ¡å’Œè®¾ç½®ä¸‹æé«˜å­¦ç”Ÿè¡¨ç°ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2bd54521a5842180b92c142c07967eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c732b42a9518a723635090284a6996d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1998015494ab96f758fcd00b5e7c9be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-894cac4e5088ea23529c9eaab6f26ecd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a635991a8b4cf7976333b7f6d9614af.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LaMMA-P-Generalizable-Multi-Agent-Long-Horizon-Task-Allocation-and-Planning-with-LM-Driven-PDDL-Planner"><a href="#LaMMA-P-Generalizable-Multi-Agent-Long-Horizon-Task-Allocation-and-Planning-with-LM-Driven-PDDL-Planner" class="headerlink" title="LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and   Planning with LM-Driven PDDL Planner"></a>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and   Planning with LM-Driven PDDL Planner</h2><p><strong>Authors:Xiaopan Zhang, Hao Qin, Fuquan Wang, Yue Dong, Jiachen Li</strong></p>
<p>Language models (LMs) possess a strong capability to comprehend natural language, making them effective in translating human instructions into detailed plans for simple robot tasks. Nevertheless, it remains a significant challenge to handle long-horizon tasks, especially in subtask identification and allocation for cooperative heterogeneous robot teams. To address this issue, we propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel multi-agent task planning framework that achieves state-of-the-art performance on long-horizon tasks. LaMMA-P integrates the strengths of the LMsâ€™ reasoning capability and the traditional heuristic search planner to achieve a high success rate and efficiency while demonstrating strong generalization across tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that features household tasks with two different levels of complexity based on the AI2-THOR environment. The experimental results demonstrate that LaMMA-P achieves a 105% higher success rate and 36% higher efficiency than existing LM-based multiagent planners. The experimental videos, code, datasets, and detailed prompts used in each module can be found on the project website: <a target="_blank" rel="noopener" href="https://lamma-p.github.io/">https://lamma-p.github.io</a>. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰å…·æœ‰å¾ˆå¼ºçš„ç†è§£è‡ªç„¶è¯­è¨€çš„èƒ½åŠ›ï¼Œä½¿å¾—å®ƒä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆç®€å•çš„æœºå™¨äººä»»åŠ¡çš„è¯¦ç»†è®¡åˆ’ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†é•¿æœŸä»»åŠ¡æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«åˆ†é…åˆä½œå‹å¼‚æ„æœºå™¨äººå›¢é˜Ÿçš„å­ä»»åŠ¡æ–¹é¢ï¼Œä»ç„¶å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“PDDLè§„åˆ’å™¨ï¼ˆLaMMA-Pï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“ä»»åŠ¡è§„åˆ’æ¡†æ¶ï¼Œåœ¨é•¿æœŸä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚LaMMA-Pç»“åˆäº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¼ ç»Ÿçš„å¯å‘å¼æœç´¢è§„åˆ’å™¨çš„ä¼˜ç‚¹ï¼Œä»¥å®ç°é«˜æˆåŠŸç‡å’Œæ•ˆç‡ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MAT-THORåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŸºäºAI2-THORç¯å¢ƒï¼Œä»¥å®¶åº­ä»»åŠ¡ä¸ºç‰¹è‰²ï¼Œåˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„å¤æ‚åº¦çº§åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMMA-Pä¸ç°æœ‰çš„åŸºäºLMçš„å¤šæ™ºèƒ½ä½“è§„åˆ’å™¨ç›¸æ¯”ï¼ŒæˆåŠŸç‡æé«˜äº†105%ï¼Œæ•ˆç‡æé«˜äº†36%ã€‚å®éªŒè§†é¢‘ã€ä»£ç ã€æ•°æ®é›†å’Œæ¯ä¸ªæ¨¡å—ä¸­ä½¿ç”¨çš„è¯¦ç»†æç¤ºå¯ä»¥åœ¨é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://lamma-p.github.io/">https://lamma-p.github.io</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20560v2">PDF</a> IEEE Conference on Robotics and Automation (ICRA 2025); Project   website: <a target="_blank" rel="noopener" href="https://lamma-p.github.io/">https://lamma-p.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLaMMA-Pçš„æ–°å‹å¤šæ™ºèƒ½ä½“ä»»åŠ¡è§„åˆ’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¼ ç»Ÿå¯å‘å¼æœç´¢è§„åˆ’å™¨ï¼Œå®ç°äº†é•¿å‘¨æœŸä»»åŠ¡çš„é«˜æˆåŠŸç‡å’Œæ•ˆç‡ã€‚åŒæ—¶ï¼Œä¸ºäº†è¯„ä¼°è¯¥æ¡†æ¶çš„æ€§èƒ½ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸ºMAT-THORçš„åŸºå‡†æµ‹è¯•ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMMA-Pç›¸è¾ƒäºç°æœ‰çš„åŸºäºè¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“è§„åˆ’å™¨ï¼ŒæˆåŠŸç‡æé«˜äº†105%ï¼Œæ•ˆç‡æé«˜äº†36%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¯å°†å…¶åº”ç”¨äºæœºå™¨äººä»»åŠ¡çš„ç¿»è¯‘å’Œè§„åˆ’ã€‚</li>
<li>å¤„ç†é•¿å‘¨æœŸä»»åŠ¡é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å­ä»»åŠ¡çš„è¯†åˆ«å’Œåˆ†é…ã€‚</li>
<li>LaMMA-Pæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ™ºèƒ½ä½“ä»»åŠ¡è§„åˆ’æ¡†æ¶ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œå¯å‘å¼æœç´¢è§„åˆ’å™¨çš„ä¼˜åŠ¿ã€‚</li>
<li>LaMMA-Påœ¨åŸºå‡†æµ‹è¯•ç¯å¢ƒä¸­å®ç°äº†é«˜æˆåŠŸç‡å’Œæ•ˆç‡ã€‚</li>
<li>MAT-THORæ˜¯ä¸€ä¸ªåŸºäºAI2-THORç¯å¢ƒçš„å®¶åº­ä»»åŠ¡åŸºå‡†æµ‹è¯•ç¯å¢ƒï¼Œåˆ†ä¸ºä¸¤ä¸ªä¸åŒéš¾åº¦çº§åˆ«ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMMA-Pç›¸è¾ƒäºå…¶ä»–æ–¹æ³•æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-063ba41ea326017ee6ba8e02453fbde4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7e7a37ce261eead676381f0f047c466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c9d5659583bd68a8cdf57e6635e5603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbd62e5b24dec1d7998a378f08cfa348.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc21215d2254da3bb62e96832ac43cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EIA-Environmental-Injection-Attack-on-Generalist-Web-Agents-for-Privacy-Leakage"><a href="#EIA-Environmental-Injection-Attack-on-Generalist-Web-Agents-for-Privacy-Leakage" class="headerlink" title="EIA: Environmental Injection Attack on Generalist Web Agents for Privacy   Leakage"></a>EIA: Environmental Injection Attack on Generalist Web Agents for Privacy   Leakage</h2><p><strong>Authors:Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun</strong></p>
<p>Generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve usersâ€™ PII, which may be exposed to potential privacy risks if web agents accidentally interact with compromised websites, a scenario that remains largely unexplored in the literature. In this work, we narrow this gap by conducting the first study on the privacy risks of generalist web agents in adversarial environments. First, we present a realistic threat model for attacks on the website, where we consider two adversarial targets: stealing usersâ€™ specific PII or the entire user request. Then, we propose a novel attack method, termed Environmental Injection Attack (EIA). EIA injects malicious content designed to adapt well to environments where the agents operate and our work instantiates EIA specifically for privacy scenarios in web environments. We collect 177 action steps that involve diverse PII categories on realistic websites from the Mind2Web, and conduct experiments using one of the most capable generalist web agent frameworks to date. The results demonstrate that EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user request. Additionally, by accessing the stealthiness and experimenting with a defensive system prompt, we indicate that EIA is hard to detect and mitigate. Notably, attacks that are not well adapted for a webpage can be detected via human inspection, leading to our discussion about the trade-off between security and autonomy. However, extra attackersâ€™ efforts can make EIA seamlessly adapted, rendering such supervision ineffective. Thus, we further discuss the defenses at the pre- and post-deployment stages of the websites without relying on human supervision and call for more advanced defense strategies. </p>
<blockquote>
<p>é€šç”¨ç½‘é¡µä»£ç†åœ¨è‡ªä¸»å®ŒæˆçœŸå®ç½‘ç«™ä¸Šçš„å„ç§ä»»åŠ¡æ–¹é¢å·²æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ï¼Œæå¤§åœ°æé«˜äº†äººç±»ç”Ÿäº§åŠ›ã€‚ç„¶è€Œï¼Œç½‘é¡µä»»åŠ¡ï¼ˆå¦‚è®¢ç¥¨ï¼‰é€šå¸¸æ¶‰åŠç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ï¼ˆPIIï¼‰ï¼Œå¦‚æœç½‘é¡µä»£ç†æ„å¤–åœ°ä¸é­æ”»å‡»çš„ç½‘ç«™äº¤äº’ï¼Œå¯èƒ½ä¼šä½¿ä¸ªäººä¿¡æ¯é¢ä¸´æ½œåœ¨çš„éšç§é£é™©ï¼Œè¿™ä¸€æƒ…æ™¯åœ¨æ–‡çŒ®ä¸­ä»å¾ˆå°‘è¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å±•å…³äºé€šç”¨ç½‘é¡µä»£ç†åœ¨æ•Œå¯¹ç¯å¢ƒä¸­çš„éšç§é£é™©çš„é¦–é¡¹ç ”ç©¶æ¥ç¼©å°è¿™ä¸€å·®è·ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºé’ˆå¯¹ç½‘ç«™çš„æ”»å‡»æå‡ºäº†ä¸€ä¸ªç°å®çš„å¨èƒæ¨¡å‹ï¼Œå…¶ä¸­æˆ‘ä»¬è€ƒè™‘äº†ä¸¤ä¸ªæ•Œå¯¹ç›®æ ‡ï¼šçªƒå–ç”¨æˆ·çš„ç‰¹å®šä¸ªäººä¿¡æ¯æˆ–æ•´ä¸ªç”¨æˆ·è¯·æ±‚ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºç¯å¢ƒæ³¨å…¥æ”»å‡»ï¼ˆEIAï¼‰ã€‚EIAæ³¨å…¥æ¶æ„å†…å®¹ï¼Œè®¾è®¡å¾—å¾ˆå¥½ï¼Œèƒ½é€‚åº”ä»£ç†è¿è¡Œçš„ç¯å¢ƒï¼Œæˆ‘ä»¬çš„å·¥ä½œå…·ä½“å®ä¾‹åŒ–äº†é’ˆå¯¹webç¯å¢ƒä¸­éšç§åœºæ™¯çš„EIAã€‚æˆ‘ä»¬ä»Mind2Webæ”¶é›†äº†æ¶‰åŠçœŸå®ç½‘ç«™ä¸Šå„ç±»ä¸ªäººä¿¡æ¯çš„177ä¸ªæ“ä½œæ­¥éª¤ï¼Œå¹¶ä½¿ç”¨è¿„ä»Šä¸ºæ­¢æœ€å¼ºå¤§çš„é€šç”¨ç½‘é¡µä»£ç†æ¡†æ¶è¿›è¡Œå®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒEIAåœ¨çªƒå–ç‰¹å®šä¸ªäººä¿¡æ¯æ–¹é¢è¾¾åˆ°äº†é«˜è¾¾70%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼Œå¯¹ç”¨æˆ·è¯·æ±‚çš„å®Œæ•´æ”»å‡»è¾¾åˆ°16%çš„ASRã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¿é—®éšè”½æ€§å¹¶å¯¹é˜²å¾¡ç³»ç»Ÿè¿›è¡Œå®éªŒæç¤ºï¼Œæˆ‘ä»¬è¡¨æ˜EIAå¾ˆéš¾æ£€æµ‹å’Œç¼“è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸é€‚åº”ç½‘é¡µçš„æ”»å‡»å¯ä»¥é€šè¿‡äººå·¥æ£€æŸ¥è¿›è¡Œæ£€æµ‹ï¼Œè¿™å¼•å‘äº†æˆ‘ä»¬å¯¹å®‰å…¨æ€§å’Œè‡ªä¸»æ€§ä¹‹é—´æƒè¡¡çš„è®¨è®ºã€‚ç„¶è€Œï¼Œé¢å¤–çš„æ”»å‡»è€…åŠªåŠ›å¯ä»¥ä½¿EIAæ— ç¼é€‚åº”ï¼Œä½¿è¿™ç§ç›‘ç£æ— æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºäº†ä¸ä¾èµ–äººå·¥ç›‘ç£çš„ç½‘ç«™é¢„éƒ¨ç½²å’Œåéƒ¨ç½²é˜¶æ®µçš„é˜²å¾¡æªæ–½ï¼Œå¹¶å‘¼åé‡‡ç”¨æ›´å…ˆè¿›çš„é˜²å¾¡ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11295v5">PDF</a> Accepted by ICLR 2025</p>
<p><strong>æ‘˜è¦</strong><br>è‡ªåŠ¨åŒ–ç½‘ç«™ä¸Šçš„å„ç§ä»»åŠ¡ï¼Œå±•ç°å‡ºå¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¤§å¤§æé«˜äº†å·¥ä½œæ•ˆç‡ã€‚ç„¶è€Œï¼Œç½‘é¡µä»»åŠ¡å¸¸æ¶‰åŠç”¨æˆ·ä¸ªäººä¿¡æ¯æ³„éœ²çš„é£é™©ã€‚ä¾‹å¦‚è®¢è´­èˆªç­æ—¶ï¼Œè‹¥ç½‘ç«™ä»£ç†æ„å¤–ä¸æ¶æ„ç½‘ç«™äº¤äº’ï¼Œç”¨æˆ·çš„ä¸ªäººä¿¡æ¯å¯èƒ½è¢«çªƒå–ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†è‡ªåŠ¨åŒ–ç½‘ç«™ä»£ç†åœ¨æ•Œå¯¹ç¯å¢ƒä¸­çš„éšç§é£é™©é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆè®¾å®šäº†ä¸€ç§å®é™…å¨èƒæ¨¡å‹æ¥æ¨¡æ‹Ÿç½‘ç«™æ”»å‡»çš„åœºæ™¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹å¼â€”â€”â€œç¯å¢ƒæ³¨å…¥æ”»å‡»â€ï¼ˆEIAï¼‰ã€‚æˆ‘ä»¬çš„æ”»å‡»æ–¹æ³•æ—¨åœ¨é€‚åº”ç½‘ç«™ä»£ç†çš„è¿è¡Œç¯å¢ƒï¼Œä¸“é—¨ç”¨äºåº”å¯¹ç½‘é¡µç¯å¢ƒä¸­çš„éšç§æ³„éœ²åœºæ™¯ã€‚é€šè¿‡å¯¹çœŸå®ç½‘ç«™çš„è¡ŒåŠ¨æ­¥éª¤è¿›è¡Œç ”ç©¶å®éªŒï¼Œæˆ‘ä»¬å‘ç°EIAçªƒå–ç‰¹å®šä¿¡æ¯çš„æˆåŠŸç‡é«˜è¾¾70%ï¼Œçªƒå–å…¨éƒ¨ç”¨æˆ·è¯·æ±‚çš„æˆåŠŸç‡ä¸º16%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼ŒEIAéš¾ä»¥è¢«æ£€æµ‹å’Œé˜²å¾¡ç³»ç»Ÿé˜»æ­¢ã€‚ä½†æŸäº›ä¸é’ˆå¯¹ç‰¹å®šç½‘é¡µçš„æ”»å‡»å¯é€šè¿‡äººå·¥æ£€æµ‹å‘ç°ã€‚è¿™å¼•å‘äº†å…³äºå®‰å…¨å’Œè‡ªä¸»æ€§ä¹‹é—´çš„æƒè¡¡è®¨è®ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¨è®ºäº†ç½‘ç«™éƒ¨ç½²å‰åçš„é˜²å¾¡ç­–ç•¥ï¼Œå‘¼åå¼€å‘æ›´å…ˆè¿›çš„é˜²å¾¡æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç½‘ç«™ä»£ç†èƒ½å¤Ÿè‡ªä¸»å®Œæˆå¤šç§ä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜å·¥ä½œæ•ˆç‡ï¼Œä½†å­˜åœ¨éšç§æ³„éœ²é£é™©ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†è‡ªåŠ¨åŒ–ç½‘ç«™ä»£ç†åœ¨æ•Œå¯¹ç¯å¢ƒä¸­çš„éšç§é£é™©é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹å¼â€”â€”â€œç¯å¢ƒæ³¨å…¥æ”»å‡»â€ï¼ˆEIAï¼‰ï¼Œèƒ½å¤Ÿé€‚åº”ç½‘ç«™ä»£ç†çš„è¿è¡Œç¯å¢ƒå¹¶çªƒå–ç”¨æˆ·ä¿¡æ¯ã€‚</li>
<li>EIAæ”»å‡»æˆåŠŸç‡é«˜ï¼Œéš¾ä»¥è¢«é˜²å¾¡ç³»ç»Ÿæ£€æµ‹å¹¶é˜»æ­¢ã€‚</li>
<li>å­˜åœ¨å®‰å…¨å’Œè‡ªä¸»æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œéœ€è¦æ¢è®¨å¦‚ä½•åœ¨ä¿éšœå®‰å…¨çš„å‰æä¸‹æé«˜è‡ªä¸»æ€§ã€‚</li>
<li>ç½‘ç«™éƒ¨ç½²å‰åçš„é˜²å¾¡ç­–ç•¥æ˜¯å¿…è¦çš„ï¼Œéœ€è¦å¼€å‘æ›´å…ˆè¿›çš„é˜²å¾¡æŠ€æœ¯æ¥åº”å¯¹æ½œåœ¨çš„æ”»å‡»ã€‚</li>
<li>EIAçš„æ”»å‡»æ•ˆæœä¸å¯¹ç½‘é¡µçš„é€‚åº”æ€§æœ‰å…³ï¼Œä¸å®Œå…¨é€‚åº”çš„æ”»å‡»å®¹æ˜“è¢«äººç±»æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81e89bec1f797cc2c0c017372e9fef31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a8141b5f2a04f4e82fbc69fc02fbe4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="COMBO-Compositional-World-Models-for-Embodied-Multi-Agent-Cooperation"><a href="#COMBO-Compositional-World-Models-for-Embodied-Multi-Agent-Cooperation" class="headerlink" title="COMBO: Compositional World Models for Embodied Multi-Agent Cooperation"></a>COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</h2><p><strong>Authors:Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, Chuang Gan</strong></p>
<p>In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agentsâ€™ actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. We evaluate our methods on three challenging benchmarks with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed methods. More videos can be found at <a target="_blank" rel="noopener" href="https://embodied-agi.cs.umass.edu/combo/">https://embodied-agi.cs.umass.edu/combo/</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å…·æœ‰èº«ä½“å®ä½“çš„å¤šæ™ºèƒ½ä½“åˆä½œé—®é¢˜ï¼Œåœ¨åªè€ƒè™‘è‡ªæˆ‘ä¸­å¿ƒè§†è§’çš„æƒ…å†µä¸‹ï¼Œåˆ†å¸ƒå¼æ™ºèƒ½ä½“å¿…é¡»è¿›è¡Œåˆä½œã€‚ä¸ºäº†åœ¨è¿™ç§ç¯å¢ƒä¸‹è¿›è¡Œæœ‰æ•ˆçš„è§„åˆ’ï¼Œä¸å•æ™ºèƒ½ä½“åœºæ™¯ä¸­å­¦ä¹ ä¸–ç•ŒåŠ¨åŠ›å­¦ä¸åŒï¼Œæˆ‘ä»¬å¿…é¡»æ¨¡æ‹Ÿåœ¨ä»…è·å¾—éƒ¨åˆ†è‡ªæˆ‘ä¸­å¿ƒè§†è§‰è§‚å¯Ÿçš„æƒ…å†µä¸‹ï¼Œä»»æ„æ•°é‡æ™ºèƒ½ä½“çš„è¡Œä¸ºæ‰€å½±å“çš„ä¸–ç•ŒåŠ¨æ€ã€‚ä¸ºäº†è§£å†³éƒ¨åˆ†å¯è§‚å¯Ÿæ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œæ ¹æ®éƒ¨åˆ†è‡ªæˆ‘ä¸­å¿ƒè§‚å¯Ÿæ¥ä¼°è®¡æ•´ä½“ä¸–ç•ŒçŠ¶æ€ã€‚ä¸ºäº†èƒ½å¤Ÿåœ¨è¿™ä¸ªä¸–ç•ŒçŠ¶æ€ä¸‹å‡†ç¡®åœ°æ¨¡æ‹Ÿå¤šç»„åŠ¨ä½œï¼Œç„¶åæˆ‘ä»¬æå‡ºäº†é€šè¿‡åˆ†è§£å¤šä¸ªæ™ºèƒ½ä½“çš„è‡ªç„¶å¯ç»„åˆè”åˆåŠ¨ä½œï¼Œå¹¶åŸºäºä¸–ç•ŒçŠ¶æ€ç»„åˆç”Ÿæˆè§†é¢‘ï¼Œæ¥å­¦ä¹ ç”¨äºå¤šæ™ºèƒ½ä½“åˆä½œçš„ç»“æ„åŒ–ä¸–ç•Œæ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è¿™ç§ç»“æ„åŒ–ä¸–ç•Œæ¨¡å‹ï¼Œç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹æ¥æ¨æ–­å…¶ä»–æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‘æœç´¢è¿‡ç¨‹æ¥æ•´åˆè¿™äº›æ¨¡å—ï¼Œä¿ƒè¿›åœ¨çº¿åˆä½œè§„åˆ’ã€‚æˆ‘ä»¬åœ¨å…·æœ‰2-4ä¸ªæ™ºèƒ½ä½“çš„ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»“æ„åŒ–ä¸–ç•Œæ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿å¾—å…·æœ‰èº«ä½“çš„æ™ºèƒ½ä½“åœ¨ä¸åŒçš„ä»»åŠ¡å’Œä»»æ„æ•°é‡çš„æ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œé«˜æ•ˆåˆä½œï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ‰€æå‡ºæ–¹æ³•çš„å¹¿é˜”å‰æ™¯ã€‚æ›´å¤šè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://embodied-agi.cs.umass.edu/combo/%E6%89%BE%E5%88%B0%E3%80%82">https://embodied-agi.cs.umass.edu/combo/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10775v2">PDF</a> Published at ICLR 2025. 24 pages. The first three authors contributed   equally</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“åˆä½œé—®é¢˜ï¼Œé’ˆå¯¹å…·æœ‰å±€éƒ¨è‡ªæˆ‘è§†è§’çš„åˆ†æ•£å¼æ™ºèƒ½ä½“ï¼Œé€šè¿‡è®­ç»ƒç”Ÿæˆæ¨¡å‹ä¼°è®¡æ•´ä½“ä¸–ç•ŒçŠ¶æ€æ¥è§£å†³éƒ¨åˆ†è§‚æµ‹é—®é¢˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºä¸€ç§ç”¨äºå¤šæ™ºèƒ½ä½“åˆä½œçš„ç»„åˆä¸–ç•Œæ¨¡å‹ï¼Œé€šè¿‡åˆ†è§£å¤šä¸ªæ™ºèƒ½ä½“çš„è‡ªç„¶å¯ç»„åˆåŠ¨ä½œï¼Œå¹¶æ ¹æ®ä¸–ç•ŒçŠ¶æ€ç»„åˆç”Ÿæˆè§†é¢‘ã€‚ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œæ¨æ–­ï¼Œä½¿ç”¨æ ‘æœç´¢è¿‡ç¨‹æ•´åˆè¿™äº›æ¨¡å—ï¼Œå®ç°åœ¨çº¿ååŒè§„åˆ’ã€‚åœ¨ä¸‰ä¸ªåŒ…å«2-4ä¸ªæ™ºèƒ½ä½“çš„æŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°äº†æ™ºèƒ½ä½“é—´çš„æœ‰æ•ˆåˆä½œå’Œä¸åŒä»»åŠ¡ä¸­æ™ºèƒ½ä½“çš„çµæ´»æ€§ã€‚æ›´å¤šè§†é¢‘å¯åœ¨ç›¸å…³ç½‘ç«™æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“åˆä½œé—®é¢˜ï¼Œè§£å†³äº†åœ¨å…·æœ‰å±€éƒ¨è‡ªæˆ‘è§†è§’ä¸‹çš„åˆ†æ•£å¼æ™ºèƒ½ä½“çš„ååŒè§„åˆ’é—®é¢˜ã€‚</li>
<li>é€šè¿‡è®­ç»ƒç”Ÿæˆæ¨¡å‹ä¼°è®¡æ•´ä½“ä¸–ç•ŒçŠ¶æ€æ¥è§£å†³éƒ¨åˆ†è§‚æµ‹é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§ç»„åˆä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºå­¦ä¹ å¤šæ™ºèƒ½ä½“åˆä½œçš„åœºæ™¯ã€‚</li>
<li>é€šè¿‡åˆ†è§£å¤šä¸ªæ™ºèƒ½ä½“çš„è‡ªç„¶å¯ç»„åˆåŠ¨ä½œï¼Œå®ç°è§†é¢‘çš„æ¡ä»¶ç”Ÿæˆã€‚</li>
<li>ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå…¶ä»–æ™ºèƒ½ä½“çš„åŠ¨ä½œæ¨æ–­ã€‚</li>
<li>ä½¿ç”¨æ ‘æœç´¢è¿‡ç¨‹æ•´åˆå„æ¨¡å—ï¼Œå®ç°åœ¨çº¿ååŒè§„åˆ’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f0f5f2c432168848182b25a20542ce86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-036994c569d13aa0a7acc4ddc460b13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9510b382527bf30d99cbc4256ed4637d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d84d47b7f2015bfbb48f2575678ef371.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Maximum-Entropy-Heterogeneous-Agent-Reinforcement-Learning"><a href="#Maximum-Entropy-Heterogeneous-Agent-Reinforcement-Learning" class="headerlink" title="Maximum Entropy Heterogeneous-Agent Reinforcement Learning"></a>Maximum Entropy Heterogeneous-Agent Reinforcement Learning</h2><p><strong>Authors:Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang, Yaodong Yang</strong></p>
<p>Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning stochastic policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose Heterogeneous-Agent Soft Actor-Critic (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to quantal response equilibrium (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration. See our page at <a target="_blank" rel="noopener" href="https://sites.google.com/view/meharl">https://sites.google.com/view/meharl</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨åˆä½œæ¸¸æˆé¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ•ˆæœã€‚ç„¶è€Œï¼Œç°æœ‰çš„å‰æ²¿æŠ€æœ¯æ–¹æ³•é¢ä¸´ç€æ ·æœ¬å¤æ‚æ€§ã€è®­ç»ƒä¸ç¨³å®šæ€§å’Œæ”¶æ•›åˆ°æ¬¡ä¼˜çº³ä»€å‡è¡¡çš„é£é™©ç­‰æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå­¦ä¹ éšæœºç­–ç•¥çš„ç»Ÿä¸€æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬å°†åˆä½œå‹MARLé—®é¢˜åµŒå…¥åˆ°æ¦‚ç‡å›¾å½¢æ¨¡å‹ä¸­ï¼Œå¹¶ä»ä¸­æ¨å¯¼å‡ºMARLçš„æœ€å¤§ç†µï¼ˆMaxEntï¼‰ç›®æ ‡ã€‚åŸºäºMaxEntæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Heterogeneous-Agent Soft Actor-Criticï¼ˆHASACï¼‰ç®—æ³•ã€‚åœ¨ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†HASACçš„å•è°ƒæ”¹è¿›å’Œæ”¶æ•›åˆ°é‡åŒ–å“åº”å‡è¡¡ï¼ˆQREï¼‰çš„ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºMaxEntç®—æ³•è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡æ¿ï¼Œåä¸ºMaximum Entropy Heterogeneous-Agent Mirror Learningï¼ˆMEHAMLï¼‰ï¼Œè¯¥æ¨¡æ¿ä¸ºä»»ä½•è¯±å¯¼æ–¹æ³•æä¾›ä¸HASACç›¸åŒçš„ä¿è¯ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹HASACè¿›è¡Œäº†è¯„ä¼°ï¼šBi-DexHandsã€Multi-Agent MuJoCoã€StarCraft Multi-Agent Challengeã€Google Research Footballã€Multi-Agent Particle Environmentå’ŒLight Aircraft Gameã€‚ç»“æœè¡¨æ˜ï¼ŒHASACå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ ·æœ¬æ•ˆç‡ã€é²æ£’æ€§å’Œè¶³å¤Ÿçš„æ¢ç´¢èƒ½åŠ›ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡µé¢<a target="_blank" rel="noopener" href="https://sites.google.com/view/meharl%E4%BA%86%E8%A7%A3%E8%AF%A6%E6%83%85%E3%80%82">https://sites.google.com/view/meharläº†è§£è¯¦æƒ…ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.10715v6">PDF</a> ICLR 2024 Spotlight</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è¿‘å¹´åˆä½œæ¸¸æˆä¸­å±•ç°å‡ºæœ‰æ•ˆæ€§ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´æ ·æœ¬å¤æ‚æ€§ã€è®­ç»ƒä¸ç¨³å®šæ€§å’Œæ”¶æ•›åˆ°æ¬¡ä¼˜çº³ä»€å‡è¡¡çš„é£é™©ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªå­¦ä¹ éšæœºç­–ç•¥çš„æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå°†åˆä½œMARLé—®é¢˜åµŒå…¥æ¦‚ç‡å›¾æ¨¡å‹ä¸­ï¼Œæ¨å¯¼å‡ºMARLçš„æœ€å¤§ç†µï¼ˆMaxEntï¼‰ç›®æ ‡ã€‚åŸºäºMaxEntæ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºHeterogeneous-Agent Soft Actor-Criticï¼ˆHASACï¼‰ç®—æ³•ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†HASACçš„å•è°ƒæ”¹è¿›å’Œå‘é‡åŒ–å“åº”å‡è¡¡ï¼ˆQREï¼‰å±æ€§çš„æ”¶æ•›æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºMaxEntç®—æ³•è®¾è®¡äº†ä¸€ä¸ªåä¸ºMaximum Entropy Heterogeneous-Agent Mirror Learningï¼ˆMEHAMLï¼‰çš„ç»Ÿä¸€æ¨¡æ¿ï¼Œä¸ºä»»ä½•è¯±å¯¼æ–¹æ³•æä¾›ä¸HASACç›¸åŒçš„ä¿è¯ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¯¹HASACè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºHASACæŒç»­ä¼˜äºå¼ºåŸºçº¿ï¼Œå±•ç°å‡ºæ›´å¥½çš„æ ·æœ¬æ•ˆç‡ã€é²æ£’æ€§å’Œè¶³å¤Ÿçš„æ¢ç´¢èƒ½åŠ›ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„ç½‘ç«™ï¼š[ç½‘ç«™åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨åˆä½œæ¸¸æˆä¸­æ•ˆæœæ˜¾è‘—ï¼Œä½†ä»å­˜åœ¨æ ·æœ¬å¤æ‚æ€§ã€è®­ç»ƒä¸ç¨³å®šæ€§å’Œæ”¶æ•›åˆ°æ¬¡ä¼˜è§£çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå­¦ä¹ éšæœºç­–ç•¥çš„æ¡†æ¶ï¼Œå°†åˆä½œMARLé—®é¢˜åµŒå…¥æ¦‚ç‡å›¾æ¨¡å‹ï¼Œå¹¶æ¨å¯¼å‡ºMaxEntç›®æ ‡ã€‚</li>
<li>æå‡ºäº†Heterogeneous-Agent Soft Actor-Criticï¼ˆHASACï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨ç†è®ºä¸Šè¢«è¯æ˜å…·æœ‰å•è°ƒæ”¹è¿›å’Œå‘é‡åŒ–å“åº”å‡è¡¡çš„æ”¶æ•›æ€§ã€‚</li>
<li>å¹¿ä¹‰çš„MaxEntç®—æ³•è®¾è®¡æ¨¡æ¿MEHAMLä¸ºä»»ä½•è¯±å¯¼æ–¹æ³•æä¾›ä¸HASACç›¸åŒçš„ä¿è¯ã€‚</li>
<li>HASACåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æŒç»­ä¼˜äºå…¶ä»–å¼ºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>HASACå±•ç°å‡ºæ›´å¥½çš„æ ·æœ¬æ•ˆç‡ã€é²æ£’æ€§å’Œè¶³å¤Ÿçš„æ¢ç´¢èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.10715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d91efc882abe2feaff4ceebf3ada0bc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46a7396143b08384adb52f7c7713c72b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786765be0e3c55f90312bfdd05b43604.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-16ac621e5356ed6f81ab09edccc21589.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  New Trends for Modern Machine Translation with Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  HybridVLA Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
