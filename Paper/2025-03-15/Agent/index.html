<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-03-15  SurgRAW Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-15-更新"><a href="#2025-03-15-更新" class="headerlink" title="2025-03-15 更新"></a>2025-03-15 更新</h1><h2 id="SurgRAW-Multi-Agent-Workflow-with-Chain-of-Thought-Reasoning-for-Surgical-Intelligence"><a href="#SurgRAW-Multi-Agent-Workflow-with-Chain-of-Thought-Reasoning-for-Surgical-Intelligence" class="headerlink" title="SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence"></a>SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for   Surgical Intelligence</h2><p><strong>Authors:Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, Yueming Jin</strong></p>
<p>Integration of Vision-Language Models (VLMs) in surgical intelligence is hindered by hallucinations, domain knowledge gaps, and limited understanding of task interdependencies within surgical scenes, undermining clinical reliability. While recent VLMs demonstrate strong general reasoning and thinking capabilities, they still lack the domain expertise and task-awareness required for precise surgical scene interpretation. Although Chain-of-Thought (CoT) can structure reasoning more effectively, current approaches rely on self-generated CoT steps, which often exacerbate inherent domain gaps and hallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent framework that delivers transparent, interpretable insights for most tasks in robotic-assisted surgery. By employing specialized CoT prompts across five tasks: instrument recognition, action recognition, action prediction, patient data extraction, and outcome assessment, SurgRAW mitigates hallucinations through structured, domain-aware reasoning. Retrieval-Augmented Generation (RAG) is also integrated to external medical knowledge to bridge domain gaps and improve response reliability. Most importantly, a hierarchical agentic system ensures that CoT-embedded VLM agents collaborate effectively while understanding task interdependencies, with a panel discussion mechanism promotes logical consistency. To evaluate our method, we introduce SurgCoTBench, the first reasoning-based dataset with structured frame-level annotations. With comprehensive experiments, we demonstrate the effectiveness of proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures, achieving the state-of-the-art performance and advancing explainable, trustworthy, and autonomous surgical assistance. </p>
<blockquote>
<p>将视觉语言模型（VLMs）整合到手术智能中面临着幻视、领域知识差距、对手术场景中任务相互依赖性的有限理解等障碍，这削弱了临床可靠性。虽然最近的VLMs表现出了强大的通用推理和思维能力，但它们仍然缺乏精确的手术场景解释所需的领域专业知识和任务意识。虽然思维链（CoT）可以更有效地结构化推理，但当前的方法依赖于自我生成的CoT步骤，这往往会加剧固有的领域差距和幻视。</p>
</blockquote>
<p>为了克服这一问题，我们提出了SurgRAW，这是一个由思维链驱动的多代理框架，为机器人辅助手术中的大多数任务提供透明、可解释的见解。通过五个任务中的专业思维链提示：仪器识别、动作识别、动作预测、患者数据提取和结果评估，SurgRAW通过结构化、领域感知推理来缓解幻视。还集成了检索增强生成（RAG）以获取外部医学知识，以弥补领域差距并提高响应可靠性。最重要的是，分层代理系统确保思维链嵌入的VLM代理有效地协作，同时理解任务相互依赖性，小组讨论机制促进了逻辑一致性。为了评估我们的方法，我们引入了SurgCoTBench，这是一个基于推理的数据集，具有结构化帧级注释。通过全面的实验，我们证明了所提出的SurgRAW的有效性，在12个机器人手术程序上相对于基线VLMs实现了29.32%的准确率提升，达到了最先进的表现，并推动了可解释、可信和自主的手术辅助技术。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10265v1">PDF</a> </p>
<p><strong>Summary</strong><br>     视觉语言模型（VLM）在手术智能集成中受到幻觉、领域知识差距和任务相互依赖性的限制，影响临床可靠性。虽然最近的VLM展现出强大的通用推理和思维能力，但它们仍然缺乏精确的手术场景解释所需的领域专业知识和任务意识。本文提出SurgRAW，一个基于思维链（CoT）的多智能体框架，为机器人辅助手术中的大多数任务提供透明、可解释性的见解。通过五个任务的专门思维链提示，SurgRAW通过结构化、领域意识推理缓解幻觉。同时集成检索增强生成（RAG）以弥补领域知识差距并提高响应可靠性。最重要的是，一个分层智能体系确保嵌入思维链的VLM智能体有效协作，理解任务相互依赖性，并通过小组讨论机制促进逻辑一致性。评估方法引入SurgCoTBench，首个基于推理的数据集，带有结构化帧级注释。实验表明，与基线VLM相比，SurgRAW实现了29.32%的准确率提升，在12项机器人手术程序中达到最佳性能，推动可解释、可信和自主手术辅助的进步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLM在手术智能中的集成受到幻觉、领域知识差距和任务相互依赖性的挑战。</li>
<li>最近的VLM虽然具有强大的通用推理能力，但缺乏手术领域的专业知识和任务意识。</li>
<li>SurgRAW框架利用CoT驱动的多智能体结构来解决这些问题，提供可解释性的手术任务见解。</li>
<li>SurgRAW通过结构化、领域意识的推理来减轻幻觉。</li>
<li>集成RAG来弥补医疗领域的知识差距，提高响应的可靠性。</li>
<li>分层智能体系确保智能体理解任务相互依赖性并有效协作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10265">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-924dc5051a215b0cb2331628406b2cd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08663a2f6ba4409243f6a1cf84bfeb5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9de395e9305ab5dbde7a7cc0f00dcd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bba250c9d2179659cd5a6bdacb753073.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents"><a href="#LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents" class="headerlink" title="LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents"></a>LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents</h2><p><strong>Authors:Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, Yali Wang</strong></p>
<p>Existing Multimodal Large Language Models (MLLMs) encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools (e.g., search engine, memory banks, OCR, retrieval models) to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our methodology consists of four key steps: 1. Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2. Perception: We design an effective retrieval scheme for long videos, improving the coverage of critical temporal segments while maintaining computational efficiency. 3. Action: Agents answer long video-related questions and exchange reasons. 4. Reflection: We evaluate the performance of each agent in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (including GPT-4o) and open-source models (including InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80% on four mainstream long video understanding tasks. Notably, on the LongVideoBench dataset, LVAgent improves accuracy by up to 14.3% compared with SOTA. </p>
<blockquote>
<p>现有的多模态大型语言模型（MLLMs）在建模长视频中的时间上下文时面临重大挑战。目前，主流的基本代理方法使用外部工具（如搜索引擎、记忆库、OCR、检索模型）来协助单个MLLM回答长视频问题。尽管有基于工具的支持，单个MLLM仍然只能对长视频有部分理解，导致性能有限。为了更好地应对长视频任务，我们引入了LVAgent，这是第一个使多轮动态协作的MLLM代理实现长视频理解的框架。我们的方法包括四个关键步骤：1.选择：我们从模型库中预先选择适当的代理，根据不同的任务形成最优代理团队。2.感知：我们为长视频设计了有效的检索方案，提高了关键时间段的覆盖率，同时保持了计算效率。3.行动：代理回答与长视频相关的问题并交流理由。4.反思：我们评估每一轮讨论中每个代理的性能，优化代理团队进行动态协作。通过MLLM代理的多轮动态协作，代理可以迭代优化答案。LVAgent是第一个在长视频理解任务中表现优于所有封闭源模型（包括GPT-4o）和开源模型（包括InternVL-2.5和Qwen2-VL）的代理系统方法。我们的LVAgent在四个主流的长视频理解任务中达到了80%的准确率。值得注意的是，在LongVideoBench数据集上，LVAgent与最新技术相比，准确率提高了高达14.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10200v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对长视频理解任务，现有单一的多模态大型语言模型（MLLM）存在建模时间上下文能力的挑战。为此，我们提出了LVAgent框架，该框架支持多轮动态协作的MLLM代理，以提高对长视频的理解能力。LVAgent通过选择、感知、行动和反思四个关键步骤实现多轮动态协作，并在长视频理解任务中取得了显著成果，超越了许多封闭和开源模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>长视频理解任务中，单一多模态大型语言模型（MLLM）存在建模时间上下文能力的挑战。</li>
<li>LVAgent框架是首个支持多轮动态协作的MLLM代理的框架，旨在更好地解决长视频任务。</li>
<li>LVAgent通过选择、感知、行动和反思四个步骤实现多轮动态协作。</li>
<li>LVAgent在主流长视频理解任务上取得了高达80%的准确率。</li>
<li>在LongVideoBench数据集上，LVAgent相较于现有最佳模型，提高了高达14.3%的准确率。</li>
<li>LVAgent通过设计有效的长视频检索方案，提高了对关键时间段的覆盖并保持了计算效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10200">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-12d002c85c1bc01456b098e7eb6c3324.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eeddcf790978ed67edc49e7d1fb51bcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e2ab58aabc5234ec6b4d074bb3d0fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5927170d078d15ffc833161cedb26f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8af1ba5b4ab1a9ecc455db683270abd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6578ceaa7dcb305bc5cccc1774d9fec3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="StepMathAgent-A-Step-Wise-Agent-for-Evaluating-Mathematical-Processes-through-Tree-of-Error"><a href="#StepMathAgent-A-Step-Wise-Agent-for-Evaluating-Mathematical-Processes-through-Tree-of-Error" class="headerlink" title="StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes   through Tree-of-Error"></a>StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes   through Tree-of-Error</h2><p><strong>Authors:Shu-Xun Yang, Cunxiang Wang, Yidong Wang, Xiaotao Gu, Minlie Huang, Jie Tang</strong></p>
<p>Evaluating mathematical capabilities is critical for assessing the overall performance of large language models (LLMs). However, existing evaluation methods often focus solely on final answers, resulting in highly inaccurate and uninterpretable evaluation outcomes, as well as their failure to assess proof or open-ended problems. To address these issues, we propose a novel mathematical process evaluation agent based on Tree-of-Error, called StepMathAgent. This agent incorporates four internal core operations: logical step segmentation, step scoring, score aggregation and error tree generation, along with four external extension modules: difficulty calibration, simplicity evaluation, completeness validation and format assessment. Furthermore, we introduce StepMathBench, a benchmark comprising 1,000 step-divided process evaluation instances, derived from 200 high-quality math problems grouped by problem type, subject category and difficulty level. Experiments on StepMathBench show that our proposed StepMathAgent outperforms all state-of-the-art methods, demonstrating human-aligned evaluation preferences and broad applicability to various scenarios. Our data and code are available at <a target="_blank" rel="noopener" href="https://github.com/SHU-XUN/StepMathAgent">https://github.com/SHU-XUN/StepMathAgent</a>. </p>
<blockquote>
<p>评估数学能力是评估大型语言模型（LLM）整体性能的关键。然而，现有的评估方法往往只关注最终答案，导致评估结果不准确、难以解释，并且无法评估证明或开放式问题。为了解决这些问题，我们提出了一种基于错误树的新型数学过程评估代理，称为StepMathAgent。该代理结合了四项内部核心操作：逻辑步骤分割、步骤评分、分数聚合和错误树生成，以及四项外部扩展模块：难度校准、简单性评估、完整性验证和格式评估。此外，我们还推出了StepMathBench，这是一个包含1000个按步骤划分的过程评估实例的基准测试，这些实例来源于按问题类型、主题类别和难度水平分组的200个高质量数学问题。在StepMathBench上的实验表明，我们提出的StepMathAgent优于所有最先进的方法，表现出与人类评估偏好相符的结果，并广泛适用于各种场景。我们的数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/SHU-XUN/StepMathAgent">https://github.com/SHU-XUN/StepMathAgent</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10105v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文强调评估大型语言模型（LLMs）的数学能力的重要性，并提出一种基于Tree-of-Error的新型数学过程评价代理——StepMathAgent。该代理具备逻辑步骤分割、步骤评分、评分聚合和错误树生成等四项内部核心操作，以及难度校准、简单性评价、完整性验证和格式评估等四项外部扩展模块。此外，文章还介绍了包含1000个分步过程评价实例的StepMathBench基准测试，该测试来源于按问题类型、学科类别和难度水平分组的高质量数学问题。实验表明，提出的StepMathAgent优于所有最先进的方法，展现出与人类评估偏好相符的评价和广泛适用的场景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>评估大型语言模型的数学能力是衡量其整体性能的关键。</li>
<li>现有评估方法主要关注最终答案，导致评价不准确、不可解释，并且无法评估证明或开放性问题。</li>
<li>StepMathAgent是一种基于Tree-of-Error的新型数学过程评价代理，包含内部核心操作和外部扩展模块。</li>
<li>StepMathAgent具备逻辑步骤分割、步骤评分、评分聚合和错误树生成等功能。</li>
<li>StepMathBench基准测试包含1000个分步过程评价实例，源于高质量数学问题，并按问题类型、学科和难度分组。</li>
<li>实验表明StepMathAgent在评估数学过程方面优于其他先进方法，且与人类评估偏好相符。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10105">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3e1525e717cf9abdfcb455b4eded9200.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e24d7ca2ec0736b75d28c6bc857bfaa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3738d36dda85d6c47fb44557e3be3ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d69f774712d310c3fc8f05350364bb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7548979ad8fa26c414a6fcffaf900a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa88e64b23ca77d2b7de3a37ba38eef1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Revisiting-Multi-Agent-Asynchronous-Online-Optimization-with-Delays-the-Strongly-Convex-Case"><a href="#Revisiting-Multi-Agent-Asynchronous-Online-Optimization-with-Delays-the-Strongly-Convex-Case" class="headerlink" title="Revisiting Multi-Agent Asynchronous Online Optimization with Delays: the   Strongly Convex Case"></a>Revisiting Multi-Agent Asynchronous Online Optimization with Delays: the   Strongly Convex Case</h2><p><strong>Authors:Lingchan Bao, Tong Wei, Yuanyu Wan</strong></p>
<p>We revisit multi-agent asynchronous online optimization with delays, where only one of the agents becomes active for making the decision at each round, and the corresponding feedback is received by all the agents after unknown delays. Although previous studies have established an $O(\sqrt{dT})$ regret bound for this problem, they assume that the maximum delay $d$ is knowable or the arrival order of feedback satisfies a special property, which may not hold in practice. In this paper, we surprisingly find that when the loss functions are strongly convex, these assumptions can be eliminated, and the existing regret bound can be significantly improved to $O(d\log T)$ meanwhile. Specifically, to exploit the strong convexity of functions, we first propose a delayed variant of the classical follow-the-leader algorithm, namely FTDL, which is very simple but requires the full information of functions as feedback. Moreover, to handle the more general case with only the gradient feedback, we develop an approximate variant of FTDL by combining it with surrogate loss functions. Experimental results show that the approximate FTDL outperforms the existing algorithm in the strongly convex case. </p>
<blockquote>
<p>我们重新研究了多智能体异步在线优化问题，该问题中存在延迟。在每一轮中，只有一个智能体被激活以做出决策，所有智能体在未知延迟后接收到相应的反馈。尽管之前的研究已经为这个问题建立了O(√dT)的遗憾界限，但他们假设最大延迟d是可知的，或者反馈到达顺序满足特殊属性，这在实践中可能不成立。在本文中，我们惊讶地发现当损失函数是强凸的时候，这些假设可以被消除，并且现有的遗憾界限可以显著改善到O(dlogT)。具体来说，为了利用函数的强凸性，我们首先提出了经典跟随领导者算法的延迟变体，即FTDL，它非常简单，但需要函数的全部信息作为反馈。此外，为了处理只有梯度反馈的更一般情况，我们通过结合替代损失函数，开发了FTDL的近似变体。实验结果表明，近似FTDL在强凸情况下优于现有算法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10013v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文研究了多智能体异步在线优化问题，其中只有一名智能体在每个回合中活跃以做出决策，所有智能体在未知延迟后都会收到反馈。尽管先前的研究为该问题建立了O(√dT)的后悔界，但它们假设最大延迟d是可知道的或反馈到达顺序满足特殊属性，这可能不符合实际情况。本文惊奇地发现，当损失函数是强凸时，可以消除这些假设，并将现有后悔界显著改善至O(dlogT)。特别是，为了利用函数的强凸性，我们提出了经典跟随领先者算法的延迟变体，即FTDL，它非常简单但需要函数的完整信息作为反馈。此外，为了处理只有梯度反馈的更一般情况，我们结合替代损失函数开发了FTDL的近似变体。实验结果表明，近似FTDL在强凸情况下优于现有算法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究了多智能体异步在线优化问题，其中涉及延迟反馈和只有一个智能体在每个回合中活跃做决策的场景。</li>
<li>之前的研究为这一问题设定了O(√dT)的后悔界，但假设最大延迟d可知或反馈到达顺序有特殊属性，这可能不符合实际。</li>
<li>当损失函数是强凸时，可以消除这些假设，并将后悔界改善至O(dlogT)。</li>
<li>提出了经典跟随领先者算法的延迟变体FTDL，该算法简单但要求反馈包含函数的完整信息。</li>
<li>为了处理仅有梯度反馈的情况，结合了替代损失函数来开发FTDL的近似变体。</li>
<li>实验结果显示，近似FTDL在强凸情况下表现优于现有算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10013">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ce69f8bbe035238b040c68365deb13ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a27e5b73d4550f50872114877f2dd77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09375b337e25fa3346d537a907c2ea44.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PCLA-A-Framework-for-Testing-Autonomous-Agents-in-the-CARLA-Simulator"><a href="#PCLA-A-Framework-for-Testing-Autonomous-Agents-in-the-CARLA-Simulator" class="headerlink" title="PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator"></a>PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator</h2><p><strong>Authors:Masoud Jamshidiyan Tehrani, Jinhan Kim, Paolo Tonella</strong></p>
<p>Recent research on testing autonomous driving agents has grown significantly, especially in simulation environments. The CARLA simulator is often the preferred choice, and the autonomous agents from the CARLA Leaderboard challenge are regarded as the best-performing agents within this environment. However, researchers who test these agents, rather than training their own ones from scratch, often face challenges in utilizing them within customized test environments and scenarios. To address these challenges, we introduce PCLA (Pretrained CARLA Leaderboard Agents), an open-source Python testing framework that includes nine high-performing pre-trained autonomous agents from the Leaderboard challenges. PCLA is the first infrastructure specifically designed for testing various autonomous agents in arbitrary CARLA environments&#x2F;scenarios. PCLA provides a simple way to deploy Leaderboard agents onto a vehicle without relying on the Leaderboard codebase, it allows researchers to easily switch between agents without requiring modifications to CARLA versions or programming environments, and it is fully compatible with the latest version of CARLA while remaining independent of the Leaderboard’s specific CARLA version. PCLA is publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/MasoudJTehrani/PCLA">https://github.com/MasoudJTehrani/PCLA</a>. </p>
<blockquote>
<p>近期关于测试自动驾驶代理的研究已显著增加，特别是在仿真环境中。CARLA模拟器通常是首选，CARLA排行榜挑战中的自主代理被认为是在此环境中表现最好的代理。然而，研究人员在测试这些代理而不是从头开始训练它们时，常常面临在自定义测试环境和场景中使用它们的挑战。为了解决这些挑战，我们推出了PCLA（预训练CARLA排行榜代理），这是一个开源Python测试框架，包含来自排行榜挑战的九个高性能预训练自主代理。PCLA是专门为在任意CARLA环境&#x2F;场景中测试各种自主代理而设计的基础设施。PCLA提供了一种简单的方法将排行榜代理部署到车辆上，而无需依赖排行榜的代码库，它允许研究人员轻松地在不修改CARLA版本或编程环境的情况下切换代理，并且它与最新版本的CARLA完全兼容，同时独立于排行榜的特定CARLA版本。PCLA可在<a target="_blank" rel="noopener" href="https://github.com/MasoudJTehrani/PCLA">https://github.com/MasoudJTehrani/PCLA</a>公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09385v2">PDF</a> This work will be published at the FSE 2025 demonstration track</p>
<p><strong>Summary</strong></p>
<p>CARLA模拟器中的自主驾驶代理测试研究日益增多，但研究者在使用定制测试环境和场景时面临挑战。为解决这些问题，推出了PCLA（预训练CARLA领导者代理）测试框架，包含九个高性能预训练自主代理，可在任意CARLA环境中进行测试。PCLA提供了简单的方法部署领导者代理到车辆上，无需依赖领导者代码库，并允许研究者轻松切换代理，同时与最新版本的CARLA兼容，独立于领导者特定的CARLA版本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>最近关于在CARLA模拟器中测试自主驾驶代理的研究显著增长。</li>
<li>研究者在定制测试环境和场景中使用现有代理时面临挑战。</li>
<li>PCLA是一个为测试各种自主代理而设计的测试框架。</li>
<li>PCLA包含九个来自领导者排行榜的高性能预训练自主代理。</li>
<li>PCLA提供了简单的方法部署代理到车辆上，无需依赖领导者代码库。</li>
<li>PCLA允许研究者轻松切换代理，无需更改CARLA版本或编程环境。</li>
<li>PCLA与最新版本的CARLA兼容，并且独立于领导者特定的CARLA版本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09385">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b8861956140edd9f2fc7648554bda48c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3908ed5d8b04c177ae0b9e3044720a03.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-to-Detect-Objects-from-Multi-Agent-LiDAR-Scans-without-Manual-Labels"><a href="#Learning-to-Detect-Objects-from-Multi-Agent-LiDAR-Scans-without-Manual-Labels" class="headerlink" title="Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual   Labels"></a>Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual   Labels</h2><p><strong>Authors:Qiming Xia, Wenkai Lin, Haoen Xiang, Xun Huang, Siheng Chen, Zhen Dong, Cheng Wang, Chenglu Wen</strong></p>
<p>Unsupervised 3D object detection serves as an important solution for offline 3D object annotation. However, due to the data sparsity and limited views, the clustering-based label fitting in unsupervised object detection often generates low-quality pseudo-labels. Multi-agent collaborative dataset, which involves the sharing of complementary observations among agents, holds the potential to break through this bottleneck. In this paper, we introduce a novel unsupervised method that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA, without using labels from external. DOtA first uses the internally shared ego-pose and ego-shape of collaborative agents to initialize the detector, leveraging the generalization performance of neural networks to infer preliminary labels. Subsequently,DOtA uses the complementary observations between agents to perform multi-scale encoding on preliminary labels, then decodes high-quality and low-quality labels. These labels are further used as prompts to guide a correct feature learning process, thereby enhancing the performance of the unsupervised object detection task. Extensive experiments on the V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art unsupervised 3D object detection methods. Additionally, we also validate the effectiveness of the DOtA labels under various collaborative perception frameworks.The code is available at <a target="_blank" rel="noopener" href="https://github.com/xmuqimingxia/DOtA">https://github.com/xmuqimingxia/DOtA</a>. </p>
<blockquote>
<p>无监督的3D目标检测是离线3D目标标注的重要解决方案。然而，由于数据稀疏和视图有限，无监督目标检测中的基于聚类的标签拟合通常会产生低质量的伪标签。多智能体协同数据集涉及智能体之间的互补观察共享，具有突破这一瓶颈的潜力。在本文中，我们介绍了一种新的无监督方法，该方法从多智能体激光雷达扫描中学习检测目标，称为DOtA，无需使用来自外部标签。DOtA首先使用智能体之间共享的内置自我姿势和自我形状来初始化检测器，利用神经网络的泛化性能来推断初步标签。随后，DOtA利用智能体之间的互补观察对初步标签执行多尺度编码，然后解码高质量和低质量标签。这些标签进一步作为提示来引导正确的特征学习过程，从而提高无监督目标检测任务的性能。在V2V4Real和OPV2V数据集上的大量实验表明，我们的DOtA优于最新的无监督3D目标检测方法。此外，我们还验证了在不同协同感知框架下DOtA标签的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/xmuqimingxia/DOtA">链接</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08421v2">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong><br>基于多智能体协同数据集的无人监督3D目标检测通过利用智能体间的互补观察突破了检测瓶颈。DOtA方法通过共享智能体的内部姿态和形状进行初步标签推断，并利用智能体间的互补观察进行多尺度编码，生成高质量和低质量标签。这些标签进一步引导特征学习过程，提高了无人监督的3D目标检测性能。在V2V4Real和OPV2V数据集上的实验证明，DOtA优于现有技术。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>多智能体协同数据集通过智能体间的互补观察增强无人监督的3D目标检测性能。</li>
<li>DOtA方法使用智能体的内部共享姿态和形状初始化检测器。</li>
<li>利用智能体间的互补观察进行多尺度编码生成标签。</li>
<li>生成的高质量标签引导特征学习过程以提高检测性能。</li>
<li>在多个数据集上的实验证明DOtA方法优于现有技术。</li>
<li>DOtA的代码已经公开，可供他人使用参考。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-32caff3b4ab5bb1e070b230871544550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aa4d22053df44aa1ac61d28bc9c00d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8738656748f8fde35a152afd7e259035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f03a725c95b78969a7897ee1fcaa0a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd6e65903b919e1bc829ce5a79d01ea7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="KG4Diagnosis-A-Hierarchical-Multi-Agent-LLM-Framework-with-Knowledge-Graph-Enhancement-for-Medical-Diagnosis"><a href="#KG4Diagnosis-A-Hierarchical-Multi-Agent-LLM-Framework-with-Knowledge-Graph-Enhancement-for-Medical-Diagnosis" class="headerlink" title="KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge   Graph Enhancement for Medical Diagnosis"></a>KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge   Graph Enhancement for Medical Diagnosis</h2><p><strong>Authors:Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio</strong></p>
<p>Integrating Large Language Models (LLMs) in healthcare diagnosis demands systematic frameworks that can handle complex medical scenarios while maintaining specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent framework that combines LLMs with automated knowledge graph construction, encompassing 362 common diseases across medical specialties. Our framework mirrors real-world medical systems through a two-tier architecture: a general practitioner (GP) agent for initial assessment and triage, coordinating with specialized agents for in-depth diagnosis in specific domains. The core innovation lies in our end-to-end knowledge graph generation methodology, incorporating: (1) semantic-driven entity and relation extraction optimized for medical terminology, (2) multi-dimensional decision relationship reconstruction from unstructured medical texts, and (3) human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized medical diagnosis systems, with capabilities to incorporate new diseases and medical knowledge. The framework’s modular design enables seamless integration of domain-specific enhancements, making it valuable for developing targeted medical diagnosis systems. We provide architectural guidelines and protocols to facilitate adoption across medical contexts. </p>
<blockquote>
<p>将大型语言模型（LLMs）整合到医疗诊断中，需要能够处理复杂医疗场景并保持专业知识的系统性框架。我们提出了KG4Diagnosis，这是一种新型分层多代理框架，它将LLMs与自动化知识图谱构建相结合，涵盖362种常见疾病，涉及医学各专业领域。我们的框架通过两层架构反映现实医疗系统：一层是通用实践者（GP）代理进行初步评估和分类，另一层是与专业领域深入诊断的专门代理协调。核心创新在于我们端到端的知识图谱生成方法，包括：（1）针对医学术语优化的语义驱动实体和关系提取，（2）从非结构化的医疗文本中重建多维决策关系，（3）用于知识扩展的人机协同推理。KG4Diagnosis作为一个可扩展的基础，为专业医疗诊断系统提供了强大的支持，有能力融入新的疾病和医学知识。该框架的模块化设计使得能够无缝集成特定领域的增强功能，因此对于开发有针对性的医疗诊断系统具有很高的价值。我们提供了架构指南和协议，以推动其在各种医疗环境中的采用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16833v3">PDF</a> 10 pages,5 figures,published to AAAI-25 Bridge Program</p>
<p><strong>Summary</strong>：KG4Diagnosis是一个结合大型语言模型（LLM）和自动化知识图谱构建的新型分级多智能体框架，用于医疗服务中的诊断工作。它能够处理复杂的医疗场景并维持专业性的知识，涵盖了医学领域的362种常见疾病。该框架采用两层结构来模拟真实的医疗系统：初级医生进行初步评估和分级诊疗，并与专科智能体协调进行特定领域的深度诊断。该框架的核心创新之处在于其端到端的知识图谱生成方法，包括面向医学术语的语义驱动实体和关系提取、从非结构化医疗文本中重建多维决策关系以及基于人工推理的知识扩展。它为专业医疗诊断系统提供了可扩展的基础，能够融入新的疾病和医学知识。框架的模块化设计使其能够无缝集成特定领域的改进，对于开发有针对性的医疗诊断系统具有重要价值。我们提供了跨医疗环境的采用指南和协议。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>KG4Diagnosis是一个集成LLM和自动化知识图谱的新型诊断框架，涵盖多种常见疾病。</li>
<li>该框架采用两层结构，初级医生进行初步评估并与专科智能体协调深度诊断。</li>
<li>KG4Diagnosis的核心创新在于其知识图谱生成方法，包括语义驱动的实体和关系提取、多维决策关系重建和基于人工推理的知识扩展。</li>
<li>框架具有可扩展性，可融入新的疾病和医学知识，且模块化设计可无缝集成特定领域的改进。</li>
<li>KG4Diagnosis为专业医疗诊断系统提供了基础，并提供了跨医疗环境的采用指南和协议。</li>
<li>该框架适用于复杂的医疗场景，并能够维持专业性的知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c3c92782c439aee2d8ad680bffdab538.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50afebfc586d931d790904ce420fc4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-144a2f441c16fa0f57c3081d6b171dc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb7c4aecc4a233f64b49a8137451e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-336e50ee4a5e3968ea86e2a2fea37f14.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DataEnvGym-Data-Generation-Agents-in-Teacher-Environments-with-Student-Feedback"><a href="#DataEnvGym-Data-Generation-Agents-in-Teacher-Environments-with-Student-Feedback" class="headerlink" title="DataEnvGym: Data Generation Agents in Teacher Environments with Student   Feedback"></a>DataEnvGym: Data Generation Agents in Teacher Environments with Student   Feedback</h2><p><strong>Authors:Zaid Khan, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal</strong></p>
<p>The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid, scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent’s goal is to improve student performance. Students are iteratively trained and evaluated on generated data, and their feedback (in the form of errors or weak skills) is reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 4 domains (math, code, VQA, and tool-use) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms. </p>
<blockquote>
<p>目前，创建用于训练模型的教学训练数据的过程是由人类驱动的，人类手动分析模型的弱点，并计划如何创建能够改进学生模型的数据。使用大型语言模型（LLMs）作为注释者的方法减少了人力投入，但仍需要人类来解释评估反馈并控制大型语言模型以产生学生所需的数据。通过创建自主数据生成代理（或教师）来自动完成这一劳动密集型过程是可取的，但这需要能够模拟数据创建中的反馈驱动、迭代和闭环的环境。为了实现对这类代理及其模块进行快速、可扩展的测试，我们推出了DataEnvGym，一个为教师环境设计的测试平台。DataEnvGym将数据生成框架设定为一项序列决策任务，涉及数据生成代理内的数据生成策略和生成引擎。其中数据生成策略用于制定创建训练数据的计划，而生成引擎则将该计划转化为实际数据。代理位于一个提供学生反馈的环境中，其目标是提高学生的表现。学生在生成的迭代数据和评估中不断学习和进步，他们的反馈（以错误或技能不足的形式）会在每次迭代后报告给代理。DataEnvGym包含多个教师环境实例，涵盖了状态表示和行为空间中的三个结构层次。更结构化的环境基于推断技能，提供更多的可解释性和课程控制。我们支持四个领域（数学、代码、视觉问答和工具使用），并测试多个学生和教师。在我们的教学环境中，示例代理可以迭代地提高学生的任务表现和设置能力。此外，我们还证明了环境能够教授不同技能水平并测试关键模块的不同变体，这为未来改进数据生成代理、引擎和反馈机制指明了方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06215v3">PDF</a> ICLR 2025 Spotlight; Project Page: <a target="_blank" rel="noopener" href="https://dataenvgym.github.io/">https://DataEnvGym.github.io</a></p>
<p><strong>摘要</strong><br>训练数据生成采用自动方式减少人工干预是当前研究的热点。该文提出了一种使用DataEnvGym测试平台的方法，将训练数据生成视为一个序列决策任务。DataEnvGym提供了一个教师环境用于数据生成代理的测试，其中包括数据生成策略和生成引擎。数据生成策略负责创建训练数据的计划，而生成引擎负责将数据转化为实际应用。环境通过学生反馈来评估数据质量，代理的目标是提高学生的表现。通过迭代训练和评估学生生成的错误或弱技能，将反馈反馈给代理，以便优化后续数据生成策略。DataEnvGym在不同结构层次的环境中提供了多个教师环境的实例，更结构化的环境基于推断技能并提供更多可解释性和课程控制。支持数学、代码、视觉问答和工具使用等四个领域，并测试了多个学生和教师。在测试环境中，教师代理能够提高学生跨任务和设置的表现，展示未来改进数据生成策略的方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>训练数据生成的自动化成为迫切需求，以替代繁琐的人力标注工作。</li>
<li>DataEnvGym被提出作为数据生成代理的测试平台，模仿真实的反馈闭环系统。</li>
<li>该平台通过将数据生成看作一个序列决策问题来处理，涉及数据生成策略和生成引擎两部分。</li>
<li>环境设计注重结构性和解释性，可根据推断技能进行课程控制。</li>
<li>支持多种领域的数据生成任务，包括数学、代码、视觉问答和工具使用等。</li>
<li>教师代理能够在不同任务和设置下提高学生表现，展示了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06215">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2bd54521a5842180b92c142c07967eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c732b42a9518a723635090284a6996d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1998015494ab96f758fcd00b5e7c9be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-894cac4e5088ea23529c9eaab6f26ecd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a635991a8b4cf7976333b7f6d9614af.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LaMMA-P-Generalizable-Multi-Agent-Long-Horizon-Task-Allocation-and-Planning-with-LM-Driven-PDDL-Planner"><a href="#LaMMA-P-Generalizable-Multi-Agent-Long-Horizon-Task-Allocation-and-Planning-with-LM-Driven-PDDL-Planner" class="headerlink" title="LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and   Planning with LM-Driven PDDL Planner"></a>LaMMA-P: Generalizable Multi-Agent Long-Horizon Task Allocation and   Planning with LM-Driven PDDL Planner</h2><p><strong>Authors:Xiaopan Zhang, Hao Qin, Fuquan Wang, Yue Dong, Jiachen Li</strong></p>
<p>Language models (LMs) possess a strong capability to comprehend natural language, making them effective in translating human instructions into detailed plans for simple robot tasks. Nevertheless, it remains a significant challenge to handle long-horizon tasks, especially in subtask identification and allocation for cooperative heterogeneous robot teams. To address this issue, we propose a Language Model-Driven Multi-Agent PDDL Planner (LaMMA-P), a novel multi-agent task planning framework that achieves state-of-the-art performance on long-horizon tasks. LaMMA-P integrates the strengths of the LMs’ reasoning capability and the traditional heuristic search planner to achieve a high success rate and efficiency while demonstrating strong generalization across tasks. Additionally, we create MAT-THOR, a comprehensive benchmark that features household tasks with two different levels of complexity based on the AI2-THOR environment. The experimental results demonstrate that LaMMA-P achieves a 105% higher success rate and 36% higher efficiency than existing LM-based multiagent planners. The experimental videos, code, datasets, and detailed prompts used in each module can be found on the project website: <a target="_blank" rel="noopener" href="https://lamma-p.github.io/">https://lamma-p.github.io</a>. </p>
<blockquote>
<p>语言模型（LMs）具有很强的理解自然语言的能力，使得它们能够有效地将人类指令翻译成简单的机器人任务的详细计划。然而，在处理长期任务时，特别是在识别分配合作型异构机器人团队的子任务方面，仍然存在重大挑战。为了解决这个问题，我们提出了一种语言模型驱动的多智能体PDDL规划器（LaMMA-P），这是一种新型的多智能体任务规划框架，在长期任务上实现了最先进的性能。LaMMA-P结合了语言模型的推理能力和传统的启发式搜索规划器的优点，以实现高成功率和效率，并在各种任务中表现出强大的泛化能力。此外，我们创建了MAT-THOR基准测试，它基于AI2-THOR环境，以家庭任务为特色，分为两个不同的复杂度级别。实验结果表明，LaMMA-P与现有的基于LM的多智能体规划器相比，成功率提高了105%，效率提高了36%。实验视频、代码、数据集和每个模块中使用的详细提示可以在项目网站上找到：<a target="_blank" rel="noopener" href="https://lamma-p.github.io/">https://lamma-p.github.io</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20560v2">PDF</a> IEEE Conference on Robotics and Automation (ICRA 2025); Project   website: <a target="_blank" rel="noopener" href="https://lamma-p.github.io/">https://lamma-p.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了语言模型在机器人任务中的应用，提出了一种名为LaMMA-P的新型多智能体任务规划框架，该框架结合了语言模型的推理能力和传统启发式搜索规划器，实现了长周期任务的高成功率和效率。同时，为了评估该框架的性能，创建了一个名为MAT-THOR的基准测试环境。实验结果表明，LaMMA-P相较于现有的基于语言模型的多智能体规划器，成功率提高了105%，效率提高了36%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型具备强大的自然语言理解能力，可将其应用于机器人任务的翻译和规划。</li>
<li>处理长周期任务面临的主要挑战是子任务的识别和分配。</li>
<li>LaMMA-P是一种新型的多智能体任务规划框架，结合了语言模型和启发式搜索规划器的优势。</li>
<li>LaMMA-P在基准测试环境中实现了高成功率和效率。</li>
<li>MAT-THOR是一个基于AI2-THOR环境的家庭任务基准测试环境，分为两个不同难度级别。</li>
<li>实验结果表明，LaMMA-P相较于其他方法有明显的性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20560">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-063ba41ea326017ee6ba8e02453fbde4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7e7a37ce261eead676381f0f047c466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c9d5659583bd68a8cdf57e6635e5603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbd62e5b24dec1d7998a378f08cfa348.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bc21215d2254da3bb62e96832ac43cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="EIA-Environmental-Injection-Attack-on-Generalist-Web-Agents-for-Privacy-Leakage"><a href="#EIA-Environmental-Injection-Attack-on-Generalist-Web-Agents-for-Privacy-Leakage" class="headerlink" title="EIA: Environmental Injection Attack on Generalist Web Agents for Privacy   Leakage"></a>EIA: Environmental Injection Attack on Generalist Web Agents for Privacy   Leakage</h2><p><strong>Authors:Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun</strong></p>
<p>Generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve users’ PII, which may be exposed to potential privacy risks if web agents accidentally interact with compromised websites, a scenario that remains largely unexplored in the literature. In this work, we narrow this gap by conducting the first study on the privacy risks of generalist web agents in adversarial environments. First, we present a realistic threat model for attacks on the website, where we consider two adversarial targets: stealing users’ specific PII or the entire user request. Then, we propose a novel attack method, termed Environmental Injection Attack (EIA). EIA injects malicious content designed to adapt well to environments where the agents operate and our work instantiates EIA specifically for privacy scenarios in web environments. We collect 177 action steps that involve diverse PII categories on realistic websites from the Mind2Web, and conduct experiments using one of the most capable generalist web agent frameworks to date. The results demonstrate that EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user request. Additionally, by accessing the stealthiness and experimenting with a defensive system prompt, we indicate that EIA is hard to detect and mitigate. Notably, attacks that are not well adapted for a webpage can be detected via human inspection, leading to our discussion about the trade-off between security and autonomy. However, extra attackers’ efforts can make EIA seamlessly adapted, rendering such supervision ineffective. Thus, we further discuss the defenses at the pre- and post-deployment stages of the websites without relying on human supervision and call for more advanced defense strategies. </p>
<blockquote>
<p>通用网页代理在自主完成真实网站上的各种任务方面已显示出显著潜力，极大地提高了人类生产力。然而，网页任务（如订票）通常涉及用户的个人信息（PII），如果网页代理意外地与遭攻击的网站交互，可能会使个人信息面临潜在的隐私风险，这一情景在文献中仍很少被探索。在这项工作中，我们通过开展关于通用网页代理在敌对环境中的隐私风险的首项研究来缩小这一差距。首先，我们为针对网站的攻击提出了一个现实的威胁模型，其中我们考虑了两个敌对目标：窃取用户的特定个人信息或整个用户请求。然后，我们提出了一种新型攻击方法，称为环境注入攻击（EIA）。EIA注入恶意内容，设计得很好，能适应代理运行的环境，我们的工作具体实例化了针对web环境中隐私场景的EIA。我们从Mind2Web收集了涉及真实网站上各类个人信息的177个操作步骤，并使用迄今为止最强大的通用网页代理框架进行实验。结果表明，EIA在窃取特定个人信息方面达到了高达70%的攻击成功率（ASR），对用户请求的完整攻击达到16%的ASR。此外，通过访问隐蔽性并对防御系统进行实验提示，我们表明EIA很难检测和缓解。值得注意的是，不适应网页的攻击可以通过人工检查进行检测，这引发了我们对安全性和自主性之间权衡的讨论。然而，额外的攻击者努力可以使EIA无缝适应，使这种监督无效。因此，我们进一步讨论了不依赖人工监督的网站预部署和后部署阶段的防御措施，并呼吁采用更先进的防御策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11295v5">PDF</a> Accepted by ICLR 2025</p>
<p><strong>摘要</strong><br>自动化网站上的各种任务，展现出广泛的应用潜力，大大提高了工作效率。然而，网页任务常涉及用户个人信息泄露的风险。例如订购航班时，若网站代理意外与恶意网站交互，用户的个人信息可能被窃取。本研究首次探讨了自动化网站代理在敌对环境中的隐私风险问题。我们首先设定了一种实际威胁模型来模拟网站攻击的场景，并提出了一种新的攻击方式——“环境注入攻击”（EIA）。我们的攻击方法旨在适应网站代理的运行环境，专门用于应对网页环境中的隐私泄露场景。通过对真实网站的行动步骤进行研究实验，我们发现EIA窃取特定信息的成功率高达70%，窃取全部用户请求的成功率为16%。此外，我们的实验显示，EIA难以被检测和防御系统阻止。但某些不针对特定网页的攻击可通过人工检测发现。这引发了关于安全和自主性之间的权衡讨论。为此，我们讨论了网站部署前后的防御策略，呼吁开发更先进的防御技术。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>网站代理能够自主完成多种任务，显著提高工作效率，但存在隐私泄露风险。</li>
<li>本研究首次探讨了自动化网站代理在敌对环境中的隐私风险问题。</li>
<li>提出了一种新的攻击方式——“环境注入攻击”（EIA），能够适应网站代理的运行环境并窃取用户信息。</li>
<li>EIA攻击成功率高，难以被防御系统检测并阻止。</li>
<li>存在安全和自主性之间的权衡，需要探讨如何在保障安全的前提下提高自主性。</li>
<li>网站部署前后的防御策略是必要的，需要开发更先进的防御技术来应对潜在的攻击。</li>
<li>EIA的攻击效果与对网页的适应性有关，不完全适应的攻击容易被人类检测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81e89bec1f797cc2c0c017372e9fef31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a8141b5f2a04f4e82fbc69fc02fbe4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d366a34e7d90734c72e931441fc957ad.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="COMBO-Compositional-World-Models-for-Embodied-Multi-Agent-Cooperation"><a href="#COMBO-Compositional-World-Models-for-Embodied-Multi-Agent-Cooperation" class="headerlink" title="COMBO: Compositional World Models for Embodied Multi-Agent Cooperation"></a>COMBO: Compositional World Models for Embodied Multi-Agent Cooperation</h2><p><strong>Authors:Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, Chuang Gan</strong></p>
<p>In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics conditioned on an arbitrary number of agents’ actions given only partial egocentric visual observations of the world. To address this issue of partial observability, we first train generative models to estimate the overall world state given partial egocentric observations. To enable accurate simulation of multiple sets of actions on this world state, we then propose to learn a compositional world model for multi-agent cooperation by factorizing the naturally composable joint actions of multiple agents and compositionally generating the video conditioned on the world state. By leveraging this compositional world model, in combination with Vision Language Models to infer the actions of other agents, we can use a tree search procedure to integrate these modules and facilitate online cooperative planning. We evaluate our methods on three challenging benchmarks with 2-4 agents. The results show our compositional world model is effective and the framework enables the embodied agents to cooperate efficiently with different agents across various tasks and an arbitrary number of agents, showing the promising future of our proposed methods. More videos can be found at <a target="_blank" rel="noopener" href="https://embodied-agi.cs.umass.edu/combo/">https://embodied-agi.cs.umass.edu/combo/</a>. </p>
<blockquote>
<p>本文研究了具有身体实体的多智能体合作问题，在只考虑自我中心视角的情况下，分布式智能体必须进行合作。为了在这种环境下进行有效的规划，与单智能体场景中学习世界动力学不同，我们必须模拟在仅获得部分自我中心视觉观察的情况下，任意数量智能体的行为所影响的世界动态。为了解决部分可观察性的问题，我们首先训练生成模型，根据部分自我中心观察来估计整体世界状态。为了能够在这个世界状态下准确地模拟多组动作，然后我们提出了通过分解多个智能体的自然可组合联合动作，并基于世界状态组合生成视频，来学习用于多智能体合作的结构化世界模型。通过利用这种结构化世界模型，结合视觉语言模型来推断其他智能体的行为，我们可以使用树搜索过程来整合这些模块，促进在线合作规划。我们在具有2-4个智能体的三个具有挑战性的基准测试上评估了我们的方法。结果表明，我们的结构化世界模型是有效的，该框架能够使得具有身体的智能体在不同的任务和任意数量的智能体之间进行高效合作，展示了我们所提出方法的广阔前景。更多视频可在<a target="_blank" rel="noopener" href="https://embodied-agi.cs.umass.edu/combo/%E6%89%BE%E5%88%B0%E3%80%82">https://embodied-agi.cs.umass.edu/combo/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10775v2">PDF</a> Published at ICLR 2025. 24 pages. The first three authors contributed   equally</p>
<p><strong>Summary</strong>：</p>
<p>本文研究了多智能体合作问题，针对具有局部自我视角的分散式智能体，通过训练生成模型估计整体世界状态来解决部分观测问题。在此基础上，提出一种用于多智能体合作的组合世界模型，通过分解多个智能体的自然可组合动作，并根据世界状态组合生成视频。结合视觉语言模型进行其他智能体的动作推断，使用树搜索过程整合这些模块，实现在线协同规划。在三个包含2-4个智能体的挑战基准测试中验证了方法的有效性，展现了智能体间的有效合作和不同任务中智能体的灵活性。更多视频可在相关网站找到。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>研究了多智能体合作问题，解决了在具有局部自我视角下的分散式智能体的协同规划问题。</li>
<li>通过训练生成模型估计整体世界状态来解决部分观测问题。</li>
<li>提出一种组合世界模型，用于学习多智能体合作的场景。</li>
<li>通过分解多个智能体的自然可组合动作，实现视频的条件生成。</li>
<li>结合视觉语言模型进行其他智能体的动作推断。</li>
<li>使用树搜索过程整合各模块，实现在线协同规划。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f0f5f2c432168848182b25a20542ce86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-036994c569d13aa0a7acc4ddc460b13c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9510b382527bf30d99cbc4256ed4637d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d84d47b7f2015bfbb48f2575678ef371.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Maximum-Entropy-Heterogeneous-Agent-Reinforcement-Learning"><a href="#Maximum-Entropy-Heterogeneous-Agent-Reinforcement-Learning" class="headerlink" title="Maximum Entropy Heterogeneous-Agent Reinforcement Learning"></a>Maximum Entropy Heterogeneous-Agent Reinforcement Learning</h2><p><strong>Authors:Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang, Yaodong Yang</strong></p>
<p>Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning stochastic policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose Heterogeneous-Agent Soft Actor-Critic (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to quantal response equilibrium (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration. See our page at <a target="_blank" rel="noopener" href="https://sites.google.com/view/meharl">https://sites.google.com/view/meharl</a>. </p>
<blockquote>
<p>近年来，多智能体强化学习（MARL）在合作游戏领域表现出了显著的效果。然而，现有的前沿技术方法面临着样本复杂性、训练不稳定性和收敛到次优纳什均衡的风险等挑战。在本文中，我们提出了一个学习随机策略的统一框架来解决这些问题。我们将合作型MARL问题嵌入到概率图形模型中，并从中推导出MARL的最大熵（MaxEnt）目标。基于MaxEnt框架，我们提出了Heterogeneous-Agent Soft Actor-Critic（HASAC）算法。在理论上，我们证明了HASAC的单调改进和收敛到量化响应均衡（QRE）的特性。此外，我们为MaxEnt算法设计了一个统一的模板，名为Maximum Entropy Heterogeneous-Agent Mirror Learning（MEHAML），该模板为任何诱导方法提供与HASAC相同的保证。我们在六个基准测试上对HASAC进行了评估：Bi-DexHands、Multi-Agent MuJoCo、StarCraft Multi-Agent Challenge、Google Research Football、Multi-Agent Particle Environment和Light Aircraft Game。结果表明，HASAC始终优于强大的基线，表现出更好的样本效率、鲁棒性和足够的探索能力。请访问我们的页面<a target="_blank" rel="noopener" href="https://sites.google.com/view/meharl%E4%BA%86%E8%A7%A3%E8%AF%A6%E6%83%85%E3%80%82">https://sites.google.com/view/meharl了解详情。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.10715v6">PDF</a> ICLR 2024 Spotlight</p>
<p><strong>Summary</strong></p>
<p>多智能体强化学习（MARL）在近年合作游戏中展现出有效性，但现有方法面临样本复杂性、训练不稳定性和收敛到次优纳什均衡的风险。本文提出一个学习随机策略的框架来解决这些问题，将合作MARL问题嵌入概率图模型中，推导出MARL的最大熵（MaxEnt）目标。基于MaxEnt框架，我们提出Heterogeneous-Agent Soft Actor-Critic（HASAC）算法。理论上，我们证明了HASAC的单调改进和向量化响应均衡（QRE）属性的收敛性。此外，我们为MaxEnt算法设计了一个名为Maximum Entropy Heterogeneous-Agent Mirror Learning（MEHAML）的统一模板，为任何诱导方法提供与HASAC相同的保证。我们在六个基准测试上对HASAC进行了评估，结果显示HASAC持续优于强基线，展现出更好的样本效率、鲁棒性和足够的探索能力。更多信息请访问我们的网站：[网站地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习（MARL）在合作游戏中效果显著，但仍存在样本复杂性、训练不稳定性和收敛到次优解的问题。</li>
<li>提出了一个学习随机策略的框架，将合作MARL问题嵌入概率图模型，并推导出MaxEnt目标。</li>
<li>提出了Heterogeneous-Agent Soft Actor-Critic（HASAC）算法，该算法在理论上被证明具有单调改进和向量化响应均衡的收敛性。</li>
<li>广义的MaxEnt算法设计模板MEHAML为任何诱导方法提供与HASAC相同的保证。</li>
<li>HASAC在六个基准测试上的表现持续优于其他强基线方法。</li>
<li>HASAC展现出更好的样本效率、鲁棒性和足够的探索能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.10715">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d91efc882abe2feaff4ceebf3ada0bc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46a7396143b08384adb52f7c7713c72b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786765be0e3c55f90312bfdd05b43604.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-15/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-16ac621e5356ed6f81ab09edccc21589.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-03-15  New Trends for Modern Machine Translation with Large Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02d0e16fc24e75ebfc3ea8ae9d8b4ed0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-15  HybridVLA Collaborative Diffusion and Autoregression in a Unified   Vision-Language-Action Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
