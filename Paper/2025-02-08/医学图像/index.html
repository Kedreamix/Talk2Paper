<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  ConceptAttention Diffusion Transformers Learn Highly Interpretable   Features">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5c265730e323ed2e7668b32b4f7572b0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    68 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-08-æ›´æ–°"><a href="#2025-02-08-æ›´æ–°" class="headerlink" title="2025-02-08 æ›´æ–°"></a>2025-02-08 æ›´æ–°</h1><h2 id="ConceptAttention-Diffusion-Transformers-Learn-Highly-Interpretable-Features"><a href="#ConceptAttention-Diffusion-Transformers-Learn-Highly-Interpretable-Features" class="headerlink" title="ConceptAttention: Diffusion Transformers Learn Highly Interpretable   Features"></a>ConceptAttention: Diffusion Transformers Learn Highly Interpretable   Features</h2><p><strong>Authors:Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, Duen Horng Chau</strong></p>
<p>Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTsï¼‰çš„ä¸°å¯Œè¡¨ç¤ºæ˜¯å¦å…·æœ‰ç‹¬ç‰¹çš„å±æ€§ï¼Œå¯ä»¥å¢å¼ºå…¶å¯è§£é‡Šæ€§ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºConceptAttentionçš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨DiTæ³¨æ„åŠ›å±‚çš„è¡¨è¾¾èƒ½åŠ›æ¥ç”Ÿæˆç²¾ç¡®å®šä½å›¾åƒä¸­æ–‡æœ¬æ¦‚å¿µçš„é«˜è´¨é‡æ˜¾è‘—æ€§å›¾ã€‚ConceptAttentionä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œå®ƒèƒ½å¤Ÿé‡æ–°åˆ©ç”¨DiTæ³¨æ„åŠ›å±‚çš„å‚æ•°æ¥äº§ç”Ÿé«˜åº¦ä¸Šä¸‹æ–‡åŒ–çš„æ¦‚å¿µåµŒå…¥ï¼Œä¸»è¦å‘ç°æ˜¯åœ¨DiTæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç©ºé—´è¿›è¡Œçº¿æ€§æŠ•å½±ï¼Œä¸å¸¸ç”¨çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ç›¸æ¯”ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´ä¸ºæ¸…æ™°çš„æ˜¾è‘—æ€§å›¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒConceptAttentionç”šè‡³åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ImageNet-Segmentationæ•°æ®é›†ä»¥åŠPascalVOCçš„å•ç±»å­é›†ä¸Šè¶…è¶Šäº†å…¶ä»–11ç§é›¶æ ·æœ¬å¯è§£é‡Šæ€§æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡è¯æ˜ï¼Œå¤šæ¨¡æ€DiTæ¨¡å‹ï¼ˆå¦‚Fluxï¼‰çš„è¡¨ç¤ºé«˜åº¦å¯è½¬ç§»åˆ°åˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ä¸Šï¼Œç”šè‡³è¶…è¶Šäº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04320v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºConceptAttentionçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„è¡¨è¾¾èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡çš„æ˜¾è‘—æ€§åœ°å›¾ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯ç²¾ç¡®å®šä½å›¾åƒä¸­çš„æ–‡æœ¬æ¦‚å¿µã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨DiTæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç©ºé—´è¿›è¡Œçº¿æ€§æŠ•å½±ï¼Œèƒ½äº§ç”Ÿæ¯”å…¶ä»–å¸¸ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ›´æ¸…æ™°é”åˆ©çš„æ˜¾è‘—æ€§åœ°å›¾ã€‚æ­¤å¤–ï¼ŒConceptAttentionåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ImageNet-Segmentationæ•°æ®é›†å’ŒPascalVOCçš„å•ç±»å­é›†ä¸Šè¶…è¶Šäº†å…¶ä»–11ç§é›¶æ ·æœ¬è§£é‡Šæ–¹æ³•ã€‚æœ¬æ–‡é¦–æ¬¡è¯æ˜å¤šæ¨¡æ€DiTæ¨¡å‹ï¼ˆå¦‚Fluxï¼‰çš„è¡¨å¾é«˜åº¦é€‚ç”¨äºåˆ†å‰²ç­‰è§†è§‰ä»»åŠ¡ï¼Œç”šè‡³è¶…è¶Šäº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å¦‚CLIPã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ConceptAttentionåˆ©ç”¨DiTæ³¨æ„åŠ›å±‚çš„ä¸°å¯Œè¡¨ç¤ºç”Ÿæˆç²¾ç¡®çš„å®šä½å›¾åƒä¸­æ–‡æœ¬æ¦‚å¿µçš„æ˜¾è‘—æ€§åœ°å›¾ã€‚</li>
<li>é€šè¿‡åœ¨DiTæ³¨æ„åŠ›å±‚çš„è¾“å‡ºç©ºé—´è¿›è¡Œçº¿æ€§æŠ•å½±ï¼Œäº§ç”Ÿæ›´æ¸…æ™°é”åˆ©çš„æ˜¾è‘—æ€§åœ°å›¾ã€‚</li>
<li>ConceptAttentionåœ¨é›¶æ ·æœ¬å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>ConceptAttentionåœ¨ImageNet-Segmentationå’ŒPascalVOCæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„ç»“æœã€‚</li>
<li>å¤šæ¨¡æ€DiTæ¨¡å‹çš„è¡¨å¾ï¼ˆå¦‚Fluxï¼‰é«˜åº¦é€‚ç”¨äºè§†è§‰ä»»åŠ¡ï¼Œå¦‚åˆ†å‰²ã€‚</li>
<li>ConceptAttentionæ–¹æ³•æ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3014c23a0b1a2f2c7159ac4917374712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d903e49b518abda4ae2ae65147b3547.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f897d1b09feb504edf62474da5e21848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6af98ad11b47f6063a2077f599b976e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee56e2016d42b6e0417f295766240781.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DEALing-with-Image-Reconstruction-Deep-Attentive-Least-Squares"><a href="#DEALing-with-Image-Reconstruction-Deep-Attentive-Least-Squares" class="headerlink" title="DEALing with Image Reconstruction: Deep Attentive Least Squares"></a>DEALing with Image Reconstruction: Deep Attentive Least Squares</h2><p><strong>Authors:Mehrsa Pourya, Erich Kobler, Michael Unser, Sebastian Neumayer</strong></p>
<p>State-of-the-art image reconstruction often relies on complex, highly parameterized deep architectures. We propose an alternative: a data-driven reconstruction method inspired by the classic Tikhonov regularization. Our approach iteratively refines intermediate reconstructions by solving a sequence of quadratic problems. These updates have two key components: (i) learned filters to extract salient image features, and (ii) an attention mechanism that locally adjusts the penalty of filter responses. Our method achieves performance on par with leading plug-and-play and learned regularizer approaches while offering interpretability, robustness, and convergent behavior. In effect, we bridge traditional regularization and deep learning with a principled reconstruction approach. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„å›¾åƒé‡å»ºå¤§å¤šä¾èµ–äºå¤æ‚ã€é«˜åº¦å‚æ•°åŒ–çš„æ·±åº¦æ¶æ„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼šå—ç»å…¸Tikhonovæ­£åˆ™åŒ–å¯å‘çš„æ•°æ®é©±åŠ¨é‡å»ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡è§£å†³ä¸€ç³»åˆ—äºŒæ¬¡é—®é¢˜æ¥è¿­ä»£ä¼˜åŒ–ä¸­é—´é‡å»ºã€‚è¿™äº›æ›´æ–°åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆiï¼‰å­¦ä¹ æ»¤æ³¢å™¨ä»¥æå–å›¾åƒçš„å…³é”®ç‰¹å¾ï¼›ï¼ˆiiï¼‰ä¸€ç§æ³¨æ„æœºåˆ¶ï¼Œå¯å±€éƒ¨è°ƒæ•´æ»¤æ³¢å™¨å“åº”çš„æƒ©ç½šåŠ›åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸é¢†å…ˆçš„å³æ’å³ç”¨å’Œå­¦ä¹ çš„æ­£åˆ™åŒ–æ–¹æ³•æ€§èƒ½ç›¸å½“ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šæ€§ã€ç¨³å¥æ€§å’Œæ”¶æ•›è¡Œä¸ºã€‚å®è´¨ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§æœ‰åŸåˆ™çš„é‡å»ºæ–¹æ³•ï¼Œæ¶èµ·äº†ä¼ ç»Ÿæ­£åˆ™åŒ–å’Œæ·±åº¦å­¦ä¹ ä¹‹é—´çš„æ¡¥æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04079v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨ã€å—Tikhonovæ­£åˆ™åŒ–å¯å‘çš„å›¾åƒé‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ±‚è§£ä¸€ç³»åˆ—äºŒæ¬¡é—®é¢˜æ¥è¿­ä»£ä¼˜åŒ–ä¸­é—´é‡å»ºç»“æœï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå­¦ä¹ æ»¤æ³¢å™¨ä»¥æå–å›¾åƒç‰¹å¾ä»¥åŠæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯å±€éƒ¨è°ƒæ•´æ»¤æ³¢å™¨çš„å“åº”æƒ©ç½šã€‚è¯¥æ–¹æ³•å®ç°äº†ä¸å³æ’å³ç”¨å’Œå­¦ä¹ çš„æ­£åˆ™åŒ–æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·å¤‡å¯è§£é‡Šæ€§ã€é²æ£’æ€§å’Œæ”¶æ•›æ€§ã€‚æœ¬æ–‡åœ¨ä¼ ç»Ÿæ­£åˆ™åŒ–ä¸æ·±åº¦å­¦ä¹ ä¹‹é—´æ¶èµ·äº†ä¸€åº§æœ‰åŸåˆ™çš„é‡å»ºæ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„å›¾åƒé‡å»ºæ–¹æ³•ï¼Œå—åˆ°Tikhonovæ­£åˆ™åŒ–çš„å¯å‘ã€‚</li>
<li>æ–¹æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–ä¸­é—´é‡å»ºç»“æœï¼Œæ±‚è§£ä¸€ç³»åˆ—äºŒæ¬¡é—®é¢˜ã€‚</li>
<li>åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå­¦ä¹ æ»¤æ³¢å™¨æå–å›¾åƒç‰¹å¾å’Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å±€éƒ¨è°ƒæ•´æ»¤æ³¢å™¨çš„å“åº”æƒ©ç½šã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ä¸ç°æœ‰ä¸»æµæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·å¤‡å¯è§£é‡Šæ€§ã€é²æ£’æ€§å’Œæ”¶æ•›æ€§ã€‚</li>
<li>æœ¬æ–‡ç»“åˆäº†ä¼ ç»Ÿæ­£åˆ™åŒ–æŠ€æœ¯å’Œæ·±åº¦å­¦ä¹ ï¼Œä¸ºå›¾åƒé‡å»ºæä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0b7d3b6d923eddf2ed6105d9adbcd65b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f740b816d4cc06a8387aff366cb5ddf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d6bc078c45e15119167ef9df558c578.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97d697b96a5c9314b89a8f57aff30d3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6340ed731e40b2f67b2296609a52083c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Self-supervised-Multimodal-Deep-Learning-Approach-to-Differentiate-Post-radiotherapy-Progression-from-Pseudoprogression-in-Glioblastoma"><a href="#A-Self-supervised-Multimodal-Deep-Learning-Approach-to-Differentiate-Post-radiotherapy-Progression-from-Pseudoprogression-in-Glioblastoma" class="headerlink" title="A Self-supervised Multimodal Deep Learning Approach to Differentiate   Post-radiotherapy Progression from Pseudoprogression in Glioblastoma"></a>A Self-supervised Multimodal Deep Learning Approach to Differentiate   Post-radiotherapy Progression from Pseudoprogression in Glioblastoma</h2><p><strong>Authors:Ahmed Gomaa, Yixing Huang, Pluvio Stephan, Katharina Breininger, Benjamin Frey, Arnd DÃ¶rfler, Oliver Schnell, Daniel Delev, Roland Coras, Charlotte Schmitter, Jenny Stritzelberger, Sabine Semrau, Andreas Maier, Siming Bayer, Stephan SchÃ¶necker, Dieter H Heiland, Peter Hau, Udo S. Gaipl, Christoph Bert, Rainer Fietkau, Manuel A. Schmidt, Florian Putz</strong></p>
<p>Accurate differentiation of pseudoprogression (PsP) from True Progression (TP) following radiotherapy (RT) in glioblastoma (GBM) patients is crucial for optimal treatment planning. However, this task remains challenging due to the overlapping imaging characteristics of PsP and TP. This study therefore proposes a multimodal deep-learning approach utilizing complementary information from routine anatomical MR images, clinical parameters, and RT treatment planning information for improved predictive accuracy. The approach utilizes a self-supervised Vision Transformer (ViT) to encode multi-sequence MR brain volumes to effectively capture both global and local context from the high dimensional input. The encoder is trained in a self-supervised upstream task on unlabeled glioma MRI datasets from the open BraTS2021, UPenn-GBM, and UCSF-PDGM datasets to generate compact, clinically relevant representations from FLAIR and T1 post-contrast sequences. These encoded MR inputs are then integrated with clinical data and RT treatment planning information through guided cross-modal attention, improving progression classification accuracy. This work was developed using two datasets from different centers: the Burdenko Glioblastoma Progression Dataset (n &#x3D; 59) for training and validation, and the GlioCMV progression dataset from the University Hospital Erlangen (UKER) (n &#x3D; 20) for testing. The proposed method achieved an AUC of 75.3%, outperforming the current state-of-the-art data-driven approaches. Importantly, the proposed approach relies on readily available anatomical MRI sequences, clinical data, and RT treatment planning information, enhancing its clinical feasibility. The proposed approach addresses the challenge of limited data availability for PsP and TP differentiation and could allow for improved clinical decision-making and optimized treatment plans for GBM patients. </p>
<blockquote>
<p>åœ¨èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGBMï¼‰æ‚£è€…ä¸­ï¼Œæ”¾å°„æ²»ç–—ï¼ˆRTï¼‰åçš„å‡æ€§è¿›å±•ï¼ˆPsPï¼‰ä¸çœŸå®è¿›å±•ï¼ˆTPï¼‰çš„å‡†ç¡®åŒºåˆ†å¯¹äºåˆ¶å®šæœ€ä½³æ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºPsPå’ŒTPçš„æˆåƒç‰¹å¾é‡å ï¼Œè¿™é¡¹ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¸¸è§„è§£å‰–MRIå›¾åƒã€ä¸´åºŠå‚æ•°å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯çš„äº’è¡¥ä¿¡æ¯ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è‡ªç›‘ç£çš„Vision Transformerï¼ˆViTï¼‰å¯¹å¤šåºåˆ—MRè„‘ä½“ç§¯è¿›è¡Œç¼–ç ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·é«˜ç»´è¾“å…¥çš„å…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚ç¼–ç å™¨åœ¨è‡ªç›‘ç£çš„ä¸Šæ¸¸ä»»åŠ¡ä¸Šæ¥å—è®­ç»ƒï¼Œåœ¨å¼€æ”¾çš„BraTS2021ã€UPenn-GBMå’ŒUCSF-PDGMæ•°æ®é›†çš„æ— æ ‡ç­¾èƒ¶è´¨æ¯ç»†èƒç˜¤MRIæ•°æ®é›†ä¸Šç”Ÿæˆç´§å‡‘ä¸”ä¸´åºŠç›¸å…³çš„è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºæ¥è‡ªFLAIRå’ŒT1å¢å¼ºåºåˆ—ã€‚ç„¶åï¼Œè¿™äº›ç¼–ç çš„MRè¾“å…¥ä¸ä¸´åºŠæ•°æ®å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯é€šè¿‡å¼•å¯¼è·¨æ¨¡æ€æ³¨æ„åŠ›è¿›è¡Œæ•´åˆï¼Œæé«˜äº†è¿›å±•åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚è¿™é¡¹å·¥ä½œä½¿ç”¨äº†æ¥è‡ªä¸¤ä¸ªä¸åŒä¸­å¿ƒçš„ä¸¤ä¸ªæ•°æ®é›†ï¼šBurdenkoèƒ¶è´¨æ¯ç»†èƒç˜¤è¿›å±•æ•°æ®é›†ï¼ˆn&#x3D;5.0ç”¨äºè®­ç»ƒå’ŒéªŒè¯ï¼‰ï¼Œä»¥åŠæ¥è‡ªåŸƒæœ—æ ¹å¤§å­¦åŒ»é™¢ï¼ˆUKERï¼‰çš„GlioCMVè¿›å±•æ•°æ®é›†ï¼ˆn&#x3D;20ç”¨äºæµ‹è¯•ï¼‰ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†75.3%çš„AUCï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸºäºæ•°æ®çš„æ–¹æ³•ã€‚é‡è¦çš„æ˜¯ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¾èµ–äºå¯è·å¾—çš„è§£å‰–MRIåºåˆ—ã€ä¸´åºŠæ•°æ®å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯ï¼Œå¢å¼ºäº†å…¶ä¸´åºŠå¯è¡Œæ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è§£å†³äº†PsPå’ŒTPåŒºåˆ†ä¸­æ•°æ®å¯ç”¨æ€§æœ‰é™çš„æŒ‘æˆ˜ï¼Œå¹¶å¯èƒ½æœ‰åŠ©äºæ”¹è¿›èƒ¶è´¨æ¯ç»†èƒç˜¤æ‚£è€…çš„ä¸´åºŠå†³ç­–å’Œä¼˜åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03999v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ”¾ç–—åé‰´åˆ«å‡æ€§è¿›å±•ï¼ˆPsPï¼‰ä¸çœŸå®è¿›å±•ï¼ˆTPï¼‰åœ¨èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGBMï¼‰æ‚£è€…ä¸­çš„å‡†ç¡®åŒºåˆ†å¯¹äºä¼˜åŒ–æ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºPsPå’ŒTPçš„æˆåƒç‰¹å¾é‡å ï¼Œè¿™ä¸€ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¸¸è§„è§£å‰–MRå›¾åƒã€ä¸´åºŠå‚æ•°å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯çš„äº’è¡¥ä¿¡æ¯ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è‡ªç›‘ç£çš„Vision Transformerï¼ˆViTï¼‰å¯¹å¤šåºåˆ—MRè„‘å®¹ç§¯è¿›è¡Œç¼–ç ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·é«˜ç»´è¾“å…¥çš„å…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚ç¼–ç å™¨åœ¨æœªç»æ ‡è®°çš„èƒ¶è´¨æ¯ç»†èƒç˜¤MRIæ•°æ®é›†ï¼ˆæ¥è‡ªBraTS2021ã€UPenn-GBMå’ŒUCSF-PDGMæ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œè‡ªç›‘ç£ä¸Šæ¸¸ä»»åŠ¡è®­ç»ƒï¼Œä»FLAIRå’ŒT1å¢å¼ºåºåˆ—ç”Ÿæˆç´§å‡‘ã€ä¸´åºŠç›¸å…³çš„è¡¨ç¤ºã€‚è¿™äº›ç¼–ç çš„MRè¾“å…¥ç„¶åä¸ä¸´åºŠæ•°æ®å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯é€šè¿‡å¼•å¯¼è·¨æ¨¡æ€æ³¨æ„åŠ›è¿›è¡Œæ•´åˆï¼Œæé«˜äº†è¿›å±•åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ªä¸åŒä¸­å¿ƒçš„ä¸¤ä¸ªæ•°æ®é›†ï¼šBurdenkoèƒ¶è´¨æ¯ç»†èƒç˜¤è¿›å±•æ•°æ®é›†ï¼ˆn&#x3D;59ï¼‰ç”¨äºè®­ç»ƒå’ŒéªŒè¯ï¼Œä»¥åŠæ¥è‡ªErlangenå¤§å­¦åŒ»é™¢ï¼ˆUKERï¼‰çš„GlioCMVè¿›å±•æ•°æ®é›†ï¼ˆn&#x3D;20ï¼‰ç”¨äºæµ‹è¯•ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†75.3%çš„AUCï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ•°æ®é©±åŠ¨æ–¹æ³•ã€‚é‡è¦çš„æ˜¯ï¼Œè¯¥æ–¹æ³•ä¾èµ–äºå¯è·å¾—çš„è§£å‰–MRIåºåˆ—ã€ä¸´åºŠæ•°æ®å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯ï¼Œå¢å¼ºäº†å…¶ä¸´åºŠå¯è¡Œæ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è§£å†³äº†PsPå’ŒTPé‰´åˆ«ä¸­æ•°æ®æœ‰é™æ€§çš„æŒ‘æˆ˜ï¼Œå¹¶å¯èƒ½æœ‰åŠ©äºæ”¹è¿›GBMæ‚£è€…çš„ä¸´åºŠå†³ç­–å’Œä¼˜åŒ–æ²»ç–—æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒºåˆ†èƒ¶è´¨æ¯ç»†èƒç˜¤æ‚£è€…æ”¾ç–—åçš„å‡æ€§è¿›å±•å’ŒçœŸå®è¿›å±•å¯¹ä¼˜åŒ–æ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰é‰´åˆ«ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå‡æ€§è¿›å±•å’ŒçœŸå®è¿›å±•çš„æˆåƒç‰¹å¾é‡å ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œç»“åˆå¸¸è§„è§£å‰–MRå›¾åƒã€ä¸´åºŠå‚æ•°å’ŒRTæ²»ç–—è®¡åˆ’ä¿¡æ¯ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨è‡ªç›‘ç£çš„Vision Transformerï¼ˆViTï¼‰å¯¹å¤šåºåˆ—MRè„‘ä½“ç§¯è¿›è¡Œç¼–ç ï¼Œæ•æ‰å…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼ŒåŒ…æ‹¬Burdenkoå’ŒGlioCMVæ•°æ®é›†ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†75.3%çš„AUCï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ•°æ®é©±åŠ¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2aa27e290ccdf32c83431b52514193b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deae04eaf91ab9a957a6b862f66d5d2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1a7e0c2842182a6551a698afa769666.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a26c5890a541d4f4b51e9f76a82162e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05df6e3720dc54e17ff2254d3c3112f5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Modeling-fast-X-ray-variability-around-an-accreting-black-hole"><a href="#Modeling-fast-X-ray-variability-around-an-accreting-black-hole" class="headerlink" title="Modeling fast X-ray variability around an accreting black hole"></a>Modeling fast X-ray variability around an accreting black hole</h2><p><strong>Authors:Yejing Zhan, Bei You, Adam Ingram, Wenkang Jiang, Fayin Wang</strong></p>
<p>X-ray inter-band time lags are observed during the outbursts of black hole X-ray binaries (BHXRBs). Timing analysis of fast variability in low Fourier frequency bands shows that high-energy photons lag behind low-energy photons, a phenomenon referred to as hard lag. Conversely, in high Fourier frequency bands, low-energy photons lag behind high-energy photons, known as soft lag. This frequency-dependent lag spectrum suggests that the lags arise from different physical processes. Notably, a trend has been observed wherein the lags shift towards shorter timescales during the rising hard state, indicating an evolution in the inner accretion flow. In this study, we simulate these inter-band lags by conducting Monte Carlo simulations of the rapid variability within the geometry of a jet base corona. We consider both inward propagating accretion rate fluctuations and reverberation (light crossing) delays in our simulations. We successfully reproduce both low-frequency hard lags and high-frequency soft lags in a self-consistent manner. We replicate the observed evolution of the frequency-dependent lag spectra by varying the geometrical scale of the corona and the viscous frequency of the disc. Finally, we discuss the potential of a spherical corona and emphasize that polarization observations from the Imaging X-ray Polarimetry Explorer (IXPE) and the enhanced X-ray Timing and Polarimetry mission (eXTP) will be crucial for distinguishing the coronaâ€™s geometry in future studies. </p>
<blockquote>
<p>åœ¨é»‘æ´Xå°„çº¿åŒæ˜Ÿï¼ˆBHXRBsï¼‰çš„çˆ†å‘æœŸé—´ï¼Œè§‚å¯Ÿåˆ°Xå°„çº¿æ³¢æ®µä¹‹é—´çš„æ—¶é—´å»¶è¿Ÿã€‚å¯¹ä½å‚…é‡Œå¶é¢‘æ®µå¿«é€Ÿå˜åŒ–çš„æ—¶åºåˆ†æè¡¨æ˜ï¼Œé«˜èƒ½å…‰å­è½åäºä½èƒ½å…‰å­ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºç¡¬å»¶è¿Ÿã€‚ç›¸åï¼Œåœ¨é«˜å‚…é‡Œå¶é¢‘æ®µï¼Œä½èƒ½å…‰å­è½åäºé«˜èƒ½å…‰å­ï¼Œè¢«ç§°ä¸ºè½¯å»¶è¿Ÿã€‚è¿™ç§ä¸é¢‘ç‡ç›¸å…³çš„å»¶è¿Ÿè°±è¡¨æ˜å»¶è¿Ÿæ¥è‡ªäºä¸åŒçš„ç‰©ç†è¿‡ç¨‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå·²ç»è§‚å¯Ÿåˆ°ä¸€ç§è¶‹åŠ¿ï¼Œå³å»¶è¿Ÿå‘è¾ƒçŸ­çš„æ—¶é—´å°ºåº¦è½¬å˜ï¼Œè¿™åœ¨ç¡¬æ€ä¸Šå‡æœŸé—´è¡¨æ˜å†…å¸ç§¯æµçš„æ¼”å˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿå–·æµåŸºå†•å‡ ä½•ç»“æ„å†…å¿«é€Ÿå˜åŒ–çš„è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿæ¥ç ”ç©¶è¿™äº›æ³¢æ®µé—´çš„å»¶è¿Ÿã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿä¸­è€ƒè™‘äº†å‘å†…ä¼ æ’­çš„å¸ç§¯ç‡æ³¢åŠ¨å’Œå›å£°ï¼ˆå…‰é€Ÿç©¿è¶Šï¼‰å»¶è¿Ÿã€‚æˆ‘ä»¬æˆåŠŸåœ°ä»¥ä¸€è‡´çš„æ–¹å¼å†ç°äº†ä½é¢‘ç¡¬å»¶è¿Ÿå’Œé«˜é¢‘è½¯å»¶è¿Ÿã€‚æˆ‘ä»¬é€šè¿‡æ”¹å˜å†•çš„å‡ ä½•å°ºåº¦å’Œç›˜çš„ç²˜æ€§é¢‘ç‡æ¥å¤åˆ¶è§‚å¯Ÿåˆ°çš„é¢‘ç‡ä¾èµ–å»¶è¿Ÿè°±çš„æ¼”åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†çƒå½¢å†•çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒæˆåƒXå°„çº¿åæŒ¯ä»ªï¼ˆIXPEï¼‰å’Œå¢å¼ºå‹Xå°„çº¿å®šæ—¶åæŒ¯ä»»åŠ¡ï¼ˆeXTPï¼‰çš„åæŒ¯è§‚æµ‹å¯¹äºæœªæ¥ç ”ç©¶ä¸­åŒºåˆ†å†•çš„å‡ ä½•å½¢çŠ¶å°†è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03995v1">PDF</a> 17 pages, 9 figures, submitted to ApJ</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é»‘æ´Xå°„çº¿åŒæ˜Ÿï¼ˆBHXRBsï¼‰çˆ†å‘æœŸé—´çš„Xå°„çº¿è·¨æ³¢æ®µæ—¶é—´å»¶è¿Ÿç°è±¡ã€‚ç ”ç©¶é€šè¿‡å¯¹å¿«é€Ÿå˜å¼‚çš„æ—¶åºåˆ†æå‘ç°ï¼Œä½èƒ½é‡å…‰å­åœ¨é«˜èƒ½é‡å…‰å­ä¹‹å‰åˆ°è¾¾ï¼Œç§°ä¸ºç¡¬æ»åç°è±¡ï¼›è€Œåœ¨é«˜å‚…ç«‹å¶é¢‘ç‡æ³¢æ®µï¼Œåˆ™æ˜¯é«˜èƒ½é‡å…‰å­å…ˆäºä½èƒ½é‡å…‰å­åˆ°è¾¾ï¼Œç§°ä¸ºè½¯æ»åç°è±¡ã€‚é€šè¿‡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼ŒæˆåŠŸå†ç°äº†è¿™ç§è·¨æ³¢æ®µæ»åç°è±¡ï¼Œå¹¶è®¨è®ºäº†çƒçŠ¶å†•çš„æ½œåœ¨å¯èƒ½æ€§ã€‚æœªæ¥ç ”ç©¶ä¸­ï¼ŒæˆåƒXå°„çº¿åæŒ¯æ¢æµ‹å™¨ï¼ˆIXPEï¼‰å’Œå¢å¼ºå‹Xå°„çº¿å®šæ—¶åæŒ¯ä»»åŠ¡ï¼ˆeXTPï¼‰çš„åæŒ¯è§‚æµ‹å¯¹äºé‰´åˆ«å†•çš„å‡ ä½•å½¢æ€è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é»‘æ´Xå°„çº¿åŒæ˜Ÿï¼ˆBHXRBsï¼‰çˆ†å‘æœŸé—´è§‚å¯Ÿåˆ°Xå°„çº¿çš„è·¨æ³¢æ®µæ—¶é—´å»¶è¿Ÿç°è±¡ã€‚</li>
<li>è·¨æ³¢æ®µæ—¶é—´å»¶è¿Ÿè¡¨ç°å‡ºé¢‘ç‡ä¾èµ–æ€§ï¼Œåæ˜ ä¸åŒçš„ç‰©ç†è¿‡ç¨‹ã€‚</li>
<li>è’™ç‰¹å¡æ´›æ¨¡æ‹ŸæˆåŠŸå†ç°äº†ç¡¬æ»åå’Œè½¯æ»åç°è±¡ã€‚</li>
<li>ç¡¬æ€ä¸Šå‡æœŸé—´çš„æ—¶é—´å»¶è¿Ÿå‘æ›´çŸ­æ—¶é—´å°ºåº¦è½¬å˜ï¼Œè¡¨æ˜å†…å¸ç§¯æµçš„æ¼”åŒ–ã€‚</li>
<li>é€šè¿‡æ”¹å˜å†•çš„å‡ ä½•å°ºåº¦å’Œç£ç›˜çš„ç²˜æ€§é¢‘ç‡ï¼Œæ¨¡æ‹Ÿäº†é¢‘ç‡ä¾èµ–çš„æ»åå…‰è°±æ¼”åŒ–ã€‚</li>
<li>çƒçŠ¶å†•çš„æ½œåœ¨å¯èƒ½æ€§è¢«è®¨è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c02bb161a788494a18b37a96ca4f40f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475287e37c41f1095bc163749419b0e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67c6d2e30b7645858ebd809efac95be.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Semantic-Feature-Division-Multiple-Access-for-Digital-Semantic-Broadcast-Channels"><a href="#Semantic-Feature-Division-Multiple-Access-for-Digital-Semantic-Broadcast-Channels" class="headerlink" title="Semantic Feature Division Multiple Access for Digital Semantic Broadcast   Channels"></a>Semantic Feature Division Multiple Access for Digital Semantic Broadcast   Channels</h2><p><strong>Authors:Shuai Ma, Zhiye Sun, Bin Shen, Youlong Wu, Hang Li, Guangming Shi, Shiyin Li, Naofal Al-Dhahir</strong></p>
<p>In this paper, we propose a digital semantic feature division multiple access (SFDMA) paradigm in multi-user broadcast (BC) networks for the inference and the image reconstruction tasks. In this SFDMA scheme, the multi-user semantic information is encoded into discrete approximately orthogonal representations, and the encoded semantic features of multiple users can be simultaneously transmitted in the same time-frequency resource. Specifically, for inference tasks, we design a SFDMA digital BC network based on robust information bottleneck (RIB), which can achieve a tradeoff between inference performance, data compression and multi-user interference. Moreover, for image reconstruction tasks, we develop a SFDMA digital BC network by utilizing a Swin Transformer, which significantly reduces multi-user interference. More importantly, SFDMA can protect the privacy of usersâ€™ semantic information, in which each receiver can only decode its own semantic information. Furthermore, we establish a relationship between performance and signal to interference plus noise ratio (SINR), which is fitted by an Alpha-Beta-Gamma (ABG) function. Furthermore, an optimal power allocation method is developed for the inference and reconstruction tasks. Extensive simulations verify the effectiveness and superiority of our proposed SFDMA scheme. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šç”¨æˆ·å¹¿æ’­ï¼ˆBCï¼‰ç½‘ç»œä¸­ç”¨äºæ¨ç†å’Œå›¾åƒé‡å»ºä»»åŠ¡çš„æ•°å­—è¯­ä¹‰ç‰¹å¾åˆ’åˆ†å¤šå€æ¥å…¥ï¼ˆSFDMAï¼‰èŒƒå¼ã€‚åœ¨SFDMAæ–¹æ¡ˆä¸­ï¼Œå¤šç”¨æˆ·è¯­ä¹‰ä¿¡æ¯è¢«ç¼–ç æˆç¦»æ•£è¿‘ä¼¼æ­£äº¤è¡¨ç¤ºï¼Œå¤šä¸ªç”¨æˆ·çš„ç¼–ç è¯­ä¹‰ç‰¹å¾å¯ä»¥åœ¨åŒä¸€æ—¶é—´-é¢‘ç‡èµ„æºä¸­åŒæ—¶ä¼ è¾“ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºç¨³å¥ä¿¡æ¯ç“¶é¢ˆï¼ˆRIBï¼‰çš„SFDMAæ•°å­—BCç½‘ç»œï¼Œå¯ä»¥åœ¨æ¨ç†æ€§èƒ½ã€æ•°æ®å‹ç¼©å’Œå¤šç”¨æˆ·å¹²æ‰°ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æ­¤å¤–ï¼Œå¯¹äºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ©ç”¨Swin Transformerå¼€å‘äº†ä¸€ç§SFDMAæ•°å­—BCç½‘ç»œï¼Œè¿™æ˜¾è‘—é™ä½äº†å¤šç”¨æˆ·å¹²æ‰°ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒSFDMAå¯ä»¥ä¿æŠ¤ç”¨æˆ·è¯­ä¹‰ä¿¡æ¯çš„éšç§ï¼Œå…¶ä¸­æ¯ä¸ªæ¥æ”¶å™¨åªèƒ½è§£ç å…¶è‡ªå·±çš„è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†æ€§èƒ½ä¸ä¿¡å¹²å™ªæ¯”ï¼ˆSINRï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡Alpha-Beta-Gammaï¼ˆABGï¼‰å‡½æ•°è¿›è¡Œæ‹Ÿåˆã€‚æ­¤å¤–ï¼Œè¿˜é’ˆå¯¹æ¨ç†å’Œé‡å»ºä»»åŠ¡å¼€å‘äº†ä¸€ç§æœ€ä¼˜åŠŸç‡åˆ†é…æ–¹æ³•ã€‚å¤§é‡ä»¿çœŸéªŒè¯äº†æ‰€æSFDMAæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03949v1">PDF</a> 14 pages, 13 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸæå‡ºäº†ä¸€ç§æ•°å­—è¯­ä¹‰ç‰¹å¾åˆ†å‰²å¤šå€è®¿é—®ï¼ˆSFDMAï¼‰èŒƒå¼ï¼Œç”¨äºå¤šç”¨æˆ·å¹¿æ’­ç½‘ç»œçš„æ¨æ–­å’Œå›¾åƒé‡å»ºä»»åŠ¡ã€‚æ­¤æ–¹æ¡ˆå¯å°†å¤šç”¨æˆ·è¯­ä¹‰ä¿¡æ¯ç¼–ç æˆç¦»æ•£è¿‘ä¼¼æ­£äº¤è¡¨ç¤ºï¼Œå¹¶åœ¨åŒä¸€æ—¶é—´-é¢‘ç‡èµ„æºä¸­åŒæ—¶ä¼ è¾“ç¼–ç çš„è¯­ä¹‰ç‰¹å¾ã€‚è®¾è®¡ç”¨äºæ¨æ–­ä»»åŠ¡çš„SFDMAæ•°å­—å¹¿æ’­ç½‘ç»œå¯å®ç°æ¨æ–­æ€§èƒ½ã€æ•°æ®å‹ç¼©å’Œå¤šç”¨æˆ·å¹²æ‰°ä¹‹é—´çš„æƒè¡¡ï¼›ç”¨äºå›¾åƒé‡å»ºä»»åŠ¡çš„ç½‘ç»œåˆ™é‡‡ç”¨Swin Transformerï¼Œæ˜¾è‘—é™ä½å¤šç”¨æˆ·å¹²æ‰°ã€‚æ­¤å¤–ï¼ŒSFDMAå¯ä¿æŠ¤ç”¨æˆ·è¯­ä¹‰ä¿¡æ¯çš„éšç§ï¼Œä¸”æ€§èƒ½ä¸ä¿¡å¹²å™ªæ¯”ï¼ˆSINRï¼‰ä¹‹é—´å­˜åœ¨å…³è”ï¼Œç”±Alpha-Beta-Gammaï¼ˆABGï¼‰å‡½æ•°æ‹Ÿåˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ•°å­—è¯­ä¹‰ç‰¹å¾åˆ†å‰²å¤šå€è®¿é—®ï¼ˆSFDMAï¼‰èŒƒå¼ï¼Œé€‚ç”¨äºå¤šç”¨æˆ·å¹¿æ’­ç½‘ç»œã€‚</li>
<li>SFDMAå°†å¤šç”¨æˆ·è¯­ä¹‰ä¿¡æ¯ç¼–ç æˆç¦»æ•£è¿‘ä¼¼æ­£äº¤è¡¨ç¤ºï¼Œå®ç°åŒæ—¶ä¼ è¾“ã€‚</li>
<li>é’ˆå¯¹æ¨æ–­ä»»åŠ¡ï¼Œè®¾è®¡åŸºäºç¨³å¥ä¿¡æ¯ç“¶é¢ˆï¼ˆRIBï¼‰çš„SFDMAæ•°å­—å¹¿æ’­ç½‘ç»œã€‚</li>
<li>é‡‡ç”¨Swin Transformeræ„å»ºç”¨äºå›¾åƒé‡å»ºä»»åŠ¡çš„SFDMAæ•°å­—å¹¿æ’­ç½‘ç»œã€‚</li>
<li>SFDMAèƒ½å¤Ÿä¿æŠ¤ç”¨æˆ·è¯­ä¹‰ä¿¡æ¯çš„éšç§ã€‚</li>
<li>æ€§èƒ½ä¸ä¿¡å¹²å™ªæ¯”ï¼ˆSINRï¼‰ä¹‹é—´å­˜åœ¨å…³è”ï¼Œç”±Alpha-Beta-Gammaï¼ˆABGï¼‰å‡½æ•°æè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-886b8f1715fb6c5cdc19b58b3394b91e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d64b4f5dd9f48e721fc4b5b777c28e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96435c504bfb6d563a270e08aaafb062.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fba567d5774ef9f76c32a428bbd57fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03803d67930e39288373dfede8ed82d8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FE-UNet-Frequency-Domain-Enhanced-U-Net-with-Segment-Anything-Capability-for-Versatile-Image-Segmentation"><a href="#FE-UNet-Frequency-Domain-Enhanced-U-Net-with-Segment-Anything-Capability-for-Versatile-Image-Segmentation" class="headerlink" title="FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything   Capability for Versatile Image Segmentation"></a>FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything   Capability for Versatile Image Segmentation</h2><p><strong>Authors:Guohao Huo, Ruiting Dai, Ling Shao, Hao Tang</strong></p>
<p>Image segmentation is a critical task in visual understanding. Convolutional Neural Networks (CNNs) are predisposed to capture high-frequency features in images, while Transformers exhibit a contrasting focus on low-frequency features. In this paper, we experimentally quantify the contrast sensitivity function of CNNs and compare it with that of the human visual system, informed by the seminal experiments of Mannos and Sakrison. Leveraging these insights, we propose the Wavelet-Guided Spectral Pooling Module (WSPM) to enhance and balance image features across the frequency domain. To further emulate the human visual system, we introduce the Frequency Domain Enhanced Receptive Field Block (FE-RFB), which integrates WSPM to extract enriched features from the frequency domain. Building on these innovations, we develop FE-UNet, a model that utilizes SAM2 as its backbone and incorporates Hiera-Large as a pre-trained block, designed to enhance generalization capabilities while ensuring high segmentation accuracy. Experimental results demonstrate that FE-UNet achieves state-of-the-art performance in diverse tasks, including marine animal and polyp segmentation, underscoring its versatility and effectiveness. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯è§†è§‰ç†è§£ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å€¾å‘äºæ•æ‰å›¾åƒä¸­çš„é«˜é¢‘ç‰¹å¾ï¼Œè€ŒTransformeråˆ™æ›´å…³æ³¨ä½é¢‘ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒé‡åŒ–CNNsçš„å¯¹æ¯”æ•æ„Ÿåº¦å‡½æ•°ï¼Œå¹¶å°†å…¶ä¸äººç±»è§†è§‰ç³»ç»Ÿè¿›è¡Œæ¯”è¾ƒï¼Œè¿™å¾—ç›ŠäºMannoså’ŒSakrisonçš„ç»å…¸å®éªŒã€‚å€ŸåŠ©è¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†å°æ³¢å¼•å¯¼è°±æ± æ¨¡å—ï¼ˆWSPMï¼‰ï¼Œä»¥å¢å¼ºå’Œå¹³è¡¡é¢‘ç‡åŸŸä¸­çš„å›¾åƒç‰¹å¾ã€‚ä¸ºäº†æ›´è¿›ä¸€æ­¥æ¨¡æ‹Ÿäººç±»è§†è§‰ç³»ç»Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢‘ç‡åŸŸå¢å¼ºæ„Ÿå—é‡å—ï¼ˆFE-RFBï¼‰ï¼Œè¯¥æ¨¡å—ç»“åˆäº†WSPMï¼Œå¯ä»é¢‘ç‡åŸŸä¸­æå–ä¸°å¯Œçš„ç‰¹å¾ã€‚åŸºäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬å¼€å‘äº†FE-UNetæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥SAM2ä½œä¸ºéª¨å¹²ç½‘ï¼Œå¹¶èå…¥äº†Hiera-Largeé¢„è®­ç»ƒå—ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹çš„é€šç”¨åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ç¡®ä¿é«˜åˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFE-UNetåœ¨ä¸åŒä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æµ·æ´‹åŠ¨ç‰©å’Œæ¯è‚‰åˆ†å‰²ï¼Œå‡¸æ˜¾äº†å…¶å¤šæ ·æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03829v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å›¾åƒåˆ†å‰²ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸äººç±»è§†è§‰ç³»ç»Ÿçš„å¯¹æ¯”ã€‚é€šè¿‡å€Ÿé‰´Mannoså’ŒSakrisonçš„ç»å…¸å®éªŒï¼Œå¯¹CNNçš„å¯¹æ¯”æ•æ„Ÿåº¦å‡½æ•°è¿›è¡Œäº†é‡åŒ–åˆ†æã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†åŸºäºå°æ³¢å¼•å¯¼è°±æ± åŒ–æ¨¡å—ï¼ˆWSPMï¼‰å’Œé¢‘ç‡åŸŸå¢å¼ºæ„Ÿå—é‡å—ï¼ˆFE-RFBï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæé«˜å’Œå¹³è¡¡å›¾åƒåœ¨ä¸åŒé¢‘ç‡åŸŸçš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå¹¶æ¨¡æ‹Ÿäººç±»è§†è§‰ç³»ç»Ÿã€‚æœ€åï¼Œå¼€å‘äº†ä¸€ç§åä¸ºFE-UNetçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»¥SAM2ä¸ºéª¨å¹²ï¼Œç»“åˆé¢„å…ˆè®­ç»ƒçš„Hiera-Largeå—ï¼Œæ—¨åœ¨æé«˜æ³›åŒ–èƒ½åŠ›å’Œåˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFE-UNetåœ¨åŒ…æ‹¬æµ·æ´‹ç”Ÿç‰©å’Œæ¯è‚‰åˆ†å‰²ç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œæ˜¾ç¤ºäº†å…¶çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²ä¸­å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸äººç±»è§†è§‰ç³»ç»Ÿçš„å¯¹æ¯”ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å¯¹CNNçš„å¯¹æ¯”æ•æ„Ÿåº¦å‡½æ•°è¿›è¡Œé‡åŒ–åˆ†æï¼Œå€Ÿé‰´äº†Mannoså’ŒSakrisonçš„ç»å…¸å®éªŒã€‚</li>
<li>å¼•å…¥å°æ³¢å¼•å¯¼è°±æ± åŒ–æ¨¡å—ï¼ˆWSPMï¼‰ä»¥å¢å¼ºå’Œå¹³è¡¡å›¾åƒåœ¨ä¸åŒé¢‘ç‡åŸŸçš„ç‰¹å¾ã€‚</li>
<li>æå‡ºé¢‘ç‡åŸŸå¢å¼ºæ„Ÿå—é‡å—ï¼ˆFE-RFBï¼‰ä»¥æ¨¡æ‹Ÿäººç±»è§†è§‰ç³»ç»Ÿã€‚</li>
<li>å¼€å‘FE-UNetæ¨¡å‹ï¼Œç»“åˆSAM2éª¨å¹²å’ŒHiera-Largeé¢„è®­ç»ƒå—ï¼Œæ—¨åœ¨æé«˜æ³›åŒ–èƒ½åŠ›å’Œåˆ†å‰²ç²¾åº¦ã€‚</li>
<li>FE-UNetåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬æµ·æ´‹ç”Ÿç‰©å’Œæ¯è‚‰åˆ†å‰²ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03829">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9b1573c764cebe8085907138d5ec439.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15923491ae3c72c3e2814c6b6c043d07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64d8c9e5643f0adf9d2701b14534fb63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b46de82a2b56a53ee4ab9ea8bdf24c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af1ab989ddb835a05ef2cb2ae01f361e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UltraBones100k-An-Ultrasound-Image-Dataset-with-CT-Derived-Labels-for-Lower-Extremity-Long-Bone-Surface-Segmentation"><a href="#UltraBones100k-An-Ultrasound-Image-Dataset-with-CT-Derived-Labels-for-Lower-Extremity-Long-Bone-Surface-Segmentation" class="headerlink" title="UltraBones100k: An Ultrasound Image Dataset with CT-Derived Labels for   Lower Extremity Long Bone Surface Segmentation"></a>UltraBones100k: An Ultrasound Image Dataset with CT-Derived Labels for   Lower Extremity Long Bone Surface Segmentation</h2><p><strong>Authors:Luohong Wu, Nicola A. Cavalcanti, Matthias Seibold, Giuseppe Loggia, Lisa Reissner, Jonas Hein, Silvan Beeler, Arnd ViehÃ¶fer, Stephan Wirth, Lilian Calvet, Philipp FÃ¼rnstahl</strong></p>
<p>Ultrasound-based bone surface segmentation is crucial in computer-assisted orthopedic surgery. However, ultrasound images have limitations, including a low signal-to-noise ratio, and acoustic shadowing, which make interpretation difficult. Existing deep learning models for bone segmentation rely primarily on costly manual labeling by experts, limiting dataset size and model generalizability. Additionally, the complexity of ultrasound physics and acoustic shadow makes the images difficult for humans to interpret, leading to incomplete labels in anechoic regions and limiting model performance. To advance ultrasound bone segmentation and establish effective model benchmarks, larger and higher-quality datasets are needed.   We propose a methodology for collecting ex-vivo ultrasound datasets with automatically generated bone labels, including anechoic regions. The proposed labels are derived by accurately superimposing tracked bone CT models onto the tracked ultrasound images. These initial labels are refined to account for ultrasound physics. A clinical evaluation is conducted by an expert physician specialized on orthopedic sonography to assess the quality of the generated bone labels. A neural network for bone segmentation is trained on the collected dataset and its predictions are compared to expert manual labels, evaluating accuracy, completeness, and F1-score.   We collected the largest known dataset of 100k ultrasound images of human lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank test with Bonferroni correction confirmed that the bone alignment after our method significantly improved the quality of bone labeling (p &lt; 0.001). The model trained on UltraBones100k consistently outperforms manual labeling in all metrics, particularly in low-intensity regions (320% improvement in completeness at a distance threshold of 0.5 mm). </p>
<blockquote>
<p>åŸºäºè¶…å£°çš„éª¨éª¼è¡¨é¢åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¶…å£°å›¾åƒå­˜åœ¨ä¿¡å·å™ªå£°æ¯”ä½å’Œå£°å½±ç­‰å±€é™æ€§ï¼Œä½¿å¾—è§£è¯»å›°éš¾ã€‚ç°æœ‰çš„éª¨éª¼åˆ†å‰²æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸»è¦ä¾èµ–äºä¸“å®¶æ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨ï¼Œè¿™é™åˆ¶äº†æ•°æ®é›†çš„å¤§å°å’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¶…å£°ç‰©ç†å’Œå£°å½±çš„å¤æ‚æ€§ä½¿å¾—å›¾åƒå¯¹äººç±»æ¥è¯´éš¾ä»¥è§£è¯»ï¼Œå¯¼è‡´æ— å£°åŒºåŸŸçš„æ ‡ç­¾ä¸å®Œæ•´ï¼Œå¹¶é™åˆ¶äº†æ¨¡å‹æ€§èƒ½ã€‚ä¸ºäº†æ¨è¿›è¶…å£°éª¨åˆ†å‰²å¹¶å»ºç«‹æœ‰æ•ˆçš„æ¨¡å‹åŸºå‡†ï¼Œéœ€è¦æ›´å¤§ã€æ›´é«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¶é›†ç¦»ä½“è¶…å£°æ•°æ®é›†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è‡ªåŠ¨ç”Ÿæˆéª¨éª¼æ ‡ç­¾ï¼ŒåŒ…æ‹¬æ— å£°åŒºåŸŸã€‚è¿™äº›æ ‡ç­¾æ˜¯é€šè¿‡å°†è·Ÿè¸ªçš„éª¨éª¼CTæ¨¡å‹å‡†ç¡®å åŠ åˆ°è·Ÿè¸ªçš„è¶…å£°å›¾åƒä¸Šè€Œå¾—å‡ºçš„ã€‚è¿™äº›åˆå§‹æ ‡ç­¾ç»è¿‡æ”¹è¿›ï¼Œä»¥è€ƒè™‘è¶…å£°ç‰©ç†ã€‚ç”±ä¸“æ”»éª¨ç§‘è¶…å£°çš„ä¸“å®¶åŒ»ç”Ÿè¿›è¡Œä¸´åºŠè¯„ä¼°ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„éª¨éª¼æ ‡ç­¾çš„è´¨é‡ã€‚åœ¨æ”¶é›†çš„æ•°æ®é›†ä¸Šè®­ç»ƒéª¨éª¼åˆ†å‰²çš„ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶é¢„æµ‹ç»“æœä¸ä¸“å®¶æ‰‹åŠ¨æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’ŒF1åˆ†æ•°ã€‚æˆ‘ä»¬æ”¶é›†äº†å·²çŸ¥æœ€å¤§è§„æ¨¡çš„ä¸‹è‚¢éª¨éª¼æ ‡ç­¾è¶…å£°å›¾åƒæ•°æ®é›†ï¼Œç§°ä¸ºUltraBones100kæ•°æ®é›†ã€‚é‡‡ç”¨å¨å°”ç§‘å…‹æ£®ç¬¦å·ç§©æ£€éªŒè¿›è¡ŒBonferroniæ ¡æ­£åè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†éª¨éª¼æ ‡è®°çš„è´¨é‡ï¼ˆp &lt; 0.001ï¼‰ã€‚åœ¨UltraBones100kæ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å¼ºåº¦åŒºåŸŸï¼ˆåœ¨è·ç¦»é˜ˆå€¼ä¸º0.5æ¯«ç±³çš„æƒ…å†µä¸‹ï¼Œå®Œæ•´æ€§æé«˜äº†320%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03783v1">PDF</a> 13 pages, 4 figures</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†åŸºäºè¶…å£°çš„éª¨éª¼è¡¨é¢åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹ä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨çš„ä¾èµ–é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆCTæ¨¡å‹å’Œè¶…å£°å›¾åƒè‡ªåŠ¨ç”Ÿæˆéª¨éª¼æ ‡ç­¾çš„æ–¹æ³•ï¼Œå¹¶å»ºç«‹äº†åŒ…å«åä¸‡å¼ äººç±»ä¸‹è‚¢è¶…å£°å›¾åƒçš„æœ€å¤§çš„éª¨éª¼æ ‡ç­¾æ•°æ®é›†UltraBones100kã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†éª¨éª¼æ ‡æ³¨çš„è´¨é‡ï¼Œå¹¶ä¸”åŸºäºè¯¥æ•°æ®é›†çš„æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½å¼ºåº¦åŒºåŸŸçš„æ€§èƒ½æœ‰å¤§å¹…æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°éª¨è¡¨é¢åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©éª¨ç§‘æ‰‹æœ¯ä¸­å¾ˆé‡è¦ï¼Œä½†å­˜åœ¨ä¿¡å·å™ªå£°æ¯”ä½å’Œå£°å½±ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹ä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨çš„ä¾èµ–é™åˆ¶äº†æ•°æ®é›†å¤§å°å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆCTæ¨¡å‹å’Œè¶…å£°å›¾åƒè‡ªåŠ¨ç”Ÿæˆéª¨éª¼æ ‡ç­¾çš„æ–¹æ³•ã€‚</li>
<li>æˆåŠŸå»ºç«‹äº†åŒ…å«åä¸‡å¼ äººç±»ä¸‹è‚¢è¶…å£°å›¾åƒçš„æœ€å¤§çš„éª¨éª¼æ ‡ç­¾æ•°æ®é›†UltraBones100kã€‚</li>
<li>ä¸´åºŠè¯„ä¼°æ˜¾ç¤ºç”Ÿæˆçš„éª¨éª¼æ ‡ç­¾è´¨é‡é«˜ã€‚</li>
<li>åŸºäºUltraBones100kæ•°æ®é›†çš„æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šå‡ä¼˜äºæ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fde67b922bce93fce52a783c96695ff3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fc158787a800e66a12cc9d59ab269a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71037e76f2baaa6e5377c6459b26b4c2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Retrospective-Systematic-Study-on-Hierarchical-Sparse-Query-Transformer-assisted-Ultrasound-Screening-for-Early-Hepatocellular-Carcinoma"><a href="#A-Retrospective-Systematic-Study-on-Hierarchical-Sparse-Query-Transformer-assisted-Ultrasound-Screening-for-Early-Hepatocellular-Carcinoma" class="headerlink" title="A Retrospective Systematic Study on Hierarchical Sparse Query   Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma"></a>A Retrospective Systematic Study on Hierarchical Sparse Query   Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma</h2><p><strong>Authors:Chaoyin She, Ruifang Lu, Danni He, Jiayi Lv, Yadan Lin, Meiqing Cheng, Hui Huang, Lida Chen, Wei Wang, Qinghua Huang</strong></p>
<p>Hepatocellular carcinoma (HCC) ranks as the third leading cause of cancer-related mortality worldwide, with early detection being crucial for improving patient survival rates. However, early screening for HCC using ultrasound suffers from insufficient sensitivity and is highly dependent on the expertise of radiologists for interpretation. Leveraging the latest advancements in artificial intelligence (AI) in medical imaging, this study proposes an innovative Hierarchical Sparse Query Transformer (HSQformer) model that combines the strengths of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance the accuracy of HCC diagnosis in ultrasound screening. The HSQformer leverages sparse latent space representations to capture hierarchical details at various granularities without the need for complex adjustments, and adopts a modular, plug-and-play design philosophy, ensuring the modelâ€™s versatility and ease of use. The HSQformerâ€™s performance was rigorously tested across three distinct clinical scenarios: single-center, multi-center, and high-risk patient testing. In each of these settings, it consistently outperformed existing state-of-the-art models, such as ConvNext and SwinTransformer. Notably, the HSQformer even matched the diagnostic capabilities of senior radiologists and comprehensively surpassed those of junior radiologists. The experimental results from this study strongly demonstrate the effectiveness and clinical potential of AI-assisted tools in HCC screening. The full code is available at <a target="_blank" rel="noopener" href="https://github.com/Asunatan/HSQformer">https://github.com/Asunatan/HSQformer</a>. </p>
<blockquote>
<p>è‚ç»†èƒç™Œï¼ˆHCCï¼‰æ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡ç‡çš„ç¬¬ä¸‰å¤§ä¸»è¦åŸå› ï¼Œæ—©æœŸå‘ç°å¯¹äºæé«˜æ‚£è€…å­˜æ´»ç‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä½¿ç”¨è¶…å£°è¿›è¡ŒHCCçš„æ—©æœŸç­›æŸ¥æ•æ„Ÿæ€§ä¸è¶³ï¼Œå¹¶ä¸”é«˜åº¦ä¾èµ–äºæ”¾å°„ç§‘åŒ»ç”Ÿçš„è§£é‡Šä¸“é•¿ã€‚æœ¬ç ”ç©¶åˆ©ç”¨åŒ»ç–—å½±åƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åˆ›æ–°çš„åˆ†å±‚ç¨€ç–æŸ¥è¯¢è½¬æ¢å™¨ï¼ˆHSQformerï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„ä¼˜ç‚¹ï¼Œæé«˜äº†è¶…å£°ç­›æŸ¥ä¸­HCCè¯Šæ–­çš„å‡†ç¡®æ€§ã€‚HSQformeråˆ©ç”¨ç¨€ç–æ½œåœ¨ç©ºé—´è¡¨ç¤ºæ¥æ•è·å„ç§ç²’åº¦ä¸‹çš„å±‚æ¬¡ç»†èŠ‚ï¼Œè€Œæ— éœ€è¿›è¡Œå¤æ‚è°ƒæ•´ï¼Œå¹¶é‡‡ç”¨äº†æ¨¡å—åŒ–ã€å³æ’å³ç”¨çš„è®¾è®¡å“²å­¦ï¼Œç¡®ä¿æ¨¡å‹çš„é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚HSQformerçš„æ€§èƒ½åœ¨ä¸‰ç§ä¸åŒçš„ä¸´åºŠåœºæ™¯ï¼šå•ä¸­å¿ƒã€å¤šä¸­å¿ƒå’Œé«˜å±æ‚£è€…æµ‹è¯•ä¸­éƒ½ç»è¿‡äº†ä¸¥æ ¼æµ‹è¯•ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå®ƒå§‹ç»ˆè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚ConvNextå’ŒSwinTransformerã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒHSQformerç”šè‡³è¾¾åˆ°äº†èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­æ°´å¹³ï¼Œå¹¶å…¨é¢è¶…è¶Šäº†åˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¡¨ç°ã€‚æœ¬ç ”ç©¶çš„å®éªŒç»“æœå¼ºçƒˆè¯æ˜äº†AIè¾…åŠ©å·¥å…·åœ¨HCCç­›æŸ¥ä¸­çš„æœ‰æ•ˆæ€§å’Œä¸´åºŠæ½œåŠ›ã€‚å®Œæ•´çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Asunatan/HSQformer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Asunatan/HSQformerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03772v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æŠ€æœ¯ï¼Œç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„ä¼˜åŠ¿ï¼Œå¼€å‘å‡ºä¸€ç§åä¸ºHSQformerçš„æ–°å‹æ¨¡å‹ï¼Œç”¨äºæé«˜è‚ç™Œï¼ˆHCCï¼‰è¶…å£°ç­›æŸ¥è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚HSQformeré‡‡ç”¨åˆ†å±‚ç¨€ç–æŸ¥è¯¢è½¬æ¢å™¨ï¼Œæ— éœ€å¤æ‚è°ƒæ•´å³å¯æ•æ‰ä¸åŒç²’åº¦çš„å±‚æ¬¡ç»†èŠ‚ï¼Œå¹¶å…·å¤‡æ¨¡å—åŒ–ã€å³æ’å³ç”¨çš„è®¾è®¡å“²å­¦ï¼Œç¡®ä¿æ¨¡å‹çš„é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚åœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸‹çš„æµ‹è¯•ä¸­ï¼ŒHSQformerè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³ä¸èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¯Šæ–­èƒ½åŠ›ç›¸åŒ¹é…å¹¶è¶…è¶Šäº†åˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HCCæ˜¯ä¸–ç•ŒèŒƒå›´å†…ç¬¬ä¸‰å¤§è‡´ç™Œæ­»å› ï¼Œæ—©æœŸæ£€æµ‹å¯¹æ”¹å–„æ‚£è€…ç”Ÿå­˜ç‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è¶…å£°ç­›æŸ¥æ–¹æ³•å­˜åœ¨çµæ•åº¦ä¸è¶³å’Œä¾èµ–æ”¾å°„ç§‘åŒ»ç”Ÿè§£è¯»çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºHSQformerçš„æ–°å‹æ¨¡å‹ï¼Œç»“åˆäº†CNNå’ŒViTçš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨æé«˜HCCè¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>HSQformeré‡‡ç”¨åˆ†å±‚ç¨€ç–æŸ¥è¯¢è½¬æ¢å™¨ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒç²’åº¦çš„å±‚æ¬¡ç»†èŠ‚ï¼Œä¸”æ¨¡å‹è®¾è®¡æ¨¡å—åŒ–ï¼Œæ˜“äºä½¿ç”¨ã€‚</li>
<li>HSQformeråœ¨ä¸åŒä¸´åºŠåœºæ™¯ä¸‹çš„æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰å…ˆè¿›æ¨¡å‹ï¼Œå¦‚ConvNextå’ŒSwinTransformerã€‚</li>
<li>HSQformerçš„è¯Šæ–­èƒ½åŠ›ä¸èµ„æ·±æ”¾å°„ç§‘åŒ»ç”Ÿç›¸åŒ¹é…ï¼Œå¹¶å…¨é¢è¶…è¶Šäº†åˆçº§æ”¾å°„ç§‘åŒ»ç”Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae4e5716feb1a868c0fe34b824a296d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f0d9aa72043a2dfcd13362f70087c51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cb214dfe5c713968760dd7cb748cd2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b04229b8920d33f278ffd0d0f6b58d37.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Identifying-Compton-thick-AGNs-in-the-COSMOS-I-Among-X-ray-AGNs-with-Low-Photon-Counts"><a href="#Identifying-Compton-thick-AGNs-in-the-COSMOS-I-Among-X-ray-AGNs-with-Low-Photon-Counts" class="headerlink" title="Identifying Compton-thick AGNs in the COSMOS. I. Among X-ray AGNs with   Low Photon Counts"></a>Identifying Compton-thick AGNs in the COSMOS. I. Among X-ray AGNs with   Low Photon Counts</h2><p><strong>Authors:Xiaotong Guo, Qiusheng Gu, Guanwen Fang, Yongyun Chen, Nan Ding, Xiaoling Yu, Hongtao Wang</strong></p>
<p>Compton-thick active galactic nuclei (CT-AGNs), characterized by a significant absorption with column densities of $\mathrm{N_H}\geqslant 1.5\times 10^{24} \ \mathrm{cm}^{-2}$, emit feeble X-ray radiation and are even undetectable by X-ray instruments, making them difficult to identify. X-ray radiation from AGNs is the predominant source of the cosmic X-ray background (CXB). Based on AGN synthesis models for the CXB, the fraction of CT-AGNs should constitute a substantial portion of AGN population, approximately 30% or more. The fraction of CT-AGNs discovered in the Cosmological Evolution Survey (COSMOS) is significantly lower than this value. This means that many CT-AGNs may be hidden in AGNs that exhibit low photon counts or that have not been detected by X-ray instruments. This work focuses on identifying CT-AGNs hidden in AGNs with low photon counts. Firstly, we selected 440 AGNs with abundant multiwavelength data as our sample. Secondly, we analyzed multiwavelength data, extracting crucial physical parameters required for the CT-AGN diagnosis. Finally, we used multiwavelength approaches to identify CT-AGNs. We have successfully identified 18 CT-AGNs in our sample. Among the CT-AGNs, four AGNs show discrepant results across different diagnostic methods. We discuss the potential reasons behind these diagnostic discrepancies. We explore the impact of estimating [O<del>III]$\lambda</del>5007$ luminosities based on [O<del>II]$\lambda</del>3727$ luminosities for the CT-AGN diagnosis. We have also found that the properties of host galaxies for CT-AGNs and non-CT-AGNs do not show significant discrepancies. </p>
<blockquote>
<p>åº·æ™®é¡¿åšæ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆCT-AGNsï¼‰ï¼Œä»¥å…¶æ˜¾è‘—å¸æ”¶ä¸ºç‰¹å¾ï¼Œå…¶æŸ±å¯†åº¦è¾¾åˆ°NHâ‰¥1.5Ã—10^24 cm^-2ï¼Œå‘å‡ºå¾®å¼±çš„Xå°„çº¿è¾å°„ï¼Œç”šè‡³æ— æ³•è¢«Xå°„çº¿ä»ªå™¨æ¢æµ‹åˆ°ï¼Œå› æ­¤éš¾ä»¥è¯†åˆ«ã€‚æ¥è‡ªAGNsçš„Xå°„çº¿è¾å°„æ˜¯å®‡å®™Xå°„çº¿èƒŒæ™¯ï¼ˆCXBï¼‰çš„ä¸»è¦æ¥æºã€‚åŸºäºCXBçš„AGNsåˆæˆæ¨¡å‹ï¼ŒCT-AGNsåº”å AGNsç›¸å½“å¤§çš„æ¯”ä¾‹ï¼Œçº¦ä¸º30%æˆ–æ›´å¤šã€‚ç„¶è€Œï¼Œåœ¨å®‡å®™æ¼”åŒ–è°ƒæŸ¥ï¼ˆCOSMOSï¼‰ä¸­å‘ç°çš„CT-AGNsæ¯”ä¾‹æ˜æ˜¾ä½äºè¿™ä¸€æ•°å€¼ã€‚è¿™æ„å‘³ç€è®¸å¤šCT-AGNså¯èƒ½éšè—åœ¨å…‰å­è®¡æ•°è¾ƒä½çš„AGNsä¸­ï¼Œæˆ–è€…å°šæœªè¢«Xå°„çº¿ä»ªå™¨æ¢æµ‹åˆ°ã€‚æœ¬å·¥ä½œç€é‡äºè¯†åˆ«éšè—åœ¨å…‰å­è®¡æ•°è¾ƒä½çš„AGNsä¸­çš„CT-AGNsã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€‰æ‹©äº†440ä¸ªå…·æœ‰ä¸°å¯Œå¤šæ³¢é•¿æ•°æ®çš„AGNsä½œä¸ºæ ·æœ¬ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ†æäº†å¤šæ³¢é•¿æ•°æ®ï¼Œæå–äº†è¿›è¡ŒCT-AGNè¯Šæ–­æ‰€éœ€çš„å…³é”®ç‰©ç†å‚æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ³¢é•¿æ–¹æ³•æ¥è¯†åˆ«CT-AGNsã€‚åœ¨æˆ‘ä»¬çš„æ ·æœ¬ä¸­ï¼Œå·²æˆåŠŸè¯†åˆ«å‡º18ä¸ªCT-AGNsã€‚å…¶ä¸­ï¼Œæœ‰4ä¸ªAGNsåœ¨ä¸åŒè¯Šæ–­æ–¹æ³•çš„ç»“æœä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™äº›è¯Šæ–­å·®å¼‚èƒŒåçš„æ½œåœ¨åŸå› ã€‚æˆ‘ä»¬æ¢è®¨äº†åŸºäº[O II]Î»3727äº®åº¦ä¼°è®¡[O III]Î»5007äº®åº¦å¯¹CT-AGNè¯Šæ–­çš„å½±å“ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼ŒCT-AGNså’ŒéCT-AGNsçš„ä¸»å®¿æ˜Ÿç³»æ€§è´¨æ²¡æœ‰æ˜¾è‘—å·®åˆ«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03745v1">PDF</a> 12 pages, 7 figures, 4 tables. Accepted in Astronomy &amp; Astrophysics</p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­æ¢è®¨äº†Comptonåšæ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆCT-AGNsï¼‰çš„ç‰¹æ€§ï¼Œå…¶ç‰¹ç‚¹æ˜¯å­˜åœ¨æ˜¾è‘—å¸æ”¶ï¼Œå¯¼è‡´Xå°„çº¿è¾å°„å¾®å¼±ç”šè‡³æ— æ³•è¢«Xå°„çº¿ä»ªå™¨æ¢æµ‹åˆ°ã€‚åŸºäºCXBçš„AGNsåˆæˆæ¨¡å‹æ¨æµ‹ï¼ŒCT-AGNsåœ¨AGNsä¸­åº”å ç›¸å½“å¤§çš„æ¯”ä¾‹ï¼ˆçº¦30%æˆ–ä»¥ä¸Šï¼‰ã€‚ç„¶è€Œï¼Œåœ¨å®‡å®™æ¼”åŒ–è°ƒæŸ¥ï¼ˆCOSMOSï¼‰ä¸­å‘ç°çš„CT-AGNsæ¯”ä¾‹è¿œä½äºæ­¤ï¼Œè¡¨æ˜è®¸å¤šCT-AGNså¯èƒ½éšè—åœ¨ä½å…‰å­è®¡æ•°æˆ–æœªè¢«Xå°„çº¿ä»ªå™¨æ¢æµ‹åˆ°çš„AGNsä¸­ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºè¯†åˆ«éšè—åœ¨ä½å…‰å­è®¡æ•°AGNsä¸­çš„CT-AGNsï¼ŒæˆåŠŸè¯†åˆ«å‡ºå…¶ä¸­çš„18ä¸ªCT-AGNsï¼Œå¹¶å¯¹å…¶ä¸­å­˜åœ¨çš„è¯Šæ–­å·®å¼‚åŠå…¶åŸå› è¿›è¡Œäº†æ¢è®¨ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹åŸºäº[O II]Î»3727äº®åº¦ä¼°ç®—çš„CT-AGNsè¯Šæ–­è¿›è¡Œäº†æ¢ç´¢ï¼Œå‘ç°CT-AGNsä¸éCT-AGNsçš„å®¿ä¸»æ˜Ÿç³»ç‰¹æ€§æ— æ˜¾è‘—å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CT-AGNså…·æœ‰æ˜¾è‘—å¸æ”¶ç‰¹æ€§ï¼Œå¯¼è‡´Xå°„çº¿è¾å°„å¾®å¼±æˆ–æ— æ³•è¢«æ¢æµ‹ã€‚</li>
<li>åŸºäºCXBçš„AGNsåˆæˆæ¨¡å‹æ¨æµ‹CT-AGNsåº”å ç›¸å½“å¤§çš„æ¯”ä¾‹ã€‚</li>
<li>COSMOSè°ƒæŸ¥ä¸­å‘ç°çš„CT-AGNsæ¯”ä¾‹è¿œä½äºé¢„æµ‹å€¼ï¼Œæš—ç¤ºè®¸å¤šCT-AGNså¯èƒ½è¢«å¿½è§†ã€‚</li>
<li>ç ”ç©¶æˆåŠŸè¯†åˆ«å‡ºéšè—åœ¨ä½å…‰å­è®¡æ•°AGNsä¸­çš„18ä¸ªCT-AGNsã€‚</li>
<li>éƒ¨åˆ†CT-AGNsåœ¨ä¸åŒè¯Šæ–­æ–¹æ³•ä¸­å­˜åœ¨ç»“æœå·®å¼‚ï¼Œè®¨è®ºäº†è¿™äº›å·®å¼‚çš„åŸå› ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†åŸºäº[O II]Î»3727äº®åº¦ä¼°ç®—çš„CT-AGNsè¯Šæ–­æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-752c6e91dfac1c9ce8d14d815e9286fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-217e6ecdac161f354773995cd6c865c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef2c68790653315d45f44c98d240339d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053f11af2aff22cbea6f6494718432a8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free"><a href="#Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free" class="headerlink" title="Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free"></a>Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free</h2><p><strong>Authors:Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: <a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/">https://faverogian.github.io/med-diffusion-classifier.github.io/</a> </p>
<blockquote>
<p>åˆ¤åˆ«åˆ†ç±»å™¨å·²æˆä¸ºåŒ»å­¦æˆåƒæ·±åº¦å­¦ä¹ ä¸­çš„åŸºç¡€å·¥å…·ï¼Œæ“…é•¿å­¦ä¹ å¤æ‚æ•°æ®åˆ†å¸ƒçš„å¯åˆ†ç‰¹å¾ã€‚ç„¶è€Œï¼Œä¸ºäº†ç¡®ä¿å®‰å…¨å’Œå¯é çš„éƒ¨ç½²ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦ç²¾å¿ƒè®¾è®¡ã€å¢å¼ºå’Œè®­ç»ƒæŠ€æœ¯ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºäºŒç»´ç”Ÿæˆå»ºæ¨¡çš„åŒä¹‰è¯ã€‚è¿™äº›æ¨¡å‹å±•ç¤ºäº†åœ¨å„ç§ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒåˆ†ç±»ï¼Œå…¶ä¸­åˆ†ç±»æ˜¯é€šè¿‡æ¯”è¾ƒä¸ºæ¯ç§å¯èƒ½çš„æ¡ä»¶è¾“å…¥ç”Ÿæˆçš„å›¾åƒçš„é‡æ„è¯¯å·®æ¥å®Œæˆçš„ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡æ¢ç´¢äº†ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆè¢«è¯æ˜å¯ä»¥æé«˜åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œåœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°ä¸æœ€æ–°åˆ¤åˆ«åˆ†ç±»å™¨ç›¸ç«äº‰çš„æ€§èƒ½ï¼Œè€Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ‰©æ•£åˆ†ç±»å™¨æœ¬è´¨ä¸Šæ˜¯å¯è§£é‡Šçš„ï¼Œå¯ç”¨äºé‡åŒ–å…¶é¢„æµ‹çš„çš„ä¸ç¡®å®šæ€§ï¼Œè¿™åœ¨å®‰å…¨å…³é”®çš„ä¸´åºŠç¯å¢ƒä¸­å¢åŠ äº†å…¶å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/">é“¾æ¥åœ°å€</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­çš„æ½œåŠ›ï¼Œæå‡ºä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œå¹¶åœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ‰©æ•£æ¨¡å‹æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£å³å¯è¾¾åˆ°æœ€å…ˆè¿›åˆ¤åˆ«åˆ†ç±»å™¨çš„æ°´å¹³ã€‚æ­¤å¤–ï¼Œæ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å¯é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œæé«˜å…¶åœ¨å®‰å…¨å…³é”®çš„åŒ»ç–—ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œæé«˜äº†åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>åœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ‰©æ•£æ¨¡å‹çš„ç«äº‰åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ€§èƒ½ä¸æœ€å…ˆè¿›åˆ¤åˆ«åˆ†ç±»å™¨ç›¸å½“ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å¯ä»¥é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f25bdeacfbc5a65cf6ba4cbabfb56a85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09b52a6ca7cb81e8cde2b8cf39ed9051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b9cb79da2ff3d4912347f214f296d5e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Clinically-Inspired-Hierarchical-Multi-Label-Classification-of-Chest-X-rays-with-a-Penalty-Based-Loss-Function"><a href="#Clinically-Inspired-Hierarchical-Multi-Label-Classification-of-Chest-X-rays-with-a-Penalty-Based-Loss-Function" class="headerlink" title="Clinically-Inspired Hierarchical Multi-Label Classification of Chest   X-rays with a Penalty-Based Loss Function"></a>Clinically-Inspired Hierarchical Multi-Label Classification of Chest   X-rays with a Penalty-Based Loss Function</h2><p><strong>Authors:Mehrdad Asadi, Komi SodokÃ©, Ian J. Gerard, Marta Kersten-Oertel</strong></p>
<p>In this work, we present a novel approach to multi-label chest X-ray (CXR) image classification that enhances clinical interpretability while maintaining a streamlined, single-model, single-run training pipeline. Leveraging the CheXpert dataset and VisualCheXbert-derived labels, we incorporate hierarchical label groupings to capture clinically meaningful relationships between diagnoses. To achieve this, we designed a custom hierarchical binary cross-entropy (HBCE) loss function that enforces label dependencies using either fixed or data-driven penalty types. Our model achieved a mean area under the receiver operating characteristic curve (AUROC) of 0.903 on the test set. Additionally, we provide visual explanations and uncertainty estimations to further enhance model interpretability. All code, model configurations, and experiment details are made available. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ ‡ç­¾èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•æé«˜äº†ä¸´åºŠè§£é‡Šæ€§ï¼ŒåŒæ—¶ä¿æŒäº†ä¸€ä¸ªç²¾ç®€çš„å•æ¨¡å‹ã€å•è¿è¡Œè®­ç»ƒæµç¨‹ã€‚æˆ‘ä»¬åˆ©ç”¨CheXpertæ•°æ®é›†å’ŒVisualCheXbertè¡ç”Ÿçš„æ ‡ç­¾ï¼Œèå…¥å±‚æ¬¡æ ‡ç­¾åˆ†ç»„ï¼Œä»¥æ•è·è¯Šæ–­ä¹‹é—´ä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„å±‚æ¬¡äºŒå…ƒäº¤å‰ç†µï¼ˆHBCEï¼‰æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°é€šè¿‡å›ºå®šæˆ–æ•°æ®é©±åŠ¨å‹çš„æƒ©ç½šç±»å‹æ¥å¼ºåˆ¶æ‰§è¡Œæ ‡ç­¾ä¾èµ–æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†å¹³å‡å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.903çš„æˆç»©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†è§†è§‰è§£é‡Šå’Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹é…ç½®å’Œå®éªŒç»†èŠ‚å‡å·²å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03591v1">PDF</a> 9 pages with 3 figures, for associated implementation see   <a target="_blank" rel="noopener" href="https://github.com/the-mercury/CIHMLC">https://github.com/the-mercury/CIHMLC</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ ‡ç­¾èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æé«˜ä¸´åºŠå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œä¿æŒäº†ä¸€ä¸ªç®€æ´çš„å•æ¨¡å‹ã€å•è¿è¡Œè®­ç»ƒæµç¨‹ã€‚é€šè¿‡åˆ©ç”¨CheXpertæ•°æ®é›†å’ŒVisualCheXbertè¡ç”Ÿçš„æ ‡ç­¾ï¼Œèå…¥å±‚æ¬¡æ ‡ç­¾åˆ†ç»„ï¼Œä»¥æ•æ‰è¯Šæ–­ä¹‹é—´çš„ä¸´åºŠæœ‰æ„ä¹‰çš„å…³ç³»ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªå®šä¹‰çš„å±‚æ¬¡äºŒè¿›åˆ¶äº¤å‰ç†µï¼ˆHBCEï¼‰æŸå¤±å‡½æ•°ï¼Œé€šè¿‡å›ºå®šæˆ–æ•°æ®é©±åŠ¨å‹çš„æƒ©ç½šç±»å‹æ¥å®æ–½æ ‡ç­¾ä¾èµ–æ€§ã€‚è¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†å¹³å‡å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.903çš„æˆç»©ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†è§†è§‰è§£é‡Šå’Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹å¤šæ ‡ç­¾èƒ¸éƒ¨Xå°„çº¿å›¾åƒåˆ†ç±»çš„æ–°æ–¹æ³•ï¼Œå…¼é¡¾ä¸´åºŠå¯è§£é‡Šæ€§å’Œç®€æ´çš„è®­ç»ƒæµç¨‹ã€‚</li>
<li>åˆ©ç”¨CheXpertæ•°æ®é›†å’ŒVisualCheXbertè¡ç”Ÿçš„æ ‡ç­¾ï¼Œèå…¥å±‚æ¬¡æ ‡ç­¾åˆ†ç»„ã€‚</li>
<li>è‡ªå®šä¹‰äº†å±‚æ¬¡äºŒè¿›åˆ¶äº¤å‰ç†µï¼ˆHBCEï¼‰æŸå¤±å‡½æ•°ï¼Œå®æ–½æ ‡ç­¾ä¾èµ–æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„å¹³å‡å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.903ã€‚</li>
<li>æä¾›è§†è§‰è§£é‡Šå’Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¢å¼ºæ¨¡å‹å¯è§£é‡Šæ€§ã€‚</li>
<li>ä»£ç ã€æ¨¡å‹é…ç½®å’Œå®éªŒç»†èŠ‚å…¨éƒ¨å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f03ee994bb0a4a5383b0be7dad70a322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1fa11a913946229be4a5343b108c36.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features"><a href="#Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features" class="headerlink" title="Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features"></a>Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features</h2><p><strong>Authors:Arman Groji, Ali Fathi Jouzdani, Nima Sanati, Amir Mahmoud Ahmadzadeh, Ren Yuan, Arman Rahmim, Mohammad R. Salmanpour</strong></p>
<p>Objectives: Lung cancer poses a significant global health challenge, necessitating improved prognostic methods for personalized treatment. This study introduces a censor-aware semi-supervised learning (SSL) framework that integrates clinical and imaging data, addressing biases in traditional models handling censored data. Methods: We analyzed clinical, PET and CT data from 199 lung cancer patients from public and local data respositories, focusing on overall survival (OS) time as the primary outcome. Handcrafted (HRF) and Deep Radiomics features (DRF) were extracted after preprocessing using ViSERA software and were combined with clinical features (CF). Feature dimensions were optimized using Principal Component Analysis (PCA), followed by the application of supervised learning (SL) and SSL. SSL incorporated pseudo-labeling of censored data to improve performance. Seven regressors and three hazard ratio survival analysis (HRSA) algorithms were optimized using five-fold cross-validation, grid search and external test bootstrapping. Results: For PET HRFs, SSL reduced the mean absolute error (MAE) by 26.5%, achieving 1.55 years with PCA+decision tree regression, compared to SLâ€™s 2.11 years with PCA+KNNR (p&lt;0.05). Combining HRFs (CT_HRF) and DRFs from CT images using SSL+PCA+KNNR achieved an MAE of 2.08 years, outperforming SLâ€™s 2.26 years by 7.96% (p&lt;0.05). In HRSA, CT_HRF applied to PCA+Component Wise Gradient Boosting Survival Analysis achieved an external c-index of 0.65, effectively differentiating high- and low-risk groups. Conclusions: We demonstrated that the SSL strategy significantly outperforms SL across PET, CT, and CF. As such, censor-aware SSL applied to HRFs from PET images significantly improved survival prediction performance by 26.5% compared to the SL approach. </p>
<blockquote>
<p>ç›®æ ‡ï¼šè‚ºç™Œç»™å…¨çƒå¥åº·å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ”¹è¿›é¢„æµ‹æ–¹æ³•ä»¥å®ç°ä¸ªæ€§åŒ–æ²»ç–—ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æœ‰å®¡æŸ¥æ„è¯†çš„åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†ä¸´åºŠå’Œæˆåƒæ•°æ®ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å®¡æŸ¥æ•°æ®æ—¶çš„åè§é—®é¢˜ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬åˆ†æäº†æ¥è‡ªå…¬å…±å’Œæœ¬åœ°æ•°æ®ä»“åº“çš„199åè‚ºç™Œæ‚£è€…çš„ä¸´åºŠã€PETå’ŒCTæ•°æ®ï¼Œä»¥æ€»ç”Ÿå­˜æœŸï¼ˆOSï¼‰æ—¶é—´ä¸ºä¸»è¦ç»“æœã€‚ä½¿ç”¨ViSERAè½¯ä»¶é¢„å¤„ç†åæå–äº†æ‰‹å·¥ç‰¹å¾ï¼ˆHRFï¼‰å’Œæ·±åº¦æ”¾å°„å­¦ç‰¹å¾ï¼ˆDRFï¼‰ï¼Œå¹¶ä¸ä¸´åºŠç‰¹å¾ï¼ˆCFï¼‰ç›¸ç»“åˆã€‚ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¼˜åŒ–ç‰¹å¾ç»´åº¦ï¼Œç„¶ååº”ç”¨ç›‘ç£å­¦ä¹ ï¼ˆSLï¼‰å’ŒSSLã€‚SSLç»“åˆäº†å®¡æŸ¥æ•°æ®çš„ä¼ªæ ‡ç­¾ä»¥æé«˜æ€§èƒ½ã€‚ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢å’Œå¤–éƒ¨æµ‹è¯•bootstrapæ–¹æ³•ä¼˜åŒ–äº†7ä¸ªå›å½’å™¨å’Œ3ç§é£é™©æ¯”ç‡ç”Ÿå­˜åˆ†æï¼ˆHRSAï¼‰ç®—æ³•ã€‚</p>
<p>ç»“æœï¼šå¯¹äºPETçš„HRFsï¼ŒSSLå°†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†26.5%ï¼Œä½¿ç”¨PCA+å†³ç­–æ ‘å›å½’è¾¾åˆ°äº†1.55å¹´ï¼Œè€ŒSL+PCA+KNNRä¸º2.11å¹´ï¼ˆp&lt;0.05ï¼‰ã€‚ä½¿ç”¨SSL+PCA+KNNRç»“åˆæ¥è‡ªCTå›¾åƒçš„HRFå’ŒDRFï¼ŒMAEä¸º2.08å¹´ï¼Œä¼˜äºSLçš„2.26å¹´ï¼Œæé«˜äº†7.96%ï¼ˆp&lt;0.05ï¼‰ã€‚åœ¨HRSAä¸­ï¼Œå°†CT_HRFåº”ç”¨äºPCA+ç»„ä»¶æ™ºæ…§æ¢¯åº¦æå‡ç”Ÿå­˜åˆ†æè¾¾åˆ°äº†å¤–éƒ¨cæŒ‡æ•°ä¸º0.65ï¼Œæœ‰æ•ˆåœ°åŒºåˆ†äº†é«˜ã€ä½é£é™©ç»„ã€‚</p>
<p>ç»“è®ºï¼šæˆ‘ä»¬è¯æ˜SSLç­–ç•¥åœ¨PETã€CTå’ŒCFæ–¹é¢æ˜¾è‘—ä¼˜äºSLã€‚å› æ­¤ï¼Œä¸SLæ–¹æ³•ç›¸æ¯”ï¼Œåº”ç”¨äºPETå›¾åƒHRFsçš„æœ‰å®¡æŸ¥æ„è¯†çš„SSLå°†ç”Ÿå­˜é¢„æµ‹æ€§èƒ½æé«˜äº†26.5%ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01661v2">PDF</a> 11 pages, 4 Figures and 4 Tables</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ—¨åœ¨åº”å¯¹è‚ºç™Œé¢„åé¢„æµ‹æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§èåˆä¸´åºŠä¸æˆåƒæ•°æ®çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œä»¥è§£å†³ä¼ ç»Ÿæ¨¡å‹ä¸­å¤„ç†æˆªæ–­æ•°æ®æ—¶çš„åè§é—®é¢˜ã€‚ç ”ç©¶é€šè¿‡é›†æˆå½±åƒä¸ä¸´åºŠæ•°æ®ï¼Œä¼˜åŒ–äº†ç‰¹å¾ç»´åº¦ä¸ç®—æ³•æ¨¡å‹ï¼Œæé«˜äº†ç”Ÿå­˜é¢„æµ‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒåŠç›‘ç£å­¦ä¹ æ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†PETå½±åƒçš„HRFsæ—¶ï¼Œé¢„æµ‹è¯¯å·®é™ä½äº†26.5%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨æ”¹å–„è‚ºç™Œçš„ä¸ªæ€§åŒ–æ²»ç–—é¢„åæ–¹æ³•ï¼Œå¼•å…¥åŠç›‘ç£å­¦ä¹ æ¡†æ¶æ•´åˆä¸´åºŠä¸æˆåƒæ•°æ®ã€‚</li>
<li>ç ”ç©¶å¤„ç†äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†æˆªæ–­æ•°æ®æ—¶çš„åè§é—®é¢˜ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ViSERAè½¯ä»¶æå–ç‰¹å¾ï¼Œå¹¶ç»“åˆä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¼˜åŒ–ç‰¹å¾ç»´åº¦ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†ç›‘ç£å­¦ä¹ ä¸åŠç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è‚ºç™Œé¢„åé¢„æµ‹ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨PETå½±åƒçš„HRFså¤„ç†ä¸­æ˜¾è‘—æé«˜äº†ç”Ÿå­˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œè¯¯å·®é™ä½äº†26.5%ã€‚</li>
<li>ç»“åˆCTå½±åƒçš„HRFsä¸DRFsä½¿ç”¨åŠç›‘ç£å­¦ä¹ æ–¹æ³•ä¹Ÿè¡¨ç°å‡ºæ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5378ec724a7a8591d7a8d8ec874eb602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b49b6dd42c087f64caa596a5d22610.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e4d6736d5da93ddaa95c5431f6631f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39683a72d4995f5a775f2286230a2b9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ed0ade3b7c93128e235d150af11cf91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8837309bbfc8fa094e0d3ef0c904d7f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d64a4206a450ce46a830048cacacd7e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a096ffbc2bdf17cc4c26ede3f598509.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed64f8376d81971b01aaf9f6dd0d291b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Moner-Motion-Correction-in-Undersampled-Radial-MRI-with-Unsupervised-Neural-Representation"><a href="#Moner-Motion-Correction-in-Undersampled-Radial-MRI-with-Unsupervised-Neural-Representation" class="headerlink" title="Moner: Motion Correction in Undersampled Radial MRI with Unsupervised   Neural Representation"></a>Moner: Motion Correction in Undersampled Radial MRI with Unsupervised   Neural Representation</h2><p><strong>Authors:Qing Wu, Chenhe Du, Xuanyu Tian, Jingyi Yu, Yuyao Zhang, Hongjiang Wei</strong></p>
<p>Motion correction (MoCo) in radial MRI is a challenging problem due to the unpredictability of subjectâ€™s motion. Current state-of-the-art (SOTA) MoCo algorithms often use extensive high-quality MR images to pre-train neural networks, obtaining excellent reconstructions. However, the need for large-scale datasets significantly increases costs and limits model generalization. In this work, we propose Moner, an unsupervised MoCo method that jointly solves artifact-free MR images and accurate motion from undersampled, rigid motion-corrupted k-space data, without requiring training data. Our core idea is to leverage the continuous prior of implicit neural representation (INR) to constrain this ill-posed inverse problem, enabling ideal solutions. Specifically, we incorporate a quasi-static motion model into the INR, granting its ability to correct subjectâ€™s motion. To stabilize model optimization, we reformulate radial MRI as a back-projection problem using the Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash encoding strategy, significantly enhancing MoCo accuracy. Experiments on multiple MRI datasets show our Moner achieves performance comparable to SOTA MoCo techniques on in-domain data, while demonstrating significant improvements on out-of-domain data. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/iwuqing/Moner">https://github.com/iwuqing/Moner</a> </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨æ ¡æ­£ï¼ˆMoCoï¼‰æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå—è¯•è€…è¿åŠ¨å…·æœ‰ä¸å¯é¢„æµ‹æ€§ã€‚ç›®å‰æœ€å…ˆè¿›çš„MoCoç®—æ³•é€šå¸¸ä½¿ç”¨å¤§é‡é«˜è´¨é‡çš„MRå›¾åƒæ¥é¢„å…ˆè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»è€Œè·å¾—å‡ºè‰²çš„é‡å»ºæ•ˆæœã€‚ç„¶è€Œï¼Œå¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚æ˜¾è‘—å¢åŠ äº†æˆæœ¬å¹¶é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Monerï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€ç›‘ç£çš„MoCoæ–¹æ³•ï¼Œå¯ä»¥è”åˆè§£å†³æ— ä¼ªå½±MRå›¾åƒå’Œä»æ¬ é‡‡æ ·çš„åˆšæ€§è¿åŠ¨æŸåçš„kç©ºé—´æ•°æ®ä¸­å‡†ç¡®è¿åŠ¨çš„é—®é¢˜ï¼Œè€Œæ— éœ€è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„è¿ç»­å…ˆéªŒçŸ¥è¯†æ¥çº¦æŸè¿™ä¸ªä¸é€‚å®šçš„åé—®é¢˜ï¼Œä»¥å®ç°ç†æƒ³çš„è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å‡†é™æ€è¿åŠ¨æ¨¡å‹çº³å…¥INRä¸­ï¼Œä½¿å…¶å…·å¤‡çº æ­£å—è¯•è€…è¿åŠ¨çš„èƒ½åŠ›ã€‚ä¸ºäº†ç¨³å®šæ¨¡å‹ä¼˜åŒ–ï¼Œæˆ‘ä»¬åˆ©ç”¨å‚…é‡Œå¶åˆ‡ç‰‡å®šç†å°†å¾„å‘MRIé‡æ–°è¡¨è¿°ä¸ºåå‘æŠ•å½±é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”±ç²—åˆ°ç»†çš„å“ˆå¸Œç¼–ç ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†MoCoçš„å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Moneråœ¨é¢†åŸŸå†…æ•°æ®ä¸Šçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„MoCoæŠ€æœ¯ç›¸å½“ï¼Œè€Œåœ¨è·¨é¢†åŸŸæ•°æ®ä¸Šåˆ™è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/iwuqing/Moner%E3%80%82">https://github.com/iwuqing/Moner</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16921v3">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMonerçš„æ— ç›‘ç£è¿åŠ¨æ ¡æ­£ï¼ˆMoCoï¼‰æ–¹æ³•ï¼Œç”¨äºè§£å†³MRIä¸­çš„è¿åŠ¨é—®é¢˜ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒæ•°æ®ï¼Œå¯ç›´æ¥ä»æ¬ é‡‡æ ·çš„ã€å—åˆšæ€§è¿åŠ¨å½±å“çš„kç©ºé—´æ•°æ®ä¸­æ¢å¤æ— ä¼ªå½±çš„MRå›¾åƒå’Œç²¾ç¡®è¿åŠ¨ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„è¿ç»­å…ˆéªŒæ¥çº¦æŸè¿™ä¸ªä¸é€‚å®šçš„é€†é—®é¢˜ï¼Œä»è€Œå®ç°ç†æƒ³è§£ã€‚Moneråœ¨å¤šä¸ªMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½ä¸æœ€æ–°MoCoæŠ€æœ¯ç›¸å½“ï¼Œå¹¶åœ¨è·¨åŸŸæ•°æ®ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Moneræ˜¯ä¸€ç§æ— ç›‘ç£çš„MoCoæ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒæ•°æ®ã€‚</li>
<li>Monerèƒ½å¤Ÿä»æ¬ é‡‡æ ·çš„ã€å—åˆšæ€§è¿åŠ¨å½±å“çš„kç©ºé—´æ•°æ®ä¸­æ¢å¤æ— ä¼ªå½±çš„MRå›¾åƒå’Œç²¾ç¡®è¿åŠ¨ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰çš„è¿ç»­å…ˆéªŒæ¥çº¦æŸä¸é€‚å®šçš„é€†é—®é¢˜ã€‚</li>
<li>Monerå°†å¾„å‘MRIé‡æ–°è¡¨è¿°ä¸ºåå‘æŠ•å½±é—®é¢˜ï¼Œè¿ç”¨å‚…ç«‹å¶åˆ‡ç‰‡å®šç†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”±ç²—åˆ°ç»†çš„å“ˆå¸Œç¼–ç ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†MoCoçš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMoneræ€§èƒ½ä¸æœ€æ–°MoCoæŠ€æœ¯ç›¸å½“ï¼Œå¹¶åœ¨è·¨åŸŸæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72abb41f4c9a277fb0d253d160758fdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2241f0d56e2488216014bb2194009eda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b39bbad73e22cbf61d723b3935529e9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Liquid-State-Modeling-from-Experimental-Scattering-Measurements"><a href="#Uncertainty-Aware-Liquid-State-Modeling-from-Experimental-Scattering-Measurements" class="headerlink" title="Uncertainty-Aware Liquid State Modeling from Experimental Scattering   Measurements"></a>Uncertainty-Aware Liquid State Modeling from Experimental Scattering   Measurements</h2><p><strong>Authors:Brennon L. Shanks</strong></p>
<p>This dissertation is founded on the central notion that structural correlations in dense fluids, such as dense gases, liquids, and glasses, are directly related to fundamental interatomic forces. This relationship was identified early in the development of statistical theories of fluids through the mathematical formulations of Gibbs in the 1910s. However, it took nearly 80 years before practical implementations of structure-based theories became widely used for interpreting and understanding the atomic structures of fluids from experimental X-ray and neutron scattering data. The breakthrough in successfully applying structure-potential relations is largely attributed to the advancements in molecular mechanics simulations and the enhancement of computational resources. Despite advancements in understanding the relationship between structure and interatomic forces, a significant gap remains. Current techniques for interpreting experimental scattering measurements are widely used, yet there is little evidence that they yield physically accurate predictions for interatomic forces. In fact, it is generally assumed that these methods produce interatomic forces that poorly model the atomistic and thermodynamic behavior of fluids, rendering them unreliable and non-transferable. This thesis aims to address these limitations by refining the statistical theory, computational methods, and philosophical approach to structure-based analyses, thereby developing more robust and accurate techniques for characterizing structure-potential relationships. </p>
<blockquote>
<p>æœ¬è®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ï¼Œå¯†é›†æµä½“ï¼ˆå¦‚è‡´å¯†æ°”ä½“ã€æ¶²ä½“å’Œç»ç’ƒï¼‰ä¸­çš„ç»“æ„å…³è”ä¸åŸºæœ¬åŸå­é—´åŠ›æœ‰ç€ç›´æ¥è”ç³»ã€‚è¿™ç§å…³ç³»åœ¨æµä½“ç»Ÿè®¡ç†è®ºå‘å±•çš„æ—©æœŸå°±è¢«è¯†åˆ«å‡ºæ¥ï¼Œå‰å¸ƒæ–¯åœ¨20ä¸–çºªæå‡ºäº†ç›¸å…³çš„æ•°å­¦å…¬å¼ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåŸºäºç»“æ„çš„ç†è®ºè¢«å¹¿æ³›ç”¨äºè§£é‡Šå’Œç†è§£ä»å®éªŒXå°„çº¿å’Œä¸­å­æ•£å°„æ•°æ®å¾—å‡ºçš„æµä½“åŸå­ç»“æ„å´è€—è´¹äº†è¿‘80å¹´çš„æ—¶é—´ã€‚æˆåŠŸåº”ç”¨ç»“æ„ç”µä½å…³ç³»çš„çªç ´åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’å› äºåˆ†å­åŠ›å­¦æ¨¡æ‹Ÿçš„è¿›æ­¥å’Œè®¡ç®—èµ„æºçš„å¢å¼ºã€‚å°½ç®¡åœ¨ç†è§£ç»“æ„ä¸åŸå­é—´åŠ›çš„å…³ç³»æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»å­˜åœ¨é‡å¤§å·®è·ã€‚ç›®å‰ç”¨äºè§£é‡Šå®éªŒæ•£å°„æµ‹é‡çš„æŠ€æœ¯è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†æœ‰è¯æ®è¡¨æ˜å®ƒä»¬å¯¹åŸå­é—´åŠ›çš„é¢„æµ‹å¹¶ä¸å‡†ç¡®ã€‚äº‹å®ä¸Šï¼Œé€šå¸¸è®¤ä¸ºè¿™äº›æ–¹æ³•äº§ç”Ÿçš„åŸå­é—´åŠ›æ¨¡å‹ä¸èƒ½å¾ˆå¥½åœ°æ¨¡æ‹Ÿæµä½“çš„åŸå­å’Œçƒ­åŠ›å­¦è¡Œä¸ºï¼Œä½¿å…¶ä¸å¯é ä¸”ä¸å¯è½¬ç§»ã€‚æœ¬è®ºæ–‡æ—¨åœ¨é€šè¿‡æ”¹è¿›ç»Ÿè®¡ç†è®ºã€è®¡ç®—æ–¹æ³•å’ŒåŸºäºç»“æ„çš„åˆ†æå“²å­¦æ–¹æ³•ï¼Œè§£å†³è¿™äº›å±€é™ï¼Œä»è€Œå¼€å‘æ›´ç¨³å¥å’Œå‡†ç¡®çš„æŠ€æœ¯æ¥è¡¨å¾ç»“æ„ç”µä½å…³ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00236v3">PDF</a> Doctoral thesis</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡èšç„¦äºå¯†é›†æµä½“ï¼ˆå¦‚è‡´å¯†æ°”ä½“ã€æ¶²ä½“å’Œç»ç’ƒï¼‰çš„ç»“æ„å…³è”ä¸åŸºæœ¬åŸå­é—´åŠ›ä¹‹é—´çš„ç›´æ¥å…³ç³»ã€‚å®ƒå›é¡¾äº†æ—©æœŸç»Ÿè®¡æµä½“ç†è®ºçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯å‰å¸ƒæ–¯åœ¨20ä¸–çºªåˆçš„æ•°å­¦è¡¨è¿°ï¼ŒæŒ‡å‡ºä¸¤è€…ä¹‹é—´çš„ç´§å¯†è”ç³»ã€‚è™½ç„¶åˆ©ç”¨ç»“æ„åŸºç¡€ç†è®ºçš„å®è·µå®ç°äº†åœ¨è¿‘å…«åå¹´çš„çªç ´ï¼Œä¸”åœ¨å®éªŒæ€§Xå°„çº¿å’Œä¸­å­æ•£å°„æ•°æ®çš„åŸºç¡€ä¸ŠæˆåŠŸåº”ç”¨ç»“æ„åŠ¿èƒ½å…³ç³»ï¼Œä½†ä»ç„¶å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å½“å‰ç”¨äºè§£é‡Šå®éªŒæ•£å°„æµ‹é‡çš„æŠ€æœ¯è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†ç¼ºä¹ç‰©ç†å‡†ç¡®é¢„æµ‹åŸå­é—´åŠ›çš„è¯æ®ã€‚è®ºæ–‡æ—¨åœ¨é€šè¿‡æ”¹è¿›ç»Ÿè®¡ç†è®ºã€è®¡ç®—æ–¹æ³•å’Œå“²å­¦æ–¹æ³•æ¥è§£å†³è¿™äº›å±€é™æ€§ï¼Œå¼€å‘æ›´ç¨³å¥å’Œå‡†ç¡®çš„ç»“æ„åŠ¿èƒ½å…³ç³»è¡¨å¾æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ­¤è®ºæ–‡è®¤ä¸ºå¯†é›†æµä½“çš„ç»“æ„å…³è”ä¸åŸºæœ¬åŸå­é—´åŠ›å­˜åœ¨ç´§å¯†è”ç³»ã€‚è¿™ä¸€ç†å¿µå§‹äºæ—©æœŸçš„ç»Ÿè®¡æµä½“ç†è®ºå‘å±•ã€‚</li>
<li>ç»“æ„åŸºç¡€ç†è®ºçš„åº”ç”¨å®è·µç»å†äº†è¿‘å…«åå¹´çš„çªç ´æœŸï¼Œå¾—ç›Šäºåˆ†å­åŠ›å­¦æ¨¡æ‹Ÿçš„è¿›æ­¥å’Œè®¡ç®—èµ„æºçš„æå‡ã€‚</li>
<li>æˆåŠŸåº”ç”¨ç»“æ„åŠ¿èƒ½å…³ç³»çš„è¿›æ­¥ä¸ºç†è§£å’Œè§£é‡Šæµä½“çš„åŸå­ç»“æ„æä¾›äº†æ–°çš„æ–¹æ³•ã€‚</li>
<li>å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰è§£é‡Šå®éªŒæ•£å°„æµ‹é‡çš„æŠ€æœ¯åœ¨é¢„æµ‹åŸå­é—´åŠ›æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ä¸èƒ½å‡†ç¡®æ¨¡æ‹Ÿæµä½“çš„åŸå­å’Œçƒ­åŠ›å­¦è¡Œä¸ºã€‚</li>
<li>å½“å‰æŠ€æœ¯ä¸­çš„æ–¹æ³•è¢«è®¤ä¸ºå¯é æ€§ä½ä¸”ç¼ºä¹å¯è½¬ç§»æ€§ï¼Œå¯¹äºå®é™…åº”ç”¨çš„é¢„æµ‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æ—¨åœ¨é€šè¿‡æ”¹è¿›ç»Ÿè®¡ç†è®ºã€è®¡ç®—æ–¹æ³•å’Œå“²å­¦æ–¹æ³•æ¥ç¼©å°å·®è·ï¼Œæé«˜ç»“æ„åŠ¿èƒ½å…³ç³»è¡¨å¾æŠ€æœ¯çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚è¿™å°†æœ‰åŠ©äºæ›´å‡†ç¡®ç†è§£æµä½“è¡Œä¸ºçš„å†…éƒ¨æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef411b7474f35a283c21e9926e0e9baa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2b750596a8c8a26d2d0a746d64d5e8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Fact-Aware-Multimodal-Retrieval-Augmentation-for-Accurate-Medical-Radiology-Report-Generation"><a href="#Fact-Aware-Multimodal-Retrieval-Augmentation-for-Accurate-Medical-Radiology-Report-Generation" class="headerlink" title="Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical   Radiology Report Generation"></a>Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical   Radiology Report Generation</h2><p><strong>Authors:Liwen Sun, James Zhao, Megan Han, Chenyan Xiong</strong></p>
<p>Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets show that our multimodal retriever outperforms state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagates fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢æ‹¥æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä»è€Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¯Šæ–­å¿ƒè„ç–¾ç—…ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„æŠ¥å‘Šå¸¸å¸¸å­˜åœ¨ä¸¥é‡çš„äº‹å®ä¸å‡†ç¡®é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºç”Ÿæˆå‡†ç¡®æ”¾å°„å­¦æŠ¥å‘Šçš„åŸºäºäº‹å®æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç®¡é“ï¼ˆFactMM-RAGï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨RadGraphæŒ–æ˜äº‹å®æŠ¥å‘Šå¯¹ï¼Œç„¶åæ•´åˆäº‹å®çŸ¥è¯†æ¥è®­ç»ƒä¸€ä¸ªé€šç”¨çš„å¤šæ¨¡æ€æ£€ç´¢å™¨ã€‚ç»™å®šä¸€å¼ æ”¾å°„å­¦å›¾åƒï¼Œæˆ‘ä»¬çš„æ£€ç´¢å™¨å¯ä»¥è¯†åˆ«é«˜è´¨é‡çš„å‚è€ƒæŠ¥å‘Šï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œä»è€Œæé«˜æŠ¥å‘Šç”Ÿæˆçš„äº‹å®å®Œæ•´æ€§å’Œæ­£ç¡®æ€§ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€æ£€ç´¢å™¨åœ¨è¯­è¨€ç”Ÿæˆå’Œæ”¾å°„å­¦ç‰¹å®šæŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºæœ€æ–°æ£€ç´¢å™¨ï¼Œåœ¨F1CheXbertå’ŒF1RadGraphä¸Šçš„å¾—åˆ†åˆ†åˆ«é«˜è¾¾6.5%å’Œ2%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œé‡‡ç”¨æˆ‘ä»¬çš„äº‹å®æ„ŸçŸ¥è®­ç»ƒç­–ç•¥ï¼Œåœ¨ä¸ä¾èµ–æ˜ç¡®çš„è¯Šæ–­æ ‡ç­¾æŒ‡å¯¼çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸåœ°å°†å¤šæ¨¡æ€æ£€ç´¢å™¨çš„äº‹å®æ„ŸçŸ¥èƒ½åŠ›ä¼ æ’­åˆ°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»è€Œæœ‰æ•ˆåœ°ç›‘ç£äº†æ”¾å°„å­¦æŠ¥å‘Šçš„ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.15268v2">PDF</a> NAACL 2025 main</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºäº‹å®æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç®¡é“ï¼ˆFactMM-RAGï¼‰ï¼Œç”¨äºç”Ÿæˆå‡†ç¡®çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚è¯¥ç ”ç©¶åˆ©ç”¨RadGraphæŒ–æ˜äº‹å®æŠ¥å‘Šå¯¹ï¼Œå¹¶é›†æˆäº‹å®çŸ¥è¯†è®­ç»ƒé€šç”¨å¤šæ¨¡æ€æ£€ç´¢å™¨ã€‚ç»™å®šæ”¾å°„å­¦å›¾åƒï¼Œè¯¥æ£€ç´¢å™¨å¯è¯†åˆ«é«˜è´¨é‡å‚è€ƒæŠ¥å‘Šï¼Œå¢å¼ºå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œä»è€Œæé«˜æŠ¥å‘Šç”Ÿæˆçš„äº‹å®å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥å¤šæ¨¡æ€æ£€ç´¢å™¨åœ¨è¯­è¨€ç”Ÿæˆå’Œæ”¾å°„å­¦ç‰¹å®šæŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ£€ç´¢å™¨ï¼Œæœ€é«˜å¯æé«˜F1CheXbertå’ŒF1RadGraphçš„åˆ†æ•°åˆ†åˆ«è¾¾6.5%å’Œ2%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œé‡‡ç”¨åŸºäºäº‹å®çš„è®­ç»ƒç­–ç•¥æä¾›äº†æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ï¼Œæ— éœ€ä¾èµ–æ˜ç¡®çš„è¯Šæ–­æ ‡ç­¾æŒ‡å¯¼ï¼Œå¹¶æˆåŠŸå°†ä»å¤šæ¨¡æ€æ£€ç´¢å™¨ä¸­è·å–çš„äº‹å®æ„ŸçŸ¥èƒ½åŠ›ä¼ æ’­åˆ°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸­ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œæœ‰åŠ©äºä¸´åºŠåŒ»ç”Ÿè¯Šæ–­å¿ƒè„ç—…ã€‚</li>
<li>ç”Ÿæˆçš„æŠ¥å‘Šå¸¸å¸¸å­˜åœ¨äº‹å®å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºäº‹å®æ„ŸçŸ¥çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç®¡é“ï¼ˆFactMM-RAGï¼‰ä»¥æé«˜æŠ¥å‘Šå‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨RadGraphæŒ–æ˜äº‹å®æŠ¥å‘Šå¯¹ï¼Œå¹¶é›†æˆäº‹å®çŸ¥è¯†è®­ç»ƒå¤šæ¨¡æ€æ£€ç´¢å™¨ã€‚</li>
<li>å¤šæ¨¡æ€æ£€ç´¢å™¨å¯è¯†åˆ«é«˜è´¨é‡å‚è€ƒæŠ¥å‘Šï¼Œå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥å¤šæ¨¡æ€æ£€ç´¢å™¨åœ¨è¯­è¨€ç”Ÿæˆå’Œæ”¾å°„å­¦ç‰¹å®šæŒ‡æ ‡ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.15268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ce490e5083420acad7a5ca034ca00c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6038c5bde7c71c255edc2f799f78dca7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c265730e323ed2e7668b32b4f7572b0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-study-of-why-we-need-to-reassess-full-reference-image-quality-assessment-with-medical-images"><a href="#A-study-of-why-we-need-to-reassess-full-reference-image-quality-assessment-with-medical-images" class="headerlink" title="A study of why we need to reassess full reference image quality   assessment with medical images"></a>A study of why we need to reassess full reference image quality   assessment with medical images</h2><p><strong>Authors:Anna Breger, Ander Biguri, Malena SabatÃ© Landman, Ian Selby, Nicole Amberg, Elisabeth Brunner, Janek GrÃ¶hl, Sepideh Hatamikia, Clemens Karner, Lipeng Ning, SÃ¶ren Dittmer, Michael Roberts, AIX-COVNET Collaboration, Carola-Bibiane SchÃ¶nlieb</strong></p>
<p>Image quality assessment (IQA) is indispensable in clinical practice to ensure high standards, as well as in the development stage of machine learning algorithms that operate on medical images. The popular full reference (FR) IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been reported in the literature, highlighting the gap between development and actual clinical application. Such inconsistencies are not surprising, as medical images have very different properties than natural images, and PSNR and SSIM have neither been targeted nor properly tested for medical images. This may cause unforeseen problems in clinical applications due to wrong judgment of novel methods. This paper provides a structured and comprehensive overview of examples where PSNR and SSIM prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore, improvement is urgently needed in particular in this era of AI to increase reliability and explainability in machine learning for medical imaging and beyond. Lastly, we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images. </p>
<blockquote>
<p>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰åœ¨ä¸´åºŠå®è·µä¸­æ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œä»¥ç¡®ä¿é«˜æ ‡å‡†ï¼Œä»¥åŠåœ¨åŒ»å­¦å›¾åƒæ“ä½œçš„æœºå™¨å­¦ä¹ ç®—æ³•çš„å¼€å‘é˜¶æ®µä¹Ÿæ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚æµè¡Œçš„å…¨å‚è€ƒï¼ˆFRï¼‰IQAæªæ–½PSNRå’ŒSSIMåœ¨ä¼—å¤šçš„è‡ªç„¶æˆåƒä»»åŠ¡ä¸­å¾—åˆ°äº†å¾ˆå¥½çš„åº”ç”¨ï¼Œä½†åœ¨åŒ»å­¦åœºæ™¯ä¸­æŠ¥é“äº†æ–‡çŒ®ä¸­çš„å·®å¼‚ï¼Œçªå‡ºäº†å¼€å‘ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚è¿™ç§ä¸ä¸€è‡´æ€§å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºåŒ»å­¦å›¾åƒå…·æœ‰ä¸è‡ªç„¶å›¾åƒéå¸¸ä¸åŒçš„å±æ€§ï¼Œå¹¶ä¸”PSNRå’ŒSSIMæ—¢æ²¡æœ‰è¢«å®šä½ä¹Ÿæ²¡æœ‰é’ˆå¯¹åŒ»å­¦å›¾åƒè¿›è¡Œé€‚å½“çš„æµ‹è¯•ã€‚è¿™å¯èƒ½å¯¼è‡´æ–°å‹æ–¹æ³•åˆ¤æ–­é”™è¯¯ï¼Œä»è€Œåœ¨ä¸´åºŠåº”ç”¨ä¸­é€ æˆæ„æƒ³ä¸åˆ°çš„é—®é¢˜ã€‚æœ¬æ–‡å¯¹PSNRå’ŒSSIMåœ¨ä¸ä½¿ç”¨ä¸åŒåŒ»å­¦å›¾åƒçš„å…¨æ–°ç®—æ³•è¯„ä¼°ä¸­è¡¨ç°ä¸ä½³çš„æ¡ˆä¾‹è¿›è¡Œäº†ç»“æ„å’Œå…¨é¢çš„æ¦‚è¿°ï¼Œè¿™äº›åŒ»å­¦å›¾åƒåŒ…æ‹¬ç°å®ä¸–ç•Œçš„MRIã€CTã€OCTã€Xå°„çº¿ã€æ•°å­—ç—…ç†å’Œå…‰å£°æˆåƒæ•°æ®ã€‚å› æ­¤ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ï¼Œè¿«åˆ‡éœ€è¦åœ¨æé«˜æœºå™¨å­¦ä¹ åœ¨åŒ»å­¦æˆåƒç­‰é¢†åŸŸçš„å¯é æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢è¿›è¡Œæ”¹è¿›ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æ€è·¯ï¼Œå¹¶ä¸ºåº”ç”¨äºåŒ»å­¦å›¾åƒçš„å…¨å‚è€ƒIQAæªæ–½çš„ä½¿ç”¨æä¾›æŒ‡å¯¼å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.19097v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰åœ¨åŒ»ç–—å›¾åƒé¢†åŸŸçš„æŒ‘æˆ˜å’Œå±€é™æ€§ã€‚è™½ç„¶PSNRå’ŒSSIMç­‰å…¨å‚è€ƒï¼ˆFRï¼‰IQAæ–¹æ³•åœ¨è‡ªç„¶æˆåƒä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦åœºæ™¯ä¸­å´å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚æœ¬æ–‡æä¾›äº†åœ¨ä¸åŒåŒ»å­¦å›¾åƒç±»å‹ä¸­ä½¿ç”¨PSNRå’ŒSSIMè¯„ä¼°æ–°æ–¹æ³•æ—¶å¯èƒ½å‡ºç°çš„é—®é¢˜çš„ä¾‹å­ï¼Œå¼ºè°ƒéœ€è¦æ”¹è¿›ä»¥å¢å¼ºAIæ—¶ä»£çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ä¸´åºŠå®è·µå’ŒåŒ»å­¦å½±åƒçš„æœºå™¨å­¦ä¹ ç®—æ³•å¼€å‘é˜¶æ®µä¸­ï¼Œå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>å…¨å‚è€ƒï¼ˆFRï¼‰IQAä¸­çš„PSNRå’ŒSSIMåœ¨è‡ªç„¶æˆåƒä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åŒ»å­¦å›¾åƒä¸­å´å­˜åœ¨å·®è·å’Œä¸ä¸€è‡´æ€§ã€‚</li>
<li>åŒ»å­¦å›¾åƒå…·æœ‰ä¸è‡ªç„¶å›¾åƒä¸åŒçš„ç‰¹æ€§ï¼Œä½¿å¾—PSNRå’ŒSSIMå¯èƒ½ä¸é€‚åˆç”¨äºåŒ»å­¦å›¾åƒçš„è¯„ä¼°ã€‚</li>
<li>åœ¨ä¸´åºŠåº”ç”¨ä¸­ï¼Œé”™è¯¯çš„åˆ¤æ–­æ–°æ–¹æ³•å¯èƒ½å¯¼è‡´æœªé¢„è§çš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« æä¾›äº†åœ¨ä¸åŒåŒ»å­¦å›¾åƒç±»å‹ä¸­ä½¿ç”¨PSNRå’ŒSSIMè¯„ä¼°ç®—æ³•æ—¶å¯èƒ½å‡ºç°é—®é¢˜çš„ä¾‹å­ã€‚</li>
<li>åœ¨AIæ—¶ä»£ï¼Œéœ€è¦æé«˜åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.19097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6b66e153ec5149321f0d31854f4f184e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eefb0555571d03d45668546a7609757.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Photoevaporation-from-Inner-Protoplanetary-Disks-Confronted-with-Observations"><a href="#Photoevaporation-from-Inner-Protoplanetary-Disks-Confronted-with-Observations" class="headerlink" title="Photoevaporation from Inner Protoplanetary Disks Confronted with   Observations"></a>Photoevaporation from Inner Protoplanetary Disks Confronted with   Observations</h2><p><strong>Authors:Yiren Lin, Lile Wang, Min Fang, Ahmad Nemer, Jeremy Goodman</strong></p>
<p>The decades-long explorations on the dispersal of protoplanetary disks involve many debates about photoevaporation versus magnetized wind launching mechanisms. This letter argues that the observed winds originating from the inner disk ($R\lesssim 0.3$ AU) cannot be explained by the photoevaporative mechanism. Energy conservation requires the presumed photoevaporative winds to be heated to $\gtrsim 10^5$ K when launched from inner disks. However, due to efficient thermal accommodation with dust grains and cooling processes at high densities, X-ray irradiation at energies above 1 keV cannot efficiently launch winds in the first place because of its high penetration. Some studies claiming X-ray wind launching have oversimplified the thermochemical couplings. Furthermore, heating the gas to escape velocity will over-ionize it, suppressing the species responsible for observed forbidden lines (e.g., [OI] 6300 $\r{A}$ ). Confirmed by semi-analytic integrations of thermochemical fluid structures, such high ionizations contradict the observed emission of neutral and singly-ionized atoms from the winds originating from the inner disks. </p>
<blockquote>
<p>å¯¹äºåŸè¡Œæ˜Ÿç›˜æ‰©æ•£å‡ åå¹´çš„æ¢ç´¢ä¸­ï¼Œæ¶‰åŠåˆ°äº†è®¸å¤šå…³äºå…‰è’¸å‘å’Œç£åŒ–é£å‘å°„æœºåˆ¶ä¹‹é—´çš„äº‰è®ºã€‚è¿™ç¯‡è®ºæ–‡è®¤ä¸ºï¼Œä»å†…ç›˜ï¼ˆRâ‰¤0.3AUï¼‰è§‚æµ‹åˆ°çš„é£æ— æ³•ç”¨å…‰è’¸å‘æœºåˆ¶æ¥è§£é‡Šã€‚èƒ½é‡å®ˆæ’è¦æ±‚ä»å†…ç›˜å‘å°„çš„å…‰è’¸å‘é£åœ¨è¢«å‘å°„æ—¶éœ€è¦åŠ çƒ­åˆ°â‰¥10^5Kã€‚ç„¶è€Œï¼Œç”±äºä¸å°˜åŸƒé¢—ç²’çš„é«˜æ•ˆçƒ­é€‚åº”å’Œåœ¨é«˜å¯†åº¦ä¸‹çš„å†·å´è¿‡ç¨‹ï¼ŒXå°„çº¿è¾å°„åœ¨é«˜äº1keVçš„èƒ½é‡ä¸‹æ— æ³•æœ‰æ•ˆåœ°å‘å°„é£ï¼Œå› ä¸ºå®ƒçš„ç©¿é€æ€§å¾ˆå¼ºã€‚ä¸€äº›å£°ç§°Xå°„çº¿é£å‘å°„çš„ç ”ç©¶è¿‡äºç®€åŒ–äº†çƒ­åŒ–å­¦è€¦åˆã€‚æ­¤å¤–ï¼Œå°†æ°”ä½“åŠ çƒ­åˆ°é€ƒé€¸é€Ÿåº¦ä¼šä½¿å…¶è¿‡åº¦ç”µç¦»ï¼Œä»è€ŒæŠ‘åˆ¶äº†è§‚æµ‹åˆ°çš„ç¦æˆ’çº¿ï¼ˆä¾‹å¦‚ï¼Œ[OI] 6300Ã…ï¼‰æ‰€è´Ÿè´£çš„ç‰©ç§ã€‚é€šè¿‡çƒ­åŒ–å­¦æµä½“ç»“æ„çš„åŠè§£æç§¯åˆ†è¯å®ï¼Œè¿™ç§é«˜ç”µç¦»ä¸æ¥è‡ªå†…ç›˜çš„é£ä¸­æ€§å’Œå•ç”µç¦»åŸå­çš„è§‚æµ‹å‘å°„ç›¸çŸ›ç›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.15419v4">PDF</a> 17 pages, 6 figures, accepted for publication by the ApJ</p>
<p><strong>Summary</strong><br>     è¿™ç¯‡æ–‡ç« æ¢è®¨äº†åŸè¡Œæ˜Ÿç›˜æ‰©æ•£å‡ åå¹´çš„æ¢ç´¢å†ç¨‹ï¼Œå…¶ä¸­å…³äºå…‰è’¸å‘å’Œç£åŒ–é£å‘å°„æœºåˆ¶çš„äº‰è®®ä¸æ–­ã€‚ä½œè€…è®¤ä¸ºä»å†…ç›˜ï¼ˆRâ‰¤0.3 AUï¼‰è§‚æµ‹åˆ°çš„é£æ— æ³•ç”¨å…‰è’¸å‘æœºåˆ¶è§£é‡Šã€‚èƒ½é‡å®ˆæ’è¦æ±‚å‡è®¾çš„å…‰è’¸å‘é£ä»å†…ç›˜å‘å°„æ—¶éœ€è¦åŠ çƒ­åˆ°â‰¥10^5 Kã€‚ä½†ç”±äºä¸å°˜åŸƒé¢—ç²’çš„çƒ­é€‚åº”å’Œé«˜å¯†åº¦ä¸‹çš„å†·å´è¿‡ç¨‹ï¼ŒXå°„çº¿è¾å°„æ— æ³•æœ‰æ•ˆå‘å°„é£ã€‚ä¸€äº›å£°ç§°Xå°„çº¿é£å‘å°„çš„ç ”ç©¶è¿‡äºç®€åŒ–äº†çƒ­åŒ–å­¦è€¦åˆã€‚æ­¤å¤–ï¼Œå°†æ°”ä½“åŠ çƒ­åˆ°é€ƒé€¸é€Ÿåº¦ä¼šè¿‡åº¦ç”µç¦»å®ƒï¼ŒæŠ‘åˆ¶äº†è§‚æµ‹åˆ°çš„ç¦æˆ’çº¿ï¼ˆå¦‚OI 6300 Aï¼‰å¯¹åº”çš„ç‰©ç§ã€‚é€šè¿‡åŠè§£æç§¯åˆ†çƒ­åŒ–å­¦æµä½“ç»“æ„è¯å®ï¼Œé«˜ç”µç¦»ä¸æ¥è‡ªå†…ç›˜çš„é£ä¸­è§‚æµ‹åˆ°çš„ä¸­æ€§åŠå•ç”µç¦»åŸå­å‘å°„ç›¸çŸ›ç›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« è®¨è®ºäº†åŸè¡Œæ˜Ÿç›˜æ‰©æ•£è¿‡ç¨‹ä¸­å…³äºå…‰è’¸å‘å’Œç£åŒ–é£å‘å°„æœºåˆ¶çš„äº‰è®®ã€‚</li>
<li>ä»å†…ç›˜è§‚æµ‹åˆ°çš„é£æ— æ³•ç”¨å…‰è’¸å‘æœºåˆ¶è§£é‡Šï¼Œå› ä¸ºè¿™éœ€è¦æ°”ä½“è¢«åŠ çƒ­åˆ°è¿‡é«˜çš„æ¸©åº¦ã€‚</li>
<li>Xå°„çº¿è¾å°„æ— æ³•æœ‰æ•ˆå‘å°„é£ï¼Œå› ä¸ºå…¶ä¸å°˜åŸƒé¢—ç²’çš„çƒ­é€‚åº”ä»¥åŠé«˜å¯†åº¦ä¸‹çš„å†·å´è¿‡ç¨‹ã€‚</li>
<li>ä¸€äº›å…³äºXå°„çº¿é£å‘å°„çš„ç ”ç©¶è¿‡äºç®€åŒ–äº†çƒ­åŒ–å­¦è€¦åˆã€‚</li>
<li>åŠ çƒ­æ°”ä½“åˆ°é€ƒé€¸é€Ÿåº¦ä¼šè¿‡åº¦ç”µç¦»å®ƒï¼ŒæŠ‘åˆ¶äº†æŸäº›è§‚æµ‹åˆ°çš„ç‰©ç§çš„ç¦æˆ’çº¿ã€‚</li>
<li>é€šè¿‡åŠè§£æç§¯åˆ†çƒ­åŒ–å­¦æµä½“ç»“æ„è¯å®ï¼Œé«˜ç”µç¦»ä¸æ¥è‡ªå†…ç›˜çš„é£çš„è§‚æµ‹ç»“æœç›¸çŸ›ç›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.15419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0401c6ffaf86e7f4caa57c03a3611b38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28af3c3443bfd4943907c617187124af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-951a0cebc0724fc9320a85f362219fb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f58139d0f16d403fd05a69bc557f5b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ed252be8362a0fa0bf55cd0f42369526.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  Llasa Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a1f5e4ff0a8f27baeb597f73c500868.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  HOG-Diff Higher-Order Guided Diffusion for Graph Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
