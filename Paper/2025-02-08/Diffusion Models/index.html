<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-08  HOG-Diff Higher-Order Guided Diffusion for Graph Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1a1f5e4ff0a8f27baeb597f73c500868.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    43 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-08-更新"><a href="#2025-02-08-更新" class="headerlink" title="2025-02-08 更新"></a>2025-02-08 更新</h1><h2 id="HOG-Diff-Higher-Order-Guided-Diffusion-for-Graph-Generation"><a href="#HOG-Diff-Higher-Order-Guided-Diffusion-for-Graph-Generation" class="headerlink" title="HOG-Diff: Higher-Order Guided Diffusion for Graph Generation"></a>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</h2><p><strong>Authors:Yiming Huang, Tolga Birdal</strong></p>
<p>Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Although diffusion models have recently made significant achievements in graph generation, these models typically adapt from the frameworks designed for image generation, making them ill-suited for capturing the topological properties of graphs. In this work, we propose a novel Higher-order Guided Diffusion (HOG-Diff) model that follows a coarse-to-fine generation curriculum and is guided by higher-order information, enabling the progressive generation of plausible graphs with inherent topological structures. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Yiminghh/HOG-Diff">https://github.com/Yiminghh/HOG-Diff</a>. </p>
<blockquote>
<p>图生成是一项至关重要的任务，但具有挑战性，因为它需要对复杂的非欧几里得结构进行深入理解。尽管扩散模型最近在图生成方面取得了重大进展，但这些模型通常是从为图像生成设计的框架中改编而来的，因此难以捕捉图的拓扑属性。在这项工作中，我们提出了一种新型的高阶引导扩散（HOG-Diff）模型，该模型遵循从粗到细的生成课程，并由高阶信息引导，能够逐步生成具有固有拓扑结构的合理图。我们还证明，我们的模型具有比传统扩散框架更强的理论保证。在分子和通用图生成任务上的大量实验表明，我们的方法始终优于或具有竞争力最先进基线。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Yiminghh/HOG-Diff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yiminghh/HOG-Diff获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的基于高阶引导扩散（HOG-Diff）模型的图生成方法，它采用由粗到细的生成策略，并结合高阶信息进行引导，可以逐步生成具有内在拓扑结构的可信任图。与传统扩散模型相比，该模型具有较强的理论保证。在分子和通用图生成任务的广泛实验中，该方法表现出出色的性能，超越了现有的主流模型。模型代码已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图生成是一项关键且具有挑战性的任务，需要深入理解复杂的非欧几里得结构。</li>
<li>现有的扩散模型在图形生成中表现不佳，因为它们主要借鉴了图像生成的框架设计，无法充分捕捉图形的拓扑特性。</li>
<li>本文提出了一种新颖的Higher-order Guided Diffusion（HOG-Diff）模型，用于图形生成。该模型遵循由粗到细的生成策略，通过高阶信息进行引导。</li>
<li>HOG-Diff模型具有更强的理论保证，与传统的扩散模型相比具有优势。</li>
<li>在分子和通用图生成任务的实验中，HOG-Diff模型表现优异，超越了现有的主流方法。</li>
<li>该模型的代码已经公开，便于其他研究者进行进一步的研究和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04308">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7e1b5133e2416b5ec197f2bd19bf43ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e00c46e36b604248fc6e2b66d9b347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5fe0de6d9d7911029fb5341287ce3d3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PartEdit-Fine-Grained-Image-Editing-using-Pre-Trained-Diffusion-Models"><a href="#PartEdit-Fine-Grained-Image-Editing-using-Pre-Trained-Diffusion-Models" class="headerlink" title="PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models"></a>PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models</h2><p><strong>Authors:Aleksandar Cvejic, Abdelrahman Eldesokey, Peter Wonka</strong></p>
<p>We present the first text-based image editing approach for object parts based on pre-trained diffusion models. Diffusion-based image editing approaches capitalized on the deep understanding of diffusion models of image semantics to perform a variety of edits. However, existing diffusion models lack sufficient understanding of many object parts, hindering fine-grained edits requested by users. To address this, we propose to expand the knowledge of pre-trained diffusion models to allow them to understand various object parts, enabling them to perform fine-grained edits. We achieve this by learning special textual tokens that correspond to different object parts through an efficient token optimization process. These tokens are optimized to produce reliable localization masks at each inference step to localize the editing region. Leveraging these masks, we design feature-blending and adaptive thresholding strategies to execute the edits seamlessly. To evaluate our approach, we establish a benchmark and an evaluation protocol for part editing. Experiments show that our approach outperforms existing editing methods on all metrics and is preferred by users 77-90% of the time in conducted user studies. </p>
<blockquote>
<p>我们首次提出了一种基于预训练扩散模型的面向物体部件的文本驱动图像编辑方法。基于扩散模型的图像编辑方法利用对图像语义扩散模型的深入理解来进行各种编辑操作。然而，现有的扩散模型对许多物体部件的理解不足，阻碍了用户要求的精细编辑。为了解决这一问题，我们提出对预训练的扩散模型进行知识扩展，使它们能够理解各种物体部件，从而进行精细编辑。我们通过高效的令牌优化过程，学习对应于不同物体部件的特殊文本令牌。这些令牌经过优化，在每个推理步骤中产生可靠的定位掩码，以定位编辑区域。利用这些掩码，我们设计了特征混合和自适应阈值策略，以无缝执行编辑操作。为了评估我们的方法，我们建立了部件编辑的基准测试和评价协议。实验表明，我们的方法在所有指标上的编辑效果优于现有编辑方法，在用户研究中，用户偏好我们的方法的时间占比达到77-90%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04050v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://partedit.github.io/PartEdit/">https://partedit.github.io/PartEdit/</a></p>
<p><strong>Summary</strong><br>     本文首次提出了基于预训练扩散模型的文本驱动图像编辑方法，用于处理对象部分的编辑。通过优化扩散模型的知识，使其能够理解各种对象部分，实现精细编辑。通过高效的文本令牌优化过程，学习对应于不同对象部分的特殊文本令牌，用于生成可靠的定位掩膜，定位编辑区域。结合这些掩膜，设计特征融合和自适应阈值策略，实现无缝编辑。实验表明，该方法在各项评估指标上均优于现有编辑方法，并在用户研究中获得用户77%-90%的青睐。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>首次提出基于预训练扩散模型的文本驱动图像编辑方法，用于处理对象部分的编辑。</li>
<li>扩散模型通过优化知识，提高对对象部分的理解，实现精细编辑。</li>
<li>通过高效的文本令牌优化过程，学习对应于不同对象部分的特殊文本令牌。</li>
<li>特殊文本令牌生成可靠的定位掩膜，用于定位编辑区域。</li>
<li>结合定位掩膜，采用特征融合和自适应阈值策略实现无缝编辑。</li>
<li>实验表明该方法在各项评估指标上优于现有编辑方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-de2ceec31464e9ea0b706b4ae2672e9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb1a4c969fcad2d6da472fcf5b31d00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7291d443022956c0e84d2ecc102762a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed5a0ed4f30894066d73034e474a2305.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa96e87201ec8d7b9d05bfbc009a3997.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeblurDiff-Real-World-Image-Deblurring-with-Generative-Diffusion-Models"><a href="#DeblurDiff-Real-World-Image-Deblurring-with-Generative-Diffusion-Models" class="headerlink" title="DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models"></a>DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models</h2><p><strong>Authors:Lingshun Kong, Jiawei Zhang, Dongqing Zou, Jimmy Ren, Xiaohe Wu, Jiangxin Dong, Jinshan Pan</strong></p>
<p>Diffusion models have achieved significant progress in image generation. The pre-trained Stable Diffusion (SD) models are helpful for image deblurring by providing clear image priors. However, directly using a blurry image or pre-deblurred one as a conditional control for SD will either hinder accurate structure extraction or make the results overly dependent on the deblurring network. In this work, we propose a Latent Kernel Prediction Network (LKPN) to achieve robust real-world image deblurring. Specifically, we co-train the LKPN in latent space with conditional diffusion. The LKPN learns a spatially variant kernel to guide the restoration of sharp images in the latent space. By applying element-wise adaptive convolution (EAC), the learned kernel is utilized to adaptively process the input feature, effectively preserving the structural information of the input. This process thereby more effectively guides the generative process of Stable Diffusion (SD), enhancing both the deblurring efficacy and the quality of detail reconstruction. Moreover, the results at each diffusion step are utilized to iteratively estimate the kernels in LKPN to better restore the sharp latent by EAC. This iterative refinement enhances the accuracy and robustness of the deblurring process. Extensive experimental results demonstrate that the proposed method outperforms state-of-the-art image deblurring methods on both benchmark and real-world images. </p>
<blockquote>
<p>扩散模型在图像生成方面取得了显著进展。预训练的Stable Diffusion（SD）模型通过提供清晰的图像先验，对图像去模糊化很有帮助。然而，直接使用模糊图像或预先去模糊的图像作为SD的条件控制会阻碍准确的结构提取，或使结果过于依赖去模糊网络。在本研究中，我们提出了一种潜核预测网络（Latent Kernel Prediction Network，LKPN）来实现稳健的现实世界图像去模糊化。具体来说，我们在潜在空间中对LKPN与条件扩散进行共同训练。LKPN学习一个空间变化的核来指导潜在空间中清晰图像的恢复。通过应用逐元素自适应卷积（Element-wise Adaptive Convolution，EAC），学习到的核被用来自适应地处理输入特征，有效地保留输入的结构信息。这一过程更有效地指导了Stable Diffusion（SD）的生成过程，提高了去模糊化的效果以及细节重建的质量。此外，每一步扩散的结果都被用来通过EAC迭代估计LKPN中的核，以更好地恢复清晰的潜在特征。这种迭代细化提高了去模糊过程的准确性和稳健性。大量的实验结果证明，所提出的方法在基准测试和真实世界图像上去模糊化的效果优于当前先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03810v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>稳定扩散模型在图像生成领域取得了显著进展，但对于去模糊任务仍存在挑战。本文提出了一种潜核预测网络（LKPN），在潜在空间与条件扩散共同训练，学习空间可变核以指导潜在空间的清晰图像恢复。通过应用元素级自适应卷积（EAC），学习到的核能够自适应处理输入特征，有效保留输入的结构信息。这一过程更有效地指导了稳定扩散（SD）的生成过程，提高了去模糊效果和细节重建质量。此外，通过在各扩散步骤的结果上迭代估计LKPN中的核，以更好地通过EAC恢复清晰的潜在图像。这种迭代优化提高了去模糊过程的准确性和稳健性。实验结果表明，该方法在基准测试和真实图像上的图像去模糊效果均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稳定扩散模型在图像生成领域取得显著进展，但去模糊任务仍具挑战。</li>
<li>潜核预测网络（LKPN）被提出，用于在潜在空间与条件扩散共同训练。</li>
<li>LKPN学习空间可变核，以指导潜在空间的清晰图像恢复。</li>
<li>元素级自适应卷积（EAC）用于自适应处理输入特征，保留结构信息。</li>
<li>EAC有效指导稳定扩散（SD）的生成过程，提高去模糊效果和细节重建质量。</li>
<li>通过迭代估计核来优化去模糊过程，提高准确性和稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-71756b51a965ffe850a50052725dbf4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72eff6aeda50f5238c64a78cf06b1286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a1f5e4ff0a8f27baeb597f73c500868.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2d6deef79f6ad881bd453cfe4763021.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1406c23a222732ee7def159d8747106d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free"><a href="#Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free" class="headerlink" title="Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free"></a>Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free</h2><p><strong>Authors:Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: <a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/">https://faverogian.github.io/med-diffusion-classifier.github.io/</a> </p>
<blockquote>
<p>判别分类器已成为医学影像深度学习中的基础工具，擅长学习复杂数据分布的可分特征。然而，为了确保这些模型的安全可靠部署，通常需要进行精心设计、数据增强和训练技术。最近，扩散模型已成为二维生成模型的代名词。这些模型展示了在各种任务中的稳健性，包括自然图像分类，分类是通过比较针对每个可能的条件输入生成的图像之间的重建误差来完成的。这项工作首次探索了二维医学图像分类中类条件扩散模型的潜力。首先，我们开发了一种新型多数投票方案，该方案已证明可以提高医学扩散分类器的性能。接下来，在CheXpert和ISIC黑色素瘤皮肤癌数据集上的大量实验表明，基础扩散模型和从头开始训练的扩散模型均达到了与最新判别分类器相当的性能水平，无需显式监督。此外，我们还证明了扩散分类器本质上是可解释的，并可用于量化其预测的确定性，从而增加了其在安全关键的医疗环境中的可信度和可靠性。更多信息请参见我们的项目页面：<a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier">https://faverogian.github.io/med-diffusion-classifier</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在二维医疗图像分类中具有巨大潜力。通过开发新型多数投票方案，扩散分类器的性能得到了提升，且在CheXpert和ISIC黑色素瘤皮肤癌数据集上的实验表明，与最先进的判别分类器相比，基础扩散模型和从头开始训练的扩散模型具有竞争力，且无需显式监督。此外，扩散分类器具有内在的可解释性，可量化预测的不确定性，从而增加其在安全关键的临床环境中的可信度和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在二维医疗图像分类中具有潜力。</li>
<li>开发了一种新型多数投票方案，提高了医疗扩散分类器的性能。</li>
<li>在CheXpert和ISIC黑色素瘤皮肤癌数据集上的实验表明，扩散模型与最先进的判别分类器具有竞争力。</li>
<li>扩散模型无需显式监督。</li>
<li>扩散分类器具有内在的可解释性。</li>
<li>扩散分类器可量化预测的不确定性。</li>
<li>扩散分类器在临床环境中具有可信度和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03687">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f25bdeacfbc5a65cf6ba4cbabfb56a85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09b52a6ca7cb81e8cde2b8cf39ed9051.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b9cb79da2ff3d4912347f214f296d5e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise"><a href="#FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise" class="headerlink" title="FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise"></a>FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise</h2><p><strong>Authors:Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang</strong></p>
<p>Text-driven video generation has advanced significantly due to developments in diffusion models. Beyond the training and sampling phases, recent studies have investigated noise priors of diffusion models, as improved noise priors yield better generation results. One recent approach employs the Fourier transform to manipulate noise, marking the initial exploration of frequency operations in this context. However, it often generates videos that lack motion dynamics and imaging details. In this work, we provide a comprehensive theoretical analysis of the variance decay issue present in existing methods, contributing to the loss of details and motion dynamics. Recognizing the critical impact of noise distribution on generation quality, we introduce FreqPrior, a novel noise initialization strategy that refines noise in the frequency domain. Our method features a novel filtering technique designed to address different frequency signals while maintaining the noise prior distribution that closely approximates a standard Gaussian distribution. Additionally, we propose a partial sampling process by perturbing the latent at an intermediate timestep during finding the noise prior, significantly reducing inference time without compromising quality. Extensive experiments on VBench demonstrate that our method achieves the highest scores in both quality and semantic assessments, resulting in the best overall total score. These results highlight the superiority of our proposed noise prior. </p>
<blockquote>
<p>基于文本的视频生成由于扩散模型的发展而取得了显著进展。除了训练和采样阶段，最近的研究还探讨了扩散模型的噪声先验，因为改进的噪声先验会产生更好的生成结果。一种最近的方法使用傅里叶变换来操作噪声，标志着在此背景下对频率操作的初步探索。然而，它通常生成的视频缺乏运动动力和成像细节。在这项工作中，我们对现有方法中存在的方差衰减问题进行了全面的理论分析，这个问题导致了细节和运动动力的损失。我们认识到噪声分布对生成质量的关键影响，因此引入了FreqPrior，这是一种新的噪声初始化策略，它在频率域中优化噪声。我们的方法采用了一种新型滤波技术，旨在处理不同的频率信号，同时保持噪声先验分布，近似于标准高斯分布。此外，我们通过在中间时间步长找到噪声先验时扰动潜在空间，提出了部分采样过程，这显著减少了推理时间，同时不妥协质量。在VBench上的广泛实验表明，我们的方法在质量和语义评估方面都获得了最高分，总分最高。这些结果突显了我们提出的噪声先验的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03496v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     扩散模型在文本驱动的视频生成领域取得了显著进展。最新研究开始探索扩散模型的噪声先验，因为更好的噪声先验能产生更好的生成结果。本文全面分析了现有方法中存在的方差衰减问题，导致细节和运动动力丢失。为了解决噪声分布对生成质量的关键影响，我们引入了FreqPrior，一种在频域优化噪声的新型噪声初始化策略。此外，我们通过在寻找噪声先验的中间步骤扰动潜在变量，提出了部分采样过程，显著减少了推理时间，同时不妥协于质量。实验表明，我们的方法在质量和语义评估方面都获得了最高分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本驱动的视频生成中有显著进展。</li>
<li>噪声先验在扩散模型中扮演重要角色，能影响生成结果的质量。</li>
<li>现有方法存在方差衰减问题，导致生成的视频缺乏运动动力和成像细节。</li>
<li>引入FreqPrior，一种新型噪声初始化策略，旨在优化频域中的噪声。</li>
<li>提出一种部分采样过程，通过扰动寻找噪声先验的中间步骤的潜在变量，以减少推理时间而不影响质量。</li>
<li>实验表明，在VBench上，该方法在质量和语义评估方面获得最高分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03496">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dffa4c70e1fc3553298911b80e80cb2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c37c94ac0a1a4dc06f70af4af0b66791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c4a1fab50665f60a48c1458a9b8d618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35ba758f62d68e0b15ee423d524ca085.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MetaFE-DE-Learning-Meta-Feature-Embedding-for-Depth-Estimation-from-Monocular-Endoscopic-Images"><a href="#MetaFE-DE-Learning-Meta-Feature-Embedding-for-Depth-Estimation-from-Monocular-Endoscopic-Images" class="headerlink" title="MetaFE-DE: Learning Meta Feature Embedding for Depth Estimation from   Monocular Endoscopic Images"></a>MetaFE-DE: Learning Meta Feature Embedding for Depth Estimation from   Monocular Endoscopic Images</h2><p><strong>Authors:Dawei Lu, Deqiang Xiao, Danni Ai, Jingfan Fan, Tianyu Fu, Yucong Lin, Hong Song, Xujiong Ye, Lei Zhang, Jian Yang</strong></p>
<p>Depth estimation from monocular endoscopic images presents significant challenges due to the complexity of endoscopic surgery, such as irregular shapes of human soft tissues, as well as variations in lighting conditions. Existing methods primarily estimate the depth information from RGB images directly, and often surffer the limited interpretability and accuracy. Given that RGB and depth images are two views of the same endoscopic surgery scene, in this paper, we introduce a novel concept referred as &#96;&#96;meta feature embedding (MetaFE)”, in which the physical entities (e.g., tissues and surgical instruments) of endoscopic surgery are represented using the shared features that can be alternatively decoded into RGB or depth image. With this concept, we propose a two-stage self-supervised learning paradigm for the monocular endoscopic depth estimation. In the first stage, we propose a temporal representation learner using diffusion models, which are aligned with the spatial information through the cross normalization to construct the MetaFE. In the second stage, self-supervised monocular depth estimation with the brightness calibration is applied to decode the meta features into the depth image. Extensive evaluation on diverse endoscopic datasets demonstrates that our approach outperforms the state-of-the-art method in depth estimation, achieving superior accuracy and generalization. The source code will be publicly available. </p>
<blockquote>
<p>从单目内镜图像进行深度估计面临着巨大的挑战，主要由于内镜手术的复杂性，例如人体软组织的不规则形状，以及光照条件的变化。现有的方法主要直接从RGB图像估计深度信息，通常存在解释性和准确性的局限性。鉴于RGB图像和深度图像是同一内镜手术场景的两个视图，本文引入了一个新概念，称为“元特征嵌入（MetaFE）”，其中内镜手术中的物理实体（例如，组织和手术器械）用可交替解码为RGB或深度图像的共享特征来表示。基于这个概念，我们提出了一个用于单目内镜深度估计的两阶段自监督学习范式。在第一阶段，我们提出了一个使用扩散模型的时序表示学习者，通过跨归一化与空间信息进行对齐，以构建MetaFE。在第二阶段，应用带有亮度校准的自监督单目深度估计来将元特征解码为深度图像。在多种内镜数据集上的广泛评估表明，我们的方法在深度估计方面优于最新技术，具有更高的准确性和泛化能力。源代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03493v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出了一种基于单目内窥镜图像深度估计的新方法，通过引入“元特征嵌入”（MetaFE）概念，将物理实体（如组织和手术器械）用共享特征表示，这些特征可以解码为RGB或深度图像。提出一种两阶段自监督学习方法，第一阶段使用扩散模型进行时间表示学习，通过与空间信息对齐构建MetaFE；第二阶段采用自监督单目内窥镜深度估计，对亮度进行校准后解码出深度图像。实验证明该方法在多种内窥镜数据集上的表现优于现有方法，准确度和泛化能力有所提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入“元特征嵌入”（MetaFE）概念，将物理实体表示为共享特征，可解码为RGB或深度图像。</li>
<li>采用两阶段自监督学习方法进行单目内窥镜深度估计。</li>
<li>第一阶段使用扩散模型进行时间表示学习，通过与空间信息对齐构建MetaFE。</li>
<li>第二阶段应用自监督学习进行深度估计，结合亮度校准解码出深度图像。</li>
<li>方法在多种内窥镜数据集上的表现优于现有方法。</li>
<li>提高了深度估计的准确性和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5fc69bad5b60d68a3edfccfa02e2b494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-960248560f8ef4331ca62872a87ca038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d3e6c7d6bcaf5e2bbc14d3a74a34db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d6a3546f49243197b86b96ccad23ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32e36d13fcab0f37c61ba359427aa588.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lanpaint-Training-Free-Diffusion-Inpainting-with-Exact-and-Fast-Conditional-Inference"><a href="#Lanpaint-Training-Free-Diffusion-Inpainting-with-Exact-and-Fast-Conditional-Inference" class="headerlink" title="Lanpaint: Training-Free Diffusion Inpainting with Exact and Fast   Conditional Inference"></a>Lanpaint: Training-Free Diffusion Inpainting with Exact and Fast   Conditional Inference</h2><p><strong>Authors:Candi Zheng, Yuan Lan, Yang Wang</strong></p>
<p>Diffusion models generate high-quality images but often lack efficient and universally applicable inpainting capabilities, particularly in community-trained models. We introduce LanPaint, a training-free method tailored for widely adopted ODE-based samplers, which leverages Langevin dynamics to perform exact conditional inference, enabling precise and visually coherent inpainting. LanPaint addresses two key challenges in Langevin-based inpainting: (1) the risk of local likelihood maxima trapping and (2) slow convergence. By proposing a guided score function and a fast-converging Langevin framework, LanPaint achieves high-fidelity results in very few iterations. Experiments demonstrate that LanPaint outperforms existing training-free inpainting techniques, outperforming in challenging tasks such as outpainting with Stable Diffusion. </p>
<blockquote>
<p>扩散模型可以生成高质量图像，但通常缺乏高效且普遍适用的补全能力，特别是在社区训练模型中。我们推出了LanPaint，这是一种无需训练的方法，适用于广泛采用的基于ODE的采样器，它利用朗之万动力学进行精确的条件推断，能够实现精确且视觉连贯的补全。LanPaint解决了朗之万补全中的两个关键挑战：（1）局部似然极大值陷阱的风险和（2）收敛速度慢。通过提出引导得分函数和快速收敛的朗之万框架，LanPaint可以在很少迭代次数内实现高保真结果。实验表明，LanPaint在补全技术方面无需训练，并且在具有挑战性的任务（如使用Stable Diffusion进行补全）中表现优于现有技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03491v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本介绍了LanPaint，一种无需训练的方法，适用于广泛采用的ODE采样器，利用朗之万动力学进行精确的条件推断，实现精确且视觉连贯的补全。该方法解决了朗之万补全中的两个关键问题：局部概率最大值的陷阱风险和收敛速度慢的问题。通过提出引导评分函数和快速收敛的朗之万框架，LanPaint在很少迭代次数内实现了高保真结果，并在外推任务中表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型虽然能生成高质量图像，但在社区训练模型中常常缺乏高效且普遍适用的补全能力。</li>
<li>LanPaint是一种无需训练的方法，适用于ODE采样器，利用朗之万动力学进行精确的条件推断。</li>
<li>LanPaint解决了朗之万动力学补全中的两个关键问题：局部概率最大值的陷阱风险和收敛速度慢。</li>
<li>通过引导评分函数和快速收敛的朗之万框架，LanPaint在较少的迭代次数内实现了高保真结果。</li>
<li>LanPaint在补全任务中表现出色，特别是在具有挑战性的外推任务中。</li>
<li>与现有的无需训练的补全技术相比，LanPaint具有更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03491">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-569eaf1f3da9979808ed7cde1ff43aba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59244e39a70a97aefbdbeec244f098e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5de1e608f6627de9e69d5076b6ed84ff.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an $\textbf{online}$ algorithm capable of collecting data during runtime and supporting a $\textbf{black-box}$ objective function. Moreover, the $\textbf{query efficiency}$ of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, $\textbf{Fast Direct}$, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>引导扩散模型生成是定制预训练扩散模型的生成过程以应对特定下游任务的一个前景方向。现有的引导扩散模型要么依赖于使用预先收集的数据集训练引导模型，要么需要目标函数可微。然而，对于大多数现实世界任务而言，离线数据集通常不可用，而且其目标函数通常不可微分，例如具有人类偏好的图像生成、用于药物发现的分子生成和材料设计。因此，我们需要一种能够在运行时收集数据并支持黑箱目标函数的<strong>在线</strong>算法。此外，算法的<strong>查询效率</strong>也非常关键，因为在现实场景中，目标查询的评估往往非常昂贵。在这项工作中，我们提出了一种新颖而简单的算法——<strong>Fast Direct</strong>，用于高效查询在线黑箱目标生成。我们的Fast Direct在数据流形上构建伪目标，以通用方向更新扩散模型的噪声序列，这有望实现高效的查询引导生成。在十二个高分辨率（1024×1024）图像目标生成任务和六个3D分子目标生成任务的大量实验显示，查询效率提高了6倍至10倍和提高了最高至的查询效率改善比例分别达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达至高达达高达达达达达达达达达达达达达达达达达大44倍。我们的实现已在以下网址公开可用：<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型在定制化生成过程中的潜力与局限性。现有引导扩散模型依赖于预收集数据集进行训练的目标函数或需要可微分的目标函数，但在现实世界的许多任务中，离线数据集往往无法获取且其目标函数往往不可微分。因此，本文提出了一种新型的在线黑盒目标生成算法——Fast Direct，该算法能够在运行时收集数据并支持黑盒目标函数，同时具有较高的查询效率。实验结果显示，Fast Direct在高分辨率图像生成和三维分子生成等任务中取得了显著的查询效率提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在定制化生成过程中具有巨大潜力，特别是在处理下游任务时。</li>
<li>现有引导扩散模型依赖于预收集数据集或可微分的目标函数，这在现实任务中并不常见。</li>
<li>Fast Direct算法解决了这一问题，通过在线收集数据并支持黑盒目标函数，实现了高效的查询。</li>
<li>Fast Direct算法在高分辨率图像生成和三维分子生成等任务中取得了显著的查询效率提升。</li>
<li>Fast Direct算法通过构建伪目标来更新扩散模型的噪声序列，并采用了通用方向进行引导生成。</li>
<li>该算法已在多个任务上进行了广泛实验验证，包括十二个高分辨率图像生成任务和六个三维分子生成任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d89fdc0114ed0a2df5ffaebd5f3bd5f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e6b3245a8fea1168ea1a33d9883dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02e30e6862644daeb69fdee46ecbd0f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiffZOO-A-Purely-Query-Based-Black-Box-Attack-for-Red-teaming-Text-to-Image-Generative-Model-via-Zeroth-Order-Optimization"><a href="#DiffZOO-A-Purely-Query-Based-Black-Box-Attack-for-Red-teaming-Text-to-Image-Generative-Model-via-Zeroth-Order-Optimization" class="headerlink" title="DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming   Text-to-Image Generative Model via Zeroth Order Optimization"></a>DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming   Text-to-Image Generative Model via Zeroth Order Optimization</h2><p><strong>Authors:Pucheng Dang, Xing Hu, Dong Li, Rui Zhang, Qi Guo, Kaidi Xu</strong></p>
<p>Current text-to-image (T2I) synthesis diffusion models raise misuse concerns, particularly in creating prohibited or not-safe-for-work (NSFW) images. To address this, various safety mechanisms and red teaming attack methods are proposed to enhance or expose the T2I model’s capability to generate unsuitable content. However, many red teaming attack methods assume knowledge of the text encoders, limiting their practical usage. In this work, we rethink the case of \textit{purely black-box} attacks without prior knowledge of the T2l model. To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt space, we propose DiffZOO which applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain. We evaluated our method across multiple safety mechanisms of the T2I diffusion model and online servers. Experiments on multiple state-of-the-art safety mechanisms show that DiffZOO attains an 8.5% higher average attack success rate than previous works, hence its promise as a practical red teaming tool for T2l models. </p>
<blockquote>
<p>当前文本到图像（T2I）合成扩散模型引发了滥用担忧，特别是在创建禁止或不适合工作场合（NSFW）的图像方面。为解决这一问题，提出了各种安全机制和红队攻击方法来增强或揭露T2I模型生成不合适内容的能力。然而，许多红队攻击方法需要了解文本编码器，从而限制了它们的实际应用。在这项工作中，我们重新考虑了无需事先了解T2I模型的“纯黑箱”攻击的情况。为了克服无法获取梯度以及在离散提示空间内无法优化攻击的局限性，我们提出了DiffZOO，它采用零阶优化来获取梯度近似值，并利用C-PRV和D-PRV在离散提示域内增强攻击提示。我们在T2I扩散模型的多重安全机制和在线服务器上评估了我们的方法。对多种最先进的安全机制的实验表明，DiffZOO的平均攻击成功率比先前的工作高出8.5%，因此在T2I模型的实用红队工具中显示出其潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11071v2">PDF</a> </p>
<p><strong>Summary</strong><br>文本生成领域中的文本到图像（T2I）合成扩散模型存在滥用风险，特别是在生成不适宜工作或禁止的图像方面。为应对这一问题，研究者提出了多种安全机制和红队攻击方法，以提升或揭示T2I模型生成不合适内容的能力。然而，许多红队攻击方法需要了解文本编码器，限制了其实用性。本研究重新思考了无需了解T2I模型的纯黑箱攻击情况。为克服无法获取梯度以及在离散提示空间内优化攻击的难题，我们提出了DiffZOO，它采用零阶优化来获取梯度近似值，并利用C-PRV和D-PRV增强离散提示域内的攻击提示。我们在多个T2I扩散模型安全机制和在线服务器上评估了该方法。实验表明，相较于以往的研究，DiffZOO的平均攻击成功率提高了8.5%，成为T2I模型实用红队工具的有力候选。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像（T2I）合成扩散模型存在生成不适宜内容的滥用风险。</li>
<li>为应对此问题，研究者提出了多种安全机制和红队攻击方法。</li>
<li>现有红队攻击方法往往需要了解文本编码器，限制了其实际应用。</li>
<li>研究提出了DiffZOO方法，能在不了解T2I模型的情况下进行纯黑箱攻击。</li>
<li>DiffZOO采用零阶优化获取梯度近似值，增强离散提示空间内的攻击提示。</li>
<li>实验表明，DiffZOO在多个安全机制上的攻击成功率高于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11071">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8233fb263083a5e3cc45058f7c7329f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c81e5054e9fdc84bbb2f0f8e41792d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2e24807b981bbc7d17f8e0cdb7fb6d0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Lightning-Fast-Image-Inversion-and-Editing-for-Text-to-Image-Diffusion-Models"><a href="#Lightning-Fast-Image-Inversion-and-Editing-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion   Models"></a>Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion   Models</h2><p><strong>Authors:Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, Rami Ben-Ari</strong></p>
<p>Diffusion inversion is the problem of taking an image and a text prompt that describes it and finding a noise latent that would generate the exact same image. Most current deterministic inversion techniques operate by approximately solving an implicit equation and may converge slowly or yield poor reconstructed images. We formulate the problem by finding the roots of an implicit equation and devlop a method to solve it efficiently. Our solution is based on Newton-Raphson (NR), a well-known technique in numerical analysis. We show that a vanilla application of NR is computationally infeasible while naively transforming it to a computationally tractable alternative tends to converge to out-of-distribution solutions, resulting in poor reconstruction and editing. We therefore derive an efficient guided formulation that fastly converges and provides high-quality reconstructions and editing. We showcase our method on real image editing with three popular open-sourced diffusion models: Stable Diffusion, SDXL-Turbo, and Flux with different deterministic schedulers. Our solution, Guided Newton-Raphson Inversion, inverts an image within 0.4 sec (on an A100 GPU) for few-step models (SDXL-Turbo and Flux.1), opening the door for interactive image editing. We further show improved results in image interpolation and generation of rare objects. </p>
<blockquote>
<p>扩散反演（Diffusion Inversion）问题是指给定一张图片和描述它的文本提示，寻找能够生成相同图片的噪声潜在因子。当前大多数的确定性反演技术都是通过近似解决隐式方程实现的，可能会导致收敛速度慢或重构的图像质量不佳。我们通过寻找隐式方程的根来表述问题，并开发了一种高效解决方法。我们的解决方案基于数值分析中的知名技术——牛顿-拉夫森方法（Newton-Raphson，简称NR）。我们发现，直接应用NR在计算上不可行，而将其简单转换为可计算的替代方案则往往收敛于非分布解，导致重构和编辑质量差。因此，我们推导出了一个高效的引导式公式，它能快速收敛，并提供高质量的重构和编辑。我们在真实图像编辑中展示了我们的方法，使用了三个流行的开源扩散模型：Stable Diffusion、SDXL-Turbo和Flux，以及不同的确定性调度器。我们的解决方案——引导式牛顿-拉夫森反演法（Guided Newton-Raphson Inversion），能在A100 GPU上实现0.4秒内对少数模型（SDXL-Turbo和Flux 1）进行图像反演，为交互式图像编辑打开了大门。我们还展示了在图像插值和稀有对象生成方面的改进结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.12540v5">PDF</a> Accepted to ICLR25. Project Page:   <a target="_blank" rel="noopener" href="https://barakmam.github.io/rnri.github.io/">https://barakmam.github.io/rnri.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散反转问题，即根据图像和描述它的文本提示找到生成相同图像的噪声潜在因素。文章提出了一种基于牛顿-拉夫森（Newton-Raphson）方法的解决方案，有效解决了扩散反转问题中的高效求解方法。该方法快速收敛，提供了高质量的重建和编辑效果，可以在实际应用中用于图像编辑、图像插值和稀有对象生成等领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散反转问题旨在从给定的图像和文本描述中找出生成该图像的噪声潜在因素。</li>
<li>当前大多数确定性反转技术通过近似解决隐式方程来操作，但可能存在收敛速度慢或重建图像质量差的问题。</li>
<li>文章提出了一种基于牛顿-拉夫森方法的解决方案，该方法在计算上更高效且能快速收敛。</li>
<li>该方法提供了高质量的图像重建和编辑效果。</li>
<li>该方法在实际应用中可用于图像编辑、图像插值和稀有对象生成等领域。</li>
<li>文章展示了在真实图像编辑中使用三种流行的开源扩散模型（Stable Diffusion、SDXL-Turbo和Flux）的结果，验证了方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.12540">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-63f80903a9294a89f04c0ee1c95afcab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880f9634c2016fee05251b5da94b95ce.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sanitizing-Hidden-Information-with-Diffusion-Models"><a href="#Sanitizing-Hidden-Information-with-Diffusion-Models" class="headerlink" title="Sanitizing Hidden Information with Diffusion Models"></a>Sanitizing Hidden Information with Diffusion Models</h2><p><strong>Authors:Preston K. Robinette, Daniel Moyer, Taylor T. Johnson</strong></p>
<p>Information hiding is the process of embedding data within another form of data, often to conceal its existence or prevent unauthorized access. This process is commonly used in various forms of secure communications (steganography) that can be used by bad actors to propagate malware, exfiltrate victim data, and discreetly communicate. Recent work has utilized deep neural networks to remove this hidden information in a defense mechanism known as sanitization. Previous deep learning works, however, are unable to scale efficiently beyond the MNIST dataset. In this work, we present a novel sanitization method called DM-SUDS that utilizes a diffusion model framework to sanitize&#x2F;remove hidden information from image-into-image universal and dependent steganography from CIFAR-10 and ImageNet datasets. We evaluate DM-SUDS against three different baselines using MSE, PSNR, SSIM, and NCC metrics and provide further detailed analysis through an ablation study. DM-SUDS outperforms all three baselines and significantly improves image preservation MSE by 50.44%, PSNR by 12.69%, SSIM by 11.49%, and NCC by 3.26% compared to previous deep learning approaches. Additionally, we introduce a novel evaluation specification that considers the successful removal of hidden information (safety) as well as the resulting quality of the sanitized image (utility). We further demonstrate the versatility of this method with an application in an audio case study, demonstrating its broad applicability to additional domains. </p>
<blockquote>
<p>信息隐藏是将数据嵌入另一种数据形式中的过程，通常用于隐藏其存在或防止未经授权的访问。这一过程中常用于各种安全通信（隐写术），不良行为者可能利用它来传播恶意软件、窃取受害者数据并进行隐秘通信。最近的工作利用深度神经网络来去除这种隐藏信息，作为一种名为清洗的防御机制。然而，以前的深度学习工作在处理MNIST数据集之外的数据时无法有效扩展。在这项工作中，我们提出了一种新的清洗方法DM-SUDS，它利用扩散模型框架来清洗&#x2F;去除CIFAR-10和ImageNet数据集中图像内图像通用和依赖隐写术的隐藏信息。我们使用均方误差（MSE）、峰值信噪比（PSNR）、结构相似性度量（SSIM）和归一化相关系数（NCC）等指标对DM-SUDS与三种不同基线进行了评估，并通过消融研究提供了进一步的分析。与之前的深度学习方法相比，DM-SUDS在图像保留的MSE、PSNR、SSIM和NCC方面分别提高了50.44%、12.69%、11.49%和3.26%，并且超越了所有三条基线。此外，我们引入了一项新的评估标准，该标准考虑了成功去除隐藏信息（安全性）以及清洗后图像的质量（效用）。我们还通过音频案例研究展示了该方法的通用性，证明了其在其他领域的广泛应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06951v2">PDF</a> Accepted to European Conference on Artificial Intelligence (ECAI),   2024</p>
<p><strong>Summary</strong><br>     本研究提出一种名为DM-SUDS的新型净化方法，利用扩散模型框架去除图像和音频中的隐藏信息，以用于防御隐藏信息的攻击。相较于之前深度学习的方法，DM-SUDS能在CIFAR-10和ImageNet数据集上更有效地去除隐藏信息，提高图像保留效果。此外，研究还引入了一种新的评价标准，同时考虑成功去除隐藏信息的安全性和净化后图像的质量。DM-SUDS方法具有广泛的应用性，可用于音频等领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>信息隐藏是将数据嵌入另一种数据形式中的过程，常用于安全通信中。</li>
<li>近期工作利用深度神经网络去除隐藏信息，作为防御机制。</li>
<li>现有方法难以有效扩展至MNIST数据集以外的领域。</li>
<li>DM-SUDS方法利用扩散模型框架，有效去除图像中的隐藏信息，并展现出对CIFAR-10和ImageNet数据集的优异性能。</li>
<li>与三种基线方法相比，DM-SUDS在图像保留效果上有显著改进。</li>
<li>研究引入了综合考虑成功去除隐藏信息与净化后图像质量的新评价标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.06951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-85b5a260424ab0cb9605761735953d15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b11e10d7874a985481cd768d64dd29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8565c6c690cb14a07b1863aafaab05d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6027626f71538b4b9ce8eccd1d89588c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56a7fdda87b7ee6715768e4e90089ccd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93fee308770d8ef938141d7a3470ebc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5c265730e323ed2e7668b32b4f7572b0.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-08  ConceptAttention Diffusion Transformers Learn Highly Interpretable   Features
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-710e0095fca0cad8157c10daaefefc91.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-02-08  GS-LiDAR Generating Realistic LiDAR Point Clouds with Panoramic   Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
