<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  HOG-Diff Higher-Order Guided Diffusion for Graph Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1a1f5e4ff0a8f27baeb597f73c500868.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-08-æ›´æ–°"><a href="#2025-02-08-æ›´æ–°" class="headerlink" title="2025-02-08 æ›´æ–°"></a>2025-02-08 æ›´æ–°</h1><h2 id="HOG-Diff-Higher-Order-Guided-Diffusion-for-Graph-Generation"><a href="#HOG-Diff-Higher-Order-Guided-Diffusion-for-Graph-Generation" class="headerlink" title="HOG-Diff: Higher-Order Guided Diffusion for Graph Generation"></a>HOG-Diff: Higher-Order Guided Diffusion for Graph Generation</h2><p><strong>Authors:Yiming Huang, Tolga Birdal</strong></p>
<p>Graph generation is a critical yet challenging task as empirical analyses require a deep understanding of complex, non-Euclidean structures. Although diffusion models have recently made significant achievements in graph generation, these models typically adapt from the frameworks designed for image generation, making them ill-suited for capturing the topological properties of graphs. In this work, we propose a novel Higher-order Guided Diffusion (HOG-Diff) model that follows a coarse-to-fine generation curriculum and is guided by higher-order information, enabling the progressive generation of plausible graphs with inherent topological structures. We further prove that our model exhibits a stronger theoretical guarantee than classical diffusion frameworks. Extensive experiments on both molecular and generic graph generation tasks demonstrate that our method consistently outperforms or remains competitive with state-of-the-art baselines. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Yiminghh/HOG-Diff">https://github.com/Yiminghh/HOG-Diff</a>. </p>
<blockquote>
<p>å›¾ç”Ÿæˆæ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä½†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦å¯¹å¤æ‚çš„éæ¬§å‡ é‡Œå¾—ç»“æ„è¿›è¡Œæ·±å…¥ç†è§£ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘åœ¨å›¾ç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹é€šå¸¸æ˜¯ä»ä¸ºå›¾åƒç”Ÿæˆè®¾è®¡çš„æ¡†æ¶ä¸­æ”¹ç¼–è€Œæ¥çš„ï¼Œå› æ­¤éš¾ä»¥æ•æ‰å›¾çš„æ‹“æ‰‘å±æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜é˜¶å¼•å¯¼æ‰©æ•£ï¼ˆHOG-Diffï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹éµå¾ªä»ç²—åˆ°ç»†çš„ç”Ÿæˆè¯¾ç¨‹ï¼Œå¹¶ç”±é«˜é˜¶ä¿¡æ¯å¼•å¯¼ï¼Œèƒ½å¤Ÿé€æ­¥ç”Ÿæˆå…·æœ‰å›ºæœ‰æ‹“æ‰‘ç»“æ„çš„åˆç†å›¾ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰æ¯”ä¼ ç»Ÿæ‰©æ•£æ¡†æ¶æ›´å¼ºçš„ç†è®ºä¿è¯ã€‚åœ¨åˆ†å­å’Œé€šç”¨å›¾ç”Ÿæˆä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæˆ–å…·æœ‰ç«äº‰åŠ›æœ€å…ˆè¿›åŸºçº¿ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yiminghh/HOG-Diff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Yiminghh/HOG-Diffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŸºäºé«˜é˜¶å¼•å¯¼æ‰©æ•£ï¼ˆHOG-Diffï¼‰æ¨¡å‹çš„å›¾ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé‡‡ç”¨ç”±ç²—åˆ°ç»†çš„ç”Ÿæˆç­–ç•¥ï¼Œå¹¶ç»“åˆé«˜é˜¶ä¿¡æ¯è¿›è¡Œå¼•å¯¼ï¼Œå¯ä»¥é€æ­¥ç”Ÿæˆå…·æœ‰å†…åœ¨æ‹“æ‰‘ç»“æ„çš„å¯ä¿¡ä»»å›¾ã€‚ä¸ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„ç†è®ºä¿è¯ã€‚åœ¨åˆ†å­å’Œé€šç”¨å›¾ç”Ÿæˆä»»åŠ¡çš„å¹¿æ³›å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä¸»æµæ¨¡å‹ã€‚æ¨¡å‹ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾ç”Ÿæˆæ˜¯ä¸€é¡¹å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦æ·±å…¥ç†è§£å¤æ‚çš„éæ¬§å‡ é‡Œå¾—ç»“æ„ã€‚</li>
<li>ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢ç”Ÿæˆä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦å€Ÿé‰´äº†å›¾åƒç”Ÿæˆçš„æ¡†æ¶è®¾è®¡ï¼Œæ— æ³•å……åˆ†æ•æ‰å›¾å½¢çš„æ‹“æ‰‘ç‰¹æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Higher-order Guided Diffusionï¼ˆHOG-Diffï¼‰æ¨¡å‹ï¼Œç”¨äºå›¾å½¢ç”Ÿæˆã€‚è¯¥æ¨¡å‹éµå¾ªç”±ç²—åˆ°ç»†çš„ç”Ÿæˆç­–ç•¥ï¼Œé€šè¿‡é«˜é˜¶ä¿¡æ¯è¿›è¡Œå¼•å¯¼ã€‚</li>
<li>HOG-Diffæ¨¡å‹å…·æœ‰æ›´å¼ºçš„ç†è®ºä¿è¯ï¼Œä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>åœ¨åˆ†å­å’Œé€šç”¨å›¾ç”Ÿæˆä»»åŠ¡çš„å®éªŒä¸­ï¼ŒHOG-Diffæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä¸»æµæ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²ç»å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e1b5133e2416b5ec197f2bd19bf43ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e00c46e36b604248fc6e2b66d9b347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5fe0de6d9d7911029fb5341287ce3d3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PartEdit-Fine-Grained-Image-Editing-using-Pre-Trained-Diffusion-Models"><a href="#PartEdit-Fine-Grained-Image-Editing-using-Pre-Trained-Diffusion-Models" class="headerlink" title="PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models"></a>PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models</h2><p><strong>Authors:Aleksandar Cvejic, Abdelrahman Eldesokey, Peter Wonka</strong></p>
<p>We present the first text-based image editing approach for object parts based on pre-trained diffusion models. Diffusion-based image editing approaches capitalized on the deep understanding of diffusion models of image semantics to perform a variety of edits. However, existing diffusion models lack sufficient understanding of many object parts, hindering fine-grained edits requested by users. To address this, we propose to expand the knowledge of pre-trained diffusion models to allow them to understand various object parts, enabling them to perform fine-grained edits. We achieve this by learning special textual tokens that correspond to different object parts through an efficient token optimization process. These tokens are optimized to produce reliable localization masks at each inference step to localize the editing region. Leveraging these masks, we design feature-blending and adaptive thresholding strategies to execute the edits seamlessly. To evaluate our approach, we establish a benchmark and an evaluation protocol for part editing. Experiments show that our approach outperforms existing editing methods on all metrics and is preferred by users 77-90% of the time in conducted user studies. </p>
<blockquote>
<p>æˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é¢å‘ç‰©ä½“éƒ¨ä»¶çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒç¼–è¾‘æ–¹æ³•åˆ©ç”¨å¯¹å›¾åƒè¯­ä¹‰æ‰©æ•£æ¨¡å‹çš„æ·±å…¥ç†è§£æ¥è¿›è¡Œå„ç§ç¼–è¾‘æ“ä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¯¹è®¸å¤šç‰©ä½“éƒ¨ä»¶çš„ç†è§£ä¸è¶³ï¼Œé˜»ç¢äº†ç”¨æˆ·è¦æ±‚çš„ç²¾ç»†ç¼–è¾‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡ŒçŸ¥è¯†æ‰©å±•ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç†è§£å„ç§ç‰©ä½“éƒ¨ä»¶ï¼Œä»è€Œè¿›è¡Œç²¾ç»†ç¼–è¾‘ã€‚æˆ‘ä»¬é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œä¼˜åŒ–è¿‡ç¨‹ï¼Œå­¦ä¹ å¯¹åº”äºä¸åŒç‰©ä½“éƒ¨ä»¶çš„ç‰¹æ®Šæ–‡æœ¬ä»¤ç‰Œã€‚è¿™äº›ä»¤ç‰Œç»è¿‡ä¼˜åŒ–ï¼Œåœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­äº§ç”Ÿå¯é çš„å®šä½æ©ç ï¼Œä»¥å®šä½ç¼–è¾‘åŒºåŸŸã€‚åˆ©ç”¨è¿™äº›æ©ç ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç‰¹å¾æ··åˆå’Œè‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥ï¼Œä»¥æ— ç¼æ‰§è¡Œç¼–è¾‘æ“ä½œã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å»ºç«‹äº†éƒ¨ä»¶ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä»·åè®®ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„ç¼–è¾‘æ•ˆæœä¼˜äºç°æœ‰ç¼–è¾‘æ–¹æ³•ï¼Œåœ¨ç”¨æˆ·ç ”ç©¶ä¸­ï¼Œç”¨æˆ·åå¥½æˆ‘ä»¬çš„æ–¹æ³•çš„æ—¶é—´å æ¯”è¾¾åˆ°77-90%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04050v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://partedit.github.io/PartEdit/">https://partedit.github.io/PartEdit/</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é¦–æ¬¡æå‡ºäº†åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç”¨äºå¤„ç†å¯¹è±¡éƒ¨åˆ†çš„ç¼–è¾‘ã€‚é€šè¿‡ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„çŸ¥è¯†ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å„ç§å¯¹è±¡éƒ¨åˆ†ï¼Œå®ç°ç²¾ç»†ç¼–è¾‘ã€‚é€šè¿‡é«˜æ•ˆçš„æ–‡æœ¬ä»¤ç‰Œä¼˜åŒ–è¿‡ç¨‹ï¼Œå­¦ä¹ å¯¹åº”äºä¸åŒå¯¹è±¡éƒ¨åˆ†çš„ç‰¹æ®Šæ–‡æœ¬ä»¤ç‰Œï¼Œç”¨äºç”Ÿæˆå¯é çš„å®šä½æ©è†œï¼Œå®šä½ç¼–è¾‘åŒºåŸŸã€‚ç»“åˆè¿™äº›æ©è†œï¼Œè®¾è®¡ç‰¹å¾èåˆå’Œè‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥ï¼Œå®ç°æ— ç¼ç¼–è¾‘ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰ç¼–è¾‘æ–¹æ³•ï¼Œå¹¶åœ¨ç”¨æˆ·ç ”ç©¶ä¸­è·å¾—ç”¨æˆ·77%-90%çš„é’çã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¦–æ¬¡æå‡ºåŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬é©±åŠ¨å›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œç”¨äºå¤„ç†å¯¹è±¡éƒ¨åˆ†çš„ç¼–è¾‘ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡ä¼˜åŒ–çŸ¥è¯†ï¼Œæé«˜å¯¹å¯¹è±¡éƒ¨åˆ†çš„ç†è§£ï¼Œå®ç°ç²¾ç»†ç¼–è¾‘ã€‚</li>
<li>é€šè¿‡é«˜æ•ˆçš„æ–‡æœ¬ä»¤ç‰Œä¼˜åŒ–è¿‡ç¨‹ï¼Œå­¦ä¹ å¯¹åº”äºä¸åŒå¯¹è±¡éƒ¨åˆ†çš„ç‰¹æ®Šæ–‡æœ¬ä»¤ç‰Œã€‚</li>
<li>ç‰¹æ®Šæ–‡æœ¬ä»¤ç‰Œç”Ÿæˆå¯é çš„å®šä½æ©è†œï¼Œç”¨äºå®šä½ç¼–è¾‘åŒºåŸŸã€‚</li>
<li>ç»“åˆå®šä½æ©è†œï¼Œé‡‡ç”¨ç‰¹å¾èåˆå’Œè‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥å®ç°æ— ç¼ç¼–è¾‘ã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰ç¼–è¾‘æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de2ceec31464e9ea0b706b4ae2672e9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb1a4c969fcad2d6da472fcf5b31d00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7291d443022956c0e84d2ecc102762a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed5a0ed4f30894066d73034e474a2305.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa96e87201ec8d7b9d05bfbc009a3997.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeblurDiff-Real-World-Image-Deblurring-with-Generative-Diffusion-Models"><a href="#DeblurDiff-Real-World-Image-Deblurring-with-Generative-Diffusion-Models" class="headerlink" title="DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models"></a>DeblurDiff: Real-World Image Deblurring with Generative Diffusion Models</h2><p><strong>Authors:Lingshun Kong, Jiawei Zhang, Dongqing Zou, Jimmy Ren, Xiaohe Wu, Jiangxin Dong, Jinshan Pan</strong></p>
<p>Diffusion models have achieved significant progress in image generation. The pre-trained Stable Diffusion (SD) models are helpful for image deblurring by providing clear image priors. However, directly using a blurry image or pre-deblurred one as a conditional control for SD will either hinder accurate structure extraction or make the results overly dependent on the deblurring network. In this work, we propose a Latent Kernel Prediction Network (LKPN) to achieve robust real-world image deblurring. Specifically, we co-train the LKPN in latent space with conditional diffusion. The LKPN learns a spatially variant kernel to guide the restoration of sharp images in the latent space. By applying element-wise adaptive convolution (EAC), the learned kernel is utilized to adaptively process the input feature, effectively preserving the structural information of the input. This process thereby more effectively guides the generative process of Stable Diffusion (SD), enhancing both the deblurring efficacy and the quality of detail reconstruction. Moreover, the results at each diffusion step are utilized to iteratively estimate the kernels in LKPN to better restore the sharp latent by EAC. This iterative refinement enhances the accuracy and robustness of the deblurring process. Extensive experimental results demonstrate that the proposed method outperforms state-of-the-art image deblurring methods on both benchmark and real-world images. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é¢„è®­ç»ƒçš„Stable Diffusionï¼ˆSDï¼‰æ¨¡å‹é€šè¿‡æä¾›æ¸…æ™°çš„å›¾åƒå…ˆéªŒï¼Œå¯¹å›¾åƒå»æ¨¡ç³ŠåŒ–å¾ˆæœ‰å¸®åŠ©ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨æ¨¡ç³Šå›¾åƒæˆ–é¢„å…ˆå»æ¨¡ç³Šçš„å›¾åƒä½œä¸ºSDçš„æ¡ä»¶æ§åˆ¶ä¼šé˜»ç¢å‡†ç¡®çš„ç»“æ„æå–ï¼Œæˆ–ä½¿ç»“æœè¿‡äºä¾èµ–å»æ¨¡ç³Šç½‘ç»œã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ½œæ ¸é¢„æµ‹ç½‘ç»œï¼ˆLatent Kernel Prediction Networkï¼ŒLKPNï¼‰æ¥å®ç°ç¨³å¥çš„ç°å®ä¸–ç•Œå›¾åƒå»æ¨¡ç³ŠåŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹LKPNä¸æ¡ä»¶æ‰©æ•£è¿›è¡Œå…±åŒè®­ç»ƒã€‚LKPNå­¦ä¹ ä¸€ä¸ªç©ºé—´å˜åŒ–çš„æ ¸æ¥æŒ‡å¯¼æ½œåœ¨ç©ºé—´ä¸­æ¸…æ™°å›¾åƒçš„æ¢å¤ã€‚é€šè¿‡åº”ç”¨é€å…ƒç´ è‡ªé€‚åº”å·ç§¯ï¼ˆElement-wise Adaptive Convolutionï¼ŒEACï¼‰ï¼Œå­¦ä¹ åˆ°çš„æ ¸è¢«ç”¨æ¥è‡ªé€‚åº”åœ°å¤„ç†è¾“å…¥ç‰¹å¾ï¼Œæœ‰æ•ˆåœ°ä¿ç•™è¾“å…¥çš„ç»“æ„ä¿¡æ¯ã€‚è¿™ä¸€è¿‡ç¨‹æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼äº†Stable Diffusionï¼ˆSDï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜äº†å»æ¨¡ç³ŠåŒ–çš„æ•ˆæœä»¥åŠç»†èŠ‚é‡å»ºçš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæ¯ä¸€æ­¥æ‰©æ•£çš„ç»“æœéƒ½è¢«ç”¨æ¥é€šè¿‡EACè¿­ä»£ä¼°è®¡LKPNä¸­çš„æ ¸ï¼Œä»¥æ›´å¥½åœ°æ¢å¤æ¸…æ™°çš„æ½œåœ¨ç‰¹å¾ã€‚è¿™ç§è¿­ä»£ç»†åŒ–æé«˜äº†å»æ¨¡ç³Šè¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•å’ŒçœŸå®ä¸–ç•Œå›¾åƒä¸Šå»æ¨¡ç³ŠåŒ–çš„æ•ˆæœä¼˜äºå½“å‰å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03810v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹äºå»æ¨¡ç³Šä»»åŠ¡ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ½œæ ¸é¢„æµ‹ç½‘ç»œï¼ˆLKPNï¼‰ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸æ¡ä»¶æ‰©æ•£å…±åŒè®­ç»ƒï¼Œå­¦ä¹ ç©ºé—´å¯å˜æ ¸ä»¥æŒ‡å¯¼æ½œåœ¨ç©ºé—´çš„æ¸…æ™°å›¾åƒæ¢å¤ã€‚é€šè¿‡åº”ç”¨å…ƒç´ çº§è‡ªé€‚åº”å·ç§¯ï¼ˆEACï¼‰ï¼Œå­¦ä¹ åˆ°çš„æ ¸èƒ½å¤Ÿè‡ªé€‚åº”å¤„ç†è¾“å…¥ç‰¹å¾ï¼Œæœ‰æ•ˆä¿ç•™è¾“å…¥çš„ç»“æ„ä¿¡æ¯ã€‚è¿™ä¸€è¿‡ç¨‹æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼äº†ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜äº†å»æ¨¡ç³Šæ•ˆæœå’Œç»†èŠ‚é‡å»ºè´¨é‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨å„æ‰©æ•£æ­¥éª¤çš„ç»“æœä¸Šè¿­ä»£ä¼°è®¡LKPNä¸­çš„æ ¸ï¼Œä»¥æ›´å¥½åœ°é€šè¿‡EACæ¢å¤æ¸…æ™°çš„æ½œåœ¨å›¾åƒã€‚è¿™ç§è¿­ä»£ä¼˜åŒ–æé«˜äº†å»æ¨¡ç³Šè¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•å’ŒçœŸå®å›¾åƒä¸Šçš„å›¾åƒå»æ¨¡ç³Šæ•ˆæœå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨³å®šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å»æ¨¡ç³Šä»»åŠ¡ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>æ½œæ ¸é¢„æµ‹ç½‘ç»œï¼ˆLKPNï¼‰è¢«æå‡ºï¼Œç”¨äºåœ¨æ½œåœ¨ç©ºé—´ä¸æ¡ä»¶æ‰©æ•£å…±åŒè®­ç»ƒã€‚</li>
<li>LKPNå­¦ä¹ ç©ºé—´å¯å˜æ ¸ï¼Œä»¥æŒ‡å¯¼æ½œåœ¨ç©ºé—´çš„æ¸…æ™°å›¾åƒæ¢å¤ã€‚</li>
<li>å…ƒç´ çº§è‡ªé€‚åº”å·ç§¯ï¼ˆEACï¼‰ç”¨äºè‡ªé€‚åº”å¤„ç†è¾“å…¥ç‰¹å¾ï¼Œä¿ç•™ç»“æ„ä¿¡æ¯ã€‚</li>
<li>EACæœ‰æ•ˆæŒ‡å¯¼ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜å»æ¨¡ç³Šæ•ˆæœå’Œç»†èŠ‚é‡å»ºè´¨é‡ã€‚</li>
<li>é€šè¿‡è¿­ä»£ä¼°è®¡æ ¸æ¥ä¼˜åŒ–å»æ¨¡ç³Šè¿‡ç¨‹ï¼Œæé«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-71756b51a965ffe850a50052725dbf4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72eff6aeda50f5238c64a78cf06b1286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a1f5e4ff0a8f27baeb597f73c500868.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2d6deef79f6ad881bd453cfe4763021.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1406c23a222732ee7def159d8747106d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free"><a href="#Conditional-Diffusion-Models-are-Medical-Image-Classifiers-that-Provide-Explainability-and-Uncertainty-for-Free" class="headerlink" title="Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free"></a>Conditional Diffusion Models are Medical Image Classifiers that Provide   Explainability and Uncertainty for Free</h2><p><strong>Authors:Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel</strong></p>
<p>Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: <a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier.github.io/">https://faverogian.github.io/med-diffusion-classifier.github.io/</a> </p>
<blockquote>
<p>åˆ¤åˆ«åˆ†ç±»å™¨å·²æˆä¸ºåŒ»å­¦å½±åƒæ·±åº¦å­¦ä¹ ä¸­çš„åŸºç¡€å·¥å…·ï¼Œæ“…é•¿å­¦ä¹ å¤æ‚æ•°æ®åˆ†å¸ƒçš„å¯åˆ†ç‰¹å¾ã€‚ç„¶è€Œï¼Œä¸ºäº†ç¡®ä¿è¿™äº›æ¨¡å‹çš„å®‰å…¨å¯é éƒ¨ç½²ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œç²¾å¿ƒè®¾è®¡ã€æ•°æ®å¢å¼ºå’Œè®­ç»ƒæŠ€æœ¯ã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹å·²æˆä¸ºäºŒç»´ç”Ÿæˆæ¨¡å‹çš„ä»£åè¯ã€‚è¿™äº›æ¨¡å‹å±•ç¤ºäº†åœ¨å„ç§ä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ï¼ŒåŒ…æ‹¬è‡ªç„¶å›¾åƒåˆ†ç±»ï¼Œåˆ†ç±»æ˜¯é€šè¿‡æ¯”è¾ƒé’ˆå¯¹æ¯ä¸ªå¯èƒ½çš„æ¡ä»¶è¾“å…¥ç”Ÿæˆçš„å›¾åƒä¹‹é—´çš„é‡å»ºè¯¯å·®æ¥å®Œæˆçš„ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡æ¢ç´¢äº†äºŒç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ç±»æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå·²è¯æ˜å¯ä»¥æé«˜åŒ»å­¦æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´å¼€å§‹è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å‡è¾¾åˆ°äº†ä¸æœ€æ–°åˆ¤åˆ«åˆ†ç±»å™¨ç›¸å½“çš„æ€§èƒ½æ°´å¹³ï¼Œæ— éœ€æ˜¾å¼ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æ‰©æ•£åˆ†ç±»å™¨æœ¬è´¨ä¸Šæ˜¯å¯è§£é‡Šçš„ï¼Œå¹¶å¯ç”¨äºé‡åŒ–å…¶é¢„æµ‹çš„ç¡®å®šæ€§ï¼Œä»è€Œå¢åŠ äº†å…¶åœ¨å®‰å…¨å…³é”®çš„åŒ»ç–—ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://faverogian.github.io/med-diffusion-classifier">https://faverogian.github.io/med-diffusion-classifier</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03687v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡å¼€å‘æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œæ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½å¾—åˆ°äº†æå‡ï¼Œä¸”åœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åˆ¤åˆ«åˆ†ç±»å™¨ç›¸æ¯”ï¼ŒåŸºç¡€æ‰©æ•£æ¨¡å‹å’Œä»å¤´å¼€å§‹è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”æ— éœ€æ˜¾å¼ç›‘ç£ã€‚æ­¤å¤–ï¼Œæ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ï¼Œå¯é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œå¢åŠ å…¶åœ¨å®‰å…¨å…³é”®çš„ä¸´åºŠç¯å¢ƒä¸­çš„å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ•°æŠ•ç¥¨æ–¹æ¡ˆï¼Œæé«˜äº†åŒ»ç–—æ‰©æ•£åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>åœ¨CheXpertå’ŒISICé»‘è‰²ç´ ç˜¤çš®è‚¤ç™Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹ä¸æœ€å…ˆè¿›çš„åˆ¤åˆ«åˆ†ç±»å™¨å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ— éœ€æ˜¾å¼ç›‘ç£ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å…·æœ‰å†…åœ¨çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨å¯é‡åŒ–é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>æ‰©æ•£åˆ†ç±»å™¨åœ¨ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰å¯ä¿¡åº¦å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f25bdeacfbc5a65cf6ba4cbabfb56a85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09b52a6ca7cb81e8cde2b8cf39ed9051.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b9cb79da2ff3d4912347f214f296d5e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise"><a href="#FreqPrior-Improving-Video-Diffusion-Models-with-Frequency-Filtering-Gaussian-Noise" class="headerlink" title="FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise"></a>FreqPrior: Improving Video Diffusion Models with Frequency Filtering   Gaussian Noise</h2><p><strong>Authors:Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang</strong></p>
<p>Text-driven video generation has advanced significantly due to developments in diffusion models. Beyond the training and sampling phases, recent studies have investigated noise priors of diffusion models, as improved noise priors yield better generation results. One recent approach employs the Fourier transform to manipulate noise, marking the initial exploration of frequency operations in this context. However, it often generates videos that lack motion dynamics and imaging details. In this work, we provide a comprehensive theoretical analysis of the variance decay issue present in existing methods, contributing to the loss of details and motion dynamics. Recognizing the critical impact of noise distribution on generation quality, we introduce FreqPrior, a novel noise initialization strategy that refines noise in the frequency domain. Our method features a novel filtering technique designed to address different frequency signals while maintaining the noise prior distribution that closely approximates a standard Gaussian distribution. Additionally, we propose a partial sampling process by perturbing the latent at an intermediate timestep during finding the noise prior, significantly reducing inference time without compromising quality. Extensive experiments on VBench demonstrate that our method achieves the highest scores in both quality and semantic assessments, resulting in the best overall total score. These results highlight the superiority of our proposed noise prior. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬çš„è§†é¢‘ç”Ÿæˆç”±äºæ‰©æ•£æ¨¡å‹çš„å‘å±•è€Œå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é™¤äº†è®­ç»ƒå’Œé‡‡æ ·é˜¶æ®µï¼Œæœ€è¿‘çš„ç ”ç©¶è¿˜æ¢è®¨äº†æ‰©æ•£æ¨¡å‹çš„å™ªå£°å…ˆéªŒï¼Œå› ä¸ºæ”¹è¿›çš„å™ªå£°å…ˆéªŒä¼šäº§ç”Ÿæ›´å¥½çš„ç”Ÿæˆç»“æœã€‚ä¸€ç§æœ€è¿‘çš„æ–¹æ³•ä½¿ç”¨å‚…é‡Œå¶å˜æ¢æ¥æ“ä½œå™ªå£°ï¼Œæ ‡å¿—ç€åœ¨æ­¤èƒŒæ™¯ä¸‹å¯¹é¢‘ç‡æ“ä½œçš„åˆæ­¥æ¢ç´¢ã€‚ç„¶è€Œï¼Œå®ƒé€šå¸¸ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹è¿åŠ¨åŠ¨åŠ›å’Œæˆåƒç»†èŠ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„æ–¹å·®è¡°å‡é—®é¢˜è¿›è¡Œäº†å…¨é¢çš„ç†è®ºåˆ†æï¼Œè¿™ä¸ªé—®é¢˜å¯¼è‡´äº†ç»†èŠ‚å’Œè¿åŠ¨åŠ¨åŠ›çš„æŸå¤±ã€‚æˆ‘ä»¬è®¤è¯†åˆ°å™ªå£°åˆ†å¸ƒå¯¹ç”Ÿæˆè´¨é‡çš„å…³é”®å½±å“ï¼Œå› æ­¤å¼•å…¥äº†FreqPriorï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å™ªå£°åˆå§‹åŒ–ç­–ç•¥ï¼Œå®ƒåœ¨é¢‘ç‡åŸŸä¸­ä¼˜åŒ–å™ªå£°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ»¤æ³¢æŠ€æœ¯ï¼Œæ—¨åœ¨å¤„ç†ä¸åŒçš„é¢‘ç‡ä¿¡å·ï¼ŒåŒæ—¶ä¿æŒå™ªå£°å…ˆéªŒåˆ†å¸ƒï¼Œè¿‘ä¼¼äºæ ‡å‡†é«˜æ–¯åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ä¸­é—´æ—¶é—´æ­¥é•¿æ‰¾åˆ°å™ªå£°å…ˆéªŒæ—¶æ‰°åŠ¨æ½œåœ¨ç©ºé—´ï¼Œæå‡ºäº†éƒ¨åˆ†é‡‡æ ·è¿‡ç¨‹ï¼Œè¿™æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¸å¦¥åè´¨é‡ã€‚åœ¨VBenchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œè¯­ä¹‰è¯„ä¼°æ–¹é¢éƒ½è·å¾—äº†æœ€é«˜åˆ†ï¼Œæ€»åˆ†æœ€é«˜ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æˆ‘ä»¬æå‡ºçš„å™ªå£°å…ˆéªŒçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03496v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ€æ–°ç ”ç©¶å¼€å§‹æ¢ç´¢æ‰©æ•£æ¨¡å‹çš„å™ªå£°å…ˆéªŒï¼Œå› ä¸ºæ›´å¥½çš„å™ªå£°å…ˆéªŒèƒ½äº§ç”Ÿæ›´å¥½çš„ç”Ÿæˆç»“æœã€‚æœ¬æ–‡å…¨é¢åˆ†æäº†ç°æœ‰æ–¹æ³•ä¸­å­˜åœ¨çš„æ–¹å·®è¡°å‡é—®é¢˜ï¼Œå¯¼è‡´ç»†èŠ‚å’Œè¿åŠ¨åŠ¨åŠ›ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³å™ªå£°åˆ†å¸ƒå¯¹ç”Ÿæˆè´¨é‡çš„å…³é”®å½±å“ï¼Œæˆ‘ä»¬å¼•å…¥äº†FreqPriorï¼Œä¸€ç§åœ¨é¢‘åŸŸä¼˜åŒ–å™ªå£°çš„æ–°å‹å™ªå£°åˆå§‹åŒ–ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨å¯»æ‰¾å™ªå£°å…ˆéªŒçš„ä¸­é—´æ­¥éª¤æ‰°åŠ¨æ½œåœ¨å˜é‡ï¼Œæå‡ºäº†éƒ¨åˆ†é‡‡æ ·è¿‡ç¨‹ï¼Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¸å¦¥åäºè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œè¯­ä¹‰è¯„ä¼°æ–¹é¢éƒ½è·å¾—äº†æœ€é«˜åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆä¸­æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å™ªå£°å…ˆéªŒåœ¨æ‰©æ•£æ¨¡å‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œèƒ½å½±å“ç”Ÿæˆç»“æœçš„è´¨é‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ–¹å·®è¡°å‡é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„è§†é¢‘ç¼ºä¹è¿åŠ¨åŠ¨åŠ›å’Œæˆåƒç»†èŠ‚ã€‚</li>
<li>å¼•å…¥FreqPriorï¼Œä¸€ç§æ–°å‹å™ªå£°åˆå§‹åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨ä¼˜åŒ–é¢‘åŸŸä¸­çš„å™ªå£°ã€‚</li>
<li>æå‡ºä¸€ç§éƒ¨åˆ†é‡‡æ ·è¿‡ç¨‹ï¼Œé€šè¿‡æ‰°åŠ¨å¯»æ‰¾å™ªå£°å…ˆéªŒçš„ä¸­é—´æ­¥éª¤çš„æ½œåœ¨å˜é‡ï¼Œä»¥å‡å°‘æ¨ç†æ—¶é—´è€Œä¸å½±å“è´¨é‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨VBenchä¸Šï¼Œè¯¥æ–¹æ³•åœ¨è´¨é‡å’Œè¯­ä¹‰è¯„ä¼°æ–¹é¢è·å¾—æœ€é«˜åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dffa4c70e1fc3553298911b80e80cb2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c37c94ac0a1a4dc06f70af4af0b66791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c4a1fab50665f60a48c1458a9b8d618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35ba758f62d68e0b15ee423d524ca085.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MetaFE-DE-Learning-Meta-Feature-Embedding-for-Depth-Estimation-from-Monocular-Endoscopic-Images"><a href="#MetaFE-DE-Learning-Meta-Feature-Embedding-for-Depth-Estimation-from-Monocular-Endoscopic-Images" class="headerlink" title="MetaFE-DE: Learning Meta Feature Embedding for Depth Estimation from   Monocular Endoscopic Images"></a>MetaFE-DE: Learning Meta Feature Embedding for Depth Estimation from   Monocular Endoscopic Images</h2><p><strong>Authors:Dawei Lu, Deqiang Xiao, Danni Ai, Jingfan Fan, Tianyu Fu, Yucong Lin, Hong Song, Xujiong Ye, Lei Zhang, Jian Yang</strong></p>
<p>Depth estimation from monocular endoscopic images presents significant challenges due to the complexity of endoscopic surgery, such as irregular shapes of human soft tissues, as well as variations in lighting conditions. Existing methods primarily estimate the depth information from RGB images directly, and often surffer the limited interpretability and accuracy. Given that RGB and depth images are two views of the same endoscopic surgery scene, in this paper, we introduce a novel concept referred as &#96;&#96;meta feature embedding (MetaFE)â€, in which the physical entities (e.g., tissues and surgical instruments) of endoscopic surgery are represented using the shared features that can be alternatively decoded into RGB or depth image. With this concept, we propose a two-stage self-supervised learning paradigm for the monocular endoscopic depth estimation. In the first stage, we propose a temporal representation learner using diffusion models, which are aligned with the spatial information through the cross normalization to construct the MetaFE. In the second stage, self-supervised monocular depth estimation with the brightness calibration is applied to decode the meta features into the depth image. Extensive evaluation on diverse endoscopic datasets demonstrates that our approach outperforms the state-of-the-art method in depth estimation, achieving superior accuracy and generalization. The source code will be publicly available. </p>
<blockquote>
<p>ä»å•ç›®å†…é•œå›¾åƒè¿›è¡Œæ·±åº¦ä¼°è®¡é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå†…é•œæ‰‹æœ¯çš„å¤æ‚æ€§ï¼Œä¾‹å¦‚äººä½“è½¯ç»„ç»‡çš„ä¸è§„åˆ™å½¢çŠ¶ï¼Œä»¥åŠå…‰ç…§æ¡ä»¶çš„å˜åŒ–ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ç›´æ¥ä»RGBå›¾åƒä¼°è®¡æ·±åº¦ä¿¡æ¯ï¼Œé€šå¸¸å­˜åœ¨è§£é‡Šæ€§å’Œå‡†ç¡®æ€§çš„å±€é™æ€§ã€‚é‰´äºRGBå›¾åƒå’Œæ·±åº¦å›¾åƒæ˜¯åŒä¸€å†…é•œæ‰‹æœ¯åœºæ™¯çš„ä¸¤ä¸ªè§†å›¾ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªæ–°æ¦‚å¿µï¼Œç§°ä¸ºâ€œå…ƒç‰¹å¾åµŒå…¥ï¼ˆMetaFEï¼‰â€ï¼Œå…¶ä¸­å†…é•œæ‰‹æœ¯ä¸­çš„ç‰©ç†å®ä½“ï¼ˆä¾‹å¦‚ï¼Œç»„ç»‡å’Œæ‰‹æœ¯å™¨æ¢°ï¼‰ç”¨å¯äº¤æ›¿è§£ç ä¸ºRGBæˆ–æ·±åº¦å›¾åƒçš„å…±äº«ç‰¹å¾æ¥è¡¨ç¤ºã€‚åŸºäºè¿™ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºå•ç›®å†…é•œæ·±åº¦ä¼°è®¡çš„ä¸¤é˜¶æ®µè‡ªç›‘ç£å­¦ä¹ èŒƒå¼ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ—¶åºè¡¨ç¤ºå­¦ä¹ è€…ï¼Œé€šè¿‡è·¨å½’ä¸€åŒ–ä¸ç©ºé—´ä¿¡æ¯è¿›è¡Œå¯¹é½ï¼Œä»¥æ„å»ºMetaFEã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåº”ç”¨å¸¦æœ‰äº®åº¦æ ¡å‡†çš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡æ¥å°†å…ƒç‰¹å¾è§£ç ä¸ºæ·±åº¦å›¾åƒã€‚åœ¨å¤šç§å†…é•œæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ·±åº¦ä¼°è®¡æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æºä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03493v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå•ç›®å†…çª¥é•œå›¾åƒæ·±åº¦ä¼°è®¡çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥â€œå…ƒç‰¹å¾åµŒå…¥â€ï¼ˆMetaFEï¼‰æ¦‚å¿µï¼Œå°†ç‰©ç†å®ä½“ï¼ˆå¦‚ç»„ç»‡å’Œæ‰‹æœ¯å™¨æ¢°ï¼‰ç”¨å…±äº«ç‰¹å¾è¡¨ç¤ºï¼Œè¿™äº›ç‰¹å¾å¯ä»¥è§£ç ä¸ºRGBæˆ–æ·±åº¦å›¾åƒã€‚æå‡ºä¸€ç§ä¸¤é˜¶æ®µè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ—¶é—´è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡ä¸ç©ºé—´ä¿¡æ¯å¯¹é½æ„å»ºMetaFEï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨è‡ªç›‘ç£å•ç›®å†…çª¥é•œæ·±åº¦ä¼°è®¡ï¼Œå¯¹äº®åº¦è¿›è¡Œæ ¡å‡†åè§£ç å‡ºæ·±åº¦å›¾åƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šç§å†…çª¥é•œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå‡†ç¡®åº¦å’Œæ³›åŒ–èƒ½åŠ›æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥â€œå…ƒç‰¹å¾åµŒå…¥â€ï¼ˆMetaFEï¼‰æ¦‚å¿µï¼Œå°†ç‰©ç†å®ä½“è¡¨ç¤ºä¸ºå…±äº«ç‰¹å¾ï¼Œå¯è§£ç ä¸ºRGBæˆ–æ·±åº¦å›¾åƒã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œå•ç›®å†…çª¥é•œæ·±åº¦ä¼°è®¡ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ—¶é—´è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡ä¸ç©ºé—´ä¿¡æ¯å¯¹é½æ„å»ºMetaFEã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåº”ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œç»“åˆäº®åº¦æ ¡å‡†è§£ç å‡ºæ·±åº¦å›¾åƒã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§å†…çª¥é•œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æé«˜äº†æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5fc69bad5b60d68a3edfccfa02e2b494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-960248560f8ef4331ca62872a87ca038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d3e6c7d6bcaf5e2bbc14d3a74a34db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0d6a3546f49243197b86b96ccad23ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32e36d13fcab0f37c61ba359427aa588.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Lanpaint-Training-Free-Diffusion-Inpainting-with-Exact-and-Fast-Conditional-Inference"><a href="#Lanpaint-Training-Free-Diffusion-Inpainting-with-Exact-and-Fast-Conditional-Inference" class="headerlink" title="Lanpaint: Training-Free Diffusion Inpainting with Exact and Fast   Conditional Inference"></a>Lanpaint: Training-Free Diffusion Inpainting with Exact and Fast   Conditional Inference</h2><p><strong>Authors:Candi Zheng, Yuan Lan, Yang Wang</strong></p>
<p>Diffusion models generate high-quality images but often lack efficient and universally applicable inpainting capabilities, particularly in community-trained models. We introduce LanPaint, a training-free method tailored for widely adopted ODE-based samplers, which leverages Langevin dynamics to perform exact conditional inference, enabling precise and visually coherent inpainting. LanPaint addresses two key challenges in Langevin-based inpainting: (1) the risk of local likelihood maxima trapping and (2) slow convergence. By proposing a guided score function and a fast-converging Langevin framework, LanPaint achieves high-fidelity results in very few iterations. Experiments demonstrate that LanPaint outperforms existing training-free inpainting techniques, outperforming in challenging tasks such as outpainting with Stable Diffusion. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†é€šå¸¸ç¼ºä¹é«˜æ•ˆä¸”æ™®éé€‚ç”¨çš„è¡¥å…¨èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾åŒºè®­ç»ƒæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬æ¨å‡ºäº†LanPaintï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€‚ç”¨äºå¹¿æ³›é‡‡ç”¨çš„åŸºäºODEçš„é‡‡æ ·å™¨ï¼Œå®ƒåˆ©ç”¨æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç¡®çš„æ¡ä»¶æ¨æ–­ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®ä¸”è§†è§‰è¿è´¯çš„è¡¥å…¨ã€‚LanPaintè§£å†³äº†æœ—ä¹‹ä¸‡è¡¥å…¨ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å±€éƒ¨ä¼¼ç„¶æå¤§å€¼é™·é˜±çš„é£é™©å’Œï¼ˆ2ï¼‰æ”¶æ•›é€Ÿåº¦æ…¢ã€‚é€šè¿‡æå‡ºå¼•å¯¼å¾—åˆ†å‡½æ•°å’Œå¿«é€Ÿæ”¶æ•›çš„æœ—ä¹‹ä¸‡æ¡†æ¶ï¼ŒLanPaintå¯ä»¥åœ¨å¾ˆå°‘è¿­ä»£æ¬¡æ•°å†…å®ç°é«˜ä¿çœŸç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒLanPaintåœ¨è¡¥å…¨æŠ€æœ¯æ–¹é¢æ— éœ€è®­ç»ƒï¼Œå¹¶ä¸”åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ˆå¦‚ä½¿ç”¨Stable Diffusionè¿›è¡Œè¡¥å…¨ï¼‰ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03491v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†LanPaintï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€‚ç”¨äºå¹¿æ³›é‡‡ç”¨çš„ODEé‡‡æ ·å™¨ï¼Œåˆ©ç”¨æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç¡®çš„æ¡ä»¶æ¨æ–­ï¼Œå®ç°ç²¾ç¡®ä¸”è§†è§‰è¿è´¯çš„è¡¥å…¨ã€‚è¯¥æ–¹æ³•è§£å†³äº†æœ—ä¹‹ä¸‡è¡¥å…¨ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå±€éƒ¨æ¦‚ç‡æœ€å¤§å€¼çš„é™·é˜±é£é™©å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚é€šè¿‡æå‡ºå¼•å¯¼è¯„åˆ†å‡½æ•°å’Œå¿«é€Ÿæ”¶æ•›çš„æœ—ä¹‹ä¸‡æ¡†æ¶ï¼ŒLanPaintåœ¨å¾ˆå°‘è¿­ä»£æ¬¡æ•°å†…å®ç°äº†é«˜ä¿çœŸç»“æœï¼Œå¹¶åœ¨å¤–æ¨ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨ç¤¾åŒºè®­ç»ƒæ¨¡å‹ä¸­å¸¸å¸¸ç¼ºä¹é«˜æ•ˆä¸”æ™®éé€‚ç”¨çš„è¡¥å…¨èƒ½åŠ›ã€‚</li>
<li>LanPaintæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€‚ç”¨äºODEé‡‡æ ·å™¨ï¼Œåˆ©ç”¨æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦è¿›è¡Œç²¾ç¡®çš„æ¡ä»¶æ¨æ–­ã€‚</li>
<li>LanPaintè§£å†³äº†æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦è¡¥å…¨ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå±€éƒ¨æ¦‚ç‡æœ€å¤§å€¼çš„é™·é˜±é£é™©å’Œæ”¶æ•›é€Ÿåº¦æ…¢ã€‚</li>
<li>é€šè¿‡å¼•å¯¼è¯„åˆ†å‡½æ•°å’Œå¿«é€Ÿæ”¶æ•›çš„æœ—ä¹‹ä¸‡æ¡†æ¶ï¼ŒLanPaintåœ¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°å†…å®ç°äº†é«˜ä¿çœŸç»“æœã€‚</li>
<li>LanPaintåœ¨è¡¥å…¨ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤–æ¨ä»»åŠ¡ä¸­ã€‚</li>
<li>ä¸ç°æœ‰çš„æ— éœ€è®­ç»ƒçš„è¡¥å…¨æŠ€æœ¯ç›¸æ¯”ï¼ŒLanPaintå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-569eaf1f3da9979808ed7cde1ff43aba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59244e39a70a97aefbdbeec244f098e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5de1e608f6627de9e69d5076b6ed84ff.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation"><a href="#Fast-Direct-Query-Efficient-Online-Black-box-Guidance-for-Diffusion-model-Target-Generation" class="headerlink" title="Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation"></a>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</h2><p><strong>Authors:Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong></p>
<p>Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an $\textbf{online}$ algorithm capable of collecting data during runtime and supporting a $\textbf{black-box}$ objective function. Moreover, the $\textbf{query efficiency}$ of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, $\textbf{Fast Direct}$, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a> </p>
<blockquote>
<p>å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ˜¯å®šåˆ¶é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ä»¥åº”å¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ä¸ªå‰æ™¯æ–¹å‘ã€‚ç°æœ‰çš„å¼•å¯¼æ‰©æ•£æ¨¡å‹è¦ä¹ˆä¾èµ–äºä½¿ç”¨é¢„å…ˆæ”¶é›†çš„æ•°æ®é›†è®­ç»ƒå¼•å¯¼æ¨¡å‹ï¼Œè¦ä¹ˆéœ€è¦ç›®æ ‡å‡½æ•°å¯å¾®ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å¤šæ•°ç°å®ä¸–ç•Œä»»åŠ¡è€Œè¨€ï¼Œç¦»çº¿æ•°æ®é›†é€šå¸¸ä¸å¯ç”¨ï¼Œè€Œä¸”å…¶ç›®æ ‡å‡½æ•°é€šå¸¸ä¸å¯å¾®åˆ†ï¼Œä¾‹å¦‚å…·æœ‰äººç±»åå¥½çš„å›¾åƒç”Ÿæˆã€ç”¨äºè¯ç‰©å‘ç°çš„åˆ†å­ç”Ÿæˆå’Œææ–™è®¾è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç®±ç›®æ ‡å‡½æ•°çš„<strong>åœ¨çº¿</strong>ç®—æ³•ã€‚æ­¤å¤–ï¼Œç®—æ³•çš„<strong>æŸ¥è¯¢æ•ˆç‡</strong>ä¹Ÿéå¸¸å…³é”®ï¼Œå› ä¸ºåœ¨ç°å®åœºæ™¯ä¸­ï¼Œç›®æ ‡æŸ¥è¯¢çš„è¯„ä¼°å¾€å¾€éå¸¸æ˜‚è´µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œç®€å•çš„ç®—æ³•â€”â€”<strong>Fast Direct</strong>ï¼Œç”¨äºé«˜æ•ˆæŸ¥è¯¢åœ¨çº¿é»‘ç®±ç›®æ ‡ç”Ÿæˆã€‚æˆ‘ä»¬çš„Fast Directåœ¨æ•°æ®æµå½¢ä¸Šæ„å»ºä¼ªç›®æ ‡ï¼Œä»¥é€šç”¨æ–¹å‘æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œè¿™æœ‰æœ›å®ç°é«˜æ•ˆçš„æŸ¥è¯¢å¼•å¯¼ç”Ÿæˆã€‚åœ¨åäºŒä¸ªé«˜åˆ†è¾¨ç‡ï¼ˆ1024Ã—1024ï¼‰å›¾åƒç›®æ ‡ç”Ÿæˆä»»åŠ¡å’Œå…­ä¸ª3Dåˆ†å­ç›®æ ‡ç”Ÿæˆä»»åŠ¡çš„å¤§é‡å®éªŒæ˜¾ç¤ºï¼ŒæŸ¥è¯¢æ•ˆç‡æé«˜äº†6å€è‡³10å€å’Œæé«˜äº†æœ€é«˜è‡³çš„æŸ¥è¯¢æ•ˆç‡æ”¹å–„æ¯”ä¾‹åˆ†åˆ«è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è‡³é«˜è¾¾è¾¾é«˜è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾è¾¾å¤§44å€ã€‚æˆ‘ä»¬çš„å®ç°å·²åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct">https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01692v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å®šåˆ¶åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ½œåŠ›ä¸å±€é™æ€§ã€‚ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¾èµ–äºé¢„æ”¶é›†æ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„ç›®æ ‡å‡½æ•°æˆ–éœ€è¦å¯å¾®åˆ†çš„ç›®æ ‡å‡½æ•°ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„è®¸å¤šä»»åŠ¡ä¸­ï¼Œç¦»çº¿æ•°æ®é›†å¾€å¾€æ— æ³•è·å–ä¸”å…¶ç›®æ ‡å‡½æ•°å¾€å¾€ä¸å¯å¾®åˆ†ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åœ¨çº¿é»‘ç›’ç›®æ ‡ç”Ÿæˆç®—æ³•â€”â€”Fast Directï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨è¿è¡Œæ—¶æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç›’ç›®æ ‡å‡½æ•°ï¼ŒåŒæ—¶å…·æœ‰è¾ƒé«˜çš„æŸ¥è¯¢æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFast Directåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆå’Œä¸‰ç»´åˆ†å­ç”Ÿæˆç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æŸ¥è¯¢æ•ˆç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å®šåˆ¶åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸‹æ¸¸ä»»åŠ¡æ—¶ã€‚</li>
<li>ç°æœ‰å¼•å¯¼æ‰©æ•£æ¨¡å‹ä¾èµ–äºé¢„æ”¶é›†æ•°æ®é›†æˆ–å¯å¾®åˆ†çš„ç›®æ ‡å‡½æ•°ï¼Œè¿™åœ¨ç°å®ä»»åŠ¡ä¸­å¹¶ä¸å¸¸è§ã€‚</li>
<li>Fast Directç®—æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åœ¨çº¿æ”¶é›†æ•°æ®å¹¶æ”¯æŒé»‘ç›’ç›®æ ‡å‡½æ•°ï¼Œå®ç°äº†é«˜æ•ˆçš„æŸ¥è¯¢ã€‚</li>
<li>Fast Directç®—æ³•åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆå’Œä¸‰ç»´åˆ†å­ç”Ÿæˆç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æŸ¥è¯¢æ•ˆç‡æå‡ã€‚</li>
<li>Fast Directç®—æ³•é€šè¿‡æ„å»ºä¼ªç›®æ ‡æ¥æ›´æ–°æ‰©æ•£æ¨¡å‹çš„å™ªå£°åºåˆ—ï¼Œå¹¶é‡‡ç”¨äº†é€šç”¨æ–¹å‘è¿›è¡Œå¼•å¯¼ç”Ÿæˆã€‚</li>
<li>è¯¥ç®—æ³•å·²åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬åäºŒä¸ªé«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä»»åŠ¡å’Œå…­ä¸ªä¸‰ç»´åˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d89fdc0114ed0a2df5ffaebd5f3bd5f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8e6b3245a8fea1168ea1a33d9883dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b02e30e6862644daeb69fdee46ecbd0f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DiffZOO-A-Purely-Query-Based-Black-Box-Attack-for-Red-teaming-Text-to-Image-Generative-Model-via-Zeroth-Order-Optimization"><a href="#DiffZOO-A-Purely-Query-Based-Black-Box-Attack-for-Red-teaming-Text-to-Image-Generative-Model-via-Zeroth-Order-Optimization" class="headerlink" title="DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming   Text-to-Image Generative Model via Zeroth Order Optimization"></a>DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming   Text-to-Image Generative Model via Zeroth Order Optimization</h2><p><strong>Authors:Pucheng Dang, Xing Hu, Dong Li, Rui Zhang, Qi Guo, Kaidi Xu</strong></p>
<p>Current text-to-image (T2I) synthesis diffusion models raise misuse concerns, particularly in creating prohibited or not-safe-for-work (NSFW) images. To address this, various safety mechanisms and red teaming attack methods are proposed to enhance or expose the T2I modelâ€™s capability to generate unsuitable content. However, many red teaming attack methods assume knowledge of the text encoders, limiting their practical usage. In this work, we rethink the case of \textit{purely black-box} attacks without prior knowledge of the T2l model. To overcome the unavailability of gradients and the inability to optimize attacks within a discrete prompt space, we propose DiffZOO which applies Zeroth Order Optimization to procure gradient approximations and harnesses both C-PRV and D-PRV to enhance attack prompts within the discrete prompt domain. We evaluated our method across multiple safety mechanisms of the T2I diffusion model and online servers. Experiments on multiple state-of-the-art safety mechanisms show that DiffZOO attains an 8.5% higher average attack success rate than previous works, hence its promise as a practical red teaming tool for T2l models. </p>
<blockquote>
<p>å½“å‰æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰åˆæˆæ‰©æ•£æ¨¡å‹å¼•å‘äº†æ»¥ç”¨æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›å»ºç¦æ­¢æˆ–ä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„å›¾åƒæ–¹é¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†å„ç§å®‰å…¨æœºåˆ¶å’Œçº¢é˜Ÿæ”»å‡»æ–¹æ³•æ¥å¢å¼ºæˆ–æ­éœ²T2Iæ¨¡å‹ç”Ÿæˆä¸åˆé€‚å†…å®¹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè®¸å¤šçº¢é˜Ÿæ”»å‡»æ–¹æ³•éœ€è¦äº†è§£æ–‡æœ¬ç¼–ç å™¨ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°è€ƒè™‘äº†æ— éœ€äº‹å…ˆäº†è§£T2Iæ¨¡å‹çš„â€œçº¯é»‘ç®±â€æ”»å‡»çš„æƒ…å†µã€‚ä¸ºäº†å…‹æœæ— æ³•è·å–æ¢¯åº¦ä»¥åŠåœ¨ç¦»æ•£æç¤ºç©ºé—´å†…æ— æ³•ä¼˜åŒ–æ”»å‡»çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DiffZOOï¼Œå®ƒé‡‡ç”¨é›¶é˜¶ä¼˜åŒ–æ¥è·å–æ¢¯åº¦è¿‘ä¼¼å€¼ï¼Œå¹¶åˆ©ç”¨C-PRVå’ŒD-PRVåœ¨ç¦»æ•£æç¤ºåŸŸå†…å¢å¼ºæ”»å‡»æç¤ºã€‚æˆ‘ä»¬åœ¨T2Iæ‰©æ•£æ¨¡å‹çš„å¤šé‡å®‰å…¨æœºåˆ¶å’Œåœ¨çº¿æœåŠ¡å™¨ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å¯¹å¤šç§æœ€å…ˆè¿›çš„å®‰å…¨æœºåˆ¶çš„å®éªŒè¡¨æ˜ï¼ŒDiffZOOçš„å¹³å‡æ”»å‡»æˆåŠŸç‡æ¯”å…ˆå‰çš„å·¥ä½œé«˜å‡º8.5%ï¼Œå› æ­¤åœ¨T2Iæ¨¡å‹çš„å®ç”¨çº¢é˜Ÿå·¥å…·ä¸­æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11071v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ç”Ÿæˆé¢†åŸŸä¸­çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰åˆæˆæ‰©æ•£æ¨¡å‹å­˜åœ¨æ»¥ç”¨é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆä¸é€‚å®œå·¥ä½œæˆ–ç¦æ­¢çš„å›¾åƒæ–¹é¢ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†å¤šç§å®‰å…¨æœºåˆ¶å’Œçº¢é˜Ÿæ”»å‡»æ–¹æ³•ï¼Œä»¥æå‡æˆ–æ­ç¤ºT2Iæ¨¡å‹ç”Ÿæˆä¸åˆé€‚å†…å®¹çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè®¸å¤šçº¢é˜Ÿæ”»å‡»æ–¹æ³•éœ€è¦äº†è§£æ–‡æœ¬ç¼–ç å™¨ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æœ¬ç ”ç©¶é‡æ–°æ€è€ƒäº†æ— éœ€äº†è§£T2Iæ¨¡å‹çš„çº¯é»‘ç®±æ”»å‡»æƒ…å†µã€‚ä¸ºå…‹æœæ— æ³•è·å–æ¢¯åº¦ä»¥åŠåœ¨ç¦»æ•£æç¤ºç©ºé—´å†…ä¼˜åŒ–æ”»å‡»çš„éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffZOOï¼Œå®ƒé‡‡ç”¨é›¶é˜¶ä¼˜åŒ–æ¥è·å–æ¢¯åº¦è¿‘ä¼¼å€¼ï¼Œå¹¶åˆ©ç”¨C-PRVå’ŒD-PRVå¢å¼ºç¦»æ•£æç¤ºåŸŸå†…çš„æ”»å‡»æç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªT2Iæ‰©æ•£æ¨¡å‹å®‰å…¨æœºåˆ¶å’Œåœ¨çº¿æœåŠ¡å™¨ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºä»¥å¾€çš„ç ”ç©¶ï¼ŒDiffZOOçš„å¹³å‡æ”»å‡»æˆåŠŸç‡æé«˜äº†8.5%ï¼Œæˆä¸ºT2Iæ¨¡å‹å®ç”¨çº¢é˜Ÿå·¥å…·çš„æœ‰åŠ›å€™é€‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰åˆæˆæ‰©æ•£æ¨¡å‹å­˜åœ¨ç”Ÿæˆä¸é€‚å®œå†…å®¹çš„æ»¥ç”¨é£é™©ã€‚</li>
<li>ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†å¤šç§å®‰å…¨æœºåˆ¶å’Œçº¢é˜Ÿæ”»å‡»æ–¹æ³•ã€‚</li>
<li>ç°æœ‰çº¢é˜Ÿæ”»å‡»æ–¹æ³•å¾€å¾€éœ€è¦äº†è§£æ–‡æœ¬ç¼–ç å™¨ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†DiffZOOæ–¹æ³•ï¼Œèƒ½åœ¨ä¸äº†è§£T2Iæ¨¡å‹çš„æƒ…å†µä¸‹è¿›è¡Œçº¯é»‘ç®±æ”»å‡»ã€‚</li>
<li>DiffZOOé‡‡ç”¨é›¶é˜¶ä¼˜åŒ–è·å–æ¢¯åº¦è¿‘ä¼¼å€¼ï¼Œå¢å¼ºç¦»æ•£æç¤ºç©ºé—´å†…çš„æ”»å‡»æç¤ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒDiffZOOåœ¨å¤šä¸ªå®‰å…¨æœºåˆ¶ä¸Šçš„æ”»å‡»æˆåŠŸç‡é«˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8233fb263083a5e3cc45058f7c7329f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c81e5054e9fdc84bbb2f0f8e41792d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2e24807b981bbc7d17f8e0cdb7fb6d0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Lightning-Fast-Image-Inversion-and-Editing-for-Text-to-Image-Diffusion-Models"><a href="#Lightning-Fast-Image-Inversion-and-Editing-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion   Models"></a>Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion   Models</h2><p><strong>Authors:Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, Rami Ben-Ari</strong></p>
<p>Diffusion inversion is the problem of taking an image and a text prompt that describes it and finding a noise latent that would generate the exact same image. Most current deterministic inversion techniques operate by approximately solving an implicit equation and may converge slowly or yield poor reconstructed images. We formulate the problem by finding the roots of an implicit equation and devlop a method to solve it efficiently. Our solution is based on Newton-Raphson (NR), a well-known technique in numerical analysis. We show that a vanilla application of NR is computationally infeasible while naively transforming it to a computationally tractable alternative tends to converge to out-of-distribution solutions, resulting in poor reconstruction and editing. We therefore derive an efficient guided formulation that fastly converges and provides high-quality reconstructions and editing. We showcase our method on real image editing with three popular open-sourced diffusion models: Stable Diffusion, SDXL-Turbo, and Flux with different deterministic schedulers. Our solution, Guided Newton-Raphson Inversion, inverts an image within 0.4 sec (on an A100 GPU) for few-step models (SDXL-Turbo and Flux.1), opening the door for interactive image editing. We further show improved results in image interpolation and generation of rare objects. </p>
<blockquote>
<p>æ‰©æ•£åæ¼”ï¼ˆDiffusion Inversionï¼‰é—®é¢˜æ˜¯æŒ‡ç»™å®šä¸€å¼ å›¾ç‰‡å’Œæè¿°å®ƒçš„æ–‡æœ¬æç¤ºï¼Œå¯»æ‰¾èƒ½å¤Ÿç”Ÿæˆç›¸åŒå›¾ç‰‡çš„å™ªå£°æ½œåœ¨å› å­ã€‚å½“å‰å¤§å¤šæ•°çš„ç¡®å®šæ€§åæ¼”æŠ€æœ¯éƒ½æ˜¯é€šè¿‡è¿‘ä¼¼è§£å†³éšå¼æ–¹ç¨‹å®ç°çš„ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ”¶æ•›é€Ÿåº¦æ…¢æˆ–é‡æ„çš„å›¾åƒè´¨é‡ä¸ä½³ã€‚æˆ‘ä»¬é€šè¿‡å¯»æ‰¾éšå¼æ–¹ç¨‹çš„æ ¹æ¥è¡¨è¿°é—®é¢˜ï¼Œå¹¶å¼€å‘äº†ä¸€ç§é«˜æ•ˆè§£å†³æ–¹æ³•ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŸºäºæ•°å€¼åˆ†æä¸­çš„çŸ¥åæŠ€æœ¯â€”â€”ç‰›é¡¿-æ‹‰å¤«æ£®æ–¹æ³•ï¼ˆNewton-Raphsonï¼Œç®€ç§°NRï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œç›´æ¥åº”ç”¨NRåœ¨è®¡ç®—ä¸Šä¸å¯è¡Œï¼Œè€Œå°†å…¶ç®€å•è½¬æ¢ä¸ºå¯è®¡ç®—çš„æ›¿ä»£æ–¹æ¡ˆåˆ™å¾€å¾€æ”¶æ•›äºéåˆ†å¸ƒè§£ï¼Œå¯¼è‡´é‡æ„å’Œç¼–è¾‘è´¨é‡å·®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„å¼•å¯¼å¼å…¬å¼ï¼Œå®ƒèƒ½å¿«é€Ÿæ”¶æ•›ï¼Œå¹¶æä¾›é«˜è´¨é‡çš„é‡æ„å’Œç¼–è¾‘ã€‚æˆ‘ä»¬åœ¨çœŸå®å›¾åƒç¼–è¾‘ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨äº†ä¸‰ä¸ªæµè¡Œçš„å¼€æºæ‰©æ•£æ¨¡å‹ï¼šStable Diffusionã€SDXL-Turboå’ŒFluxï¼Œä»¥åŠä¸åŒçš„ç¡®å®šæ€§è°ƒåº¦å™¨ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆâ€”â€”å¼•å¯¼å¼ç‰›é¡¿-æ‹‰å¤«æ£®åæ¼”æ³•ï¼ˆGuided Newton-Raphson Inversionï¼‰ï¼Œèƒ½åœ¨A100 GPUä¸Šå®ç°0.4ç§’å†…å¯¹å°‘æ•°æ¨¡å‹ï¼ˆSDXL-Turboå’ŒFlux 1ï¼‰è¿›è¡Œå›¾åƒåæ¼”ï¼Œä¸ºäº¤äº’å¼å›¾åƒç¼–è¾‘æ‰“å¼€äº†å¤§é—¨ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨å›¾åƒæ’å€¼å’Œç¨€æœ‰å¯¹è±¡ç”Ÿæˆæ–¹é¢çš„æ”¹è¿›ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.12540v5">PDF</a> Accepted to ICLR25. Project Page:   <a target="_blank" rel="noopener" href="https://barakmam.github.io/rnri.github.io/">https://barakmam.github.io/rnri.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£åè½¬é—®é¢˜ï¼Œå³æ ¹æ®å›¾åƒå’Œæè¿°å®ƒçš„æ–‡æœ¬æç¤ºæ‰¾åˆ°ç”Ÿæˆç›¸åŒå›¾åƒçš„å™ªå£°æ½œåœ¨å› ç´ ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºç‰›é¡¿-æ‹‰å¤«æ£®ï¼ˆNewton-Raphsonï¼‰æ–¹æ³•çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰æ•ˆè§£å†³äº†æ‰©æ•£åè½¬é—®é¢˜ä¸­çš„é«˜æ•ˆæ±‚è§£æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¿«é€Ÿæ”¶æ•›ï¼Œæä¾›äº†é«˜è´¨é‡çš„é‡å»ºå’Œç¼–è¾‘æ•ˆæœï¼Œå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­ç”¨äºå›¾åƒç¼–è¾‘ã€å›¾åƒæ’å€¼å’Œç¨€æœ‰å¯¹è±¡ç”Ÿæˆç­‰é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£åè½¬é—®é¢˜æ—¨åœ¨ä»ç»™å®šçš„å›¾åƒå’Œæ–‡æœ¬æè¿°ä¸­æ‰¾å‡ºç”Ÿæˆè¯¥å›¾åƒçš„å™ªå£°æ½œåœ¨å› ç´ ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°ç¡®å®šæ€§åè½¬æŠ€æœ¯é€šè¿‡è¿‘ä¼¼è§£å†³éšå¼æ–¹ç¨‹æ¥æ“ä½œï¼Œä½†å¯èƒ½å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢æˆ–é‡å»ºå›¾åƒè´¨é‡å·®çš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºç‰›é¡¿-æ‹‰å¤«æ£®æ–¹æ³•çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—ä¸Šæ›´é«˜æ•ˆä¸”èƒ½å¿«é€Ÿæ”¶æ•›ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†é«˜è´¨é‡çš„å›¾åƒé‡å»ºå’Œç¼–è¾‘æ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¯ç”¨äºå›¾åƒç¼–è¾‘ã€å›¾åƒæ’å€¼å’Œç¨€æœ‰å¯¹è±¡ç”Ÿæˆç­‰é¢†åŸŸã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†åœ¨çœŸå®å›¾åƒç¼–è¾‘ä¸­ä½¿ç”¨ä¸‰ç§æµè¡Œçš„å¼€æºæ‰©æ•£æ¨¡å‹ï¼ˆStable Diffusionã€SDXL-Turboå’ŒFluxï¼‰çš„ç»“æœï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.12540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63f80903a9294a89f04c0ee1c95afcab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880f9634c2016fee05251b5da94b95ce.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sanitizing-Hidden-Information-with-Diffusion-Models"><a href="#Sanitizing-Hidden-Information-with-Diffusion-Models" class="headerlink" title="Sanitizing Hidden Information with Diffusion Models"></a>Sanitizing Hidden Information with Diffusion Models</h2><p><strong>Authors:Preston K. Robinette, Daniel Moyer, Taylor T. Johnson</strong></p>
<p>Information hiding is the process of embedding data within another form of data, often to conceal its existence or prevent unauthorized access. This process is commonly used in various forms of secure communications (steganography) that can be used by bad actors to propagate malware, exfiltrate victim data, and discreetly communicate. Recent work has utilized deep neural networks to remove this hidden information in a defense mechanism known as sanitization. Previous deep learning works, however, are unable to scale efficiently beyond the MNIST dataset. In this work, we present a novel sanitization method called DM-SUDS that utilizes a diffusion model framework to sanitize&#x2F;remove hidden information from image-into-image universal and dependent steganography from CIFAR-10 and ImageNet datasets. We evaluate DM-SUDS against three different baselines using MSE, PSNR, SSIM, and NCC metrics and provide further detailed analysis through an ablation study. DM-SUDS outperforms all three baselines and significantly improves image preservation MSE by 50.44%, PSNR by 12.69%, SSIM by 11.49%, and NCC by 3.26% compared to previous deep learning approaches. Additionally, we introduce a novel evaluation specification that considers the successful removal of hidden information (safety) as well as the resulting quality of the sanitized image (utility). We further demonstrate the versatility of this method with an application in an audio case study, demonstrating its broad applicability to additional domains. </p>
<blockquote>
<p>ä¿¡æ¯éšè—æ˜¯å°†æ•°æ®åµŒå…¥å¦ä¸€ç§æ•°æ®å½¢å¼ä¸­çš„è¿‡ç¨‹ï¼Œé€šå¸¸ç”¨äºéšè—å…¶å­˜åœ¨æˆ–é˜²æ­¢æœªç»æˆæƒçš„è®¿é—®ã€‚è¿™ä¸€è¿‡ç¨‹ä¸­å¸¸ç”¨äºå„ç§å®‰å…¨é€šä¿¡ï¼ˆéšå†™æœ¯ï¼‰ï¼Œä¸è‰¯è¡Œä¸ºè€…å¯èƒ½åˆ©ç”¨å®ƒæ¥ä¼ æ’­æ¶æ„è½¯ä»¶ã€çªƒå–å—å®³è€…æ•°æ®å¹¶è¿›è¡Œéšç§˜é€šä¿¡ã€‚æœ€è¿‘çš„å·¥ä½œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å»é™¤è¿™ç§éšè—ä¿¡æ¯ï¼Œä½œä¸ºä¸€ç§åä¸ºæ¸…æ´—çš„é˜²å¾¡æœºåˆ¶ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ·±åº¦å­¦ä¹ å·¥ä½œåœ¨å¤„ç†MNISTæ•°æ®é›†ä¹‹å¤–çš„æ•°æ®æ—¶æ— æ³•æœ‰æ•ˆæ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¸…æ´—æ–¹æ³•DM-SUDSï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶æ¥æ¸…æ´—&#x2F;å»é™¤CIFAR-10å’ŒImageNetæ•°æ®é›†ä¸­å›¾åƒå†…å›¾åƒé€šç”¨å’Œä¾èµ–éšå†™æœ¯çš„éšè—ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰å’Œå½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼ˆNCCï¼‰ç­‰æŒ‡æ ‡å¯¹DM-SUDSä¸ä¸‰ç§ä¸åŒåŸºçº¿è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶é€šè¿‡æ¶ˆèç ”ç©¶æä¾›äº†è¿›ä¸€æ­¥çš„åˆ†æã€‚ä¸ä¹‹å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒDM-SUDSåœ¨å›¾åƒä¿ç•™çš„MSEã€PSNRã€SSIMå’ŒNCCæ–¹é¢åˆ†åˆ«æé«˜äº†50.44%ã€12.69%ã€11.49%å’Œ3.26%ï¼Œå¹¶ä¸”è¶…è¶Šäº†æ‰€æœ‰ä¸‰æ¡åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œè¯¥æ ‡å‡†è€ƒè™‘äº†æˆåŠŸå»é™¤éšè—ä¿¡æ¯ï¼ˆå®‰å…¨æ€§ï¼‰ä»¥åŠæ¸…æ´—åå›¾åƒçš„è´¨é‡ï¼ˆæ•ˆç”¨ï¼‰ã€‚æˆ‘ä»¬è¿˜é€šè¿‡éŸ³é¢‘æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†è¯¥æ–¹æ³•çš„é€šç”¨æ€§ï¼Œè¯æ˜äº†å…¶åœ¨å…¶ä»–é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06951v2">PDF</a> Accepted to European Conference on Artificial Intelligence (ECAI),   2024</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºDM-SUDSçš„æ–°å‹å‡€åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶å»é™¤å›¾åƒå’ŒéŸ³é¢‘ä¸­çš„éšè—ä¿¡æ¯ï¼Œä»¥ç”¨äºé˜²å¾¡éšè—ä¿¡æ¯çš„æ”»å‡»ã€‚ç›¸è¾ƒäºä¹‹å‰æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼ŒDM-SUDSèƒ½åœ¨CIFAR-10å’ŒImageNetæ•°æ®é›†ä¸Šæ›´æœ‰æ•ˆåœ°å»é™¤éšè—ä¿¡æ¯ï¼Œæé«˜å›¾åƒä¿ç•™æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä»·æ ‡å‡†ï¼ŒåŒæ—¶è€ƒè™‘æˆåŠŸå»é™¤éšè—ä¿¡æ¯çš„å®‰å…¨æ€§å’Œå‡€åŒ–åå›¾åƒçš„è´¨é‡ã€‚DM-SUDSæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯ç”¨äºéŸ³é¢‘ç­‰é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯éšè—æ˜¯å°†æ•°æ®åµŒå…¥å¦ä¸€ç§æ•°æ®å½¢å¼ä¸­çš„è¿‡ç¨‹ï¼Œå¸¸ç”¨äºå®‰å…¨é€šä¿¡ä¸­ã€‚</li>
<li>è¿‘æœŸå·¥ä½œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå»é™¤éšè—ä¿¡æ¯ï¼Œä½œä¸ºé˜²å¾¡æœºåˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæ‰©å±•è‡³MNISTæ•°æ®é›†ä»¥å¤–çš„é¢†åŸŸã€‚</li>
<li>DM-SUDSæ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œæœ‰æ•ˆå»é™¤å›¾åƒä¸­çš„éšè—ä¿¡æ¯ï¼Œå¹¶å±•ç°å‡ºå¯¹CIFAR-10å’ŒImageNetæ•°æ®é›†çš„ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ä¸ä¸‰ç§åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒDM-SUDSåœ¨å›¾åƒä¿ç•™æ•ˆæœä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ç»¼åˆè€ƒè™‘æˆåŠŸå»é™¤éšè—ä¿¡æ¯ä¸å‡€åŒ–åå›¾åƒè´¨é‡çš„æ–°è¯„ä»·æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.06951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85b5a260424ab0cb9605761735953d15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b11e10d7874a985481cd768d64dd29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8565c6c690cb14a07b1863aafaab05d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6027626f71538b4b9ce8eccd1d89588c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56a7fdda87b7ee6715768e4e90089ccd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93fee308770d8ef938141d7a3470ebc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-5c265730e323ed2e7668b32b4f7572b0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  ConceptAttention Diffusion Transformers Learn Highly Interpretable   Features
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-710e0095fca0cad8157c10daaefefc91.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  GS-LiDAR Generating Realistic LiDAR Point Clouds with Panoramic   Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
