<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  Ola Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9a07efacb8deb5c3753163a995d0302f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-08-æ›´æ–°"><a href="#2025-02-08-æ›´æ–°" class="headerlink" title="2025-02-08 æ›´æ–°"></a>2025-02-08 æ›´æ–°</h1><h2 id="Ola-Pushing-the-Frontiers-of-Omni-Modal-Language-Model-with-Progressive-Modality-Alignment"><a href="#Ola-Pushing-the-Frontiers-of-Omni-Modal-Language-Model-with-Progressive-Modality-Alignment" class="headerlink" title="Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment"></a>Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive   Modality Alignment</h2><p><strong>Authors:Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao</strong></p>
<p>Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/Ola-Omni/Ola">https://github.com/Ola-Omni/Ola</a>. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯GPT-4oä¹‹åï¼Œå¼•å‘äº†äººä»¬å¯¹å¼€å‘èƒ½å¤Ÿç†è§£æ›´å¤šæ¨¡æ€çš„é€šç”¨æ¨¡å‹çš„æµ“åšå…´è¶£ã€‚è™½ç„¶å·²ç»å‡ºç°äº†ä¸€äº›å¼€æºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨æ€§èƒ½ä¸Šä»ç„¶æ˜æ˜¾è½åäºä¸“ä¸šçš„å•æ¨¡æ€æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Olaï¼Œä¸€ç§é€šç”¨æ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œåœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£æ–¹é¢ä¸ä¸“ç”¨æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚Olaçš„æ ¸å¿ƒè®¾è®¡åœ¨äºå…¶æ¸è¿›çš„æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ”¯æŒæ¨¡æ€ã€‚æˆ‘ä»¬çš„è®­ç»ƒç®¡é“å§‹äºæœ€ç‹¬ç‰¹çš„æ¨¡æ€ï¼šå›¾åƒå’Œæ–‡æœ¬ï¼Œç„¶åä½¿ç”¨è¿æ¥è¯­è¨€å’ŒéŸ³é¢‘çŸ¥è¯†çš„è¯­éŸ³æ•°æ®ä»¥åŠè¿æ¥æ‰€æœ‰æ¨¡æ€çš„è§†é¢‘æ•°æ®ï¼Œé€æ­¥æ‰©å±•æ¨¡å‹æŠ€èƒ½é›†ã€‚æ¸è¿›å¼å­¦ä¹ ç®¡é“è¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¿æŒç›¸å¯¹è¾ƒå°çš„è·¨æ¨¡æ€å¯¹é½æ•°æ®é›†è§„æ¨¡ï¼Œä»è€Œè½»æ¾ä½æˆæœ¬åœ°ä»ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å¼€å‘é€šç”¨æ¨¡æ€ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£é”åƒGPT-4oè¿™æ ·çš„é«˜çº§äº¤äº’ä½“éªŒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§åŸºäºå¥å­çš„è§£ç è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæµå¼è¯­éŸ³ç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOlaåœ¨æ‰€æœ‰æ¨¡æ€ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨ç±»ä¼¼è§„æ¨¡çš„ä¸“ä¸šæ¨¡å‹ä¸­å–å¾—äº†é«˜åº¦ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿Olaæˆä¸ºä¸€ä¸ªå®Œå…¨å¼€æºçš„é€šç”¨å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆï¼Œä»¥æ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚æ¨¡å‹æƒé‡ã€ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ola-Omni/Ola%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Ola-Omni/Olaå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04328v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯GPT-4oä¹‹åï¼Œæ¿€å‘äº†å¼€å‘èƒ½å¤Ÿç†è§£å’Œå¤„ç†æ›´å¤šæ¨¡æ€çš„é€šç”¨å¤šæ¨¡æ€æ¨¡å‹çš„å…´è¶£ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾åä¸ºOlaçš„é€šç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºä¸ä¸“é¡¹å•æ¨¡æ€æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚Olaçš„æ ¸å¿ƒè®¾è®¡åœ¨äºå…¶æ¸è¿›å¼æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ”¯æŒæ¨¡æ€ã€‚å…¶è®­ç»ƒæµç¨‹ä»æœ€å…·ç‰¹è‰²çš„æ¨¡æ€ï¼ˆå›¾åƒå’Œæ–‡æœ¬ï¼‰å¼€å§‹ï¼Œç„¶åé€šè¿‡è¿æ¥è¯­è¨€å’ŒéŸ³é¢‘çŸ¥è¯†çš„è¯­éŸ³æ•°æ®ä»¥åŠè¿æ¥æ‰€æœ‰æ¨¡æ€çš„è§†é¢‘æ•°æ®ï¼Œé€æ­¥æ‰©å±•æ¨¡å‹æŠ€èƒ½é›†ã€‚è¿™ç§æ¸è¿›å¼å­¦ä¹ æµç¨‹è¿˜ä½¿å¾—è·¨æ¨¡æ€å¯¹é½æ•°æ®é›†çš„è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œä½¿å¾—ä»ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹å‡ºå‘å¼€å‘é€šç”¨å¤šæ¨¡æ€æ¨¡å‹å˜å¾—æ›´åŠ å®¹æ˜“å’Œæˆæœ¬æ›´ä½ã€‚æ­¤å¤–ï¼ŒOlaè¿˜è®¾è®¡äº†ä¸€ç§å¥å­çº§çš„è§£ç è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæµå¼è¯­éŸ³ç”Ÿæˆï¼Œè§£é”äº†ç±»ä¼¼GPT-4oçš„é«˜çº§äº¤äº’ä½“éªŒã€‚å®éªŒè¡¨æ˜ï¼ŒOlaåœ¨æ‰€æœ‰æ¨¡æ€ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨ç±»ä¼¼è§„æ¨¡çš„ä¸“ä¸šæ¨¡å‹ä¸­è¡¨ç°å‡ºé«˜åº¦ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿Olaæˆä¸ºå®Œå…¨å¼€æºçš„é€šç”¨å¤šæ¨¡æ€ç†è§£è§£å†³æ–¹æ¡ˆï¼Œä»¥æ¨åŠ¨è¿™ä¸€æ–°å…´é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Olaæ˜¯ä¸€æ¬¾é€šç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºä¸ä¸“é¡¹å•æ¨¡æ€æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ã€‚</li>
<li>Olaçš„æ ¸å¿ƒè®¾è®¡åœ¨äºå…¶æ¸è¿›å¼æ¨¡æ€å¯¹é½ç­–ç•¥ï¼Œé€šè¿‡é€æ­¥æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ”¯æŒæ¨¡æ€æ¥å®ç°å¤šæ¨¡æ€ç†è§£ã€‚</li>
<li>Olaçš„è®­ç»ƒæµç¨‹ä»å›¾åƒå’Œæ–‡æœ¬å¼€å§‹ï¼Œç„¶åé€æ­¥å¼•å…¥è¯­éŸ³å’Œè§†é¢‘æ•°æ®ï¼Œä»¥æ‰©å±•æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>æ¸è¿›å¼å­¦ä¹ æµç¨‹ä½¿å¾—è·¨æ¨¡æ€å¯¹é½æ•°æ®é›†çš„è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œé™ä½äº†å¼€å‘æˆæœ¬ã€‚</li>
<li>Olaè®¾è®¡äº†ä¸€ç§å¥å­çº§çš„è§£ç è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæµå¼è¯­éŸ³ç”Ÿæˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„äº¤äº’æ€§ã€‚</li>
<li>Olaåœ¨æ‰€æœ‰æ¨¡æ€ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a05bfbd151e6cb895ca73ba5da37c9df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-626c4fc88e7d0b71cec8356f0d2ae844.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3091b78a842b040147a3b51e76f39499.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d2f0d54b6f8fa60a718ec67b9aa10d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59fbfbd3e80017c232a18668c39d327b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WorldSense-Evaluating-Real-world-Omnimodal-Understanding-for-Multimodal-LLMs"><a href="#WorldSense-Evaluating-Real-world-Omnimodal-Understanding-for-Multimodal-LLMs" class="headerlink" title="WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal   LLMs"></a>WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal   LLMs</h2><p><strong>Authors:Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, Weidi Xie</strong></p>
<p>In this paper, we introduce WorldSense, the first benchmark to assess the multi-modal video understanding, that simultaneously encompasses visual, audio, and text inputs. In contrast to existing benchmarks, our WorldSense has several features: (i) collaboration of omni-modality, we design the evaluation tasks to feature a strong coupling of audio and video, requiring models to effectively utilize the synergistic perception of omni-modality; (ii) diversity of videos and tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual synchronised videos, systematically categorized into 8 primary domains and 67 fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice QA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii) high-quality annotations, all the QA pairs are manually labeled by 80 expert annotators with multiple rounds of correction to ensure quality. Based on our WorldSense, we extensively evaluate various state-of-the-art models. The experimental results indicate that existing models face significant challenges in understanding real-world scenarios (48.0% best accuracy). We hope our WorldSense can provide a platform for evaluating the ability in constructing and understanding coherent contexts from omni-modality. </p>
<blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†WorldSenseï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡å¼è§†é¢‘ç†è§£çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒæ—¶æ¶µç›–äº†è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬è¾“å…¥ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„WorldSenseå…·æœ‰å‡ ä¸ªç‰¹ç‚¹ï¼šï¼ˆiï¼‰å…¨æ¨¡å¼åä½œï¼Œæˆ‘ä»¬è®¾è®¡çš„è¯„ä¼°ä»»åŠ¡æ—¨åœ¨çªå‡ºéŸ³é¢‘å’Œè§†é¢‘çš„å¼ºçƒˆè€¦åˆï¼Œè¦æ±‚æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨å…¨æ¨¡å¼çš„ååŒæ„ŸçŸ¥ï¼›ï¼ˆiiï¼‰è§†é¢‘å’Œä»»åŠ¡å¤šæ ·æ€§ï¼ŒWorldSenseæ¶µç›–äº†å¤šæ ·åŒ–çš„1662ä¸ªéŸ³è§†é¢‘åŒæ­¥è§†é¢‘ï¼Œç³»ç»Ÿåœ°åˆ†ä¸º8ä¸ªä¸»è¦é¢†åŸŸå’Œ67ä¸ªç²¾ç»†å­ç±»åˆ«ï¼Œä»¥æ¶µç›–å¹¿æ³›çš„åœºæ™¯ï¼Œä»¥åŠ26ä¸ªä¸åŒä»»åŠ¡çš„3172ä¸ªå¤šé¡¹é€‰æ‹©é—®ç­”å¯¹ï¼Œä»¥å®ç°å…¨é¢è¯„ä¼°ï¼›ï¼ˆiiiï¼‰é«˜è´¨é‡æ³¨é‡Šï¼Œæ‰€æœ‰é—®ç­”å¯¹å‡ç”±80åä¸“ä¸šæ³¨é‡Šå‘˜æ‰‹åŠ¨æ ‡æ³¨ï¼Œç»è¿‡å¤šè½®æ ¡æ­£ä»¥ç¡®ä¿è´¨é‡ã€‚åŸºäºæˆ‘ä»¬çš„WorldSenseï¼Œæˆ‘ä»¬å¯¹å„ç§æœ€æ–°æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨ç†è§£çœŸå®åœºæ™¯æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ˆæœ€ä½³ç²¾åº¦ä¸º48.0%ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›WorldSenseèƒ½ä¸ºè¯„ä¼°ä»å…¨æ¨¡å¼æ„å»ºå’Œç†è§£è¿è´¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›æä¾›ä¸€ä¸ªå¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04326v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†WorldSenseï¼Œé¦–ä¸ªè¯„ä¼°å¤šæ¨¡æ€è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚è¯¥å¹³å°èåˆäº†è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå…·æœ‰å¤šæ¨¡æ€åä½œã€è§†é¢‘ä»»åŠ¡å¤šæ ·ä»¥åŠé«˜è´¨é‡æ³¨é‡Šç­‰ç‰¹ç‚¹ã€‚é€šè¿‡å¯¹ç°æœ‰æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨ç†è§£çœŸå®åœºæ™¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚WorldSenseæœ‰æœ›ä¸ºè¯„ä¼°å¤šæ¨¡æ€æ„å»ºå’Œç†è§£è¿è´¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›æä¾›å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>WorldSenseæ˜¯é¦–ä¸ªè¯„ä¼°å¤šæ¨¡æ€è§†é¢‘ç†è§£çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œèåˆäº†è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬è¾“å…¥ã€‚</li>
<li>å¹³å°è®¾è®¡è¯„ä»·ä»»åŠ¡ï¼Œå¼ºè°ƒéŸ³é¢‘å’Œè§†é¢‘çš„å¼ºè€¦åˆï¼Œè¦æ±‚æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡æ€çš„ååŒæ„ŸçŸ¥ã€‚</li>
<li>WorldSenseåŒ…å«1662ä¸ªéŸ³è§†é¢‘åŒæ­¥è§†é¢‘å’Œ3172ä¸ªå¤šé€‰æ‹©é¢˜å¯¹ï¼Œè¦†ç›–26ä¸ªä¸åŒä»»åŠ¡ï¼Œå…·æœ‰è§†é¢‘å’Œä»»åŠ¡çš„å¤šæ ·æ€§ã€‚</li>
<li>æ‰€æœ‰é—®é¢˜å¯¹éƒ½æ˜¯ç»è¿‡80åä¸“å®¶æ ‡æ³¨è€…æ‰‹åŠ¨æ ‡æ³¨ï¼Œå¹¶ç»è¿‡å¤šè½®ä¿®æ­£ï¼Œä¿è¯äº†é«˜è´¨é‡æ³¨é‡Šã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ç†è§£çœŸå®åœºæ™¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œæœ€ä½³å‡†ç¡®ç‡ä»…ä¸º48.0%ã€‚</li>
<li>WorldSenseå¯ä»¥ä¸ºè¯„ä¼°æ„å»ºå’Œç†è§£å¤šæ¨¡æ€è¿è´¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›æä¾›å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b74e68a895afb4294e2f83b1dc85d38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ba48c509bd1d0d197d719743005a96c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36e1b7abb14bbf1d4e9df75befa243f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c2ff3fd99d6d24c598fe431b51c956b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293b814fe854c76dbb9996e9b61c414f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a752a76cbf4f5fcbe71f7d12c5c21411.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ChamaleonLLM-Batch-Aware-Dynamic-Low-Rank-Adaptation-via-Inference-Time-Clusters"><a href="#ChamaleonLLM-Batch-Aware-Dynamic-Low-Rank-Adaptation-via-Inference-Time-Clusters" class="headerlink" title="ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time   Clusters"></a>ChamaleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time   Clusters</h2><p><strong>Authors:Kamer Ali Yuksel, Hassan Sawaf</strong></p>
<p>Recent advances in large language models (LLMs) have shown remarkable performance across diverse tasks. However, these models are typically deployed with fixed weights, which limits their ability to adapt dynamically to the variability inherent in real-world data during inference. This paper introduces ChamaleonLLM, a novel framework that enables inference-time adaptation of LLMs by leveraging batch-aware clustering and on-the-fly generation of low-rank updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation (LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable masks), our method dynamically generates adaptive modifications to the decoder weights based on the aggregated statistics of clustered batches. By intelligently grouping similar inputs and computing context-aware low-rank updates via a hyper-network, ChamaleonLLM achieves significant performance gains, outperforming conventional LoRA methods while eliminating the overhead of maintaining multiple expert models. Our experiments highlight the potential of our approach to serve as a versatile and highly adaptive solution for language model inference. ChamaleonLLM is open-sourced to ensure the reproducibility of our experiments: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ChamaleonLLM/">https://anonymous.4open.science/r/ChamaleonLLM/</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä½¿ç”¨å›ºå®šçš„æƒé‡è¿›è¡Œéƒ¨ç½²ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€‚åº”ç°å®ä¸–ç•Œæ•°æ®å›ºæœ‰å¯å˜æ€§çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ChamaleonLLMï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ‰¹å¤„ç†æ„ŸçŸ¥èšç±»å’Œå³æ—¶ç”Ÿæˆä½ç§©æ›´æ–°æ¥å®ç°LLMæ¨ç†æ—¶é—´è‡ªé€‚åº”çš„æ–°å‹æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æˆ–ä¾èµ–äºé¢„å­¦ä¹ ç»Ÿä¸€é›†ï¼ˆå¯æ›´æ”¹çš„æ©ç ï¼‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ ¹æ®èšç±»æ‰¹æ¬¡çš„èšåˆç»Ÿè®¡åŠ¨æ€ç”Ÿæˆå¯¹è§£ç å™¨æƒé‡çš„è‡ªé€‚åº”ä¿®æ”¹ã€‚é€šè¿‡æ™ºèƒ½åœ°åˆ†ç»„ç›¸ä¼¼è¾“å…¥å¹¶é€šè¿‡è¶…ç½‘ç»œè®¡ç®—ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä½ç§©æ›´æ–°ï¼ŒChamaleonLLMå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ä»…è¶…è¶Šäº†ä¼ ç»Ÿçš„LoRAæ–¹æ³•ï¼Œè€Œä¸”æ¶ˆé™¤äº†ç»´æŠ¤å¤šä¸ªä¸“å®¶æ¨¡å‹çš„å¼€é”€ã€‚æˆ‘ä»¬çš„å®éªŒçªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•ä½œä¸ºè¯­è¨€æ¨¡å‹æ¨ç†çš„é€šç”¨å’Œé«˜åº¦è‡ªé€‚åº”è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚ChamaleonLLMå·²å¼€æºï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å®éªŒçš„å¯é‡å¤æ€§ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/ChamaleonLLM/">https://anonymous.4open.science/r/ChamaleonLLM/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04315v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åœ¨æ¨ç†æ—¶å…·æœ‰å›ºå®šçš„æƒé‡ï¼Œæ— æ³•åŠ¨æ€é€‚åº”ç°å®æ•°æ®ä¸­çš„å˜åŒ–ã€‚æœ¬æ–‡æå‡ºChamaleonLLMæ¡†æ¶ï¼Œé€šè¿‡æ‰¹é‡æ„ŸçŸ¥èšç±»å’Œå³æ—¶ç”Ÿæˆä½ç§©æ›´æ–°ï¼Œå®ç°LLMçš„æ¨ç†æ—¶é—´é€‚åº”ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¦‚LoRAæˆ–ä¾èµ–äºé¢„å­¦ä¹ ç»Ÿä¸€ï¼ˆå¯æ›´æ¢è’™ç‰ˆï¼‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ ¹æ®èšç±»æ‰¹æ¬¡çš„èšåˆç»Ÿè®¡åŠ¨æ€ç”Ÿæˆè§£ç å™¨æƒé‡çš„è‡ªé€‚åº”ä¿®æ”¹ã€‚ChamaleonLLMé€šè¿‡æ™ºèƒ½åˆ†ç»„ç›¸ä¼¼è¾“å…¥å¹¶é€šè¿‡è¶…ç½‘ç»œè®¡ç®—ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä½ç§©æ›´æ–°ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ä»…è¶…è¶Šäº†ä¼ ç»Ÿçš„LoRAæ–¹æ³•ï¼Œè€Œä¸”æ¶ˆé™¤äº†ç»´æŠ¤å¤šä¸ªä¸“å®¶æ¨¡å‹çš„å¼€é”€ã€‚æˆ‘ä»¬çš„å®éªŒå±•ç¤ºäº†è¯¥æ–¹æ³•çš„é€šç”¨æ€§å’Œé«˜åº¦é€‚åº”æ€§ï¼Œå¯ä½œä¸ºè¯­è¨€æ¨¡å‹æ¨ç†çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ChamaleonLLMå·²å¼€æºï¼Œç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ChamaleonLLMæ¡†æ¶å®ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ—¶é—´é€‚åº”ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ‰¹é‡æ„ŸçŸ¥èšç±»å’Œä½ç§©æ›´æ–°ï¼Œæå‡äº†LLMçš„æ€§èƒ½ã€‚</li>
<li>ChamaleonLLMèƒ½åŠ¨æ€ç”Ÿæˆè§£ç å™¨æƒé‡çš„è‡ªé€‚åº”ä¿®æ”¹ï¼ŒåŸºäºèšç±»æ‰¹æ¬¡çš„èšåˆç»Ÿè®¡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¦‚LoRAæœ‰æ‰€åŒºåˆ«ï¼Œå¹¶è¶…è¶Šäº†é¢„å­¦ä¹ ç»Ÿä¸€ï¼ˆå¯æ›´æ¢è’™ç‰ˆï¼‰çš„æ–¹æ³•ã€‚</li>
<li>ChamaleonLLMé€šè¿‡æ™ºèƒ½åˆ†ç»„ç›¸ä¼¼è¾“å…¥å’Œè®¡ç®—ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ä½ç§©æ›´æ–°ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ChamaleonLLMæ–¹æ³•å…·æœ‰é«˜åº¦çš„é€šç”¨æ€§å’Œé€‚åº”æ€§ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„è¯­è¨€æ¨¡å‹æ¨ç†éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04315">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0cba8f0b2b3e1ef2b9e88461aa515802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6188554556ed2681331b2f432fdb1fa1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ScoreFlow-Mastering-LLM-Agent-Workflows-via-Score-based-Preference-Optimization"><a href="#ScoreFlow-Mastering-LLM-Agent-Workflows-via-Score-based-Preference-Optimization" class="headerlink" title="ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference   Optimization"></a>ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference   Optimization</h2><p><strong>Authors:Yinjie Wang, Ling Yang, Guohao Li, Mengdi Wang, Bryon Aragam</strong></p>
<p>Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ScoreFlow">https://github.com/Gen-Verse/ScoreFlow</a> </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¥è§£å†³å¤æ‚é—®é¢˜ï¼ŒåŒæ—¶åŠªåŠ›å‡å°‘æ„å»ºå®ƒä»¬æ‰€éœ€çš„äººå·¥åŠªåŠ›ï¼Œæ¨åŠ¨è‡ªåŠ¨æ™ºèƒ½ä½“å·¥ä½œæµç¨‹ä¼˜åŒ–æ–¹æ³•çš„å‘å±•ã€‚ç„¶è€Œï¼Œç”±äºè¡¨å¾çš„å±€é™æ€§ã€ç¼ºä¹é€‚åº”æ€§å’Œåœ¨ä¾èµ–ç¦»æ•£ä¼˜åŒ–æŠ€æœ¯æ—¶çš„ç³Ÿç³•å¯æ‰©å±•æ€§ï¼Œç°æœ‰æ–¹æ³•ä»ç„¶ä¸å¤Ÿçµæ´»ã€‚æˆ‘ä»¬é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ¨å‡ºäº†ScoreFlowæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œé«˜æ€§èƒ½çš„æ¡†æ¶ï¼Œåˆ©ç”¨è¿ç»­ç©ºé—´ä¸­çš„é«˜æ•ˆåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ã€‚ScoreFlowç»“åˆäº†Score-DPOï¼Œè¿™æ˜¯ä¸€ç§è€ƒè™‘å®šé‡åé¦ˆçš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•çš„æ–°å˜ç§ã€‚åœ¨æ¶µç›–é—®ç­”ã€ç¼–ç å’Œæ•°å­¦æ¨ç†çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒScoreFlowç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•å®ç°äº†8.2%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½ä½¿å°å‹æ¨¡å‹åœ¨æ¨ç†æˆæœ¬è¾ƒä½çš„æƒ…å†µä¸‹è¶…è¶Šå¤§å‹æ¨¡å‹ã€‚é¡¹ç›®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ScoreFlow">https://github.com/Gen-Verse/ScoreFlow</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04306v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ScoreFlow">https://github.com/Gen-Verse/ScoreFlow</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”¨äºå¤æ‚é—®é¢˜æ±‚è§£ï¼Œå‡å°‘äººå·¥æ„å»ºå·¥ä½œé‡ã€‚ç°æœ‰è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“å·¥ä½œæµç¨‹ä¼˜åŒ–æ–¹æ³•å› è¡¨è¾¾å±€é™ã€ç¼ºä¹é€‚åº”æ€§å’Œç¦»æ•£ä¼˜åŒ–æŠ€æœ¯å¯¼è‡´çš„å¯æ‰©å±•æ€§å·®è€Œæ˜¾å¾—ä¸å¤Ÿçµæ´»ã€‚ScoreFlowæ¡†æ¶é‡‡ç”¨è¿ç»­ç©ºé—´ä¸­çš„é«˜æ•ˆæ¢¯åº¦ä¼˜åŒ–ï¼Œè§£å†³è¿™äº›é—®é¢˜ã€‚ScoreFlowåŒ…å«Score-DPOï¼Œä¸€ç§è€ƒè™‘å®šé‡åé¦ˆçš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•çš„æ–°å˜ç§ã€‚åœ¨æ¶µç›–é—®ç­”ã€ç¼–ç å’Œæ•°å­¦æ¨ç†çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒScoreFlowè¾ƒç°æœ‰åŸºçº¿æé«˜äº†8.2%ã€‚æ­¤å¤–ï¼Œå®ƒä½¿å°å‹æ¨¡å‹èƒ½ä»¥è¾ƒä½çš„æ¨ç†æˆæœ¬è¶…è¶Šå¤§å‹æ¨¡å‹ã€‚é¡¹ç›®ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/ScoreFlow">GitHubåœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¢«ç”¨äºè§£å†³å¤æ‚é—®é¢˜ï¼Œç›®æ ‡æ˜¯å‡å°‘æ„å»ºæ—¶çš„æ‰‹åŠ¨å·¥ä½œé‡ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–æ™ºèƒ½ä½“å·¥ä½œæµç¨‹ä¼˜åŒ–æ–¹æ³•å­˜åœ¨è¡¨è¾¾å±€é™ã€ç¼ºä¹é€‚åº”æ€§å’Œåœ¨ç¦»æ•£ä¼˜åŒ–æŠ€æœ¯ä¸­å¯æ‰©å±•æ€§å·®çš„é—®é¢˜ã€‚</li>
<li>ScoreFlowæ¡†æ¶é‡‡ç”¨è¿ç»­ç©ºé—´ä¸­çš„é«˜æ•ˆæ¢¯åº¦ä¼˜åŒ–æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ScoreFlowåŒ…å«Score-DPOï¼Œè¿™æ˜¯ä¸€ç§è€ƒè™‘å®šé‡åé¦ˆçš„ç›´æ¥åå¥½ä¼˜åŒ–æ–¹æ³•çš„æ–°å˜ç§ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒScoreFlowè¾ƒç°æœ‰æ–¹æ³•æé«˜äº†8.2%çš„æ€§èƒ½ã€‚</li>
<li>ScoreFlowä½¿å°å‹æ¨¡å‹èƒ½å¤Ÿåœ¨è¾ƒä½æ¨ç†æˆæœ¬ä¸‹è¡¨ç°å‡ºè¶…è¶Šå¤§å‹æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7532e3b3870997258a7077ca98bcd3a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-feec3264cc14960818599cc31c4042d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06a6ed7e0a0e1d3f3d46c2c647a8cf3b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beyond-Prompt-Content-Enhancing-LLM-Performance-via-Content-Format-Integrated-Prompt-Optimization"><a href="#Beyond-Prompt-Content-Enhancing-LLM-Performance-via-Content-Format-Integrated-Prompt-Optimization" class="headerlink" title="Beyond Prompt Content: Enhancing LLM Performance via Content-Format   Integrated Prompt Optimization"></a>Beyond Prompt Content: Enhancing LLM Performance via Content-Format   Integrated Prompt Optimization</h2><p><strong>Authors:Yuanye Liu, Jiahang Xu, Li Lyna Zhang, Qi Chen, Xuan Feng, Yang Chen, Zhongxin Guo, Yuqing Yang, Cheng Peng</strong></p>
<p>Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/HenryLau7/CFPO">https://github.com/HenryLau7/CFPO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå…¶ç°å®ä¸–ç•Œçš„æœ‰æ•ˆæ€§é€šå¸¸å—åˆ°æç¤ºè®¾è®¡çš„é©±åŠ¨ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ä¾§é‡äºä¼˜åŒ–æç¤ºå†…å®¹ï¼Œä½†æç¤ºæ ¼å¼çš„ä½œç”¨ä½œä¸ºä¸€ä¸ªå…³é”®ä½†å¸¸è¢«å¿½è§†çš„ç»´åº¦ï¼Œåªå¾—åˆ°äº†æœ‰é™çš„ç³»ç»Ÿç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å†…å®¹æ ¼å¼é›†æˆæç¤ºä¼˜åŒ–ï¼ˆCFPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è¿­ä»£ç»†åŒ–è¿‡ç¨‹è”åˆä¼˜åŒ–æç¤ºå†…å®¹å’Œæ ¼å¼çš„åˆ›æ–°æ–¹æ³•ã€‚CFPOåˆ©ç”¨è‡ªç„¶è¯­è¨€å˜å¼‚æ¥æ¢ç´¢å†…å®¹å˜åŒ–ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€æ ¼å¼æ¢ç´¢ç­–ç•¥æ¥ç³»ç»Ÿè¯„ä¼°ä¸åŒçš„æ ¼å¼é€‰é¡¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡å’Œå¼€æºLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸ä»…ä¼˜åŒ–å†…å®¹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCFPOæ˜¾ç¤ºå‡ºå¯è¡¡é‡çš„æ€§èƒ½æ”¹è¿›ã€‚è¿™å¼ºè°ƒäº†é›†æˆå†…å®¹æ ¼å¼ä¼˜åŒ–çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§æé«˜LLMæ€§èƒ½çš„å®ç”¨ã€æ¨¡å‹æ— å…³çš„æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HenryLau7/CFPO%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HenryLau7/CFPOä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04295v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå…¶å®é™…æ•ˆæœå¾€å¾€ä¾èµ–äºæç¤ºè®¾è®¡ã€‚å°½ç®¡æœ€è¿‘æœ‰ç ”ç©¶ä¼˜åŒ–äº†æç¤ºå†…å®¹ï¼Œä½†æç¤ºæ ¼å¼è¿™ä¸€é‡è¦ä½†å¸¸è¢«å¿½è§†çš„æ–¹é¢å¹¶æœªå¾—åˆ°å……åˆ†ç³»ç»Ÿçš„ç ”ç©¶ã€‚æœ¬æ–‡ä»‹ç»äº†å†…å®¹æ ¼å¼é›†æˆæç¤ºä¼˜åŒ–ï¼ˆCFPOï¼‰è¿™ä¸€åˆ›æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿­ä»£è¿‡ç¨‹å…±åŒä¼˜åŒ–æç¤ºå†…å®¹å’Œæ ¼å¼ã€‚CFPOåˆ©ç”¨è‡ªç„¶è¯­è¨€å˜å¼‚æ¥æ¢ç´¢å†…å®¹å˜åŒ–ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€æ ¼å¼æ¢ç´¢ç­–ç•¥æ¥ç³»ç»Ÿåœ°è¯„ä¼°å„ç§æ ¼å¼é€‰é¡¹ã€‚åœ¨å¤šä¸ªä»»åŠ¡å’Œå¼€æºLLMä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸ä»…ä¼˜åŒ–å†…å®¹çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCFPOåœ¨æ€§èƒ½ä¸Šå–å¾—äº†å¯è¡¡é‡çš„æ”¹è¿›ã€‚è¿™å¼ºè°ƒäº†é›†æˆå†…å®¹æ ¼å¼ä¼˜åŒ–çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ç§æé«˜LLMæ€§èƒ½çš„å®ç”¨ä¸”æ¨¡å‹æ— å…³çš„æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>LLMçš„å®é™…æ•ˆæœå–å†³äºæç¤ºè®¾è®¡ï¼ŒåŒ…æ‹¬æç¤ºå†…å®¹å’Œæ ¼å¼ã€‚</li>
<li>å†…å®¹æ ¼å¼é›†æˆæç¤ºä¼˜åŒ–ï¼ˆCFPOï¼‰æ˜¯ä¸€ç§åŒæ—¶ä¼˜åŒ–æç¤ºå†…å®¹å’Œæ ¼å¼çš„æ–¹æ³•ã€‚</li>
<li>CFPOé€šè¿‡è‡ªç„¶è¯­è¨€å˜å¼‚å’ŒåŠ¨æ€æ ¼å¼æ¢ç´¢ç­–ç•¥è¿›è¡Œå·¥ä½œã€‚</li>
<li>å¹¿æ³›çš„è¯„ä¼°è¡¨æ˜ï¼ŒCFPOåœ¨å¤šä¸ªä»»åŠ¡å’Œå¼€æºLLMä¸Šå®ç°äº†æ¯”ä»…ä¼˜åŒ–å†…å®¹æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>CFPOçš„é‡è¦æ€§åœ¨äºé›†æˆäº†å†…å®¹æ ¼å¼çš„ä¼˜åŒ–ã€‚</li>
<li>CFPOæä¾›äº†ä¸€ç§æé«˜LLMæ€§èƒ½çš„å®ç”¨ä¸”æ¨¡å‹æ— å…³çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee22e8f5e5421a262fc35af7ce53b725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8fdf20bfac1f66eab4f59854b9dd8de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fc0df7338e6ad1f2c57e7caf0d0403d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd97884f536adbb4ef63efd7914a3e52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b66b967f8d4775dcaf0da3f83bfcf65.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Keep-It-Light-Simplifying-Image-Clustering-Via-Text-Free-Adapters"><a href="#Keep-It-Light-Simplifying-Image-Clustering-Via-Text-Free-Adapters" class="headerlink" title="Keep It Light! Simplifying Image Clustering Via Text-Free Adapters"></a>Keep It Light! Simplifying Image Clustering Via Text-Free Adapters</h2><p><strong>Authors:Yicen Li, Haitz SÃ¡ez de OcÃ¡riz Borde, Anastasis Kratsios, Paul D. McNicholas</strong></p>
<p>Many competitive clustering pipelines have a multi-modal design, leveraging large language models (LLMs) or other text encoders, and text-image pairs, which are often unavailable in real-world downstream applications. Additionally, such frameworks are generally complicated to train and require substantial computational resources, making widespread adoption challenging. In this work, we show that in deep clustering, competitive performance with more complex state-of-the-art methods can be achieved using a text-free and highly simplified training pipeline. In particular, our approach, Simple Clustering via Pre-trained models (SCP), trains only a small cluster head while leveraging pre-trained vision model feature representations and positive data pairs. Experiments on benchmark datasets including CIFAR-10, CIFAR-20, CIFAR-100, STL-10, ImageNet-10, and ImageNet-Dogs, demonstrate that SCP achieves highly competitive performance. Furthermore, we provide a theoretical result explaining why, at least under ideal conditions, additional text-based embeddings may not be necessary to achieve strong clustering performance in vision. </p>
<blockquote>
<p>è®¸å¤šå…·æœ‰ç«äº‰åŠ›çš„èšç±»æµç¨‹å…·æœ‰å¤šæ¨¡æ€è®¾è®¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å…¶ä»–æ–‡æœ¬ç¼–ç å™¨ä»¥åŠæ–‡æœ¬å›¾åƒå¯¹ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ä¸‹æ¸¸åº”ç”¨ä¸­å¾€å¾€æ— æ³•è·å¾—è¿™äº›èµ„æºã€‚æ­¤å¤–ï¼Œæ­¤ç±»æ¡†æ¶é€šå¸¸è®­ç»ƒå¤æ‚ä¸”éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä½¿å¾—éš¾ä»¥å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æ·±åº¦èšç±»ä¸­ï¼Œä½¿ç”¨æ— æ–‡æœ¬å’Œé«˜åº¦ç®€åŒ–çš„è®­ç»ƒæµç¨‹å³å¯å®ç°ä¸æ›´å¤æ‚çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬çš„æ–¹æ³•â€”â€”é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç®€å•èšç±»ï¼ˆSCPï¼‰ï¼Œåªè®­ç»ƒä¸€ä¸ªå°å‹çš„èšç±»å¤´ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ç‰¹å¾è¡¨ç¤ºå’Œæ­£æ•°æ®å¯¹ã€‚åœ¨åŒ…æ‹¬CIFAR-10ã€CIFAR-20ã€CIFAR-100ã€STL-10ã€ImageNet-10å’ŒImageNet-Dogsç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSCPå®ç°äº†é«˜åº¦ç«äº‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»ç†è®ºä¸Šç»™å‡ºäº†ç»“æœï¼Œè§£é‡Šäº†è‡³å°‘åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œä¸ºäº†å®ç°è§†è§‰èšç±»çš„å¼ºå¤§æ€§èƒ½ï¼Œä¸ºä½•å¯èƒ½ä¸éœ€è¦é¢å¤–çš„æ–‡æœ¬åµŒå…¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04226v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„ç®€åŒ–èšç±»æ–¹æ³•ï¼ˆSCPï¼‰ï¼Œè¯¥æ–¹æ³•æ— éœ€ä½¿ç”¨æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®ï¼Œä»…é€šè¿‡è®­ç»ƒå°å‹é›†ç¾¤å¤´ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ç‰¹å¾è¡¨ç¤ºå’Œæ­£æ•°æ®å¯¹å³å¯å®ç°ä¸å¤æ‚çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„ç«äº‰æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ…æ‹¬CIFAR-10ã€CIFAR-20ã€CIFAR-100ã€STL-10ã€ImageNet-10å’ŒImageNet-Dogsç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†é«˜åº¦ç«äº‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ç†è®ºç»“æœï¼Œè§£é‡Šäº†è‡³å°‘åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œå®ç°è§†è§‰å¼ºèšç±»æ€§èƒ½æ—¶ï¼Œä¸å¿…ä½¿ç”¨é¢å¤–çš„æ–‡æœ¬åµŒå…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç®€åŒ–èšç±»æ–¹æ³•ï¼ˆSCPï¼‰æ— éœ€ä½¿ç”¨æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®ï¼Œä»…é€šè¿‡è®­ç»ƒå°å‹é›†ç¾¤å¤´å³å¯å®ç°é«˜åº¦ç«äº‰çš„æ€§èƒ½ã€‚</li>
<li>SCPæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹ç‰¹å¾è¡¨ç¤ºå’Œæ­£æ•°æ®å¯¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSCPåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ä¸å¤æ‚çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„ç«äº‰æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ä¸€ç§ç†è®ºè§£é‡Šï¼Œè¯´æ˜åœ¨ç†æƒ³æ¡ä»¶ä¸‹ï¼Œå®ç°è§†è§‰å¼ºèšç±»æ€§èƒ½æ—¶ï¼Œä¸å¿…ä¾èµ–é¢å¤–çš„æ–‡æœ¬åµŒå…¥ã€‚</li>
<li>SCPæ–¹æ³•ç®€åŒ–äº†å¤šæ¨¡æ€è®¾è®¡çš„å¤æ‚æ€§ï¼Œå¹¶é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ï¼Œæœ‰åŠ©äºæ›´å¹¿æ³›çš„é‡‡ç”¨ã€‚</li>
<li>SCPæ–¹æ³•ä¸ºæ·±åº¦èšç±»æä¾›äº†ä¸€ç§æ–°çš„ã€é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a85b633dd1bfe2d662bedbe8c305c80b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69d80f2625fcb946ece1f477f844d58e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc9b20d0c3decffc71c71139c63bb39a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e5f9d095551a662d75457ef85817720.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Eclair-â€“-Extracting-Content-and-Layout-with-Integrated-Reading-Order-for-Documents"><a href="#Eclair-â€“-Extracting-Content-and-Layout-with-Integrated-Reading-Order-for-Documents" class="headerlink" title="Ã‰clair â€“ Extracting Content and Layout with Integrated Reading Order   for Documents"></a>Ã‰clair â€“ Extracting Content and Layout with Integrated Reading Order   for Documents</h2><p><strong>Authors:Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno SeppÃ¤nen, Jupinder Parmar, Joseph Jennings, Andrew Tao, Karan Sapra</strong></p>
<p>Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure â€“ including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages â€“ as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce &#39;Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, &#39;Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. &#39;Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate &#39;Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards. </p>
<blockquote>
<p>å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯å¹¿æ³›åº”ç”¨äºä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬ï¼Œä¿ƒè¿›äº†é«˜æ•ˆçš„æ•°å­—åŒ–å’Œæ£€ç´¢ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å¤æ‚æ–‡æ¡£æ—¶ï¼Œä»…ä»…æå–æ–‡æœ¬æ˜¯ä¸å¤Ÿçš„ã€‚è¦å®Œå…¨ç†è§£è¿™äº›æ–‡æ¡£ï¼Œéœ€è¦äº†è§£å®ƒä»¬çš„ç»“æ„ï¼ŒåŒ…æ‹¬æ ¼å¼ã€å…¬å¼ã€è¡¨æ ¼ä»¥åŠè·¨å¤šé¡µçš„å¤šä¸ªå—å’Œåˆ—çš„æ–‡æœ¬é˜…è¯»é¡ºåºç­‰ï¼Œè¿˜éœ€è¦æ£€æµ‹è„šæ³¨å’Œå›¾åƒæ ‡é¢˜ç­‰å…ƒç´ çš„è¯­ä¹‰ä¿¡æ¯ã€‚è¿™ç§å…¨é¢çš„ç†è§£å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚æ£€ç´¢ã€æ–‡æ¡£é—®ç­”ä»¥åŠè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ•°æ®æ•´ç†ç­‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºâ€Eclairâ€çš„é€šç”¨æ–‡æœ¬æå–å·¥å…·ï¼Œå®ƒä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†å„ç§ç±»å‹çš„æ–‡æ¡£ã€‚ç»™å®šä¸€å¼ å›¾åƒï¼Œâ€Eclairâ€èƒ½å¤ŸæŒ‰é˜…è¯»é¡ºåºæå–æ ¼å¼åŒ–æ–‡æœ¬ï¼Œå¹¶æä¾›è¾¹ç•Œæ¡†åŠå…¶ç›¸åº”çš„è¯­ä¹‰ç±»åˆ«ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›æ–°é¢–çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ ·åŒ–çš„äººå·¥æ³¨é‡ŠåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºæ–‡æ¡£çº§åˆ«çš„OCRå’Œè¯­ä¹‰åˆ†ç±»ã€‚â€Eclairâ€åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾åº¦æ°´å¹³ï¼Œåœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•é›†ä¸Šå¯¹â€Eclairâ€è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å®ƒåœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸‹çš„é€šç”¨æ€§å’Œä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04223v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰æŠ€æœ¯å¹¿æ³›åº”ç”¨äºä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬ï¼Œä¿ƒè¿›æ•°å­—åŒ–å’Œæ£€ç´¢æ•ˆç‡çš„æå‡ã€‚ä½†å•çº¯æå–æ–‡æœ¬å¯¹äºå¤æ‚æ–‡æ¡£æ˜¯ä¸å¤Ÿçš„ã€‚å…¨é¢ç†è§£æ–‡æ¡£éœ€è¦ç†è§£å…¶ç»“æ„ï¼ŒåŒ…æ‹¬æ ¼å¼ã€å…¬å¼ã€è¡¨æ ¼ä»¥åŠè·¨å¤šé¡µçš„å¤šä¸ªå—å’Œåˆ—çš„é˜…è¯»é¡ºåºç­‰ï¼Œå¹¶æ£€æµ‹è„šæ³¨å’Œå›¾åƒæ ‡é¢˜ç­‰å…ƒç´ ä¹Ÿéœ€è¦è¯­ä¹‰ä¿¡æ¯ã€‚è¿™ç§å…¨é¢çš„ç†è§£å¯¹äºä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚æ£€ç´¢ã€æ–‡æ¡£é—®ç­”å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ•°æ®æ•´ç†ç­‰ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šç”¨æ–‡æœ¬æå–å·¥å…·Eclairï¼Œä¸“ä¸ºå¤„ç†å„ç§æ–‡æ¡£ç±»å‹è€Œè®¾è®¡ã€‚ç»™å®šä¸€å¼ å›¾ç‰‡ï¼ŒEclairèƒ½å¤Ÿä»¥é˜…è¯»é¡ºåºæå–æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼Œå¹¶å¸¦æœ‰è¾¹ç•Œæ¡†å’Œç›¸åº”çš„è¯­ä¹‰ç±»åˆ«ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›æ–°åŠŸèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ ·åŒ–çš„äººæ ‡æ³¨æ–‡æ¡£çº§OCRå’Œè¯­ä¹‰åˆ†ç±»åŸºå‡†æµ‹è¯•ã€‚Eclairåœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ—¢å®šçš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹Eclairè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸‹çš„é€šç”¨æ€§å’Œå®åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OCRæŠ€æœ¯å¹¿æ³›åº”ç”¨äºä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬ï¼Œä½†ä»…æå–æ–‡æœ¬ä¸è¶³ä»¥ç†è§£å¤æ‚æ–‡æ¡£çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>Eclairæ˜¯ä¸€ç§é€šç”¨æ–‡æœ¬æå–å·¥å…·ï¼Œæ—¨åœ¨å¤„ç†å„ç§ç±»å‹çš„æ–‡æ¡£ã€‚</li>
<li>Eclairèƒ½å¤Ÿä»å›¾åƒä¸­æå–æ ¼å¼åŒ–çš„æ–‡æœ¬å¹¶ä»¥é˜…è¯»é¡ºåºå‘ˆç°ã€‚</li>
<li>Eclairèƒ½å¤Ÿè¯†åˆ«æ–‡æ¡£çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¦‚è„šæ³¨å’Œå›¾åƒæ ‡é¢˜ç­‰ã€‚</li>
<li>Eclairåœ¨å¤šæ ·åŒ–çš„æ–‡æ¡£çº§OCRå’Œè¯­ä¹‰åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>Eclairåœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–OCRæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c178113996210bcc10337fa244c2c1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-850bcf607797d29bedc36decd0678a88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce93083325846c8d553689bac439ec3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ad36589bb3ef8776df14bc3f8fb45fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5125c37cb10142525a694dd953ff5b58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-135dd90bd50fd5fbd0ee3ac54c894078.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="â€œShort-lengthâ€-Adversarial-Training-Helps-LLMs-Defend-â€œLong-lengthâ€-Jailbreak-Attacks-Theoretical-and-Empirical-Evidence"><a href="#â€œShort-lengthâ€-Adversarial-Training-Helps-LLMs-Defend-â€œLong-lengthâ€-Jailbreak-Attacks-Theoretical-and-Empirical-Evidence" class="headerlink" title="â€œShort-lengthâ€ Adversarial Training Helps LLMs Defend â€œLong-lengthâ€   Jailbreak Attacks: Theoretical and Empirical Evidence"></a>â€œShort-lengthâ€ Adversarial Training Helps LLMs Defend â€œLong-lengthâ€   Jailbreak Attacks: Theoretical and Empirical Evidence</h2><p><strong>Authors:Shaopeng Fu, Liang Ding, Di Wang</strong></p>
<p>Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts. To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, i.e., training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks. During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs. This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $\Theta(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $\Theta(\sqrt{M})$. Theoretically, we analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers. The bound depends on the term $\Theta(\sqrt{M_{\text{test}}}&#x2F;M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the number of adversarially perturbed in-context samples during training and testing. Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths. Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix during jailbreaking to the length during AT. Our findings show that it is practical to defend â€œlong-lengthâ€ jailbreak attacks via efficient â€œshort-lengthâ€ AT. The code is available at <a target="_blank" rel="noopener" href="https://github.com/fshp971/adv-icl">https://github.com/fshp971/adv-icl</a>. </p>
<blockquote>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¶Šç‹±æ”»å‡»æ—¨åœ¨é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ€§æç¤ºæ¥è¯±å¯¼LLMäº§ç”Ÿæœ‰å®³è¡Œä¸ºã€‚ä¸ºäº†ç¼“è§£æ”»å‡»ï¼Œä¸€ç§æ–¹æ³•æ˜¯è¿›è¡ŒåŸºäºå¯¹æŠ—æ€§è®­ç»ƒï¼ˆATï¼‰çš„å¯¹é½ï¼Œå³è®­ç»ƒLLMå¯¹ä¸€äº›æœ€å…·å¯¹æŠ—æ€§çš„æç¤ºï¼Œä»¥å¸®åŠ©ä»–ä»¬å­¦ä¹ å¦‚ä½•åœ¨æ”»å‡»ä¸‹å®‰å…¨åœ°è¡Œä¸ºã€‚åœ¨ATæœŸé—´ï¼Œå¯¹æŠ—æ€§æç¤ºçš„é•¿åº¦å¯¹äºå¯¹é½LLMçš„ç¨³å¥æ€§èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬æ–‡ä¸“æ³¨äºå¯¹æŠ—æ€§åç¼€è¶Šç‹±æ”»å‡»ï¼Œå¹¶æ­ç¤ºä¸ºäº†é˜²å¾¡å…·æœ‰é•¿åº¦ä¸ºÎ˜ï¼ˆMï¼‰çš„å¯¹æŠ—æ€§åç¼€çš„è¶Šç‹±æ”»å‡»ï¼Œåªéœ€åœ¨å¯¹æç¤ºè¿›è¡Œå¯¹é½æ—¶ï¼Œä½¿ç”¨é•¿åº¦ä¸ºÎ˜ï¼ˆâˆšMï¼‰çš„å¯¹æŠ—æ€§åç¼€å°±è¶³å¤Ÿäº†ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬åˆ†æäº†çº¿æ€§å›å½’ä»»åŠ¡ä¸­çº¿æ€§å˜æ¢å™¨çš„å¯¹æŠ—æ€§ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶ä¸ºè®­ç»ƒè¿‡çš„å˜æ¢å™¨è¯æ˜äº†ç¨³å¥çš„æ³›åŒ–ç•Œé™ã€‚ç•Œé™å–å†³äºé¡¹Î˜ï¼ˆâˆšMæµ‹è¯•&#x2F;Mè®­ç»ƒï¼‰ï¼Œå…¶ä¸­Mè®­ç»ƒ å’Œ Mæµ‹è¯• æ˜¯è®­ç»ƒå’Œæµ‹è¯•æœŸé—´çš„å¯¹æŠ—æ€§æ‰°åŠ¨ä¸Šä¸‹æ–‡æ ·æœ¬çš„æ•°é‡ã€‚ä»ç»éªŒä¸Šè®²ï¼Œæˆ‘ä»¬å¯¹æµè¡Œçš„å¼€æºLLMè¿›è¡Œäº†ATï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹æŠ—ä¸åŒå¯¹æŠ—æ€§åç¼€é•¿åº¦çš„è¶Šç‹±æ”»å‡»çš„ç¨³å¥æ€§ã€‚ç»“æœè¯å®ï¼Œè¶Šç‹±æ”»å‡»çš„æˆåŠŸç‡ä¸è¶Šç‹±æ—¶çš„å¯¹æŠ—æ€§åç¼€é•¿åº¦å¹³æ–¹æ ¹ä¸ATæœŸé—´é•¿åº¦çš„æ¯”ç‡ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æœ‰æ•ˆçš„â€œçŸ­é•¿åº¦â€ATæ¥é˜²å¾¡â€œé•¿é•¿åº¦â€è¶Šç‹±æ”»å‡»æ˜¯åˆ‡å®å¯è¡Œçš„ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/fshp971/adv-icl%E3%80%82">https://github.com/fshp971/adv-iclã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04204v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Jailbreakæ”»å‡»é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ€§æç¤ºæ¥è¯±å¯¼LLMäº§ç”Ÿæœ‰å®³è¡Œä¸ºã€‚ä¸ºäº†ç¼“è§£æ”»å‡»ï¼Œä¸€ç§æ–¹æ³•æ˜¯è¿›è¡ŒåŸºäºå¯¹æŠ—è®­ç»ƒï¼ˆATï¼‰çš„å¯¹é½ã€‚æœ¬æ–‡ä¸“æ³¨äºå¯¹æŠ—æ€§åç¼€Jailbreakæ”»å‡»ï¼Œå¹¶æ­ç¤ºäº†å¯¹æŠ—åç¼€çš„é•¿åº¦åœ¨é˜²å¾¡Jailbreakæ”»å‡»ä¸­çš„é‡è¦æ€§ã€‚ç†è®ºä¸Šï¼Œæœ¬æ–‡åˆ†æäº†çº¿æ€§å›å½’ä»»åŠ¡ä¸­çº¿æ€§å˜å‹å™¨çš„å¯¹æŠ—æ€§ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶ä¸ºè®­ç»ƒè¿‡çš„å˜å‹å™¨è¯æ˜äº†ç¨³å¥çš„æ³›åŒ–è¾¹ç•Œã€‚å®è¯æ–¹é¢ï¼Œæˆ‘ä»¬å¯¹æµè¡Œçš„å¼€æºLLMè¿›è¡Œäº†ATï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹æŠ—ä¸åŒå¯¹æŠ—æ€§åç¼€é•¿åº¦çš„Jailbreakæ”»å‡»çš„ç¨³å¥æ€§ã€‚ç»“æœè¯å®ï¼Œæ”»å‡»æˆåŠŸç‡ä¸å¯¹æŠ—è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å¯¹æŠ—æ€§åç¼€é•¿åº¦çš„å¹³æ–¹æ ¹ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡é«˜æ•ˆçš„â€œçŸ­é•¿åº¦â€ATæ¥é˜²å¾¡â€œé•¿é•¿åº¦â€çš„Jailbreakæ”»å‡»æ˜¯åˆ‡å®å¯è¡Œçš„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Jailbreakæ”»å‡»æ—¨åœ¨é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å¯¹æŠ—æ€§æç¤ºè¯±å¯¼LLMäº§ç”Ÿæœ‰å®³è¡Œä¸ºã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒï¼ˆATï¼‰æ˜¯ä¸€ç§ç¼“è§£Jailbreakæ”»å‡»çš„æ–¹æ³•ã€‚</li>
<li>å¯¹æŠ—æ€§åç¼€çš„é•¿åº¦åœ¨é˜²å¾¡Jailbreakæ”»å‡»ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>ç†è®ºä¸Šï¼Œæœ¬æ–‡åˆ†æäº†çº¿æ€§å›å½’ä»»åŠ¡ä¸­çº¿æ€§å˜å‹å™¨çš„å¯¹æŠ—æ€§ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ”»å‡»æˆåŠŸç‡ä¸å¯¹æŠ—è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å¯¹æŠ—æ€§åç¼€é•¿åº¦çš„å¹³æ–¹æ ¹ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚</li>
<li>é€šè¿‡é«˜æ•ˆçš„â€œçŸ­é•¿åº¦â€ATæ¥é˜²å¾¡â€œé•¿é•¿åº¦â€çš„Jailbreakæ”»å‡»æ˜¯å¯è¡Œçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04204">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2bf916fd17646eeb3f981c71c9e1f1f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="The-Best-Instruction-Tuning-Data-are-Those-That-Fit"><a href="#The-Best-Instruction-Tuning-Data-are-Those-That-Fit" class="headerlink" title="The Best Instruction-Tuning Data are Those That Fit"></a>The Best Instruction-Tuning Data are Those That Fit</h2><p><strong>Authors:Dylan Zhang, Qirun Dai, Hao Peng</strong></p>
<p>High-quality supervised fine-tuning (SFT) data are crucial for eliciting strong capabilities from pretrained large language models (LLMs). Typically, instructions are paired with multiple responses sampled from other LLMs, which are often out of the distribution of the target model to be fine-tuned. This, at scale, can lead to diminishing returns and even hurt the modelsâ€™ performance and robustness. We propose <strong>GRAPE</strong>, a novel SFT framework that accounts for the unique characteristics of the target model. For each instruction, it gathers responses from various LLMs and selects the one with the highest probability measured by the target model, indicating that it aligns most closely with the target modelâ€™s pretrained distribution; it then proceeds with standard SFT training.   We first evaluate GRAPE with a controlled experiment, where we sample various solutions for each question in UltraInteract from multiple models and fine-tune commonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on GRAPE-selected data. GRAPE significantly outperforms strong baselines, including distilling from the strongest model with an absolute gain of up to 13.8%, averaged across benchmarks, and training on 3x more data with a maximum performance improvement of 17.3%. GRAPEâ€™s strong performance generalizes to realistic settings. We experiment with the post-training data used for Tulu3 and Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data by 6.1% and a state-of-the-art data selection approach by 3% on average performance. Remarkably, using 1&#x2F;3 of the data and half the number of epochs, GRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%. </p>
<blockquote>
<p>é«˜è´¨é‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®å¯¹äºæ¿€å‘é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºå¤§èƒ½åŠ›è‡³å…³é‡è¦ã€‚é€šå¸¸ï¼ŒæŒ‡ä»¤ä¼šä¸ä»å…¶ä»–LLMé‡‡æ ·çš„å¤šä¸ªå“åº”é…å¯¹ï¼Œè¿™äº›å“åº”å¾€å¾€è¶…å‡ºäº†è¦å¾®è°ƒçš„ç›®æ ‡æ¨¡å‹çš„åˆ†å¸ƒã€‚è¿™åœ¨å¤§è§„æ¨¡æƒ…å†µä¸‹å¯èƒ½ä¼šå¯¼è‡´æ”¶ç›Šé€’å‡ï¼Œç”šè‡³ä¼šæŸå®³æ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹SFTæ¡†æ¶<strong>GRAPE</strong>ï¼Œå®ƒè€ƒè™‘äº†ç›®æ ‡æ¨¡å‹çš„ç‹¬ç‰¹ç‰¹æ€§ã€‚å¯¹äºæ¯ä¸ªæŒ‡ä»¤ï¼Œå®ƒä»å„ç§LLMä¸­æ”¶é›†å“åº”ï¼Œå¹¶é€‰æ‹©ç›®æ ‡æ¨¡å‹æµ‹é‡æ¦‚ç‡æœ€é«˜çš„ä¸€ä¸ªï¼Œè¿™è¡¨æ˜å®ƒä¸ç›®æ ‡æ¨¡å‹çš„é¢„è®­ç»ƒåˆ†å¸ƒæœ€å¯†åˆ‡å¯¹é½ï¼›ç„¶åå®ƒç»§ç»­è¿›è¡Œæ ‡å‡†SFTè®­ç»ƒã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡æ§åˆ¶å®éªŒè¯„ä¼°GRAPEï¼Œåœ¨è¯¥å®éªŒä¸­ï¼Œæˆ‘ä»¬ä»å¤šä¸ªæ¨¡å‹ä¸­å¯¹UltraInteractä¸­çš„æ¯ä¸ªé—®é¢˜é‡‡æ ·å„ç§è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä½¿ç”¨GRAPEé€‰å®šçš„æ•°æ®å¯¹å¸¸ç”¨çš„LMï¼ˆå¦‚LLaMA3.1-8Bã€Mistral-7Bå’ŒQwen2.5-7Bï¼‰è¿›è¡Œå¾®è°ƒã€‚GRAPEæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹ï¼ŒåŒ…æ‹¬ä»æœ€å¼ºæ¨¡å‹è’¸é¦çš„ç»å¯¹å¢ç›Šé«˜è¾¾13.8%ï¼Œå¹³å‡è·¨åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠåœ¨æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•å¤šä¸‰å€çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½æœ€é«˜æé«˜äº†17.3%ã€‚GRAPEçš„å¼ºå¤§æ€§èƒ½å¯æ¨å¹¿åˆ°ç°å®åœºæ™¯ã€‚æˆ‘ä»¬å¯¹ç”¨äºTulu3å’ŒOlmo-2çš„åæœŸè®­ç»ƒæ•°æ®è¿›è¡Œäº†å®éªŒã€‚GRAPEåœ¨æ¯”ç°æœ‰æœ€ä½³æ–¹æ³•å¤š4.5å€çš„æ•°æ®ä¸Šè®­ç»ƒçš„åŸºçº¿æ¨¡å‹ä¸Šé«˜å‡º6.1%ï¼Œå¹¶ä¸”åœ¨å¹³å‡æ€§èƒ½ä¸Šæ¯”æœ€å…ˆè¿›çš„æ•°æ®é€‰æ‹©æ–¹æ³•é«˜å‡º3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ä¸‰åˆ†ä¹‹ä¸€çš„æ•°æ®å’Œå‡å°‘ä¸€åŠçš„è®­ç»ƒå‘¨æœŸï¼ŒGRAPEä½¿LLaMA3.1-8Bçš„æ€§èƒ½è¶…è¿‡äº†Tulu3-SFTçš„3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé«˜è´¨é‡ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰æ•°æ®å¯¹äºæ¿€å‘æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›è‡³å…³é‡è¦ã€‚é€šå¸¸ï¼ŒæŒ‡ä»¤ä¸å…¶ä»–LLMç”Ÿæˆçš„å¤šä¸ªå“åº”é…å¯¹ï¼Œè¿™äº›å“åº”å¾€å¾€è¶…å‡ºç›®æ ‡æ¨¡å‹çš„åˆ†å¸ƒèŒƒå›´ï¼Œå¤§è§„æ¨¡æƒ…å†µä¸‹å¯èƒ½å¯¼è‡´æ”¶ç›Šé€’å‡ç”šè‡³æŸå®³æ¨¡å‹æ€§èƒ½å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹SFTæ¡†æ¶GRAPEï¼Œå®ƒè€ƒè™‘äº†ç›®æ ‡æ¨¡å‹çš„ç‹¬ç‰¹ç‰¹æ€§ã€‚å¯¹äºæ¯ä¸ªæŒ‡ä»¤ï¼Œå®ƒä»å„ç§LLMä¸­æ”¶é›†å“åº”ï¼Œå¹¶é€‰æ‹©ç›®æ ‡æ¨¡å‹æµ‹é‡çš„æ¦‚ç‡æœ€é«˜çš„ä¸€ä¸ªï¼Œè¡¨æ˜å®ƒä¸ç›®æ ‡æ¨¡å‹çš„é¢„è®­ç»ƒåˆ†å¸ƒæœ€æ¥è¿‘ï¼›ç„¶åè¿›è¡Œæ ‡å‡†SFTè®­ç»ƒã€‚GRAPEåœ¨æ§åˆ¶å®éªŒä¸­çš„è¡¨ç°æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œåœ¨é€šç”¨æ¨¡å‹å¦‚LLaMA3.1-8Bã€Mistral-7Bå’ŒQwen2.5-7Bä¸Šè¿›è¡ŒGRAPEé€‰å®šçš„æ•°æ®å¾®è°ƒã€‚GRAPEçš„å¹³å‡æ€§èƒ½æé«˜å¹…åº¦é«˜è¾¾13.8%ï¼Œå¹¶ä¸”åœ¨ç°å®ç¯å¢ƒä¸­ä¹Ÿå…·æœ‰å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ã€‚åœ¨ä¸Tulu3å’ŒOlmo-2çš„åæœŸè®­ç»ƒæ•°æ®å®éªŒä¸­ï¼ŒGRAPEè¡¨ç°å‡ºè‰²ï¼Œå¹³å‡æ€§èƒ½ä¼˜äºä½¿ç”¨æ›´å¤šæ•°æ®çš„å¼ºåŸºçº¿6.1%ï¼Œå¹¶ä¼˜äºæœ€æ–°æ•°æ®é€‰æ‹©æ–¹æ³•å¹³å‡æ€§èƒ½3%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨ä¸‰åˆ†ä¹‹ä¸€çš„æ•°æ®å’Œä¸€åŠçš„è®­ç»ƒå‘¨æœŸï¼ŒGRAPEä½¿LLaMA3.1-8Bçš„æ€§èƒ½è¶…è¿‡äº†Tulu3-SFTçš„3.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡çš„ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰æ•°æ®å¯¹äºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿçš„SFTæ–¹æ³•å¯èƒ½åœ¨å¤§è§„æ¨¡åº”ç”¨æ—¶å¯¼è‡´æ”¶ç›Šé€’å‡å’Œæ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>GRAPEæ¡†æ¶è€ƒè™‘ç›®æ ‡æ¨¡å‹çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œé€šè¿‡é€‰æ‹©ä¸ç›®æ ‡æ¨¡å‹é¢„è®­ç»ƒåˆ†å¸ƒæœ€æ¥è¿‘çš„å“åº”æ¥è¿›è¡ŒSFTã€‚</li>
<li>GRAPEåœ¨æ§åˆ¶å®éªŒä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹³å‡æ€§èƒ½æå‡é«˜è¾¾13.8%ã€‚</li>
<li>GRAPEåœ¨ç°å®ç¯å¢ƒå’Œä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸ä½¿ç”¨æ›´å¤šæ•°æ®å’Œæ›´é•¿æ—¶é—´è®­ç»ƒçš„å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒGRAPEä½¿ç”¨æ›´å°‘çš„æ•°æ®å’Œè®­ç»ƒå‘¨æœŸå®ç°äº†æ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a07efacb8deb5c3753163a995d0302f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-420d90b287f86944645d92cd1f9c931f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc43e72d841d653fb33f385663a143d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-074717c906782769d17772e4d7b8891d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PixFoundation-Are-We-Heading-in-the-Right-Direction-with-Pixel-level-Vision-Foundation-Models"><a href="#PixFoundation-Are-We-Heading-in-the-Right-Direction-with-Pixel-level-Vision-Foundation-Models" class="headerlink" title="PixFoundation: Are We Heading in the Right Direction with Pixel-level   Vision Foundation Models?"></a>PixFoundation: Are We Heading in the Right Direction with Pixel-level   Vision Foundation Models?</h2><p><strong>Authors:Mennatullah Siam</strong></p>
<p>Multiple works have emerged to push the boundaries on multi-modal large language models (MLLMs) towards pixel-level understanding. Such approaches have shown strong performance on benchmarks for referring expression segmentation and grounded conversation generation. The current trend in pixel-level MLLMs is to train with pixel-level grounding supervision on large-scale labelled data. However, we show that such MLLMs when evaluated on recent challenging vision centric benchmarks, exhibit a weak ability in visual question answering. Surprisingly, some of these methods even downgrade the grounding ability of MLLMs that were never trained with such supervision. In this work, we propose two novel challenging benchmarks and show that MLLMs without pixel-level grounding supervision can outperform the state of the art in such tasks when evaluating both the pixel-level grounding and visual question answering. We propose simple baselines to extract the grounding information that can be plugged into any MLLM, which we call as PixFoundation. More importantly, we study the research question of &#96;&#96;When does grounding emerge in MLLMs that are not trained with pixel-level grounding supervision?â€™â€™ We show that grounding can coincide with object parts or location&#x2F;appearance information. Code repository is at <a target="_blank" rel="noopener" href="https://github.com/MSiam/PixFoundation/">https://github.com/MSiam/PixFoundation/</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¾¹ç•Œæ¨åŠ¨ä½œå“ä¸æ–­å‡ºç°ï¼Œå®ƒä»¬æœç€åƒç´ çº§ç†è§£çš„æ–¹å‘å‘å±•ã€‚è¿™ç±»æ–¹æ³•åœ¨å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²å’ŒåŸºäºåœºæ™¯å¯¹è¯ç”Ÿæˆç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç›®å‰åƒç´ çº§MLLMsçš„è¶‹åŠ¿æ˜¯åœ¨å¤§è§„æ¨¡æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œåƒç´ çº§åœ°é¢ç›‘ç£è®­ç»ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨æœ€è¿‘çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°è¿™äº›MLLMsæ—¶ï¼Œå‘ç°å®ƒä»¬åœ¨è§†è§‰é—®ç­”æ–¹é¢çš„èƒ½åŠ›è¾ƒå¼±ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä¸€äº›æ–¹æ³•ç”šè‡³é™ä½äº†ä»æœªæ¥å—è¿‡æ­¤ç±»ç›‘ç£çš„MLLMsçš„æ¥åœ°èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªæ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜åœ¨æ²¡æœ‰åƒç´ çº§åœ°é¢ç›‘ç£çš„MLLMså¯ä»¥åœ¨è¯„ä¼°åƒç´ çº§æ¥åœ°å’Œè§†è§‰é—®ç­”æ—¶è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„åŸºçº¿ï¼Œå¯ä»¥æå–æ¥åœ°ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ’å…¥ä»»ä½•MLLMä¸­ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºPixFoundationã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç ”ç©¶é—®é¢˜ï¼šâ€œåœ¨æ²¡æœ‰è¿›è¡Œåƒç´ çº§åœ°é¢ç›‘ç£çš„æƒ…å†µä¸‹ï¼ŒMLLMsä¸­çš„æ¥åœ°ä½•æ—¶å‡ºç°ï¼Ÿâ€æˆ‘ä»¬è¡¨æ˜ï¼Œæ¥åœ°å¯ä»¥ä¸å¯¹è±¡éƒ¨åˆ†æˆ–ä½ç½®&#x2F;å¤–è§‚ä¿¡æ¯ç›¸ç¬¦ã€‚ä»£ç ä»“åº“ä½äº<a target="_blank" rel="noopener" href="https://github.com/MSiam/PixFoundation/%E3%80%82">https://github.com/MSiam/PixFoundation/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04192v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åƒç´ çº§ç†è§£æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå¹¶æå‡ºä¸¤ä¸ªæ–°çš„æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åœ¨æœªæ¥å—åƒç´ çº§ç›‘ç£è®­ç»ƒçš„MLLMsï¼Œåœ¨åƒç´ çº§ç†è§£å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šä¹Ÿèƒ½è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æ¢è®¨äº†â€œæœªæ¥å—åƒç´ çº§ç›‘ç£è®­ç»ƒçš„MLLMsä¸­ï¼Œä½•æ—¶ä¼šå‡ºç°æ¥åœ°ç°è±¡ï¼Ÿâ€è¿™ä¸€é—®é¢˜ï¼Œå¹¶å‘ç°æ¥åœ°å¯ä»¥ä¸å¯¹è±¡éƒ¨åˆ†æˆ–ä½ç½®&#x2F;å¤–è§‚ä¿¡æ¯ç›¸ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ­£åœ¨å‘åƒç´ çº§ç†è§£æ¨è¿›ï¼Œå±•ç¤ºå‡ºåœ¨å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²å’ŒåŸºäºåœ°é¢çš„å¯¹è¯ç”Ÿæˆæ–¹é¢çš„å¼ºå¤§æ€§èƒ½ã€‚</li>
<li>å½“å‰çš„è¶‹åŠ¿æ˜¯ä½¿ç”¨åƒç´ çº§åœ°é¢ç›‘ç£åœ¨å¤§è§„æ¨¡æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸€äº›MLLMsåœ¨è§†è§‰é—®ç­”èƒ½åŠ›æ–¹é¢è¡¨ç°è¾ƒå¼±ï¼Œç”šè‡³é™çº§äº†ä»æœªæ¥å—è¿‡æ­¤ç±»ç›‘ç£è®­ç»ƒçš„MLLMsçš„åœ°é¢èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸¤ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæœªæ¥å—åƒç´ çº§åœ°é¢ç›‘ç£çš„MLLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç®€å•çš„åŸºçº¿æ¥æå–åœ°é¢ä¿¡æ¯ï¼Œå¯ä»¥å°†å…¶æ’å…¥ä»»ä½•MLLMä¸­ï¼Œç§°ä¸ºPixFoundationã€‚</li>
<li>ç ”ç©¶äº†â€œæœªæ¥å—åƒç´ çº§ç›‘ç£è®­ç»ƒçš„MLLMsä¸­ï¼Œä½•æ—¶å‡ºç°æ¥åœ°ç°è±¡ï¼Ÿâ€è¿™ä¸€é—®é¢˜ï¼Œå¹¶å‘ç°æ¥åœ°ä¸å¯¹è±¡éƒ¨åˆ†æˆ–ä½ç½®&#x2F;å¤–è§‚ä¿¡æ¯æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-585d8d5bf48f7811a3f93a5aece995a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-663b887c5c31f3ea6e85e907d6b8524e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2332804f9267c49ca5f13d3da6b2d852.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47c8a151cd65fb9387cc8aaec98c9e99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bf9f55e823bec95cc37bc98253d11da.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UltraIF-Advancing-Instruction-Following-from-the-Wild"><a href="#UltraIF-Advancing-Instruction-Following-from-the-Wild" class="headerlink" title="UltraIF: Advancing Instruction Following from the Wild"></a>UltraIF: Advancing Instruction Following from the Wild</h2><p><strong>Authors:Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, Baobao Chang</strong></p>
<p>Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/kkk-an/UltraIF">https://github.com/kkk-an/UltraIF</a>. </p>
<blockquote>
<p>æŒ‡ä»¤éµå¾ªä½¿ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆä¸ºæœ‰ç”¨çš„åŠ©æ‰‹ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚æŒ‡ä»¤ä¸Šé©¾é©­LLMçš„å…³é”®ä»ç„¶ç¥ç§˜è«æµ‹ï¼Œå› ä¸ºç”±å¼€æºç¤¾åŒºè®­ç»ƒæ¨¡å‹å’Œç”±é¢†å…ˆå…¬å¸è®­ç»ƒæ¨¡å‹ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹æ³•UltraIFï¼Œç”¨äºæ„å»ºèƒ½å¤Ÿéµå¾ªå¤æ‚æŒ‡ä»¤çš„LLMï¼Œå¹¶ä½¿ç”¨å¼€æºæ•°æ®ã€‚UltraIFé¦–å…ˆä¼šå°†ç°å®ä¸–ç•Œä¸­çš„ç”¨æˆ·æç¤ºåˆ†è§£ä¸ºæ›´ç®€å•çš„æŸ¥è¯¢ã€çº¦æŸå’Œç›¸åº”çš„è¯„ä¼°é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªUltraComposerï¼Œå°†çº¦æŸç›¸å…³çš„æç¤ºä¸è¯„ä¼°é—®é¢˜ç»„åˆåœ¨ä¸€èµ·ã€‚è¿™ä¸ªæç¤ºä½œæ›²å®¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆæˆå¤æ‚çš„æŒ‡ä»¤ï¼Œå¹¶ä½¿ç”¨è¯„ä¼°é—®é¢˜è¿‡æ»¤å“åº”ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°ä½¿ç”¨ä»…8Bæ¨¡å‹ä½œä¸ºå“åº”ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œé¦–æ¬¡å®ç°äº†LLaMA-3.1-8B-Baseä¸æŒ‡ä»¤ç‰ˆæœ¬åœ¨äº”ä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¯¹é½ï¼Œä¸”ä¸ä½¿ç”¨ä»»ä½•åŸºå‡†æµ‹è¯•ä¿¡æ¯ã€‚å¯¹é½æ¨¡å‹åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå–å¾—äº†æœ‰ç«äº‰åŠ›çš„åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†UltraIFå¯ä»¥é€šè¿‡è‡ªæˆ‘å¯¹é½è¿›ä¸€æ­¥æ”¹è¿›LLaMA-3.1-8B-Instructï¼Œè¿™ä¸ºè¯¥æ–¹æ³•çš„åº”ç”¨æä¾›äº†æ›´å¹¿æ³›çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/kkk-an/UltraIF%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/kkk-an/UltraIFä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04153v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•UltraIFï¼Œç”¨äºæ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿéµå¾ªå¤æ‚çš„æŒ‡ä»¤å¹¶åˆ©ç”¨å¼€æºæ•°æ®ã€‚UltraIFé€šè¿‡å°†ç°å®ä¸–ç•Œç”¨æˆ·æŒ‡ä»¤åˆ†è§£ä¸ºç®€å•æŸ¥è¯¢ã€çº¦æŸå’Œç›¸åº”çš„è¯„ä¼°é—®é¢˜æ¥ç¼©å°æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªUltraComposeræ¥ç»„åˆçº¦æŸç›¸å…³çš„æç¤ºå’Œè¯„ä¼°é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°å°†LLaMA-3.1-8B-Baseä¸æŒ‡ä»¤ç‰ˆæœ¬å¯¹é½ï¼Œåœ¨äº”ä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä»…ä½¿ç”¨8Bæ¨¡å‹ä½œä¸ºå“åº”ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ã€‚æ­¤å¤–ï¼ŒUltraIFè¿˜å¯ä»¥é€šè¿‡è‡ªæˆ‘å¯¹é½è¿›ä¸€æ­¥ä¼˜åŒ–LLaMA-3.1-8B-Instructï¼Œä¸ºæ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯æä¾›åŠ¨åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UltraIFæ˜¯ä¸€ç§æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿéµå¾ªå¤æ‚çš„æŒ‡ä»¤ã€‚</li>
<li>UltraIFé€šè¿‡åˆ†è§£ç”¨æˆ·æŒ‡ä»¤ï¼Œå°†ç°å®ä¸–ç•Œä¸­çš„å¤æ‚æŒ‡ä»¤è½¬åŒ–ä¸ºç®€å•æŸ¥è¯¢ã€çº¦æŸå’Œè¯„ä¼°é—®é¢˜ã€‚</li>
<li>UltraIFè®­ç»ƒäº†ä¸€ä¸ªæç¤ºç»„åˆå™¨ï¼ˆUltraComposerï¼‰æ¥ç»„åˆçº¦æŸç›¸å…³çš„æç¤ºå’Œè¯„ä¼°é—®é¢˜ï¼Œä»è€Œåˆæˆå¤æ‚æŒ‡ä»¤å¹¶è¿‡æ»¤å“åº”ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒUltraIFæˆåŠŸåœ°å°†LLaMA-3.1-8B-Baseæ¨¡å‹ä¸æŒ‡ä»¤ç‰ˆæœ¬å¯¹é½ï¼Œåœ¨å¤šä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>UltraIFä»…ä½¿ç”¨8Bæ¨¡å‹ä½œä¸ºå“åº”ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œå®ç°äº†ä¸å…¶ä»–åŸºå‡†æµ‹è¯•çš„ç«äº‰æ€§èƒ½ã€‚</li>
<li>UltraIFèƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–LLaMA-3.1-8B-Instructæ¨¡å‹ï¼Œé€šè¿‡è‡ªæˆ‘å¯¹é½æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e02ba49b95344fbc05c6c104bee81db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-797300e41d9074577b6d1beaf95902fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc679a421a7ea57e1781f19956a697ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bbe5cdb75788966fb21077c00cd65c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3586c1b70f70ede470379a1136aefed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f0f1fdb6de2538fa211d3de55da92a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Llasa-Scaling-Train-Time-and-Inference-Time-Compute-for-Llama-based-Speech-Synthesis"><a href="#Llasa-Scaling-Train-Time-and-Inference-Time-Compute-for-Llama-based-Speech-Synthesis" class="headerlink" title="Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis"></a>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based   Speech Synthesis</h2><p><strong>Authors:Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue</strong></p>
<p>Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available. </p>
<blockquote>
<p>æœ€è¿‘æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯GPTç³»åˆ—å’ŒO1æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´è®¡ç®—ä¸Šçš„æ‰©å±•æ•ˆæœã€‚ç„¶è€Œï¼Œå½“å‰å…ˆè¿›TTSç³»ç»Ÿåˆ©ç”¨LLMé€šå¸¸æ˜¯å¤šé˜¶æ®µçš„ï¼Œéœ€è¦å•ç‹¬æ¨¡å‹ï¼ˆä¾‹å¦‚LLMä¹‹åçš„æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œè¿™å¢åŠ äº†åœ¨è®­ç»ƒæˆ–æµ‹è¯•æœŸé—´æ‰©å±•ç‰¹å®šæ¨¡å‹çš„å†³ç­–å¤æ‚æ€§ã€‚æœ¬ç ”ç©¶åšå‡ºäº†ä»¥ä¸‹è´¡çŒ®ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¯­éŸ³åˆæˆçš„è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´è®¡ç®—çš„æ‰©å±•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºè¯­éŸ³åˆæˆçš„ç®€å•æ¡†æ¶Llasaï¼Œå®ƒé‡‡ç”¨å•å±‚å‘é‡é‡åŒ–ï¼ˆVQï¼‰ç¼–è§£ç å™¨å’Œå•ä¸€Transformeræ¶æ„ï¼Œä¸æ ‡å‡†LLMï¼ˆå¦‚Llamaï¼‰å®Œå…¨å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰©å¤§Llasaçš„è®­ç»ƒæ—¶é—´è®¡ç®—æŒç»­æé«˜äº†åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆæ›´å¤æ‚å’Œå‡†ç¡®çš„è¯­è°ƒæ¨¡å¼ã€‚æ­¤å¤–ï¼Œä»æ‰©å¤§æ¨ç†æ—¶é—´è®¡ç®—çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åœ¨æœç´¢è¿‡ç¨‹ä¸­é‡‡ç”¨äº†è¯­éŸ³ç†è§£æ¨¡å‹ä½œä¸ºéªŒè¯å™¨ï¼Œå‘ç°æ‰©å¤§æ¨ç†æ—¶é—´è®¡ç®—ä¼šä½¿é‡‡æ ·æ¨¡å¼è½¬å‘ç‰¹å®šéªŒè¯å™¨çš„åå¥½ï¼Œä»è€Œæé«˜æƒ…æ„Ÿè¡¨ç°åŠ›ã€éŸ³è‰²ä¸€è‡´æ€§å’Œå†…å®¹å‡†ç¡®æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†TTSæ¨¡å‹ï¼ˆ1Bã€3Bã€8Bï¼‰å’Œç¼–è§£ç å™¨æ¨¡å‹çš„æ£€æŸ¥ç‚¹å’Œè®­ç»ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04128v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨ã€‚æ–‡ç« ä»‹ç»äº†LLMsåœ¨è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´è®¡ç®—æ‰©å±•æ–¹é¢çš„è¿›å±•ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºLlasaçš„ç®€å•è¯­éŸ³åˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨å•å±‚å‘é‡é‡åŒ–ç¼–ç å™¨å’Œå•ä¸€çš„Transformeræ¶æ„ä¸æ ‡å‡†LLMså¯¹é½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´è®¡ç®—å¯ä»¥æé«˜åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦å¹¶ç”Ÿæˆæ›´å¤æ‚çš„éŸµå¾‹æ¨¡å¼ã€‚åŒæ—¶ï¼Œåœ¨æ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢ï¼Œé‡‡ç”¨è¯­éŸ³ç†è§£æ¨¡å‹ä½œä¸ºéªŒè¯å™¨å¯ä»¥æ”¹å–„é‡‡æ ·æ¨¡å¼ï¼Œæé«˜æƒ…æ„Ÿè¡¨è¾¾ã€éŸ³è‰²ä¸€è‡´æ€§å’Œå†…å®¹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œä½œè€…å…¬å¼€äº†TTSæ¨¡å‹ï¼ˆ1Bã€3Bã€8Bï¼‰å’Œç¼–ç æ¨¡å‹çš„æ£€æŸ¥ç‚¹å’Œè®­ç»ƒä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æ¢è®¨äº†LLMsåœ¨è¯­éŸ³åˆæˆä¸­çš„åº”ç”¨åŠå…¶è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´è®¡ç®—çš„æ‰©å±•æ•ˆæœã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºLlasaçš„è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨å•å±‚å‘é‡é‡åŒ–ç¼–ç å™¨å’Œå•ä¸€çš„Transformeræ¶æ„ã€‚</li>
<li>å¢åŠ è®­ç»ƒæ—¶é—´è®¡ç®—å¯ä»¥æé«˜åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦å’Œç”Ÿæˆæ›´å¤æ‚çš„éŸµå¾‹æ¨¡å¼ã€‚</li>
<li>é‡‡ç”¨è¯­éŸ³ç†è§£æ¨¡å‹ä½œä¸ºéªŒè¯å™¨å¯ä»¥æ”¹å–„é‡‡æ ·æ¨¡å¼ï¼Œæé«˜æƒ…æ„Ÿè¡¨è¾¾ã€éŸ³è‰²ä¸€è‡´æ€§å’Œå†…å®¹å‡†ç¡®æ€§ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†ä½œè€…åœ¨æ¨ç†æ—¶é—´è®¡ç®—æ–¹é¢çš„ç ”ç©¶ç»“æœï¼ŒæŒ‡å‡ºå…¶å¯å½±å“é‡‡æ ·æ¨¡å¼ä»¥ç¬¦åˆç‰¹å®šéªŒè¯å™¨çš„åå¥½ã€‚</li>
<li>ä½œè€…å…¬å¼€äº†TTSæ¨¡å‹å’Œç¼–ç æ¨¡å‹çš„æ£€æŸ¥ç‚¹å’Œè®­ç»ƒä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4a497b86087d30975f7d2be28602b0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ef02b68accb03690d0fa15abb2b1409.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation"><a href="#DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation" class="headerlink" title="DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation"></a>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation</h2><p><strong>Authors:Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang</strong></p>
<p>Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness. </p>
<blockquote>
<p>è¿‘æœŸæœ‰å‡ é¡¹ç ”ç©¶å°è¯•ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ï¼Œåœ¨æ— ç¦»æ•£è¯­éŸ³æ ‡è®°çš„æƒ…å†µä¸‹ç”Ÿæˆè¿ç»­çš„è¯­éŸ³è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é¢ä¸´è®¡ç®—è´Ÿè½½è¿‡å¤§æˆ–ç»“æœä¸ç†æƒ³ç­‰æŒ‘æˆ˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£è½¬æ¢å™¨è‡ªå›å½’å»ºæ¨¡ï¼ˆDiTARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£è½¬æ¢å™¨çš„åŸºäºè¡¥ä¸çš„è‡ªå›å½’æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­æ ‡è®°çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œè¡¥ä¸ç”Ÿæˆï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹å¤„ç†èšåˆçš„è¡¥ä¸åµŒå…¥ï¼Œç„¶åæ‰©æ•£è½¬æ¢å™¨æ ¹æ®è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æè®®å°†æ¸©åº¦å®šä¹‰ä¸ºåœ¨åå‘æ‰©æ•£ODEè¿‡ç¨‹ä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹ï¼Œä»¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚åœ¨å¹¿æ³›çš„è§„æ¨¡åˆ†æä¸­ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜DiTARå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­ï¼ŒDiTARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03930v1">PDF</a> 16 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Diffusion Transformer Autoregressive Modelingï¼ˆDiTARï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å˜å‹å™¨ï¼Œé‡‡ç”¨åŸºäºè¡¥ä¸çš„è‡ªåŠ¨å›å½’æ¡†æ¶ï¼Œæœ‰æ•ˆæé«˜äº†è¿ç»­æ ‡è®°çš„è‡ªåŠ¨å›å½’æ¨¡å‹çš„æ•ˆç‡å¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œè¡¥ä¸ç”Ÿæˆï¼Œé€šè¿‡è¯­è¨€æ¨¡å‹å¤„ç†èšåˆçš„è¡¥ä¸åµŒå…¥ï¼Œç„¶åæ‰©æ•£å˜å‹å™¨åŸºäºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å®šä¹‰æ¸©åº¦æ¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ï¼Œæ¸©åº¦æ˜¯åœ¨åå‘æ‰©æ•£ODEä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹ã€‚DiTARåœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆæ–¹é¢å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨å¥å£®æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiTARç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å˜å‹å™¨ï¼Œæå‡ºäº†åŸºäºè¡¥ä¸çš„è‡ªåŠ¨å›å½’æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨æé«˜è¿ç»­æ ‡è®°çš„è‡ªåŠ¨å›å½’æ¨¡å‹çš„æ•ˆç‡å¹¶é™ä½è®¡ç®—éœ€æ±‚ã€‚</li>
<li>DiTARä½¿ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œè¡¥ä¸ç”Ÿæˆï¼Œé€šè¿‡è¯­è¨€æ¨¡å‹å¤„ç†è¡¥ä¸åµŒå…¥ã€‚</li>
<li>æ‰©æ•£å˜å‹å™¨åŸºäºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚</li>
<li>é€šè¿‡å®šä¹‰æ¸©åº¦æ¥å¹³è¡¡æ¨ç†è¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚</li>
<li>DiTARåœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ba16a3dd13b4fe07b3032a64829c9a3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa1a0942a07751f603ae646b5cd3489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deccac9101784d52c9f8d96b095792c2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Transformers-Boost-the-Performance-of-Decision-Trees-on-Tabular-Data-across-Sample-Sizes"><a href="#Transformers-Boost-the-Performance-of-Decision-Trees-on-Tabular-Data-across-Sample-Sizes" class="headerlink" title="Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes"></a>Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes</h2><p><strong>Authors:Mayuka Jayawardhana,  Renbo, Samuel Dooley, Valeriia Cherepanova, Andrew Gordon Wilson, Frank Hutter, Colin White, Tom Goldstein, Micah Goldblum</strong></p>
<p>Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on numerous tables for in-context learning, has demonstrated excellent performance for dataset sizes up to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically trained from scratch on each dataset without benefiting from pretraining data and must learn the relationships between columns from their entries alone since they lack natural language understanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is essential, yet they are not competitive with GBDTs on medium or large datasets, since their context lengths are limited. In this paper, we propose a simple and lightweight approach for fusing large language models and TabPFN with gradient-boosted decision trees, which allows scalable GBDTs to benefit from the natural language capabilities and pretraining of transformers. We name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at sufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on a wide range of dataset sizes in between. We demonstrate state-of-the-art performance against numerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best average performance among all methods we test for all but very small dataset sizes. We release our code at <a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°éå¸¸å‡ºè‰²ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä»æè¿°ç‰¹å¾å’Œæ ‡ç­¾çš„è‡ªç„¶è¯­è¨€åˆ—æ ‡é¢˜ä¸­æå–æ„ä¹‰ã€‚ä¸æ­¤ç±»ä¼¼ï¼ŒTabPFNæ˜¯ä¸€ä¸ªæœ€è¿‘çš„éLLMè½¬æ¢å™¨ï¼Œå®ƒåœ¨ä¼—å¤šè¡¨æ ¼ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç”¨äºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶å·²æ˜¾ç¤ºå‡ºåœ¨å¤šè¾¾ä¸€åƒä¸ªæ ·æœ¬çš„æ•°æ®é›†ä¸Šçš„å‡ºè‰²è¡¨ç°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¢¯åº¦æå‡å†³ç­–æ ‘ï¼ˆGBDTsï¼‰é€šå¸¸æ˜¯åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šä»å¤´å¼€å§‹è®­ç»ƒçš„ï¼Œæ— æ³•å—ç›Šäºé¢„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å®ƒä»¬åªèƒ½å•ç‹¬ä»æ¡ç›®ä¸­å­¦ä¹ åˆ—ä¹‹é—´çš„å…³ç³»ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹è‡ªç„¶è¯­è¨€ç†è§£ã€‚LLMså’ŒTabPFNåœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†è‡³å…³é‡è¦ï¼Œä½†åœ¨ä¸­ç­‰æˆ–å¤§å‹æ•°æ®é›†ä¸Šï¼Œå®ƒä»¬ä¸GBDTsç›¸æ¯”å¹¶ä¸å…·å¤‡ç«äº‰åŠ›ï¼Œå› ä¸ºå®ƒä»¬çš„ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯æœ‰é™çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œè½»é‡çº§çš„èåˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒTabPFNä¸æ¢¯åº¦æå‡å†³ç­–æ ‘çš„æ–¹æ³•ï¼Œè¿™ä½¿å¾—å¯æ‰©å±•çš„GBDTså—ç›Šäºè½¬æ¢å™¨çš„è‡ªç„¶è¯­è¨€èƒ½åŠ›å’Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„èåˆæ–¹æ³•åˆ†åˆ«å‘½åä¸ºLLM-Boostå’ŒPFN-Boostã€‚è™½ç„¶åœ¨å°æ•°æ®é›†ä¸ŠLLM-Boostå’ŒPFN-Boostçš„æ€§èƒ½ä¸è½¬æ¢å™¨ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼Œå¹¶ä¸”åœ¨è¶³å¤Ÿå¤§çš„æ•°æ®é›†ä¸Šè¶…è¶ŠGBDTsçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ä¸€ç³»åˆ—ä¸­ç­‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¿‡äº†ä¸¤ç§ç‹¬ç«‹ç»„ä»¶ã€‚æˆ‘ä»¬ä¸ä¼—å¤šåŸºå‡†çº¿å’Œé›†æˆç®—æ³•ç›¸æ¯”ï¼Œå±•ç¤ºäº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°é™¤äº†éå¸¸å°çš„æ•°æ®é›†å¤–ï¼Œåœ¨æ‰€æœ‰æµ‹è¯•çš„æ•°æ®é›†å¤§å°ä¸­ï¼ŒPFN-Boostçš„å¹³å‡æ€§èƒ½è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost">http://github.com/MayukaJ/LLM-Boost</a>å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02672v2">PDF</a> 12 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>LLMsä¸TabPFNåœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¾—ç›Šäºå®ƒä»¬å¯¹è‡ªç„¶è¯­è¨€åˆ—æ ‡é¢˜çš„ç†è§£å’Œé¢„è®­ç»ƒçš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œåœ¨ä¸­ç­‰æˆ–å¤§å‹æ•°æ®é›†ä¸Šï¼Œå®ƒä»¬ä¸å¦‚æ¢¯åº¦å¢å¼ºå†³ç­–æ ‘ï¼ˆGBDTsï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•è½»é‡çº§çš„èåˆæ–¹æ³•ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸TabPFNä¸æ¢¯åº¦å¢å¼ºå†³ç­–æ ‘ç›¸ç»“åˆï¼Œä½¿GBDTså—ç›Šäºtransformersçš„è‡ªç„¶è¯­è¨€èƒ½åŠ›å’Œé¢„è®­ç»ƒã€‚èåˆæ–¹æ³•LLM-Boostå’ŒPFN-Booståœ¨å¤šç§æ•°æ®é›†å¤§å°ä¸Šè¡¨ç°å‡ºè¶…è¶Šå•ä¸€ç»„ä»¶çš„æ€§èƒ½ï¼Œå…¶ä¸­PFN-Booståœ¨é™¤æå°æ•°æ®é›†å¤–çš„æ‰€æœ‰æµ‹è¯•æ–¹æ³•ä¸­å¹³å‡æ€§èƒ½æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’ŒTabPFNåœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå› ä¸ºå®ƒä»¬èƒ½åˆ©ç”¨è‡ªç„¶è¯­è¨€åˆ—æ ‡é¢˜çš„æ„ä¹‰å’Œé¢„è®­ç»ƒçš„ä¼˜åŠ¿ã€‚</li>
<li>æ¢¯åº¦å¢å¼ºå†³ç­–æ ‘ï¼ˆGBDTsï¼‰é€šå¸¸åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ä¾èµ–é¢„è®­ç»ƒæ•°æ®ï¼Œä½†å®ƒä»¬åœ¨ç†è§£åˆ—å…³ç³»æ–¹é¢ä¾èµ–æ•°æ®æ¡ç›®ã€‚</li>
<li>LLMså’ŒTabPFNåœ¨ä¸­ç­‰æˆ–å¤§å‹æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œä¸»è¦å› ä¸ºå®ƒä»¬å¤„ç†ä¸Šä¸‹æ–‡çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§èåˆLLMså’ŒTabPFNä¸æ¢¯åº¦å¢å¼ºå†³ç­–æ ‘çš„ç®€å•è½»é‡çº§æ–¹æ³•ï¼Œåä¸ºLLM-Boostå’ŒPFN-Boostã€‚</li>
<li>LLM-Boostå’ŒPFN-Booståœ¨å„ç§æ•°æ®é›†å¤§å°ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†å•ä¸€çš„LLMsã€TabPFNå’ŒGBDTsã€‚</li>
<li>PFN-Booståœ¨é™¤æå°æ•°æ®é›†å¤–çš„æ‰€æœ‰æµ‹è¯•æ–¹æ³•ä¸­å¹³å‡æ€§èƒ½æœ€ä½³ã€‚</li>
<li>ç ”ç©¶ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="http://github.com/MayukaJ/LLM-Boost%E3%80%82">http://github.com/MayukaJ/LLM-Boostã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04f2e981d7961f2e058e74efce737041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557f160475fc9ac7bb65d414f94dda85.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Relaxed-Recursive-Transformers-Effective-Parameter-Sharing-with-Layer-wise-LoRA"><a href="#Relaxed-Recursive-Transformers-Effective-Parameter-Sharing-with-Layer-wise-LoRA" class="headerlink" title="Relaxed Recursive Transformers: Effective Parameter Sharing with   Layer-wise LoRA"></a>Relaxed Recursive Transformers: Effective Parameter Sharing with   Layer-wise LoRA</h2><p><strong>Authors:Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster</strong></p>
<p>Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit â€œlayer tyingâ€ as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller â€œRecursive Transformersâ€ that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines â€“ and can even recover most of the performance of the original â€œfull-sizeâ€ model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éƒ¨ç½²æˆæœ¬å¾ˆé«˜ã€‚å‚æ•°å…±äº«ä¸ºå®ç°é™ä½å…¶è§„æ¨¡å’Œæˆæœ¬æä¾›äº†å¯èƒ½çš„è·¯å¾„ï¼Œä½†åœ¨ç°ä»£LLMä¸­çš„æ•ˆæœä»ç„¶ç›¸å½“æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°è®¿é—®äº†Transformerä¸­çš„â€œå±‚ç»‘å®šâ€ä½œä¸ºå‚æ•°å…±äº«çš„ä¸€ç§å½¢å¼ï¼Œå¹¶ä»‹ç»äº†å°†ç°æœ‰LLMè½¬æ¢ä¸ºè¾ƒå°çš„â€œé€’å½’Transformerâ€çš„æ–°æ–¹æ³•ï¼Œè¿™äº›é€’å½’Transformeråœ¨å±‚ä¹‹é—´å…±äº«å‚æ•°ï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±æœ€å°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„é€’å½’Transformeræ˜¯ä»æ ‡å‡†é¢„è®­ç»ƒTransformeræœ‰æ•ˆåˆå§‹åŒ–çš„ï¼Œä½†åªä½¿ç”¨ä¸€ä¸ªå”¯ä¸€çš„å—å±‚ï¼Œç„¶ååœ¨å¾ªç¯ä¸­å¤šæ¬¡é‡å¤ã€‚é€šè¿‡å¼•å…¥å…·æœ‰æ·±åº¦æ–¹å‘ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—çš„çµæ´»é€’å½’Transformerï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œä»ç„¶ä¿æŒäº†æ•´ä½“æ¨¡å‹çš„ç´§å‡‘æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„é€’å½’æ¨¡å‹ï¼ˆä¾‹å¦‚é€’å½’Gemma 1Bï¼‰ä¼˜äºç±»ä¼¼å¤§å°çš„æ™®é€šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚TinyLlama 1.1Bå’ŒPythia 1Bï¼‰å’ŒçŸ¥è¯†è’¸é¦åŸºçº¿â€”â€”ç”šè‡³å¯ä»¥æ¢å¤åŸå§‹â€œå…¨å°ºå¯¸â€æ¨¡å‹çš„å¤§éƒ¨åˆ†æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œæ— å…±äº«å‚æ•°çš„Gemma 2Bï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è¿ç»­æ·±åº¦æ–¹å‘æ‰¹å¤„ç†ï¼Œè¿™æ˜¯ç”±é€’å½’Transformerä¸æ—©æœŸé€€å‡ºç›¸ç»“åˆè€Œå®ç°çš„ä¸€ç§æœ‰å‰é€”çš„æ–°æ¨ç†èŒƒå¼ã€‚åœ¨ç†è®ºåˆ†æä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™æœ‰å¯èƒ½å¯¼è‡´æ¨ç†ååé‡å®ç°æ˜¾è‘—ï¼ˆ2-3å€ï¼‰çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20672v2">PDF</a> ICLR 2025; 47 pages, 17 figures, 17 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²æˆæœ¬é«˜æ˜‚ï¼Œå‚æ•°å…±äº«æˆä¸ºé™ä½å…¶è§„æ¨¡ä¸æˆæœ¬çš„å¯èƒ½é€”å¾„ä¹‹ä¸€ï¼Œä½†åœ¨ç°ä»£LLMä¸­çš„æ•ˆæœæœ‰é™ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†Transformerä¸­çš„â€œå±‚ç»‘å®šâ€ä½œä¸ºå‚æ•°å…±äº«æ–¹å¼ï¼Œå¹¶å¼•å…¥æ–°æ–¹æ³•å°†ç°æœ‰LLMè½¬æ¢ä¸ºæ›´å°çš„â€œé€’å½’Transformerâ€ï¼Œåœ¨æ€§èƒ½æŸå¤±æœ€å°çš„æƒ…å†µä¸‹å®ç°è·¨å±‚å‚æ•°å…±äº«ã€‚é€’å½’Transformerä»æ ‡å‡†é¢„è®­ç»ƒTransformeré«˜æ•ˆåˆå§‹åŒ–ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªç‹¬ç‰¹å±‚å—ï¼Œåœ¨å¾ªç¯ä¸­å¤šæ¬¡é‡å¤ã€‚é€šè¿‡å¼•å…¥çµæ´»çš„â€œæ¾å¼›é€’å½’Transformerâ€å’Œæ·±åº¦ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—ï¼Œæˆ‘ä»¬æé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„ç´§å‡‘æ€§ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„é€’å½’æ¨¡å‹ï¼ˆå¦‚é€’å½’Gemma 1Bï¼‰åœ¨ç±»ä¼¼å¤§å°çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚TinyLlama 1.1Bå’ŒPythia 1Bï¼‰å’ŒçŸ¥è¯†è’¸é¦åŸºå‡†æµ‹è¯•ä¸­éƒ½è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç”šè‡³å¯ä»¥æ¢å¤å¤§éƒ¨åˆ†åŸâ€œå…¨å°ºå¯¸â€æ¨¡å‹ï¼ˆå¦‚æ²¡æœ‰å…±äº«å‚æ•°çš„Gemma 2Bï¼‰çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç”±é€’å½’Transformerä¸æ—©æœŸé€€å‡ºç›¸ç»“åˆå®ç°çš„â€œè¿ç»­æ·±åº¦åˆ†æ‰¹å¤„ç†â€è¿™ä¸€æœ‰å‰æ™¯çš„æ–°æ¨ç†èŒƒå¼ï¼Œå¹¶åœ¨ç†è®ºåˆ†æä¸­æ˜¾ç¤ºå…¶æœ‰æ½œåŠ›å®ç°æ¨ç†ååé‡çš„æ˜¾è‘—ï¼ˆ2-3å€ï¼‰æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å±‚ç»‘å®šï¼ˆå‚æ•°å…±äº«ï¼‰æ¥å‡å°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„æ¨¡å¹¶é™ä½æˆæœ¬ã€‚</li>
<li>é€šè¿‡å¼•å…¥é€’å½’Transformeræ¨¡å‹ï¼Œå®ç°äº†åœ¨æ€§èƒ½æŸå¤±æœ€å°åŒ–æƒ…å†µä¸‹çš„è·¨å±‚å‚æ•°å…±äº«ã€‚</li>
<li>é€’å½’Transformerèƒ½å¤Ÿä»æ ‡å‡†é¢„è®­ç»ƒTransformeræ¨¡å‹ä¸­é«˜æ•ˆåˆå§‹åŒ–ï¼Œå¹¶åªä½¿ç”¨ä¸€ä¸ªç‹¬ç‰¹å±‚å—è¿›è¡Œå¤šæ¬¡é‡å¤ã€‚</li>
<li>å¼•å…¥äº†çµæ´»çš„æ¾å¼›é€’å½’Transformerï¼Œé€šè¿‡æ·±åº¦ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ç´§å‡‘ã€‚</li>
<li>é€’å½’æ¨¡å‹åœ¨ç±»ä¼¼å¤§å°çš„é¢„è®­ç»ƒæ¨¡å‹å’ŒçŸ¥è¯†è’¸é¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>é€’å½’æ¨¡å‹èƒ½å¤Ÿæ¢å¤å¤§éƒ¨åˆ†å…¨å°ºå¯¸æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d2891ed6231da395b30efe1ed1a8bb42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03d1482a558dfbf22aaab98782f6fc91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fed3902a230970a56b83b9b877209a71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddb3b1f04877fb02ad8be7e50715109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eaf858d3bc72062db466888df68fa2e7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Bench4Merge-A-Comprehensive-Benchmark-for-Merging-in-Realistic-Dense-Traffic-with-Micro-Interactive-Vehicles"><a href="#Bench4Merge-A-Comprehensive-Benchmark-for-Merging-in-Realistic-Dense-Traffic-with-Micro-Interactive-Vehicles" class="headerlink" title="Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense   Traffic with Micro-Interactive Vehicles"></a>Bench4Merge: A Comprehensive Benchmark for Merging in Realistic Dense   Traffic with Micro-Interactive Vehicles</h2><p><strong>Authors:Zhengming Wang, Junli Wang, Pengfei Li, Zhaohan Li, Peng Li, Yilun Chen</strong></p>
<p>While the capabilities of autonomous driving have advanced rapidly, merging into dense traffic remains a significant challenge, many motion planning methods for this scenario have been proposed but it is hard to evaluate them. Most existing closed-loop simulators rely on rule-based controls for other vehicles, which results in a lack of diversity and randomness, thus failing to accurately assess the motion planning capabilities in highly interactive scenarios. Moreover, traditional evaluation metrics are insufficient for comprehensively evaluating the performance of merging in dense traffic. In response, we proposed a closed-loop evaluation benchmark for assessing motion planning capabilities in merging scenarios. Our approach involves other vehicles trained in large scale datasets with micro-behavioral characteristics that significantly enhance the complexity and diversity. Additionally, we have restructured the evaluation mechanism by leveraging large language models to assess each autonomous vehicle merging onto the main road. Extensive experiments have demonstrated the advanced nature of this evaluation benchmark. Through this benchmark, we have obtained an evaluation of existing methods and identified common issues. The environment and vehicle motion planning models we have designed can be accessed at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Bench4Merge-EB5D">https://anonymous.4open.science/r/Bench4Merge-EB5D</a> </p>
<blockquote>
<p>è™½ç„¶è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†åœ¨æ‹¥å µçš„äº¤é€šç¯å¢ƒä¸­æ±‡å…¥ä»æ˜¯å·¨å¤§çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€åœºæ™¯ï¼Œå·²æå‡ºäº†è®¸å¤šè¿åŠ¨è§„åˆ’æ–¹æ³•ï¼Œä½†éš¾ä»¥è¿›è¡Œè¯„ä¼°ã€‚ç›®å‰å¤§å¤šæ•°å°é—­å¾ªç¯æ¨¡æ‹Ÿå™¨ä¾èµ–äºåŸºäºè§„åˆ™çš„è½¦è¾†æ§åˆ¶ï¼Œå¯¼è‡´ç¼ºä¹å¤šæ ·æ€§å’Œéšæœºæ€§ï¼Œå› æ­¤æ— æ³•å‡†ç¡®è¯„ä¼°åœ¨é«˜åº¦äº¤äº’åœºæ™¯ä¸­è¿åŠ¨è§„åˆ’çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡ä¸è¶³ä»¥å…¨é¢è¯„ä¼°æ‹¥å µäº¤é€šç¯å¢ƒä¸­çš„åˆå¹¶æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå°é—­å¾ªç¯è¯„ä¼°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°åˆå¹¶åœºæ™¯ä¸­çš„è¿åŠ¨è§„åˆ’èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒå…¶ä»–è½¦è¾†ï¼Œä»¥å…·æœ‰å¾®è§‚è¡Œä¸ºç‰¹å¾çš„æ–¹å¼æ˜¾è‘—å¢å¼ºå¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¯¹æ¯è¾†æ±‡å…¥ä¸»å¹²é“çš„è‡ªä¸»è½¦è¾†è¿›è¡Œè¯„ä¼°æ¥é‡å»ºè¯„ä¼°æœºåˆ¶ã€‚å¤§é‡å®éªŒè¯æ˜æ­¤è¯„ä¼°åŸºå‡†å…·æœ‰å…ˆè¿›æ€§ã€‚é€šè¿‡æ­¤åŸºå‡†ï¼Œæˆ‘ä»¬è·å¾—äº†å¯¹ç°æœ‰æ–¹æ³•çš„è¯„ä¼°å¹¶å‘ç°äº†å¸¸è§é—®é¢˜ã€‚æˆ‘ä»¬è®¾è®¡çš„ç¯å¢ƒå’Œè½¦è¾†è¿åŠ¨è§„åˆ’æ¨¡å‹å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Bench4Merge-EB5D%E3%80%82">https://anonymous.4open.science/r/Bench4Merge-EB5Dã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15912v2">PDF</a> 6 pages, 7 figures, on submitted</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨é©¾é©¶æŠ€æœ¯è™½å‘å±•è¿…é€Ÿï¼Œä½†åœ¨å¯†é›†äº¤é€šåœºæ™¯ä¸­å¹¶å…¥ä¸»æµä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯„ä¼°å…¶è¿åŠ¨è§„åˆ’èƒ½åŠ›çš„æ–¹æ³•å¤šæ ·ä½†éš¾ä»¥è¯„ä¼°å…¶æ•ˆæœã€‚ç°æœ‰çš„é—­ç¯æ¨¡æ‹Ÿå™¨åŸºäºè§„åˆ™æ§åˆ¶å…¶ä»–è½¦è¾†ï¼Œç¼ºä¹å¤šæ ·æ€§å’Œéšæœºæ€§ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°é«˜åº¦äº¤äº’åœºæ™¯ä¸­çš„è¿åŠ¨è§„åˆ’èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé—­ç¯è¯„ä¼°åŸºå‡†ï¼Œé‡‡ç”¨å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒå…¶ä»–è½¦è¾†å¾®è¡Œä¸ºç‰¹å¾ï¼Œæé«˜äº†å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°æ„å»ºè¯„ä¼°æœºåˆ¶ï¼Œä»¥è¯„ä¼°æ¯è¾†è‡ªä¸»è½¦è¾†å¹¶å…¥ä¸»è·¯çš„æƒ…å†µã€‚å®éªŒè¯æ˜äº†è¯¥è¯„ä¼°åŸºå‡†çš„å…ˆè¿›æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶åœ¨å¯†é›†äº¤é€šåœºæ™¯ä¸­å¹¶å…¥ä¸»æµä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è¿åŠ¨è§„åˆ’æ–¹æ³•çš„è¯„ä¼°å›°éš¾ï¼Œå› ä¸ºç¼ºä¹å‡†ç¡®å’Œå…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ¨¡æ‹Ÿå™¨ç¼ºä¹å¤šæ ·æ€§å’Œéšæœºæ€§ï¼Œæ— æ³•å‡†ç¡®åæ˜ å®é™…äº¤é€šæƒ…å†µã€‚</li>
<li>æå‡ºçš„é—­ç¯è¯„ä¼°åŸºå‡†é‡‡ç”¨å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒå…¶ä»–è½¦è¾†çš„å¾®è¡Œä¸ºç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°æ„å»ºè¯„ä¼°æœºåˆ¶ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°è‡ªä¸»è½¦è¾†å¹¶å…¥ä¸»è·¯çš„æƒ…å†µã€‚</li>
<li>å®éªŒè¯æ˜äº†è¯¥è¯„ä¼°åŸºå‡†çš„å…ˆè¿›æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b02ffaa54ce4cf17cfd0711c7bc331b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9454a0ca4bf921cd945115aec103fa83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69384c0ec873f6fd008a12a0323470f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84e30a7f09286efa1cf41448d08f5069.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bb429fae6102fbd69f1626a20fbf55b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f36a2d8e76e0926d403f9ceb905cf08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c5e7c6db566ef4f0d72318d63121f6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14bed0380e5dd191220c28a65b6302f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b395ca705dd4a11ac08de70b86a2d18e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="G-Designer-Architecting-Multi-agent-Communication-Topologies-via-Graph-Neural-Networks"><a href="#G-Designer-Architecting-Multi-agent-Communication-Topologies-via-Graph-Neural-Networks" class="headerlink" title="G-Designer: Architecting Multi-agent Communication Topologies via Graph   Neural Networks"></a>G-Designer: Architecting Multi-agent Communication Topologies via Graph   Neural Networks</h2><p><strong>Authors:Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, Tianlong Chen, Dawei Cheng</strong></p>
<p>Recent advancements in large language model (LLM)-based agents have demonstrated that collective intelligence can significantly surpass the capabilities of individual agents, primarily due to well-crafted inter-agent communication topologies. Despite the diverse and high-performing designs available, practitioners often face confusion when selecting the most effective pipeline for their specific task: \textit{Which topology is the best choice for my task, avoiding unnecessary communication token overhead while ensuring high-quality solution?} In response to this dilemma, we introduce G-Designer, an adaptive, efficient, and robust solution for multi-agent deployment, which dynamically designs task-aware, customized communication topologies. Specifically, G-Designer models the multi-agent system as a multi-agent network, leveraging a variational graph auto-encoder to encode both the nodes (agents) and a task-specific virtual node, and decodes a task-adaptive and high-performing communication topology. Extensive experiments on six benchmarks showcase that G-Designer is: \textbf{(1) high-performing}, achieving superior results on MMLU with accuracy at $84.50%$ and on HumanEval with pass@1 at $89.90%$; \textbf{(2) task-adaptive}, architecting communication protocols tailored to task difficulty, reducing token consumption by up to $95.33%$ on HumanEval; and \textbf{(3) adversarially robust}, defending against agent adversarial attacks with merely $0.3%$ accuracy drop. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººçš„è¿›å±•è¡¨æ˜ï¼Œé›†ä½“æ™ºèƒ½å¯ä»¥æ˜¾è‘—è¶…è¶Šä¸ªä½“ä»£ç†äººçš„èƒ½åŠ›ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç²¾å¿ƒè®¾è®¡çš„ä»£ç†äººä¹‹é—´çš„é€šä¿¡æ‹“æ‰‘ã€‚å°½ç®¡å­˜åœ¨å„ç§é«˜æ€§èƒ½çš„è®¾è®¡æ–¹æ¡ˆï¼Œä½†ä»ä¸šè€…åœ¨é€‰æ‹©é’ˆå¯¹å…¶ç‰¹å®šä»»åŠ¡çš„æœ€æœ‰æ•ˆç®¡é“æ—¶å¸¸å¸¸æ„Ÿåˆ°å›°æƒ‘ï¼šé’ˆå¯¹æˆ‘çš„ä»»åŠ¡é€‰æ‹©å“ªç§æ‹“æ‰‘æ˜¯æœ€å¥½çš„é€‰æ‹©ï¼ŒåŒæ—¶é¿å…ä¸å¿…è¦çš„é€šä¿¡ä»¤ç‰Œå¼€é”€å¹¶ç¡®ä¿é«˜è´¨é‡çš„è§£å†³æ–¹æ¡ˆï¼Ÿé’ˆå¯¹è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†G-Designerï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”ã€é«˜æ•ˆä¸”ç¨³å¥çš„å¤šä»£ç†éƒ¨ç½²è§£å†³æ–¹æ¡ˆï¼Œå®ƒå¯åŠ¨æ€è®¾è®¡ä»»åŠ¡æ„ŸçŸ¥çš„è‡ªå®šä¹‰é€šä¿¡æ‹“æ‰‘ã€‚å…·ä½“è€Œè¨€ï¼ŒG-Designerå°†å¤šä»£ç†ç³»ç»Ÿå»ºæ¨¡ä¸ºå¤šä»£ç†ç½‘ç»œï¼Œåˆ©ç”¨å˜åˆ†å›¾è‡ªåŠ¨ç¼–ç å™¨å¯¹èŠ‚ç‚¹ï¼ˆä»£ç†ï¼‰å’Œç‰¹å®šä»»åŠ¡çš„è™šæ‹ŸèŠ‚ç‚¹è¿›è¡Œç¼–ç ï¼Œå¹¶è§£ç å‡ºä»»åŠ¡è‡ªé€‚åº”ä¸”é«˜æ€§èƒ½çš„é€šä¿¡æ‹“æ‰‘ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒG-Designerå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰é«˜æ€§èƒ½ï¼Œåœ¨MMLUä¸Šçš„å‡†ç¡®åº¦è¾¾åˆ°84.50%ï¼Œåœ¨HumanEvalä¸Šçš„pass@1è¾¾åˆ°89.90%ï¼›ï¼ˆ2ï¼‰ä»»åŠ¡è‡ªé€‚åº”ï¼Œé’ˆå¯¹ä»»åŠ¡éš¾åº¦æ„å»ºé€šä¿¡åè®®ï¼Œåœ¨HumanEvalä¸Šå°†ä»¤ç‰Œæ¶ˆè€—å‡å°‘é«˜è¾¾95.33%ï¼›ï¼ˆ3ï¼‰å¯¹æŠ—æ€§ç¨³å¥ï¼ŒæŠµå¾¡ä»£ç†äººå¯¹æŠ—æ€§æ”»å‡»æ—¶å‡†ç¡®ç‡ä»…ä¸‹é™0.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11782v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸLLMæ¨¡å‹çš„ç ”ç©¶è¡¨æ˜ï¼Œé›†ä½“æ™ºèƒ½é€šè¿‡å·§å¦™çš„è·¨æ™ºèƒ½ä½“é€šä¿¡æ‹“æ‰‘è®¾è®¡å¯ä»¥å¤§å¹…è¶…è¶Šä¸ªä½“æ™ºèƒ½çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­é€‰æ‹©åˆé€‚çš„é€šä¿¡æ‹“æ‰‘æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§è‡ªé€‚åº”ã€é«˜æ•ˆä¸”ç¨³å¥çš„å¤šæ™ºèƒ½ä½“éƒ¨ç½²è§£å†³æ–¹æ¡ˆâ€”â€”G-Designerã€‚å®ƒèƒ½æ ¹æ®ä»»åŠ¡åŠ¨æ€ç”Ÿæˆå®šåˆ¶åŒ–çš„é€šä¿¡æ‹“æ‰‘ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯å…¶åœ¨æ€§èƒ½ã€ä»»åŠ¡é€‚åº”æ€§å’Œå¯¹æŠ—ç¨³å¥æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹è¡¨æ˜é›†ä½“æ™ºèƒ½èƒ½å¤Ÿè¶…è¶Šä¸ªä½“æ™ºèƒ½çš„è¡¨ç°ï¼Œè¿™å¾—ç›Šäºç²¾å¿ƒè®¾è®¡çš„æ™ºèƒ½ä½“é—´é€šä¿¡æ‹“æ‰‘ã€‚</li>
<li>å®è·µè€…åœ¨é€‰æ‹©é€šä¿¡æ‹“æ‰‘æ—¶é¢ä¸´å›°æƒ‘ï¼Œéœ€è¦ä¸€ç§è§£å†³æ–¹æ¡ˆèƒ½æ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>G-Designeræ˜¯ä¸€ç§è‡ªé€‚åº”ã€é«˜æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åŠ¨æ€è®¾è®¡ä»»åŠ¡æ„ŸçŸ¥çš„å®šåˆ¶é€šä¿¡æ‹“æ‰‘ã€‚</li>
<li>G-Designerå°†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå»ºæ¨¡ä¸ºå¤šæ™ºèƒ½ä½“ç½‘ç»œï¼Œå¹¶åˆ©ç”¨å˜åˆ†å›¾è‡ªç¼–ç å™¨å¯¹èŠ‚ç‚¹ï¼ˆæ™ºèƒ½ä½“ï¼‰å’Œä»»åŠ¡ç‰¹å®šè™šæ‹ŸèŠ‚ç‚¹è¿›è¡Œç¼–ç ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒG-Designeråœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¦‚åœ¨MMLUä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†84.5%ï¼Œåœ¨HumanEvalä¸Šçš„pass@1è¾¾åˆ°äº†89.9%ã€‚</li>
<li>G-Designerå…·æœ‰ä»»åŠ¡é€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡éš¾åº¦å®šåˆ¶é€šä¿¡åè®®ï¼Œå¹¶åœ¨HumanEvalä¸Šå‡å°‘äº†é«˜è¾¾95.33%çš„ä»¤ç‰Œæ¶ˆè€—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-deb8d8cfabcb0344e9524501b8bc5ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de0bf946f29989862a861f7a12646de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314d413102920e01d6db2bd3dd30e42f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HiRT-Enhancing-Robotic-Control-with-Hierarchical-Robot-Transformers"><a href="#HiRT-Enhancing-Robotic-Control-with-Hierarchical-Robot-Transformers" class="headerlink" title="HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers"></a>HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers</h2><p><strong>Authors:Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen</strong></p>
<p>Large Vision-Language-Action (VLA) models, leveraging powerful pre trained Vision-Language Models (VLMs) backends, have shown promise in robotic control due to their impressive generalization ability. However, the success comes at a cost. Their reliance on VLM backends with billions of parameters leads to high computational costs and inference latency, limiting the testing scenarios to mainly quasi-static tasks and hindering performance in dynamic tasks requiring rapid interactions. To address these limitations, this paper proposes HiRT, a Hierarchical Robot Transformer framework that enables flexible frequency and performance trade-off. HiRT keeps VLMs running at low frequencies to capture temporarily invariant features while enabling real-time interaction through a high-frequency vision-based policy guided by the slowly updated features. Experiment results in both simulation and real-world settings demonstrate significant improvements over baseline methods. Empirically, in static tasks, we double the control frequency and achieve comparable success rates. Additionally, on novel real-world dynamic ma nipulation tasks which are challenging for previous VLA models, HiRT improves the success rate from 48% to 75%. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åˆ©ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åç«¯ï¼Œç”±äºå…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœºå™¨äººæ§åˆ¶æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒæˆåŠŸæ˜¯æœ‰ä»£ä»·çš„ã€‚å®ƒä»¬å¯¹æ‹¥æœ‰æ•°åäº¿å‚æ•°çš„VLMåç«¯çš„ä¾èµ–å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿï¼Œè¿™é™åˆ¶äº†æµ‹è¯•åœºæ™¯ä¸»è¦ä¸ºå‡†é™æ€ä»»åŠ¡ï¼Œå¹¶åœ¨éœ€è¦å¿«é€Ÿäº¤äº’çš„åŠ¨æ€ä»»åŠ¡ä¸­å½±å“äº†æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†HiRTï¼Œä¸€ä¸ªåˆ†å±‚æœºå™¨äººè½¬æ¢å™¨æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°çµæ´»é¢‘ç‡å’Œæ€§èƒ½æƒè¡¡ã€‚HiRTä¿æŒVLMä»¥è¾ƒä½é¢‘ç‡è¿è¡Œä»¥æ•è·æš‚æ—¶ä¸å˜çš„ç‰¹å¾ï¼ŒåŒæ—¶é€šè¿‡ç”±ç¼“æ…¢æ›´æ–°çš„ç‰¹å¾å¼•å¯¼çš„é«˜é¢‘è§†è§‰ç­–ç•¥å®ç°å®æ—¶äº¤äº’ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ã€‚åœ¨é™æ€ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†æ§åˆ¶é¢‘ç‡æé«˜äº†ä¸€å€ï¼Œå¹¶å®ç°äº†ç›¸å½“çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œå¯¹äºä»¥å‰å¯¹VLAæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°å‹çœŸå®ä¸–ç•ŒåŠ¨æ€æ“ä½œä»»åŠ¡ï¼ŒHiRTå°†æˆåŠŸç‡ä»48%æé«˜åˆ°75%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05273v3">PDF</a> Accepted to CORL 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åç«¯ï¼Œåœ¨æœºå™¨äººæ§åˆ¶é¢†åŸŸå±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§å‰æ™¯ã€‚ç„¶è€Œï¼Œå…¶æˆåŠŸèƒŒåå­˜åœ¨è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œæ¨ç†å»¶è¿Ÿçš„é—®é¢˜ï¼Œé™åˆ¶äº†æµ‹è¯•åœºæ™¯ä¸»è¦ä¸ºé™æ€ä»»åŠ¡ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦å¿«é€Ÿäº¤äº’çš„åŠ¨æ€ä»»åŠ¡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºHiRTï¼Œä¸€ç§åˆ†å±‚æœºå™¨äººè½¬æ¢å™¨æ¡†æ¶ï¼Œå¯å®ç°çµæ´»é¢‘ç‡å’Œæ€§èƒ½æƒè¡¡ã€‚HiRTä½¿VLMä»¥è¾ƒä½é¢‘ç‡è¿è¡Œä»¥æ•æ‰æš‚æ—¶ä¸å˜çš„ç‰¹å¾ï¼ŒåŒæ—¶é€šè¿‡ç”±ç¼“æ…¢æ›´æ–°çš„ç‰¹å¾å¼•å¯¼çš„é«˜é¢‘è§†è§‰ç­–ç•¥å®ç°å®æ—¶äº¤äº’ã€‚ä»¿çœŸå’ŒçœŸå®ç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒHiRTåœ¨é™æ€ä»»åŠ¡ä¸Šå®ç°äº†æ§åˆ¶é¢‘ç‡çš„ç¿»å€ï¼Œå¹¶è·å¾—äº†ç›¸å½“çš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œåœ¨ä¹‹å‰å¯¹VLAæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°ç°å®åŠ¨æ€æ“ä½œä»»åŠ¡ä¸Šï¼ŒHiRTå°†æˆåŠŸç‡ä»48%æé«˜åˆ°75%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹VLAæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„VLMåç«¯å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚</li>
<li>VLAæ¨¡å‹é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿçš„é—®é¢˜ã€‚</li>
<li>HiRTæ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°çµæ´»é¢‘ç‡å’Œæ€§èƒ½æƒè¡¡ã€‚</li>
<li>HiRTé€šè¿‡ä½¿VLMä»¥ä½é¢‘ç‡è¿è¡Œæ•æ‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨é«˜é¢‘è§†è§‰ç­–ç•¥å®ç°å®æ—¶äº¤äº’ã€‚</li>
<li>åœ¨é™æ€ä»»åŠ¡ä¸Šï¼ŒHiRTç›¸æ¯”åŸºå‡†æ–¹æ³•å®ç°äº†æ§åˆ¶é¢‘ç‡ç¿»å€ã€‚</li>
<li>åœ¨åŠ¨æ€ä»»åŠ¡ä¸Šï¼ŒHiRTæ˜¾è‘—æé«˜äº†ä¹‹å‰çš„VLAæ¨¡å‹çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69a7375d24ea8b6ba9ccb5395e8d5a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75ca8f8c7248118af7da629777d665e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c33e05999e815526c5fd3285f4b415a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bbc4ee696cb1c6e488c2ea72c69aea5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Parameter-Efficient-Tuning-Framework-for-Language-guided-Object-Grounding-and-Robot-Grasping"><a href="#A-Parameter-Efficient-Tuning-Framework-for-Language-guided-Object-Grounding-and-Robot-Grasping" class="headerlink" title="A Parameter-Efficient Tuning Framework for Language-guided Object   Grounding and Robot Grasping"></a>A Parameter-Efficient Tuning Framework for Language-guided Object   Grounding and Robot Grasping</h2><p><strong>Authors:Houjian Yu, Mingen Li, Alireza Rezazadeh, Yang Yang, Changhyun Choi</strong></p>
<p>The language-guided robot grasping task requires a robot agent to integrate multimodal information from both visual and linguistic inputs to predict actions for target-driven grasping. While recent approaches utilizing Multimodal Large Language Models (MLLMs) have shown promising results, their extensive computation and data demands limit the feasibility of local deployment and customization. To address this, we propose a novel CLIP-based multimodal parameter-efficient tuning (PET) framework designed for three language-guided object grounding and grasping tasks: (1) Referring Expression Segmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring Grasp Affordance (RGA). Our approach introduces two key innovations: a bi-directional vision-language adapter that aligns multimodal inputs for pixel-level language understanding and a depth fusion branch that incorporates geometric cues to facilitate robot grasping predictions. Experiment results demonstrate superior performance in the RES object grounding task compared with existing CLIP-based full-model tuning or PET approaches. In the RGS and RGA tasks, our model not only effectively interprets object attributes based on simple language descriptions but also shows strong potential for comprehending complex spatial reasoning scenarios, such as multiple identical objects present in the workspace. Project page: <a target="_blank" rel="noopener" href="https://z.umn.edu/etog-etrg">https://z.umn.edu/etog-etrg</a> </p>
<blockquote>
<p>è¯­è¨€å¼•å¯¼æœºå™¨äººæŠ“å–ä»»åŠ¡éœ€è¦æœºå™¨äººä»£ç†æ•´åˆè§†è§‰å’Œè¯­è¨€è¾“å…¥çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»¥é¢„æµ‹ç›®æ ‡é©±åŠ¨çš„æŠ“å–åŠ¨ä½œã€‚è™½ç„¶æœ€è¿‘é‡‡ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ–¹æ³•å·²ç»å–å¾—äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å…¶åºå¤§çš„è®¡ç®—å’Œæ•°æ®å¤„ç†éœ€æ±‚é™åˆ¶äº†å…¶åœ¨æœ¬åœ°éƒ¨ç½²å’Œå®šåˆ¶åŒ–çš„å¯è¡Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„å¤šæ¨¡æ€å‚æ•°é«˜æ•ˆè°ƒæ•´ï¼ˆPETï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨ç”¨äºä¸‰é¡¹è¯­è¨€å¼•å¯¼çš„å¯¹è±¡å®šä½å’ŒæŠ“å–ä»»åŠ¡ï¼šï¼ˆ1ï¼‰æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰ï¼Œï¼ˆ2ï¼‰æŒ‡ä»£æŠ“å–åˆæˆï¼ˆRGSï¼‰ï¼Œï¼ˆ3ï¼‰æŒ‡ä»£æŠ“å–å±æ€§ï¼ˆRGAï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€ä¸ªåŒå‘è§†è§‰è¯­è¨€é€‚é…å™¨ï¼Œç”¨äºå¯¹é½å¤šæ¨¡æ€è¾“å…¥ä»¥å®ç°åƒç´ çº§è¯­è¨€ç†è§£ï¼›ä¸€ä¸ªæ·±åº¦èåˆåˆ†æ”¯ï¼Œç”¨äºç»“åˆå‡ ä½•çº¿ç´¢ä»¥ä¿ƒè¿›æœºå™¨äººæŠ“å–é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨RESå¯¹è±¡å®šä½ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„åŸºäºCLIPçš„å…¨æ¨¡å‹è°ƒæ•´æˆ–PETæ–¹æ³•ã€‚åœ¨RGSå’ŒRGAä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…å¯ä»¥æ ¹æ®ç®€å•çš„è¯­è¨€æè¿°æœ‰æ•ˆåœ°è§£é‡Šå¯¹è±¡å±æ€§ï¼Œè€Œä¸”è¿˜æ˜¾ç¤ºå‡ºåœ¨å¤æ‚çš„ç©ºé—´æ¨ç†åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¦‚å·¥ä½œç©ºé—´ä¸­å­˜åœ¨å¤šä¸ªç›¸åŒå¯¹è±¡çš„æƒ…å†µã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://z.umn.edu/etog-etrg%E3%80%82">https://z.umn.edu/etog-etrgã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19457v3">PDF</a> Accepted for ICRA 2025. Project page:   <a target="_blank" rel="noopener" href="https://sites.google.com/umn.edu/etog-etrg/home">https://sites.google.com/umn.edu/etog-etrg/home</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­è¨€å¼•å¯¼æœºå™¨äººæŠ“å–ä»»åŠ¡ï¼Œéœ€è¦æœºå™¨äººä»£ç†èåˆè§†è§‰å’Œè¯­è¨€è¾“å…¥çš„å¤šæ¨¡æ€ä¿¡æ¯æ¥é¢„æµ‹ç›®æ ‡é©±åŠ¨æŠ“å–çš„åŠ¨ä½œã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºCLIPçš„å¤šæ¨¡æ€å‚æ•°é«˜æ•ˆè°ƒæ•´ï¼ˆPETï¼‰æ¡†æ¶ï¼Œç”¨äºè¯­è¨€å¼•å¯¼å¯¹è±¡å®šä½å’ŒæŠ“å–ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å¼•å…¥åŒå‘è§†è§‰è¯­è¨€é€‚é…å™¨å’Œæ·±åº¦èåˆåˆ†æ”¯ï¼Œåˆ†åˆ«ç”¨äºåƒç´ çº§è¯­è¨€ç†è§£å’Œæœºå™¨äººæŠ“å–é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¯¹è±¡å®šä½ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨æŠ“å–åˆæˆå’ŒæŠ“å–å±æ€§ä»»åŠ¡ä¸Šå…·æœ‰å¼ºå¤§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€å¼•å¯¼æœºå™¨äººæŠ“å–ä»»åŠ¡éœ€è¦èåˆè§†è§‰å’Œè¯­è¨€è¾“å…¥çš„å¤šæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„å¤šæ¨¡æ€å‚æ•°é«˜æ•ˆè°ƒæ•´ï¼ˆPETï¼‰æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…å«åŒå‘è§†è§‰è¯­è¨€é€‚é…å™¨å’Œæ·±åº¦èåˆåˆ†æ”¯ã€‚</li>
<li>é€‚é…å™¨ç”¨äºåƒç´ çº§è¯­è¨€ç†è§£ï¼Œè€Œèåˆåˆ†æ”¯ç»“åˆäº†å‡ ä½•çº¿ç´¢ä»¥æ”¯æŒæœºå™¨äººæŠ“å–é¢„æµ‹ã€‚</li>
<li>åœ¨å¯¹è±¡å®šä½ä»»åŠ¡ä¸Šï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰çš„CLIPå…¨æ¨¡å‹è°ƒä¼˜æˆ–PETæ–¹æ³•ã€‚</li>
<li>åœ¨æŠ“å–åˆæˆå’ŒæŠ“å–å±æ€§ä»»åŠ¡ä¸Šï¼Œè¯¥æ¨¡å‹ä¸ä»…èƒ½å¤ŸåŸºäºç®€å•è¯­è¨€æè¿°è§£é‡Šå¯¹è±¡å±æ€§ï¼Œè¿˜å±•ç°å‡ºå¤„ç†å¤æ‚ç©ºé—´æ¨ç†åœºæ™¯çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-516be3f761cb0cc389b7cd491d398c3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8fd4c59b72ad158e1481fb4390bb1ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23bf83d90ffd0401253f0ea255d4d7cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8086199daa6c0f449f09fdcc7761eb98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d53e6765344a22f849e6be58d3ecc05.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AgentGen-Enhancing-Planning-Abilities-for-Large-Language-Model-based-Agent-via-Environment-and-Task-Generation"><a href="#AgentGen-Enhancing-Planning-Abilities-for-Large-Language-Model-based-Agent-via-Environment-and-Task-Generation" class="headerlink" title="AgentGen: Enhancing Planning Abilities for Large Language Model based   Agent via Environment and Task Generation"></a>AgentGen: Enhancing Planning Abilities for Large Language Model based   Agent via Environment and Task Generation</h2><p><strong>Authors:Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jianguang Lou, Qingwei Lin, Ping Luo, Saravan Rajmohan</strong></p>
<p>Large Language Model-based agents have garnered significant attention and are becoming increasingly popular. Furthermore, planning ability is a crucial component of an LLM-based agent, which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLMs through instruction tuning, referred to as agent training. Recent studies have demonstrated that utilizing expert-level trajectory for instruction-tuning LLMs effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. We introduce a framework, AgentGen, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, Bi-Evol, that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve. The evaluation results derived from AgentBoard show that AgentGen greatly improves LLMsâ€™ planning ability, e.g., the AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall performance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves state-of-the-art results in planning tasks. Project page: <a target="_blank" rel="noopener" href="https://agent-gen.github.io/">https://agent-gen.github.io/</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†å·²ç»å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œå¹¶è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æ­¤å¤–ï¼Œè§„åˆ’èƒ½åŠ›æ˜¯åŸºäºLLMçš„ä»£ç†çš„é‡è¦ç»„æˆï¼Œè¿™é€šå¸¸æ¶‰åŠä»åˆå§‹çŠ¶æ€å®ç°é¢„æœŸç›®æ ‡ã€‚æœ¬æ–‡ç ”ç©¶äº†é€šè¿‡æŒ‡ä»¤è°ƒæ•´å¢å¼ºLLMçš„è§„åˆ’èƒ½åŠ›ï¼Œç§°ä¸ºä»£ç†è®­ç»ƒã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ä¸“å®¶çº§è½¨è¿¹å¯¹LLMè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºå®ƒä»¬çš„è§„åˆ’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä»æ‰‹åŠ¨è®¾è®¡çš„è§„åˆ’ä»»åŠ¡å’Œç¯å¢ƒä¸­åˆæˆè½¨è¿¹ã€‚åˆ›å»ºè¿™äº›ç¯å¢ƒå’Œä»»åŠ¡çš„åŠ³åŠ¨å¯†é›†å‹ç‰¹æ€§é˜»ç¢äº†è¶³å¤Ÿå¤šæ ·å’Œå¹¿æ³›çš„è½¨è¿¹çš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æ¢è®¨äº†è‡ªåŠ¨åŒ–åˆæˆå¤šæ ·ç¯å¢ƒå’Œä»ç®€å•åˆ°å¤æ‚çš„é€æ¸è§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºAgentGençš„æ¡†æ¶ï¼Œå®ƒé¦–å…ˆåˆ©ç”¨LLMç”Ÿæˆç¯å¢ƒï¼Œç„¶åæ ¹æ®è¿™äº›ç¯å¢ƒç”Ÿæˆè§„åˆ’ä»»åŠ¡ã€‚ä¸ºäº†æé«˜ç¯å¢ƒå¤šæ ·æ€§ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ç”±å„ç§é¢†åŸŸç‰¹å®šçš„æ–‡æœ¬ç‰‡æ®µç»„æˆçš„çµæ„Ÿè¯­æ–™åº“ä½œä¸ºåˆæˆç¯å¢ƒçš„ä¸Šä¸‹æ–‡ã€‚è€Œä¸”ï¼Œä¸ºäº†æé«˜ç”Ÿæˆè§„åˆ’ä»»åŠ¡çš„éš¾åº¦å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒå‘è¿›åŒ–æ–¹æ³•Bi-Evolï¼Œè¯¥æ–¹æ³•ä»å®¹æ˜“å’Œå›°éš¾çš„ä¸¤ä¸ªæ–¹å‘è¿›åŒ–è§„åˆ’ä»»åŠ¡ï¼Œä»¥åˆæˆä¸€ä¸ªéš¾åº¦æ›²çº¿æ›´å¹³æ»‘çš„ä»»åŠ¡é›†ã€‚æ¥è‡ªAgentBoardçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAgentGenæå¤§åœ°æé«˜äº†LLMçš„è§„åˆ’èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œç»è¿‡AgentGenæŒ‡ä»¤è°ƒæ•´çš„Llama-3.1-8Båœ¨æ•´ä½“æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-3.5ã€‚æ­¤å¤–ï¼Œä½¿ç”¨AgentGenè°ƒæ•´çš„Llama-3.1-70Bæ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://agent-gen.github.io/%E3%80%82">https://agent-gen.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00764v3">PDF</a> Accepted by KDD 2025 (Research Track). Project page:   <a target="_blank" rel="noopener" href="https://agent-gen.github.io/">https://agent-gen.github.io/</a></p>
<p><strong>æ‘˜è¦</strong><br>    å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿‘å¹´æ¥å—åˆ°å¹¿æ³›å…³æ³¨å¹¶é€æ¸æµè¡Œèµ·æ¥ã€‚è®¡åˆ’èƒ½åŠ›æ˜¯LLMä¸­çš„ä¸€ä¸ªå…³é”®éƒ¨åˆ†ï¼Œé€šå¸¸éœ€è¦å®ç°æ—¢å®šçš„ç›®æ ‡çŠ¶æ€ã€‚æœ¬ç ”ç©¶é€šè¿‡æŒ‡ä»¤å¾®è°ƒæ¥æå‡LLMçš„è®¡åˆ’èƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨ä¸“å®¶çº§åˆ«çš„è½¨è¿¹å¯¹LLMè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¯ä»¥å¢å¼ºå…¶è§„åˆ’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä»æ‰‹åŠ¨è®¾è®¡çš„è§„åˆ’å’Œç¯å¢ƒä¸­åˆæˆè½¨è¿¹ï¼Œè¿™ç§åŠ³åŠ¨å¯†é›†å‹çš„ç‰¹æ€§é™åˆ¶äº†è½¨è¿¹çš„å¤šæ ·æ€§å’Œå¹¿æ³›æ€§ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†è‡ªåŠ¨åŒ–åˆæˆå¤šæ ·åŒ–çš„ç¯å¢ƒå’Œä¸€ç³»åˆ—è§„åˆ’ä»»åŠ¡ï¼Œä»ç®€å•åˆ°å¤æ‚ã€‚å¼•å…¥AgentGenæ¡†æ¶ï¼Œé¦–å…ˆåˆ©ç”¨LLMç”Ÿæˆç¯å¢ƒï¼Œç„¶åæ ¹æ®è¿™äº›ç¯å¢ƒç”Ÿæˆè§„åˆ’ä»»åŠ¡ã€‚ä¸ºæé«˜ç¯å¢ƒå¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨åŒ…å«å„ç§é¢†åŸŸç‰¹å®šæ–‡æœ¬ç‰‡æ®µçš„çµæ„Ÿè¯­æ–™åº“ä½œä¸ºåˆæˆç¯å¢ƒçš„ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ï¼Œä¸ºæé«˜ç”Ÿæˆè§„åˆ’ä»»åŠ¡çš„éš¾åº¦å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŒå‘è¿›åŒ–æ–¹æ³•Bi-Evolï¼Œè¯¥æ–¹æ³•ä»ç®€å•å’Œå›°éš¾ä¸¤ä¸ªæ–¹å‘è¿›åŒ–è§„åˆ’ä»»åŠ¡ï¼Œåˆæˆå¹³æ»‘çš„éš¾åº¦æ›²çº¿ä»»åŠ¡é›†ã€‚æ¥è‡ªAgentBoardçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒAgentGenæå¤§åœ°æé«˜äº†LLMçš„è§„åˆ’èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨AgentGenæŒ‡ä»¤è°ƒæ•™çš„Llama-3.1-8Båœ¨æ•´ä½“æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-3.5ã€‚æ­¤å¤–ï¼Œä½¿ç”¨AgentGenè°ƒæ•™çš„Llama-3.1-70Bæ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://agent-gen.github.io/">https://agent-gen.github.io/</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºä»£ç†çš„è®­ç»ƒæ­£åœ¨å—åˆ°å…³æ³¨ï¼Œå› ä¸ºè¿™ç§è®­ç»ƒæ–¹å¼å¯ä»¥æå‡æ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ä¸“å®¶çº§åˆ«çš„è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒè¢«è¯å®æ˜¯å¢å¼ºLLMè§„åˆ’èƒ½åŠ›çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å½“å‰çš„å·¥ä½œä¸»è¦é¢ä¸´çš„é—®é¢˜æ˜¯æ‰‹åŠ¨è®¾è®¡è§„åˆ’å’Œç¯å¢ƒï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢è´¹æ—¶åˆè´¹åŠ›ï¼Œé™åˆ¶äº†è½¨è¿¹çš„å¤šæ ·æ€§å’Œå¹¿æ³›æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶AgentGenï¼Œè¯¥æ¡†æ¶å¯ä»¥è‡ªåŠ¨ç”Ÿæˆç¯å¢ƒå’Œè§„åˆ’ä»»åŠ¡ï¼Œä»è€Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>AgentGené€šè¿‡ä½¿ç”¨çµæ„Ÿè¯­æ–™åº“æ¥æé«˜ç¯å¢ƒå¤šæ ·æ€§ï¼Œè¯¥è¯­æ–™åº“åŒ…å«å„ç§é¢†åŸŸç‰¹å®šçš„æ–‡æœ¬ç‰‡æ®µã€‚</li>
<li>ä¸ºäº†å¢åŠ ç”Ÿæˆè§„åˆ’ä»»åŠ¡çš„éš¾åº¦å¤šæ ·æ€§ï¼ŒAgentGené‡‡ç”¨äº†åŒå‘è¿›åŒ–æ–¹æ³•Bi-Evolï¼Œä½¿å¾—ç”Ÿæˆçš„è§„åˆ’ä»»åŠ¡ä»ç®€å•åˆ°å¤æ‚ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨AgentGenè°ƒæ•™çš„LLMæ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæœªä½¿ç”¨è¯¥æ–¹æ³•è°ƒæ•™çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f88b6b22a5d267cce15a4a2f615c05c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-385a1672019a23f0e62a5d5edc8b4f83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e3d5a785d9a07f32cb1efb2f6d9841d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c11e1618fc1d72ffd8d1059e16b3d732.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-08/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4e3d5a785d9a07f32cb1efb2f6d9841d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  ScoreFlow Mastering LLM Agent Workflows via Score-based Preference   Optimization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ba55b6d1d0ccd514f951f37b4ebc020e.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-07  Distilling Implicit Multimodal Knowledge into Large Language Models for   Zero-Resource Dialogue Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
